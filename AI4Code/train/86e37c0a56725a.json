{"cell_type":{"d8ce861d":"code","be02beb6":"code","166a0b0e":"code","207828c4":"code","c6d0a951":"code","a086d1cc":"code","826de6d4":"code","4250d315":"code","41334c36":"code","82d9edd3":"code","8feebb13":"code","137f3dd7":"code","35cc3ba7":"code","892b15aa":"code","84d9242b":"code","f651cf86":"code","19e85e96":"code","740d1e9a":"code","01f7a1ee":"code","a6395c63":"code","176f070b":"code","33c1a127":"code","4d8b0def":"code","261b4b56":"code","11ae56c3":"code","801ed98a":"code","8f946346":"code","efb3d381":"code","f21a6eea":"code","0bb6d3a4":"code","6833c141":"code","5d01a497":"code","fa49d5e2":"markdown","042a85a5":"markdown","c3e42e59":"markdown","a34d040d":"markdown","a560c267":"markdown","2824c1d6":"markdown","6fac4f76":"markdown","d0045d50":"markdown","4a175332":"markdown","e8a69f2e":"markdown","770857fd":"markdown","28884de3":"markdown","b82e97be":"markdown","81fcaf71":"markdown","8a94eff5":"markdown","4b75a34b":"markdown","71b7cfb4":"markdown","30de8dc7":"markdown","babe10a3":"markdown","6d277500":"markdown","ab074d94":"markdown"},"source":{"d8ce861d":"#!\/usr\/bin\/python\n\nimport os\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nimport math\nfrom scipy.stats import pearsonr, ttest_ind#, binom_test\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# # # IterativeImputer Experimental, default parameters may change without notice.\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, OrdinalEncoder, FunctionTransformer, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom functools import partial\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\nSEED = 8675389","be02beb6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","166a0b0e":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Set index.\n# # Or, drop it in the pipeline?\ntrain_df.set_index(keys=['PassengerId'], inplace=True)\ntest_df.set_index(keys=['PassengerId'], inplace=True)\n\n# Peel off labels.\nX_train_df = train_df.drop(columns=['Survived'])\ny_train_df = train_df[['Survived']]\n\ny_train_df.head()\ny_train_df.info()\ny_train_df.describe()\nX_train_df.head()\nX_train_df.info()\nX_train_df.describe()\n\ntest_df.head()\ntest_df.info()\ntest_df.describe()","207828c4":"# Impute to mode (Sex, Age)\nsimp_imp_mode = SimpleImputer(strategy='most_frequent')","c6d0a951":"# Iteratively impute Fare and Pclass, and SibSp and Parch\n# # Listing all parameters because it's experimental and subject to change without notice.\niter_imp_mode = IterativeImputer(estimator=ExtraTreesRegressor(), initial_strategy='most_frequent',\n                        random_state=SEED, # Current defaults follow, but keep missing_values=np.nan:\n                        missing_values=np.nan, sample_posterior=False, max_iter=10, tol=0.001,\n                        n_nearest_features=None, imputation_order='ascending',\n                        skip_complete=False, min_value=-np.inf, max_value=np.inf, verbose=0,\n                        add_indicator=False)\n\n# Make it one step for the pipeline.\n# # Index {0:Pclass, 1:Name, 2:Sex, 3:Age, 4:SibSp, 5:Parch, 6:Ticket, 7:Fare, 8:Cabin, 9:Embarked}\ndrop_and_imp_ct = ColumnTransformer([\n    ('drop', 'drop', [1, 6, 8, 9]),           # Drop Name, Ticket, Cabin, Embarked.\n    ('simp_imp_mode', simp_imp_mode, [2, 3]), # Impute Sex and Age to most frequent.\n    ('iter_imp_fam', iter_imp_mode, [4, 5]),  # Multivariately impute SibSp and Parch.\n    ('iter_imp_class', iter_imp_mode, [0, 7]) # Multivariately impute Pclass and Fare.\n])\n\nX_train_arr = drop_and_imp_ct.fit_transform(X_train_df)\nX_train_arr\n# # New index: {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass, 5:Fare}","a086d1cc":"onehot_bin_enc = OneHotEncoder(drop='if_binary')","826de6d4":"# # {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass, 5:Fare}\nbin_age_disc = KBinsDiscretizer(n_bins=8, strategy='uniform', encode='ordinal')\n_ = bin_age_disc.fit_transform(X_train_arr[:, 1].reshape(-1, 1))\nprint(\"Age binned by decade, roughly:\")\nbin_age_disc.bin_edges_","4250d315":"# make_bin_ft = np.vectorize(lambda x: 0 if x == 0 else 1)\n# # Can't pickle lambda, even if it's wrapped.\n# # And, can't pickle redefined functions.\n\ndef make_bin(x):\n    return 0 if x == 0 else 1 # Instead of bool(x), to cover bad data.\nmake_bin_vec = np.vectorize(make_bin)\nmake_bin_ft = FunctionTransformer(make_bin_vec)","41334c36":"# Make it one step for the pipeline.\n# # {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass, 5:Fare}\nkeep_orginals_ct = ColumnTransformer([# Leave out Sex, since it's a string object.\n    ('drop', 'drop', [0]),            # Do it explicitly for easier tracking.\n    ('passthrough', 'passthrough', slice(1, 6))\n])\nmake_cats_ct = ColumnTransformer([\n    ('onehot_bin_enc', onehot_bin_enc, [0]), # One-hot Sex.\n    ('bin_age_disc', bin_age_disc, [1]),     # Bin Age by decade, roughly.  \n    ('make_bin_ft', make_bin_ft, [2, 3]),    # Make SibSp and Parch binary.\n    ('drop', 'drop', [4]),                   # Not going to engineer Pclass, other than scaling.\n    ('passthrough', 'passthrough', [5]),     # Passthrough Fare, explicitly to maintain order\/readability.\n])\n\nmake_cats_keep_orig_fu = FeatureUnion([\n    ('keep_orginals_ct', keep_orginals_ct),\n    ('make_cats_ct', make_cats_ct)\n])\n\nX_train_arr = make_cats_keep_orig_fu.fit_transform(X_train_arr)\nX_train_arr\n\n# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}","82d9edd3":"# Log scale Fare\ndef log_it(x):\n    return np.log10(x) if x > 0 else 0\nlog10_vec = np.vectorize(log_it)\nlog10_ft = FunctionTransformer(log10_vec)\n\nlog10_ct = ColumnTransformer([\n    ('passthrough', 'passthrough', slice(0, 9)),\n    ('log10_ft', log10_ft, [9])\n])\n\nX_train_arr = log10_ct.fit_transform(X_train_arr)\nX_train_arr\n\n# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}","8feebb13":"# MinMax scale everything.\nX_train_arr = MinMaxScaler(copy=False).fit_transform(X_train_arr)\nX_train_arr\n\n# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}","137f3dd7":"# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}\n\nprint(\"Correlation of familied features (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 1], X_train_arr[:, 2])\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 7], X_train_arr[:, 8])\nprint(\"\\n\")\n\nprint(\"Correlation of SibSp to survival (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 1], y_train_df['Survived'].values)\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 7], y_train_df['Survived'].values)\nprint(\"\\n\")\n\nprint(\"Correlation of Parch to survival (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 2], y_train_df['Survived'].values)\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 8], y_train_df['Survived'].values)\nprint(\"\\n\")\n\nprint(\"Surival rates of each combination of familiedness:\")\ndf = pd.DataFrame(X_train_arr,\n                  columns=['Age', 'SibSp', 'Parch', 'Pclass', 'Fare', 'Sex_eng', 'Age_eng',\n                           'SibSp_eng', 'Parch_eng', 'Fare_eng'])\ndf['Survived'] = y_train_df['Survived'].values\ndf.groupby(['SibSp_eng', 'Parch_eng']).mean()['Survived']","35cc3ba7":"# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}\n\nprint(\"Correlation of class features (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 3], X_train_arr[:, 4])\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 3], X_train_arr[:, 9])\nprint(\"\\n\")\n\nprint(\"Correlation of Pclass to survival (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 3], y_train_df['Survived'].values)\nprint(\"\\n\")\n\nprint(\"Correlation of Fare to survival (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 4], y_train_df['Survived'].values)\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 9], y_train_df['Survived'].values)\nprint(\"\\n\")\n\nprint(\"Correlation of (Pclass * Fare) to survival (Pearson's R, p-value):\")\nprint(\"Original:\")\npearsonr(X_train_arr[:, 3] * X_train_arr[:, 4], y_train_df['Survived'].values)\nprint(\"Engineered:\")\npearsonr(X_train_arr[:, 3] * X_train_arr[:, 9], y_train_df['Survived'].values)","892b15aa":"plot = sns.stripplot(y=df['Fare'].values, x=df['Pclass'].values, hue=df['Survived'].values,\n                     jitter=True, alpha=.3)\nplot.set(xlabel='Pclass', ylabel='Fare')","84d9242b":"plot = sns.stripplot(y=df['Fare_eng'].values, x=df['Pclass'].values, hue=df['Survived'].values,\n                     jitter=True, alpha=.3)\nplot.set(xlabel='Pclass', ylabel='Fare_eng')","f651cf86":"# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}\n\nsel_kb = SelectKBest(k=10).fit(X_train_arr, y_train_df['Survived'])\n\n# # Why does this produce all NaNs?\n# pd.DataFrame(columns={'scores': sel_kb.scores_, 'p-values': sel_kb.pvalues_},\n#              index=['Age', 'SibSp', 'Parch', 'Pclass', 'Fare', 'Sex', 'Age_eng', 'SibSp_eng',\n#                     'Parch_eng', 'Fare_eng']).sort_values(by='scores', ascending=False)\n\npd.DataFrame([sel_kb.scores_, sel_kb.pvalues_],\n             columns=['Age', 'SibSp', 'Parch', 'Pclass', 'Fare',\n                      'Sex', 'Age_eng', 'SibSp_eng', 'Parch_eng', 'Fare_eng'],\n             index=['scores', 'p-values']).T.sort_values(by='scores', ascending=False)","19e85e96":"# # New index: {       0:Age, 1:SibSp, 2:Parch, 3:Pclass, 4:Fare, <= Original features.\n# Engineered => 5:Sex, 6:Age, 7:SibSp, 8:Parch,           9:Fare}\n\ndrop_ct = ColumnTransformer([\n    ('passthrough', 'passthrough', [0, 3, 5, 7, 8])\n])\nX_train_arr = drop_ct.fit_transform(X_train_arr)\nX_train_arr\n\n# # New index: {0:Age, 1:Pclass, 2:Sex, 3:SibSp, 4:Parch}","740d1e9a":"X_train_arr = PCA(copy=False, random_state=SEED).fit_transform(X_train_arr)\nX_train_arr\n# # New index: {0:Age, 1:Pclass, 2:Sex, 3:SibSp, 4:Parch}","01f7a1ee":"def run_skl(method, X_train, y_train, X_test, y_test, perf_series, **kwargs):\n    '''Train and test sklearn supervised ML models, and get scores.\n    Parameters:\n        method: callable sklearn supervised ML model.\n        X_train: training feature matrix.\n        y_train: training label 1D array. Series may throw warning.\n        X_test: testing feature matrix.\n        y_test: testing label 1D array. Series may throw warning.\n        per_series: {str} Name of series\/column (name of model).\n        **kwargs: arguments to pass to method.\n    Returns:\n        perf_sr: pandas Series of model and metrics\n            (i.e. confusion matrix, sklearn.metrics.precision_recall_fscore_support results, weighted).\n    '''\n\n    perf_sr = None\n    \n    clf = method(**kwargs)\n    clf = clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    conf = confusion_matrix(y_true=y_test, y_pred=pred)\n    prf = precision_recall_fscore_support(y_true=y_test, y_pred=pred)\n    \n    perf_sr = pd.Series(\n        {\n            'model': clf,\n            'neg_prec': prf[0][0],\n            'pos_prec': prf[0][1],\n            'neg_rec': prf[1][0],\n            'pos_rec': prf[1][1],\n            'neg_f': prf[2][0],\n            'pos_f': prf[2][1],\n            'neg_sup': prf[3][0],\n            'pos_sup': prf[3][1],\n            't_neg': conf[0][0],\n            'f_neg': conf[0][1],\n            'f_pos': conf[1][0],\n            't_pos': conf[1][1]\n        },\n        name=perf_series\n    )\n    \n    return perf_sr","a6395c63":"# Make partial callables preloaded with arguments as needed.\nDecisionTreeClassifier_partial = partial(DecisionTreeClassifier, random_state=SEED)\nSVC_partial = partial(SVC, kernel='rbf')\nLogisticRegression_partial = partial(LogisticRegression, random_state=SEED)\nRandomForestClassifier_partial = partial(RandomForestClassifier, random_state=SEED)\nAdaBoostClassifier_partial = partial(AdaBoostClassifier, random_state=SEED)\nGradientBoostingClassifier_partial = partial(GradientBoostingClassifier, random_state=SEED)\n\nmodels_dict = {\n    'knn_clf': KNeighborsClassifier,\n    'dt_clf': DecisionTreeClassifier_partial,\n    'svc_clf': SVC_partial,\n    'logreg_clf': LogisticRegression_partial,\n    'gnb_clf': GaussianNB,\n    'rf_clf': RandomForestClassifier_partial,\n    'ab_clf': AdaBoostClassifier_partial,\n    'gbc_clf': GradientBoostingClassifier_partial\n}\n\nreport_df = pd.DataFrame(columns=models_dict.keys())\n\nfor key, clf in models_dict.items():\n    perf_ser = run_skl(method=clf, X_train=X_train_arr, y_train=y_train_df['Survived'].values,\n                       X_test=X_train_arr, y_test=y_train_df['Survived'].values, perf_series=key)\n    report_df[perf_ser.name] = perf_ser\n    \nreport_df","176f070b":"# params = {\n#     'n_estimators': [25, 50, 100, 150, 175], # default 100\n#     'max_depth': [None, 2, 4, 8, 16, 32, 64, 128], # default None\n#     'min_samples_split': [2, 4, 8, 16], # default 2\n#     'min_samples_leaf': [1, 2, 4, 8, 16], # default 1\n#     'max_features': [1, 2, 3, 4, 5, 6], # default 'auto' (sqrt(n))\n#     'oob_score': [True, False], # default False\n#     'warm_start': [True, False], # default False\n#     'class_weight': ['balanced', 'balanced_subsample'], # default None\n#     'max_samples': [None, 500, 600, 700, 800], # default None\n# }\n\n# cv_clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, scoring='f1_weighted', n_jobs=-1, verbose=3)\n# cv_clf\n# cv_clf.fit(X_train_arr, y_train_df['Survived'].values)\n\n# # Pickle it! It's done!\n# with open('titanic-classifiers\/clf_cv.pkl', 'wb') as file:\n#     pickle.dump(obj=cv_clf, file=file)","33c1a127":"cv_clf = None\nwith open('\/kaggle\/input\/titanic-classifiers\/clf_cv.pkl', 'rb') as file:\n    cv_clf = pickle.load(file=file)\n    \ncv_clf\ncv_clf.best_estimator_\ncv_clf.best_score_","4d8b0def":"# params = {\n#     'n_estimators': [12, 25, 37], # default 100\n#     'max_depth': [None, 4, 6, 8, 10, 12], # default None\n#     'min_samples_split': [2, 3, 4, 5], # default 2\n#     'min_samples_leaf': [1, 2, 3, 4], # default 1\n#     'max_features': ['auto', 4, 5], # default 'auto' (sqrt(n))\n#     'oob_score': [True, False], # default False\n#     'warm_start': [True, False], # default False\n#     'class_weight': ['balanced', 'balanced_subsample'], # default None\n#     'max_samples': [None, 650, 700, 750], # default None\n# }\n\n# cv_clf2 = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, scoring='f1_weighted', n_jobs=-1, verbose=3)\n# cv_clf2.fit(X_train_arr, y_train_df['Survived'].values)\n\n# with open('\/titanic-classifiers\/clf_cv2.pkl', 'wb') as file:\n#     pickle.dump(obj=cv_clf2, file=file)","261b4b56":"cv_clf2 = None\nwith open('\/kaggle\/input\/titanic-classifiers\/clf_cv2.pkl', 'rb') as file:\n    cv_clf2 = pickle.load(file=file)\n    \ncv_clf2\ncv_clf2.best_estimator_\ncv_clf2.best_score_","11ae56c3":"def get_perf_ser(X, y, clf, name):\n    pred = clf.predict(X)\n    conf = confusion_matrix(y_true=y, y_pred=pred)\n    prf = precision_recall_fscore_support(y_true=y.values, y_pred=pred)\n\n    perf_ser = pd.Series(\n        {\n            'model': clf,\n            'neg_prec': prf[0][0],\n            'pos_prec': prf[0][1],\n            'neg_rec': prf[1][0],\n            'pos_rec': prf[1][1],\n            'neg_f': prf[2][0],\n            'pos_f': prf[2][1],\n            'neg_sup': prf[3][0],\n            'pos_sup': prf[3][1],\n            't_neg': conf[0][0],\n            'f_neg': conf[0][1],\n            'f_pos': conf[1][0],\n            't_pos': conf[1][1]\n        },\n        name = name\n    )\n\n    return perf_ser","801ed98a":"pd.concat([get_perf_ser(X=X_train_arr, y=y_train_df['Survived'], clf=cv_clf, name='cv_clf_scores'),\n           get_perf_ser(X=X_train_arr, y=y_train_df['Survived'], clf=cv_clf2, name='cv_clf2_scores')],\n         axis=1)","8f946346":"# ### Drop features and impute missing values. #############################################################################\n\nsimp_imp_mode = SimpleImputer(strategy='most_frequent')\n\n# Iteratively impute Fare and Pclass, and SibSp and Parch\n# # Listing all parameters because it's experimental and subject to change without notice.\niter_imp_mode = IterativeImputer(estimator=ExtraTreesRegressor(), initial_strategy='most_frequent',\n                        random_state=SEED, # Current defaults follow, but keep missing_values=np.nan:\n                        missing_values=np.nan, sample_posterior=False, max_iter=10, tol=0.001,\n                        n_nearest_features=None, imputation_order='ascending',\n                        skip_complete=False, min_value=-np.inf, max_value=np.inf, verbose=0,\n                        add_indicator=False)\n\n# # Index {0:Pclass, 1:Name, 2:Sex, 3:Age, 4:SibSp, 5:Parch, 6:Ticket, 7:Fare, 8:Cabin, 9:Embarked}\ndrop_and_imp_ct = ColumnTransformer([\n    ('drop', 'drop', [1, 6, 8, 9]),           # Drop Name, Ticket, Cabin, Embarked.\n    ('simp_imp_mode', simp_imp_mode, [2, 3]), # Impute Sex and Age to most frequent.\n    ('iter_imp_fam', iter_imp_mode, [4, 5]),  # Multivariately impute SibSp and Parch.\n    ('iter_imp_class', iter_imp_mode, [0, 7]) # Multivariately impute Pclass and Fare.\n])\n# # New index: {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass, 5:Fare}\n\ndrop_fare_ct = ColumnTransformer([\n    ('passthrough', 'passthrough', slice(0,5))\n])\n# # New index: {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass}\n\n\n# ### Handle categoricals. #################################################################################################\n\nonehot_bin_enc = OneHotEncoder(drop='if_binary')\n\ndef make_bin(x):\n    return 0 if x == 0 else 1 # Instead of bool(x), to cover bad data.\nmake_bin_vec = np.vectorize(make_bin)\nmake_bin_ft = FunctionTransformer(make_bin_vec)\n\nmake_cats_ct = ColumnTransformer([\n    ('onehot_bin_enc', onehot_bin_enc, [0]), # One-hot Sex.\n    ('passthrough', 'passthrough', [1]),     # Passthrough Age explicitly for easier tracking.\n    ('make_bin_ft', make_bin_ft, [2, 3]),    # Make SibSp and Parch binary.\n    ('passthrough1', 'passthrough', [4]),     # Passthrough Pclass.\n])\n# # Same index: {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass}\n\n\n# ### Scale features. ######################################################################################################\n\n# MinMax scale everything.\nminmax_scl = MinMaxScaler(copy=False)\n# # {0:Sex, 1:Age, 2:SibSp, 3:Parch, 4:Pclass}\n\n\n# ### Tune and fit model. ##################################################################################################\n\n# # You could use GridSearchCV from within the pipeline,\n# # but I tuned and selected outside of the pipeline, so there's no need to rerun it to test the pipeline.\n# # For full reproduction, uncomment here and in the last step of the pipeline.\n\n# params = {\n#     'n_estimators': [25, 50, 100, 150, 175], # default 100\n#     'max_depth': [None, 2, 4, 8, 16, 32, 64, 128], # default None\n#     'min_samples_split': [2, 4, 8, 16], # default 2\n#     'min_samples_leaf': [1, 2, 4, 8, 16], # default 1\n#     'max_features': [1, 2, 3, 4, 5, 6], # default 'auto' (sqrt(n))\n#     'oob_score': [True, False], # default False\n#     'warm_start': [True, False], # default False\n#     'class_weight': ['balanced', 'balanced_subsample'], # default None\n#     'max_samples': [None, 500, 600, 700, 800], # default None\n# }\n\n# params2 = {\n#     'n_estimators': [12, 25, 37], # default 100\n#     'max_depth': [None, 4, 6, 8, 10, 12], # default None\n#     'min_samples_split': [2, 3, 4, 5], # default 2\n#     'min_samples_leaf': [1, 2, 3, 4], # default 1\n#     'max_features': ['auto', 4, 5], # default 'auto' (sqrt(n))\n#     'oob_score': [True, False], # default False\n#     'warm_start': [True, False], # default False\n#     'class_weight': ['balanced', 'balanced_subsample'], # default None\n#     'max_samples': [None, 650, 700, 750], # default None\n# }\n\n# cv_clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, scoring='f1_weighted', n_jobs=-1, verbose=3)\n\n\n# ### Put it all in a pipe. ################################################################################################\n\npipe = Pipeline([\n    ('drop_and_imp_ct', drop_and_imp_ct), # Drop Name, Ticket, Cabin, Embarked. Impute missing values.\n    ('drop_fare_ct', drop_fare_ct),       # Drop Fare.\n    ('make_cats_ct', make_cats_ct),       # Handle categoricals: Sex, SibSp, Parch.\n    ('minmax_scl', minmax_scl),           # MinMax scale everything.\n#     ('cv_clf', cv_clf)                    # GridSearchCV.\n    # Did it already. Use results instead to test pipeline. Swap this step and the next to fully reproduce.\n    ('clf', cv_clf2.best_estimator_)       # To test pipeline, use best estimator rather than run the whole GridSearchCV.\n    # # Don't really need to refit it here since already done in notebook, but I want to test the pipeline.\n])\n\n\n# ### Send the data through. ###############################################################################################\n\n# From the top.\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv').set_index('PassengerId')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv').set_index('PassengerId')\n\n# Peel off labels.\nX_train_df = train_df.drop(columns=['Survived'])\ny_train_df = train_df[['Survived']]\n\n# Send it.\npipe.fit(X_train_df, y_train_df['Survived'].values)\n\n# Check the score. Silly to do in the train set, but don't have the test labels.\npipe.score(X_train_df, y_train_df['Survived'].values)\n\n# Predict test labels.\npred = pipe.predict(test_df)\n\n# # Pickle the pipe!\n# with open('\/kaggle\/working\/titanic-pipe.pkl', 'wb') as file:\n#     pickle.dump(obj=pipe, file=file)","efb3d381":"# Create dataframe of both train and test sets.\ntest_pred_df = pd.DataFrame({'Survived': pred}, index=test_df.index)\npred_test_df = pd.concat([test_pred_df, test_df], axis=1)\nfull_df = pd.concat([train_df, pred_test_df])\nfull_df['train_test'] = 'train'\nfull_df.loc[full_df.index.values < 892, ('train_test')] = 'test'\n\n# Add Age by decade as a feature.\nfull_df['Decade'] = full_df['Age'].apply(lambda x: math.floor(x \/ 10) if not np.isnan(x) else np.nan)\n\n# Does the test have the same survival rate as the training set?\nfull_df.groupby('train_test')[['Survived']].mean()","f21a6eea":"# Do the categories within features have similar survival rates?\nfull_df.groupby(['Sex', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Pclass', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Embarked', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Decade', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['SibSp', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Parch', 'train_test'])[['Survived']].mean()","0bb6d3a4":"# Subgroups?\nfull_df.groupby(['Sex', 'Pclass', 'Decade', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Pclass', 'Decade', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Sex', 'Pclass', 'train_test'])[['Survived']].mean()\nfull_df.groupby(['Sex', 'Decade', 'train_test'])[['Survived']].mean()","6833c141":"# True frame:\n# Null hypothesis: Test prediction survival rate differs from known training survival rate.\n# Hypothesis: Predicted survival rate is the same as the known survival rate.\n\n# Test frame:\n# Hypothesis: Test prediction survival rate differs from known training survival rate.\n# Null hypothesis: Predicted survival rate is the same as the known survival rate.\n\ndef test_H(ctrl, exp, alpha=0.1):#, alternative='two-sided'):\n    #Kaggle scipy version (1.5.4) doesn't alternative argument. Two-sided by default.\n    \n    statistic, pvalue = ttest_ind(a=ctrl, b=exp)#, alternative=alternative)\n    \n    if pvalue <= alpha:\n        print(\"Reject the null hypothesis (p <= %0.3f)\" % alpha)\n    else:\n        print(\"Fail to reject the null hypothesis (p > %0.3f)\" % alpha)\n        \n    print(\"statistic: %1.4f. p-value: %0.4f)\" % (statistic, pvalue))\n    \n    return\n    \nprint(\"T-test of whole set:\")\ntest_H(ctrl=train_df['Survived'].values, exp=pred)\nprint(\"\\n\")\n\nprint(\"T-tests of subgroups:\")\nprint(\"\\n\")\n\nfeats = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch', 'Decade']\nfor feat in feats:\n    cats = set(full_df[feat].values)\n    for cat in cats:\n        if not pd.isna(cat):\n            print(\"T-test for %s %s:\" % (feat, cat))\n            test_H(ctrl=full_df[(full_df['train_test'] == 'train') \\\n                               & (full_df[feat] == cat)]['Survived'],\n                   exp=full_df[(full_df['train_test'] == 'test') \\\n                               & (full_df[feat] == cat)]['Survived'])\n            print(\"\\n\")","5d01a497":"test_pred_df.to_csv('\/kaggle\/working\/titanic_test_pred.csv')","fa49d5e2":"## Principal component analysis\n\nBecause there are some potential overlaps in information (e.g. probably a lot more adult men than adult women travelled alone in that era; I cut my EDA short of discovering most of these relationships and interactions between features), I'll use sklearn's PCA to compress some of that information so it's not unduly influential in the model.\n\n(This didn't seem to help or hinder the first test models, so it doesn't seem worth adding the step. But, I'll keep it for fun.)","042a85a5":"# Prediction\n\nFinally, I've predicted the test target class values. I've not used any of the test data to inform my cleaning and engineering process, so I'm hopeful that I don't have an overfit model and that it will do well on the test set.\n\n## Check it\n\nI can't see how I did before submitting my predictions, but I can do some quick sanity checks assuming the train-test split was well stratified (randomly assigned). Does the test set have a similar survival rate as the training set? Do the categories within features have similar survival rates? You could make other more complicated checks, splitting further into subgroups for instance.\n\nI'll first peak at the surival rates out of curiosity. But, I can run a t-test (per category or on the whole) to see if predicted survival in the test set significantly differs from known survival in the training set.\n\n### Subgroup survival rates comparisons\n\nJudging from the raw survival rates of groups and subgroups, it looks like the model may be overestimating the effects of socioeconomic and familied statuses while underestimating the effect of sex. The predictions make the situation look more dire for lower class and single folks than it was known to be, and less dire for males. This makes sense as familiedness is overrepresented with two features. Also, sex is more skewed than passenger class, making it more difficult to use precisely. Survival rates by age (decade) look pretty similar between train and test sets.\n\nBut, are these differences\/similarities between known and predicted survival rates significant?","c3e42e59":"# Preprocessing\n\n## Cleaning\n\nThis data set is pretty kind, so not a lot of cleaning is needed.\n\nI need to handle missing\/unexpected values, and anticipate them in the test and hidden sets.\n\nFare is the only variable with outliers, and it has quite a few. I can handle them by scaling to log10 to help normalize it (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)).\n\nI'm also dropping Name, Ticket, and Cabin as signature features, though you might be able to tease out information about things like race\/ethnicity and location on the boat if you wanted to do a lot of extra and interesting work.\n\nI'm assuming the hidden set is similarly stratified, maybe some surprise values but no surprise distributions.\n\n### Missing values\n\n#### Univariate imputation\n\nRather than drop rows with missing values, I'll impute them. I'm dropping the column with far too many missing values to impute sensibly, Cabin. Age is missing about 25% of its values, but I don't want to lose that many records nor that important of a feature. So, I'll impute the best I can and hope the other features make up for the noise.\n\nSince there aren't many missing values in most sets, if any, I can use sklearn's SimpleImputer in most cases. And, based on the skewed distributions (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)), I'll use the mode ('most_frequent').","a34d040d":"# Evaluation\n\nAgain, this is a bit of a skewed target class, so I'm less concerned with accuracy than I am with F1, survival recall, and fatality precision. Accuracy will get me closer to the top of the leader board on the test set, but the other metrics will avoid overfit and get me closer to the top of the leader board after the shakeup.\n\nAlso, I don't have access to the test labels, so I have to evaluate based on training labels. That really won't tell me much because I trained the model on those labels.\n\nI'll check both tuned models. It sounds like there's a final hidden set on Kaggle, so the test set that I have for which I can submit multiple predictions is actually a validation set, and the hidden set is the true test set.","a560c267":"## Feature Engineering\n\n### Discretizing categorical features\n\n#### Handle existing categorical features\n\nSex is the only categorical remaining, though I will create more. I'll encode it as nominal, after imputation. With now no missing values, I'll use sklearn's OneHotEncoder, dropping one dummy feature because it's considered binary in this set.","2824c1d6":"# Intro\n\nThis is the complete process of how I generated my predictions in the Titanic Kaggle competition. I constructed a SciKit-Learn pipeline that includes feature engineering, model selection, tuning, validation, and evaluation.\n\nSee my initial [Exploratory Data Analysis (EDA) here](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda).\n\nScroll to see my commentary along with my code.","6fac4f76":"## Training: cross-validation and tuning\n\nI'll use sklearn's GridSearchCV for hyperparameter tuning with cross-validation. I'll optimize for weighted F1 since this is a slightly skewed target class with about 38% survival.\n\nI could use GridSearchCV to tune the whole pipeline (i.e. PCA and RandomForestClassifier in this case), but I'll cut down on compute and just tune the classifier.\n\nIt might also be wise to tune a couple of models, not just RandomForestClassifier, since the cross-validated scores would be more reliable than the performance of the model on the training set. You wouldn't want to do it too much or you risk overfit. And, you'd want to add an additional hold-out set to further validate and avoid the multiple comparison's problem. (Comparing multiple untuned models to choose which to tune is a little problematic, but not as much since you're comparing final tuned models.) But, rather than train-test splitting the training set, I'll stick with tuning one model because I just want to toss something on Kaggle.\n\nThat said, I'll run one broad parameter search, then run it again in a narrower and finer search space around the best parameters from the first round. The first round of tuning took about 7 hours on my computer. The second round took 19 minutes.\n\nI've pickled the results and commented out the grid search so the kernel can be restarted and everything else repeated without having to rerun the gridsearch.","d0045d50":"The second round of tuning slightly improved fatality precision, survivor recall, and F1. But, these scores are worse than the untuned model. That's actually a good sign that cross validation and bootstrapping reduced the overfit from testing on the training data. I wouldn't be surprised if the first tuned model outperforms the second on the test set.\n\n# Pipeline\n\nNow, I'll put it all together into one pipeline using the parameters GridSearchCV selected. So it's visible in one place, I'll repeat the whole series of transformations from the beginning, except I'll remove unnecessary steps. So, I'll drop Fare from the start instead of log scaling it then dropping it, but not before I use it to inform how to impute Pclass. And, I'll leave Age unengineered, other than the scaling.","4a175332":"### Surival rate significance tests\n\nI'll compare predicted and known survivals using SciPy's t-test, two-sided. My hypothesis is that the model's predicted survival rate is the same as the know survival rate. Because of the way the test function is written, the result frames the hypothesis as my null hypothesis that the model's predicted survival rate differs from the know survival rate. So, a failure to reject the null hypothesis is a good sign that my model is accurate, but not conclusive; rejecting the null is a strong suggestion to adopt the alternative hypothesis that the predicted survival rate is different than the true survival rate.\n\n---\n**NOTE**\nI created this notebook with scipy version 1.6.3, but Kaggle is currently running version 1.5.4. The T-test results are different here for some reason, but the general conclusions are about the same. Rather than messing with the environment (`!conda install scipy=1.6.3` was taking forever to run and appeared to not be automatically handling conflicts), I'll just accept these results for this project.\n---...\n\nOverall, I stoutly fail to reject the null (p = 0.898); so far so good. In sex, I reject the null; the differences I noticed in survival rates within sex group were in fact significant. I mostly fail to reject the null in class groups, 2nd class being the acception (p = 0.04). Survival rate of age groups by decade don't seem to not not differ.","e8a69f2e":"## Feature selection\n\nI'm pretty confident that the engineered features will be useful in prediction (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)), but let me reconsider it. And, I'm not entirely sure I've improved on the original features. After a bit of reasoning, and checking linear correlations (which is instructive, though this is a classification problem, not a regression, and not strictly linear), I'll compare feature importances according to ANOVA.\n\n### SibSp and Parch\n\nYou could say that familied privilege is over-represented by the redundancy of SibSp and Parch. Their relationship has a Pearson's R of 0.42, which *is* something, but not enough for me to say that each doesn't contain unique information. Having at least one parent or child *and* having at least one sibling or spouse looks to be an advantage over having one or the other, let alone none. So, I think it's worth keeping both.\n\nAlso, engineering improved the correlation to survival, though it's still weak.","770857fd":"My engineered features look to do a better job of explaining the difference between survivor and nonsurvivor groups, except for Age. I'll drop the original features, except for age.\n\nAge is a relatively weak predictor, and doesn't inspire a lot of confidence (p > 0.10). That said, it contains information that other features don't, though it may overlap some with them. I'll go ahead and keep the original feature and drop the engineered feature. (I tried the first quick and dirty models with and without Age, and got better performance with it.)\n\nAgain, as far as information, Fare probably isn't doing much that Pclass isn't already doing better. I think I can drop Fare.\n\nAnd, again, Parch and SibSp interact to improve prediction, so I'll keep both.\n\nSex is most important, over three times as important as its runner up, Pclass. Sex would probably make a somewhat decent predictor all on its own.","28884de3":"### Scaling\n\nTo be kind to a broad range of ML algorithms, I'll just scale features to a (0-1) range using their minimums and maximums. There might be a more intelligent choice based on the data distributions and the potential ML algorithms, but I want to keep it simple. I'll do it to the original features as well.\n\nI first need to scale Fare to log10 to help normalize it a bit. In the case of Age, the distance and rank is meaningful, and scaling won't change that the values will remain nine distinct values.","b82e97be":"## Recommendations\n\nSex appears to be under-influential in the model, though it is by far the single most predictive feature; the distribution of predictions differ significantly from expected predictions within sex groups.\n- This may be partially due to the skew of Sex (36% female), and may be somewhat unavoidable, thought there may be an algorithm that I don't know that can mitigate this.\n- Reducing the over-representation of familiedness by removing either SibSp or Parch count may help give more weight to Sex, but you'll lose the interactivity of SibSp with Parch.\n- On the other hand, removing Age alogether may lend weight to Sex; and, we saw earlier (SelectKBest) that Age was one of the least important and only dubiously significant (p ~ 0.11).\n\nAlso, as I mentioned in my [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda), some of the features I dropped may be teased to get at unique underlying information. Cabin and ticket numbers may encode something like location on the boat, which may contribute to survival advantage. Similarly, names and titles may be teased to get at race\/ethnicity, which may be associated with survivor privilege.\n\nI'll go ahead and stick with the model as it is, which is sufficient for my current purposes: to make my first Kaggle submission.\n\n## Save final predictions","81fcaf71":"#### Create new categorical features\n\n##### Age\n\nAge is polymodal (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)), and thus not suited as-is to ML algorithms that assume normal distribution. Plus, I want to avoid overfit, so I'll make it ordinal by decade. This makes the distribution more normal for models that assume that, while allowing for discretization for models that can handle that. Using wider bins might reduce overfit due to right-skew, but it would also lose the poignant mode of children under ten. I'll use sklearn's KBinsDiscretizer. It only allows you to set the number of bins, not their widths. The max age in the training set is 80, so I'll consider everyone 80 and older as in the same decade.","8a94eff5":"#### Multivariate imputation\n\nIn some cases, I want a more intelligent choice. For instance, I might want to use multivariate imputation (sklearn's IterativeImputer) on Pclass, Fare, and Embarked because they reflect socioeconomic status and thus can inform each other. Socioeconomic status seemed to be fairly predictive of survival, so I want to capture this information the best I can. There are few missing values (two in Embarked), so I would expect IterativeImputer to do well.\n\nThe same is true for SibSp (count of siblings and\/or spouses on board) and Parch (count of parents and\/or children on board). These indicators of network privilege inform each other and may benefit from multivariate imputation.\n\nThe challenge is that Embarked is a categorical string, and not ordinal. IterativeImputer does not accept strings. I would need to first encode it as ordinal (sklearn's OrdinalEncoder) to make it numeric, even though it's not ordinal. That way I can keep the missing (and unexpected) values in place. Using sklearn's OneHotEncoder would give them their own column and thus I would not be able to impute them. Because Embarked is not actually ordinal, I would then use DecisionTreeRegressor as IterativeImputer's estimator, but just to be on the safe side, I'd make sure the Embarked's ordinal labels are ranked according to its association with Pclass. Then, I would one-hot encode it as nominally categorical.\n\nInstead, I'll just drop Embarked. It seems to encode socioeconomic status (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)), which is already well-represented in the data by Pclass and Fare. I will potentially lose other key information that may or may not be encoded in Embarked, like location on the boat. But, it's not worth the trouble, especially since I will decompose these features to reduce the over-representation of socioeconomic status, thus losing Embark's non-linear categorical state. I'll still use IterativeImputer on Fare and Pclass.","4b75a34b":"##### SibSp and Parch\n\nI'll make SibSp (number of siblings and\/or spouses on board) and Parch (number of parents and\/or children on board) binary, indicating those with and without family. These features are indicators of network privilege; it helps to have family looking out for you. Passengers with at least one family member on board seemed to benefit substantially compared to having none at all (see [EDA](https:\/\/www.kaggle.com\/kalebcoberly\/titanic-r-eda)). Even if there's no law of diminishing returns (the law network effects suggesting exponential returns), counts of records for values above 1 get very low, so this additional information is weak.\n\nI'll create a custom function wrapped in sklearn's FunctionTransformer.","71b7cfb4":"Unsurprisingly, DecisionTreeClassifier and RandomForestClassifier did much better across the board. And, of course, random forest did better than a single decision tree on the important metrics survival recall (pos_rec), fatality precision (neg_prec). It faired pretty much the same on F1 (neg_f and pos_f).\n\nI'll take random forest to tuning.","30de8dc7":"### Pclass and Fare\n\nThe same could be said about Pclass and Fare, except they are more strongly correlated to each other, especially after engineering. Plus, Fare probably doesn't contain much unique information. So, there's a stronger case to drop one.\n\nEngineering did improve Fare's correlation to survival, but it's still no better than Pclass.\n\nThe correlation to survival actually drops when you take the product of the two.","babe10a3":"Looking at feature importances according to sklearn's SelectKBest (ANOVA), Pclass and Fare score much higher than the bottom three features, though not nearly as much as the top. I'm going to keep them.","6d277500":"# Modeling\n\n## Algorithm selection\n\nIncorporating nominal sex in the classification task, I'm guessing that sklearn algorithms like KNeighborsClassifier and DecisionTree will do well. I am interested to see how SVC with an RBF kernel does, and how LogisticRegression does. I don't expect Gaussian Naive Bayes to do well because most variables are not normally distributed. Because I think DecisionTree might do well, I think RandomForest will do better. I'm curious to see how other ensembles do, like Adaboost and GradientBoostingClassifier.\n\nI'll try each of these and pick the best performing model for tuning. To determine which is best, I'll consider F1, survival recall, and fatality precision because the target class is skewed with about 38% survival. Accuracy will get me closer to the top of the leader board, but the other metrics will avoid overfit and give me a better chance at accuracy on the test sets.","ab074d94":"##### Wrap into one pipeline step\n\nI'll use sklearn's ColumnTransformer to differentiate the operations I perform on each column. And, I'll use sklearn's FeatureUnion to keep the original features (other than Sex because it's an object) as well to see how they compare later in feature selection."}}