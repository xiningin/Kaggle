{"cell_type":{"1922152b":"code","0e746a0f":"code","3350a1dc":"code","dac1a8f1":"code","872ed9e7":"code","afbfbfa8":"code","ab93f56b":"code","adbc8020":"code","5918c694":"code","763b8884":"code","a2cda05d":"code","3c99e114":"code","82cd2fc8":"code","c15d4e54":"code","916bde82":"code","09938cc5":"code","118c5062":"code","c403cb93":"code","cb9d436f":"code","d285c2c2":"code","b9a5eafe":"code","40c6a9b0":"code","cc907faf":"code","8ed78757":"code","4b1d998f":"code","03e4e9b8":"code","0dc9c95e":"code","382e8914":"code","3d5b3565":"code","2265a6e8":"code","2941d6b0":"code","d0e4e80a":"code","c9d6effb":"code","17d5eaec":"code","e9b2dce1":"code","b5afd9b6":"code","1bc5f84f":"code","2b039e06":"code","37fb6163":"code","791a99c1":"code","095c03ff":"code","46d1d157":"code","16388eac":"code","f167dadc":"code","11923bc8":"code","bc0a0db7":"code","d9c25e86":"code","83222bc2":"code","6f21cb20":"code","320e400a":"code","a4e0a9b7":"code","367439c8":"code","6f792fca":"code","c9d7793f":"code","2488341e":"code","d499b575":"code","39dca872":"code","446fa8af":"code","33149025":"code","ca859649":"code","b9fd155d":"code","3c574714":"code","2a8237f8":"code","3b7d60ea":"code","ec60b078":"code","9a69f5f6":"code","c7ceb1ec":"code","8a868488":"markdown","6678eee9":"markdown","db6ea667":"markdown","6c4c4def":"markdown","ee8d8037":"markdown","e4ec8105":"markdown","ece7a4d9":"markdown","98ccbda2":"markdown","0a6d760b":"markdown","cc32c101":"markdown","8744e1d7":"markdown","8667ea52":"markdown","11042959":"markdown","77a42709":"markdown","849da0bc":"markdown","0da02a30":"markdown","eaf762df":"markdown","d41bce69":"markdown","0eae7c25":"markdown","84fe97a8":"markdown","ef140002":"markdown","94ecea55":"markdown","f07019c3":"markdown","69445878":"markdown","a701be2b":"markdown","fbf90520":"markdown","f9bf7cc7":"markdown","bbe7ca42":"markdown","3e5546e5":"markdown","70714f5c":"markdown","34c10dce":"markdown","2361f76b":"markdown","cd59bbce":"markdown","8021412a":"markdown","03d022a3":"markdown","6cde3af1":"markdown","aec9b787":"markdown","c36d43ed":"markdown","e619db0e":"markdown"},"source":{"1922152b":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 5000)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#sns.set_style('darkgrid')\nfrom scipy.stats import norm, skew, probplot\nfrom scipy.special import boxcox1p\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge as krr\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor as rfr, GradientBoostingRegressor as gbr\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.cluster import DBSCAN\n\nimport optuna\nfrom functools import partial","0e746a0f":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3350a1dc":"train.head()","dac1a8f1":"test.head()","872ed9e7":"print(f'train size: {train.shape}')\nprint(f'test size: {test.shape}')","afbfbfa8":"f = plt.figure(facecolor='whitesmoke', figsize=(20, 5))\n\nax_left = f.add_axes([0,0,.2,1], facecolor='whitesmoke')\nax_left.axis('off')\nax_left.text(.4, .9, 'SalePrice', color='crimson', size=22, weight='bold')\nax_left.text(.1, .8, f'Skew: {train.SalePrice.skew():.2f}', size=20)\nax_left.text(.1, .7, f'kurt: {train.SalePrice.kurt():.2f}', size=20)\nax_left.text(.1, .6, f'missing count: {train.SalePrice.isnull().sum()}', size=20)\nax_left.text(.1, .4, 'conclusion: ', color='crimson', size=20)\nax_left.text(.1, .3, 'It is necessary to convert', size=20)\nax_left.text(.1, .2, 'such as log transformation.', size=20)\n\nax_right1 = f.add_axes([0.25,0,.3,.8], facecolor='whitesmoke')\nsns.distplot(train.SalePrice, fit=norm, ax=ax_right1)\nax_right1.spines[['top', 'right']].set_visible(False)\nax_right1.set_title('histogram', color='crimson', weight='bold', size=15)\n\nax_right2 = f.add_axes([.57,0,.3,.8], facecolor='whitesmoke')\nprobplot(train.SalePrice, plot=ax_right2)\nax_right2.spines[['top', 'right']].set_visible(False)\nax_right2.set_title('QQ plot', color='crimson', weight='bold', size=15)\n\nplt.show()","ab93f56b":"num_vars = train.columns[train.dtypes != 'object']\nobj_vars = train.columns[train.dtypes == 'object']\n\nprint('\\nNumerical vars: ')\nprint(num_vars.values)\nprint('\\nObject vars: ')\nprint(obj_vars.values)","adbc8020":"all_data = pd.concat((train, test)).drop(['SalePrice'], axis=1)\ncnt_missing = all_data.isnull().sum().sort_values(ascending=False)\ncnt_percent = cnt_missing \/ all_data.shape[0] * 100\nmissing_table = pd.DataFrame([cnt_missing, cnt_percent], \n                             index=['missing count', 'missing percent']).T\nmissing_table = missing_table[missing_table['missing count'] > 0]\nmissing_table = missing_table.reset_index()\nmissing_table['missing count'] = missing_table['missing count'].astype(int)","5918c694":"color_list=[['whitesmoke', 'white', 'white']]\n\nfig = plt.figure(facecolor='whitesmoke')\nax1 = fig.add_axes([0, 0, 1, 0.1]) \nax2 = fig.add_axes([1.5, -2.3, 1, 2.3], facecolor='whitesmoke') \n\nax2.spines[['top', 'right']].set_visible(False)\n\nax1.set_axis_off()\n\ntable=ax1.table(cellText = missing_table.values[:20], colLabels=missing_table.columns,\n                  colColours=['crimson']*3, cellColours=color_list*20)\ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \nax1.text(0.67, .9, 'Missing count and percent', color='crimson', fontsize=20, fontweight='bold')\nax1.text(1.4, .9, 'by values', fontsize=20, fontweight='bold')\n\nsns.barplot(y=missing_table['index'], x=missing_table['missing percent'], orient = \"h\", ax=ax2)\nplt.show()","763b8884":"f, ax = plt.subplots(7, 6, figsize=(25, 25), facecolor='whitesmoke')\nnum_vars = all_data.columns[all_data.dtypes != 'object']\nfor i, c in enumerate(num_vars):\n    g = sns.distplot(all_data[c], fit=norm, ax=ax[i\/\/6, i%6], color='crimson')\n    g.set_facecolor('whitesmoke')\n    g.spines[['top', 'right']].set_visible(False)\nf.text(0.4, .92, 'Distribution of numerical vars', size=20, weight='bold', color='crimson')\nplt.show()","a2cda05d":"f, ax = plt.subplots(9, 6, figsize=(25, 26), facecolor='whitesmoke')\ncat_vars = all_data.columns[all_data.dtypes == 'object']\nfor i, c in enumerate(cat_vars):\n    g = sns.barplot(data=pd.DataFrame(all_data[c].value_counts()).reset_index(), x='index', y=c, ax=ax[i\/\/6, i%6], color='crimson')\n    g.set(xticks=[])\n    g.set(title=c)\n    g.set_facecolor('whitesmoke')\n    g.spines[['top', 'right']].set_visible(False)\nf.text(0.4, .92, 'Distribution of categorical vars', size=20, weight='bold', color='crimson')\nplt.show()","3c99e114":"f, ax = plt.subplots(figsize=(10, 7))\nhighcorr_vars = (abs(train.corr().SalePrice).sort_values(ascending=False)[:7]).index\nsns.heatmap(train[highcorr_vars].corr(), annot=True)\nplt.show()","82cd2fc8":"def hypo_test(x, y, cat=False):\n    f, ax = plt.subplots(1, 4, figsize=(25, 5), facecolor='whitesmoke')\n    if cat:\n        sns.boxplot(x=train[x], y=train[y], ax=ax[0], color='crimson')\n    else:\n        sns.scatterplot(x=train[x], y=train[y], ax=ax[0], color='crimson')\n        sns.regplot(x=train[x], y=train[y], ax=ax[0], color='crimson')\n    sns.residplot(x=train[x], y=train[y], ax=ax[1], color='crimson')\n    sns.distplot(train[x], fit=norm, ax=ax[2], color='crimson')\n    probplot(train[x], plot=ax[3])\n    ax[0].set_facecolor('whitesmoke')\n    ax[1].set_facecolor('whitesmoke')\n    ax[2].set_facecolor('whitesmoke')\n    ax[3].set_facecolor('whitesmoke')\n    ax[0].spines[['top', 'right']].set_visible(False)\n    ax[1].spines[['top', 'right']].set_visible(False)\n    ax[2].spines[['top', 'right']].set_visible(False)\n    ax[3].spines[['top', 'right']].set_visible(False)\n    \n    f.suptitle(f'{x}', color='crimson', weight='bold', size=20)\n    \n    plt.show()","c15d4e54":"hypo_test('OverallQual', 'SalePrice', True)","916bde82":"hypo_test('GrLivArea', 'SalePrice')","09938cc5":"hypo_test('GarageArea', 'SalePrice')","118c5062":"skews = abs(all_data.skew()).sort_values(ascending=False)\nkurts = abs(all_data.kurt()).sort_values(ascending=False)\nskew_kurt_table = pd.DataFrame([skews, kurts], index=['skew', 'kurt']).T\nntv = skew_kurt_table[skew_kurt_table['skew'] > 0.5].index\n\nplt.subplots(figsize=(15, 5))\nsns.barplot(x=skew_kurt_table.loc[ntv].index, y=skew_kurt_table.loc[ntv]['skew'])\nplt.xticks(rotation='90')\nplt.title('skew by variable', size=20)\nplt.xlabel('vars', size=15)\nplt.ylabel('skew', size=15)\nplt.show()","c403cb93":"train_id = train.Id\ntest_id = test.Id\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","cb9d436f":"plt.subplots(figsize=(20, 10))\noutlier_idx = train.GrLivArea.sort_values(ascending=False)[:2].index\nsns.scatterplot(x=train['GrLivArea'], y=train.SalePrice)\nsns.scatterplot(x=train.iloc[outlier_idx]['GrLivArea'], y=train.iloc[outlier_idx].SalePrice, color='r', s=300, alpha=.6)\nplt.show()","d285c2c2":"scaled_data = pd.DataFrame(StandardScaler().fit_transform(train[['GrLivArea', 'SalePrice']]), columns=['GrLivArea', 'SalePrice'])\ndbscan_model = DBSCAN(eps=1.5, min_samples=3).fit(scaled_data)\ntmp = pd.concat((scaled_data, pd.DataFrame(dbscan_model.labels_, columns=['label'])), axis=1)","b9a5eafe":"tmp.label.value_counts()","40c6a9b0":"sns.pairplot(tmp, hue='label', size=5)\nplt.show()","cc907faf":"train.drop(train.GrLivArea.sort_values(ascending=False)[:2].index, axis=0, inplace=True)","8ed78757":"train_size = train.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test), axis=0).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","4b1d998f":"train.shape, test.shape, all_data.shape, y_train.shape","03e4e9b8":"all_data.columns[all_data.isnull().sum() > 0]","0dc9c95e":"sns.scatterplot(x=all_data.Neighborhood, y=all_data.LotFrontage)\nplt.xticks(rotation='90')\nplt.show()","382e8914":"all_data.Alley.value_counts()","3d5b3565":"all_data.Utilities.value_counts()","2265a6e8":"all_data.GarageArea.value_counts()","2941d6b0":"for c in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual',\n          'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"PoolQC\",\n          'Alley','Fence','MiscFeature','FireplaceQu','MasVnrType','Utilities']:\n    all_data[c] = all_data[c].fillna('None')\n    \nfor c in ['GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrArea','BsmtFinSF1',\n          'BsmtFinSF2','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n          'BsmtUnfSF','TotalBsmtSF']:\n    all_data[c] = all_data[c].fillna(0)\n\nfor c in ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']:\n    all_data[c] = all_data[c].fillna(all_data[c].mode()[0])\n\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nall_data['Functional'] = all_data['Functional'].fillna('Typ')","d0e4e80a":"all_data.isnull().sum().sum()","c9d6effb":"train[['MSSubClass', 'GarageYrBlt', 'MoSold', 'OverallCond', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'YrSold']] =\\\ntrain[['MSSubClass', 'GarageYrBlt', 'MoSold', 'OverallCond', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'YrSold']].astype('str')\n\ntest[['MSSubClass', 'GarageYrBlt', 'MoSold', 'OverallCond', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'YrSold']] =\\\ntest[['MSSubClass', 'GarageYrBlt', 'MoSold', 'OverallCond', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'YrSold']].astype('str')\n\norder_vars = [\n    'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n    'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageYrBlt', 'GarageQual',\n    'GarageCond', 'PoolQC', 'MoSold', 'YrSold'\n]","17d5eaec":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness.Skew) > 0.5]\nskewness","e9b2dce1":"skewness = skewness[abs(skewness) > 0.5]\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], boxcox_normmax(all_data[feat] + 1))","b5afd9b6":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness.Skew) > 0.5]\nskewness","1bc5f84f":"for c in order_vars:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","2b039e06":"y_train = np.log1p(y_train)","37fb6163":"all_data['Haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['Hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['Hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['Hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nall_data['TotalSF'] = (all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])\nall_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))                              \nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])","791a99c1":"all_data = pd.get_dummies(all_data)","095c03ff":"all_data.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1, inplace=True)","46d1d157":"all_data.shape","16388eac":"X_train, X_test = all_data.iloc[:train_size, :], all_data.iloc[train_size:, :]","f167dadc":"X_train.shape, X_test.shape, y_train.shape","11923bc8":"def rmsle_cv(model):\n    return np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring='neg_mean_squared_error',\n                   cv=5, verbose=0, n_jobs=-1))","bc0a0db7":"# model_lasso = Pipeline([\n#     ('scaler', RobustScaler()),\n#     ('model', Lasso())\n# ])\n# model_elasticNet = Pipeline([\n#     ('scaler', RobustScaler()),\n#     ('model', ElasticNet(max_iter=5000))\n# ])\n# model_krr = Pipeline([\n#     ('scaler', RobustScaler()),\n#     ('model', krr())\n# ])\n\n# model_svr = Pipeline([\n#     ('scaler', RobustScaler()),\n#     ('model', SVR())\n# ])\n\n# grid_param_lasso = {\n#     'model__alpha': 0.0001 * np.arange(1, 100)\n# }\n# grid_param_elasticNet = {\n#     'model__alpha': 0.0001 * np.arange(1, 100),\n#     'model__l1_ratio': 0.001 * np.arange(1, 10)\n# }\n# grid_param_krr = {\n#     'model__alpha': 0.0001 * np.arange(1, 100),\n#     'model__degree': [1, 2, 3],\n#     'model__kernel': ['polynomial'],\n#     'model__coef0': [2.5]\n# }\n# grid_param_svr = {\n#     'model__C': [0.001, 0.1, 1, 10, 20],\n#     'model__gamma': [.0001, .0002, .0003, .0004, .0005, .0006, .0007, .0008, .0009, .001],\n#     'model__epsilon': [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1]\n# }\n\n# best_params = {}","d9c25e86":"# search_lasso = GridSearchCV(model_lasso, grid_param_lasso, scoring='neg_mean_squared_error',\n#                            cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\n# best_params['Lasso'] = search_lasso.best_params_","83222bc2":"# search_elasticNet = GridSearchCV(model_elasticNet, grid_param_elasticNet, scoring='neg_mean_squared_error',\n#                            cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\n# best_params['ElasticNet'] = search_elasticNet.best_params_","6f21cb20":"# search_krr = GridSearchCV(model_krr, grid_param_krr, scoring='neg_mean_squared_error',\n#                            cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\n# best_params['KernelRidge'] = search_krr.best_params_","320e400a":"# search_svr = GridSearchCV(model_svr, grid_param_svr, scoring='neg_mean_squared_error',\n#                            cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\n# best_params['SVR'] = search_svr.best_params_","a4e0a9b7":"# def objective_xgb(trial, X, y):\n#     param = {\n#         'n_estimators': 1900,\n#         'max_depth': trial.suggest_int('max_depth', 3, 11),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.01),\n#         'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n#         'colsample_bylevel': trial.suggest_categorical('colsample_bylevel', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n#         'n_jobs': -1\n#     }\n#     train_scores, test_scores = [], []\n#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n#     model = XGBRegressor(**param)\n#     for train_idx, test_idx in kf.split(X):\n#         tmp_X_train, tmp_X_test = X_train.iloc[train_idx, :], X_train.iloc[test_idx, :]\n#         tmp_y_train, tmp_y_test = y_train[train_idx], y_train[test_idx]\n#         model.fit(tmp_X_train, tmp_y_train,\n#                  eval_set=[(tmp_X_test, tmp_y_test)], eval_metric=['rmse'],\n#                  early_stopping_rounds=30, verbose=0,\n#                  callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key='validation_0-rmse')])\n#         train_score = np.sqrt(mse(tmp_y_train, model.predict(tmp_X_train)))\n#         test_score = np.sqrt(mse(tmp_y_test, model.predict(tmp_X_test)))\n#         train_scores.append(train_score)\n#         test_scores.append(test_score)\n#     train_score = np.array(train_scores).mean()\n#     test_score = np.array(test_scores).mean()\n#     print(f'train score: {train_score}')\n#     print(f'test score: {test_score}')\n#     return test_score","367439c8":"# def objective_lgbr(trial, X, y):\n#     param = {\n#         'n_estimators': 2000,\n#         'max_depth': trial.suggest_int('max_depth', 3, 11),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.01),\n#         'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n#         'n_jobs': -1\n#     }\n#     train_scores, test_scores = [], []\n#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n#     model = LGBMRegressor(**param)\n#     for train_idx, test_idx in kf.split(X):\n#         tmp_X_train, tmp_X_test = X_train.iloc[train_idx, :], X_train.iloc[test_idx, :]\n#         tmp_y_train, tmp_y_test = y_train[train_idx], y_train[test_idx]\n#         model.fit(tmp_X_train, tmp_y_train,\n#                  eval_set=[(tmp_X_test, tmp_y_test)], eval_metric=['rmse'],\n#                  early_stopping_rounds=30, verbose=0,\n#                  callbacks=[optuna.integration.LightGBMPruningCallback(trial, 'rmse')])\n#         train_score = np.sqrt(mse(tmp_y_train, model.predict(tmp_X_train)))\n#         test_score = np.sqrt(mse(tmp_y_test, model.predict(tmp_X_test)))\n#         train_scores.append(train_score)\n#         test_scores.append(test_score)\n#     train_score = np.array(train_scores).mean()\n#     test_score = np.array(test_scores).mean()\n#     print(f'train score: {train_score}')\n#     print(f'test score: {test_score}')\n#     return test_score","6f792fca":"# optimizer_xgbr = partial(objective_xgb, X=X_train, y=y_train)\n# study_xgbr = optuna.create_study(direction='minimize')\n# study_xgbr.optimize(optimizer_xgbr, n_trials=100)\n# best_params['XGBoost'] = study_xgbr.best_params","c9d7793f":"# optimizer_lgbr = partial(objective_lgbr, X=X_train, y=y_train)\n# study_lgbr = optuna.create_study(direction='minimize')\n# study_lgbr.optimize(optimizer_lgbr, n_trials=100)\n# best_params['LightGBM'] = study_lgbr.best_params","2488341e":"# optuna.visualization.plot_optimization_history(study_xgbr)","d499b575":"# optuna.visualization.plot_slice(study_xgbr)","39dca872":"# optuna.visualization.plot_optimization_history(study_lgbr)","446fa8af":"# optuna.visualization.plot_slice(study_lgbr)","33149025":"# best_params","ca859649":"model_lasso = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', Lasso(alpha=0.0002))\n])\n\nmodel_enet = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', ElasticNet(alpha=0.0078000000000000005, l1_ratio=0.009000000000000001, random_state=3))\n])\n\nmodel_krr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', krr(alpha=0.99,\n                        kernel='polynomial',\n                        degree=1,\n                        coef0=2.5))\n])\nmodel_svr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', SVR(C= 20, epsilon= 0.05, gamma=0.0005))])\n\nmodel_gbr = gbr(n_estimators=3000, learning_rate=0.009995774699700678,\n                                   max_depth=8, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5)\n#model_xgbr = XGBRegressor(n_estimators=2200, random_state =42, **study_xgbr.best_params)\nmodel_xgbr = XGBRegressor(colsample_bytree=0.4, learning_rate=0.00898718134841855, max_depth=8, \n                             n_estimators=2200, reg_alpha=0.036142628805195254, reg_lambda=0.03188665185506858,\n                             subsample=0.6, random_state =42)\n#model_lgbm = LGBMRegressor(objective='regression', n_estimators=2200, **study_lgbr.best_params)\nmodel_lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4, learning_rate=0.01, n_estimators=9000,\n                                       max_bin=200, bagging_fraction=0.75, bagging_freq=5, \n                                       bagging_seed=7, feature_fraction=0.2, feature_fraction_seed=7,\n                                       verbose=-1)\nstack_gen = StackingCVRegressor(regressors=(model_lgbm, model_lasso, model_enet, model_krr, model_svr, model_gbr),\n                               meta_regressor=model_xgbr,\n                               use_features_in_secondary=True)","b9fd155d":"models = [\n    model_lasso, model_enet, model_krr, model_svr, model_gbr, model_xgbr, model_lgbm\n]\ncross_score = {\n    'Lasso': 0,\n    'ElasticNet': 0,\n    'Kernel Ridge': 0,\n    'SVR': 0,\n    'GradientBoosting': 0,\n    'XGBoost': 0,\n    'LightGBM': 0,\n}\n\nfor idx, model in enumerate(models):\n    cross_score[list(cross_score.keys())[idx]] = rmsle_cv(model).mean()","3c574714":"cross_score","2a8237f8":"def blend(X):\n    return ((0.10 * model_lasso.predict(X)) + \\\n            (0.10 * model_enet.predict(X)) + \\\n            (0.10 * model_krr.predict(X)) + \\\n            (0.10 * model_svr.predict(X)) + \\\n            (0.10 * model_xgbr.predict(X)) + \\\n            (0.10 * model_lgbm.predict(X)) + \\\n            (0.40 * stack_gen.predict(np.array(X))))","3b7d60ea":"for model in models:\n    model = model.fit(X_train, y_train)","ec60b078":"stack_gen = stack_gen.fit(X_train, y_train)","9a69f5f6":"np.sqrt(mse(y_train, blend(X_train)))","c7ceb1ec":"sub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = score = np.expm1(blend(X_test))\nsub.to_csv('submission.csv',index=False)","8a868488":"<h3 style=\"text-align:center;\">4. Exploring independent variables - distribution<\/h3>","6678eee9":"<p>\n    <font color='red'>Conclusion:<\/font><br>\n    I analyzed the correlation of each variable for SalePrice and confirmed that variables such as OverallQual, GrLivArea, and CarArea had high linearity.<br>\n    Linearity was visualized using a box plot and scatter plot.<br>\n    And I thought these variables were important variables, so I drew a residual diagram for the homodis variance test, a histogram for the normality test, and a QQ plot for them.<br>\n    <br>\n    Each variable does not satisfy the equal variance because the points of the residual degree are not randomly sprayed and have some pattern or shape.<br>\n    I could see that each variable had no ideal normality through the results of the histogram and QQplot.<br>\n    The above problems may be solved through log transformation or boxcox transformation.<br>\n    <br>\n    It was also confirmed that some independent variables were correlated with each other. As expected, I visualized it as a scatter plot.<br>\n    The high correlation between independent variables causes multicollinearity. The explanatory power of the model loses its reliability.<br>\n    I decided to use regulation rather than choosing variables or using dimension reduction right away.<br>\n    The linear model may solve the above problem through regulation (normal1, normal2).<br>\n    <br>\n    It was difficult to visualize all variables, so I looked at the independent variables that required conversion through skewness and kurtosis.\n<\/p>","db6ea667":"<a id=\"section1\"><\/a>\n# EDA","6c4c4def":"<h2 style=\"text-align:center;\">Module import<\/h2>","ee8d8037":"Some variables seem to be able to follow a normal distribution through log transformation or box cox transformation. Some variables have zero. It seems that +1 should be done when converting.\n\nDraw a bar chart for categorical variables.\n\nSome variables were extremely biased toward one value (0). So they don't seem to be very important variables.","e4ec8105":"Let's check the cross-validation results of each model.","ece7a4d9":"<a id=\"section4\"><\/a>\n# Modeling","98ccbda2":"<h3 style=\"text-align:center;\">3. Train, Test merge, separating dependent variables.<\/h3><br>\n<h4>Outline<\/h4>\nI removed outliers from the training data. Now I combine with the test data. The dependent variable is separated separately.","0a6d760b":"<h3 style=\"text-align:center;\">5. Transform numeric variables - LabelEncoder, Boxcox, Log<\/h3><br>\nLog and boxcox transformations are applied to secure the normality and equal variance of continuous variables.<br>\nLabel encoding is applied to ordered (priority) variables.<br>\nOne-hot encoding is applied to equal variables.<br>\nLog transformation is applied to the dependent variable to secure the normality of the dependent variable.","cc32c101":"<p>\n    <font color='red'>Conclusion:<\/font><br>\n    Two outliers were found through the scatterplot. Two points were removed.<br>\n    In the case of DBSCAN, a total of 4 points were judged as outliers.\n<\/p>","8744e1d7":"After creating a blend function that can harmonize the results of multiple models, traning each model","8667ea52":"ex3) Utilities are extremely skewed to AllPub, so replace them with the lowest value.","11042959":"ex4) Garage Area is a missing value in the absence of Garage, so it is replaced by 0.","77a42709":"<h4>Many variables have missing values. Some variables have extremely large amounts of missing values.<\/h4>","849da0bc":"<h3 style=\"text-align:center;\">2. Optuna<\/h3><br>\n\n<p>\nThe hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n\nSo, I looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n\nThe hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n\nI looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n\nPreparations: 'optuna', 'functions-partial', objective function\n\nPre-understanding:\n\nOptuna is a framework that helps optimize hyperparameters. An objective function is required.\n\nOptuna's objective function selects a new hyperparameter combination of the model every trial.\n\nOptuna's study object is an object that performs optimization. The optimization of the study object requires a partial object and the number of attempts.\n\nThe partial object is an object that binds X, y with the objective function to be used by optuna.\n\nStudy objects store results that meet the purpose for each trial. Finally, remember the most purposeful hyperparameter combination.\n\nThe trial factor of objective embeds the function of specifying the range and value of hyperparameters. It has a hyperparameter name, range or list as a factor in common.\n    <ol>\n        <li><b>Suggest_int:<\/b> Select an integer value within the range.<\/li>\n        <li><b>Suggest_uniform:<\/b> Select an equal distribution value within a range.<\/li>\n        <li><b>Suggest_discrete_uniform:<\/b> Select a discrete uniform distribution value within the range.<\/li>\n        <li><b>Suggest_loguniform:<\/b> Select a logarithmic function linear value within a range.<\/li>\n        <li><b>Suggest_category:<\/b> Select a value in the list.<\/li>\n    <\/ol>\n<\/p>","0da02a30":"<h3 style=\"text-align:center;\">2. Clensing - Outlier<\/h3><br>\n<h4>Outline<\/h4>\nA dataset with outliers can degrade the performance of the model.<br>\nIt is optional to remove outliers existing in the training data. I found two outliers in the scatter plot of the variable with a strong correlation with the dependent variable.<br>\nThe two points were located in a place far off the straight line.<br><br>\n\n<h4>Finding outliers<\/h4>\n<ol>\n    <li><b>Statistics for univariate - normal distribution, IQR:<\/b><br>For independent variables, points outside the threshold can be judged as outliers using normal distributions, likelihood functions, IQR, etc.<br>\n        However, it cannot be judged that the point outside the threshold is always an outlier. For example, the scatterplot of GrLiv Area has two points apart at the top right.<br>\n        These differ greatly in value from other points, but they are important because they can prove linearity.<br>Therefore, it is necessary to carefully deal with the determination of outliers for univariate quantities.<\/li>\n    <li><b>Scattering point:<\/b><br>If you draw a scatterplot of two variables with a pattern (e.g., a linear relationship), you can intuitively find outliers.<br>\n        I previously found some variables that have a strong correlation with the dependent variable. Two outliers were found through GrLiv Area's scatter plot.<\/li>\n    <li><b>Clustering - DBSCAN:<\/b><br>DBSCAN can detect outliers using distance.<br>\n        DBSCAN has a set range (epsilon) and required peripheral points (min_samples), and generates clusters by calculating key points and peripheral points.<br>\n        Points that do not have key points around and do not have the minimum required neighboring points are outliers.<br>\n        I applied scaling to GrLiv Area and SalePrice and tried DBSCAN.<br><\/li>\n<\/ol>","eaf762df":"<a id=\"section2\"><\/a>\n# Feature Engineering","d41bce69":"<a id=\"section3\"><\/a>\n# Optimization (GridSearch, Optuna)<br>\nIn order to maximize the performance of the model, the process of finding the appropriate hyperparameters is required. I used GridSearch to find hyperparameters for each model.\n<br><br>\nSklearn's GridSearchCV was used for Laso, Ridge, ElasticNet, and SVM, and Optuna was used for tree-based models.","0eae7c25":"<b>However, we can see that MSSubClass is a Nominal variable.<br>\n<b>And we can see that OverallQual, QverallCond, and Year, Month are ordered (or ranked) variables.","84fe97a8":"<h3 style=\"text-align:center;\">3. Exploring independent variables - Check the missing values<\/h3>","ef140002":"ex2) Ally is a ranking variable that can have NA, and since these values have been treated as missing values, replace them with None.","94ecea55":"<p>\n    Variables that are currently missing.\n<\/p>","f07019c3":"<h2 style=\"text-align:center;\">Load Data<\/h2>","69445878":"<h3 style=\"text-align:center;\">2. Exploring independent variables - Define Type<\/h3><br>\n<p>\n    I classified the variables into <b style='color:crimson'>categorical variables and numeric variables.<\/b><br>\n    We can read the description of the variable, determine the type of variable, and further derive ideas for generating derived variables and converting data.<br><br>\n    The types of variables are classified as follows.<br>\n    <ol>\n        <li><b style='color:crimson'>Categorical<\/b><\/li>\n        <ol>\n            <li>Nominal vars<\/li>\n            <li>Order(Rank) vars<\/li>\n        <\/ol>\n        <li><b style='color:crimson'>Numerical<\/b><\/li>\n        <ol>\n            <li>Interval vars<\/li>\n            <li>Ratio vars<\/li>\n        <\/ol>\n    <\/ol>\n    The method of searching and preprocessing is determined by the type of variable.<br>\n    <br>\n    By using Pandas, we can find out the data type (numerical type and object) of the variable.<br>\n    However, not all numerical variables can be determined as continuous variables.<br>\n    I divided them subjectively by referring to <b style='color:crimson'>data_description file.<\/b><br>\n    <br>\n<\/p>\nCheck numerical and object variables!","a701be2b":"Create each model using the hyperparameter combination found earlier (Some parameters are corrected through several trials and errors).","fbf90520":"Create partial objects using the object function and create study objects of optuna.<br>\nBy optimizing study, parameters can be optimized.","f9bf7cc7":"<h3 style=\"text-align:center;\">1. GridSearch Cross Validation<\/h3><br>","bbe7ca42":"<h3 style=\"text-align:center;\">8. Training, test data division<\/h3><br>\nDivide the training data and test data again.","3e5546e5":"<h3 style=\"text-align:center;\">1. Remove ID<\/h3><br>\n<h4>Outline<\/h4>\nI removed the ID variable that was not needed for analysis.","70714f5c":"<h1 style='text-align: center;'>HousePrices Regression<\/h1><br>\n\n# Introduction\n<p>Hello!<br><br>This is my first notebook. After studying some notebooks of this competition, I summarized the overall basic process of regression analysis. This kernel introduces the following.<\/p>\n\n* [EDA](#section1)<br>\nThe process of searching for data is the most important step in analysis.<br>\nI will introduce how to define the type of each variable and how to identify and visualize the form.\n* [Feature Engineering](#section2)<br>\nThis part is the step of preprocessing each variable based on the EDA results.<br>\nI will introduce preprocessing tasks such as outlier processing, missing value processing, derivative variable generation, and variable conversion.\n* [Optimization (GridSearch, Optuna)](#section3)<br>\nThe more complex the model is, the more complex the hyperparameter setting is required.<br>\nHyperparameter combinations can have a significant impact on the performance of the model.<br>\nI will introduce how to optimize the hyperparameters of the model using GridSearchCV and Optuna.<\/li>\n* [Modeling](#section4)<br>\nThis part is the step of creating and learning basic models that can be used for regression analysis.<br>\nI will use linear regression models such as Lasso and Ridge, SVM, and some tree-based algorithms. And I will use Stacking to maximize generalization performance.<\/li>","34c10dce":"<h3 style=\"text-align:center;\">7. Select variables<\/h3><br>\nUsing variable selection can help improve the performance of the model.<br>\nThere are various approaches to the variable selection method, but I removed only a few variables that were most simply biased to one side.<br>","2361f76b":"Optuna can be used to visualize the optimal value for each trial or variable.","cd59bbce":"ex1) LotFrontage is replaced by the median value per Neighborhood.","8021412a":"<h3 style=\"text-align:center;\">1. Exploring dependent variables<\/h3><br>\nFirst, I tried to understand the dependent variable.<br>\nI calculated skewness and kurtosis to determine the normality of the dependent variable, and wrote a histogram and a QQ plot.<br>\n<h4 style='color:crimson'>Normality<\/h4>\nNormality means that the distribution of variables <b style='color:crimson'>follows a normal distribution.<\/b><br>\nThe easiest way to test normality is to draw a <b style='color:crimson'>Histogram and a QQ plot.<\/b><br>\nIt is recommended that the histogram form a bell shape. It is good to understand to see skewness and kurtosis together.<br>\nIf there is a shape extending along the baseline to the top right in the QQ plot, we can see that the data has normality.<br><br>\nIf it violates normality, <b style='color:crimson'>log transformation<\/b> or boxcox transformation can be applied.","03d022a3":"Create Object Function.<br>\nHyperparameters and cross-validation were implemented inside the function.","6cde3af1":"<h3 style=\"text-align:center;\">6. Derivative variable generation<\/h3><br>\n\nDerivative variables are methods that can improve the quality of analysis. There are several ideas for how to generate derivative variables.\n\nIt is a continuous variable and means an observation of an object, and if this value is 0, it means that there is no object.\nTherefore, it is possible to add categorical variables as to whether or not the object is present. Categorical variables with binary values can be stored as 0 and 1.\n\nIf you can express continuous variables of the same series in association, use a four-line operation.\nNew variables can be created. It is similar to calculating BMI with height and weight.\n\nThe above judgment can be used using min and max values after describe.\n\n","aec9b787":"<h4>I selected only the top 10 variables with the highest correlation with the dependent variable and conducted the correlation analysis again.<br>\n\"OverallQual\" and \"GrLiv Area\" are the strongest variables.<br>\nGarage Cars and Garage Area are also highly correlated. High correlation between independent variables is not good because it causes multicollinearity.<br><br>\nI used the following chart for some tests.<br>\n    <b style='color:crimson'>Scatter, Boxplot, resid plot, histogram, QQplot<\/b><\/h4>","c36d43ed":"<h3 style=\"text-align:center;\">5. Bivariate search - correlation analysis, heat map, finding important variables<\/h3><br>\n<h4>Outline<\/h4>\nCorrelation analysis is a technique to find out the correlation between the two variables.<br>\nWe can find variables with relatively strong <b>'linearity'<\/b> using the correlation coefficient with the dependent variable.<br>\nI designated these variables as relatively important variables and tested <b>'homogeneity'<\/b>.<br>\nIn addition, I tested <b>'independence'<\/b> through the correlation between independent variables.<br>\n<br>\n<h4>Some assumptions for a good regression model<\/h4>\nPreviously, we checked the normality of the dependent variable and the residual.<br>\nIn addition, several more assumptions are needed.<br>\n<ol>\n    <li>\n        <b>Linearity<\/b>:<br>\n        It is recommended that the independent variable and the dependent variable have a linear relationship.<br>\n        The 'variable transformation' or 'dimensional increase' method can help to have linearity.\n    <\/li>\n    <li>\n        <b>homogeneity:<\/b>:<br>\n        The variance of the residuals must be constant.<br>\n        Drawing a residual diagram for an independent variable can test the equal variance.<br>\n        If the points follow randomly based on the baseline, they satisfy the equal variance.\n    <\/li>\n    <li>\n        <b>Independence<\/b>:<br>\n        Independence means that there should be no correlation between independent variables.<br>\n        The high correlation between independent variables causes multicollinearity. As a result, the performance of the model becomes incredible.\n    <\/li>\n    <li>\n        <b>Irregularity<\/b>:<br>\n        There should be no correlation between residuals.<br>\n        Durbin-Watson' helps test for non-correlation.\n    <\/li>\n<\/ol>","e619db0e":"<h3 style=\"text-align:center;\">4. Clensing - Missing Value<\/h3><br>\n<h4>Outline<\/h4>\nMissing values are objects that must be removed. The missing values are processed based on the understanding of each independent variable.\n<br><br>\n<h4>How to deal with missing values.<\/h4>\n<ol>\n    <li><b>Delete:<\/b><br>\n        It is the easiest and most powerful way. Clear rows or columns. However, avoid erasing rows because missing values may also exist in the test data.<br>\n        Clearing the columns risks removing important variables, so avoid them if possible.<\/li>\n    <li><b>Replace a specific value:<\/b><br>\n        It is a way to try if you have knowledge of variables. For example, if there are extremely many missing values of iso-interstitial variables that can have negative meanings,<br>\n        Missing values are likely to have a negative meaning.\n    <li><b>Replacement of central propensity:<\/b><br>\n        It can be replaced with a central tendency value such as an average, a median value, and a minimum value. It is possible to replace the continuous variable with the median value and the category variable with the lowest value.<\/li>\n    <li><b>Other than that: Simple probability replacement, multiple confrontation method, etc.<\/b><br>\n<\/ol>\nAfter looking at the explanation of each variable, I treated it as follows.<br>\n<ul>\n    <li>\n        Continuous: Select one relevant category variable and replace the median for each category.<br>or\n        Alternate to 0 if it is bound to be a missing value.\n    <\/li>\n    <li>\n        Category type: value_counts to identify the distribution and replace it with None if there is zero negative meaning, such as NA or POOL. or<br>Select one related category variable, identify the distribution of values for each category, and replace the poorest value.\n    <\/li>\n<\/ul>"}}