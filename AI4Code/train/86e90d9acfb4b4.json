{"cell_type":{"4e9884c7":"code","c1feac97":"code","260726f0":"code","e6d688b9":"code","4a6ba28b":"code","aa996172":"code","22852bea":"code","6d5850f5":"code","94c5e924":"code","17a16fe0":"code","010b2477":"code","40120aac":"code","b9e5511f":"code","98d297e8":"code","433ec292":"code","590515a4":"code","447708c9":"code","cfa11ea7":"code","aa9c5caa":"code","fe874832":"code","ff5bb884":"code","d3d3ecb6":"code","6f32a893":"code","ae356744":"code","41957bb9":"code","2ae39e44":"code","8b1f0245":"code","58ca442d":"code","8e21e400":"code","7e8f22ff":"code","fec0ed4f":"code","ddd5af5d":"code","32c1bf32":"code","b340b621":"code","2ddff93a":"code","284119ef":"code","2efbd1e8":"code","649fd706":"code","78ecea04":"code","79bc4fc9":"code","bd357a52":"code","e3b394cc":"code","03519b0a":"code","13a93530":"code","74eb973f":"code","d442a945":"markdown","8ec748d3":"markdown","36372b8f":"markdown","9a3a5efa":"markdown","44a7d46b":"markdown","fcf7ae72":"markdown","fbea6561":"markdown","428a924c":"markdown","2d7db0b5":"markdown","f71698e9":"markdown","aa245453":"markdown","474d8e7e":"markdown","6ca2c58d":"markdown","5169bb33":"markdown","35965e83":"markdown"},"source":{"4e9884c7":"from collections import Counter\nimport json\nimport os\nimport random\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfrom scipy import sparse\nfrom scipy.sparse import linalg\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import euclidean_distances","c1feac97":"sns.set()\nsns.set_context('talk')","260726f0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e6d688b9":"NUM_KLAT_LINES = 5_343_564\nMIN_UNIGRAM_COUNT = 100\nWINDOW = 5\nMAX_PAGES = 100_000\nINTROS_ONLY = True\nkdwd_path = os.path.join(\"\/kaggle\", \"input\", \"kensho-derived-wikimedia-data\")","4a6ba28b":"def tokenizer(text):\n    return text.strip().lower().split()","aa996172":"class KdwdLinkAnnotatedText:\n    \n    def __init__(self, file_path, intros_only=False, max_pages=1_000_000_000):\n        self._file_path = file_path\n        self._intros_only = intros_only\n        self._max_pages = max_pages\n        self._num_lines = NUM_KLAT_LINES\n        self.pages_to_parse = min(self._num_lines, self._max_pages)\n        \n    def __iter__(self):\n        with open(self._file_path) as fp:\n            for ii, line in enumerate(fp):\n                page = json.loads(line)\n                for section in page['sections']:\n                    yield section['text']\n                    if self._intros_only:\n                        break\n                if ii + 1 >= self.pages_to_parse:\n                    break","22852bea":"file_path = os.path.join(kdwd_path, \"link_annotated_text.jsonl\")\nklat_intros_2 = KdwdLinkAnnotatedText(file_path, intros_only=True, max_pages=2)\nklat_intros = KdwdLinkAnnotatedText(file_path, intros_only=INTROS_ONLY, max_pages=MAX_PAGES)","6d5850f5":"two_intros = [intro for intro in klat_intros_2]","94c5e924":"two_intros","17a16fe0":"def filter_unigrams(unigrams, min_unigram_count):\n    tokens_to_drop = [\n        token for token, count in unigrams.items() \n        if count < min_unigram_count]                                                                                 \n    for token in tokens_to_drop:                                                             \n        del unigrams[token]\n    return unigrams","010b2477":"def get_unigrams(klat):\n    unigrams = Counter()\n    for text in tqdm(\n        klat, total=klat.pages_to_parse, desc='calculating unigrams'\n    ):\n        tokens = tokenizer(text)\n        unigrams.update(tokens)\n    return unigrams","40120aac":"unigrams = get_unigrams(klat_intros)\nprint(\"token count: {}\".format(sum(unigrams.values())))                          \nprint(\"vocabulary size: {}\".format(len(unigrams)))","b9e5511f":"unigrams = filter_unigrams(unigrams, MIN_UNIGRAM_COUNT)\nprint(\"token count: {}\".format(sum(unigrams.values())))                          \nprint(\"vocabulary size: {}\".format(len(unigrams))) ","98d297e8":"tok2indx = {tok: indx for indx, tok in enumerate(unigrams.keys())}\nindx2tok = {indx: tok for tok, indx in tok2indx.items()}","433ec292":"def get_skipgrams(klat, max_window, tok2indx, seed=938476):\n    rnd = random.Random()\n    rnd.seed(a=seed)\n    skipgrams = Counter()\n    for text in tqdm(\n        klat, total=klat.pages_to_parse, desc='calculating skipgrams'\n    ):\n        \n        tokens = tokenizer(text)\n        vocab_indices = [tok2indx[tok] for tok in tokens if tok in tok2indx]\n        num_tokens = len(vocab_indices)\n        if num_tokens == 1:\n            continue\n        for ii_word, word in enumerate(vocab_indices):\n            \n            window = rnd.randint(1, max_window)\n            ii_context_min = max(0, ii_word - window)\n            ii_context_max = min(num_tokens - 1, ii_word + window)\n            ii_contexts = [\n                ii for ii in range(ii_context_min, ii_context_max + 1) \n                if ii != ii_word]\n            for ii_context in ii_contexts:\n                context = vocab_indices[ii_context]\n                skipgram = (word, context)\n                skipgrams[skipgram] += 1 \n\n    return skipgrams","590515a4":"skipgrams = get_skipgrams(klat_intros, WINDOW, tok2indx)\nprint(\"number of unique skipgrams: {}\".format(len(skipgrams)))\nprint(\"number of skipgrams: {}\".format(sum(skipgrams.values())))\nmost_common = [\n    (indx2tok[sg[0][0]], indx2tok[sg[0][1]], sg[1]) \n    for sg in skipgrams.most_common(25)]\nprint('most common: {}'.format(most_common))","447708c9":"def get_count_matrix(skipgrams, tok2indx):\n    row_indxs = []                                                                       \n    col_indxs = []                                                                       \n    dat_values = []                                                                      \n    for skipgram in tqdm(\n        skipgrams.items(), \n        total=len(skipgrams), \n        desc='building count matrix row,col,dat'\n    ):\n        (tok_word_indx, tok_context_indx), sg_count = skipgram\n        row_indxs.append(tok_word_indx)\n        col_indxs.append(tok_context_indx)\n        dat_values.append(sg_count)\n    print('building sparse count matrix')\n    return sparse.csr_matrix((dat_values, (row_indxs, col_indxs)))","cfa11ea7":"count_matrix = get_count_matrix(skipgrams, tok2indx)","aa9c5caa":"# normalize rows\ncount_matrix_l2 = normalize(count_matrix, norm='l2', axis=1)","fe874832":"# demonstrate normalization\nirow=10\nrow = count_matrix_l2.getrow(irow).toarray().flatten()\nprint(np.sqrt((row*row).sum()))\n\nrow = count_matrix.getrow(irow).toarray().flatten()\nprint(np.sqrt((row*row).sum()))","ff5bb884":"xx1 = count_matrix.data\nxx2 = count_matrix_l2.data\nnbins = 30\n\nfig, axes = plt.subplots(1, 2, figsize=(18,8))\n\nax = axes[0]\ncounts, bins, patches = ax.hist(xx1, bins=nbins, density=True, log=True)\nax.set_xlabel('count_matrix')\nax.set_ylabel('fraction')\n\nax = axes[1]\ncounts, bins, patches = ax.hist(xx2, bins=nbins, density=True, log=True)\nax.set_xlabel('count_matrix_l2')\nax.set_xlim(-0.05, 1.05)\nax.set_ylim(1e-4, 5e1)\n\nfig.suptitle('Distribution of Embedding Matrix Values');","d3d3ecb6":"def ww_sim(word, mat, tok2indx, topn=10):\n    \"\"\"Calculate topn most similar words to word\"\"\"\n    indx = tok2indx[word]\n    if isinstance(mat, sparse.csr_matrix):\n        v1 = mat.getrow(indx)\n    else:\n        v1 = mat[indx:indx+1, :]\n    sims = cosine_similarity(mat, v1).flatten()\n    #dists = cosine_distances(mat, v1).flatten()\n    dists = euclidean_distances(mat, v1).flatten()\n    sindxs = np.argsort(-sims)\n    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n    return sim_word_scores","6f32a893":"word = 'city'","ae356744":"ww_sim(word, count_matrix, tok2indx)","41957bb9":"ww_sim(word, count_matrix_l2, tok2indx)","2ae39e44":"word = \"the\"\ncontext = \"of\"\nword_indx = tok2indx[word]\ncontext_indx = tok2indx[context]\nprint('pound_wc for ({},{}) from skipgrams: {}'.format(\n    word, context, skipgrams[(word_indx, context_indx)]))\nprint('pound_wc for ({},{}) from count_matrix: {}'.format(\n    word, context, count_matrix[word_indx, context_indx]))","8b1f0245":"sum_over_words = np.array(count_matrix.sum(axis=0)).flatten()    # sum over rows\nsum_over_contexts = np.array(count_matrix.sum(axis=1)).flatten() # sum over columns\n\npound_w_check1 = count_matrix.getrow(word_indx).sum()\npound_w_check2 = sum_over_contexts[word_indx]\nprint('pound_w for \"{}\" from getrow then sum: {}'.format(word, pound_w_check1))\nprint('pound_w for \"{}\" from sum_over_contexts: {}'.format(word, pound_w_check2))\n\npound_c_check1 = count_matrix.getcol(context_indx).sum()\npound_c_check2 = sum_over_words[context_indx]\nprint('pound_c for \"{}\" from getcol then sum: {}'.format(context, pound_c_check1))\nprint('pound_c for \"{}\" from sum_over_words: {}'.format(context, pound_c_check2))","58ca442d":"def get_ppmi_matrix(skipgrams, count_matrix, tok2indx, alpha=0.75):\n    \n    # for standard PPMI\n    DD = sum(skipgrams.values())\n    sum_over_contexts = np.array(count_matrix.sum(axis=1)).flatten()\n    sum_over_words = np.array(count_matrix.sum(axis=0)).flatten()\n        \n    # for context distribution smoothing (cds)\n    sum_over_words_alpha = sum_over_words**alpha\n    Pc_alpha_denom = np.sum(sum_over_words_alpha)\n        \n    row_indxs = []\n    col_indxs = []\n    ppmi_dat_values = []   # positive pointwise mutual information\n    \n    for skipgram in tqdm(\n        skipgrams.items(), \n        total=len(skipgrams), \n        desc='building ppmi matrix row,col,dat'\n    ):\n        \n        (tok_word_indx, tok_context_indx), pound_wc = skipgram\n        pound_w = sum_over_contexts[tok_word_indx]\n        pound_c = sum_over_words[tok_context_indx]\n        pound_c_alpha = sum_over_words_alpha[tok_context_indx]\n\n        Pwc = pound_wc \/ DD\n        Pw = pound_w \/ DD\n        Pc = pound_c \/ DD\n        Pc_alpha = pound_c_alpha \/ Pc_alpha_denom\n\n        pmi = np.log2(Pwc \/ (Pw * Pc_alpha))\n        ppmi = max(pmi, 0)\n        \n        row_indxs.append(tok_word_indx)\n        col_indxs.append(tok_context_indx)\n        ppmi_dat_values.append(ppmi)\n\n    print('building ppmi matrix')    \n    return sparse.csr_matrix((ppmi_dat_values, (row_indxs, col_indxs)))","8e21e400":"ppmi_matrix = get_ppmi_matrix(skipgrams, count_matrix, tok2indx)","7e8f22ff":"word = 'city'\nww_sim(word, ppmi_matrix, tok2indx)","fec0ed4f":"embedding_size = 200\nuu, ss, vv = linalg.svds(ppmi_matrix, embedding_size)","ddd5af5d":"print('vocab size: {}'.format(len(unigrams)))\nprint('ppmi size: {}'.format(ppmi_matrix.shape))\nprint('embedding size: {}'.format(embedding_size))\nprint('uu.shape: {}'.format(uu.shape))\nprint('ss.shape: {}'.format(ss.shape))\nprint('vv.shape: {}'.format(vv.shape))","32c1bf32":"# Dont do this for full run or we'll run out of RAM\n\n#x = (uu.dot(np.diag(ss)).dot(vv))[word_indx, :]\n#y = (uu.dot(np.diag(ss)).dot(vv))[context_indx, :]\n#print((x * y).sum())\n\n#x = (uu.dot(np.diag(ss)))[word_indx, :]\n#y = (uu.dot(np.diag(ss)))[context_indx, :]\n#print((x * y).sum())","b340b621":"p = 0.5\nsvd_word_vecs = uu.dot(np.diag(ss**p))\nprint(svd_word_vecs.shape)","2ddff93a":"nbins = 20\nfig, axes = plt.subplots(2, 2, figsize=(16,14), sharey=False)\n\nax = axes[0,0]\nxx = count_matrix.data\nax.hist(xx, bins=nbins, density=True, log=True)\nax.set_xlabel('word_counts')\nax.set_ylabel('fraction')\n\nax = axes[0,1]\nxx = count_matrix_l2.data\nax.hist(xx, bins=nbins, density=True, log=True)\nax.set_xlim(-0.05, 1.05)\nax.set_xlabel('word_counts_l2')\n\nax = axes[1,0]\nxx = ppmi_matrix.data\nax.hist(xx, bins=nbins, density=True, log=True)\nax.set_xlabel('PPMI')\nax.set_ylabel('fraction')\n\nax = axes[1,1]\nxx = svd_word_vecs.flatten()\nax.hist(xx, bins=nbins, density=True, log=True)\nax.set_xlabel('SVD(p=0.5)-PPMI')\n\nfig.suptitle('Distribution of Embedding Matrix Values');","284119ef":"word = 'car'\nsims = ww_sim(word, svd_word_vecs, tok2indx)\nfor sim in sims:\n    print('  ', sim)","2efbd1e8":"word = 'king'\nsims = ww_sim(word, svd_word_vecs, tok2indx)\nfor sim in sims:\n    print('  ', sim)","649fd706":"word = 'queen'\nsims = ww_sim(word, svd_word_vecs, tok2indx)\nfor sim in sims:\n    print('  ', sim)","78ecea04":"word = 'news'\nsims = ww_sim(word, svd_word_vecs, tok2indx)\nfor sim in sims:\n    print('  ', sim)","79bc4fc9":"word = 'hot'\nsims = ww_sim(word, svd_word_vecs, tok2indx)\nfor sim in sims:\n    print('  ', sim)","bd357a52":"svd_word_vecs.shape","e3b394cc":"from sklearn.manifold import TSNE","03519b0a":"svd_2d = TSNE(n_components=2, random_state=3847).fit_transform(svd_word_vecs)","13a93530":"svd_2d","74eb973f":"word='city'\nsize = 3\nindx = tok2indx[word]\ncen_vec = svd_2d[indx,:]\ndxdy = np.abs(svd_2d - cen_vec) \nbmask = (dxdy[:,0] < size) & (dxdy[:,1] < size)\nsub = svd_2d[bmask]\n\n\nfig, ax = plt.subplots(figsize=(15,15))\nax.scatter(sub[:,0], sub[:,1])\nax.set_xlim(cen_vec[0] - size, cen_vec[0] + size)\nax.set_ylim(cen_vec[1] - size, cen_vec[1] + size)\nfor ii in range(len(indx2tok)):\n    if not bmask[ii]:\n        continue\n    plt.annotate(\n        indx2tok[ii],\n        xy=(svd_2d[ii,0], svd_2d[ii,1]),\n        xytext=(5, 2),\n        textcoords='offset points',\n        ha='right',\n        va='bottom')","d442a945":"# Skipgrams\nNow lets calculate word-context pairs (i.e., skipgrams).  We will loop through each token in a section (the \"word\") and then use a `word2vec` style dynamic window to sample a context token to form skipgrams.","8ec748d3":"A few years ago I wrote a [kernel exploring word vectors](https:\/\/www.kaggle.com\/gabrielaltay\/word-vectors-from-pmi-matrix) calculated from Pointwise Mutual Information.  This is a reboot of that kernel using the Kensho Derived Wikimedia Dataset. This new version includes a dynamic context window, context distribution smoothing, and eigenvalue weighting.  ","36372b8f":"# Sparse Matrices\nWe will calculate several matrices that store word-word information. These matrices will be $N \\times N$  where $N \\approx 100,000$  is the size of our vocabulary. We will need to use a sparse format so that it will fit into memory. A nice implementation is available in [scipy.sparse.csr_matrix](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.csr_matrix.html). To create these sparse matrices we create three iterables that store row indices, column indices, and data values.","9a3a5efa":"# Pointwise Mutual Information Matrices\n\n## Definitions\n\n$$\n\\begin{align}\nPMI(w, c) = \n\\log \\frac\n{\\hat{P}(w,c)}\n{\\hat{P}(w)\\hat{P}(c)} =\n\\log \\frac\n{\\#(w,c) \\, \\cdot \\lvert D \\rvert}\n{\\#(w) \\cdot \\#(c)}\n\\\\\n\\\\\nPPMI(w, c) = {\\rm max} \\left[ PMI(w, c), 0 \\right]\n\\\\\n\\\\\n\\#(w) = \\sum_{c^{\\prime}} \\#(w, c^{\\prime}),\n\\quad\n\\#(c) = \\sum_{w^{\\prime}} \\#(w^{\\prime}, c)\n\\end{align}\n$$\n\n\n## Context Distribution Smoothing\n\n$$\n\\begin{align}\nPMI_{\\alpha}(w, c) = \n\\log \\frac\n{\\hat{P}(w,c)}\n{\\hat{P}(w)\\hat{P}_{\\alpha}(c)}\n\\\\\n\\\\\n\\hat{P}_{\\alpha}(c) = \n\\frac\n{\\#(c)^{\\alpha}}\n{\\sum_c \\#(c)^{\\alpha}}\n\\end{align}\n$$\n","44a7d46b":"# Word-Word Count Matrix\nOur very first word vectors will come from a word-word count matrix. This matrix is symmetric so we can (equivalently) take the word vectors to be the rows or columns. However we will try and code as if the rows are word vectors and the columns are context vectors.","fcf7ae72":"Note that the normalized vectors will produce the same similarities as the un-normalized vectors as long as we are using cosine similarity. ","fbea6561":"# Unigrams\nNow lets calculate a unigram vocabulary. The following code assigns a unique ID to each token, stores that mapping in two dictionaries (`tok2indx` and `indx2tok`), and counts how often each token appears in the corpus.","428a924c":"# Read Data and Preview\nLets start with a class to read the Wikipedia text.  We'll give ourselves the option to only use *Introduction* sections and to limit the number of pages.","2d7db0b5":"LGD15 use $\\#(w)$ to denote the number of times a word appears anywhere in the corpus and $\\#(c)$ to denote the number of times a context appears anywhere in the corpus.  We can calculate $\\#(w)$ by summing over the columns of `count_matrix` and $\\#(c)$ by summing over the rows in `count_matrix`.","f71698e9":"# Singular Value Decomposition\nWith the PPMI matrix in hand, we can apply a singular value decomposition (SVD) to create dense word vectors from the sparse ones we've been using.  SVD factorizes a matrix $M$ into a product of matrices $M = U \\cdot \\Sigma \\cdot V^T$ where $U$ and $V$ are orthonormal and $\\Sigma$ is a diagonal matrix of eigenvalues. By keeping the top $d$ elements of $\\Sigma$, we obtain $M_d = U_d \\cdot \\Sigma_d \\cdot V_d^T$. \n\nWord and context vectors are typically represented by:\n\n$$\nW = U_d \\cdot \\Sigma_d, \\quad C = V_d\n$$\n\nIt has been shown empirically that weighting the eigenvalue matrix can effect performance.  \n\n$$\nW = U_d \\cdot \\Sigma_d^p\n$$\n\nLGD15 suggest always using this weighting but that $p$ should be tuned to the task.  They investigate values of $p=0.5$ and $p=0$ (with $p=1$ corresponding to the traditional case).  Lets try $p=0.5$. ","aa245453":"Lets explain how these equations relate to our variables. LGD15 use $\\#(w,c)$ to denote the number of times a word-context pair appears in the corpus. We first calculated these numbers in our `skipgrams` variable and then stored them in `count_matrix`. The rows in `count_matrix` represent words and the columns represent contexts. Given a word token and a context token we can look up their indices in `tok2indx` and access the count via `skipgrams` or `count_matrix`","474d8e7e":"Lets check that dot-products between rows of $M_d$ are equal to dot-products between rows of $W$ where, \n\n$$\nM_d = U_d \\cdot \\Sigma_d \\cdot V_d^T, \\quad W = U_d \\cdot \\Sigma_d\n$$","6ca2c58d":"Now lets create our final embeddings.","5169bb33":"# Word Similarity with Sparse Count Matrices","35965e83":"# Kensho Derived Wikimedia Dataset - Word Vectors from Decomposing a Word-Word Pointwise Mutual Information Matrix\nLets create some simple [word vectors](https:\/\/en.wikipedia.org\/wiki\/Word_embedding) by applying a [singular value decomposition](https:\/\/en.wikipedia.org\/wiki\/Singular-value_decomposition) to a [pointwise mutual information](https:\/\/en.wikipedia.org\/wiki\/Pointwise_mutual_information) word-word matrix.  There are many other ways to create word vectors, but matrix decomposition is one of the most straightforward.  A well cited description of the technique used in this notebook can be found in Chris Moody's blog post [Stop Using word2vec](https:\/\/multithreaded.stitchfix.com\/blog\/2017\/10\/18\/stop-using-word2vec\/).    If you are interested in reading further about the history of word embeddings and a discussion of modern approaches check out the following blog post by Sebastian Ruder, [An overview of word embeddings and their connection to distributional semantic models](http:\/\/blog.aylien.com\/overview-word-embeddings-history-word2vec-cbow-glove\/).  Especially interesting to me is the work by Omar Levy, Yoav Goldberg, and Ido Dagan which shows that tuning hyperparameters is as (if not more) important as the algorithm chosen to build word vectors. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/570) (LGD15). \n\nWe will be using the [Kensho Derived Wikimedia Dataset](https:\/\/www.kaggle.com\/kenshoresearch\/kensho-derived-wikimedia-data) which contains the text of English Wikipedia from 2019-12-01. In this notebook tutorial we will implement as much as we can without using libraries that obfuscate the algorithm.  We're not going to write our own linear algebra or sparse matrix routines, but we will calculate unigram frequency, skipgram frequency, and the pointwise mutual information matrix \"by hand\".  We will also use the notation from LGD15 so you can follow along using that paper. Hopefully this will make the method easier to understand!   "}}