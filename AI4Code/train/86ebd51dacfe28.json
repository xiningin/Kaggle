{"cell_type":{"f9e5afa7":"code","56f69008":"code","e4bcf468":"code","e5290479":"code","a662ae73":"code","bcd3b9b2":"code","10f0388a":"code","ded849b9":"code","f1b26755":"code","8be6e0c1":"code","7c9c11f2":"code","f0f2a3be":"code","bca08b21":"code","93c7f57f":"code","88198db1":"code","2805c2bd":"code","a9f59973":"code","450da27a":"code","61ff3243":"code","aab261f0":"code","2558ebcd":"code","4215f756":"code","f5c33417":"code","9bb08184":"code","8637f4af":"code","778c0de7":"code","e60edbba":"code","283d94d8":"code","6454e8a1":"code","84dae9b4":"code","7a2cffe9":"code","f9f52431":"code","2c56ae81":"code","35b45e3f":"code","ad097875":"code","7b211da8":"markdown","45879bec":"markdown","c38aa996":"markdown","4e3955fc":"markdown","57ef6674":"markdown","838385bc":"markdown","90fdc036":"markdown","c96c76aa":"markdown","099b9a54":"markdown","148658e4":"markdown","d03a8c75":"markdown"},"source":{"f9e5afa7":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport statsmodels.api as sm\nimport seaborn as sns","56f69008":"bc = pd.read_csv('..\/input\/data.csv')","e4bcf468":"bc.head()","e5290479":"bc.describe()","a662ae73":"bc.info()","bcd3b9b2":"dum = pd.get_dummies(bc.diagnosis)\nbc = pd.concat([bc, dum], axis = 1)\nbc = bc.drop('diagnosis', axis = 1)\nbc = bc.drop('B', axis = 1)\nbc = bc.drop('Unnamed: 32', axis = 1)","10f0388a":"bc.head()","ded849b9":"bc.drop(['id'], axis = 1).hist(figsize = (14,14))\nplt.show()","f1b26755":"plt.figure(figsize = (12,10))\nsns.heatmap(bc.corr())\nplt.show()","8be6e0c1":"def num_densityplot():\n    for n in range(1, 31):\n        plt.subplot(9, 4, n)\n        bc.iloc[:, n].plot.kde()\n        plt.xlabel(bc.iloc[:, n].name)\n        \nplt.figure(figsize = (25, 60))\nnum_densityplot()\nplt.show()","7c9c11f2":"bc2 = bc[['radius_mean','perimeter_mean','area_mean','concavity_mean',\n         'concave points_mean','radius_worst','perimeter_worst','area_worst',\n         'concave points_worst']]","f0f2a3be":"a = pd.plotting.scatter_matrix(bc2, figsize = (15, 10))\nplt.xticks(rotation = 45)\nplt.yticks(rotation = 45)\nplt.show()","bca08b21":"sns.pairplot(bc, x_vars = bc2.columns[0: 4], y_vars = ['M'], kind = 'reg')\nplt.yticks([0.0, 1.0],['Benign', 'Malignant'])\nsns.pairplot(bc, x_vars = bc2.columns[4: ], y_vars = ['M'], kind = 'reg')\nplt.yticks([0.0, 1.0],['Benign', 'Malignant'])\nplt.show()","93c7f57f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom xgboost import XGBClassifier\n\nfrom yellowbrick.classifier import ConfusionMatrix","88198db1":"X = bc.drop(['id','M'], axis = 1).values\ny = bc.M","2805c2bd":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)","a9f59973":"def model_fit(x):\n    x.fit(X_train, y_train)\n    y_pred = x.predict(X_test)\n    model_fit.accuracy = accuracy_score(y_test, y_pred)\n    print('Accuracy Score',accuracy_score(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    model_cm = ConfusionMatrix(\n    x, classes = ['Malignat', 'Benign'],\n    label_encoder = {1 : 'Malignat', 0 : 'Benign'})\n    model_cm.fit(X_train, y_train)\n    model_cm.score(X_test, y_test)\n    \n    model_cm.poof() ","450da27a":"list = []\nfor i in range(1,10): \n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    \n    list.append(accuracy_score(y_test, y_pred))\n   \nfor n in range(0, len(list)):\n    if list[n] == max(list):\n        i = n+1","61ff3243":"model_fit(KNeighborsClassifier(n_neighbors = i))\nKNN = model_fit.accuracy","aab261f0":"from sklearn.linear_model import LogisticRegression","2558ebcd":"model_fit(LogisticRegression())\nLogistic = model_fit.accuracy","4215f756":"from sklearn.naive_bayes import GaussianNB","f5c33417":"model_fit(GaussianNB())\nGaussian = model_fit.accuracy","9bb08184":"from sklearn import tree","8637f4af":"model_fit(tree.DecisionTreeClassifier())\nTree = model_fit.accuracy","778c0de7":"from sklearn.ensemble import RandomForestClassifier","e60edbba":"model_fit(RandomForestClassifier(n_estimators = 100, max_depth =10, random_state = 1))\nRandomForest = model_fit.accuracy","283d94d8":"list=[]\nival = range(1, 100)\njval = range(1,100)\nfor i,j in zip(ival, jval): \n    clfr = RandomForestClassifier(n_estimators = i, max_depth = j, random_state = 1)\n    clfr.fit(X_train, y_train)\n    y_pred = clfr.predict(X_test)\n    \n    list.append((accuracy_score(y_test, y_pred)))","6454e8a1":"list = pd.DataFrame(list)","84dae9b4":"list[list == list.max()].dropna().head()","7a2cffe9":"from xgboost import XGBClassifier","f9f52431":"model_fit(XGBClassifier())\nXGBClf = model_fit.accuracy","2c56ae81":"scores_list_1 = ['KNN','Logistic','Gaussian','Tree','RandomForest','XGBClassifier']\nscores_1 = [KNN, Logistic, Gaussian, Tree, RandomForest, XGBClf]","35b45e3f":"score_df_classification = pd.DataFrame([scores_list_1, scores_1]).T","ad097875":"score_df_classification.index = score_df_classification[0]\ndel score_df_classification[0]\nscore_df_classification","7b211da8":"https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\/version\/2","45879bec":"In the data set the concavity is a bulge (a rounded swelling or protuberance that distorts a flat surface - synonyms:\tswelling, bump, lump, protuberance) in the cell.","c38aa996":"We are now going to use the Logistic Regression","4e3955fc":"We will now make a new model called random forest","57ef6674":"We can observe that there is a direct correlation between the x variable and the tumor's malignancy.  The higher the x variable the greater diagnois of malignancy. ","838385bc":"It appears that XGBClassifier is the best choice at 95.614%.  Using this model, we are able to predict a diagnsis of malignancy.","90fdc036":"These plots are showing the distribtution of the data over a continuous interval.","c96c76aa":"We will now be using the naive bayes model","099b9a54":"We will now make a new model - decsion tree","148658e4":"Data Dictionary\n\n1) ID number \n2) Diagnosis (M = malignant, B = benign) \n3-32) \n\nTen real-valued features are computed for each cell nucleus: \n\na) radius (mean of distances from center to points on the perimeter) \nb) texture (standard deviation of gray-scale values) \nc) perimeter \nd) area \ne) smoothness (local variation in radius lengths) \nf) compactness (perimeter^2 \/ area - 1.0) \ng) concavity (severity of concave portions of the contour) \nh) concave points (number of concave portions of the contour) \ni) symmetry \nj) fractal dimension (\"coastline approximation\" - 1)","d03a8c75":"We were able to see that 4, 5, and 7 was a better value giving us the highest score after using a loop using a range 1 to 10.  We will go with 7, choosing the odd number because it is advisable to take odd values for binary classification to avoid the ties (two class labels achieving the same score)."}}