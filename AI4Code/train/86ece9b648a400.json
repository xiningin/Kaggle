{"cell_type":{"394c4aab":"code","f5a6f747":"code","bcfa2996":"code","c9ecd256":"code","7e39a0d4":"code","4d972221":"code","dd0216a8":"code","d8fd5a78":"code","ba7fe37a":"code","f3e8a4fb":"code","20f8ffdf":"code","fcd2f2db":"code","cc6036de":"code","dafd71ba":"code","557e122b":"code","733413e2":"code","a473cb1f":"code","e0f48d77":"code","daf63506":"code","b73e3b68":"code","5b2eba00":"code","681f1b56":"code","819371b6":"code","324949dd":"code","452d54ed":"code","4fa07b6e":"code","bfbcc1bb":"code","33973033":"code","08f64969":"code","94ed755e":"code","12503e4c":"code","3ec98825":"code","855c0fa3":"code","49af0066":"code","fb4a620a":"code","336b43be":"code","50b0a3b7":"code","3f236108":"code","ce6aea95":"code","355f1ded":"code","bc422d8b":"code","39e0717a":"code","bcccf745":"code","ab05d48d":"code","580b14cb":"code","b3469e69":"code","5060ff6c":"code","7dc367df":"code","12e2c154":"code","95807f17":"code","a8517883":"code","01f406e8":"markdown","d01c4ef9":"markdown","2e4f0aab":"markdown","58655ad2":"markdown","5fc16b47":"markdown","04dd8400":"markdown","88e1df1a":"markdown","0ef10a47":"markdown","19f9ce83":"markdown","6ce608bd":"markdown","cb9787f9":"markdown","f49032d9":"markdown","3fe660c1":"markdown","bd2cc74b":"markdown","5b09508a":"markdown","40768711":"markdown","f37b0fc0":"markdown","596fcafa":"markdown","b4738a23":"markdown","5924c0a1":"markdown","2017bc09":"markdown","6333065e":"markdown","72b22851":"markdown","759c6588":"markdown","d8d502bd":"markdown","3d5cd3da":"markdown","facd1650":"markdown"},"source":{"394c4aab":"import numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\nimport time\nimport glob\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.pyplot import imshow\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport cv2\nimport shutil\nimport lxml\nimport xml.etree.ElementTree as ET\nfrom PIL import Image, ImageDraw, ImageEnhance\nimport albumentations as albu","f5a6f747":"dirlist = ['..\/input\/tbx-11\/TBX11K\/imgs\/health', '..\/input\/tbx-11\/TBX11K\/imgs\/sick', '..\/input\/tbx-11\/TBX11K\/imgs\/tb']\nclasses = ['Healthy', 'Sick', 'Tuberculosis']\nfilepaths = []\nlabels = []\nfor d, c in zip(dirlist, classes):\n    flist = os.listdir(d)\n    for f in flist:\n        fpath = os.path.join(d, f)\n        filepaths.append(fpath)\n        labels.append(c)\nprint ('filepaths: ', len(filepaths), '   labels: ', len(labels))","bcfa2996":"Fseries=pd.Series(filepaths, name='file_paths')\nLseries=pd.Series(labels, name='labels')\ndf=pd.concat([Fseries,Lseries], axis=1)\ndf=pd.DataFrame(np.array(df).reshape(8400,2), columns = ['file_paths', 'labels'])\nprint(df['labels'].value_counts())","c9ecd256":"file_count = 800\nsamples = []\nfor category in df['labels'].unique():    \n    category_slice = df.query(\"labels == @category\")\n    samples.append(category_slice.sample(file_count, replace=False,random_state=1))\ndf = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\nprint (df['labels'].value_counts())\nprint (len(df))","7e39a0d4":"df.head()","4d972221":"plt.figure(figsize=(14,10))\nfor i in range(20):\n    random = np.random.randint(1,len(df))\n    plt.subplot(4,5,i+1)\n    img = df.loc[random,\"file_paths\"]\n    plt.imshow(plt.imread(img))\n    plt.title(df.loc[random, \"labels\"], size = 10, color = \"black\") \n    plt.xticks([])\n    plt.yticks([])\n\n    \nplt.show()","dd0216a8":"train_df, test_df = train_test_split(df, train_size=0.95, random_state=0)\ntrain_df, valid_df = train_test_split(train_df, train_size=0.9, random_state=0)\nprint(train_df.labels.value_counts())\nprint(valid_df.labels.value_counts())\nprint(test_df.labels.value_counts())","d8fd5a78":"target_size=(224,224)\nbatch_size=16","ba7fe37a":"train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input, horizontal_flip=True)\ntest_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)\ntrain_gen = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\nvalid_gen = test_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\ntest_gen = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')","f3e8a4fb":"base_model = tf.keras.applications.EfficientNetB7(include_top=False, input_shape=(224,224,3), weights='imagenet')\nclassify_model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(), \n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.BatchNormalization(), \n    tf.keras.layers.Dropout(0.2), \n    tf.keras.layers.Dense(3, activation='softmax')\n])","20f8ffdf":"lr=0.001\nclassify_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])","fcd2f2db":"patience = 2\nstop_patience = 5\nfactor = 0.5\n\ncallbacks = [ \n    tf.keras.callbacks.ModelCheckpoint(\"classify_model.h5\", monitor='val_loss', save_best_only=True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=stop_patience, monitor='val_loss', verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, verbose=1)\n]","cc6036de":"epochs = 30\nhistory = classify_model.fit(train_gen, validation_data=valid_gen, epochs=epochs, callbacks=callbacks, verbose=1)","dafd71ba":"plt.plot(history.history['loss'], label='Loss (training data)')\nplt.plot(history.history['val_loss'], label='Loss (validation data)')\nplt.title('Loss for Training')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc=\"upper left\")\nplt.show()\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","557e122b":"best_model = classify_model\nbest_model.load_weights('.\/classify_model.h5')\nbest_model.evaluate(test_gen)","733413e2":"def create_boundbox_df(path):\n    xml_list = []\n    for xml_file in glob.glob(path + '\/*.xml'):\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        for member in root.findall('object'):\n            value = (root.find('filename').text,\n                     int(root.find('size')[0].text),\n                     int(root.find('size')[1].text),\n                     int(member[4][0].text),\n                     int(member[4][1].text),\n                     int(member[4][2].text),\n                     int(member[4][3].text)\n                     )\n            xml_list.append(value)\n    column_name = ['filename', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax']\n    bbox_df = pd.DataFrame(xml_list, columns=column_name)\n    return bbox_df\n\nbbox_df = create_boundbox_df('..\/input\/tbx-11\/TBX11K\/annotations\/xml')\nbbox_df.head()","a473cb1f":"width_ratio = 256 \/ bbox_df.width\nheight_ratio = 256 \/ bbox_df.height\nwidth_ratio = width_ratio.rename('width_ratio')\nheight_ratio = height_ratio.rename('height_ratio')\nbbox_df = pd.concat([bbox_df, width_ratio, height_ratio], axis=1)\nbbox_df.head()","e0f48d77":"true_xmin = bbox_df.xmin * bbox_df.width_ratio\ntrue_ymin = bbox_df.ymin * bbox_df.height_ratio\ntrue_width = (bbox_df.xmax - bbox_df.xmin) * bbox_df.width_ratio\ntrue_height = (bbox_df.ymax - bbox_df.ymin) * bbox_df.height_ratio\ntrue_xmin = true_xmin.rename('true_xmin')\ntrue_ymin = true_ymin.rename('true_ymin')\ntrue_width = true_width.rename('true_width')\ntrue_height = true_height.rename('true_height')\nbbox_df = pd.concat([bbox_df, true_xmin, true_ymin, true_width, true_height], axis=1)\nbbox_df.head()","daf63506":"bbox_df = bbox_df.round(2)\nbbox_df.head()","b73e3b68":"bbox = []\nfor i in range(len(bbox_df)):\n    x = bbox_df.iloc[i, 9]\n    y = bbox_df.iloc[i, 10]\n    w = bbox_df.iloc[i, 11]\n    h = bbox_df.iloc[i, 12]\n    bbox.append([x, y, w, h])\nbbox = pd.Series(bbox)\nbbox = bbox.rename('bbox')\nbbox_df = pd.concat([bbox_df, bbox], axis=1)\nbbox_df.head()","5b2eba00":"annots_df = pd.concat([bbox_df.filename, bbox_df.bbox], axis=1)\nannots_df.head()","681f1b56":"def bboxOnly(group):\n    return group['bbox']\n\nlabels = annots_df.groupby('filename').apply(bboxOnly)\nlabels.head()","819371b6":"def aggregator(filename):\n    bbox_list = []\n    for i in range(labels[filename].size):\n        bbox_list.append(labels[filename].values[i])\n    return bbox_list","324949dd":"bboxes = []\nfiles = []\nfor f in annots_df.filename:\n    bboxes.append(aggregator(f))\n    files.append(f)","452d54ed":"Fseries=pd.Series(files, name='file_id')\nLseries=pd.Series(bboxes, name='bbox')\nimage_annots_df=pd.concat([Fseries,Lseries], axis=1)\nimage_annots_df=pd.DataFrame(np.array(image_annots_df).reshape(1211,2), columns = ['file_id', 'bbox'])\nimage_annots_df = image_annots_df.drop_duplicates(subset =\"file_id\")\nimage_annots_df.head()","4fa07b6e":"image_annots_df = image_annots_df.set_index('file_id')\nimage_annots_df.head()","bfbcc1bb":"train_image_ids = np.unique(image_annots_df.index.values)[0:780]\nval_image_ids = np.unique(image_annots_df.index.values)[780:799]","33973033":"def load_image(image_id):\n    image = Image.open('..\/input\/tbx-11\/TBX11K\/imgs\/tb\/' + image_id + \".png\")\n    image = image.resize((256, 256))\n    \n    return np.asarray(image)","08f64969":"train_pixels = {}\ntrain_labels = {}\n\nfor image_id in train_image_ids:\n    train_pixels[image_id] = load_image(image_id)\n    train_labels[image_id] = image_annots_df.loc[image_id, 'bbox'].copy()\n\nval_pixels = {}\nval_labels = {}\n\nfor image_id in val_image_ids:\n    val_pixels[image_id] = load_image(image_id)    \n    val_labels[image_id] = image_annots_df.loc[image_id, 'bbox'].copy()","94ed755e":"for bbox in train_labels['tb0500']:\n    print(bbox)","12503e4c":"def draw_bboxes(image_id, bboxes, source='train'):  \n    image = Image.open('..\/input\/tbx-11\/TBX11K\/imgs\/tb\/'+ image_id + \".png\")\n    image = image.resize((256,256))\n    \n    draw = ImageDraw.Draw(image)\n            \n    for bbox in bboxes:\n        draw_bbox(draw, bbox)\n    \n    return np.asarray(image)\n\n\ndef draw_bbox(draw, bbox):\n    xmin, ymin, width, height = bbox\n    xmin = xmin\n    ymin = ymin\n    xmax = xmin + width\n    ymax = ymin + height\n    draw.rectangle([xmin, ymin, xmax, ymax], width=2, outline='red')\n    \ndef show_images(image_ids, bboxes, source='train'):\n    pixels = []\n    \n    for image_id in image_ids:\n        pixels.append(\n            draw_bboxes(image_id, bboxes[image_id], source)\n        )\n    \n    num_of_images = len(image_ids)\n    fig, axes = plt.subplots(\n        1, \n        num_of_images, \n        figsize=(5 * num_of_images, 5 * num_of_images)\n    )\n    \n    for i, image_pixels in enumerate(pixels):\n        axes[i].imshow(image_pixels)","3ec98825":"show_images(train_image_ids[0:4], train_labels)","855c0fa3":"class DataGenerator(tf.keras.utils.Sequence):\n\n    def __init__(self, image_ids, image_pixels, labels=None, batch_size=1, shuffle=False, augment=False):\n        self.image_ids = image_ids\n        self.image_pixels = image_pixels\n        self.labels = labels\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.on_epoch_end()\n        \n        self.image_grid = self.form_image_grid()\n        \n        \n    def form_image_grid(self):    \n        image_grid = np.zeros((32, 32, 4))\n\n        # x, y, width, height\n        cell = [0, 0, 256 \/ 32, 256 \/ 32] \n\n        for i in range(0, 32):\n            for j in range(0, 32):\n                image_grid[i,j] = cell\n\n                cell[0] = cell[0] + cell[2]\n\n            cell[0] = 0\n            cell[1] = cell[1] + cell[3]\n\n        return image_grid\n\ndef __len__(self):\n    return int(np.floor(len(self.image_ids) \/ self.batch_size))\n\n\ndef on_epoch_end(self):\n    self.indexes = np.arange(len(self.image_ids))\n\n    if self.shuffle == True:\n        np.random.shuffle(self.indexes)\n\n\nDataGenerator.__len__ = __len__\nDataGenerator.on_epoch_end = on_epoch_end","49af0066":"DataGenerator.train_augmentations = albu.Compose([\n        albu.HorizontalFlip(always_apply=False, p=0.5),\n#         albu.CLAHE(p=1),\n        albu.ToGray(p=1)\n    ], \n    bbox_params={'format': 'coco', 'label_fields': ['labels']})\n\nDataGenerator.val_augmentations = albu.Compose([\n#     albu.CLAHE(p=1),\n    albu.ToGray(p=1)\n])","fb4a620a":"def __getitem__(self, index):\n    indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n    batch_ids = [self.image_ids[i] for i in indexes]\n\n    X, y = self.__data_generation(batch_ids)\n\n    return X, y\n\n\ndef __data_generation(self, batch_ids):\n    X, y = [], []\n\n    # Generate data\n    for i, image_id in enumerate(batch_ids):\n        pixels = self.image_pixels[image_id]\n        bboxes = self.labels[image_id]\n\n        if self.augment:     \n            pixels, bboxes = self.augment_image(pixels, bboxes)\n        else:\n            pixels = self.contrast_image(pixels)\n            bboxes = self.form_label_grid(bboxes)\n\n        X.append(pixels)\n        y.append(bboxes)\n\n    return np.array(X), np.array(y)\n\n\ndef augment_image(self, pixels, bboxes):\n    bbox_labels = np.ones(len(bboxes))\n\n    aug_result = self.train_augmentations(image=pixels, bboxes=bboxes, labels=bbox_labels)\n\n    bboxes = self.form_label_grid(aug_result['bboxes'])\n\n    return np.array(aug_result['image']) \/ 255, bboxes\n\n\ndef contrast_image(self, pixels):        \n    aug_result = self.val_augmentations(image=pixels)\n    return np.array(aug_result['image']) \/ 255\n\n\nDataGenerator.__getitem__ = __getitem__\nDataGenerator.__data_generation = __data_generation\nDataGenerator.augment_image = augment_image\nDataGenerator.contrast_image = contrast_image","336b43be":"def form_label_grid(self, bboxes):\n    label_grid = np.zeros((32, 32, 10))\n\n    for i in range(0, 32):\n        for j in range(0, 32):\n            cell = self.image_grid[i,j]\n            label_grid[i,j] = self.rect_intersect(cell, bboxes)\n\n    return label_grid\n\n\ndef rect_intersect(self, cell, bboxes): \n    cell_x, cell_y, cell_width, cell_height = cell\n    cell_x_max = cell_x + cell_width \n    cell_y_max = cell_y + cell_height\n    \n    anchor_one = np.array([0, 0, 0, 0, 0])\n    anchor_two = np.array([0, 0, 0, 0, 0])\n\n    # check all boxes\n    for bbox in bboxes:\n        box_x, box_y, box_width, box_height = bbox\n        box_x_centre = box_x + (box_width \/ 2)\n        box_y_centre = box_y + (box_height \/ 2)\n\n        if(box_x_centre >= cell_x and box_x_centre < cell_x_max and box_y_centre >= cell_y and box_y_centre < cell_y_max):\n            \n            if anchor_one[0] == 0:\n                anchor_one = self.yolo_shape(\n                    [box_x, box_y, box_width, box_height], \n                    [cell_x, cell_y, cell_width, cell_height]\n                )\n            \n            elif anchor_two[0] == 0:\n                anchor_two = self.yolo_shape(\n                    [box_x, box_y, box_width, box_height], \n                    [cell_x, cell_y, cell_width, cell_height]\n                )\n                \n            else:\n                break\n\n    return np.concatenate((anchor_one, anchor_two), axis=None)\n\n\ndef yolo_shape(self, box, cell):\n    box_x, box_y, box_width, box_height = box\n    cell_x, cell_y, cell_width, cell_height = cell\n\n    # top left x,y to centre x,y\n    box_x = box_x + (box_width \/ 2)\n    box_y = box_y + (box_height \/ 2)\n\n    # offset bbox x,y to cell x,y\n    box_x = (box_x - cell_x) \/ cell_width\n    box_y = (box_y - cell_y) \/ cell_height\n\n    # bbox width,height relative to cell width,height\n    box_width = box_width \/ 256\n    box_height = box_height \/ 256\n\n    return [1, box_x, box_y, box_width, box_height]\n\n\nDataGenerator.form_label_grid = form_label_grid\nDataGenerator.rect_intersect = rect_intersect\nDataGenerator.yolo_shape = yolo_shape","50b0a3b7":"train_generator = DataGenerator(\n    train_image_ids,\n    train_pixels,\n    train_labels, \n    batch_size=6, \n    shuffle=True,\n    augment=True\n)\n\nval_generator = DataGenerator(\n    val_image_ids, \n    val_pixels,\n    val_labels, \n    batch_size=10,\n    shuffle=False,\n    augment=True\n)\n\nimage_grid = train_generator.image_grid","3f236108":"x_input = tf.keras.Input(shape=(256,256,3))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x_input)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n########## block 1 ##########\nx = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(2):\n    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n\n########## block 2 ##########\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(2):\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## block 3 ##########\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n    \n########## block 4 ##########\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## block 5 ##########\nx = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(4):\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## output layers ##########\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\npredictions = tf.keras.layers.Conv2D(10, (1, 1), strides=(1, 1), activation='sigmoid')(x)\n\nobject_detection_model = tf.keras.Model(inputs=x_input, outputs=predictions)","ce6aea95":"def custom_loss(y_true, y_pred):\n    binary_crossentropy = prob_loss = tf.keras.losses.BinaryCrossentropy(\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n    )\n    \n    prob_loss = binary_crossentropy(\n        tf.concat([y_true[:,:,:,0], y_true[:,:,:,5]], axis=0), \n        tf.concat([y_pred[:,:,:,0], y_pred[:,:,:,5]], axis=0)\n    )\n    \n    xy_loss = tf.keras.losses.MSE(\n        tf.concat([y_true[:,:,:,1:3], y_true[:,:,:,6:8]], axis=0), \n        tf.concat([y_pred[:,:,:,1:3], y_pred[:,:,:,6:8]], axis=0)\n    )\n    \n    wh_loss = tf.keras.losses.MSE(\n        tf.concat([y_true[:,:,:,3:5], y_true[:,:,:,8:10]], axis=0), \n        tf.concat([y_pred[:,:,:,3:5], y_pred[:,:,:,8:10]], axis=0)\n    )\n    \n    bboxes_mask = get_mask(y_true)\n    \n    xy_loss = xy_loss * bboxes_mask\n    wh_loss = wh_loss * bboxes_mask\n    \n    return prob_loss + xy_loss + wh_loss\n\n\ndef get_mask(y_true):\n    anchor_one_mask = tf.where(\n        y_true[:,:,:,0] == 0, \n        0.5, \n        5.0\n    )\n    \n    anchor_two_mask = tf.where(\n        y_true[:,:,:,5] == 0, \n        0.5, \n        5.0\n    )\n    \n    bboxes_mask = tf.concat(\n        [anchor_one_mask,anchor_two_mask],\n        axis=0\n    )\n    \n    return bboxes_mask","355f1ded":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nobject_detection_model.compile(\n    optimizer=optimizer, \n    loss=custom_loss\n)","bc422d8b":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath='.\/object_detection_model.h5', save_best_only=True)\n]","39e0717a":"object_detection_history = object_detection_model.fit(train_generator, validation_data=val_generator, epochs=80, callbacks=callbacks)","bcccf745":"plt.plot(object_detection_history.history['loss'], label='Loss (training data)')\nplt.plot(object_detection_history.history['val_loss'], label='Loss (validation data)')\nplt.title('Loss for Training')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc=\"upper left\")\nplt.show()","ab05d48d":"def prediction_to_bbox(bboxes, image_grid):    \n    bboxes = bboxes.copy()\n    \n    im_width = (image_grid[:,:,2] * 32)\n    im_height = (image_grid[:,:,3] * 32)\n    \n    # descale x,y\n    bboxes[:,:,1] = (bboxes[:,:,1] * image_grid[:,:,2]) + image_grid[:,:,0]\n    bboxes[:,:,2] = (bboxes[:,:,2] * image_grid[:,:,3]) + image_grid[:,:,1]\n    bboxes[:,:,6] = (bboxes[:,:,6] * image_grid[:,:,2]) + image_grid[:,:,0]\n    bboxes[:,:,7] = (bboxes[:,:,7] * image_grid[:,:,3]) + image_grid[:,:,1]\n    \n    # descale width,height\n    bboxes[:,:,3] = bboxes[:,:,3] * im_width \n    bboxes[:,:,4] = bboxes[:,:,4] * im_height\n    bboxes[:,:,8] = bboxes[:,:,8] * im_width \n    bboxes[:,:,9] = bboxes[:,:,9] * im_height\n    \n    # centre x,y to top left x,y\n    bboxes[:,:,1] = bboxes[:,:,1] - (bboxes[:,:,3] \/ 2)\n    bboxes[:,:,2] = bboxes[:,:,2] - (bboxes[:,:,4] \/ 2)\n    bboxes[:,:,6] = bboxes[:,:,6] - (bboxes[:,:,8] \/ 2)\n    bboxes[:,:,7] = bboxes[:,:,7] - (bboxes[:,:,9] \/ 2)\n    \n    # width,heigth to x_max,y_max\n    bboxes[:,:,3] = bboxes[:,:,1] + bboxes[:,:,3]\n    bboxes[:,:,4] = bboxes[:,:,2] + bboxes[:,:,4]\n    bboxes[:,:,8] = bboxes[:,:,6] + bboxes[:,:,8]\n    bboxes[:,:,9] = bboxes[:,:,7] + bboxes[:,:,9]\n    \n    return bboxes\n\ndef non_max_suppression(predictions, top_n):\n    probabilities = np.concatenate((predictions[:,:,0].flatten(), predictions[:,:,5].flatten()), axis=None)\n    \n    first_anchors = predictions[:,:,1:5].reshape((32*32, 4))\n    second_anchors = predictions[:,:,6:10].reshape((32*32, 4))\n    \n    bboxes = np.concatenate(\n        (first_anchors,second_anchors),\n        axis=0\n    )\n    \n    bboxes = switch_x_y(bboxes)\n    bboxes, probabilities = select_top(probabilities, bboxes, top_n=top_n)\n    bboxes = switch_x_y(bboxes)\n    \n    return bboxes\n\n\ndef switch_x_y(bboxes):\n    x1 = bboxes[:,0].copy()\n    y1 = bboxes[:,1].copy()\n    x2 = bboxes[:,2].copy()\n    y2 = bboxes[:,3].copy()\n    \n    bboxes[:,0] = y1\n    bboxes[:,1] = x1\n    bboxes[:,2] = y2\n    bboxes[:,3] = x2\n    \n    return bboxes\n\n\ndef select_top(probabilities, boxes, top_n=10):\n    top_indices = tf.image.non_max_suppression(\n        boxes = boxes, \n        scores = probabilities, \n        max_output_size = top_n, \n        iou_threshold = 0.3,\n        score_threshold = 0.3\n    )\n    \n    top_indices = top_indices.numpy()\n    \n    return boxes[top_indices], probabilities[top_indices]\n\ndef process_predictions(predictions, image_ids, image_grid):\n    bboxes = {}\n    \n    for i, image_id in enumerate(image_ids[:10]):\n        predictions[i] = prediction_to_bbox(predictions[i], image_grid)\n        bboxes[image_id] = non_max_suppression(predictions[i], top_n=100)\n        \n        # back to coco shape\n        bboxes[image_id][:,2:4] = bboxes[image_id][:,2:4] - bboxes[image_id][:,0:2]\n    \n    return bboxes","580b14cb":"val_predictions = object_detection_model.predict(val_generator)\nval_predictions = process_predictions(val_predictions, val_image_ids, image_grid)","b3469e69":"show_images(val_image_ids[0:4], val_predictions)","5060ff6c":"show_images(val_image_ids[0:4], val_labels)","7dc367df":"show_images(val_image_ids[4:8], val_predictions)","12e2c154":"show_images(val_image_ids[4:8], val_labels)","95807f17":"show_images(val_image_ids[8:10], val_predictions)","a8517883":"show_images(val_image_ids[8:10], val_labels)","01f406e8":"# **Import Libraries**","d01c4ef9":"# **Custom Callbacks**","2e4f0aab":"# **Object Detection Model**","58655ad2":"Disclaimer: I am very new to object detection so this isn't a great model. This is my first notebook on object detection too. I used this [notebook](https:\/\/www.kaggle.com\/mattbast\/object-detection-tensorflow-end-to-end#Evaluate-Model) heavily when creating this object detection model. Go check it out if you want. ","5fc16b47":"# **Callbacks**","04dd8400":"# **EfficientNetB7 Model**","88e1df1a":"# **Visualize Images with Bounding Boxes**","0ef10a47":"# **Organize Bounding Boxes by Filename**","19f9ce83":"Looks like we can do horizontal flips with ImageDataGenerator.","6ce608bd":"# **Create Dataframe with Bound Boxes**","cb9787f9":"# **Split Dataframe into Train, Valid, and Test**","f49032d9":"# **Balance Dataset**","3fe660c1":"# **Visualize Images**","bd2cc74b":"# **Split Bounding Boxes into Train, Valid, and Test**","5b09508a":"# **Reshape Bounding Boxes into COCO**","40768711":"# **Load Tuberculosis Images**","f37b0fc0":"# **ImageDataGenerator**","596fcafa":"# **Model Training**","b4738a23":"# **Prediction Post Processing**","5924c0a1":"# **Create Dataframe from Images**","2017bc09":"As we can see, the object detection model doesn't detect bounding boxes too well. ","6333065e":"# **Classification Model**","72b22851":"# **Create Custom Loss**","759c6588":"# **Visualizations of Predictions on Validation Dataset**","d8d502bd":"# **Create Custom Data Pipeline**","3d5cd3da":"# **Model Architecture**","facd1650":"# **Training**"}}