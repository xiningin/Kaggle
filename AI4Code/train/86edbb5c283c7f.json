{"cell_type":{"0ba1a680":"code","870b74b2":"code","7f118baf":"code","c81b0c34":"code","c7c26b85":"code","cf98412a":"code","c10e326a":"code","49fc11b5":"code","2c2f2c49":"code","e925e674":"code","9b977df5":"code","ededab9e":"code","61d9b80f":"code","95af4cc9":"code","1adb2104":"code","d664c24d":"code","48b1efd6":"code","c22c6cc3":"code","9b40aea3":"code","9053b566":"code","4d253bc4":"code","1c81c3e2":"code","4d698759":"code","ac517b56":"code","93c0f73d":"code","5210fdd8":"code","132982b3":"code","c85a693d":"code","cd605cd5":"code","0528652e":"code","107baaa6":"code","29444282":"code","29ccee3f":"code","0cae0937":"code","dc12d20a":"code","85a8b08f":"code","31876121":"code","d36474e2":"code","f410725c":"code","f0f98307":"code","4dc0e214":"code","6d26b802":"code","b35a6765":"code","17ff77d1":"code","68aa4aed":"code","c30e0163":"code","5ba465c4":"code","d1c66d33":"code","4ca64fd2":"markdown","1b046a7b":"markdown","0395f7d9":"markdown","07235dee":"markdown","3aafe61a":"markdown","19d2bee6":"markdown","3d64d700":"markdown","b829a125":"markdown","bcc3ec77":"markdown","f6e7751d":"markdown","e0f0eadd":"markdown","a095ba6a":"markdown","1d9be0bf":"markdown","3029ed94":"markdown","4bd17983":"markdown","13137917":"markdown","b6ae9335":"markdown","7f65a074":"markdown","60b45699":"markdown","33f13327":"markdown","2b6968bc":"markdown","0bb3c324":"markdown","efa41b7c":"markdown","150841b5":"markdown","4db9420e":"markdown","a26a8556":"markdown","85ae0bab":"markdown","42e5d0f9":"markdown","c87b8c4a":"markdown","4b026ce0":"markdown","832b07ae":"markdown"},"source":{"0ba1a680":"# import the all of libraries I need.\n\nfrom collections import defaultdict\nfrom functools import partial\nimport glob\nimport os\nimport os.path\nimport time\nfrom typing import List\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge, RANSACRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, PowerTransformer\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\npd.options.display.float_format = '{:3f}'.format","870b74b2":"# Declare constants\n\n# Number of cv iterations, expected to be > 2\nN_SPLITS = 9\n\n# Number of bins when stratifying `SalePrice`\nN_TARGET_BINS = 3\n\n# Random seed\nSEED = 2021","7f118baf":"# Show the all of filepaths\n!dir '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'","c81b0c34":"%%time\n# sample_submission.csv\nsample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\n# data_description.txt\nwith open('\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt') as f:\n    data_description = f.read()\n    f.close()\n\n# train.csv & test.csv\nnew_names = {\"BedroomAbvGr\": \"Bedroom\", \"KitchenAbvGr\": \"Kitchen\"}\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv').rename(columns=new_names)\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv').rename(columns=new_names)","c7c26b85":"train.head(10)","cf98412a":"test.head(10)","c10e326a":"sample_submission.head(10)","49fc11b5":"data_description","2c2f2c49":"# set utility for print data description\n\ndata_description_dict = defaultdict(lambda: {'Summary': '', 'Detail': ''})\n\n# If a line starts alphabet or numeric, it indicates that one data's explanation ends at previous line \n# and new data's explanation starts there. Otherwise, that line is still an explanation of previous one.\nalnum = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\nfor l in data_description.splitlines():\n    if not l:\n        continue\n    elif l[0] in alnum:\n        feature_name = l.split(':')[0].strip()\n        summary = [x for x in l.split(':')[1:]]\n        data_description_dict[feature_name]['Summary'] = l\n    else:\n        data_description_dict[feature_name]['Detail'] += f'{l}{os.linesep}'\n\n\ndef get_description_of(feature_name: str, contain_detail: bool = True) -> str:\n    '''Get data description of given feature.'''\n    if feature_name not in data_description_dict.keys():\n        raise KeyError(f'There are no description about {feature_name} in \"data_description.txt\"')\n    out_str = data_description_dict[feature_name]['Summary']\n    if contain_detail:\n        out_str = os.linesep.join([out_str, data_description_dict[feature_name]['Detail']])\n    return out_str.strip()","e925e674":"# Extract features' metadata (Number of unique values and data type)\n\n# Number of unique values\nn_unique_values = train.nunique() \\\n                       .reset_index() \\\n                       .rename(columns={'index': 'FeatureName', 0: 'NUniqueValues'})\n\n# Data type\ntypes_of_features = train.dtypes \\\n                         .reset_index() \\\n                         .rename(columns={'index': 'FeatureName', 0: 'Dtype'})\n\n# Join them\nmetadata = pd.merge(n_unique_values, types_of_features, on='FeatureName', how='inner') \\\n          .sort_values(['NUniqueValues', 'FeatureName']) \\\n          .reset_index(drop=True)\nmetadata['Dtype'] = metadata.Dtype.apply(str)  # convert np.dtype -> str\n\n# Show top 30 highest cardinality\nmetadata.tail(30)  ","9b977df5":"# Low cardinality integer features, some of them might be discrete\n# Print data_description and values of them\nmay_be_discrete = metadata.query('Dtype == \"int64\" and NUniqueValues < 30') \\\n                          .FeatureName.values\nfor feature_name in may_be_discrete:\n    print(f'''{get_description_of(feature_name)}\n    \\t<Unique values>\n        - train.csv: {np.sort(train[feature_name].unique())}\n        - test.csv : {np.sort(test[feature_name].unique())}\n    ''')","ededab9e":"# Visualize coefficient correration to SalePrice by Heatmap\n\nfig = plt.figure(figsize=(15, 15))\ncorr_coef = train[may_be_discrete.tolist() + ['SalePrice']].corr()\nax = sns.heatmap(corr_coef, annot=True, vmin=-1, vmax=1)\nax.set_title('Linear correration between target', fontsize=16)\nsns.despine()","61d9b80f":"# Store list of feature names\n\n# descrete features\nobject_type_features = [c for c in train.select_dtypes('object').columns\n                        if c not in ('Id', 'SalePrice')]\ndiscrete_features = object_type_features \\\n                  + ['BsmtHalfBath', 'Kitchen', 'OverallCond', 'YrSold', 'MoSold', 'MSSubClass']\n\n# continuous features\ncontinuous_features = [c for c in train.columns\n                       if c not in ('Id', 'SalePrice') and c not in discrete_features]\n\n# all features\nall_features = continuous_features + discrete_features\n\n# Display number of features (discrete_features, continuous_features, all_features)\nlen(discrete_features), len(continuous_features), len(all_features)","95af4cc9":"%%time\n# train.csv\nmissing_value_rate_train = train.isnull().sum() \/ train.shape[0]\nmissing_value_rate_train = missing_value_rate_train[missing_value_rate_train > 0].sort_index()\n\n# test.csv\nmissing_value_rate_test = test.isnull().sum() \/ test.shape[0]\nmissing_value_rate_test = missing_value_rate_test[missing_value_rate_test > 0].sort_index()\n\n# Visualize\nfig = plt.figure(figsize=(12, 6.5))\nplt.suptitle('Rate of missing values (show > 0% only)', fontsize=16)\nax1 = plt.subplot(1, 2, 1)\nsns.barplot(x=missing_value_rate_train, y=missing_value_rate_train.index, ax=ax1)\nax1.set_title('train.csv')\nsns.despine()\nax2 = plt.subplot(1, 2, 2)\nsns.barplot(x=missing_value_rate_test, y=missing_value_rate_test.index, ax=ax2)\nax2.set_title('test.csv')\nsns.despine()","1adb2104":"# Get feature names contains missing values\nfeatures_contains_null = missing_value_rate_train.index.tolist() \\\n                         + missing_value_rate_test.index.tolist()\nfeatures_contains_null = list(set(features_contains_null))  # remove duplication\n\n# Print description\nfor feature_name in sorted(features_contains_null):\n    type_ = 'Discrete' if feature_name in discrete_features else 'Continuous'\n    rate_train = 0 if feature_name not in missing_value_rate_train.index \\\n                 else missing_value_rate_train[feature_name]\n    rate_test = 0 if feature_name not in missing_value_rate_test.index \\\n                else missing_value_rate_test[feature_name]\n    print(f'''{get_description_of(feature_name)}\n        <Data type>\n        - {type_}\n        <Rate of missing values>\n        - train.csv: {rate_train:.5f}\n        - test.csv : {rate_test:.5f}\n    ''')","d664c24d":"def is_nan(x):\n    return isinstance(x, float)","48b1efd6":"# Get feature names\nmasvnr_features = [f for f in train.columns if f.startswith('MasVnr')]\nmasvnr_features","c22c6cc3":"# Print description\nfor feature_name in masvnr_features:\n    type_ = 'Discrete' if feature_name in discrete_features else 'Continuous'\n    rate_train = 0 if feature_name not in missing_value_rate_train.index \\\n                 else missing_value_rate_train[feature_name]\n    rate_test = 0 if feature_name not in missing_value_rate_test.index \\\n                else missing_value_rate_test[feature_name]\n    print(get_description_of(feature_name) + f'''\\t<Data type>\n        - {type_}\n        <Rate of missing values>\n        - train.csv: {rate_train:.5f}\n        - test.csv : {rate_test:.5f}\n    ''')","9b40aea3":"# Display row of dataframe whose `MasVnrType` is np.nan\ndisplay(train[train.MasVnrType.apply(is_nan)][masvnr_features])\ndisplay(test[test.MasVnrType.apply(is_nan)][masvnr_features])","9053b566":"def cleanse_mas_vnr_area(df: pd.DataFrame) -> np.ndarray:\n    mas_vnr_area = df.MasVnrArea.copy()\n    mas_vnr_area[df.MasVnrType.apply(is_nan)] = np.nan\n    return mas_vnr_area.values","4d253bc4":"# Get feature names\nbsmt_features = [f for f in train.columns if f.startswith('Bsmt')]\nbsmt_features","1c81c3e2":"# Print description\nfor feature_name in bsmt_features:\n    type_ = 'Discrete' if feature_name in discrete_features else 'Continuous'\n    rate_train = 0 if feature_name not in missing_value_rate_train.index \\\n                 else missing_value_rate_train[feature_name]\n    rate_test = 0 if feature_name not in missing_value_rate_test.index \\\n                else missing_value_rate_test[feature_name]\n    print(get_description_of(feature_name) + f'''\\t<Data type>\n        - {type_}\n        <Rate of missing values>\n        - train.csv: {rate_train:.5f}\n        - test.csv : {rate_test:.5f}\n    ''')","4d698759":"# Display information of rows whose `BsmtCond` is np.nan\ndisplay(train[train.BsmtCond.apply(is_nan)][bsmt_features ].info())\ndisplay(test[test.BsmtCond.apply(is_nan)][bsmt_features].info())","ac517b56":"# Display rows whose `BsmtCond` is np.nan but 3 other Bmst* features are not\ntest[test.BsmtCond.apply(is_nan) & ~test.BsmtQual.apply(is_nan)][bsmt_features]","93c0f73d":"garage_features = ['GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt']\ndisplay(train[train.GarageCond.apply(is_nan)][garage_features].info())\ndisplay(test[test.GarageCond.apply(is_nan)][garage_features].info())","5210fdd8":"# Display rows whose `GarageCond` is np.nan but `GarageType` not\ntest[test.GarageCond.apply(is_nan) & ~test.GarageType.apply(is_nan)][garage_features]","132982b3":"# Count number of rows by `GarageCond` in \"train.csv\", whose `GarageType` is 'Detchd'\ntrain[train.GarageType == \"Detchd\"].GarageCond.value_counts(dropna=False)","c85a693d":"class PassthroughTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        '''Do nothing'''\n        return X","cd605cd5":"def get_ridge_model(continuous_features: List[str],\n                    discrete_features  : List[str],\n                    *,\n                    n_jobs             : int = -1,\n                    random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('contiouous_imputer', SimpleImputer(strategy='median')),\n        ('scaler', PowerTransformer())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', Ridge(random_state=random_state))\n    ])\n    return model","0528652e":"def get_ransac_model(continuous_features: List[str],\n                     discrete_features  : List[str],\n                     *,\n                     n_jobs             : int = -1,\n                     random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('contiouous_imputer', SimpleImputer(strategy='median')),\n        ('scaler', PowerTransformer())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', RANSACRegressor(random_state=random_state))\n    ])\n    return model","107baaa6":"def get_svm_model(continuous_features: List[str],\n                  discrete_features  : List[str],\n                  *,\n                  n_jobs             : int = -1,\n                  random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('contiouous_imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', SVR())\n    ])\n    return model","29444282":"def get_knn_model(continuous_features: List[str],\n                  discrete_features  : List[str],\n                  *,\n                  n_jobs             : int = -1,\n                  random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('contiouous_imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', KNeighborsRegressor(n_jobs=n_jobs))\n    ])\n    return model","29ccee3f":"def get_catboost_model(continuous_features: List[str],\n                       discrete_features  : List[str],\n                       *,\n                       n_jobs             : int = -1,\n                       random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('do_nothing', PassthroughTransformer())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OrdinalEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    reg = CatBoostRegressor(random_state=random_state,\n                            logging_level='Silent')\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', reg)\n    ])\n    return model","0cae0937":"def get_xgb_model(continuous_features: List[str],\n                  discrete_features  : List[str],\n                  *,\n                  n_jobs             : int = -1,\n                  random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('do_nothing', PassthroughTransformer())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OrdinalEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', XGBRegressor(n_jobs=n_jobs, random_state=random_state))\n    ])\n    return model","dc12d20a":"def get_lgbm_model(continuous_features: List[str],\n                   discrete_features  : List[str],\n                   *,\n                   n_jobs             : int = -1,\n                   random_state       : int = SEED) -> Pipeline:\n    continuous_transformer = Pipeline(steps=[\n        ('do_nothing', PassthroughTransformer())\n    ])\n    discrete_transformer = Pipeline(steps=[\n        ('discrete_imputer', SimpleImputer(strategy='constant', fill_value='NaN')),\n        ('encoder', OrdinalEncoder(handle_unknown='ignore'))\n    ])\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('continuous', continuous_transformer, continuous_features),\n            ('discrete', discrete_transformer, discrete_features)\n        ]\n    )\n    model = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', LGBMRegressor(n_jobs=n_jobs,\n                                    random_state=random_state,\n                                    importance_type='gain'))\n    ])\n    return model","85a8b08f":"model_factories = {\n    'CatBoost': get_catboost_model,\n    'LightGBM': get_lgbm_model,\n    'XGBoost' : get_xgb_model,\n    'KNN'     : get_knn_model,\n    'SVM'     : get_svm_model,\n    'RANSAC'  : get_ransac_model,\n    'Ridge'   : get_ridge_model\n}","31876121":"# Function to calculate RMSE (competition metrics)\nmetric_func = partial(mean_squared_error, squared=False)","d36474e2":"def get_kfold_splitter(random_state: int = SEED,\n                       n_splits    : int = N_SPLITS):\n    return KFold(n_splits=n_splits, random_state=random_state, shuffle=True)","f410725c":"def get_stratifiedkfold_splitter(random_state: int = SEED,\n                                 n_splits    : int = N_SPLITS):\n    return StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)","f0f98307":"splitter_factories = {\n    'NonStratified': get_kfold_splitter,\n    'Stratified'   : get_stratifiedkfold_splitter\n}\n\n# Prepare for Stratified-KFold\nsplitter = StratifiedKFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\ntrain['SalePrice'] = np.log(train.SalePrice)\ntarget_bin = pd.cut(train.SalePrice, N_TARGET_BINS, labels=list(range(N_TARGET_BINS)), retbins=True)[0]\ntarget_bin.value_counts()","4dc0e214":"%%time\n\nsince = time.time()\n\n# Store prediction result here\npred_train, pred_valid = pd.DataFrame(), pd.DataFrame()\n\n# Store trained regressors here \n# (key: model name, value: list of trained regressor (length = number of cv iterations))\nregressors = defaultdict(list)\n\n# Store metrics here (Key: model name, valud: cv metrics)\ntrain_metrics, valid_metrics = {}, {}\n\n# Train & evaluate regressors\ncombinations = [(algorithm, model_factory, cv, splitter_factory)\n                for algorithm, model_factory in model_factories.items()\n                for cv, splitter_factory in splitter_factories.items()]\nfor algorithm, model_factory, cv, splitter_factory in combinations:\n    # Model name, prefix of model identifier\n    # (i.e. \"NonStratified-CatBoost\", \"Stratified-SVM\")\n    model_name = '-'.join([cv, algorithm])\n    print('Start to train {} with {} cross validation ({:.3f} seconds passed)' \\\n          .format(algorithm, cv, time.time() - since))\n\n    # Run cross validation   \n    splitter = splitter_factory()\n    y = None if cv == 'NonStratified' else target_bin.copy()\n    train_metrics_, valid_metrics_ = [], []\n    for i, (train_index, valid_index) in enumerate(splitter.split(train, y)):\n\n        num_fold = i + 1  # Number of cv iteration, starting from 1\n        print('Fold {}\/{} start!'.format(num_fold, N_SPLITS))\n\n        # Split dataset into train\/validation fold\n        # X_*: features, y_*: training data, k_*: row identifier\n        (X_train, X_valid, y_train, y_valid, k_train, k_valid) = (\n                train.iloc[train_index][all_features],\n                train.iloc[valid_index][all_features],\n                train.iloc[train_index].SalePrice,\n                train.iloc[valid_index].SalePrice,\n                train.iloc[train_index].Id,\n                train.iloc[valid_index].Id\n        )\n\n        # Train regressor\n        regressor = model_factory(continuous_features=continuous_features,\n                                  discrete_features=discrete_features)\n        print('Start training({:.3f} seconds passed)'.format(time.time() - since))\n        regressor.fit(X_train, y_train)\n        print('Complete training({:.3f} seconds passed)'.format(time.time() - since))\n\n        # Store regressor\n        regressors[model_name].append(regressor)\n        \n        # Evaluate regressor by RMSE\n        pred_train_ = regressor.predict(X_train)\n        train_metric = metric_func(y_train, pred_train_)\n        pred_valid_ = regressor.predict(X_valid)\n        valid_metric = metric_func(y_valid, pred_valid_)\n        print('Train loss: {:.5f}, Validation loss: {:.5f}'.format(train_metric, valid_metric))\n        \n        # Store metrics\n        train_metrics_.append(train_metric)\n        valid_metrics_.append(valid_metric)\n\n        # Store prediction result\n        temp_pred_train = pd.DataFrame()  # Train set\n        temp_pred_train['Id']         = k_train.values\n        temp_pred_train['Prediction'] = pred_train_\n        temp_pred_train['Actual']     = y_train.values\n        temp_pred_train['Algorithm']  = algorithm\n        temp_pred_train['CV']         = cv\n        temp_pred_train['ModelName']  = model_name\n        temp_pred_train['NumFold']    = num_fold\n        pred_train = pd.concat([pred_train, temp_pred_train])\n        temp_pred_valid = pd.DataFrame()  # Validation set\n        temp_pred_valid['Id']         = k_valid.values\n        temp_pred_valid['Prediction'] = pred_valid_\n        temp_pred_valid['Actual']     = y_valid.values\n        temp_pred_valid['Algorithm']  = algorithm\n        temp_pred_valid['CV']         = cv\n        temp_pred_valid['ModelName']  = model_name\n        temp_pred_valid['NumFold']    = num_fold\n        pred_valid = pd.concat([pred_valid, temp_pred_valid])\n    print('Complete to train {} with {} cross validation ({:.3f} seconds passed)' \\\n          .format(algorithm, cv, time.time() - since))\n\n    # Aggregate cv result\n    print('Train metrics: mean\/std = {:.5f}\/{:.5f}, Validation metrics: mean\/std = {:.5f}\/{:.5f}' \\\n          .format(np.mean(train_metrics_), np.std(train_metrics_),\n                  np.mean(valid_metrics_), np.std(valid_metrics_)))\n    train_metrics[model_name] = np.mean(train_metrics_)\n    valid_metrics[model_name] = np.mean(valid_metrics_)\n\n# Store cv prediction\npred_train.to_csv('prediction_train.zip', index=False)\npred_valid.to_csv('prediction_valid.zip', index=False)","6d26b802":"fig = plt.figure(figsize=(16, 6))\nplt.suptitle('Metrics (mean RMSE of cross validation)', fontsize=16)\nax1 = plt.subplot(1, 2, 1)\nax1 = sns.barplot(y=list(train_metrics.keys()), x=list(train_metrics.values()), ax=ax1)\nax1.set_title('Training fold')\nsns.despine()\nax2 = plt.subplot(1, 2, 2)\nax2 = sns.barplot(y=list(valid_metrics.keys()), x=list(valid_metrics.values()), ax=ax2)\nax2.set_title('Validation fold')\nsns.despine()","b35a6765":"# Extract feature importances from tree models.\ndef is_name_of_tree_model(model_name: str) -> bool:\n    '''Return True if given model_name is assumed to be one of tree model.'''\n    return (model_name.endswith('CatBoost') or\n            model_name.endswith('LightGBM') or\n            model_name.endswith('XGBoost'))\n\ntree_models = []\nfeature_importances = pd.DataFrame(index=all_features)\nfor model_name, list_of_regressors in regressors.items():\n    if not is_name_of_tree_model(model_name):\n        continue\n    tree_models.append(model_name)\n    feature_importances[model_name] = 0\n    for regressor in list_of_regressors:\n        final_estimator = regressor[-1]\n        feature_importances[model_name] += final_estimator.feature_importances_\n    feature_importances[model_name] = feature_importances[model_name] \/ len(list_of_regressors)\nfeature_importances = feature_importances \\\n                     .reset_index() \\\n                     .rename(columns={'index': 'Feature'})\nfeature_importances","17ff77d1":"# Visualize\ntop_n = 25\nfig = plt.figure(figsize=(15, 18))\nplt.suptitle(f'Top {top_n} important features by model', fontsize=18)\nfor i, model_name in enumerate(tree_models):\n    plt.subplot(3, 2, i + 1)\n    sns.barplot(y='Feature', x=model_name,\n                data=feature_importances.sort_values(model_name, ascending=False)[:top_n])\n    plt.xlabel('Importance')\n    plt.title(model_name)\n    sns.despine()","68aa4aed":"prediction = pd.DataFrame(index=test.Id)\nfor model_name, list_of_regressors in regressors.items():\n    prediction[model_name] = 0\n    for regressor in list_of_regressors:\n        prediction[model_name] += regressor.predict(test[all_features].copy())\n    prediction[model_name] = prediction[model_name] \/ len(list_of_regressors)\nprediction","c30e0163":"plt.figure(figsize=(12.5, 12.5))\nplt.title('Correlation coefficients between models', fontsize=16)\nsns.heatmap(prediction.corr(), vmax=1, annot=True)\nsns.despine()","5ba465c4":"# Blending\nprediction['Blend'] = prediction.median(axis=1)\nprediction","d1c66d33":"# Write and visualize my prediction results.\n\nfigure = plt.figure(figsize=(18, 18))\nfigure.tight_layout()\nplt.suptitle('Distribution of predicted by model', fontsize=16)\nfor i, model_name in enumerate(prediction.columns):\n    # Write submission files.\n    sample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col=['Id'])\n    my_submission = pd.merge(sample_submission.drop(columns=['SalePrice']).reset_index(),\n                             prediction[[model_name]].reset_index(),\n                             how='inner', on='Id')\n    assert(my_submission.shape[0] == sample_submission.shape[0])\n    my_submission.rename(columns={model_name: 'SalePrice'}, inplace=True)\n    my_submission['SalePrice'] = np.exp(my_submission['SalePrice'])  # log scale -> original scale\n    my_submission.to_csv(f'submission_{model_name}.csv', index=False)\n\n    # Visualize\n    ax = plt.subplot(4, 4, i + 1)\n    ax.set_title(model_name)\n    sns.histplot(my_submission.SalePrice)\n    ax.set_xlabel('')\n    sns.despine()","4ca64fd2":"## How to handle missing values","1b046a7b":"## Feature importances of tree models","0395f7d9":"# Prediction","07235dee":"There are no _np.nan_. I therefore think it is a data error and `GarageType` should be _np.nan_ if `GarageCond` is _np.nan_.","3aafe61a":"According to _np.nan_ `GarageCond` means there are no garage, and 'Detchd' `GaregeType` means garage is detached from home. I think it is data error because in \"train.csv\", there are 387 rows whose `GarageType` is 'Detchd', but all of their `GarageCond` is not _np.nan_.","19d2bee6":"### 1. `MasVnr*` features","3d64d700":"Features, whose \"Dtype\" is _object_, should be discrete features. There are, however some _int64_ features of low cardinality (meaning that having less than 30 unique values). Are they really continuous features? I will determine that by data description and correration coefficients to `SalePrice`.","b829a125":"# Constants","bcc3ec77":"Load all of the dataset.  \nBy the way, about \"train.csv\" and \"test.csv\", I will rename 2 columns' name here. I assume that true names of `BedroomAbvGr` and `KitchenAbvGr` in \"train.csv\" and \"test.csv\" are `Bedroom` and `Kitchen`. It is because, \"train.csv\" and \"test.csv\" have `BedroomAbvGr` and `KitchenAbvGr` in their columns, which cannot be, however, found in \"data_description.txt\". Additionally there are description about `Bedroom` and `Kitchen` in \"data_description.txt\", both dose \nnot exist in columns of \"train.csv\" and \"test.csv\".","f6e7751d":"# Table of content\n\n- [Libraries](#Libraries)\n- [Load dataset](#Load-dataset)\n- [Feature engineering](#Feature-engineering)\n- [Train regressors](#Train-regressors)\n- [Compare cross validation results](#Compare-cross-validation-results)","e0f0eadd":"In \"train.csv\", all of `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, and `BsmtQual` are _np.nan_ if `BsmtCond` is _np.nan_ (meaing No Basement in house), but not always in \"test.csv\".","a095ba6a":"The graphs above indicates followings.\n\n- Except for `Electrical`, if a feature has any missing values in \"train.csv\", that feature also has ones in \"test.csv\".\n- More features have missing values in \"test.csv\" than \"train.csv\".\n- Missing rate of `Alley`, `Fence`, `FirePlaceQu`, `MiscFeature`, `PoolQC` are very high.\n- Missing value rate of following feature's combination seems to be perfectly or almost equal, and feature names looks very similar. I will consider them latter.\n  - `MasVnrType`, `MasVnrArea`\n  - `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, `BsmtQual`\n  - `GarageCond`, `GarageFinish`, `GarageQual`, `GarageType`, `GarageYrBlt`\n  \nI will consider how to cleanse dataset and handling missing values in following 4 parts.\n\n1. `MasVnr*` features\n2. `Bsmt*` features\n3. `Garage*` features\n4. Other continuous features\n5. Other discrete features","1d9be0bf":"# Compare cross validation results","3029ed94":"I will consider following feature groups later.\n\n- `MasVnrType`, `MasVnrArea`\n- `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`,`BsmtQual`\n- `GarageCond`, `GarageFinish`, `GarageQual`, `GarageType`, `GarageYrBlt`\n\nI consider other features here.\n\n- Continous features\n  ","4bd17983":"# Feature Engineering","13137917":"Calculate and visualize the rate of missing values.","b6ae9335":"# What to do\n\nTry quickly to train some regressor.\n\n- Algorithms  \n  I wiil try following algorithms.\n\n  - GBDTs  \n    CatBoost, XGBoost, LightGBM  \n    \n  - Linear models  \n    Ridge, RANSAC  \n    \n  - Others  \n    SVM, KNN\n\n- Feature engineering  \n  Minimum feature engineering will be done. I shall explain it at the section of _Feature Engineering_.\n  \n- CV  \n  Try plain KFold and StratifiedKfold. Number of iteration is 15.","7f65a074":"Accoreding to data description, the value of `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, and `BsmtQual` are _np.nan_ if there are no basement in the house. Then at least one of `BsmtCond`, or 3 other features are incorrect. I do not have any reasonable evidence, but I will fix `BstmCond` , believing 3 other features are correct in this case.","60b45699":"### 2. `Bsmt*` features","33f13327":"## Discrete or continuous","2b6968bc":"At first, I will determine which features are descrite(categorical) or continuous by data type, cardinality, and correration coefficient to `SalePrice`. Second, I will consider how to handle missing values.","0bb3c324":"Check data description of features containing missing values.","efa41b7c":"In \"train.csv\", all garage type features (meaning `GarageCond`, `GarageFinish`, `GarageQual`, `GarageType`, and `GarageYrBlt`) are _np.nan_ if `GrageCond` is _np.nan_. In \"test.csv\", however, 2 `GarageType` is not _np.nan_ even if `GrageCond` is _np.nan_.","150841b5":"### `GarageCond`, `GarageFinish`, `GarageQual`, `GarageType`, and `GarageYrBlt`","4db9420e":"In \"test.csv\", one `MasVnrArea` is 198 even if `MasVnrType` is _np.nan_, meaning no masonary veneer in the house. I therefore think it is a data error and `MasVnrArea` should be _np.nan_ if `MasVnrType` is _np.nan_.","a26a8556":"These are just my impression but I would like to start with following hypothesis.\n\n- `Id` is, off course, not a feature.\n- A feature shoule be treated as descrete if its data type is _object_.\n- Data type of `BsmtHalfBath`, `Kitchen`, `OverallCond`, `YrSold`, `MoSold` and `MSSubClass` is _int64_, but I will treat them as descrete features.\n  - All (or almost all) of the values are shared between \"train.csv\" and \"test.csv\".\n  - According to heatmap, their correration coefficient to `SalePrice` is very low, then I assume it might be better to treat them non-ordianl for prediction.\n- The others should be treated as continuous features.","85ae0bab":"# Train regressors","42e5d0f9":"Visualize cv result.","c87b8c4a":"# Load dataset","4b026ce0":"# Libraries","832b07ae":"Be careful that Feature engineering section is not completed in this version."}}