{"cell_type":{"46eb45e7":"code","07dc0a83":"code","c57918ad":"code","dc8fccae":"code","ae0b5e16":"code","503a1e88":"code","7877bb70":"code","580137c5":"code","2a1fdee3":"code","3dc52d98":"code","534b5a25":"code","1aecaf05":"code","b6c933f1":"code","c936da8a":"code","30ac1ccb":"code","32081e7e":"code","4bc5412d":"code","2a80570c":"code","fc60abf9":"code","489f6dfb":"code","a5ae633a":"code","0f72f28a":"code","5ef9a707":"code","834d1b4c":"code","a0f0dc2e":"code","29d5a6ca":"code","49fc76ef":"code","efdb0c60":"code","9efc0fe6":"code","05a0806c":"code","083e770c":"code","4d4cc7ac":"code","8b25f1fb":"code","91631b20":"code","182ca753":"code","fd8fd340":"code","25528837":"code","ec2ccdef":"code","70c2e42e":"code","24e84308":"code","7f41855c":"code","ad181227":"code","6659aaab":"code","ae25906e":"code","8b27c536":"code","6792c03b":"code","8e0604bd":"code","59906ddb":"code","2026fe06":"code","3cedb601":"code","ed24267d":"code","bca54657":"code","1d609f21":"code","457f63af":"code","69cdafd8":"markdown","a78cabb1":"markdown","1da12b78":"markdown","424238ed":"markdown","191a9d2a":"markdown","09f7d3f2":"markdown","c4125895":"markdown","507fa4b2":"markdown","c8bee4a5":"markdown","12c84bb7":"markdown"},"source":{"46eb45e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport operator\nimport re\nimport string\nimport os\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPool2D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","07dc0a83":"!ls ..\/input\/glovetwitter27b100dtxt","c57918ad":"train=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsample=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","dc8fccae":"train.shape,test.shape","ae0b5e16":"train.head()","503a1e88":"## Load the embeddings:\n\ndef load_embedding(file):\n    def get_coefs(word,*arr):\n        return word,np.array(arr,dtype='float32')\n    \n    embeddings_index=dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    \n    \n    return embeddings_index\n    ","7877bb70":"glove='..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt'","580137c5":"embed_glove = load_embedding(glove)","2a1fdee3":"print(len(embed_glove))","3dc52d98":"def build_vocab(text):\n    #sentence=text.apply(lambda x:x.split()).values\n    vocab={}\n    for s in tqdm(text):\n        for word in s:\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab\n            ","534b5a25":"def check_coverage(vocab,embedding_index):\n    known_words={}\n    unknown_words={}\n    nb_known_words=0\n    nb_unknown_words=0\n    \n    for word in tqdm(vocab.keys()):\n        try:\n            known_words[word]=embedding_index[word]\n            nb_known_words+=vocab[word]\n        except:\n            unknown_words[word]=vocab[word]\n            nb_unknown_words+=vocab[word]\n            \n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    \n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    \n    return unknown_words","1aecaf05":"sentences = train[\"text\"].progress_apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:5]})\n","b6c933f1":"## Check coverage:\noov=check_coverage(vocab,embed_glove)","c936da8a":"oov[:10]","30ac1ccb":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","32081e7e":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","4bc5412d":"print(\"- Known Contractions -\")\nprint(\"   Glove(Twitter) :\")\nprint(known_contractions(embed_glove))","2a80570c":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","fc60abf9":"train['text_clean']=train['text'].progress_apply(lambda x:clean_contractions(x,contraction_mapping))\nsentence=train['text_clean'].apply(lambda x:x.split())\nvocab=build_vocab(sentence)","489f6dfb":"oov=check_coverage(vocab,embed_glove)","a5ae633a":"'.' in embed_glove","0f72f28a":"'2' in embed_glove","5ef9a707":"## Clean the text\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    \n    return x","834d1b4c":"train['text_clean']=train['text_clean'].progress_apply(lambda x:clean_text(x))\nsentence=train['text_clean'].apply(lambda x:x.split())\nvocab=build_vocab(sentence)","a0f0dc2e":"oov=check_coverage(vocab,embed_glove)","29d5a6ca":"oov[:10]","49fc76ef":"print('I' in embed_glove)\nprint('i' in embed_glove)","efdb0c60":"def clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    x = re.sub('[0-9]','#',x)\n    return x","9efc0fe6":"train['text_clean']=train['text_clean'].progress_apply(lambda x: clean_numbers(x))\ntrain['text_clean']=train['text_clean'].apply(lambda x:x.lower())","05a0806c":"sentence=train['text_clean'].apply(lambda x:x.split())\nvocab=build_vocab(sentence)\noov=check_coverage(vocab,embed_glove)","083e770c":"oov[:100]","4d4cc7ac":"'disease' in embed_glove","8b25f1fb":"mispell_dict={'mh###':'mh','\\x89\u00fb':'#','\\x89\u00fb\u00f2':'','#th':'th','prebreak':'pre break','##yr':'year','re\\x89\u00fb':'re','\\x89\u00fb\u00f3':'#','bestnaijamade':'best i made','##pm':'pm','diseasese':'disease','diseasese':'disease','udhampur':'city','#km':'km','mediterraneanean':'mediterranean','#x#':'x','qew#c#m#xd':'','rea\\x89\u00fb':'real','ww#':'w','###pm':'pm','enchanged':'exchanged','#rd':\"rd\",'bb#':'b','ps#':'ps','bioterrorism':'bio terrorism','bioterror':'bio terror','soudelor':'shoulder','disea':'disease','funtenna':'fun','don\\x89\u00fb\u00aat':'do not','crematoria':'crematorium','utc####':'utc','time####':'time','#km':'km','#pm':'pm','\\x89\u00fb\u00efwhen':'when','#st':'st','##km':'km','#nd':'nd','#th':'th','##w':'w','irandeal':'iran deal','spos':'spot','mediterran':'mediterranean','inundation':'inundate','it\\x89\u00fb\u00aas':'it is','o###':'o','you\\x89\u00fb\u00aave':'you have','china\\x89\u00fb\u00aas':'china','animalrescue':'animal rescue','canaanites':'canaan','linkury':'malware','###w':'w','##inch':'inch','r\u00ec\u00a9union':'reunion','mhtw#fnet':'net','microlight':'micro light','i#':'i','##m':'m','a#':'a','lt#':'it','ices\\x89\u00fb':'ice','djicemoon':'moon','icemoon':'ice moon','be\\x89\u00fb':'be','ksawlyux##':'as','sinjar':'jar','#wd':'wd','by\\x89\u00fb':'by','prophetmuhammad':'prophet muhammad','m###':'m','i\\x89\u00fb\u00aam':'i am','mikeparractor':'mike','##rd':'rd','dorret':'do ret','##s':'s','read\\x89\u00fb':'read','x####':'x','encmhz#y##':'enchanged','q#eblokeve':'blok','let\\x89\u00fb\u00aas':'let us','can\\x89\u00fb\u00aat':'can not','kerricktrial':'trial','twia':'tw','naved':'paved','nasahurricane':'nasa hurricane','vvplfqv##p':'v','pantherattack':'panther attack','youngheroesid':'young hero id','injuryi':'injury','america\\x89\u00fb\u00aas':'america','s\\x89\u00fb':'s','socialnews':'social news','##k':'k','z##':'z','of\\x89\u00fb':'of','cybksxhf#d':'c','strategicpatience':'strategic patience','sittwe':'sit','here\\x89\u00fb\u00aas':'here','summerfate':'summer fate','b#':'b','#am':'am','bb##':'b','usagov':'usa gov','grief\\x89\u00fb\u00aa':'grief','#\u00f2':'o',\n'#th':'th',\n're#':'re',\n'#\u00f3':'o',\n'diseasese':'disease',\n'don#\u00aat':'do not',\n'#\u00efwhen':'when',\n'#km':'km',\n'mediterraneanean':'mediterranean',\n'it#\u00aas':'it is ',\n'#pm':'pm',\n'you#\u00aave':'you have',\n'china\u00aas':'china',\n'be#':'be',\n'ices#':'ice',\n's#':'s',\n'by#':'by',\n'i\u00aam':'I am',\n'#rd':'rd',\n'read#':'read',\n'enchanged':'exchanged',\n'let#\u00aas':'let us',\n'can#\u00aat':'can not',\n'america\u00aas':'america',\n'of#':'of',\n'here#\u00aas':'here is',\n'grief#\u00aa':'grief',\n'#\u00f7politics':'politics',\n'idfire':'id fire',\n'karymsky':'sky',\n'rexyy':'rex',\n'jap\u00ecn':'japan',\n'#wisenews':'wise news',\n'carryi':'carrying',\n'offensive#\u00aa':'offensive',\n'#\u00f7extremely':'extremely',\n'lulgzimbestpicts':'pics',\n'ramag':'ram',\n'diamondkesawn':'diamond',\n'raynbowaffair':'rainbow affair',\n'viwxy#xdyk':'#',\n'lvlh#w#awo':'#',\n'yazidis':'yazid',\n'#pcs':'pcs',\n'waimate':'mate',\n'otrametlife':'#',\n'#\u00efa':'#',\n'#aug':'august',\n'##jst':'just',\n'rqwuoy#fm#':'#',\n'ks###':'#',\n'##l':'#',\n'i\u00aave':'I have',\n'unsuckdcmetro':'metro',\n'realdonaldtrump':'donald trump',\n'abbswinston':'winston',\n'moll#vd#yd':'#',\n'oppressions':'oppression',\n'slanglucci':'slang',\n'fettilootch':'#',\n'worstsummerjob':'worst summer job',\n'#st':'st',\n'i##':'#',\n'\u00e5\u00ea':'#',\n'\u00e5\u00e8':'#',\n'#\u00efthe':'the',\n'warfighting':'war fighting',\n'mitt#\\x9d':'#',\n'#\u00efwe':'#',\n'michael#sos':'michael',\n'kurtschlichter':'#',\n'beforeitsnews':'before it is news',\n'zujwuiomb':'#',\n'lonewolffur':'lone wolf',\n'm##':'#',\n'localarsonist':'local arson',\n'wvj##abgm':'#',\n'thoyhrhkfj':'#',\n'o##f#cyy#r':'#',\n'nnmqlzo#':'#',\n'temporary###':'temporary',\n'pbban':'#',\n'm#':'#',\n'votejkt##id':'vote',\n'rockyfire':'rock fire',\n'throwingknifes':'throwing knife',\n'dannyonpc':'danny on picture',\n'godslove':'god love',\n'ptsdchat':'chat',\n'cbcca':'#',\n'usnwsgov':'us government',\n'metrofmtalk':'metro fm talk',\n'#pack':'pack',\n'bookboost':'book boost',\n'ibooklove':'book love',\n'aoms':'#',\n'foxysiren':'fox',\n'blowmandyup':'blow',\n'dehmym#lpk':'#',\n'itunesmusic':'i tunes music',\n'\\x89\u00fb\u00f7politics':'politics','viralspell':'viral spell'}","91631b20":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n","182ca753":"train['text_clean']=train['text_clean'].progress_apply(lambda x: correct_spelling(x,mispell_dict))\nsentence=train['text_clean'].apply(lambda x:x.split())\nvocab=build_vocab(sentence)\noov=check_coverage(vocab,embed_glove)\n","fd8fd340":"test['text_clean']=test['text'].progress_apply(lambda x:clean_contractions(x,contraction_mapping))\ntest['text_clean']=test['text_clean'].progress_apply(lambda x:clean_text(x))\ntest['text_clean']=test['text_clean'].progress_apply(lambda x: clean_numbers(x))\ntest['text_clean']=test['text_clean'].apply(lambda x:x.lower())\ntest['text_clean']=test['text_clean'].progress_apply(lambda x: correct_spelling(x,mispell_dict))\nsentence=test['text_clean'].apply(lambda x:x.split())\nvocab=build_vocab(sentence)\noov=check_coverage(vocab,embed_glove)\n","25528837":"train_df,val_df=train_test_split(train,test_size=0.1,random_state=100)","ec2ccdef":"embed_size=300\nmax_features=20000\nmaxlen=300\n","70c2e42e":"train_X=train_df['text_clean'].values\nvalid_X=val_df['text_clean'].values\ntest_X=test['text_clean'].values\n","24e84308":"tokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X=tokenizer.texts_to_sequences(train_X)\nvalid_X=tokenizer.texts_to_sequences(valid_X)\ntest_X=tokenizer.texts_to_sequences(test_X)\n\ntrain_X=pad_sequences(train_X,maxlen=maxlen)\nvalid_X=pad_sequences(valid_X,maxlen=maxlen)\ntest_X=pad_sequences(test_X,maxlen=maxlen)\n\ntrain_Y=train_df['target'].values\nvalid_Y=val_df['target'].values\n","7f41855c":"train_X.shape,valid_X.shape","ad181227":"all_embs = np.stack(embed_glove.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\n","6659aaab":"len(word_index),emb_mean,emb_std,embed_size","ae25906e":"nb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in tqdm(word_index.items()):\n    if i >= max_features: continue\n    embedding_vector = embed_glove.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","8b27c536":"embedding_matrix.shape","6792c03b":"## Simple TF model with bidirectional LSTM\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\nx=SpatialDropout1D(0.2)(x)\nx = Bidirectional(LSTM(128,return_sequences=True))(x)\nx=GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","8e0604bd":"model.fit(train_X, train_Y, batch_size=64, epochs=5, validation_data=(valid_X, valid_Y))","59906ddb":"## Validate with 0.6 as arbitary threshold\npred_glove_val_y = model.predict([valid_X], batch_size=64, verbose=1)\nprint(f'F1 Score {f1_score(valid_Y,(pred_glove_val_y>0.6).astype(int))}')","2026fe06":"pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)","3cedb601":"sample.head()","ed24267d":"sample['target']=(pred_glove_test_y>0.6).astype(int)","bca54657":"sample.head()","1d609f21":"sample['target'].value_counts()","457f63af":"sample.to_csv('sample_submission.csv',index=False)","69cdafd8":"#### Reference:\n\nMy work is inspired from the below kernels.If you consider upvoting my kernel ,pls upvote these kernels too.\n\n1. https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2\n2. https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings\n3. https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\n4. https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\n\n","a78cabb1":"## Real or Not NLP Modelling","1da12b78":"We see that the embed_glove do not have capitalized letters.Therefore we convert the tweets in lowercase.","424238ed":"We see that the items are contractions,numbers and punctuations.Lets clean them.Also ... is found 147 times.Lets check if . is present in embedding index.Also contractions too make up a significant number.Lets remove them and build the embedding again.","191a9d2a":"Lets take care of spelling mistakes.","09f7d3f2":"We replace the numbers with # which is found in the glove embedding.","c4125895":"Only 55% of the words in the text are found in the embedding matrix.Lets check the top 10 words which do not have pre defined embeddings.","507fa4b2":"There seems to be a marginal improvement in the coverage.","c8bee4a5":"Check for the contractions in the embedding.","12c84bb7":"Thus by replacing the numbers and converting the text to lower case we were able to find embeddings for 90 % of the words in the text.Now lets check the oov again."}}