{"cell_type":{"92fa7efb":"code","6ff3998b":"code","aab436d3":"code","69e0f2e6":"code","dbace2eb":"code","d8ec7472":"code","c1d4351e":"code","ef09f363":"code","2cf7432c":"code","2b350c90":"code","0218e0fa":"code","0cf8e336":"code","81e1b435":"code","a2edf665":"code","4593356a":"code","d631e1a5":"code","5fd86e79":"code","c596befd":"code","ac271c26":"code","5f4ed156":"code","c9ca3f2a":"code","a6b334bb":"code","e87f3721":"code","dcc6c41b":"code","c080d66f":"code","46c524df":"code","aed1ddb7":"code","5443fa9e":"code","df726042":"code","1174eb72":"code","a3eaed66":"code","f49cfba6":"code","9bd14524":"code","d8d53967":"code","4754184d":"code","0934ad98":"code","959a4f63":"code","b0beee6d":"code","5d0008ee":"code","65b3ef16":"code","a3901437":"code","c4684cad":"code","0c772d11":"code","5dceb22c":"code","d4477964":"code","78171fa9":"code","20b027b8":"code","35361caf":"code","a603e6ef":"code","86d81863":"code","bacd69eb":"code","3c1bf115":"markdown","64d72cc1":"markdown","7d4a5a9d":"markdown","37777842":"markdown","b83e7a73":"markdown","8bb7b5a5":"markdown","b7bd9283":"markdown","ed281c01":"markdown","93814a90":"markdown","44c2c260":"markdown"},"source":{"92fa7efb":"import numpy as np\nimport cupy, cudf\nimport gc\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nimport random\nimport torch\nimport torchvision\nfrom torchvision import  models, transforms\nfrom transformers import BertTokenizer, BertModel\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport glob\nfrom PIL import Image\nimport seaborn as sns\nimport cv2, matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom textwrap import wrap","6ff3998b":"device = 'cuda'if torch.cuda.is_available() else 'cpu'\ndevice","aab436d3":"PATH = '..\/input\/shopee-product-matching\/'\nPATH_TO_IMG = '..\/input\/shopee-product-matching\/train_images\/'\nPATH_TO_TEST = '..\/input\/shopee-product-matching\/test_images\/'\nos.listdir(PATH)","69e0f2e6":"COMPUTE_CV = True\nif len(pd.read_csv(PATH + 'test.csv')) > 3: COMPUTE_CV = False","dbace2eb":"if COMPUTE_CV:\n    dataset = pd.read_csv(PATH + 'train.csv')\n    tmp = dataset.groupby('label_group').posting_id.agg('unique').to_dict()\n    dataset['target'] = dataset.label_group.map(tmp)\nelse:    \n    dataset = pd.read_csv(PATH + 'test.csv')","d8ec7472":"dataset.head()","c1d4351e":"def show_random_img():\n    # choose randomly two instances per each class\n    labels_to_show = np.random.choice(dataset.label_group.unique(), \n                                      replace=False, size=24)\n    img_to_show = []\n    for label in labels_to_show:\n        rows = dataset[dataset.label_group==label].copy()\n        pair = np.random.choice([i for i in range(len(rows))], \n                                    replace=False, size=2)\n        img_pair = rows.iloc[pair][['image', 'title']].values\n        \n        img_to_show += list(img_pair)\n    \n    fig, axes = plt.subplots(figsize = (18, 12), nrows=4,ncols=6)\n    for imp, ax in zip(img_to_show, axes.ravel()):\n        img = cv2.imread(PATH_TO_IMG + imp[0])\n        title = '\\n'.join(wrap(imp[1], 20))\n        ax.set_title(title)\n        ax.imshow(img)\n        ax.axis('off')\n\n    fig.tight_layout()","ef09f363":"if COMPUTE_CV:\n    show_random_img()","2cf7432c":"class ResNetEmbedder(nn.Module):\n    \n    def __init__(self, device='cpu'):\n        super(ResNetEmbedder, self).__init__()\n        self.model = models.resnet50(pretrained=False)\n        self.device = device\n        path = '..\/input\/pretrained-model-weights-pytorch\/resnet50-19c8e357.pth'\n        self.model.load_state_dict(torch.load(path))\n#         to freeze weights\n        for param in self.model.parameters():\n                param.requires_grad = False\n        self.model.to(device)\n        \n    \n    def transform(self, img):\n        image_transform = torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize(256),\n                transforms.CenterCrop(224),\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225)\n                ),\n            ]\n        )\n        return image_transform(img)\n    \n    def forward(self, img):\n        img_tr = self.transform(img).unsqueeze(0)\n        img_tr = img_tr.to(self.device)\n        features = self.model(img_tr).squeeze()\n        return features","2b350c90":"model_img = ResNetEmbedder(device)","0218e0fa":"def vectorize_img(img_path):\n    img = Image.open(img_path).convert('RGB')\n    model_img.eval()\n    with torch.no_grad():\n        output = model_img(img).cpu().numpy()\n    return output","0cf8e336":"%%time\nif COMPUTE_CV:\n    dataset['resnet_v'] = dataset['image'].progress_apply(lambda x: vectorize_img(PATH_TO_IMG + x))\nelse:\n    dataset['resnet_v'] = dataset['image'].progress_apply(lambda x: vectorize_img(PATH_TO_TEST + x))","81e1b435":"del model_img","a2edf665":"vectors = np.stack(dataset.resnet_v)\nvectors = torch.Tensor(vectors).to(device)\nvectors = F.normalize(vectors)","4593356a":"preds = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(dataset)\/\/CHUNK\nif len(dataset)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(dataset))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = torch.matmul( vectors, vectors[a:b].T).T\n    cts = cts.cpu().numpy()\n    \n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.9)[0]\n        o = dataset.iloc[IDX].posting_id.values\n        preds.append(o)\n\ndel vectors, cts, IDX, o\n_ = gc.collect()","d631e1a5":"dataset['preds_resnet'] = preds\ndataset.head()","5fd86e79":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","c596befd":"if COMPUTE_CV:\n    dataset['f1_resnet'] = dataset.apply(getMetric('preds_resnet'), axis=1)\n    print('CV score for baseline =', dataset.f1_resnet.mean())","ac271c26":"class BERTEmbedder(nn.Module):\n    \n    def __init__(self, device='cpu'):\n        super(BERTEmbedder, self).__init__()\n        self.bert_path = \"..\/input\/sentence-transformer\/\"\n        self.model = BertModel.from_pretrained(self.bert_path)\n#         to freeze weights\n        for param in self.model.parameters():\n                param.requires_grad = False\n        self.model.to(device)\n        \n    def transform(self, txt):\n        tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n        encoded_input  = tokenizer.encode_plus( txt, \n                                                truncation=True, \n                                                max_length=128,\n                                                add_special_tokens=True,\n                                                padding=True,\n                                                return_tensors='pt').values()\n        return encoded_input\n    \n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings \/ sum_mask\n        \n    def forward(self, txt):\n        inputs_ids, token_type_ids, attention_mask = self.transform(txt)\n        inputs_ids, token_type_ids, attention_mask = inputs_ids.to(device), \\\n                                                token_type_ids.to(device), attention_mask.to(device)\n        with torch.no_grad():\n            encoded_layers = self.model(inputs_ids, \n                                        attention_mask=attention_mask, \n                                        token_type_ids=token_type_ids)\n        features = self.mean_pooling(encoded_layers, attention_mask)\n        return features","5f4ed156":"model_txt = BERTEmbedder(device)","c9ca3f2a":"def vectorize_txt(txt):\n    model_txt.eval()\n    with torch.no_grad():\n        output = model_txt(txt).cpu().numpy()\n    return output","a6b334bb":"%%time\ndataset['sbert_v'] = dataset['title'].progress_apply(lambda x: vectorize_txt(x))","e87f3721":"del model_txt","dcc6c41b":"vectors = np.stack(dataset.sbert_v).squeeze(1)\nvectors = torch.Tensor(vectors).to(device)\nvectors = F.normalize(vectors)","c080d66f":"preds = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(dataset)\/\/CHUNK\nif len(dataset)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(dataset))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = torch.matmul( vectors, vectors[a:b].T).T\n    cts = cts.cpu().numpy()\n    \n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.95)[0]\n        o = dataset.iloc[IDX].posting_id.values\n        preds.append(o)\n\ndel vectors, cts, IDX, o\n_ = gc.collect()","46c524df":"dataset['preds_sbert'] = preds\ndataset.head()","aed1ddb7":"del preds","5443fa9e":"if COMPUTE_CV:\n    dataset['f1_sbert'] = dataset.apply(getMetric('preds_sbert'), axis=1)\n    print('CV score for baseline =', dataset.f1_sbert.mean())","df726042":"def concat():\n    def cat(row):\n        comm = np.concatenate([row.resnet_v,row.sbert_v.squeeze()])\n        return comm\n    return cat","1174eb72":"dataset['concat_v'] = dataset.progress_apply(concat(), axis=1)","a3eaed66":"vectors = np.stack(dataset.concat_v)","f49cfba6":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(vectors)","9bd14524":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(vectors)\/\/CHUNK\nif len(vectors)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(vectors))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(vectors[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<35.0)[0]\n        IDS = indices[k,IDX]\n        o = dataset.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices, vectors, IDX, o, IDS\n_ = gc.collect()","d8d53967":"dataset['preds_concat'] = preds\ndataset.head()","4754184d":"del preds","0934ad98":"if COMPUTE_CV:\n    dataset['f1_concat'] = dataset.apply(getMetric('preds_concat'), axis=1)\n    print('CV score for baseline =', dataset.f1_concat.mean())","959a4f63":"tmp = dataset.groupby('image_phash').posting_id.agg('unique').to_dict()\ndataset['preds_phash'] = dataset.image_phash.map(tmp)\ndataset.head()","b0beee6d":"del tmp","5d0008ee":"dataset_gf = cudf.DataFrame(dataset[['posting_id', 'title']])","65b3ef16":"model = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(dataset_gf.title)","a3901437":"del model","c4684cad":"preds = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(dataset)\/\/CHUNK\nif len(dataset)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(dataset))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = text_embeddings.dot(text_embeddings[a:b].T).T.toarray()\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = dataset.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \ndel text_embeddings, IDX, o, cts\n_ = gc.collect()","0c772d11":"dataset['preds_tfidf'] = preds","5dceb22c":"del preds","d4477964":"if COMPUTE_CV:\n    dataset['f1_tfidf'] = dataset.apply(getMetric('preds_tfidf'), axis=1)\n    print('CV score for baseline =', dataset.f1_tfidf.mean())","78171fa9":"def combine_for_sub(row):\n    x = np.concatenate([row.preds_concat,row.preds_phash, row.preds_tfidf])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_train(row):\n    x = np.concatenate([row.preds_concat,row.preds_phash, row.preds_tfidf])\n    return list(np.unique(x))","20b027b8":"if COMPUTE_CV:\n    dataset['matches'] = dataset.apply(combine_for_train, axis=1)\nelse:\n    dataset['matches'] = dataset.apply(combine_for_sub, axis=1)","35361caf":"dataset.to_pickle('train_data.pkl')","a603e6ef":"dataset[['posting_id', 'matches']].to_csv('submission.csv',index=False)","86d81863":"subm = pd.read_csv('submission.csv')\nsubm.head()","bacd69eb":"if COMPUTE_CV:\n    dataset['f1_final'] = dataset.apply(getMetric('matches'), axis=1)\n    print('CV score for baseline =', dataset.f1_final.mean())","3c1bf115":"Big thanks to Chris for his [kernel](https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700)","64d72cc1":"To familiarize with implementation details please see [documentation](https:\/\/huggingface.co\/sentence-transformers\/bert-base-nli-mean-tokens).","7d4a5a9d":"# Phash block","37777842":"# Submission block","b83e7a73":"# TF-IDF block","8bb7b5a5":"# ResNet block","b7bd9283":"# Concatenation block","ed281c01":"# Data import","93814a90":"Check cosine metrics. Vectors should be normalized.","44c2c260":"# Sentence Bert block"}}