{"cell_type":{"908d6a17":"code","66fe177d":"code","29898ba5":"code","3396aad8":"code","28d8281f":"code","17f9ffdf":"code","e6f58169":"code","a9444e16":"code","7cb16a89":"code","93e1a427":"code","a0851164":"code","7ae0b8b2":"code","30b64dde":"code","acc9753e":"code","a3bfb7cf":"code","2def7704":"code","ee9213e7":"code","072a5a25":"code","504a4395":"code","e0902e01":"code","c684c3c6":"code","5c9612a6":"code","c03611f7":"code","3f0edd8f":"code","cf6200a7":"code","e4771d49":"code","03e47998":"code","3490b8a7":"code","5110af6a":"code","d8122a30":"code","5c147a48":"code","318cac16":"code","fba5b2e9":"markdown","2ec3810b":"markdown","c1dd59b8":"markdown","02a5e240":"markdown","3eb45480":"markdown","6cd103d7":"markdown","7ec992f6":"markdown","802577b9":"markdown","94783e1c":"markdown","9db4e0be":"markdown","c2a956fd":"markdown","aaf3a1d4":"markdown","01516f6b":"markdown","25fecb1b":"markdown","f20751ce":"markdown","5073adef":"markdown","dbea2f6c":"markdown","5a9d5de2":"markdown","d93719e8":"markdown","7ee41105":"markdown","83ae2209":"markdown","ad906811":"markdown","3ab67f5d":"markdown","9b771a82":"markdown","74478089":"markdown","d927cd47":"markdown","00bf134b":"markdown","16800ffc":"markdown"},"source":{"908d6a17":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import base","66fe177d":"df_train=pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ndf_test=pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","29898ba5":"print('train data set has got {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))\nprint('test data set has got {} rows and {} columns'.format(df_test.shape[0],df_test.shape[1]))\n","3396aad8":"df_train.head()","28d8281f":"df_train.info()","17f9ffdf":"X=df_train.drop(['target'],axis=1)\ny=df_train['target']\n#X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)","e6f58169":"x=y.value_counts()\nplt.bar(x.index,x)\nplt.gca().set_xticks([0,1])\nplt.title('distribution of target variable')\nplt.show()","a9444e16":"from sklearn.preprocessing import LabelEncoder","7cb16a89":"%%time\n\ntrain=pd.DataFrame()\nlabel=LabelEncoder()\nfor c in  X.columns:\n    if(X[c].dtype=='object'):\n        train[c]=label.fit_transform(X[c])\n    else:\n        train[c]=X[c]\n        \ntrain.head(3)    ","93e1a427":"\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n","a0851164":"def logistic(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    y_pre=lr.predict(X_test)\n    print('Accuracy : ',accuracy_score(y_test,y_pre))\n","7ae0b8b2":"logistic(train,y)","30b64dde":"#train=pd.get_dummies(X).astype(np.int8)\n#print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","acc9753e":"from sklearn.preprocessing import OneHotEncoder","a3bfb7cf":"%%time \n\none=OneHotEncoder()\n\none.fit(X)\ntrain=one.transform(X)\n\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","2def7704":"logistic(train,y)","ee9213e7":"from sklearn.feature_extraction import FeatureHasher","072a5a25":"%%time\n\nX_train_hash=X.copy()\nfor c in X.columns:\n    X_train_hash[c]=X[c].astype('str')      \nhashing=FeatureHasher(input_type='string')\ntrain=hashing.transform(X_train_hash.values)","504a4395":"\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n\n","e0902e01":"logistic(train,y)","c684c3c6":"%%time\n\nX_train_stat=X.copy()\nfor c in X_train_stat.columns:\n    if(X_train_stat[c].dtype=='object'):\n        X_train_stat[c]=X_train_stat[c].astype('category')\n        counts=X_train_stat[c].value_counts()\n        counts=counts.sort_index()\n        counts=counts.fillna(0)\n        counts += np.random.rand(len(counts))\/1000\n        X_train_stat[c].cat.categories=counts\n    \n        \n        ","5c9612a6":"X_train_stat.head(3)","c03611f7":"print('train data set has got {} rows and {} columns'.format(X_train_stat.shape[0],X_train_stat.shape[1]))\n        ","3f0edd8f":"logistic(X_train_stat,y)","cf6200a7":"%%time\n\nX_train_cyclic=X.copy()\ncolumns=['day','month']\nfor col in columns:\n    X_train_cyclic[col+'_sin']=np.sin((2*np.pi*X_train_cyclic[col])\/max(X_train_cyclic[col]))\n    X_train_cyclic[col+'_cos']=np.cos((2*np.pi*X_train_cyclic[col])\/max(X_train_cyclic[col]))\nX_train_cyclic=X_train_cyclic.drop(columns,axis=1)\n\nX_train_cyclic[['day_sin','day_cos']].head(3)","e4771d49":"one=OneHotEncoder()\n\none.fit(X_train_cyclic)\ntrain=one.transform(X_train_cyclic)\n\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))\n","03e47998":"logistic(train,y)","3490b8a7":"%%time\n\nX_target=df_train.copy()\nX_target['day']=X_target['day'].astype('object')\nX_target['month']=X_target['month'].astype('object')\nfor col in X_target.columns:\n    if (X_target[col].dtype=='object'):\n        target= dict ( X_target.groupby(col)['target'].agg('sum')\/X_target.groupby(col)['target'].agg('count'))\n        X_target[col]=X_target[col].replace(target).values\n        \n    \n    \n\nX_target.head(4)","5110af6a":"logistic(X_target.drop('target',axis=1),y)","d8122a30":"X['target']=y\ncols=X.drop(['target','id'],axis=1).columns","5c147a48":"%%time\n\nX_fold=X.copy()\nX_fold[['ord_0','day','month']]=X_fold[['ord_0','day','month']].astype('object')\nX_fold[['bin_3','bin_4']]=X_fold[['bin_3','bin_4']].replace({'Y':1,'N':0,'T':1,\"F\":0})\nkf = KFold(n_splits = 5, shuffle = False, random_state=2019)\nfor train_ind,val_ind in kf.split(X):\n    for col in cols:\n        if(X_fold[col].dtype=='object'):\n            replaced=dict(X.iloc[train_ind][[col,'target']].groupby(col)['target'].mean())\n            X_fold.loc[val_ind,col]=X_fold.iloc[val_ind][col].replace(replaced).values\n\n            ","318cac16":"X_fold.head()","fba5b2e9":"#### So,let's begin...","2ec3810b":"**You can try two or more of this approaches together,and encode the dataset in suitable way to acheive higher accuracy**\n","c1dd59b8":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"red\">If you like my kernel please consider upvoting it<\/font><\/p>\n<p><font size=\"4\" color=\"blue\">Don't hesitate to give your suggestions in the comment section<\/font><\/p>\n<p><font size=\"3\" color=\"green\">Thank you...<\/font><\/p>\n\n\n<\/body>\n<\/html>\n","02a5e240":"## Method 5 : Target encoding <a id='7'><\/a>\n \t\t\nTarget-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.\n\nfor example,\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country<\/th>\n    <th>Target<\/th>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>1<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>0<\/td>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>0<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>1<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>1<\/td>\n    <\/tr>\n<\/table>\n\n","3eb45480":"- **Method 1 :** [Label encoding](#1)\n- **Method 2 :** [OnHot encoding](#2)\n- **Method 3 :** [Feature Hashing](#3)\n- **Method 4 :** [Encoding categories with dataset statistics](#4)\n- **Cyclic features :** [Encoding cyclic features](#6)\n- **Method 5:** [Target Encoding](#7)\n- **Method 6 :** [K-Fold target encoding](#8)\n- **Summary :** [Summary of model performance](#5)","6cd103d7":"Now we will do these three steps to label encode our data:\n- Initialize the labelencoder class\n- Call the fit() method to fit the data\n- Transform data to labelencoded data","7ec992f6":"## Method 3 : Feature hashing (a.k.a the hashing trick)  <a id='3'><\/a>","802577b9":"Feature hashing is a very cool technique to represent categories in a \u201cone hot encoding style\u201d as a sparse matrix but with a much lower dimensions. In feature hashing we apply a hashing function to the category and then represent it by its indices. for example, if we choose a dimension of 5 to represent \u201cNew York\u201d we will calculate H(New York) mod 5 = 3 (for example) so New York representation will be (0,0,1,0,0).","94783e1c":"Through this kernel,We are going to learn and try some of the most commonly used encoding techniques.As this competition mainly deals with encoding I hope that it would be a great time to refresh some the most common and effective encoding techniques currently in use.\n<p><font size=\"3\" color=\"#6c3483\">We will also run and test each of these encoding techniques in a simple logistic regression model and finally observe the performance of each type of encoding.<\/font><\/p>","9db4e0be":"This produces output as a pandas dataframe.Alternatively we can use *OneHotEncoder()* method available in* sklearn* to convert out data to on-hot encoded data.But this method produces a sparse metrix.The advantage of this methos is that is uses very less memory\/cpu resourses.\nTo do that,we need to :\n- Import OneHotEncoder from sklean.preprocessing\n- Initialize the OneHotEncoder\n- Fit and then transform our data","c2a956fd":"## Method 4 :Encoding categories with dataset statistics  <a id='4'><\/a>","aaf3a1d4":"![](https:\/\/media.giphy.com\/media\/H4DjXQXamtTiIuCcRU\/giphy.gif)","01516f6b":"Now we will use OnHotEncoder to encode other variables,then feed the data to our model.","25fecb1b":"### Defining the train and target","f20751ce":"<font color='blue' size=3>If you think this notebook is worth reading and has gained some knowledge from this,please consider upvoting my kernel.Your appreciation means a lot to me<\/font>","5073adef":"## Method 2 : On hot encoding  <a id='2'><\/a>\nOur second method is encoding each category as a one hot encoding (OHE) vector (or dummy variables). OHE is a representation method that takes each category value and turns it into a binary vector of size |i|(number of values in category i) where all columns are equal to zero besides the category column. Here is a little example:   \n\n\n![](https:\/\/miro.medium.com\/max\/878\/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n\nTo implement on-hot encoding we will use *get_dummies()* function in *pandas*.\n\n","dbea2f6c":"Encoding for India = [Number of true targets under the label India\/ Total Number of targets under the label India] \nwhich is 2\/3 = 0.66\n\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country<\/th>\n    <th>Target<\/th>\n    <\/tr>\n    <tr>\n    <td>India<\/td>\n    <td>0.66<\/td>\n    <\/tr>\n    <tr>\n    <td>China<\/td>\n    <td>0.5<\/td>\n    <\/tr>\n<\/table>\n\n","5a9d5de2":"### Logistic regression","d93719e8":"# Summary <a id='5'><\/a>\n\nHere you can see the summary of our model performance against each of the encoding techniques we have used.\nIt is clear that OnHotEncoder together with cyclic feature encoding yielded maximum accuracy.\n\n<table style=\"width : 50%\">\n    <tr>\n    <th>Encoding<\/th>\n    <th>Score<\/th>\n    <th>Wall time<\/th>\n    <\/tr>\n    <tr>\n    <td>Label Encoding<\/td>\n    <td>0.692<\/td>\n    <td> 973 ms<\/td>\n    <\/tr>\n    <tr>\n    <td>OnHotEncoder<\/td>\n    <td>0.759<\/td>\n    <td>1.84 s<\/td>\n    <\/tr>\n    <tr>\n    <td>Feature Hashing<\/td>\n    <td>0.751<\/td>\n    <td>4.96 s<\/td>\n    <\/tr>\n    <tr>\n    <td>Dataset statistic encoding<\/td>\n    <td>0.694<\/td>\n    <td>894 ms<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>Cyclic + OnHotEncoding<\/td>\n    <td>0.759<\/td>\n    <td>431 ms<\/td>\n    <\/tr>\n    <\/tr>\n    <tr>\n    <td>Target encoding<\/td>\n    <td>0.694<\/td>\n    <td>2min 5s<\/td>\n    <\/tr>\n    \n<\/table>\n    ","7ee41105":"![](https:\/\/miro.medium.com\/max\/1955\/1*ZKD4eZXzd_FdN0SQDszFVQ.png)","83ae2209":"<html>\n    <body>\n        <p><font size=\"6\" color=\"blue\">Introduction<\/font><\/p>\n    <\/body>","ad906811":"Before getting into encoding,I will just breif you with types data variables present in this data:\n- **Binary data** : A  binary variable a variable that has only 2 values..ie 0\/1\n- **Categorical data** : A categorical variable is a variable that can take some limited number of values.for example,day of the week.It can be one of 1,2,3,4,5,6,7 only.\n- **Ordinal data** : An ordinal variable is a categorical variable that has some order associated with it.for example,the ratings that are given to a movie by a user.\n- **Nominal data** :  Nominal value is a variable that has no numerical importance,such as occupation,person name etc..\n- **Timeseries data** : Time series data has a temporal value attached to it, so this would be something like a date or a time stamp that you can look for trends in time.\n\n","3ab67f5d":"## Importing required libraries","9b771a82":"## Encoding cyclic features  <a id='6'><\/a>\n![](https:\/\/miro.medium.com\/max\/343\/1*70cevmU8wNggGJEdLam1lw.png)\n\nSome of our features are cyclic in nature.ie day,month etc.\n\nA common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n\n","74478089":"## Method 1: Label encoding <a id='1'><\/a>\nIn this method we change every categorical data to a number.That is each type will be subtuted by a number.for example we will substitute 1 for Grandmaster,2 for master ,3 for expert etc..\nFor implementing this we will first import *Labelencoder* from  *sklearn* module.","d927cd47":"Here you can see the label encoded output train data.We will check the shape of train data now and verify that there is no change in the number of columns.","00bf134b":"### K-Fold target encoding <a id='8' ><\/a>\n\nk-fold target encoding can be applied to reduce the overfitting. In this method, we divide the dataset into the k-folds, here we consider 5 folds. Fig.3 shows the first round of the 5 fold cross-validation. We calculate mean-target for fold 2, 3, 4 and 5 and we use the calculated values, mean_A = 0.556 and mean_B = 0.285 to estimate mean encoding for the fold-1.","16800ffc":" Now we will try to give our models a numeric representation for every category with a small number of columns but with an encoding that will put similar categories close to each other. The easiest way to do it is replace every category with the number of times that we saw it in the dataset. This way if New York and New Jersey are both big cities, they will probably both appear many times in our dataset and the model will know that they are similar."}}