{"cell_type":{"e0c3cb52":"code","0d9dced8":"code","007a5598":"code","8cc47ded":"code","4b6418a4":"code","be77734e":"code","cea52353":"code","f5b30211":"code","534a70b9":"code","07a4a530":"code","908ae4f9":"code","d7070e54":"code","be2a01db":"code","142b6049":"code","1d7a6e24":"code","0701c475":"code","e0ee8376":"code","ac963ef6":"code","c1444092":"code","bca52443":"code","36fb67a2":"code","62badc7b":"code","d36751c2":"code","6c2bf383":"code","8b7e3ec4":"code","36e79c0f":"code","12f1a9b7":"code","b2dce2e4":"code","9e165a35":"code","6027532b":"code","90710531":"code","b9c96d3c":"code","d352d1ae":"code","e799151b":"markdown","53255767":"markdown","975c3f6d":"markdown","2d7b3ca0":"markdown","fa0f80b4":"markdown","7e11c122":"markdown","36756609":"markdown","b4576ca7":"markdown","2bed01ae":"markdown","d4700035":"markdown","fb20631e":"markdown","45c38844":"markdown","a6e21a46":"markdown","df543be0":"markdown","f84ce664":"markdown","58c79011":"markdown","46976b80":"markdown"},"source":{"e0c3cb52":"import os\nimport warnings \nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D","0d9dced8":"df = pd.read_csv(\"..\/input\/mltitanicproject\/titanic\/train.csv\")  # data frame of training data supplied by kaggle\ndf.head()  # first five rows of data","007a5598":"df.describe()","8cc47ded":"df.isna().any()  # Review Columns (Features) Available and Discover if any information is missing","4b6418a4":"sex_mapping = {'male': 0, 'female': 1}  # Codify and map Sex strings to integer values\ndf['Sex'] = df['Sex'].map(sex_mapping)","be77734e":"df[\"AgeFilled\"] = 1  # Create new feature where rows with Age NaNs are replaced with median age\ndf.loc[df[\"Age\"] > 0, \"AgeFilled\"] = 0\n\ndf[\"Age\"].fillna(28.0, inplace=True)  # Replace null data in Age column with the median Age = 28.0","cea52353":"# Add a Z-Score column for Fare data because it has outliers that skew some classification models. \ndf['FareZ'] = (df['Fare'] - df['Fare'].mean()) \/ df['Fare'].std()\ndf.head()","f5b30211":"features = [\"Pclass\",\"Sex\",\"Age\",\"FareZ\", \"SibSp\", \"Parch\", \"AgeFilled\"]\ntarget = [\"Survived\"]\ndf[features+target].isna().any()  # Make sure no nulls remain in data","534a70b9":"# Set some styling variables for plots\nsns.set_style('whitegrid')\nsns.set_palette(sns.diverging_palette(220, 10, sep=80, n=2), 2)\n\n# Define a function to make a bar plot of survivors for each of the categories in the column.\ndef plot_categorical(x_column, hue_column, df):\n    '''Plot a bar plot for the average survivor rate for different groups.\n    x_column          (str): The column name of a categorical field.\n    hue_column        (str): The column name of a second categorical field.\n    df   (pandas.DataFrame): The pandas DataFrame (just use df here!)\n    '''\n    fig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\n    sns.barplot(x=x_column, y='Survived', hue=hue_column, data=df, errwidth=0)\n    plt.title('Survivors separated by {} and {}'.format(x_column, hue_column))\n    plt.show()\n\n\n# Define a function to plot the distribution for survivors and non-survivors for a continuous variable.\ndef plot_distribution(column, df):\n    '''Plot a bar plot for the average survivor rate for different groups.\n    column            (str): The column name of a continuous (numeric) field.\n    df   (pandas.DataFrame): The pandas DataFrame (just use df here!)\n    '''\n    fig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\n    sns.distplot(df[df['Survived'] == 1][column].dropna(), label='Survived')\n    sns.distplot(df[df['Survived'] == 0][column].dropna(), label='Did not survive')\n    plt.legend()\n    plt.title('{} distribution'.format(column))\n    plt.show()","07a4a530":"plot_categorical('Pclass', 'Sex', df)  # recall sex 0 = male, 1 = female\nplot_distribution('Fare', df[df['Fare'] < 100]) # df[df['Fare'] < 100] simply removes some outliers!\nplot_distribution('Age', df)\nplot_distribution('SibSp', df)\nplot_distribution('Parch', df)","908ae4f9":"features = [\"Pclass\",\"Sex\",\"Age\",\"FareZ\", \"SibSp\", \"Parch\", \"AgeFilled\"]\ntarget = [\"Survived\"]\n\ncor = df[features+target].corr()\nf, ax = plt.subplots(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nsns.set_palette(\"husl\")\nsns.heatmap(cor, vmin=-1.0, vmax=1.0, cmap=sns.diverging_palette(10, 220, sep=80, n=11)) #, cmap=blues)","d7070e54":"sns.pairplot(df[features+target], hue=\"Survived\", palette=sns.diverging_palette(10, 220, sep=80, n=2), height=3, diag_kind=\"kde\", diag_kws=dict(shade=True, bw=.05, vertical=False) )","be2a01db":"features = [\"Pclass\",\"Sex\",\"Age\",\"FareZ\", \"SibSp\", \"Parch\", \"AgeFilled\"]\ntarget = [\"Survived\"]","142b6049":"np.random.seed(0) # Set a fixed random seed so we can reproduce results.\nscoring_method = \"f1\"  # Our competition is using the evaluation metric of the F1-Score\ncv_folds = 3  # Number of folds used in cross validation","1d7a6e24":"# import sklearn packages that will get used in many model prototypes\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix","0701c475":"# Prototype the Model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1.0)  # Instantiate model object\ncv_score = cross_val_score(lr, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))","e0ee8376":"# Optimization of the Model\nfrom sklearn.linear_model import LogisticRegression\nC_range = np.linspace(0.01,1.5,1000)\nscores = []\n\nfor c_val in C_range:\n    lr = LogisticRegression(C=c_val)  # Instantiate model object\n    cv_score = cross_val_score(lr, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\n    scores.append(np.mean(cv_score))\n    \nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nplot_labels = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\nblues = [\"#66D7EB\", \"#51ACC5\", \"#3E849E\", \"#2C5F78\", \"#1C3D52\", \"#0E1E2B\"]\n#plt.plot(C_range, m_accuracy, '-', label=plot_labels[0], color=blues[0])\n#plt.plot(C_range, m_recall, '--', label=plot_labels[1], color=blues[1])\n#plt.plot(C_range, m_precision, '-.', label=plot_labels[2], color=blues[2])\n#plt.plot(C_range, m_f1, ':', label=plot_labels[3], color=blues[3])\nplt.plot(C_range, scores, '-', label=plot_labels[3], color=blues[1])\nplt.legend(loc='lower right')\nplt.title('Logistic Regression Classification - Performance vs C Value')\nplt.xlabel('C Value')\nplt.ylabel('F1 Score')\nplt.show() \nprint(\"The Logistic Regression Classifier performance appears to reasonably plateau around \"+str(round(np.max(scores),4))+\" when the inverse \\nof regularization parameter C is greater than 0.1.\")","ac963ef6":"# Prototype the Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)  # Instantiate model object\ncv_score = cross_val_score(knn, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))","c1444092":"# Optimization of the Model\nfrom sklearn.neighbors import KNeighborsClassifier\nk_range = list(range(1,29,2))\nscores = []\n   \nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)  # Instantiate model object\n    cv_score = cross_val_score(knn, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\n    scores.append(np.mean(cv_score))\n    #m_accuracy.append(accuracy_score(y_test, y_predicted))\n    #m_recall.append(recall_score(y_test, y_predicted))\n    #m_precision.append(precision_score(y_test, y_predicted))\n    #m_f1.append(f1_score(y_test, y_predicted))\n\nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nplot_labels = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\nblues = [\"#66D7EB\", \"#51ACC5\", \"#3E849E\", \"#2C5F78\", \"#1C3D52\", \"#0E1E2B\"]\n#plt.plot(k_range, m_accuracy, '-', label=plot_labels[0], color=blues[0])\n#plt.plot(k_range, m_recall, '--', label=plot_labels[1], color=blues[1])\n#plt.plot(k_range, m_precision, '-.', label=plot_labels[2], color=blues[2])\n#plt.plot(k_range, m_f1, ':', label=plot_labels[3], color=blues[3])\nplt.plot(k_range, scores, '-', label=plot_labels[3], color=blues[1])\nplt.legend(loc='lower right')\nplt.title('K Nearest Neighbors Classification - Performance vs k Value')\nplt.xlabel('k Value')\nplt.ylabel('F1 Score')\nplt.show()\nprint(\"The K-Nearest Neighbors Classification Model reaches a peak F1 score of \"+str(round(np.max(scores),4))+\" when k=3.\")","bca52443":"# Prototype the Model\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()  # Instantiate model object\ncv_score = cross_val_score(gnb, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))","36fb67a2":"# Prototype the Model\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=None)  # Instantiate model object\ncv_score = cross_val_score(dt, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))","62badc7b":"# Optimization of the Model\nfrom sklearn.tree import DecisionTreeClassifier\n\ndepth_range = list(range(1,15))\nleaf_range = list(range(2,20))\nscores = np.zeros((len(depth_range), len(leaf_range)))\n\nfor d_idx in range(len(depth_range)):\n    for l_idx in range(len(leaf_range)):\n        dt = DecisionTreeClassifier(max_depth=depth_range[d_idx], max_leaf_nodes=leaf_range[l_idx])  # Instantiate model object\n        cv_score = cross_val_score(dt, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\n        scores[d_idx, l_idx] = np.mean(cv_score)\n\nX, Y = np.meshgrid(leaf_range, depth_range)\nZ = scores\nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap=\"RdYlGn\")\nax.view_init(azim=110, elev=15)\nax.set_zlabel('F1 Score')\nax.set_ylabel('Max Depth')\nax.set_xlabel('Max Leaf Nodes')\nplt.title('Decision Tree Performance')\nplt.show()\nX, Y = np.meshgrid(leaf_range, depth_range)\nZ = scores\nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')  # plot a second angle\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap=\"RdYlGn\")\nax.view_init(azim=350, elev=15)\nax.set_zlabel('F1 Score')\nax.set_ylabel('Max Depth')\nax.set_xlabel('Max Leaf Nodes')\nplt.title('Decision Tree Performance')\nplt.show()\nmax_score = str(round(np.max(scores),4))\nindices = np.where(scores == scores.max())\nprint(\"Decision Tree F1-Score Performance peaks at \"+max_score+\" where Max Leaf Nodes = \"+str(leaf_range[indices[1][0]])+\" and Max Depth >= \"+str(depth_range[indices[0][0]])+\".\")","d36751c2":"# Prototype the Model\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear', C=1.0)  # Instantiate model object\ncv_score = cross_val_score(svc, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))\n\nsvc = SVC(kernel='rbf', C=1.0)  # Instantiate model object\ncv_score = cross_val_score(svc, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score: \"+str(round(np.mean(cv_score),4)))","6c2bf383":"# Optimization of the Model\nfrom sklearn.svm import SVC\n\n# Linear Kernel\ngamma_range = np.linspace(0.05, 3.0, num=10)  # num=30 originally, reduced to 3 for processing speed, no effect on curve\nC_range  = np.linspace(0.1,1.5,15)\nlin_scores = np.zeros((len(gamma_range), len(C_range)))\nkernel_type = 'linear'\nfor g_idx in range(len(gamma_range)):\n    for c_idx in range(len(C_range)):\n        svc = SVC(kernel=kernel_type, C=C_range[c_idx], gamma=gamma_range[g_idx])  # Instantiate model object\n        cv_score = cross_val_score(svc, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\n        lin_scores[g_idx, c_idx] = np.mean(cv_score)\nX, Y = np.meshgrid(C_range, gamma_range)\nZ = lin_scores\nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap=\"RdYlGn\")\nax.view_init(azim=110, elev=15)\nax.set_zlabel('F1 Score')\nax.set_ylabel('gamma')\nax.set_xlabel('C parameter')\nplt.title('Support Vector Machine Performance - Kernel: Linear')\nplt.show()\nmax_score = str(round(np.max(lin_scores),4))\nindices = np.where(lin_scores == lin_scores.max())\nprint(\"SVM F1-Score plateaus at an expected value of \"+str(round(lin_scores[min(indices[0]),min(indices[1])],4))+\" when using the linear kernel and the \\ninverse of regularization strength parameter C is greater than \"+str(C_range[min(indices[1])])+\".\")","8b7e3ec4":"# Optimization of the Model\nfrom sklearn.svm import SVC\nwarnings.filterwarnings(\"ignore\")  # suppress warnings for cross_val_scores where F-1 score = 0\n\n# Radial Basis Function Kernel\ngamma_range = np.linspace(0.01, 3.0, num=20)\nC_range  = np.linspace(0.1,5.0,50)\nrbf_scores = np.zeros((len(gamma_range), len(C_range)))\nkernel_type = 'rbf'\nfor g_idx in range(len(gamma_range)):\n    for c_idx in range(len(C_range)):\n        svc = SVC(kernel=kernel_type, C=C_range[c_idx], gamma=gamma_range[g_idx])  # Instantiate model object\n        cv_score = cross_val_score(svc, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\n        rbf_scores[g_idx, c_idx] = np.mean(cv_score)\nX, Y = np.meshgrid(C_range, gamma_range)\nZ = rbf_scores\nfig=plt.figure(figsize=(14, 8), dpi= 120, facecolor='w', edgecolor='k')\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap=\"RdYlGn\")\nax.view_init(azim=110, elev=15)\nax.set_zlabel('F1 Score')\nax.set_ylabel('gamma')\nax.set_xlabel('C parameter')\nplt.title('Support Vector Machine Performance - Kernel: Radial Basis Function')\nplt.show()\nmax_score = str(round(np.max(rbf_scores),4))\nindices = np.where(rbf_scores == rbf_scores.max())\nprint(\"SVM F1-Score increases for small gamma values and an increasing regularization strength parameter C, \\nreaching an an expected maxiumum value of \"+str(round(rbf_scores[0,13],4))+\" when using the radial basis function kernel with \\nC parameter = \"+str(C_range[min(indices[1])])+\" and gamma = \"+str(round(gamma_range[min(indices[0])],4))+\".\")","36e79c0f":"features = [\"Pclass\",\"Sex\",\"Age\",\"FareZ\", \"SibSp\", \"Parch\", \"AgeFilled\"]\ntarget = [\"Survived\"]\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=0.175)  # Instantiate model object using optimal parameters\ncv_score = cross_val_score(lr, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score for optimized Logistic Regression model: \"+str(round(np.mean(cv_score),4)))\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)  # Instantiate model object\ncv_score = cross_val_score(knn, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score for optimized k-Nearest Neighbors model: \"+str(round(np.mean(cv_score),4)))\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3, max_leaf_nodes=7)  # Instantiate model object using optimal parameters\ncv_score = cross_val_score(dt, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score for optimized Decision Tree model: \"+str(round(np.mean(cv_score),4)))\n\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf', C=4.7, gamma=0.01)  # Instantiate model object using optimal parameters\ncv_score = cross_val_score(svc, df[features], df[target].values.ravel(), cv=cv_folds, scoring=scoring_method)\nprint(\"Mean cross validation score for optimized SVC model: \"+str(round(np.mean(cv_score),4)))","12f1a9b7":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=0.175)  # Instantiate model object using optimal parameters\nlr.fit(df[features], df[target].values.ravel())  # Fit model using all available training data","b2dce2e4":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)  # Instantiate model object using optimal parameters\nknn.fit(df[features], df[target].values.ravel())  # Fit model using all available training data","9e165a35":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=9)  # Instantiate model object using optimal parameters\ndt.fit(df[features], df[target].values.ravel())  # Fit model using all available training data","6027532b":"from sklearn.svm import SVC\nsvc = SVC(kernel='rbf', C=1.4, gamma=0.05)  # Instantiate model object using optimal parameters\nsvc.fit(df[features], df[target].values.ravel())  # Fit model using all available training data","90710531":"test_df = pd.read_csv(\"..\/input\/mltitanicproject\/titanic\/test.csv\")  # Read in the Test data supplied by kaggle\ntest_df.head()  # same as train.csv except for the missing Survived column","b9c96d3c":"# Data Cleansing and Feature Engineering\nfeatures = [\"Pclass\",\"Sex\",\"Age\",\"FareZ\", \"SibSp\", \"Parch\", \"AgeFilled\"]\ntarget = [\"Survived\"]\n\nsex_mapping = {'male': 0, 'female': 1}  # Codify and map Sex strings to integer values\ntest_df['Sex'] = test_df['Sex'].map(sex_mapping)\ntest_df[\"AgeFilled\"] = 1  # Create new feature where rows with Age NaNs are replaced with median age\ntest_df.loc[test_df[\"Age\"] > 0, \"AgeFilled\"] = 0\ntest_df[\"Age\"].fillna(28.0, inplace=True)  # Replace null data in Age column with the median Age = 28.0\ntest_df[\"Fare\"].fillna(14.5, inplace=True)  # Replace null data in Fare column with the median Fare = 14.5\ntest_df['FareZ'] = (test_df['Fare'] - test_df['Fare'].mean()) \/ test_df['Fare'].std()","d352d1ae":"lr_test_df = test_df\nknn_test_df = test_df\ndt_test_df = test_df\nsvm_test_df = test_df\n\nlr_test_predictions = lr.predict(lr_test_df[features])\nlr_test_df['Survived'] = lr_test_predictions\nlr_test_df[['PassengerId', 'Survived']].to_csv('lr_submission.csv', index=False)\n\nknn_test_predictions = knn.predict(knn_test_df[features])\nknn_test_df['Survived'] = knn_test_predictions\nknn_test_df[['PassengerId', 'Survived']].to_csv('knn_submission.csv', index=False)\n\ndt_test_predictions = dt.predict(dt_test_df[features])\ndt_test_df['Survived'] = dt_test_predictions\ndt_test_df[['PassengerId', 'Survived']].to_csv('dt_submission.csv', index=False)\n\nsvm_test_predictions = svc.predict(svm_test_df[features])\nsvm_test_df['Survived'] = svm_test_predictions\nsvm_test_df[['PassengerId', 'Survived']].to_csv('svm_submission.csv', index=False)","e799151b":"### Support Vector Machine","53255767":"## Load Test Data and Generate Predictions\n\nUse our trained and optimized models to predict suvival for passengers in the \"test.csv\". Since model preformance was fairly similar between many of the models surveyed, the approach forward would be to produce and submit a submission generated by each model. \n\n### Create and Fit Optimized Models with all Training Data","975c3f6d":"### Data Cleaning and Feature Engineering","2d7b3ca0":"### Naive Bayes","fa0f80b4":"### Feature Distributions","7e11c122":"### Load Test Data and Prepare it for Input to Model","36756609":"### Decision Trees","b4576ca7":"### k-Nearest Neighbors Classifier","2bed01ae":"### Make Predictions on Test Data & Create Submission Files","d4700035":"### Data Observations\n\n<ul>\n    <li>There were approximately 1.5 times more third-class passengers than first or second class passengers. <\/li>\n    <li>A high percentage of those third-class passengers were men.<\/li>\n    <li>First class passengers skew slightly older than other classes.<\/li>\n    <li>First-class passengers were more likely to survive than die in the tragedy. while third-class passengers had a noticeably low survival rate. <\/li>\n    <li>More women survived than perished while it appears a very low percentage of men survived<\/li>\n    <li>Survival tied to Age is relatively flat but it does appear than some small bias exists that shows favorable survival rates to younger passengers<\/li>\n    <\/ul>\n    ","fb20631e":"## Data Exploration and Feature Engineering","45c38844":"## Prototype and Fine Tune Promising Models\nThis project aims to solve a binary classification problem - predict a Titanic passenger's Survival based on their Sex, Age, Class, Fare Paid, and Family Connections on the ship. \n\nFor this initial investigation, the following classification models will be explored:\n<ul>\n    <li>Logistic Regression Classifier<\/li>\n    <li>k Nearest Neighbors Classifier<\/li>\n    <li>Naive Bayes<\/li>\n    <li>Decision Trees<\/li>\n    <li>Support Vector Machine<\/li>\n    <\/ul>\n    \nFor each prototyped model, k-fold cross validation will be used to split the data into multiple validation sets, allowing us to be more confident that our model is good when selecting hyperparameters.","a6e21a46":"### Results\nThe competition scored my results as follows:\n<ol>\n    <li><b>Logistic Regression Classifier<\/b>: 0.74999<\/li>\n    <li><b>k-Nearest Neighbors Classifier<\/b>: 0.66666<\/li>\n    <li><b>Decision Tree  Classifier<\/b>: 0.75609<\/li>\n    <li><b>Support Vector Machine Classifier<\/b>: 0.78571<\/li>\n    <\/ol>","df543be0":"# Titanic: Machine Learning from Disaster\n **Hands On Machine Learning Part I Class Project by Jeffrey Egan**\n \nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we are tasked to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to create a binary classification model capable of predicting which passengers survived the tragedy.","f84ce664":"### Pair Plots and Correlation Heat Maps","58c79011":"### Logistic Regression Classifier","46976b80":"#### Data Dictionary\n<ol>\n    <li><b>PassengerId<\/b>: \tPassenger unique ID, useful for submission of solution<\/li>\n    <li><b>Survived<\/b>: \tBinary value for survived:\t0 = No, 1 = Yes<\/li>\n    <li><b>Pclass<\/b>: \t \tTicket class \t1 = 1st, 2 = 2nd, 3 = 3rd<\/li>\n    <li><b>Name<\/b>: \t \tPassenger's Name - string<\/li>\n    <li><b>Sex<\/b>: Passenger's Sex<\/li>\n    <li><b>Age<\/b>: \t \tPassenger's Age in years \t<\/li>\n    <li><b>SibSp<\/b>: \t \t# of siblings \/ spouses aboard the Titanic \t<\/li>\n    <li><b>Parch<\/b>: \t \t# of parents \/ children aboard the Titanic \t<\/li>\n    <li><b>Ticket<\/b>: \t \tTicket number \t<\/li>\n    <li><b>Fare<\/b>: \t \tPassenger fare \t<\/li>\n    <li><b>Cabin<\/b>: \t \tCabin number \t<\/li>\n    <li><b>Embarked<\/b>: \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton<\/li>\n<\/ol>\n\nIn this analysis we will be leveraging the background information that women, children, and the upper-class were more likey to survive. In this notebook, analysis will be carrying forward the obviously useful features of `Pclass`, `Sex`, and `Age` to develop a classification model along with the feature of `Survived` which is our target label. \n\nThe data point of `Fare` may also be useful with ascertaining finer resolution on a passenger's socio-economic class, but given its distribution, is it meaningful to train and predict with? It appears to have several 0.0 values which may be glorified nulls along with what may be some erroneously high fare outliers. Models that require normalized data inputs may suffer from these outliers if data was min-max normalized so a z-score column `FareZ` was added for this feature.\n\n`Sex` as noted, is a useful feature but currently populated with \"male\" and \"female\" strings, a column of codified data will have to be created to replace it before it is a useful feature for machine learning algorithms.\n\n`Age` should be a useful feature but has null values present. Entries with null Age values with need feature engineering. Options include excluding rows with null `Age` values or replacing the null values with median or mean age. Initially, this notebook will proceed by replacing the null `Age` values with the median age of 28.\n\nRegarding \"women and children\" trend, family connections may or may not prove to be usefulfeature for predicting survival. To that end, the features `SibSp` and `Parch` will be preserved.\n\nLastly, the feature `PassengerId` will also be preserved as it is an inportant feature for the logistics of the competition submission but will not be used as an input to any machine learning models as it is meaningless to survival.\n\nThe remaining features in the data, namely `Name`, `Ticket`, `Cabin`, and `Embarked` will be omitted from the remaining analyses and model training."}}