{"cell_type":{"514b0672":"code","183dbf3c":"code","8bc8bd8b":"code","4cf76847":"code","d2fe5f15":"code","1a8b6d28":"code","f80f09c0":"code","c69a19c1":"code","309e23dc":"code","ba7f7d9a":"code","55c17b66":"code","99bea72c":"code","01216346":"code","32b63617":"code","75542a19":"code","c55874e8":"code","af8c8d1b":"code","e96b7837":"code","42fd81f9":"markdown","970310e7":"markdown","323d0744":"markdown","9b3dc56e":"markdown","960a4411":"markdown","56084fc8":"markdown","0951360b":"markdown","18eb591f":"markdown","2a9c8cfb":"markdown","967022ed":"markdown","0d0e4474":"markdown","26f7ddb4":"markdown","1b13b0a0":"markdown","d6a4168d":"markdown","2b415b12":"markdown","96a4a14c":"markdown","04a78008":"markdown","3a0ea05f":"markdown","8f7717f7":"markdown","f5e0023a":"markdown","fc0b9187":"markdown","a0eb3ef4":"markdown","6bd0c824":"markdown"},"source":{"514b0672":"import pandas                  as     pd\nimport numpy                   as     np\nfrom   sklearn.cluster         import AgglomerativeClustering, KMeans\nimport sklearn.datasets\nfrom   scipy.cluster.hierarchy import dendrogram, linkage\nfrom   matplotlib              import pyplot as plt\n%matplotlib inline\nnp.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation","183dbf3c":"data     =  sklearn.datasets.load_iris()\nx        =  pd.DataFrame(data.data, columns = list(data.feature_names))","8bc8bd8b":"z   = linkage(x, method = 'median')\n\nplt.figure(figsize=(20,7))\n\nden = dendrogram(z)\n\nplt.title('Dendrogram for the clustering of the dataset iris)')\nplt.xlabel('Type')\nplt.ylabel('Euclidean distance in the space with other variables')","4cf76847":"cluster_H = AgglomerativeClustering(n_clusters = 3, affinity = 'cosine', linkage = 'complete')","d2fe5f15":"model = cluster_H.fit(x)\nprint(model)","1a8b6d28":"ms          = np.column_stack((data.target,model.labels_))\ndf          = pd.DataFrame(ms, columns = ['Actual', 'Clusters'])\npd.crosstab(df['Actual'], df['Clusters'], margins=True)","f80f09c0":"n_clusters = 3\nplt.figure()\nplt.axes([0, 0, 1, 1])\nfor l, c in zip(np.arange(n_clusters), 'rgbk'):\n    plt.plot(x[model.labels_ == l].T, c=c, alpha=.5)\n    plt.axis('tight')\n    plt.axis('off')\n    plt.suptitle(\"AgglomerativeClustering(affinity=%s)\" % 'cosine', size=20)","c69a19c1":"df['Clusters']  = model.labels_.astype('int32')\ndf['Target']    = data.target.astype('int32')","309e23dc":"pd.crosstab(df['Target'], df['Clusters'])","ba7f7d9a":"iris_X  = x.values\niris_X  = np.array(iris_X)\niris_Y1 = df['Clusters']\niris_Y1 = np.array(iris_Y1)","55c17b66":"plt.scatter(iris_X[iris_Y1 == 0, 0], iris_X[iris_Y1 == 0, 1], s = 80, c = 'orange', label = 'Iris-setosa')\nplt.scatter(iris_X[iris_Y1 == 1, 0], iris_X[iris_Y1 == 1, 1], s = 80, c = 'yellow', label = 'Iris-versicolour')\nplt.scatter(iris_X[iris_Y1 == 2, 0], iris_X[iris_Y1 == 2, 1], s = 80, c = 'green', label = 'Iris-virginica')\nplt.legend()","99bea72c":"wcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)","01216346":"plt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')","32b63617":"cluster_Kmeans = KMeans(n_clusters = 3, random_state = 25)\nmodel_kmeans   = cluster_Kmeans.fit(x)\nest_kmeans    = model_kmeans.labels_\nest_kmeans","75542a19":"df.drop(columns       = ['Clusters', 'Target'], inplace = True)\ndf['Clusters']        = est_kmeans\ndf['Target']          = data.target\npd.crosstab(df['Target'], df['Clusters'])","c55874e8":"iris_X = x.values\niris_X = np.array(iris_X)\niris_Y1 = est_kmeans\niris_Y1 = np.array(iris_Y1)","af8c8d1b":"plt.scatter(iris_X[iris_Y1 == 0, 0], iris_X[iris_Y1 == 0, 1], s = 80, c = 'orange', label = 'Iris-setosa')\nplt.scatter(iris_X[iris_Y1 == 1, 0], iris_X[iris_Y1 == 1, 1], s = 80, c = 'yellow', label = 'Iris-versicolour')\nplt.scatter(iris_X[iris_Y1 == 2, 0], iris_X[iris_Y1 == 2, 1], s = 80, c = 'green', label = 'Iris-virginica')\nplt.legend()","e96b7837":"n_clusters = 3\nplt.figure()\nplt.axes([0, 0, 1, 1])\nfor l, c in zip(np.arange(n_clusters), 'rgbk'):\n    plt.plot(x[model_kmeans.labels_ == l].T, c=c, alpha=.5)\n    plt.axis('tight')\n    plt.axis('off')\n    plt.suptitle(\"K Means( clusters = %d)\" % n_clusters, size=20)","42fd81f9":"We observe that Iris-setosa has been correctly formed into a separate well-defined cluster, but the other two classes are not formed well.","970310e7":"### Visualize the clusters.","323d0744":"Plot a dendrogram to help us in deciding the threshold values for the clustering algorithm. \nWe decide the number of clusters by using this dendrogram.","9b3dc56e":"## Three distance metrics\n\n### 1) Euclidean distance or l2\n\nThe Euclidean distance between points a and b is the length of the line segment connecting them .\n$d_{Euc}{(a,b)}$ = $\\sqrt{(\\sum(a_i - b_i)}^2$\n\n### 2) Manhattan or l1\n\nThe Manhattan distance between points is calculate by adding the absolute value of the difference between the dimensions.\n\n$d_{Manhattan}{(a,b)}$ = ${\\sum{|(a_i - b_i)|}}$\n\n### 3) Cosine\n\nCosine distance is squared Euclidean distance with the data normalized to unit length.\n","960a4411":"We will use Agglomerative Clustering method. \n\nWe shall use a Dendrogram to decide the number of clusters required for the dataset.\nA *Dendrogram* is a tree diagram illustrating the arrangement of clusters.\n\nWe will import the package AgglomerativeClustering for building an agglomerative clustering model. We shall import other required packages like pandas, matplotlib, numpy etc.\nWe shall import datasets for obtaining the iris dataset.\nWe shall import the package dendrogram which allows us to create dendrogram.","56084fc8":"## Based on the dissimilarity of the clusters, we arrive at three clusters. ","0951360b":"## Linkage methods\n\nhttps:\/\/www.dummies.com\/programming\/big-data\/data-science\/data-science-performing-hierarchical-clustering-with-python\/\n\n### 1. Ward\nThis looks for spherical clusters, very cohesive inside and extremely differentiated from other groups. \nThis finds clusters of similar size and works well with Euclidean distance.\n\n### 2. Complete\nThis looks for most dissimilar data points and links clusters. Clusters created using this method comprises highly similar observations, making the clusters compact.\n\n### 3. Average\nThis looks for centroids of data points, ignoring their boundaries and links clusters. \nThese clusters are can be of different sizes and shapes making it attractive technique to be used in the field of biological sciences.","18eb591f":"### Building an Agglomerative Clustering Model\n\n#### 1. Initialize the model with number of clusters = 3, affinity = 'cosine' and linkage = 'complete'","2a9c8cfb":"The above table shows that we have classified 134 observations (50 + 48 + 36) out of 150 observations.","967022ed":"http:\/\/www.cad.zju.edu.cn\/home\/zhx\/csmath\/lib\/exe\/fetch.php?media=2011:presentation_ml_by_ibrar.pdf","0d0e4474":"Load the iris data ","26f7ddb4":"### K means","1b13b0a0":"## 1. K-means clustering\nK-means clustering is an algorithm to group your objects based on attributes into k number of groups and k > 0.\nGrouping is done by minimizing the sum of squares of distances between data and the corresponding cluster centroid. \n\nhttp:\/\/www.cad.zju.edu.cn\/home\/zhx\/csmath\/lib\/exe\/fetch.php?media=2011:presentation_ml_by_ibrar.pdf","d6a4168d":"Refer the above elbow graph. Observe the points where the drop falls and the line smoothens out. So k=3 because till k = 3, the decline is sharp. ","2b415b12":"## Two common algorithms:\n### 1. Hierarchical clustering\n### 2. K-mean clustering","96a4a14c":"Decided the number of clusters, k\nBy performing Elbow analysis by running K-means for the value of k from 1 to 11, we find the optimum value of k.\nfor i in range(1, 11):","04a78008":"#### 2. Fitting the model\n\nAfter building the model, fit the iris data set by taking only independent variables.","3a0ea05f":"In hierarchical clustering, we don't use a set number of clusters but rather we arrange the data in a hierarchy where on top of the hierarchy there is a single big cluster and at the bottom of the hierarchy we have as many clusters as many observations in the data set.\n\n## Two common methods of hierarchical clustering algorithms:\n\n### Agglomerative Hierarchical Clustering Algorithms\n\nIn this approach, we initially assign different clusters to each observation. Based on similarity we consolidate until we arrive at one single big cluster.\n\n### Divisive Hierarchical Clustering Algorithm\n\nIn this approach, we initially assign a single cluster to  all the observations.The clusters are divided continuously until we have one cluster for each observation.","8f7717f7":"Here also we found that Iris-setosa has been clearly formed into separate cluster \nwhile the cluster, virginica overlap with Versicolor.","f5e0023a":"K-means is the most commonly used method for clustering.\nIt requires us to define the number of clusters. \n","fc0b9187":"## Clustering\n\nIn Machine Learning, unsupervised learning is a class of problems in which one seeks to determine how the data are organized. It is different from supervised learning and reinforcement learning in that learner is given only unlabeled examples.\n< to change>\n\nClustering is a method of unsupervised learning for performing statistical analysis used in many fields.\nClustering is about dividing the data set into sub sets, or clusters, wherein the observations should be similar to those in the same cluster but differ greatly from the observations in other clusters. ","a0eb3ef4":"Retrieve the predicted labels from model.labels_ and compare with the target labels by using the crosstab(), function.","6bd0c824":"## 2. Hierarchical clustering\nHierarchical  (aka agglomerative) clustering tries to link each data point by a distance measure, to its nearest neighbour, creating a cluster. Reiterating the algorithm using different linkage methods, the algorithm gathers all the available points into a rapidly diminishing number of clusters, until in the end all the points reunite into a single group.\n"}}