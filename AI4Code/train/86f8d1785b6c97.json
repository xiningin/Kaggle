{"cell_type":{"f220b92b":"code","d3ed85e8":"code","89999aa6":"code","2c963eb8":"code","ba11cce3":"code","380aaa6a":"code","9b3ceb76":"code","5a5806b6":"code","8b446a7c":"code","876ac1aa":"code","a087e939":"code","705246e9":"code","5b4fe2a9":"code","4b22db61":"code","a1b0fe11":"code","d67e4095":"code","d6f1c83c":"code","2127b603":"code","40b97c83":"code","1a211678":"code","d0df5f28":"code","ecadd933":"code","a1ff32fe":"code","00f2b050":"code","7ebbd61d":"code","9f4c7ddb":"code","65c1b020":"code","7ed2fd72":"code","dc1d7cc6":"code","985918d0":"code","1c33be1e":"code","fd31ab31":"code","ed12edda":"code","a37410ae":"code","039d817c":"code","131efeae":"code","0f60f85a":"code","6db8f728":"code","edbe0368":"code","e6b2ffcf":"code","67582bdd":"code","0a622b3a":"code","b018529b":"code","e123c506":"code","8c37b93f":"code","ed7afc23":"code","82f534f0":"code","133705cc":"code","0a3d0c65":"code","b9c43ba8":"code","ffb6e515":"code","e13a40cc":"code","add5689b":"code","4164c057":"code","5a36f582":"code","393bab61":"code","8d902b1a":"code","27c88406":"code","5a793ea0":"code","964840c3":"code","4bdc978b":"code","6d4cf18f":"code","7b18cf12":"code","dce0f1ab":"code","d8761eba":"code","c5425ed3":"code","74341e3e":"code","63b8ccf3":"markdown","6a12348f":"markdown","8b898cc0":"markdown","46c1d70d":"markdown","3c24be74":"markdown","6f9b18e0":"markdown","32e4a6a4":"markdown","747cf07a":"markdown","62e788b9":"markdown","fd0d1a66":"markdown","7d19e29e":"markdown","6e55a87d":"markdown","2ac9a85e":"markdown","3cb465b0":"markdown","ed786fc8":"markdown","99b370b8":"markdown","928da361":"markdown","5b1c5d37":"markdown","0ae4e28a":"markdown","9f5d5bb4":"markdown","73b08d6b":"markdown","daca6194":"markdown","8f1d8938":"markdown","4941870d":"markdown","c7fa2346":"markdown","d12af6da":"markdown","9a675ef2":"markdown","b041b69a":"markdown","d8152eb9":"markdown","938d47d5":"markdown","8cb32db8":"markdown","b4c40bf3":"markdown","4223a04a":"markdown","266e0415":"markdown","8c8e5aa4":"markdown","a4b882f9":"markdown","4e1c2b4d":"markdown"},"source":{"f220b92b":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D, SpatialDropout2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image","d3ed85e8":"# Selecting Dataset Folder Paths\nf_dir_ = Path('..\/input\/eyes-rtte\/femaleeyes')\nm_dir_ = Path('..\/input\/eyes-rtte\/maleeyes')\nfemaleeyes_filepaths = list(f_dir_.glob(r'**\/*.jpg'))\nmaleeyes_filepaths = list(m_dir_.glob(r'**\/*.jpg'))\n\n# Mapping the labels\nfm_labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], femaleeyes_filepaths))\nml_labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], maleeyes_filepaths))\n\n# Paths & labels femalee eyes\nfm_filepaths = pd.Series(femaleeyes_filepaths, name = 'File').astype(str)\nfm_labels = pd.Series(fm_labels, name='Label')\n\n# Paths & labels malee eyes\nml_filepaths = pd.Series(maleeyes_filepaths, name = 'File').astype(str)\nml_labels = pd.Series(ml_labels, name='Label')\n\n# Concatenating...\nfemaleeyes_df = pd.concat([fm_filepaths, fm_labels], axis=1)\nmaleeyes_df = pd.concat([ml_filepaths, ml_labels], axis=1)\n\ndf = pd.concat([femaleeyes_df, maleeyes_df])\n\ndf = df.sample(frac = 1, random_state = 56).reset_index(drop = True)","89999aa6":"vc = df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc)\nplt.title(\"Number of images for each category in the Training Dataset\", fontsize = 11)\nplt.show()","2c963eb8":"plt.style.use(\"dark_background\")","ba11cce3":"figure = plt.figure(figsize=(2,2))\nx = plt.imread(df[\"File\"][34])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(df[\"Label\"][34])","380aaa6a":"figure = plt.figure(figsize=(2, 2))\nx = plt.imread(df[\"File\"][11])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(df[\"Label\"][11])","9b3ceb76":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (7, 7),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df[\"File\"][i]))\n    ax.set_title(df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","5a5806b6":"trainset_df, testset_df = train_test_split(df, train_size = 0.75, random_state = 4)\n\ndisplay(trainset_df.head())\n\ntestset_df.head()","8b446a7c":"# converting the Label to a numeric format for testing later...\nLE = LabelEncoder()\n\ny_test = LE.fit_transform(testset_df[\"Label\"])","876ac1aa":"# Viewing data in training dataset\nprint('Training Dataset:')\n\nprint(f'Number of images: {trainset_df.shape[0]}')\n\nprint(f'Number of images with malee eyes: {trainset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with femalee eyes: {trainset_df[\"Label\"].value_counts()[1]}\\n')\n\n# Viewing data in test dataset\nprint('Test Dataset:')\n\nprint(f'Number of images: {testset_df.shape[0]}')\n\nprint(f'Number of images with malee eyes: {testset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with femalee eyes: {testset_df[\"Label\"].value_counts()[1]}\\n')","a087e939":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                    shear_range = 0.2,\n                                    zoom_range = 0.1,\n                                    rotation_range = 20,\n                                    width_shift_range = 0.1,\n                                    height_shift_range = 0.1,\n                                    horizontal_flip = True,\n                                    vertical_flip = True,\n                                    validation_split = 0.1)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","705246e9":"print(\"Preparing the training dataset ...\")\ntraining_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode = \"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"training\")\n\nprint(\"Preparing the validation dataset ...\")\nvalidation_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"validation\")\n\nprint(\"Preparing the test dataset ...\")\ntest_set = test_datagen.flow_from_dataframe(\n    dataframe = testset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (75, 75),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    shuffle = False,\n    batch_size = 32)\n\nprint('Data generators are ready!')","5b4fe2a9":"print(\"Training: \")\nprint(training_set.class_indices)\nprint(training_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Validation: \")\nprint(validation_set.class_indices)\nprint(validation_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Test: \")\nprint(test_set.class_indices)\nprint(test_set.image_shape)","4b22db61":"# Callbacks\ncb = [EarlyStopping(monitor = 'loss', mode = 'min', patience = 15, restore_best_weights = True)]","a1b0fe11":"CNN = Sequential()\n\nCNN.add(Conv2D(32, (3, 3), input_shape = (75, 75, 3), activation = 'relu'))\nCNN.add(BatchNormalization())","d67e4095":"CNN.add(MaxPooling2D(pool_size = (2, 2)))","d6f1c83c":"CNN.add(Conv2D(32, (3, 3), activation = 'relu'))\nCNN.add(MaxPooling2D(pool_size = (2, 2)))","2127b603":"CNN.add(Conv2D(64, (3, 3), activation = 'relu'))\nCNN.add(SpatialDropout2D(0.2))\nCNN.add(MaxPooling2D(pool_size = (2, 2)))","40b97c83":"CNN.add(Flatten())","1a211678":"# Input layer\nCNN.add(Dense(units = 128, activation = 'relu'))\nCNN.add(Dropout(0.2))\n# Output layer (binary classification)\nCNN.add(Dense(units = 1, activation = 'sigmoid'))\n\nprint(CNN.summary())","d0df5f28":"plot_model(CNN, to_file='CNN_model.png', show_layer_names = True , show_shapes = True)","ecadd933":"# Compile\nCNN.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time...\nstart = dt.datetime.now()\n\n# Train\nCNN_model = CNN.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of counting time...\nend = dt.datetime.now()\ntime_CNN = end - start\nprint ('\\nTraining and validation time is: ', time_CNN)","a1ff32fe":"acc = CNN_model.history['accuracy']\nval_acc = CNN_model.history['val_accuracy']\nloss = CNN_model.history['loss']\nval_loss = CNN_model.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","00f2b050":"score_CNN = CNN.evaluate(test_set)\nprint(\"Test Loss:\", score_CNN[0])\nprint(\"Test Accuracy:\", score_CNN[1])","7ebbd61d":"y_pred_CNN = CNN.predict(test_set)\ny_pred_CNN = np.round(y_pred_CNN)\n\nrecall_CNN = recall_score(y_test, y_pred_CNN)\nprecision_CNN = precision_score(y_test, y_pred_CNN)\nf1_CNN = f1_score(y_test, y_pred_CNN)\nroc_CNN = roc_auc_score(y_test, y_pred_CNN)","9f4c7ddb":"print(classification_report(y_test, y_pred_CNN))","65c1b020":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_CNN),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","7ed2fd72":"# Save the model\nmodelFileName = 'cats-dogs-classifier.h5'\nCNN.save(modelFileName)\nprint('model saved as', modelFileName)","dc1d7cc6":"CNN_base_inc = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')","985918d0":"for layer in CNN_base_inc.layers:\n    layer.trainable = False","1c33be1e":"x = layers.Flatten()(CNN_base_inc.output)","fd31ab31":"x = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\n\nCNN_inc = Model(CNN_base_inc.input, x)","ed12edda":"# Compilation\nCNN_inc.compile(optimizer = RMSprop(lr = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_inc_history = CNN_inc.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_inc = end - start\nprint ('\\nTraining and validation time is: ', time_CNN_inc)","a37410ae":"acc = CNN_inc_history.history['accuracy']\nval_acc = CNN_inc_history.history['val_accuracy']\nloss = CNN_inc_history.history['loss']\nval_loss = CNN_inc_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","039d817c":"score_inc = CNN_inc.evaluate(test_set)\nprint(\"Test Loss:\", score_inc[0])\nprint(\"Test Accuracy:\", score_inc[1])","131efeae":"y_pred_inc = CNN_inc.predict(test_set)\ny_pred_inc = np.round(y_pred_inc)\n\nrecall_inc = recall_score(y_test, y_pred_inc)\nprecision_inc = precision_score(y_test, y_pred_inc)\nf1_inc = f1_score(y_test, y_pred_inc)\nroc_inc = roc_auc_score(y_test, y_pred_inc)","0f60f85a":"print(classification_report(y_test, y_pred_inc))","6db8f728":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_inc),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","edbe0368":"# Save the model\nmodelFileName = 'fire_classifier_model-inc.h5'\nCNN_inc.save(modelFileName)\nprint('model saved as', modelFileName)","e6b2ffcf":"CNN_base_xcep = Xception(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\nCNN_base_xcep.trainable = False","67582bdd":"CNN_xcep = Sequential()\nCNN_xcep.add(CNN_base_xcep)\nCNN_xcep.add(GlobalAveragePooling2D())\nCNN_xcep.add(Dense(128))\nCNN_xcep.add(Dropout(0.1))\nCNN_xcep.add(Dense(1, activation = 'sigmoid'))\n\nCNN_xcep.summary()","0a622b3a":"plot_model(CNN_xcep, show_layer_names = True , show_shapes = True)","b018529b":"# Compilation\nCNN_xcep.compile(optimizer='adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_xcep_history = CNN_xcep.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_xcep = end - start\nprint ('\\nTraining and validation time: ', time_CNN_xcep)","e123c506":"acc = CNN_xcep_history.history['accuracy']\nval_acc = CNN_xcep_history.history['val_accuracy']\nloss = CNN_xcep_history.history['loss']\nval_loss = CNN_xcep_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","8c37b93f":"score_xcep = CNN_xcep.evaluate(test_set)\nprint(\"Test Loss:\", score_xcep[0])\nprint(\"Test Accuracy:\", score_xcep[1])","ed7afc23":"y_pred_xcep = CNN_xcep.predict(test_set)\ny_pred_xcep = np.round(y_pred_xcep)\n\nrecall_xcep = recall_score(y_test, y_pred_xcep)\nprecision_xcep = precision_score(y_test, y_pred_xcep)\nf1_xcep = f1_score(y_test, y_pred_xcep)\nroc_xcep = roc_auc_score(y_test, y_pred_xcep)","82f534f0":"print(classification_report(y_test, y_pred_xcep))","133705cc":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_xcep),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","0a3d0c65":"modelFileName = 'fire_classifier_model-xcep.h5'\nCNN_xcep.save(modelFileName)\nprint('model saved as', modelFileName)","b9c43ba8":"CNN_base_mobilenet = MobileNet(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')","ffb6e515":"for layer in CNN_base_mobilenet.layers:\n    layer.trainable = False","e13a40cc":"CNN_mobilenet = Sequential()\nCNN_mobilenet.add(BatchNormalization(input_shape = (75, 75, 3)))\nCNN_mobilenet.add(CNN_base_mobilenet)\nCNN_mobilenet.add(BatchNormalization())\nCNN_mobilenet.add(GlobalAveragePooling2D())\nCNN_mobilenet.add(Dropout(0.5))\nCNN_mobilenet.add(Dense(1, activation = 'sigmoid'))\n\nCNN_mobilenet.summary()","add5689b":"plot_model(CNN_mobilenet, show_layer_names = True , show_shapes = True)","4164c057":"# Compilation\nCNN_mobilenet.compile(optimizer='adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_mobilenet_history = CNN_mobilenet.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_mobilenet = end - start\nprint ('\\nTraining and validation time: ', time_CNN_mobilenet)","5a36f582":"acc = CNN_mobilenet_history.history['accuracy']\nval_acc = CNN_mobilenet_history.history['val_accuracy']\nloss = CNN_mobilenet_history.history['loss']\nval_loss = CNN_mobilenet_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","393bab61":"score_mn = CNN_mobilenet.evaluate(test_set)\nprint(\"Test Loss:\", score_mn[0])\nprint(\"Test Accuracy:\", score_mn[1])","8d902b1a":"y_pred_mn = CNN_mobilenet.predict(test_set)\ny_pred_mn = np.round(y_pred_mn)\n\nrecall_mn = recall_score(y_test, y_pred_mn)\nprecision_mn = precision_score(y_test, y_pred_mn)\nf1_mn = f1_score(y_test, y_pred_mn)\nroc_mn = roc_auc_score(y_test, y_pred_mn)","27c88406":"print(classification_report(y_test, y_pred_mn))","5a793ea0":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_mn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","964840c3":"# Save the model\nmodelFileName = 'fire_classifier_model-mobilenet.h5'\nCNN_mobilenet.save(modelFileName)\nprint('model saved as', modelFileName)","4bdc978b":"models= [('ConvNet', time_CNN, np.mean(CNN_model.history['accuracy']), np.mean(CNN_model.history['val_accuracy'])),\n         ('Inception', time_CNN_inc, np.mean(CNN_inc_history.history['accuracy']), np.mean(CNN_inc_history.history['val_accuracy'])),\n         ('Xception', time_CNN_xcep, np.mean(CNN_xcep_history.history['accuracy']), np.mean(CNN_xcep_history.history['val_accuracy'])),\n         ('MobileNet', time_CNN_mobilenet, np.mean(CNN_mobilenet_history.history['accuracy']), np.mean(CNN_mobilenet_history.history['val_accuracy']))]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Time', 'Training accuracy (%)', 'Validation Accuracy (%)'])\n\ndf_all_models","6d4cf18f":"models = [('ConvNet', score_CNN[1], recall_CNN, precision_CNN, f1_CNN, roc_CNN),\n          ('Inception', score_inc[1], recall_inc, precision_inc, f1_inc, roc_inc),\n          ('Xception', score_xcep[1], recall_xcep, precision_xcep, f1_xcep, roc_xcep),\n          ('MobileNet', score_mn[1], recall_mn, precision_mn, f1_mn, roc_mn)]\n\ndf_all_models_testset = pd.DataFrame(models, columns = ['Model', 'Test accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n\ndf_all_models_testset","7b18cf12":"plt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models_testset['Test accuracy (%)'], x = df_all_models_testset['Model'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","dce0f1ab":"r_probs = [0 for _ in range(len(y_test))]\nr_auc = roc_auc_score(y_test, r_probs)\nr_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n\nfpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_pred_CNN)\nfpr_inc, tpr_inc, _ = roc_curve(y_test, y_pred_inc)\nfpr_xcep, tpr_xcep, _ = roc_curve(y_test, y_pred_xcep)\nfpr_mn, tpr_mn, _ = roc_curve(y_test, y_pred_mn)","d8761eba":"sns.set_style('darkgrid')\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n\nplt.plot(fpr_cnn, tpr_cnn, marker='.', label='ConvNet (AUROC = %0.3f)' % roc_CNN)\nplt.plot(fpr_inc, tpr_inc, marker='.', label='Inception (AUROC = %0.3f)' % roc_inc)\nplt.plot(fpr_xcep, tpr_xcep, marker='.', label='Xception (AUROC = %0.3f)' % roc_xcep)\nplt.plot(fpr_mn, tpr_mn, marker='.', label='MobileNet (AUROC = %0.3f)' % roc_mn)\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend() \nplt.show()","c5425ed3":"test_set.class_indices","74341e3e":"plt.style.use(\"dark_background\")\n\n\nfig, axes = plt.subplots(nrows = 4,\n                         ncols = 4,\n                         figsize = (15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"].iloc[i]))\n    ax.set_title(f\"True: {testset_df.Label.iloc[i]}\\n Predicted:\\nConvNet: {y_pred_CNN[i]}\\nInception: {y_pred_inc[i]}\\nXception: {y_pred_xcep[i]}\\nMobileNet: {y_pred_mn[i]}\")\nplt.tight_layout()\nplt.show()","63b8ccf3":"## 1. Imports from libraries","6a12348f":"## 9. Construction of the second model (Inception)\nThe [InceptionV3](https:\/\/keras.io\/api\/applications\/inceptionv3\/) model proposed by Szegedy et al. (2015), is a CNN architecture that seeks to solve several large-scale image recognition problems and can also be used in transfer learning problems. Its differential is the presence of convolutional characteristics extractor modules. These modules have the functionality to learn with fewer parameters that contain a greater range of information.\n\n<p><img src = \"https:\/\/cloud.google.com\/tpu\/docs\/images\/inceptionv3onc--oview.png?hl=pt-br\" alt><\/p>","8b898cc0":"###### Step 8 - Viewing results and generating forecasts","46c1d70d":"## 12. Construction of the fourth model (MobileNet)\nThe MobileNet model proposed by Howard et al. (2017), is a CNN architecture that were created to perform computer vision tasks on mobile devices and embedded systems. They are based on in-depth separable convolution operations, which lessens the burden of operations in the first layers.\n\n<p><img src = \"https:\/\/nitheshsinghsanjay.github.io\/images\/mobtiny_fig.PNG\" alt><\/p>","3c24be74":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","6f9b18e0":"###### Step 3 - Hidden Layers","32e4a6a4":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","747cf07a":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","62e788b9":"###### Step 4 - Flattening\n    \n     Transforming the matrix to a vector to enter the Artificial Neural Network layer","fd0d1a66":"###### Step 8 - Viewing results and generating forecasts","7d19e29e":"###### Step 5 - Dense Neural Networks\n\nParameters of the `` RNA``:\n\n     Dense - All neurons connected\n     units - Number of neurons that are part of the hidden layer\n     activation - Activation function that will be inserted\n     Dropout - is used to decrease the chance of overfitting (20% of the input neurons are zeroed)\n\nParameters of the ``EarlyStopping``:\n\n     monitor - Metric to be monitored\n     patience - Number of seasons without improvement in the model, after the training is interrupted\n     restore_best_weights - Restores the best weights if training is interrupted","6e55a87d":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","2ac9a85e":"## 5. Directory of training, validation and test images\n\nHere we make the division of the image bases for training, validation and testing of the model, for that we use the [flow_from_dataframe](https:\/\/keras.io\/api\/preprocessing\/image\/#flowfromdataframe-method)\n\nParameters of ``flow_from_directory``:\n\n    dataframe - Dataframe containing the images directory\n    x_col - Column name containing the images directory\n    y_col - Name of the column containing what we want to predict\n    target_size - size of the images (remembering that it must be the same size as the input layer)\n    color_mode - RGB color standard\n    class_mode - binary class mode (cat\/dog)\n    batch_size - batch size (32)\n    shuffle - Shuffle the data\n    seed - optional random seed for the shuffle\n    subset - Subset of data being training and validation (only used if using validation_split in ImageDataGenerator)","3cb465b0":"###### Step 2 - Flattening\n    Transforming the matrix to a vector to enter the Artificial Neural Network layer","ed786fc8":"###### Step 1 - Convolution\nFeature Detector and Feature Map\n\n    Number of filters (32)\n    Dimensions of the feature detector (3, 3)\n    Definition of height \/ width and RGB channels (128, 128, 3)\n    Activation function to remove negative values from the image - 'relu'\n    Processing acceleration - BatchNormalization","99b370b8":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","928da361":"## 10. Construction of the third model (Xception)\nThe [Xception](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/Xception) model proposed by Chollet et al.(2016), is a CNN architecture similar to the Inception described above and, has the difference that the initiation modules were replaced by separable convolutions in depth. Xception has the same amount of parameters as InceptionV3 with a total of 36 convolutional layers. Thus, having a more efficient use of parameters.\n\n<p><img src = \"https:\/\/miro.medium.com\/max\/1688\/1*J8dborzVBRBupJfvR7YhuA.png\" alt><\/p>","5b1c5d37":"## 6. Construction of the first model (ConvNet)\n\nCNNs are a specific type of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, that is, that pixels close to each other are often related.\n\nThe construction of a CNN begins with specifying the model type. In our case, we will use a ``Sequential`` model.\n\n<p><img src = \"https:\/\/i.ibb.co\/0jWhFsW\/ConvNet.png\" alt><\/p>","0ae4e28a":"###### Step 1 - Base model creation\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","9f5d5bb4":"## 3. Observing the images","73b08d6b":"## Detection of Female and Male eyes using Convolutional Neural Networks\n\n<p><img src = \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRQtQnWtMBaRN0OznlOnl98spYju8ijAMTUVA&usqp=CAU\" alt align=\"center\"><\/p>\n\n#### Dataset information:\n\n- The data was collected to train a model to distinguish between images containing Female eyes and images of Male eyes, so the whole problem is binary classification.\n\n\nThe data is divided into 2 folders:\n- The folder `` femaleeyes`` contains 5202 images and the folder `` maleeyes`` contains 6323 images for training and testing the model.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\n- https:\/\/www.kaggle.com\/pavelbiz\/eyes-rtte","daca6194":"## 3. Dividing into training and testing sets\nNow we need to convert our data into training and testing sets. We will use 75% of the images as our training data and test our model on the remaining 25% with Scikit-learn's train_test_split function.","8f1d8938":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","4941870d":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","c7fa2346":"###### Step 7 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","d12af6da":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","9a675ef2":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","b041b69a":"## 4. Generating batches of images\nIn this part we will generate batches of images increasing the training data, for the test database we will just normalize the data using [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class)\n\nParameters of ``ImageDataGenerator``:\n\n    rescale - Transform image size (normalization of data)\n    shear_range - Random geometric transformations\n    zoom_range - Images that will be zoomed\n    rotation_range - Degree of image rotation\n    width_shift_range - Image Width Change Range\n    height_shift_range - Image height change range\n    horizontal_flip - Rotate images horizontally\n    vertical_flip - Rotate images vertically\n    validation_split - Images that have been reserved for validation (0-1)","d8152eb9":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","938d47d5":"Use of callbacks to monitor models and see if metrics will improve, otherwise training is stopped.\n\n``EarlyStopping`` parameters:\n\n    monitor - Metrics that will be monitored\n    patience - Number of times without improvement in the model, after these times the training is stopped\n    restore_best_weights - Restores best weights if training is interrupted","8cb32db8":"###### Step 3 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","b4c40bf3":"###### Step 6 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","4223a04a":"###### Step 8 - Viewing results and generating forecasts","266e0415":"## 13. Viewing the results of all models","8c8e5aa4":"###### Step 2 - Max Pooling\nReduced image size by focusing on the most important features\n\n     Matrix definition with a total of 4 pixels (2, 2)","a4b882f9":"###### Step 6 - Viewing results and generating forecasts","4e1c2b4d":"## 2. Organizing Training and Testing Dataframes"}}