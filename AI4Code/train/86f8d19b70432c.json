{"cell_type":{"761d4c52":"code","b0cc1827":"code","ffb7e229":"code","b7dd5c20":"code","7012c729":"code","31d5e3bc":"code","494ebb7e":"code","8ae4a8ca":"code","bf6c3a6b":"code","f90a2685":"code","d6ce7c9a":"code","5bdc046c":"code","c8bb9226":"code","bb3977cf":"code","742bfea1":"code","31ada265":"code","03dfb424":"code","8758b7b1":"code","70193ddb":"code","d085a234":"code","21a9366c":"code","4a1c1916":"code","2acf994b":"code","040f3d54":"code","23df07e1":"code","863063c2":"code","5355fd58":"code","e8b35f3c":"code","313afccf":"code","5443274f":"code","c7777d35":"code","66752442":"code","c1433cb8":"code","6957468e":"code","2e9e4e4b":"code","c048fcbb":"code","1f8181c0":"code","ce6a853a":"code","954e56dc":"code","bfa69be4":"code","6fb570e0":"code","c27fe04d":"code","bcba5452":"code","ad864afc":"code","6861eedf":"code","da3b56fe":"code","02495a1b":"code","27cf442a":"code","cbd7f3ff":"code","1953a9e4":"code","4467680a":"code","731e293f":"code","1bc38558":"code","335dacb3":"code","84944379":"code","97c47477":"code","6c86f361":"code","e17d354f":"code","88925265":"code","23413ae5":"code","2b35b731":"code","47fc78dc":"code","82aede11":"code","dcde7889":"code","de7cdd24":"code","af61883e":"code","af6f9f7c":"code","a1ac6df4":"code","b06e035e":"code","6ba85486":"markdown","46e05c13":"markdown","4408965e":"markdown","f7e3967c":"markdown","8620248b":"markdown","687d3659":"markdown","d1c8c53c":"markdown","9460cc9f":"markdown","73ee5d66":"markdown","2443edd6":"markdown","39f60e86":"markdown","ebf265dc":"markdown","da4c1657":"markdown","9244257f":"markdown","1cc26f84":"markdown","d0684557":"markdown","b1d7017c":"markdown","b68fbacc":"markdown","9b850664":"markdown","11fe32bb":"markdown","67369b4e":"markdown","5f4bcdef":"markdown","e5d0ebac":"markdown","49afb905":"markdown","d7d7a71f":"markdown","03b488ec":"markdown","9ddea8e3":"markdown","a26f3877":"markdown","dbdd89e3":"markdown","bc5a3bc0":"markdown","831f67c5":"markdown","85e5da55":"markdown","c4c3cc67":"markdown","bc826ad2":"markdown","061d9411":"markdown","b6649ce3":"markdown","65a42df1":"markdown","c921b210":"markdown","3d9dc704":"markdown","f2a7dcab":"markdown","3fff27bc":"markdown","7f5c1561":"markdown","678cd7b5":"markdown","f3198152":"markdown","eba27cef":"markdown","62e3ef17":"markdown","ae3f630c":"markdown","4b433ea5":"markdown","18cc01fb":"markdown","ea65a64f":"markdown","64e4dcde":"markdown","16bddbf5":"markdown","b000a437":"markdown","ea5d8140":"markdown","0ebdbc59":"markdown","3c7949f4":"markdown","ca6673bc":"markdown","3608a5a7":"markdown","66b2350f":"markdown","79083b94":"markdown","e72dea59":"markdown","f66e0b1e":"markdown","06d00c4b":"markdown","40007500":"markdown","b6c6c47c":"markdown","d3522ff2":"markdown","2dc5a0dd":"markdown","0b68962f":"markdown","e7a4f729":"markdown","34643a4c":"markdown","8fded6ab":"markdown","77d90eb4":"markdown","c38a4aba":"markdown","ad79304b":"markdown","e447ef84":"markdown"},"source":{"761d4c52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b0cc1827":"#import pylab\nimport calendar\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nfrom scipy import stats\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport warnings\n\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline","ffb7e229":"df = pd.read_csv(\"\/kaggle\/input\/ml-training-vlib\/vlib.csv\", sep = ',')","b7dd5c20":"df._______","7012c729":"df._______","31d5e3bc":"df._______","494ebb7e":"df._______","8ae4a8ca":"df._______","bf6c3a6b":"df._______()._______()","f90a2685":"for i in df.columns:\n    print(i,\" : \",df[i]._______().sum())","d6ce7c9a":"list_variables = [\"_______\",\"season\",\"weather\",\"holiday\",\"workingday\"]","5bdc046c":"\"2011-01-01 00:00:00\".split()","c8bb9226":"df[\"date\"] = df.datetime.apply(lambda x : x.split()[0])\ndf[\"date\"]","bb3977cf":"df[\"hour\"] = df.datetime._______(lambda x : x.split()[1])\ndf[\"hour\"]","742bfea1":"df[\"hour\"] = df.datetime._______(lambda x : x.split()[1].split(\":\")[0])\ndf[\"hour\"] ","31ada265":"df[\"weekday\"] = df.date.apply(lambda x : calendar.day_name[datetime.strptime(x,\"%Y-%m-%d\").weekday()])\ndf[\"month\"] = df.date.apply(lambda x : calendar.month_name[datetime.strptime(x,\"%Y-%m-%d\")._______])","03dfb424":"df.head()","8758b7b1":"dictionnaire_saisons = {2: \"_______\", 3 : \"_______\", 4 : \"_______\", 1 :\"_______\" }\ndf[\"season\"] = df.season.map(dictionnaire_saisons)\n\ndf[\"weather\"] = df.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })","70193ddb":"df.head()","d085a234":"categoryVariableList = [\"hour\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\"]\nfor var in categoryVariableList:\n    df[var] = df[var].astype(\"_______\")","21a9366c":"dropFeatures = [\"_______\"]\n\ndf  = df.drop(dropFeatures, axis=1) # Suppression d'une colonne","4a1c1916":"fig, axes = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(8, 8)\nsn.boxplot(data=_______,y=\"count\",orient=\"v\",ax=axes)\n\naxes.set(ylabel='Count',title=\"Box Plot On Count\")","2acf994b":"fig, axes = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(8, 8)\nsn.boxplot(data=df,y=\"_______\",x=\"season\",orient=\"v\",ax=axes)\naxes.set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")","040f3d54":"fig, axes = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(8, 8)\nsn.boxplot(data=df,y=\"count\",x=\"_______\",orient=\"v\",ax=axes)\naxes.set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")","23df07e1":"fig, axes = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(8, 8)\nsn.boxplot(data=_______,y=\"_______\",x=\"_______\",orient=\"v\",ax=axes)\naxes.set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")","863063c2":"df_without_outliers = df[\n    np.abs(df[\"count\"] - df[\"count\"].mean()) <= (3 * df[\"count\"].std())\n] ","5355fd58":"df.shape[0] - df[np.abs(df[\"count\"] - df[\"count\"].mean()) <= (3 * df[\"count\"].std())].shape[0]","e8b35f3c":"print (\"Shape Of The Before Ouliers: \",df._______)\nprint (\"Shape Of The After Ouliers: \",df_without_outliers._______)","313afccf":"corrMatt = df.corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)","5443274f":"fig,ax = plt.subplots()\nfig.set_size_inches(12, 5)\nsn.regplot(x=\"registered\", y=\"count\", data=_______,ax=ax)","c7777d35":"sortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\nmonthAggregated = pd.DataFrame(df.groupby(\"_______\")[\"count\"].mean()).reset_index()","66752442":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\n\nsn.barplot(data=monthAggregated,x=\"month\",y=\"count\",ax=ax,order=sortOrder)\nax.set(xlabel='Month', ylabel='Avearage Count',title=\"Average Count By Month\")","c1433cb8":"hourAggregated = pd.DataFrame(df.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()","6957468e":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax)\nax.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')","2e9e4e4b":"hourAggregated = pd.DataFrame(df.groupby([\"hour\",\"weekday\"],sort=True)[\"count\"].mean()).reset_index()","c048fcbb":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\n\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weekday\"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax)\nax.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')","1f8181c0":"hourTransformed = pd.melt(df[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\nhourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\nhourAggregated.head()","ce6a853a":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\n\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax)\nax.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')","954e56dc":"dropFeatures = [\"_______\",\"date\",\"_______\"]\ndf = df.drop(dropFeatures, axis=1)","bfa69be4":"df = pd.get_dummies(df,columns=[\"_______\"])\ndf = pd.get_dummies(df,columns=[\"weekday\"])\ndf = pd.get_dummies(df,columns=[\"month\"])\ndf = pd.get_dummies(df,columns=[\"season\"])\ndf = pd.get_dummies(df,columns=[\"weather\"])\ndf.columns","6fb570e0":"df.head()","c27fe04d":"import sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Variables Descr\nX = df.drop('count', axis=1)\n\n# Variable \u00e0 pr\u00e9dire\nY = df['count']\n\nX_train, X_test, Y_train, Y_test = _______(X, Y, test_size = 0.2)","bcba5452":"print(X_train.shape, X_test._______)\nprint(Y_train.shape, Y_test._______)","ad864afc":"# M\u00e9trique \u00e0 cr\u00e9er\nfrom sklearn import metrics\n\ndef rmse(y, y_):\n    return np.sqrt(np.mean((y-y_)**2))\n\ndef mae(y, y_):\n    return np.mean(np.abs(y-y_))","6861eedf":"from sklearn.dummy import DummyRegressor\n\ndummy_regr = _______(strategy=\"mean\")\n\ndummy_regr.fit(X_train, Y_train)\n\ny_pred = dummy_regr.predict(X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","da3b56fe":"from sklearn.linear_model import LinearRegression\nlModel = _______()\nlModel.fit(X=X_train, y=Y_train)\n\ny_pred = _______.predict(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","02495a1b":"from sklearn.linear_model import Ridge\n\nlModel = _______()\nlModel._______(X=X_train, y=Y_train)\n\ny_pred = lModel._______(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","27cf442a":"from sklearn.linear_model import Lasso\nlModel = _______()\n_______._______(X=X_train, y=Y_train)\n\ny_pred = _______._______(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","cbd7f3ff":"from sklearn import tree\n\nlModel = tree._______()\nlModel.fit(X=X_train, y=Y_train)\n\ny_pred = lModel._______(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","1953a9e4":"from sklearn.ensemble import RandomForestRegressor\n\nlModel = _______()\nlModel._______(X=X_train, y=Y_train)\n\ny_pred = _______._______(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","4467680a":"from sklearn.ensemble import ExtraTreesRegressor\n\nlModel = _______()\nlModel._______(X=X_train, y=Y_train)\n\ny_pred = lModel._______(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","731e293f":"from sklearn.ensemble import GradientBoostingRegressor\n\nlModel = _______()\nlModel.fit(X=X_train, y=Y_train)\n\ny_pred = _______.predict(X= X_test)\n\nprint(\"R2: \", metrics.r2_score(Y_test, y_pred))\nprint(\"MAE: \", mae(Y_test, y_pred))","1bc38558":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nlm = LinearRegression()\nmae_scorer = metrics.make_scorer(mae, greater_is_better=False)\n\n# Train the model with 10-folds\n# Warning : without `scoring` parameter it returns a R2 score ! \nscores = cross_val_score(lm, scaler.fit_transform(X_train), Y_train, cv=10)#, scoring=mae_scorer) \nprint(\"Cross-validated scores:\", scores.mean())","335dacb3":"# RIDGE\nlm = ____()\nmae_scorer = metrics.make_scorer(mae, greater_is_better=False)\n\n# Train the model with 10-folds\n# Warning : without `scoring` parameter it returns a R2 score ! \nscores = cross_val_score(lm, scaler.fit_transform(X_train), Y_train, cv=10)#, scoring=mae_scorer) \nprint(\"Cross-validated scores:\", scores.mean())","84944379":"# idem pour LASSO","97c47477":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge_m_ = _____()\nridge_params_ = {'alpha':[25, 30,35, 40, 50, 60, 70, 100, 1000]}\n#rmsle_scorer = metrics.make_scorer(rmse, greater_is_better=False)\ngrid_ridge_m = _____(ridge_m_, \n                            ridge_params_, \n                            #scoring = rmsle_scorer,\n                            cv=5)\n\ngrid_ridge_m.fit(X_train, Y_train)\npreds = grid_ridge_m.predict(X=X_test)\n\nprint (\"Best parameters:\", _____.best_params_)\nprint (\"R2 Value For Ridge Regression: \", metrics.r2_score(Y_test, preds))\nprint (\"RMSE Value For Ridge Regression: \", rmse(Y_test, preds))\nprint (\"MAE Value For Ridge Regression: \", mae(Y_test, preds))","6c86f361":"pd.DataFrame(_____.cv_results_)","e17d354f":"# Plot scores\nfig,ax= plt.subplots()\nfig.set_size_inches(7,3)\ndf = pd.DataFrame(grid_lasso_m.cv_results_)[['params',________]]\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[________])\ndf[\"r2\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsn.pointplot(data=df,x=\"alpha\",y=\"r2\",ax=ax)","88925265":"from sklearn.linear_model import Lasso\n\nlasso_m_ = _____()\n\nalpha  = 1\/np.array([1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000])\nlasso_params_ = { 'alpha':alpha}\ngrid_lasso_m = GridSearchCV(lasso_m_,\n                            lasso_params_,\n                            #scoring = rmsle_scorer,\n                            cv=5)\n\ngrid_lasso_m.fit(X_train, Y_train)\npreds = grid_lasso_m.predict(X=X_test)\n\nprint (\"Best parameters:\", grid_lasso_m.best_params_)\nprint (\"R2 Value For Lasso Regression: \", metrics.r2_score(Y_test, preds))\nprint (\"RMSE Value For Lasso Regression: \", rmse(Y_test, preds))\nprint (\"MAE Value For Lasso Regression: \", mae(Y_test, preds))","23413ae5":"# Plot scores\nfig,ax= plt.subplots()\nfig.set_size_inches(7,3)\ndf = pd.DataFrame(grid_lasso_m.cv_results_)[['params','mean_test_score']]\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"_____\"])\ndf[\"r2\"] = df[\"mean_test_score\"].apply(lambda x:x)\nsn.pointplot(data=df,x=\"alpha\",y=\"r2\",ax=ax)","2b35b731":"from sklearn.ensemble import RandomForestRegressor\nrf_Model = _____()\n\nrf_params = { 'n_estimators':[150],'max_depth':[2, 3, 5, 7, 9, 12, 15, 17, 20]}\ngrid_rf_m = GridSearchCV(rf_Model, \n                         rf_params, \n                         #scoring=rmsle_scorer, \n                         cv=5)\n\ngrid_rf_m.fit(X_train, Y_train)\npreds = grid_rf_m.predict(X=X_test)\n\nprint (\"Best parameters:\", grid_rf_m._____)\nprint (\"R2 Value For Random Forest: \", metrics.r2_score(Y_test, preds))\nprint (\"RMSE Value For Random Forest: \", rmse(Y_test, preds))\nprint (\"MAE Value For Random Forest: \", mae(Y_test, preds))","47fc78dc":"# Plot scores\nfig,ax= plt.subplots()\nfig.set_size_inches(7,3)\ndf = pd.DataFrame(grid_rf_m.cv_results_)[['params','mean_test_score']]\ndf[\"max_depth\"] = df[\"params\"].apply(lambda x:x[\"____\"])\ndf[\"r2\"] = df[\"mean_test_score\"].apply(lambda x:x)\nsn.pointplot(data=df,x=\"max_depth\",y=\"r2\",ax=ax)","82aede11":"from sklearn.ensemble import GradientBoostingRegressor\ngbm = ______()\n\ngbm_params = { 'n_estimators':[500], 'learning_rate':[0.001 ,0.01, 0.1]}\ngrid_gbm = GridSearchCV(gbm, \n                        gbm_params, \n                        #scoring=rmsle_scorer, \n                        cv=5)\n\ngrid_gbm.fit(X_train, Y_train)\npreds = ______.predict(X=______)\n\nprint (\"Best parameters:\", ______.______)\nprint (\"R2 Value For Gradient Boosting: \", metrics.______(______, ______))\nprint (\"RMSE Value For Gradient Boosting: \", rmse(______, ______))\nprint (\"MAE Value For Gradient Boosting: \", mae(______, ______))","dcde7889":"# Plot scores\nfig,ax= plt.subplots()\nfig.set_size_inches(7,3)\ndf = pd.DataFrame(grid_ridge_m.cv_results_)[['params','mean_test_score']]\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[_________])\ndf[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsn.pointplot(data=df,x=_________,y=_________,ax=ax)","de7cdd24":"lm = ______(max_depth= ______, n_estimators= ______)\n\n# Warning : without `scoring` parameter it returns a R2 score ! \nscores = cross_val_score(lm, X_train, Y_train, cv=5)\nprint(\"Cross-validated scores:\", scores.mean())","af61883e":"lm.fit(______, ______)\ny_pred = lm.predict(X_test)\n\nprint(\"R2: \", metrics.r2_score(______, y_pred))\nprint(\"MAE: \", mae(Y_test, ______))","af6f9f7c":"from catboost import CatBoostRegressor\n\ncb = CatBoostRegressor(verbose = 0, cat_features = ['workingday','hour'])\n\ncb.fit(_____, Y_train)\ny_pred = cb.predict(_____)\n\n# Warning : without `scoring` parameter it returns a R2 score ! \nscores = _____(cb, X_train, Y_train, cv=5)\n\nprint(\"Cross-validated scores:\", _____)\n\nprint(\"R2: \", metrics.r2_score(_____, _____))\nprint(\"MAE: \", _____)","a1ac6df4":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(_____, Y_train)\nlgb_test = lgb.Dataset(X_test, _____, reference=lgb_train)\n\nparams = {\n    'num_leaves': 5,\n    'metric': ['l1', 'l2'],\n    'verbose': -1,\n    'n_estimators': _____\n}\n\nevals_result = {}  # to record eval results for plotting\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=100,\n                valid_sets=[lgb_train, lgb_test],\n                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n                categorical_feature=[21],\n                evals_result=evals_result,\n                verbose_eval=0)\n\ny_pred = _____\n\nprint(\"R2: \", _____)\nprint(\"MAE: \", _____)","b06e035e":"import xgboost as xgb\n\nX_train['hour'] = X_train['hour'].astype('int8')\nX_train['workingday'] = X_train['workingday'].astype('int8')\n\nX_test['hour'] = X_test['hour'].astype('int8')\nX_test['workingday'] = X_test['workingday'].astype('int8')\n\n#gbmdata_dmatrix = xgb.DMatrix(data=X_train, label=Y_train)\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.9, learning_rate = 0.1, n_estimators = _____)\n\nxg_reg._____\ny_pred = _____\n\nprint(\"R2: \", _____)\nprint(\"MAE: \", _____)","6ba85486":"Afficher l'\u00e9volution du score selon le param\u00e8tre max_depth.","46e05c13":"### Question 1.2\nEst on dans une probl\u00e9matique de r\u00e9gression ou de classification?","4408965e":"![](https:\/\/slideplayer.fr\/slide\/3308585\/11\/images\/14\/COEFFICIENT+DE+DETERMINATION+-+R%C2%B2.jpg)","f7e3967c":"### D\u00e9roul\u00e9 des questions : ","8620248b":"### Question 1.2 :\nLes variables cat\u00e9gorielles doivent \u00eatre trait\u00e9s pour \u00eatre utilisable par des mod\u00e8les lin\u00e9aires. Pour cela il faut cr\u00e9er une variable par cat\u00e9gorie. \n1. Utiliser data = pd.get_dummies(data,columns=[\"nom_col]) pour transformer les variables cat\u00e9gorielles que vous avez identif\u00e9s\n2. Afficher les noms des colonnes du DataFrame","687d3659":"### Question 3.4\nUtiliser - un mod\u00e8le de Gradient boosting. R\u00e9utiliser le code d\u00e9velopp\u00e9 ci dessus pour affich\u00e9 les m\u00e9triques associ\u00e9s \u00e0 nos nouveaux r\u00e9gresseurs.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html","d1c8c53c":"### Question 3.1\n1. Tester un premier mod\u00e8le simple : la r\u00e9gression lin\u00e9aire. R\u00e9aliser un graphe permettant de voir visuellement la qualit\u00e9 de ce mod\u00e8le. Par exemple scatter de matplotlib permettant de croiser les y_test et les y_pred.\n2. Afficher les m\u00e9triques associ\u00e9es \u00e0 ce nouveau r\u00e9gresseur\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html","9460cc9f":"**Partie 1 exploration et visualisation des donn\u00e9es**\n\nCette premi\u00e8re partie permet de v\u00e9rifier la qualit\u00e9 de la donn\u00e9e qui sera utiliser pa la suite dans la partie Mod\u00e9lisation. Elle permet aussi de mettre en place une partie important d'un pipeline de Machine Learning, qui est le feature Engineering. Certaines variables ne sont pas exploitable directement par un mod\u00e8le de ML. Il est donc n\u00e9c\u00e9ssaire de les retravailler afin d'en exploiter l'information pertinente. \n\nLa variable \u00e0 pr\u00e9dire et les variables explicatives doivent \u00eatre maitris\u00e9s et il est donc n\u00e9cessaire d'investiguer sur le typage, les valeurs manquantes ou encore leur distribution. Cette partie poss\u00e8de de nombreux graphiques utilis\u00e9s en Data Visusalisation afin d'identifier visuellement certains ph\u00e9nom\u00e8nes.\n\nLa partie redondance de donn\u00e9es ou encore doublons est aussi \u00e0 traiter dans cette premi\u00e8re partie.\n\n** Partie 2 mod\u00e9lisation**\n\nCette seconde partie vise \u00e0 mettre en place des mod\u00e8les de Machine Learning. Le process ici sera de tester un mod\u00e8le simple, puis des mod\u00e8les plus complexes afin d'en retenir un dans chaque cat\u00e9gorie. La 3eme partie visera \u00e0 chercher les hyper param\u00e8tres et finalement d'obtenir notre mod\u00e8le final.","73ee5d66":"1. Expliquer rapidement \u00e0 quoi va servir Pandas?","2443edd6":"## Partie 1 : Visualisation et feature engineering","39f60e86":"# V\u00e9rification des fichiers \u00e0 disposition","ebf265dc":"### Question 1.4\nPour chaque colonne v\u00e9rifier si il y a des valeurs manquantes. Par exemple avec .isnull() de pandas. Puis v\u00e9rifier si il y a des doublons. Vous pouvez utiliser .duplicated()","da4c1657":"3. Expliquer rapidement \u00e0 quoi va servir scipy?","9244257f":"![](https:\/\/image.slidesharecdn.com\/regresslineairesimpleimp-140113164038-phpapp02\/95\/regress-lineaire-simple-imp-42-638.jpg?cb=1389631325)","1cc26f84":"<p align=\"center\">\n  <img width=\"600\" height=\"200\" src=\"https:\/\/img.20mn.fr\/E_l1DZgVR-K26DkVcvjpYQ\/830x532_velib-paris-illustration.jpg\">\n<\/p>","d0684557":"Afficher l'\u00e9volution du score selon le param\u00e8tre test\u00e9 pour la regrssion ridge. (alpha)","b1d7017c":"### LASSO & RIDGE\n![](https:\/\/miro.medium.com\/max\/1142\/1*T-DWh1s4XG6I_bVH9bj1Dw.png)","b68fbacc":"### Question 4 : S\u00e9lection de mod\u00e8le\nChoisir 1 mod\u00e8le lin\u00e9aire, 1 mod\u00e8le de type arbre de d\u00e9cision et un mod\u00e8le de gradient boosting. Puis effectuer un GridSearchCV avec une cross-validation (avec K=5) pour chacun des mod\u00e8les choisi.\nAfficher les meilleurs param\u00e8tres pour chacun des mod\u00e8les ainsi que les r\u00e9sultats sur le sous ensemble train. \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html","9b850664":"2. Expliquer rapidement \u00e0 quoi va servir numpy?","11fe32bb":"** GridSearch mod\u00e8le Lasso **","67369b4e":"![](https:\/\/miro.medium.com\/max\/2272\/1*-8_kogvwmL1H6ooN1A1tsQ.png)","5f4bcdef":"#### **Projet**\n\nLes syst\u00e8mes de v\u00e9lo en libre-service sont un moyen de louer des v\u00e9los o\u00f9 le processus d'obtention de l'adh\u00e9sion, de location et de restitution des v\u00e9los est automatis\u00e9 gr\u00e2ce \u00e0 un r\u00e9seau de kiosques r\u00e9partis dans toute la ville. Gr\u00e2ce \u00e0 ces syst\u00e8mes, les gens peuvent louer un v\u00e9lo \u00e0 un endroit donn\u00e9 et le rendre \u00e0 un autre endroit en fonction de leurs besoins. Actuellement, il existe plus de 500 programmes de v\u00e9lo en libre-service dans le monde.\n\n#### **Donn\u00e9es du fichier**\n\n* **datetime** : Date et heure en timestamp\n* **season** : 4 = \u00e9t\u00e9, 3 = printemps, 2 = automns, 1 = hiver \n* **holiday** : Si consid\u00e9r\u00e9 comme vacance alors 1 sinon 0\n* **workingday** : Si le jour est ni un week end ni un jour de vacances\n* **weather** : \n    * 1: Clair, Peu de nuages, Partiellement nuageux, Partiellement nuageux\n    * 2: Brouillard + nuages, Brouillard + nuages fragment\u00e9s, Brouillard + quelques nuages, Brouillard\n    * 3: Neige l\u00e9g\u00e8re, pluie l\u00e9g\u00e8re + orage + nuages \u00e9pars, pluie l\u00e9g\u00e8re + nuages \u00e9pars\n    * 4: Forte pluie + palettes de glace + orage + brume, neige + brouillard \n* **temp** : temperature en degr\u00e9 Celsius\n* **atemp** : temperature ressentis en degr\u00e9 Celsius\n* **humidity** : humidit\u00e9 relative\n* **windspeed** : vitesse du vent\n* **casual** : nombre d'utilisateurs non abonn\u00e9s\n* **registered** : nombre d'utilisateurs  abonn\u00e9s\n* **count** : nombre total d'utilisateurs","e5d0ebac":"# Importation des packages n\u00e9cessaires","49afb905":"### Question 5.1\nLes valeurs aberrantes ou outliers, doivent \u00eatre trait\u00e9es ou du moins identifier. Il existe de nombreuses m\u00e9thodes pour le faire: Tests Statistiques, Machine Learning ou statistiques basiques. Ici nous commencerons par utiliser un calcul basique se basant sur la variance et la moyenne.\n\nDans un premier temps nous allons utiliser des boxplot afin de les identifier visuellement. Utiliser Seaborn pour afficher les boxplot de la variable \"count\", puis \"count\" par \"season\", \"count\" par \"weekday\" et \"count\" par \"hour\".\n\nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html","d7d7a71f":"Que se passe t'il pour la r\u00e9gression lin\u00e9aire standard?","03b488ec":"### Question 3.3\nUtiliser - Des mod\u00e8les non lin\u00e9aires tel que les arbres de d\u00e9cision (DecisionTree), RandomForest, ExtrTrees. R\u00e9utiliser le code d\u00e9velopp\u00e9 ci dessus pour affich\u00e9 les m\u00e9triques associ\u00e9s \u00e0 nos nouveaux r\u00e9gresseurs.\n\n- https:\/\/scikit-learn.org\/stable\/modules\/tree.html\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor","9ddea8e3":"# Statistiques","a26f3877":"Que dire de la variable mois? saison? et Weekday? Semblent-elles pertinentes pour la pr\u00e9diction de \"count\"?","dbdd89e3":"### Question 7\nNous avons un variable temporelle dans notre jeu de donn\u00e9es. Afin de v\u00e9rifier la coh\u00e9rence de la donn\u00e9e et de v\u00e9rifier certaines hypoth\u00e9s, afficher l'\u00e9volution du nombre de v\u00e9lo par jour par heures pour les 4 saisons, les jours de la semaine et pour la variable \"weekday\". pointplot() de seborn.\n\nAfficher un diagramme \u00e0 bar permmettant de mettre en avant le nombre de v\u00e9lo lou\u00e9 par mois. barplot() de seaborn. \n\n1. Aggr\u00e9ger la donn\u00e9\u00e9 count par dimension: Season, mois, weekday. Pour cela vous utiliserez cette syntaxe: featureAggregated = pd.DataFrame(df.groupby(\"feature\")[\"count\"].mean()).reset_index(). Ici la dimension est \"feature\"\n2. Faire la m\u00eame chose avec deux dimensions: Hour + season, Hour + mois, hour + weekday. ici feature = [\"col1\", \"col2\"]","bc5a3bc0":"# Question Bonus:\n- Utiliser XGB, LGBM & CatBoost","831f67c5":"### Question 2.2\nAvant de mettre en palce un mod\u00e8le, nous devons d\u00e9finir une m\u00e9trique qui va nous permettre de comparer les diff\u00e9rents mod\u00e8les entre eux afin de d\u00e9finir le plus pertinent mais aussi afin de juger de la qualit\u00e9 pr\u00e9dictive de notre mod\u00e8le final.\n\nPour cela imprter metrics de sklearn. Nous utiliserons le R2 (r2_score) qui est le coefficient de d\u00e9termination et calcul la corr\u00e9lation au carr\u00e9 entre la r\u00e9alit\u00e9 et la pr\u00e9diction puis une m\u00e9trique que nous allons cr\u00e9er. Il est toujours int\u00e9r\u00e9ssant d'avoir plusieurs m\u00e9triques pour comparer nos mod\u00e8les.\n\n1. importer metrics\n2. Remplir la fonction mae ","85e5da55":"4. Expliquer rapidement \u00e0 quoi va servir matplotlib?","c4c3cc67":"# Import et description","bc826ad2":"R\u00e9ponse question 5: >>>>>>>>>>>>>> ICI <<<<<<<<<<<<<","061d9411":"### Arbre de d\u00e9cision\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b7\/Arbre_de_decision.jpg)","b6649ce3":"** GridSearch for Ridge Regression **\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","65a42df1":"# Machine Learning with Python","c921b210":"### Question 2.1\nRandomiser les donn\u00e9es et s\u00e9parer le jeu de donn\u00e9es. Verifier la taille de vos 2 sous ensembles train \/ test. Pour cela vous allez utiliser la fonction train_test_split de scikit learn.\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html. Ajouter une graine avec random_state pour que l'\u00e9chantillonnage se fait de la m\u00eame mani\u00e8re \u00e0 chaque lancement du code.\n\nDans notre exemple nous prendre 80% d'individus pour le train et 20% pour le test.\n\n1. Importer sklearn\n2. importer train_test_split()\n2. Utiliser train_test_plit()\n\nPour rappel notre variable \u00e0 pr\u00e9dire est \"count\" et le reste est consid\u00e9r\u00e9 comme variables pr\u00e9dictives. Il est necessaire d'isoler la variable \u00e0 pr\u00e9dire dans une nouvelle variable Y et les variables pr\u00e9dictives dans une variable X avant d'utiliser le train_test_split().","3d9dc704":"5. Expliquer rapidement \u00e0 quoi va servir seaborn?","f2a7dcab":"Pour avoir un mod\u00e8le plus robuste il faudrait r\u00e9aliser plusieurs cross validation pour obtenir plusieurs mod\u00e8les puis terminer par un vote ou une moyenne des diff\u00e9rents mod\u00e8les obtenus.","3fff27bc":"Maintenant que nous avons d\u00e9fini nos 2 m\u00e9triques, nous allons commencer par utiliser un \"mod\u00e8le\" tr\u00e8s b\u00eate! Le mod\u00e8le appliquant la moyenne ou la m\u00e9diane \u00e0 chaque individu dont nous voulons pr\u00e9dire le nombre de v\u00e9lo qui sera lou\u00e9s.\n\nL'hsitorique - l'\u00e9chantillon d'apprentissage, poss\u00e8de \u00e0 la fois les variables explicatives et la variable \u00e0 expliquer. Ainsi l'apprentissage d'un DummyRegressor ce fait de la mani\u00e8re suivante: Calcul de la moyenne ou de la m\u00e9diane de y_train puis application de cette valeur \u00e0 l'ensemble des X_test. Ainsi pour calculer la performance de notre mod\u00e8le nous allons comparer y_test et y_pred, qui correspond \u00e0 la moyenne ou la m\u00e9diane de ce que l'on trouve dans l'\u00e9chantillon d'apprentissage.\n\n1. Appliquer Dummyregressor afin d'avoir une baseline - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.dummy.DummyRegressor.html. \n2. Calculer les m\u00e9triques associ\u00e9es \u00e0 ce regresseurs.","7f5c1561":"### Question pr\u00e9liminaire : ","678cd7b5":"### Gradient Boosting\n![](https:\/\/www.researchgate.net\/profile\/Maria_Peraita-Adrados\/publication\/326379229\/figure\/fig5\/AS:647978477948928@1531501516288\/A-simple-example-of-visualizing-gradient-boosting.png)","f3198152":"Afficher les r\u00e9sultats du GridsearchCV, Quelles sont les informations pertinentes ici pour choisir notre mod\u00e8le? \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","eba27cef":"### Question 1.1\n\nNous allons mettre un mod\u00e8le de Machine Learning permettant de pr\u00e9dire le nombre de v\u00e9los lou\u00e9s par jour. Supervis\u00e9 ou non supervis\u00e9?\n\n","62e3ef17":"<p align=\"center\">\n  <img width=\"600\" height=\"200\" src=\"https:\/\/miro.medium.com\/max\/18000\/1*2c21SkzJMf3frPXPAR_gZA.png\">\n<\/p>\n\n<p align=\"center\">\n  <img width=\"600\" height=\"200\" src=\"https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2013\/09\/how-to-find-outliers.jpg\">\n<\/p>\n","ae3f630c":"### Question 1.1\nVerifier que vous avez supprim\u00e9 les variable inutiles ou qui se d\u00e9duisent de la variable cible. Il y en a 3 ou 4 \u00e0 supprimer.","4b433ea5":"L'ensemble des mod\u00e8les utilis\u00e9s ont des param\u00e8tres. Ici nous les avons test\u00e9s avec les param\u00e8tres par d\u00e9faut. Cela nous a permit de voir quel type de mod\u00e8le perform\u00e9 le mieux. Nous allons maintenant s\u00e9lectionner celui qui est le plus prometteur et tester diff\u00e9rents bunch de param\u00e8tres afin de trouver les Hyper paraml\u00e8tres de notre mod\u00e8le. C'est \u00e0 dire les param\u00e8tres qui permettent \u00e0 notre mod\u00e8le d'\u00eatre encore meilleur.","18cc01fb":"## Exemple : Location de v\u00e9los","ea65a64f":"### Question 3.2\nUtiliser - D'autres mod\u00e8les lin\u00e9aires - Ridge, Lasso. R\u00e9utiliser le code d\u00e9velopp\u00e9 ci dessus pour affich\u00e9 les m\u00e9triques associ\u00e9s \u00e0 nos nouveaux r\u00e9gresseurs.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/linear_model.html","64e4dcde":"** GridSearch mod\u00e8le Random Forest **","16bddbf5":"R\u00e9ponse question 3: >>>>>>>>>>>>>> ICI <<<<<<<<<<<<<","b000a437":"Avec pd.DataFrame(grid_ridge_m.cv_results_) nous pouvons r\u00e9cup\u00e9rer un tas de m\u00e9triques selon les param\u00e8tres test\u00e9s. Il est int\u00e9ressant d'afficher l'impact d'un param\u00e8tre sur la m\u00e9trique afin d'identifier ceux qui m\u00e9rite d'\u00eatre creus\u00e9s plus que d'autres.\n\nAfficher l'\u00e9volution selon un param\u00e8tre s\u00e9lectionn\u00e9, faire cela pour les derniers mod\u00e8les choisi","ea5d8140":"### Question 2 :\nEtudier les types des donn\u00e9es, peux-t-on trouver un sch\u00e9ma de donner plus judicieux ? Y'a t'il des variables avec un typage qui n'est pas optimal?\nIdentifier ces variables afin de pouvoir les isoler par la suite et les retraiter. Cr\u00e9er un liste avec le nom de ces variables.","0ebdbc59":"R\u00e9ponse:","3c7949f4":"R\u00e9ponse question 1: >>>>>>>>>>>>>> ICI <<<<<<<<<<<<<","ca6673bc":"### Question 4 :\nTransformer les variables qui devraient \u00eatre cat\u00e9gorielles en variables cat\u00e9gorielles en utilisant pandas. Pour faire cela rapidement il est possible d'utiliser la fonction .astype(). Elle s'applique sur les colonnes du DataFrame.\n\nFaire une boucle \u00e0 partir de la liste des variables identifier comme n'ayant pas le bon typage et les assigner \u00e0u typage \"category\". N'oubliez pas de mettre \u00e0 jour la liste des variables cat\u00e9gorielles avec les nouvelles features cr\u00e9es.\n\nSupprimer la variable inutile, celle dont nous avons extrait les informations pertinentes","3608a5a7":"R\u00e9ponse:","66b2350f":"** Cross-Validation for Linear Regression, Ridge & Lasso **","79083b94":"Afficher l'\u00e9volution du score selon le param\u00e8tre test\u00e9 pour la regrssion Lasso. (alpha)","e72dea59":"### Question 5.2\n\nPour l'approche basique nous associerons les outliers de \"count\" \u00e0 des individus qui ne respecte pas la fonction suivante: np.abs(dailyData[\"count\"] - dailyData[\"count\"].mean()) <= (3 * dailyData[\"count\"].std())\n\n1. Cr\u00e9ation d'un nouveau DataFrame appel\u00e9 df_without_outliers par exemple partant du DataFrame original en filtrant sur les outliers tels qu'ils sont d\u00e9finis ci dessus.\n2. Combien d'outliers \u00e0-t-on filtr\u00e9 ?","f66e0b1e":"R\u00e9ponse question 4: >>>>>>>>>>>>>> ICI <<<<<<<<<<<<<","06d00c4b":"### Question 1.3\n1. Lire le dataset vlib.csv\n2. D\u00e9crire le dataset avec .info(), .describe() \n3. Afficher la taille d'un point de vue ligne et colonne de notre dataframe\n4. Afficher les 5 premi\u00e8res lignes du DataFrame. Puis afficher les 5 derni\u00e8res lignes du DataFrame\n5. Afficher le noms des colonnes du dataset","40007500":"### Question 3.1 :\nEtudier la variable datetime, quelles features peut-t-on cr\u00e9er\/extraire avec cette variable ? La variable datetime n'est pas exploitable tel quel. Cependant il est possible d'extraire des informations de celle ci. Comme le mois, le jour ou encore l'heure. Il est tout \u00e0 fait coh\u00e9rent de penser que ces variables joueront un r\u00f4le dans la pr\u00e9diction du nombre de v\u00e9los lou\u00e9s par jour.\n\nExtraire date, hour, weekday et month. Pour cela vous utiliserez la fonction .apply(lambda x: ...). L'id\u00e9e ici est de d\u00e9couper les informations dans le datetime et d'isoler les informations qui nous int\u00e9resse afin d'en cr\u00e9er des nouvelle variable.\n\n1. S\u00e9parer de datetime la partie date et hour. Stock\u00e9 respectivement dans la colonne \"date\" et \"hour\" du DataFrame. Proposer un algorithme permettant de le faire. Puis impl\u00e9menter le. exemple: \"2011-01-01 00:00:00\".split()\n2. De la nouvelle variable \"hour\" appliquer la fonction .apply() afin d'isoler l'heure. Comment isoler l'heure de notre variable \"hour\"?\n3. De la variable date, nous voulons extraire le mois, le jour et si oui on c'est un weekend. Pour cela nous allons utiliser le package calendar dont voici un exemple de syntaxe:\ndailyData.date.apply(lambda x : calendar.day_name[datetime.strptime(x,\"%Y-%m-%d\").function()]) - Trouver la fonction() permettant d'identifier le jour de la semaine puis celle pour le mois. Stocker ces nouvelles variables dans \"weekday\" & \"month\"\n\n\nExemples: \n- datetime.strptime(\"2011-01-01\",\"%Y-%m-%d\").weekday()\n- calendar.day_name[datetime.strptime(\"2011-01-01\",\"%Y-%m-%d\").weekday()]","b6c6c47c":"### Question 6\nL'\u00e9tude de la corr\u00e9lation fait partie aussi de la v\u00e9rification de la qualit\u00e9 de la donn\u00e9e. Cela peut permettre \u00e0 la fois d'identifier un redondance d'information ou encore des variables qui sont induites de notre variables cible, d\u00e9pendante de celle ci.\n\nEtudier les corr\u00e9lations entre les diff\u00e9rentes variables gr\u00e2ce \u00e0 la fonction, Quelles variables sont tr\u00e8s corr\u00e9l\u00e9es ? Visualiser ces corr\u00e9lations gr\u00e2ce \u00e0 la fonction heatmap de seaborn.\n\n1. Identfiier les variables trops corr\u00e9l\u00e9s\n2. Faut il supprimer des variables de notre jeux de donn\u00e9es avant de passer \u00e0 l'\u00e9tape de Machine Learning? Attention \u00e0 la fuite de donn\u00e9e - Data Leakage. Identifier ce qu'est le Data Learkage.\n3. Afficher le graphique regplot() de seaborn avec les variables \"count\" et \"registered\" ","d3522ff2":"### Question 3.2 :\nRemplacer les cat\u00e9gories des variables \"season\" and \"weather\" par des cat\u00e9gories plus explicite. Voir la partie Donn\u00e9es du fichier en d\u00e9but de notebook. Pour cela utiliser la fonction .map(dict) avec le dictionnaire ad\u00e9quat.\n\n1. Cr\u00e9er le dictionnaire\n2. Appliquer le remplacement avec .map()\n\nExemple:\n- dictionnaire_saisons = {2: \"Spring\", 3 : \"Summer\", 4 : \"Fall\", 1 :\"Winter\" }","2dc5a0dd":"### Random Forest\n![](https:\/\/i.stack.imgur.com\/iY55n.jpg)","0b68962f":"![](https:\/\/miro.medium.com\/max\/602\/1*QVzTd8Top6ImHR-3U3QdqQ.png)","e7a4f729":"## Partie 2 : Mod\u00e9lisation\n\nMaintenant que nous avons trait\u00e9 notre fichier et que nous avons notre SmartData, nous allons pouvoir commencer par mettre en place notre pipeline de Machine Learning.","34643a4c":"** GridSearch mod\u00e8le Gradient Boosting **","8fded6ab":"![](https:\/\/camo.githubusercontent.com\/64139b0aad34b1a5e2e7d853fd4a5a32fca96ad4\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a726762613142494f5579733777516358634c345535412e706e67)\n","77d90eb4":"R\u00e9ponse question 2: >>>>>>>>>>>>>> ICI <<<<<<<<<<<<<","c38a4aba":"### Question 1.5\nV\u00e9rifier si il y a des valeurs manquantes par colonnes\/ Faire une boucle et utilise .isnull().","ad79304b":"### Question 5 : Conclusion\nIdentifier le meilleur mod\u00e8le et r\u00e9-entrainer le mod\u00e8le avec ses param\u00e8tres optimaux. \nAfficher les r\u00e9sultats sur le sous ensemble de test. Utiliser cross_val_score, puis reprendre le code ou le train_test_split a \u00e9t\u00e9 r\u00e9alis\u00e9. Appliquer votre mod\u00e8le sur X_train, y_train et test\u00e9 sur X_test.","e447ef84":"R\u00e9ponse:"}}