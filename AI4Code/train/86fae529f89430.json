{"cell_type":{"d14ef18a":"code","bb0b8db3":"code","23592cd7":"code","b74d35ea":"code","89ba4728":"code","f8c63667":"code","d5f33113":"code","2c1f3beb":"code","7de63b02":"code","a991f568":"code","64d24a5c":"code","c25bba86":"code","89961cb4":"code","71e423c3":"code","436376c8":"code","c6afc7ec":"code","1702e553":"code","a8e63fb0":"code","89313244":"code","6e9afb0a":"code","a0ab629e":"code","4a5b7979":"code","8a858ddb":"code","9c1a8ee7":"code","6307f1b1":"code","9f68e875":"code","2164a22c":"code","da28e13d":"markdown","2b43d9eb":"markdown","16f60a3d":"markdown","d8c0165d":"markdown","d1544cba":"markdown","b27ddbb3":"markdown","96cabf1d":"markdown","f8aa8031":"markdown","55f61430":"markdown","05890dba":"markdown","4486d403":"markdown","9bc42913":"markdown","f4587593":"markdown","f240a580":"markdown","d7f37f5d":"markdown","fb376b72":"markdown","23e89f16":"markdown","7c26f69a":"markdown"},"source":{"d14ef18a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","bb0b8db3":"# Next, we'll load the Iris flower dataset, which is in the \"..\/input\/\" directory\niris = pd.read_csv(\"..\/input\/Iris.csv\") # the iris dataset is now a Pandas DataFrame\n\n# Let's see what's in the iris data - Jupyter notebooks print the result of the last thing you do\niris.head()","23592cd7":"iris.describe()","b74d35ea":"# We'll use this to make a scatterplot of the Iris features.\niris.plot(kind=\"scatter\", x=\"SepalLengthCm\", y=\"SepalWidthCm\")","89ba4728":"# A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure\nsns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=iris, size=5)","f8c63667":"# We'll use seaborn's FacetGrid to color the scatterplot by species\nsns.FacetGrid(iris, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n   .add_legend()","d5f33113":"# We can look at an individual feature in Seaborn through a boxplot\nsns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)","2c1f3beb":"# One way we can extend this plot is adding a layer of individual points on top of\n# it through Seaborn's striplot\n\n# We'll use jitter=True so that all the points don't fall in single vertical lines\n# above the species\n# Saving the resulting axes as ax each time causes the resulting plot to be shown\n# on top of the previous axes\nax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)\nax = sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, jitter=True, edgecolor=\"gray\")","7de63b02":"# A violin plot combines the benefits of the previous two plots and simplifies them\n# Denser regions of the data are fatter, and sparser thiner in a violin plot\nsns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, size=6)","a991f568":"# A final seaborn plot useful for looking at univariate relations is the kdeplot,\n# which creates and visualizes a kernel density estimate of the underlying feature\nsns.FacetGrid(iris, hue=\"Species\", size=6) \\\n   .map(sns.kdeplot, \"PetalLengthCm\") \\\n   .add_legend()","64d24a5c":"# From the pairplot, we'll see that the Iris-setosa species is separataed from the other\n# two across all feature combinations\nsns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3)","c25bba86":"# Box plot grid\niris.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(12, 6))","89961cb4":"# Andrews Curves involve using attributes of samples as coefficients for Fourier series\n# and then plotting these\nfrom pandas.plotting import andrews_curves\nandrews_curves(iris.drop(\"Id\", axis=1), \"Species\")","71e423c3":"# Another multivariate visualization technique pandas has is parallel_coordinates\n# Parallel coordinates plots each feature on a separate column & then draws lines\n# connecting the features for each data sample\nfrom pandas.plotting import parallel_coordinates\nparallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")","436376c8":"\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.plotting import radviz\nradviz(iris.drop(\"Id\", axis=1), \"Species\")","c6afc7ec":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split","1702e553":"# Seperating the data into dependent and independent variables\nX = iris.iloc[:, :-1].values\ny = iris.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","a8e63fb0":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","89313244":"# K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier(n_neighbors=8)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","6e9afb0a":"# Support Vector Machine's \nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","a0ab629e":"# Decision Tree's\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier()\n\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","4a5b7979":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","8a858ddb":"# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","9c1a8ee7":"# Bernoulli Naive Bayes\nfrom sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","6307f1b1":"# Complement Naive Bayes\nfrom sklearn.naive_bayes import ComplementNB\nclassifier = ComplementNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","9f68e875":"from sklearn.metrics import accuracy_score, log_loss\nclassifiers = [\n    GaussianNB(),\n    MultinomialNB(),\n    BernoulliNB(),\n    ComplementNB(),               \n                  ]\n \n# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n \nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    log_entry = pd.DataFrame([[name, acc*100, 11]], columns=log_cols)\n    log = log.append(log_entry)\n    \n    print(\"=\"*30)\n","2164a22c":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()","da28e13d":"Thirdly , with **SVM (Support Vector Machines)**.","2b43d9eb":"**Training the model**\n\nUsing some of the commonly used algorithms, we will be training our model to check how accurate every algorithm is. We will be implementing these algorithms to compare:\n\n1] Logistic Regression\n\n2] K \u2013 Nearest Neighbour (KNN)\n\n3] Support Vector Machine (SVM)\n\n4] Decision Trees\n\n5] Naive Bayes classifier\n\nLet us start building our model and predicting accuracy of every algorithm used. We can also check which gives the best result.","16f60a3d":" A final multivariate visualization technique ","d8c0165d":"Secondly , let us see at the box plot of the dataset, which shows us the visual representation of how our data is scattered over the the plane. Box plot is a percentile-based graph, which divides the data into four quartiles of 25% each. This method is used in statistical analysis to understand various measures such as mean, median and deviation.","d1544cba":"Next , is my favorite , **decision trees** !","b27ddbb3":"And now , let's see some special visuals !! One cool more sophisticated technique pandas has available is called Andrews Curves","96cabf1d":"**This is a special plot called violin plot**","f8aa8031":"Next is a visual based on probability density , called kernel density plots. (KD Plots)","55f61430":"**PROBLEM STATEMENT**\n\nThis data set consists of the physical parameters of three species of flower \u2014 Versicolor, Setosa and Virginica. The numeric parameters which the dataset contains are Sepal width, Sepal length, Petal width and Petal length. In this data we will be predicting the classes of the flowers based on these parameters.The data consists of continuous numeric values which describe the dimensions of the respective features. We will be training the model based on these features.","05890dba":"**Understanding the Data**\n\nLet us try to understand the basic desciption of the data , in terms of basic mathematics.","4486d403":"Now , let us see the scores with **K-Nearest Neighbors** technique.","9bc42913":"# **INTO THE REALM OF MACHINE LEARNING**\n\n**Dividing the data for training and testing**\n\nOnce we have understood what the dataset is about, we can start training a model based on the algorithms. Here, we will be implementing some of the commonly used algorithms in machine learning. Let us start by training our model with some of the samples. We will be using an inbuilt library called \u2018train_test_split\u2019 which divides our data set into a ratio of 80:20. ","f4587593":"Note: This work is highly inspired from few other kaggle kernels , github sources and other data science resources. Any traces of replications, which may appear , is purely co-incidental. Due respect & credit to all  my fellow kagglers. Thanks !!","f240a580":"And lastly , the **Naive Bayes classifier**. (Variants included)","d7f37f5d":"**Analysing the data visually**\n\nAt the outset , let us look at a simple scatter plot , to get a visual feel of the data. (We are going to view a host of them)\n","fb376b72":"We can start with the first algorithm **Logistic Regression**. We can build our model like below:","23e89f16":"# **INTRODUCTION**\n\nEvery machine learning project begins by understanding what the data and drawing the objectives. While applying machine learning algorithms to your data set, you are understanding, building and analyzing the data as to get the end result.\n\nFollowing are the steps involved in creating a well-defined ML project:\n\n1] Understand and define the problem\n\n2] Prepare the data\n\n3] Explore and Analyse the data\n\n4] Apply the algorithms\n\n5] Reduce the errors\n\n6] Predict the result\n\nTo understand various machine learning algorithms let us use the Iris data set, one of the most famous datasets available.","7c26f69a":"**Another useful seaborn plot is a hybrid plot called pairplot, which shows the bivariate relation between each pair of features. Lets see the same**"}}