{"cell_type":{"f280a2ee":"code","324ed7aa":"code","d9ab814e":"code","e193d7f6":"code","c0e7b781":"code","113cacf2":"code","5398c4d5":"code","77c7f630":"code","75ba4c78":"code","18ffe6b0":"code","4d3f5501":"code","12402630":"code","970ebf05":"code","9d5bd9de":"code","29b4b688":"code","af3019cd":"code","9035859e":"code","0e5cd45a":"code","ad660a53":"code","16ce874f":"code","0c17f5dd":"code","bb321799":"code","f4b71c0b":"code","b1ceb28a":"code","ed664164":"code","6354c578":"code","2a1ab925":"code","ec651691":"code","7d8cdd78":"code","d8627549":"code","d06f16c1":"code","54f59ba8":"code","912c082a":"code","899c10bd":"code","ca9931da":"code","8e5ba684":"code","b9fddd40":"code","227c160d":"code","ad279dfc":"code","c9defca0":"code","e0eb1832":"code","0e5b68b6":"markdown","ba0bac9a":"markdown","4bef3988":"markdown","644ec75c":"markdown","9bc356a0":"markdown","75c7c553":"markdown","128b5623":"markdown","a2fec4f2":"markdown","a6304609":"markdown","256d9dd9":"markdown","91193d90":"markdown","94761732":"markdown","296f3cac":"markdown","b0f25f66":"markdown","781317f5":"markdown","b0af52f6":"markdown","88d185df":"markdown"},"source":{"f280a2ee":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt","324ed7aa":"seed = 77\n\nnp.random.seed(seed)","d9ab814e":"data_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","e193d7f6":"data_train = data_train.drop('Id', axis = 1)\ndata_train = data_train.drop(data_train[(data_train['GrLivArea']>4000) & (data_train['SalePrice']<300000)].index)\ndata_train.head()","c0e7b781":"data_test_id =data_test['Id']\ndata_test = data_test.drop('Id', axis = 1) \ndata_test.head()","113cacf2":"# Trabalhando com os dados nulos.\n# Quero que apare\u00e7a somente as variaveis que tem valores nulos.\n\npd.DataFrame(data_train.isnull().sum()).rename(columns={0: 'NaN'}).query('NaN > 0').T\n\n# Apenas as vari\u00e1veis com valores nulos:","5398c4d5":"# Modificando os valores nulos de teste e de treino...\n\ndata_train = data_train.fillna(value='None')\n\ndata_test = data_test.fillna(value='None')\n\n\n# Checando valores nulos\n\nprint(data_train.isnull().sum())\nprint(' ')\nprint(' ')\nprint(data_test.isnull().sum())","77c7f630":"data_train_NaN = data_train.drop(list(data_train.select_dtypes(include=('int64', 'float64')).columns), axis =1)\ndata_train_NaN = data_train_NaN.drop(['LotFrontage','MasVnrArea','GarageYrBlt'], axis = 1)  # Essas vari\u00e1veis eram do tipo num\u00e9rica por\u00e9m tinham \"None\" no meio... Tem que fazer o tratamento separado dessas vari\u00e1veis.\n\nNaN = list(data_train_NaN.select_dtypes(include='object').columns)\n\nfor i in NaN:\n    print(i, \":\", data_train[i].unique())","75ba4c78":"encoder = LabelEncoder()\n\nfor i in NaN:\n    data_train[i] = encoder.fit_transform(data_train_NaN[i])\n\nprint(data_train.dtypes)\nprint(' ')\ndata_train.head()","18ffe6b0":"data_train[['LotFrontage','MasVnrArea','GarageYrBlt']].value_counts()","4d3f5501":"# Usando o \"replace\". \n# Transformando em 0. Pois tem o mesmo valor nesses sentido. Significa que n\u00e3o existe ou que n\u00e3o foi informado.\n\ndata_train = data_train.replace('None', 0)\ndata_train[['LotFrontage','MasVnrArea','GarageYrBlt']].value_counts()","12402630":"# Transformando as vari\u00e1veis ['LotFrontage','MasVnrArea','GarageYrBlt'] em int64\n\n\nfor i in ['LotFrontage','MasVnrArea','GarageYrBlt']:\n    \n    data_train[i] = np.array(data_train[i]).astype(int)\n    \ndata_train[['LotFrontage','MasVnrArea','GarageYrBlt']].dtypes","970ebf05":"for i in data_train.keys():\n    print(i, \":\", data_train[i].dtypes)\n    \n#Dataframe pronto !","9d5bd9de":"data_train.head()","29b4b688":"# Transformando o dataset data_test:\n\ndata_test_NaN = data_test.drop(list(data_test.select_dtypes(include=('int64', 'float64')).columns), axis =1)\ndata_test_NaN = data_test_NaN.drop(['LotFrontage','MasVnrArea','GarageYrBlt','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageArea','BsmtFullBath','BsmtHalfBath','GarageCars'], axis = 1)\n\n\nNaN = list(data_test_NaN.select_dtypes(include='object').columns)\n\n#for i in ['LotFrontage','MasVnrArea','GarageYrBlt','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageArea']:\n    \n    #data_test[i] = np.array(data_test[i]).astype(int)\n\nfor i in NaN:\n    print(i, \":\", data_test[i].unique())","af3019cd":"# Fazendo o Encode:\n\nfor i in NaN:\n    data_test[i] = encoder.fit_transform(data_test_NaN[i])\n\nprint(data_test.dtypes)\nprint(' ')\ndata_test.head()","9035859e":"# modificando alguns valores que precisa\n\ndata_test = data_test.replace('None', 0)\n\n# Ainda est\u00e3o como float64 ... Al\u00e9m de ter outros tipos de dados que devem ser modificados.","0e5cd45a":"for i in data_test.keys():\n    print(i, \":\", data_test[i].dtypes)","ad660a53":"# Modificiando para o tipo int64\n\nto_transform = ['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageYrBlt','GarageCars','GarageArea','BsmtFullBath','BsmtHalfBath']\n\nfor i in to_transform:\n    \n    data_test[i] = np.array(data_test[i]).astype(int)\n\ndata_test[['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageYrBlt','GarageCars','GarageArea','BsmtFullBath','BsmtHalfBath']].dtypes","16ce874f":"for i in data_test.keys():\n    print(i, \":\", data_test[i].dtypes)","0c17f5dd":"# Data_test pronto para usar.\n\ndata_test.head()","bb321799":"data_train['TotalSF'] = data_train['TotalBsmtSF'] + data_train['1stFlrSF'] + data_train['2ndFlrSF']\ndata_test['TotalSF'] = data_test['TotalBsmtSF'] + data_test['1stFlrSF'] + data_test['2ndFlrSF']","f4b71c0b":"x = data_train.drop('SalePrice', axis =1)\ny = data_train['SalePrice']","b1ceb28a":"# Selecionando as melhores features\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,random_state=77)","ed664164":"# DummyClassifier\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_stratified = DummyClassifier().fit(x_train, y_train)\nacuracia = dummy_stratified.score(x_test, y_test)\n\nprint(f'A acuracia do algorimto base foi de: {acuracia}')","6354c578":"# KBests\n\nkb = SelectKBest(chi2, k=15)\nkb.fit(x_train, y_train)\nx_train_kb = kb.transform(x_train)\nx_test_kb = kb.transform(x_test)","2a1ab925":"from sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier().fit(x_train, y_train)\nfeatures = pd.DataFrame()\nfeatures['feature']=x_train.columns\nfeatures['importancia'] = clf.feature_importances_\n\nbest_features = features[features['importancia']>np.mean(features['importancia'])].sort_values(by='importancia', ascending = False)[:16].set_index('feature')\n\n# Features that are more importante for ours model...\nbest_features\n\n\n# every time have modify on this dataset.... so i don't no whats its (why)\n# but are the same features....","ec651691":"# Let see the data in graphics\n\nfor i in best_features.index:\n    plt.figure(figsize=(10,6))\n    sns.regplot(data = data_train,x=x[i],y = y)","7d8cdd78":"# with thats imgs we can see the data behavior... It's not very \"regular\"","d8627549":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\ndef models(x_train,x_test, y_train, y_test):\n    \n    list_score=[]\n    \n    # Random Forest\n    rf_model = RandomForestRegressor(n_estimators=300, random_state=77).fit(x_train,y_train)\n    rf_score = rf_model.score(x_test, y_test)\n    list_score.append({\"Random Forest\":rf_score})\n    one = pd.DataFrame({'Predict':rf_model.predict(x_test),'Real':y_test})\n    plt.figure(figsize=(16,8))\n    plt.title(f'O resultado da Random Forest \u00e9: {rf_score}', pad=15)\n    sns.scatterplot(data=one)\n    \n    # DecisionTreeRegressor\n    tree = DecisionTreeRegressor(random_state= 77).fit(x_train,y_train)\n    tree_score = tree.score(x_test, y_test)\n    list_score.append({'DecicionTree':tree_score})\n    two = pd.DataFrame({'Predict':tree.predict(x_test),'Real':y_test})\n    plt.figure(figsize=(16,8))\n    plt.title(f'O resultado da Decision tree \u00e9: {tree_score}', pad=15)\n    sns.scatterplot(data=two)\n        \n    # Linear Regression\n    lm_model = LinearRegression().fit(x_train,y_train)\n    lm_score = lm_model.score(x_test, y_test)\n    list_score.append({'Linear Regression':lm_score})\n    five = pd.DataFrame({'Predict':lm_model.predict(x_test),'Real':y_test})\n    plt.figure(figsize=(16,8))\n    plt.title(f'O resultado do LinearRegression \u00e9: {lm_score}', pad=15)\n    sns.scatterplot(data=five)","d06f16c1":"models(x_train, x_test, y_train, y_test)","54f59ba8":"rf_model = RandomForestRegressor(n_estimators=300, random_state=77)","912c082a":"import xgboost as xgb\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","899c10bd":"import lightgbm as lgb\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","ca9931da":"from sklearn.model_selection import KFold, cross_val_score\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nscore = rmsle_cv(rf_model)\nscore1 = rmsle_cv(model_xgb)\nscore2 = rmsle_cv(model_lgb)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) # mean & the standard deviation\nprint(\"\\nxgb score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nprint(\"\\nlgb score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8e5ba684":"rf_model.fit(x_train, y_train)\nrf_predict = rf_model.predict(data_test)\n\nmodel_xgb.fit(x_train, y_train)\nxgb_pred = model_xgb.predict(data_test)\n\nmodel_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(data_test)","b9fddd40":"ensemble = rf_predict*0.70 + xgb_pred*0.15 + lgb_pred*0.15","227c160d":"rf_model.fit(x_train, y_train)\n\nfinal_results = rf_model.predict(data_test)","ad279dfc":"results = pd.DataFrame({'Id':data_test_id,'SalePrice':final_results}).set_index('Id')\nresults.head()","c9defca0":"results.to_csv('results_rf.csv')","e0eb1832":"sub = pd.DataFrame()\nsub['Id'] = data_test_id\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","0e5b68b6":"## Data test results","ba0bac9a":"## Loading the database","4bef3988":"## Selecionando as melhores features","644ec75c":"## Applying models","9bc356a0":"## Looking the data base","75c7c553":"## Cross-validation","128b5623":"## Selecting the best features","a2fec4f2":"### Modifying Data Train","a6304609":"## Libs used","256d9dd9":"## ExtraTreesClassifier","91193d90":"## Transform data base (NaN and Null)","94761732":"### Modifying Data Test","296f3cac":"## Algoritmo Base","b0f25f66":"## Making ensenble","781317f5":"## Transforming and Enconde","b0af52f6":"## Separing the x and y from datatrain","88d185df":"## Creating a new feature"}}