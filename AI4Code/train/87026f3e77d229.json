{"cell_type":{"e866401a":"code","b0fd7e03":"code","83f0618c":"code","a517b6a8":"code","029ed358":"code","43843eb1":"code","4c5d3acf":"code","cdf841f9":"code","25d18740":"code","e92c48d1":"code","b0670ae0":"code","d6743108":"code","396f0aba":"code","29b03d93":"code","3753dd9b":"code","e8d54b5d":"code","494bba63":"code","f30c563f":"code","1012af94":"code","3e0e05a9":"code","c4c13dd6":"code","36b751e9":"code","1a05e3e5":"markdown","d4012405":"markdown","3700f097":"markdown","28aebc89":"markdown","95e8c204":"markdown","e6273ccf":"markdown","ed73189f":"markdown","cef762a9":"markdown","c000cae5":"markdown","60091e73":"markdown","de7c5ea6":"markdown","8d2419f9":"markdown","e2e62715":"markdown"},"source":{"e866401a":"#Import packages used for dataset\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n","b0fd7e03":"#Load the dataset\nhepatitis_data_set = pd.read_csv('..\/input\/hepatitis-c-dataset\/HepatitisCdata.csv')","83f0618c":"#find out about data set\nprint(hepatitis_data_set.dtypes, hepatitis_data_set.info(), hepatitis_data_set.describe())","a517b6a8":"# We are going to drop the rows with na values \nhepatitis_data_set.dropna(inplace=True)\n# What do the rows look like\nprint(len(hepatitis_data_set.index), '\\n',hepatitis_data_set.head())","029ed358":"#What kind of values are in the Category column\nlist(set(hepatitis_data_set['Category']))","43843eb1":"# Though there are many categories, we are breaking it into binary: healthy and unhealthy.\nhepatitis_data_set['Category'].loc[hepatitis_data_set['Category'].isin([\"1=Hepatitis\",\"2=Fibrosis\", \"3=Cirrhosis\"])] = 1\nhepatitis_data_set['Category'].loc[hepatitis_data_set['Category'].isin([\"0=Blood Donor\", \"0s=suspect Blood Donor\"])] = 0\n","4c5d3acf":"column_titles = hepatitis_data_set.columns\nhepatitis_data_set.columns[4:]\nplotting_columns = hepatitis_data_set.columns[4:]","cdf841f9":"# Let's Create some visualizations for the data: We will create a plot for each variable as it relates to age\nfig,ax= plt.subplots(nrows=len(plotting_columns),ncols=1,sharex=True, figsize=(10,30))\nfig_data = hepatitis_data_set.copy()\nX = hepatitis_data_set['Age']\nfor i in range(len(plotting_columns)):\n    y = hepatitis_data_set[plotting_columns[i]]\n    scatter = ax[i].scatter(x=X,y=y,c=hepatitis_data_set['Category'], cmap='winter')\n    ax[i].set(title=f'{plotting_columns[i]} by Age and Liver Function', xlabel='Age', ylabel=plotting_columns[i])\n    ax[i].legend(*scatter.legend_elements(), title='Hepatitis')\nplt.subplots_adjust(hspace=1)\nfig.suptitle('Hepatitis C Analysis', size=20)\nplt.savefig('Hepatitis C Analysis.png',bbox_inches='tight')\n;","25d18740":"# To highligth the point lets consider again how many Hepatitis patients are in our data set.\nhepatitis_data_set['Category'].loc[hepatitis_data_set['Category']==1].sum()","e92c48d1":"# With only 56 data points with this classification, we lack enough data for normalization.","b0670ae0":"# We also need to use numerical data for the sex column\nhepatitis_data_set['Sex'].loc[hepatitis_data_set['Sex']=='m']=1\nhepatitis_data_set['Sex'].loc[hepatitis_data_set['Sex']=='f']=0\n#Check if the formatting is correct\nhepatitis_data_set.head(),hepatitis_data_set.dtypes","d6743108":"# Our category and sex columns are still object types. They need to be an integer data type\nhepatitis_data_set['Sex']=hepatitis_data_set['Sex'].astype('int')\nhepatitis_data_set['Category']=hepatitis_data_set['Category'].astype('int')","396f0aba":"#Now we will split the data\nfrom sklearn.model_selection import train_test_split\n#Since the split is randomized, let's make it reproducible with numpy's random seed\nnp.random.seed(32)\n# The X variable will be all fields except for liver condition.\nX= hepatitis_data_set.drop('Category', axis=1)\n# The Y variable will be a single liver condition column\ny= hepatitis_data_set['Category']\nx_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.25)\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\ny_preds = knn.predict(x_test)\naccuracy_score(y_test,y_preds)","29b03d93":"from sklearn.model_selection import cross_val_score\n#We first need to shuffle the data, so that there are consistent cross validation sets.\nnp.random.seed(31)\nshuffled_data = hepatitis_data_set.sample(frac=1)\n\n#We can assign new cross validation variables\nX2 = shuffled_data.drop('Category', axis=1)\ny2 = shuffled_data['Category']\n\nvalidation_scores = cross_val_score(knn, X2,y2,cv=5)\nprint(f'average score of cross validation sets: {np.mean(validation_scores)}')","3753dd9b":"probability_predictions = knn.predict_proba(x_test)\nprobability_predictions\n# This outputs the probability of the input being classifier 0  or 1","e8d54b5d":"# We can find out which predicion it is more certain of by running a simple average\nnegative_classifier_probability = np.mean([i for i in probability_predictions[:,0] if i > 0.5])\npositive_classifier_probability = np.mean([i for i in probability_predictions[:,1] if i > 0.5])\nnegative_classifier_probability, positive_classifier_probability","494bba63":"# The cross validate function will show various metrics tested over every subset of the data set.\nfrom sklearn.model_selection import cross_validate\ncross_validation_scores = cross_validate(knn, X2, y2, cv=5,\n                                        scoring=['accuracy','precision','recall','recall_macro'])\ncross_validation_scores\n","f30c563f":"np.std([0.98305085, 1.        , 0.99152542, 1.        , 1.        ]), np.mean(cross_validation_scores['test_accuracy'])","1012af94":"grid = {'n_neighbors': [3,5,8,12],\n       'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n       'leaf_size':[30,50,100]\n       }\ngs_model = GridSearchCV(knn, param_grid=grid, cv=5, verbose=2)\ngs_model.fit(x_train, y_train)","3e0e05a9":"gs_model.best_params_","c4c13dd6":"from joblib import dump\ndump(knn, 'hepatitis-C.joblib')","36b751e9":"submission= pd.DataFrame(data=y_preds, index=list(y_test.index), columns=['predictions'])\nsubmission.to_csv('hepc-ml-submission.csv', index=False)","1a05e3e5":"## Visualizing the Data","d4012405":" This grid search shows that the out of the box knn hyperparameters are optimal.","3700f097":"## Creating a Predictions Model ","28aebc89":"## Getting the Data Ready","95e8c204":"## Plotting Conclusions\n\n* The data can't be used for prediction for those under 33 years old.\n    All individuals have Hepatitis C and the model used will likely put too much weight on this coincidence due to such strong correlation. As we will be using the k neighbor algorithm, the model would carve out that part of the graph as highly probable hepatitis territory. See an an example here https:\/\/ml-playground.com\/#.\n* Correlation is slight for any single factor: there is a high degree of overlap, although the two population do have visible trends: Higher AST levels and lower ALT for individuals with Hepatitis. This limits our certainty in the predictions the model provides. Even if it's predictions are successful withing this sample, we likely don't have a robust enough sample size for the trends to manifest clearly.","e6273ccf":"## Cleaning and Treatment of Data","ed73189f":"To reiterate the conclusions above, this model does not have enough data points describing the liver function of young people. This makes it a weak analytical tool for analyzing the possibility of hepatitis C in people under 35. \n\nThe model, at under 600 data points and only 56 with hepatitis or worse, could certainly be more robust. It is possible that a sample size as small as this is not representative of the population and extrapolation to the success of the model in the future should be tempered by this uncertainty.\n\nAnd yet, our confidence interval is extremely slim using this testing method, and all signs point to the success of the model. We are ready to dump our model with joblib for future use.","cef762a9":"This Model seems to work incredibly well. It has a high amount of accuracy and consistency despite only a middling amount of data.\n\nThis isn't the most accurate way to test the model. Let's create some cross validation.","c000cae5":"## Testing and Validation","60091e73":"While it is good that the model showed high certainty for each label, the lower certainty for the second label, label 1, is cause for concern. If the model is less certain about the incidence of hepatitis C, we might assume it is more likely to deliver a false negative than a false positive, which would dangerous for patients. If this were the case, more individuals would be told that they do not have hepatitis C when they do. It would then go untreated and cause untold problems in the future. \n\nHowever this hypothesis may not be correct; while the model's certainty might reflect it's accuracy anectdotally, the model could also get proportionately more false positives due misguided confidence in the algorithm.\n\nIn order to check this hypothesis and make sure that the accuracy is a representative statistic of the model's success,  other metrics of the data need to be verified in a more comprehensive fashion.","de7c5ea6":"# Hepatitis C Predictive Modelling\n\nThis notebook is aimed at exploring and modelling Hepatitis C Data obtained from kaggle: https:\/\/www.kaggle.com\/fedesoriano\/hepatitis-c-dataset \n        \nIn this project we will conduct\n* Preliminary exploration of the data\n* Cleaning and treatment\n* Visualization to understand relationships\n* Modelling and optimization of the predictive model","8d2419f9":"There are some anomalies in the precision scores of some validation sets, but every metric is consistently high.\n\nThe fact that recall is higher than precision on in the cross_validate test means that there our hypothesis was not correct in this specific test. The incidence of false negatives (0%) is much higher than the chance of false positives (5%). With only a 5% rate of false positives, the need for retesting would neither be often or costly, making this predictor efficient and safe for hospitals and patients. While this concludes our project, it is important to communicate the shortcomings of the model and the improvements possible.","e2e62715":"Even with more rigorous testing parameters, the score is more likely to be perfect than not, which is impressive.\n\nNormally, we would produce a classification report for data on which type of error is present, but without enough errors, it will be hard to get a conclusive sample.\n\nInstead, our last analysis will be to investigate the models certainty in its predictions."}}