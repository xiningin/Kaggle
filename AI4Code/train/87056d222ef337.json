{"cell_type":{"7563e39c":"code","0238e3dd":"code","5a0a48ef":"code","bbdd3d81":"code","f4b51963":"code","0bd83f1b":"code","a8524d32":"code","d93d351e":"code","2fcfb092":"code","3624327f":"code","94b10b5f":"code","0e01bc8b":"code","7f831ee0":"code","2b97f659":"code","8bbecc35":"code","af30d8be":"code","423c6021":"code","2f93cbf4":"code","2fc24694":"markdown","5322475f":"markdown","e357a2d2":"markdown","9dba4681":"markdown","5fc43d90":"markdown","69d1e52b":"markdown","44539c94":"markdown","0f13bc42":"markdown","c8b95e45":"markdown","b0847321":"markdown","97506ca4":"markdown","db3ae86f":"markdown","b30a7449":"markdown","e1bc5f7b":"markdown","e2ac6666":"markdown","ab380081":"markdown"},"source":{"7563e39c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_validate\nimport warnings\n!pip install plotly==3.10.0\nfrom plotly.offline import iplot\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\n# evaluate an xgboost regression model on the housing dataset\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom xgboost import XGBRegressor\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom xgboost import XGBRegressor\nimport scipy.stats as ss   #Statistic \ntest_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\"\ntrain_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\"\nsubmission_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\"\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsubmission = pd.read_csv(submission_path)","0238e3dd":"train_backup = train.iloc[:,1:-1]\ntrain=train.iloc[:,1:]\ntest=test.iloc[:,1:]\nprint('train shape is: ',train.shape)\nprint('test shape is: ',test.shape)","5a0a48ef":"train.describe()","bbdd3d81":"y = train[\"loss\"]\nx = train.iloc[:,:-1]\nprint(type(x),type(y))\nprint( x.shape,y.shape)","f4b51963":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nplt.figure(figsize = (14, 15))\nplt.title('Corelation matrix for Train data')\nsns.heatmap(corr, mask = mask, cmap = 'BuPu', linewidths = .5)\nplt.show()","0bd83f1b":"y_numpy =y.values\nuniqueValues, occurCount = np.unique(y_numpy, return_counts=True)\nprint(\"Unique Values : \" , uniqueValues)\nprint(\"Occurrence Count : \", occurCount)","a8524d32":"from plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom plotly.tools import FigureFactory as FF\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport scipy\n\ntrace = go.Histogram(x=y_numpy, xbins=dict(start=np.min(y_numpy), size=0.20, end=np.max(y_numpy)), marker=dict(color='rgb(90, 20, 100)'))\nlayout = go.Layout(title=\"Histogram Frequency Counts\")\nfig = go.Figure(data=go.Data([trace]), layout=layout)\nfig.show()","d93d351e":"list_features = []\nfeatures = train.iloc[:,:-1]\nfor f in features.columns:\n    list_features.append(f)\nprint(list_features)","2fcfb092":"import scipy.stats as ss   #Statistic \nfor var in list_features:\n    gp = train[[var, 'loss']].groupby(['loss'])\n    gp_array = [group[var].to_numpy() for name, group in gp]\n    kstat, p = ss.kruskal(*gp_array)\n    kstat, p = round(kstat, 6), round(p, 6)\n    print(f'For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}')\n    ","3624327f":"train1 = train.iloc[:,:-1]","94b10b5f":"from sklearn.preprocessing import RobustScaler, QuantileTransformer, StandardScaler\n\n#sc = StandardScaler()\n#train1  = sc.fit_transform(train1)\n#test = sc.transform(test)","0e01bc8b":"x_train, x_test, y_train, y_test = train_test_split( train1 , y, test_size=0.1)","7f831ee0":"import xgboost\nfrom sklearn.model_selection import cross_validate\nprint(xgboost.__version__)\n\n\nmodel = XGBRegressor(learning_rate=0.01,\n                     n_estimators=800, \n                     subsample=1.0,\n                     reg_alpha=3, \n                     reg_lambda=2,\n                     gamma=2,\n                     colsample_bytree=0.6,\n                     tree_method = 'gpu_hist')\n","2b97f659":"model.fit(x_train, y_train)","8bbecc35":"def modelscorer(model, x_test, y_test):\n    y_pred = model.predict(x_test)\n    r2 = r2_score(y_test, y_pred)\n    print('R2: ', r2)\n    mse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n    rmse=mse**(0.5)\n    print('RMSE: ', rmse)\n    return r2, rmse\n\nmodelscorer(model, x_test, y_test)","af30d8be":"y_submission=model.predict(test)\npred = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\npred.loss = y_submission\npred.head()","423c6021":"pred.to_csv('submission.csv', index=False)","2f93cbf4":"\nmodel_imp = model.feature_importances_\nx = np.arange(0, len(train_backup.columns))\nheight = 0.3\nfig, ax = plt.subplots(figsize=(10, 15))\nbars1 = ax.barh(x-height, model_imp, height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label='loss')\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(train_backup.columns, fontsize=8)\nax.tick_params(axis=\"x\", labelsize=10)\nax.grid(axis=\"x\")\nax.legend(fontsize=13, loc=\"lower right\")\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()\n","2fc24694":"For variable f86, Kruskal-Wallis H-test: 31.875142 and p value: 0.871632\n\nFor variable f89, Kruskal-Wallis H-test: 34.47002 and p value: 0.788896\n\nFor variable f75, Kruskal-Wallis H-test: 43.202076 and p value: 0.419733\n\nFor variable f56, Kruskal-Wallis H-test: 45.18417 and p value: 0.340395\n\nFor variable f22, Kruskal-Wallis H-test: 45.675884 and p value: 0.321954\n\nFor variable f33, Kruskal-Wallis H-test: 45.912215 and p value: 0.313287\n\nFor variable f36, Kruskal-Wallis H-test: 46.999991 and p value: 0.275116\n\nFor variable f83, Kruskal-Wallis H-test: 47.622653 and p value: 0.254592\n\nFor variable f45, Kruskal-Wallis H-test: 49.111712 and p value: 0.20958\n\nFor variable f0, Kruskal-Wallis H-test: 53.062508 and p value: 0.117774\n\nFor variable f87, Kruskal-Wallis H-test: 53.092827 and p value: 0.117216\n\nFor variable f14, Kruskal-Wallis H-test: 53.603765 and p value: 0.108125","5322475f":"Cross validation takes excessive time, after 9 hours running still processing so that i quit using cv for this dataset.","e357a2d2":"#### Submission format setting ","9dba4681":"* The original dataset deals with calculating the loss associated with a loan defaults. However, this data set is prepared synthetically by the owner.","5fc43d90":"Xgboost\n\n* n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.\n* max_depth: The maximum depth of each tree, often values are between 1 and 10.\n* eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n* subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n* colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.","69d1e52b":"## $\\color{Pink}{\\text{Chapter 3. XGBOOST}}$ <a class=\"anchor\" id=\"chapter3\"><\/a>","44539c94":"Drop the id columns from test and train dataframes. The shape of the train is +1 according to test since it has the target feature already, which is \"loss\". We will divide it later as target funtion.","0f13bc42":"## $\\color{Pink}{\\text{Chapter 1. Reading the Data}}$ <a class=\"anchor\" id=\"chapter1\"><\/a>","c8b95e45":"## Section 1.2 Features <a class=\"anchor\"  id=\"section1_2\"><\/a>","b0847321":"![makine-ogrenimi-machine-learning-nedir.jpeg](attachment:fe2a988e-f8af-415d-b341-9e917a021c62.jpeg)\n\n\n## $\\color{Pink}{\\text{Table of Contents}}$\n\n* [Chapter 1. Reading the Data](#chapter1)     \n* [Chapter 2. EDA](#chapter2)\n    * [Section 2.1 Target Function](#section_2_1)\n    * [Section 2.1 Features](#section_2_2)\n* [Chapter 3. XGBOOST](#chapter3)\n\n\n\n\n\n ### ****$\\color{orange}{\\text{If You like my work, Please upvote!}}$****","97506ca4":"## $\\color{Pink}{\\text{Chapter 2. EDA}}$ <a class=\"anchor\" id=\"chapter2\"><\/a>\n\n","db3ae86f":"Target is loss. And we have 100 features. \n\n* Loss  feature is consist of integers between 0 to 42. So that we can yield with a regression problem as well as a 43 classes Classification problem. Since we do not know about the submission document, it will be to take the problem as a regression problem which predicts an integer at the end of the progress.","b30a7449":"## Feature Importance","e1bc5f7b":"**Kruskal-Wallis H-test**\n\nThe Kruskal-Wallis H test is also called the \"one-way ANOVA on ranks\". It is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable.","e2ac6666":"## Standart Scaling\n\nStandart Scaling is neccessary for the scale dimention differences. Since the corresponding data has scale differences, we will apply standard scale","ab380081":"## Section 1.1 Target Function <a class=\"anchor\"  id=\"section1_1\"><\/a>"}}