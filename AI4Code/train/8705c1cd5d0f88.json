{"cell_type":{"0f72994a":"code","edbd9422":"code","559ef4af":"code","475b3c6f":"code","3d38b2ef":"code","bf0ab062":"code","9f44772e":"code","56293975":"markdown","bf3207d5":"markdown","892a01e6":"markdown","40a2f237":"markdown","0b199a18":"markdown","c6b0a70c":"markdown","7917d384":"markdown","097b6ae9":"markdown"},"source":{"0f72994a":"import numpy as np \nimport pandas as pd \n\nfull_df= pd.read_csv('..\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv', low_memory= False)\n\n#create smaller dataframe with only the person-specific features, non-person-specific features, and what we're trying to predict\npersonal_feats= ['Age', 'Sex', 'Clo','Met',\"Subject\u00abs height (cm)\", \"Subject\u00abs weight (kg)\"]\nother_feats= ['Year', 'Season', 'Koppen climate classification', 'Climate', 'City', 'Country', 'Building type', 'Cooling startegy_building level', 'Heating strategy_building level','Air temperature (C)', 'Outdoor monthly air temperature (C)']\nlabel= ['Thermal preference']\ndf= full_df[personal_feats + other_feats + label]\ndf= df.dropna()\nprint('Dataset size: ', len(df))\ndf.head()","edbd9422":"from sklearn.model_selection import train_test_split\n\n#define features and labels\nfeats= df.loc[:, df.columns != label[0]]\nlabs= df.loc[:, df.columns == label[0]]\n\n#create 90\/10 train\/test split\nx_train, x_test, y_train, y_test= train_test_split(feats, labs, test_size= 0.1, random_state= 0)\n\n#see how many different levels there are for each categorical feature\ncateg_feats= feats.select_dtypes(include= ['category', object]).columns\nfor i in categ_feats:\n    print(i, df[i].nunique(), '\\n')  ","559ef4af":"from sklearn.preprocessing import OneHotEncoder\n\nencoder= OneHotEncoder(handle_unknown= 'ignore', sparse= False)\n\nOH_feats= pd.DataFrame(encoder.fit_transform(feats[categ_feats])) #encode the categorical features\nOH_feats.columns = encoder.get_feature_names(categ_feats) #ensure encoded col names are meaningful\nOH_feats.index= feats.index #indices need to match in order to add one-hot encodings to original dataframe\nfeats= feats.drop(categ_feats, axis= 1) #delete categorical cols from original dataframe\nfeats= pd.concat([feats, OH_feats], axis= 1) \n\n#create 90\/10 train\/test split\nx_train, x_test, y_train, y_test= train_test_split(feats, labs, test_size= 0.1, random_state= 0)","475b3c6f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndef important_feats(x_train, y_train, x_test):\n    \"\"\"\n    Function that fits a random forest and extracts the importance weight from the forest for each feature to determine which features are most important\n    (Features with an importance weight greater than the median weight are most important)\n    \n    INPUTS: x_train is a pandas dataframe where each row is one example and each column is a feature (training data)\n    y_train is a pandas dataframe with the corresponding labels to each example in x_train\n    x_test is a pandas dataframe where each row is one example and each column is a feature (test data)\n    \n    OUTPUTS: x_train_new is the same as x_train except with only the most important features retained\n    x_test_new is the same as x_test except with only the most important features retained\n\n    \"\"\"\n    #define and fit tree\n    forest= RandomForestClassifier(n_estimators= 1000, random_state= 0)\n    forest.fit(x_train, np.ravel(y_train))\n\n    #select most important features\n    selector= SelectFromModel(forest, threshold= 'median')\n    selector.fit(x_train, np.ravel(y_train))\n    important_feats= np.array([]) #store the names of the most important features\n    for i in selector.get_support(indices= True):\n        important_feats= np.append(important_feats, x_train.columns[i])\n    \n    #return only the most important features (for both training and test sets)\n    x_train_new= pd.DataFrame(selector.transform(x_train), columns= important_feats)\n    x_test_new= pd.DataFrame(selector.transform(x_test), columns= important_feats)\n    \n    return important_feats, x_train_new, x_test_new\n\n\n#redefine the columns that are person-specific features (names are different now because of the one-hot encoding!)\npersonal_feats= ['Sex_Female', 'Sex_Male', 'Age', 'Clo', 'Met', 'Subject\u00abs height (cm)', 'Subject\u00abs weight (kg)']\n\n#for forest that uses only person-specific features:\nx_train_personal= x_train.loc[:, personal_feats]\nx_test_personal= x_test.loc[:, personal_feats]\n\n#identify the most important person-specific features:\npersonal_important_feats, x_train_personal_new, x_test_personal_new= important_feats(x_train_personal, y_train, x_test_personal)\nprint(personal_important_feats)","3d38b2ef":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import balanced_accuracy_score\n\ndef train_eval_knn(x_train, y_train, x_test, y_test):\n    \"\"\"\n    Function that trains and tests a kNN multi-class classifier and returns the average recall on the test set\n    \n    INPUTS: x_train and x_test are 2D numpy arrays where each row is one example and each column is a feature,\n    y_train and y_test are pandas dataframes with the corresponding label for the examples in x_train\/x_test\n    OUTPUT: test_recall is the average recall for the test set (single float value)\n    \"\"\"\n    knn= KNeighborsClassifier()\n    knn.fit(x_train, np.ravel(y_train))\n    #note: default settings on kNN uses the Euclidean distance as the distance metric and equally weights all examples\n    \n    test_predicts= knn.predict(x_test)\n    test_recall= balanced_accuracy_score(y_test, test_predicts)\n    \n    return test_recall\n\n#scale the training data so that all features have a mean of 0 and unit variance\nscaler= StandardScaler()\nx_train_personal_new= scaler.fit_transform(x_train_personal_new)\n\n#scale the test data using the mean and variance calculated from the training data\nx_test_personal_new= scaler.transform(x_test_personal_new)\n\n#train and test kNN that uses the most important person-specific features\npersonal_recall= train_eval_knn(x_train_personal_new, y_train, x_test_personal_new, y_test)\nprint(personal_recall)","bf0ab062":"#redefine the columns that are non-person-specific features (names are different now because of the one-hot encoding!)\nother_feats= set(x_train.columns) - set(personal_feats)\n\n#get training and test training sets that have only the non-person-specific features\nx_train_other= x_train.loc[:, other_feats]\nx_test_other= x_test.loc[:, other_feats]\n\n#identify the most important non-person-specific features:\nother_important_feats, x_train_other_new, x_test_other_new= important_feats(x_train_other, y_train, x_test_other)\nprint(other_important_feats)\n\n#scale feature values before running kNN...\nx_train_other_new= scaler.fit_transform(x_train_other_new)\nx_test_other_new= scaler.transform(x_test_other_new)\n\n#train and test kNN that uses the most important non-person-specific features\nother_recall= train_eval_knn(x_train_other_new, y_train, x_test_other_new, y_test)\nprint(other_recall)","9f44772e":"#identify the most important features:\nimportant_feats, x_train_new, x_test_new= important_feats(x_train, y_train, x_test)\nprint(important_feats)\n\n#scale feature values before running kNN...\nx_train_new= scaler.fit_transform(x_train_new)\nx_test_new= scaler.transform(x_test_new)\nx_train= scaler.fit_transform(x_train)\nx_test= scaler.fit_transform(x_test)\n\n#train and test kNN that uses the most important features\nrecall= train_eval_knn(x_train_new, y_train, x_test_new, y_test)\nprint(recall)\n\n#train and test kNN that uses all of the features, not just the most important ones\nrecall_allfeats= train_eval_knn(x_train, y_train, x_test, y_test)\nprint(recall_allfeats)","56293975":"**Therefore, when considering only non-person-specific features, the most important features for predicting thermal preference are Outdoor monthly air temperature, Climate, City, Koppen climate classification, Season, Air temperature, and Year. The average recall using these most important non-person-specific features in a kNN model is ~0.59.** Interesting! Our predictions are better when we use non-person-specific features as input to our model compared to when we use person-specific features. Let's see what the most important features are and what our predictive performance is like when we consider both the person-specific and non-person-specific features.\n\n# Random forest & kNN - All features","bf3207d5":"We can also see that some of our features (columns) are numerical while others are categorical. For each of the categorical features, we'll see how many different levels there are and decide how we should encode these features.","892a01e6":"A recall of 0.37 is not great! Can we achieve better prediction performance if we consider non-person-specific features? Let's see - we'll do the same feature selection approach and training and testing a kNN model using only non-person-specific features.\n\n# Random forest & kNN - Other features","40a2f237":"**Therefore, when considering both person-specific and non-person-specific features, the most important features for predicting thermal preference are Age, Clo, Met, Subject height, Subject weight, Sex, Outdoor monthly air temperature, Climate, City, Season, and Koppen climate classification. The average recall using these most important features in a kNN model is ~0.58. Whether we use only these most important features or all of the features, we get almost the same average recall.** Since we get the same average recall either way, this means that the excluded features do not improve the model's predictive performance very much and don't need to be included. Future thermal comfort studies that will be added to the ASHRAE database can be designed in such a way so that these excluded features are not measured.\n\n# Conclusions\nThe purpose of this notebook was to answer \"How do human and personal factors influence thermal comfort perception?\" using metadata from thermal comfort studies. When considering only person-specific features to predict thermal preference, the most important features to predict thermal preference are Clo, Met, Subject height, and Subject weight. **This suggests that these personal factors are the ones that most strongly influence thermal comfort perception.** However, these person-specific features are not the best to use to predict thermal preference. Instead, the non-person-specific features of Outdoor monthly air temperature, Climate, City, Koppen climate classification, Season, Air temperature, and Year yield the best average recall (~0.59) in a kNN model used to predict thermal preference. A kNN model that used these non-person-specific features performed better than a kNN model that used both person-specific and non-person-specific features, suggesting that environmental factors rather than personal factors most strongly influence thermal comfort perception.\n\n## Suggestions for future work\nAfter data wrangling, the final dataset used for the analyses was very small (1043 examples compared to the original >100,000). Future work should consider using different features and\/or using a different technique to deal with NaN values to increase the dataset size. Also along these lines, since the dataset was so small, using cross-validation rather than a train\/test split likely would have been a better\/more representative approach to the random forest and kNN models. A final suggestion for those wanting to pursue this analysis further is to perform a grid search to determine the best hyperparameter settings for the kNN models rather than using the default settings. By finding the optimal hyperparameter settings, there's a possibility that the average recall for the various kNN models tested here would be higher. Overall I hope this notebook acts as a good spring board for those wanting to further explore \"How do human and personal factors influence thermal comfort perception?\" \n","0b199a18":"**Therefore, when considering only person-specific features, the most important features for predicting thermal preference are Clo, Met, Subject height, and Subject weight. The values of these features are the most informative for predicting one's thermal preference.** For curiousity's sake, we'll train and evaluate a kNN model that uses only these features. First we'll do standardize the features such that each feature has a mean of 0 and unit variance. Just like the random forests, we'll use the default hyperparameter settings for this and all other kNN models. To evaluate the kNN model, we'll use the average recall for each class.\n\n# kNN - Personal features","c6b0a70c":"Since there are not many levels for each of our categorical features, we will one-hot encode each categorical feature and replace the original columns with their one-hot encodings. *Note: how the one-hot encoding has been implemented in this notebook requires that any additional\/future test sets must not have different levels from the levels in the training set for each of the encoded categorical features.* Then, although our dataset is small (1043 examples), we will use a 90\/10 train\/test split. The same split will be used for all random forests and kNN models.","7917d384":"# Abstract\n**This notebook aims to answer \"How do human and personal factors influence thermal comfort perception?\", a question posed as a task under the ASHRAE Global Thermal Comfort Database II dataset. To answer this, three different random forests will first be developed to identify the most important features of those provided in the dataset. Once the most important features are identified, they will be used in kNN models to predict thermal preference.** One random forest using only person-specific features (e.g. age, sex) as input features will be used to identify the most important person-specific features for predicting thermal preference. A second random forest using non-person-specific features (e.g. season, city, heating strategy) will be used to identify the most important non-person-specific features for predicting thermal preference. A third random forest using both person-specific and non-person-specific features will be used to identify the most important features for predicting thermal preference. For this final random forest, we will develop two kNN models predicting thermal preference (multi-class classification) and compare their performance. One kNN model will use all features and the other kNN model will use only the most important features as identified by this final random forest. **By the end of this notebook, we'll know which features are most important for predicting thermal preference and we will have the accuracy of a model predicting thermal preference using these most important features.**\n\n## Concepts used\n- One-hot encoding categorical data\n- Train\/test split\n- Random forest for feature selection \n- kNN for multi-class classification\n- Standardizing feature values\n\nExplanations of each of these concepts are not provided in this notebook.\n\n\n# Data Wrangling, Encoding categorical features\nIn the dataset, for a row that has a NaN for a particular column, the NaN means that the study did not record this particular feature. Having some NaNs is expected considering that the dataset contains the metadata for field studies on thermal comfort and not all study designs are identical. Since scikit's implementation of a random forest cannot tolerate NaN values, columns from the main dataset that contain many NaNs will not be used. Furthermore, rather than trying to infer the value of those features that have a NaN, any entries\/rows that contain a NaN will be omitted. Based on this, the columns we'll use are Year, Season, Koppen climate classification, Climate, City, Country, Building type, Cooling straight_building level, Heating strategy_building level, Age, Sex, Thermal preference, Clo, Met, Air temperature (C), Outdoor monthly air temperature (C), Subject's height (cm), Subject's weight (kg). We will extract these columns and delete any rows that have NaN values.","097b6ae9":"# Random Forest - Personal Features\nNow we will train a random forest predicting thermal preference using only the personal features as input features and identify which of these features are most important for predicting thermal preference. For simplicity, for the random forest, the default settings for all hyperparameters will be used except for the number of trees (estimators) and random state. Once we've fit our tree, we'll then determine the importance weights for each of the features and identify the features with an importance weight greater than the median importance weight. **These features will be our most important person-specific features for predicting thermal preference.**\n\nSince we will be completing this process two additional times, we'll write a function that accomplishes all of this."}}