{"cell_type":{"4db974cd":"code","2d4e70d2":"code","5e9a05de":"code","67488de4":"code","d0082932":"code","4e086bf7":"code","d053293c":"code","60e2616d":"code","b0fa6951":"code","5fc47531":"code","318aa169":"code","b755c848":"code","ffd9906b":"code","aa22e067":"code","91f57844":"code","6669acfa":"code","c846eb67":"code","be1ab592":"code","380bfb89":"code","cfd14031":"code","92fbe339":"code","4b046196":"code","6f72d9fc":"code","04f84e38":"code","15f9799d":"code","b46b42fb":"code","13bd8924":"code","05278498":"code","0c64973a":"code","a17c82a1":"code","48f2dc3a":"code","c40766e0":"markdown","a7d35ce9":"markdown","79b55f29":"markdown","7c2736ea":"markdown","eaf8078f":"markdown","8f7f2ebd":"markdown","34867827":"markdown","cf0039cf":"markdown","e2520759":"markdown","dd783ef6":"markdown","3449a744":"markdown","f9b82313":"markdown","69fe7844":"markdown","d5c2a71a":"markdown","4ea98921":"markdown","60e5e00a":"markdown"},"source":{"4db974cd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport bottleneck as bn # library used for moving average\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.optimizers import SGD\nfrom keras.layers import Dense, Dropout,BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","2d4e70d2":"# load the dataset into a pandas dataframe\ndataset = pd.read_csv('..\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')\ndataset.head()","5e9a05de":"# check if the dataset contains NaN values\ndataset.isnull().values.any()","67488de4":"dataset.describe()","d0082932":"# plot the boxplots of all features\nplt.tight_layout(pad=0.9)\nplt.figure(figsize=(20,15)) \nplt.subplots_adjust(wspace = 0.2)\nnbr_columns = 4 \nnbr_graphs = len(dataset.columns)\nnbr_rows = int(np.ceil(nbr_graphs\/nbr_columns)) \ncolumns = list(dataset.columns.values) \nwith sns.axes_style(\"whitegrid\"):\n    for i in range(0,len(columns)-1): \n        plt.subplot(nbr_rows,nbr_columns,i+1) \n        ax1=sns.boxplot(x= columns[i], data= dataset, orient=\"h\",color=sns.color_palette(\"Blues\")[3]) \n    plt.show() ","4e086bf7":"# plotting the Correlation matrix\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(dataset.corr(),annot=True)\nplt.show()","d053293c":"# print the list of all testruns\nprofile_id_list = dataset.profile_id.unique()\nprint(profile_id_list)\nprint(\"amount of test runs: {0}\".format(profile_id_list.size))","60e2616d":"# plotting 'stator_yoke','stator_tooth','stator_winding' for a random set of testruns\ncolumns = ['stator_yoke','stator_tooth','stator_winding']\nprofile_id_list = np.random.choice(profile_id_list, size=8, replace=False)    \nnbr_column = 2 \nnbr_graph= len(profile_id_list) \nnbr_row = int(np.ceil(nbr_graph\/nbr_column)) \nkolomlijst = list(dataset.columns.values) \nplt.figure(figsize=(30,nbr_row*5)) \n    \nwith sns.axes_style(\"whitegrid\"):    \n    for i in range(0,nbr_graph): \n        plt.subplot(nbr_row,nbr_column,i+1) \n        temp = dataset.loc[dataset['profile_id'] == profile_id_list[i]]\n        temp = temp.loc[:,columns]\n        temp = temp.iloc[::100, :]\n        ax1=sns.lineplot(data=temp.loc[:,columns], \n                        dashes = False,\n                        palette=sns.color_palette('Dark2',n_colors=len(columns)))\n        ax1.set_title(\"profile id: {0}\".format(profile_id_list[i]))\n    plt.show    ","b0fa6951":"# plotting the stator winding temperature in comparison to torque and motorspeed\nprofile_id = 6\nfeat_plot_1 = ['stator_winding']\nfeat_plot_2 = ['torque','motor_speed']\ntemp = dataset.loc[dataset['profile_id'] == profile_id]\ntemp = temp.iloc[::10, :]\n\nwith sns.axes_style(\"whitegrid\"):\n    fig = plt.figure(figsize=(20, 10))\n    ax1 = fig.add_subplot(211)\n    ax1 = sns.lineplot(data=temp.loc[:,feat_plot_1], dashes = False,\n                       palette=sns.color_palette('Dark2',n_colors=len(feat_plot_1)),linewidth=0.8)\n    ax2 = fig.add_subplot(212)\n    ax2 = sns.lineplot(data=temp.loc[:,feat_plot_2], dashes = False,\n                       palette=sns.color_palette('Dark2',n_colors=len(feat_plot_2)),linewidth=0.8)\n    plt.show()","5fc47531":"# Seperating input and output variables\nX = dataset.drop('torque', axis=1).loc[:,'ambient':'i_q'].values \ny = dataset.loc[:,'stator_winding'].values \n\n# split up in training and test data\nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.3)","318aa169":"X.shape","b755c848":"# training the Random Forest Regressor on the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nRFR_model = RandomForestRegressor(n_estimators = 10, random_state = 0)\nRFR_model.fit(X_train, y_train)","ffd9906b":"# Calculate MSE and MAE for the entire testset\ny_pred = RFR_model.predict(X_test)\nRFR_MSE = mean_squared_error(y_test, y_pred)\nRFR_MAE = mean_absolute_error(y_test, y_pred)\nprint(\"MSE: {0}\".format(RFR_MSE))\nprint(\"MAE: {0}\".format(RFR_MAE))","aa22e067":"# plot the true vs predicted values for multiple testruns\ntest_run_list = np.array([27,45,60,74])\n#test_run_list = np.random.choice(profile_id_list, size=4, replace=False)]\noutput_value = 'stator_winding'\nmodel = RFR_model\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, axs = plt.subplots(len(test_run_list),1,figsize=(20,len(test_run_list)*5),squeeze=False)\n    \n    for i in range(0,len(test_run_list)):\n        X_plot = dataset.drop('torque', axis=1).loc[dataset['profile_id'] == test_run_list[i],'ambient':'i_q'].values \n        y_plot = dataset.loc[dataset['profile_id'] == test_run_list[i],output_value].values \n        y_pred_plot = model.predict(X_plot)\n\n        time = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n        axs[i,0].plot(time,y_pred_plot,label='predict',color='red',alpha=0.4,linewidth=0.8)\n        axs[i,0].plot(time,y_plot,label='True',color='black',linewidth=1)\n        axs[i,0].legend(loc='best')\n        axs[i,0].set_title(\"profile id: {0}\".format(test_run_list[i]))\n    plt.show()   ","91f57844":"# This is the testrun we will use as an example for the evaluation of the models\n# change this value to use a different testrun\nchoosen_example_testrun = 76","6669acfa":"# plot the true vs predicted values for a choosen testrun without and with moving average smoothing:\nprofile_id = choosen_example_testrun\noutput_value = 'stator_winding'\nmodel = RFR_model\nmoving_average_window = 100\n\nX_plot = dataset.drop('torque', axis=1).loc[dataset['profile_id'] == profile_id,'ambient':'i_q'].values \ny_plot = dataset.loc[dataset['profile_id'] == profile_id,output_value].values \ny_pred_plot = model.predict(X_plot)\ny_pred_plot_smooth = bn.move_mean(y_pred_plot,moving_average_window,1)\ntime = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n\nwith sns.axes_style(\"whitegrid\"):\n    fig = plt.figure(figsize=(20, 10))\n    \n    ax1 = fig.add_subplot(211)\n    ax1.plot(time,y_pred_plot,label='predict without smoothing',color='red',alpha=0.4,linewidth=0.8)\n    ax1.plot(time,y_plot,label='True',color='black',linewidth=1)\n    ax1.legend(loc='best')\n    ax1.set_title(\"profile id: {0} without smoothing\".format(profile_id))\n    \n    ax2 = fig.add_subplot(212)\n    ax2.plot(time,y_pred_plot_smooth,label='predict with smoothing',color='red',alpha=0.8,linewidth=0.8)\n    ax2.plot(time,y_plot,label='True',color='black',linewidth=1)\n    ax2.legend(loc='best')\n    ax2.set_title(\"profile id: {0} with smoothing\".format(profile_id))\n    \n    plt.show()\n\n# Calculate MSE and MAE for the choosen testrun without and with moving average smoothing:\nMSE_RFR_model = mean_squared_error(y_plot, y_pred_plot)\nMAE_RFR_model = mean_absolute_error(y_plot, y_pred_plot)\nprint(\"metrics without moving average smoothing:\")\nprint(\"MSE: {0}\".format(MSE_RFR_model))\nprint(\"MAE: {0}\".format(MAE_RFR_model))\nMSE_RFR_model = mean_squared_error(y_plot, y_pred_plot_smooth)\nMAE_RFR_model = mean_absolute_error(y_plot, y_pred_plot_smooth)\nprint(\"metrics with moving average smoothing:\")\nprint(\"MSE: {0}\".format(MSE_RFR_model))\nprint(\"MAE: {0}\".format(MAE_RFR_model))","c846eb67":"# constructing and training the neural network\nnr_epochs=200\nb_size=1000\n\nNN_reg_model = Sequential()\nNN_reg_model.add(Dense(11, input_dim=X_train.shape[1], activation='relu'))\nNN_reg_model.add(Dense(9, activation='relu'))\nNN_reg_model.add(Dense(7, activation='relu'))\nNN_reg_model.add(Dense(5, activation='relu'))\nNN_reg_model.add(Dense(1))\nNN_reg_model.compile(loss='mean_squared_error', optimizer='adam',metrics = [\"mean_squared_error\"])\nhistory = NN_reg_model.fit(X_train, y_train, validation_split=0.33,epochs=nr_epochs, batch_size=b_size, verbose=0)","be1ab592":"#plot the history of the model accuracy during training\nplt.figure(figsize=(18,6))\nax1=plt.subplot(1, 2, 1)\nax1=plt.plot(history.history['mean_squared_error'],color='blue')\nax1=plt.plot(history.history['val_mean_squared_error'],color='red',alpha=0.5)\nax1=plt.title('model accuracy')\nax1=plt.ylabel('accuracy')\nax1=plt.xlabel('epoch')\nax1=plt.legend(['train', 'test'], loc='upper left')\n\n# plot the history of the model loss during training\nax2=plt.subplot(1, 2, 2)\nax2=plt.plot(history.history['loss'],color='blue')\nax2=plt.plot(history.history['val_loss'],color='red',alpha=0.5)\nax2=plt.title('model loss')\nax2=plt.ylabel('loss')\nax2=plt.xlabel('epoch')\nax2=plt.legend(['train', 'test'], loc='upper left')\nplt.show()","380bfb89":"# Calculate MSE and MAE of the entire testset\ny_pred = NN_reg_model.predict(X_test)\nNN_MSE = mean_squared_error(y_test, y_pred)\nNN_MAE = mean_absolute_error(y_test, y_pred)\nprint(\"MSE: {0}\".format(NN_MSE))\nprint(\"MAE: {0}\".format(NN_MAE))","cfd14031":"# plot the true vs predicted values for multiple testruns\ntest_run_list = np.array([27,45,60,74])\n#test_run_list = np.random.choice(profile_id_list, size=4, replace=False)]\noutput_value = 'stator_winding'\nmodel = NN_reg_model\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, axs = plt.subplots(len(test_run_list),1,figsize=(20,len(test_run_list)*5),squeeze=False)\n    \n    for i in range(0,len(test_run_list)):\n        X_plot = dataset.drop('torque', axis=1).loc[dataset['profile_id'] == test_run_list[i],'ambient':'i_q'].values \n        y_plot = dataset.loc[dataset['profile_id'] == test_run_list[i],output_value].values \n        y_pred_plot = model.predict(X_plot)\n\n        time = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n        axs[i,0].plot(time,y_pred_plot,label='predict',color='red',alpha=0.4,linewidth=0.8)\n        axs[i,0].plot(time,y_plot,label='True',color='black',linewidth=1)\n        axs[i,0].legend(loc='best')\n        axs[i,0].set_title(\"profile id: {0}\".format(test_run_list[i]))\n    plt.show()   ","92fbe339":"# plot the true vs predicted values for a choosen testrun without and with moving average smoothing:\nprofile_id = choosen_example_testrun\noutput_value = 'stator_winding'\nmodel = NN_reg_model\nmoving_average_window = 100\n\nX_plot = dataset.drop('torque', axis=1).loc[dataset['profile_id'] == profile_id,'ambient':'i_q'].values \ny_plot = dataset.loc[dataset['profile_id'] == profile_id,output_value].values \ny_pred_plot = model.predict(X_plot).flatten()\ny_pred_plot_smooth = bn.move_mean(y_pred_plot,moving_average_window,1)\ntime = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n\nwith sns.axes_style(\"whitegrid\"):\n    fig = plt.figure(figsize=(20, 10))\n    \n    ax1 = fig.add_subplot(211)\n    ax1.plot(time,y_pred_plot,label='predict without smoothing',color='red',alpha=0.4,linewidth=0.8)\n    ax1.plot(time,y_plot,label='True',color='black',linewidth=1)\n    ax1.legend(loc='best')\n    ax1.set_title(\"profile id: {0} without smoothing\".format(profile_id))\n    \n    ax2 = fig.add_subplot(212)\n    ax2.plot(time,y_pred_plot_smooth,label='predict with smoothing',color='red',alpha=0.8,linewidth=0.8)\n    ax2.plot(time,y_plot,label='True',color='black',linewidth=1)\n    ax2.legend(loc='best')\n    ax2.set_title(\"profile id: {0} with smoothing\".format(profile_id))\n    \n    plt.show()\n\n# Calculate MSE and MAE for the choosen testrun without and with moving average smoothing:\nMSE_RFR_model = mean_squared_error(y_plot, y_pred_plot)\nMAE_RFR_model = mean_absolute_error(y_plot, y_pred_plot)\nprint(\"metrics without moving average smoothing:\")\nprint(\"MSE: {0}\".format(MSE_RFR_model))\nprint(\"MAE: {0}\".format(MAE_RFR_model))\nMSE_RFR_model = mean_squared_error(y_plot, y_pred_plot_smooth)\nMAE_RFR_model = mean_absolute_error(y_plot, y_pred_plot_smooth)\nprint(\"metrics with moving average smoothing:\")\nprint(\"MSE: {0}\".format(MSE_RFR_model))\nprint(\"MAE: {0}\".format(MAE_RFR_model))","4b046196":"# plot the amount of samples per testrun\nplt.figure(figsize=(18,5))\nsns.countplot(x=\"profile_id\", data=dataset,color=sns.color_palette(\"Blues\")[2])\nplt.show()","6f72d9fc":"# find the testrun with the least amount of samples\nmax_batch_size = dataset.profile_id.value_counts().min()\nprofile_id_list = dataset.profile_id.unique()\nprint(\"list of testruns:\")\nprint(profile_id_list)\nprint(\"smallest amount of samples in one testrun: {0}\".format(max_batch_size))\nprint(\"testrun with smallest amount of samples: {0}\".format(dataset.profile_id.value_counts().idxmin()))","04f84e38":"# function to create time-step windows for LSTM\n\ndef sliding_window(profile_id_list,max_sample_count,sample_rate=1,window_size=100):\n    # profile_id_list: list of testruns we want to use to extract our samples\n    # max_sample_count: the total amount of samples we want in our trainingset\n    # sample rate: amount of samples to skip between the previous and next sample\n    # window_size: amount of time steps (samples in the past) the window contains\n    \n    nr_of_features = 7 #number of columns minus 'stator_winding','profile_id'\n    sample_count = 0\n\n    i = 0\n    X = np.zeros((max_sample_count,window_size,nr_of_features))\n    y = np.zeros((max_sample_count))\n\n    for profile_id in profile_id_list:\n        temp=(dataset[dataset['profile_id']==profile_id]).iloc[lambda x: x.index % sample_rate==0]     \n        temp_y = temp['stator_winding']\n        temp_X = temp.drop('torque', axis=1).loc[:,'ambient':'i_q']\n    \n        i=0\n        while i < len(temp_X)-window_size and sample_count < max_sample_count:\n            X[sample_count] = temp_X.iloc[i:i+window_size]\n            y[sample_count] = temp_y.iloc[i+window_size]\n            sample_count +=1\n            i +=1\n    return (X,y) \n        ","15f9799d":"# split the testruns in a training and testset\nprofile_id_list_train ,profile_id_list_test = train_test_split(profile_id_list,test_size=0.3)\nprint(\"the list of testruns used for extracting the training sample windows:\")\nprint(profile_id_list_train)\nprint(\"the list of testruns used for extracting the testing sample windows:\")\nprint(profile_id_list_test)","b46b42fb":"# constructing and training the LSTM\nwindow_Size = 100\nsample_amount = 5000\nsample_rate = 10\nepoch= 200\nb_size = 500\n\nX_train, y_train = sliding_window(profile_id_list_train,sample_amount,sample_rate,window_Size)\nX_test, y_test = sliding_window(profile_id_list_test,sample_amount,sample_rate,window_Size)\n\nLSTM_model = Sequential()\nLSTM_model.add(LSTM(128, input_shape = (window_Size,7),return_sequences=True))\nLSTM_model.add(LSTM(64, return_sequences=False))\nLSTM_model.add(Dense(32, activation='relu'))\nLSTM_model.add(Dropout(0.2))\nLSTM_model.add(Dense(16, activation='relu'))\nLSTM_model.add(Dense(8, activation='relu'))\nLSTM_model.add(Dense(1))\nLSTM_model.compile(loss='mean_squared_error', optimizer='adam',metrics = [\"mean_squared_error\"])\nhistory = LSTM_model.fit(X_train,y_train,validation_split=0.33, epochs = epoch, batch_size = b_size, verbose = 0)","13bd8924":"#plot the history of the model accuracy during training\nplt.figure(figsize=(18,6))\nax1=plt.subplot(1, 2, 1)\nax1=plt.plot(history.history['mean_squared_error'],color='blue')\nax1=plt.plot(history.history['val_mean_squared_error'],color='red',alpha=0.5)\nax1=plt.title('model accuracy')\nax1=plt.ylabel('accuracy')\nax1=plt.xlabel('epoch')\nax1=plt.legend(['train', 'test'], loc='upper left')\n\n# plot the history of the model loss during training\nax2=plt.subplot(1, 2, 2)\nax2=plt.plot(history.history['loss'],color='blue')\nax2=plt.plot(history.history['val_loss'],color='red',alpha=0.5)\nax2=plt.title('model loss')\nax2=plt.ylabel('loss')\nax2=plt.xlabel('epoch')\nax2=plt.legend(['train', 'test'], loc='upper left')\nplt.show()","05278498":"# Calculate MSE and MAE of the entire testset\ny_pred_LSTM = LSTM_model.predict(X_test)\nLSTM_MSE = mean_squared_error(y_test, y_pred_LSTM)\nLSTM_MAE = mean_absolute_error(y_test, y_pred_LSTM)\nprint(\"MSE: {0}\".format(LSTM_MSE))\nprint(\"MAE: {0}\".format(LSTM_MAE))","0c64973a":"# plot the true vs predicted values for multiple testruns\ntest_run_list = np.array([27,45,60,74])\n#test_run_list = np.random.choice(profile_id_list, size=4, replace=False)]\noutput_value = 'stator_winding'\nmodel = LSTM_model\n\nwindow_Size = 100\nsample_rate = 10\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, axs = plt.subplots(len(test_run_list),1,figsize=(20,len(test_run_list)*5),squeeze=False)\n    \n    for i in range(0,len(test_run_list)):\n        sample_amount = (len(dataset[dataset['profile_id']==test_run_list[i]])-window_Size)\/\/sample_rate\n        X_plot, y_plot = sliding_window([test_run_list[i]],sample_amount,sample_rate,window_Size)\n        y_pred_plot = model.predict(X_plot)\n\n        time = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n        axs[i,0].plot(time,y_pred_plot,label='predict',color='red',alpha=0.4,linewidth=0.8)\n        axs[i,0].plot(time,y_plot,label='True',color='black',linewidth=1)\n        axs[i,0].legend(loc='best')\n        axs[i,0].set_title(\"profile id: {0}\".format(test_run_list[i]))\n    plt.show()   ","a17c82a1":"# plot the true vs predicted values for one run\ntest_run_list = np.array([choosen_example_testrun])\n#test_run_list = np.random.choice(profile_id_list, size=4, replace=False)]\noutput_value = 'stator_winding'\nmodel = LSTM_model\n\nwindow_Size = 100\n#sample_amount = 10000\nsample_rate = 10\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, axs = plt.subplots(len(test_run_list),1,figsize=(20,len(test_run_list)*5),squeeze=False)\n    \n    for i in range(0,len(test_run_list)):\n        sample_amount = (len(dataset[dataset['profile_id']==test_run_list[i]])-window_Size)\/\/sample_rate\n        X_plot, y_plot = sliding_window([test_run_list[i]],sample_amount,sample_rate,window_Size)\n        y_pred_plot = model.predict(X_plot)\n\n        time = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n        axs[i,0].plot(time,y_pred_plot,label='predict',color='red',alpha=0.4,linewidth=0.8)\n        axs[i,0].plot(time,y_plot,label='True',color='black',linewidth=1)\n        axs[i,0].legend(loc='best')\n        axs[i,0].set_title(\"profile id: {0}\".format(test_run_list[i]))\n    plt.show()\n    \nprint(\"MSE: {0}\".format(mean_squared_error(y_plot, y_pred_plot)))\nprint(\"MAE: {0}\".format(mean_absolute_error(y_plot, y_pred_plot)))","48f2dc3a":"# plot the true vs predicted values for multiple testruns for the Random Forest Regressor\n#test_run_list = np.array([27,45,60,74])\ntest_run_list = np.random.choice(profile_id_list, size=4, replace=False)\noutput_value = 'stator_winding'\nmodel = RFR_model\nmoving_average = 100\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, axs = plt.subplots(len(test_run_list),1,figsize=(20,len(test_run_list)*5),squeeze=False)\n    \n    for i in range(0,len(test_run_list)):\n        X_plot = dataset.drop('torque', axis=1).loc[dataset['profile_id'] == test_run_list[i],'ambient':'i_q'].values \n        y_plot = dataset.loc[dataset['profile_id'] == test_run_list[i],output_value].values \n        y_pred_plot = bn.move_mean(model.predict(X_plot),moving_average_window,1)\n\n        time = np.linspace(0, y_plot.shape[0],num=y_plot.shape[0])\n        axs[i,0].plot(time,y_pred_plot,label='predict',color='red',alpha=0.4,linewidth=0.8)\n        axs[i,0].plot(time,y_plot,label='True',color='black',linewidth=1)\n        axs[i,0].legend(loc='best')\n        axs[i,0].set_title(\"profile id: {0}\".format(test_run_list[i]))\n    plt.show()   ","c40766e0":"When we compare the Feed forward neural network with the Random Forest Regressor we can conclude that the neural network does not outperform the Random Forest in correctly predicting the stator temperature. (We could probably increase the performance by tweaking the neural network, but it doesn't seem very feasable to use a more complex solution like a Feed forward neural network when we achieve a descent and comparable performance using the Random Forest Regressor.)","a7d35ce9":"##  Random Forest Regressor\n\nWe start by training a Random Forest Regressor model.  ","79b55f29":"The lineplots confirm that all three temperatures follow the same trend. The stator winding temperatures shows the biggest variation followed by the stator tooth and stator yoke temperature. This is especially noticeable when there is a lot of variation in the stator winding temperature. If this is the case, the stator tooth and yoke temperatures follow a smoother path than the temperature recorded on the stator winding. In other words, the heat dissipated by the stator windings takes some time to heat up the stator tooth and yoke due to the thermal inertia of both stator parts.\n\nA second observation that can be made on the various lineplots is that sometimes the stator yoke temperature has a higher value than the stator winding. Because of the presumed normalisation mentioned earlier, we can not determine whether this is due to the normalisation method used or whether these values actually represent higher temperatures measured on the stator yoke. ","7c2736ea":"After exploring the dataset, following observations can be made: \n* The various testruns (52 in total) are labelled by the profile_id (as mentioned in the dataset description). The indexes for these testruns however, are not incremental.\n* The dataset description provides us with no references to the units used for each of the samples, making it harder to interpret the values measured. \n* When we look at the statistical overview of the dataset and the histograms, it seems the dataset already has had some kind of normalisation. \n* Features like torque, motor speed, rotor temperature (pm), stator yoke, winding and tooth temperature (resp. stator_yoke, stator_winding, stator_tooth), coolwater temperature (coolant) are all reasonably self-explanatory. The active and reactive current and voltage (resp. i_d,i_q,u_d,u_q) of the PMSM however, require some background knowledge of how a synchronous motor works.\n* The ambient temperature is measured by a thermal sensor located closely to the stator (as stated in the explanatory notes of the dataset). We can therefore assume that this will have an impact on the selfcooling capacity of the motor. A higher ambient temperature will probably result in a higher temperature for both the motor's stator and rotor.\n* The correlation matrix shows that there is a significant correlation between the three different stator temperatures.\n","eaf8078f":"# Predicting the Stator winding temperature\nBecause measuring the torque, rotor and stator temperatures of the electromotor is not reliable nor economically feasible in commercial applications (as stated in the dataset description), we will try to predict the stator temperature by using the other available features in the dataset.  We will start with removing the torque, rotor and stator temperatures from our dataset and use the stator winding temperature as our target value. \n\nOnce this is done, we will train multiple models to predict the correct stator winding temperature as an output value for the input variables given.","8f7f2ebd":"# Introduction\nIn this notebook we will try to predict the stator temperature of a permanent magnet synchronous motor (PMSM) deployed on a test bench.\nAs suggested in the explanatory notes of the dataset, the metrics \"rotor temperature\", \"stator temperature\" and \"torque\" are hard to measure in practical applications. A sufficiently accurate prediction model would therefore eliminate the need for actual measurements to determine the stator temperature.\n\nPlease feel free to comment on any possible additions or improvements.","34867827":"## LSTM\n\nWe already discovered that the stator temperature is not only influenced by the current input variables but also by the previous states. We will therefore try to train an LSTM and check if it outperforms our earlier trained Random Forest.\n\nIn order to be able to train the LSTM we will need to transform our dataset to a 3D input matrix which includes a time window for every inputvalue per sample. \n","cf0039cf":"# Exploring the data","e2520759":"## Feed Forward Neural network\nLet us now check whether a Feed forward neural network would do better in predicting the stator winding temperature. Just like with the Random Forest model, we will only use the actual values of the input variables to predict the stator winding temperature. ","dd783ef6":"In the second part of the above testrun there seems to be a relation between the stator temperatures, torque and motorspeed. When the torque and\/or motorspeed is increased the temperature rises and vice versa. The temperatures follow the change in torque and\/or motorspeed in what seems a  $1-e^{t}$ type of equation. However, looking at the first part of the plot, this is not always the case. Even with constant torque and motor speed the stator winding temperature shows several sudden changes in temperature. In other words, there seem to be one or more other variables which have an impact on the stator temperature.","3449a744":"# Predicting the stator temperature\nWe already discovered a significant correlation between the stator winding, yoke and tooth temperature. This is of course due to the fact that the stator winding is wounded round the stator tooth which in his turn is connected to the stator yoke. \nTo get a better insight into the relationship between the three features we will plot the feature values for various randomly selected testruns.","f9b82313":"Now we have prepared the trainingsdata, we can start training our LSTM.","69fe7844":"The graphs depicted above show the true stator winding temperature in comparison to the values predicted by the Random Forest Regressor. As we can see, the Random Forest Regressor seems to be able to capture the global trend in the stator winding value but still there is a lot of noise in the predicted values, especially when the true signal shows a lot of variation. \nTo filter out the noise generated by the model, we will apply a moving average function to the output signal.","d5c2a71a":"# Conclusion\nWhen we compare the performance of the three models, we can conclude that in this case the Random Forest Regressor with moving average smoothing outperforms both a classic Feedforward Neural Network and an LSTM. The underperforming of the two deep learning models can probably be reduced by changing the models structure and hyperparameters. \nHowever, because the Random Forest Regressor with moving average smoothing performs more than adequate in predicting the stator temperature (as the various examples below show) using a relative modest model like this is preferred above more computational and memory intensive alternatives like an NN or LSTM.  ","4ea98921":"As shown in the plot above, the length of the various testruns varies. This is important to keep in mind when composing our time windows because we can not allow the time windows to overlap 2 or more testruns. The function below will therefore only take time windows from one testrun as long as enough samples are present in that specific run, otherwise it will need to proceed to the next testrun until we have the required amount of samples with it's corresponding time windows.\nFurthermore the function will allow us to define a sample rate for taking the samples in the testruns.","60e5e00a":"Combined with the moving average function, the Random Forest Regressor gives us a fairly accurate prediction of the stator winding temperature. The moving average has reduced the noise in the predicted output values and has had a positive impact on the MSE for this specific testrun."}}