{"cell_type":{"b1a969ef":"code","dcf7b5c8":"code","bd8b8648":"code","2a8dc914":"code","a87e1fc1":"code","16ffe69c":"code","7d637269":"code","44bfb1f1":"code","3f5521fe":"code","a1203532":"code","b8b8155f":"code","a87b5fa0":"code","504ad6a8":"code","a3aeb87c":"code","79b7574b":"code","7dfa1d8d":"code","d414dcc8":"code","6c68ebd4":"code","088ce43b":"code","dcf1bf28":"code","12da941f":"code","932b0f77":"code","bff70f5c":"code","d610710b":"code","e0987513":"code","d0105451":"code","8e245f28":"code","ea41f94c":"code","b675721b":"code","55bfffe4":"code","eaa315ac":"code","6d813728":"code","f2e1e9b1":"code","11aca4ab":"code","ec21828b":"code","2b6a3f38":"code","20a52380":"code","6fd98c0b":"code","46357c29":"code","86782b69":"code","f1e1ecae":"code","3c9a6eee":"code","e412dd07":"code","84930cc0":"code","07ffda99":"markdown","271000d7":"markdown","94e823f6":"markdown","24d1bb7b":"markdown","b0997a8c":"markdown","89376736":"markdown","34b01eda":"markdown","f4ab6452":"markdown","dd4f4d1c":"markdown","b84c80c6":"markdown","1747a104":"markdown","fbd64a0a":"markdown","5183fcd4":"markdown","0f3a3482":"markdown","dd0ecafc":"markdown","30166cfa":"markdown","4336696e":"markdown","6a106669":"markdown","c1fea5e3":"markdown","e5fe0a37":"markdown","966f7818":"markdown","d77462c4":"markdown","3f608785":"markdown","134a7da2":"markdown","9590262d":"markdown","9c2c9e31":"markdown","618997b4":"markdown","3847cbe5":"markdown","470bcfe3":"markdown","3cb0908b":"markdown","97e7a021":"markdown","fb2fa373":"markdown","76c8a1c2":"markdown","81e34765":"markdown","e3510e10":"markdown","bc352b46":"markdown"},"source":{"b1a969ef":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport catboost\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool, cv\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder","dcf7b5c8":"train=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","bd8b8648":"display(train.shape)\ndisplay(test.shape)","2a8dc914":"train.head()","a87e1fc1":"lencoder = LabelEncoder()\nY = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","16ffe69c":"X=train.copy()\nX.drop('target',axis=1,inplace=True)","7d637269":"display(X.shape)\ndisplay(Y.shape)","44bfb1f1":"Y['target'].unique()## 4 classes","3f5521fe":"X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.20, random_state=42)","a1203532":"Y.target.unique()","b8b8155f":"scaler = StandardScaler()\nscaler.fit(X_train.iloc[:,1:])\ntrain_sc=scaler.transform(X_train.iloc[:,1:])","a87b5fa0":"test_sc=scaler.transform(X_test.iloc[:,1:])","504ad6a8":"pool_train=Pool(train_sc,Y_train)\npool_val=Pool(test_sc,Y_test)","a3aeb87c":"### Define a cv function to fit on data and find the optimal number of iteration keeping other parameters fixed\n### Function takes input = xgb object with default params , train data ,train y data \ndef modelfit(params,poolX,useTrainCV=True,cv_folds=5,early_stopping_rounds=10):\n    if useTrainCV:\n        cvresult = cv(params=params, pool=poolX,nfold=cv_folds,early_stopping_rounds=early_stopping_rounds,plot=True,verbose=50)\n    return cvresult ## return dataframe for the iteration till the optimal iteration is reached","79b7574b":"## Prepara a cv class params\nparams={\n    'loss_function':'MultiClass',\n    'iterations':1500,\n    'verbose':50\n}","7dfa1d8d":"### Object return the optimal number of trees to grow\nn_est=modelfit(params,pool_train)","d414dcc8":"n_est.shape[0]### number of optimal iteration","6c68ebd4":"### Fit the model with iteration=885\ncboost1=CatBoostClassifier(iterations=885,loss_function='MultiClass',random_seed=123,verbose=50)\ncboost1.fit(train_sc,Y_train)","088ce43b":"#Predict training set:\ntrain_predictions = cboost1.predict(train_sc)\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Accuracy :{}\".format(metrics.accuracy_score(Y_train, train_predictions)))\nprint(\"precision: {}\".format(metrics.precision_score(Y_train, train_predictions,average=None)))\nprint(\"recall: {}\".format(metrics.recall_score(Y_train, train_predictions,average=None)))\nprint(\"f1score: {}\".format(metrics.f1_score(Y_train, train_predictions,average=None)))","dcf1bf28":"#Predict test set:\ntest_predictions = cboost1.predict(test_sc)\n#Print model report:\nprint(\"\\nModel Report Test\")\nprint(\"Accuracy :{}\".format(metrics.accuracy_score(Y_test, test_predictions)))\nprint(\"precision: {}\".format(metrics.precision_score(Y_test, test_predictions,average=None)))\nprint(\"recall: {}\".format(metrics.recall_score(Y_test, test_predictions,average=None)))\nprint(\"f1score: {}\".format(metrics.f1_score(Y_test, test_predictions,average=None)))","12da941f":"cboost1.get_all_params()","932b0f77":"### Use grid search by keepin n_estimators from above = 885 and tune max_depth \n### This param are mostly for controlling the complexity of the model\n## Define the grid\n\nparam_test1 = {\n    'depth':np.arange(6,11,1)\n}\n\ngsearch1 = GridSearchCV(estimator = CatBoostClassifier(iterations=885,loss_function='MultiClass',random_seed=123,depth=6), \n                                    param_grid = param_test1, scoring='accuracy',\n                                    n_jobs=4, verbose=50,\n                                    cv=5)\ngsearch1.fit(train_sc,Y_train)\ngsearch1.best_params_, gsearch1.best_score_","bff70f5c":"### Fix depth=6 and tune learning rate\nparam_test2 = {\n    'learning_rate':[x\/10.0 for x in np.arange(1,10,1)]\n}\ngsearch2 = GridSearchCV(estimator = CatBoostClassifier(iterations=885,loss_function='MultiClass',random_seed=123,depth=6), \n                                    param_grid = param_test2, scoring='accuracy',\n                                    n_jobs=4, \n                                    cv=5)\ngsearch2.fit(train_sc,Y_train)\ngsearch2.best_params_, gsearch1.best_score_","d610710b":"### Keep iterations =885 and tune l2_leaf_Reg.\n### Fix depth=6 and tune l2 reg.\nparam_test3 = {\n    'l2_leaf_reg':[1,2,3,4,5]\n}\ngsearch3 = GridSearchCV(estimator = CatBoostClassifier(iterations=885,\n                                                       loss_function='MultiClass',\n                                                       random_seed=123,\n                                                       depth=6), \n                                                       param_grid = param_test3, \n                                                       scoring='accuracy',\n                                                       n_jobs=4, \n                                                       cv=5)\ngsearch3.fit(train_sc,Y_train)\ngsearch3.best_params_, gsearch1.best_score_","e0987513":"## Fit the carboost with the above params and check the train results\n## Prepara a cv class\nparams={\n    'loss_function':'MultiClass',\n    'iterations':885,\n    'l2_leaf_reg':4,\n    'depth':6,\n}","d0105451":"### Object return the optimal number of trees to grow\nn_est_1=modelfit(params,pool_train)","8e245f28":"n_est_1.shape[0]###885","ea41f94c":"scaler = StandardScaler()\nscaler.fit(X.iloc[:,1:])\nX_sc=scaler.transform(X.iloc[:,1:])","b675721b":"### Fit the model with iteration=\ncboost2=CatBoostClassifier(iterations=837,loss_function='MultiClass',random_seed=123,l2_leaf_reg=4,depth=6,verbose=50)\ncboost2.fit(X_sc,Y)","55bfffe4":"#Predict training set:\ntrain_predictions = cboost2.predict(X_sc)\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Accuracy :{}\".format(metrics.accuracy_score(Y, train_predictions)))\nprint(\"precision: {}\".format(metrics.precision_score(Y, train_predictions,average=None)))\nprint(\"recall: {}\".format(metrics.recall_score(Y, train_predictions,average=None)))\nprint(\"f1score: {}\".format(metrics.f1_score(Y, train_predictions,average=None)))","eaa315ac":"### scale the test set\nscaler = StandardScaler()\nscaler.fit(test.iloc[:,1:])\ntest_sc=scaler.transform(test.iloc[:,1:])","6d813728":"test_prediction=cboost2.predict(test_sc,prediction_type='Probability')","f2e1e9b1":"test_prob=pd.DataFrame(test_prediction)","11aca4ab":"submission=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","ec21828b":"submission=pd.concat([test['id'],test_prob],axis=1)\nsubmission.columns=['id','Class_1','Class_2','Class_3','Class_4']","2b6a3f38":"### File for submission\nsubmission.to_csv('submission.csv',index=False)","20a52380":"np.array(cboost2.get_feature_importance(prettified=True))","6fd98c0b":"shap_values = cboost2.get_feature_importance(\n    pool_val, \n    'ShapValues'\n)","46357c29":"### Get expected value for record 0 for 4 classes, last value is expected value\nexpected_value_cls1 = shap_values[0,0,-1]\nexpected_value_cls2 = shap_values[0,1,-1]\nexpected_value_cls3 = shap_values[0,2,-1]\nexpected_value_cls4 = shap_values[0,3,-1]\n\n### Get shap values for 4 classes \nshap_values_cls1 = shap_values[0,0,:-1]\nshap_values_cls2 = shap_values[0,1,:-1]\nshap_values_cls3 = shap_values[0,2,:-1]\nshap_values_cls4 = shap_values[0,3,:-1]","86782b69":"import shap\nshap.initjs()\nshap.force_plot(expected_value_cls1, shap_values_cls1, X_test.iloc[0,1:])","f1e1ecae":"import shap\nshap.initjs()\nshap.force_plot(expected_value_cls2, shap_values_cls2, X_test.iloc[0,1:])","3c9a6eee":"import shap\nshap.initjs()\nshap.force_plot(expected_value_cls3, shap_values_cls3, X_test.iloc[0,1:])","e412dd07":"import shap\nshap.initjs()\nshap.force_plot(expected_value_cls4, shap_values_cls3, X_test.iloc[0,1:])","84930cc0":"for i in range(4):\n    shap.summary_plot(shap_values[:,i,:], X_test)","07ffda99":"### Create a function to get the optimal number of trees","271000d7":"<a id=\"4\"><\/a>\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">SHAP<\/span><\/h1><br>\n\nShap values are calculated for each object in a data set. There is a fix value which is assigned to each object and the features are weighted as per the importance explaining its significance. The sum of the shap values of the features equates to the predictive value(sum of predictions-non probabilistic)\n\nImportance of Shap values:\n\nExplains the global interpretability : the collective SHAP values can show how much each predictor contributes, either positively or negatively, to the target variable. This is like the variable importance plot.\nExplains the local interpretability : each observation gets its own set of SHAP values. This greatly increases its transparency. We can explain why a case receives its prediction and the contributions of the predictors. Traditional variable importance algorithms only show the results across the entire population but not on each individual case.","94e823f6":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">Model Building<\/span><\/h1><br>","24d1bb7b":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">Contents<\/span><\/h1><br>\n\n* [1.Introduction](#1)\n* [2.About Catboost](#2)\n    * [2.1.Tree Structure Type](#2.1)\n    * [2.2.Categorical Feature Support](#2.2)\n    * [2.3.Differentiating from classical boosting](#2.3)\n* [3.Model Building](#3)\n* [4.Shap](#4)","b0997a8c":"### Test results","89376736":"### Train the model with full data","34b01eda":"### Round 3","f4ab6452":"### Scale the data","dd4f4d1c":"<html>\n    <p style='background:Orange; color:white; font-size:30px; padding:7px;text-align:center;border-width: 5px;border-color: coral;border-style: solid'><b>Catboost-Tabular Playground Series<\/b><\/p>\n<\/html>\n\n<img src=\"https:\/\/avatars.mds.yandex.net\/get-bunker\/56833\/dba868860690e7fe8b68223bb3b749ed8a36fbce\/orig\">","b84c80c6":"### Round 2 \nTrain accuracy =0.59 better than Catboost default. Fix iterations=885 and tune other parameters like max_depth.","1747a104":"### Collective feature importance\n\nThe below plot helps to interpret the impact of feature on observation. The feature importance value is in terms of feature importance with highly impacting feature on the top and low impacting feature at the bottom.","fbd64a0a":"### Class 3 SHAP values","5183fcd4":"### Class 4 SHAP values","0f3a3482":"<a id=\"2\"><\/a>\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">About Catboost<\/span><\/h1><br>\n\nCatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It is available as an open source library.It is a powerfull library build by Yandex community. This note book deals with indepth understanding of how to implement the catboost algorithm on the data and improving the accuracy of the model.\n\nFollowing notebook will help you to understand the model trained using Catboost algorithm following which we will also look into model predition as well as some useful features like feature importances and SHAP interpretation.\n\n<a id=\"2.1\"><\/a>\n<b>1. Tree Structure Type <\/b> \n\nCatboost supports the tree to grow as full symmetric binary tree ; i.e. for each level of the tree the split definition will be the same. Using FSBT , the results doesnt change a lot with the parameters. Stability is maintained. Because of which there is not much requirement of parameter tuning and with default value giving good results. Below is the example of full symmetric binary tree.\n\n<img src=\"https:\/\/miro.medium.com\/max\/2008\/1*AjrRnwvBuu-zK8CvEfM29w.png\">\n\n<a id=\"2.2\"><\/a>\n<b>2. Categorical Features Support <\/b>\n\nOne hot encoding : Catboost supports categorical features with converting them to OHE under the hood without manual interventions.\nAverage label value(CAtegorical feature with label value) : Taking the average of 1's across the feature combination and adding a new column with those value; but this leads to data leak and hence the target leakage.\nUsing permuation of data : permuting the records and calculating the average feature value with target label before the object (not including that object)\nCreating the feature combination but in a greedy manner (taking only those with high impactibility) to avoid creating combination with feature having many categories.\n\n<a id=\"2.3\"><\/a>\n<b>3. Differentiating from Classical Boosting <\/b>\n\nDifferentiating from Classical Boosting\nClassical boosting techniques uses the weighted sum of the gradients of the objects in the leaf as an estimate which is prone to overfitting. As the estimate is biased because the tree is making estimate on the same object on which the tree is build.\n\nOrdered Boosting uses the classical permutation of the objects before the leaf on which the tree is build. Making the estimate on the object before the one.\n\n\n<b>Source<b>\nhttps:\/\/catboost.ai\/news\/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus","dd0ecafc":"### Predict on Test","30166cfa":"<a id=\"3\"><\/a>\n","4336696e":"### Insight round 4\nl2_leaf_reg=4 does not improve the Accuracy","6a106669":"### Insight Round 2\nNot improved Accuracy. We will keep depth =6 only. Tune learning rate.\n","c1fea5e3":"### Label Encoding","e5fe0a37":"<hr style=\"width:5\"><\/hr>","966f7818":"### Round 5","d77462c4":"### Round 3\n","3f608785":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">Import required Libraries<\/span><\/h1><br>\n","134a7da2":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:papyrus; font-size:20px; color:#03e8fc; padding:10px\">Introduction<\/span><\/h1><br>\n\n\n<b>About the Data <\/b>\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\n<b>Files<\/b>\n\ntrain.csv - the training data, one product (id) per row, with the associated features (feature_*) and class label (target)\ntest.csv - the test data; you must predict the probability the id belongs to each class\nsample_submission.csv - a sample submission file in the correct format","9590262d":"### Class 1 SHAP values","9c2c9e31":"### Split Data into Train test","618997b4":"### Insight Round 3\nNo imrovement from round 2. Freeze n_estimators as it is and tune l2_leaf_reg","3847cbe5":"<hr style=\"margin-width:10\"><\/hr>","470bcfe3":"### Import Data","3cb0908b":"### Feature importance\n\nThese feature importances are non negative. They are normalized and sum to 1, so you can look on these values like percentage of importance.","97e7a021":"### Check the records","fb2fa373":"### Train results","76c8a1c2":"### Create the Pool object","81e34765":"### Train Results","e3510e10":"### Import Shape","bc352b46":"### Class 2 SHAP values"}}