{"cell_type":{"2d113443":"code","24eb99ae":"code","cb0beefe":"code","a079c327":"code","23d5c00e":"code","37d97375":"code","ac9c4ec8":"code","8e6e3430":"code","ad4af1a8":"code","a0302cf1":"code","f962318f":"code","97c70df8":"code","a1a56126":"code","7a156941":"code","7ff052a1":"code","d922f151":"code","bbf2638d":"code","853bd37e":"code","c0b82be2":"code","98f24b76":"code","2eb22599":"code","7c007c07":"code","92c02ece":"code","e2139eb6":"code","585fecf3":"code","7942949f":"code","d4fb2d9a":"code","a3e745fd":"code","4db5df77":"code","65b4b942":"code","5ec857f9":"code","f086d8b8":"code","f6ae30d2":"code","c55652bd":"code","906ea688":"code","f77330e7":"code","c78a4088":"code","3ab408d4":"code","8867388c":"code","4f6b0229":"code","740f5da3":"code","2687d61c":"code","20efd045":"code","81eba746":"code","cea4e386":"code","c442ea86":"code","59ad493c":"code","e93fd4c9":"code","135f5e8b":"code","8d4ed6df":"code","2011b63f":"code","3ab25a49":"code","9f8926dd":"code","e7e50099":"code","f74aa280":"code","5f525be2":"code","8284abee":"code","262e11a8":"code","1221d03b":"code","9359b53b":"code","4ebfaecb":"code","17754f2c":"markdown","e11f6721":"markdown","ef0c2a05":"markdown","4fe2310d":"markdown","50da298b":"markdown","839e59e1":"markdown","4b40e97f":"markdown","7dda1acb":"markdown","9c9071ab":"markdown","771c3ceb":"markdown","9e37f60d":"markdown","86f7aa07":"markdown","9be00d82":"markdown","af178e60":"markdown","b58bc901":"markdown","2eb938ef":"markdown","502e80a2":"markdown","74ff69b7":"markdown","f7c65b9c":"markdown","af470855":"markdown","39341c6f":"markdown","1d4753eb":"markdown","950013de":"markdown","0deb1f1e":"markdown","07a89975":"markdown","51a4132b":"markdown","83123704":"markdown","a04b7977":"markdown","d9936a5b":"markdown","b2c03998":"markdown","a32f68f4":"markdown","d52ee880":"markdown","e3d0acdd":"markdown","194340fb":"markdown","dd7742b6":"markdown","9e9e4585":"markdown","786c1b79":"markdown","de551f75":"markdown","dce87eb3":"markdown","a0f0de76":"markdown","0099f4a6":"markdown","e5f835a9":"markdown","3054ec72":"markdown","a411d864":"markdown","19a1c1a0":"markdown","5b0ba468":"markdown","bff24320":"markdown","52bbe721":"markdown","e39f9bec":"markdown","0596be63":"markdown","66cf3c8c":"markdown","65768dc4":"markdown","697b0c6f":"markdown","54a5566b":"markdown","baa91761":"markdown","7ca7a9af":"markdown"},"source":{"2d113443":"# importing libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc","24eb99ae":"data = pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\n\ndata.drop(\"sl_no\", axis=1, inplace=True) # Removing Serial Number","cb0beefe":"print(\"Number of rows in data :\",data.shape[0])\nprint(\"Number of columns in data :\", data.shape[1])","a079c327":"data.head()","23d5c00e":"data.info()","37d97375":"# Percentage of null values present in salary column\n\np = data['salary'].isnull().sum()\/(len(data))*100\n\nprint(f\"Salary column has {p.round(2)}% null values.\")","ac9c4ec8":"data.describe()","8e6e3430":"# Let's peek at the object data types seperately\n\ndata.select_dtypes(include=['object']).head()","ad4af1a8":"# getting the object columns\nobject_columns = data.select_dtypes(include=['object']).columns\n\n# iterating over each object type column\nfor col in object_columns:\n    print('-' * 40 + col + '-' * 40 , end='-')\n    display(data[col].value_counts())","a0302cf1":"sns.countplot(\"gender\", data = data)\nplt.show()","f962318f":"# Let's look at more important plot i.e gender vs status (target)\n\nsns.countplot(\"gender\", hue=\"status\", data=data)\nplt.show()","97c70df8":"sns.countplot(\"ssc_b\", data = data)\nplt.show()","a1a56126":"# Let's see the impact of taking a spcific board in 10th grade on placements\n\nsns.set(rc={'figure.figsize':(8.7,5.27)})\n\nsns.countplot(\"ssc_b\", hue=\"status\", data=data)\nplt.show()","7a156941":"# Let's plot percentage vs status to see how much effect they make\n\nsns.barplot(x=\"status\", y=\"ssc_p\", data=data)","7ff052a1":"# Let's see the how much percentage was scored by students in different boards\n\nsns.barplot(x=\"ssc_b\", y=\"ssc_p\", data=data)","d922f151":"# Let's look at how many students opted for central this time?\n\nsns.countplot(\"hsc_b\", data = data)\nplt.show()","bbf2638d":"# Let's see the impact of a spcific board on placements\n\nsns.set(rc={'figure.figsize':(8.7,5.27)})\n\nsns.countplot(\"hsc_b\", hue=\"status\", data=data)\nplt.show()","853bd37e":"# Let's plot percentage vs status to see how much effect they make\n\nsns.barplot(x=\"status\", y=\"hsc_p\", data=data)","c0b82be2":"# Let's see the how much percentage was scored by students in 12th grade in different boards\n\nsns.barplot(x=\"hsc_b\", y=\"hsc_p\", data=data)","98f24b76":"# Let's see what count of students opted for in 12th grade\n\nsns.countplot(\"hsc_s\", data=data)","2eb22599":"# Let's look at how well each specialisation students performed\n\nax = sns.barplot(x=\"hsc_s\", y=\"hsc_p\", data=data)","7c007c07":"# Let's see the impact of taking a spcific branch on placements\n\nsns.countplot(\"hsc_s\", hue=\"status\", data=data)","92c02ece":"# Let's see what count of students opted for what after 12th grade\n\nsns.countplot(\"degree_t\", data=data)","e2139eb6":"# Let's look at how well each field students performed\n\nsns.barplot(x=\"degree_t\", y=\"degree_p\", data=data)","585fecf3":"# Let's see the impact of taking a field on placements\n\nsns.countplot(\"degree_t\", hue=\"status\", data=data)","7942949f":"# Let's see if the work experience impacts on placements or not\n\ndata['status'] = data['status'].map( {'Placed':1, 'Not Placed':0})\n\nsns.barplot(x=\"workex\", y=\"status\", data=data)","d4fb2d9a":"sns.barplot(x=\"status\", y=\"etest_p\", data=data)","a3e745fd":"# Let's see how specialisation effects the placement of candidates\n\nsns.countplot(\"specialisation\", hue=\"status\", data=data)","4db5df77":"sns.barplot(x=\"status\", y=\"mba_p\", data=data)\nplt.title(\"Salary vs MBA Percentage\")","65b4b942":"# Let's look at the distribution of salary\n\nplt.figure(figsize=(10,5))\nsns.distplot(data['salary'], bins=50, hist=False)\nplt.title(\"Salary Distribution\")\nplt.show()","5ec857f9":"sns.barplot(x=\"gender\", y=\"salary\", data=data)\nplt.title(\"Salary vs gender\")","f086d8b8":"sns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"specialisation\"])\nplt.title(\"Salary vs Gender based on specialisation\")","f6ae30d2":"sns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"workex\"])\nplt.title(\"Gender vs Salary based on work experience\")","c55652bd":"sns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"ssc_b\"])\nplt.title(\"Salary vs Gender based on Board in 10th grade\")","906ea688":"sns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"hsc_b\"])\nplt.title(\"Salary vs Gender based on Board in 12th grade\")","f77330e7":"sns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"degree_t\"])\nplt.title(\"Salary vs Gender based on Degree Type\")","c78a4088":"# Dropping useless columns\n\ndata.drop(['ssc_b','hsc_b', 'salary'], axis=1, inplace=True)","3ab408d4":"# Using simple binary mapping on two class categorical variables (gender, workerx, specialisation)\n\ndata[\"gender\"] = data.gender.map({\"M\":0,\"F\":1})\ndata[\"workex\"] = data.workex.map({\"No\":0, \"Yes\":1})\ndata[\"specialisation\"] = data.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})","8867388c":"# Using get dummies for 3 class categorical variables (hsc_s and degree_t)\n\nfor column in ['hsc_s', 'degree_t']:\n    dummies = pd.get_dummies(data[column])\n    data[dummies.columns] = dummies","4f6b0229":"# Now let's clean up the left overs (already encoded so no use now)\n\ndata.drop(['degree_t','hsc_s'], axis=1, inplace=True)","740f5da3":"# Now let us look at the data\n\ndata.head()","2687d61c":"# Let's do a sanity check by peeking at the data\n\ndata.head()","20efd045":"# Let's plot correlation matrix to find out less correlated variable to drop them\n\ncor=data.corr()\nplt.figure(figsize=(14,6))\nsns.heatmap(cor,annot=True)","81eba746":"# From the correlation matrix we can see that some of the features are not much useful like \"Others\" and \"Arts\" which are negatively \n# correlated as well as have low value.\n\n# Another reason to remove these variables is the so called Dummy variable trap which occurs when we do encoding of multiclass features\n\ndata.drop(['Others', 'Arts'], axis=1, inplace=True)","cea4e386":"# target vector\ny = data['status']\n\n# dropping as it is not a predictor\ndata.drop('status', axis = 1, inplace = True)\n\n# scaling the data so as to get rid of any dramatic results during modelling\nsc = StandardScaler()\n\n# predictors\nX = sc.fit_transform(data)\n\n# Let us now split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n\n\nprint(\"X-Train:\",X_train.shape)\nprint(\"X-Test:\",X_test.shape)\nprint(\"Y-Train:\",y_train.shape)\nprint(\"Y-Test:\",y_test.shape)","c442ea86":"# creating our model instance\nlog_reg = LogisticRegression()\n\n# fitting the model\nlog_reg.fit(X_train, y_train)","59ad493c":"# predicting the target vectors\n\ny_pred=log_reg.predict(X_test)","e93fd4c9":"# creating confusion matrix heatmap\n\nconf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","135f5e8b":"# getting precision, recall and f1-score via classification report\n\nprint(classification_report(y_test, y_pred))","8d4ed6df":"# let's look at our accuracy\n\naccuracy = accuracy_score(y_pred, y_test)\n\nprint(f\"The accuracy on test set using Logistic Regression is: {np.round(accuracy, 3)*100.0}%\")","2011b63f":"# plotting the ROC curve\n\nauc_roc = roc_auc_score(y_test, log_reg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, log_reg.predict_proba(X_test)[:,1])\n\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label='Average ROC curve (area = {0:0.3f})'.format(auc_roc))\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', \n         label= 'Average ROC curve (area = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","3ab25a49":"# calculate auc \nauc_score = auc(fpr, tpr)\nprint(f\"Our auc_score came out to be {round(auc_score, 3)}.\")","9f8926dd":"# creating a list of depths for performing Decision Tree\ndepth = list(range(1,10))\n\n# list to hold the cv scores\ncv_scores = []\n\n# perform 10-fold cross validation with default weights\nfor d in depth:\n  dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=d, random_state=42)\n  scores = cross_val_score(dt, X_train, y_train, cv=10, scoring='accuracy', n_jobs = -1)\n  cv_scores.append(scores.mean())\n\n# finding the optimal depth\noptimal_depth = depth[cv_scores.index(max(cv_scores))]\nprint(\"The optimal depth value is: \", optimal_depth)","e7e50099":"# plotting accuracy vs depth\nplt.plot(depth, cv_scores)\nplt.xlabel(\"Depth of Tree\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs depth Plot\")\nplt.grid()\nplt.show()\n\nprint(\"Accuracy scores for each depth value is : \", np.round(cv_scores, 3))","f74aa280":"# create object of classifier\ndt_optimal = DecisionTreeClassifier(criterion=\"gini\", max_depth=optimal_depth, random_state=42)\n\n# fit the model\ndt_optimal.fit(X_train,y_train)\n\n# predict on test vector\ny_pred = dt_optimal.predict(X_test)\n\n# evaluate accuracy score\naccuracy = accuracy_score(y_test, y_pred)*100\nprint(f\"The accuracy on test set using optimal depth = {optimal_depth} is {np.round(accuracy, 3)}%\")","5f525be2":"# creating a list of our models\nensembles = [log_reg, dt_optimal]\n\n# Train each of the model\nfor estimator in ensembles:\n    print(\"Training the\", estimator)\n    estimator.fit(X_train,y_train)","8284abee":"# Find the scores of each estimator\n\nscores = [estimator.score(X_test, y_test) for estimator in ensembles]\n\nscores","262e11a8":"# Training a voting classifier with hard voting and using logistic regression and decision trees as estimators\n\nfrom sklearn.ensemble import VotingClassifier\n\nnamed_estimators = [\n    (\"log_reg\",log_reg),\n    (\"dt_tree\", dt_optimal),\n\n]","1221d03b":"# getting an instance for our Voting classifier\n\nvoting_clf = VotingClassifier(named_estimators)","9359b53b":"# fit the classifier\n\nvoting_clf.fit(X_train,y_train)","4ebfaecb":"# Let's look at our accuracy\nacc = voting_clf.score(X_test,y_test)\n\nprint(f\"The accuracy on test set using voting classifier is {np.round(acc, 4)*100}%\")","17754f2c":"# Exploratory data analysis","e11f6721":"### ssc_b (Board of Education - Secondary)","ef0c2a05":"<h3>OBSERVATIONS:<\/h3>\n\n**(I)** There is **count of central board students is very high as compared to all other boards**.It might be because **central board is easy**.\n\n**(II)** The count of placed students from central board is little more than others category which doesn't say much.\n","4fe2310d":"### degree_t & degree_p (Degree Type and Degree percentage)","50da298b":"**Looks like max_depth = 8 is good for our model**\n\n**Lets plot some graph to see what was the trend of our accuracies.**","839e59e1":"**Our data is now completely encoded. GOOD JOB!**","4b40e97f":"**This doesn't look awesome but isn't bad either so let's trust our CV and use the optimal max_depth = 8 to train our model.**","7dda1acb":"## Exploring the columns","9c9071ab":"We can see that getting good percentages in **MBA does not guarantee placement** of the candidate.","771c3ceb":"<h3>OBSERVATION:<\/h3>\n\nWe can see that getting good percentages in **employability test does not guarantee placement** of the candidate.","9e37f60d":"# Model Prediction\n\n**Problem:** Predict whether the candidate will be placed or not based on some predictors.\n\n**Nature of Problem:** As the target is a binary data thus it is a **binary classification problem.**","86f7aa07":"**We have 7 columns with real values and 8 with object datatype**\n\n**It is clear that only salary has null columns. Let's see how much?**","9be00d82":"### hsc_s (Specialization in Higher Secondary Education) ","af178e60":"### etest_p (Employability test percentage)","b58bc901":"### Gender (Male, Female)","2eb938ef":"**We didn't do anything awesome yet. We just made a list of both of our previously trained classifiers. Let's add some awesomeness!**","502e80a2":"<h3>OBSERVATIONS:<\/h3>\n\n**(I)** The number of **male students are almost double as compared to female**.<br>\n**(II)** As the fraction of placed vs not placed for female candidates is significantly low as compared to male candidates thus we can conclude **male candidates are accepted more often than female.**","74ff69b7":"**Looking at the precision, recall and f1_score we can saw that our Logistic Regression model did fairly well!**","f7c65b9c":"**We achieved 86% accuracy which is similiar to what we achieved using logistic regression so they seem to work equally well.**\n\n**What if we could combine the power of our two heroes to get a superhero? Sounds weird? It is!**","af470855":"<h3>OBSERVATIONS:<\/h3>\n\n**(I)** Salary column for male candidates seems to have more outliers than females which means that a lot **more male candidates got more than the average CTC.**\n\n**(II) Mean salary is somewhere around 220k**.\n\n**(III) Mkt&Fin students are given higher salaries as compared to Mkt&HR.**","39341c6f":"<h3>OBSERVATIONS:<\/h3>\n\n\n**(I)** Percentage in secondary education has a clear impact on placements.**Higher percentage candidates have a very good chance of getting placed**.\n\n**(II)** Looks like there is not impace of boards on percentages of students.","1d4753eb":"### hsc_p (Percentage- 12th Grade)","950013de":"### hsc_b (Board of Education - Higher)","0deb1f1e":"### ssc_p (Secondary Education - 10th grade)","07a89975":"<h3>OBSERVATION:<\/h3>\n\nLooks like except for hsc_s and degree_t with 3 classes, all other have 2 classes each and also we can see that this data is slightly imbalanced as we have 148 placed students and 67 not placed students.","51a4132b":"<h3>OBSERVATIONS:<\/h3>\n\n    \n**(I)** The **most popular branch turns out to be commerce** or maybe as most of students get average marks so they were admitted to got commerce on based of their marks. **Science is the second most popular and the least popular is arts.**\n\n**(II)** **Almost every branch students performed equally but commerce students have slightly better score than other two.**\n\n**(III)** Looking at the fraction of placed and not placed we can say that **science branch students have more chance of getting placed than commerce students and most around 45% of the students in arts are not placed**","83123704":"<b>We have a total of 8 columns with non-integer or float data. Let's now look at the amount of classes (unique values) these columns.<b>","a04b7977":"### Splitting the data ","d9936a5b":"### Some Insights:\n\nOur confusion Matrix looks decent. We have correctly predicted 42 (placed) + 14 (not-placed) correct predictions and 7 (not placed as placed) + 2(placed as not-placed) incorrect predictions.\n\nWe need to decrease these incorrect predictions because a good candidate can be rejected (false positive) [Type I error] and a unfit candidate can be selected (false negatives) [Type II Error]. ","b2c03998":"### mba_p (MBA percentage)","a32f68f4":"### Encoding\n\nWe have **gender, hsc_s, degree_t, workex and specialisation** as categorical so let's encode them.","d52ee880":"<h3>OBSERVATIONS:<\/h3>\n\n**(I)** Work Experience is a clear indicator as **more work experience results in higher CTC jobs.**\n\n**(II) The maximum salary in male candidates with experience is >1M and for female it is ~700k.\nThe maximum salary in male candidates without experience is ~550k and for female it is ~430k.**","e3d0acdd":"### Salary ","194340fb":"<h3>OBSERVATIONS:<\/h3>\n\n\n**(I)** Percentage in higher secondary education also has a clear impact on placements. **Higher percentage candidates have a very good chance of getting placed**.<br>\n\n**(II) Board isn't a determinant in defining how much precentage students score.**<br>\n\nThus, it turns out that a piece of paper can definately decide your future atleast for placements, so study hard!","dd7742b6":"**Looking at the distribution we can say that the most of the students get a package between 200k-400k and most salaries above 400,000 are outliers.**","9e9e4585":"###  Decision Tree Classifier\n\n**Let's try some decision trees now and see how well they perform but as Decision trees are easy to overfit so I will use K-FOLD CV first to find the best depth.**","786c1b79":"<h3>OBSERVATION:<\/h3>\n\n\n**Male candidates are making more money as compared to female candidates.**","de551f75":"**This means that around 31% candidates were not placed which is sad but let's see what were the reasons :)**","dce87eb3":"### Ensemble Modelling\n\n**We will train a voting classifier using our previously trained logistic regeression and Decision tree model**","a0f0de76":"<h3>OBSERVATION:<\/h3>\n\nMale candidates from Central board got higher CTC as compared to other boards whereas this was totally opposite in case of female candidates thus there is not much guarantee that either of the board will fetch higher CTCs.","0099f4a6":"<h3>OBSERVATION:<\/h3>\n\nCompanies prefer candidates with work experience so the **students with internships and past job experience have better chances of being placed.**","e5f835a9":"**We achieved 86% without doing any tuning so it means we did preprocessing steps really well. Great!**","3054ec72":"### workex (Work Experience)","a411d864":"### Logistic Regression\n\n**Let's apply logistic regression as it is a classification algorithm that works well with binary data.**","19a1c1a0":"<h3>OBSERVATION:<\/h3>\n\nSpecialisation is a clear indicator in placements. Compared to MktandFin, **Mkt&HR students have low placements**. This might be because there is low requirements for HR in a company. ","5b0ba468":"## Thanks for reading!","bff24320":"**We went from 86.4% to 92.3% accuracy score!**\n\n**If that isn't amazing I don't know what is.**","52bbe721":"# Preprocessing","e39f9bec":"<ol>\n<li>More male candidates got placed as compared to female candidates.<\/li>\n<li>Male Candidates got higher CTCs as compared to female candidates.<\/li>\n<li>Type of Board choosen does not have any effect on placements thus we can drop in preprocessing steps.<\/li>\n<li>Most of the students preferred Central board in 10th grade whereas other boards in 12th grade.<\/li>\n<li>Candidates with higher percentages have better chance of placements.<\/li>\n<li>Choosing Science and Commerce as Specialisation seems to have perk when it comes to placments.<\/li>\n<li>Maximum package was bagged by male candidate from Mkt&Fin branch which is around 940k.<\/li>\n<li>Commerce is the most popular branch among candidates.<\/li>\n<li>Mean CTC is around 220k for male and female candidates individually.<\/li>\n<li>Choosing Sci&Tech and Comm&Mngmt as degree will fetch you higher CTCs.<\/li>\n<li>Mkt&Fin major have higher salaries and more placement chance as compared to Mkt&HR.<\/li>\n<li>Employability test percentage and MBA percentage does not effect the placements<\/li>","0596be63":"<h3>OBSERVATION:<\/h3>\n\n\n**(I)** The students opted for following fields:\n\n<ol>\n<li>Science and Technology (must be science students)<\/li>\n<li>Commerce and management (might be a mixture of commerce and Arts)<\/li>\n<li>Others<\/li>\n<\/ol>\n\n**(II)** There is **not much difference in performace of students from Science and Commerce** but there but **students who opted for \"Others\" have low performance**\n\n**(III)** Looks like **Commerce and Science degree students are preffered by companies** which is obvious. **Students who opted for Others have very low placement chance.**","66cf3c8c":"<h3>OBSERVATIONS:<\/h3>\n\n\n**(I)** Looks like **more number of candidates opted for others for 12th grade as compared to 10th grade.**\n\n**(II)** This time also not much difference between in the fraction of placed candidates in respective boards. Thus, **board doesn't matter in placements**.","65768dc4":"### specialisation (Post Grad - MBA)","697b0c6f":"**The data doesn't have any missing values except for salary (not useful - we will see why) so there is not much data cleaning but we will have to do encoding of categorical variables, note that - target(status) is already been encoded during EDA.**\n\n**Before that, we will drop both secondary and higher secondary boards as discussed in conclusions of EDA.**\n\n**We will also remove salary column as it is clearly depends on whether the candidate got placed or not so it is not at all useful and can fool us by giving 100%. (BIG MISTAKE!)**","54a5566b":"<h3>OBSERVATION:<\/h3>\n\n\nBoth Male and Female candidates from Central board got higher CTC as compared to other boards thus we can that central board in 10th grade might fetch you higher CTCs.","baa91761":"<h3>OBSERVATIONS:<\/h3>\n\n**(I)** Both male and female candidate got high CTCs choosing Comm&Mgmt as their degree.\n\n**(II)** Male candidates from Sci&Tech got high CTCs as compared to Female candidates.\n\n**(III)** None of the male candidates got placed from \"Others\" category whereas for female candidates the package is close to what female Sci&Tech candidates got.","7ca7a9af":"# Conclusions Drawn"}}