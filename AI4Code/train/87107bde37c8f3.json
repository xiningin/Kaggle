{"cell_type":{"23ac7c6a":"code","6087f38b":"code","1f5badcf":"code","aa11610d":"code","b974f8e1":"code","e8eb7236":"code","2c244efb":"code","89376bf7":"code","8a85ceb4":"code","52c185a3":"code","26090682":"code","8c410cdd":"code","ab904bef":"code","8c73844a":"code","7d4ecb38":"code","74e8a6bd":"code","0905b47d":"code","da6f8a35":"code","d9d6933c":"code","3554fe45":"code","186dfca7":"code","21e18f12":"code","fdb2ec44":"code","1a4a8db8":"code","6abfad26":"code","6a9fc50c":"code","36a5a05e":"code","f98fb4be":"code","e1666c20":"code","f58b4e57":"code","8902035b":"code","c8962057":"code","a9d7a38d":"code","3bc805a4":"code","947f0b89":"code","54c51e75":"code","b8212f5b":"code","3653c544":"code","caeafff0":"code","adcedb07":"code","941eb504":"code","2bbe005f":"code","ea0caba2":"code","22b5312a":"markdown","183d0beb":"markdown","f1f360e3":"markdown","11f66d30":"markdown","5abe2ef4":"markdown","f34615a5":"markdown","531c35b0":"markdown","8a94bb83":"markdown","e29fc919":"markdown","35a067b7":"markdown","f8bd6c91":"markdown","6e5cb4ea":"markdown","c0dc78ce":"markdown","35a36d19":"markdown","c04f6233":"markdown","e7171044":"markdown","665b937d":"markdown","90b1420a":"markdown","2e17ae85":"markdown","8b82b246":"markdown","c681c0cc":"markdown","0807c965":"markdown","77b63e07":"markdown","16dd4d13":"markdown","0b8874b4":"markdown","2ab840ef":"markdown"},"source":{"23ac7c6a":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\nimport plotly.express as px\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n \n# to display the float values upto 6 decimal places     \npd.options.display.float_format = '{:.6f}'.format\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import StandardScaler to perform scaling\n\nfrom sklearn.preprocessing import StandardScaler \n\n# import various functions from sklearn\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\n\n# import the functions for visualizing the decision tree\nimport pydotplus\nfrom IPython.display import Image  ","6087f38b":"pip install pydotplus","1f5badcf":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","aa11610d":"df.head()","b974f8e1":"df.info()","e8eb7236":"z = df.columns\nprint(z)","2c244efb":"sns.distplot(df['Pregnancies'],hist=False)\nplt.show()","89376bf7":"df['Pregnancies'].skew() #it is moderately skewed to the right","8a85ceb4":"df['Pregnancies'].kurt() # it has leptokurtic distribution\n","52c185a3":"plt.boxplot(df['Pregnancies'])\nplt.show()","26090682":"df[df['Pregnancies']>13]   # The three outlier in the pregnencies column","8c410cdd":"sns.countplot(df['Pregnancies'])\nplt.show()","ab904bef":"sns.distplot(df['Glucose'])\nplt.show()\n\n","8c73844a":"df['Glucose'].skew()   # [-0.5,0.5]  -->approximately symmetrical","7d4ecb38":"df['Glucose'].kurt()# it has gor a leptokurtic distribution","74e8a6bd":"plt.boxplot(df['Glucose'])\nplt.show()\n","0905b47d":"sns.distplot(df['BloodPressure']) ","da6f8a35":"df['BloodPressure'].skew() # blood presssure is a little left skewed","d9d6933c":"df[df['BloodPressure']==0]","3554fe45":"for i in df.columns[3:-1]:\n    plt.subplots()\n    sns.distplot(df[i])\n    print('\\n')\n    print(f\"the skewness of {i} is {df[i].skew()}\")\n    print('\\n')\n    print(f\" the kurtosis of {i} is {df[i].kurt()}\")\n    ","186dfca7":"plt.figure(figsize=(8,8))\nfig = px.pie(df,names='Outcome')\nfig.show()","21e18f12":"plt.figure(figsize=(16,6))\nsns.lineplot(y=df['Age'],x=df['BloodPressure'],hue=df['Outcome'])\nplt.show()\n","fdb2ec44":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()\n","1a4a8db8":"plt.figure(figsize=(16,6))\nsns.lineplot(y = df['Age'],x = df['Pregnancies'],hue=df['Outcome'])","6abfad26":"sns.swarmplot(y = df['Age'],x=df['Outcome'])","6a9fc50c":"plt.figure(figsize=(9,7))\nsns.scatterplot(x = df['Age'],y=df['Insulin'],hue=df['Outcome'])","36a5a05e":"plt.figure(figsize=(9,7))\nsns.scatterplot(x = df['BloodPressure'],y=df['Insulin'],hue=df['Outcome'])","f98fb4be":"df[(df[\"BloodPressure\"]==0) & ((df['Insulin']==0) & (df['Outcome']==1))]","e1666c20":"len(df[(df[\"BloodPressure\"]==0) & ((df['Insulin']==0) & (df['Outcome']==1))]) ","f58b4e57":"df.head(1)","8902035b":"X = df.drop(['Outcome'],axis=1)\n\ny = df['Outcome']","c8962057":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3,random_state=1)","a9d7a38d":"# create a generalized function to calculate the metrics values for test set\ndef get_test_report(model):\n    \n    # for test set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data \n    test_pred = model.predict(X_test)\n\n    # return the performace measures on test set\n    return(classification_report(y_test, test_pred))\n\n\n\n# create a generalized function to calculate the metrics values for train set\ndef get_train_report(model):\n    \n    # for test set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data \n    train_pred = model.predict(X_train)\n\n    # return the performace measures on test set\n    return(classification_report(y_train, train_pred))","3bc805a4":"# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion\n# pass the 'random_state' to obtain the same samples for each time you run the code\n\ndecision_tree_classification = DecisionTreeClassifier( random_state = 10)\n\n# fit the model using fit() on train data\ndecision_tree = decision_tree_classification.fit(X_train, y_train)","947f0b89":"# save the column names in 'labels'\nlabels = X_train.columns\n\n# export a decision tree in DOT format\n# pass the 'dt_model' to export it to Graphviz\n# pass the column names to 'feature_names'\n# pass the required class labels to 'class_names'\ndot_data = tree.export_graphviz(decision_tree_classification, feature_names = labels, class_names = [\"0\",\"1\"],filled=True, rounded=True,\n                special_characters=True )\n\n# plot the decision tree using DOT format in 'dot_data'\ngraph = pydotplus.graph_from_dot_data(dot_data)  \n\n# display the decision tree\nImage(graph.create_png())\n\n# double-click on the image below to get an expanded view","54c51e75":"print(get_train_report(decision_tree))","b8212f5b":"# create a dictionary with hyperparameters and its values\n# pass the criteria 'entropy' and 'gini' to the parameter, 'criterion' \n# pass the range of values to 'max_depth' that assigns maximum depth of the tree\n# 'max_features' assigns maximum number of features to consider for the best split. We pass the string 'sqrt' and 'log2'\n# 'sqrt' considers maximum number of features equal to the square root of total features\n# 'log2' considers maximum number of features equal to the log of total features with base 2\n# pass the range of values to 'min_samples_split' that assigns minimum number of samples to split an internal node\n# pass the range of values to 'min_samples_leaf' that assigns minimum number of samples required at the terminal\/leaf node\n# pass the range of values to 'max_leaf_nodes' that assigns maximum number of leaf nodes in the tree\ntuned_paramaters = [{'criterion': ['entropy', 'gini'], \n                     'max_depth': range(2, 10),\n                     'max_features': [\"sqrt\", \"log2\"],\n                     'min_samples_split': range(2,10),\n                     'min_samples_leaf': range(1,10),\n                     'max_leaf_nodes': range(2, 10)}]\n \n# instantiate the 'DecisionTreeClassifier' \n# pass the 'random_state' to obtain the same samples for each time you run the code\ndecision_tree_classification = DecisionTreeClassifier(random_state = 10)\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the decision tree classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\ntree_grid = GridSearchCV(estimator = decision_tree_classification, \n                         param_grid = tuned_paramaters, \n                         cv = 5)\n\n# fit the model on X_train and y_train using fit()\ntree_grid_model = tree_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for decision tree classifier: ', tree_grid_model.best_params_, '\\n')","3653c544":"# instantiate the 'DecisionTreeClassifier'\n# 'best_params_' returns the dictionary containing best parameter values and parameter name  \n# 'get()' returns the value of specified parameter\n# pass the 'random_state' to obtain the same samples for each time you run the code\ndt_model = DecisionTreeClassifier(criterion = tree_grid_model.best_params_.get('criterion'),\n                                  max_depth = tree_grid_model.best_params_.get('max_depth'),\n                                  max_features = tree_grid_model.best_params_.get('max_features'),\n                                  max_leaf_nodes = tree_grid_model.best_params_.get('max_leaf_nodes'),\n                                  min_samples_leaf = tree_grid_model.best_params_.get('min_samples_leaf'),\n                                  min_samples_split = tree_grid_model.best_params_.get('min_samples_split'),\n                                  random_state = 10)\n\n# use fit() to fit the model on the train set\ndt_model = dt_model.fit(X_train, y_train)","caeafff0":"# print the performance measures for train set for the model with best parameters\n# call the function 'get_test_report'\n# pass the decision tree using GridSearch to the function\nprint('Classification Report for train set: \\n', get_train_report(dt_model))","adcedb07":"# save the column names in 'labels'\nlabels = X_train.columns\n\n# export a decision tree in DOT format\n# pass the 'dt_model' to export it to Graphviz\n# pass the column names to 'feature_names'\n# pass the required class labels to 'class_names'\ndot_data = tree.export_graphviz(dt_model, feature_names = labels, class_names = [\"0\",\"1\"])  \n\n# plot the decision tree using DOT format in 'dot_data'\ngraph = pydotplus.graph_from_dot_data(dot_data)  \n\n# display the decision tree\nImage(graph.create_png())\n\n# double-click on the image below to get an expanded view","941eb504":"# create a dictionary with hyperparameters and its values\n# pass the criteria 'entropy' and 'gini' to the parameter, 'criterion' \n# pass a list of values to 'n_estimators' to build the different number of trees in the random forest\n# pass a list of values to 'max_depth' that assigns maximum depth of the tree\n# 'max_features' assigns maximum number of features to consider for the best split. We pass the string 'sqrt' and 'log2'\n# 'sqrt' considers maximum number of features equal to the square root of total features\n# 'log2' considers maximum number of features equal to the log of total features with base 2\n# pass a list of values to 'min_samples_split' that assigns minimum number of samples to split an internal node\n# pass a list of values to 'min_samples_leaf' that assigns minimum number of samples required at the terminal\/leaf node\n# pass a list of values to 'max_leaf_nodes' that assigns maximum number of leaf nodes in the tree\ntuned_paramaters = [{'criterion': ['entropy', 'gini'],\n                     'n_estimators': [10, 30, 50, 70, 90],\n                     'max_depth': [10, 15, 20],\n                     'max_features': ['sqrt', 'log2'],\n                     'min_samples_split': [2, 5, 8, 11],\n                     'min_samples_leaf': [1, 5, 9],\n                     'max_leaf_nodes': [2, 5, 8, 11]}]\n \n# instantiate the 'RandomForestClassifier' \n# pass the 'random_state' to obtain the same samples for each time you run the code\nrandom_forest_classification = RandomForestClassifier(random_state = 10)\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the random forest classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\nrf_grid = GridSearchCV(estimator = random_forest_classification, \n                       param_grid = tuned_paramaters, \n                       cv = 5)\n\n# use fit() to fit the model on the train set\nrf_grid_model = rf_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for random forest classifier: ', rf_grid_model.best_params_, '\\n')","2bbe005f":"# instantiate the 'RandomForestClassifier'\n# 'best_params_' returns the dictionary containing best parameter values and parameter name  \n# 'get()' returns the value of specified parameter\n# pass the 'random_state' to obtain the same samples for each time you run the code\nrf_model = RandomForestClassifier(criterion = rf_grid_model.best_params_.get('criterion'), \n                                  n_estimators = rf_grid_model.best_params_.get('n_estimators'),\n                                  max_depth = rf_grid_model.best_params_.get('max_depth'),\n                                  max_features = rf_grid_model.best_params_.get('max_features'),\n                                  max_leaf_nodes = rf_grid_model.best_params_.get('max_leaf_nodes'),\n                                  min_samples_leaf = rf_grid_model.best_params_.get('min_samples_leaf'),\n                                  min_samples_split = rf_grid_model.best_params_.get('min_samples_split'),\n                                  random_state = 10)\n\n# use fit() to fit the model on the train set\nrf_model = rf_model.fit(X_train, y_train)\n\n# print the performance measures for test set for the model with best parameters\nprint('Classification Report for test set:\\n', get_test_report(rf_model))","ea0caba2":"# create a dataframe that stores the feature names and their importance\n# 'feature_importances_' returns the features based on the gini importance\nimportant_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': rf_model.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","22b5312a":"### Understanding thre relationship bwtween 2 variables \n\n1. Bivariate analysis\n2. Multivariate analysis","183d0beb":"### The last column","f1f360e3":"### let's take first column under analysis ","11f66d30":"####  it is showing a highly posotive correlation between Age and Pregnancies","5abe2ef4":"#### from here we csn see pregnancy with value 2 has the highest frequqncy whereas 14,15,17 has the lowest","f34615a5":"#  splitting the data","531c35b0":"### the boxplot is showing the outlier below the lower whisker! ie, zero.","8a94bb83":"# 2 Glucose","e29fc919":"# 3 .Blood Pressure!","35a067b7":"##  Tune the Hyperparameters using GridSearchCV (Random Forest)\u00b6","f8bd6c91":"### Build the model using the tuned hyperparameters.","6e5cb4ea":"#### Let's understand the trend between two variables!","c0dc78ce":"## Identify the Important Features\nLet us create a barplot to identify the important feature in the dataset.\n\nThe method feature_importances_ returns the value corresponding to each feature which is defined as the ratio of total decrease in Gini impurity across every tree in the forest where the feature is used to the total count of trees in the forest. This is also caled as, Gini Importance.\n\nThere is another accuracy-based method. It calculates the average decrease in the accuracy calculated on the out-of-bag samples, with and without shuffling the variable across all the trees in the random forest. The out-of-bag samples are the samples in the training dataset which are not considered whild building a tree.","35a36d19":"## importing the libraries","c04f6233":"## 3.1 Tune the Hyperparameters using GridSearchCV (Decision Tree)\n\nHyperparameters are the parameters in the model that are preset by the user. GridSearch considers all the combinations of hyperparameters and returns the best hyperparameter values. We pass some of the hyperparameters in the decision tree to the GridSearchCV() and build the tree using the optimal values obtained using GridSearch method.","e7171044":"### pretty messy tree right?\n\n1. Advantage of decision tree is that it explains everything in a layman terms! ","665b937d":"# The dataset has 65.10% as non diabetic and 34.90 as diabetic","90b1420a":"#### there are 35 entires with blood pressure =0\n#### If the blood pressure then falls completely to zero this means the heart is no longer beating or is in fibrillation. The patient is then clinically dead and permanent brain damage will occur within minutes unless appropriate steps are taken to restore the blood pressure. If this does not occur then biological death will soon follow.\n\n#### what we can do?\n\n1. Either we should drop the rows associated with  zero blood pressure or we can replace the blood pressure value with mean or median\n2. The most robust approach would be Applying a mchine learning algorithm to predict the Continuous values of blood pressure!\n3. Regession would do the job!\n\n","2e17ae85":"#### normal blood pressure is at approx. 127\/79 mmHg in men, and 122\/77 mmHg in women","8b82b246":"## Decision Tree for Classification\n\nDecision Tree is a non-parametric supervised learning method. It builds a model in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets, which is called splitting. A decision node is a node on which a decision of split is to be made. A node that can not be split further is known as the terminal\/leaf node. A leaf node represents the decision. A decision tree can work with both numerical and categorical variables.\n\nA decision tree for classification is built using criteria like the Gini index and entropy.\n\n# Gini Index\nGini index measures the probability of the sample being wrongly classified. The value of the Gini index varies between 0 and 1. We choose the variable with a low Gini index. The Gini index of the variable is calculated as:\n\n                                             \ud835\udc3a\ud835\udc56\ud835\udc5b\ud835\udc56=1\u2212\u2211\ud835\udc5b\ud835\udc56=1\ud835\udc5d2\ud835\udc56\n\nWhere,\n\ud835\udc5d\ud835\udc56: Probability of occurrence of the class 'i'\n\n# Entropy\nEntropy is one of the criteria used to build the decision tree. It calculates the heterogeneity of the sample. The entropy is zero if the sample is completely homogeneous, and it is equal to 1 if the sample is equally divided. Entropy of the variable 'X' is calculated as:\n\n                                            \ud835\udc38(\ud835\udc4b)=\u2212\u2211\ud835\udc50\ud835\udc56=1\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc542\ud835\udc5d\ud835\udc56\n\nWhere,\n\ud835\udc5d\ud835\udc56: Probability of occurrence of the class 'i'\n\nAnd the conditional emtropy of the variable is given as:\n\n                                            \ud835\udc38(\ud835\udc47,\ud835\udc4b)=\u2211\ud835\udc50\ud835\udf16\ud835\udc4b\ud835\udc43(\ud835\udc50)\ud835\udc38(\ud835\udc50)\n\nWhere,\n\ud835\udc43(\ud835\udc50): Probability of occurrence of the class 'c'\n\ud835\udc38(\ud835\udc50): Entropy of the class 'c'\n\nThe information gain is the difference between the entropy of the target variable and the entropy of the target variable given an independent variable. We split the on the variable that corresponds to the highest information gain.\n\nBuild a full decision tree model on a train dataset using 'entropy'.\u00b6","c681c0cc":"#### With swarmplot we can identify problems like at age 50 how many people going though diabetes","0807c965":"#### from the graph the outlier seems to be abover the upper whisker!","77b63e07":"#### The data is showing something very intersting information, like people having zero insulin  aslo have Diabetes!\n","16dd4d13":"## Multivariate Analysis","0b8874b4":"##  Clear overfit!!!\n\n#### find the best parameters","2ab840ef":"#### Plotting distribution of columns alltogether!"}}