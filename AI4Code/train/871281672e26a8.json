{"cell_type":{"8d07713b":"code","1887f66b":"code","a9544ee2":"code","654917c1":"code","95079d18":"code","c96946bf":"code","123e5cd2":"code","32b20fa1":"code","3fee9821":"code","dbd1f1c6":"code","262ad66a":"code","c7181c2c":"code","99030494":"code","fe3f3bb8":"code","601b922f":"code","33560145":"code","bb29b6be":"code","7759d0b6":"code","9c1cad6e":"code","47eee3af":"code","07d183d6":"code","f8f398ee":"code","523db16c":"code","d08f0c36":"code","698d5d3d":"markdown","a4dea76e":"markdown","ff92e40e":"markdown","3b81b522":"markdown","e3b09a3b":"markdown","c1ab90bb":"markdown","60eac8cc":"markdown","8e92e594":"markdown","5e0b2b51":"markdown","8bbb95fb":"markdown","aca092ab":"markdown"},"source":{"8d07713b":"# General imports\n\nimport numpy as np # I don't think I used numpy at all, but I'll keep it here\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%pylab inline","1887f66b":"# Import scikit-learn libraries here\n\nfrom sklearn.model_selection import train_test_split\n\"\"\"\nThis is for splitting the training and testing data\n\nIf you train your model on all of the data,\nit might get really good at predicting, but only on that dataset\nbecause it can simply memorize the data. By splitting the data\ninto a train, and a test set, we can score the model on the test set\nand get a more reflective result compare to real world data\n\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\n\"\"\"\nA RandomForest model is a cluster of DecisionTree models,\ncommitting votes amongst its trees to determine the final results.\nHowever, RandomForests tend to overfit, meaning being really good at\npredicting only on that dataset, so tuning its hyperparameters is necessary.\n\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\"\"\"\nThat brings us to GridSearchCV. This lets us look through various combinations\nof hyperparameters and determine the best one.\n\"\"\"","a9544ee2":"# Load the data, and set the \"Id\" column as index\n# .read_csv() allows us to parse through a .csv file and return a pandas Dataframe\ndata = pd.read_csv(\"..\/input\/iris\/Iris.csv\", index_col=\"Id\")\n# The .head() method allows us to peek at the first 5 rows of a Dataframe\n# If you want to look at the final rows, you can use .tail()\ndata.head()","654917c1":"# Load proper columns into X\n# The columns used for the model to infer info is traditionally called X\nX_labels = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\nX = data[X_labels]\nX.head()","95079d18":"# Target column is traditionally named y\ny = data[\"Species\"]\n# Because it only contains one column, this is a pandas Series, not a Dataframe\ny.head()","c96946bf":"# The .describe method allows us to see a Dataframe's statistical values\ndata.describe()","123e5cd2":"# The .unique() methods return an array of unique values in a Series\ny.unique()","32b20fa1":"# The .groupby() function is quite interesting\n# It lets us group rows depending on their unique values in a column\n# Then perform a statistical operation on it like .median(), .std()\ndata.groupby([\"Species\"]).mean()","3fee9821":"# X.corr() returns the correlation values of every pair of data in a Dataframe\nsns.heatmap(X.corr(), annot=True)","dbd1f1c6":"sns.pairplot(data=data, hue=\"Species\")","262ad66a":"# We now divide the Length by the Width, then round it to the nearest hundred for the new ratio column\ndata[\"SepalWidthLengthRatioCm\"] = round(data[\"SepalLengthCm\"] \/ data[\"SepalWidthCm\"], 2)\n# Attach it to the X labels\nX_labels.append(\"SepalWidthLengthRatioCm\")\nX = data[X_labels]\n# Then drop the other two columns as they're no longer necessary\nX = X.drop([\"SepalLengthCm\", \"SepalWidthCm\"], axis=1)","c7181c2c":"sns.swarmplot(data=data, x=\"Species\", y=\"SepalWidthLengthRatioCm\")","99030494":"# We use a lambda along with ternary operators to get our results\ndata[\"SpeciesEncoded\"] = data[\"Species\"].apply(lambda x: 1 if x ==\"Iris-setosa\" else 2 if x ==\"Iris-versicolor\" else 3)\n# Set y to the new encoded column\ny = data[\"SpeciesEncoded\"]\ndata.head()","fe3f3bb8":"data[\"SpeciesEncoded\"].unique()","601b922f":"X.head()","33560145":"y.head()","bb29b6be":"# The random_state is for the randomization process to be predictable\n# Meaning it'll randomize the same for every run, ensuring a stable result\nrf = RandomForestClassifier(random_state=0)","7759d0b6":"# We split the data here\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nprint(\"There are {} samples in the training set and {} samples in the test set\".format(X_train.shape[0], X_test.shape[0]))","9c1cad6e":"grid_params  = {\n    'bootstrap' : [True, False],\n    'n_estimators' : list(range(10,101,10)),\n    'criterion' : ['gini', 'entropy'],\n    'min_samples_leaf' : list(range(1,10,2)),\n    'max_features' : ['sqrt', 'log2']\n}","47eee3af":"grid_search = GridSearchCV(estimator=rf, param_grid=grid_params, cv=3, verbose=1)","07d183d6":"grid_search.fit(X_train, y_train)","f8f398ee":"# Here are the best parameters that GridSearch found\ngrid_search.best_params_","523db16c":"final_model = grid_search\n# It scored a ~97% accuracy, pretty good!\nfinal_model.score(X_test, y_test)\n\"\"\"\nBe wary with high accuracy results though. \nYes, it can mean that your model is really good,\nor it can mean that your model is really good on that one dataset.\nWhenever you are suspicious, try tuning the hyperparameters again,\nor test it out on real data.\n\"\"\"","d08f0c36":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(final_model, X_test, y_test)","698d5d3d":"Aha! Now there's more of a distinct relationship. It is still pretty much impossible to differ between a Versicolor and a Virginica though, but we still have the Petals to infer from.\n\nAnyhow, now we'll convert the Species to its integer form for easier training.\nSectosa = 1\nVersicolor = 2\nVirginica = 3","a4dea76e":"# Data visualization\n\nLet's begin with a correlation heatmap","ff92e40e":"# Machine Learning\n\nNow comes the fun part. We'll use a RandomForestClassifier","3b81b522":"# This will be the Iris classifier replica\n\nFirst we begin with general imports, ML imports and load the data","e3b09a3b":"Got the ratio! Now we try to plot it down.","c1ab90bb":"# Data Manipulation","60eac8cc":"GridSearchCV time.","8e92e594":"# Understanding Data\n\nThrough statistical operations","5e0b2b51":"On the Sepal Length scale, it is Sectosa < Versicolor < Virginica, and it seems to be the same case for Petal Length. On the Petal Width, the scale is pretty much the same, at Sectosa < Versicolor < Virginica. However, on the Sepal Width, it is Versicolor ~ Virginica < Sectosa.\n\nAnyhow, we can spot very clear distinctions in almost every pair scatter, except for SepalWidth:SepalLength. Perhaps we can try to make it into something else? A ratio?\n","8bbb95fb":"Seems good, let's carry on!","aca092ab":"We can see that the Iris setosa species have **very** small petals overall, while the other two species are rather hard to predict."}}