{"cell_type":{"b7d038f5":"code","17a1cb35":"code","e418847c":"code","9c7c1530":"code","b377f65b":"code","18b97f55":"code","21a1af1c":"code","5ee06195":"code","87410e29":"code","73f24cb6":"code","4b540fbf":"code","0f1d3766":"code","15c30a5b":"code","76c03b5e":"code","63029eec":"code","b3df73b2":"code","2edf63e9":"code","2caeeae2":"code","5778899b":"code","537ef8bd":"code","29cf5d2a":"code","aa8ff3ff":"code","7e49cfdd":"code","1078d0c8":"code","00794d2c":"code","d7e26664":"code","7b5065c2":"code","efc6d592":"code","2c31e4cc":"code","447ae210":"code","3b9d2f0e":"code","6f8a5b25":"code","6f9e8f8b":"code","b6662f17":"code","cf22319f":"code","ae65d4c2":"code","03db20f2":"code","26a7c39c":"code","316d0a16":"code","df48fd0c":"code","dda8fcfc":"code","23250188":"markdown","1f309fee":"markdown","c1037c4d":"markdown","64b515cc":"markdown","fb3816b7":"markdown"},"source":{"b7d038f5":"import sys\nsys.path.append(\"..\/input\/iterative-stratification\/iterative-stratification-master\")\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport gc\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n \nimport warnings\nwarnings.filterwarnings('ignore')","17a1cb35":"data_dir = '..\/input\/lish-moa\/'\nos.listdir(data_dir)","e418847c":"train_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ntrain_targets_scored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\")\ntrain_targets_nonscored = train_targets_nonscored[train_features[\"cp_type\"] != \"ctl_vehicle\"]\ntest_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")\ndrug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","9c7c1530":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))","b377f65b":"!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/\n!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.metrics import Metric","18b97f55":"SEED = 42\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(seed_value=SEED)\n\n\n# Function to map and filter control groups\ndef mapping_filter(train, train_targets, test):\n    \n    # Encode time_dose variable for train and test\n    time_dose = {(24, \"D1\"): 1., (24, \"D2\"): 2., (48, \"D1\"): 3., (48, \"D2\"): 4., (72, \"D1\"): 5., (72, \"D2\"): 6.}\n    for df in [train, test]:\n        df[\"time_dose\"] = df[[\"cp_time\", \"cp_dose\"]].apply(lambda row: (row[\"cp_time\"], row[\"cp_dose\"]), axis=1)\n        df[\"time_dose\"] = df[\"time_dose\"].map(time_dose)\n    \n    # Select the ctl samples in the train and test\n    ctl_train = train[train[\"cp_type\"] == \"ctl_vehicle\"]\n    ctl_test  = test[test[\"cp_type\"] == \"ctl_vehicle\"]\n    \n    # remove observation with 'cp_type' == 'ctl_vehicle'\n    train_targets = train_targets[train[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n    train = train[train['cp_type'] != \"ctl_vehicle\"].reset_index(drop=True)\n    test  = test[test['cp_type'] != \"ctl_vehicle\"].reset_index(drop=True)\n    \n    \n    # Delete unnecessary columns\n    del train[\"cp_type\"], test[\"cp_type\"]\n    gc.collect()\n    return train, test, train_targets, ctl_train, ctl_test\n\n\n\n# Function to extract common stat features\ndef fe_stats(train, test):\n    \n    # Seperate the gene and cell viability\n    g_list = [col for col in train.columns if \"g-\" in col]\n    c_list = [col for col in train.columns if \"c-\" in col]\n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n    \n    # Put all statistical features in dataframe\n    fe_train = pd.DataFrame(); fe_test = pd.DataFrame()\n    fe_train[\"sig_id\"] = train[\"sig_id\"]; fe_test[\"sig_id\"] = test[\"sig_id\"]\n    for df in [(fe_train, train), (fe_test, test)]:\n        df[0][\"g_mean\"]    = df[1][g_list].mean(axis=1)\n        df[0][\"g_var\"]     = df[1][g_list].var(axis=1)\n        df[0][\"g_kurt\"]    = df[1][g_list].kurtosis(axis=1)\n        df[0][\"g_skew\"]    = df[1][g_list].skew(axis=1)\n        df[0][\"g_extent\"]  = df[1][g_list].max(axis=1) - df[1][g_list].min(axis=1)\n        df[0][\"g_mad\"]     = df[1][g_list].mad(axis=1)\n        df[0][\"g_median\"]  = df[1][g_list].median(axis=1)\n        df[0][\"g_inter_q\"] = df[1][g_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1)\n        df[0][\"g_AV_a\"]    = df[1][g_list].quantile(0.25, axis=1) - 1.5*(df[1][g_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1))\n        df[0][\"g_AV_b\"]    = df[1][g_list].quantile(0.75, axis=1) + 1.5*(df[1][g_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1))\n        df[0][\"g_rsd\"]     = df[1][g_list].std(axis=1) \/ df[1][g_list].mean(axis=1)\n        df[0][\"c_mean\"]    = df[1][c_list].mean(axis=1)\n        df[0][\"c_var\"]     = df[1][c_list].var(axis=1)\n        df[0][\"c_kurt\"]    = df[1][c_list].kurtosis(axis=1)\n        df[0][\"c_skew\"]    = df[1][c_list].skew(axis=1)\n        df[0][\"c_extent\"]  = df[1][c_list].max(axis=1) - df[1][g_list].min(axis=1)\n        df[0][\"c_mad\"]     = df[1][c_list].mad(axis=1)\n        df[0][\"c_median\"]  = df[1][c_list].median(axis=1)\n        df[0][\"c_inter_q\"] = df[1][c_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1)\n        df[0][\"c_AV_a\"]    = df[1][c_list].quantile(0.25, axis=1) - 1.5*(df[1][g_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1))\n        df[0][\"c_AV_b\"]    = df[1][c_list].quantile(0.75, axis=1) + 1.5*(df[1][g_list].quantile(0.75, axis=1) - df[1][g_list].quantile(0.25, axis=1))\n        df[0][\"c_rsd\"]     = df[1][c_list].std(axis=1) \/ df[1][c_list].mean(axis=1)\n        \n        # Multiplication of c_cells\n        df[0]['c52_c42'] = df[1]['c-52'] * df[1]['c-42']\n        df[0]['c13_c73'] = df[1]['c-13'] * df[1]['c-73']\n        df[0]['c26_c13'] = df[1]['c-23'] * df[1]['c-13']\n        df[0]['c33_c6']  = df[1]['c-33'] * df[1]['c-6']\n        df[0]['c11_c55'] = df[1]['c-11'] * df[1]['c-55']\n        df[0]['c38_c63'] = df[1]['c-38'] * df[1]['c-63']\n        df[0]['c38_c94'] = df[1]['c-38'] * df[1]['c-94']\n        df[0]['c13_c94'] = df[1]['c-13'] * df[1]['c-94']\n        df[0]['c4_c52']  = df[1]['c-4']  * df[1]['c-52']\n        df[0]['c4_c42']  = df[1]['c-4']  * df[1]['c-42']\n        df[0]['c13_c38'] = df[1]['c-13'] * df[1]['c-38']\n        df[0]['c55_c2']  = df[1]['c-55'] * df[1]['c-2']\n        df[0]['c55_c4']  = df[1]['c-55'] * df[1]['c-4']\n        df[0]['c4_c13']  = df[1]['c-4']  * df[1]['c-13']\n        df[0]['c82_c42'] = df[1]['c-82'] * df[1]['c-42']\n        df[0]['c66_c42'] = df[1]['c-66'] * df[1]['c-42']\n        df[0]['c6_c38']  = df[1]['c-6']  * df[1]['c-38']\n        df[0]['c2_c13']  = df[1]['c-2']  * df[1]['c-13']\n        df[0]['c62_c42'] = df[1]['c-62'] * df[1]['c-42']\n        df[0]['c90_c55'] = df[1]['c-90'] * df[1]['c-55']\n        \n        \n        for feature in c_list:\n             df[0][f'{feature}_squared'] = df[1][feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[0][f'{feature}_squared'] = df[1][feature] ** 2   \n    \n    return fe_train, fe_test\n\n\ndef ctl_fe_stats(train, test, ctl_train, ctl_test):\n    \n    # Seperate the gene and cell viability\n    g_list = [col for col in train_features.columns if \"g-\" in col]\n    c_list = [col for col in train_features.columns if \"c-\" in col]\n    \n    # Put all statistical features in dataframe\n    ctl_fe_train = pd.DataFrame(); ctl_fe_test = pd.DataFrame()\n    ctl_fe_train[\"sig_id\"] = train_features[\"sig_id\"]; ctl_fe_test[\"sig_id\"] = test_features[\"sig_id\"]\n    ctl_fe_train[\"time_dose\"] = train_features[\"time_dose\"]; ctl_fe_test[\"time_dose\"] = test_features[\"time_dose\"]\n\n    \n          \n    # Stat features with ctl_samples\n    for df in [(ctl_fe_train, ctl_train, train), (ctl_fe_test, ctl_test, test)]:\n        ctl_g_mean = df[1].groupby(\"time_dose\")[g_list].mean().mean(axis=1)\n        ctl_c_mean = df[1].groupby(\"time_dose\")[c_list].mean().mean(axis=1)\n        df[0][\"diff_g_mean\"] = df[2][\"g_mean\"] - df[0][\"time_dose\"].map(ctl_g_mean)\n        df[0][\"diff_c_mean\"] = df[2][\"c_mean\"] - df[0][\"time_dose\"].map(ctl_c_mean)\n        ctl_g_var = df[1].groupby(\"time_dose\")[g_list].var().var(axis=1)\n        ctl_c_var = df[1].groupby(\"time_dose\")[c_list].var().var(axis=1)\n        df[0][\"diff_g_var\"] = df[2][\"g_var\"] - df[0][\"time_dose\"].map(ctl_g_var)\n        df[0][\"diff_c_var\"] = df[2][\"c_var\"] - df[0][\"time_dose\"].map(ctl_c_var)\n        ctl_g_extent = df[1].groupby(\"time_dose\")[g_list].max().max(axis=1) - df[1].groupby(\"time_dose\")[g_list].min().min(axis=1)\n        ctl_c_extent = df[1].groupby(\"time_dose\")[c_list].max().max(axis=1) - df[1].groupby(\"time_dose\")[c_list].max().max(axis=1)\n        df[0][\"diff_g_extent\"] = df[2][\"g_extent\"] - df[0][\"time_dose\"].map(ctl_g_extent)\n        df[0][\"diff_c_extent\"] = df[2][\"c_extent\"] - df[0][\"time_dose\"].map(ctl_c_extent)\n        ctl_g_mad = df[1].groupby(\"time_dose\")[g_list].mad().mad(axis=1)  \n        ctl_c_mad = df[1].groupby(\"time_dose\")[c_list].mad().mad(axis=1) \n        df[0][\"diff_g_mad\"] = df[2][\"g_mad\"] -  df[0][\"time_dose\"].map(ctl_g_mad)\n        df[0][\"diff_c_mad\"] = df[2][\"c_mad\"] -  df[0][\"time_dose\"].map(ctl_c_mad)\n        ctl_g_median = df[1].groupby(\"time_dose\")[g_list].median().median(axis=1)  \n        ctl_c_median = df[1].groupby(\"time_dose\")[c_list].median().median(axis=1) \n        df[0][\"diff_g_median\"] = df[2][\"g_median\"] - df[0][\"time_dose\"].map(ctl_g_median)\n        df[0][\"diff_c_median\"] = df[2][\"c_median\"] - df[0][\"time_dose\"].map(ctl_c_median)\n        ctl_g_inter_q = df[1].groupby(\"time_dose\")[g_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[g_list].quantile(0.25).quantile(0.25, axis=1)   \n        ctl_c_inter_q = df[1].groupby(\"time_dose\")[c_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[c_list].quantile(0.25).quantile(0.25, axis=1)\n        df[0][\"diff_g_inter_q\"] = df[2][\"g_inter_q\"] - df[0][\"time_dose\"].map(ctl_g_inter_q)\n        df[0][\"diff_c_inter_q\"] = df[2][\"c_inter_q\"] - df[0][\"time_dose\"].map(ctl_c_inter_q)\n        ctl_g_AV_a = df[1].groupby(\"time_dose\")[g_list].quantile(0.25).quantile(0.25, axis=1) - 1.5*(df[1].groupby(\"time_dose\")[g_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[g_list].quantile(0.25).quantile(0.25, axis=1))  \n        ctl_c_AV_a = df[1].groupby(\"time_dose\")[c_list].quantile(0.25).quantile(0.25, axis=1) - 1.5*(df[1].groupby(\"time_dose\")[c_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[c_list].quantile(0.25).quantile(0.25, axis=1)) \n        df[0][\"diff_g_AV_a\"] = df[2][\"g_AV_a\"] - df[0][\"time_dose\"].map(ctl_g_AV_a)\n        df[0][\"diff_c_AV_a\"] = df[2][\"c_AV_a\"] - df[0][\"time_dose\"].map(ctl_c_AV_a)\n        ctl_g_AV_b = df[1].groupby(\"time_dose\")[g_list].quantile(0.75).quantile(0.75, axis=1) - 1.5*(df[1].groupby(\"time_dose\")[g_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[g_list].quantile(0.25).quantile(0.25, axis=1))  \n        ctl_c_AV_b = df[1].groupby(\"time_dose\")[c_list].quantile(0.75).quantile(0.75, axis=1) + 1.5*(df[1].groupby(\"time_dose\")[c_list].quantile(0.75).quantile(0.75, axis=1) - df[1].groupby(\"time_dose\")[c_list].quantile(0.25).quantile(0.25, axis=1)) \n        df[0][\"diff_g_AV_b\"] = df[2][\"g_AV_b\"] - df[0][\"time_dose\"].map(ctl_g_AV_b)\n        df[0][\"diff_c_AV_b\"] = df[2][\"c_AV_b\"] - df[0][\"time_dose\"].map(ctl_c_AV_b)\n        ctl_g_rsd = df[1].groupby(\"time_dose\")[g_list].std().std(axis=1) \/ df[1].groupby(\"time_dose\")[g_list].mean().mean(axis=1) \n        ctl_c_rsd = df[1].groupby(\"time_dose\")[c_list].std().std(axis=1) \/ df[1].groupby(\"time_dose\")[c_list].mean().mean(axis=1)\n        df[0][\"diff_g_rsd\"] = df[2][\"g_rsd\"] - df[0][\"time_dose\"].map(ctl_g_AV_b)\n        df[0][\"diff_c_rsd\"] = df[2][\"c_rsd\"] - df[0][\"time_dose\"].map(ctl_c_AV_b)\n        \n    del ctl_fe_train[\"time_dose\"], ctl_fe_test[\"time_dose\"]   \n    gc.collect()\n    return ctl_fe_train, ctl_fe_test","21a1af1c":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ntrain_features[GENES+CELLS] = qt.fit_transform(train_features[GENES+CELLS])\ntest_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])","5ee06195":"# Data preparation for the training phase\ntrain_features, test_features, train_features_scored, ctl_train, ctl_test= mapping_filter(train_features, train_targets_scored, test_features)\nfe_train_features, fe_test_features = fe_stats(train_features, test_features)\nctl_fe_train, ctl_fe_test = ctl_fe_stats(fe_train_features, fe_test_features, ctl_train, ctl_test)\n#pca_feat_train, pca_feat_test = pca_features(train_features, test_features)\n# Merge all the engineered features for train and test data\ntrain_features = pd.merge(train_features, fe_train_features, on=\"sig_id\")\ntrain_features = pd.merge(train_features, ctl_fe_train, on=\"sig_id\")\n#train_features = pd.merge(train_features, pca_feat_train, on='sig_id')\n\ntest_features = pd.merge(test_features, fe_test_features,    on=\"sig_id\")\ntest_features = pd.merge(test_features, ctl_fe_test, on=\"sig_id\")\n#test_features = pd.merge(test_features, pca_feat_test,       on=\"sig_id\")","87410e29":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","73f24cb6":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","4b540fbf":"c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_time', 'cp_dose', 'time_dose']]\nmask = (train_features[c_n].var() >= 0.85).values\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_dose', 'cp_time']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_dose', 'cp_time']], tmp], axis=1)","0f1d3766":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\ntrain  = train_features.merge(train_targets_scored, on=\"sig_id\")\ntrain  = train.merge(train_targets_nonscored, on=\"sig_id\")\ntrain = train.merge(drug, on=\"sig_id\", how=\"left\")\ntrain_5layers = train.copy()\ntrain_4layers = train.copy()\ntrain_3layers = train.copy()\ntrain_tabnet = train.copy()\ntrain_blend = train.copy()\ntarget = train[target_cols]\ntest = test_features.copy()","15c30a5b":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    data = data.drop([\"cp_dose_D1\", \"cp_time_72\"], axis=1)\n    return data","76c03b5e":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_all_targets = len(all_target_cols)\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_features = len(feature_cols)\nnum_features","63029eec":"print(train.shape)\nprint(test.shape)\nprint(sample_submission.shape)","b3df73b2":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","2edf63e9":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n      \n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","2caeeae2":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass NormalLinear(nn.Module):\n    \"\"\" \n    Linear layer with normalized weights\n    \"\"\"\n    def __init__(self, size_in, size_out, bias=True):\n        super().__init__()\n        self.size_in, self.size_out = size_in, size_out\n        # weights vector\n        weights_v = torch.Tensor(size_out, size_in)\n        nn.init.kaiming_uniform_(weights_v, a=np.sqrt(5)) \n        self.weights_v = nn.Parameter(weights_v)\n        # weights magnitude\n        weights_m = torch.norm(weights_v, dim=1, keepdim=True)\n        self.weights_m = nn.Parameter(weights_m)\n\n        if bias:\n            bias_v = torch.Tensor(size_out)    \n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weights_v)\n            bound = 1 \/ np.sqrt(fan_in)\n            nn.init.uniform_(bias_v, -bound, bound)\n            self.bias = nn.Parameter(bias_v)\n        else:\n            self.register_parameter('bias', None)\n\n        self._normalize_weights()\n\n    def _normalize_weights(self):\n        with torch.set_grad_enabled(False):\n            norm_per_output = torch.norm(self.weights_v, dim=1, keepdim=True)\n            self.weights_v.div_(norm_per_output)\n\n    def forward(self, x):\n        self._normalize_weights()\n        return nn.functional.linear(x, self.weights_v * self.weights_m, self.bias)\n\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","5778899b":"class NNet_5Layers(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(NNet_5Layers, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.45, 0.25, 0.2, 0.15]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = NormalLinear(self.hidden_size[3], num_targets)\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n","537ef8bd":"class FineTuneScheduler_5Layers:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new, last_layer):\n        self.frozen_layers = []\n\n        model_new = NNet_5Layers(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = int(name.split('.')[0][-1])\n\n            if layer_index == last_layer:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = NormalLinear(model_new.hidden_size[-1], num_targets_new)\n        model_new = model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","29cf5d2a":"class NNet_4Layers(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(NNet_4Layers, self).__init__()\n        self.hidden_size = [1500, 1250, 1000]\n        self.dropout_value = [0.35, 0.2, 0.15]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = NormalLinear(self.hidden_size[2], num_targets)\n\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n\n        return x","aa8ff3ff":"class FineTuneScheduler_4Layers:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new, last_layer):\n        self.frozen_layers = []\n\n        model_new = NNet_4Layers(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = int(name.split('.')[0][-1])\n\n            if layer_index == last_layer:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm4 = nn.BatchNorm1d(model_new.hidden_size[2])\n        model_new.dropout4 = nn.Dropout(model_new.dropout_value[2])\n        model_new.dense4 = NormalLinear(model_new.hidden_size[-1], num_targets_new)\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","7e49cfdd":"class NNet_3Layers(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(NNet_3Layers, self).__init__()\n        self.hidden_size = [1500, 1250]\n        self.dropout_value = [0.35, 0.2]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = NormalLinear(self.hidden_size[1], num_targets)\n\n      \n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n\n        return x","1078d0c8":"class FineTuneScheduler_3Layers:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new, last_layer):\n        self.frozen_layers = []\n\n        model_new = NNet_3Layers(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = int(name.split('.')[0][-1])\n\n            if layer_index == last_layer:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm3 = nn.BatchNorm1d(model_new.hidden_size[1])\n        model_new.dropout3 = nn.Dropout(model_new.dropout_value[1])\n        model_new.dense3 = NormalLinear(model_new.hidden_size[-1], num_targets_new)\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","00794d2c":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 1e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 1e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","d7e26664":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEED, NFOLDS):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[(vc==6)|(vc==7)|(vc==8)|(vc==11)|(vc==12)|(vc==13)|(vc==14)|(vc==18)].index.sort_values()\n    vc2 = vc.loc[(vc!=6)&(vc!=7)&(vc!=8)&(vc!=11)&(vc!=12)&(vc!=13)&(vc!=14)&(vc!=18)].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}\n    dct2 = {}\n\n    skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n                                        random_state=SEED)\n    tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n    for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n        dd = {k: fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf =  KFold(n_splits=NFOLDS, shuffle=True, \n                     random_state=SEED)\n    tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n    for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n        dd = {k: fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    train[\"kfold\"] = train.drug_id.map(dct1)\n    train.loc[train[\"kfold\"].isna(), \"kfold\"] = train.loc[train[\"kfold\"].isna(), 'sig_id'].map(dct2)\n    train[\"kfold\"] = train[\"kfold\"].astype('int8')\n        \n    return train\n\nNFOLDS = 14\n\ntrain = make_cv_folds(train, SEED, NFOLDS)\nfolds = train.copy()\ntrain.head()","7b5065c2":"def training_5layersnnet(fold, last_layer, seed):\n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    trn_idx = train_[train_[\"kfold\"] != fold].index\n    val_idx = train_[train_[\"kfold\"] == fold].index\n    \n    train_df = train_[train_[\"kfold\"] != fold].reset_index(drop=True)\n    valid_df = train_[train_[\"kfold\"] == fold].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.0001)\n        \n      \n\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler_5Layers(EPOCHS)\n\n    pretrained_model = NNet_5Layers(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n    \n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = NNet_5Layers(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets, last_layer)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = NNet_5Layers(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","efc6d592":"def training_4layersnnet(fold, last_layer, seed):\n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    trn_idx = train_[train_[\"kfold\"] != fold].index\n    val_idx = train_[train_[\"kfold\"] == fold].index\n    \n    train_df = train_[train_[\"kfold\"] != fold].reset_index(drop=True)\n    valid_df = train_[train_[\"kfold\"] == fold].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.0001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler_4Layers(EPOCHS)\n\n    pretrained_model = NNet_4Layers(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = NNet_4Layers(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets, last_layer)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = NNet_4Layers(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","2c31e4cc":"def training_3layersnnet(fold, last_layer, seed):\n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    trn_idx = train_[train_[\"kfold\"] != fold].index\n    val_idx = train_[train_[\"kfold\"] == fold].index\n    \n    train_df = train_[train_[\"kfold\"] != fold].reset_index(drop=True)\n    valid_df = train_[train_[\"kfold\"] == fold].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.0001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler_3Layers(EPOCHS)\n\n    pretrained_model = NNet_3Layers(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = NNet_3Layers(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets, last_layer)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = NNet_3Layers(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","447ae210":"from time import time\n\n# Averaging on multiple SEEDS\noof_5layers = np.zeros((len(train), len(target_cols)))\npredictions_5layers = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor fold in range(NFOLDS):\n        oof_, pred_ = training_5layersnnet(fold, 5, SEED)\n        predictions_5layers += pred_ \/ NFOLDS\n        oof_5layers += oof_\n\ntime_diff = time() - time_begin\n\ntrain_5layers[target_cols] = oof_5layers","3b9d2f0e":"from time import time\n\n# Averaging on multiple SEEDS\noof_4layers = np.zeros((len(train), len(target_cols)))\npredictions_4layers = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor fold in range(NFOLDS):\n        oof_, pred_ = training_4layersnnet(fold, 4, SEED)\n        predictions_4layers += pred_ \/ NFOLDS\n        oof_4layers += oof_\n\ntime_diff = time() - time_begin\n\ntrain_4layers[target_cols] = oof_4layers","6f8a5b25":"from time import time\n\n# Averaging on multiple SEEDS\noof_3layers = np.zeros((len(train), len(target_cols)))\npredictions_3layers = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor fold in range(NFOLDS):\n        oof_, pred_ = training_3layersnnet(fold, 3, SEED)\n        predictions_3layers += pred_ \/ NFOLDS\n        oof_3layers += oof_\n\ntime_diff = time() - time_begin\n\ntrain_3layers[target_cols] = oof_3layers","6f9e8f8b":"folds = process_data(folds)\ntest  = process_data(test)\ncategorical_columns = ['cp_dose_D2', 'cp_time_24', 'cp_time_48']\ncat_idxs = [i for i, f in enumerate(feature_cols) if f in categorical_columns]\ncat_dims = [len(folds[f].unique()) for f in feature_cols if f in categorical_columns]","b6662f17":"MAX_EPOCH=200\ntabnet_params = dict(n_d=25, n_a=30, cat_idxs=cat_idxs, cat_dims=cat_dims, \n                     n_steps=1, gamma=1.4239102813435471, n_shared=3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=10,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=2,\n                     seed=SEED,\n                     )","cf22319f":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits + 1e-15) + y_true*np.log(logits + 1e-15)\n        return np.mean(-aux)","ae65d4c2":"X_test = test[feature_cols].values\noof_tabnet = np.zeros((len(train), len(target_cols)))\npredictions_tabnet = np.zeros((len(test), len(target_cols)))\nscores_tabnet = []\n\nfor fold in range(NFOLDS):\n    \n    print(\"FOLD %i\"%(fold+1), ' ', end='')\n    X_train, y_train = folds.loc[folds.kfold != fold, feature_cols].values, folds.loc[folds.kfold != fold, target_cols].values\n    X_val_idx = folds.loc[folds.kfold == fold].index\n    X_val, y_val = folds.loc[folds.kfold == fold, feature_cols].values, folds.loc[folds.kfold == fold, target_cols].values\n    model = TabNetRegressor(**tabnet_params)\n    \n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=32,\n              num_workers=0, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=SmoothBCEwLogits(smoothing=5e-5))\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)\n    ## save oof to compute the CV later\n    oof_tabnet[X_val_idx] = preds\n    scores_tabnet.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    predictions_tabnet += (1 \/ (1 + np.exp(-preds_test))) \/ NFOLDS\n\ntrain_tabnet[target_cols] = oof_tabnet\nprint(f\"Average CV : {np.mean(scores_tabnet)}\")\nprint(f\"Std CV : {np.std(scores_tabnet)}\")","03db20f2":"def show_weights(results):\n    print(\"\\n\")\n    for i, w in enumerate(results.x):\n        print(f\"weights_{i}: {w}\")\n        \ndef sanity_check():\n    # All probabilities have to be between 0 and 1.\n    if ((blend_func(res.x) > 0) & (blend_func(res.x) < 1)).all():\n        print('\\nAll probabilities are between 0 and 1. \\n    Good to go!')\n    else:\n        print('\\nProbabilities are not between 0 and 1! \\nS    Something is wrong!')\n        \n# CPMP's logloss from https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/183010\ndef log_loss_numpy(y_pred):\n    y_true_ravel = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n    return loss.mean()\n\ndef func_numpy_metric(weights):\n    oof_blend = np.tensordot(weights, oofs, axes = ((0), (0)))\n    return log_loss_numpy(oof_blend)","26a7c39c":"from scipy.optimize import minimize\n\n#valid_results_nnet   = train_targets_scored.drop(columns=target_cols).merge(train_nnet[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n#valid_results_tabnet = train_targets_scored.drop(columns=target_cols).merge(train_tabnet[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true        = train_features_scored[target_cols].values\n#y_pred_nnet   = valid_results_nnet[target_cols].values\n#y_pred_tabnet = valid_results_tabnet[target_cols].values\n\noofs = np.array([oof_5layers, oof_4layers, oof_3layers, oof_tabnet])\n\ntol = 1e-10\ninit_guess = [1 \/ oofs.shape[0]] * oofs.shape[0]\nbnds = [(0, 1) for _ in range(oofs.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1, \n        'jac': lambda x: [1] * len(x)}\n\nprint('Inital Blend OOF:', func_numpy_metric(init_guess))\nstart_time = time()\nres_scipy = minimize(fun = func_numpy_metric, \n                     x0 = init_guess, \n                     method = 'SLSQP', \n                     bounds = bnds, \n                     constraints = cons,\n                     options={'disp': True,\n                        'maxiter': 100000},\n                     tol = tol)\n#print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)","316d0a16":"valid_results_5layers   = train_targets_scored.drop(columns=target_cols).merge(train_5layers[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nvalid_results_4layers   = train_targets_scored.drop(columns=target_cols).merge(train_4layers[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nvalid_results_3layers   = train_targets_scored.drop(columns=target_cols).merge(train_3layers[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nvalid_results_tabnet    = train_targets_scored.drop(columns=target_cols).merge(train_tabnet[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ntrain_blend[target_cols] = res_scipy.x[0]*oof_5layers + res_scipy.x[1]*oof_4layers + res_scipy.x[2]*oof_3layers + res_scipy.x[3]*oof_tabnet \nvalid_blend = train_targets_scored.drop(columns=target_cols).merge(train_blend[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true         = train_targets_scored[target_cols].values\ny_pred_5layers = valid_results_5layers[target_cols].values\ny_pred_4layers = valid_results_4layers[target_cols].values\ny_pred_3layers = valid_results_3layers[target_cols].values\ny_pred_tabnet  = valid_results_tabnet[target_cols].values\ny_pred_blend   = valid_blend[target_cols].values\n\nscore_5layers = 0\nscore_4layers = 0\nscore_3layers = 0\nscore_tabnet = 0\nscore_blend = 0\nfor i in range(len(target_cols)):\n    score_5layers_ = log_loss(y_true[:, i], y_pred_5layers[:, i]) \n    score_4layers_ = log_loss(y_true[:, i], y_pred_4layers[:, i]) \n    score_3layers_ = log_loss(y_true[:, i], y_pred_3layers[:, i]) \n    tabnet_score_ = log_loss(y_true[:, i], y_pred_tabnet[:, i]) \n    score_blend_ = log_loss(y_true[:, i], y_pred_blend[:, i])\n    score_5layers += score_5layers_ \/ target.shape[1]\n    score_4layers += score_4layers_ \/ target.shape[1]\n    score_3layers += score_3layers_ \/ target.shape[1]\n    score_tabnet += tabnet_score_ \/ target.shape[1]\n    score_blend += score_blend_ \/ target.shape[1]\n    \nprint(\"CV nnet_5layers log_loss: \", score_5layers)\nprint(\"CV nnet_4layers log_loss: \", score_4layers)\nprint(\"CV nnet_3layers log_loss: \", score_3layers)\nprint(\"CV tabnet log_loss: \", score_tabnet)\nprint(\"CV blend loss: \", score_blend)","df48fd0c":"test[target_cols] = res_scipy.x[0]*predictions_5layers + res_scipy.x[1]*predictions_4layers + res_scipy.x[2]*predictions_3layers + res_scipy.x[3]*predictions_tabnet\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","dda8fcfc":"sub.shape","23250188":"# Dataset Classes","1f309fee":"# Single fold training","c1037c4d":"# Preprocessing steps","64b515cc":"# Model","fb3816b7":"# feature Selection using Variance Encoding"}}