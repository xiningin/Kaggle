{"cell_type":{"13c5ec79":"code","041bdbbf":"code","857e309a":"code","d9394819":"code","0ab113a9":"code","41dad4b0":"code","a08ccc99":"code","7c5b61e0":"code","936f936f":"code","4490aefe":"code","05e00ee8":"code","fb52bd2a":"code","084ef426":"code","a11e7a06":"code","5ce5b82a":"code","11f8dfdc":"code","06e8af44":"code","805f06b9":"markdown"},"source":{"13c5ec79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","041bdbbf":"\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\nimport seaborn as sns\nfrom matplotlib import rcParams","857e309a":"train = pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\nsubmit = pd.read_csv(\"\/kaggle\/input\/fake-news\/submit.csv\")","d9394819":"train.head()","0ab113a9":"train[\"label\"].value_counts()","41dad4b0":"rcParams[\"figure.figsize\"] = 10,8\nsns.countplot(x = train[\"label\"])","a08ccc99":"test=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+test['text']\ntrain['total']=train['title']+' '+train['author']+train['text']","7c5b61e0":"transformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(train['total'].values)\ntfidf = transformer.fit_transform(counts)","936f936f":"targets = train['label'].values\ntest_counts = count_vectorizer.transform(test['total'].values)\ntest_tfidf = transformer.fit_transform(test_counts)","4490aefe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)\n","05e00ee8":"Extr = ExtraTreesClassifier(n_estimators=5,n_jobs=4)\nExtr.fit(X_train, y_train)\nprint('Accuracy of ExtrTrees classifier on training set: {:.2f}'\n     .format(Extr.score(X_train, y_train)))\nprint('Accuracy of Extratrees classifier on test set: {:.2f}'\n     .format(Extr.score(X_test, y_test)))","fb52bd2a":"from sklearn.tree import DecisionTreeClassifier\n\nAdab= AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=5)\nAdab.fit(X_train, y_train)\nprint('Accuracy of Adaboost classifier on training set: {:.2f}'\n     .format(Adab.score(X_train, y_train)))\nprint('Accuracy of Adaboost classifier on test set: {:.2f}'\n     .format(Adab.score(X_test, y_test)))\n","084ef426":"\n\nRando= RandomForestClassifier(n_estimators=5)\n\nRando.fit(X_train, y_train)\nprint('Accuracy of randomforest classifier on training set: {:.2f}'\n     .format(Rando.score(X_train, y_train)))\nprint('Accuracy of randomforest classifier on test set: {:.2f}'\n     .format(Rando.score(X_test, y_test)))\n\n","a11e7a06":"from sklearn.naive_bayes import MultinomialNB\n\nNB = MultinomialNB()\nNB.fit(X_train, y_train)\nprint('Accuracy of NB  classifier on training set: {:.2f}'\n     .format(NB.score(X_train, y_train)))\nprint('Accuracy of NB classifier on test set: {:.2f}'\n     .format(NB.score(X_test, y_test)))","5ce5b82a":"\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e5)\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Lasso classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Lasso classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))\n\n","11f8dfdc":"\n\ntargets = train['label'].values\nlogreg = LogisticRegression()\nlogreg.fit(counts, targets)\n\nexample_counts = count_vectorizer.transform(test['total'].values)\npredictions = logreg.predict(example_counts)\npred=pd.DataFrame(predictions,columns=['label'])\npred['id']=test['id']\npred.groupby('label').count()\n\n","06e8af44":"pred.to_csv('countvect5.csv', index=False)","805f06b9":"## There is a equal number of distribution of target feature"}}