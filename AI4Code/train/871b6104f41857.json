{"cell_type":{"5a81e2bd":"code","6338922f":"code","48bdf4fc":"code","54063543":"code","b0ed7fbe":"code","fe3ed471":"code","f0a61b30":"code","676e42c6":"code","2439f23d":"code","0fb7bbde":"code","5146fbd2":"code","d559ec03":"code","68a05f86":"code","24cef572":"code","033a0602":"code","cfb8119a":"code","b6084137":"code","e34b94d3":"code","090d9e5c":"code","031c0de0":"code","d7d6b978":"code","aceb6b36":"code","dd979fe2":"code","284333f0":"code","69e07455":"code","40a35fd3":"code","6eeaaca9":"code","32090c36":"code","b14a5343":"code","7a7b1ba4":"code","d0eb240f":"code","26cfabab":"code","e7c528ba":"code","64a312ea":"code","200b2a70":"code","da6b20b2":"code","5c7b46fb":"code","dbe2a137":"code","3061aa55":"code","311d0b86":"code","6004cbf9":"code","febe4009":"code","6302caed":"code","c459b0c6":"code","fa79f937":"code","605db3e8":"code","5528cbb6":"code","bd8d81e9":"code","52c87974":"code","5668a578":"code","dea81332":"code","335fef7a":"markdown","45980ff2":"markdown","f49e0604":"markdown","9d8adbad":"markdown","d19b55d1":"markdown","72f74821":"markdown","fbda15e6":"markdown"},"source":{"5a81e2bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6338922f":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n","48bdf4fc":"train_df","54063543":"test_df","b0ed7fbe":"train_Y = train_df['Survived']\ntrain_features = train_df.drop(['Survived'], axis=1)","fe3ed471":"train_Y","f0a61b30":"df = train_features.append(test_df)","676e42c6":"df","2439f23d":"df = df.reset_index()","0fb7bbde":"df['PassengerId'].value_counts()","5146fbd2":"df.head()","d559ec03":"df.info()","68a05f86":"(df.isna().sum() \/ len(df))*100","24cef572":"df.describe()","033a0602":"# replace null value for age with mean\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\ndf['Age'].describe()","cfb8119a":"# replace null fare with average\ndf['Fare'].fillna(df['Fare'].mean(), inplace=True)\ndf['Fare'].describe()","b6084137":"# since 77% of the values for cabin attribute is null, dropping the column\ndf.drop(['Cabin'], inplace=True, axis=1)\ndf.info()","e34b94d3":"plt.boxplot(df['Pclass'])","090d9e5c":"plt.boxplot(df['Age'])","031c0de0":"plt.boxplot(df['SibSp'])","d7d6b978":"plt.boxplot(df['Parch'])","aceb6b36":"df['Parch'].value_counts()","dd979fe2":"plt.boxplot(df['Fare'])","284333f0":"df['Ticket'].value_counts()","69e07455":"df","40a35fd3":"# drop name column as it is not contributing\ndf.drop(['Name'], axis=1, inplace=True)\ndf","6eeaaca9":"# hot encoding for ticket\nencoded_column_ticket = pd.get_dummies(df['Ticket'], prefix='Ticket')\nencoded_column_ticket\ndf = df.join(encoded_column_ticket)\ndf","32090c36":"# hot encoding for Embarked\nencoded_column_embarked = pd.get_dummies(df['Embarked'], prefix='Embarked')\nencoded_column_embarked\ndf = df.join(encoded_column_embarked)\ndf","b14a5343":"# hot encdoing for sex attribute\nencoded_column_sex = pd.get_dummies(df['Sex'], prefix='Sex')\ndf = df.join(encoded_column_sex)\ndf","7a7b1ba4":"# hot encoding for Pclass attribute\nencoded_column_pclass = pd.get_dummies(df['Pclass'], prefix='Pclass')\ndf = df.join(encoded_column_pclass)\ndf","d0eb240f":"# drop unused columns\ndf.drop(['index', 'Sex', 'Embarked', 'Pclass', 'Ticket'], axis=1,inplace=True)\ndf","26cfabab":"# Normalize features\ncols = df.columns\ncols = cols.delete(0) # remove PassengerId from normalization process\n\nfor col in cols:\n    df[col] = (df[col] - df[col].min())\/(df[col].max() - df[col].min())  \ndf","e7c528ba":"train = df.loc[df['PassengerId'] <= 891]\ntest = df.loc[df['PassengerId'] > 891]\n\n# drop PassengerId column\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\n# extract passengerId before dropping\ntest_PId = test['PassengerId']\ntest.drop(['PassengerId'], axis=1, inplace=True)","64a312ea":"train","200b2a70":"test","da6b20b2":"train_X = train.values\ntrain_X","5c7b46fb":"train_Y= train_Y.values\ntrain_Y","dbe2a137":"test_X = test.values\ntest_X","3061aa55":"# initialize classifier\nclassifier = Sequential()\n\n# define dimension\ninp_dimension = len(train.columns)\nout_dimension = int((inp_dimension + 1)\/2)\n\n# # first hidden layer\nclassifier.add(Dense(out_dimension, activation = 'relu', input_dim = inp_dimension))\n\n# second hidden layer\nclassifier.add(Dense(out_dimension, activation = 'relu'))\n\n# classifier.add(Dense(4, activation = 'relu'))\n\n# output layer\nclassifier.add(Dense(1, activation = 'sigmoid')) # since it is a binary classification","311d0b86":"# compiling ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","6004cbf9":"out_dimension","febe4009":"classifier.summary()","6302caed":"classifier.fit(train_X, train_Y, batch_size = 10,epochs = 1000)","c459b0c6":"predict_Y = classifier.predict(test_X)\npredict_Y = (predict_Y > 0.5).astype(int)","fa79f937":"type(predict_Y)","605db3e8":"predict_Y.shape","5528cbb6":"predict_Y = predict_Y.flatten()","bd8d81e9":"predict_Y.shape","52c87974":"predict_Y","5668a578":"pId = test_PId.reset_index()\npId.drop(['index'], axis=1, inplace=True)\npId","dea81332":"result = pd.concat([pId, pd.Series(predict_Y, name='Survived')], axis=1)\nresult.to_csv(\"titanic.csv\", index = False)\nresult","335fef7a":"### 5. Train classification model","45980ff2":"### 1. Read data","f49e0604":"### 7. Submission","9d8adbad":"### 6. Prediction","d19b55d1":"### 2. Data Exploration","72f74821":"### 4. Train & Test Data Split","fbda15e6":"### 3. Data Transformation & Feature Selection"}}