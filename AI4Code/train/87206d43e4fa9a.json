{"cell_type":{"d91356c6":"code","5f2f3c74":"code","fc648395":"code","ede1ba37":"code","a0a87528":"code","df5ed8b4":"code","7cfc9a2c":"code","4baaa157":"code","529704a1":"code","34092e05":"code","d19d256b":"code","10130f81":"code","21a4bb4e":"code","11708229":"code","2093959d":"code","8cbaa194":"code","09962387":"code","cbf4ccf7":"code","160512f5":"code","0f33805c":"code","f6cea0d7":"code","e756d7b0":"code","a126b9c8":"code","c8aded6e":"code","059ac96d":"code","5fd92c77":"code","d2105ba6":"code","621c12a7":"code","2a0fc960":"code","97fc64ee":"code","69d33e29":"code","aefc572b":"code","d826055e":"code","b68b69b1":"code","0c852f47":"code","181fade3":"code","7a7bd73d":"code","03a5bf89":"code","dbf3ff40":"code","61298df4":"code","b5528dd9":"code","e60391b6":"code","213b8e72":"code","19407a77":"code","5e5a19a1":"code","55b7729f":"code","d92b7672":"code","17ffb772":"code","d20c882c":"code","9f22b2dc":"code","5aa52c33":"code","2fba56f6":"code","67665749":"code","b35e02d7":"code","08695d1f":"code","0827d96d":"code","cbc0e119":"code","b00797fd":"code","c9966eb6":"code","8e3fe22a":"code","6c96a202":"code","8f73dd9f":"code","66053252":"code","36db4c17":"code","131d0eae":"code","79d7b763":"code","ea8e1c4a":"code","4576c6d9":"code","3d977c6c":"code","acef56ec":"code","aec216df":"markdown","747f3c78":"markdown","2cf9f653":"markdown","76765387":"markdown","01dab5bc":"markdown","72b359f0":"markdown","c0e8e54b":"markdown","012bf09a":"markdown","c01282f7":"markdown","5fb73cfe":"markdown","7d1801a7":"markdown","5eaff069":"markdown","b8dd66ea":"markdown","b8d329d2":"markdown","395f42ed":"markdown","c4cd03d5":"markdown","56e0526f":"markdown","2886531e":"markdown","4f2924df":"markdown","5590ae6e":"markdown","204f7029":"markdown","47ddd605":"markdown","29525c3e":"markdown","d93b2ed5":"markdown","4f145793":"markdown","a47bc444":"markdown","f5d483d2":"markdown","c2e5489d":"markdown","6fdd1683":"markdown","afce45ee":"markdown","3b3fd8aa":"markdown"},"source":{"d91356c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f2f3c74":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew, kurtosis\nfrom scipy import stats\nimport warnings\nimport time\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, GroupShuffleSplit, KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, auc, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import LabelEncoder\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegression, RidgeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n\n\nwarnings.filterwarnings(\"ignore\")","fc648395":"train = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv\")","ede1ba37":"print(\"Test Row : \" + str(test.shape[0]) + \" | \" + \"Test Column : \" + str(test.shape[1]))\nprint(\"*\"*40)\nprint(\"Train Row : \" + str(train.shape[0]) + \" | \" + \"Train Column : \" + str(train.shape[1]))","a0a87528":"train.head()","df5ed8b4":"train.info()","7cfc9a2c":"test.info()","4baaa157":"train.describe().T","529704a1":"test.describe().T","34092e05":"#skew and kurtosis function\ndef skewANDkurtosis(data,column):\n    print(column + \" - Skewness : \" + str(data[column].skew()) + \" | \" + \\\n          column + \" - Kurtosis : \" + str(data[column].kurtosis()) )","d19d256b":"for dt in [train,test]:\n    for col in [\"Age\",\"Annual_Premium\"]:\n        skewANDkurtosis(data = dt,column=col)\n    print(\" \")","10130f81":"#---------------LAYOUT-----------------------\nfig, ax = plt.subplots(7,2,figsize=(20,50))\n#---------------GRAPHS-----------------------\nsns.countplot(x=\"Gender\", data=train, ax=ax[0,0])\nsns.countplot(x=\"Gender\", data=test, ax=ax[0,1])\nsns.distplot(train[\"Age\"], ax=ax[1,0])\nsns.distplot(test[\"Age\"], ax=ax[1,1])\nsns.countplot(\"Driving_License\",data = train, ax=ax[2,0])\nsns.countplot(\"Driving_License\",data  = test, ax=ax[2,1])\nsns.countplot(\"Previously_Insured\",data = train,ax=ax[3,0])\nsns.countplot(\"Previously_Insured\",data = test,ax=ax[3,1])\nsns.countplot(\"Vehicle_Age\",data = train,ax=ax[4,0])\nsns.countplot(\"Vehicle_Age\",data = test,ax=ax[4,1])\nsns.countplot(\"Vehicle_Damage\",data = train,ax=ax[5,0])\nsns.countplot(\"Vehicle_Damage\",data = test,ax=ax[5,1])\nsns.distplot(train[\"Annual_Premium\"], ax=ax[6,0])\nsns.distplot(test[\"Annual_Premium\"], ax=ax[6,1])\n\n\n#--------TITLES-----------------------------\nax[0,0].set_title(\"Train - Gender\")\nax[0,1].set_title(\"Test - Gender\")\nax[1,0].set_title(\"Train - Age\")\nax[1,1].set_title(\"Test - Age\")\nax[2,0].set_title(\"Train - Driving_License\")\nax[2,1].set_title(\"Test - Driving_License\")\nax[3,0].set_title(\"Train - Previously_Insured\")\nax[3,1].set_title(\"Test - Previously_Insured\")\nax[4,0].set_title(\"Train - Vehicle_Age\")\nax[4,1].set_title(\"Test - Vehicle_Age\")\nax[5,0].set_title(\"Train - Vehicle_Damage\")\nax[5,1].set_title(\"Test - Vehicle_Damage\")\nax[6,0].set_title(\"Train - Annual_Premium\")\nax[6,1].set_title(\"Test - Annual_Premium\");","21a4bb4e":"fig, ax = plt.subplots(2,1,figsize = (25,15))\nsns.countplot(\"Region_Code\",data = train,ax=ax[0])\nsns.countplot(\"Region_Code\",data = test,ax=ax[1])\nax[0].set_title(\"Train - Region_Code\")\nax[1].set_title(\"Test - Region_Code\");","11708229":"train.Response.value_counts().plot.bar()","2093959d":"train.head()","8cbaa194":"fig ,ax = plt.subplots(3,1,figsize=(10,7))\nsns.countplot(x = \"Gender\" , hue = \"Vehicle_Damage\", data = train, ax = ax[0]);\nsns.countplot(x = \"Gender\" , hue = \"Response\", data = train, ax = ax[1]);\nsns.countplot(x = \"Gender\" , hue = \"Vehicle_Age\", data = train, ax = ax[2]);","09962387":"Age = sns.FacetGrid(data=train, hue = 'Response', aspect=6 )\nAge.map(sns.kdeplot, 'Age', shade= True )\nAge.set(xlim=(0 , train['Age'].max()))\nAge.add_legend()","cbf4ccf7":"Age = sns.FacetGrid(data=train, hue = 'Gender', aspect=6 )\nAge.map(sns.kdeplot, 'Age', shade= True )\nAge.set(xlim=(0 , train['Age'].max()))\nAge.add_legend()","160512f5":"Age = sns.FacetGrid(data=train, hue = 'Vehicle_Damage', aspect=6 )\nAge.map(sns.kdeplot, 'Age', shade= True )\nAge.set(xlim=(0 , train['Age'].max()))\nAge.add_legend()","0f33805c":"fig ,ax = plt.subplots(4,1,figsize=(10,7))\nsns.countplot(x = \"Driving_License\" , hue = \"Vehicle_Damage\", data = train, ax = ax[0]);\nsns.countplot(x = \"Driving_License\" , hue = \"Response\", data = train, ax = ax[1]);\nsns.countplot(x = \"Driving_License\" , hue = \"Vehicle_Age\", data = train, ax = ax[2]);\nsns.countplot(x = \"Driving_License\" , hue = \"Gender\", data = train, ax = ax[3]);","f6cea0d7":"train = train.drop(\"Driving_License\",axis = 1)\ntest = test.drop(\"Driving_License\",axis = 1)","e756d7b0":"region = train[train[\"Region_Code\"] == 28.0]\nsns.countplot(x=\"Region_Code\",hue=\"Response\",data=region)","a126b9c8":"yes = train[train.Response == 1.0].groupby([\"Region_Code\",\"Response\"])\\\n                         .count() .sort_values(by = \"id\")[\"id\"].to_frame()\nno = train[train.Response == 0.0].groupby([\"Region_Code\",\"Response\"])\\\n                         .count().sort_values(by = \"id\")[\"id\"].to_frame()\n\nyes.rename(columns = {'id': 'YES'}, inplace = True)\nno.rename(columns = {'id': 'NO'}, inplace = True)\n\nno.reset_index(level=[\"Region_Code\",\"Response\"]).drop(\"Response\",axis=1)\nyes.reset_index(level=[\"Region_Code\",\"Response\"]).drop(\"Response\",axis=1)\n\nregion_Code = pd.merge(yes, no, how=\"inner\", on = \"Region_Code\")\n\ndef change(value):\n    if value >=4000:\n        return \"High\"\n    if (value <4000) & (value >1000):\n        return \"Medium\"\n    else:\n        return \"Low\"\n\nregion_Code[\"Region_St\"] = region_Code[\"YES\"].apply(change)\n\nregion_Code = region_Code.reset_index(level=[\"Region_Code\"])\\\n                         .drop([\"YES\", \"NO\"], axis = 1)\n\ntrain  =  pd.merge(train, region_Code, how=\"inner\", on = \"Region_Code\")\n","c8aded6e":"train.head()","059ac96d":"sns.countplot(x = \"Region_St\", hue = \"Response\", data = train);","5fd92c77":"train.drop(\"Region_Code\",axis = 1,inplace=True)","d2105ba6":"fig ,ax = plt.subplots(4,1,figsize=(10,7))\nsns.countplot(x = \"Previously_Insured\" , hue = \"Vehicle_Damage\", data = train, ax = ax[0])\nsns.countplot(x = \"Previously_Insured\" , hue = \"Response\", data = train, ax = ax[1])\nsns.countplot(x = \"Previously_Insured\" , hue = \"Vehicle_Age\", data = train, ax = ax[2])\nsns.countplot(x = \"Previously_Insured\" , hue = \"Gender\", data = train, ax = ax[3]);","621c12a7":"sns.distplot(train[train['Previously_Insured']==0]['Age'],kde=True,color='r',bins=5)\nsns.distplot(train[train['Previously_Insured']==1]['Age'],kde=True,color='g',bins=5);","2a0fc960":"yes = train[train.Response == 1.0].groupby([\"Policy_Sales_Channel\",\"Response\"])\\\n                         .count() .sort_values(by = \"id\")[\"id\"].to_frame()\nno = train[train.Response == 0.0].groupby([\"Policy_Sales_Channel\",\"Response\"])\\\n                         .count().sort_values(by = \"id\")[\"id\"].to_frame()\n\nyes.rename(columns = {'id': 'YES'}, inplace = True)\nno.rename(columns = {'id': 'NO'}, inplace = True)\n\nno.reset_index(level=[\"Policy_Sales_Channel\",\"Response\"]).drop(\"Response\",axis=1)\nyes.reset_index(level=[\"Policy_Sales_Channel\",\"Response\"]).drop(\"Response\",axis=1)\n\nPolicy_Sales_Channel = pd.merge(yes, no, how=\"inner\", on = \"Policy_Sales_Channel\")\n\nPolicy_Sales_Channel[\"RATE\"] = Policy_Sales_Channel.YES \/ Policy_Sales_Channel.NO\n\ndef change(value):\n    if value>=0.40:\n        return \"High\"\n    elif value<0.4 and value >=0.25:\n        return \"Middle\"\n    elif value<0.25 and value>=0.1:\n        return \"Low Middle\"\n    else:\n        return \"Low\"\n        \nPolicy_Sales_Channel[\"Policy_Channel_St\"] = Policy_Sales_Channel[\"RATE\"].apply(change)\n\n\nPolicy_Sales_Channel = Policy_Sales_Channel.reset_index(level=[\"Policy_Sales_Channel\"])\\\n                         .drop([\"YES\", \"NO\",\"RATE\"], axis = 1)\n\n\ntrain  =  pd.merge(train, Policy_Sales_Channel, how=\"inner\", on = \"Policy_Sales_Channel\")","97fc64ee":"train.drop(\"Policy_Sales_Channel\",axis = 1,inplace=True)","69d33e29":"index_list = []\nfor column in [\"Age\", \"Annual_Premium\",\"Vintage\"]:\n    for resp in train.Response.unique():\n        resp_data = train[train[\"Response\"] == resp]\n        resp_column = resp_data[column]\n        \n        Q1 = np.percentile(resp_column,25)\n        Q3 = np.percentile(resp_column,75)\n        IQR = Q3 - Q1\n        STEP = 1.5 * IQR\n        MAX_BORDER = Q3 + STEP\n        MIN_BORDER = Q1 - STEP\n        \n        train.loc[(train[\"Response\"] == resp) & (train[column] > MAX_BORDER), column] = MAX_BORDER\n        train.loc[(train[\"Response\"] == resp) & (train[column] < MIN_BORDER), column] = MIN_BORDER\n        ","aefc572b":"train.head()","d826055e":"train[\"Gender\"] = LabelEncoder().fit_transform(train[[\"Gender\"]])\ntrain[\"Vehicle_Damage\"] = LabelEncoder().fit_transform(train[[\"Vehicle_Damage\"]])\ntrain = pd.get_dummies(data=train, columns= [\"Vehicle_Age\",\"Region_St\", \"Policy_Channel_St\"])","b68b69b1":"train.head()","0c852f47":"train.head()","181fade3":"train.drop(\"id\",axis=1,inplace=True)","7a7bd73d":"y = train.Response\nX = train.drop(\"Response\", axis =1)","03a5bf89":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1845)","dbf3ff40":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","61298df4":"log = LogisticRegression(solver=\"liblinear\")\nlog_model = log.fit(X_train,y_train)\nlog_model","b5528dd9":"log_model.intercept_","e60391b6":"log_model.coef_","213b8e72":"y_pred = log_model.predict(X_test)","19407a77":"confusion_matrix(y_test, y_pred)","5e5a19a1":"# precision = TP \/ (TP + FP)\nprecision = 100135 \/ (100135 + 64)\nprecision","55b7729f":"#recall = TP \/ (TP + FN)\nrecall = 100135 \/ (100135 + 14066)\nrecall","d92b7672":"accuracy_score(y_test, y_pred)","17ffb772":"print(classification_report(y_test,y_pred))","d20c882c":"logit_roc_auc = roc_auc_score(y_test, log_model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, log_model.predict_proba(X_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label = \"AUC (area = %0.2f)\"% logit_roc_auc)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","9f22b2dc":"nb = GaussianNB()\nnb_model = nb.fit(X_train,y_train)\nnb_model","5aa52c33":"y_pred = nb_model.predict(X_test)","2fba56f6":"nb_model.predict_proba(X_test)","67665749":"accuracy_score(y_test,y_pred)","b35e02d7":"cross_val_score(nb_model, X_train, y_train, cv=10).mean()","08695d1f":"print(classification_report(y_test,y_pred))","0827d96d":"nb_roc_auc = roc_auc_score(y_test, nb_model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, nb_model.predict_proba(X_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label = \"AUC (area = %0.2f)\"% nb_roc_auc)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","cbc0e119":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train,y_train)\nknn_model","b00797fd":"y_predict = knn_model.predict(X_test)","c9966eb6":"cross_val_score(knn_model,X_train, y_train, cv = 10).mean()","8e3fe22a":"accuracy_score(y_test,y_predict)","6c96a202":"knn_roc_auc = roc_auc_score(y_test, knn_model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, knn_model.predict_proba(X_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label = \"AUC (area = %0.2f)\"% knn_roc_auc)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","8f73dd9f":"mlpc = MLPClassifier().fit(X_train,y_train)","66053252":"y_pred = mlpc.predict(X_test)","36db4c17":"accuracy_score(y_test,y_pred)","131d0eae":"xgb_model = XGBClassifier().fit(X_train, y_train)","79d7b763":"X_train.head()","ea8e1c4a":"columnss = {\"Vehicle_Age_1-2 Year\" : \"Vehicle_Age_1_2\",\"Vehicle_Age_< 1 Year\" : \"Vehicle_Age_1\",\"Vehicle_Age_> 2 Years\" : \"Vehicle_Age_2\" }\nX_train.rename(columns=columnss, inplace=True)\nX_test.rename(columns=columnss, inplace=True)","4576c6d9":"xgb_model = XGBClassifier().fit(X_train, y_train)","3d977c6c":"y_pred = xgb_model.predict(X_test)\naccuracy_score(y_test,y_pred)","acef56ec":"print(classification_report(y_test,y_pred))","aec216df":"### 2.4.4. Gaussian Naive Bayes","747f3c78":"### Situations of Skew\n\n1. A symmetrical distribution will have a skewness of 0.\n1. If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n1. If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n1. If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.","2cf9f653":"## 1.3. Superficial Examination","76765387":"### 2.4.3. Splitting ","01dab5bc":"**a-) Qualitative data**\n* Gender\n* Driving_License \n* Region_Code\n* Previously_Insured\n* Vehicle_Damage\n* Policy_Sales_Channel\n* Vehicle_Age\n* Response(train)\n\n**b-) Quantitative data**\n* Age\n* Annual_Premium\n* Vintage\n","72b359f0":"### 2.2.4. Region_Code","c0e8e54b":"# 1. Introduction","012bf09a":"### 2.4.2. Standardization","c01282f7":"### 2.2.1. Gender ","5fb73cfe":"I can already say that the Age and Annual_Premium are skew but I have to look deeply ","7d1801a7":"#### Comment : Age and Annual_Premium have very high kurtosis so their peak is very higher and sharper for each dataset.","5eaff069":"## 1.2. Datasets","b8dd66ea":"### 2.2.2. Age","b8d329d2":"### 2.2.3. Driving_License","395f42ed":"### 2.2.6 Policy_Sales_Channel\t","c4cd03d5":"## 2.4.6 Neural Networks","56e0526f":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQplV4m4ZEr1HaVCGtNeMFB9azlaH7OOYmuzPMTma6Z0Q&usqp=CAU&ec=45702844)","2886531e":"### 2.4.1. ENCODING","4f2924df":"### It is an important and vital part for XGBOOST so I didn't change the names of columns. If your dataset includes non-string column names, XGBOOST will give an error.","5590ae6e":"## 2.4.6 XGBOOST ","204f7029":"### 2.4.5 KNN","47ddd605":"## 1.1. Libraries ","29525c3e":"## 2.2. Bivariate Analysis","d93b2ed5":"# 2. Exploratory Data Analysis (EDA)","4f145793":"### 2.2.5. Previously_Insured","a47bc444":"### 2.4.3 Logistic Regression","f5d483d2":"#### 2.4.3.3. Prediction And Model Tuning","c2e5489d":"### Situations of Kurtosis\n\n* **Mesocurtic** : This definition is used so that the standard normal distribution has a kurtosis of three.\n* **Leptokurtic(Kurtosis > 3)** : Distribution is longer, tails are fatter. The peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers.\n* **Platykurtic(Kurtosis < 3)** :  Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that data are light-tailed or lack of outliers.","6fdd1683":"## 2.4. MODEL SELECTION","afce45ee":"## 2.3 Outlier ","3b3fd8aa":"train[\"Age\"] = StandardScaler().fit_transform(train[[\"Age\"]])\ntrain[\"Annual_Premium\"] = StandardScaler().fit_transform(train[[\"Annual_Premium\"]])\ntrain[\"Vintage\"] = StandardScaler().fit_transform(train[[\"Vintage\"]])"}}