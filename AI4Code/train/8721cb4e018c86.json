{"cell_type":{"a9cce04b":"code","c59decf3":"code","ab6dd0ed":"code","d650fabf":"code","5805a40f":"code","26f26c19":"code","0959f530":"code","5007915d":"code","0c0b97aa":"code","a08d38bc":"code","2ab433ce":"code","e9cdedbc":"code","d59ccad9":"code","921aca1e":"code","8c1c5957":"code","68d4ac41":"code","50c574d2":"code","fc153b9a":"code","dd623b79":"code","26b1e0bf":"code","b315bf1f":"code","b9fbb156":"code","e639b02e":"code","b5b9fc06":"code","ed069f56":"code","d31db8db":"code","4cc4956a":"code","6e32a81c":"code","17ce471a":"code","44221db0":"markdown","d64cf766":"markdown","cae07b72":"markdown","f6326743":"markdown","33848e5e":"markdown","4933041d":"markdown","1cba48fa":"markdown","06e40bfe":"markdown","45c14afb":"markdown","aec391af":"markdown","17718ca6":"markdown","b2c38078":"markdown","db44c947":"markdown","56b7c4ff":"markdown","6a35b355":"markdown","6b9542d7":"markdown","91542c4b":"markdown","06e3e9b5":"markdown","f9328717":"markdown","99235d69":"markdown","67290202":"markdown","f024af66":"markdown","1d9d9f31":"markdown","553d1689":"markdown","2e63ab2a":"markdown","bdd02a52":"markdown","c7949183":"markdown","1de0be23":"markdown","159e0dd1":"markdown","206f3185":"markdown","f18db61f":"markdown","f8fa1b11":"markdown"},"source":{"a9cce04b":"# Importing libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport collections\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom sklearn.tree import _tree\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn import model_selection\nfrom datetime import datetime\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed","c59decf3":"# Load data\ndataset_name = 'https:\/\/raw.githubusercontent.com\/geniusai-research\/interns_task\/main\/sampled_data2.csv'\ndf = pd.read_csv(dataset_name)\nprint(df.shape)","ab6dd0ed":"df.head()","d650fabf":"#Lists to hold the lower (l) and upper (u) bounds\nl = []\nu = []\nfor i in np.array(df['age']):\n    if 'to' in i:\n        l.append(i.split(\"to\")[0])\n        u.append(i.split(\"to\")[1])\n    elif 'lt' in i:\n        u.append(int(i.replace('lt','')))\n        l.append(-1)\n    elif 'gt' in i:\n        l.append(int(i.replace('gt','')))\n        u.append(-1)\n    else:\n        l.append(-1)\n        u.append(-1)","5805a40f":"# making separate lower bound column \ndf[\"lower_age\"]= np.array(l,dtype=int)\n  \n# making separate upper bound column \ndf[\"upper_age\"]= np.array(u,dtype=int)\n\ndf.drop(columns=['age'],inplace=True)","26f26c19":"entity_column = 'customer'\ntarget_column= 'fraud'\ndatetime_columns = 'step'\n\nif entity_column == '':\n    df['entity_column']= df.index.tolist()\n    entity_column = 'entity_column' \n    \nif datetime_columns == '':\n    df['datetime_columns']= df.index.tolist()    \n    datetime_columns = 'datetime_columns'\n\nnumerical_columns = df._get_numeric_data().columns.values.tolist()\nprint(numerical_columns)","0959f530":"df['month'] = [int(i.split('-')[1]) for i in df[datetime_columns]]\ndf['step'] = pd.to_datetime(df['step']) # Converting to DateTime datatype from stringd","5007915d":"min(df['step']) # Checking the minimum date ","0c0b97aa":"#Using collections package to perform frequency count of customer ids\ncounter=collections.Counter(df['customer'])\n# Using the counter dictionary to fill in the columns based on the customers\ndf['freq'] = [int(counter[i]) for i in df['customer']]","a08d38bc":"def feat(c):\n    \n    merch_f = []\n    cust_f = []\n    cat_f =[]\n    \n    i = df['merchant'][c]\n    d = df['step'][c]\n    #Compting new DF based on Merchant ID and Timestamp less than d\n    new = df[(df.merchant==i) & (df['step']<d)]\n    s = sum(new['fraud']) #Summing all fraud values\n    merch_f.append(s)    \n \n    i = df['customer'][c]\n    #Compting new DF based on Customer ID and Timestamp less than d\n    new = df[(df.customer==i) & (df['step']<d)]\n    s = sum(new['fraud'])\n    cust_f.append(s)  \n    \n    i = df['category'][c]\n    #Compting new DF based on Category and Timestamp less than d\n    new = df[(df.category==i) & (df['step']<d)]\n    s = sum(new['fraud'])\n    cat_f.append(s) \n    \n    return merch_f,cust_f,cat_f\n    \nfr = Parallel(n_jobs=-1,verbose=1)(delayed(feat)(i) for i in range(0,len(df['merchant'])))","2ab433ce":"fr = np.reshape(fr,(25200,3)) #converting the tensor (25200,3,1) to 2d matrix of size (25200,3)","e9cdedbc":"df['cust_past_fraud'] = fr[:,1]\ndf['merc_past_fraud'] = fr[:,0]\ndf['cat_past_fraud']  = fr[:,2]\n\n#Replacing 0 values with -1 for all three columns\ndf['cust_past_fraud'] = df['cust_past_fraud'].replace(to_replace=0,value=-1) \ndf['merc_past_fraud'] = df['merc_past_fraud'].replace(to_replace=0,value=-1)\ndf['cat_past_fraud'] = df['cat_past_fraud'].replace(to_replace=0,value=-1)","d59ccad9":"mean = {}\nfor i in np.unique(df['category']):\n    new = df[df.category==i]\n    summ = np.mean(new['amount'])\n    mean[i] = summ","921aca1e":"keys = mean.keys()\nvalues = mean.values()\n\nplt.bar(keys, values)\nplt.xticks(rotation = 90)\nplt.show()","8c1c5957":"lo = []\nfor i in range(len(df)):\n    c = df.iloc[i].category\n    a = df.iloc[i].amount\n    if np.log(mean[c]) > 0:\n        lo.append(np.log(a)\/np.log(mean[c]))\n    else:\n        lo.append(0)\ndf['log based mean'] = lo\ndf['log based mean'] = df['log based mean'].replace(min(lo),-100)","68d4ac41":"df.head()","50c574d2":"plt.figure(figsize = (15,5))\nax= sns.countplot(df['fraud'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 45)\nplt.show()","fc153b9a":"plt.figure(figsize = (20,10))\nax= sns.countplot(x = \"freq\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","dd623b79":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"lower_age\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","26b1e0bf":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"upper_age\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","b315bf1f":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"category\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","b9fbb156":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"month\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","e639b02e":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"gender\", hue = \"fraud\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()","b5b9fc06":"X = df.drop(columns=[target_column,datetime_columns,'lower_age'])\ny = df[target_column]\n\n# Label Encoding of Categorical Columns\ncategorical_names = {}\nfor feature in ['gender','category','merchant',entity_column]:\n    X[feature] =  X[feature].fillna('') \n    le =  LabelEncoder()\n    le.fit(X[feature])\n    X[feature] = le.transform(X[feature])\n    categorical_names[feature] = le.classes_\n\n# Filling Unknowns as -1\nX = X.fillna(-1)\nplt.figure(figsize = (10,5))\nsns.heatmap(X.corr(), annot = True)\nplt.show()","ed069f56":"X.drop(columns=['cat_past_fraud'],inplace=True)","d31db8db":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)","4cc4956a":"grid  = RandomForestClassifier(criterion='entropy', class_weight='balanced', random_state=42)\ngrid.fit(X_train,y_train)\n\npredictions = grid.predict_proba(X_test)\npredictions_class = grid.predict(X_test)\n\n\nif df[target_column].nunique()> 2:\n    rules_score = roc_auc_score(y_test, predictions[:,1], multi_class = 'ovo')\nelse:\n    rules_score = roc_auc_score(y_test, predictions[:,1])\n\nprint(\"Obtained ROC_ACC :\",rules_score)\n\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, predictions_class)\nprint(\"Confusion Matrix = \")\nprint(cf_matrix)\n\nimport seaborn as sns\nsns.heatmap(cf_matrix, annot=True)","6e32a81c":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n\ngrid  = RandomForestClassifier(criterion='entropy', class_weight='balanced', random_state=42)\ngrid.fit(X_train_s, y_train_s)\n\npredictions = grid.predict_proba(X_test)\npredictions_class = grid.predict(X_test)\n\n\nif df[target_column].nunique()> 2:\n    rules_score = roc_auc_score(y_test, predictions[:,1], multi_class = 'ovo')\nelse:\n    rules_score = roc_auc_score(y_test, predictions[:,1])\n\nprint(\"Obtained ROC_ACC :\",rules_score)\n\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, predictions_class)\nprint(\"Confusion Matrix = \")\nprint(cf_matrix)\n\nimport seaborn as sns\nsns.heatmap(cf_matrix, annot=True)","17ce471a":"from imblearn.under_sampling import NearMiss\n\nnm = NearMiss()\n\nX_train_n, y_train_n = nm.fit_resample(X_train, y_train)\ngrid  = RandomForestClassifier(criterion='entropy', class_weight='balanced', random_state=42)\ngrid.fit(X_train_n, y_train_n)\n\npredictions = grid.predict_proba(X_test)\npredictions_class = grid.predict(X_test)\n\n\nif df[target_column].nunique()> 2:\n    rules_score = roc_auc_score(y_test, predictions[:,1], multi_class = 'ovo')\nelse:\n    rules_score = roc_auc_score(y_test, predictions[:,1])\n\nprint(\"Obtained ROC_ACC :\",rules_score)\n\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, predictions_class)\nprint(\"Confusion Matrix = \")\nprint(cf_matrix)\n\nimport seaborn as sns\nsns.heatmap(cf_matrix, annot=True)","44221db0":"### Gender vs Fraud\nGender has 4 categorical variables,\n* Male\n* Female\n* Enterprise \n* Unknown\n\nFrom the given data, we can see that females have a higher chance of fraud compared to the other categories.","d64cf766":"### Fraud History Dealing with Customers and Merchant\nThe past history of a given customer can tell us if he\/she would default again, analogously even merchant's past history can reveal the chances of fraud. to achieve this we iterate through each merchant and customer id and check if there are instance of fraud to the date prior to the one we are currently at.","cae07b72":"# Feature Engineered Dataset","f6326743":"# Analyzing the relation between new features and target value\nAs seen below, the dataset is imbalanced, with the fraud values being nearly on 28% of the total data. To tackle this we can adopt SMOTE methodologies to oversample the data. On experimenting with SMOTE and its variations, there was a drastic decrease in False Negatives and not much change in False Positives. (Results in the bottom)","33848e5e":"As we can see, the correaltion between Merchant's past fraud history and Category based fraud is very high, hence we will be dropping cat_past_fraud.","4933041d":"____","1cba48fa":"**False Positives : 196**  \nThere is a 35.5% decrease in False Positives hence an increase in the performance of the model.","06e40bfe":"### Extracting the Month value from the ***step*** column","45c14afb":"### Splitting Age as Lower and Upper Bound","aec391af":"The age column is formated as {}-{} or lt{} or gt{}, hence via conditional for loops we can extract the lower and upper bounds. For the format lt{}, the upper bound will the the value inside {} and lower bound will be -1 and vice verse for gt{}.","17718ca6":"___","b2c38078":"# Loading and Feature Engineering\nThe data is loaded from the git repo for convinience.","db44c947":"### Age vs Fraud\nSince the variation of Upper and Lower age's are similr w.r.t. target value, we will be using only upper bound.","56b7c4ff":"### Category vs Fraud\nAs seen below, category plays a role in the fraud.\n* Transportation has the least chance of fraud \n* Sports and toys the highest","6a35b355":"The inital course of action from the given dataset:\n* Convert Age (String Field) to 2 Column int field with upper and lower bounds\n* Analyze the Uniqeness of customer id and merchan id\n* Analyze the correlation between category and fraud\n* Analyse if the past history of fraud by customer causes fraud in the future\n* Extract the month from the given ***step*** to analyze the relation between fraud and month ","6b9542d7":"### Scaling the Final Feature extracted data and splitting it for analysing model performance","91542c4b":"Adding the columns for mechant and customer's past fradulent history. Simuntaneously, we will be checking if categories reveal the chances of fraud, we also compute the frauds category wise prior to the given date.","06e3e9b5":"______","f9328717":"Since the date string is in format YYYY-MM-DD, we can split the string based on '-' value and extract month. Since all the dates correspond to the year 2021, we will only be dealing with months.","99235d69":"# Applying SMOTE for oversampling\nSynthetic Minority Oversampling Technique is an oversampling technique, works by utilizing a k-nearest neighbor algorithm to create synthetic data. SMOTE first start by choosing random data from the minority class, then k-nearest neighbors from the data are set.","67290202":"### Customer Frequency vs Fraud\nAs shown in the below figure, there is a lesser chance of fraud for customer's occurence less than 10.","f024af66":"# Model Performance\nThe Given initial metrics for the same model is :\n\nROC_AUC = 0.9960853703703704  \nConfusion Matrix = \n                    [[13193   307]\n                    [  243  5157]]\n               \n**False Positive : 307**\n","1d9d9f31":"### Extracted Month vs Fraud","553d1689":"### Mean based Log\nComputing the mean values of amount for each category and dividng values after log","2e63ab2a":"Adding the columns **lower** and **upper** bound to the dataframe and dropping **age**","bdd02a52":"___","c7949183":"# Applying NearMiss for undersampling\nNearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to the minority class.","1de0be23":"Currently the numerical columns of our dataframe are :","159e0dd1":"___","206f3185":"# Preparing the Training and Testing Data\nAs explained before we will be dropping ***lower bound of age*** and ***step***. In this step all the categorical variables such as Gender, Category merchant id, etc will be encoded. We will further be plotting a correlation matrix between variables to check if any unneccasy features are present.","f18db61f":"### Frequency of Customer ID \nThere are numerous occurences of a given single customer id, hence we compute the frequecy of occurece of every unique Customer ID.","f8fa1b11":"__________________"}}