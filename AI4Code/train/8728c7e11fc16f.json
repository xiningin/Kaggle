{"cell_type":{"cfe81417":"code","0232a262":"code","724a6a3e":"code","1c8d378f":"code","6ebc9ed3":"code","aa460e21":"code","eea67431":"code","992acbf3":"code","4b6eff74":"code","1d201b19":"code","92283047":"code","c95e720f":"code","bcad8fa0":"code","08dab334":"code","43382af3":"code","63796e59":"code","105c11b0":"code","8cfc54e4":"code","2df2ff25":"code","6397b541":"code","afc8b848":"code","d5695fff":"code","f66c43b6":"code","f2594d08":"code","731cc05f":"code","d9c6639c":"code","053b7f93":"code","78633f19":"code","a5ca8de7":"code","5673d639":"code","87002bf0":"code","44910adc":"code","02bfe1c8":"code","6cab8fa3":"code","dc68844e":"code","03695175":"markdown"},"source":{"cfe81417":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy.stats import skew, norm \nfrom warnings import filterwarnings as filt\nimport plotly.express as px \n\nplt.style.use('_classic_test_patch')\nplt.rcParams['figure.figsize'] = (12,6)\nfilt('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0232a262":"pd.set_option('display.max_columns', None)\ndf = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\ndf.head()","724a6a3e":"df.shape","1c8d378f":"df.isnull().values.sum()","6ebc9ed3":"sns.countplot(df.fetal_health)","aa460e21":"sns.heatmap(df.corr(), cmap = 'icefire')","eea67431":"df.corrwith(df.fetal_health).sort_values(ascending = False)","992acbf3":"import eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom pdpbox import pdp\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.feature_selection import mutual_info_classif\n\ndef permImp(x, y):\n    model = rfc().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return eli5.show_weights(perm , feature_names = x.columns.tolist())\n\ndef isolate(x, y, col):\n    model = rfc().fit(x, y)\n    pdp_dist = pdp.pdp_isolate(model, dataset = x, model_features = x.columns, feature = col)\n    return pdp.pdp_plot(pdp_dist, feature_name = col)\n\ndef forceplot(x, y, n_class = 0):\n    model = rfc().fit(x, y)\n    explainer = shap.TreeExplainer(model)\n    shap_value = explainer.shap_values[n_class]\n    expected_value = explainer.expected_value[n_class]\n    return shap.force_plot(expected_value, shap_value, feature_names = x.columns)\n\ndef plot_mi(score):\n    score = score.sort_values('mi_score', ascending = True)\n    return plt.barh(score.index, score.mi_score)\n\ndef mi_score(x, y):\n    score = pd.DataFrame(mutual_info_classif(x, y, discrete_features = False), index = x.columns, columns = ['mi_score'])\n    plot_mi(score)\n    return score.sort_values('mi_score', ascending = False)","4b6eff74":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold","1d201b19":"x = df.drop(['fetal_health'], axis = 1)\ny = df.fetal_health\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","92283047":"permImp(x_train, y_train)","c95e720f":"mscore = mi_score(x_train, y_train)","bcad8fa0":"sns.scatterplot(data = df, x = 'abnormal_short_term_variability', y = 'mean_value_of_short_term_variability', hue = 'fetal_health')","08dab334":"sns.scatterplot(data = df, y = 'histogram_number_of_zeroes', x = 'mean_value_of_short_term_variability', hue = 'fetal_health')","43382af3":"df.describe()","63796e59":"df.nunique()","105c11b0":"skewness = pd.DataFrame(np.abs(skew(df)), columns = ['skew_score'], index = df.columns).sort_values('skew_score', ascending = True)\nplt.barh(skewness.index, skewness.skew_score)\nplt.title('skewness score')","8cfc54e4":"high_skewness = skewness[skewness.skew_score > 2].index\nfig, ax = plt.subplots(len(high_skewness), 2, figsize = (16, 10))\nfig.tight_layout()\nfor ind,col in enumerate(high_skewness):\n#     plt.figure(ind)\n    sns.distplot(df[col], ax = ax[ind, 0])\n    sns.boxplot(df[col], ax = ax[ind, 1])","2df2ff25":"def outliers(df, col):\n    quant = df[col].quantile(q = [0.25, 0.75])\n    q1 = quant.loc[0.25]\n    q3 = quant.loc[0.75]\n    iqr = q3 - q1\n    lower_bound = q1 - (1.5 * iqr)\n    upper_bound = q3 + (1.5 * iqr)\n    return df[(df[col] < lower_bound) | (df[col] > upper_bound)].index    ","6397b541":"testing = df.copy()\nfig, ax = plt.subplots(len(high_skewness), 2, figsize = (16, 10))\nfig.tight_layout()\nfor ind, col in enumerate(high_skewness):\n    idx = outliers(testing, col)\n    testing.loc[idx, col] = testing[col].mean()\n#     plt.figure(ind)\n    sns.distplot(testing[col], ax = ax[ind, 0])\n    sns.boxplot(testing[col], ax = ax[ind, 1])","afc8b848":"sns.countplot(df.severe_decelerations, hue = df.fetal_health)","d5695fff":"isolate(x_train, y_train, 'severe_decelerations')","f66c43b6":"def interact(x, y, cols):\n    model = rfc().fit(x, y)\n    pdp_dist = pdp.pdp_interact(model, dataset = x, model_features = x.columns , features = cols)\n    return pdp.pdp_interact_plot(pdp_dist, feature_names = cols)","f2594d08":"interact(x_train, y_train, ['severe_decelerations','fetal_movement'])","731cc05f":"t_x = testing.drop(['fetal_health'], axis = 1)\nt_y = testing.fetal_health\nnew_x_train, new_x_test, new_y_train, new_y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\nnew_x_train.shape, new_x_test.shape, new_y_train.shape, new_y_test.shape","d9c6639c":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBRFClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import cross_validate, KFold\n\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler , RobustScaler, MinMaxScaler","053b7f93":"def best_model(x, y):\n    xgb.set_config(verbosity=0)\n    models = [SVC(), KNeighborsClassifier(), GaussianNB(), rfc(), XGBRFClassifier(), LGBMClassifier()]\n    names = ['svm', 'knn', 'naive bayes', 'random forest', 'xgboost', 'lightgb']\n    scores = [[] for _ in range(4)]\n    for model in models:\n        for idx, scaler in enumerate([None, StandardScaler(), RobustScaler(), MinMaxScaler()]):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            #cv = StratifiedKFold(5, shuffle = True, random_state = 123)\n            cv = KFold(5, shuffle = True, random_state = 123)\n            score = cross_validate(model, X = x, y = y, cv = cv, scoring = 'f1_micro')['test_score'].mean()\n            scores[idx].append(score)\n    return pd.DataFrame(scores, columns = names, index = ['None', 'std', 'robust', 'minmax']).T\n\ndef clf_report(yt, pred):\n    print()\n    print(classification_report(yt,  pred))\n    print()\n    \ndef get_score(xt, yt, xtest, ytest, model, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    print(' Report '.center(60,'='))\n    print()\n    print(f\"training score  :===>  {model.score(xt, yt)}\")\n    print(f\"testing score   :===>  {model.score(xtest, ytest)}\")\n    clf_report(ytest, pred)\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n    \n    \ndef gridcv(xt, yt, model, params, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    cv = KFold(5, shuffle = True, random_state = 123)\n    clf = GridSearchCV(model, param_grid = params, cv = cv, scoring = 'f1_micro', return_train_score = True, verbose = 1)\n    clf.fit(xt, yt)\n    res = pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending = False)\n    return clf.best_estimator_, clf.best_params_, res[['mean_train_score','mean_test_score','params']]\n    ","78633f19":"# normal x and y train\nbest_model(x_train, y_train)","a5ca8de7":"# replaced all the outliers for high skewed features with their mean x and y train \nbest_model(new_x_train, new_y_train)","5673d639":"from imblearn.over_sampling import SMOTE ","87002bf0":"smot = SMOTE()\nsmot_x_train, smot_y_train = smot.fit_resample(x_train, y_train)\nsns.countplot(smot_y_train)","44910adc":"# over-sampled x and y train \nbest_model(smot_x_train, smot_y_train)","02bfe1c8":"smot_x_train.shape, smot_y_train.shape","6cab8fa3":"params = {\n    'model__max_depth' : [8, 13, 16, 18, -1],\n    'model__boosting_type' : ['gbdt', 'dart'],\n    'model__n_estimators' : [100, 200, 300],\n    'model__reg_lambda' : [0,1],\n    'model__reg_alpha'  : [0.1,0.3,0.5]\n}\ngridcv(smot_x_train, smot_y_train, LGBMClassifier(), params, StandardScaler())","dc68844e":"get_score(smot_x_train, smot_y_train, x_test, y_test, LGBMClassifier(max_depth = 8, n_estimators = 200, reg_alpha = 0.4), StandardScaler())","03695175":"##### 0 - Normal\n##### 1 - Suspect\n##### 2 - Pathological\n\n* if the fetal movement and severe deceleration is low then there's a 78% chance that the fetus is normal\n* if the severe_deceleration increase and no matter how the fetal moves then there's a low chance that the fetus is not normal "}}