{"cell_type":{"e62d5b95":"code","ac96ebfc":"code","2a492808":"code","9b9229c7":"code","b30da5dc":"code","db18146d":"code","729f43fe":"code","b98e8991":"code","39663965":"code","8fffcbf5":"code","fa8a77d7":"code","52f1f731":"code","7719b771":"code","9e3de54b":"code","7c2fb9d8":"code","a881f9e0":"code","83d68c92":"code","275bf22c":"code","f73ba70c":"code","aca16b42":"markdown","00a43a26":"markdown","48c753a4":"markdown","0c4fcddb":"markdown","60f0dd2b":"markdown"},"source":{"e62d5b95":"import os\nimport sys\nimport time\nimport shutil\n\nimport random\n\nfrom tqdm import tqdm\nfrom glob import glob\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","ac96ebfc":"valset_dir = '..\/input\/hotel-comment-valset-testset\/valuating\/'\nlen(glob(valset_dir+'sample_*.npz'))","2a492808":"testset_dir = '..\/input\/hotel-comment-valset-testset\/testing\/'\n# testset_dir = '..\/input\/sentiment-testing-smallbert\/testing\/'\nlen(glob(testset_dir+'sample_*.npz'))","9b9229c7":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","b30da5dc":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","db18146d":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","729f43fe":"pip install stellargraph","b98e8991":"from stellargraph.layer import (\n    GraphConvolution, GCN, \n    GraphAttention, GAT\n)\nfrom stellargraph.utils import plot_history","39663965":"labels_embeddings = np.load('..\/input\/hotel-comment-valset-testset\/labels_embeddings.npy')\nif len(labels_embeddings.shape) == 2:\n    labels_embeddings = np.expand_dims(labels_embeddings, axis=0)\nlabels_embeddings","8fffcbf5":"N_LABELS = labels_embeddings.shape[1]\nN_LABELS","fa8a77d7":"class DataGenerator(Sequence):\n\n    def __init__(self,\n                 data_root,\n                 labels_fixed: np.array,\n                 embedding_dim: 512,\n                 batch_size: int = 16, \n                 label_smoothing: bool = True,\n                 shuffle: bool = True):\n        \n        self.data_root = data_root\n        self.labels_fixed = labels_fixed\n        self.embedding_dim = embedding_dim\n        self.label_smoothing = label_smoothing\n             \n        # list of files containing both word-embeddings and multi-labels\n        if isinstance(self.data_root, str):\n            self.files = glob(os.path.join(self.data_root, 'sample_*.npz'))\n        elif isinstance(self.data_root, (list, tuple)):\n            self.files = []\n            for data_dir in self.data_root:\n                self.files += glob(os.path.join(data_dir, 'sample_*.npz'))\n                \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.indices = np.array(list(range(len(self.files))))\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"\n        Denotes the number of batches per epoch\n        \"\"\"\n        return int(len(self.files) \/\/ self.batch_size)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate one batch of data\n        \"\"\"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        wb_batch = []\n        mtl_batch = []\n        for idx in indices:\n            \n            sample_file = self.files[idx]\n            \n            # Load word embeddings\n            wb_pad = np.zeros((512, self.embedding_dim))\n            wb = np.load(sample_file)['emb']\n            wb_pad[:wb.shape[0],:] = wb\n            wb_batch += [wb_pad]\n            \n            # Load multi-labels\n            mtl = np.load(sample_file)['mtl']\n            if self.label_smoothing:\n                mtl = self.smooth_labels(mtl)\n            mtl_batch += [mtl]\n            \n        return [np.array(wb_batch), self.labels_fixed], np.array(mtl_batch)\n\n    def smooth_labels(self, labels, factor=0.1):\n        # smooth the labels\n        labels *= (1 - factor)\n        labels += (factor \/ labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \"\"\"\n        Update indices after each epoch\n        \"\"\"\n        if self.shuffle:\n            np.random.shuffle(self.indices)","52f1f731":"test_generator = DataGenerator(testset_dir, labels_embeddings, embedding_dim=512, batch_size=1, label_smoothing=False, shuffle=False)\nlen(test_generator)","7719b771":"class Adjacency(Layer):\n\n    def __init__(self, nodes=1, weights=None, init_method='identity'):\n        super(Adjacency, self).__init__()\n\n        self.shape = (1, nodes, nodes)\n\n        if weights is not None:\n            assert weights.shape==(nodes, nodes), \\\n                f'Adjacency Matrix must have shape ({nodes}, {nodes})' + \\\n                f' while its shape is {weights.shape}'\n            w_init = tf.convert_to_tensor(weights)\n        else:\n            init_method = init_method.lower()\n            if init_method == 'identity':\n                initializer = tf.initializers.Identity()\n            elif init_method in ['xavier', 'glorot']:\n                initializer = tf.initializers.GlorotNormal()\n            w_init = initializer(shape=(nodes, nodes))\n\n        self.w = tf.Variable(\n            initial_value=tf.expand_dims(w_init, axis=0), \n            dtype=\"float32\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.convert_to_tensor(self.w)\n\n    def compute_output_shape(self):\n        return self.shape","9e3de54b":"def create_model(\n    n_labels,\n    batch_size,\n    sequence_length=512, \n    embedding_dim=768,\n    lstm_units=64,  \n    attention_heads=[4, 2],\n    adjacency_matrix=None,\n    adjacency_generation='identity', # 'identity' or 'xavier' or 'glorot'\n    feed_text_embeddings=True, # if False, add additional Embedding layer\n    text_embeddings_matrix=None, # initialized weights for text Embedding layer\n    feed_label_embeddings=True, # if False, add additional Embedding layer\n    label_embeddings_matrix=None, # initialized weights for label Embedding layer\n) -> Model:\n\n    if isinstance(attention_heads, int):\n        attention_heads = [attention_heads, attention_heads]\n    elif not isinstance(attention_heads, (list, tuple)):\n        raise ValueError('`attention_heads` must be INT, LIST or TUPLE')\n\n    # 1. Sentence Representation\n    sentence_model = Sequential(name='sentence_model')\n    if feed_text_embeddings:\n        sentence_model.add(Dropout(0.3, input_shape=(sequence_length, embedding_dim), name='word_embeddings'))\n        word_inputs, word_embeddings = sentence_model.inputs, sentence_model.outputs\n    else:\n        word_inputs = Input(shape=(sequence_length, ), name='word_inputs')\n        embedding_args = {\n            'input_dim': sequence_length,\n            'output_dim': embedding_dim,\n            'name': 'word_embeddings'\n        }\n        if text_embeddings_matrix is not None \\\n            and text_embeddings_matrix.shape==(sequence_length, embedding_dim):\n            embedding_args['weights'] = [text_embeddings_matrix]\n        word_embeddings = Embedding(**embedding_args)(word_inputs)\n        word_embeddings = Dropout(0.3, name='WE_dropout')(word_embeddings)\n\n    forward_rnn = LSTM(units=lstm_units, return_sequences=True, name='forward_rnn')\n    backward_rnn = LSTM(units=lstm_units, return_sequences=True, name='backward_rnn', go_backwards=True)\n    bidir_rnn = Bidirectional(layer=forward_rnn, backward_layer=backward_rnn, merge_mode=\"concat\", name='bidir_rnn')\n    \n    sentence_repr = bidir_rnn(word_embeddings)\n    sentence_repr = K.mean(sentence_repr, axis=1)\n    # print(f\"sentence_repr: {K.int_shape(sentence_repr)}\")\n\n    # 2. Labels Representation\n    if feed_label_embeddings:\n        label_inputs = Input(batch_shape=(1, n_labels, embedding_dim), name='label_embeddings')\n        label_embeddings = label_inputs\n    else:\n        label_inputs = Input(batch_shape=(1, n_labels), name='label_inputs')\n        embedding_args = {\n            'input_dim': n_labels,\n            'output_dim': embedding_dim,\n            'name': 'label_embeddings'\n        }\n        if label_embeddings_matrix is not None \\\n            and label_embeddings_matrix.shape==(n_labels, embedding_dim):\n            embedding_args['weights'] = [label_embeddings_matrix]\n        label_embeddings = Embedding(**embedding_args)(label_inputs)\n        label_embeddings = Dropout(rate=0.2, name='LE_dropout')(label_embeddings)\n    # print(f\"label_inputs: {K.int_shape(label_inputs)}\")\n\n    label_correlation = Adjacency(nodes=n_labels, \n                                  weights=adjacency_matrix,\n                                  init_method=adjacency_generation)(label_inputs)\n    # print(f\"label_correlation: {K.int_shape(label_correlation)}\")\n\n    label_attention = GraphAttention(\n        units=embedding_dim\/\/attention_heads[0],\n        activation='tanh',\n        attn_heads=attention_heads[0],\n        in_dropout_rate=0.3,\n        attn_dropout_rate=0.3,\n    )([label_embeddings, label_correlation])\n    # print(f\"label_attention: {K.int_shape(label_attention)}\")\n\n    label_residual = Add(name='label_residual')([label_attention, label_embeddings])\n    # print(f\"label_residual: {K.int_shape(label_residual)}\")\n\n    label_repr = GraphAttention(\n        units=2*lstm_units,\n        activation='tanh',\n        attn_heads=attention_heads[1],\n        attn_heads_reduction='average',\n        in_dropout_rate=0.3,\n        attn_dropout_rate=0.3,\n    )([label_residual, label_correlation])\n\n    label_repr = K.sum(label_repr, axis=0, keepdims=False)\n    # print(f\"label_repr: {K.int_shape(label_repr)}\")\n\n    # 3. Prediction\n    prediction = tf.einsum('Bk,Nk->BN', sentence_repr, label_repr)\n    prediction = sigmoid(prediction)\n    # prediction = softmax(prediction, axis=-1)\n    # print(f\"prediction: {K.int_shape(prediction)}\")\n\n    return Model(inputs=[word_inputs, label_inputs], \n                 outputs=prediction, \n                 name='MAGNET')","7c2fb9d8":"# with tpu_strategy.scope():\nmodel = create_model(n_labels=N_LABELS,\n                     batch_size=128, \n                     sequence_length=512, \n                     embedding_dim=512,\n                     lstm_units=64,\n                     attention_heads=[4, 2],\n                     adjacency_generation='xavier',\n                     # adjacency_matrix=correlation_matrix\n                     )\n# models_dir = '..\/input\/bert-small-2311'\n# model_ckpt = 'ep012_acc0.434_val_acc0.478'\nmodels_dir = '..\/input\/bertsmall-dataset156k-models'\nmodel_ckpt = 'ep038_acc0.459_val_acc0.514'\nmodel.load_weights(f'{models_dir}\/{model_ckpt}.h5')\nmodel.summary()","a881f9e0":"pred_dir = '\/kaggle\/working\/predictions'\nif not os.path.isdir(pred_dir):\n    os.makedirs(pred_dir)","83d68c92":"pred_record = pd.DataFrame(columns=['ground_truth', 'prediction'])\nrow_idx = 0\nfor x, y_true in tqdm(test_generator):\n    y_pred = model.predict(x).tolist()[0]\n    pred_record.loc[row_idx] = [y_true, y_pred]\n    row_idx += 1","275bf22c":"display(pred_record.sample(n=11))","f73ba70c":"pred_record.to_csv(f'{pred_dir}\/{model_ckpt}.csv', index=False)","aca16b42":"# **Load data**","00a43a26":"# **Load Model**","48c753a4":"# **Import Libraries**","0c4fcddb":"# **Evaluate**","60f0dd2b":"# **Data Generator**"}}