{"cell_type":{"b45bb636":"code","128cc934":"code","08515b98":"code","8ca5c3f1":"code","f14c9f97":"code","6ffd071d":"code","7a587abf":"code","58a52883":"code","0eb7f5f5":"code","4b648c3f":"code","c89552c4":"code","5b71539d":"code","9647cec8":"code","685f366d":"code","4f0293a9":"code","483eb235":"code","7d05f43d":"code","cf53abb4":"code","e5dc2abe":"code","590eb518":"code","f60b85d4":"code","f3a0e8d6":"code","5ada5194":"code","8a145c85":"code","ea99549e":"code","26f16181":"code","7a8e4898":"code","26e12b37":"code","ba9c95e7":"code","b35c5242":"code","0b915481":"code","ac10b069":"code","323aed43":"code","7a2575b3":"code","87244d71":"code","8dae2065":"code","e04cbee8":"code","da1632d0":"code","0388b31f":"code","cf1de966":"code","7debd389":"code","ee94f181":"code","fc6ffaa3":"code","3b3c218d":"code","2e19eff6":"code","024663f6":"code","82d5dcda":"code","5573134b":"code","78b5092a":"code","b14b7ca0":"code","a50527c2":"code","e12e03fd":"code","f8d139a6":"code","06b57139":"code","ae5ab1a6":"code","ba47830a":"code","6b7117e5":"code","592041a8":"code","2317028b":"code","1bfafe8d":"code","146f95ab":"code","96f72ace":"code","7eb880d9":"code","5c637052":"code","0731b6fc":"code","1a9618fc":"code","1425e323":"code","a5c33191":"code","76a4da63":"code","b599c434":"code","b1938b48":"code","b0dbeadc":"code","b319915b":"code","086edc45":"code","b82981e0":"code","26ff2b93":"code","5cf2ac5d":"code","d8fa65cd":"code","904e2208":"code","50ae0b05":"code","c60b2b35":"code","60f232a5":"code","a70397b7":"code","274bc9cb":"code","4b2d08fc":"code","e298f4f9":"code","8404f1f4":"code","74a5d774":"code","cb4cafde":"code","35a5f257":"code","01fc3866":"code","ce0de344":"markdown","9ad1e3b0":"markdown","6e8d3909":"markdown","b58b1799":"markdown","b63fe1ec":"markdown","3d867578":"markdown","3056c2b3":"markdown","4edd1f89":"markdown","64d199b4":"markdown","dafbee5d":"markdown","3d122f4f":"markdown","decabf79":"markdown","2d57c206":"markdown","20fa6ca5":"markdown","13068eff":"markdown","5c08c198":"markdown","3669d97a":"markdown"},"source":{"b45bb636":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","128cc934":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom statistics import mean,stdev\nfrom numpy import arange\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","08515b98":"df = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\ndf.head(20)","8ca5c3f1":"df.shape","f14c9f97":"df.info()","6ffd071d":"df.describe()","7a587abf":"df.dtypes","58a52883":"# Checking unique values in each column\ndf.nunique()","0eb7f5f5":"# Checking null values in Rows\nsns.countplot((100*(df.isnull().sum(axis = 1) \/ df.shape[1])).round(2).sort_values(ascending = False))","4b648c3f":"# Droppping rows which has less than 15 not null entries\ndf = df.dropna(axis = 0, thresh = 15)\nsns.countplot((100*(df.isnull().sum(axis = 1) \/ df.shape[1])).round(2).sort_values(ascending = False))","c89552c4":"df.shape","5b71539d":"# Checking null values in Columns\n(100*(df.isnull().sum() \/ df.shape[0])).round(2).sort_values(ascending = False) ","9647cec8":"columns_to_be_imputed = []\nfor i in df.columns:\n    null_percent = (100*(df[i].isnull().sum() \/ df.shape[0]))\n    if(null_percent > 0):\n        columns_to_be_imputed.append(i)\n\ncolumns_to_be_imputed","685f366d":"# Seeing distributions of all the features before imputing missing values\ndf.hist(figsize=(12,12))","4f0293a9":"# Separating Numerical and Categorical Features \nnumeric_features = ['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\ncategoric_features = ['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']","483eb235":"#Imputing Features\nfor i in columns_to_be_imputed:\n    df[i] = df[i].fillna(df[i].mode()[0])","7d05f43d":"# Checking null values in Columns\n(100*(df.isnull().sum() \/ df.shape[0])).round(2).sort_values(ascending = False) ","cf53abb4":"# Calculating Skewness and Kurtosis of the inputs\ndef checking_skew(dff):\n    sk = skew(dff[numeric_features])\n    skew_df = pd.DataFrame()\n    skew_df['Columns'] = numeric_features\n    skew_df['Skew'] = sk\n    skew_df['IsSkewed'] = np.absolute(sk > 1)\n    print(skew_df)","e5dc2abe":"checking_skew(df)","590eb518":"df=df[df.glucose<np.nanpercentile(df['glucose'], 98.5)]\ndf=df[df.cigsPerDay<np.nanpercentile(df['cigsPerDay'], 98.5)]\ndf=df[df.sysBP<np.nanpercentile(df['sysBP'], 99)]\nchecking_skew(df)","f60b85d4":"# Converting education and BPMeds from float to int\nprint(df['education'])\nprint(df['BPMeds'])","f3a0e8d6":"df['education'] = df['education'].astype('int64')\ndf['BPMeds'] = df['education'].astype('int64')","5ada5194":"# Converting Categorical Numeric Values to type category\nfor i in categoric_features:\n    df[i] = df[i].astype('category')","8a145c85":"df.dtypes","ea99549e":"df.head()","26f16181":"dummy = pd.get_dummies(df[categoric_features], drop_first=True)\ndummy.head()","7a8e4898":"df1 = df.drop(categoric_features, axis = 1)","26e12b37":"df1 = pd.concat([df1, dummy], axis = 1)\ndf1.head()","ba9c95e7":"df1.columns","b35c5242":"df1.shape","0b915481":"# Plotting Categorical Features\ndef plotting_cat_features(df, cols):\n    for i in cols:\n        plt.figure(figsize=(15,5))\n        sns.countplot(df[i], hue = df['TenYearCHD'])\n        plt.title(i.upper(), fontsize=18, color=\"indigo\")\n        plt.show()","ac10b069":"plotting_cat_features(df, categoric_features)","323aed43":"#Plotting Numerical Features\n\nplt.figure(figsize=(12,12))\ni = 1\nfor c in numeric_features:\n    plt.subplot(3,3,i)\n    sns.boxplot(y = c, data = df, color='cyan')\n    plt.title(c.upper(), fontsize=18, color=\"indigo\")\n    i = i+1\n    plt.tight_layout()","7a2575b3":"#Plotting Correlation\nplt.figure(figsize = (10,5))\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()","87244d71":"# Multivariate Data Analysis\n\nsns.pairplot(df,diag_kind='kde',hue='TenYearCHD')\nplt.show()","8dae2065":"plt.figure(figsize = (13,13))\nsns.heatmap(df1.corr(), cmap=\"YlGnBu\", annot = True, fmt='.2f')\nplt.show()","e04cbee8":"# Plotting Target Variable\nplt.figure(figsize=(10,5))\nsns.countplot(df['TenYearCHD'])\nplt.title(\"TenYearCHD\", fontsize=18, color=\"indigo\")\nplt.show()","da1632d0":"df1.shape","0388b31f":"X = df1.drop(['TenYearCHD'], axis = 1)\ny = df1.TenYearCHD","cf1de966":"#ROC score over imbalanced data\n\nmodel = LogisticRegression(solver = 'liblinear')\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","7debd389":"#ROC score after over-sampling minority type of Target Feature \n\nmodel = LogisticRegression(solver='liblinear')\n\nprint(\"Shape Before Resampling\", X.shape, y.shape)\nprint(\"Value count of Target Feature before resampling\")\nprint(y.value_counts(), '\\n')\n\nsmote = SMOTE(random_state = 1)\nX_res, y_res = smote.fit_resample(X, y)\n\nprint(\"Shape After Resampling\", X_res.shape, y_res.shape)\nprint(\"Value count of Target Feature after resampling\")\nprint(y_res.value_counts())\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X_res, y_res, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))                             ","ee94f181":"def plot_score(range1, scores):\n    plt.plot(range1, scores, linewidth=4)\n    plt.title(\"SMOTE Learning Curve\", fontsize=16)\n    plt.gca().set_xlabel(\"# of Points per Class\", fontsize=14)\n    plt.gca().set_ylabel(\"Training Accuracy\", fontsize=14)\n    sns.despine()","fc6ffaa3":"# Determing the learning curve for OverSampling\nscores = []\nfor i in arange(0.2, 1.0, 0.1):\n    smote = SMOTE(sampling_strategy = i, random_state = 1)\n    X_a, y_a = smote.fit_resample(X, y)\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    score = cross_val_score(model, X_a, y_a, scoring='roc_auc', cv=cv, n_jobs=-1)\n    scores.append(mean(score))\n    \nplot_score(arange(0.2, 1.0, 0.1), scores)","3b3c218d":"scores1 = []\n\n#Plotting learning curve for Sampling\nfor i in arange(0.2, 0.7, 0.1):\n    if i <= 0.5:\n        smote = SMOTE(sampling_strategy = i, random_state = 1)\n        X_a, y_a = smote.fit_resample(X, y)\n        under = RandomUnderSampler(sampling_strategy = 1 - i, random_state = 1)\n        X_b, y_b = under.fit_resample(X_a, y_a)\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n        score = cross_val_score(model, X_b, y_b, scoring='roc_auc', cv=cv, n_jobs=-1)\n        scores1.append(mean(score))\n    else:\n        if i == 0.6:\n            b = 0.9\n        else:\n            b = 0.8\n        smote = SMOTE(sampling_strategy = i, random_state = 1)\n        X_a, y_a = smote.fit_resample(X, y)\n        under = RandomUnderSampler(sampling_strategy = b, random_state = 1)\n        X_b, y_b = under.fit_resample(X_a, y_a)\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n        score = cross_val_score(model, X_b, y_b, scoring='roc_auc', cv=cv, n_jobs=-1)\n        scores1.append(mean(score))\n        \nplot_score(arange(0.2, 0.7, 0.1), scores1)","2e19eff6":"#ROC score after over-sampling minority type and under-sampling of majority type of Target Feature\n\nmodel = LogisticRegression(solver='liblinear')\n\nprint(\"Shape Before Resampling\", X.shape, y.shape)\nprint(\"Value count of Target Feature before resampling\")\nprint(y.value_counts())\nprint('\/n')\n\nover = SMOTE(sampling_strategy=0.7, random_state = 1)\nX_res, y_res = over.fit_resample(X, y)\nprint(\"Shape After Oversampling\", X_res.shape, y_res.shape)\nprint(\"Value count of Target Feature after Oversampling\")\nprint(y_res.value_counts())\nprint('\/n')\n\nunder = RandomUnderSampler(sampling_strategy = 0.8, random_state = 1)\nX_res, y_res = under.fit_resample(X_res, y_res)\nprint(\"Shape After Undersampling\", X_res.shape, y_res.shape)\nprint(\"Value count of Target Feature after Undersampling\")\nprint(y_res.value_counts())\nprint('\/n')\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X_res, y_res, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","024663f6":"X = X_res\ny = y_res","82d5dcda":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of y_train\", y_train.shape)\nprint(\"Shape of y_test\", y_test.shape)","5573134b":"final_cols = X_train.columns","78b5092a":"X_train.head()","b14b7ca0":"scaler = MinMaxScaler()\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = scaler.transform(X_test[numeric_features])","a50527c2":"# Functions to repeat Logistic regression model and VIF calculations\n\n# Build logistic regression model\ndef building_log_reg_model(X_train, cols):\n    X_train_local = X_train[cols]\n    X_train_sm = sm.add_constant(X_train_local)\n    log_model = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial()).fit()\n    return(log_model)\n\n# Calculate VIF\ndef calculate_VIF(X_train, cols):\n    vif = pd.DataFrame()\n    vif['Features'] = cols\n    vif['VIF'] = [variance_inflation_factor(X_train[cols].values,i) for i in range(X_train[cols].shape[1])]\n    vif['VIF'] = round(vif['VIF'],2)\n    vif.sort_values(by='VIF', ascending = False, inplace=True)  \n    return(vif)","e12e03fd":"log_reg1 = building_log_reg_model(X_train, final_cols)\nlog_reg1.summary()","f8d139a6":"calculate_VIF(X_train, final_cols)","06b57139":"cols_cols = final_cols.drop(['education_2', 'education_3', 'education_4'])\nlog_reg2 = building_log_reg_model(X_train, cols_cols)\nlog_reg2.summary()","ae5ab1a6":"calculate_VIF(X_train, cols_cols)","ba47830a":"cols_cols = cols_cols.drop(['glucose'])\nlog_reg3 = building_log_reg_model(X_train, cols_cols)\nlog_reg3.summary()","6b7117e5":"calculate_VIF(X_train, cols_cols)","592041a8":"cols_cols = cols_cols.drop(['prevalentHyp_1'])\nlog_reg4 = building_log_reg_model(X_train, cols_cols)\nlog_reg4.summary()","2317028b":"calculate_VIF(X_train, cols_cols)","1bfafe8d":"cols_cols = cols_cols.drop(['diaBP'])\nlog_reg5 = building_log_reg_model(X_train, cols_cols)\nlog_reg5.summary()","146f95ab":"calculate_VIF(X_train, cols_cols)","96f72ace":"cols_cols = cols_cols.drop(['BMI'])\nlog_reg6 = building_log_reg_model(X_train, cols_cols)\nlog_reg6.summary()","7eb880d9":"calculate_VIF(X_train, cols_cols)","5c637052":"cols_cols = cols_cols.drop(['totChol'])\nlog_reg7 = building_log_reg_model(X_train, cols_cols)\nlog_reg7.summary()","0731b6fc":"calculate_VIF(X_train, cols_cols)","1a9618fc":"cols_cols = cols_cols.drop(['sysBP'])\nlog_reg8 = building_log_reg_model(X_train, cols_cols)\nlog_reg8.summary()","1425e323":"calculate_VIF(X_train, cols_cols)","a5c33191":"cols_cols = cols_cols.drop(['prevalentStroke_1'])\nlog_reg9 = building_log_reg_model(X_train, cols_cols)\nlog_reg9.summary()","76a4da63":"calculate_VIF(X_train, cols_cols)","b599c434":"cols_cols = cols_cols.drop(['heartRate'])\nlog_reg10 = building_log_reg_model(X_train, cols_cols)\nlog_reg10.summary()","b1938b48":"calculate_VIF(X_train, cols_cols)","b0dbeadc":"sns.heatmap(X_train[cols_cols].corr(), annot=True)","b319915b":"log_reg_final = log_reg10\nX_train_final = X_train[cols_cols]\n\nX_train_sm = sm.add_constant(X_train_final)\ny_train_pred = log_reg_final.predict(X_train_sm).values\ny_train_pred","086edc45":"y_train_pred_df = pd.DataFrame()\ny_train_pred_df['CHD Positive'] = y_train.values\ny_train_pred_df['CHD Predict Prob'] = y_train_pred\ny_train_pred_df['CHD Predicted'] = y_train_pred_df['CHD Predict Prob'].map(lambda x : 1 if x > 0.5 else 0)\ny_train_pred_df.head()","b82981e0":"# Checking Training Accuracy Score\naccuracy_score(y_true= y_train_pred_df['CHD Positive'], y_pred= y_train_pred_df['CHD Predicted'])","26ff2b93":"cf_matrix = confusion_matrix(y_true= y_train_pred_df['CHD Positive'], y_pred= y_train_pred_df['CHD Predicted'])\ncf_matrix","5cf2ac5d":"TP = cf_matrix[1,1] \nTN = cf_matrix[0,0] \nFP = cf_matrix[0,1] \nFN = cf_matrix[1,0] \n\nprint(\"Sensitivity =\", (TP\/(TP+FN)).round(2))\nprint(\"Specificity =\", (TN\/(TN+FP)).round(2))\nprint(\"False Positive Rate =\", (FP\/(TN+FP)).round(2))\nprint(\"Positive Predictive Power =\", (TP\/(TP+FP)).round(2))\nprint(\"Negative Predictive Power =\", (TN\/(TN+ FN)).round(2))","d8fa65cd":"#ROC Curve\n\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndraw_roc(y_train_pred_df['CHD Positive'],y_train_pred_df['CHD Predicted'])","904e2208":"range_ = arange(0.0, 1.0, 0.1)\n\nfor i in range_:\n    col_name = \"CutOff=\"+ str(i.round(1))\n    y_train_pred_df[col_name] = y_train_pred_df['CHD Predict Prob'].map(lambda x : 1 if x > i else 0)\n\ny_train_pred_df.head()","50ae0b05":"cutoff_df = pd.DataFrame(columns = ['Cutoff','Accuracy','Sensitivity','Specificity'])\n\nfor i in range_:\n    col_name = \"CutOff=\"+ str(i.round(1))\n    cf_mat =  confusion_matrix(y_true= y_train_pred_df['CHD Positive'], y_pred= y_train_pred_df[col_name])\n    total=sum(sum(cf_mat))\n    accuracy = (cf_mat[0,0]+cf_mat[1,1])\/total\n    speci = cf_mat[0,0]\/(cf_mat[0,0]+cf_mat[0,1])\n    sensi = cf_mat[1,1]\/(cf_mat[1,0]+cf_mat[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    \ncutoff_df","c60b2b35":"# Plotting - Accuracy , Sensitivity and specificity\nplt.figure(figsize=(18,18))\ncutoff_df.plot.line(x='Cutoff', y=['Accuracy','Sensitivity','Specificity'])\nplt.xticks(np.arange(0,1,step=0.05),size=8)\nplt.axvline(x=0.49, color='r', linestyle='--') # additing axline\nplt.yticks(size=12)\nplt.show()","60f232a5":"y_train_pred_df['CHD Predicted'] = y_train_pred_df['CHD Predict Prob'].map(lambda x : 1 if x > 0.49 else 0)\ny_train_pred_df.head()","a70397b7":"accuracy_score(y_train_pred_df['CHD Positive'], y_train_pred_df['CHD Predicted'])","274bc9cb":"cf_matrix = confusion_matrix(y_true= y_train_pred_df['CHD Positive'], y_pred= y_train_pred_df['CHD Predicted'])\ncf_matrix","4b2d08fc":"TP = cf_matrix[1,1] \nTN = cf_matrix[0,0] \nFP = cf_matrix[0,1] \nFN = cf_matrix[1,0] \n\nprint(\"Sensitivity =\", (TP\/(TP+FN)).round(2))\nprint(\"Specificity =\", (TN\/(TN+FP)).round(2))\nprint(\"False Positive Rate =\", (FP\/(TN+FP)).round(2))\nprint(\"Positive Predictive Power =\", (TP\/(TP+FP)).round(2))\nprint(\"Negative Predictive Power =\", (TN\/(TN+ FN)).round(2))","e298f4f9":"X_test_final = X_test[cols_cols]\nX_test.head()","8404f1f4":"X_test_sm = sm.add_constant(X_test_final)\n\ny_test_pred = log_reg_final.predict(X_test_sm).values\ny_test_pred","74a5d774":"y_test_pred_df = pd.DataFrame()\ny_test_pred_df['CHD Positive'] = y_test.values\ny_test_pred_df['CHD Predict Prob'] = y_test_pred\ny_test_pred_df['CHD Predicted'] = y_test_pred_df['CHD Predict Prob'].map(lambda x : 1 if x > 0.49 else 0)\ny_test_pred_df.head()","cb4cafde":"# Checking Testing Accuracy Score\naccuracy_score(y_true= y_test_pred_df['CHD Positive'], y_pred= y_test_pred_df['CHD Predicted'])","35a5f257":"#Confusion Matrix\ncf_matrix = confusion_matrix(y_true= y_test_pred_df['CHD Positive'], y_pred= y_test_pred_df['CHD Predicted'])\ncf_matrix","01fc3866":"TP = cf_matrix[1,1] \nTN = cf_matrix[0,0] \nFP = cf_matrix[0,1] \nFN = cf_matrix[1,0] \n\nprint(\"Sensitivity =\", (TP\/(TP+FN)).round(2))\nprint(\"Specificity =\", (TN\/(TN+FP)).round(2))\nprint(\"False Positive Rate =\", (FP\/(TN+FP)).round(2))\nprint(\"Positive Predictive Power =\", (TP\/(TP+FP)).round(2))\nprint(\"Negative Predictive Power =\", (TN\/(TN+ FN)).round(2))","ce0de344":"**Not much NaNs - Column-wise. We have to impute them**","9ad1e3b0":"# Calculating p-values and VIF","6e8d3909":"# Scaling Data","b58b1799":"**Glucose, sysBP, cigsPerDay are skewed. We have to correct it**","b63fe1ec":"# Loading, Understanding and Preparing Data","3d867578":"**Now Almost all the features are Moderately skewed, which is fine**","3056c2b3":"# Imports","4edd1f89":"# EDA","64d199b4":"**Education and BPMeds are in Categorical Float, should change them to int64**","dafbee5d":"**Since we have only 16 columns, I will set the threshold as minimum 15 columns should be Not Null**","3d122f4f":"# Using SMOTE to balance the imbalanced Data Set","decabf79":"# Model Evaluation","2d57c206":"**Most of the columns are skewed, so we have to impute using Mode**","20fa6ca5":"**This is an imbalanced Dataset so we can try SMOTE in this**","13068eff":"# Splitting Train Test Data","5c08c198":"**As we oversample and undersample more, the roc_score keeps on increasing**","3669d97a":"# Test Data Prediction"}}