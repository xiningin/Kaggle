{"cell_type":{"f45f56b3":"code","e40c7440":"code","96c48a2a":"code","2ca46548":"code","be8134f2":"code","b6dea9c7":"code","fc865054":"code","3be6dc57":"code","04a5347f":"code","4d84ca4c":"code","1e43d629":"code","1d95f454":"code","0d656556":"code","fd1e6d3a":"code","5186c083":"code","ad2b8842":"code","6b753618":"code","648c3c7e":"code","1e309df7":"code","8615c624":"code","af95b1d4":"code","131ceaca":"code","e22da6ec":"code","e6120a4c":"code","03f35af2":"code","1707084f":"code","616e74ae":"code","4b42aa3a":"code","5c30248a":"code","c45d06ba":"code","f00cce6a":"code","1e9858e1":"code","52bbd1f4":"code","68799af7":"code","18a7900e":"code","4bf9acb3":"code","fc9ebf31":"code","34c880b9":"code","8fce2af4":"code","903844f0":"code","86d4db24":"code","3e0aaa3e":"code","2b04f71d":"code","e2a9225c":"code","3d64b4f5":"code","2ce8e2a2":"code","ba1ae718":"code","0ca2ee59":"code","f823d212":"code","89501cf7":"code","9a9607e4":"code","45f5f2ab":"code","65dddc7d":"code","e42ddf62":"code","ce37df85":"code","7ef2fcd4":"code","f885947c":"code","35ef53b2":"code","a6ce3746":"code","df0a64b1":"code","3be329a3":"markdown","29d109f4":"markdown","a6a533cb":"markdown","ef58e242":"markdown","26b280f4":"markdown","f663e13e":"markdown","aa42e67f":"markdown","02d40ac2":"markdown","6260be15":"markdown","0f1b21b0":"markdown","823dadc6":"markdown","4208da62":"markdown","eaa53585":"markdown","80e31ddb":"markdown","60fb404e":"markdown","88b9997e":"markdown","7e736abe":"markdown","7c6a6566":"markdown","a116e1b7":"markdown","10500427":"markdown","438046de":"markdown","b59c9d35":"markdown","1773db5f":"markdown","34bc5fe0":"markdown","f286b8b6":"markdown","9afb1ad2":"markdown","2ced26fb":"markdown","7aa3e455":"markdown","1f16fe3a":"markdown","b8a44e30":"markdown"},"source":{"f45f56b3":"import numpy as np\nimport pandas as pd","e40c7440":"data_train = pd.read_csv('..\/input\/train.csv')\ndata_train.head()","96c48a2a":"data_test = pd.read_csv('..\/input\/test.csv')\ndata_test.head()","2ca46548":"df = data_train.append(data_test, sort = True)\ndf.shape","be8134f2":"df.info()\nprint('-------------------------------------')\nprint(pd.isnull(df).sum())","b6dea9c7":"df.describe()","fc865054":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.style.use('bmh')\nplt.rc('font', family='DejaVu Sans', size=13)","3be6dc57":"cat_list = ['Cabin', 'Embarked', 'Name', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Ticket']\nfor n, i in enumerate(cat_list):\n    cat_num = len(df[i].value_counts().index)\n    print('The feature \"%s\" has %d values.' % (i, cat_num))","04a5347f":"f, [ax1, ax2, ax3] = plt.subplots(1, 3, figsize = (20, 5))\nsns.countplot(x = 'Sex', hue = 'Survived', data = data_train, ax = ax1)\nsns.countplot(x = 'Pclass', hue = 'Survived', data = data_train, ax = ax2)\nsns.countplot(x = 'Embarked', hue = 'Survived', data = data_train, ax = ax3)\nf.suptitle('Nominal\/Ordinal feature', size = 20, y = 1.1)\n\nf, [ax1, ax2] = plt.subplots(1, 2, figsize = (20, 5))\nsns.countplot(x = 'SibSp', hue = 'Survived', data = data_train, ax = ax1)\nsns.countplot(x = 'Parch', hue = 'Survived', data = data_train, ax = ax2)\n\nplt.show()","4d84ca4c":"grid = sns.FacetGrid(df, col = 'Pclass', hue = 'Sex', palette = 'seismic', height = 5)\ngrid.map(sns.countplot, 'Embarked', alpha = 0.8)\ngrid.add_legend()","1e43d629":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'Pclass', hue = 'Survived', palette = 'seismic', height = 5)\ngrid.map(sns.countplot, 'Embarked', alpha = 0.8)\ngrid.add_legend()","1d95f454":"f, ax = plt.subplots(figsize = (10, 5))\nsns.kdeplot(data_train.loc[data_train.Survived == 0, 'Age'], color = 'gray', shade = True, label = 'dead')\nsns.kdeplot(data_train.loc[data_train.Survived == 1, 'Age'], color = 'green', shade = True, label = 'survived')\nplt.title('Survival rate in different age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')","0d656556":"f, [ax1, ax2] = plt.subplots(1, 2, figsize = (20, 6))\nsns.boxplot(x = 'Pclass', y = 'Age', data = data_train, ax = ax1)\nsns.swarmplot(x = 'Pclass', y = 'Age', data = data_train, ax = ax1)\nsns.kdeplot(data_train.loc[data_train.Pclass == 3, 'Age'], color = 'b', shade = True, label = 'Pclass 3', ax = ax2)\nsns.kdeplot(data_train.loc[data_train.Pclass == 2, 'Age'], color = 'g', shade = True, label = 'Pclass 2', ax = ax2)\nsns.kdeplot(data_train.loc[data_train.Pclass == 1, 'Age'], color = 'r', shade = True, label = 'Pclass 1', ax = ax2)\nax1.set_title('Pclass-Age box-plot')\nax2.set_title('Pclass-Age kde-plot')\nf.show()","fd1e6d3a":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'Pclass', hue = 'Survived', palette = 'seismic', height = 3.5)\ngrid.map(plt.scatter, 'PassengerId', 'Age', alpha = 0.8)\ngrid.add_legend()","5186c083":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'Embarked', hue = 'Survived', palette = 'seismic', height = 3.5)\ngrid.map(plt.scatter, 'PassengerId', 'Age', alpha = 0.8)\ngrid.add_legend()","ad2b8842":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'SibSp', hue = 'Survived', palette = 'seismic', height = 3.5)\ngrid.map(plt.scatter, 'PassengerId', 'Age', alpha = 0.8)\ngrid.add_legend()","6b753618":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'Parch', hue = 'Survived', palette = 'seismic', height = 3.5)\ngrid.map(plt.scatter, 'PassengerId', 'Age', alpha = 0.8)\ngrid.add_legend()","648c3c7e":"f, ax = plt.subplots(figsize = (10, 5))\nsns.kdeplot(data_train.loc[data_train.Survived == 0, 'Fare'], color = 'gray', shade = True, label = 'dead')\nsns.kdeplot(data_train.loc[data_train.Survived == 1, 'Fare'], color = 'green', shade = True, label = 'survived')\nplt.title('Survival rate in different fare')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')","1e309df7":"f, [ax1, ax2] = plt.subplots(1, 2, figsize = (20, 6))\nsns.boxplot(x = 'Pclass', y = 'Fare', data = data_train, ax = ax1)\nsns.swarmplot(x = 'Pclass', y = 'Fare', data = data_train, ax = ax1)\nsns.kdeplot(data_train.loc[data_train.Pclass == 3, 'Fare'], color = 'b', shade = True, label = 'Pclass 3', ax = ax2)\nsns.kdeplot(data_train.loc[data_train.Pclass == 2, 'Fare'], color = 'g', shade = True, label = 'Pclass 2', ax = ax2)\nsns.kdeplot(data_train.loc[data_train.Pclass == 1, 'Fare'], color = 'r', shade = True, label = 'Pclass 1', ax = ax2)\nax1.set_title('Pclass-Fare box-plot')\nax2.set_title('Pclass-Fare kde-plot')\nf.show()","8615c624":"grid = sns.FacetGrid(data_train, row = 'Sex', col = 'Pclass', hue = 'Survived', palette = 'seismic', height = 3.5)\ngrid.map(plt.scatter, 'Age', 'Fare', alpha = 0.8)\ngrid.add_legend()","af95b1d4":"g = sns.pairplot(data_train[['Survived', 'Pclass', 'Sex', 'Age', 'Parch', 'Fare', 'Embarked']], hue = 'Survived', palette = 'seismic', height = 4, diag_kind = 'kde', diag_kws = dict(shade = True), plot_kws = dict(s = 50, alpha = 0.8))\ng.set(xticklabels = [])","131ceaca":"df.isnull().sum()","e22da6ec":"df[df.Fare.isnull()]","e6120a4c":"df[(df.Pclass == 3) & (df.Age > 60) & (df.Sex == 'male')]","03f35af2":"fare_mean = df[(df.Pclass == 3) & (df.Age > 60) & (df.Sex == 'male')].Fare.mean()\ndf.loc[df.PassengerId == 1044, 'Fare'] = fare_mean\ndf[df.PassengerId == 1044]","1707084f":"df[df.Embarked.isnull()]","616e74ae":"df.Embarked = df.Embarked.fillna('C')","4b42aa3a":"df.Cabin = df.Cabin.fillna('0')\nlen(df.Cabin.value_counts().index)","5c30248a":"df['CabinCat'] = pd.Categorical(df.Cabin.apply(lambda x : x[0])).codes","c45d06ba":"f, ax = plt.subplots(figsize = (10, 5))\nsns.countplot('CabinCat', hue = 'Survived', data = df, ax = ax)\nf.show()","f00cce6a":"import re\nfrom sklearn.preprocessing import LabelEncoder","1e9858e1":"# Title\ndf['Title'] = df.Name.apply(lambda x : re.search(' ([a-zA-Z]+)\\.', x).group(1))\ntitle_mapping = {'Mr' : 1, 'Miss' : 2, 'Mrs' : 3, 'Master' : 4, 'Dr' : 5, 'Rev' : 6, 'Major' : 7, 'Col' : 7, 'Mlle' : 2, 'Mme' : 3, 'Don' : 9, 'Dona' : 9, 'Lady' : 10, 'Countess' : 10, 'Jonkheer' : 10, 'Sir' : 9, 'Capt' : 7, 'Ms' : 2}\ndf['TitleCat'] = df.Title.map(title_mapping)\n\n# FamilySize\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\n# FamilyName\ndf['FamilyName'] = df.Name.apply(lambda x : str.split(x, ',')[0])\n\n# IsAlone\ndf['IsAlone'] = 0\ndf.loc[df.FamilySize == 1, 'IsAlone'] = 1\n\n# NameLength\nle = LabelEncoder()\ndf['NameLength'] = df.Name.apply(lambda x : len(x))\ndf['NameLengthBin'] = pd.qcut(df.NameLength, 5)\ndf['NameLengthBinCode'] = le.fit_transform(df.NameLengthBin)\n\n# Embarked\ndf['Embarked'] = pd.Categorical(df.Embarked).codes\n\n# Sex\ndf = pd.concat([df, pd.get_dummies(df.Sex)], axis = 1)\n\n# Ticket\ntable_ticket = pd.DataFrame(df.Ticket.value_counts())\ntable_ticket.rename(columns = {'Ticket' : 'TicketNum'}, inplace = True)\ntable_ticket['TicketId'] = pd.Categorical(table_ticket.index).codes\ntable_ticket.loc[table_ticket.TicketNum < 3, 'TicketId'] = -1\ndf = pd.merge(left = df, right = table_ticket, left_on = 'Ticket', right_index = True, how = 'left', sort = False)\ndf['TicketCode'] = list(pd.cut(df.TicketId, bins = [-2, 0, 500, 1000], labels = [0, 1, 2]))\n\n# CabinNum\nregex = re.compile('\\s*(\\w+)\\s*')\ndf['CabinNum'] = df.Cabin.apply(lambda x : len(regex.findall(x)))","52bbd1f4":"from sklearn.ensemble import ExtraTreesRegressor","68799af7":"classers = ['Fare','Parch','Pclass','SibSp','TitleCat', 'CabinCat','female','male', 'Embarked', 'FamilySize', 'IsAlone', 'NameLengthBinCode','TicketNum','TicketCode']\n\netr = ExtraTreesRegressor(n_estimators = 200, random_state = 0)\nage_X_train = df[classers][df.Age.notnull()]\nage_y_train = df.Age[df.Age.notnull()]\nage_X_test = df[classers][df.Age.isnull()]\n\netr.fit(age_X_train, np.ravel(age_y_train))\nage_pred = etr.predict(age_X_test)\ndf.loc[df.Age.isnull(), 'Age'] = age_pred","18a7900e":"age_X_test['Age'] = age_pred\n\nf, ax = plt.subplots(figsize = (10, 5))\nsns.boxplot('Pclass', 'Age', data = age_X_test, ax = ax)\nsns.swarmplot('Pclass', 'Age', data = age_X_test, ax = ax)","4bf9acb3":"# Identity\nchildAge = 18\ndef getIdentity(passenger):\n    age, sex = passenger\n    \n    if age < childAge:\n        return 'child'\n    elif sex == 'male':\n        return 'male_adult'\n    else:\n        return 'female_adult'\n\ndf = pd.concat([df, pd.DataFrame(df[['Age', 'Sex']].apply(getIdentity, axis = 1), columns = ['Identity'])], axis = 1)\ndf = pd.concat([df, pd.get_dummies(df.Identity)], axis = 1)","fc9ebf31":"# FamilySurvival\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\n\nfor _, grp_df in df.groupby(['FamilyName', 'Fare']):\n    if len(grp_df) != 1 :\n        for index, row in grp_df.iterrows():\n            smax = grp_df.drop(index).Survived.max()\n            smin = grp_df.drop(index).Survived.min()\n            pid = row.PassengerId\n            \n            if smax == 1:\n                df.loc[df.PassengerId == pid, 'FamilySurvival'] = 1.0\n            elif smin == 0:\n                df.loc[df.PassengerId == pid, 'FamilySurvival'] = 0.0\nfor _, grp_df in df.groupby(['Ticket']):\n    if len(grp_df != 1):\n        for index, row in grp_df.iterrows():\n            if (row.FamilySurvival == 0.0 or row.FamilySurvival == 0.5):\n                smax = grp_df.drop(index).Survived.max()\n                smin = grp_df.drop(index).Survived.min()\n                pid = row.PassengerId\n                \n                if smax == 1:\n                    df.loc[df.PassengerId == pid, 'FamilySurvival'] = 1.0\n                elif smin == 0:\n                    df.loc[df.PassengerId == pid, 'FamilySurvival'] = 0.0\n                    \ndf.FamilySurvival.value_counts()","34c880b9":"# FareBinCode\ndf['FareBin'] = pd.qcut(df.Fare, 5)\n\nle = LabelEncoder()\ndf['FareBinCode'] = le.fit_transform(df.FareBin)","8fce2af4":"# AgeBinCode\ndf['AgeBin'] = pd.qcut(df.Age, 4)\n\nle = LabelEncoder()\ndf['AgeBinCode'] = le.fit_transform(df.AgeBin)","903844f0":"from sklearn.preprocessing import MinMaxScaler, StandardScaler","86d4db24":"target = data_train['Survived'].values\nselect_features = ['AgeBinCode', 'Embarked', 'FareBinCode', 'Parch', 'Pclass', 'SibSp', 'CabinCat', 'TitleCat', 'FamilySize', 'IsAlone', 'FamilySurvival', 'NameLengthBinCode', 'female', 'male', 'TicketNum', 'TicketCode', 'CabinNum', 'child', 'female_adult', 'male_adult']","3e0aaa3e":"#unimportant_features = ['Parch', 'Embarked', 'child', 'CabinNum']\n#select_features = list(set(select_features).difference(unimportant_features))","2b04f71d":"#scaler = MinMaxScaler()\nscaler = StandardScaler()\n\n#df_scaled = df[select_features]\ndf_scaled = scaler.fit_transform(df[select_features])\n\ntrain = df_scaled[0:891].copy()\ntest = df_scaled[891:].copy()","e2a9225c":"from sklearn.feature_selection import SelectKBest, f_classif","3d64b4f5":"selector = SelectKBest(f_classif, len(select_features))\nselector.fit(train, target)\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], select_features[indices[i]]))","2ce8e2a2":"df_corr = df[select_features].copy()\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize = (16, 16))\nsns.heatmap(df_corr.corr(), linewidths = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)\nplt.show()","ba1ae718":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nkf = KFold(n_splits = 5, random_state = 1)","0ca2ee59":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_parameters = {'max_depth' : [5], 'n_estimators' : [500], 'min_samples_split' : [9], 'random_state' : [1], 'n_jobs' : [-1]}\nrfc = RandomForestClassifier()\nclf_rfc = GridSearchCV(rfc, rfc_parameters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\n\nclf_rfc.fit(train, target)","f823d212":"rfc2_parameters = {'max_depth' : [2, 5, 8, 10, 20, 50], 'n_estimators' : [10, 50, 100, 200, 500, 1000, 2000], 'min_samples_split' : [2, 3, 5, 9, 20]}\nrfc2 = RandomForestClassifier(random_state = 1, n_jobs = -1)\nclf_rfc2 = RandomizedSearchCV(rfc2, rfc2_parameters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\n\nclf_rfc2.fit(train, target)","89501cf7":"importance = clf_rfc.best_estimator_.feature_importances_\nindices = np.argsort(importance)[::-1]\n\nprint(clf_rfc.best_score_)\nprint(clf_rfc.score(train, target))\nprint(clf_rfc.best_params_)\nprint('\\nFeature importance:')\nfor i in range(len(select_features)):\n    print('%.2f %s' % (importance[indices[i]], select_features[indices[i]]))","9a9607e4":"from sklearn.linear_model import LogisticRegression\n\nlr_paramaters = {'C' : [0.05, 0.1, 0.2], 'random_state' : [1]}\nlr = LogisticRegression()\n\nclf_lr = GridSearchCV(lr, lr_paramaters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\nclf_lr.fit(train, target)","45f5f2ab":"print(clf_lr.best_score_)\nprint(clf_lr.score(train, target))\nprint(clf_lr.best_params_)","65dddc7d":"from sklearn.svm import SVC\n\nsvc_paramaters = {'C' : [5.5, 6, 6.5], 'kernel' : ['linear', 'rbf'], 'gamma' : ['auto', 'scale'], 'random_state' : [1]}\nsvc = SVC()\n\nclf_svc = GridSearchCV(svc, svc_paramaters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\nclf_svc.fit(train, target)","e42ddf62":"print(clf_svc.best_score_)\nprint(clf_svc.score(train, target))\nprint(clf_svc.best_params_)","ce37df85":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbdt_parameters = {'subsample' : [1], 'min_samples_leaf' : [3], 'learning_rate' : [0.1], 'n_estimators' : [50], 'min_samples_split' : [2], 'max_depth' : [3], 'random_state' : [1]}\ngbdt = GradientBoostingClassifier()\n\nclf_gbdt = GridSearchCV(gbdt, gbdt_parameters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\nclf_gbdt.fit(train, target)","7ef2fcd4":"print(clf_gbdt.best_score_)\nprint(clf_gbdt.score(train, target))\nprint(clf_gbdt.best_params_)","f885947c":"from xgboost import XGBClassifier\n\nxgb_paramaters = {'subsample' : [0.7], 'min_child_weight' : [1], 'max_depth' : [3], 'learning_rate' : [0.1], 'n_estimators' : [100], 'n_jobs' : [-1], 'random_state' : [1]}\nxgb = XGBClassifier()\n\nclf_xgb = GridSearchCV(xgb, xgb_paramaters, n_jobs = -1, cv = kf, scoring = 'roc_auc')\nclf_xgb.fit(train, target)","35ef53b2":"print(clf_xgb.best_score_)\nprint(clf_xgb.score(train, target))\nprint(clf_xgb.best_params_)","a6ce3746":"prediction = clf_rfc2.predict(test)","df0a64b1":"submission = pd.DataFrame({'Survived' : prediction}, index = data_test.PassengerId)\nsubmission.to_csv('submission.csv', index_label = ['PassengerId'])","3be329a3":"## *Explore data*","29d109f4":"- Pclass is related to age.","a6a533cb":"- Children have high survival rate.","ef58e242":"- The passenger with null value of **Fare**:\n    - Pclass: 3\n    - Age: >60","26b280f4":"### *SVM*","f663e13e":"- Use the similar passenger's mean **Fare** value.","aa42e67f":"# *Titanic Survivor Prediction*","02d40ac2":"- **Age, Cabin, Embarked, Fare** has null value.","6260be15":"- **Age** has about 20% null values.Because of its relevance to the other features, we can fit the null values later.","0f1b21b0":"### *Data cleaning*","823dadc6":"### *Data conversion*","4208da62":"- After data visualization, we found that passenger whose **Pclass** is 1 and **Sex** is female have most C in **Embarked**, so using C to fill the null.","eaa53585":"### *Feature selection*","80e31ddb":"---\n## *Data visualization*","60fb404e":"- Features:\n    - Age: \u5e74\u9f84\n    - Cabin: \u5ba2\u8231\n    - Embarked: \u767b\u8239\u6e2f\u53e3\n    - Fare: \u7968\u4ef7\n    - Name:  \u59d3\u540d\n    - Parch: \u7236\u6bcd\u5b50\u5973\u4e2a\u6570\n    - PassengerId: \u4e58\u5ba2id\n    - Pclass: \u4e58\u5ba2\u7b49\u7ea7\uff08\u4e00\u3001\u4e8c\u3001\u4e09\u7b49\u8231\uff09\n    - Sex: \u6027\u522b\n    - SibSp: \u5802\u5144\u5f1f\u59d0\u59b9\u4e2a\u6570\n    - Ticket: \u8239\u7968\u4fe1\u606f\n- Target:\n    - Survived: \u662f\u5426\u751f\u8fd8\n\n- The feature **Age, Cabin, Embarked, Fare** has null value, the most is **Cabin**. ","88b9997e":"---\n## *Output*","7e736abe":"---\n## *Predict*","7c6a6566":"### *GBDT*","a116e1b7":"- The rich have higher survival rate.","10500427":"-  The feature **Cabin, Name, Ticket** has to many values.","438046de":"### *LogisticRegression*","b59c9d35":"- In the five features, **Sex\/Pclass** is important.","1773db5f":"### *XGBoost*","34bc5fe0":"- Ladies and children have high survival rate, especially in the rich.","f286b8b6":"### *RandomForest*","9afb1ad2":"### *Normalize*","2ced26fb":"- **Cabin** has about 75% null values, it's too much, we can regard null value as a unique value.","7aa3e455":"- Fit the null values of **Age**.","1f16fe3a":"---\n## *Data preprocessing*","b8a44e30":"- **Cabin** has 187 different kind of values, we need to classify it.Here we can classify by initials, to make a new feature **CabinCat**"}}