{"cell_type":{"b9ba74c5":"code","534dfc33":"code","1b24e0c3":"code","fd79d2b2":"code","6048513e":"code","66dc1cf1":"code","92511e41":"code","8775a8df":"code","c96ff8a5":"code","95fc40e2":"code","c80c6428":"code","073c1fd1":"code","6d4a67e8":"code","a0ab10fa":"code","03010799":"code","6aa6d853":"code","c2678265":"code","b20faa40":"code","747f6435":"code","071a3e43":"code","171ce1e4":"code","ced2b8a5":"code","20003994":"code","a575a923":"code","7af0bc8b":"code","3d489e70":"code","28793a0d":"code","a0b60762":"code","ecd1bd3c":"code","0de74e77":"code","d6c462fe":"code","83c44e32":"code","e68d90d4":"code","4f209b37":"code","304f6248":"code","4dac678f":"code","3ae8dcba":"code","e37c7efc":"code","e0b2955b":"code","f17ad8ef":"code","d6ae5b23":"code","d7a8b74b":"code","19dc7235":"code","2c03c590":"code","1b4ddd9b":"code","66bde6db":"code","31b6cb2e":"code","0c96149c":"code","c7eb63bd":"code","3e1f185a":"code","5a694386":"code","18c564e9":"code","4bb4a61a":"code","e32dbdc0":"code","fb509168":"code","a09c1056":"code","be01360d":"code","18683594":"code","ed0437e5":"code","5059982a":"code","d5ead509":"code","b6f67fa0":"code","f1f5c637":"code","7c6e92a9":"code","c67acfa1":"markdown","056bed12":"markdown","d4752cbb":"markdown","5585cbc9":"markdown","a92a097e":"markdown","d0805bd7":"markdown"},"source":{"b9ba74c5":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nmit_test_data = pd.read_csv(\"..\/input\/mitbih_test.csv\", header=None)\nmit_train_data = pd.read_csv(\"..\/input\/mitbih_train.csv\", header=None)\n\nprint(\"MIT test dataset\")\nprint(mit_test_data.info())\nprint(\"MIT train dataset\")\nprint(mit_train_data.info())","534dfc33":"print(\"Train data\")\nprint(\"Type\\tCount\")\nprint((mit_train_data[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"Test data\")\nprint(\"Type\\tCount\")\nprint((mit_test_data[187]).value_counts())","1b24e0c3":"# We will use the Seaborn library\nimport seaborn as sns\nsns.set()\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg' ","fd79d2b2":"sns.countplot(x=187, data=mit_train_data);","6048513e":"sns.countplot(x=187, data=mit_test_data);","66dc1cf1":"mit_train_data.describe()","92511e41":"M = mit_train_data.values\nX = M[:, :-1]\ny = M[:, -1].astype(int)","8775a8df":"# \u7ed8\u5236 5 \u79cd\u4e0d\u540c\u7c7b\u522b\u7684\u5fc3\u7535\u56fe\nC0 = np.argwhere(y == 0).flatten()\nC1 = np.argwhere(y == 1).flatten()\nC2 = np.argwhere(y == 2).flatten()\nC3 = np.argwhere(y == 3).flatten()\nC4 = np.argwhere(y == 4).flatten()","c96ff8a5":"import seaborn as sb\nimport scipy as sc\nimport matplotlib.pyplot as plt\nsb.set_style(\"whitegrid\") # \u663e\u793a\u7f51\u683c","95fc40e2":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C0, :][0], label='Cat. N')\n\nplt.legend()\nplt.title(\"ECG for C0\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","c80c6428":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C1, :][0], label='Cat. S')\nplt.legend()\nplt.title(\"ECG for C1\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","073c1fd1":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C2, :][0], label='Cat. V')\nplt.legend()\nplt.title(\"ECG for C2\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","6d4a67e8":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C3, :][0], label='Cat. F')\nplt.legend()\nplt.title(\"ECG for C3\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","a0ab10fa":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C4, :][0], label='Cat. Q')\nplt.legend()\nplt.title(\"ECG for C4\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","03010799":"x = np.arange(0, 187)*0.008\n\nplt.figure(figsize=(10,5))\nplt.plot(x, X[C0, :][0], label='Cat. N')\nplt.plot(x, X[C1, :][0], label='Cat. S')\nplt.plot(x, X[C2, :][0], label='Cat. V')\nplt.plot(x, X[C3, :][0], label='Cat. F')\nplt.plot(x, X[C4, :][0], label='Cat. Q')\nplt.legend()\nplt.title(\"1-beat ECG for every category\", fontsize=15)\nplt.ylabel(\"Amplitude\", fontsize=10)\nplt.xlabel(\"Time (s)\", fontsize=10)\nplt.show()","6aa6d853":"mit_train_data = mit_train_data.sample(frac=1)","c2678265":"from keras.utils import to_categorical\n\nprint(\"--- X ---\")\nX = mit_train_data.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\ny = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\ntestX = mit_test_data.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\ntesty = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","b20faa40":"from keras import backend as K\n    \ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n \n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n        Only computes a batch-wise average of precision.\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","747f6435":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation,BatchNormalization,Dropout\n\ndef get_model_base():\n    model = Sequential()\n\n    model.add(Dense(50, input_dim=187, init='normal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(30, init='normal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(5, activation='softmax'))\n    model.summary()\n    \n    return model\n\nmodel = get_model_base()\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])\n\nhistory = model.fit(X, y, validation_split=0.2,epochs=100,shuffle=True,class_weight='auto')","071a3e43":"print(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","171ce1e4":"history.history.keys()","ced2b8a5":"plt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['f1'])\nplt.plot(history.history['val_f1'])\nplt.legend(labels=['loss','val_loss','f1','val_f1'],loc='best')\nplt.show()","20003994":"X.shape","a575a923":"y.shape","7af0bc8b":"X = np.expand_dims(X,2)\ntestX = np.expand_dims(testX,2)\nX.shape","3d489e70":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, SeparableConv1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint","28793a0d":"n_obs, feature, depth = X.shape\nbatch_size = 1024","a0b60762":"def get_model_cnn():\n    input_img = Input(shape=(feature, depth), name='ImageInput')\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling1D(2, name='pool1')(x)\n    \n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling1D(2, name='pool2')(x)\n    \n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    \n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling1D(2, name='pool3')(x)\n    x = Dropout(0.6, name='dropout0')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(256, activation='relu', name='fc1')(x)\n    x = Dropout(0.6, name='dropout1')(x)\n    x = Dense(128, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(5, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","ecd1bd3c":"model =  get_model_cnn()\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])","0de74e77":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath=\"\/tmp\/weights.hdf5\", verbose=1, save_best_only=True)\nhistory = model.fit(X, y, validation_split=0.2,epochs=75,batch_size=batch_size,shuffle=True,class_weight='auto',callbacks=[checkpointer])","d6c462fe":"print(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","83c44e32":"model.save('cnn-0.985.h5')","e68d90d4":"y_pred = model.predict(testX, batch_size=1000)","4f209b37":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nprint(classification_report(testy.argmax(axis=1), y_pred.argmax(axis=1)))","304f6248":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testy.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=1)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(7, 7))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix')\nplt.show()","4dac678f":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3ae8dcba":"from keras.utils import to_categorical\n\nprint(\"--- X ---\")\nX = mit_train_data.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\ny = mit_train_data.loc[:, mit_train_data.columns == 187]\n#y = to_categorical(y)\n\nprint(\"--- testX ---\")\ntestX = mit_test_data.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\ntesty = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","e37c7efc":"X.shape,y.shape","e0b2955b":"y = y.values.squeeze()","f17ad8ef":"X = np.array(X)","d6ae5b23":"C0 = np.argwhere(y == 0).flatten()\nC1 = np.argwhere(y == 1).flatten()\nC2 = np.argwhere(y == 2).flatten()\nC3 = np.argwhere(y == 3).flatten()\nC4 = np.argwhere(y == 4).flatten()","d7a8b74b":"print(C0.shape[0],C1.shape[0],C2.shape[0],C3.shape[0],C4.shape[0])","19dc7235":"import random\nfrom scipy.signal import resample\n\ndef stretch(x):\n    l = int(187 * (1 + (random.random()-0.5)\/3))\n    y = resample(x, l)\n    if l < 187:\n        y_ = np.zeros(shape=(187, ))\n        y_[:l] = y\n    else:\n        y_ = y[:187]\n    return y_\n\ndef amplify(x):\n    alpha = (random.random()-0.5)\n    factor = -alpha*x + (1+alpha)\n    return x*factor\n\ndef augment(x):\n    result = np.zeros(shape= (5, 187))\n    for i in range(3):\n        if random.random() < 0.33:\n            new_y = stretch(x)\n        elif random.random() < 0.66:\n            new_y = amplify(x)\n        else:\n            new_y = stretch(x)\n            new_y = amplify(new_y)\n        result[i, :] = new_y\n    return result","2c03c590":"import matplotlib.pyplot as plt\nimport random\nplt.plot(X[0, :])\nplt.plot(amplify(X[0, :]))\nplt.plot(stretch(X[0, :]))\nplt.show()","1b4ddd9b":"result_C1 = np.apply_along_axis(augment, axis=1, arr=X[C1]).reshape(-1, 187)\nprint(result_C1.shape)\nclass_C1 = np.ones(shape=(result_C1.shape[0],), dtype=int)*3\nprint(class_C1.shape)\nresult_C3 = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\nclass_C3 = np.ones(shape=(result_C3.shape[0],), dtype=int)*3\n\n# result_C32 = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\n# class_C32 = np.ones(shape=(result_C32.shape[0],), dtype=int)*3\n\n#X = np.vstack([X, result_C1, result_C3])\n#y = np.hstack([y, class_C1, class_C3])\n\nX = np.vstack([X,  result_C3])\ny = np.hstack([y,  class_C3])","66bde6db":"X.shape, y.shape","31b6cb2e":"y = to_categorical(y)","0c96149c":"from sklearn.utils import shuffle\nX, y = shuffle(X,y,random_state=0)","c7eb63bd":"X = np.expand_dims(X,2)\ntestX = np.expand_dims(testX,2)","3e1f185a":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, SeparableConv1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nn_obs, feature, depth = X.shape\nbatch_size = 1024","5a694386":"def build_model_cnn2():\n    input_img = Input(shape=(feature, depth), name='ImageInput')\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling1D(2, name='pool1')(x)\n    \n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling1D(2, name='pool2')(x)\n    \n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = Dropout(0.3, name='dropout3-2')(x)\n    \n    x = SeparableConv1D(512, 3, activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling1D(2, name='pool3')(x)\n    x = Dropout(0.3, name='dropout3-3')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(512, activation='relu', name='fc1')(x)\n    x = Dropout(0.6, name='dropout1')(x)\n    x = Dense(256, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(5, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","18c564e9":"model =  build_model_cnn2()\nmodel.summary()","4bb4a61a":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])","e32dbdc0":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath=\"\/tmp\/weights-aug.hdf5\", monitor='val_f1', mode='max', verbose=1, save_best_only=True)","fb509168":"history = model.fit(X, y, validation_split=0.2,epochs=75,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","a09c1056":"model.load_weights('\/tmp\/weights-aug.hdf5')\nprint(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","be01360d":"K.set_value(model.optimizer.lr, 1e-4)\nmodel.fit(X, y, validation_split=0.2,epochs=30,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","18683594":"model.load_weights('\/tmp\/weights-aug.hdf5')\nprint(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","ed0437e5":"K.set_value(model.optimizer.lr, 1e-5)\nmodel.fit(X, y, validation_split=0.2,epochs=30,batch_size=batch_size,class_weight='auto',callbacks=[checkpointer])","5059982a":"model.load_weights('\/tmp\/weights-aug.hdf5')\nprint(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","d5ead509":"y_pred = model.predict(testX, batch_size=1000)","b6f67fa0":"import itertools\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testy.argmax(axis=1), y_pred.argmax(axis=1))\n#np.set_printoptions(precision=0)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(5, 5))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix')\nplt.show()","f1f5c637":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7c6e92a9":"#K.set_value(model.optimizer.lr, 1e-3)\nhistory = model.fit(X, y, validation_data=(testX,testy),epochs=150,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","c67acfa1":"The last Step: Using all the training data and fitting more epochs","056bed12":"## \u653e\u5927C1 C3 \u7c7b\uff0cC1\u7c7b\u653e\u59274\u500d\uff0cC3\u7c7b\u653e\u59278\u500d","d4752cbb":"# ECG Heartbeat Categorization\n\nThe goal is to be able to classify heart disease from heartbeat signal. There is a lot of data, let's try to make sens out of it.","5585cbc9":"\u5728\u5b9e\u9a8c\u4e2d\uff0c\u53ea\u4f7f\u7528 ECG \u5bfc\u8054 II\uff0cMI \u53ca\u5065\u5eb7\u7684\u6837\u672c\u3002\u4e14\u5728\u5b9e\u9a8c\u4e2d\uff0c\u91cd\u65b0\u7684\u91c7\u6837\u9891\u7387\u4e3a 125 Hz\u3002\u5373\u4e3a 0.008s\u3002\u4e00\u4e2a\u4fe1\u53f7\u5305\u542b 187 \u4e2a\u503c\uff0c\u5927\u7ea6\u6709 1.5s\u3002","a92a097e":"## \u6570\u636e\u589e\u5f3a","d0805bd7":"\u4f7f\u7528CNN"}}