{"cell_type":{"4b88455f":"code","326df6ca":"code","f758f7f1":"code","422af215":"code","0d94263d":"code","28967692":"code","43b11283":"code","b0a59605":"code","1940bcf4":"code","0919fd6a":"code","63bf063a":"code","cdc6cf23":"code","d198f755":"code","d17b75a7":"code","62accc51":"code","7fc949f7":"code","c11fead7":"code","1cc87ba8":"code","8cdc397d":"code","667b3bb5":"code","c295e513":"code","477ba18c":"code","99929925":"code","b93d4b23":"code","403da823":"code","1f8cbe37":"code","c6fe5e88":"code","2faf2640":"code","c807f86d":"code","639f5cd2":"code","164a025a":"code","fc6b8e07":"code","f168eb35":"code","289f6155":"code","7c21f332":"code","7bd1e95e":"code","52d4ab23":"code","22032040":"code","f9980fcf":"code","2760d6c8":"code","fa12adc7":"code","787796bc":"code","baabc842":"code","d9efc785":"code","6677b7b5":"code","cdfc60df":"code","9f5801ad":"code","19458813":"code","4c2f9206":"code","7c7c7fbf":"code","4e4430fd":"code","a8156723":"code","f59db339":"code","eca27715":"code","13b5d983":"code","0411d8b7":"code","370229dc":"code","26303b45":"code","aa9f9846":"code","c87cf906":"code","492e217d":"code","93c0b9b9":"code","6e2d889d":"code","5d3620b2":"code","b51f1fdd":"markdown","d4786e08":"markdown","28b82a63":"markdown","4989ebb1":"markdown","f7b7eb00":"markdown","2d3d2153":"markdown","09166807":"markdown","f0edc3c4":"markdown","937c8bf7":"markdown","c47b8013":"markdown","36f0afce":"markdown","0759cd14":"markdown","a03fa46c":"markdown","73db48f8":"markdown","1fda48d8":"markdown","ab65d21f":"markdown","6023c868":"markdown","c79e2638":"markdown","aa4891bf":"markdown","912293d5":"markdown","323d3f1d":"markdown","19994c8c":"markdown","8f6c2cf6":"markdown","c2e14a2a":"markdown","c8f3c875":"markdown","7c192ee5":"markdown","0e01dcd1":"markdown","bb2440ca":"markdown","3bcd27f3":"markdown","c1c0768b":"markdown","74050512":"markdown","b987c510":"markdown","d5f95a9f":"markdown","1e608bf1":"markdown","e1efff9b":"markdown","bd5258e1":"markdown","e8659f1f":"markdown","1eb467cc":"markdown","cbcfb213":"markdown","8a137323":"markdown","5c507939":"markdown","25af4eb0":"markdown","e3448c86":"markdown","208939c2":"markdown","5dedfe4b":"markdown","faba866e":"markdown","acbff0b6":"markdown","9f61e320":"markdown","0539decf":"markdown","7bf7e385":"markdown","b66738a2":"markdown","eb030812":"markdown","96e30f99":"markdown","8df10db2":"markdown","f6e87107":"markdown","a1567a0b":"markdown","9c1935a9":"markdown","5692c473":"markdown","483c28b0":"markdown","659a820d":"markdown","a3a5960d":"markdown","d62f59b1":"markdown","3570327d":"markdown","de29bd23":"markdown","8d0144da":"markdown","2eab70d8":"markdown","5475ef72":"markdown","0e7c76c1":"markdown"},"source":{"4b88455f":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport math\nimport cv2\nimport seaborn as sns\nimport tensorflow as tf\n\n# To calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n# To get Recall and precision values\nfrom sklearn.metrics import classification_report\n\n# !pip install -q pydicom\n# After installing pydicom. This is needed to load .dcm files\nimport pydicom\nimport pydicom as dcm\nfrom pydicom import dcmread\n\nfrom matplotlib import pyplot\nimport matplotlib.patches as patches\n\nfrom skimage import measure\nfrom skimage.transform import resize\n\nimport tensorflow.keras.utils as pltUtil\nfrom tensorflow.keras.utils import Sequence\n\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.layers import Concatenate, UpSampling2D, Conv2D, Reshape, BatchNormalization, Activation\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.applications.mobilenet import preprocess_input \n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom tensorflow.keras.applications.resnet import preprocess_input as resnetProcess_input\n","326df6ca":"# Load train set image metadata\ndataDirPath = '..\/input\/rsna-pneumonia-detection-challenge\/'\n\nTrain_Image_path = dataDirPath + 'stage_2_train_images'\n\nSAVED_FILES_ROOT = '..\/input\/reportfiles\/'\nRES_FILES_ROOT = '..\/input\/resreportfiles\/'","f758f7f1":"class ImageMetadata():\n    \"\"\"Structure to hold image metadata\n\n    Arguments:\n        setName -- name of the data set\n\n        file -- file name\n    \"\"\"\n    \n    def __init__(self, setName, file):\n        # print(name, file)\n        # dataset name(train or test)\n        self.setName = setName\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.imagePath()\n\n    def imagePath(self):\n        return os.path.join(self.setName, self.file) \n    \n\n# function to load image metadada   \ndef loadImageMetadata(dataSetName):\n    \"\"\"Load image file names from the images from dataset\n\n    Arguments:\n        dataSetName -- path of the data set folder\n    \"\"\"\n    imageMetadata = []\n    for f in os.listdir(dataSetName):\n        # Check file extension. Allow only .dcm files.\n        ext = os.path.splitext(f)[1]\n        if ext == '.dcm' :\n            imageMetadata.append(ImageMetadata(dataSetName, f))\n            \n    # Return an array of filenames\n    return np.array(imageMetadata)","422af215":"def loadImage(path):\n    \"\"\"Load image from the path \n\n    Arguments:\n        path -- path of the image\n    \"\"\"\n    img = pydicom.dcmread(path)\n    \n    # Return image\n    return img\n\n\ndef getImgId(imgPath) :\n    \"\"\"Get image name which is patientId from the given image path \n\n    Arguments:\n        imgPath -- path of the image\n    \"\"\"\n    \n    # Return patientId \n    return str(imgPath).split(\".dcm\")[0].split(\"\/\")[4]","0d94263d":"trainSetImageMetadata = loadImageMetadata(Train_Image_path)\n\nprint(\"trainSetImageMetadata.shape : \", trainSetImageMetadata.shape)\n\nprint(\"Sample image path : \", trainSetImageMetadata[0])","28967692":"# with given index\nimgIndex = 4\nimgPath = trainSetImageMetadata[imgIndex]\nimgPath = imgPath.imagePath()\nimgData = loadImage(imgPath)\n\npyplot.imshow(imgData.pixel_array, cmap=pyplot.cm.bone)","43b11283":"trainSetImageMetadata_df = pd.DataFrame(trainSetImageMetadata, columns=[\"Path\"])\ntrainSetImageMetadata_df.head(2)\n\nimageIdPaths = pd.DataFrame(columns=[\"patientId\", \"imgPath\"])\nimageIdPaths[\"patientId\"] = trainSetImageMetadata_df[\"Path\"].apply(getImgId)\nimageIdPaths[\"imgPath\"] = trainSetImageMetadata_df[\"Path\"]\n\nprint(\"imageIdPaths\", imageIdPaths.shape)\nimageIdPaths.head(2)","b0a59605":"classesPath =  dataDirPath + 'stage_2_detailed_class_info.csv'\n\ndetailedClasses = pd.read_csv(classesPath)\n\ndetailedClasses.head(2)","1940bcf4":"detailedClasses.isna().apply(pd.value_counts)","0919fd6a":"print(\"detailedClasses.shape : \", detailedClasses.shape)\n\n# File has 30227 rows and 2 columns - PatientID & Class","63bf063a":"print(\"Unique patientIds : \", detailedClasses['patientId'].nunique())","cdc6cf23":"print(\"Unique patientIds : \", detailedClasses['class'].nunique(), )\n\nprint(detailedClasses['class'].unique)","d198f755":"sns.countplot(x=\"class\",hue=\"class\",data=detailedClasses)","d17b75a7":"labelsPath = dataDirPath + 'stage_2_train_labels.csv'\n\ntrainLabels = pd.read_csv(labelsPath)\n\ntrainLabels.head(2)","62accc51":"trainLabels.isna().apply(pd.value_counts)","7fc949f7":"trainLabels[trainLabels['Target']==0].head(2)","c11fead7":"print(\"Unique patientIds : \", trainLabels['patientId'].nunique(), )","1cc87ba8":"print(\"Unique patientIds : \", trainLabels['Target'].nunique(), )\n\nprint(\"Unique patientIds : \", trainLabels['Target'].unique(), )","8cdc397d":"trainLabels.sort_values(\"patientId\", inplace=True)\ndetailedClasses.sort_values(\"patientId\", inplace=True)","667b3bb5":"Combined_Data = pd.concat([trainLabels, detailedClasses[\"class\"]], axis=1, sort=False)\nCombined_Data.head(3)","c295e513":"Combined_Data.shape","477ba18c":"Combined_Data.isna().apply(pd.value_counts)","99929925":"Combined_Data[Combined_Data[\"Target\"] == 1].isna().apply(pd.value_counts)","b93d4b23":"Combined_Data[Combined_Data[\"Target\"] == 0].isna().apply(pd.value_counts)","403da823":"Combined_Data[Combined_Data[\"class\"] == \"Lung Opacity\"].isna().apply(pd.value_counts)","1f8cbe37":"# Conver data to only two classes, 'Normal' and 'Lung Opacity'\nCombined_Data[\"class\"].replace(\"No Lung Opacity \/ Not Normal\", \"Normal\", inplace=True)\nCombined_Data.head(3)","c6fe5e88":"\ntrain_CombinedData = Combined_Data[0:15000]\nvalidate_CombinedData = Combined_Data[15000:25000]\ntest_CombinedData = Combined_Data[25000:30227]\n\nprint(\"train_CombinedData.shape : \", train_CombinedData.shape)\nprint(\"validate_CombinedData.shape : \", validate_CombinedData.shape)\nprint(\"test_CombinedData.shape : \", test_CombinedData.shape)\n\nprint(\"\\nunique train patients : \", train_CombinedData[\"patientId\"].nunique())\nprint(\"unique validate patients : \", validate_CombinedData[\"patientId\"].nunique())\nprint(\"unique test patients : \", test_CombinedData[\"patientId\"].nunique())\n\nprint(\"\\nTotal unique patients : \", imageIdPaths[\"patientId\"].nunique())\nprint(\"Total of unique train and test : \", train_CombinedData[\"patientId\"].nunique() + validate_CombinedData[\"patientId\"].nunique() + test_CombinedData[\"patientId\"].nunique())\n\nprint(\"\\nLast from train set : \", train_CombinedData.iloc[14999][\"patientId\"])\nprint(\"First from validate set : \", validate_CombinedData.iloc[0][\"patientId\"])\nprint(\"\\nLast from validate set : \", validate_CombinedData.iloc[9999][\"patientId\"])\nprint(\"First from test set : \", test_CombinedData.iloc[0][\"patientId\"])\n\n# Set all NaN values to 0 in train and test data sets. While training NaN will not have any meaning.\n#    * x, y, width and hight values as zero(0) means no bounding box.\ntrain_CombinedData.fillna(0, inplace=True)\nvalidate_CombinedData.fillna(0, inplace=True)\ntest_CombinedData.fillna(0, inplace=True)","2faf2640":"imageIdPaths.sort_values(\"patientId\", inplace=True)\n\ntrain_imageIdPaths = imageIdPaths[0:13163]\nvalidate_imageIdPaths = imageIdPaths[13163:21764]\ntest_imageIdPaths = imageIdPaths[21764:26684]\n\nprint(\"train_imageIdPaths.shape : \", train_imageIdPaths.shape)\nprint(\"validate_imageIdPaths.shape : \", validate_imageIdPaths.shape)\nprint(\"test_imageIdPaths.shape : \", test_imageIdPaths.shape)\n\nprint(\"\\nunique train patients : \", train_imageIdPaths[\"patientId\"].nunique())\nprint(\"unique validate patients : \", validate_imageIdPaths[\"patientId\"].nunique())\nprint(\"unique test patients : \", test_imageIdPaths[\"patientId\"].nunique())\n\nprint(\"\\nTotal unique patients : \", imageIdPaths[\"patientId\"].nunique())\nprint(\"Total of unique train and test : \", train_imageIdPaths[\"patientId\"].nunique() + validate_imageIdPaths[\"patientId\"].nunique() + test_imageIdPaths[\"patientId\"].nunique())\n\nprint(\"\\nLast from train set : \", train_imageIdPaths.iloc[13162][\"patientId\"])\nprint(\"First from validate set : \", validate_imageIdPaths.iloc[0][\"patientId\"])\nprint(\"Last from validate set : \", validate_imageIdPaths.iloc[8600][\"patientId\"])\nprint(\"First from test set : \", test_imageIdPaths.iloc[0][\"patientId\"])","c807f86d":"sns.countplot(x=\"Target\",hue=\"class\",data=train_CombinedData)","639f5cd2":"sns.countplot(x=\"Target\",hue=\"class\",data=validate_CombinedData)","164a025a":"sns.countplot(x=\"Target\",hue=\"class\",data=test_CombinedData)","fc6b8e07":"IMAGE_SIZE = 224\n\nIMG_WIDTH = 1024\nIMG_HEIGHT = 1024\n\nTRAIN_BATCH_SIZE = 10\nTEST_BATCH_SIZE = 10\n\nALPHA = 1.0","f168eb35":"# define iou or jaccard loss function\ndef iou_loss(y_true, y_pred):\n    \"\"\"Get Intersection over Union(IoU) ratio from ground truth and predicted masks.\n    Arguments:\n        y_true -- ground truth mask\n        \n        y_pred -- predicted mask\n    \"\"\"\n    y_true = tf.reshape(y_true, [-1])\n    y_pred = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true * y_pred)\n    score = (intersection + 1.) \/ (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n    return 1 - score\n\n# mean iou as a metric\ndef mean_iou(y_true, y_pred):\n    \"\"\"Get mean Intersection over Union(IoU) ratio from ground truth and predicted masks.\n    Arguments:\n        y_true -- ground truth mask\n        \n        y_pred -- predicted mask\n    \"\"\"\n    y_pred = tf.round(y_pred)    \n    intersect = tf.reduce_sum(y_true * y_pred, axis=[1]) \n    union = tf.reduce_sum(y_true, axis=[1]) + tf.reduce_sum(y_pred, axis=[1])\n    smooth = tf.ones(tf.shape(intersect))\n    return tf.reduce_mean((intersect + smooth) \/ (union - intersect + smooth))","289f6155":"def iouFromCoords(boxA, boxB) :\n    \"\"\"Get Intersection over Union(IoU) ratio from ground truth and predicted box coordinates.\n    Arguments:\n        boxA -- ground truth mask\n        \n        boxB -- predicted mask\n    \"\"\"\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # compute the area of intersection rectangle\n    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n    if interArea == 0:\n        return 0\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea \/ float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou","7c21f332":"\ndef showMaskedImage(_imageSet, _maskSet, _index) :\n    \"\"\"To show image with imposing mask on it for the given index, form the given sets of images and masks.\n    Arguments:\n        _imageSet -- set of images\n        \n        _maskSet -- set of masks\n        \n        _index -- index of a set\/collection\n    \"\"\"\n    maskImage = _imageSet[_index]\n\n    #pyplot.imshow(maskImage[:,:,0], cmap=pyplot.cm.bone)\n    maskImage[:,:,0] = _maskSet[_index] * _imageSet[_index][:,:,0]\n    maskImage[:,:,1] = _maskSet[_index] * _imageSet[_index][:,:,1]\n    maskImage[:,:,2] = _maskSet[_index] * _imageSet[_index][:,:,2]\n\n    pyplot.imshow(maskImage[:,:,0], cmap=pyplot.cm.bone)\n","7bd1e95e":"\nclass UNetTrainGenerator(Sequence):\n    \"\"\"Generator class to get data batches for training model. Extends Sequence class.\n        \n    Arguments:\n        _imageIdPaths -- dataframe having patientId and image paths to load image for the patientId\n        \n        _CombinedData -- dataframe having patientId, image labels, target and class values combined together\n        \n        idx -- index of a batch\n    \"\"\"\n    def __init__(self, _imageIdPaths, _CombinedData):       \n        self.pids = _CombinedData[\"patientId\"].to_numpy()\n        self.imgIdPaths = _imageIdPaths\n        self.coords = _CombinedData[[\"x\", \"y\", \"width\", \"height\"]].to_numpy()\n        # Resize Bounding box\n        self.coords = self.coords * IMAGE_SIZE \/ IMG_WIDTH\n        \n\n    def __len__(self):\n        return math.ceil(len(self.coords) \/ TRAIN_BATCH_SIZE)\n    \n\n    def __getitem__(self, idx): # Get a batch\n        batch_coords = self.coords[idx * TRAIN_BATCH_SIZE:(idx + 1) * TRAIN_BATCH_SIZE] # Image coords\n        batch_pids = self.pids[idx * TRAIN_BATCH_SIZE:(idx + 1) * TRAIN_BATCH_SIZE] # Image pids    \n        \n        batch_images = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n        batch_masks = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE))\n        for _indx, _pid in enumerate(batch_pids):\n            _path = self.imgIdPaths[self.imgIdPaths[\"patientId\"] == _pid][\"imgPath\"].array[0]\n            _imgData = loadImage(str(_path)) # Read image\n            img = _imgData.pixel_array \n            \n            # Resize image\n            resized_img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n    \n            #print(\"batch_images[_indx] shape :\", batch_images[_indx][:,:,0].shape)\n            # preprocess image for the batch\n            batch_images[_indx][:,:,0] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,1] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,2] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array  \n            \n            x = int(batch_coords[_indx, 0])\n            y = int(batch_coords[_indx, 1])\n            width = int(batch_coords[_indx, 2])\n            height = int(batch_coords[_indx, 3])\n            \n            batch_masks[_indx][y:y+height, x:x+width] = 1\n            \n        # Returns images and ground truth masks for training\n        return batch_images, batch_masks","52d4ab23":"# Generator for Predicting\/Testing model\nclass UNetTestGenerator(Sequence):\n    \"\"\"Generator class to get data batches for testing model. Used to get batches of test data used for getting predictions.\n        \n    Arguments:\n        _imageIdPaths -- dataframe having patientId and image paths to load image for the patientId\n        \n        _CombinedData -- dataframe having patientId, image labels, target and class values combined together\n        \n        idx -- index of a batch\n    \"\"\"\n    def __init__(self, _imageIdPaths, _CombinedData):       \n        self.pids = _CombinedData[\"patientId\"].to_numpy()\n        self.imgIdPaths = _imageIdPaths\n        self.coords = _CombinedData[[\"x\", \"y\", \"width\", \"height\", \"Target\"]].to_numpy() #for (1024, 1024)\n        self.classes = _CombinedData[\"class\"]\n        # Resize Bounding box\n        self.coordsOrig = self.coords #for (1024, 1024)\n        self.coords = self.coords * IMAGE_SIZE \/ IMG_WIDTH   #for (224, 224)\n        \n\n    def __len__(self):\n        # Returns total number of batches\n        return math.ceil(len(self.coords) \/ TEST_BATCH_SIZE)\n    \n\n    def __getitem__(self, idx): # Get a batch\n        batch_coords = self.coords[idx * TEST_BATCH_SIZE:(idx + 1) * TEST_BATCH_SIZE] # Image coords for (224, 224)\n        batch_coordsOrig = self.coordsOrig[idx * TEST_BATCH_SIZE:(idx + 1) * TEST_BATCH_SIZE] # Image coords for (1024, 1024)\n        batch_pids = self.pids[idx * TEST_BATCH_SIZE:(idx + 1) * TEST_BATCH_SIZE] # Image pids    \n        batch_classes = self.classes[idx * TEST_BATCH_SIZE:(idx + 1) * TEST_BATCH_SIZE] # Image classes           \n        \n        batch_images = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n        batch_masks = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE))\n        for _indx, _pid in enumerate(batch_pids):\n            _path = self.imgIdPaths[self.imgIdPaths[\"patientId\"] == _pid][\"imgPath\"].array[0]\n            _imgData = loadImage(str(_path)) # Read image\n            img = _imgData.pixel_array \n            \n            # Resize image\n            resized_img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA) #(224, 224)\n            #resized_img = cv2.resize(img[200:824, 200:824], dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n    \n            #print(\"batch_images[_indx] shape :\", batch_images[_indx][:,:,0].shape)\n            # preprocess image for the batch\n            batch_images[_indx][:,:,0] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,1] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,2] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array  \n            \n            x = int(batch_coords[_indx, 0])\n            y = int(batch_coords[_indx, 1])\n            width = int(batch_coords[_indx, 2])\n            height = int(batch_coords[_indx, 3])\n            target = int(batch_coords[_indx, 4])\n            \n            batch_coords[_indx, 0] = x\n            batch_coords[_indx, 1] = y \n            batch_coords[_indx, 2] = width \n            batch_coords[_indx, 3] = height    \n            batch_coords[_indx, 4] = target \n            \n            batch_masks[_indx][y:y+height, x:x+width] = 1\n\n        # Returns images, ground truth masks, patientIds, resized-coordinates, class targets and ground truth coordinates\/lables.   \n        return batch_images, batch_masks, batch_pids, batch_coords, batch_classes, batch_coordsOrig  #for (224, 224) and (1024, 1024)","22032040":"\ndef create_UNetModel(trainable=True):\n    \"\"\"Function to create UNet architecture with MobileNet.\n        \n    Arguments:\n        trainable -- Flag to make layers trainable. Default value is 'True'.\n    \"\"\"\n    # Get all layers with 'imagenet' weights\n    model = MobileNet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\") \n    # Top layer is last layer of the model\n    \n    # Make all layers trainable\n    for layer in model.layers:\n        layer.trainable = trainable\n\n    # Add all the UNET layers here\n    convLayer_112by112 = model.get_layer(\"conv_pw_1_relu\").output\n    convLayer_56by56 = model.get_layer(\"conv_pw_3_relu\").output\n    convLayer_28by28 = model.get_layer(\"conv_pw_5_relu\").output\n    convLayer_14by14 = model.get_layer(\"conv_pw_11_relu\").output\n    convLayer_7by7 = model.get_layer(\"conv_pw_13_relu\").output\n    # The last layer of mobilenet model is of dimensions (7x7x1024)\n\n    # Start upsampling from 7x7 to 14x14 ...up to 224x224 to form UNET\n    # concatinate with the original image layer of the same size from MobileNet\n    x = Concatenate()([UpSampling2D()(convLayer_7by7), convLayer_14by14])\n    x = Concatenate()([UpSampling2D()(x), convLayer_28by28])\n    x = Concatenate()([UpSampling2D()(x), convLayer_56by56])\n    x = Concatenate()([UpSampling2D()(x), convLayer_112by112])\n    x = UpSampling2D(name=\"unet_last\")(x) # upsample to 224x224\n\n    # Add classification layer\n    x = Conv2D(1, kernel_size=1, activation=\"sigmoid\", name=\"masks\")(x)\n    x = Reshape((IMAGE_SIZE, IMAGE_SIZE))(x) \n\n    return Model(inputs=model.input, outputs=x)","f9980fcf":"\ndef conv_block_simple(prevlayer, filters, prefix, strides=(1, 1)):\n    \"\"\"Function to create part of UNet architecture with ResNet50. \n    Adds convolutional layer followed by Batch Normalization and Activation layers.\n        \n    Arguments:\n        prevlayer -- previous layer of the convolution block\n        \n        filters -- number of filters for convolution\n        \n        prefix -- prefix for the layer name\n        \n        strides -- convolution stride. Default is 1x1.\n    \"\"\"\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    \n    # Returns the built layers of the block.\n    return conv","2760d6c8":"\ndef create_ResNetUNetModel(trainable=True):\n    \"\"\"Function to create UNet architecture with ResNet50.\n        \n    Arguments:\n        trainable -- Flag to make layers trainable. Default value is 'True'.\n    \"\"\"\n    resnetLayers = ResNet50(weights='imagenet', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False) # Load pre-trained Resnet\n    # Top layer is last layer of the model\n\n    for layer in resnetLayers.layers:\n        layer.trainable = trainable\n\n    # Add all the UNet layers here\n    convLayer_112by112 = resnetLayers.get_layer(\"conv1_relu\").output\n    convLayer_56by56 = resnetLayers.get_layer(\"conv2_block3_out\").output # conv2_block3_2_relu\n    convLayer_28by28 = resnetLayers.get_layer(\"conv3_block4_out\").output # conv3_block4_2_relu\n    convLayer_14by14 = resnetLayers.get_layer(\"conv4_block6_out\").output # conv4_block6_2_relu\n    convLayer_7by7 = resnetLayers.get_layer(\"conv5_block3_out\").output # conv5_block3_2_relu\n    # The last layer of resnet model(conv5_block3_out) is of dimensions (7x7x2048)\n\n    # Start upsampling from 7x7 to 14x14 ...up to 224x224 to form UNet\n    # concatinate with the original image layer of the same size from ResNet50\n    up14by14 = Concatenate()([UpSampling2D()(convLayer_7by7), convLayer_14by14])\n    upConvLayer_14by14 = conv_block_simple(up14by14, 256, \"upConvLayer_14by14_1\")\n    upConvLayer_14by14 = conv_block_simple(upConvLayer_14by14, 256, \"upConvLayer_14by14_2\")\n    \n    up28by28 = Concatenate()([UpSampling2D()(upConvLayer_14by14), convLayer_28by28])\n    upConvLayer_28by28 = conv_block_simple(up28by28, 256, \"upConvLayer_28by28_1\")\n    upConvLayer_28by28 = conv_block_simple(upConvLayer_28by28, 256, \"upConvLayer_28by28_2\")\n     \n    up56by56 = Concatenate()([UpSampling2D()(upConvLayer_28by28), convLayer_56by56])\n    upConvLayer_56by56 = conv_block_simple(up56by56, 256, \"upConvLayer_56by56_1\")\n    upConvLayer_56by56 = conv_block_simple(upConvLayer_56by56, 256, \"upConvLayer_56by56_2\")    \n    \n    up112by112 = Concatenate()([UpSampling2D()(upConvLayer_56by56), convLayer_112by112])\n    upConvLayer_112by112 = conv_block_simple(up112by112, 256, \"upConvLayer_112by112_1\")\n    upConvLayer_112by112 = conv_block_simple(upConvLayer_112by112, 256, \"upConvLayer_112by112_2\")   \n    \n    up224by224 = UpSampling2D(name=\"unet_last\")(upConvLayer_112by112) # upsample to 224x224\n    upConvLayer_224by224 = conv_block_simple(up224by224, 256, \"upConvLayer_224by224_1\")\n    upConvLayer_224by224 = conv_block_simple(upConvLayer_224by224, 256, \"upConvLayer_224by224_2\")   \n\n    # Add classification layer\n    upConvLayer_224by224 = Conv2D(1, kernel_size=1, activation=\"sigmoid\", name=\"masks\")(upConvLayer_224by224)\n    upConvLayer_224by224 = Reshape((IMAGE_SIZE, IMAGE_SIZE))(upConvLayer_224by224) \n\n    return Model(inputs=resnetLayers.input, outputs=upConvLayer_224by224)","fa12adc7":"\ndef showConfusionMatrixFromFile(report30IOU) :\n    \"\"\"To show Confusion Matrix and Classification report from given dataframe.\n        \n    Arguments:\n        report30IOU -- dataframe having target and prediction columns.\n    \"\"\"\n    report30IOU.fillna(0, inplace=True) # set NA IOU values to zero\n    # Get Targets and Predictions\n    y_30_test = report30IOU[\"Target\"]\n    y_30_predicted = report30IOU[\"predTarget\"]\n    print(\"Predictions above 30% IOU :\\n\")\n    print(\"Confusion Matrix:- \\n\", metrics.confusion_matrix(y_30_test, y_30_predicted), \"\\n\")\n    print(\"Classification Report:- \\n\", metrics.classification_report(y_30_test, y_30_predicted))\n    \n    \ndef showConfusionMatrixFromYs(y_test, y_predicted) :\n    \"\"\"To show Confusion Matrix and Classification report from given dataframes y_test and y_predicted.\n        \n    Arguments:\n        y_test -- 1D dataframe having target values.\n        y_predicted -- 1D dataframe having predicted values.\n    \"\"\"\n    print(\"Predictions above 30% IOU :\\n\")\n    print(\"Confusion Matrix:- \\n\", metrics.confusion_matrix(y_test, y_predicted), \"\\n\")\n    print(\"Classification Report:- \\n\", metrics.classification_report(y_test, y_predicted))","787796bc":"\n# Funstion to plot history curves\ndef plotHistory(_HISTORY_FILE) :\n    \"\"\"To plot history curves for training and validation losses\n        \n    Arguments:\n        _HISTORY_FILE -- training history file name with path.\n    \"\"\"\n    unetSavedHistory = np.load(_HISTORY_FILE, allow_pickle=True).item()\n    unetSavedHistoryDF = pd.DataFrame(unetSavedHistory)\n    \n    # list data in history\n    # summarize history for loss\n    pyplot.plot(unetSavedHistoryDF['loss'])\n    pyplot.plot(unetSavedHistoryDF['val_loss'])\n    pyplot.title('model loss')\n    pyplot.ylabel('loss')\n    pyplot.xlabel('epoch')\n    pyplot.legend(['train', 'validation'], loc='best')\n    pyplot.show()\n    # summarize history for mean IOU\n    pyplot.plot(unetSavedHistoryDF['mean_iou'])\n    pyplot.plot(unetSavedHistoryDF['val_mean_iou'])\n    pyplot.plot(unetSavedHistoryDF['lr'])\n    pyplot.title('model IOU and Leraning rate')\n    pyplot.ylabel('IOU and LR')\n    pyplot.xlabel('epoch')\n    pyplot.legend(['train', 'validation', 'Lerning Rate'], loc='best')\n    pyplot.show()\n","baabc842":"\ndef predictBatches(_test_CombinedData, _test_imageIdPaths, _UNetModel) :\n    \"\"\"Method to predict test data set and save the submission report into a csv file.\n        \n    Arguments:\n        _test_CombinedData -- test set dataframe having patientId, image labels, target and class values combined together\n\n        _imageIdPaths -- test set dataframe having patientId and image paths to load image for the patientId\n        \n        _UNetModel -- UNet model with trainined weights used for predicting test data\n    \"\"\"\n    print('Number of Test Samples :', _test_CombinedData[\"patientId\"].nunique()) # about 20% of the dataset\n    \n    # create test generator instance\n    testUNetDataGen = UNetTestGenerator(_test_imageIdPaths, _test_CombinedData) #for (224, 224)\n    \n    # create submission dafa frame with column names\n    submissionDF = pd.DataFrame(columns=['patientId', 'x', 'y', 'width', 'height', 'Target', 'class', 'x_pred', 'y_pred', 'width_pred', \n                                         'height_pred', 'predTarget', 'iou', 'class_pred'])\n    dfIndex = 0\n    iouThreshold = 0.3 # IoU above 30%\n\n    # loop through testset\n    # for batches from testUNetDataGen\n    print(\"Predicting Batches \", end='')\n    for batchImages, gtBatchMasks, batchPids, batchCoords, batchClasses, batchCoordsOrig in testUNetDataGen:    #for (224, 224)\n        print(\".\", end = '')    \n        \n        # predict batch of images\n        batchPreds = _UNetModel.predict(batchImages)    #for (224, 224)\n\n        prevPid = \"\"\n        # loop through batch\n        for pred, gtMask, pid, coords, gtClass, coordsOrig in zip(batchPreds, gtBatchMasks, batchPids, batchCoords, batchClasses, batchCoordsOrig):   #for (224, 224)\n\n            if prevPid != pid :\n                prevPid = pid\n\n                # resize predicted mask\n                pred = resize(pred, (1024, 1024), mode='reflect')   #for (1024, 1024)\n                # recompute coords for resized pred\n                coords = coordsOrig   #for (1024, 1024)\n\n                # threshold predicted mask\n                strongPred = pred[:, :] > 0.5   \n\n                # apply connected components\n                strongPred = measure.label(strongPred)\n\n                loopIndx = 0\n                # collect all reagions for the prediction\n                iouCoordsDF = pd.DataFrame(columns=['iou', 'x', 'y', 'width', 'height'])\n                for region in measure.regionprops(strongPred) :\n                    # retrieve x, y, height and width\n                    y, x, y2, x2 = region.bbox\n                    height = y2 - y\n                    width = x2 - x\n                    # Get IOUs\n                    coordsXYs = np.array([coords[0], coords[1], coords[2]+coords[0], coords[3]+coords[1]])\n                    regionXYs = np.array([x, y, x2, y2])\n                    IOU = iouFromCoords(coordsXYs, regionXYs)\n                    #print(\"IOU \", IOU)\n                    iouCoordsRow = [IOU, x, y, width, height]\n                    iouCoordsDF.loc[loopIndx] = iouCoordsRow\n                    loopIndx = loopIndx + 1\n\n                GTDFRow = [pid, coords[0], coords[1], coords[2], coords[3], coords[4], gtClass] # ground truth data \n                prevGTDFRow = []\n                # Get top 2 predictions based on IOU \n                iouCoordsDF.sort_values(\"iou\", ascending=False, inplace=True)\n                predIOUCoordCount = 0\n                # If predictions exist\n                if len(iouCoordsDF) > 0 :\n                    for predIOUCoordIdx in (0, len(iouCoordsDF)-1) :\n                        if iouCoordsDF.loc[predIOUCoordIdx][\"iou\"] > iouThreshold :\n                            # add row with ground truth and prediction values to data frame    \n                            submissionDFRow = [pid, coords[0], coords[1], coords[2], coords[3], coords[4],\n                                               gtClass, int(iouCoordsDF.loc[predIOUCoordIdx][\"x\"]), int(iouCoordsDF.loc[predIOUCoordIdx][\"y\"]), \n                                               int(iouCoordsDF.loc[predIOUCoordIdx][\"width\"]), int(iouCoordsDF.loc[predIOUCoordIdx][\"height\"]), \n                                               1, iouCoordsDF.loc[predIOUCoordIdx][\"iou\"], \"Lung Opacity\"]\n                            if predIOUCoordCount < 2 :\n                                if GTDFRow != prevGTDFRow : \n                                    submissionDF.loc[dfIndex] = submissionDFRow\n                                    dfIndex = dfIndex + 1 \n                                    predIOUCoordCount = predIOUCoordCount + 1\n                                    prevGTDFRow = GTDFRow\n                            else :\n                                break;\n                        else : # Normal if IOU below threshold\n                            # add row with ground truth and prediction values to data frame\n                            if GTDFRow != prevGTDFRow :  \n                                submissionDFRow = [pid, coords[0], coords[1], coords[2], coords[3], coords[4], \n                                                   gtClass, 0, 0, 0, 0, 0, iouCoordsDF.loc[predIOUCoordIdx][\"iou\"], \"Normal\"]\n                                submissionDF.loc[dfIndex] = submissionDFRow\n                                dfIndex = dfIndex + 1  \n                                prevGTDFRow = GTDFRow\n                                break;\n                            # end of if\n                        # end of if\n                    # end of for\n\n                else : # else of If predictions exist. Normal if no predictions\n                    # add row with ground truth and prediction values to data frame\n                    submissionDFRow = [pid, coords[0], coords[1], coords[2], coords[3], coords[4], \n                                       gtClass, 0, 0, 0, 0, 0, 'NA', \"Normal\"]\n                    submissionDF.loc[dfIndex] = submissionDFRow\n                    dfIndex = dfIndex + 1      \n\n        # To stop at certain count. Used to debug code.\n#             if len(submissionDF) >= 5 :\n#                 break\n\n    # save dictionary as csv file\n    submissionDF.to_csv('submission.csv', index=False)\n    print(\"Prediction Complete!\")\n    \n    test_y = submissionDF[\"Target\"]\n    predicted_y = submissionDF[\"predTarget\"]\n    \n    return test_y.apply(int), predicted_y.apply(int)","d9efc785":"\ndef visualizePredictions(_predReportDF, _topNum) :\n    \"\"\"Method to visualize predictions by displaying ground truth and predicted bounding boxes.\n        \n    Arguments:\n        _predReportDF -- dataframe having target and prediction coordinate columns; patientId and IoUs.\n\n        _topNum -- number indicating the count of top predictions to be visualized.\n    \"\"\"\n    # Sort on IOU to get higher IOUs on top\n    _predReportDF.sort_values(\"iou\", ascending=False, inplace=True)\n    # Get patientIds\n    topPids = _predReportDF[\"patientId\"].head(_topNum)\n    topPidsAry = np.array(topPids)\n    # Get IOUs\n    topIOUs = _predReportDF[\"iou\"].head(_topNum)\n    topIOUsAry = np.array(topIOUs)\n\n    # To get ground truth images for top IOU scored pids\n    imageCollc = np.zeros((_topNum, IMG_WIDTH, IMG_HEIGHT), np.float32) # (1024, 1024)\n\n    # Get ground truth coordinates for top IOU scored rows and prepare masks\n    gtCoordCollc = _predReportDF[[\"x\", \"y\", \"width\", \"height\"]].to_numpy()  # (1024, 1024)\n    # To get ground truth masks\n    gtMaskCollc  = np.zeros((_topNum, IMG_WIDTH, IMG_HEIGHT), np.int) # (1024, 1024)\n\n    # Get ground truth coordinates for top IOU scored rows and prepare masks\n    predCoordCollc = _predReportDF[[\"x_pred\", \"y_pred\", \"width_pred\", \"height_pred\"]].to_numpy()  # (1024, 1024)\n    # To get ground truth masks\n    predMaskCollc  = np.zeros((_topNum, IMG_WIDTH, IMG_HEIGHT), np.int)\n\n    # Get ground truth and prediction masks\n    for indx in range(0, _topNum) :\n        # Get images\n        path = test_imageIdPaths[test_imageIdPaths[\"patientId\"] == topPidsAry[indx]][\"imgPath\"].array[0]\n        imgData = loadImage(str(path)) # Read image\n        img = imgData.pixel_array\n        imageCollc[indx][:,:] = preprocess_input(np.array(img[:,:], dtype=np.float32)) # Convert to float32 array\n\n        # prepare ground truth masks\n        x = int(gtCoordCollc[indx, 0])\n        y = int(gtCoordCollc[indx, 1])\n        width = int(gtCoordCollc[indx, 2])\n        height = int(gtCoordCollc[indx, 3])\n        gtMaskCollc[indx][y:y+height, x:x+width] = 1   # (1024, 1024)\n\n        # prepare predicted masks\n        x_pred = int(predCoordCollc[indx, 0])\n        y_pred = int(predCoordCollc[indx, 1])\n        width_pred = int(predCoordCollc[indx, 2])\n        height_pred = int(predCoordCollc[indx, 3])\n        predMaskCollc[indx][y_pred:y_pred+height_pred, x_pred:x_pred+width_pred] = 1   # (1024, 1024)\n        \n    # Show images and bounding boxes\n    imageArea, axesArry = pyplot.subplots(int(_topNum\/2), 2, figsize=(18,18))\n    axesArry = axesArry.ravel()\n    for axidx in range(0, _topNum) :\n        axesArry[axidx].imshow(imageCollc[axidx][:, :], cmap=pyplot.cm.bone)\n\n        gtComp = gtMaskCollc[axidx][:, :] > 0.5\n        # apply connected components\n        gtComp = measure.label(gtComp)\n        # apply ground truth bounding boxes\n        for region in measure.regionprops(gtComp):\n            # retrieve x, y, height and width\n            y1, x1, y2, x2 = region.bbox\n            heightReg = y2 - y1\n            widthReg = x2 - x1\n            axesArry[axidx].add_patch(patches.Rectangle((x1, y1), widthReg, heightReg, linewidth=1, edgecolor='r', \n                                                        facecolor='none'))\n\n        predComp = predMaskCollc[axidx][:, :] > 0.5\n        # apply connected components\n        predComp = measure.label(predComp)\n        # apply predicted bounding boxes\n        for region_pred in measure.regionprops(predComp):\n            # retrieve x, y, height and width\n            y1_pred, x1_pred, y2_pred, x2_pred = region_pred.bbox\n            heightReg_pred = y2_pred - y1_pred\n            widthReg_pred = x2_pred - x1_pred\n            axesArry[axidx].add_patch(patches.Rectangle((x1_pred, y1_pred), widthReg_pred, heightReg_pred, linewidth=1, edgecolor='b', \n                                                        facecolor='none'))\n            axesArry[axidx].set_title('IOU : '+str(topIOUsAry[axidx]))\n    # Show subplots\n    pyplot.show()","6677b7b5":"trainUNetDataGen = UNetTrainGenerator(train_imageIdPaths, train_CombinedData)\nvalidateUNetDataGen = UNetTrainGenerator(validate_imageIdPaths, validate_CombinedData)\n\nprint(len(trainUNetDataGen), \"# of iterations in one train epoch\")\nprint(len(validateUNetDataGen), \"# of iterations in one validate epoch\")","cdfc60df":"imageSet0 = trainUNetDataGen[0][0]\nmaskSet0 = trainUNetDataGen[0][1]    \nshowMaskedImage(imageSet0, maskSet0, 5)","9f5801ad":"trainUnetModel = False\n\nEPOCHS = 3\nWEIGHTS_FILE = SAVED_FILES_ROOT + \"unetModel-0.73.h5\"\n\nadamOptimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\ncheckpoint = ModelCheckpoint(\"unetModel-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n                             save_weights_only=True, mode=\"min\", period=1)\nstop = EarlyStopping( monitor=\"loss\", patience=5, mode=\"min\")\nreduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=1, mode=\"min\")\n\nUNetModel = create_UNetModel()\nUNetModel.compile(loss=iou_loss, optimizer=adamOptimizer, metrics=[mean_iou]) \n\nUNetModel.load_weights(WEIGHTS_FILE) # train with saved weights\n\nif trainUnetModel==True :\n    # Make layers trainable\n    for layer in UNetModel.layers:\n        layer.trainable = True\n\n    hist = UNetModel.fit_generator(generator=trainUNetDataGen,\n                        epochs=EPOCHS,\n                        validation_data=validateUNetDataGen,\n                        callbacks=[checkpoint, reduce_lr, stop],\n                        shuffle=True,\n                        verbose=1)\n    \n    unet_history = np.array(hist.history)\n    np.save(\"unetTrainHist-3\", unet_history, allow_pickle=True)\n    ","19458813":"HISTORY_FILE = SAVED_FILES_ROOT + \"unetTrainHist.npy\"\nplotHistory(HISTORY_FILE)","4c2f9206":"pltUtil.plot_model(UNetModel,\n                    to_file=\"UnetModel.png\",\n                    show_shapes=True,\n                    show_layer_names=True,\n                    expand_nested=False,\n                    dpi=70)","7c7c7fbf":"# Check sample ground truth masked image and predicted masked image \nimageSet0 = trainUNetDataGen[0][0]\nmaskSet0 = trainUNetDataGen[0][1]\nprint(\"Ground Truth Box\/Mask\")\nshowMaskedImage(imageSet0, maskSet0, 5)","4e4430fd":"predMasks = UNetModel.predict(imageSet0)\nprint(\"Predicted Box\/Mask\")\nshowMaskedImage(imageSet0, predMasks, 5)","a8156723":"predictUnetModel = False\n\nif predictUnetModel == True :\n    y_test, y_predicted = predictBatches(test_CombinedData, test_imageIdPaths, UNetModel)","f59db339":"REPORT_30_FILE = SAVED_FILES_ROOT + \"submission_30.csv\"\nreport30IOU = pd.read_csv(REPORT_30_FILE)\nshowConfusionMatrixFromFile(report30IOU)\n","eca27715":"topNum = 6 # provide even number\nvisualizePredictions(report30IOU, topNum)","13b5d983":"trainResnetUnetModel = False\nEPOCHS = 1\nWEIGHTS_FILE = RES_FILES_ROOT + \"resnetModel-0.78.h5\"\n\nres_adamOptimizer = Adam(lr=1e-1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nres_checkpoint = ModelCheckpoint(\"resnetModel-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=False,\n                             save_weights_only=True, mode=\"min\", period=1)\nres_stop = EarlyStopping( monitor=\"loss\", patience=5, mode=\"min\")\nres_reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-4, verbose=1, mode=\"min\")\n\nResNetUNetModel = create_ResNetUNetModel()\nResNetUNetModel.compile(loss=iou_loss, optimizer=res_adamOptimizer, metrics=[mean_iou]) \n\nResNetUNetModel.load_weights(WEIGHTS_FILE)\n\nif trainResnetUnetModel==True :\n    # Make layers trainable\n    for layer in ResNetUNetModel.layers:\n        layer.trainable = True\n\n    reshist = ResNetUNetModel.fit_generator(generator=trainUNetDataGen,\n                        epochs=EPOCHS,\n                        validation_data=validateUNetDataGen,\n                        callbacks=[res_checkpoint, res_reduce_lr, res_stop],\n                        shuffle=True,\n                        verbose=1)\n    \n    resnetUnet_history = np.array(reshist.history)\n    np.save(\"resnetUnetTrainHist\", resnetUnet_history, allow_pickle=True)","0411d8b7":"pltUtil.plot_model(ResNetUNetModel,\n                    to_file=\"resnetUnetModel.png\",\n                    show_shapes=True,\n                    show_layer_names=True,\n                    expand_nested=False,\n                    dpi=70)","370229dc":"RES_HISTORY_FILE = RES_FILES_ROOT + \"resnetUnetTrainHist.npy\"\nplotHistory(RES_HISTORY_FILE)","26303b45":"predictResNetUNetModel = False\n\nif predictResNetUNetModel == True :\n    y_test_res, y_predicted_res = predictBatches(test_CombinedData, test_imageIdPaths, ResNetUNetModel)\n","aa9f9846":"RESREPORT_30_FILE = RES_FILES_ROOT + \"resSubmission_30_IOU_4thEpoch.csv\"\n\nif predictResNetUNetModel == True :\n    showConfusionMatrixFromYs(y_test_res, y_predicted_res)\nelse :\n    ResReport30IOU = pd.read_csv(RESREPORT_30_FILE)\n    showConfusionMatrixFromFile(ResReport30IOU)\n","c87cf906":"topNum = 10 # provide even number\nvisualizePredictions(ResReport30IOU, topNum)","492e217d":"TP = 372\nTN = 4214\ncorrectPredictions = TP + TN\ntotalPredictions = 4954\n\nclassificationAccuracy = correctPredictions \/ totalPredictions * 100.0\nprint(\"Classification Accuracy :\", classificationAccuracy)\n\n\nFP = 0\nFN = 368\nincorrectPredictions = FP + FN\n\nclassificationError = incorrectPredictions \/ totalPredictions\nprint(\"Classification Error :\", classificationError)","93c0b9b9":"chosenProbability = 1.96    # 1.96 for (95%)\nnumOfObservations = 4954\n\nclassificationInterval = (chosenProbability * math.sqrt(classificationError * (1 - classificationError) \/ numOfObservations))\nprint(\"Classification Interval :\", classificationInterval)\n\nconfidenceIntervalUpper = classificationError + classificationInterval\nprint(\"Confidence Interval for classification error Upper Level:\", confidenceIntervalUpper)\n\nconfidenceIntervalLower = classificationError - classificationInterval\nprint(\"Confidence Interval for classification error Lower :\", confidenceIntervalLower)\n","6e2d889d":"\ntest_y = ResReport30IOU[\"Target\"].apply(int)\npredicted_y = ResReport30IOU[\"predTarget\"].apply(int)\n# IoU values are prediction probabilities. \npred_prob_y = ResReport30IOU[\"iou\"] # Taking float values.\n\nimport matplotlib.pyplot as plt\nfpr, tpr, thresholds = metrics.roc_curve(test_y, pred_prob_y)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for pnuemonia classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","5d3620b2":"threshold = 0.3 # The IoU threshold set to classify as \"Lung Opacity\"\nprint(\"Sensitivity:\", tpr[thresholds > threshold][-1])\nprint(\"Specificity:\", 1 - fpr[thresholds > threshold][-1])\nprint(\"AUC :\", metrics.roc_auc_score(test_y, pred_prob_y))\n","b51f1fdd":"#### Analyzing Train Lables Dataset","d4786e08":"#### Getting Metadata Information","28b82a63":"3 Unique classes observed \n1 - No Lung Opacity\/Not Normal,\n2 - Normal,\n3 - Lung Opacity","4989ebb1":"#### Function to load the metadata from images","f7b7eb00":"From Above Analysis our concatenated data is correct","2d3d2153":"### Constants","09166807":"* We started with exploring data and checking how clean the data is. Observed three different classes and after analysis we logically categorized them into two to build the model.\n\n* Model selection was a challenge as both localization and classification was needed into one. After interim sumbmission we started to Build Faster R-CNN architecture. After spending a week on that we realized that for binarry classification U-Net is better and simpler than Faster R-CNN. We found that U-Net is also widely used in medical applications. Hence switched to U-Net.\n\n* First we built siple U-Net with MobileNet. We achieved near to 90% performance on it. Further we made advancement to it by choosing ResNet50 for U-Net. It was a struggle to identify the right points from ResNet50 layers for the crossconnections to the upsampling part of the U-Net. With some experiments we built the right architecture and achieved better results near to 93%.\n\n* Had we saved a time what we spent on building Faster R-CNN, we would have done some upsampling of the 'Lung Opacity' class using augmentations. This would have helped us improving Recall value for 'Lung Opacity' class. \n\n* Overall it was a great experience working on this project and we learned a lot in the journey. ","f0edc3c4":"### Classification Accuracy and Error","937c8bf7":"Step 3 -> Checking Unique Patients","c47b8013":"Total number of unique patients in data - 26684\n\n**Observation - As we have total 30227 records and out of that 26684 are unique records, this shows presence of multiple records for some patients**","36f0afce":"## Build UNet with ResNet50\n\n* We started with a simple UNet architecture using MobileNet layers pretrained with 'imagenet' weights.\n\n* Now we do advancement to it by using ResNet50 layers  pretrained with 'imagenet' weights to build the UNet architecture.","0759cd14":"# Exploratory Data Analysis on Train Labels and Detail Info CSV data sets","a03fa46c":"No Missing Values found","73db48f8":"# Build UNet","1fda48d8":"Step 2 -> Check for Missing Values\n","ab65d21f":"Step 4 -> Checkin unqiue Targets","6023c868":"#### Lets see some of the top predictions alongwith the bounding box IoU values.\n\n* Red boxes are ground truths\n* Blue boxes are predictions","c79e2638":"* Observation\n    * train, validation and test sets are not well balanced. \n    * Images with 'Lung Opacity' are less in proportion in all these sets.","aa4891bf":"Step 4 -> Checking unique Classes","912293d5":"## Prepare data for training\n\n\nStep 1 -> \n* Convert data to only two classes, 'Normal' and 'Lung Opacity'\n* Splitting the data in three parts, train, validation and test sets.\n* Replace all 'NaN' values to zero(0)\n","323d3f1d":"#### Show Confusion Matrix and Classification Report","19994c8c":"#### Predict the Test data set","8f6c2cf6":"Total Unique patients found - 26684\n\nThis is same as the number of patients in Detailed CSV sheet hence both sheets share the information for same patients","c2e14a2a":"* Trained all layers of ResNet50 starting with initial pre-trained  weights on \u2018imagenet\u2019. All U-Net decoder layer weights are trained from zero. \n* We have image dataset very different from the \u2018imagenet\u2019. Here identifying features\/segments for pattern is important, than finding an object. Hence decided to train all the weights, starting with pre-loaded \u2018imagenet\u2019 weights.\n","c8f3c875":"**Analyzing Detailed Classes CSV file**","7c192ee5":"#### visualize sample image with a mask to verify train generator instance ","0e01dcd1":"### Confidence Interval\n\nRather than presenting just a single error score, a confidence interval can be calculated and presented as part of the model skill.\n\nA confidence interval is comprised of two things:\n\nRange:- This is the lower and upper limit on the skill that can be expected on the model.\n\nProbability:- This is the probability that the skill of the model will fall within the range.\n\nIn general, the confidence interval for classification error can be calculated as follows:\n\n**error +\/- const * sqrt( (error * (1 - error)) \/ n)**\n\nWhere error is the classification error, const is a constant value that defines the chosen probability, sqrt is the square root function, and n is the number of observations (rows) used to evaluate the model. Technically, this is called the Wilson score interval.\n\nThe values for const are provided from statistics, and common values used are:\n\n1.64 (90%)\n\n1.96 (95%)\n\n2.33 (98%)\n\n2.58 (99%)\n","bb2440ca":"#### After 4th epoch we see that the validation loss getting increased which indicates overfitting of the model.\n* Hence we take the weights saved after 4th epoch (i.e. **\u2018resnetModel-0.78.h5\u2019**) for performing predictions.\n","3bcd27f3":"*  ** Even though some of the IoU valuses are less than 80%, we observe that the ground truth boxes are completely covered within the predicted boxes. **\n\n* **It is important to see that images with different brightness, clarity, contrast are amoung the top predictions.**","c1c0768b":"Step 3 -> Checking unique Patients","74050512":"#### Preparing Dataset with patient id and respective image paths","b987c510":"### Top predicted masks with bounding boxes\n\n* Lets see some of the the top predictions\n* Red boxes are ground truth\n* Blue boxes are predictions","d5f95a9f":"Step 1 -> Sorting both the datasets based on patientId","1e608bf1":"#### Observation - For around 20672 patients Bounding box cordinates not available where as for 9555 patients its avaialable","e1efff9b":"**As we have 3 Classes in Detailed_Info dataset and 2 Target Variables in Train_Labels, concatenating to get better insight into the data**","bd5258e1":"#### Plot training history curves","e8659f1f":"#### Train the Model","1eb467cc":"Step 1 -> Define and read the Detail_Info CSV File ","cbcfb213":"#### Insights from Metadata\n\nThere are 26684 images in the Training data\nThese are DICOM Images which has pixel information as well as several tags added to it like patientid, age,gender etc.","8a137323":"* **There is a 95% likelihood that the confidence interval [0.0670, 0.0816] covers the true classification error of the model on unseen data.**\n    * That means the model will give classification error between 6.70% and 8.16%; 95% of the time on unseen data, that means in production. \n\n\n*** In other words, the model will give accuracy between **91.84%**(100-8.16) and **93.3%**(100-6.70) with 95% of confidence(or 95% of the time)**","5c507939":"* Classification Accuracy is 92.57%. The model is 92.57% accurate.\n* Classification Error is 7.42%.","25af4eb0":"#### As mentioned above we are using weights saved after 4th epoch. \n* Here we load the submission file saved with these weights and showing confusion matrix. ","e3448c86":"Step 1 -> Reading the data ","208939c2":"* AUC(Area Under Curve) is the percentage of the ROC plot that is underneath the curve.\n* If you randomly chose one 'Lung Opacity' and one 'Normal' observation, AUC represents the likelihood that the model will assign a higher predicted probability to the 'Lung Opacity' observation.\n* This tells the power of a test which in this case is about 82%.","5dedfe4b":"# Summary","faba866e":"![image.png](attachment:image.png)\n\n**Providing a screenshot here of the training process output. \nFurther we are loading the best saved weights for prediction.**","acbff0b6":"Step 2 -> Concatenating the data\n\n* To get the Target and classes into one dataframe","9f61e320":"#### Loaging a sample Image","0539decf":"Few records have observed with missing values in **x, y, width** and **height** coulmn, but no missing values observed in **patientId** and **Target**.\n\nAlso this is observed such missing columns are present for those records with **Target** as '0'.\n\n**x, y, width** and **height** columns have the information for bounding boxes in Images where Penumonia is detected.\n\n**Explaination on missing values - These are not the missing values instead it is expected not to have Bounding Box co-ordinates for those images where Pneumonia is not detected (Target - '0')**\n\nHence concluding there are no missing values in this dataset as well","7bf7e385":"### Function definations","b66738a2":"### Create Generator instances for Train and Validation datasets\n\n* These are further used in fit() method while training the model.","eb030812":"#### Functions to load image and patientId(file name) from the given path","96e30f99":"#### Setting up project path","8df10db2":"### Function and Class definations","f6e87107":"Step 3 -> Checking the shape of data frame","a1567a0b":"**Validating the concatenation results**","9c1935a9":"### Importing all the python Libraries","5692c473":"Trail label has only 2 target variables [0 & 1] \n\n**Conclusion - In Train labels only two target variables are present 0 & 1, where as in Detailed_Info sheet we have 3 classes.**","483c28b0":"#### Visualize the UNet architecture","659a820d":"Step 2 -> Checking the missing values if any","a3a5960d":"# Overview\n******\nPneumonia is an infection in one or both lungs. The infection causes inflammation in the air sacs in your lungs, which are called **alveoli**. The alveoli fill with fluid or pus, making it difficult to breathe.\n\n![Pneumonia Inflammation](https:\/\/www.physio-pedia.com\/images\/9\/94\/Pneumonia_Inflammation.jpg)","d62f59b1":"## Build UNet with MobileNet\n\n* We start with a simple UNet architecture using MobileNet layers pretrained with 'imagenet' weights.","3570327d":"\nDoctors or radiologiests conduct a physical exam and use **CXR**(chest x-ray) to examin and detect pneumonia. In CXR it shows opacity in the reagion.\n\n![Pneumonia CXRs](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0092867418301545-figs6_lrg.jpg)\n\n\n* There are multiple causes of opacity n CXR other than pneumonia like;\n    * fluid overload (pulmonary edema)\n    * bleeding\n    * volume loss (atelectasis or collapse)\n    * lung cancer\n    * post-radiation or surgical changes\n    * Outside of the lungs, fluid in the pleural space (pleural effusion) \n\nA number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR.\n\n\n\nPneumonia opacity can occour in different reagions of chest and the opacity can be of different kinds. This makes it the problem of detection(regression) as well as recognition(classification). For such purpose we can think of using the pre-existing models like 'Faster R-CNN' or 'Yolo'.\n\n![Types and Regions](http:\/\/adigaskell.org\/wp-content\/uploads\/2017\/11\/pneumonia-xray.jpg)\n\n","de29bd23":"#### Train the Model","8d0144da":"### ROC and AUC","2eab70d8":"#### This Confusion matrix and Classification report are much better that that of U-Net with MobileNet.\n\n#### Analyze Confusion matrix\n* In this capstone project, the goal is to build a pneumonia detection system, to locate the position of inflammation in an image.\n* Thought there are three classes given in the dataset, for this classification we have considered \u201cNot Normal No Lung Opacity\u201d class as \"Normal\" and reduced the problem to binary classification.\n* So we are interested in predicting patients who has pneumonia, i.e. \"Lung Opacity\" class.\n* Since we have a data imbalance considering classes, the overall accuracy is not the right measure to evaluate the model. Hence it is important to analyze confusion martix. This gives accuracy at the class level.\n\nLets analyze confusion martix with this interest in mind.\n\nIn actual test data;\n* True Positives(TP) are;\n    * 372 patients who have actually 'Lung Opacity' correctly predicted as has 'Lung Opacity'.\n\n\n* True Negatives(TN) are;\n    * 4214 patients who have actually 'No Lung Opacity'\/'Normal' correctly predicted as 'Normal'.\n\n\n* False Positives(FP) are;\n    * 0 patients(No patients) who have actually 'No Lung Opacity'\/'Normal' incorrectly predicted as having 'Lung Opacity'. \n    * This means there are no model mistakes in identifying 'Normal' as 'Lung Opacity'. Type-I error is zero.\n    * This is a big positive point for a medical test. \n    \n\n* False Negatives(FN) are;\n    * 368 patients who have actually 'Lung Opacity' incorrectly predicted as'Normal'. \n    * These are model mistakes. \n    \n\n#### Analyze Precision and Recall\n\n* Precision For 'Lung Opacity' = TP\/(TP+FP)\n    * The above FP value impacts 'Precision' value. Hence we see 'Precision' for category '1' i.e. 'Lung Opacity' is 1 (100%). \n\n\n* Recall For 'Lung Opacity' = TP\/(TP+FN) \n    * The 'Recall' value for  category '0' i.e. 'Normal' is 1 (100%).\n    * The above FN value impacts 'Recall' value. There is a scope to improve the model 'Recall' value for category '1' i.e. 'Lung Opacity'.\n\n\n* f1-score is harmonic mean of Precision and Recall. Ths is used when we are interested in both.\n    * 'accuracy' f1-score is 93%\n    * 'weighted avg' f1-score is 91%\n\n","5475ef72":"#### Observe ResNetUNet Model training history\n* See how loss and mean-IoU improved over the epochs","0e7c76c1":"* 'False Positive Rate' is the total number of FP divide by 'total number of negatives' in the test set.\n    * FPR = FP\/N = FP\/FP+TN \n\n\n* 'True Positive Rate' is the total number of TP divide by 'total number of positives' in the test set.\n    * TPR = TP\/TP+FN\n    * This is same as 'Recall' for class 'Lung Opacity'.\n    \n    \n* ROC curve is not affected by the imbalance(skeweness) of the classes in the dataset.\n* This curve is with the different probability cutoff values for the target class.\n\n\n* The curve shows non-zero value for zero FPR. This is beacuse the FP value is zero in confusion matrix. \n    * That means for any threshold on the predicted probabilities; when the FP is zero, TP is always above zero.\n"}}