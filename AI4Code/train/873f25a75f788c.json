{"cell_type":{"2cc3b0ce":"code","d1b48620":"code","e524665e":"code","4f2e60b6":"code","cc546c0d":"code","f7b9c903":"code","acc17cfb":"code","65f00c63":"code","96107106":"code","5d5869db":"code","2d4102c0":"code","eae6c9ce":"code","82e88c86":"code","ae2b5f3a":"code","d81ee804":"code","217b2c44":"code","763c5b37":"code","5c77838f":"code","cc2f028b":"code","d3bcf25b":"code","0dc3df1e":"code","6527d18a":"code","fd821ddd":"code","291c7848":"code","509f2093":"code","00443a77":"code","04bb08e3":"code","81d0c5ba":"code","d883405e":"markdown","d0ec3c4a":"markdown","5d6d504a":"markdown","33e47098":"markdown","39026314":"markdown","44acb152":"markdown","2b323ec8":"markdown","07178860":"markdown","0a3f7e7c":"markdown","6673302a":"markdown","1d122c81":"markdown","6822ab5e":"markdown","514e3473":"markdown","88e1fc78":"markdown","bff71256":"markdown"},"source":{"2cc3b0ce":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\n","d1b48620":"# to display all the columns in the dataset\npd.pandas.set_option('display.max_columns', None)","e524665e":"# load data\ntrain = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\")\ncalendar = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nsell_prices = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")","4f2e60b6":"train.shape,calendar.shape,sell_prices.shape","cc546c0d":"train.info()","f7b9c903":"calendar.info()","acc17cfb":"train.head()","65f00c63":"calendar.head()","96107106":"sell_prices.head()","5d5869db":"train.isnull().sum().sort_values(ascending = False)","2d4102c0":"calendar.isnull().sum().sort_values(ascending = False)","eae6c9ce":"for i in range(1942,1970):\n    col = \"d_\"+ str(i)\n    train[col] = 0","82e88c86":"#Downcast in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","ae2b5f3a":"train = downcast(train)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","d81ee804":"grid_df = pd.melt(train, \n                  id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                  var_name = 'd', \n                  value_name = \"sales\")","217b2c44":"grid_df.head()","763c5b37":"#del train\n#gc.collect()","5c77838f":"master = pd.merge(grid_df,calendar, on = \"d\")\nmaster.head()","cc2f028b":"del calendar,grid_df\ngc.collect()","d3bcf25b":"master = pd.merge(master, sell_prices, on=['store_id','item_id','wm_yr_wk'], how='left') \nmaster.head()","0dc3df1e":"del sell_prices\ngc.collect()","6527d18a":"# convert numeric variables into categorical variables\nconv_id = dict(zip(master.id.cat.codes, master.id))\nconv_item_id = dict(zip(master.item_id.cat.codes, master.item_id))\nconv_dept_id = dict(zip(master.dept_id.cat.codes, master.dept_id))\nconv_cat_id = dict(zip(master.cat_id.cat.codes, master.cat_id))\nconv_store_id = dict(zip(master.store_id.cat.codes, master.store_id))\nconv_d_state_id = dict(zip(master.state_id.cat.codes, master.state_id))","fd821ddd":"master.d = master['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = master.dtypes.index.tolist()\ntypes = master.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        master[cols[i]] = master[cols[i]].cat.codes","291c7848":"master.head()","509f2093":"master.drop('date',1,inplace = True)","00443a77":"valid = master[(master['d']>=1914) & (master['d']<1942)][['id','d','sales']]\ntest = master[master['d']>=1942][['id','d','sales']]\neval_preds = test['sales']\nvalid_preds = valid['sales']","04bb08e3":"cats = master.cat_id.astype('category').cat.codes.unique().tolist()\nfor cat in cats:\n    df = master[master['cat_id']==cat]\n    \n    # split the data into train,validate and test\n    X_train, y_train = df[df['d']<1914].drop('sales',axis=1), df[df['d']<1914]['sales']\n    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sales',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sales']\n    X_test = df[df['d']>=1942].drop('sales',axis=1)\n    \n    #model\n    model = LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        max_depth=8,\n        num_leaves=50,\n        min_child_weight=300\n    )\n    print('*****Prediction for Category: {}*****'.format(conv_cat_id[cat]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n             eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n    valid_preds[X_valid.index] = model.predict(X_valid)\n    eval_preds[X_test.index] = model.predict(X_test)\n    del model, X_train, y_train, X_valid, y_valid\n    gc.collect()","81d0c5ba":"valid['sales'] = valid_preds\nvalidation = valid[['id','d','sales']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sales').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(conv_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sales'] = eval_preds\nevaluation = test[['id','d','sales']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sales').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(conv_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.to_csv('submission.csv',index=False)","d883405e":"# Check Null Values","d0ec3c4a":"# Final Submission","5d6d504a":"# MELT the Dataset (wide form to long form) ","33e47098":"# Let's take a sneak peek of the data","39026314":"# Model Training & Validation","44acb152":"# Add Zero sales for dates d_1942 to d_1969","2b323ec8":"We can see that the dataset has been transformed, a new column **\"d\"** is added, this column will have all the different dates (d_1 to d_1969), there is another column added \"**sales**\", this column will have the sales info for that particular day.","07178860":"# Load Data","0a3f7e7c":"Lots of zeros above shows particular item was either not sold on that particular day or was not in stock","6673302a":"# Model Building - LGBM","1d122c81":"# Create Train,Validity and Test Dataframes","6822ab5e":"# Memory Usage Reduction\n\nWe need to melt the dataset in order to proceed further. but before we do that, we need to reduce the memory usage. if we dont reduce memory usage, we may get memory usage errors.","514e3473":"**Let's convert Categorical Variables into numeric variables**","88e1fc78":"# Introduction\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin.\n\n# Task in hand\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the test set.\n\nThe rows  [d_1914 - d_1941] form the validation set.\n\nRemaining rows form the training set.\n\n    This notebook covers Modelling only, to check EDA, check https:\/\/www.kaggle.com\/jagdmir\/m5-forecasting-part-one-eda.","bff71256":"# Merge the datasets\n\nFirst we will merge grid_df which actuall has sales information for individual items with calendar dataframe, so that we can repalce d_1 etc values with actual dates"}}