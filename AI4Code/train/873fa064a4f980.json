{"cell_type":{"bad377e6":"code","ae3fbfed":"code","2dd7feb7":"code","7d606ca7":"code","691cf738":"code","820cec55":"code","872c4cc8":"code","82137bbc":"code","a175e79f":"code","d8831f66":"code","a16703da":"code","b74c32c3":"code","d222cb41":"code","a91ff895":"code","82c0f970":"code","6d76eeb2":"code","be57da13":"code","ea5a46d7":"code","7a868d14":"code","35cd43ee":"code","54ea162d":"code","450bab5d":"code","eaa0617b":"code","d11c9634":"code","fb2f4968":"code","c475df22":"code","48c2a514":"code","7cb961cf":"code","11d45406":"code","15967c50":"code","b9677a59":"code","efb89547":"code","571bf7ce":"code","4b0b2d27":"code","aac8dbb9":"code","c08faa90":"code","18403d35":"code","4e081e9c":"code","9858316e":"code","b6a8cbb9":"code","9b732728":"code","b22ddb77":"code","7aa9bf12":"code","37bb0f8e":"code","87dab550":"code","b66f61c0":"code","0b04850e":"code","24db9145":"code","e774b322":"code","7a05fd91":"code","4e139453":"code","389a3dbe":"code","c00f328f":"code","1a641c5a":"code","58120c29":"code","1b853a1e":"code","f8c1ba90":"code","a00ac108":"code","988c2057":"code","84985a01":"code","eb22e08b":"code","439faf1e":"code","08840a39":"code","2a7da96a":"code","caf1465e":"code","d76d4183":"code","7f44efb7":"code","6b68ce88":"code","7d49dad1":"code","08ea158d":"code","c9de88ac":"code","582bbb14":"code","9b42e1af":"code","703b7f27":"code","4a5754a9":"code","08b29071":"code","515d06e5":"code","d5e8c0d8":"code","a1c64dab":"code","f7275eed":"code","6d641c09":"code","63222d34":"code","697a1455":"code","10fad547":"code","5d3da555":"code","d2dd0d63":"code","f31f71c1":"code","e0ab6d83":"code","d6ce8941":"code","8102b1a1":"code","cc93139f":"code","dbe8de8f":"code","58f6ed10":"code","3d2d9ffe":"code","5795c0c0":"code","773594d6":"code","a2d965f7":"code","16c3a224":"code","fd24dd2e":"code","74fd9166":"code","64e33125":"code","e804197c":"code","8dc53726":"code","9d7b431a":"code","303cc51b":"code","3275ed8e":"code","6ac95b6c":"code","38a6387a":"code","7f6e6c72":"code","fbfcd0c1":"code","9d4701b2":"code","ef948651":"markdown","1719244d":"markdown","0bd987d9":"markdown","d6d1a1ab":"markdown","67391e5f":"markdown","de983dfb":"markdown","5668234f":"markdown","5622af70":"markdown","953e660e":"markdown","65c2582c":"markdown","4dc3d232":"markdown","ddc62d19":"markdown","3ef61639":"markdown","d596e387":"markdown","df6555ee":"markdown","be5320bd":"markdown","81d7db1b":"markdown","417487a9":"markdown","ec32fbc8":"markdown","c2e2e311":"markdown","9f27b9fd":"markdown","86ac9df3":"markdown","161d3e48":"markdown","1e2f2dd0":"markdown","f0312ed5":"markdown","e06f7589":"markdown","1d90e279":"markdown","6234fc09":"markdown","f4b07d02":"markdown","e3cb4a7e":"markdown","8bbbd356":"markdown","32d647c3":"markdown","24c2a064":"markdown","8baca77e":"markdown","4494d28d":"markdown","5e7bd42b":"markdown","43767990":"markdown","c9791dc3":"markdown","2d0e9909":"markdown","9d45e4ac":"markdown","c5f6b880":"markdown","74c13a1f":"markdown","d8e89872":"markdown","d14f35ee":"markdown","68739611":"markdown","e7433e92":"markdown","8055a7eb":"markdown","853bd6b7":"markdown","4d6dc5e6":"markdown","35682a64":"markdown","243cda7c":"markdown","728bcd24":"markdown","4c619501":"markdown","ed4181a9":"markdown","b34836bb":"markdown","301a8ba4":"markdown","bee829b4":"markdown","b5e7fc1d":"markdown","cae3c815":"markdown","37a60747":"markdown","03eafff2":"markdown","19c20a31":"markdown","674c47a9":"markdown","c1f9a632":"markdown","1ad7d9e9":"markdown","0a406aab":"markdown","f9f195d6":"markdown","b8cec30e":"markdown","40c550ea":"markdown","d66afc2c":"markdown","f31e61e1":"markdown","98542e15":"markdown"},"source":{"bad377e6":"# Importation of useful libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport random \nimport datetime as dt\nimport re\nimport pickle\nimport nltk, warnings\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom string import digits, punctuation\n\n\nfrom scipy.stats import chi2_contingency\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))","ae3fbfed":"# Importing the database \n\ndata = pd.read_csv(\"..\/input\/data.csv\", encoding=\"ISO-8859-1\", dtype={'CustomerID': str,'InvoiceID': str})","2dd7feb7":"data.head(5)","7d606ca7":"data.info()","691cf738":"plt.figure(figsize=(5, 5))\ndata.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","820cec55":"nan_rows = data[data.isnull().T.any().T]\nnan_rows.head(5)","872c4cc8":"data[data['InvoiceNo']== '536414']","82137bbc":"data[data['InvoiceNo']== '536544'][:5]","a175e79f":"data = data.dropna(subset=[\"CustomerID\"])","d8831f66":"plt.figure(figsize=(5, 5))\ndata.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","a16703da":"print('Dupplicate entries: {}'.format(data.duplicated().sum()))\ndata.drop_duplicates(inplace = True)","b74c32c3":"data.Country.nunique()","d222cb41":"customer_country=data[['Country','CustomerID']].drop_duplicates()\ncustomer_country.groupby(['Country'])['CustomerID'].aggregate('count').reset_index().sort_values('CustomerID', ascending=False)","a91ff895":"data.describe()","82c0f970":"data[(data['Quantity']<0)].head(5)","6d76eeb2":"# Constucting a basket for later use\ntemp = data.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nnb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})","be57da13":"nb_products_per_basket.InvoiceNo = nb_products_per_basket.InvoiceNo.astype(str)\nnb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\nlen(nb_products_per_basket[nb_products_per_basket['order_canceled']==1])\/len(nb_products_per_basket)*100","ea5a46d7":"nb_products_per_basket[nb_products_per_basket['order_canceled']==1][:5]","7a868d14":"data[data['CustomerID'] == '12346']","35cd43ee":"test = data[data['Quantity'] < 0][['CustomerID','Quantity',\n                                                   'StockCode','Description','UnitPrice']]\nfor index, col in  test.iterrows():\n    if data[(data['CustomerID'] == col[0]) & (data['Quantity'] == -col[1]) \n                & (data['Description'] == col[2])].shape[0] == 0: \n        print(test.loc[index])\n        print('Our initial hypothesis is wrong')\n        break","54ea162d":"data[data['CustomerID'] == '14527'].head(5)","450bab5d":"data_check = data[(data['Quantity'] < 0) & (data['Description'] != 'Discount')][\n                                 ['CustomerID','Quantity','StockCode',\n                                  'Description','UnitPrice']]\n\nfor index, col in  data_check.iterrows():\n    if data[(data['CustomerID'] == col[0]) & (data['Quantity'] == -col[1]) \n                & (data['Description'] == col[2])].shape[0] == 0: \n        print(index, data_check.loc[index])\n        print('The second hypothesis is also wrong')\n        break","eaa0617b":"data[(data['CustomerID'] == '15311') & (data['Description'] == 'SET OF 3 COLOURED  FLYING DUCKS')]","d11c9634":"df_cleaned = data.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0\n\nentry_to_remove = [] ; doubtfull_entry = []\n\nfor index, col in  data.iterrows():\n    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n    df_test = data[(data['CustomerID'] == col['CustomerID']) &\n                         (data['StockCode']  == col['StockCode']) & \n                         (data['InvoiceDate'] < col['InvoiceDate']) & \n                         (data['Quantity']   > 0)].copy()\n    #_________________________________\n    # Cancelation WITHOUT counterpart\n    if (df_test.shape[0] == 0): \n        doubtfull_entry.append(index)\n    #________________________________\n    # Cancelation WITH a counterpart\n    elif (df_test.shape[0] == 1): \n        index_order = df_test.index[0]\n        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n        entry_to_remove.append(index)        \n    #______________________________________________________________\n    # Various counterparts exist in orders: we delete the last one\n    elif (df_test.shape[0] > 1): \n        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n        for ind, val in df_test.iterrows():\n            if val['Quantity'] < -col['Quantity']: continue\n            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n            entry_to_remove.append(index) \n            break    ","fb2f4968":"print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\nprint(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))","c475df22":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","48c2a514":"df_cleaned.drop(remaining_entries.index, axis = 0, inplace = True)","7cb961cf":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes","11d45406":"df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'POST']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'D']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'C2']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'M']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'BANK CHARGES']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'PADS']\ndf_cleaned = df_cleaned[df_cleaned['StockCode']!= 'DOT']","15967c50":"df_cleaned.describe()","b9677a59":"df_cleaned[(df_cleaned['UnitPrice'] == 0)].head(5)","efb89547":"def unique_counts(data):\n   for i in data.columns:\n       count = data[i].nunique()\n       print(i, \": \", count)\nunique_counts(df_cleaned)","571bf7ce":"# Total price feature\n\ndf_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])","4b0b2d27":"revenue_per_countries = df_cleaned.groupby([\"Country\"])[\"TotalPrice\"].sum().sort_values()\nrevenue_per_countries.plot(kind='barh', figsize=(15,12))\nplt.title(\"Revenue per Country\")","aac8dbb9":"No_invoice_per_country = df_cleaned.groupby([\"Country\"])[\"InvoiceNo\"].count().sort_values()\nNo_invoice_per_country.plot(kind='barh', figsize=(15,12))\nplt.title(\"Number of Invoices per Country\")","c08faa90":"le = LabelEncoder()\nle.fit(df_cleaned['Country'])","18403d35":"l = [i for i in range(37)]\ndict(zip(list(le.classes_), l))","4e081e9c":"df_cleaned['Country'] = le.transform(df_cleaned['Country'])","9858316e":"with open('labelencoder.pickle', 'wb') as g:\n    pickle.dump(le, g)","b6a8cbb9":"df_cleaned.head(5)","9b732728":"df_cleaned['InvoiceDate'].min()","b22ddb77":"df_cleaned['InvoiceDate'].max()","7aa9bf12":"# I'll just fix the date to be one day after the last entry in the databse\n\nNOW = dt.datetime(2011,12,10)\ndf_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])","37bb0f8e":"custom_aggregation = {}\ncustom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"TotalPrice\"] = \"sum\"\n\n\nrfmTable = df_cleaned.groupby(\"InvoiceNo\").agg(custom_aggregation)","87dab550":"rfmTable[\"Recency\"] = NOW - rfmTable[\"InvoiceDate\"]\nrfmTable[\"Recency\"] = pd.to_timedelta(rfmTable[\"Recency\"]).astype(\"timedelta64[D]\")","b66f61c0":"rfmTable.head(5)","0b04850e":"custom_aggregation = {}\n\ncustom_aggregation[\"Recency\"] = [\"min\", \"max\"]\ncustom_aggregation[\"InvoiceDate\"] = lambda x: len(x)\ncustom_aggregation[\"TotalPrice\"] = \"sum\"\n\nrfmTable_final = rfmTable.groupby(\"CustomerID\").agg(custom_aggregation)","24db9145":"rfmTable_final.columns = [\"min_recency\", \"max_recency\", \"frequency\", \"monetary_value\"]","e774b322":"rfmTable_final.head(5)","7a05fd91":"first_customer = df_cleaned[df_cleaned['CustomerID']=='12747']\nfirst_customer.head(5)","4e139453":"quantiles = rfmTable_final.quantile(q=[0.25,0.5,0.75])\nquantiles = quantiles.to_dict()","389a3dbe":"segmented_rfm = rfmTable_final","c00f328f":"def RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4\n    \ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1","1a641c5a":"segmented_rfm['r_quartile'] = segmented_rfm['min_recency'].apply(RScore, args=('min_recency',quantiles,))\nsegmented_rfm['f_quartile'] = segmented_rfm['frequency'].apply(FMScore, args=('frequency',quantiles,))\nsegmented_rfm['m_quartile'] = segmented_rfm['monetary_value'].apply(FMScore, args=('monetary_value',quantiles,))\nsegmented_rfm.head()","58120c29":"segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str)\nsegmented_rfm.head()","1b853a1e":"segmented_rfm[segmented_rfm['RFMScore']=='111'].sort_values('monetary_value', ascending=False)","f8c1ba90":"segmented_rfm.head(5)","a00ac108":"segmented_rfm = segmented_rfm.reset_index()","988c2057":"segmented_rfm.head(5)","84985a01":"df_cleaned = pd.merge(df_cleaned,segmented_rfm, on='CustomerID')","eb22e08b":"df_cleaned.columns","439faf1e":"df_cleaned = df_cleaned.drop(columns=['r_quartile', 'f_quartile', 'm_quartile'])","08840a39":"df_cleaned['Month'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.month)","2a7da96a":"df_cleaned['Month'].value_counts()","caf1465e":"df_cleaned['Weekday'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.weekday())\ndf_cleaned['Day'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.day)\ndf_cleaned['Hour'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.hour)","d76d4183":"df_cleaned.head(5)","7f44efb7":"X = df_cleaned[\"Description\"].unique()\n\nstemmer = nltk.stem.porter.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\n\ndef stem_and_filter(doc):\n    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n    return [token for token in tokens if token.isalpha()]\n\nanalyzer = TfidfVectorizer().build_analyzer()\nCV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\nTF_IDF_matrix = CV.fit_transform(X)\nprint(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)","6b68ce88":"svd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\n\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\nprint(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)","7d49dad1":"score_tfidf = []\n\nx = list(range(5, 155, 10))\n\nfor n_clusters in x:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n    kmeans.fit(TF_IDF_embedded)\n    clusters = kmeans.predict(TF_IDF_embedded)\n    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)\n\n    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n    score_tfidf.append(silhouette_avg)","08ea158d":"plt.figure(figsize=(20,16))\n\nplt.subplot(2, 1, 1)\nplt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\nplt.title(\"Evolution of the Silhouette Score\")\nplt.legend()","c9de88ac":"n_clusters = 135\n\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\nproj = kmeans.fit_transform(TF_IDF_embedded)\nclusters = kmeans.predict(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"ACP with 135 clusters\", fontsize=\"20\")","582bbb14":"tsne = TSNE(n_components=2)\nproj = tsne.fit_transform(TF_IDF_embedded)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")","9b42e1af":"plt.figure(figsize=(20,8))\nwc = WordCloud()\n\nfor num, cluster in enumerate(random.sample(range(100), 12)) :\n    plt.subplot(3, 4, num+1)\n    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(\"Cluster {}\".format(cluster))\n    plt.axis(\"off\")\nplt.figure()","703b7f27":"pd.Series(clusters).hist(bins=100)","4a5754a9":"dict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}","08b29071":"with open('product_clusters.pickle', 'wb') as h:\n    pickle.dump(dict_article_to_cluster, h)","515d06e5":"cluster = df_cleaned['Description'].apply(lambda x : dict_article_to_cluster[x])\ndf2 = pd.get_dummies(cluster, prefix=\"Cluster\").mul(df_cleaned[\"TotalPrice\"], 0)\ndf2 = pd.concat([df_cleaned['InvoiceNo'], df2], axis=1)\ndf2_grouped = df2.groupby('InvoiceNo').sum()","d5e8c0d8":"custom_aggregation = {}\ncustom_aggregation[\"TotalPrice\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"min_recency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"max_recency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"frequency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"monetary_value\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Quantity\"] = \"sum\"\ncustom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n\n\ndf_grouped = df_cleaned.groupby(\"InvoiceNo\").agg(custom_aggregation)","a1c64dab":"df2_grouped_final = pd.concat([df_grouped['CustomerID'], df2_grouped], axis=1).set_index(\"CustomerID\").groupby(\"CustomerID\").sum()\ndf2_grouped_final = df2_grouped_final.div(df2_grouped_final.sum(axis=1), axis=0)\ndf2_grouped_final = df2_grouped_final.fillna(0)","f7275eed":"custom_aggregation = {}\ncustom_aggregation[\"TotalPrice\"] = ['min','max','mean']\ncustom_aggregation[\"min_recency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"max_recency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"frequency\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"monetary_value\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Quantity\"] = \"sum\"\ncustom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n\ndf_grouped_final = df_grouped.groupby(\"CustomerID\").agg(custom_aggregation)","6d641c09":"df_grouped_final.head(5)","63222d34":"df_grouped_final.columns = [\"min\", \"max\", \"mean\", \"min_recency\", \"max_recency\", \"frequency\", \"monetary_value\", \"quantity\", \"country\"]","697a1455":"df_grouped_final.head(5)","10fad547":"df2_grouped_final.head(5)","5d3da555":"X1 = df_grouped_final.as_matrix()\nX2 = df2_grouped_final.as_matrix()\n\nscaler = StandardScaler()\nX1 = scaler.fit_transform(X1)\nX_final_std_scale = np.concatenate((X1, X2), axis=1)","d2dd0d63":"x = list(range(2, 12))\ny_std = []\nfor n_clusters in x:\n    print(\"n_clusters =\", n_clusters)\n    \n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n    kmeans.fit(X_final_std_scale)\n    clusters = kmeans.predict(X_final_std_scale)\n    silhouette_avg = silhouette_score(X_final_std_scale, clusters)\n    y_std.append(silhouette_avg)\n    print(\"The average silhouette_score is :\", silhouette_avg, \"with Std Scaling\")","f31f71c1":"kmeans = KMeans(init='k-means++', n_clusters = 8, n_init=30, random_state=0)  # random state just to be able to provide cluster number durint analysis\nkmeans.fit(X_final_std_scale)\nclusters = kmeans.predict(X_final_std_scale)","e0ab6d83":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(clusters, bins=8)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of customers per cluster\")\nplt.xticks([rect.get_x()+ rect.get_width() \/ 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(8)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() \/ 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      \n        (x_value, y_value),         \n        xytext=(0, space),          \n        textcoords=\"offset points\", \n        ha='center',                \n        va=va)","d6ce8941":"df_grouped_final[\"cluster\"] = clusters","8102b1a1":"final_dataset = pd.concat([df_grouped_final, df2_grouped_final], axis = 1)\nfinal_dataset.head()","cc93139f":"final_dataset_V2 = final_dataset.reset_index()","dbe8de8f":"final_dataset_V2.to_csv(\"final_dataset_V2.csv\",index=False)","58f6ed10":"with open('df_cleaned.pickle', 'wb') as f:\n    pickle.dump(df_cleaned, f)","3d2d9ffe":"tsne = TSNE(n_components=2)\nproj = tsne.fit_transform(X_final_std_scale)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"25\")","5795c0c0":"final_dataset[final_dataset['cluster']==0]","773594d6":"final_dataset[final_dataset['cluster']==0].mean()","a2d965f7":"temp_final_df = final_dataset.reset_index()","16c3a224":"cust0 = list(temp_final_df[temp_final_df['cluster']==0]['CustomerID'])","fd24dd2e":"cluster0 = df_cleaned[df_cleaned['CustomerID'].isin(cust0)]\ncluster0[['Quantity', 'UnitPrice', 'QuantityCanceled', 'TotalPrice', 'frequency', 'min_recency'\n         , 'monetary_value']].mean()","74fd9166":"cluster0['Description'].value_counts()[:10]","64e33125":"custom_aggregation = {}\ncustom_aggregation[\"Country\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"RFMScore\"] = lambda x:x.iloc[0]\n\ncluster0_grouped = cluster0.groupby(\"CustomerID\").agg(custom_aggregation)","e804197c":"cluster0_grouped['RFMScore'].value_counts()","8dc53726":"cluster0_grouped['Country'].value_counts()","9d7b431a":"cluster0['Month'].value_counts()","303cc51b":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(cluster0['Month'], bins=12)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of invoices per month\")\nplt.xticks([rect.get_x()+ rect.get_width() \/ 2 for rect in patches], [\"Month {}\".format(x) for x in range(1, 13)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() \/ 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      \n        (x_value, y_value),         \n        xytext=(0, space),          \n        textcoords=\"offset points\", \n        ha='center',                \n        va=va)","3275ed8e":"temp['Year'] = cluster0[cluster0['Month']==12]['InvoiceDate'].map(lambda x: x.year)\ntemp['Year'].value_counts()","6ac95b6c":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(cluster0['Weekday'], bins=7)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of invoices per day of the week\")\nplt.xticks([rect.get_x()+ rect.get_width() \/ 2 for rect in patches], [\"Day {}\".format(x) for x in range(0, 7)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() \/ 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      \n        (x_value, y_value),         \n        xytext=(0, space),          \n        textcoords=\"offset points\", \n        ha='center',                \n        va=va)","38a6387a":"cluster0['Day'].nunique()","7f6e6c72":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(cluster0['Day'], bins=31)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of invoices per day of the month\")\nplt.xticks([rect.get_x()+ rect.get_width() \/ 2 for rect in patches], [\"Day {}\".format(x) for x in range(1,32)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() \/ 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      \n        (x_value, y_value),         \n        xytext=(0, space),          \n        textcoords=\"offset points\", \n        ha='center',                \n        va=va)","fbfcd0c1":"cluster0['Hour'].nunique()","9d4701b2":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(cluster0['Hour'], bins=14)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of invoices per hour of the day\")\nplt.xticks([rect.get_x()+ rect.get_width() \/ 2 for rect in patches], [\"Hour {}\".format(x) for x in (sorted(cluster0['Hour'].unique()))])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() \/ 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      \n        (x_value, y_value),         \n        xytext=(0, space),          \n        textcoords=\"offset points\", \n        ha='center',                \n        va=va)","ef948651":"Let's quickly classify the clusters in terms of importance to Datazon :\n\n- Cluster 2: high frequency with a lot of quantity (mean basket price of 513) bought on average and high monetary value (VIP clients)\n- Cluster 7 : very high purchase frequency with a mean basket price of 150 but good monetary value.\n- Cluster 4: very high basket price (huge quantity of products bought on average)\n- Cluster 0: good average customers \n- Cluster 6: good foreign customers \n- Cluster 1: almost lost customers\n- Cluster 5: highest monetary value but only one or two purchases over the year \n- Cluster 3: lost customers ","1719244d":"**1.3.2 RFM Principle**","0bd987d9":"We don't need the quartiles anymore, let's drop them.","d6d1a1ab":"I'll implement here the RFM principle to classify the customers in this database. This part is inspired by the work of Susan Li. RFM stands for Recency, Frequency and Monetary. It is a customer segmentation technique that uses past purchase behavior to divide customers into groups.","67391e5f":"Let's take a look at the missing values","de983dfb":"**2.4.9 Conclusion**","5668234f":"**1.2.3 InvoiceNo - Cancelation Code**","5622af70":"It appears that we can't replace the missing values and we can't keep data without the value in the customer id columns since we want to classify the customers. So I'll drop the lines with missing values on the customer ID column.","953e660e":"**2.4.4 Cluster 3**","65c2582c":"Let's go deeper in these missing values","4dc3d232":"**2.4.8 Cluster 7**","ddc62d19":"![](https:\/\/i.imgur.com\/z6FQJZm.jpg)","3ef61639":"**2.2 Final dataset grouped by customers**","d596e387":"**1.2.5 Outliers**","df6555ee":"In this notebook, I'll use the Online Retail Database which contains one year worth of clients' transactions from an e-commerce website. The objective here is to classify a client as soon as possible so the marketing team can push the right offers to the corresponding clients. \n\n","be5320bd":"This cluster is quiete heterogeneous since there are 17 best customers, 6 lost cheap customers and so on. They do have a high mean basket price of 505 but it's mostly due to the mean quantity they buy (130) because the mean unit price is very low (3.26)\n\nFor the time features, what is interesting is that these customers shop less on weekend and they shopped more at the end of the year.\n\n* Min Basket Price : 247\n* Mean Basket Price : 505\n* Max Basket Price : 1023\n* Quantity            130.299145\n* UnitPrice             3.264359\n* QuantityCanceled      2.332590\n* TotalPrice          184.308595\n\nTOP 10 products bought :\n\n* JUMBO BAG RED RETROSPOT :               38\n* BLACK RECORD COVER FRAME :            31\n* RECORD FRAME 7\" SINGLE SIZE :         28\n* REGENCY CAKESTAND 3 TIER :           25\n* WORLD WAR 2 GLIDERS ASSTD DESIGNS :    24\n* WHITE HANGING HEART T-LIGHT HOLDER :   24\n* PARTY BUNTING :                        24\n* LUNCH BOX I LOVE LONDON :               23\n* RED  HARMONICA IN BOX :                23\n* CHILLI LIGHTS :                        23","81d7db1b":"**1.3 Feature engineering**","417487a9":"We can now look at the countries' monetary value thanks to this feature","ec32fbc8":"<h1 style=\"font-family:verdana;\"> <center>2 years after working on the \"Online Retail Dataset\" I wanted to come back to this project and see if I could push it further and make the best out of this data. Luckily for me, a V2 of the dataset emerged and we now have the 2009 data. Can you please go see my latest notebook about this dataset. I've put a lot of work in it <br> https:\/\/www.kaggle.com\/miljan\/customer-segmentation-and-visualizations-in-plotly<\/center> <\/h1>\n\n\n***","c2e2e311":"It appears that when there is a discount there are no counterparts.\nLet's try again but without the discount values","9f27b9fd":"I already have done all the same analysis for the other clusters, so I won't put all of the code here but the idea is the same","86ac9df3":"**2.3.1. Cluster 0**","161d3e48":"**2. Creating customer categories**","1e2f2dd0":"Since the missing values are only in the CustomerID column and the description column, we could try to look at the InvoiceNo and see if maybe we can find the Customer ID","f0312ed5":"The highest value for the silhouette score is when there are 135 clusters. So we'll chose this value.","e06f7589":"This is very interesting since we can see that Netherlands is the 2nd country in value even though it has less invoices than countries like Germany or France for example and 10 times less customers. (95, 87 and 9 for Germany, France and Netherlands respectively)","1d90e279":"This cluster represents almost lost customers. The weird part about them is that there are some months when they didn't shop at all, it looks like a pattern. \n\n\nKey figures: \n* Min Basket Price: 20.83\n* Mean Basket Price: 33.77\n* Max Basket Price: 26.43\n\n* Quantity:            9.06\n* UnitPrice:           2.68\n* QuantityCanceled:     0.02\n* TotalPrice:           13.77\n* Frequency            3.065758\n* Recency         36.131902\n\nTOP 10 bought products :\n\n* PAPER CHAIN KIT 50'S CHRISTMAS:        267\n* BAKING SET 9 PIECE RETROSPOT:          263\n* WHITE HANGING HEART T-LIGHT HOLDER:    250\n* ASSORTED COLOUR BIRD ORNAMENT:         247\n* REX CASH+CARRY JUMBO SHOPPER:          223\n* HOT WATER BOTTLE KEEP CALM:            215\n* REGENCY CAKESTAND 3 TIER:              208\n* RABBIT NIGHT LIGHT:                    200\n* GARDENERS KNEELING PAD KEEP CALM:      194\n* SPOTTY BUNTING:                        193","6234fc09":"The minimum value for the unitprice is 0, let's see why is that.","f4b07d02":"What is very specific about this cluster is that there are no customers from UK, it's only foreign countries (Germany, France, Belgium, Italy and Finland).\nThis cluster is also heterogeneous in terms of RFM since the 2 most represented categories are Best customer and Lost cheap customer. The average basket is very low (33) comparing the ones above but I guess that the more customers we have in a cluster and the more the average customer will be represented which doesn't spent 500$ per transactions like the ones above. \n\nOctober and november have the most invoices which isn't surpring approaching Christmas. \n\nKey figures: \n* Min Basket Price: 20.58\n* Mean Basket Price: 33.55\n* Max Basket Price: 59.31\n\n* Quantity:            13.785663\n* UnitPrice:           2.884687\n* QuantityCanceled:     0.057975\n* TotalPrice:          23.749951\n* Frequency            7.865563\n* Recency         46.622343\n\nTOP 10 bought products :\n\n* ROUND SNACK BOXES SET OF4 WOODLAND:     233\n* REGENCY CAKESTAND 3 TIER:               161\n* PLASTERS IN TIN WOODLAND ANIMALS:       150\n* ROUND SNACK BOXES SET OF 4 FRUITS:      146\n* RED TOADSTOOL LED NIGHT LIGHT:          144\n* PLASTERS IN TIN CIRCUS PARADE:          141\n* SPACEBOY LUNCH BOX:                     137\n* RABBIT NIGHT LIGHT:                     120\n* PLASTERS IN TIN SPACEBOY:               120\n* WOODLAND CHARLOTTE BAG:                 111","e3cb4a7e":"![](http:\/\/)","8bbbd356":"By looking at these results, it appears that there is a counterpart to the canceled transaction in the database. Let's see if this is always the case.","32d647c3":"**2.4.5 Cluster 4**","24c2a064":"These customers seems to be good for datazone since they have good RFM scores, the 4 most represented categories are (111, 211, 322, 222). They seem to be normal customers. \n\n\nKey figures: \n* Min Basket Price: 10.86\n* Mean Basket Price: 30.60\n* Max Basket Price: 68.57\n\n* Quantity:            10.00\n* UnitPrice:           2.87\n* QuantityCanceled:     0.04\n* TotalPrice:           17.09\n* Frequency           11.134050\n* Recency         24.574626\n\nTOP 10 bought products :\n\n* WHITE HANGING HEART T-LIGHT HOLDER:    1345\n* JUMBO BAG RED RETROSPOT:               1079\n* REGENCY CAKESTAND 3 TIER:               960\n* ASSORTED COLOUR BIRD ORNAMENT:          926\n* PARTY BUNTING:                          924\n* LUNCH BAG RED RETROSPOT:                898\n* LUNCH BAG  BLACK SKULL:                753\n* SET OF 3 CAKE TINS PANTRY DESIGN:       725\n* LUNCH BAG CARS BLUE:                    679\n* LUNCH BAG PINK POLKADOT:                676","8baca77e":"**2.1 Intermediate dataset grouped by invoices**","4494d28d":"**2.3 Interpreting the clusters**","5e7bd42b":"This is very interesting since we can see various things here :\n* The stock code values aren't only numerical, there are speciales values like D which means Discount \n* The InvoiceNo aren't also only numerical since there is a C before the other numbers for every negative value in the quantity column, this could mean that the order was canceled.\n\nI'll analyse the InvoiceNo to find any patterns.","43767990":"**2.4.2 Cluster 1**","c9791dc3":"This part was inspired by Fabien Daniel's brilliant work in his Notebook on customer segmentation. ","2d0e9909":"**1.2.4 Stockcode**","9d45e4ac":"**1.2 Exploratory Analysis**","c5f6b880":"It seels that the customer can also cancel just a part of the transaction which is logical so we need to take this into account for later.","74c13a1f":"**PART 1 : Analyze and Clean the dataset**\n\n*     Cleaning the data\n*     Exploratory analysis\n*     Feature engineering\n   \n    \n**PART 2 : Creating customer categories**\n\n*     Intermediate dataset grouped by invoices\n*     Final dataset grouped by customers \n*     K-means clustering\n*     Interpreting the clusters\n    \n    \n **PART 3 : Testing models for predicition**\n \n*      Classic sklearn models\n*      Random Forest\n*      XGBoost\n \n","d8e89872":"**2.4.7 Cluster 6**","d14f35ee":"**2.4.6 Cluster 5**","68739611":"I am tempted to replace the null values by the most common one but it might be a special discount or something else so I'll leave it like that.\nHere let's remove the items that got completely canceled in order to harmonize the futur clusters and not have too much special values.","e7433e92":"More than 90% of the data is coming from UK ! ","8055a7eb":"I'll now create some time features, although I might not use them. It could be interesting to see if there are any paterns due to seasonality.","853bd6b7":"The cluster 7 contains 19 customers who are considered as best customers since they by the most, very frequently (75) and recently. The difference with cluster 2 is that they cluster 7's customers buy more frequently (75 vs 60) but have a lower monetary value (58000 vs 249000).\nThey have a mean basket price lower than the other clusters.\n\n* Min Basket Price : 10\n* Mean Basket Price : 138\n* Max Basket Price : 648\n* Quantity            23.257769\n* UnitPrice            2.615444\n* QuantityCanceled     0.109129\n* TotalPrice          34.916436\n* Frequency           121.570291\n* Recency           2.599109\n\nTOP 10 products bought :\n\n* REGENCY CAKESTAND 3 TIER:              136\n* JUMBO BAG RED RETROSPOT:               135\n* WHITE HANGING HEART T-LIGHT HOLDER:    121\n* CHILLI LIGHTS:                         102\n* PAPER BUNTING RETROSPOT:                97\n* LUNCH BAG  BLACK SKULL:                95\n* GUMBALL COAT RACK:                      93\n* LUNCH BAG RED RETROSPOT:                91\n* JUMBO BAG PINK POLKADOT:                84\n* LUNCH BAG CARS BLUE:                    81","4d6dc5e6":"Graphically the clusters are distinctive enough.\nLet's take a closer look at the clusters that contain few customers.","35682a64":"These are specific operations which doesn't characterize our customers so I'll just drop these transactions from our database","243cda7c":"There are no more missing values.\nI'll now check the dupplicate values and drop them if there's any. ","728bcd24":"The first customer has shopped a lot !","4c619501":"**1.3.1 Total Price**","ed4181a9":"Here we'll apply a score on each feature of RFM","b34836bb":"Importing the useful libraries","301a8ba4":"It appears that more than 16% of the transactions were canceled which is significant. Let's take a look at some rows where the transaction was canceled.","bee829b4":"==> Score's definition\n\n![](https:\/\/i.imgur.com\/YmItbbm.png?)","b5e7fc1d":"**2.3 Clustering customers**","cae3c815":"We want to have at least 5, 6 clusters so we won't take 2 or 3 clusters even though they have the highest silhouette scores, 8 clusters would fit the best here. ","37a60747":"** PART 1 : ANALYSE AND CLEAN THE CLUSTERS **","03eafff2":"**1.3.4 Product categories**","19c20a31":"The cluster 5 contains 3 customers which are very much alike. Indeed, they bought only once or twice a few items at a huge quantity. It might be some profesionnals which bought it at discount and will sell back the commodity. Even if they have a high monetary value they're not very interesting for Datazon and we could consider them as lost customers.\n* Min Basket Price : 3368\n* Mean Basket Price : 3697\n* Max Basket Price : 3533\n* Quantity            2213.777778\n* UnitPrice              2.386667\n* QuantityCanceled       0.000000\n* TotalPrice          3890.091111\n* Frequency              1.666667\n* Min_recency          210.888889","674c47a9":"The cluster 2 represents the best customers with a high recency which have around 60 visits, a lot of quantity bought on average, a high moneraty value and also a high frequency around 60 visits. These customers must be taken care.\n* Min Basket Price : 13\n* Mean Basket Price : 513\n* Max Basket Price : 3812\n* Quantity            117.083422\n* UnitPrice             2.830180\n* QuantityCanceled      0.069282\n* TotalPrice          258.683970\n* frequency            67.812655\n* min_recency           1.679039","c1f9a632":"Here we have an example of customers with a score of 111 which means that they are classified as our best customers.","1ad7d9e9":"**2.4.3 Cluster 2**","0a406aab":"**1.2.1 Countries**","f9f195d6":"**1.3.3 Time Features**","b8cec30e":"This cluster is full of lost customers. Indeed, as we can see in the month histogramm there are almost no invoices after july. We can see that there are in december but it's december of the past year. So this cluster is pretty bad for datazone, they don't want to have new customers in there. Furthermore they are cheap customers since the mean basket price is 28.91$ ... \n\n\n\nKey figures: \n* Min Basket Price: 24.20\n* Mean Basket Price: 28.91\n* Max Basket Price: 34.52\n\n* Quantity:            8.25\n* UnitPrice:           3.29\n* QuantityCanceled:     0.04\n* TotalPrice:          15.20\n* Frequency             2.606359\n* Recency         237.013433\n\nTOP 10 bought products :\n\n* WHITE HANGING HEART T-LIGHT HOLDER:    227\n* REGENCY CAKESTAND 3 TIER:              182\n* PARTY BUNTING:                         137\n* ASSORTED COLOUR BIRD ORNAMENT:         125\n* REX CASH+CARRY JUMBO SHOPPER:          103\n* SET OF 3 CAKE TINS PANTRY DESIGN:      100\n* NATURAL SLATE HEART CHALKBOARD:        100\n* JAM MAKING SET WITH JARS:               99\n* HEART OF WICKER SMALL:                  98\n* HEART OF WICKER LARGE:                  86","40c550ea":"Finally we'll set a score for each customer in the database. ","d66afc2c":"**I want to say many thanks to those of you who got this far, I know this is a very long notebook but I enjoyed working on this very rich dataset. it gave me many ideas for futur applications. Feel free to ask me below if you have any question or improvement to make **\n![](https:\/\/i.imgur.com\/gXh0Xwf.jpg)","f31e61e1":"**1.1 Cleaning the data**","98542e15":"**1.2.2 Quantity**"}}