{"cell_type":{"7552c636":"code","ad6e68c6":"code","139f8af1":"code","47b09892":"code","321ff9f6":"code","b389e18f":"code","eb227b5a":"code","b5f4aa2f":"code","69fa35d6":"code","9c75c0f3":"code","bc79063d":"code","9e2c05c3":"code","f0fa4032":"code","00074f2a":"code","2b97cf0c":"code","c953aef8":"code","d381d286":"code","8ef06ae3":"code","e330a96c":"code","f7f6d14d":"code","a96db1ca":"code","fbcfef35":"code","996ff5bc":"code","61a1c308":"code","2fb58a7c":"code","5eb16f38":"code","854d98eb":"code","89923eb9":"code","3628289b":"code","6f51a2b9":"code","3ec41fcf":"code","b302c4a0":"code","199f6666":"code","72e53c36":"code","e9eb411c":"code","1edba45d":"code","04f366f7":"code","cbc32950":"code","3346d807":"code","f2e351a5":"code","3a9055f1":"code","e6cd8f24":"code","c1ae0b70":"code","a17679ba":"markdown","17acb544":"markdown","23b4f783":"markdown","3d6a9845":"markdown","b1e4946e":"markdown","f4baa9ab":"markdown","d939b51b":"markdown","e572e651":"markdown","99c554d0":"markdown","df95db8d":"markdown","9961daaf":"markdown","8369bcb6":"markdown","554725b1":"markdown","a55027e1":"markdown","0666ca8e":"markdown","56747bcb":"markdown","19d5b091":"markdown","ae82d861":"markdown","e8473fd3":"markdown","86274c1d":"markdown","fb2acbfd":"markdown","2c06625e":"markdown","35364f3e":"markdown","ed91856c":"markdown","af00f773":"markdown","d55e27a7":"markdown","9fc892f9":"markdown","51b5ba71":"markdown"},"source":{"7552c636":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re  # regular expressions","ad6e68c6":"from pandas.api.types import is_string_dtype, is_numeric_dtype\n\n\ndef train_cats(df):\n    \"\"\"\n    Change any columns of strings in a pandas' dataframe to a column of\n    categorical values. This applies the changes inplace.\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\n\ndef fix_missing(df, col, name, na_dict):\n    \"\"\"\n    Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\n\ndef numericalize(df, col, name, max_n_cat):\n    \"\"\"\n    Changes the column col from a categorical type to it's integer codes.\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1\n\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\"\n    Takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe. For each column of df \n    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n    median value of the column.\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res","139f8af1":"train = pd.read_csv('..\/input\/train\/train.csv')\n#to_drop = ['Name', 'Description', 'RescuerID']   # optional\n#to_drop = ['Description', 'RescuerID']           # optional\nto_drop = ['PetID', 'Description', 'RescuerID']  # optional: categorical data with too many unique values\nto_drop.append('Name')                           # optional (we try to make the best out of XGB)\ntrain.drop(to_drop, axis=1, inplace=True)        # optional\ntrain.head()","47b09892":"train.shape","321ff9f6":"#train.isna().sum()\n#train.Description = train.Description.fillna('None')\n\n# replace nan values with a string:\n#train.Name = train.Name.fillna('NoNameSpecified')\n##train.Name = train.Name.fillna('Baby')\n\n# clean: remove all characters except for alphanumeric:\n# (NB: removed characters are replaced with a zero length string)\n##train.Name = train.Name.map(lambda name: re.sub('[^A-Za-z0-9]+', '', name))\n\n# replacing zero length strings that appeared after cleaning (above):\n##train.loc[train.Name == '', 'Name'] = 'NoNameLostIt'\n\n#train.isna().sum()","b389e18f":"# count unique Names:\n##train.Name.nunique()","eb227b5a":"##train.Name.value_counts()\n\n#train[train.Name.apply(lambda x: len(str(x))) < 2].Name.value_counts()\n#train[train.Name.str.startswith('No')].Name.value_counts()","b5f4aa2f":"train.shape","69fa35d6":"train.dtypes","9c75c0f3":"#plt.figure(figsize=(20,16))\ntrain.hist(figsize=(20,16));  # info about features","bc79063d":"# feature correlation\n#plt.figure(figsize=(20,16))\nplt.figure(figsize=(10,8))\ncorr = train.corr()\nsns.heatmap(corr, cmap='YlGnBu',  #annot=True,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values);","9e2c05c3":"# drop features that are badly correlated with others\n#feat_drop = ['Breed2', 'MaturitySize', 'VideoAmt', 'State']\n\n# drop features that are too well correlated with each other (gives better results in classification)\nfeat_drop = ['Dewormed', 'Vaccinated', 'Sterilized']\n\ntrain.drop(feat_drop, axis=1, inplace=True)\ntrain.shape","f0fa4032":"##train_cats(train)\n##train.Name.cat.set_categories(list(train.Name.cat.categories), ordered=True, inplace=True)\n##train.Name = train.Name.cat.codes\n\n#train.PetID.cat.set_categories(list(train.PetID.cat.categories), ordered=True, inplace=True)\n#train.PetID = train.PetID.cat.codes","00074f2a":"##train.head()","2b97cf0c":"##train.dtypes  # now the dataset is numeric entirely","c953aef8":"test = pd.read_csv('..\/input\/test\/test.csv')\ntest.drop(to_drop, axis=1, inplace=True)  # optional\ntest.head()","d381d286":"#test.isna().sum()\n#test.Description = test.Description.fillna('None')\n\n# replace nan values with a string:\n#test.Name = test.Name.fillna('NoNameSpecified')\n##test.Name = test.Name.fillna('Baby')\n\n# clean: remove all characters except for alphanumeric:\n# (NB: removed characters are replaced with a zero length string)\n##test.Name = test.Name.map(lambda name: re.sub('[^A-Za-z0-9]+', '', name))\n\n# replacing zero length strings that appeared after cleaning (above):\n##test.loc[test.Name == '', 'Name'] = 'NoNameLostIt'\n\n#test.isna().sum()","8ef06ae3":"##test.Name.value_counts()","e330a96c":"#test.isna().sum()\ntest.shape","f7f6d14d":"test.drop(feat_drop, axis=1, inplace=True)\ntest.shape","a96db1ca":"##train_cats(test)\n##test.Name.cat.set_categories(list(test.Name.cat.categories), ordered=True, inplace=True)\n##test.Name = test.Name.cat.codes\n\n#test.PetID.cat.set_categories(list(test.PetID.cat.categories), ordered=True, inplace=True)\n#test.PetID = test.PetID.cat.codes","fbcfef35":"test.head()","996ff5bc":"#breeds = pd.read_csv('..\/input\/breed_labels.csv')\n#breeds.head()","61a1c308":"#breeds.shape","2fb58a7c":"#colors = pd.read_csv('..\/input\/color_labels.csv')\n#colors","5eb16f38":"#states = pd.read_csv('..\/input\/state_labels.csv')\n#states","854d98eb":"from sklearn.model_selection import GridSearchCV, StratifiedKFold  #, train_test_split\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","89923eb9":"train.AdoptionSpeed.unique()  #hist(figsize=(6,4));","3628289b":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\n\ndef Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = Cmatrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","6f51a2b9":"#kappa_scorer = make_scorer(cohen_kappa_score(weights='quadratic'), greater_is_better=True)  # wrong\nkappa_scorer = make_scorer(quadratic_weighted_kappa, greater_is_better=True)","3ec41fcf":"X, y, nan = proc_df(train, 'AdoptionSpeed')\n\n#X = train.drop(['AdoptionSpeed'], axis=1)\n#y = train.AdoptionSpeed\n#X_train, X_test, y_train, y_test = train_test_split(X, y)","b302c4a0":"#tree = DecisionTreeClassifier()\n#tree_params = {\n#    'criterion' : ['gini', 'entropy'],\n#    'max_depth' : list(range(2, 11))\n#}\n#tree_grid = GridSearchCV(tree, tree_params, n_jobs=-1, cv=5, verbose=True, scoring=kappa_scorer)\n#tree_grid.fit(X, y);","199f6666":"#tree_grid.best_params_, tree_grid.best_score_","72e53c36":"#max_depth_values = range(2, 16)\n#max_features_values = range(2, 16)\n#n_estimators = [60, 70]  #[40, 50, 60]\n#forest_params = {'max_depth': max_depth_values,\n#                 'max_features': max_features_values,\n#                 'n_estimators': n_estimators}","e9eb411c":"#skf = StratifiedKFold(n_splits=5, shuffle=True)","1edba45d":"#%%time\n#forest = RandomForestClassifier()\n#rf_grid = GridSearchCV(forest, forest_params, n_jobs=-1, scoring=kappa_scorer, cv=skf)\n#rf_grid.fit(X, y);","04f366f7":"#rf_grid.best_params_, rf_grid.best_score_","cbc32950":"skf = StratifiedKFold(n_splits=5, shuffle=True)","3346d807":"max_depth_values = range(3, 9)\nmin_child_weight = range(1, 3)\nn_estimators = [80]\nxgb_params = {\n    'max_depth' : max_depth_values,\n    'min_child_weight' : min_child_weight,\n    'n_estimators' : n_estimators,\n}","f2e351a5":"%%time\nxgb = XGBClassifier()  #subsample=0.7)\nxgb_grid = GridSearchCV(xgb, xgb_params, n_jobs=-1, scoring=kappa_scorer, cv=skf)  #5)\nxgb_grid.fit(X, y);","3a9055f1":"xgb_grid.best_params_, xgb_grid.best_score_","e6cd8f24":"xgb_grid.best_estimator_.predict(test)","c1ae0b70":"sub = pd.read_csv('..\/input\/test\/sample_submission.csv')\n\n#pred = tree_grid.predict(test)\n#pred = rf_grid.predict(test)\npred = xgb_grid.predict(test)\n\nsub['AdoptionSpeed'] = pd.Series(pred)\nsub.to_csv('submission.csv', index=False)","a17679ba":"### BREEDS, COLORS AND STATES sets","17acb544":"Use fast.ai library to transform categorical data (Name) for classification:","23b4f783":"# Import fast.ai library\nhttps:\/\/course.fast.ai\n\nCode in the following cell is taken from: https:\/\/github.com\/fastai\/fastai\/blob\/master\/old\/fastai\/structured.py","3d6a9845":"Best parameters search","b1e4946e":"# Main information","f4baa9ab":"Same as the train set:\n* clean and fill nan for Name.","d939b51b":"Some more info: current shape, column types and feature correlation","e572e651":"Use fast.ai library to transform categorical data (Name) for classification:","99c554d0":"Drop features the same way as in the train set","df95db8d":"How are we predicting?\n\nSubmissions are scored based on the **quadratic weighted kappa**, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores which are expected\/known and the predicted scores.\n\nPrepare the scorer: ","9961daaf":"Let's see what names we have here:","8369bcb6":"Names in the test set:","554725b1":"We see above that mostly, features are slightly correlated with each other, but there are some exceptions.\n\nLet's work with that below:","a55027e1":"### TEST set","0666ca8e":"Make changes to the Name column:\n* clean and fill nan.","56747bcb":"## DecisionTreeClassifier","19d5b091":"# Data","ae82d861":"# Models","e8473fd3":"In this competition we predict the speed at which a pet is adopted, based on the pet\u2019s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted.\n\n**File descriptions**\n* train.csv - Tabular\/text data for the training set\n* test.csv - Tabular\/text data for the test set\n* sample_submission.csv - A sample submission file in the correct format\n* breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat. \n* color_labels.csv - Contains ColorName for each ColorID\n* state_labels.csv - Contains StateName for each StateID\n\n**Data Fields**\n* PetID - Unique hash ID of pet profile\n* AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n* Type - Type of animal (1 = Dog, 2 = Cat)\n* Name - Name of pet (Empty if not named)\n* Age - Age of pet when listed, in months\n* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n* Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n* Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n* Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n* Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n* MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n* FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n* Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n* Quantity - Number of pets represented in profile\n* Fee - Adoption fee (0 = Free)\n* State - State location in Malaysia (Refer to StateLabels dictionary)\n* RescuerID - Unique hash ID of rescuer\n* VideoAmt - Total uploaded videos for this pet\n* PhotoAmt - Total uploaded photos for this pet\n* Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n\n**AdoptionSpeed**\n\nContestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n* 0 - Pet was adopted on the same day as it was listed. \n* 1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n* 2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n* 3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n* 4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days). ","86274c1d":"* [The Pet-Finder Competition on Kaggle](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction)\n* [My notebook location](https:\/\/www.kaggle.com\/reginashay\/petfinder-xgb)","fb2acbfd":"## XGBClassifier","2c06625e":"## RandomForestClassifier","35364f3e":"### TRAIN set","ed91856c":"Split the data using fast.ai's proc_df function:","af00f773":"Parameters search","d55e27a7":"What are we predicting?","9fc892f9":"Result:\n* names are represented as numbers.","51b5ba71":"# Submission"}}