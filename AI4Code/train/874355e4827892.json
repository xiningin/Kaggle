{"cell_type":{"c87fda38":"code","5f2a6e53":"code","92b14a0c":"code","9ced9e13":"code","f67ba004":"code","38957d1e":"code","2c952690":"code","18f073b9":"code","192c091f":"code","68328c9f":"code","dc6752aa":"code","8e864943":"code","a00120eb":"code","15a1284f":"code","de17f3a3":"code","d03b537b":"code","c7cb7cbe":"code","41b91238":"code","46de475a":"code","a58720cf":"code","f67c5e9d":"code","ec4bf29b":"markdown","933340f7":"markdown","787e6f65":"markdown","9e962150":"markdown","e9a1d8b8":"markdown","ee4e7365":"markdown","61ab7f21":"markdown","4d2bc9f0":"markdown","4494423b":"markdown","190e232c":"markdown","4243c83e":"markdown","e657540f":"markdown","604c6ee8":"markdown","eef1bd6a":"markdown","299e72e6":"markdown","fce14fe7":"markdown","4832dc3b":"markdown","fe11d5c8":"markdown","3fb45ebf":"markdown","120a1f18":"markdown","dd46bad8":"markdown","1a6c9066":"markdown","88424ab5":"markdown","680b36b3":"markdown"},"source":{"c87fda38":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","5f2a6e53":"# Each image is of size 64x64 pixels,\n# There are 10 unique signs in the images 0 to 9\n\n\nx_1 = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nY_1 = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\n\nimg_size = 64\n\nplt.subplot(1, 2, 1)\nplt.imshow(x_1[260].reshape(img_size, img_size))\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(x_1[900].reshape(img_size, img_size))\nplt.axis('off')","92b14a0c":"# Sign zero is from 204 to 409 and size one is from 822 to 1027, we'll only select 205 samples from each sign\n\nX = np.concatenate((x_1[204:409], x_1[822:1027] ), axis=0)\n\nz = np.zeros(205)\no = np.ones(205)\n\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","9ced9e13":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","f67ba004":"print('X_train shape', X_train.shape)\nprint('X_test shape', X_test.shape)","38957d1e":"# Now we have three dimensional input arrays X_train and X_test, but we need to \n# flatten them to 2D so that we can feed it to outr Deep Learning model,\n\nX_train = X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])  # New shape (348, 4096)\nX_test = X_test.reshape(number_of_test, X_test.shape[1]*X_test.shape[2])     # New shape (62, 4096)\n\nprint('X_train shape', X_train.shape)\nprint('X_test shape', X_test.shape)","2c952690":"# Transposing the values\n\nx_train = X_train.T \nx_test = X_test.T\ny_train = Y_train.T\ny_test = Y_test.T\n\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","18f073b9":"# Let's initialize our weights and bias\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)  # Fills the array of shape (dimension, 1) with 0.01\n    b = 0\n    return w, b\n\nw, b = initialize_weights_and_bias(4096)","192c091f":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\nsigmoid(0)","68328c9f":"def forward_propogation(w, b, x_train, y_train):\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]                        # x_train.shape[1]  is for scaling\n    return cost","dc6752aa":"def forward_backward_propagation(w, b, x_train, y_train):\n    \n    # Forward Propogation (Weights and bias to cost)\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # Backward Propogation (from cost to updating weights and bias)\n    derivate_weight = (np.dot(x_train, ((y_head - y_train).T)))\/x_train.shape[1]\n    derivate_bias = np.sum(y_head - y_train)\/x_train.shape[1]\n    gradients = {'derivative_weight':derivate_weight, 'derivative_bias':derivate_bias}\n    return cost, gradients","8e864943":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","a00120eb":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","15a1284f":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","de17f3a3":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=42, max_iter=42)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","d03b537b":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","c7cb7cbe":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense","41b91238":"# Sequential Neural Network\nmodel = Sequential()\n\n# Input Layer\nmodel.add(Dense(units=8, activation='relu', input_dim=x_train.shape[1]))\n\n# Hidden Layer\nmodel.add(Dense(units=4, activation='relu'))\n\n# Output Layer\nmodel.add(Dense(units=1, activation='tanh'))","46de475a":"# Compiling the model (Configuring the model for training)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","a58720cf":"# Training\nmodel.fit(X_train, y_train, epochs=100, validation_split=0.2)","f67c5e9d":"# Predictions\npreds = model.predict(X_test)","ec4bf29b":"## What is Deep Learning?\n- Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\n\n<img src='https:\/\/www.royalcyber.com\/wp-content\/uploads\/2018\/01\/aws-machine-learning.png'>","933340f7":"* It means we fit the data. \n* In order to predict we have parameters. Let's predict.\n* In prediction step we have x_test as a input and while using it, we make forward prediction.","787e6f65":"## Logistic Regression\n- Logistic Regression is actually a very simple neural network\n- In order to understand how Logistic Regression works, we first need to understand computational graphs\n- Read more about <a href='https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc'>Logistic Regression<\/a> here.","9e962150":"# Deep Learning\n- In this kernel you will learn about Deep Learning.\n- We'll use the <a href='https:\/\/www.kaggle.com\/ardamavi\/sign-language-digits-dataset'>Sign Language Digits Dataset.<\/a>","e9a1d8b8":"## Loading the data","ee4e7365":"## Implementing a ANN with keras\nLets look at some parameters of keras library:\n* units: output dimensions of node\n* kernel_initializer: to initialize weights\n* activation: activation function, we use relu\n* input_dim: input dimension that is number of pixels in our images (4096 px)\n* optimizer: we use adam optimizer\n    * Adam is one of the most effective optimization algorithms for training neural networks.\n    * Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters\n* loss: Cost function is same. By the way the name of the cost function is cross-entropy cost function that we use previous parts.\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n* metrics: it is accuracy.\n* cross_val_score: use cross validation.\n* epochs: number of iteration","61ab7f21":"## Initializing parameters\n* As you know input is our images that has 4096 pixels(each image in x_train).\n* Each pixels have own weights.\n* The first step is multiplying each pixels with their own weights.\n* The question is that what is the initial value of weights?\n    * There are some techniques that I will explain at artificial neural network but for this time initial weights are 0.01.\n    * Okey, weights are 0.01 but what is the weight array shape? As you understand from computation graph of logistic regression, it is (4096,1)\n    * Also initial bias is 0.\n* Lets write some code. In order to use at coming topics like artificial neural network (ANN), I make definition(method).","4d2bc9f0":"## Forward Propagation\n* The all steps from pixels to cost is called forward propagation\n    * z = (w.T)x + b => in this equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n    * Then we put z into sigmoid function that returns y_head(probability).\n    * Then we calculate loss(error) function. \n    * Cost function is summation of all loss(error).\n    * Lets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)","4494423b":"## Why Deep Learning?\n- In traditional Machine learning techniques, most of the applied features need to be identified by an domain expert in order to reduce the complexity of the data and make patterns more visible to learning algorithms to work. The biggest advantage Deep Learning algorithms as discussed before are that they try to learn high-level features from data in an incremental manner. This eliminates the need of domain expertise and hard core feature extraction.\n\n\n<img src='https:\/\/cdn-images-1.medium.com\/max\/1600\/1*ZX05x1xYgaVoa4Vn2kKS9g.png'>","190e232c":"- Instead of implementing Logistic Regression ourselves, SkLearn has already done for us which works better than our implementation","4243c83e":"- Now let's put all of it together","e657540f":"<a id=\"7\"><\/a> <br>\n##  Optimization with Gradient Descent\n* Well, now we know what is our cost that is error.\n* Therefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction.\n* Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n* In order to decrease cost, we need to update weights and bias.\n* In other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.\n* Lets make an example:\n    * We have w = 5 and bias = 0 (so ignore bias for now). Then we make forward propagation and our cost function is 1.5.\n    * It looks like this. (red lines)\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/dAaYJH\/7.jpg\" alt=\"7\" border=\"0\"><\/a>\n    * As you can see from graph, we are not at minimum point of cost function. Therefore we need to go through minimum cost. Okey, lets update weight. ( the symbol := is updating)\n    * w := w - step. The question is what is this step? Step is slope1. Okay, it looks remarkable. In order to find minimum point, we can use slope1. Then lets say slope1 = 3 and update our weight. w := w - slope1 => w = 2.\n    * Now, our weight w is 2. As you remember, we need to find cost function with forward propagation again. \n    * Lets say according to forward propagation with w = 2, cost function is 0.4. Hmm, we are at right way because our cost function is decrease. We have new value for cost function that is cost = 0.4. Is that enough? Actually I do not know lets try one more step.\n    * Slope2 = 0.7 and w = 2. Lets update weight w : = w - step(slope2) => w = 1.3 that is new weight. So lets find new cost.\n    * Make one more forward propagation with w = 1.3 and our cost = 0.3. Okay, our cost even decreased, it looks like fine but is it enough or do we need to make one more step? The answer is again I do not know, lets try.\n    * Slope3 = 0.01 and w = 1.3. Updating weight w := w - step(slope3) => w = 1.29 ~ 1.3. So weight does not change because we find minimum point of cost function. \n    * Everything looks like good but how we find slope? If you remember from high school or university, in order to find slope of function(cost function) at given point(at given weight) we take derivative of function at given point. Also you can ask that okqy well we find slope but how does it know where to go. You can say that it can go more higher cost values instead of going minimum point. The asnwer is that slope(derivative) gives both step and direction of step. Therefore do not worry :)\n    * Update equation is this. It says that there is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with  \u03b1 learning rate. Then update weight. (In order to explain I ignore bias but these all steps will be applied for bias)\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hYTTJH\/8.jpg\" alt=\"8\" border=\"0\"><\/a>\n    * Now, I am sure you are asking what is learning rate that I mentioned never. It is very simple term that determines learning rate. Hovewer there is tradeoff between learning fast and never learning. For example you are at Paris(current cost) and want to go Madrid(minimum cost). If your speed(learning rate) is small, you can go Madrid very slowly and it takes too long time. On ther other hand, if your speed(learning rate) is big, you can go very fast but maybe you make crash and never go to Madrid. Therefore, we need to choose wisely our speed(learning rate).\n    * Learning rate is also called hyperparameter that need to be chosen and tuned. I will explain it more detailed in artificial neural network with other hyperparameters. For now just say learning rate is 1 for our previous example.\n  \n* I think now you understand the logic behind forward propagation(from weights and bias to cost) and backward propagation(from cost to weights and bias to update them). Also you learn gradient descent. Before implementing the code you need to learn one more thing that is how we take derivative of cost function according to weights and bias. It is not related with python or coding. It is pure mathematic. There are two option first one is to google how to take derivative of log loss function and second one is even to google what is derivative of log loss function :) I choose second one because I cannot explain math without talking :) \n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","604c6ee8":"## Flattening the data","eef1bd6a":"- Now let's look at the Computational Graph of Logistic Regression\n<a href=\"http:\/\/ibb.co\/c574qx\"><img src=\"http:\/\/preview.ibb.co\/cxP63H\/5.jpg\" alt=\"5\" border=\"0\"><\/a>\n* Parameters are weight and bias.\n    * Weights: coefficients of each pixels\n    * Bias: intercept\n    * z = (w.t)x + b  => z equals to (transpose of weights times input x) + bias \n    * In an other saying => z = b + px1*w1 + px2*w2 + ... + px4096*w4096\n    * y_head = sigmoid(z)\n    * Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n* Why we use sigmoid function?\n    * It gives probabilistic result\n    * It is derivative so we can use it in gradient descent algorithm (we will see as soon.)\n* Lets make example:\n    * Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification result is 1 with 90% probability.\n* Now lets start with from beginning and examine each component of computation graph more detailed.","299e72e6":"## Splitting the data","fce14fe7":"# References\n- Andrew NG's Course on Deep Learning: https:\/\/www.coursera.org\/specializations\/deep-learning\n- https:\/\/machinelearningmastery.com\/what-is-deep-learning\/\n- https:\/\/www.forbes.com\/sites\/bernardmarr\/2018\/10\/01\/what-is-deep-learning-ai-a-simple-guide-with-8-practical-examples\/#47ea7dbe8d4b\n- https:\/\/towardsdatascience.com\/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063?gi=40d36bacdbbd","4832dc3b":"<img src='https:\/\/image.ibb.co\/fOqCSc\/3.png'>","fe11d5c8":"- For the sake of simplicity, we'll only use ZERO and ONE sign images.\n- And we'll select 205 images from sign ZERO and 205 images from sign ONE.","3fb45ebf":"## Logistic Regression with Sklearn\n* In sklearn, there is a Logistic Regression model that eases implementing logistic regression.\n* The accuracies are different from what we find. Because logistic regression method use a lot of different feature that we do have not implemented.\n* Read mote about <a href='http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html'>sklearn<\/a> here.","120a1f18":"* Up to this point we learn \n    * Initializing parameters (implemented)\n    * Finding cost with forward propagation and cost function (implemented)\n    * Updating(learning) parameters (weight and bias). Now lets implement it.","dd46bad8":"* Lets learn what is loss(error) function\n* Lets take an example, I put one image as input then multiply it with their weights and add bias term so I find z. Then put z into sigmoid method so I find y_head. Up to this point we know what we did. Then e.g y_head became 0.9 that is bigger than 0.5 so our prediction is image is sign one image. Okey every thing looks like fine. But, is our prediction is correct and how do we check whether it is correct or not? The answer is with loss(error) function:\n    * Mathematical expression of log loss(error) function is that: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>\n    * It says that if you make wrong prediction, loss(error) becomes big.\n        * Example: our real image is sign one and its label is 1 (y = 1), then we make prediction y_head = 1. When we put y and y_head into loss(error) equation the result is 0. We make correct prediction therefore our loss is 0. However, if we make wrong prediction like y_head = 0, loss(error) is infinity.\n* After that, the cost function is summation of loss function. Each image creates loss function. Cost function is summation of loss functions that is created by each input image.\n* Lets implement forward propagation.\n","1a6c9066":"# Computational Graphs\n- are a way of visualizing Mathematical Expressions\n- If we have the expression $$c = \\sqrt{a^2 + b^2}$$\n- It's Computational Graph will look like,\n<a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hWn6Lx\/d.jpg\" alt=\"d\" border=\"0\"><\/a>","88424ab5":"# Artificial Neural Network (ANN)\n<img src='https:\/\/miro.medium.com\/max\/978\/0*0mia7BQKjUAuXeqZ.jpeg'><br>\n* It is also called deep neural network or deep learning.\n* **What is neural network:** It is basically taking logistic regression and repeating it at least 2 times.\n* In logistic regression, there are input and output layers. However, in neural network, there is at least one hidden layer between input and output layer.\n* **What is deep, in order to say \"deep\" how many layer do I need to have:** When I ask this question to my teacher, he said that \"\"Deep\" is a relative term; it of course refers to the \"depth\" of a network, meaning how many hidden layers it has. \"How deep is your swimming pool?\" could be 12 feet or it might be two feet; nevertheless, it still has a depth--it has the quality of \"deepness\". 32 years ago, I used two or three hidden layers. That was the limit for the specialized hardware of the day. Just a few years ago, 20 layers was considered pretty deep. In October, Andrew Ng mentioned 152 layers was (one of?) the biggest commercial networks he knew of. Last week, I talked to someone at a big, famous company who said he was using \"thousands\". So I prefer to just stick with \"How deep?\"\"\n* **Why it is called hidden:** Because hidden layer does not see inputs(training set)\n* For example you have input, one hidden and output layers. When someone ask you \"hey my friend how many layers do your neural network have?\" The answer is \"I have 2 layer neural network\". Because while computing layer number input layer is ignored. \n* Lets see 2 layer neural network: \n<a href=\"http:\/\/ibb.co\/eF315x\"><img src=\"http:\/\/preview.ibb.co\/dajVyH\/9.jpg\" alt=\"9\" border=\"0\"><\/a>\n* Step by step we will learn this image.\n    * As you can see there is one hidden layer between input and output layers. And this hidden layer has 3 nodes. If you're curious why I choose number of node 3, the answer is there is no reason, I only choose :). Number of node is hyperparameter like learning rate. Therefore we will see hyperparameters at the end of artificial neural network.\n    * Input and output layers do not change. They are same like logistic regression.\n    * In image, there is a tanh function that is unknown for you. It is a activation function like sigmoid function. Tanh activation function is better than sigmoid for hidden units bacause mean of its output is closer to zero so it centers the data better for the next layer. Also tanh activation function increase non linearity that cause our model learning better.\n    * As you can see with purple color there are two parts. Both parts are like logistic regression. The only difference is activation function, inputs and outputs.\n        * In logistic regression: input => output\n        * In 2 layer neural network: input => hidden layer => output. You can think that hidden layer is output of part 1 and input of part 2.\n* Thats all. We will follow the same path like logistic regression for 2 layer neural network.\n   \n    \n    ","680b36b3":"## Importing Libraries"}}