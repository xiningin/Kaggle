{"cell_type":{"ca297e97":"code","42e60095":"code","936a9f13":"code","dd8410bf":"code","314c6ed7":"code","9d551bbb":"code","d84b4267":"code","db818ef9":"code","1ed25a25":"code","45e519eb":"code","347e8791":"code","b74b67e0":"code","503f0fcf":"code","9342e099":"code","b2d92dc0":"code","9fa19620":"code","f57dd336":"code","8f590f5f":"code","7587e039":"code","49d75125":"code","333b6b3e":"code","7f4bba13":"code","f6afa612":"code","26c19259":"code","621513ac":"code","9757e6ca":"code","490d1dfc":"code","4f7a700f":"code","9ba56a7a":"code","a2861f0e":"code","a5c09c7b":"code","de9639a9":"code","73d4e381":"code","caaad61f":"code","59c71983":"code","0e0c019f":"code","bc258627":"code","33f25dfc":"code","69f7ff9c":"code","ba751ee0":"code","13678aec":"code","23298114":"code","5afe7c6b":"code","924bda6e":"code","e2bb645d":"code","604158c6":"code","6350ba04":"code","ce19731b":"code","fa3d761c":"code","8118bee6":"code","d777bcf3":"code","27ea7b59":"code","cca826e3":"code","590daa27":"code","63b3267d":"code","eaf4c161":"code","1ea5c283":"code","2adce81a":"code","9d7adfed":"code","4e871f4b":"code","32c7c636":"code","d10b5da8":"code","7238d931":"code","744243e4":"code","26fcefc8":"code","620d0923":"code","35f9d241":"code","5259363d":"code","d3b10b40":"code","31752190":"code","a2b76356":"markdown","c484f349":"markdown","66517bc1":"markdown","d510c888":"markdown","812cc93d":"markdown","b5d108c4":"markdown","fd4b6a60":"markdown","87d48ac2":"markdown","e7385754":"markdown","71e03668":"markdown","b01e3d04":"markdown","1172e5cd":"markdown","c7553313":"markdown","56619012":"markdown","2fae6d31":"markdown","aca52219":"markdown","a2b6fb3e":"markdown","b579e7bb":"markdown","cf0acc80":"markdown","a408fe58":"markdown","892dff34":"markdown","95ada7f4":"markdown","c612c24f":"markdown","cb620733":"markdown","5808362d":"markdown","52179e8a":"markdown","c2406092":"markdown","67fe13cf":"markdown","ea4958fa":"markdown","356818ae":"markdown","8869c345":"markdown","664a345c":"markdown","fed575e4":"markdown","6999502e":"markdown","cef6cd91":"markdown","ec8de7b8":"markdown","d0f71abc":"markdown","b11e6d8b":"markdown","881173b8":"markdown","6f649da8":"markdown","06488ec4":"markdown","35068606":"markdown","0d25739f":"markdown","bdc8bae5":"markdown","72986bf3":"markdown","490f4b7e":"markdown","e0127fd4":"markdown","e4508912":"markdown","1222b8c3":"markdown","812ae745":"markdown","8d727748":"markdown","3bb4bb83":"markdown","31a3f368":"markdown","0b4fb325":"markdown","71a738d0":"markdown","5793e46e":"markdown","7ed7b716":"markdown","8547db68":"markdown","02250312":"markdown","83090a58":"markdown","50e92fed":"markdown","892c923d":"markdown","27ebf07b":"markdown","86bc0ef7":"markdown","a0faa011":"markdown","f942a7c1":"markdown","8387c391":"markdown","f5c2dabc":"markdown","e08cc64e":"markdown","96c6640b":"markdown","480333af":"markdown","0f31d76c":"markdown","b5819842":"markdown"},"source":{"ca297e97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nimport re\nimport unidecode\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB as NB\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nimport math\nfrom ipywidgets import widgets\nfrom IPython.display import display\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","42e60095":"# We are loading all data sets here just in case we need them, but the most important one we are going to be working on is \"review_data\"\/\n# game_df = pd.read_csv(\"\/kaggle\/input\/boardgamegeek-reviews\/2019-05-02.csv\")\nreview_df = pd.read_csv(\"\/kaggle\/input\/boardgamegeek-reviews\/bgg-13m-reviews.csv\")\n# game_detail_df = pd.read_csv(\"\/kaggle\/input\/boardgamegeek-reviews\/games_detailed_info.csv\")","936a9f13":"# Let's take a look at how the data look like before the drop.\nreview_df.head()","dd8410bf":"# Let's look at the shape of the data before the drop.\nreview_df.shape","314c6ed7":"# Let's drop all the rows that has \"NaN\" in the \"comment\".\nreview_df.dropna(subset=[\"comment\"], inplace=True)","9d551bbb":"# Let's take a look at how the data look like after the drop.\nreview_df.head()","d84b4267":"# Let's look at the shape after the drop.\nreview_df.shape","db818ef9":"review_df[\"comment\"] = review_df[\"comment\"].str.lower()","1ed25a25":"# Creating a function for punctuation removal, similar to the one in my practice kaggle notebook.\ndef functuation_removal(comment):\n    return comment.translate(comment.maketrans('','', string.punctuation))\n\nreview_df[\"comment\"] = review_df[\"comment\"].apply(lambda comment:functuation_removal(comment))","45e519eb":"def miscellaneous_removal(comment):\n    pattern_html = re.compile(r'<.*?>')\n    pattern_url = re.compile(r'https?:\/\/\\S+\\www\\.\\S+')\n    comment = pattern_html.sub(r'', comment)\n    comment = pattern_url.sub(r'', comment)\n    comment = unidecode.unidecode(comment)\n    return comment\n\nreview_df[\"comment\"] = review_df[\"comment\"].apply(lambda comment: miscellaneous_removal(comment))","347e8791":"tokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\n\ndef stop_words_removal(comment):\n    return \" \".join([word for word in tokenizer.tokenize(comment) if word not in stop_words])\n\nreview_df[\"comment\"] = review_df[\"comment\"].apply(stop_words_removal)","b74b67e0":"words = set(nltk.corpus.words.words())\n\ndef remove_non_english_words(comment):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(comment) if w.lower() in words or not w.isalpha())\n\nreview_df[\"comment\"] = review_df[\"comment\"].apply(remove_non_english_words)","503f0fcf":"# This works as discretization.\nreview_df[\"rating\"] = review_df[\"rating\"].round(0).astype(int)","9342e099":"# Since we are using rating and comment columns to build the prediction model, therefore, we should remove the unnecesarry columns that we do not need.\nreview_df.drop(review_df.columns[0], axis=1, inplace=True) # This removes the original index column","b2d92dc0":"review_df.drop(review_df.columns[3], axis=1, inplace=True)","9fa19620":"review_df.drop(review_df.columns[0], axis=1, inplace=True)","f57dd336":"review_df.drop(review_df.columns[2], axis=1, inplace=True)","8f590f5f":"review_df.head()","7587e039":"review_df = review_df.sample(frac=1).reset_index(drop=True)","49d75125":"# Transforming DataFrame to Numpy\nreview_np = review_df.to_numpy()","333b6b3e":"# \"x\" is the comment column of the data and \"y\" is the rating column of the data\nx = review_np[:, 1:]\ny = review_np[:, :1]","7f4bba13":"# Now, let us use skilearn function call to split the data into 80% training and 20% testing\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","f6afa612":"# Let's look the shape of the data for \nprint(\"-------------------------------------------\")\nprint(\"The shape of the x_train: \", x_train.shape)\nprint(\"The shape of the y_train: \",y_train.shape)\nprint(\"-------------------------------------------\")\nprint(\"The shape of the x_test: \", x_test.shape)\nprint(\"The shape of the y_test: \",y_test.shape)\nprint(\"-------------------------------------------\")","26c19259":"# Let us try 30% of the original data size and add more later if it is needed. \n# Original shape of x_train, y_train is (2110204, 1) and original shape of x_test, y_test is (527552, 1)\n\nx_train_0, y_train_0 = x_train[:100000, :], y_train[:100000, :]\nx_test, y_test = x_test[:150000, :], y_test[:150000, :]","621513ac":"x_train_1, y_train_1 = x_train[100000:200000, :], y_train[100000:200000, :]\nx_train_2, y_train_2 = x_train[200000:300000, :], y_train[200000:300000, :]","9757e6ca":"# Let's look the shape of the data for \nprint(\"-------------------------------------------\")\nprint(\"The shape of the x_train: \", x_train.shape)\nprint(\"The shape of the y_train: \",y_train.shape)\nprint(\"-------------------------------------------\")\nprint(\"The shape of the x_test: \", x_test.shape)\nprint(\"The shape of the y_test: \",y_test.shape)\nprint(\"-------------------------------------------\")","490d1dfc":"x_train_0, x_test, y_train_0, y_test = pd.DataFrame(data=x_train_0, columns=[\"comment\"]), pd.DataFrame(data=x_test, columns=[\"comment\"]),\\\npd.DataFrame(data=y_train_0, columns=[\"rating\"]), pd.DataFrame(data=y_test, columns=[\"rating\"])","4f7a700f":"x_train_1, y_train_1 = pd.DataFrame(data=x_train_1, columns=[\"comment\"]), pd.DataFrame(data=y_train_1, columns=[\"rating\"])","9ba56a7a":"x_train_2, y_train_2 = pd.DataFrame(data=x_train_2, columns=[\"comment\"]), pd.DataFrame(data=y_train_2, columns=[\"rating\"])","a2861f0e":"# Let us look at the head of our data set now.\nreview_df.head(20)","a5c09c7b":"# Let us look at the shape of the data at this point.\nprint(review_df.shape)","de9639a9":"# The following is a rating count from 0 to 10 after making everything discrete.\nreview_df.groupby([\"rating\"]).count()","73d4e381":"# Histogram for the distribtuion of the rating across 1 to 10.\nplt = review_df['rating'].plot.hist()\nplt.set_xlabel(\"Rating from 0 to 10\")\nplt.set_ylabel(\"Counts\")","caaad61f":"vectorizer = feature_extraction.text.CountVectorizer()\ntrain_vector = vectorizer.fit_transform(x_train_0[\"comment\"])\ntest_vector = vectorizer.transform(x_test[\"comment\"])","59c71983":"# We are using MultinomialNB here to check accuracy.\nNaive_Bayes_Multinomial = NB()\ntarget_train_vector = np.asarray(y_train_0[\"rating\"], dtype=\"|S6\")\nNaive_Bayes_Multinomial.fit(train_vector, target_train_vector)\npred = Naive_Bayes_Multinomial.predict(test_vector)","0e0c019f":"# If we use the whole 2.7 million rows of data, then the accuracy result is around 30%; however if we use 30% of the whole data, we can still\n# achieve an accuraccy of 30% which is not too shaby at all. This is a good thing about Naive Byes With this, we can shorten the run time of all the experiments and also the restart\n# needs during the consturction of this notebook by substantial amount.\ntarget_test_vector = np.asarray(y_test[\"rating\"],dtype=\"|S6\")\nacc = accuracy_score(target_test_vector,pred)\nprint(\"The accuracy for the first Naive Bayes classifier trained over the FIRST 10% of the data is \",\"{:.2f}\".format(acc * 100), \"%\")","bc258627":"vectorizer_1 = feature_extraction.text.CountVectorizer()\ntrain_vector_1 = vectorizer_1.fit_transform(x_train_1[\"comment\"])\ntest_vector_1 = vectorizer_1.transform(x_test[\"comment\"])","33f25dfc":"Naive_Bayes_Multinomial_1 = NB()\ntarget_train_vector_1 = np.asarray(y_train_1[\"rating\"], dtype=\"|S6\")\nNaive_Bayes_Multinomial_1.fit(train_vector_1, target_train_vector_1)\npred_1 = Naive_Bayes_Multinomial_1.predict(test_vector_1)","69f7ff9c":"target_test_vector_1 = np.asarray(y_test[\"rating\"],dtype=\"|S6\")\nacc_1 = accuracy_score(target_test_vector_1,pred_1)\nprint(\"The accuracy for the first Naive Bayes classifier trained over the SECOND 10% of the data is \",\"{:.2f}\".format(acc_1 * 100), \"%\")","ba751ee0":"vectorizer_2 = feature_extraction.text.CountVectorizer()\ntrain_vector_2 = vectorizer_2.fit_transform(x_train_2[\"comment\"])\ntest_vector_2 = vectorizer_2.transform(x_test[\"comment\"])","13678aec":"Naive_Bayes_Multinomial_2 = NB()\ntarget_train_vector_2 = np.asarray(y_train_2[\"rating\"], dtype=\"|S6\")\nNaive_Bayes_Multinomial_2.fit(train_vector_2, target_train_vector_2)\npred_2 = Naive_Bayes_Multinomial_2.predict(test_vector_2)","23298114":"target_test_vector_2 = np.asarray(y_test[\"rating\"],dtype=\"|S6\")\nacc_2 = accuracy_score(target_test_vector_2,pred_2)\nprint(\"The accuracy for the first Naive Bayes classifier trained over the LAST 10% of the data is \",\"{:.2f}\".format(acc_2 * 100), \"%\")","5afe7c6b":"for i in range(10):\n    if pred[i] == pred_1[i] and pred_1[i] == pred_2[i]:\n        continue\n    else:\n        print(\"row \", i, \", classfier ONE prediction: \",str(pred[i])[2], \", classfier TWO prediction: \",str(pred_1[i])[2], \", classfier THREE prediction: \",str(pred_2[i])[2],\\\n              \", correct class is: \", y_test.iloc[i][0])","924bda6e":"ensemble_pred = []\nfor i in range(pred.shape[0]):\n    if pred[i] == pred_1[i] and pred_1[i] == pred_2[i]:\n        ensemble_pred.append(int(str(pred[i])[2]))\n    else:\n        temp_sum = int(str(pred[i])[2]) + int(str(pred_1[i])[2]) + int(str(pred_2[i])[2])\n        temp_pred = round(temp_sum\/3)\n        ensemble_pred.append(temp_pred)","e2bb645d":"# Brief debugg sector to make sure boolean checking goes through.\nprint(ensemble_pred[1] == y_test.iloc[1][0])","604158c6":"total = len(ensemble_pred)\ncount = 0\n\nfor i in range(total):\n    if ensemble_pred[i] == y_test.iloc[i][0]:\n        count += 1","6350ba04":"new_accuracy = count \/ total\nprint(new_accuracy)","ce19731b":"import matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [acc, acc_1, acc_2, new_accuracy]\nlabels = [\"NB1\", \"NB2\", \"NB3\", \"My Ensemble\"] \nplt.bar(x, y, tick_label = labels, width = 0.7, color = [\"blue\"]) \nplt.xlabel(\"Name of the classfiers\") \nplt.ylabel(\"Accuracy\") \nplt.title(\"Accuracy Chart without Considering Distance\") \nplt.show() ","fa3d761c":"total = y_test.shape[0]\ntotal_distance = 0\nfor i in range(total):\n    tmp_pred_rating = int(str(pred[i])[2])\n    tmp_true_rating = y_test.iloc[i][0]\n    if tmp_pred_rating != tmp_true_rating:\n        total_distance += abs(tmp_pred_rating - tmp_true_rating)\n    else:\n        continue\n\naverage_distance_0 = total_distance \/ total","8118bee6":"print(\"Average distance of the FIRST classfier prediction rating to the true rating: \", average_distance_0)","d777bcf3":"total = y_test.shape[0]\ntotal_distance = 0\nfor i in range(total):\n    tmp_pred_rating = int(str(pred_1[i])[2])\n    tmp_true_rating = y_test.iloc[i][0]\n    if tmp_pred_rating != tmp_true_rating:\n        total_distance += abs(tmp_pred_rating - tmp_true_rating)\n    else:\n        continue\n\naverage_distance_1 = total_distance \/ total","27ea7b59":"print(\"Average distance of the SECOND classfier prediction rating to the true rating: \", average_distance_1)","cca826e3":"total = y_test.shape[0]\ntotal_distance = 0\nfor i in range(total):\n    tmp_pred_rating = int(str(pred_2[i])[2])\n    tmp_true_rating = y_test.iloc[i][0]\n    if tmp_pred_rating != tmp_true_rating:\n        total_distance += abs(tmp_pred_rating - tmp_true_rating)\n    else:\n        continue\n\naverage_distance_2 = total_distance \/ total","590daa27":"print(\"Average distance of the THRID classfier prediction rating to the true rating: \", average_distance_2)","63b3267d":"total = y_test.shape[0]\ntotal_distance = 0\nfor i in range(total):\n    tmp_pred_rating = ensemble_pred[i]\n    tmp_true_rating = y_test.iloc[i][0]\n    if tmp_pred_rating != tmp_true_rating:\n        total_distance += abs(tmp_pred_rating - tmp_true_rating)\n    else:\n        continue\n\naverage_distance_ensemble = total_distance \/ total","eaf4c161":"print(\"Average distance of my ensemble classfier prediction rating to the true rating: \", average_distance_ensemble)","1ea5c283":"import matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [average_distance_0, average_distance_1, average_distance_2, average_distance_ensemble]\nlabels = [\"NB1\", \"NB2\", \"NB3\", \"My Ensemble\"] \nplt.bar(x, y, tick_label = labels, width = 0.7, color = [\"blue\"]) \nplt.xlabel(\"Name of the classfiers\") \nplt.ylabel(\"Average Distance\") \nplt.title(\"Average Distance to the True Rating\") \nplt.ylim(1.2, 1.3)\nplt.show() ","2adce81a":"def new_accuracy(pred, y_test):\n    total = pred.shape[0]\n    count = 0\n    for i in range(total):\n        temp_pred = int(str(pred[i])[2])\n        temp_y = y_test.iloc[i][0]\n        if temp_pred == temp_y or abs(temp_pred - temp_y) <= 1:\n            count += 1\n            \n    return count \/ total","9d7adfed":"new_acc = new_accuracy(pred, y_test)\nprint(new_acc)","4e871f4b":"new_acc_1 = new_accuracy(pred_1, y_test)\nprint(new_acc_1)","32c7c636":"new_acc_2 = new_accuracy(pred_2, y_test)\nprint(new_acc_2)","d10b5da8":"total = len(ensemble_pred)\ncount = 0\n\nfor i in range(total):\n    if ensemble_pred[i] == y_test.iloc[i][0] or abs(ensemble_pred[i] - y_test.iloc[i][0]) <= 1:\n        count += 1\n\nnew_accuracy = count \/ total\nprint(new_accuracy)","7238d931":"import matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [new_acc, new_acc_1, new_acc_2, new_accuracy]\nlabels = [\"NB1\", \"NB2\", \"NB3\", \"My Ensemble\"] \nplt.bar(x, y, tick_label = labels, width = 0.7, color = [\"blue\"]) \nplt.xlabel(\"Name of the classfiers\") \nplt.ylabel(\"Accuracy\") \nplt.title(\"Accuracy Chart with Average Distance Considered\")\nplt.ylim(0.64, 0.70)\nplt.show() ","744243e4":"def ensemble_voting_predict(comment):\n    input_np = np.array([[comment]])\n    input_df = pd.DataFrame(data=input_np, columns=[\"comment\"])\n    input_vector_0 = vectorizer.transform(input_df[\"comment\"])\n    input_vector_1 = vectorizer_1.transform(input_df[\"comment\"])\n    input_vector_2 = vectorizer_2.transform(input_df[\"comment\"])\n    pred_0 = Naive_Bayes_Multinomial.predict(input_vector_0)\n    pred_1 = Naive_Bayes_Multinomial_1.predict(input_vector_1)\n    pred_2 = Naive_Bayes_Multinomial_2.predict(input_vector_2)\n    \n#     print(pred_0, pred_1, pred_2)\n\n    rating_0 = int(str(pred_0[0])[2])\n    if int(str(pred_0[0])[2]) == 1 and str(pred_0[0])[3] == '0':\n        rating_0 = 10\n        \n    rating_1 = int(str(pred_1[0])[2])\n    if int(str(pred_1[0])[2]) == 1 and str(pred_1[0])[3] == '0':\n        rating_1 = 10\n        \n    rating_2 = int(str(pred_2[0])[2])\n    if int(str(pred_2[0])[2]) == 1 and str(pred_2[0])[3] == '0':\n        rating_2 = 10\n        \n    final_rating = (rating_0 + rating_1 + rating_2) \/ 3\n    \n    return round(final_rating)","26fcefc8":"class EnsembleVoting:\n    # x_train, y_train, x_test, and y_test are all\n    def __init__(self, x_train, y_train, x_test, y_test, use_percentage=1, num_of_classifier=3):\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.y_test = y_test\n        self.num_of_classifier = num_of_classifier\n        self.upper_index = math.ceil(x_train.shape[0] * use_percentage)\n        self.increment = math.ceil(self.upper_index \/ num_of_classifier)\n        self.x_df_container = []\n        self.y_df_container = []\n        self.vectorizers = self.instantiate_vectorizers()\n        self.naive_bayes = self.instantiate_naive_bayes()\n    \n    def fit_all_naive_bayes(self):\n        for i in range(self.num_of_classifier):\n            train_vector = self.vectorizers[i].fit_transform(self.x_df_container[i][\"comment\"])\n            target_train_vector = np.asarray(self.y_df_container[i][\"rating\"], dtype=\"|S6\")\n            self.naive_bayes[i].fit(train_vector, target_train_vector)\n    \n    def instantiate_naive_bayes(self):\n        naive_bayes = []\n        for i in range(self.num_of_classifier):\n            t_naive_bayes = NB()\n            naive_bayes.append(t_naive_bayes)\n        \n        return naive_bayes\n    \n    def instantiate_vectorizers(self):\n        vectorizers = []\n        for i in range(self.num_of_classifier):\n            t_vectorizer = feature_extraction.text.CountVectorizer()\n            vectorizers.append(t_vectorizer)\n        \n        return vectorizers\n        \n    def groom_data(self):\n        split_indices = [0]\n        current_index = 0\n        while(True):\n            current_index += self.increment\n            if current_index <= self.upper_index:\n                split_indices.append(current_index)\n            else:\n                split_indices.append(self.upper_index)\n                break;\n                \n        for i in range(len(split_indices)-1):\n            l = i\n            r = i + 1\n            t_x_train, t_y_train = self.x_train[split_indices[l]:split_indices[r], :], self.y_train[split_indices[l]:split_indices[r], :]\n            t_x_train, t_y_train = pd.DataFrame(data=t_x_train, columns=[\"comment\"]), pd.DataFrame(data=t_y_train, columns=[\"rating\"])\n            self.x_df_container.append(t_x_train)\n            self.y_df_container.append(t_y_train)\n    \n    def ensemble_voting_predict(self, comment):\n        input_np = np.array([[comment]])\n        input_df = pd.DataFrame(data=input_np, columns=[\"comment\"])\n        ratings = []\n        \n        for i in range(self.num_of_classifier):\n            t_input_vector = self.vectorizers[i].transform(input_df[\"comment\"])\n            t_pred = self.naive_bayes[i].predict(t_input_vector)\n            rating = int(str(t_pred[0])[2])\n            \n            if int(str(t_pred[0])[2]) == 1 and str(t_pred[0])[3] == '0':\n                rating = 10\n            \n            ratings.append(rating)\n            \n        return round(sum(ratings) \/ len(ratings))\n        \n    \n    # Here is my effort to attempt to convert this into webapp(extra credit), have to clear all data first, and then serialize it and then just use the trained\n    # classifiers for the app program as there is size requirement from PythonAnyWhere\n    def clear_data_set(self):\n        self.x_train, self.y_train, self.x_test, self.y_test = None, None, None, None\n        self.x_df_container, self.y_df_container = None, None\n    ","620d0923":"EV = EnsembleVoting(x_train, y_train, x_test, y_test, use_percentage=1)\nEV.groom_data()\nEV.fit_all_naive_bayes()","35f9d241":"# This step is not in Kaggle, this is a step in serialization for the webapp after everything is trained.\n# EV.clear_data_set()","5259363d":"# This step is not in Kaggle, this is a step in serialization for the webapp after everything is trained.\n# import pickle\n# with open('EV.txt', 'wb') as fh:\n#     pickle.dump(EV, fh)","d3b10b40":"print(\"Sample prediction of my EnsembleVoting classfier: \", EV.ensemble_voting_predict(\"this game is so wonderful, the best game i have ever played.\"))","31752190":"input_field = widgets.Text()\ndisplay(input_field)\noutput_field = widgets.Text(disabled=True)\ndisplay(output_field)\npredict_button = widgets.Button(description=\"Predict Rating\")\ndisplay(predict_button)\nclear_button = widgets.Button(description=\"Clear Entry\")\ndisplay(clear_button)\n\n\n\ndef handle_enter(sender):\n    rating = EV.ensemble_voting_predict(input_field.value)\n    output_field.value = str(rating)\n\ndef handle_clear(v):\n    input_field.value = ''\n    output_field = ''\n\ninput_field.on_submit(handle_enter)\npredict_button.on_click(handle_enter)\nclear_button.on_click(handle_clear)","a2b76356":"## We are going to split data into train set and test set and because of that the data should be shuffled to make the distribution more even.","c484f349":"# Intention of my Post\n## The intention of this post if to illustrate the term project of my Data Mining class. First and foremost, since preproecessing is a huge and necessary part of data mining, this post show step by step on how the preprocessing is done. Also at the end the preprocessing, a breif summary will be provided for the readers to remind what we have accomplished up on till that point. Secondly, a quick data analysis will be provided to see how the data look like after all the preprocessing work, this part gathers good intelligence of the data which we will be working with for the rest of the post. Thirdly, the prediction of the rating based on text comments is awefully similar to our third assignment which we are predicting movie reviews based on text reviews; however, the difference is that the ratings of the board games are continuous numbers from 0 to 10. I am planning on discretizing these ratings so that there are only 11 class labels (0 through 10). After that, I will use sklearn's Multinomial Naive Bayes classifiers to experiment with the data set and gather all the accuracies. My contribution would be implementing ensemble voting, analyze my mechanism, introducing different measurements. Fourthly, I will finalize a ensemble classfier to predict input reviews from outside. Lastly, readers should also read \"Important Note\" which reveals critical information throughout the post if reader wants to try running the notebook himself.","66517bc1":"# Preprocessing Step: Splitting data set into Train and Test","d510c888":"# Links\n## 1. AnywherePython Flask Webapp Deployment with Serialized Classfier: \nhttp:\/\/kaiteli14.pythonanywhere.com\n   \n## 2. Github Repo Link (Timestamps): \nhttps:\/\/github.com\/kaiteli14\/DataMiningTermProject\n\n## 3. Video Demo:\nhttps:\/\/www.youtube.com\/watch?v=z9A6rye8Elw&feature=youtu.be\n\n## 4. Kaggle Notebook Link:\nhttps:\/\/www.kaggle.com\/kaiteli14\/data-mining-term-project-board-game-geek-final\n","812cc93d":"### Breif Summary at this Point\nUnderstanding a data scientist's job is more than 60% of data preprocessing and also due to the fact that this data set is quite sizable, I have done the following preprocessing steps up on till this point:\n\n1. Remove rows that have no comment, the reason for this is because if the comment is blank, there is nothing we can work with as far as building a predictor is concerned; hence, a removal would be reasonable.\n2. Make all comment lower case because this way we won't classify same word into different categories.\n3. Punctuations have been removed.\n4. Urls, htmls, and accented characters are also removed to make the data even cleaner.\n5. Remove all the English stop words.\n6. Round all rating from 1 to 10, this is a better way to fit these rating to classfiers while maintain the correct meaning.\n7. Remove unnecessary columns.\n8. Shuffling all the rows of our data to make it even more natural before analysis and classification.\n9. Splitting data into 80% for traning and 20% for testing.\n","b5d108c4":"### Firstly, let's look at all the rows that our 3 classifiers have different prediciton and also the correct prediction from our y_test. The reason for this is because if all 3 classfiers have the same predictions, it does not matter if the prediction is right or wrong that any ensemble voting on 3 identical predictions will still be the same.","fd4b6a60":"# Preprocessing Step: Remove punctuations","87d48ac2":"# References\n\n1. https:\/\/kite.com\/python\/answers\/how-to-drop-empty-rows-from-a-pandas-dataframe-in-python\n2. http:\/\/www.kaiteli.io\/Kaite%20Li_1001645704_Practice.html (This is my own work for Kaggle practice)\n3. https:\/\/www.kaggle.com\/c\/spooky-author-identification\/discussion\/42289\n4. https:\/\/stackoverflow.com\/questions\/49153253\/pandas-rounding-when-converting-float-to-integer\n5. https:\/\/cmdlinetips.com\/2018\/02\/how-to-subset-pandas-dataframe-based-on-values-of-a-column\/\n6. https:\/\/stackoverflow.com\/questions\/45333530\/pandas-drop-columns\n7. https:\/\/towardsdatascience.com\/5-minute-guide-to-plotting-with-pandas-e8c0f40a1df4\n8. https:\/\/stackoverflow.com\/questions\/29576430\/shuffle-dataframe-rows\n9. https:\/\/www.geeksforgeeks.org\/getting-frequency-counts-of-a-columns-in-pandas-dataframe\/\n10. https:\/\/medium.com\/@contactsunny\/how-to-split-your-dataset-to-train-and-test-datasets-using-scikit-learn-e7cf6eb5e0d\n11. https:\/\/www.geeksforgeeks.org\/graph-plotting-in-python-set-1\/\n12. https:\/\/intellipaat.com\/community\/5638\/removing-non-english-words-from-text-using-python\n13. https:\/\/blog.dominodatalab.com\/interactive-dashboards-in-jupyter\/\n14. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove#Data-Cleaning\n15. https:\/\/stackoverflow.com\/questions\/11585793\/are-numpy-arrays-passed-by-reference\n16. https:\/\/flask.palletsprojects.com\/en\/0.12.x\/quickstart\/#rendering-templates\n17. https:\/\/www.youtube.com\/watch?v=IIi6e5oDZ68\n18. https:\/\/www.stefaanlippens.net\/python-pickling-and-dealing-with-attributeerror-module-object-has-no-attribute-thing.html\n","e7385754":"## Second Naive Bayes Classfier Analysis","71e03668":"# Preprocessing Step: Shuffling rows of the data set","b01e3d04":"## Preprocessing Step: Remove Non English Words","1172e5cd":"# Preprocessing Step: Transform comment into lower case","c7553313":"## Predicting my demonstration comments before doing the actual demonstration.","56619012":"## All the url, html, and acceted characters are just noise to our training, we should also clear them out here.","2fae6d31":"# Data Analysis: View the data after the preprocessing","aca52219":"## Since the rating ranges from 0 to 10 and I want to implment Ensemble Voting and Bagging with chosen classfier as Naive Bayes, it is necessary to discretize all the ratings by rounding all of them.","a2b6fb3e":"# Contribution - Building my own Ensemble Voting to Test","b579e7bb":"## As we can see above, all three Naive Bayes classfiers have very similar average distance to the true rating and my ensemble voting classfier yeild smaller average distance. My point here is, even if the accuracy of my classfier is exactly the same as the sklearn one, we should use my classfier because my average distance to the true rating is smaller by quite a few percent.","cf0acc80":"## I found that there are some spanish, russian comments and because the prediction would be in English therefore I do not want other languages show up in the training.","a408fe58":"# Preprocessing Step: Remove unnecessary columns","892dff34":"\n\n# Library Import","95ada7f4":"### Imporant Note: \n#### because each time you run the model, it shuffles the entire data set before splitting, you sometimes see ensemble voting method being the slightly higher or slightly lesser than the sklearn Multinomial Naive Bayes classfiers, however, this matters not, because next section we will talk about average distance to the true rating which emsemble voting method almost always closes the gap statistically speaking, it would be an ultra rare conincidence that my method here does NOT close the gap.","c612c24f":"## Instantiate my class, groom the input data, and fit all the naive bayes classifiers in my ensemblevoting class","cb620733":"## Because we will deal with multiple ratings for our prediciton. If a comment is empty, then there is nothing we can work with and I chose to drop those columns. The other good thing is, dropping the comment will size down the data set and making the run time better.","5808362d":"## Now we do the splitting.","52179e8a":"## Up on till now, we understand that the emsemble voting technique yields lesser average distance to the true rating and I would like to factor this idea into my accuracy calcuation because it would reflect a more truthfully the reality that ensemble voting is indeed worth to use on top of existing methods. Hence, I would consider the prediction accurate if it lands within 1 rating distance to the true rating which is completely reasonable. If you think about a comment stated \"this is a great game\", how much less accurate am I if I rate it as 9 instead of the true rating 10? Not much, negligible fair to say. Therefore, I will go on and recaculate my result in such way.","c2406092":"## Third Naive Bayes Classfier Analysis","67fe13cf":"## Here is the average distance of classifier 2 prediction rating to the true rating","ea4958fa":"# Contribution - Finish up building my Ensemble Classfier\n## This section is to design my ensemble voting classfier and make sure outside intput is predicted when enter through text box.","356818ae":"## The following 2 code blocks are for object serialziation which is used for building my web app. The idea is to use the \"pickle\" module to serialize my trained EnsembleVoting class object and then upload it to cloud and retreive it with my Flask application there with which I can predict comment's rating live.","8869c345":"## The following section is my finalized EnsembleVoting class which will take x_train, y_train, x_test, y_test in numpy format, a percentage of the portion of the entire data you want to use and the number of Naive Bayes classfiers as need with which my class will split the data accordingly to do the bagging before training them. This class is really useful because it generalizes my EnsembleVoting method, provides automatically data processing, and also opens the door for serialization which is necessary for Web Application deployment.","664a345c":"## Graph of the Average Distance","fed575e4":"## Now with the consideration of the distance into the measurement, almost every time, the ensemble voting classfier beats the sklearn one by a few percents which I consider my way ensemble implementation and understanding to be reasonable at this point in time.","6999502e":"## Although I have deployed my serilized classfier as a Flask Web Application vis AnywherePython which enable prediction online, I still wrote a small widget interface below in case the reader wants to run this notebook and test the predictions within the notebook. Please see the following.","cef6cd91":"# Preprocessing Step: Remove url, html, and acceted characters","ec8de7b8":"### The following is partition of roughly 5% or first 100,000 rows of data to fit our first Naive Bay Classifier.","d0f71abc":"## First Naive Bayes Classfier Analysis","b11e6d8b":"# Models - Naive Bayes Classifiers\n### The purpose of this section is to analyze the accuracies of sklearn Multinomial Naive Bayes classifier on our data in numerous ways. The reason I decided to use Naive Bayes classifiers instead of any other classifiers is that rating prediction based on text comment reviews are extremely similar to classification of positive\/negative comments based on text comments (I have made my own Naive Bayes model in Assignment 3 which you can see that on my homepage). Since I have preprocessed my data into discrete format (0-10), we can think about the prediction as classification of the data into 11 classes intead of just 2. What I want to achieve and contribute after this section is to use ensemble method to improve accuracy which I learned from this course. I think it is good to try applying in class theory into practice. Furthermore, the entire data has been partitioned into 3 sectors of 33% each for the classifiers. The reason the proportion is decided this way is because this porportion will have accuracy around 31.5% which is very close to the accuracy of 32.6% when we use the entire 2.7 million rows of data. If we cut the data into even smaller pieces,then the accuracy will significantly drops. The whole point is to increase accuracy and we have to very careful when it comes to splitting data for ensemble method.","881173b8":"## Writing an Auxiliary Function for our Calculation ","6f649da8":"## In the English language, we will have stop words, such as \"to\", \"of\", \"by\", \"for\", which are also pointless to the training, we should remove them from the comment to make the data even cleaner.","06488ec4":"# Contribution - New way of defining accuracy incorporating distance measurement.","35068606":"## We only need \"bbg-13-reviews.csv\" file for this project because we are only dealing with rating and comment. Therefore, the rest two .csv files are commented out.","0d25739f":"# Contribution - Demonstration Section","bdc8bae5":"## As we can see, our implementation in the above section works as expected. The point is that even if the accuracies is extremely similar after doing ensemble voting, it is still better to choose ensemble voting because we have not taken distance into the consideration. ","72986bf3":"# Contribution - Creating my own Ensemble Voting Class with Helpful Member Functions","490f4b7e":"### The following section is extracting all the predicitions from all of the 3 sklearn Multinomial Naive Bayes classifiers and average the result to correct error. From a statistical stand point, it is possible some correct answers will be wrongly corrected; however, the goal of the end result is to improve the exisitng classifier accuracy by implementing voting mechanism and bagging data when building the model. Also, by voting, we also close the distance between the prediction rating and the true rating even if we are not right on the rating. ","e0127fd4":"# Conclusion and Challenges\n## Truth be told, I have spent weeks of work on this project. From brainstorming ideas on how to do this project, incrementally refining the data, trying out different methods before deciding on implementing ensemble voting mechanism to better the sklearn Naive Bayes classifier to finally writing everything and also making sure I give the extra mile of effort by implementing extra credit portion of the requirement which is Flask Web App deployment with my own class of ensemble voting classifer to make the prediction live. I have learned a tremendous amount of knowledge through this project and I truely feel that I am very comfortable with Kaggle\/Jupyter Notebook. Aside from the time consuming aspect of this endeavor, I find deployment portion very hard and I will talk a little about how I overcame the difficulty. First of all, the data set is 1 GB and the upper limit of AnywherePython is only 100MB, therefore, there is no way we can preporcess and cut the size down to that amount. Even if we can, doing the training on each run of the Webapp would be utterly pointless. I then thought about the Python pickle module which I can use to serialize my EnsembleVoting classfier object after training and clearing all the data reference. After that, I spent hours on understanding how Flask works with Python and HTML. Then, I finished my Flask APP writing; however, when I was about to deployed it, there was an extremely subtle difference between serializing using Juypter Notebook and serializing using just Python file which makes the former not work with the deployment. To debug this portion, I spent almost 10 hours to do my research and I finally find that serilization only works if my class is defined in another file, otherwise, \"__main__\" prefix will be attached to my serialized byte code when I process the storage with the \"pickle\". Finally, I am able to overcome all these chanllenge and finish this term project. Thank you!","e4508912":"## New Accuracy for classfier 3","1222b8c3":"### Using ensemble voting over the 3 classifier predictions we have.","812ae745":"## Here is the average distance of classifier 1 prediction rating to the true rating","8d727748":"# Data Analysis: Rating distribution visualized through histogram","3bb4bb83":"## New Accuracy for my Ensemble Voting Clissfier","31a3f368":"## Similar to the idea above, punctuations are not good features that we should keep. They are for the most part meaningless to our training. Hence, we should just drop them all together.","0b4fb325":"# Models - Vectorization","71a738d0":"# Data Mining Term Project - Board Game Geek Rating Prediction\/Experimentations\n# Student: Kaite (Kurt) Li\n# ID: 100 164 5704\n","5793e46e":"### Imporant Note: Because the size of the data is to large, the Kaggle Notebook didn't finish for more than an hour; therefore I have to comment out the original \"x\" and \"y\" and take a subset of them. Please be aware of this if you want to run it yourself.","7ed7b716":"# Preprocessing Step: Drop rows that has no comment","8547db68":"## Lowering all the comments to a uniform case is a very important step because we would NOT want to see \"Word\" and \"word\" be considered as two different features in training.","02250312":"## Besides the Web App, the following is a Juypter\/Kaggle Notebook widget that we can use to test the predictor.","83090a58":"## Important Note: Please type in a legitmate comments with sizeable entry that mimics what a real review will look like for better result at the deployment website.","50e92fed":"## Here is the average distance of my own ensemble voting prediction rating to the true rating","892c923d":"## For those of you that are new to Jupyter Notebook, the following section is importing all the library modules. I find it more efficient to keep everything in the same place. Even if I need something new later on in the post, I would always go back here to do the import.","27ebf07b":"## Remove all columns other than \"rating\" and \"comment\"","86bc0ef7":"### Important Note: \n#### Please be patient when you run this notebook, because even after making this notebook as concise as I can, the running time is still very long as we are dealing with multiple classifiers using ensemble over a 2.7 million row data set. Thank you.","a0faa011":"# Data Analysis: Rating counts after decretization (0 through 10)","f942a7c1":"# Contribution - Analysis and Experiment and Undersanding of Distance.\n## As we all know by now, the accuracy is constructed in such a way that if the true rating is 9 and even if we predicted a 8 or 10, it is considered wrong. Now, let's assume we have two classifiers, if both of them have the same accuracy, but one's prediction is averagely much more closer to the true rating, do we acknowledge that this predictor to be better? Of course, we do. The purpose here is to realize that accuracy as defined here is not the only measurement of which classifier is better, we should deploy more metrics to analyze the ensemble method and this section devotes to that.","8387c391":"## Here is the average distance of classifier 3 prediction rating to the true rating","f5c2dabc":"## New Accuracy for classfier 1","e08cc64e":"## New Accuracy for classfier 2","96c6640b":"# Preprocessing Step: Remove stop words","480333af":"# Preprocessing Step: Loading data","0f31d76c":"# Preprocessing Step: Rounding all ratings","b5819842":"### The following are another two partition are also roughly 5% of the total data size."}}