{"cell_type":{"5a92d108":"code","95ccdb0c":"code","b437fe85":"code","c635947c":"code","9fe386ab":"code","ccd7e151":"code","51000808":"code","f7c219ad":"code","1998737f":"code","432ae492":"code","aea5fa08":"code","65a073a4":"code","ed64fe11":"code","0c2b9bc3":"code","eade4f85":"code","e7ccac99":"code","d803aca2":"code","0abcae3a":"code","e5084248":"code","817d0029":"code","259beb48":"code","8cabd186":"code","0d9eef5c":"code","2847893f":"code","6baf8859":"code","7a44be03":"code","73e49808":"code","74c3445e":"code","3745d70e":"code","501c9128":"code","f54d9a41":"code","79d0edaa":"code","a896fa73":"code","0ef9d234":"code","c1ff0fd3":"code","52b0a1d1":"code","2602eaeb":"code","22b0707c":"code","515060ca":"code","32af72b8":"code","1f928293":"code","cc2e68cc":"code","940be5e9":"code","961fa2dd":"code","b8291a57":"code","d6a5e0d5":"code","cdf77c4d":"code","1c79614c":"code","c785d784":"code","d0ca2021":"code","64e58f6a":"code","f259d588":"code","707764c8":"code","4387f6fd":"markdown","664eb52c":"markdown","5425c445":"markdown","54723136":"markdown","4bd5c31a":"markdown","ea9b98da":"markdown","6670d37b":"markdown","a514a7e4":"markdown","3d9db29d":"markdown","abf21f1c":"markdown","0ccd585d":"markdown","dbd5e838":"markdown","0cbf86ad":"markdown","0065236b":"markdown","08a280b4":"markdown","3c7099bc":"markdown","b548f1d9":"markdown","653fb066":"markdown"},"source":{"5a92d108":"!pip install gdown","95ccdb0c":"import pandas as pd\nimport numpy as np\nfrom pymongo import MongoClient\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\n\n%matplotlib inline\nimport matplotlib as mpl\nmpl.rcParams['agg.path.chunksize'] = 1000000\n\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport warnings\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, plot, iplot\n\nfrom IPython.display import Image, display\n\nimport os\nimport sys\nimport re\nimport json\nfrom datetime import datetime\nimport copy\n\nfrom geopy.geocoders import Nominatim\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import FastMarkerCluster\n\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport missingno as msno\nimport glob\n\n\nfrom datetime import datetime\nfrom enum import Enum\n\nprint(mpl.__version__)","b437fe85":"!mkdir -p mydata\n!mkdir -p myimage\n\n# https:\/\/drive.google.com\/file\/d\/1jENy39QaQSKebQy7MGpoV5eDOfCvEAKw\/view?usp=sharing\n# product_one_shot\n!gdown --id 1jENy39QaQSKebQy7MGpoV5eDOfCvEAKw\n\n# https:\/\/drive.google.com\/file\/d\/1RD_zGiKTHG8nTbmTrfLGRqVh8-3iHydP\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1aiwNF9yc2n7yakOhOA61R7LRbcxDHXfF\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1_bnJQmOl_jWSL1o35dp2S3RR6u1YAbHV\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1F4ZTyfv_yAcz7GQyOfxXD5cvQHyYWk9f\/view?usp=sharing\n#  engagement_dist_data.pickle for alex's regression\n!gdown --id 1F4ZTyfv_yAcz7GQyOfxXD5cvQHyYWk9f\n\n# https:\/\/drive.google.com\/file\/d\/1-qmRz9uzq4r0cPBiJHFNeEqd46g_gFuc\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1CFOTMtc0BhozFOOyvBQDw-KA7jSJEOi2\/view?usp=sharing\n# engagement_dist_product.pickle\n# !gdown --id 1CFOTMtc0BhozFOOyvBQDw-KA7jSJEOi2\n\n# https:\/\/drive.google.com\/file\/d\/1-mqPTn799Xd4HETSomMxkrAwoC-r8BxU\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1FpkqX3A-LaVxc4lPbp4_UdTAws2H8TID\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1nXxYMTLry1y-TRWl49xYXoWV4ZV-rrGU\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1uc0-BnjrNJluj5YTdYImRsLkSl3DA818\/view?usp=sharing\n# engagement_dist_data_label.pickle for re-charting \n!gdown --id 1uc0-BnjrNJluj5YTdYImRsLkSl3DA818\n\n# https:\/\/drive.google.com\/file\/d\/1M5Op1vVE3TXBtSOTkJRVCn6tIEWs5N91\/view?usp=sharing\n# policy_data.pickle\n!gdown --id 1M5Op1vVE3TXBtSOTkJRVCn6tIEWs5N91\n\n# https:\/\/drive.google.com\/file\/d\/1oaaPUtjK0qAjYx-Ir8gsU1TKnkCRWoNq\/view?usp=sharing\n# engagement_data_aggreated_district.pickle\n!gdown --id 1oaaPUtjK0qAjYx-Ir8gsU1TKnkCRWoNq\n\n# https:\/\/drive.google.com\/file\/d\/1-BoF4Vg16fbJevjDqUW8JP5cG7deAXCC\/view?usp=sharing\n# engagement_dist_model.pickle\n!gdown --id 1-BoF4Vg16fbJevjDqUW8JP5cG7deAXCC\n\n# https:\/\/drive.google.com\/file\/d\/1-0_MBGZVr6rbCKAe4fi9pyGPmM64Ak_N\/view?usp=sharing\n# district_combined.pickle\n!gdown --id 1-0_MBGZVr6rbCKAe4fi9pyGPmM64Ak_N\n\n# https:\/\/drive.google.com\/file\/d\/1-3C-cna4Drmxr4tsXE2GKLTyHKcg4zt8\/view?usp=sharing\n# engagement_dist_model_wd\n!gdown --id 1-3C-cna4Drmxr4tsXE2GKLTyHKcg4zt8\n\n!mv *.pickle mydata\/\n\n# https:\/\/drive.google.com\/file\/d\/1-g8yQqkxVOcOzFMP1_CTtHZZ1WNxI_XS\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-koX8muct46TCV2rAtlikvAdKZmXYxtH\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-bpoVQ9d3hYedxWRhJZKyLiVYoK4BpYi\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-bSLcwCmRD6sQJFO6VL4jSI1JoNcJQhG\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-F7TSa3QlcVZEtc5v3WQaSjllrKJDcK3\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1WsbfOe9KSO_icBrYecz4ep9sFGCRKGSD\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1050BU_q3SiHKcoOrL_zoZXWtYRPxBO-f\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/102WmHfsuxI3ZctnzdifIn1622H8cxFqT\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-Fb0mQwF5hKqSsUO_eH3fRzdJBOKmB43\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-ycmVrKWY3LBJCVdYbENZMp91OzKj5TP\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/14dIBrvE1pIRitEEY9fFyGCWt5OGG9Cbq\/view?usp=sharing\n# https:\/\/drive.google.com\/file\/d\/1-8Ylj7yCL1eUQWdFVEkI9d-XYaWt3hvt\/view?usp=sharing\n\n!gdown --id 1-g8yQqkxVOcOzFMP1_CTtHZZ1WNxI_XS\n!gdown --id 1-koX8muct46TCV2rAtlikvAdKZmXYxtH\n!gdown --id 1-bpoVQ9d3hYedxWRhJZKyLiVYoK4BpYi\n!gdown --id 1-bSLcwCmRD6sQJFO6VL4jSI1JoNcJQhG\n!gdown --id 1-F7TSa3QlcVZEtc5v3WQaSjllrKJDcK3\n!gdown --id 1WsbfOe9KSO_icBrYecz4ep9sFGCRKGSD\n!gdown --id 1050BU_q3SiHKcoOrL_zoZXWtYRPxBO-f\n!gdown --id 102WmHfsuxI3ZctnzdifIn1622H8cxFqT\n!gdown --id 1-Fb0mQwF5hKqSsUO_eH3fRzdJBOKmB43\n!gdown --id 1-ycmVrKWY3LBJCVdYbENZMp91OzKj5TP\n!gdown --id 14dIBrvE1pIRitEEY9fFyGCWt5OGG9Cbq\n!gdown --id 1-8Ylj7yCL1eUQWdFVEkI9d-XYaWt3hvt\n!gdown --id 1-VDreqemtDRl63t3hM5OA5BYb3gEU93u\n!gdown --id 1-kV7CUyETMnj1oFNMSCxfrnmjc2847mu\n!gdown --id 1-GodyfVJiYcCN1a82safEc72L-eK7erg\n!gdown --id 1-QWRlX8J1-qivsbSF8L_BtZLP_0r0-Cs\n!gdown --id 1-TkBJcymuSMJNaZjvdekAh_SKEW20CIz\n!gdown --id 1-heNw3YU0HFvCmAF4GCeleHN6GsUhZKH\n!gdown --id 1-VDreqemtDRl63t3hM5OA5BYb3gEU93u\n!mv *.png myimage\n\n","c635947c":"# from google.colab import drive\nif 'google.colab' in sys.modules:\n    drive.mount('\/content\/gdrive\/')","9fe386ab":"rebuildDataFile = False\nreplotChart = False","ccd7e151":"data_source_path = None\nmy_data_path = None\nmy_image_path = None\nassets_path = None\npolicy_path = None\n\n# kaggle notebook\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE',''):\n  # print(\"We are in Kaggle\")\n  data_source_path = r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning'\n  my_data_path = 'mydata'\n  my_image_path = 'myimage'\n  assets_path = 'assets'\n  policy_path = r'\/kaggle\/input\/covid19-us-state-policy-database\/COVID-19 US state policy database 8_13_2021.xlsx - State policy changes .csv'\n\nif 'google.colab' in sys.modules:\n  # print(\"We are in Google Colab\")\n  data_source_path = r'\/content\/gdrive\/MyDrive\/kaggle\/learnplatform-covid19-impact-on-digital-learning'\n  my_data_path = data_source_path + '\/output\/mydata'\n  my_image_path =  data_source_path + '\/output\/myimage'\n  assets_path =  data_source_path + '\/output\/assets'\n  policy_path=os.path.join(data_source_path, 'COVID-19 US state policy database 8_13_2021.xlsx - State policy changes .csv')\n\nengagement_charts_path = os.path.join(my_image_path, 'engagement_charts')\nengagement_data_path=os.path.join(data_source_path, 'engagement_data')\ndistricts_info_path=os.path.join(data_source_path, 'districts_info.csv')\nproducts_info_path=os.path.join(data_source_path, 'products_info.csv')","51000808":"if rebuildDataFile:\n  # uncomment this cell if you need reprocess the product data\n  products_df = pd.read_csv(products_info_path)\n  # Dummy encode product sectors\n  temp_sectors = products_df['Sector(s)'].str.get_dummies(sep=\"; \")\n  temp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\n  products_df = products_df.join(temp_sectors)\n  products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n  del temp_sectors\n\n  # Split product primary essential function\n  products_df['primary_function_main'] = products_df['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\n  products_df['primary_function_sub'] = products_df['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n  # Synchronize similar values\n  products_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n  products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\n  products_df.to_pickle(my_data_path + \"\/product_one_shots.pickle\")\n\n\n\nproducts_df = pd.read_pickle(my_data_path + \"\/product_one_shots.pickle\")\n# else:\n#     products_df = pd.read_csv(products_info_path)","f7c219ad":"class DayType(Enum):\n  WEEKEND = \"weekend\"\n  WEEKDAY = \"weekday\"\n  ALL = \"all\"\n\ndef get_days(start, end, day_type = None):\n  date_format = \"%Y-%m-%d\"\n  a = datetime.strptime(start, date_format)\n  b = datetime.strptime(end, date_format)\n  total = None\n  if (day_type == DayType.WEEKEND):\n    total = np.busday_count( a.date(), b.date(), weekmask = [0,0,0,0,0,1,1] )\n  elif (day_type == DayType.WEEKDAY):\n    total = np.busday_count( a.date(), b.date(), weekmask = [1,1,1,1,1,0,0] )\n  else:\n    delta = b - a\n    total = delta.days\n\n  return total\n\ndef get_days_date(start, end, day_type = None):\n  a = start\n  b = end\n  total = None\n  if (day_type == DayType.WEEKEND):\n    total = np.busday_count( a.date(), b.date(), weekmask = [0,0,0,0,0,1,1] )\n  elif (day_type == DayType.WEEKDAY):\n    total = np.busday_count( a.date(), b.date(), weekmask = [1,1,1,1,1,0,0] )\n  else:\n    delta = b - a\n    total = delta.days\n\n  return total","1998737f":"Pre_Start = '2020-01-01'\nPre_End= '2020-03-15'\ndef Pre_Days(day_type = None): \n  return get_days(Pre_Start, Pre_End, day_type )\n\nPan_Start = '2020-03-16'\nPan_End = '2020-06-01'\ndef Pan_Days(day_type = None): \n  return get_days(Pan_Start, Pan_End, day_type )\n\nPost_Start = '2020-09-01'\nPost_End = '2020-12-31'\ndef Post_Days(day_type = None): \n  return get_days(Post_Start, Post_End, day_type )\n\n#the color of Jan\ncolor3 =  \"#fea303\"\ncolor3_bar = \"#fea30340\"\n\n#the color of Mar\ncolor1 = \"#d62b83ff\"\ncolor1_bar =  \"#d62b83ee\"\n\n#the color of Sep\ncolor2 = '#059dc099'\ncolor2_bar = '#059dc030'","432ae492":"districts_df = pd.read_csv(districts_info_path)","aea5fa08":"if rebuildDataFile:  \n  all_files = glob.glob(engagement_data_path + \"\/*.csv\")\n  engagement_df = None\n  li = []\n  print(f\"number of products {len(all_files)}\")\n  for filename in all_files:\n      df = pd.read_csv(filename, index_col=None, header=0)\n      district_id = filename.split(\"\/\")[-1].split(\".\")[0]\n      df[\"district_id\"] = district_id\n      df['district_id'] = df['district_id'].astype(int)\n  #     df = pd.merge(df, districts_df[[\"district_id\", \"state\", \"locale\",\"pct_free\/reduced\",\"pct_black\/hispanic\"]], how='inner', on = 'district_id')\n      df = pd.merge(df, districts_df[[\"district_id\"]], how='inner', on = 'district_id')\n  #    df = pd.merge(df, products_df, how='right', right_on = 'LP ID', left_on='lp_id')\n      df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d')\n      # df = df.dropna()\n      li.append(df)\n      \n  engagement_df = pd.concat(li)\n  engagement_df = engagement_df.reset_index(drop=True)\n\n  # engagement_df.to_pickle(my_data_path + \"\/engagement_full_dist_data.pickle\")\n  # engagement_df.to_pickle(my_data_path + \"\/engagement_dist_product.pickle\")\n    \nif replotChart:\n  engagement_df.to_pickle(my_data_path + \"\/engagement_dist_data.pickle\")\n","65a073a4":"\neng_all_df = pd.read_pickle(my_data_path + \"\/engagement_dist_data.pickle\")\n# eng_all_df = pd.read_pickle(my_data_path + \"\/engagement_full_dist_data.pickle\")\n# eng_prod_df = pd.read_pickle(my_data_path + \"\/engagement_dist_product.pickle\")","ed64fe11":"def weekday_weekend_query(df, date_type):\n  if date_type == DayType.WEEKDAY:\n    return df['time'].dt.dayofweek.isin([0,1,2,3,4])\n  elif date_type == DayType.WEEKEND:\n    return df['time'].dt.dayofweek.isin([5,6])\n  else:\n    return df","0c2b9bc3":"vv_grey = '#BBBBBB'\nvv_orange = '#FFAE49'\nvv_blue = '#5FB6DB'\nvv_pink = '#CA2881'\nvv_purple = '#918DFA'\nvv_cyan = '#27BFC7'\n\nvv_palette = [vv_grey, vv_orange, vv_blue, vv_cyan, vv_purple, vv_pink ]","eade4f85":"if replotChart:\n  # https:\/\/datavizpyr.com\/visualizing-missing-data-with-seaborn-heatmap-and-displot\/\n  plt.figure(figsize=(10,6))\n  sns.heatmap(eng_all_df.isna().transpose(),\n              cmap=\"YlGnBu\",\n              cbar_kws={'label': 'Missing Data'})\n  plt.savefig( my_image_path + \"\/engagement_data_missing_data.png\", dpi=100)\nelse:\n  display(Image(filename=my_image_path + \"\/engagement_data_missing_data.png\")  )","e7ccac99":"if replotChart:\n  plt.figure(figsize=(10,6))\n  sns.heatmap(districts_df.isna().transpose(),\n              cmap=\"YlGnBu\",\n              cbar_kws={'label': 'Missing Data district df'})\n  plt.savefig(my_image_path + \"\/district_missing_data.png\", dpi=100)\nelse:\n  display(Image(filename=my_image_path + \"\/district_missing_data.png\")  )","d803aca2":"if replotChart:\n  plt.figure(figsize=(10,6))\n  sns.heatmap(products_df.isna().transpose(),\n              cmap=\"YlGnBu\",\n              cbar_kws={'label': 'Missing Data for product df'})\n  plt.savefig(my_image_path + \"\/product_missing_data.png\", dpi=100)\nelse:\n  display(Image(filename=my_image_path + \"\/product_missing_data.png\")  )","0abcae3a":"if replotChart:\n  plt.figure(figsize=(10,6))\n  sns.heatmap(eng_prod_df.isna().transpose(),\n              cmap=\"YlGnBu\",\n              cbar_kws={'label': 'Missing Data for engagement product join df'})\n  plt.savefig(my_image_path + \"\/engagement_product_missing_data.png\", dpi=100)\nelse:\n  display(Image(filename=my_image_path + \"\/engagement_product_missing_data.png\")  )","e5084248":"dis_chart_df = districts_df.copy(deep = True)\n\nif replotChart:\n  dis_chart_df[\"county_connections_ratio\"] = dis_chart_df['county_connections_ratio'].fillna('Missing')\n  ccr_type = CategoricalDtype(categories=[\"Missing\", \"[0.18, 1[\", \"[1, 2[\"], ordered=True)\n  dis_chart_df['county_connections_ratio'] = dis_chart_df['county_connections_ratio'].astype(ccr_type)\n\n  dis_chart_df[\"pct_free\/reduced\"] = dis_chart_df['pct_free\/reduced'].fillna('Missing')\n  pfrl_type = CategoricalDtype(categories=[\"Missing\", \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 0.6[\", \"[0.6, 0.8[\", \"[0.8, 1[\"], ordered=True)\n  dis_chart_df['pct_free\/reduced'] = dis_chart_df['pct_free\/reduced'].astype(pfrl_type)\n\n  dis_chart_df[\"pct_black\/hispanic\"] = dis_chart_df['pct_black\/hispanic'].fillna('Missing')\n  pbh_type = CategoricalDtype(categories=[\"Missing\", \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 0.6[\", \"[0.6, 0.8[\", \"[0.8, 1[\"], ordered=True)\n  dis_chart_df['pct_black\/hispanic'] = dis_chart_df['pct_black\/hispanic'].astype(pbh_type)\n\n  dis_chart_df[\"pp_total_raw\"] = dis_chart_df['pp_total_raw'].fillna('Missing')\n  ptr_type = CategoricalDtype(categories=[\"Missing\", '[4000, 6000[', '[6000, 8000[', '[8000, 10000[','[10000, 12000[', '[12000, 14000[', '[16000, 18000[', '[18000, 20000[','[20000, 22000[','[22000, 24000[','[32000, 34000[' ], ordered=True)\n  dis_chart_df['pp_total_raw'] = dis_chart_df['pp_total_raw'].astype(ptr_type)\n\n  fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(20, 12),gridspec_kw={'hspace': 0.6, 'wspace': 0.2})\n\n  fig.suptitle(\"Missing data and value count for each columns in district df\", fontsize=25)\n    \n  pp_total_raw_chart = sns.countplot(x=\"pp_total_raw\", data=dis_chart_df,  ax=ax[0,0], palette = vv_palette)\n  pp_total_raw_chart.set_xticklabels(pp_total_raw_chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n  pp_total_raw_chart.set_title(f'Value count by pp_total_raw', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  pct_black_hispanic_chart = sns.countplot(x=\"pct_black\/hispanic\", data=dis_chart_df,  ax=ax[0,1], palette = vv_palette)\n  pct_black_hispanic_chart.set_xticklabels(pct_black_hispanic_chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n  pct_black_hispanic_chart.set_title(f'Value count by pct_black\/hispanic', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  pct_free_reduced_chart = sns.countplot(x=\"pct_free\/reduced\", data=dis_chart_df,  ax=ax[1,0], palette = vv_palette)\n  pct_free_reduced_chart.set_xticklabels(pct_free_reduced_chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n  pct_free_reduced_chart.set_title(f'Value count by pct_free\/reduced', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  county_connections_ratio_chart = sns.countplot(x=\"county_connections_ratio\", data=dis_chart_df,  ax=ax[1,1], palette = vv_palette)\n  county_connections_ratio_chart.set_xticklabels(county_connections_ratio_chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n  county_connections_ratio_chart.set_title(f'Value count by county_connections_ratio', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n \n  fig.subplots_adjust(wspace=0.1)\n  plt.savefig(my_image_path + \"\/missing_data_value_count_district.png\", dpi=100)\n  fig.show()\nelse:\n  display(Image(filename=my_image_path + \"\/missing_data_value_count_district.png\")  )","817d0029":"def combine_pct_free_reduced (row):\n   if  row['pct_free\/reduced'] in ['[0.6, 0.8[', '[0.8, 1[']:\n      return '[0.6, 1['\n   return row['pct_free\/reduced']\n\ndef combine_pct_black_hispanic (row):\n   if  row['pct_black\/hispanic'] in ['[0.4, 0.6[', '[0.6, 0.8[', '[0.8, 1[']:\n      return '[0.4, 1['\n   return row['pct_black\/hispanic']\n\ndef combine_pp_total_raw (row):\n  if  row['pp_total_raw'] in ['[4000, 6000[', '[6000, 8000[', '[8000, 10000[']:\n      return '[4000, 10000['\n  elif row['pp_total_raw'] in ['[10000, 12000[', '[12000, 14000[', '[16000, 18000[', '[18000, 20000[']:\n    return '[10000, 20000['\n  elif row['pp_total_raw'] in ['[20000, 22000[','[22000, 24000[','[32000, 34000[']:\n    return '[20000, 34000['\n  \n  return row['pp_total_raw']\n\n\nif rebuildDataFile: \n  districts_df['pct_free\/reduced'] = districts_df.apply (lambda row: combine_pct_free_reduced(row), axis=1)\n  districts_df['pct_black\/hispanic'] = districts_df.apply (lambda row: combine_pct_black_hispanic(row), axis=1)\n  districts_df['pp_total_raw'] = districts_df.apply (lambda row: combine_pp_total_raw(row), axis=1)\n  \n  districts_df.to_pickle(my_data_path + \"\/district_combined.pickle\")\n  \ndistricts_df = pd.read_pickle(my_data_path + \"\/district_combined.pickle\")","259beb48":"\ndis_chart_df = districts_df.copy(deep = True)\n\nif replotChart:\n\n  sns.set_style('darkgrid')\n  fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(16, 12),gridspec_kw={'hspace': 0.4, 'wspace': 0.2})\n  fig.suptitle(\"Missing data and value count in district df after combined data\", fontsize=25)\n  \n  dis_chart_df[\"pp_total_raw\"] = dis_chart_df['pp_total_raw'].fillna('Missing')\n  dis_chart_df[\"pct_black\/hispanic\"] = dis_chart_df['pct_black\/hispanic'].fillna('Missing')\n  dis_chart_df[\"pct_free\/reduced\"] = dis_chart_df['pct_free\/reduced'].fillna('Missing')\n  dis_chart_df[\"county_connections_ratio\"] = dis_chart_df['county_connections_ratio'].fillna('Missing')\n\n  pfrl_type = CategoricalDtype(categories=[\"Missing\", \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 0.6[\", \"[0.6, 1[\"], ordered=True)\n  dis_chart_df['pct_free\/reduced'] = dis_chart_df['pct_free\/reduced'].astype(pfrl_type)\n\n  pbh_type = CategoricalDtype(categories=[\"Missing\", \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 1[\"], ordered=True)\n  dis_chart_df['pct_black\/hispanic'] = dis_chart_df['pct_black\/hispanic'].astype(pbh_type)\n\n  ptr_type = CategoricalDtype(categories=[\"Missing\", \"[4000, 10000[\", \"[10000, 20000[\", \"[20000, 34000[\"], ordered=True)\n  dis_chart_df['pp_total_raw'] = dis_chart_df['pp_total_raw'].astype(ptr_type)\n\n  ccr_type = CategoricalDtype(categories=[\"Missing\", \"[0.18, 1[\", \"[1, 2[\"], ordered=True)\n  dis_chart_df['county_connections_ratio'] = dis_chart_df['county_connections_ratio'].astype(ccr_type)\n  \n  pp_total_raw_chart = sns.countplot(x=\"pp_total_raw\", data=dis_chart_df,  ax=ax[0,0], palette = vv_palette)\n  pp_total_raw_chart.set_xticklabels(pp_total_raw_chart.get_xticklabels(), rotation=0, horizontalalignment='center')\n  pp_total_raw_chart.set_title(f'Value count by pp_total_raw', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  pct_black_hispanic_chart = sns.countplot(x=\"pct_black\/hispanic\", data=dis_chart_df,  ax=ax[0,1], palette = vv_palette)\n  pct_black_hispanic_chart.set_xticklabels(pct_black_hispanic_chart.get_xticklabels(), rotation=0, horizontalalignment='center', fontdict= { 'fontsize': 14, 'fontweight':'light'})\n  pct_black_hispanic_chart.set_title(f'Value count by pct_black\/hispanic', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  pct_free_reduced_chart = sns.countplot(x=\"pct_free\/reduced\", data=dis_chart_df,  ax=ax[1,0], palette = vv_palette)\n  pct_free_reduced_chart.set_xticklabels(pct_free_reduced_chart.get_xticklabels(), rotation=0, horizontalalignment='center', fontdict= { 'fontsize': 14, 'fontweight':'light'})\n  pct_free_reduced_chart.set_title(f'Value count by pct_free\/reduced', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n\n  county_connections_ratio_chart = sns.countplot(x=\"county_connections_ratio\", data=dis_chart_df,  ax=ax[1,1], palette = vv_palette)\n  county_connections_ratio_chart.set_xticklabels(county_connections_ratio_chart.get_xticklabels(), rotation=0, horizontalalignment='center', fontdict= { 'fontsize': 14, 'fontweight':'light'})\n  county_connections_ratio_chart.set_title(f'Value count by county_connections_ratio', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n      \n\n  fig.subplots_adjust(wspace=0.1)\n  plt.savefig(my_image_path + \"\/missing_data_value_count_district_combine.png\", dpi=100)\n  fig.show()\nelse:\n  display(Image(filename=my_image_path + \"\/missing_data_value_count_district_combine.png\")  )","8cabd186":"# uncomment this if we need re-process policy data\nif rebuildDataFile:  \n  policy_raw_df = pd.read_csv(policy_path)\n  policy_raw_df = policy_raw_df.iloc[4: , :]\n  policy_raw_df['CLSCHOOL'] = pd.to_datetime(policy_raw_df['CLSCHOOL'], format='%m\/%d\/%Y', errors='coerce')\n  policy_raw_df['STAYHOME'] = pd.to_datetime(policy_raw_df['STAYHOME'], format='%m\/%d\/%Y', errors='coerce')\n  policy_raw_df['END_STHM'] = pd.to_datetime(policy_raw_df['END_STHM'], format='%m\/%d\/%Y', errors='coerce')\n\n  policy_df = policy_raw_df[['STATE','CLSCHOOL', 'STAYHOME', 'END_STHM']]\n  policy_df.set_index('STATE', inplace=True, drop = True)\n  policy_df.to_pickle(my_data_path + \"\/policy_data.pickle\")\npolicy_df = pd.read_pickle(my_data_path + \"\/policy_data.pickle\")","0d9eef5c":"PRE_P = \"PreP\"\nIN_P = \"InP\"\nAFTER_P = \"AfterP\"\nSUMMER = \"Summer\"\n\n\ndef get_pandemic_text(pan):\n  if (pan == PRE_P):\n    return \"Before COVID\"\n  elif pan == IN_P:\n    return \"Beginning of COVID\"\n  elif pan == AFTER_P:\n    return \"New School Year\"\n  elif pan == SUMMER:\n    return \"Summer\"\n  else:\n    return None\n\nSTART_OF_YEAR = datetime.strptime('01\/01\/2020', '%m\/%d\/%Y')\nSECOND_HALF_START = datetime.strptime('09\/01\/2020', '%m\/%d\/%Y')\nFIRST_HALF_END = datetime.strptime('06\/15\/2020', '%m\/%d\/%Y')\nEND_OF_YEAR = datetime.strptime('12\/31\/2020', '%m\/%d\/%Y')\n\n\n\n\nresult = policy_df.to_json(orient=\"index\", date_format='epoch')\npolicy_json_raw = json.loads(result)\npolicy_json =  {}\n\nfor i, (j,k) in enumerate(policy_json_raw.items()):\n\n  val = {\n        \"CLSCHOOL\": datetime.fromtimestamp(k['CLSCHOOL']\/1000) if k['CLSCHOOL'] else None,\n        \"STAYHOME\": datetime.fromtimestamp(k['STAYHOME']\/1000) if k['STAYHOME'] else None,\n        \"END_STHM\": datetime.fromtimestamp(k['END_STHM']\/1000) if k['END_STHM'] else None,\n        }\n  val[PRE_P] = {}\n  val[PRE_P][DayType.ALL] = get_days_date(START_OF_YEAR, val['CLSCHOOL'], DayType.ALL ) if val['CLSCHOOL']  else None\n  val[PRE_P][DayType.WEEKDAY] = get_days_date(START_OF_YEAR, val['CLSCHOOL'], DayType.WEEKDAY ) if val['CLSCHOOL']  else None\n  val[PRE_P][DayType.WEEKEND] = get_days_date(START_OF_YEAR, val['CLSCHOOL'], DayType.WEEKEND ) if val['CLSCHOOL']  else None\n  val[IN_P] = {}\n  val[IN_P][DayType.ALL] = get_days_date(val['CLSCHOOL'], FIRST_HALF_END, DayType.ALL ) if val['CLSCHOOL']  else None\n  val[IN_P][DayType.WEEKDAY] = get_days_date(val['CLSCHOOL'], FIRST_HALF_END, DayType.WEEKDAY ) if val['CLSCHOOL']  else None\n  val[IN_P][DayType.WEEKEND] = get_days_date(val['CLSCHOOL'], FIRST_HALF_END, DayType.WEEKEND ) if val['CLSCHOOL']  else None\n  val[AFTER_P]={}\n  val[AFTER_P][DayType.ALL] = get_days_date(SECOND_HALF_START, END_OF_YEAR, DayType.ALL )\n  val[AFTER_P][DayType.WEEKDAY] = get_days_date(SECOND_HALF_START, END_OF_YEAR, DayType.WEEKDAY )\n  val[AFTER_P][DayType.WEEKEND] = get_days_date(SECOND_HALF_START, END_OF_YEAR, DayType.WEEKEND )\n  val[SUMMER] = {}\n  val[SUMMER][DayType.ALL] = get_days_date(FIRST_HALF_END, SECOND_HALF_START, DayType.ALL )\n  val[SUMMER][DayType.WEEKDAY] = get_days_date(FIRST_HALF_END, SECOND_HALF_START, DayType.WEEKDAY )\n  val[SUMMER][DayType.WEEKEND] = get_days_date(FIRST_HALF_END, SECOND_HALF_START, DayType.WEEKEND )\n  \n  policy_json[j] = val\n","2847893f":"# print(policy_json)","6baf8859":"def label_pandemic_date(state, date):\n  # print(f\"{state}. {type(state)}\")\n  if (state == 'nan' or state is np.nan or not state or not date or pd.isna(state)):\n    return None;\n\n  dt = date.to_pydatetime()\n  \n  if (dt >= SECOND_HALF_START ):\n    return AFTER_P;\n  elif (dt < SECOND_HALF_START and dt >= FIRST_HALF_END):\n    return SUMMER;\n  else:\n    if state == 'District Of Columbia':\n      state = 'District of Columbia'\n    state_policy = policy_json[state]\n\n    if (dt >= state_policy['CLSCHOOL']):\n      return IN_P;\n    else:\n      return PRE_P\n\nWEEKDAY = \"Weekday\"\nWEEKEND = \"Weekend\"\n\ndef label_weekday(date):\n  weekno = date.weekday()\n\n  if weekno < 5:\n      return WEEKDAY;\n  else:  # 5 Sat, 6 Sun\n      return WEEKEND\n","7a44be03":"if rebuildDataFile: \n  all_files = glob.glob(engagement_data_path + \"\/*.csv\")\n  engagement_df = None\n  li = []\n  print(f\"number of products {len(all_files)}\")\n\n  for filename in all_files:\n      df = pd.read_csv(filename, index_col=None, header=0)\n      district_id = filename.split(\"\/\")[-1].split(\".\")[0]\n      df[\"district_id\"] = district_id\n      df['district_id'] = df['district_id'].astype(int)\n      df = pd.merge(df, districts_df[[\"district_id\", \"state\"]], how='inner', on = 'district_id')\n      # df = pd.merge(df, products_df, how='right', right_on = 'LP ID', left_on='lp_id')\n      df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d')\n      df['weekday'] = df.apply(lambda x: label_weekday(x['time']),axis=1)\n      df['pandemic'] = df.apply(lambda x: label_pandemic_date(x['state'], x['time']), axis=1)\n      # df = df.dropna()\n      li.append(df)\n\n      \n  engagement_df = pd.concat(li)\n  engagement_df = engagement_df.reset_index(drop=True)\n\n  engagement_df.to_pickle(my_data_path + \"\/engagement_dist_data_label.pickle\")\n  # engagement_df.head(1000)\n    \neng_df = None\nif replotChart:\n    eng_df = pd.read_pickle(my_data_path + \"\/engagement_dist_data_label.pickle\")","73e49808":"\nif rebuildDataFile: \n  # uncomment this if we need reprocess enagement\n\n  agg_df = eng_df.groupby(['district_id','weekday','pandemic','state']).agg(\n    {\n      'pct_access': ['count', 'mean', 'sum', 'nunique'],\n      'engagement_index':['count', 'mean','sum']\n    }\n  ).reset_index()\n  agg_df.columns = [\"_\".join(a).strip(\"_\") for a in agg_df.columns.to_flat_index()]\n\n  agg_df.dropna(inplace=True)\n\n  agg_df = pd.merge(agg_df, districts_df, how='inner', on = 'district_id')\n  # df = pd.merge(df, products_df, how='right', right_on = 'LP ID', left_on='lp_id')\n\n  agg_df.to_pickle(my_data_path + \"\/engagement_data_aggreated_district.pickle\")\n\nagg_df = pd.read_pickle(my_data_path + \"\/engagement_data_aggreated_district.pickle\")","74c3445e":"def divided_by_period_days(df, col, state_col, pan_col, wd_col = None):\n  state = df[state_col]\n  pandemic = df[pan_col]\n  day_type = DayType.ALL\n  if wd_col: \n    day_type = DayType.WEEKDAY if df[wd_col] == 'Weekday' else DayType.WEEKEND\n\n  if ( (not isinstance(state, str)) or state == 'nan' or state is np.nan or not state or not pandemic or not df[col]):\n    print(f\"{df['district_id']}, {state_col} {pandemic} {df[col]}\")\n    return None;\n\n  if state == 'District Of Columbia':\n      state = 'District of Columbia'\n  # print(f\"{state}. {type(state)}\")\n  state_policy = policy_json[state]\n\n  num_day = state_policy[pandemic][day_type]\n  if (not num_day):\n    print(f\"{df['district_id']}, {state_col} {pandemic} {df[col]}  No num_day\")\n    return None;\n  \n  # print(f\"{df['district_id']}, {state_col} {pandemic} {df[col]} {float(df[col]) \/ float(num_day)}\")\n  return float(df[col]) \/ float(num_day)\n","3745d70e":"def rename_category_value(df, col):\n  prefix = \"\"\n  if (col == 'pp_total_raw'):\n    prefix = 'ptr'\n  elif col == 'pct_black\/hispanic':\n    prefix = 'pbh'\n  elif col == 'pct_free\/reduced':\n    prefix = 'pfr'\n  elif col == 'county_connections_ratio':\n    prefix = 'ccr'\n  valstr = df[col]\n  if ( (not isinstance(valstr, str)) or valstr == 'nan' or valstr is np.nan or not valstr ):\n    return None;\n  var = prefix + \"_\" + valstr.strip(\"[]0\").replace(\"0\",\"\").replace(\".\",\"\").replace(\",\",\"_\").strip(\"_\")\n  return var","501c9128":"\nif rebuildDataFile: \n\n  agg_df['pct_access_count_avg'] = agg_df.apply(lambda x: divided_by_period_days(x,'pct_access_count', 'state_x', 'pandemic', None ) ,axis=1)\n  agg_df['pct_access_sum_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'pct_access_sum', 'state_x', 'pandemic', None ) ,axis=1)\n  agg_df['engagement_index_count_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'engagement_index_count', 'state_x', 'pandemic', None ) ,axis=1)\n  agg_df['engagement_index_sum_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'engagement_index_sum', 'state_x', 'pandemic', None ) ,axis=1)\n\n  # agg_df['pp_total_raw'] =  agg_df.apply(lambda x: rename_category_value(x,'pp_total_raw') ,axis=1)\n  # agg_df['pct_black\/hispanic'] =  agg_df.apply(lambda x: rename_category_value(x,'pct_black\/hispanic') ,axis=1)\n  # agg_df['pct_free\/reduced'] =  agg_df.apply(lambda x: rename_category_value(x,'pct_free\/reduced') ,axis=1)\n  # agg_df['county_connections_ratio'] =  agg_df.apply(lambda x: rename_category_value(x,'county_connections_ratio') ,axis=1)\n  agg_df.drop(columns=['state_y'], inplace= True)\n  agg_df.rename(columns={'state_x': 'state'}, inplace=True)\n  # agg_df.dropna(inplace=True)\n  agg_df.to_pickle(my_data_path + \"\/engagement_dist_model.pickle\")\n\n# model_df = pd.read_pickle(my_data_path + \"\/engagement_data_aggreated_district.pickle\")\nmodel_df = pd.read_pickle(my_data_path + \"\/engagement_dist_model.pickle\")\n","f54d9a41":"\n# model_df = model_df.loc[model_df['pandemic'] != 'Summer']\npandemic_type = CategoricalDtype(categories=[\"PreP\", \"InP\",  \"AfterP\"], ordered=True) #\"Summer\",\nmodel_df['pandemic'] = model_df['pandemic'].astype(pandemic_type)\n\npfrl_type = CategoricalDtype(categories=[ \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 0.6[\", \"[0.6, 1[\"], ordered=True)\nmodel_df['pct_free\/reduced'] = model_df['pct_free\/reduced'].astype(pfrl_type)\npbh_type = CategoricalDtype(categories=[ \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 1[\"], ordered=True)\nmodel_df['pct_black\/hispanic'] = model_df['pct_black\/hispanic'].astype(pbh_type)\nptr_type = CategoricalDtype(categories=[\"[4000, 10000[\", \"[10000, 20000[\", \"[20000, 34000[\"], ordered=True)\nmodel_df['pp_total_raw'] = model_df['pp_total_raw'].astype(ptr_type)\nccr_type = CategoricalDtype(categories=[ \"0.18, 1[\", \"[1, 2[\"], ordered=True)\nmodel_df['county_connections_ratio'] = model_df['county_connections_ratio'].astype(ccr_type)","79d0edaa":"# https:\/\/towardsdatascience.com\/how-to-make-bar-and-hbar-charts-with-labels-using-matplotlib-b701ce70ba9c\ndef add_bat_label(bars, ax, labels):\n  for idx, bar in enumerate(bars):\n    height = bar.get_height()\n    label_x_pos = bar.get_x() + bar.get_width() \/ 2\n    ax.text(label_x_pos, height, s=f\"{int(round(labels[idx]))}%\", ha='center', va='bottom')","a896fa73":"def autolabel(ax, rects, data_labels):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    first = len(data_labels) == 0\n    currentLbl = []\n    # for rect in rects:\n    for idx, rect in enumerate(rects):\n        height = rect.get_height()\n        currentLbl.append(height)\n        if(not first):\n          pre = data_labels[idx]\n          cur = height\n          ax.annotate(f\"{(cur-pre)*100\/pre:.0f}%\",\n                      xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                      xytext=(0, 3),  # 3 points vertical offset\n                      textcoords=\"offset points\",\n                      ha='center', va='bottom')\n    return currentLbl","0ef9d234":"def create_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by = \"pandemic\"):\n  total_charts = len(display_cols) * len(sub_chart_by)\n  charts_row = total_charts \/\/ 2 if (total_charts % 2 == 0) else 1 + total_charts \/\/ 2\n\n  #sns.set_style('whitegrid')\n  sns.set_style('darkgrid')\n  chart_col = 2\n  if (total_charts == 1):\n    chart_col = 1\n  # print(f\"charts_row {charts_row} total_charts {total_charts} chart_col {chart_col}\")\n  fig, axx = plt.subplots(nrows = charts_row, ncols = chart_col, figsize=(chart_col * 10, charts_row * 6 ), constrained_layout=False)\n  plt.margins(y=0.9 ) \n  #qualitative_colors = [vv_grey, vv_orange, vv_blue]\n  # sns.palplot(diverging_colors)\n\n  chart_idx = 0\n  for dis_col, dis_lbl in zip(display_cols,display_cols_label):\n    # print(f\"{dis_col}. {dis_lbl} ....\")\n    for sub_chart, sub_chart_lbl in zip(sub_chart_by,sub_chart_by_label):\n      # chat_x = chart_idx \/\/ 2\n      # chat_y = chart_idx % 2\n      # print(f\"chat_x {chat_x}. chat_y {chat_y} ....\")\n      # print(f\"chart_idx {chart_idx}\")\n      if sub_chart == \"state\":\n        chart_df = model_df.loc[model_df['state'].isin(state_list)]\n      else:\n        chart_df = model_df\n      \n      ax = sns.barplot(x=sub_chart, y=dis_col, hue=split_by, data=chart_df,  ax=axx[chart_idx], palette = vv_palette,  edgecolor=\".9\", ci=None  ) #, \n      ax.set_xticklabels(ax.get_xticklabels(), rotation=00, horizontalalignment='center', fontdict= { 'fontsize': 12, 'fontweight':'light'})\n      # ax.set_yticklabels(chart_df[dis_col],fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n      # ax.legend(title = split_by, bbox_to_anchor = (1, chat_y))\n      ax.legend([],[],frameon=False)\n      # ax.set(xlabel=f\"{sub_chart_lbl}\", ylabel=f\"{dis_lbl}\" )\n      ax.set_xlabel(f\"{sub_chart_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'light'})\n      ax.set_ylabel(f\"{dis_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'normal'})\n      ax.set_title(f'{dis_lbl} by {sub_chart_lbl}', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n      ax.margins(y=0.2) \n      count = 0\n      prePandemic = None\n      data_labels = []\n      for container in ax.containers:\n        # use autolabel to add text on bar. This applies to older version\n        new_labels = autolabel(ax,container.patches, data_labels)\n        #we only use the first label as base.\n        if (len(data_labels) == 0):\n          data_labels = new_labels\n        \n        # end of autolabel\n\n        # -- bar_label is only available after matlibplot 3.4.4 .\n        # data_labels =  [\" \"] * len(container.datavalues) \n        # print(container.datavalues)\n        # if (count == 0):\n        #   prePandemic = copy.deepcopy(container.datavalues)\n        # else:\n        #   data_labels = [f\"{(cur-pre)*100\/pre:.0f}%\"  for index, (pre, cur) in enumerate(zip(prePandemic, container.datavalues))]\n        # ax.bar_label(container, padding=3,  labels = data_labels )     \n        # -- bar_label\n        count += 1\n      \n      ax.set_ymargin(0.2)\n      chart_idx += 1\n\n\n  handles, labels = axx[-1].get_legend_handles_labels()\n  if (total_charts % 2 != 0 and total_charts > 1):\n    handles, labels = axx[-2].get_legend_handles_labels()\n\n  legend_lables = [get_pandemic_text(x) for x in labels]\n  if (total_charts % 2 != 0 and total_charts > 1):\n    fig.delaxes(axx[-1])\n  plt.tight_layout()\n\n  fig.suptitle(title, fontsize=25, y=1.1)\n  fig.legend(handles, legend_lables, ncol=3,   bbox_to_anchor=(0.73, 1.1*0.915))\n  fig.subplots_adjust(top=0.90, wspace = 0.2, hspace = 1.4)\n\n  plt.savefig(my_image_path + f\"\/{filename}.png\", dpi=300, bbox_inches='tight')\n\n  fig.show()","c1ff0fd3":"def create_single_chart(model_df, dis_col, dis_lbl, sub_chart, sub_chart_lbl, state_list, title, filename, split_by = \"pandemic\"):\n\n  #sns.set_style('whitegrid')\n  sns.set_style('darkgrid')\n\n  if sub_chart == \"state\":\n    chart_df = model_df.loc[model_df['state'].isin(state_list)]\n  else:\n    chart_df = model_df\n\n  fig, ax = plt.subplots( figsize=(10, 6 ), constrained_layout=False)\n  plt.margins(y=0.9 ) \n  #qualitative_colors = [vv_grey, vv_orange, vv_blue]\n  # sns.palplot(diverging_colors)\n\n  sns.barplot(x=sub_chart, y=dis_col, hue=split_by, data=chart_df, palette = vv_palette,  edgecolor=\".9\", ci=None  ) #, \n  ax.set_xticklabels(ax.get_xticklabels(), rotation=00, horizontalalignment='center', fontdict= { 'fontsize': 12, 'fontweight':'light'})\n  # ax.set_yticklabels(chart_df[dis_col],fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n  # ax.legend(title = split_by, bbox_to_anchor = (1, chat_y))\n  ax.legend([],[],frameon=False)\n  # ax.set(xlabel=f\"{sub_chart_lbl}\", ylabel=f\"{dis_lbl}\" )\n  ax.set_xlabel(f\"{sub_chart_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'light'})\n  ax.set_ylabel(f\"{dis_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'normal'})\n  ax.set_title(f'{dis_lbl} by {sub_chart_lbl}', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n  ax.margins(y=0.2) \n  count = 0\n  prePandemic = None\n  data_labels = []\n  for container in ax.containers:\n    # use autolabel to add text on bar. This applies to older version\n    new_labels = autolabel(ax,container.patches, data_labels)\n    #we only use the first label as base.\n    if (len(data_labels) == 0):\n      data_labels = new_labels\n\n  ax.set_ymargin(0.2)\n\n\n\n  handles, labels = ax.get_legend_handles_labels()\n\n  legend_lables = [get_pandemic_text(x) for x in labels]\n\n  plt.tight_layout()\n\n  fig.suptitle(title, fontsize=25, y=1.1)\n  fig.legend(handles, legend_lables, ncol=3,   bbox_to_anchor=(0.73, 1.1*0.915))\n  fig.subplots_adjust(top=0.90, wspace = 0.2, hspace = 1.4)\n\n  plt.savefig(my_image_path + f\"\/{filename}.png\", dpi=300, bbox_inches='tight')\n\n  fig.show()","52b0a1d1":"filename = \"pct_access_enagement_index_free_reduced_lunch\"\nif replotChart:\n  \n  split_by = \"pandemic\"\n  display_cols = ['pct_access_sum_avg', 'engagement_index_sum_avg']\n  display_cols_label = ['pct_access sum per day', 'engagement_index sum per day']\n  sub_chart_by = [\"pct_free\/reduced\"]\n  sub_chart_by_label = [\"free\/reduced lunch\"]\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 01. pct_access and engagement index by free\/reduced lunch\"\n\n  create_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\")  )\n","2602eaeb":"filename = \"pct_access_unique_by_free_reduced_lunch\"\nif replotChart:\n  \n  split_by = \"pandemic\"\n  display_cols = 'pct_access_nunique'\n  display_cols_label = 'pct_access nunique count'\n  sub_chart_by = \"pct_free\/reduced\"\n  sub_chart_by_label = \"free\/reduced lunch ratio\"\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 02.Number of products by free\/reduced lunch\"\n\n  create_single_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\")  )","22b0707c":"filename = \"pct_access_enagement_index_pct_black_hispanic\"\nif replotChart:\n  \n  split_by = \"pandemic\"\n  display_cols = ['pct_access_sum_avg', 'engagement_index_sum_avg']\n  display_cols_label = ['pct_access sum per day', 'engagement_index sum per day']\n  sub_chart_by = [\"pct_black\/hispanic\"]\n  sub_chart_by_label = [\"pct_black\/hispanic ratio\"]\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 03. pct_access and engagement index by pct_black\/hispanic\"\n\n  create_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\")  )\n","515060ca":"filename = \"pct_access_unique_by_pct_black_hispanic\"\nif replotChart:\n  \n  split_by = \"pandemic\"\n  display_cols = 'pct_access_nunique'\n  display_cols_label = 'pct_access nunique count'\n  sub_chart_by = \"pct_black\/hispanic\"\n  sub_chart_by_label = \"pct_black\/hispanic ratio\"\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 04.Number of products by pct_black\/hispanic\"\n\n  create_single_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\", width=640) )","32af72b8":"filename = \"pct_access_sum_by_state\"\nif replotChart:\n  \n  split_by = \"pandemic\"\n  display_cols = 'pct_access_sum_avg'\n  display_cols_label = 'pct_access_sum per day'\n  sub_chart_by = \"state\"\n  sub_chart_by_label = \"state\"\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 05. pct_access sum by state\"\n\n  create_single_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\")  )","1f928293":"# replotChart = True\n# if replotChart: \n#   split_by = \"pandemic\"\n#   display_cols = ['pct_access_nunique', 'pct_access_count','pct_access_count_avg', 'pct_access_sum_avg', 'engagement_index_sum_avg']\n#   display_cols_label = ['pct_access nunique', 'pct_access count','pct_access count per day', 'pct_access sum per day', 'engagement_index sum per day']\n#   sub_chart_by = [\"pct_free\/reduced\", \"pct_black\/hispanic\", \"pp_total_raw\", \"state\"]\n#   sub_chart_by_label = [\"pct of free\/reduced lunch\", \"pct black\/hispanic\", \"per-pupil total expenditure\", \"state\"]\n#   state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n\n#   sns.set_style('darkgrid')\n#   fig, axx = plt.subplots(nrows = len(display_cols) * 2, ncols = 2, figsize=(20, len(display_cols) * 9 ), constrained_layout=False)\n\n#   qualitative_colors = [vv_grey, vv_orange, vv_blue]\n#   # sns.palplot(diverging_colors)\n\n#   def autolabel(ax, rects, data_labels):\n#       \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n#       first = len(data_labels) == 0\n#       currentLbl = []\n#       # for rect in rects:\n#       for idx, rect in enumerate(rects):\n#           height = rect.get_height()\n#           currentLbl.append(height)\n#           if(not first):\n#             pre = data_labels[idx]\n#             cur = height\n#             ax.annotate(f\"{(cur-pre)*100\/cur:.0f}%\",\n#                         xy=(rect.get_x() + rect.get_width() \/ 2, height),\n#                         xytext=(0, 3),  # 3 points vertical offset\n#                         textcoords=\"offset points\",\n#                         ha='center', va='bottom')\n#       return currentLbl\n\n#   chart_idx = 0\n#   for dis_col, dis_lbl in zip(display_cols,display_cols_label):\n#     for sub_chart, sub_chart_lbl in zip(sub_chart_by,sub_chart_by_label):\n#       chat_x = chart_idx \/\/ 2\n#       chat_y = chart_idx % 2\n#       if sub_chart == \"state\":\n#         chart_df = model_df.loc[model_df['state'].isin(state_list)]\n#       else:\n#         chart_df = model_df\n#       axx[chat_x][chat_y] = sns.barplot(x=sub_chart, y=dis_col, hue=split_by, data=chart_df,  ax=axx[chat_x,chat_y], palette = qualitative_colors,  edgecolor=\".9\", ci=None  ) #, \n#       axx[chat_x][chat_y].set_xticklabels(axx[chat_x][chat_y].get_xticklabels(), rotation=00, horizontalalignment='center', fontdict= { 'fontsize': 12, 'fontweight':'light'})\n#       # axx[chat_x][chat_y].set_yticklabels(chart_df[dis_col],fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n#       # axx[chat_x][chat_y].legend(title = split_by, bbox_to_anchor = (1, chat_y))\n#       axx[chat_x][chat_y].legend([],[],frameon=False)\n#       # axx[chat_x][chat_y].set(xlabel=f\"{sub_chart_lbl}\", ylabel=f\"{dis_lbl}\" )\n#       axx[chat_x][chat_y].set_xlabel(f\"{sub_chart_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'light'})\n#       axx[chat_x][chat_y].set_ylabel(f\"{dis_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'normal'})\n#       axx[chat_x][chat_y].set_title(f'{dis_lbl} by {sub_chart_lbl}', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n#       count = 0\n#       prePandemic = None\n#       data_labels = []\n#       for container in axx[chat_x][chat_y].containers:\n#         new_labels = autolabel(axx[chat_x][chat_y],container.patches, data_labels)\n#         #we only use the first label as base.\n#         if (len(data_labels) == 0):\n#           data_labels = new_labels\n#         # -- bar_label is only available after matlibplot 3.4.4 .\n#         # data_labels = [\"X\"] * 10 # len(container.datavalues)\n#         # print(container.datavalues)\n#         # if (count == 0):\n#         #   prePandemic = copy.deepcopy(container.datavalues)\n#         # else:\n#         #   data_labels = [f\"{(cur-pre)*100\/cur:.0f}%\"  for index, (pre, cur) in enumerate(zip(prePandemic, container.datavalues))]\n#         # axx[chat_x][chat_y].bar_label(container, padding=3,  labels = data_labels )     \n#         count += 1\n      \n#       axx[chat_x][chat_y].set_ymargin(0.2)\n#       chart_idx += 1\n\n#   handles, labels = axx[-1][-1].get_legend_handles_labels()\n\n#   legend_lables = [get_pandemic_text(x) for x in labels]\n\n#   plt.tight_layout(h_pad=2)\n\n#   fig.suptitle(\"engagement data difference in difference (DID)\", fontsize=30, y=0.94)\n#   fig.legend(handles, legend_lables, ncol=3,   bbox_to_anchor=(0.95, 0.92))\n#   fig.subplots_adjust(top=0.90, wspace = 0.2)\n\n#   plt.savefig(my_image_path + \"\/engagement_data_difference_in_difference_did.png\", dpi=100)\n\n#   fig.show()\n# else:\n#   display(Image(filename=my_image_path + \"\/engagement_data_difference_in_difference_did.png\")  )\n\n","cc2e68cc":"agg_df = pd.read_pickle(my_data_path + \"\/engagement_data_aggreated_district.pickle\")\nif rebuildDataFile: \n\n  agg_df['pct_access_count_avg'] = agg_df.apply(lambda x: divided_by_period_days(x,'pct_access_count', 'state_x', 'pandemic', 'weekday' ) ,axis=1)\n  agg_df['pct_access_sum_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'pct_access_sum', 'state_x', 'pandemic',  'weekday' ) ,axis=1)\n  agg_df['engagement_index_count_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'engagement_index_count', 'state_x', 'pandemic',  'weekday' ) ,axis=1)\n  agg_df['engagement_index_sum_avg'] =  agg_df.apply(lambda x: divided_by_period_days(x,'engagement_index_sum', 'state_x', 'pandemic',  'weekday' ) ,axis=1)\n\n  # agg_df['pp_total_raw'] =  agg_df.apply(lambda x: rename_category_value(x,'pp_total_raw') ,axis=1)\n  # agg_df['pct_black\/hispanic'] =  agg_df.apply(lambda x: rename_category_value(x,'pct_black\/hispanic') ,axis=1)\n  # agg_df['pct_free\/reduced'] =  agg_df.apply(lambda x: rename_category_value(x,'pct_free\/reduced') ,axis=1)\n  # agg_df['county_connections_ratio'] =  agg_df.apply(lambda x: rename_category_value(x,'county_connections_ratio') ,axis=1)\n  agg_df.drop(columns=['state_y'], inplace= True)\n  agg_df.rename(columns={'state_x': 'state'}, inplace=True)\n\n  weekday_agg_df = agg_df.loc[agg_df['weekday'] == 'Weekday']\n\n  weekend_agg_df = agg_df.loc[agg_df['weekday'] == 'Weekend']\n\n  wkdiff_df = pd.merge(weekday_agg_df, weekend_agg_df,  how='left', left_on=['district_id','pandemic','state'], right_on = ['district_id','pandemic','state'])\n\n  wkdiff_df['pct_access_count'] = wkdiff_df['pct_access_count_x'] - wkdiff_df['pct_access_count_y']\n  wkdiff_df['pct_access_mean'] = wkdiff_df['pct_access_mean_x'] - wkdiff_df['pct_access_mean_y']\n  wkdiff_df['pct_access_sum'] = wkdiff_df['pct_access_sum_x'] - wkdiff_df['pct_access_sum_x']\n  wkdiff_df['pct_access_nunique'] = wkdiff_df['pct_access_nunique_x'] - wkdiff_df['pct_access_nunique_y']\n  wkdiff_df['engagement_index_count'] = wkdiff_df['engagement_index_count_x'] - wkdiff_df['engagement_index_count_y']\n  wkdiff_df['engagement_index_mean'] = wkdiff_df['engagement_index_mean_x'] - wkdiff_df['engagement_index_mean_y']\n  wkdiff_df['engagement_index_sum'] = wkdiff_df['engagement_index_sum_x'] - wkdiff_df['engagement_index_sum_y']\n  wkdiff_df['pct_access_count_avg'] = wkdiff_df['pct_access_count_avg_x'] - wkdiff_df['pct_access_count_avg_y']\n  wkdiff_df['pct_access_sum_avg'] = wkdiff_df['pct_access_sum_avg_x'] - wkdiff_df['pct_access_sum_avg_y']\n  wkdiff_df['engagement_index_count_avg'] = wkdiff_df['engagement_index_count_avg_x'] - wkdiff_df['engagement_index_count_avg_y']\n  wkdiff_df['engagement_index_sum_avg'] = wkdiff_df['engagement_index_sum_avg_x'] - wkdiff_df['engagement_index_sum_avg_y']\n\n  wkdiff_df.drop(['locale_y', 'pct_black\/hispanic_y', \"pct_free\/reduced_y\", \"county_connections_ratio_y\", \"pp_total_raw_y\"], axis=1, inplace= True)\n  wkdiff_df.rename({'locale_x': 'locale', 'pct_black\/hispanic_x': 'pct_black\/hispanic', 'pct_free\/reduced_x': 'pct_free\/reduced', 'pp_total_raw_x': 'pp_total_raw'}, axis=1, inplace= True) \n  # agg_df.dropna(inplace=True)\n  wkdiff_df.to_pickle(my_data_path + \"\/engagement_dist_model_wd.pickle\")\n\n# model_df = pd.read_pickle(my_data_path + \"\/engagement_data_aggreated_district.pickle\")\nwkdiff_df = pd.read_pickle(my_data_path + \"\/engagement_dist_model_wd.pickle\")\n","940be5e9":"replotChart = True\nfilename = \"pct_access_sum_by_pct_free_reduced_weekday_weekend\"\nif replotChart:\n  model_df = wkdiff_df.loc[wkdiff_df['pandemic'] != \"Summer\"]\n  pfrl_type = CategoricalDtype(categories=[ \"[0, 0.2[\", \"[0.2, 0.4[\", \"[0.4, 0.6[\", \"[0.6, 1[\"], ordered=True)\n  model_df['pct_free\/reduced'] = model_df['pct_free\/reduced'].astype(pfrl_type)\n  split_by = \"pandemic\"\n  display_cols = 'pct_access_sum_avg'\n  display_cols_label = 'pct_access_sum per day'\n  sub_chart_by = \"pct_free\/reduced\"\n  sub_chart_by_label = \"pct_free\/reduced\"\n  state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n  title = \"Figure 06. Weekday\/Weekend pct_access sum by pct_free\/reduced lunch\"\n\n  create_single_chart(model_df, display_cols, display_cols_label, sub_chart_by, sub_chart_by_label, state_list, title, filename, split_by)\nelse:\n  display(Image(filename=my_image_path + f\"\/{filename}.png\")  )","961fa2dd":"# model_df = wkdiff_df.loc[wkdiff_df['pandemic'] != \"Summer\"]\n# replotChart = True\n# if replotChart: \n#   split_by = \"pandemic\"\n#   display_cols = ['pct_access_nunique','pct_access_count_avg', 'pct_access_sum_avg', 'engagement_index_sum_avg']\n#   display_cols_label = ['pct_access nunique','pct_access count per day', 'pct_access sum per day', 'engagement_index sum per day']\n#   sub_chart_by = [\"pct_free\/reduced\", \"pct_black\/hispanic\", \"pp_total_raw\", \"state\"]\n#   sub_chart_by_label = [\"pct of free\/reduced lunch\", \"pct black\/hispanic\", \"per-pupil total expenditure\", \"state\"]\n#   state_list = ['Connecticut', 'Utah',  'Illinois', 'California']\n# #   plt.style.use('fivethirtyeight')\n#   sns.set_style('darkgrid')\n#   fig, axx = plt.subplots(nrows = len(display_cols) * 2, ncols = 2, figsize=(20, len(display_cols) * 9 ), constrained_layout=False)\n\n#   qualitative_colors = [vv_grey, vv_orange, vv_blue]\n#   # sns.palplot(diverging_colors)\n\n#   def autolabel(ax, rects, data_labels):\n#       \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n#       first = len(data_labels) == 0\n#       currentLbl = []\n#       # for rect in rects:\n#       for idx, rect in enumerate(rects):\n#           height = rect.get_height()\n#           currentLbl.append(height)\n#           if(not first):\n#             pre = data_labels[idx]\n#             cur = height\n#             ax.annotate(f\"{(cur-pre)*100\/cur:.0f}%\",\n#                         xy=(rect.get_x() + rect.get_width() \/ 2, height),\n#                         xytext=(0, 3),  # 3 points vertical offset\n#                         textcoords=\"offset points\",\n#                         ha='center', va='bottom')\n#       return currentLbl\n\n#   chart_idx = 0\n#   for dis_col, dis_lbl in zip(display_cols,display_cols_label):\n#     for sub_chart, sub_chart_lbl in zip(sub_chart_by,sub_chart_by_label):\n#       chat_x = chart_idx \/\/ 2\n#       chat_y = chart_idx % 2\n#       if sub_chart == \"state\":\n#         chart_df = model_df.loc[model_df['state'].isin(state_list)]\n#       else:\n#         chart_df = model_df\n#       axx[chat_x][chat_y] = sns.barplot(x=sub_chart, y=dis_col, hue=split_by, data=chart_df,  ax=axx[chat_x,chat_y], palette = qualitative_colors,  edgecolor=\".9\", ci=None  ) #, \n#       axx[chat_x][chat_y].set_xticklabels(axx[chat_x][chat_y].get_xticklabels(), rotation=00, horizontalalignment='center', fontdict= { 'fontsize': 12, 'fontweight':'light'})\n#       # axx[chat_x][chat_y].set_yticklabels(chart_df[dis_col],fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n#       # axx[chat_x][chat_y].legend(title = split_by, bbox_to_anchor = (1, chat_y))\n#       axx[chat_x][chat_y].legend([],[],frameon=False)\n#       # axx[chat_x][chat_y].set(xlabel=f\"{sub_chart_lbl}\", ylabel=f\"{dis_lbl}\" )\n#       axx[chat_x][chat_y].set_xlabel(f\"{sub_chart_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'light'})\n#       axx[chat_x][chat_y].set_ylabel(f\"{dis_lbl}\", fontdict= { 'fontsize': 14, 'fontweight':'normal'})\n#       axx[chat_x][chat_y].set_title(f'{dis_lbl} by {sub_chart_lbl}', fontdict= { 'fontsize': 14, 'fontweight':'bold', 'verticalalignment': 'bottom'} )\n#       count = 0\n#       prePandemic = None\n#       data_labels = []\n#       for container in axx[chat_x][chat_y].containers:\n#         new_labels = autolabel(axx[chat_x][chat_y],container.patches, data_labels)\n#         #we only use the first label as base.\n#         if (len(data_labels) == 0):\n#           data_labels = new_labels\n#         # -- bar_label is only available after matlibplot 3.4.4 .\n#         # data_labels = [\"X\"] * 10 # len(container.datavalues)\n#         # print(container.datavalues)\n#         # if (count == 0):\n#         #   prePandemic = copy.deepcopy(container.datavalues)\n#         # else:\n#         #   data_labels = [f\"{(cur-pre)*100\/cur:.0f}%\"  for index, (pre, cur) in enumerate(zip(prePandemic, container.datavalues))]\n#         # axx[chat_x][chat_y].bar_label(container, padding=3,  labels = data_labels )     \n#         count += 1\n      \n#       axx[chat_x][chat_y].set_ymargin(0.2)\n#       chart_idx += 1\n\n#   handles, labels = axx[-1][-1].get_legend_handles_labels()\n\n#   legend_lables = [get_pandemic_text(x) for x in labels]\n\n#   plt.tight_layout(h_pad=2)\n\n#   fig.suptitle(\"engagement data weekday minus weekend\", fontsize=30, y=0.94)\n#   fig.legend(handles, legend_lables, ncol=3,   bbox_to_anchor=(0.95, 0.92))\n#   fig.subplots_adjust(top=0.90, wspace = 0.2)\n\n#   plt.savefig(my_image_path + \"\/engagement_data_weekday_minus_weekend.png\", dpi=100)\n\n#   fig.show()\n# else:\n#   display(Image(filename=my_image_path + \"\/engagement_data_weekday_minus_weekend.png\")  )\n\n","b8291a57":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols","d6a5e0d5":"# merging the three data sets districts_df, products_df and eng_all_df.\n\nli = []\ndf = pd.merge(eng_all_df, districts_df[['district_id','state','pct_free\/reduced','pct_black\/hispanic']], how='inner', on = 'district_id')\ndf = pd.merge(df, products_df[['LP ID','primary_function_main','primary_function_sub']], how = 'right', right_on = 'LP ID', left_on = 'lp_id')\nli.append(df)\nreg_df = pd.concat(li)\ndel li\n\n# dropping all the mssing values in pct_free\/reduced\nreg_df = reg_df.replace('Missing', np.nan)\nreg_df =reg_df.dropna(subset=[\"pct_free\/reduced\"])\n\n# defining functions to sort dataset\ndef get_one_period_dist_id(eng_df, query):\n   period_df = eng_df.loc[query]\n   agg_df = period_df.groupby(['district_id']).agg(\n     {\n      'pct_access': ['count', 'mean', 'sum'],\n      'engagement_index':['count', 'mean','sum']\n     }\n   ).reset_index()\n   agg_df.columns = [\"_\".join(a) for a in agg_df.columns.to_flat_index()]\n   # print(agg_df.info())\n   return agg_df\n\ndef combine_freelunch (row):\n   if  row['pct_free\/reduced'] in ['[0.6, 0.8[', '[0.8, 1[']:\n      return '[0.6, 1['\n   return row['pct_free\/reduced']\n\ndef Diff_in_Diff_Categories(eng_df,day_type):\n    jan_query = (eng_df['time'] >= Pre_Start) & (eng_df['time'] <= Pre_End)\n    jan_df = get_one_period_dist_id(eng_df, jan_query)\n    ls = []\n    for i in range(jan_df.shape[0]):\n        ls.append(0)\n    ls = pd.DataFrame(ls).rename(columns = {0 : 'Pandemic_Stage'}, inplace = False)\n    jan_df = pd.merge(jan_df, ls, left_index = True, right_index = True)\n    del ls\n    jan_df['pct_access_sum'] = jan_df[\"pct_access_sum\"]\/Pre_Days(day_type)\n    jan_df['engagement_index_sum'] = jan_df[\"engagement_index_sum\"]\/Pre_Days(day_type)\n    jan_df['pct_access_mean'] = jan_df[\"pct_access_mean\"]\n    jan_df['engagement_index_mean'] = jan_df[\"engagement_index_mean\"]\n    jan_df['pct_access_count'] = jan_df[\"pct_access_count\"]\/Pre_Days(day_type)\n    jan_df['engagement_index_count'] = jan_df[\"engagement_index_count\"]\/Pre_Days(day_type)\n    \n    mar_query = (eng_df['time'] >= Pan_Start) & (eng_df['time'] <= Pan_End)\n    mar_df = get_one_period_dist_id(eng_df, mar_query)\n    ls = []\n    for i in range(mar_df.shape[0]):\n        ls.append(1)\n    ls = pd.DataFrame(ls).rename(columns = {0 : 'Pandemic_Stage'}, inplace = False)\n    mar_df = pd.merge(mar_df, ls, left_index = True, right_index = True)\n    del ls\n    mar_df['pct_access_sum'] = mar_df[\"pct_access_sum\"]\/Pan_Days(day_type)\n    mar_df['engagement_index_sum'] = mar_df[\"engagement_index_sum\"]\/Pan_Days(day_type)\n    mar_df['pct_access_mean'] = mar_df[\"pct_access_mean\"]\n    mar_df['engagement_index_mean'] = mar_df[\"engagement_index_mean\"]\n    mar_df['pct_access_count'] = mar_df[\"pct_access_count\"]\/Pre_Days(day_type)\n    mar_df['engagement_index_count'] = mar_df[\"engagement_index_count\"]\/Pre_Days(day_type)\n    \n    sept_query = (eng_df['time'] >= Post_Start) & (eng_df['time'] <= Post_End)\n    sept_df = get_one_period_dist_id(eng_df, sept_query)\n    ls = []\n    for i in range(sept_df.shape[0]):\n        ls.append(2)\n    ls = pd.DataFrame(ls).rename(columns = {0 : 'Pandemic_Stage'}, inplace = False)\n    sept_df = pd.merge(sept_df, ls, left_index = True, right_index = True)\n    del ls\n    sept_df['pct_access_sum'] = sept_df[\"pct_access_sum\"]\/Post_Days(day_type)\n    sept_df['engagement_index_sum'] = sept_df[\"engagement_index_sum\"]\/Post_Days(day_type)\n    sept_df['pct_access_mean'] = sept_df[\"pct_access_mean\"]\n    sept_df['engagement_index_mean'] = sept_df[\"engagement_index_mean\"]\n    sept_df['pct_access_count'] = sept_df[\"pct_access_count\"]\/Pre_Days(day_type)\n    sept_df['engagement_index_count'] = sept_df[\"engagement_index_count\"]\/Pre_Days(day_type)\n    \n    merged_df = pd.merge(jan_df, mar_df, how = 'outer')\n    merged_df = pd.merge(merged_df, sept_df, how = 'outer')\n    merged_df.rename({'district_id_': 'district_id'}, axis=1, inplace=True)\n    merged_df = pd.merge(merged_df, districts_df, how = 'inner', on = 'district_id')\n    \n    return merged_df\n\ndef prim_func_merge(eng_df):\n    weekend_query = eng_df['time'].dt.dayofweek.isin([5,6])\n    weekday_query = eng_df['time'].dt.dayofweek.isin([0,1,2,3,4])\n    weekday_eng_df = eng_df.loc[weekday_query]\n    weekend_eng_df = eng_df.loc[weekend_query]\n    weekday_merged_df = Diff_in_Diff_Categories(weekday_eng_df,'weekday')\n    weekend_merged_df = Diff_in_Diff_Categories(weekend_eng_df,'weekend')\n    \n    ls = []\n    for i in range(weekday_merged_df.shape[0]):\n        ls.append(0)\n    ls = pd.DataFrame(ls).rename(columns = {0 : 'Week_End'}, inplace = False)\n    weekday_merged_df = pd.merge(weekday_merged_df, ls, left_index = True, right_index = True)\n    del ls\n    \n    ls = []\n    for i in range(weekend_merged_df.shape[0]):\n        ls.append(1)\n    ls = pd.DataFrame(ls).rename(columns = {0 : 'Week_End'}, inplace = False)\n    weekend_merged_df = pd.merge(weekend_merged_df, ls, left_index = True, right_index = True)\n    del ls\n    \n    allweek_merged_df = pd.merge(weekday_merged_df, weekend_merged_df, how = 'outer')\n\n    return allweek_merged_df\n\n#running merge functions\n\nmerge_reg_df = prim_func_merge(reg_df)\n","cdf77c4d":"#dummies for pandemic stage\npandemic_stage_dummies = pd.get_dummies(merge_reg_df['Pandemic_Stage'],prefix='pandemic_stage')\npandemic_stage_dummies = pandemic_stage_dummies.rename(columns = {\"pandemic_stage_0\":\"pandemic_stage_before_COVID\"})\npandemic_stage_dummies = pandemic_stage_dummies.rename(columns = {\"pandemic_stage_1\":\"pandemic_stage_beginning_of_COVID\"})\npandemic_stage_dummies = pandemic_stage_dummies.rename(columns = {\"pandemic_stage_2\":\"pandemic_stage_new_school_year\"})\ndel pandemic_stage_dummies[\"pandemic_stage_before_COVID\"]\n#dummies for black_hispanic\npct_black_hispanic_dummies = pd.get_dummies(merge_reg_df['pct_black\/hispanic'],prefix='pct_black_hispanic')\npct_black_hispanic_dummies = pct_black_hispanic_dummies.rename(columns=lambda x: x.replace('1[','10').replace('[','').replace('0, ','00').replace('.','').replace(', ',''))\ndel pct_black_hispanic_dummies['pct_black_hispanic_0002']\n#dummies for free lunch\npct_free_dummies = pd.get_dummies(merge_reg_df['pct_free\/reduced'],prefix='pct_free_reduced')\npct_free_dummies = pct_free_dummies.rename(columns=lambda x: x.replace('1[','10').replace('[','').replace('0, ','00').replace('.','').replace(', ',''))\ndel pct_free_dummies['pct_free_reduced_0002']\n#Asigning the input variables\nx = pandemic_stage_dummies\nx = pd.merge(x, pct_free_dummies,left_index=True,right_index=True)\nx = pd.merge(x, pct_black_hispanic_dummies, left_index = True, right_index = True)\nx = pd.merge(x, merge_reg_df['Week_End'],left_index=True,right_index=True)\nx.head()\n#Asigning the ouput\ny0 = merge_reg_df['pct_access_sum']\ny1 = merge_reg_df['engagement_index_mean']\n#merging data\nols_df = pd.merge(y1,x,left_index=True,right_index=True)\nols_df = pd.merge(y0,ols_df,left_index=True,right_index=True)","1c79614c":"#running the regression\nformula = \"pct_access_sum ~ pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year + Week_End + pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610 + pct_black_hispanic_0204 + pct_black_hispanic_0410\"\nmodel = ols(formula, ols_df).fit()\nprint(model.summary())","c785d784":"formula = \"engagement_index_mean ~ pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year + Week_End + pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610 + pct_black_hispanic_0204 + pct_black_hispanic_0410\"\nmodel = ols(formula, ols_df).fit()\nprint(model.summary())","d0ca2021":"formula = \"pct_access_sum ~ (pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year) + Week_End + (pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610) + (pct_black_hispanic_0204 + pct_black_hispanic_0410) + (pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year)*Week_End + (pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year)*(pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610) + (pandemic_stage_beginning_of_COVID + pandemic_stage_new_school_year)*(pct_black_hispanic_0204 + pct_black_hispanic_0410) + Week_End*(pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610) + Week_End*(pct_black_hispanic_0204 + pct_black_hispanic_0410)+(pct_free_reduced_0204+ pct_free_reduced_0406 + pct_free_reduced_0610)*(pct_black_hispanic_0204 + pct_black_hispanic_0410)\"\nmodel = ols(formula, ols_df).fit()\nprint(model.summary())","64e58f6a":"!pip install pycausalimpact\nimport causalimpact","f259d588":"Connecticut_df = reg_df[reg_df['state']=='Connecticut']\nCalifornia_df = reg_df[reg_df['state']=='California']\nIllinois_df = reg_df[reg_df['state']=='Illinois']\nUtah_df = reg_df[reg_df['state']=='Utah']\nWisconsin_df = reg_df[reg_df['state']=='Wisconsin']\nWashington_df = reg_df[reg_df['state']=='Washington']\ndef agg_state_data(state_df):\n    state_agg_df = state_df.groupby(['time']).agg(\n        {\n        'pct_access':['count','mean','sum'],\n        'engagement_index':['count','mean','sum']\n        }\n    )\n    state_agg_df.columns = [\"_\".join(a) for a in state_agg_df.columns.to_flat_index()]\n    return state_agg_df\nConnecticut_agg_df = agg_state_data(Connecticut_df)\nCalifornia_agg_df = agg_state_data(California_df)\nIllinois_agg_df = agg_state_data(Illinois_df)\nUtah_agg_df = agg_state_data(Utah_df)\nWisconsin_agg_df = agg_state_data(Wisconsin_df)\nWashington_agg_df = agg_state_data(Washington_df)\n\ndata = Connecticut_agg_df['pct_access_sum']\ndata = pd.merge(data,California_agg_df['pct_access_sum'],left_index=True,right_index=True)\ndata = data.rename(columns = {'pct_access_sum_x':'Connecticut','pct_access_sum_y':'California'})\ndata = pd.merge(data,Illinois_agg_df['pct_access_sum'],left_index=True,right_index=True)\ndata = pd.merge(data,Utah_agg_df['pct_access_sum'],left_index=True,right_index=True)\ndata = data.rename(columns = {'pct_access_sum_x':'Illinois','pct_access_sum_y':'Utah'})\ndata = pd.merge(data,Wisconsin_agg_df['pct_access_sum'],left_index=True,right_index=True)\ndata = pd.merge(data,Washington_agg_df['pct_access_sum'],left_index=True,right_index=True)\ndata = data.rename(columns = {'pct_access_sum_x':'Wisconsin','pct_access_sum_y':'Washington'})\npre_period = ['2020-01-01','2020-03-15']\npost_period = ['2020-03-16','2020-06-01']\nend_period = ['2020-09-01','2020-12-31']\nci_Conn = causalimpact.CausalImpact(data, pre_period, post_period, nseasons=[{'period': 7, 'harmonics': 2}])","707764c8":"print(ci_Conn.summary())\nci_Conn.plot()\nprint(ci_Conn.summary('report'))","4387f6fd":"<p>As reveled in the analysis in the above section, students in the state of Connecticut seems to be impacted less at the onset of the pandemic. We conjecture that the help offered by the state to ensure more residents can access internet from home play a critical role. To evaluate the potential causal impact of such a state level policy, we conduct the causal analysis using the approach of synthetic control. Leveraging the free Causal Impact courtesy of Google, we present our results here. <\/p>\n<p>On March 14th, very early into the pandemic, the FCC launched the <b>Keep Americans Connected Pledge<\/b> to ensure that internet service providers would ensure connectivity during the pandemic by not terminating business or residential service for anyone who can\u2019t pay their bill due to hardship caused by COVID-19. According to Connecticut's <a href:\"https:\/\/portal.ct.gov\/Coronavirus\/Information-For\/Internet-Access\">official state website<\/a>, in addition to the <b>Keep Americans Connected Pledge<\/b>, several internet providers in CT took a step further by providing <b>free internet for low-income households<\/b> and providing 60 days free broadband services for students who have not already subscribed to cope with online education. These initiatives could be some of the reasons that explain the unique data pattern of CT during the initial period of the pandemic.<\/p>\n    <p>In addition, Connecticut also houses a respectable broadband (25 megabits per second download and 3 megabits per second upload) availability and usage number in comparison to other states. According to <a href:\"https:\/\/blogs.microsoft.com\/on-the-issues\/2019\/07\/31\/signs-of-progress-on-broadband-mapping-but-more-work-still-to-do\/\">microsoft's research in 2019<\/a>, broadband is available to 93.5% of the people in US, yet only 52% of the people use the internet at broadband speed. Different counties in different states have disparate broadband usage percentage. For example, while the average broadband usage in California is 61.6%, the lowest of its counties is 23.6%. When these disparaties are common across most states, Connecticut stands out with an average bandwith of 52.6% and a lowest bandwith of 45.5%. The gap is at a low 7.1% in contrast to California's 38.0%.Under such circumstances broadband reachout programs in response to COVID-19 pandemic may benefit online education positively. Disadvantaged family's inability to pay expensive mobile data does seem to have a higher correlation with the digital divide.<\/p>\n<p>The treatment group is consisted of a time series of pct_access_sum of Connecticut state. The control group consists  of time series pct_access_sum of California, Illinois, Washington, Wisconsin and Utah. Using these states as control variables does not necessary mean that these other states did not respond to the internet inequality as quickly as Connecticutt. ","664eb52c":"<a id='Heterogeneous'><\/a>\n<h1> 3. Heterogeneous Impact of COVID-19 <\/h1>","5425c445":"<p> At the end of the kernal our team decided to run a least squares regression analysis. The aim of running the regression does not attempt to make predictions of student engagement on learning platforms, but rather to better understand the relationship between the different parameters and student engagement on learning platforms. We hope to reensure that the variables that we identify as important influencers of the disparity between the different pct_access_sum across different states can be further supported. The variables incorporated in the model are as follows:\n<\/p>\n<ol>\n    <li><b>pct_free\/reduced:<\/b> Percentage of students in the districts eligible for free or reduced-price lunch.<\/li>\n    <li><b>pandemic_stage:<\/b> The stage of pandemic as divided with our difference in difference method<\/li>\n    <li><b>pct_black_hispanic:<\/b> Percentage of students in the districts identified as Black or Hispanic.<\/li>\n    <li><b>week_day: <\/b>Type of day, since there is a lot of difference between the engagement during week days and the engagement during weekends.<\/li>\n<\/ol>\n<p>All of these input variables are set as dummy variables of different degrees. While for the output variable we can have pct_access_count and engagement_index_mean; which can be denoted with $Y_i$ for $i = 0,1$. We can suppose the relationship between the input variables and the output:\n    $$Y_i = \\alpha_0 + \\beta_l L_i + \\beta_t T_i + \\beta_r R_i + \\beta_d D_i + \\epsilon$$\n    <\/p>\n<p>The notations are uppercased, because they are matrices of dummy variables with multiple classes. To make the notations comprehensible, let $L_i$ indicate pct_free\/reduced lunch which has four dummy variables:<\/p>\n\n$$\n\\begin{array}{cols} \nE[\\beta_lL_i|l_{1i}=0,l_{2}=0,l_{3i}=0] & = & \\text{pct_free\/reduced [0.0-0.2]},\\\\ \nE[\\beta_lL_i|l_{1i}=1,l_{2}=0,l_{3i}=0] & = & \\text{pct_free\/reduced [0.2-0.4]},\\\\\nE[\\beta_lL_i|l_{1i}=0,l_{2}=1,l_{3i}=0] & = & \\text{pct_free\/reduced [0.4-0.6]},\\\\\nE[\\beta_lL_i|l_{1i}=0,l_{2}=0,l_{3i}=1] & = & \\text{pct_free\/reduced [0.6-1.0]}.\n\\end{array}\n$$\n<p> Let $T_i$ be a time variable that indicates pandemic_stage:<\/p>\n$$\n\\begin{array}{cols} \nE[\\beta_tT_i|t_{1i}=0,t_{2}=0] & \\mbox{=} & \\text{before_COVID},\\\\ \nE[\\beta_tT_i|t_{1i}=1,t_{2}=0] & \\mbox{=} & \\text{beginning_of_COVID},\\\\\nE[\\beta_tT_i|t_{1i}=0,t_{2}=1] & \\mbox{=} & \\text{new_school_year}.\n\\end{array}\n$$\n<p> Let $R_i$ indicate the race variable, pct_black_hispanic:<\/p>\n$$\n\\begin{array}{cols} \nE[\\beta_rR_i|r_{1i}=0,r_{2}=0] & \\mbox{=} & \\text{pct_black_hispanic [0.0-0.2]},\\\\ \nE[\\beta_rR_i|r_{1i}=1,r_{2}=0] & \\mbox{=} & \\text{pct_black_hispanic [0.2-0.4]},\\\\\nE[\\beta_rR_i|r_{1i}=0,r_{2}=1] & \\mbox{=} & \\text{pct_black_hispanic [0.4-1.0]}.\n\\end{array}\n$$\n<p> Let $D_i$ indicate the day type variable differentiating weekday and weekend:<\/p>\n$$\n\\begin{array}{cols} \nE[\\beta_dD_i|d_{1i}=0] & \\mbox{=} & \\text{Week_Day},\\\\\nE[\\beta_dD_i|d_{1i}=1] & \\mbox{=} & \\text{Week_End}.\n\\end{array}\n$$\n<p> As a result, the baseline formula for the model when all dummies are zero is:\n$$ \n\\alpha_0  =  \\text{pct_free\/reduced [0.0-0.2]} + \\text{pct_black_hispanic [0.0-0.2]} + \\text{before_COVID} + \\text{Week_Day}\n$$ ","54723136":"<p>Note: The kaggle is quite slow to process large amount of the data. To save the time and improve the efficiency, we made the calculation on google colab and copy the intermediate files over. <p><\/p> The same code will yield same result when you enable the <b>rebuildDataFile<\/b> and <b>replotChart<\/b> flag.<\/p>","4bd5c31a":"<a id='Causal'><\/a>\n<h1> 5. Causal Analysis for State Help of Internet Access <\/h1>","ea9b98da":"<p>Although the break of the pandemic has unavoidable impact on students' online learning activities across the nation, the severity of its impacts vary dramatically across school districts. To understand the differential impacts, and its associations with the social-economic factors, we deploy a series of plots to visualize our findings.<\/p>\n<p> In the following plots, we leverage three socio-economic variables from the school district data,  including Percentage of students who received reduced or free lunch, Percentage of black\/hispanic students, Total expenditure per pupil.<\/p>\n<p>The studies focused on the following three activity variables summarized from the engagement data, including pct_access_nunique (copy the current paragraph in the notebook)\npct_access_sum_avg(copy the current paragraph in the notebook)\nengagement_index_sum_avg(copy the current paragraph in the notebook)<\/p>\n<p>We use difference in difference for this analysis. The following process was conducted beforehand:<\/p>\n<ol>\n    <li>Labeled the engagement data by period of the pandemic. We divided the year 2020 into 4 periods:: \n        <ul>\n            <li><b>Before pandemic (BP)<\/b>: From the begining of the year to the time when the state closed the school.<\/li>\n            <li><b>In pandemic (IP)<\/b>: From the time when school closed to the Jun. 15th which is the begining of the summer.<\/li>\n            <li><b>Summer<\/b>: From the Jun. 15th to Sept. 1st. This period has no significant activities, so we excluded this period.<\/li>\n            <li><b>New School Year (NSY)<\/b>: From Sept. 1st to end of the year.<\/li>\n        <\/ul> \n    <\/li>\n    <li>We also want see the <b>difference between the weekday and weekend<\/b>, so we label the engagement data by weekday as well. <\/li>\n    <li>Aggregate the engagement data by district_id, weekday, pandemic period, state. Calculate count, sum, unique, mean of pct_access and count, sum, mean of engagement_index.<\/li>\n    <li>Because each period has a different amount of days, we calculated the daily average of pct_access_count, pct_access_sum, engagement_index_count and engagement_index_sum.<\/li>\n    <li>Some categories in the <b>district data have close to no data<\/b>. For example, \"pct_free\/reduced\" has very few districts in \"\\[0.8, 1\\[\" category. So, we have <b>combined some of these categories with others.<\/b><\/li>    \n<\/ol>\n<p>\u200b\nWe researched the following engagement data variables: 'pct_access_nunique', 'pct_access_count','pct_access_count_avg', 'pct_access_sum_avg', and 'engagement_index_sum_avg'. For each of the data variables above, we sliced the data using \"pct of free\/reduced lunch\", \"pct black\/hispanic\", \"per-pupil total expenditure\", and \"state\". <b>We examined and compared the difference between each pandemic period based on these variables.<\/b><\/p>\n<ul>\n    <li><b>pct_access_nunique<\/b> represents how many unique products are in a particular district in a particular period. In general, we observed that the number of <b>total products used does not change before and during the pandemic. However, it greatly increased at the beginning of the new school year.<\/b>   <\/li>\n    <li><b>pct_access_sum_avg<\/b> represents the average amount of people that accessed the internet on given day within a district (per thousand students). It is apparent that schools with <b>a higher per-pupil total expenditure<\/b>,also have a <b>higher pct_access_sum_avg before and in the pandemic<\/b>. Furthermore, they had a <b>significantly higher increase at the beginning of the new school year<\/b>. There is a similar pattern when we base schools on <b>low free\/reduce lunch ratio<\/b> instead of per-pupil total expenditure.  <b>The schools with a higher percentage of free lunch were hit significantly harder in the pandemic with a smaller recovery in the fall semester.<\/b> On a side note, we did not observe any significant differences between races.<\/li>\n    <li><b>engagement_index_sum_avg<\/b> inidcates the total activity through the digital learning platform. <b>This follows the pattern we observed in pct_acess_sum_avg but with a bigger magnitude.<\/b>  <\/li>\n    <li>Regarding state's response to the pandemic, <b>Connecticut outperformed the rest of the states<\/b>. It is the only state that has a better pct_acces_sum at the start of the pandemic. It could due to the fact that <a href=\"https:\/\/portal.ct.gov\/Office-of-the-Governor\/News\/Press-Releases\/2020\/03-2020\/Governor-Lamont-Coronavirus-Update-March-14-2020-5PM\">Connecticut implemented a policy<\/a> to provide emergency internet plans for low-income families at the start of March, while other states, such as Illinois and Utah, implemented similar policies later.\n   <\/li>\n\n<\/ul>","6670d37b":"<a id='|data'><\/a>\n<h1> 2. Data Cleansing <\/h1>","a514a7e4":"<p>According to the interaction regression results: <\/p>\n<ul>\n    <li>There are not many significant results in this model, however the R-squared has improved to 0.610. We can still make some interpretations with the parameters.\n    <\/li>\n    <li>The interaction between pandemic period and free\/reduced lunch ratio is interesting. Remember the previous results? In the previous model beginning_COVID yielded a p-value of 0.294, yet the through interaction with the pct_free_reduced of 0204, 0406 and 0610 the regression results show improvment with p-values of 0.152, 0.330 and 0.163. These interactions yield negative coeficients with the beginning_COVID:pct_free_reduced_0610 being the lowest of them all. This means that schools from districts with higher free lunch rate have a lower pct_access_sum than schools with lower free lunch rate during beginning_COVID. We can interpret that schools from poorer districts are not as prepared equiped with IT resources and IT skills to face such a pandemic.\n    <\/li>\n<\/ul>     ","3d9db29d":"<h2> b. OLS regression <\/h2>","abf21f1c":"<h3> Summary of findings: <\/h3>\n<ol>\n    <li>School districts with higher free lunch rate (poorer schools) have lower product access rate during in pandemic period than school districts with lower free lunch rate (more resourceful schools). This may have many different causes:\n        <ul>\n        <li>Schools from poorer regions are less prepared to move online, due to the school's lack of IT resources and the teacher's lack of IT pedagogical skills.\n        <\/li>\n        <li>Students from poorer regions are less prepared to move online, due to the lack of resources such as<a href\uff1a\"https:\/\/blogs.microsoft.com\/on-the-issues\/2019\/07\/31\/signs-of-progress-on-broadband-mapping-but-more-work-still-to-do\/\">broadband<\/a>,and <a href:\"https:\/\/www.frontiersin.org\/articles\/10.3389\/fpsyg.2020.592670\/full\">self motivation<\/a>.\n        <\/li>\n        <\/ul>\n    <\/li>\n    <li>School districts with the highest free lunch rate and highest black hispanic ratio have the lowest product access rate. This means that in terms of regions with the highest free lunch rate, those whom have a higher black hispanic ratio perform worse.<\/li>\n    \n<\/ol>","0ccd585d":"<p>The data set is not immune to the common problem of \"missing data\". To gauge its severity, in this section, we visualize the missing data across the three data sets, including the learning platform engagement data, the school district data, and the product data.<\/p>","dbd5e838":"<h3> Regression with interaction <\/h3>","0cbf86ad":"<h3> Regression without interaction <\/h3>\n<p>We first run a least squares regression without any interactions to get a straight forward visualization of the relationship of the variables with the number of education products being accessed.<\/p>","0065236b":"<p>According to the regression results: <\/p>\n<ul>\n    <li>For pandemic stage with before pandemic as the baseline, post pandemic is a significant predictor for pct_access_sum. This means that there is a significant increase in online education products accessed from before the pandemic to New_school_year. In pandemic is not a significant indicator, however this doesn't mean that the access rate is not affected in the pandemic, but rather that there is a mixed number of responses by different states resulting in data that isn't linear and normalized. These mixed responses can be observed clearer by asigning interaction variables, which will be explained later.\n    <\/li>\n    <li>The weekend is a significant predictor with a negative parameter, which means that education products have a lower access number in the weekend compared to weekdays (kind of obvious).<\/li>\n    <li>In agreement with our hypothesis, school districts with different free lunch rates responded differently to the pandemic. Generally, less free lunch is an indicator of higher product access number. It doesn't mean that the higher free lunch rate is the cause of lower product access, but rather because of the socio-economic circumstances that is correlated with free lunch.<\/li>\n    <li>Contrary to our opinion, the data shows that black & hispanic ratio is not a predictor of product accessed. This is perhaps because socio-economic differences is a greater indicator of inequality than race differences. Yet again, this result shows otherwise in the next model.<\/li>\n    <li>The products variables show that different products have different access rate, which is in correlation to our bar charts.<\/li>\n    <\/ul>","08a280b4":"<a id='engagement_did'><\/a>\n<h1> 4. Difference in Difference (DID) Analysis <\/h1>","3c7099bc":"<a id='executive_summary'><\/a>\n<h1> 1. Executive Summary <\/h1>\n<body>\n<p>We are the team of college student, highschooler, and middle schooler under the guidance of Prof. Dong. Our first hand experience in the pandemic motivated us to participate in this competition. One of our members, whose school has a 38% in free\/reduced lunch programs, experienced an an \u201cabsolute tragedy\u201d at the start of the pandemic. There were no zoom meetings and zero lectures. It was essentially just a list of assignments from google classroom. When zoom meetings finally came about, his school had an average attendance rate of 43%. <\/p>\n \n<p>To gain a better understanding of the effect of COVID-19 on online learning, we split the data into four parts: Before COVID (January - March), Beginning of COVID (March -June), summer (July-August), New School Year (September-December).<\/p>\n<p>Considering the specialty of the summer time period (July-August), in our analysis, we focus on the other three time periods. <\/p>\n<p>Our initial data exploration discovers a striking decline of online learning access during the initial period of the pandemic (March \u2013 June), which was later recovered after the new school year starts. Although it is consistent with one of our team members\u2019 personal experiences during that time period, it attracts our attention to this special time period when the break of the pandemic initially strikes. <\/p>\n<p>Throughout this study, we focus on two variables measuring online activities at the district level, including<\/p>\n<ul>\n     <li>Number of students accessing the online learning platform, which is measured as the percentage of students who accessed at least one page per 1000 students in the school district. <\/li>\n     <li>Total number of pages accessed on the learning platform, which is measured as total engagement index. <\/li>\n<\/ul>\n<ul>\n    <li>Heterogeneous impacts of the pandemic through data visualization<\/li>\n    <li>Difference in difference analysis<\/li>\n    <li>Causal analysis evaluating the internet access policy implemented by Connecticut<\/li>\n    <\/ul>\n<h2>Our findings can be summarized as the following: <\/h2>\n    <h3>First, Initial Response to the Pandemic hit<\/h3>\n    <p>When pandemic first hit, across all school districts, we see increases in total access to the learning platform, and further increases when the new school year starts. However, in most of the school districts, the number of students accessing the online platform was actually decreased at the onset of the pandemic, although it climbed back during the new school year. The decrease is especially dramatic among the districts with lower economic resources, as measured by the following three metrics: <\/p>\n    <ol type='a'>\n        <li>percentage of students with free or reduced lunch<\/li>\n        <li>total expenditure per pupil<\/li>\n        <li>percentage of students that are Hispanic or black<\/li>\n    <\/ol>\n    <p>Further investigations shows that the number of products accessed at the beginning of COVID also decreased except for the wealthier school districts. While this may be the result of two possible action changes, including\uff1a\n        <ul>\n            <li>Due to a financial hit, some school district may have reduced the number of products accessible to their students. Considering the short response time period, this is unlikely.<\/li>\n            <li>Additional readings on <a href = 'https:\/\/www.theguardian.com\/world\/2020\/apr\/13\/coronavirus-covid-19-exposes-cracks-us-digital-divide'>news articles<\/a> reveal that while internet access can be a luxury to some families, they were leveraging the free offering from some public places, like caf\u00e9 . The shutdown of those public places due to the break of the pandemic dramatically hindered those families in their access to internet.<\/li>\n    <\/ul>\n    <\/p>\n    <h3>Second, State Level Case Study<\/h3>\n    <p>Aggregating the data into State level, we found that among the top four states covered in the data, Connecticut stands out as the one being hit the least. <a href = 'https:\/\/portal.ct.gov\/Coronavirus\/Information-For\/Internet-Access'> Connecticut's COIVD-19 Response page<\/a> indicates that this may be due to an emergency internet access assistance program implemented at the state level. Consistent with our conjecture above, the internet accessibility can be a key driver to the student\u2019s access to the learning platform. <\/p>\n    <h3>Third, Weekend vs. Weekdays<\/h3>\n    <p>An interesting finding from our time series analysis reveals that although the difference in access to the platform during the weekdays vs. weekends was much higher before the pandemic hits, that difference is significantly decreased after. This may be the result of the additional flexibility brought by the pandemic, and the reduced outdoor opportunities during the weekend due to the shutdown orders and concerns of big crowds. <\/p>\n    <h3>Forth, Difference in Difference (DID) Analysis<\/h3>\n    <p>Leveraging the DID approach, we measure the differential impact on school district with different economic situations, before and after the pandemic. We find that:<\/p>\n <ol>\n    <li>School districts with higher free lunch rate (poorer schools) have lower product access rate during in pandemic period than school districts with lower free lunch rate (more resourceful schools). This may have many different causes:\n        <ul>\n        <li>Schools from poorer regions are less prepared to move online, due to the school's lack of IT resources and the teacher's lack of IT pedagogical skills.\n        <\/li>\n        <li>Students from poorer regions are less prepared to move online, due to the lack of resources such as<a href\uff1a\"https:\/\/blogs.microsoft.com\/on-the-issues\/2019\/07\/31\/signs-of-progress-on-broadband-mapping-but-more-work-still-to-do\/\">broadband<\/a>,and <a href:\"https:\/\/www.frontiersin.org\/articles\/10.3389\/fpsyg.2020.592670\/full\">self motivation<\/a>.\n        <\/li>\n        <\/ul>\n    <\/li>\n    <li>School districts with the highest free lunch rate and highest black hispanic ratio have the lowest product access rate. This means that in terms of regions with the highest free lunch rate, those whom have a higher black hispanic ratio perform worse.<\/li>\n    \n<\/ol>\n    <h3>Fifth, Causal Analysis<\/h3>\n    <p>Based on the previous findings, the unique help to internet access offered by the state of Connecticut seems to play an important role in reducing the negative impact of the pandemic hit. To understand the causal influence of such a policy, we employed the approach of synthetic control approach to understand the causal influence of such special state level policy. Leveraging the free access of the <a href = 'https:\/\/pypi.org\/project\/pycausalimpact\/'>Causal Impact package<\/a>, we found that the positive effect of pct_access_sum by Connecticut observed during the intervention period (the beginning of COVID) is statistically significant and unlikely to be due to random fluctuations.<\/p>\n\n<h2>Suggestions for Education Policy<\/h2>\n<p>We suggest that following policy change could significanly close the gap of the online learning engagement.<\/p>\n<ol>\n   <li>Equity investment to different district. Focus funding to school districts with less expenditure.<\/li>\n    <li>Internet accessibility is a \"Human Right\". Increase support to provide internet access to families, especially in poor communities with high free\/reduce lunch ratio.<\/li>\n    <li>In coordination with increase support of internet access, schools in high free\/reduce lunch ratio districts should also provide training in basic IT skills and training in digital citizenship. Communities in poor districts should acknowledge their digital rights.\n<\/ol>\n","b548f1d9":"<h2>Regression Results<\/h2>\n\n<table style=\"width:100%\">\n  <tr>\n    <th><\/th>\n    <th>$Y_0$ pct_access_sum<\/th>\n    <th>$Y_1$ engagement_index_mean<\/th>\n  <\/tr>\n  <tr><td colspan=\"3\"><hr><\/td><\/tr>\n  <tr>\n    <td>Intercept<\/td>\n    <td><center>$\\begin{array}{column} 151.9224^{***} \\\\ (4.832)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} 342.6838^{***} \\\\ (17.718)\\end{array}$<\/td>\n  <\/tr>\n  <tr>\n    <td>pandemic_stage_beginning_of_COVID<\/td>\n    <td><center>$\\begin{array}{column} -4.9724 \\\\ (4.735)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} 86.9663^{***} \\\\ (17.315)\\end{array}$<\/td>\n  <\/tr>\n  <tr>\n    <td>pandemic_stage_new_school_year<\/td>\n    <td><center>$\\begin{array}{column} 15.73804^{***} \\\\ (4.731)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} 83.7838^{***} \\\\ (17.227)\\end{array}$<\/td>\n  <\/tr>\n  <tr>\n    <td>Week_End <\/td>\n    <td><center>$\\begin{array}{column} -132.0966^{***} \\\\ (3.865)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} -275.1784^{***} \\\\ (14.096)\\end{array}$<\/td>\n    <\/tr>\n  <tr>\n    <td>pct_free_reduced_0204<\/td>\n    <td><center>$\\begin{array}{column} -19.9182^{***} \\\\ (5.060)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} -31.4833^{*} \\\\ (18.444)\\end{array}$<\/td>\n  <\/tr>\n  <tr>\n    <td>pct_free_reduced_0406<\/td>\n    <td><center>$\\begin{array}{column} -15.2596^{***} \\\\ (5.713)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} -15.9280 \\\\ (20.795)\\end{array}$<\/td>\n  <\/tr>\n  <tr>\n    <td>pct_free_reduced_0610<\/td>\n    <td><center>$\\begin{array}{column} -19.9566^{**} \\\\ (9.045)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} 10.1253 \\\\ (33.168)\\end{array}$<\/td>\n    <\/tr>\n  <tr>\n    <td>pct_black_hispanic_0204<\/td>\n    <td><center>$\\begin{array}{column} -3.1836 \\\\ (5.901)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} -61.2751^{***} \\\\ (21.348)\\end{array}$<\/td>\n  <\/tr>\n    <tr>\n    <td>pct_black_hispanic_0410<\/td>\n    <td><center>$\\begin{array}{column} -8.9438 \\\\ (6.801)\\end{array}$<\/td>\n    <td><center>$\\begin{array}{column} -85.4458^{***} \\\\ (25.267)\\end{array}$<\/td>\n  <\/tr>\n   <tr><td colspan=\"3\"><hr><\/td><\/tr>\n    <tr>\n    <td>R-squared:<\/td>\n    <td><center>$0.586$<\/td>\n    <td><center>$0.343$<\/td>\n  <\/tr>\n    <tr>\n    <td>F-statistics:<\/td>\n    <td><center>$152.2$<\/td>\n    <td><center>$55.2$<\/td>\n  <\/tr>\n<\/table>\n<p> Standard errors are in parenthesis.<\/p>\n<p> $ \\ast\\ast\\ast \\quad   p<0.01$,     $ \\ast\\ast \\quad   p<0.05$,     $ \\ast \\quad   p<0.1$<\/p>","653fb066":"<h1> LearnPlatform COVID-19 Impact on Digital Learning <\/h1>\n\n<center><img src=\"https:\/\/i.imgur.com\/z83jkm1.jpeg\"><\/center>\n\n<h2> Contributors: <\/h2>\n<ul>\n  <li>Alexandre Wang: https:\/\/www.kaggle.com\/wangalexandre<\/li>\n  <li>Jesse Yao: https:\/\/www.kaggle.com\/jessetyao<\/li>\n  <li>Ranya Zhang: https:\/\/www.kaggle.com\/ranyaz<\/li>\n  <li>Amy Dong: https:\/\/www.kaggle.com\/xhshuju<\/li>\n<\/ul>  \n\n\n<h2> Table of Content: <\/h2>\n<ol>\n<li><a href='#Executive_Summary'>Executive Summary<\/a><\/li>\n<li><a href='#data'>Data Cleansing<\/a><\/li>\n<li><a href='#Heterogeneous'>Heterogeneous Impact of COVID-19<\/a><\/li>\n<li><a href='#engagement_did'>Difference in Difference (DID) Analysis<\/a><\/li>\n<li><a href='#Causal'>Causal Analysis for State Help of Internet Access<\/a><\/li>\n<\/ol>\n\n\n"}}