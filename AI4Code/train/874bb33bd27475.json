{"cell_type":{"48cb8839":"code","0a908a79":"code","57be9bd6":"code","d3436e6e":"code","9ff669bd":"code","bb4779e3":"code","8c675f9a":"code","1dbdcd57":"code","c3a91933":"code","5db5e37d":"code","c2e34b83":"code","63ecc088":"code","3c6fbc75":"code","c35c758d":"code","038ff56a":"code","2a4edf8c":"code","d4b89023":"code","677f3002":"code","04c61fff":"code","254a20d1":"code","2a3c1389":"code","173e273d":"code","72e44043":"code","91938d2a":"code","401fd2a5":"code","e6c82870":"code","0665a490":"code","5bc34967":"code","3a0a26c7":"code","f9ae9c17":"code","c9af5e12":"code","bb1ad402":"code","d8cb1006":"code","b6e8bb87":"code","b3dcad6a":"code","19533e75":"code","2c90c597":"code","a10e3899":"code","633107c4":"code","32fd945b":"code","3d05da4a":"code","6077afe5":"code","018abdae":"code","f6f4104f":"code","856fc58e":"code","0a524506":"markdown","4030591d":"markdown","8fcf3610":"markdown","8afee909":"markdown","df8f59de":"markdown","6eafd8cb":"markdown","0e5a7980":"markdown","26618372":"markdown","41e5de66":"markdown","67b4ae30":"markdown","f52fbe21":"markdown","3b38286b":"markdown","3972da19":"markdown","e3d4bec4":"markdown","6c9fce7a":"markdown","1adad42f":"markdown","9d8c0115":"markdown","8e5c883c":"markdown","9df9391a":"markdown","a2f02f19":"markdown"},"source":{"48cb8839":"!pip install -U -q wandb","0a908a79":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\n\nimport wandb\nfrom wandb.keras import WandbCallback","57be9bd6":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","d3436e6e":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","9ff669bd":"%%time\ntrain = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv').set_index(\"Id\")\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv').set_index(\"Id\")\npseudo = pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv').set_index(\"Id\")\n\ntrain = pd.concat([train, pseudo], axis=0)\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\nfeature_cols = test.columns.tolist()\ncnt_cols = [col for col in feature_cols if (not col.startswith(\"Soil_Type\")) and (not col.startswith(\"Wilderness_Area\"))]\nbin_cols = [col for col in feature_cols if col not in cnt_cols]","bb4779e3":"plt.figure(figsize=(10,5))\naxs = sns.countplot(x=\"Cover_Type\", data=train)\nplt.xlabel(\"Cover Type\")\naxs.bar_label(axs.containers[0])\nplt.show()","8c675f9a":"train[\"Cover_Type\"] = train[\"Cover_Type\"] - 1","1dbdcd57":"for col in feature_cols:\n    if col in cnt_cols:\n        train[col] = train[col].astype(\"float32\")\n        test[col] = test[col].astype(\"float32\")\n    else:\n        train[col] = train[col].astype(\"bool\")\n        test[col] = test[col].astype(\"bool\")","c3a91933":"train.describe().T","5db5e37d":"%%time\nisf = IsolationForest(random_state=42)\ntrain[\"outlier_isf\"] = isf.fit_predict(train[feature_cols])\ntest[\"outlier_isf\"] = isf.predict(test[feature_cols])\n\nprint(train[\"outlier_isf\"].value_counts())\nprint(test[\"outlier_isf\"].value_counts())","c2e34b83":"train[\"outlier_isf\"] = train[\"outlier_isf\"] == -1\ntest[\"outlier_isf\"] = test[\"outlier_isf\"] == -1\n\ntrain[\"outlier_isf\"] = train[\"outlier_isf\"].astype(\"bool\")\ntest[\"outlier_isf\"] = test[\"outlier_isf\"].astype(\"bool\")","63ecc088":"plt.figure(figsize=(10,5))\naxs = sns.countplot(x=train.loc[train.outlier_isf==True,\"Cover_Type\"])\naxs.bar_label(axs.containers[0])\nplt.title(\"Outliers Count Isolation Forest\")\nplt.show()","3c6fbc75":"del isf\n_ = gc.collect()","c35c758d":"feature_cols.append(\"outlier_isf\")\nbin_cols.append(\"outlier_isf\")","038ff56a":"sc = StandardScaler()\nx = train.copy()\nt = test.copy()\nx[cnt_cols] = sc.fit_transform(x[cnt_cols])\nt[cnt_cols] = sc.transform(t[cnt_cols])","2a4edf8c":"%%time\nn_clusters = 14\ncd_feature = False # cluster distance instead of cluster number  \n\nkmeans = MiniBatchKMeans(n_clusters=n_clusters, max_iter=300, batch_size=256*5, random_state=42)\n\nif cd_feature:\n    cluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\n    \n    X_cd = kmeans.fit_transform(x[feature_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=x.index)\n    train = train.join(X_cd)\n    \n    X_cd = kmeans.transform(t[feature_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=t.index)\n    test = test.join(X_cd)\n\nelse:\n    cluster_cols = [\"cluster\"]  \n    train[\"cluster\"] = kmeans.fit_predict(x[feature_cols])\n    test[\"cluster\"] = kmeans.predict(t[feature_cols])\n    \n\nfeature_cols += cluster_cols\n\ntrain.head()","d4b89023":"plt.figure(figsize=(20,8))\nax = sns.countplot(x=\"cluster\", data=train, hue=\"Cover_Type\")\nplt.xlabel(\"Clusters\")\nplt.show()","677f3002":"x[cluster_cols] = train[cluster_cols].copy()\nt[cluster_cols] = test[cluster_cols].copy()","04c61fff":"pca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(x[feature_cols])\nT_pca = pca.transform(t[feature_cols])\n\npca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n\nX_pca = pd.DataFrame(X_pca, columns=pca_cols, index=train.index)\nT_pca = pd.DataFrame(T_pca, columns=pca_cols, index=test.index)\n\ntrain = pd.concat([train, X_pca], axis=1)\ntest = pd.concat([test, T_pca], axis=1)\ntrain.head()","254a20d1":"del x, t, X_pca, T_pca\n_ = gc.collect()","2a3c1389":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=train, x=\"PC1\", y=\"PC2\", hue=\"Cover_Type\", alpha=0.8, palette=\"deep\")\nplt.show()","173e273d":"feature_cols += [\"PC1\", \"PC2\"]","72e44043":"train[\"likely_type3\"] = train[\"PC2\"] < -2.2\ntrain[\"likely_type2\"] = (train[\"PC2\"] < 0) & (train[\"PC2\"] > -2.2)\ntrain[\"likely_type7\"] = train[\"PC2\"] > 3.9\ntrain[\"likely_type1\"] = (train[\"PC2\"] > 1) & (train[\"PC2\"] < 4)\n\ntest[\"likely_type3\"] = test[\"PC2\"] < -2.2\ntest[\"likely_type2\"] = (test[\"PC2\"] < 0) & (train[\"PC2\"] > -2.2)\ntest[\"likely_type7\"] = test[\"PC2\"] > 3.9\ntest[\"likely_type1\"] = (test[\"PC2\"] > 1) & (train[\"PC2\"] < 4)","91938d2a":"feature_cols += [\"likely_type3\", \"likely_type2\", \"likely_type7\", \"likely_type1\"]\nbin_cols += [\"likely_type3\", \"likely_type2\", \"likely_type7\", \"likely_type1\"]","401fd2a5":"def r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ntrain['Aspect2'] = train.Aspect.map(r)\ntest['Aspect2'] = test.Aspect.map(r)\n\ntrain.loc[train[\"Aspect\"] < 0, \"Aspect\"] += 360\ntest.loc[test[\"Aspect\"] < 0, \"Aspect\"] += 360\n\ntrain.loc[train[\"Aspect\"] > 359, \"Aspect\"] -= 360\ntest.loc[test[\"Aspect\"] > 359, \"Aspect\"] -= 360","e6c82870":"train['Highwater'] = train.Vertical_Distance_To_Hydrology < 0\ntest['Highwater'] = test.Vertical_Distance_To_Hydrology < 0\n\ntrain['DistHydro'] = train.Horizontal_Distance_To_Hydrology < 0\ntest['DistHydro'] = test.Horizontal_Distance_To_Hydrology < 0\n\ntrain['DistRoad'] = train.Horizontal_Distance_To_Roadways < 0\ntest['DistRoad'] = test.Horizontal_Distance_To_Roadways < 0\n\ntrain['DistFire'] = train.Horizontal_Distance_To_Fire_Points < 0\ntest['DistFire'] = test.Horizontal_Distance_To_Fire_Points < 0\n\ntrain['Hillshade_3pm_is_zero'] = train.Hillshade_3pm == 0\ntest['Hillshade_3pm_is_zero'] = test.Hillshade_3pm == 0","0665a490":"train['EHiElv'] = train['Horizontal_Distance_To_Roadways'] * train['Elevation']\ntest['EHiElv'] = test['Horizontal_Distance_To_Roadways'] * test['Elevation']\n\ntrain['EViElv'] = train['Vertical_Distance_To_Hydrology'] * train['Elevation']\ntest['EViElv'] = test['Vertical_Distance_To_Hydrology'] * test['Elevation']","5bc34967":"train['EVDtH'] = train.Elevation-train.Vertical_Distance_To_Hydrology\ntest['EVDtH'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\ntrain['EHDtH'] = train.Elevation-train.Horizontal_Distance_To_Hydrology*0.2\ntest['EHDtH'] = test.Elevation-test.Horizontal_Distance_To_Hydrology*0.2","3a0a26c7":"train['Distanse_to_Hydrolody'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\ntest['Distanse_to_Hydrolody'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\n\ntrain['Hydro_Fire_1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntest['Hydro_Fire_1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\n\ntrain['Hydro_Fire_2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntest['Hydro_Fire_2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n\ntrain['Hydro_Road_1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntest['Hydro_Road_1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n\ntrain['Hydro_Road_2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntest['Hydro_Road_2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n\ntrain['Fire_Road_1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntest['Fire_Road_1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n\ntrain['Fire_Road_2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntest['Fire_Road_2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])","f9ae9c17":"train[\"new_f1\"] = train[\"Elevation\"] + train[\"Horizontal_Distance_To_Roadways\"] + train[\"Horizontal_Distance_To_Fire_Points\"]\ntest[\"new_f1\"] = test[\"Elevation\"] + test[\"Horizontal_Distance_To_Roadways\"] + test[\"Horizontal_Distance_To_Fire_Points\"]\n\ntrain[\"new_f2\"] = (train[\"Hillshade_Noon\"] + train[\"Hillshade_3pm\"]) - train[\"Hillshade_9am\"]\ntest[\"new_f2\"] = (test[\"Hillshade_Noon\"] + test[\"Hillshade_3pm\"]) - test[\"Hillshade_9am\"]","c9af5e12":"train.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","bb1ad402":"feature_cols += [\"new_f1\", \"new_f2\", \"Aspect2\", \"Highwater\", \"EVDtH\", \"EHDtH\",  'EHiElv', 'EViElv', 'Hillshade_3pm_is_zero',\n                 \"Distanse_to_Hydrolody\", \"Hydro_Fire_1\", \"Hydro_Fire_2\", \"Hydro_Road_1\", \"Hydro_Road_2\", \"Fire_Road_1\", \"Fire_Road_2\"]\ncnt_cols += [\"new_f1\", \"new_f2\", \"Aspect2\", \"EVDtH\", \"EHDtH\", 'EHiElv', 'EViElv', \n                 \"Distanse_to_Hydrolody\", \"Hydro_Fire_1\", \"Hydro_Fire_2\", \"Hydro_Road_1\", \"Hydro_Road_2\", \"Fire_Road_1\", \"Fire_Road_2\"]\nbin_cols += [\"Highwater\", 'Hillshade_3pm_is_zero']","d8cb1006":"%%time\nx = train.iloc[:5000,:][feature_cols].copy()\ny = train.iloc[:5000,:]['Cover_Type'].copy()\nmi_scores = mutual_info_regression(x, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","b6e8bb87":"top = 10\nplt.figure(figsize=(20,7))\nfig = sns.barplot(x=mi_scores.values[:top], y=mi_scores.index[:top], palette=\"summer\")\nplt.title(f\"Top {top} Strong Relationships Between Feature Columns and Target Column\")\nplt.xlabel(\"Relationship with Target\")\nplt.ylabel(\"Feature Columns\")\nplt.savefig(\"mi_scores.png\")\nplt.show()","b3dcad6a":"sc = StandardScaler()\ntrain[cnt_cols] = sc.fit_transform(train[cnt_cols]).astype(np.float32)\ntest[cnt_cols] = sc.transform(test[cnt_cols]).astype(np.float32)","19533e75":"cnt_cols += cluster_cols\ncnt_cols += [\"PC1\", \"PC2\"]","2c90c597":"x_cnt = train[cnt_cols].values.astype(np.float32)\nx_bin = train[bin_cols].values.astype(np.float32)\ny  = train['Cover_Type'].values","a10e3899":"def get_model():\n    AF = \"selu\"\n    KI = \"lecun_normal\"\n    input_1 = layers.Input(shape=(x_cnt.shape[-1]), name=\"continuous\")\n    x_1 = layers.Dense(128, activation=AF, kernel_initializer=KI)(input_1)\n    x_1 = layers.BatchNormalization()(x_1)\n    x_1 = layers.Dense(128, activation=AF, kernel_initializer=KI)(x_1)\n    x_1 = layers.BatchNormalization()(x_1)\n    x_1 = layers.Dense(128, activation=AF, kernel_initializer=KI)(x_1)\n    x_1 = layers.BatchNormalization()(x_1)\n    \n    input_2 = layers.Input(shape=x_bin.shape[-1], name=\"categories\")\n    x_2 = layers.Dense(128, activation=AF, kernel_initializer=KI)(input_2)\n    x_2 = layers.BatchNormalization()(x_2)\n    x_2 = layers.Dense(128, activation=AF, kernel_initializer=KI)(x_2)\n    x_2 = layers.BatchNormalization()(x_2)\n    x_2 = layers.Dense(128, activation=AF, kernel_initializer=KI)(x_2)\n    x_2 = layers.BatchNormalization()(x_2)\n\n\n    x = layers.Concatenate()([x_1,x_2])\n    x = layers.Dense(128, activation=AF, kernel_initializer=KI)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(128, activation=AF, kernel_initializer=KI)(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(7, activation=\"softmax\", name=\"output\")(x)\n\n    model = tf.keras.Model([input_1,input_2], output)\n    return model\n\nwith strategy.scope():\n    model = get_model()\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n    \ntf.keras.utils.plot_model(model, show_shapes=True)","633107c4":"CONFIG = dict(competition=\"TPS Dec\",  \n              Notebook=\"TPS Dec 2021 - TensorFlow NN (TPU) and W&B\", \n              Desc=\"Added Unsupervised Features - Added Pseudo\")\nrun = wandb.init(project=\"TPS_Dec\", name=\"log_unsupervised_pseudo\", entity=\"kaveh\", anonymous=anony, config=CONFIG)\n\nwandb.config = {\n  \"learning_rate\": 0.001,\n  \"epochs\": 40,\n  \"batch_size\": 1024,\n}\n\nwandb.log({\"MI scores of features\": wandb.Image(\".\/mi_scores.png\")})\nwandb.log({\"Model Architecture\": wandb.Image(\".\/model.png\")})","32fd945b":"cb_wb = WandbCallback()\ncb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1)\ncb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=2, mode=\"max\", min_lr=0.00005, verbose=1)\n\nhistory = model.fit((x_cnt, x_bin), \n                    y, \n                    epochs=40, \n                    validation_split=0.2, \n                    batch_size=1024, \n                    validation_batch_size=1024,\n                    callbacks=[cb_es, cb_lr, cb_wb])","3d05da4a":"plt.figure(figsize=(25,7))\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1) \nax1 = plt.subplot(1,2,1)\nax1.plot(epochs, acc, 'r')\nax1.plot(epochs, val_acc, 'b')\nax1.set_xticks([i for i in epochs])\nax1.set_title('Training and validation Accuracy')\nax1.legend([\"Training\", \"Validation\" ])\nax1.set_xlabel(\"epochs\")\nax1.set_ylabel(\"Accuracy\")\n\nax2 = plt.subplot(1,2,2)\nax2.plot(epochs, loss, 'r')\nax2.plot(epochs, val_loss, 'b')\nax2.set_xticks([i for i in epochs])\nax2.legend([\"Training\", \"Validation\" ])\nax2.set_xlabel(\"Epochs\")\nax2.set_ylabel(\"Loss\")\nax2.set_title('Training and validation loss')\n\nplt.show()","6077afe5":"preds = model.predict((test[cnt_cols].values.astype(np.float32), test[bin_cols].values.astype(np.float32)))\np = np.argmax(preds, axis=1) + 1","018abdae":"plt.figure(figsize=(10,5))\nax = sns.countplot(x=p)\nplt.title(\"Predictions\")\nplt.xlabel(\"Cover Type\")\nax.bar_label(ax.containers[0])\nplt.savefig(\"predictions.png\")\nplt.show()","f6f4104f":"wandb.log({\"Predictions Stats\": wandb.Image(\".\/predictions.png\")})\nwandb.finish()","856fc58e":"sample_submission['Cover_Type'] = p\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","0a524506":"# Add More Features\n\nThanks to [@lucamassaron](https:\/\/www.kaggle.com\/lucamassaron) for [this discussion](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291839).","4030591d":"# WandB Login\n\nThanks to [@usharengaraju](https:\/\/www.kaggle.com\/usharengaraju) for her useful notebooks with W&B","8fcf3610":"# Config TPU","8afee909":"## Define Model","df8f59de":"# PCA","6eafd8cb":"## Train Model","0e5a7980":"![wandb](https:\/\/i.imgur.com\/gb6B4ig.png)","26618372":"# Neural Network Model","41e5de66":"# Isolation Forest (Outlier Detection)\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.","67b4ae30":"# Mutual Information","f52fbe21":"# Scale Data","3b38286b":"# Submission","3972da19":"# Reduce Memory Usage","e3d4bec4":"# Load Data","6c9fce7a":"## Prepare Data","1adad42f":"## WandB Log","9d8c0115":"# Predict","8e5c883c":"## Plot Metrics","9df9391a":"# MiniBatch KMeans\n\nThe MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.","a2f02f19":"You can find **Pseudo Labels** Dataset [here](https:\/\/www.kaggle.com\/remekkinas\/tps12-pseudolabels) by [@remekkinas](https:\/\/www.kaggle.com\/remekkinas)"}}