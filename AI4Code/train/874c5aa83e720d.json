{"cell_type":{"835d35b6":"code","fc28ee25":"code","3dfb5d95":"code","bea7ea4e":"code","042a9bbc":"code","0766a11c":"code","19d5ef30":"code","c1f98296":"code","7460b15a":"code","4ee3807d":"code","875d342d":"code","85c74309":"code","cd8d6530":"code","02c5c0fd":"code","61f32c69":"code","52a0eab5":"code","1b4031eb":"code","ff5cd716":"code","5fdee4b3":"code","c2620aae":"code","6b65a014":"code","9a3cdbbf":"code","b14db07d":"code","6fddf251":"code","dc9f834d":"code","60fdcd7c":"code","25940120":"code","3d7b2401":"code","7a52ea8b":"code","a13436fb":"code","5bd20aa5":"code","bb0e1bcd":"code","93ec2815":"code","691b9f6b":"code","488a43e5":"code","506f8cdf":"code","f8f257f7":"code","b5fb6013":"markdown","51e01e99":"markdown","4d992a7b":"markdown","3e036df2":"markdown","2d356fa1":"markdown","cdf9c067":"markdown","d51fdc31":"markdown","d0afb4b0":"markdown","6099e270":"markdown","a1406131":"markdown","a536e40a":"markdown","d6958ae4":"markdown","8eb0def5":"markdown","74d4d8e7":"markdown","5ce3fddf":"markdown","7be6e6af":"markdown","aa6a4164":"markdown","445fc435":"markdown","d961362b":"markdown","962b27da":"markdown","1658f55c":"markdown","50584d9b":"markdown","64d93d80":"markdown","9a37c1d1":"markdown","60cc6f78":"markdown"},"source":{"835d35b6":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","fc28ee25":"print(f'Train Shape :  {train.shape}')\nprint(f'Test Shape :  {test.shape}')","3dfb5d95":"target = train['loss']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","bea7ea4e":"train.head(2)","042a9bbc":"test.head(2)","0766a11c":"train.info()","19d5ef30":"test.info(max_cols=10)","c1f98296":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_cnt = train['loss'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt, color=['#799EFF' if i%2==0 else '#CCDAFF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(10):\n    ax.annotate(f'{target_cnt[i]\/len(train)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',\n               )\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","7460b15a":"target_cnt_df = pd.DataFrame(target_cnt)\ntarget_cnt_df['ratio(%)'] = target_cnt_df\/target_cnt.sum()*100\ntarget_cnt_df.sort_values('ratio(%)', ascending=False, inplace=True)\ntarget_cnt_df['cummulated_sum(%)'] = target_cnt_df['ratio(%)'].cumsum()\ntarget_cnt_df.style.bar(subset=['cummulated_sum(%)'], color='#CCDAFF').background_gradient(subset=['ratio(%)'], cmap='binary')\n# target_cnt_df.style.bar(subset=['ratio(%)'], color='#799EFF')","4ee3807d":"train.describe()","875d342d":"discrete_features = []\n\nfor col in train.columns:\n    if np.array_equal(train[col].values, train[col].values.astype(int)):\n        discrete_features.append(col)\n\nprint(f'Total {len(discrete_features)} : ')\nfor dcol in discrete_features:\n    print(f'{dcol} unique value : {train[dcol].nunique()}')","85c74309":"f1_loss = train.groupby(['f1'])['loss'].mean().sort_values()\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f1_loss)), f1_loss, alpha=0.7, color='#799EFF', label='Train Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f1', loc='left', fontweight='bold')\nax.legend()\nplt.show()","cd8d6530":"f86_loss = train.groupby(['f86'])['loss'].mean().sort_values()\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f86_loss)), f86_loss, alpha=0.7, color='#799EFF', label='Train Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f86', loc='left', fontweight='bold')\nax.legend()\nplt.show()","02c5c0fd":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","61f32c69":"from matplotlib.pyplot import cm\nfig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, vmin=-0.5, vmax=0.5, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 220, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","52a0eab5":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True,color = '#799EFF',\n                ax=ax)\n#     sns.kdeplot(data=test, x=f'f{idx}', \n#                 fill=True,color = 'grey',\n#                 ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature) Train Dataset', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","1b4031eb":"import warnings\nwarnings.filterwarnings('ignore')\nfig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.boxplot(train[train.columns.tolist()[:100][i]], linewidth = 2,color = '#799EFF',saturation=1)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","ff5cd716":"fig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 220, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold',)     \n\nplt.show()","5fdee4b3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\ny = train['loss']\ntrain.drop('loss',axis=1,inplace=True)\nfeatures = []\nfor feature in train.columns:\n    features.append(feature)\n# print(features)","c2620aae":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\ntrain[features] = mm.fit_transform(train[features])\ntest[features] = mm.transform(test[features])\nX = train","6b65a014":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    \n    model = LGBMRegressor(**params, random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","9a3cdbbf":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","b14db07d":"lgb_params = {'reg_alpha': 0.4972562469417825, 'reg_lambda': 0.3273637203281044, \n          'num_leaves': 50, 'learning_rate': 0.032108486615557354, \n          'max_depth': 40, 'n_estimators': 4060, \n          'min_child_weight': 0.0173353329222102,\n          'subsample': 0.9493343850444064, \n          'colsample_bytree': 0.5328221263825876, 'min_child_samples': 80,'device':'gpu'}\nlgb_params","6fddf251":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.values[train_idx], y.values[train_idx]\n        x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","dc9f834d":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","60fdcd7c":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter.\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4)\n    } \n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","25940120":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","3d7b2401":"xgb_params = {'tweedie_variance_power': 2.0,\n 'max_depth': 9,\n 'n_estimators': 4000,\n 'eta': 0.01200085275863839,\n 'subsample': 0.8,\n 'colsample_bytree': 0.7,\n 'colsample_bylevel': 0.4,\n 'min_child_weight': 2.824928835841522,\n 'reg_lambda': 67.43522142240646,\n 'reg_alpha': 0.00012103217663028774,\n 'gamma': 0.012432559904494572,'tree_method':'gpu_hist'}\nxgb_params","7a52ea8b":"xgb_model = cross_val(X, y, XGBRegressor, xgb_params)","a13436fb":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.03 , 0.04),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.32 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    \n    \n    model = CatBoostRegressor(**params,task_type='GPU', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","5bd20aa5":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","bb0e1bcd":"cat_params = {'iterations': 1224,\n 'od_wait': 1243,\n 'learning_rate': 0.03632022350716054,\n 'reg_lambda': 0.3257139588327784,\n 'subsample': 0.9741256425198503,\n 'random_strength': 41.06792107841663,\n 'depth': 12,\n 'min_data_in_leaf': 27,\n 'leaf_estimation_iterations': 10,'task_type':'GPU'}\ncat_params","93ec2815":"cat_model = cross_val(X, y, CatBoostRegressor, cat_params)","691b9f6b":"cat = CatBoostRegressor(**cat_params)\nlgb = LGBMRegressor(**lgb_params)\nxgb = XGBRegressor(**xgb_params)","488a43e5":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n                ('xgb', xgb)\n            ],\n            weights = [0.15, 0.65]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits","506f8cdf":"sub['loss'] = lgb_model.predict(test)\nsub.to_csv(f'lgb.csv',index = False)\n\nsub['loss'] = xgb_model.predict(test)\nsub.to_csv(f'xgb.csv',index = False)\n\nsub['loss'] = cat_model.predict(test)\nsub.to_csv(f'cat.csv',index = False)\n\nsub['loss'] = predictions\nsub.to_csv(f'vote.csv',index = False)","f8f257f7":"%%html\n<marquee style='width: 90% ;height:70%; color: #799EFF ;'>\n    <b> Do UPVOTE if you like my work, I will be adding some more content to this kernel :) <\/b><\/marquee>","b5fb6013":"# Submission","51e01e99":"#### Observations\n- Depending on the value of f1, we can check the imbalance of loss.\n- In 5 cases, we confirmed that the loss is all 0.","4d992a7b":"#### Observations:\n- The data is very diverse as you can see \n- Feature 0 and 4 can show it very easily, if you do deep dive you will see feature 16,52,60,75,91 also showing very high values and also very high standard deviation \n- There seem to be some data points with discrete values (integer values)\n - f1\n - f16\n - f27\n - f55\n - f60\n - f86\n","3e036df2":"## Credits to the codes that have helped me make this notebook: \n- [Notebook by Subin An ](https:\/\/www.kaggle.com\/subinium\/tps-aug-simple-eda)\n- [Notebook by BIZEN](https:\/\/www.kaggle.com\/hiro5299834\/tps-aug-2021-lgbm-xgb-catboost)","2d356fa1":"#### Observations:\n- It's scaled up, but it's a pretty interesting aspect of the data.\n\n- It is safe to assume that the distributions of train and test are almost the same.","cdf9c067":"#### Observations\n- There are a total of 43 discrete losses.\n- The top 12 distributions account for 80% of the total.\n- All except the order of 2 and 1 are in increasing order.","d51fdc31":"#### Observations:\n- While the total number of data is 250000, most of the data in f16 and f60 are confirmed as continuous with different values\n- But the remaining f1, f27, f55, and f86 look relatively categorical.\n\n#### Looking at f1 and f86 with a small number of unique values: For the relationship with the loss, we averaged after groupby.","d0afb4b0":"### Target & Feature Relation\n- As the value of targets increases, the mean moves away from zero.","6099e270":"#### Observations: \n- We have 101 columns + 1 target column\n- We have a total data of 250k for the train data and 150k for the test data","a1406131":"### A deeper dive into these 6 features with discrete values","a536e40a":"# Feature Distribution","d6958ae4":"#### I have divided this Notebook in two parts:\n- EDA\n- AutoML (I am just learning this)","8eb0def5":"# Min Max Scaler","74d4d8e7":"# Importing Libraries and Data for the EDA ","5ce3fddf":"# Scaling the data","7be6e6af":"#### Observations:\n- Most correlations are close to 0","aa6a4164":"# Info about the train and the test data","445fc435":"# Statistics Check\nThe scale of this data is really diverse. Which makes me think that scaling should be done in this case. Usually we don't need to scaled data if we're using a tree-based model but it is important in case the data is as diverse as this here! \n","d961362b":"# We have a ton of features in this competition, should be fun! \n\n### Initial Observations \n- No missing value.\n- There are 100 numerical continuous features.\n- The target variable loss ranges from 0 to 42 for a total of 43 discrete values. \n- However, this is a regression problem and it is OK to submit as decimal values. ***But can we do a regression + classification?***","962b27da":"# LightGBM","1658f55c":"# CatBoost","50584d9b":"#### Now since we are done with the inital data exploration, Let's have a look at the Target Variable Distribution to get an understanding of how the target values are spread ","64d93d80":"#### Having a look at the top 2 rows of the train and the test data","9a37c1d1":"# XGBoost","60cc6f78":"# Now we do the Modelling "}}