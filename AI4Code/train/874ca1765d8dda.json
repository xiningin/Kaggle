{"cell_type":{"e3d52343":"code","ccf0fd1d":"code","2031d653":"code","1beeac00":"code","d63ac968":"code","acb67b01":"code","7631f56d":"code","db0272c4":"code","941af556":"code","65303491":"code","61e242d8":"code","bf1ba6e6":"code","bef8c783":"code","930048e5":"code","e2368319":"code","ee88301f":"code","aaccd0c6":"code","2f02a13e":"code","29d2f5a6":"code","d9d601a8":"code","afa6f0ba":"code","9f9f1d9b":"code","7fb3f304":"code","8a685772":"markdown","d0d23fb7":"markdown","4843d734":"markdown","2d9b2df4":"markdown","bf13a2ab":"markdown","d4192032":"markdown","4e8430aa":"markdown","744e3f7c":"markdown","7934cb3c":"markdown","d701ce5a":"markdown","8db8c446":"markdown","87258345":"markdown","fb491d42":"markdown","8783bd3a":"markdown","0ce719a3":"markdown","1981e58d":"markdown","df9c4ef2":"markdown"},"source":{"e3d52343":"import tensorflow_datasets as tfds\nimport tensorflow as tf\n\ntfds.disable_progress_bar()","ccf0fd1d":"dataset, info = tfds.load('imdb_reviews\/subwords8k', \n                          with_info=True,\n                          as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']","2031d653":"encoder = info.features['text'].encoder\nprint('Vocabulary Size: ', encoder.vocab_size)","1beeac00":"sample_string = 'Hello Dear Kegglers :) '\n\nencoded_string = encoder.encode(sample_string)\nprint('Encoded string is {}'.format(encoded_string))\n\noriginal_string = encoder.decode(encoded_string)\nprint('The original string: \"{}\"'.format(original_string))","d63ac968":"assert original_string == sample_string\nfor index in encoded_string:\n  print('{} ----> {}'.format(index, encoder.decode([index])))","acb67b01":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64","7631f56d":"train_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE)\n\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE)","db0272c4":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","941af556":"from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\ndef build_lrfn(lr_start=5e-5, lr_max=1e-4, \n               lr_min=0, lr_rampup_epochs=12, \n               lr_sustain_epochs=2, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=True)\n\ncheckpoint1 = ModelCheckpoint(\n    filepath='best_weights_single_LSTM.hdf5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\ncheckpoint2 = ModelCheckpoint(\n    filepath='best_weights_double_LSTM.hdf5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\ncallbacks1 = [lr_schedule, checkpoint1]\ncallbacks2 = [lr_schedule, checkpoint2]","65303491":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","61e242d8":"train_history1 = model.fit(train_dataset, epochs=20,\n                    validation_data=test_dataset,\n                    callbacks = callbacks1,\n                    validation_steps=100)","bf1ba6e6":"def visualize_training(history, lw = 3):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10,10))\n    plt.subplot(2,1,1)\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.legend(fontsize = 'x-large')\n    \n\n    plt.subplot(2,1,2)\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Loss Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)\n    plt.title('Learning Rate')\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.show()","bef8c783":"visualize_training(train_history1) ","930048e5":"test_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","e2368319":"def pad_to_size(vec, size):\n    zeros = [0] * (size - len(vec))\n    vec.extend(zeros)\n    return vec\n# conditional encoder. It encodes with padding \ndef sample_predict(sample_pred_text, pad):\n    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n\n    if pad:\n        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n\n    return predictions","ee88301f":"# predict on a sample text without padding.\n\nsample_pred_text = ('The movie was good. overall it was a hit movie '\n                    'I would recomment watching this movie. ')\n\nprint('\\n Before adding the complex phrase: ')\nprint(sample_pred_text)\npredictions = sample_predict(sample_pred_text, pad=False)\nprint('Prediction without padding: ', predictions)\n\npredictions = sample_predict(sample_pred_text, pad=True)\nprint('Prediction with padding: ', predictions)\n\n\n# After we add the complex phrase\nprint('\\n\\nAfter adding the complex phrase: ')\nsample_pred_text = ('The movie was good.  Though the movie lacks in minor details, '\n                    'overall it was a hit movie '\n                    'I would recomment watching this movie. ')\nprint(sample_pred_text)\n\npredictions = sample_predict(sample_pred_text, pad=False)\nprint('Prediction without padding: ', predictions)\n\npredictions = sample_predict(sample_pred_text, pad=True)\nprint('Prediction with padding: ', predictions)","aaccd0c6":"westworld_review = ('What a miserable letdown. A rambling piece of garbage. '\n                    'Westworld started out great with the actual reason the book was written for. '\n                    'Fantasmical worlds for the rich. It ended up being car chases in LA somewhere.')\n\npredictions = sample_predict(westworld_review, pad=False)\nprint('Prediction without padding: ', predictions)\n\npredictions = sample_predict(westworld_review, pad=True)\nprint('Prediction with padding: ', predictions)","2f02a13e":"westworld_review2 = ('I really enjoyed the 1st season of Westworld and I think '\n                     'it has the best special effects on television')\n\npredictions = sample_predict(westworld_review2, pad=False)\nprint('Prediction without padding: ', predictions)\n\npredictions = sample_predict(westworld_review2, pad=True)\nprint('Prediction with padding: ', predictions)","29d2f5a6":"westworld_review3 = ('started to get lost in season #2 and now I am totally lost on season #3.' \n                     ' I would have preferred they spend the money on another season of game of thrones. ' \n                     'I really enjoyed the 1st season of Westworld and I think it has the best special effects on television but')\n\npredictions = sample_predict(westworld_review3, pad=False)\nprint('Prediction without padding: ', predictions)\n\npredictions = sample_predict(westworld_review3, pad=True)\nprint('Prediction with padding: ', predictions)","d9d601a8":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\nmodel.summary()","afa6f0ba":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","9f9f1d9b":"train_history2 = model.fit(train_dataset, epochs=20,\n                    validation_data=test_dataset,\n                    callbacks = callbacks2,\n                    validation_steps=30)\n","7fb3f304":"visualize_training(train_history2)","8a685772":"### Take care of the mask\nNow if we want to make prediction on any new review outside the dataset, it needs to be of the same size of the standard dataset. However, in the original training process, size was selected as 64. The above model does not mask the padding applied to the sequences. The strings was processed like that.  This can lead to skew if trained on padded sequences and test on un-padded sequences. Ideally we should use masking to avoid this, but as you can see below it only have a small effect on the output.","d0d23fb7":"### Compile the model\nYou observe one thing that I choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. You can have a look at [Keras RNN guide](https:\/\/www.tensorflow.org\/guide\/keras\/rnn#rnn_state_reuse) for more details.\n\nNow we are done with defining the model architecture, we can simply compile the model with BinaryCrossEntropy Loss and Adam optimizer with learning rate  1e-4. Here we use the option `from_logits = True` because, we have a very large vector where a lot of values will be very close to zero or close to 1. Therefore if we use logits, it is numerically more stable to calculate those fine fractions as after converting to log(), they are very well defined numbers. ","4843d734":"### Observation (interesting !!!)\n* You see that the model is working moderately ok. However it becomes a bit tricky when we add additional phrase **Though the movie lacks in minor details,** in the middle of the string. Observe that, though the added phrase doesn't make that movie a heavy blunder, however, the model fails to understand the complete meaning of the complex phrases like that. This is one of the limitations of the LSTM models. \n\n* Let's have a look at several other instances for a better understandings. Feel free to add, remove, or modify the sentences to understand how the model is handling different kinds of dependencies. ","2d9b2df4":"In my previous notebook [NLP-101: Understanding Word Embedding](https:\/\/www.kaggle.com\/redwankarimsony\/nlp-101-understanding-word-embedding) about NLP, I discussed about word embedding and also explored some mathematical background to them. In this notebook I will simply explore how we can classify the  **IMDB Review Classification** with LSTM (one of the variants of RNN). We all know that there are tons of better models than this out there in the internet like LSTM,  [Transformers](https:\/\/arxiv.org\/abs\/1706.03762), [GPTs](https:\/\/www.kdnuggets.com\/2020\/06\/gpt-3-deep-learning-nlp.html),  [BERT](https:\/\/arxiv.org\/abs\/1810.04805). \nDince I am new to the field of NLP, I will learn from the bottom to all the way up to BERT. I am gradually learning different techniques in NLP and I will keep updating this notebook in near future. ","bf13a2ab":"### Create the Model\n\nBuild a `tf.keras.Sequential` model and start with an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n\nThis index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n\nA recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input\u2014and then to the next.\n\nThe `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies.","d4192032":"## Stack two or more LSTM layers\nKeras recurrent layers have two available modes that are controlled by the return_sequences constructor argument:\n\n* Return either the full sequences of successive outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n* Return only the last output for each input sequence (a 2D tensor of shape `(batch_size, output_features)`).","4e8430aa":"Now let's have a look at the vocabulary itself. If you observe, at the time of loading data, I also donloaeded **info** which contains the vocabulary list and encoder itself. ","744e3f7c":"This encoding and decoding is totally a lossless process. Whatever input we give, the exact same thing will come out at the decoding phase. ","7934cb3c":"### Prepare the data for training\nNext create batches of these encoded strings. Use the `padded_batch` method to zero-pad the sequences to the length of the longest string in the batch:","d701ce5a":"### I am a beginer in the field of  NLP and gradually working my way up in the ladder. Though this simple old techniques might feel obsolete today, however based on them a lot of development occured in the past. Therefore, importance of this methods can not be overlooked. \n\n### If you like this notebooks methods, please upvote. It keep me motivated to learn newer things.. Thank you.. ","8db8c446":"### Setup input pipeline\nThe IMDB large movie review dataset is a binary classification dataset\u2014all the reviews have either a positive or negative sentiment.","87258345":"![](https:\/\/www.easy-tensorflow.com\/images\/NN\/01.png)","fb491d42":"Now let's play with some of the encoding and decoding syntax so that we can understand what we are doing here. ","8783bd3a":"### Train the model","0ce719a3":"Now let us check the performance of the trained model below.","1981e58d":"Now let's have a look at the prediction with and without padding. Though, here the difference is not that much of significant, when we will work with bigger model, it will have profound impact on the score. ","df9c4ef2":"<h1 align='center'>Text Classification with RNN<\/h1>\n\n"}}