{"cell_type":{"3070d247":"code","f77435d5":"code","00a13b9a":"code","f567dc75":"code","e5c7817b":"code","12883f28":"code","dd96bde5":"code","f2778090":"code","574b298c":"code","d68397dc":"code","f0102a0a":"code","075d879b":"code","1789ae1f":"code","e48a732f":"code","62213c92":"code","9fd509d7":"code","c7334896":"code","80e83ba7":"code","b9d217a3":"code","6091d1a1":"code","2d2eebea":"code","dc6f5845":"code","54562afa":"code","62cdf6fa":"code","1a85b785":"code","6c4a52ac":"code","7dab6d87":"code","b6799c02":"code","e49afffb":"code","91977e3d":"code","dfd632d3":"code","c9ccfa72":"markdown","85e39ccd":"markdown","8c49e923":"markdown","cbc719c4":"markdown","095c0e8b":"markdown","46e236e6":"markdown","69bbb629":"markdown","644a571c":"markdown","968e6729":"markdown","8336b851":"markdown"},"source":{"3070d247":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n# List data files that are connected to the kernel\nos.listdir('..\/input\/')\n\n# \u0410\u043d\u0430\u043b\u0438\u0437 \u0442\u0435\u0445 \u043a\u0442\u043e \u0432\u044b\u0436\u0438\u043b \u0438\u043b \u043d\u0435\u0442\n#df_1 = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n#df_1.head()\n#df_1.shape\n#df_1[\"Survived\"].value_counts()\n\n\n\n\n\n","f77435d5":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n\nprint(df.shape)\nprint(df.columns)\ndf[\"Sex\"].value_counts(normalize=True)\ndf.describe()\n#df_2[\"Age\"].hist()\ndf.corr()\nsurvived_rate_of_women = (df[df[\"Sex\"] == \"female\"][\"Survived\"].mean())#0.74 \nsurvived_rate_of_men = df[df[\"Sex\"]!= \"female\"][\"Survived\"].mean()#.0.18890814558058924 \nprint(survived_rate_of_women )\n","00a13b9a":"# (C = Cherbourg; Q = Queenstown; S = Southampton)\ndf.tail()\n\n","f567dc75":"df[\"Sex\"] = df[\"Sex\"].map({\"male\": 1, \"female\": 0})\ndf[\"Embarked\"] = df[\"Embarked\"].map({\"C\": int(2), \"Q\": 1, \"S\": 0})\n#df[\"Embarked\"].astype(\"int32\")\ndf.head()\ndf.drop(\"Name\",axis = 1,inplace=True)\ndf.drop(\"Ticket\",axis = 1,inplace=True)\n","e5c7817b":"# what to do with age\nAge = df[\"Age\"]\n\ndf['categorical_Age'] = pd.cut(df.Age, 7, labels=range(1, 8))  # so we transform numerical age to  categorical\n\nAge\n\n# 0 - 18 : 1\n# 18- 30 : 2\n# 30 - 40 : 3\n# 40 - 50 : 4\n# 50 - 60 : 5\n# 60 - 70 : 6\n# 70> : 7 \ndf.head(23)\nAge_saved = Age.copy()\nAge_saved\nsns.countplot(x='categorical_Age',hue=\"Survived\",data=df); # if you are young , you are more likely to  will survive ","12883f28":"#what to do with fare\ndf['categorical_Fare'] = pd.cut(df.Fare, 7, labels=range(1, 8))  # so we transform numerical age to  categorical with pd.cut\nsns.countplot(x='categorical_Fare',hue=\"Survived\",data=df); \n","dd96bde5":"# what to do with Cabin\n#df[\"floor\"] = df.iloc[\"Cabin\"] * 1.882\n\n#df[df[\"Cabin\"].apply(lambda Cabin: Cabin[0] == \"C\")]]\ndf.head()\ndf = df.drop( ['Cabin'], axis=1)","f2778090":"df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False) # woman are likely to survive 74% from 18% \n#sns.countplot(x=\"Sex\",hue=\"Survived\",data=df)\n","574b298c":"sns.countplot(x=\"Embarked\",hue=\"Survived\",data = df)\n","d68397dc":"df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f0102a0a":"df[\"Age\"].plot(kind =\"density\")","075d879b":"sns.boxplot(x=\"Survived\",y=\"Age\",data = df )","1789ae1f":"df[\"Survived\"].value_counts(normalize=True)#38,38 \u0432\u044b\u0436\u0438\u043b\u043e","e48a732f":"df[df[\"Pclass\"]==1][\"Survived\"].value_counts() #136 \u0432\u044b\u0436\u0438\u043b\u043e \u0438\u0437 216  \u0434\u043b\u044f 1 \u043a\u043b\u0430\u0441\u0441\u0430\ndf[df[\"Pclass\"]==2][\"Survived\"].value_counts() #97 \u0432\u044b\u0436\u0438\u043b\u043e \u0438\u0437 185   \u0434\u043b\u044f 2 \u043a\u043b\u0430\u0441\u0441\u0430\ndf[df[\"Pclass\"]==3][\"Survived\"].value_counts() #119 \u0432\u044b\u0436\u0438\u043b\u043e \u0438\u0437 491  \u0434\u043b\u044f 3 \u043a\u043b\u0430\u0441\u0441\u0430\nsns.countplot(x=\"Survived\", data= df, hue=\"Pclass\")\n","62213c92":"# parch - Number of Parents\/Children Aboard\ndf[\"Parch\"].unique()\nsns.countplot(x = \"Parch\", data= df, hue=\"Survived\")\ndf[df[\"Parch\"]==0][\"Survived\"].value_counts() \ndf[df[\"Parch\"]==1][\"Survived\"].value_counts() \ndf[df[\"Parch\"]==2][\"Survived\"].value_counts()\ndf[df[\"Parch\"]==3][\"Survived\"].value_counts()\ndf[df[\"Parch\"]==4][\"Survived\"].value_counts()\nrelatives5 = df[df[\"Parch\"]==5][\"Survived\"].value_counts()\n\n#df[df[\"Parch\"]==6][\"Survived\"].value_counts() # 1 person survived out of 1\n\n\n","9fd509d7":"df = df.drop( ['Age'], axis=1)\ndf = df.drop( ['Fare'], axis=1)\ndf.head()","c7334896":"df['categorical_Age'].fillna(2, inplace = True)\n\ndf.dropna(axis=0, how='all', subset=[\"Embarked\"], inplace=True)\ndf[\"categorical_Age\"] = df[\"categorical_Age\"].astype(\"int64\")\ndf[\"categorical_Fare\"] = df[\"categorical_Fare\"].astype(\"int64\")\ndf.info()\n","80e83ba7":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n","b9d217a3":"\nY= df[\"Survived\"].astype(\"int32\")\nX=df.drop(\"Survived\",axis=1)\nX_train, X_holdout, Y_train, Y_holdout = train_test_split(\n    X, Y, test_size=0.02, random_state=17\n)","6091d1a1":"\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n\nfrom sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n                                     cross_val_score)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.tree","2d2eebea":"KNeighborsClassifier?","dc6f5845":"#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n#knn = KNeighborsClassifier()\n#gridsearch = GridSearchCV(knn, {'n_neighbors' : range(2,40,2)}, cv=5, verbose=1, n_jobs=-1)\n#model = gridsearch.fit(X_train, Y_train)\n#pred1 = model .predict(X_holdout)\n#pred2 = model .predict(X_train)\n#print(accuracy_score(pred1,Y_holdout))\n#print(accuracy_score(pred2,Y_train))\n\n\n\n","54562afa":"from sklearn.ensemble import VotingClassifier","62cdf6fa":"\n\n\n\n\n#let's start with logistic regression\n\n#logregression = LogisticRegression(C=1, random_state=17, solver='liblinear')\n\n#logregression.fit(X_train,Y_train)\n#logregressionpred = logregression.predict(X_holdout)\n\n#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n#C = 1e4\n#c_values = np.logspace(-2, 3, 500)\n#logit = LogisticRegression(C=C, random_state=17,solver='liblinear')\n#logit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=1, n_jobs=-1,solver='liblinear')\n#logit_searcher.fit(X_train, Y_train)\n\n#accuracy_score(Y_holdout, logit_searcher)\n\n\n\n#tree = DecisionTreeClassifier(max_depth=4, random_state=17)\n#knn = KNeighborsClassifier(n_neighbors=5)\n#mean_value = (cross_val_score(logregression, X_train,Y_train, cv=5))\n\n\n\n#tree.fit(X_train, Y_train)\n\n\n\n\n#tree_pred = tree.predict(X_holdout)\n#accuracy_score(Y_holdout, tree_pred) #79 percents \n\n\n\n#tree_params = {\"max_depth\": range(1, 15), \"max_features\": range(4, 19)}\n#GridSearchCV?\n#tree_grid = GridSearchCV(tree, tree_params, cv=5,n_jobs=-1, verbose=True)\n\n#tree_grid.fit(X_train, Y_train)\n#tree_grid_pred = tree_grid.predict(X_holdout)\n#accuracy_score(Y_holdout, tree_grid_pred)\n#tree_grid.best_params_\n#from sklearn.ensemble import RandomForestClassifier\n#skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=17)\n#forest = RandomForestClassifier( n_jobs=-1, random_state=17)\n#forest_params = {\"max_depth\": range(2, 21), 'min_samples_leaf': [1,3,5,7,9,11,13,14,15,17],'n_estimators': [1,2,3,4,5,6,7,8,9,10,25,50,75,100,125,150,175,225]}\nboosting = GradientBoostingClassifier(random_state=1)\nboosting_params ={'n_estimators':range(21,130,10),'max_depth':range(5,22,2),'min_samples_split':range(200,1001,200),'max_features':range(4,20,2), 'min_samples_leaf':range(30,120,10)}\nboost = GridSearchCV(boosting,boosting_params,n_jobs = -1,cv=5,verbose=1)\nknn = KNeighborsClassifier(n_neighbors=5)\ngcv = VotingClassifier(\n    estimators = [('boost',boost),('knn',knn)], voting = 'hard')\ngcv.fit(X_train, Y_train)\n\n#print(np.mean(cross_val_score(forest, X_train, Y_train, cv=5)))  \n#forest_params = {\"max_depth\": range(2, 21), \"max_features\": range(4, 19)}\n\n#forest_grid = GridSearchCV(forest, forest_params, cv=5, n_jobs=-1, verbose=True)\n\n#forest_grid.fit(X_train, Y_train)\n\n#forest_grid.best_params_, forest_grid.best_score_  \"\"\"","1a85b785":"\n\n#scaler = StandardScaler()\n#X_train_scaled = scaler.fit_transform(X_train)\n#X_holdout_scaled = scaler.transform(X_holdout)\n#knn.fit(X_train_scaled, Y_train)\n#knn_pred = knn.predict(X_holdout_scaled)\n#accuracy_score(Y_holdout, knn_pred)\n","6c4a52ac":"#from sklearn.ensemble import RandomForestClassifier\n\n#forest = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=17)\n#print(np.mean(cross_val_score(forest, X_train, Y_train, cv=5)))  \n#forest_params = {\"max_depth\": range(2, 21), \"max_features\": range(4, 19)}\n\n#forest_grid = GridSearchCV(forest, forest_params, cv=5, n_jobs=-1, verbose=True)\n\n#forest_grid.fit(X_train, Y_train)\n\n#forest_grid.best_params_, forest_grid.best_score_  \"\"\"\n","7dab6d87":"df2 = pd.read_csv('..\/input\/titanic\/test.csv')\ndf2.head()","b6799c02":"df2[\"Sex\"] = df2[\"Sex\"].map({\"male\": 1, \"female\": 0})\ndf2[\"Embarked\"] = df2[\"Embarked\"].map({\"C\": int(2), \"Q\": 1, \"S\": 0})\ndf[\"Embarked\"].astype(\"int32\")\n\ndf2.drop(\"Name\",axis = 1,inplace=True)\ndf2.drop(\"Ticket\",axis = 1,inplace=True)\n# what to do with age\n#Age = df2[\"Age\"]\n\ndf2['categorical_Age'] = pd.cut(df2.Age, 7, labels=range(1, 8))  # so we transform numerical age to  categorical\ndf2.head()\n#Age\n\n# 0 - 18 : 1\n# 18- 30 : 2\n# 30 - 40 : 3\n# 40 - 50 : 4\n# 50 - 60 : 5\n# 60 - 70 : 6\n# 70> : 7 \n#df2.head(23)\n#Age_saved = Age.copy()\n#Age_saved\n # if you are young , you are more likely to  will survive \n#what to do with fare\ndf2['categorical_Fare'] = pd.cut(df2.Fare, 7, labels=range(1, 8))  # so we transform numerical age to  categorical with pd.cut\n\n# what to do with Cabin\n#df[\"floor\"] = df.iloc[\"Cabin\"] * 1.882\n\n#df[df[\"Cabin\"].apply(lambda Cabin: Cabin[0] == \"C\")]]\n#df2.head()\ndf2 = df2.drop( ['Cabin'], axis=1)\n\ndf2 = df2.drop( ['Age'], axis=1)\ndf2 = df2.drop( ['Fare'], axis=1)\ndf2['categorical_Age'].fillna(2, inplace = True)\ndf2['categorical_Fare'].fillna(1, inplace = True)\n\n\ndf2[\"categorical_Age\"] = df2[\"categorical_Age\"].astype(\"int64\")\ndf2[\"categorical_Fare\"] = df2[\"categorical_Fare\"].astype(\"int64\")\ndf2.info()\n","e49afffb":"#scaler = StandardScaler()\n#X_train_scaled = scaler.fit_transform(X_train)\n#X_holdout_scaled = scaler.transform(X_holdout)\n#knn.fit(X_train_scaled, Y_train)\n#knn_pred = knn.predict(X_holdout_scaled)\n#accuracy_score(Y_holdout, knn_pr)\npred2 = gcv.predict(df2)\n#pred2\nprediction = pd.DataFrame(pred2)\n#prediction\n#prediction\n","91977e3d":"#df_sub = pd.read_csv('.\/gender_submission.csv')\n#df_sub.info()","dfd632d3":"#df3 = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n#df3.head()\n#df3[\"Survived\"] = prediction[0]\n#submission = df3\n\n#submission[\"Survived\"] = submission[\"Survived\"].astype('int64')\n\n#submission.info()\n#submission.to_csv('submission.csv' , index=False)\n\n","c9ccfa72":"****df['categorical_Age'] = pd.cut(df.Age, 5, labels=range(1, 6)) # how************","85e39ccd":" **Decision Tree** ![image.png](attachment:78eb1119-a75c-41df-869b-56d4b677ec73.png)![image.png](attachment:32d623b7-037e-49fe-9ff1-b7aa007c8338.png)","8c49e923":"Now! let's predict","cbc719c4":"Random forest\n","095c0e8b":"\u041e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c NaN","46e236e6":"Work with test_data","69bbb629":"****we need to do some binning****","644a571c":"Decision Tree with hyperparams","968e6729":"Logistic Regression","8336b851":"First baseline"}}