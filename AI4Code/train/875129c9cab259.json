{"cell_type":{"7c8ddaa2":"code","fbac2dc8":"code","71f25868":"code","4bb2d977":"code","1b0dde6d":"code","5f898b5f":"code","bcbb1ae1":"code","f7f7646f":"code","b91bc98f":"code","84bcbbae":"code","6a22298e":"code","0d8962d6":"code","5819a2ff":"code","086911cc":"code","1714ecec":"code","b092dda5":"code","698f52dd":"code","38bbdc0c":"code","57b06e1f":"code","4adb1e55":"code","d0f23707":"code","97186231":"code","07e00f41":"code","6a3b1dce":"code","4b8cb2a6":"code","3807838d":"code","90865723":"code","2856561f":"code","40955aa5":"markdown","667ad715":"markdown","ac17447d":"markdown","2300330c":"markdown","c1825bec":"markdown","7061d5dd":"markdown","273effa9":"markdown","02ef67f3":"markdown","1fcddfd7":"markdown","7d5fd3e2":"markdown","95fc0fbb":"markdown","8a7b5641":"markdown","b65ff3e0":"markdown","8dbaae10":"markdown","c731708a":"markdown","5472a2ef":"markdown","9fdd40a4":"markdown","6e39aba9":"markdown","4b5baeba":"markdown","cc4015dc":"markdown","75d646d5":"markdown","7d30e5ce":"markdown","ca458ac3":"markdown","12845686":"markdown","698a5a09":"markdown"},"source":{"7c8ddaa2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport seaborn as sns \nimport time\n#from scipy import stats\n\n%matplotlib inline","fbac2dc8":"seed = 2\n\ntrain_size = 0.8\ntest_size = 1- train_size\n\nlabeled_images = pd.read_csv('..\/input\/train.csv')\ntest_data=pd.read_csv('..\/input\/test.csv') #The kaggle test set (to get leaderboard score). For kaggle kernel add: ..\/input\/\nsample_size = len(labeled_images.index) #total number of instances for training and testing","71f25868":"def plot_as_grid(images, labels, m):\n    \"Plot N x M grid of digits and labels\/predictions\"\n    n_pixels = len(images.columns)\n    dimension = int(np.sqrt(n_pixels))\n\n    # set up the figure\n    fig = plt.figure(figsize=(6, 6))  # figure size in inches\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n    # plot the digits: each image is max mxm pixels\n    for i in range( min(m*m, len(images.index))):\n        ax = fig.add_subplot(m, m, i + 1, xticks=[], yticks=[])    \n    \n        img=images.iloc[i].values.reshape((dimension,dimension))\n    \n        ax.imshow(img, cmap=plt.cm.binary, interpolation='nearest')\n        ax.text(0, 7, str(labels.iloc[i,0]))","4bb2d977":"def param_selection(model, X, y, param_random, n_iter = 20, n_jobs = 2, n_folds = 3):\n    \"Determines the best parameters using randomSearch (slow)\"\n    search = RandomizedSearchCV(model, param_random, n_iter = n_iter, n_jobs = n_jobs ,\n                                random_state = seed, cv=n_folds, iid=True) \n    \n    search.fit(X, y)\n    return search.best_params_","1b0dde6d":"def inspect_performance(model, train_images, train_labels, test_images, test_labels, ypred):\n    \"Prints training performance, test performance and a performance report\"\n    print(\"Training error: \", model.score(train_images,train_labels))\n    print(\"Test error: \", model.score(test_images,test_labels))\n    print(\"Test report: \")\n    print(metrics.classification_report(ypred, test_labels))","5f898b5f":"def plot_confusion_matrix(labels, predictions):\n    \"Plots a confusion matrix using a heatmap\"\n    mat = confusion_matrix(labels, predictions)\n    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')","bcbb1ae1":"def plot_incorrect_classifications(ypred, test_labels, test_images):\n    \"Plots incorrectly classified images and corresponding prediction\"\n    ypred = pd.DataFrame(ypred)\n    ypred = ypred.set_index(test_labels.index.values)\n    ypred.columns = ['prediction']\n    predict_df = pd.concat([ypred, test_labels], axis=1)\n    predict_df['Incorrect'] = predict_df.prediction != predict_df.label\n    idx = predict_df.index[predict_df['Incorrect']]\n\n    plot_as_grid(test_images.loc[idx], predict_df['prediction'].loc[idx].to_frame(), 5)","f7f7646f":"def make_submission_file(model, data, name):\n    \"Makes a submission file by predicting on test_data and returning the predictions\"\n    data[data>0]=1\n    results=model.predict(data) \n\n    #Set to proper format:\n    df = pd.DataFrame(results)\n    df.index.name='ImageId'\n    df.index+=1\n    df.columns=['Label']\n    df.to_csv(name + '.csv', header=True)\n    return(results)","b91bc98f":"labeled_images_sample = labeled_images.sample(n=sample_size, random_state= seed)\nimages = labeled_images_sample.iloc[:,1:] # select images (all columns except the first)\n\nmax_value = images.values.max()\nimages \/= max_value #scale\n\nlabels = labeled_images_sample.iloc[:,:1] # select labels (only first column)\n\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=train_size, \n                                                                       test_size=test_size, random_state=seed)","84bcbbae":"plot_as_grid(train_images, train_labels, 8) #digit and label","6a22298e":"clf = svm.SVC(gamma='auto', C=1.0, kernel='rbf')\nclf.fit(train_images, train_labels.values.ravel())\nypred_svm = clf.predict(test_images) ","0d8962d6":"#takes long, for now commented out\n#inspect_performance(clf, train_images, train_labels, test_images, test_labels, ypred_svm)    ","5819a2ff":"plot_confusion_matrix(test_labels, ypred_svm)","086911cc":"plot_incorrect_classifications(ypred_svm, test_labels, test_images) #digit and classification  ","1714ecec":"ypred_svm_test = make_submission_file(clf, test_data, \"results_svm\")","b092dda5":"start_time = time.clock()\nparam_random = {\"n_estimators\": [400, 700], # you can try multiple options, as long as you update n_iter accordingly:  \"n_estimators\": [400, 500, 600, 700], then n_iter should be 4*2\n                'max_features' : ['auto', 'log2']} \nparameters = param_selection(RandomForestClassifier(), train_images, train_labels.values.ravel(), param_random, n_iter = 8, n_folds = 3 ) \nprint(parameters)\nprint (time.clock() - start_time, \"seconds\")","698f52dd":"rf = RandomForestClassifier(n_estimators = parameters['n_estimators'], max_features = parameters['max_features'], \n                            random_state=seed,n_jobs=2)\nrf.fit(train_images, train_labels.values.ravel())\nypred_rf = rf.predict(test_images) ","38bbdc0c":"inspect_performance(rf, train_images, train_labels, test_images, test_labels, ypred_rf)    ","57b06e1f":" plot_confusion_matrix(test_labels, ypred_rf)","4adb1e55":"plot_incorrect_classifications(ypred_rf, test_labels, test_images) #digit and classification    ","d0f23707":"ypred_rf_test = make_submission_file(rf, test_data, \"results_rf\")","97186231":"start_time = time.clock()\nparam_random = {\"eta\": [0.3], # you can try multiple options, as long as you update n_iter accordingly:  \"eta\": [0.01, 0.02, 0.05, 0.1, 0.25, 0.5], then n_iter should be 6\n                \"early_stopping_rounds\": [50],\n                \"n_estimators\" : [100],\n                \"eval_metric\" : [\"merror\"]}\nparameters = param_selection(XGBClassifier(), train_images, train_labels.values.ravel(), param_random, n_iter = 1) \nprint(parameters)\nprint (time.clock() - start_time, \"seconds\")\n\n","07e00f41":"xgb = XGBClassifier(eta = parameters[\"eta\"], early_stopping_rounds = parameters[\"early_stopping_rounds\"], \n                    n_estimators = parameters[\"n_estimators\"],eval_metric = parameters[\"eval_metric\"],n_jobs=2)\nxgb.fit(train_images,train_labels.values.ravel())\nypred_xgb = xgb.predict(test_images)  ","6a3b1dce":"inspect_performance(xgb, train_images, train_labels, test_images, test_labels, ypred_xgb)    ","4b8cb2a6":"plot_confusion_matrix(test_labels, ypred_xgb)","3807838d":"plot_incorrect_classifications(ypred_rf, test_labels, test_images) #digit and classification","90865723":"ypred_xgb_test = make_submission_file(rf, test_data, \"results_xgb\")","2856561f":"stack_train = pd.concat([pd.DataFrame(ypred_svm) , pd.DataFrame(ypred_rf), pd.DataFrame(ypred_xgb)], axis=1)\nstack_test = pd.concat([pd.DataFrame(ypred_svm_test) , pd.DataFrame(ypred_rf_test), pd.DataFrame(ypred_xgb_test)], axis=1)\n\nlr = LogisticRegression(solver ='newton-cg', multi_class=\"auto\")\nlr.fit(stack_train,test_labels.values.ravel())\nypred_lr = lr.predict(stack_test)  \n\nresults=ypred_lr\nname = \"results_stacking\"\n#Set to proper format:\ndf = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv(name + '.csv', header=True)","40955aa5":"# Model 2: Random Forest\nAccording to following steps:\n- train model with optimal parameters\n- inspect performance\n- error analysis\n- write to .csv","667ad715":"### Inspect performance","ac17447d":"### Error analysis","2300330c":"# Model 4: Stacking classifier\nStack SVM, RF and XGB together in a stacking classifier. Parameter tuning turned out to be a computational nightmare so I will try this approach instead.","c1825bec":"### Write to .csv","7061d5dd":"# Model 1: SVM\nAccording to following steps:\n- train model\n- inspect performance\n- error analysis\n- write to .csv\n\n**Note**: There is no parameter tuning for SVM because it is typically a slow algorithm. It takes too long to train. Also, I expect SVM not to be the best model for digit recognition so I won't spend too much time on it. ","273effa9":"### Inspect performance","02ef67f3":"### Pre-processing","1fcddfd7":"### Error analysis","7d5fd3e2":"These different models can be submitted. Please let me know if this notebook was helpful and\/or if you have feedback.","95fc0fbb":"# MNIST classification\nIn this notebook I tried to train and optimize a range of models on the MNIST dataset. The final result will be a stacking classifier of SVM, RF and XGB. I used other people's work and combined this into a more complete analysis. I am a Python beginner (more used to R) so bare with me..","8a7b5641":"### Imports","b65ff3e0":"### Visualisation(s)","8dbaae10":"### Write to .csv","c731708a":"# Basics","5472a2ef":"### Train model with optimal parameters","9fdd40a4":"### Error analysis","6e39aba9":"### Inspect performance","4b5baeba":"### Global variables","cc4015dc":"### Write to .csv","75d646d5":"### Train model with optimal parameters","7d30e5ce":"# Model 3: Xtreme Gradient Boosting\nAccording to following steps:\n- train model with optimal parameters\n- inspect performance\n- error analysis\n- write to .csv","ca458ac3":"### Functions","12845686":"### Sources (among others)\n- https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.08-random-forests.html\n- https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification\n","698a5a09":"### Train model"}}