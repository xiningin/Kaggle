{"cell_type":{"01f088ae":"code","6f00e823":"code","c68376a6":"code","8aff2358":"code","3ff5cee8":"code","929a89b0":"code","e63a5bc5":"code","fc2e7038":"code","eedcd672":"code","229a6fad":"code","6e5278a5":"code","827af79c":"code","1eba56ce":"code","b50aadba":"code","ea191d1b":"code","c774aee8":"code","2dc4abd0":"code","9fd4d943":"code","f1dbd3e8":"code","ec3b60c6":"code","a6661438":"code","1f2fec5d":"code","ce5d8ac8":"code","4517e08e":"code","3f9cfb92":"code","30a68ba5":"code","d3be7f1e":"code","05474995":"code","cc240829":"code","91e6dc31":"code","24dd9c71":"code","b7c37c7e":"code","1de8d05a":"code","1184059c":"code","09784aef":"code","d499083f":"code","9bb3ea34":"code","2bd977b9":"code","0d438db7":"code","170fcdc1":"code","01764fbb":"code","f332136a":"code","f5720668":"code","2ad6f4ca":"code","ac889304":"code","cbf822aa":"code","57028ab7":"code","2521d34d":"code","7fbd2d1a":"code","5169e97c":"code","e6ba5862":"code","84ee85f5":"code","06756a32":"code","b3f116e5":"code","6417afcd":"code","99b780bb":"code","83ff5024":"code","06851b7e":"code","f2e9c874":"code","77bcfad9":"code","59feef42":"code","f4227545":"code","bb0db8c6":"code","2d9a4ddf":"code","5ea34f31":"code","e5e88fb2":"code","83f90cb4":"code","49c69985":"code","6746586e":"code","4af4905a":"code","f35a8d0a":"code","bf51b562":"code","9899e04f":"code","dcca30e5":"code","de2959e4":"code","65a0ea96":"code","b150b7ee":"code","f9d4dc92":"code","575435f1":"code","a5e928b6":"code","703cb205":"code","8f2348d7":"code","9f7e0384":"code","36b28da7":"code","c79e9e25":"code","eb6c9c1c":"code","ab91b6c9":"code","38986d27":"code","ed9fea2f":"code","a4c3de3f":"code","d155958f":"code","5efc7389":"code","62d89391":"code","b5a8da1e":"code","454c4cac":"code","9292c4a4":"code","6a222aad":"code","6d09467d":"code","24b45fcf":"code","85267e82":"code","7b5aaa11":"code","145b02f6":"code","2289dc05":"code","f98ce035":"code","4a630be0":"code","96bcb75f":"code","71318218":"code","7524a49e":"code","5544df4d":"code","2cdd59f8":"code","2a44ab11":"code","3ef5b33f":"code","c25a823b":"markdown","1bb5ba07":"markdown","142b4087":"markdown","9a73a9c8":"markdown","5bd2c416":"markdown","a276ad48":"markdown","bf954e14":"markdown","d9322b86":"markdown","726f1ed9":"markdown","2faf76a4":"markdown","0298584f":"markdown","b55689ef":"markdown","4f24f16f":"markdown","7d9513a2":"markdown","e41ad0fa":"markdown","6bbdf272":"markdown","e3f45955":"markdown","5f819a2f":"markdown","c073bf30":"markdown","57c1f705":"markdown","f78f6935":"markdown","8cf7db91":"markdown","fe5aa6e3":"markdown","1d66a1eb":"markdown","ef4f243a":"markdown","7d2feee3":"markdown","a8397480":"markdown","62bf3b26":"markdown","bf1823a5":"markdown","28079740":"markdown","f2aed332":"markdown","762683be":"markdown","d163dbf2":"markdown","05337bdd":"markdown","c7d47d04":"markdown","3c1870dc":"markdown","55121ee4":"markdown","c12f450f":"markdown","46138307":"markdown","b14a2ee9":"markdown","f195604d":"markdown","772d4a37":"markdown","4a0c6667":"markdown","e8b2c251":"markdown","186f9bf8":"markdown","88b93d95":"markdown","2ac48cb4":"markdown","c6a1c0b7":"markdown","cabdce1f":"markdown","f1d182d9":"markdown","70f5679d":"markdown","ab7c1c55":"markdown","33a2e5cd":"markdown","5fb725d2":"markdown","6166e1c1":"markdown","95bb6d05":"markdown","058dfc56":"markdown","ac6f4409":"markdown","6271f89e":"markdown","afa91998":"markdown","b0ed682b":"markdown","beb27fe7":"markdown","77af6ebe":"markdown"},"source":{"01f088ae":"%matplotlib inline","6f00e823":"%%time\nimport gzip\nimport json\nimport string\nimport re\n\nimport nltk # imports the natural language toolkit\nimport pandas as pd\nimport plotly\n\nfrom collections import Counter\nfrom datetime import datetime\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nnltk.download('punkt')\n\n\n# we can tell pandas that our file is in gzip format and it will handle the decompression for us\n# we also use `lines=True` to indicate that each line of the file is its own JSON object\ninstant_video = pd.read_json(\"..\/input\/claseec4\/Amazon_Instant_Video_5.json\", lines=True) #, compression='gzip'\n\n\n# ----------\n# The Movies and TV file is very big. If you have problems loading it, you can load only the first \n# 100,000 reviews by using 'chunksize' (uncomment the line with 'chunksize' and comment out the line \n# after that which loads the entire file into `movies_tv`). All of the analysis can be \n# done in the same way using only the subset of reviews but some of the results might be different from the examples.\n# ----------\n# movies_tv = next(pd.read_json(\"reviews_Movies_and_TV_5.json.gz\", lines=True, compression='gzip', chunksize=10000))\nmovies_tv = next(pd.read_json(\"..\/input\/claseec4\/Movies_and_TV_5.json\", lines=True, chunksize=10000)) #, compression='gzip'","c68376a6":"print(len(instant_video))\ninstant_video.head(5)","8aff2358":"print(len(movies_tv))\nmovies_tv.head(5)","3ff5cee8":"%%time\nmovies_tv['datetime'] = pd.to_datetime(movies_tv['reviewTime'], format=\"%m %d, %Y\")\ninstant_video['datetime'] = pd.to_datetime(instant_video['reviewTime'], format=\"%m %d, %Y\")","929a89b0":"movies_tv = movies_tv.drop(columns = ['reviewerID', 'asin', 'reviewerName', 'reviewTime'])\ninstant_video = instant_video.drop(columns = ['reviewerID', 'asin', 'reviewerName', 'reviewTime'])\nmovies_tv.head(5)","e63a5bc5":"instant_video = instant_video.drop(columns = ['unixReviewTime'])\nmovies_tv = movies_tv.drop(columns = ['unixReviewTime'])\ninstant_video.head(5)","fc2e7038":"# Option1\ninstant_video['ishelpful'] = instant_video.helpful.str[0]\ninstant_video['isnothelpful'] = instant_video['helpful'].str[1]\n\n#Option2\ninstant_video['ishelpful'] = instant_video.helpful.apply(lambda x: x[0])\ninstant_video['isnothelpful'] = instant_video.helpful.apply(lambda x: x[1])","eedcd672":"# Option1\n# instant_video[instant_video['ishelpful']<15].ishelpful.hist(bins=10)\n#Option2\nimport seaborn as sns\n# sns.distplot(instant_video['ishelpful'], bins=100, hist=True, kde=True, rug=True)\n#Option3\ninstant_video['ishelpful'].plot.hist(bins=250, alpha=0.4, xlim=[0,15], legend='ishelpful')\ninstant_video['isnothelpful'].plot.hist(bins=250, alpha=0.4, xlim=[0,15], legend='isnothelpful')","229a6fad":"sns.distplot(instant_video['overall'])","6e5278a5":"instant_video['year'] = instant_video['datetime'].dt.year\ninstant_video['month'] = instant_video['datetime'].dt.month","827af79c":"import matplotlib.pyplot as plt\nf, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(nrows=6, ncols=1,figsize=(10,18))\n\ninstant_video['ishelpful'][instant_video['ishelpful']<10].hist(bins=10,ax=ax1)\nax1.set_title('ishelpful')\n\ninstant_video['isnothelpful'][instant_video['isnothelpful']<10].hist(bins=10,ax=ax2)\nax2.set_title('isnothelpful')\n\ninstant_video['datetime'].hist(ax=ax3)\nax3.set_title('datetime')\n\n\ninstant_video['overall'].hist(bins=5,ax=ax4)\nax4.set_title('overall')\n\n\ninstant_video['year'].hist(alpha=0.4, bins=50,ax=ax5)\nax5.set_xlabel('Year')\nax5.set_ylabel('Number of Reviews')\n\ninstant_video['month'].hist(alpha=0.4, bins=50,ax=ax6)\nax6.set_xlabel('Month')\nax6.set_ylabel('Number of Reviews')","1eba56ce":"#Option1\ninstant_video_byyear = instant_video.groupby(['year']).mean().overall.plot()","b50aadba":"#Option 2\ninstant_video_byyear = instant_video.groupby(['year']).mean()\ninstant_video_byyear = instant_video_byyear.reset_index()\n\nplt.figure(figsize=(10,4))\nsns.barplot(x='year', y='overall',data=instant_video_byyear, color='teal')","ea191d1b":"instant_video_byyear.plot.hist(y='year')","c774aee8":"instant_video_byyear.plot.hist(y='overall')","2dc4abd0":"instant_video_byyear.plot(x='year', y='overall')","9fd4d943":"# Option3\ninstant_video_byyear = instant_video[['overall','datetime']].groupby(pd.Grouper(key='datetime', freq='Y')).mean()\ninstant_video_byyear = instant_video_byyear.dropna()\ninstant_video_byyear.plot()","f1dbd3e8":"len(nltk.word_tokenize(instant_video['reviewText'][0]))","ec3b60c6":"#Option1\ndef num_palabras(x):\n    return len(nltk.word_tokenize(x))\n\ninstant_video['review_words_lengths'] = instant_video['reviewText'].apply(num_palabras)","a6661438":"#Option2\ninstant_video['review_words_lengths'] = instant_video['reviewText'].apply(lambda x: len(nltk.word_tokenize(x)))","1f2fec5d":"video_wlbyyear = instant_video.groupby(['year']).mean().reset_index()\nvideo_wlbyyear = video_wlbyyear[['year', 'review_words_lengths']]\nvideo_wlbyyear = video_wlbyyear.rename(columns={'review_words_lengths':'avg_review_wl'})\nvideo_wlbyyear","ce5d8ac8":"plt.figure(figsize=(10,4))\nsns.barplot(x='year', y='avg_review_wl',data=video_wlbyyear, color='teal')","4517e08e":"from nltk.corpus import stopwords\n#stopwords = set (stopwords.words('english'))\nAllReviews=instant_video['reviewText']\n\n# Code used by Natesh\ndef get_top_n_words(text, n=1, k=1):\n    vec = CountVectorizer(ngram_range=(k, k),stop_words='english').fit(text)\n    bag_of_words = vec.transform(text)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq [:n]","3f9cfb92":"#Option 1\ncommon_words_all = get_top_n_words(AllReviews, 10, 1)\ncommon_words_all","30a68ba5":"#Option2\nprint(\"Top 10 words appearing in all reviews for INSTANT_VIDEO\",get_top_n_words(instant_video['reviewText'], n=10,k=1))","d3be7f1e":"pd.set_option('display.max_colwidth', 50)","05474995":"instant_video['reviewText'].str.contains(\"34\")","cc240829":"#Option1\npd.set_option('display.max_colwidth', None)\n#pd.set_option('display.max_colwidth', -1)\ninstant_video[instant_video['reviewText'].str.contains(\"34\")].head()","91e6dc31":"#Option1\ninstant_video['reviewText'] = instant_video['reviewText'].apply(lambda x: x.replace('&#34', ''))","24dd9c71":"#Option2\ndef replace_fun (x):\n    return x.replace('&#34', '')\n\ninstant_video['reviewText'] = instant_video['reviewText'].apply(replace_fun)","b7c37c7e":"print (len(instant_video[instant_video['reviewText'].str.contains(\"34\")]))","1de8d05a":"instant_video[instant_video['reviewText'].str.contains(\"34\")].head(5)","1184059c":"get_top_n_words(instant_video['reviewText'], n=10,k=1)","09784aef":"#Select thereshold\n#Option1\npositive_video = instant_video[instant_video['overall']>3]\nnegative_video = instant_video[instant_video['overall']<4]\n#Option2\nnegative_video = instant_video[instant_video['overall']<=3]\n\n# Get the review texts\npositive_reviews = positive_video['reviewText']\nnegative_reviews = negative_video['reviewText']","d499083f":"#Option1\n#get_top_n_words(positive_reviews, n=10,k=1)\n#Option2\nget_top_n_words(positive_video['reviewText'], n=10,k=1)","9bb3ea34":"get_top_n_words(negative_video['reviewText'], n=10,k=1)","2bd977b9":"# Get the n most common words of both positive and negative reviews\ncommon_words_positive = get_top_n_words(positive_reviews, 200, 1)\ncommon_words_negative = get_top_n_words(negative_reviews, 200, 1)","0d438db7":"common_words_positivedf = pd.DataFrame(common_words_positive, columns=['word','occurrences'])\ncommon_words_negativedf = pd.DataFrame(common_words_negative, columns=['word','occurrences'])\ncommon_words_positivedf.head()","170fcdc1":"import numpy as np\nnegative = np.array(common_words_negativedf['word'])\npositive = np.array(common_words_positivedf['word'])","01764fbb":"print (type(common_words_positive))\nprint (type(common_words_positive[0]))","f332136a":"#Suboption1\npositive = [item[0] for item in common_words_positive]\n\n#Suboption2\ndef extract(lst): \n    return [item[0] for item in lst]\n\npositive = extract(common_words_positive)\npositive","f5720668":"len(positive)","2ad6f4ca":"#Suboption1\nnegative = [item[0] for item in common_words_negative]\n\n#Suboption2\ndef extract(lst): \n    return [item[0] for item in lst]\n\nnegative = extract(common_words_negative)\nnegative","ac889304":"only_positive = set (positive) - (set(negative) & set (positive))\nonly_negative = set (negative) - (set(negative) & set (positive))\nonly_negative","cbf822aa":"only_negative = []\nfor i in negative:\n    if i in positive:\n        None\n    else:\n        only_negative.append(i)","57028ab7":"only_negative","2521d34d":"negative_withgood = [each for each in negative_video['reviewText'] if 'good' in each]\nnegative_withgood[0:10]","7fbd2d1a":"negative_video.head()","5169e97c":"negative_withgood = [each for each in negative_video['reviewText'] if 'good' in each]\nnegative_withgood","e6ba5862":"#Option1\nnegative_withgood = negative_video[negative_video['reviewText'].str.contains('good')]\nnegative_withgood.head()","84ee85f5":"def func (string):\n    return 'substring' in string","06756a32":"#Option2\nnegative_withgood = instant_video[instant_video['overall']<4]['reviewText'].where(instant_video['reviewText'].apply(lambda x: 'good' in x)).dropna()\nnegative_withgood = negative_video['reviewText'].where(instant_video['reviewText'].apply(lambda x: 'good' in x)).dropna()\n\nnegative_withgood.reset_index(drop=True,inplace=True)\nnegative_withgood.head()","b3f116e5":"# librer\u00eda para hacer el pos-tag\nnltk.download('averaged_perceptron_tagger')","6417afcd":"text = negative_withgood.iloc[0]\ntext","99b780bb":"sentence = nltk.sent_tokenize(text)\nsentence","83ff5024":"# Opcion 1\nwords = nltk.word_tokenize(sentence[0])\nprint (words)","06851b7e":"words=[word for word in words if word.isalpha()]\nprint (words)","f2e9c874":"new_words = []\nfor word in words:\n    if word.isalpha():\n        new_words.append(word)\n        \nprint(new_words)","77bcfad9":"#Opcion 2\nimport re\nmodified_sentence = re.sub(r'[^\\w\\s]','',sentence[0]) # ^\\w = \\W = ^[a-zA-Z0-9_]\nmodified_sentence","59feef42":"words = nltk.word_tokenize(modified_sentence)\nprint (words)","f4227545":"# Opcion3\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\nprint(tokenizer.tokenize(sentence[0]))","bb0db8c6":"#Opcion 1\nwordsdf = pd.DataFrame(words, columns=['word'])\nwordsdf","2d9a4ddf":"index_good = wordsdf[(wordsdf['word']=='good')].index\n\ninteresting = []\nfor i in range (len(index_good)):\n    interesting.append(wordsdf.word.iloc[index_good[i]+1])\n\nprint (interesting)","5ea34f31":"# Opcion 2 - Ahora no es con el dataframe sino trabajar directamente con la lista de palabras\ninteresting = []\n\nfor i in range (len(words)):\n    if words[i].lower()=='good':\n        interesting.append(words[i+1])\n        \nprint (interesting)","e5e88fb2":"interesting=[words[i+1] for i in range (len(words)) if words[i].lower()=='good']\nprint (interesting)","83f90cb4":"modified_sentence","49c69985":"re.findall(r'(?<=good\\s)(\\w+)', modified_sentence)","6746586e":"all_negative = ' '.join(negative_video['reviewText'])\nall_negative = re.sub (r'[^\\w\\s]',\"\",all_negative)\n#all_negative = re.sub (r'[\\W]',\" \",all_negative)\nall_negative","4af4905a":"print (re.findall(r'(?<=good\\s)(\\w+)', all_negative))","f35a8d0a":"def good_search_any (sentence, n):\n    modified_sentence = re.sub(r'[^\\w\\s]','',sentence)\n    words = nltk.word_tokenize(modified_sentence)\n    words = pd.DataFrame(words, columns=['word'])\n    index_good = words[(words['word']=='good')].index\n    interesting = []\n    for i in range (len(index_good)):\n        interesting.append(words.word.iloc[index_good[i]+n])\n    return (interesting)","bf51b562":"good_search_any (sentence[0], 1)","9899e04f":"def text_search (text):\n    interesting = []\n    for sentence in text:\n        interesting+=(good_search_any (sentence, 1))\n    return (interesting)","dcca30e5":"sentence","de2959e4":"text_search (sentence)","65a0ea96":"#\n#negative_withgood['any_after_good'] = negative_withgood.reviewText.apply(text_search)","b150b7ee":"sentence[0]","f9d4dc92":"words = nltk.word_tokenize(sentence[0])\nwords_pos = nltk.pos_tag(words)\nwords_pos","575435f1":"words_pos = pd.DataFrame(words_pos, columns=['word','pos'])\nwords_pos","a5e928b6":"words_pos_selected = words_pos[words_pos['pos'].isin(['CD','NN','NNS','NNP','NNPS']) | (words_pos['word']=='good')]\nwords_pos_selected","703cb205":"#Opcion 1\nwords_pos_selected = words_pos_selected.reset_index()\nwords_pos_selected = words_pos_selected.drop(columns = ['index'])\nindex_good = words_pos_selected[(words_pos_selected['word']=='good')].index\nindex_good","8f2348d7":"words_pos_selected","9f7e0384":"# Metodo de buscar los indices en el dataframe\n# Antes de good\ninteresting = []\nfor i in range (len(index_good)):\n    if words_pos_selected.word.iloc[index_good[i]-1] != 'good':\n        interesting.append(words_pos_selected.word.iloc[index_good[i]-1])\n\nprint (interesting)","36b28da7":"# Despues de good\ninteresting = []\nfor i in range (len(index_good)-1):\n    if words_pos_selected.word.iloc[index_good[i]+1] != 'good':\n        interesting.append(words_pos_selected.word.iloc[index_good[i]+1])\nprint (interesting)","c79e9e25":"#Opcion 2\n' '.join(words_pos_selected.word)","eb6c9c1c":"# Comprimir el metodo del dataframe\n\ndef good_search (sentence, n):\n    words = nltk.word_tokenize(sentence)\n    words_pos = nltk.pos_tag(words)\n    pos_negative = pd.DataFrame(words_pos, columns=['word','pos'])\n    pos_negative = pos_negative[pos_negative['pos'].isin(['CD','NN','NNS','JJ'])]\n    indexnames = pos_negative[(pos_negative['word']!='good') & (pos_negative['pos']=='JJ')]\n    pos_negative = pos_negative.drop(indexnames.index)\n    pos_negative = pos_negative.reset_index()\n\n    index_good = pos_negative[(pos_negative['word']=='good')].index\n    interesting = []\n    for i in range (len(index_good)):\n        if pos_negative.word.iloc[index_good[i]+n] != 'good':\n            interesting.append(pos_negative.word.iloc[index_good[i]+n])\n    return (interesting)","ab91b6c9":"good_search (sentence[0], -1)","38986d27":"bigrams_positive = get_top_n_words(positive_video['reviewText'], 50, 2)\nbigrams_negative = get_top_n_words(negative_video['reviewText'], 50, 2)\nbigrams_positive = pd.DataFrame(bigrams_positive, columns=['words','occurrences'])\nbigrams_negative = pd.DataFrame(bigrams_negative, columns=['words','occurrences'])","ed9fea2f":"bigrams_positive","a4c3de3f":"bigrams_negative","d155958f":"negative = np.array(bigrams_negative['words'])\npositive = np.array(bigrams_positive['words'])\nonly_positivebi = set (positive) - (set(negative) & set (positive))\nonly_negativebi = set (negative) - (set(negative) & set (positive))","5efc7389":"print (only_positivebi)","62d89391":"print (only_negativebi)","b5a8da1e":"trigrams_positive = get_top_n_words(positive_video['reviewText'], 30, 3)\ntrigrams_negative = get_top_n_words(negative_video['reviewText'], 30, 3)\ntrigrams_positive = pd.DataFrame(trigrams_positive, columns=['words','occurrences'])\ntrigrams_negative = pd.DataFrame(trigrams_negative, columns=['words','occurrences'])","454c4cac":"trigrams_positive","9292c4a4":"print(only_negative)","6a222aad":"print(only_positive)","6d09467d":"example_sentence = 'That was a good, long movie'","24b45fcf":"words = nltk.word_tokenize(example_sentence)\nwords_pos = nltk.pos_tag(words)\nwords_pos = pd.DataFrame(words_pos, columns=['word','pos'])\nwords_pos","85267e82":"words = nltk.word_tokenize(example_sentence)\nwords_pos = nltk.pos_tag(words)\nwords_pos = pd.DataFrame(words_pos, columns=['word','pos'])\nwords_pos = words_pos[words_pos['pos'].isin(['NN','NNS','NNP','NNPS', 'JJ'])] \nwords_pos = words_pos.reset_index()\nwords_pos = words_pos.drop(columns=['index'])\nwords_pos","7b5aaa11":"words = nltk.word_tokenize(example_sentence)\nwords_pos = nltk.pos_tag(words)\nwords_pos","145b02f6":"def get_dataframe_pos (sentence):\n    words = nltk.word_tokenize(example_sentence)\n    words_pos = nltk.pos_tag(words)\n    words_pos = pd.DataFrame(words_pos, columns=['word','pos'])\n    words_pos = words_pos[words_pos['pos'].isin(['NN','NNS','NNP','NNPS', 'JJ'])]\n    words_pos = words_pos.reset_index()\n    words_pos = words_pos.drop(columns=['index'])\n    return (words_pos)","2289dc05":"def adj_noun_pairing (sentence):\n    \n    words_pos = get_dataframe_pos (sentence)\n    \n    interesting = []\n    for i in range (len(words_pos)-1):\n        if words_pos['pos'][i]=='JJ':\n            #print (words_pos['pos'][i])\n            phrase = []\n            phrase.append(words_pos['word'][i])\n            #phrase = words_pos['pos'][i]\n            #noun=False\n        \n        j=i\n        \n        while (j<=len(words_pos)): #and noun==False\n            j+=1\n            if (words_pos['pos'][j]=='NN' or words_pos['pos'][j]=='NNS'):\n                phrase.append(words_pos['word'][j])\n                #phrase += words_pos['word'][j]\n                #phrase = phrase + words_pos['word'][j]\n                #noun=True\n                break\n\n        interesting.append(phrase)\n        \n    return (interesting)","f98ce035":"adj_noun_pairing (example_sentence)","4a630be0":"words = nltk.word_tokenize(example_sentence)\nwords","96bcb75f":"adjectives = []\nnouns = []\nidx = 0\n    \nfor i in words:\n    tag =  nltk.pos_tag([i])[0][1]\n    if (tag == 'NN' | tag == 'NNS'):\n        nouns.append((i,idx))\n    if tag == 'JJ' or tag == 'RB':\n        adjectives.append((i,idx))\n    idx = idx + 1","71318218":"nouns","7524a49e":"adjectives","5544df4d":"def link_adjective_noun(sentence):\n    words = nltk.word_tokenize(sentence)\n    adjectives = []\n    nouns = []\n    idx = 0\n    \n    for i in words:\n        tag =  nltk.pos_tag([i])[0][1]\n        if tag == 'NN':\n            nouns.append((i,idx))\n        if tag == 'JJ' or tag == 'RB':\n            adjectives.append((i,idx))\n        idx = idx + 1\n        \n    r_list = []\n    \n    for i in adjectives:\n        for j in nouns:\n            if j[1]>i[1]:\n                r_list.append(i[0]+\" \"+j[0])\n                break\n    return r_list","2cdd59f8":"link_adjective_noun(example_sentence)","2a44ab11":"example_sentence2 = 'the bananas were delicious. However, grapes were not'","3ef5b33f":"example_sentence2 = 'delicious and juicy pears, but terrible grapes'\nexample_sentence3 = 'delicious bananas and pears, but terrible grapes'\n\ndef adj_noun_pairing (sentence):\n    \n    words = nltk.word_tokenize(sentence)\n    words_pos = nltk.pos_tag(words)\n    words_pos = pd.DataFrame(words_pos, columns=['word','pos'])\n    words_pos = words_pos[words_pos['pos'].isin(['NN','NNS','NNP','NNPS', 'JJ'])]\n    words_pos = words_pos.reset_index()\n    words_pos = words_pos.drop(columns=['index'])\n    \n    interesting = []\n    for i in range (len(words_pos)-1):\n        if words_pos['pos'][i]=='JJ':\n            phrase = []\n            phrase.append(words_pos['word'][i])\n            noun_condition = 1\n            adj_condition = 1\n            #To check that the appending stops in these two cases>\n            #noun_condition - delicious and juicy pears, but terrible grapes              \n            #adj_condition - delicious bananas and pears, but terrible grapes\n            #The loop ends when it finds a second adjective after the nouns after the original adjective\n            \n            while (noun_condition == 1 and adj_condition == 1):\n                i += 1\n\n                if (words_pos['pos'][i].isin(['NN','NNS','NNP','NNPS'])):\n                    phrase.append(words_pos['word'][i])\n                    noun_condition == 0\n                    print('noun condition')\n                if (words_pos['pos'][i]=='JJ'):    \n                    adj_condition == 0\n                    print ('adj condition')\n                if (i >= len(words_pos)-1):\n                    break\n                \n        interesting.append(phrase)\n                         \n    return (interesting)","c25a823b":"**Answer.**","1bb5ba07":"<h3>Exercise 1:<\/h3>\n<h4>1.1<\/h4>\n<p>Plot histograms of all numeric quantities. Do you notice anything interesting about them?<\/p>","142b4087":"Tambien se podria hacer de una vez con el dataframe que solo tiene las rese\u00f1as con good","9a73a9c8":"<h4>1.2<\/h4>\n<p>How do average ratings change over time? Plot the average rating for each year and note any trends.<\/p>","5bd2c416":"Option 2 - list comprehension","a276ad48":"-------","bf954e14":"**Answer.**","d9322b86":"str.contains('good') = 'good' in str","726f1ed9":"Agrupando por a\u00f1o, se pueden emplear las opciones de agrupaci\u00f3n ya aprendidas en el punto 1.2.","2faf76a4":"<h3>Exercise 2:<\/h3>\n<h4>2.1<\/h4>\n<p>Find the ten most frequently occuring non-stop words acrooss: (i) all reviews, (ii) positive reviews, (iii) negative reviews. Do the results surprise you? Why or why not?<\/p>","0298584f":"Ejemplo 1","b55689ef":"Option 1","4f24f16f":"Tokenizar por palabras","7d9513a2":"<h3>Exercise 7:<\/h3>\n<p>Write a function(s) that transforms a sentence into a new text list by iteratively pairing each adjective in the sentence with the next noun that follows it in the sentence. For example, the text \"That was a good, long movie\" should return <code>[\"good movie\", \"long movie\"]<\/code>.<\/p>","e41ad0fa":"Option 2","6bbdf272":"Tokenizar por oraciones","e3f45955":"-------","5f819a2f":"-------","c073bf30":"Option 1 - sets. The <code>set()<\/code> method is used to convert any of the iterable to sequence of unique iterable elements with dintinct elements, commonly called Set.","57c1f705":"**Answer.**","f78f6935":"<h4>2.2<\/h4>\n<p>Find words that are indicative of bad reviews. That is, words that appear often in bad reviews and <em>not<\/em> in good reviews. What are these words and are they surprising?<\/p>","8cf7db91":"Ahora para hallar la palabra (cualquiera) despues de good","fe5aa6e3":"-------","1d66a1eb":"The reason that the 34 is so repeated is because it corresponds in the Unicode Decimal Code <code>&#34<\/code> to the Quotation Mark symbol (\"). We could try to remove them","ef4f243a":"<h2>Examining the data<\/h2>\n<p>We take a look at the first 5 rows of each dataset to see what attributes are available. These are<\/p>\n<ul>\n<li><strong>reviewerID:<\/strong> A unique ID to identify the author of the review.<\/li>\n<li><strong>asin:<\/strong> The <a href=\"https:\/\/www.amazon.com\/gp\/seller\/asin-upc-isbn-info.html\">\"Amazon Standard Identification Number\"<\/a> which provides more information about the exact product and version.<\/li>\n<li><strong>reviewerName:<\/strong> The username chosen by the reviewer.<\/li>\n<li><strong>helpful:<\/strong> A record of how many users indicated that the review was helpful\/not helpful.<\/li>\n<li><strong>reviewText:<\/strong> The full text of the review.<\/li>\n<li><strong>overall:<\/strong> The overall rating (1-5) left by the reviewer.<\/li>\n<li><strong>summary:<\/strong> A short version of the review, used as the title.<\/li>\n<li><strong>unixReviewTime:<\/strong> The date that the review was created, in <a href=\"https:\/\/en.wikipedia.org\/wiki\/Unix_time\">Unix Epoch<\/a> format.<\/li>\n<li><strong>reviewTime:<\/strong> A human readable date giving the day, month, and year.\n<\/font><\/li>\n<\/ul>","7d2feee3":"movies - &quot","a8397480":"Entender el modelo con una \u00fanica rese\u00f1a. Usaremos la primera rese\u00f1a del dataframe <code>negative_withgood<\/code>, que tiene las rese\u00f1as que contienen la palabra <code>good<\/code> seg\u00fan el filtro que se aplic\u00f3 en el anterior ejercicio. Usamos el comando <code>iloc<\/code> indicando la posici\u00f3n espec\u00edfica de la rese\u00f1a","62bf3b26":"**Answer.**","bf1823a5":"Duda Andres","28079740":"-------","f2aed332":"-------","762683be":"Usar un <code>lookbehind<\/code> de RegEx <code>?<=<\/code> se usa para lookbehind y <code>?=<\/code> para lookahead","d163dbf2":"Tokenizar la frase y a\u00f1adir POS tagging con los m\u00e9todos previamente vistos","05337bdd":"**Answer.**","c7d47d04":"Tambien se puede usar un RegexpTokenizer que coge las secuencias alfannum\u00e9ricas y elimina lo dem\u00e1s\n\n<code>\\w<\/code> - car\u00e1cter alfanum\u00e9rico, <code>+<\/code> Indica que suma todos los caracteres aceptados continuos que se encuentre, es un operador de repetici\u00f3n\n\n<code>\\w<\/code> es equivalente a <code>[a-zA-Z0-9_]<\/code>\n","3c1870dc":"-------","55121ee4":"**Answer.**","c12f450f":"CD cardinal digit\n\nNN noun, singular \u2018desk\u2019\n\nNNS noun plural \u2018desks\u2019\n\nNNP proper noun, singular \u2018Harrison\u2019\n\nNNPS proper noun, plural \u2018Americans\u2019","46138307":"<h3>Exercise 6:<\/h3>\n<p>Throughout the above search for informative words, we have seen that unigrams are not enough, but important words (such as \"good\") are not always next to the informative words that they describe. Devise a method to extract these informative words. Provide a brief description of how you will extract the informative words.<\/p>","b14a2ee9":"Ya lo tenemos para el caso particular de una oraci\u00f3n, ahora lo extendemos","f195604d":"Ejemplo 2","772d4a37":"-------","4a0c6667":"### Palabras antes y despues de good que sean sustantivos o cardinales","e8b2c251":"-------","186f9bf8":"<h3>Exercise 3:<\/h3>\n<p>Manually inspect the first 10 negative reviews containing the word \"good\". What do you notice? How does this suggest we ought to proceed next?<\/p>","88b93d95":"<h4>1.3<\/h4>\n<p>Look at the average length of the review by year. Do you notice any trends?<\/p>","2ac48cb4":"wonderful, awesome, favorite, excellent, perfect, nice, amazing, loved (verb)","c6a1c0b7":"**Answer.**","cabdce1f":"<h1>Generating useful features for further analysis on Amazon reviews<\/h1>\n<h2>Introduction<\/h2>\n<p><strong>Business Context.<\/strong> You are a business consultant with new clients that are interested in analyzing reviews of their products on Amazon (as opposed to Yelp). They want to answer business questions like: \"What are the most important factors driving negative reviews?\", \"Have there been any large changes to customer satisfaction\/reviews over time?\", etc.<\/p>\n<p><strong>Business Problem.<\/strong> Your main task is to <strong>explore the given data and use the results of your investigation to engineer relevant features that could facilitate subsequent analysis and model-building<\/strong>.<\/p>\n<p><strong>Analytical Context.<\/strong> The dataset provided is a large body of reviews related to movies and television left on Amazon between 1996 and 2014. When exploring our dataset, we will quickly encounter a familiar problem we discussed in the previous case: the word \"good\" is one of the most important words in both positive <em>and<\/em> negative reviews. Thus, we must develop methods to put \"good\" in the appropriate context.<\/p>","f1d182d9":"(i) All reviews","70f5679d":"Positive reviews","ab7c1c55":"We could just get the 11 most frequent words <code>common_words_all = get_top_n_words(AllReviews, 11, 1)<\/code> and delete the 34 or we could try to guess why is 34 between the options.","33a2e5cd":"<h2>Loading the data<\/h2>\n<p>We use a dataset of around 37,000 video reviews from Amazon Instant Video and 1,700,000 movie and TV reviews, all obtained from the website: http:\/\/jmcauley.ucsd.edu\/data\/amazon\/. Note that there are much larger datasets available at the same site. We can expect better and more consistent results on larger datasets (such as book reviews). Note that these datasets are compressed (gzipped), and they are in <a href=\"https:\/\/en.wikipedia.org\/wiki\/JSON\">JSON<\/a> format, with each line representing a review and each line being its own JSON object.<\/p>\n<p>We begin by loading the dataset below:<\/p>","5fb725d2":"<h3>Exercise 5:<\/h3>\n<p>We have seen that individual words are not always very informative. Look for the most informative bigrams and trigrams, in both positive and negative reviews. Show the most informative bigrams and trigrams and give a brief analysis of the n-grams you identified.<\/p>","6166e1c1":"Now if we try to run again:","95bb6d05":"wrong, overall, instead, felt, stupid, problem, boring...","058dfc56":"**Answer.**","ac6f4409":"**Answer.**","6271f89e":"**Answer.**","afa91998":"<p>We notice that <code>movies_tv<\/code> is extremely long with nearly 2 million reviews, and several columns seem uninteresting or hard to work with (e.g. <code>reviewerID<\/code>, <code>asin<\/code>, <code>reviewername<\/code>, <code>reviewtime<\/code>). We drop some information to make some of our later analysis more efficient. We also add a datetime column with Python datetime objects to more easily summarize the data:<\/p>","b0ed682b":"Negative Reviews","beb27fe7":"-------","77af6ebe":"<h3>Exercise 4:<\/h3>\n<p>Go through the list of bad reviews containing the word \"good\" that we found in the last question. For each review, extract the following:<\/p>\n<ol>\n<li>The first word after \"good\"<\/li>\n<li>The first word after \"good\" that is a noun or cardinal<\/li>\n<li>The last word before \"good\" that is a noun or cardinal<\/li>\n<\/ol>"}}