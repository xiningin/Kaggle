{"cell_type":{"380482e1":"code","0e7229ed":"code","20855c7a":"code","255266f9":"code","998f46f0":"code","a33f466a":"code","2e57dc13":"code","8996623a":"code","6fcf7dc0":"code","207d9ca3":"code","8e8d7bc0":"code","10f0d106":"code","3fbe87da":"code","5bad6c32":"code","ab2d5e09":"code","a2aa169b":"code","6edb6d54":"code","64e61361":"code","c8205019":"code","85ac07f3":"code","6c49145c":"code","90956023":"code","3634f637":"code","385172a6":"code","a84067f2":"code","fa387b0f":"code","12139da7":"code","1878ffe8":"code","963d85c0":"code","192e91f6":"code","b8e0634e":"code","7de96625":"code","43a63060":"code","ba6c5764":"code","8f1236fe":"code","543fdb4d":"code","982c4932":"code","f9ab40c5":"markdown","2673fdf2":"markdown","1e6255f6":"markdown","3cb8e4ac":"markdown","2cd6f163":"markdown","8ef940af":"markdown","0e71f986":"markdown","b9cf1c73":"markdown","555dd574":"markdown","10aaa7d8":"markdown","dedee895":"markdown"},"source":{"380482e1":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Model\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Verbosity is now 5\n\ntf.autograph.set_verbosity(0)\n","0e7229ed":"!nvidia-smi","20855c7a":"data=pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')","255266f9":"data.info()","998f46f0":"np.unique(data.Score,return_counts=True)","a33f466a":"data.head()","2e57dc13":"data.shape","8996623a":"data.loc[data['Score'] <=2, 'Score'] = 0\ndata.loc[data['Score'] > 3, 'Score'] = 1","6fcf7dc0":"data.drop(data[data['Score']==3].index,inplace=True)","207d9ca3":"data['len'] = data.Text.apply(lambda x: len(x.split()))","8e8d7bc0":"data = data[data.len<50]","10f0d106":"data = data.sample(n=50000, random_state=30)","3fbe87da":"import re","5bad6c32":"data['Text']=data['Text'].apply(lambda row : re.sub('<.*?>','',row))","ab2d5e09":"from sklearn.model_selection import train_test_split","a2aa169b":"X_train, X_test, y_train, y_test = train_test_split(data[['Text','len']],data.Score, test_size=0.2, random_state=42, stratify=data.Score)","6edb6d54":"X_train.shape,X_test.shape","64e61361":"max_seq_length = 55\ntf.keras.backend.clear_session()\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n                # ids of words\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n                # masking of ids\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n                # segment of ids\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=False)\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\nbert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)","c8205019":"bert_model.summary()","85ac07f3":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()","6c49145c":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","90956023":"import tokenization","3634f637":"def bert_preprocess(sentence,tokenizer,max_seq_len=512):\n    tokens=[]\n    masks=[]\n    segments=[]\n    for word in sentence:\n        word=tokenizer.tokenize(word)\n        word=word[:max_seq_len-2]\n        seq=[\"[CLS]\"] + word + [\"[SEP]\"]\n        token = tokenizer.convert_tokens_to_ids(seq)\n        padding_tokens=token+[0]*(max_seq_len-len(token))\n        mask=[1]*len(seq)\n        masking=mask+[0]*(max_seq_len-len(token))\n        segment=np.zeros(max_seq_length)\n        \n        tokens.append(padding_tokens)\n        masks.append(masking)\n        segments.append(segment)\n    return np.array(tokens),np.array(masks),np.array(segments)","385172a6":"tokenizer =tokenization.FullTokenizer(vocab_file,do_lower_case)","a84067f2":"bert_model.input","fa387b0f":"bert_model.output","12139da7":"X_train_tokens, X_train_mask, X_train_segment=bert_preprocess(X_train.Text.values,tokenizer,55)\nX_test_tokens, X_test_mask, X_test_segment=bert_preprocess(X_test.Text.values,tokenizer,55)","1878ffe8":"X_train_pooled_output=bert_model.predict([X_train_tokens,X_train_mask,X_train_segment])\nX_test_pooled_output=bert_model.predict([X_test_tokens,X_test_mask,X_test_segment])","963d85c0":"from tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.models import Model","192e91f6":"import datetime\nfrom sklearn.metrics import roc_auc_score","b8e0634e":"auc=[]\nval_auc=[]\nclass LossHistory(tf.keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.history={'val_accuracy': []}\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        ## on end of each epoch, we will get logs and update the self.history dict\n        if logs.get('val_accuracy', -1) != -1:\n            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n        auc_tr=(roc_auc_score(y_train,(self.model.predict(X_train_pooled_output))) )\n        auc_test=(roc_auc_score (y_test, (self.model.predict(X_test_pooled_output) ) ) )\n        print('\\n  train_auc : ',auc_tr)\n        print('  val_auc : ',auc_test)\n        auc.append(auc_tr)\n        val_auc.append(auc_test)\n        loss=logs.get('loss')\n","7de96625":"#Callbacks\nfilepath=\"model1_save\/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')\n\nhistory_own = LossHistory()\n\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max', patience=2, verbose=1)","43a63060":"##create an NN and \ninput = Input(shape=[768,],name='input_layer')\ndense1=Dense(400, activation='relu',name='dense_1')(input)\ndense2=Dense(220, activation='relu',name='dense_2')(dense1)\ndense3=Dense(40, activation='relu',name='dense_3')(dense2)\noutput=Dense(1, activation='sigmoid',name='dense_4')(dense3)\nmodel1 = Model(inputs=input, outputs=output)\ntf.keras.utils.plot_model(model1,\"model_1.png\",show_shapes=True)","ba6c5764":"opt= tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel1.compile(optimizer=opt,loss=tf.keras.losses.BinaryCrossentropy(),metrics='accuracy')","8f1236fe":"X_test_pooled_output.shape","543fdb4d":"!rm -rf .\/logs_model1\/ \ntf.keras.backend.clear_session()\nmodel1.fit(X_train_pooled_output,y_train,validation_data=(X_test_pooled_output,y_test),epochs=50,callbacks=[history_own])","982c4932":"import matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\nplt.figure(figsize=(12,10))\nplt.plot(np.arange(len(auc)),auc,color=\"red\")\nplt.plot(np.arange(len(auc)),val_auc,color='blue')\nplt.scatter(np.arange(len(auc)),auc,color='red')\nplt.scatter(np.arange(len(auc)),val_auc,color='blue')\nplt.xticks(np.arange(0,len(auc)+1,5),rotation=-35)\nplt.xlabel('Epochs')\nplt.ylabel('AUC Score')\nplt.legend(['Train AUC','Test AUC'])\nplt.grid()\nplt.show()","f9ab40c5":"### We are considering first 50 words only ","2673fdf2":"# Read data","1e6255f6":"### We are doing sentiment analysis hence we will convert label to 0 or 1\n### 0: Negative Review\n### 1: Positive Review","3cb8e4ac":"### Removing html tags","2cd6f163":"### Unique labels","8ef940af":"### Dropping Score=3 as it is neutral value","0e71f986":"# We have achieved 95%+ AUC without overfitting model","b9cf1c73":"### There are no null values","555dd574":"If you want to take fix data, then use below line","10aaa7d8":"Splitting of data","dedee895":"we are importing bert layer from tensorflow hub and creating a model with pooled output only so it will take less time to train"}}