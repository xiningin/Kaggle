{"cell_type":{"3905f32c":"code","cdb7aff3":"code","b66da672":"code","00152c8c":"code","38297b6f":"code","57b39bbe":"code","c10d397e":"code","90cff49a":"code","ca617950":"code","91d6e821":"code","6e5d1f58":"code","4c6a19b7":"code","0647a1b4":"code","30dc0b4e":"code","78d6e481":"code","a40b8e2e":"code","aad6ed15":"code","059bbb84":"code","6920b662":"code","6f6de048":"code","25f3a400":"code","b8c874b6":"code","1241fd45":"code","6c946d12":"code","148e030b":"code","4a4b45e8":"code","7fa38a20":"code","12b84435":"code","35b571a0":"code","2684f742":"code","50169496":"markdown","af7ac559":"markdown","9b6e3b7f":"markdown","4be711f2":"markdown","d6254147":"markdown","d42466d4":"markdown","8c85acad":"markdown","423c0087":"markdown","d306cb64":"markdown","3a075271":"markdown","f8ba7b15":"markdown","19815763":"markdown","e9813d37":"markdown","a765f280":"markdown","eb4bcf39":"markdown","7d7d54f2":"markdown","801d4e79":"markdown","dcaf2535":"markdown","e42751b6":"markdown","b8e62292":"markdown","8a71c0b0":"markdown","7022fb01":"markdown","8d835e11":"markdown","8b4036a1":"markdown","0811fcf7":"markdown","6390dc0f":"markdown","074793a2":"markdown","acaf61ad":"markdown","f7e7f4b0":"markdown"},"source":{"3905f32c":"import os\nimport re\n\nimport numpy as np\nimport tensorflow as tf\n\nnp.random.seed(1)\ntf.set_random_seed(2)\n\nimport pandas as pd\nimport keras\n# from tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import f1_score, classification_report, log_loss\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\nfrom keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nprint(os.listdir('..\/input'))","cdb7aff3":"#training constants\nMAX_SEQ_LEN = 25 #this is based on a quick analysis of the len of sequences train['text'].apply(lambda x : len(x.split(' '))).quantile(0.95)\nDEFAULT_BATCH_SIZE = 128","b66da672":"data = pd.read_csv('..\/input\/first-gop-debate-twitter-sentiment\/Sentiment.csv')\n# data = data[data['sentiment'] != 'Neutral']\ntrain, test = train_test_split(data, random_state = 42, test_size=0.1)\nprint(train.shape)\nprint(test.shape)","00152c8c":"# Mapping of common contractions, could probbaly be done better\nCONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n                       \"It's\": 'It is', \"Can't\": 'Can not',\n                      }","38297b6f":"def clean_text(text, mapping):\n    replace_white_space = [\"\\n\"]\n    for s in replace_white_space:\n        text = text.replace(s, \" \")\n    replace_punctuation = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\", \"\\'\", r\"\\'\"]\n    for s in replace_punctuation:\n        text = text.replace(s, \"'\")\n    \n    # Random note: removing the URL's slightly degraded performance, it's possible the model learned that certain URLs were positive\/negative\n    # And was able to extrapolate that to retweets. Could also explain why re-training the Embeddings improves performance.\n    # remove twitter url's\n#     text = re.sub(r\"http[s]?:\/\/t.co\/[A-Za-z0-9]*\",\"TWITTERURL\",text)\n    mapped_string = []\n    for t in text.split(\" \"):\n        if t in mapping:\n            mapped_string.append(mapping[t])\n        elif t.lower() in mapping:\n            mapped_string.append(mapping[t.lower()])\n        else:\n            mapped_string.append(t)\n    return ' '.join(mapped_string)","57b39bbe":"# Get tweets from Data frame and convert to list of \"texts\" scrubbing based on clean_text function\n# CONTRACTION_MAPPING is a map of common contractions(e.g don't => do not)\ntrain_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in train['text'].values]\ntest_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in test['text'].values]\n\n\n# tokenize the sentences\ntokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(train_text_vec)\ntrain_text_vec = tokenizer.texts_to_sequences(train_text_vec)\ntest_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n\n# pad the sequences\ntrain_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\ntest_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n\nprint('Number of Tokens:', len(tokenizer.word_index))\nprint(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n\nprint('Sample Tweet Before Processing:', train[\"text\"].values[0])\nprint('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n\nprint('What the model will interpret:', train_text_vec[0].tolist())\n","c10d397e":"# One Hot Encode Y values:\nencoder = LabelEncoder()\n\ny_train = encoder.fit_transform(train['sentiment'].values)\ny_train = to_categorical(y_train) \n\ny_test = encoder.fit_transform(test['sentiment'].values)\ny_test = to_categorical(y_test) ","90cff49a":"# get an idea of the distribution of the text values\nfrom collections import Counter\nctr = Counter(train['sentiment'].values)\nprint('Distribution of Classes:', ctr)\n\n# get class weights for the training data, this will be used data\ny_train_int = np.argmax(y_train,axis=1)\ncws = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\nprint(cws)","ca617950":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\nnp.set_printoptions(precision=4)\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    # Compute confusion matrix\n    classes = classes[unique_labels(y_true, y_pred)]\n    _cm = confusion_matrix(y_true, y_pred)\n\n    print(classification_report(y_true, y_pred, target_names=classes))\n        \n    def _build_matrix(fig, ax, cm, normalize = False):\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n        \n        if normalize:\n            cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n        im = ax.imshow(cm, cmap=cmap)\n#         fig.colorbar(im, ax=ax)\n        \n        # We want to show all ticks...\n        ax.set(xticks=np.arange(cm.shape[1]),\n               yticks=np.arange(cm.shape[0]),\n               # ... and label them with the respective list entries\n               xticklabels=classes, \n               yticklabels=classes,\n               title=title,\n               ylabel='True label',\n               xlabel='Predicted label')\n\n        # Rotate the tick labels and set their alignment.\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n        # Loop over data dimensions and create text annotations.\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() \/ 2.\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                ax.text(j, i, format(cm[i, j], fmt),\n                        ha=\"center\", va=\"center\",\n                        color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    fig, [ax1, ax2] = plt.subplots(nrows = 1, ncols = 2, figsize=(8, 4))\n    _build_matrix(fig, ax1, cm = _cm, normalize=False)\n    _build_matrix(fig, ax2, cm = _cm, normalize=True)\n    fig.tight_layout()\n","91d6e821":"# \nprint('Dominant Class: ', ctr.most_common(n = 1)[0][0])\nprint('Baseline Accuracy Dominant Class', (ctr.most_common(n = 1)[0][0] == test['sentiment'].values).mean())\n\npreds = np.zeros_like(y_test)\npreds[:, 0] = 1\npreds[0] = 1 #done to suppress warning from numpy for f1 score\nprint('F1 Score:', f1_score(y_test, preds, average='weighted'))\n","6e5d1f58":"# Naive Bayse Baseline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])\ntext_clf.fit(tokenizer.sequences_to_texts_generator(train_text_vec), y_train.argmax(axis=1))\npredictions = text_clf.predict(tokenizer.sequences_to_texts_generator(test_text_vec)) \nprint('Baseline Accuracy Using Naive Bayes: ', (predictions == y_test.argmax(axis = 1)).mean())\nprint('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions, average='weighted'))\n\n_ = plot_confusion_matrix(y_test.argmax(axis = 1), predictions, classes=encoder.classes_, title='Confusion matrix, without normalization')","4c6a19b7":"# Random Forest Baseline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', RandomForestClassifier(class_weight='balanced', n_estimators=100)), #100 estimators will be the new default in version 0.22\n])\ntext_clf.fit(tokenizer.sequences_to_texts_generator(train_text_vec), y_train.argmax(axis=1))\npredictions = text_clf.predict(tokenizer.sequences_to_texts_generator(test_text_vec)) \nprint('Baseline Accuracy Using RFC: ', (predictions == y_test.argmax(axis = 1)).mean())\nprint('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions, average='weighted'))\n\n_ = plot_confusion_matrix(y_test.argmax(axis = 1), predictions, classes=encoder.classes_)","0647a1b4":"\ndef threshold_search(y_true, y_proba, average = None):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold, average=average)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\n\n\ndef train(model, \n          X_train, y_train, X_test, y_test, \n          checkpoint_path='model.hdf5', \n          epcohs = 25, \n          batch_size = DEFAULT_BATCH_SIZE, \n          class_weights = None, \n          fit_verbose=2,\n          print_summary = True\n         ):\n    m = model()\n    if print_summary:\n        print(m.summary())\n    m.fit(\n        X_train, \n        y_train, \n        #this is bad practice using test data for validation, in a real case would use a seperate validation set\n        validation_data=(X_test, y_test),  \n        epochs=epcohs, \n        batch_size=batch_size,\n        class_weight=class_weights,\n         #saves the most accurate model, usually you would save the one with the lowest loss\n        callbacks= [\n            ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True),\n            EarlyStopping(patience = 2)\n        ],\n        verbose=fit_verbose\n    ) \n    print(\"\\n\\n****************************\\n\\n\")\n    print('Loading Best Model...')\n    m.load_weights(checkpoint_path)\n    predictions = m.predict(X_test, verbose=1)\n    print('Validation Loss:', log_loss(y_test, predictions))\n    print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n    print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))\n    plot_confusion_matrix(y_test.argmax(axis = 1), predictions.argmax(axis = 1), classes=encoder.classes_)\n    plt.show()    \n    return m #returns best performing model","30dc0b4e":"def model_1():\n    model = Sequential()\n    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n    model.add(LSTM(128))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nm1 = train(model_1, \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           checkpoint_path='model_1.h5',\n           class_weights=cws\n          )\n","78d6e481":"def model_1b():\n    \"\"\"\n    Using a Bidiretional LSTM. \n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25)))\n    model.add(Dense(64, activation='relu'))\n#     model.add(Dropout(0.3))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n_ = train(model_1b, \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           checkpoint_path='model_1b.h5',\n           class_weights=cws,\n           print_summary = True\n          )\n","a40b8e2e":"def model_1c():\n    \"\"\"\n    Adding dropout to reduce overfitting using a bidiretional LSTM\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Conv1D(64, 4))\n#     model.add(Flatten())\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#     print(model.summary())\n    return model\n\n\n_ = train(model_1c, \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           checkpoint_path='model_1c.h5',\n           class_weights=cws,\n           print_summary = True\n          )\n","aad6ed15":"def model_1d():\n    \"\"\"\n    Just for fun below is a model only using covolutions. This is pretty good and also trains very quickly(and predictions would also likely be fast) compared to the LSTM...\n    It's equivalent to using an n-gram based approach.\n    Usually in practice you would use a more complex architecture with multiple parallel convolutions that are combined before pooling(and usually both max and avg).\n    Pure Convolutional NLP is definitely a solution worth exploring further.\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Conv1D(64, 5))\n    model.add(Conv1D(64, 3))\n    model.add(Conv1D(64, 2))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\n_ = train(model_1d, \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           checkpoint_path='model_1d.h5',\n           class_weights=cws,\n           print_summary = True\n          )","059bbb84":"def get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\ndef get_embdedings_matrix(embeddings_index, word_index, nb_words = None):\n    all_embs = np.stack(embeddings_index.values())\n    print('Shape of Full Embeddding Matrix', all_embs.shape)\n    embed_dims = all_embs.shape[1]\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n\n    #best to free up memory, given the size, which is usually ~3-4GB in memory\n    del all_embs\n    if nb_words is None:\n        nb_words = len(word_index)\n    else:\n        nb_words = min(nb_words, len(word_index))\n    \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_dims))\n    found_vectors = 0\n    words_not_found = []\n    for word, i in tqdm(word_index.items()):\n        if i >= nb_words: \n            continue\n        embedding_vector = None\n        if word in embeddings_index:\n            embedding_vector = embeddings_index.get(word)\n        elif word.lower() in embeddings_index:\n            embedding_vector = embeddings_index.get(word.lower())\n        # for twitter check if the key is a hashtag\n        elif '#'+word.lower() in embeddings_index:\n            embedding_vector = embeddings_index.get('#'+word.lower())\n            \n        if embedding_vector is not None: \n            found_vectors += 1\n            embedding_matrix[i] = embedding_vector\n        else:\n            words_not_found.append((word, i))\n\n    print(\"% of Vectors found in Corpus\", found_vectors \/ nb_words)\n    return embedding_matrix, words_not_found","6920b662":"def load_glove(word_index):\n#     print('Loading Glove')\n    embed_file_path = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_file_path)))\n    print(\"Built Embedding Index:\", len(embeddings_index))\n    return get_embdedings_matrix(embeddings_index, word_index)\n\ndef load_twitter(word_index):\n#     print('Loading Twitter')\n    embed_file_path = '..\/input\/glove-twitter-27b-200d-txt\/glove.twitter.27B.200d.txt'\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_file_path)))\n    print(\"Built Embedding Index:\", len(embeddings_index))\n    return get_embdedings_matrix(embeddings_index, word_index)\n","6f6de048":"print('Loading Glove Model...')\nglove_embed_matrix, words_not_found =  load_glove(tokenizer.word_index)","25f3a400":"print('Loading Twitter Model...')\ntwitter_embed_matrix, words_not_found =  load_twitter(tokenizer.word_index)","b8c874b6":"def model_2(embed_matrix):\n    \"\"\"\n    Extends model_1 with a glove embedding\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = embed_matrix.shape[0], output_dim = embed_matrix.shape[1], input_length = MAX_SEQ_LEN,  weights=[embed_matrix], trainable=False))\n    model.add(LSTM(128))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\nm2 = train(lambda : model_2(glove_embed_matrix), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           checkpoint_path='model_2.h5',\n           class_weights=cws,\n           fit_verbose = 2,\n           print_summary = False\n          )","1241fd45":"def model_3(embed_matrix):\n    \"\"\"\n    Extends model 1c, will be trained with multiple embeddings\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = embed_matrix.shape[0], output_dim = embed_matrix.shape[1], input_length = MAX_SEQ_LEN,  weights=[embed_matrix], trainable=False))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Conv1D(64, 4))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","6c946d12":"print(\"Model 3 w\/ Glove Embedding\")\n_ = train(lambda : model_3(glove_embed_matrix), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=0,\n           print_summary = False\n\n          )\n\nprint(\"\\n++++++++++++++++++++++++++++++++++++++++++\\n\")\n\nprint(\"Model 3 w\/ Twitter Embedding\")\n_ = train(lambda : model_3(twitter_embed_matrix), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=0,\n           print_summary = False\n\n          )\n\nprint(\"\\n++++++++++++++++++++++++++++++++++++++++++\\n\")\n\nprint(\"Model 3 w\/ Stacked Embedding\")\n_ = train(lambda : model_3(np.hstack((twitter_embed_matrix, glove_embed_matrix))), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=0,\n           print_summary = False\n\n          )\n\n","148e030b":"def model_4(embed_matrix):\n\n    model = Sequential()\n    model.add(Embedding(input_dim = embed_matrix.shape[0], output_dim = embed_matrix.shape[1], input_length = MAX_SEQ_LEN,  weights=[embed_matrix], trainable=False))\n    model.add(SpatialDropout1D(0.25))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Conv1D(64, 4))\n    model.add(Conv1D(32, 4))\n    model.add(Conv1D(16, 4))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\n_ = train(lambda : model_4(np.hstack((twitter_embed_matrix, glove_embed_matrix))), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=2,\n           print_summary = True\n          )\n\n","4a4b45e8":"def model_5(embed_matrix):\n    \"\"\"\n    Extends Model 3, but makes the embedding trainable\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = embed_matrix.shape[0], output_dim = embed_matrix.shape[1], input_length = MAX_SEQ_LEN,  weights=[embed_matrix], trainable=True))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Conv1D(64, 4))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\nm5 = train(lambda : model_5(np.hstack((twitter_embed_matrix, glove_embed_matrix))), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=2,\n           print_summary = True\n          )","7fa38a20":"def model_6(embed_matrix):\n    \"\"\"\n    Extends Model 5 and adds another Bidirectional LSTM layer\n    \"\"\"\n    model = Sequential()\n    model.add(Embedding(input_dim = embed_matrix.shape[0], output_dim = embed_matrix.shape[1], input_length = MAX_SEQ_LEN,  weights=[embed_matrix], trainable=True))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n    model.add(Conv1D(64, 4))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\nm6 = train(lambda : model_6(np.hstack((twitter_embed_matrix, glove_embed_matrix))), \n           train_text_vec,\n           y_train,\n           test_text_vec,\n           y_test,\n           class_weights=cws,\n           fit_verbose=2,\n           print_summary = False\n          )\n\n","12b84435":"preds = m6.predict(test_text_vec)\nprint('Prediction, based on highest class:', (preds.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n#print('Prediction, based on class > 0.5:', ((y_test * preds).max(axis = 1) > 0.5).mean())\n\n# Also consider searching the threshold, though this requires re-thinking the results, since you're now outputting up to 2 options for a class\n# But should help with calling out ambiguous cases\nthreshold = threshold_search(y_test, preds, average='weighted')\nprint('Threshold Search:', threshold)\n# print('Prediction, after Threshold Search:', (preds.argmax > threshold == y_test.argmax(axis = 1)).mean())","35b571a0":"from sklearn.metrics import confusion_matrix\nprint('Residuals Analysis:', )\nprint(confusion_matrix(y_test.argmax(axis = 1),preds.argmax(axis = 1)))\n\n\nctr = 0\nfor i in range(y_test.shape[0]):\n    true_label = y_test[i].argmax()\n    pred_label = preds[i].argmax()\n    if true_label != pred_label:\n        print('idx:', i)\n        print('True Label:', encoder.classes_[true_label])\n        print('Predicted Label:', encoder.classes_[pred_label])\n        print('Probability Prediction', preds[i])\n        print(test['sentiment'].values[i], '::',  test['text'].values[i], '\\n')\n        ctr += 1\n    \n    if ctr > 20:\n        break\n        \n","2684f742":"words_not_found[:20]","50169496":"## First Neural network is a simple implementation of an LSTM. The layers are as follows:\n* Embedding - Matrix of Word Vectors, where each vector store the \"meaning\" of the word. These can be trained on the fly or you can leverage existing pre-trained vector.\n* LSTM - Recurrent Neural Network that allows for the \"building\" of state over time\n* Dense(64) - Feed Forward Neural Net used to interpret the LSTM Output\n* Dense(3) - This it the output of the model, 3 nodes corresponding to each class. The softmax output will ensure that the sum of values = 1.0 for each output.\n\n### Even this basic architecture of just 3 Layers already beats the Random Forest Classifer in terms of Accuracy and F1 Score (resutls may depend on run)\n","af7ac559":"### Tokenize Text, which involves converting each sequence into an integer encoded represenation and normalizing the length of the sequences. Please note very little pre-processing is done to the text. More specifically there's no stemming or POS tagging, common in NLP tasks. Additionally, text is left in original case (i.e not cast to lowercase).\n\n### This is one of the main selling points of Deep Learning methods, that it's able to produce high quality results with minimal pre-processing or \"expert\" feature engineering. Though in practice adding \"novel\" features can improve the quality of the model (speed to convergence and accuracy of results). ","9b6e3b7f":"# [KD Nuggets Benchmarking RNNs to Machine Learning Based Approaches](https:\/\/www.kdnuggets.com\/2018\/07\/overview-benchmark-deep-learning-models-text-classification.html)\n\n![Image From KD Nuggets](https:\/\/ahmedbesbes.com\/images\/article_5\/rnn_cnn_model.png)","4be711f2":"## By blending the embeddings we should have a model that's a signicant improvement over the baselines (~5-7% increase in F1 Score).\n## Can we improve on the architecture further to increase our performance? Let's test a few hypotheses:","d6254147":"\n# Building(and Evolving) Neural Networks\n----\n\n###  So now that we've done all of the prep work it's time to do some actual Neural Network-ing\n### Neural Nets are like legos! And part of the fun (and frustration) comes from experimenting with different configurations to see which architecture produces the best results. One nice aspect of Neural Nets is that unlike other ML Approaches you can \"stack\" modules onto a Neural Net to improve there performance as we'll see below the first Neural Net will surpass the performance of the Random Forest, and by the end we should have one that has an F1 Score > ***0.7***, ~ 8% improvement (before any tuning). \n\n### To get there we'll train around 7 different architectures each one building on the prior.\n\n----\n","d42466d4":"### Prepping output data. In this case the sentiment is classified as Negative, Neutral or Positive. To make it suitable for a deep learning model each sentiment will be converted to a vector of length 3, where each position corresponds to a sentiment class: Negative = 0, Neutral = 1 and Positive = 2","8c85acad":"## Hypothesis 1: What if we extend the depth of the convolution (common in image classification)?\n## Result: Negligible","423c0087":"# Important Note, when Evaluating Models - Look at Loss not Accuracy....\n\n### For each model we'll produces several stats, including the Accuracy and F1, and Loss on the Validation Set. In most cases you want to minimize the Loss. But you might also find it helpful to maximize the Accuracy or F1-Score (or other performance metric). For more complicated metrics it might not be possible to directy optimize your model for that metric and you might want to consider alternative techniques (e.g. In this case I use Model CheckPointing and EarlyStopping based on Accuracy to ensure that I pick the \"best\" model. For Example, F1-Score is not differentiable, so there's no straighforward way (currently) to train your model to  optimize for it. Though Accuracy can be a good proxy to start.\n\n### In some cases below I'll make \"improvements\" that substantially decrease the Loss, but may have a nominal effect on the Accuracy(or even lower accuracy), building on those lower Loss solutions though will generally produce a better model.\n","d306cb64":"### Naive Bayes is a good baseline, but is not robust to imblanced  datasets, common with classification of \"real-world\" data. See [here](https:\/\/stats.stackexchange.com\/questions\/99667\/naive-bayes-with-unbalanced-classes) for some methods for overcoming class imbalance. In this case rebalancing the data would require throwing away a lot of negative examples or significantly oversampling the positive an neutral cases. \n\n### Another good baseline is a RandomForestClassifier, which for years prior to the Deep Learning explosion reigned as the model of choice for classifcation tasks, below is a simple implementation, with some recommended parameters (one specifically to handle classs imbalance, the other increases the \"complexity\" of the model to the new standard default).","3a075271":"### This will be used to weight the training, useful when you have large class imbalances, can speed training and improve results. Alternatively you can balance the classes in the input, so there are equal numbers","f8ba7b15":"# Basic Imports for a Deep Learning(DL) Project\n-----\n\nMost deep learning projects will use the following set of libraries:\n.\n* numpy  - Core numerical processing libarry in python, moslty a python wrapper for optimized C and Fortran.\n* pandas - Default Data processing libriary, stores a dataset in \"DataFrame\" object that's an in-memory columnar store(by default), builds upon numpy.\n* sklearn - Default Machine Learning library for python, lot's of useful data processing utils, works well with pandas.\n* keras - Deep Learning Library that wraps and abstracts tensorflow (or theano), provides a simple intutive high level API for developing deep learning models and is the most popular solution by far for writing Deep Learning Code. The specific layers you import from keras will differ based on the task. Below are some common Layers used in NLP(e.g. Embedding, LSTM, GRU)\n* tensorflow - Google's DL Library, Keras code runs tensorflow \"graphs\" to handle the computations, which are actually run in C++","19815763":"## Model 2: Takes the original Model model_1, and uses a pre-trained GloVe embedding. This should improve the loss slightly and depending on the case a small improvement in accuracy.","e9813d37":"## Hypothesis 2: What if we make the Embedding layer trainable, so that it's weights are updated, by the training data? This should help with addressing Out of Vocabulary Tokens.\n\n## Results: Provides another small boost in accuracy. Retraining the vectors is not alway recommended as you might \"lose\" useful information from the pre-trained embeddings, but in this context it proves helpful.","a765f280":"# Using Transfer Learning\n---\n### Already we have a pretty decent model that makes a sizable improvement upon our initial baselines, but we can do better. Currently we're training our word vectors from scratch and with such a small sample size it's unlikely that the model has sufficiently learned to distinguish the meaning or intent of all 21K tokens. \n\n### Fortunately, you can access pre-trained word vectors, trained against billions of documents. These are generally recommended for NLP tasks as they provide a substantial increase in performance over embeddings trained on the fly.\n\n### This is a form of Transfer Leanring where existing model weights are re-purposed for a new task (this much more prevalent amongst Image Classification tasks).\n\n### We'll use the GloVe Embeddings, which ar trained on several sources including Twitter(which is particularly relavent).","eb4bcf39":"## Below is a quick method used to train the models","7d7d54f2":"#  Import Data and Prep For Model \n----\n\nSteps:\n* Data is loaded into pandas\n* Data is split into train and test datasets (training is used to train the model and test is used to evaluate the predictions)\n * In most implementations you also have a validation dataset that you use to tune your model\n* Text Data is cleaned for common mappings\n* Data is tokenized into integer keys for lookups to the Embedding Matrix(discussed below)","801d4e79":"# Baselines\n---\n\nIt's highly recommended to define some baselines for the model for the following reasons:\n* It allows you to determine if your model beats basic ML approaches. Most Deep Learning models should improve over simple ML Algorithms so if you're not seeing that you might want to either tune your model or debug it for errors\n* It gives you an idea if the trade-offs are meaningful. Deep Learning usually requires more expensive comupte costs (e.g. GPUs) and depending on your goal, the increase in performance from a DL approach might not offset the added cost \/ time complexity. Simple Algorithms like Logistic Regression can even be implemented as in line SQL, while pretty much any DL task will require setting up additional infrastructure to deploy your model. \n\nSome common Baseline's:\n* Predict the dominat class: if you just guess the most frequent class what accuracy do you get? This is the simplest (and should be least accurate) model possible. In the case of twitter always assume the tweet is negative...\n* Other common classification models. In this case I tried NaiveBayes (an oldy but a goody) and RandomForestClassifier (the go to model for ML Tasks before Deep Learning became dominant). \n* Have an expert (or several) perform the same classification task themselves and use their Accuracy\/Error Rate as a Human Level Benchmark.\n\n\n\nThe goal is not to necessarily tune these to extract high performance, but just get a sense of how the perform (and if you're DL model is improving).\n\n","dcaf2535":"### Import data into pandas and split into train and test set (usually would also have a 3rd split for validation)","e42751b6":"## model_3 with glove, twitter and stacked embeddings\n","b8e62292":"## Reviewing our Baselines\n\nThe metric we'll be interested are Accuracy, which is self-explanatory and [F1 Score](https:\/\/en.wikipedia.org\/wiki\/F1_score), which blends the precision and recall scores for each class. F1 Score is generally a better measure of the performance of a model, especially when classes are imbalanced.\n\nBetween our Baselines the Random Forest Classifier performs the best by a large margin (mostly because it's robust to class imbalance). \n\n***Our goal is to see if we can produce a model that has a F1 Score > ~0.65***\n\nSpecifically, you'll see the model become better at distinguishing the Neutral and Positive Tweets from the Negative one's","8a71c0b0":"## Hypothesis 3: Let's try stacking the LSTM Layers.\n## At this point we should see minimal change from our prior model","7022fb01":"## Now that we have a good baseline Neural Network Architecture we'll begin building on it. By adding more layers and increasing the complexity.\n#### The first improvement is to use a Bi-Derectional LSTM. As the name suggest this Layer \"reads\" text both forwards and backwards and allows the model to get information from past and future states simultaneously. It also usually provides a nice boost in performance over the single pass LSTM. \n\n#### Dropout is also added to reduce overfitting, as the number of paramters in the model increases. A Dropout Layer, drops data from the input, but only during training, which encourages the model to be more \"robust\" and not become overly dependent on specific signals from the training data to make predictions. Since we have a relatively small data-set compared to our model size, drop-out is critical to ensure the model doesnt just quickly overfit the training set. ","8d835e11":"### Also, there are plenty of words that arent found in the corpus, that might help improve the interpretation of the text...","8b4036a1":"# But the question remains what will work better, the Standard GloVe Embedding the Twitter Embedding or Both?\n## In order to test that out we'll run an experiment taking the architecture from model_1c and running it with each variation of the embedding. This experiment should provide us with good data to make a choice on which embedding to use going forward.","0811fcf7":"### So why is the model plateua-ing?\n1. We havent done much hyper-parameter tuning, which might help further improve the model's accuracy (e.g. changing batch size, kernel size, LSTM capacity)?\n2. We have a small dataset and there's limited information we can extract from it.\n3. Human sentiment is a messy thing to measure, See below to look at some of the misclassifications. Would you have correclty classified them?","6390dc0f":"# Where to go from here? Next Steps\n----\n\n### The final model approaches an Accuracy\/F1 of 70 - 71%(almost 10% improvement in F1 from our baseline) and it seems like there might be a bit of plateau caused in-part by the amiguity of the data. So now that learning has plateau'd there are still some areas to explore to further improve on our model.\n1. Hyperparameter Tuning - Batch Size, DropOut Rate, LSTM Size, etc.\n2. Try to generate\/collect new data to help with performance (translation augmentation is one technique). \n3. Perform Threshold Search to see if that can improve the accuracy of the model, by changing the decision threshold for classifcation (default is 0.5). Usually good for a small \"free\" boost.\n4. Consider adding meta-data(likes, retweets, num followers) or additional statistical feature e.g. Count of Punctuation, Count all CAPS words, etc.\n5. Use cross validation to analyze architecture. In this example I mostly just did one pass on a give model, to see what choices would make the greatest impact, but when fine-tuning a model it's often helpful to use 10-Fold Cross Validation to confirm if the architectural change produces a statistically significant imrpovement, especiially when considering using the models in a commercial setting where cost would be a factor.\n6. Start Exploring more complex architectures to get an additional edge, sinc deep learning is an evolving field it's always good to keep reading and learning new techniques, which seem to get better every 2-3 months.","074793a2":"### All of the neural nets we'll be using are based on Recurrent Neural Network(RNN) Architecture. \n#### An RNN is a specific type of Deep Learning Architecture that's specially adapted to handle time dependent information such as human language (e.g. Text, Audio), signal outputs from an IoT Device, or any other Time Series based data(stock prices, though not recommeneded). An LSTM is a specific type of RNN and is used in a lot of commercial applications(Siri), also consider GRU's, which can occasionally outperform LSTMs on certain tasks and ther are more advanced models as well (like Attention, BERT, 1D Convolutions, Capsules, etc).\n\n### Below is an illustration of an RNN Style Architecture\n#### For each timestep you have some data that's input `x` and an activation `A`, which outputs a new state `h`, the important thing about RNN and LSTM's is that the state information from prior states is used as input to the next state, which means that the model builds a state vector overtime that incporporates prior information. This improves upon the bag-of-words models that are used in classical ML, which are word\/token based and don't normally incorporate the context that words are used in (e.g. \"the leaves are orange in fall\" vs \"he often leaves from work at noon\"). This was addressed through n-gram based models, but the exponential increase in input data makes such models intractable in most applications.\n\n![LSTM Drawing](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/RNN-unrolled.png)\n\n","acaf61ad":"# [Winning architecture for Quora Challenge](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568)\n![Imaget](https:\/\/i.imgur.com\/zUY9tVN.png)","f7e7f4b0":"### A modern architecture for Text Classification usually includes the addition of Convolutions stacked on top of each state vector in the LSTM(as opposed to just predicting based on the last state). This technique is borrowed from Image Classification and assumes that related information is locally \"grouped\" together (a kernel). It's common to either take the Average and\/or Max of the Convolutional Layer.\n\n###  It's definitely not a sure-fire choice, but can sometime perform better on certain tasks. Depending on the run this will perform as well or worse than the vanilla Bi-Directional LSTM, though we'll test it out later as well"}}