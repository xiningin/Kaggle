{"cell_type":{"79d91e1f":"code","5b16a3df":"code","93a44110":"code","037d99f0":"code","36147f07":"code","8b4611ef":"code","63b2fd3a":"code","fe1b44d7":"code","adb64bc4":"code","a3fae490":"code","7c197e09":"code","9f1e80e0":"code","b80abe3d":"code","e790dfb9":"code","09d75317":"code","47d792c5":"code","fba3851e":"code","d5ce60d2":"code","55a219ff":"code","2fc7c682":"code","d528167a":"code","1f3313a8":"code","ad91f9c8":"code","0697a5c3":"code","b90505de":"code","aaf7ba9b":"code","a436579e":"code","2ea10803":"code","19f288cb":"code","c4be4235":"code","e76829bf":"markdown","3cda8cad":"markdown","8effa07b":"markdown","00c84a1a":"markdown","a1c271b8":"markdown","86ce8e11":"markdown","f6e874b3":"markdown","b7b787ff":"markdown","03817840":"markdown","07dbc534":"markdown","c31d2681":"markdown","86593b9f":"markdown","73540c81":"markdown","48ceb69f":"markdown","24c77171":"markdown"},"source":{"79d91e1f":"from fastai.vision.all import *\nfrom tqdm.notebook import tqdm\nimport sklearn.feature_extraction.text\nfrom transformers import (BertTokenizer, AutoConfig, AutoModel)\nfrom sklearn.model_selection import StratifiedKFold\nimport regex","5b16a3df":"PATH = Path('..\/input\/shopee-product-matching')\n\nBERT_PATH = '..\/input\/bertindo15g'\nbert_model_file = '..\/input\/shopee-small-models\/bert_indo_val0.pth'\n\nimage_model_file = '..\/input\/shopee-small-models\/resnet18val0.pth'\n","93a44110":"# thees are thresholds used for Neighborhood Blending\nRECIPROCAL_THRESHOLD = .97\nMIN_PAIR_THRESHOLD = .6","037d99f0":"class BertTextModel(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_model = bert_model\n    def forward(self, x):\n        output = self.bert_model(*x)\n        return output.last_hidden_state[:,0,:]","36147f07":"def load_bert_model(fname):\n    model = AutoModel.from_config(AutoConfig.from_pretrained(BERT_PATH))\n    state = torch.load(fname)\n    model.load_state_dict(state)\n    return BertTextModel(model).cuda().eval()","8b4611ef":"#Taken from https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/233605#1278984\ndef string_escape(s, encoding='utf-8'):\n    return s.encode('latin1').decode('unicode-escape').encode('latin1').decode(encoding)\n\nclass TitleTransform(Transform):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n               \n    def encodes(self, row):\n        text = row.title\n        text=string_escape(text)\n        encodings = self.tokenizer(text, padding = 'max_length', max_length=100, truncation=True,return_tensors='pt')\n        keys =['input_ids', 'attention_mask', 'token_type_ids'] \n        return tuple(encodings[key].squeeze() for key in keys)\n\ndef get_text_dls():\n    tfm = TitleTransform()\n\n    data_block = DataBlock(\n        blocks = (TransformBlock(type_tfms=tfm), \n                  CategoryBlock(vocab=train_df.label_group.to_list())),\n        splitter=ColSplitter(),\n        get_y=ColReader('label_group'),\n        )\n    return  data_block.dataloaders(train_df, bs=256)","63b2fd3a":"class ResnetModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.body = create_body(resnet18, cut=-2, pretrained=False)\n        self.after_conv=nn.Sequential(\n            AdaptiveConcatPool2d(),\n            Flatten(),\n            nn.BatchNorm1d(1024)\n        )\n    def forward(self, x):\n        x = self.body(x)\n        return self.after_conv(x)\n        ","fe1b44d7":"def load_image_model(fname):\n    state_dict = torch.load(fname)\n    model = ResnetModel()\n    model.load_state_dict(state_dict)\n    model = model.eval().cuda()\n    return model","adb64bc4":"def get_img_file(row):\n    img =row.image\n    fn  = PATH\/'train_images'\/img\n    if not fn.is_file():\n        fn = PATH\/'test_images'\/img\n    return fn\n\ndef get_image_dls(size, bs):\n    data_block = DataBlock(blocks = (ImageBlock(), CategoryBlock(vocab=train_df.label_group.to_list())),\n                 splitter=ColSplitter(),\n                 get_y=ColReader('label_group'),\n                 get_x=get_img_file,\n                 item_tfms=Resize(int(size*1.5), resamples=(Image.BICUBIC,Image.BICUBIC)), \n                 batch_tfms=aug_transforms(size=size, min_scale=0.75)+[Normalize.from_stats(*imagenet_stats)],\n                 )\n    return data_block.dataloaders(train_df, bs=bs)\n\n","a3fae490":"\ndef add_target_groups(data_df, source_column='label_group', target_column='target'):\n    target_groups = data_df.groupby(source_column).indices\n    data_df[target_column]=data_df[source_column].map(target_groups)\n    return data_df\n\ndef add_splits(train_df, valid_group=0):\n    grouped = train_df.groupby('label_group').size()\n\n    labels, sizes =grouped.index.to_list(), grouped.to_list()\n\n    skf = StratifiedKFold(5)\n    splits = list(skf.split(labels, sizes))\n\n    group_to_split =  dict()\n    for idx in range(5):\n        labs = np.array(labels)[splits[idx][1]]\n        group_to_split.update(dict(zip(labs, [idx]*len(labs))))\n\n    train_df['split'] = train_df.label_group.replace(group_to_split)\n    train_df['is_valid'] = train_df['split'] == valid_group\n    return train_df\n\ndef embs_from_model(model, dl):\n    all_embs = []\n    all_ys=[]\n    for batch in tqdm(dl):\n        if len(batch) ==2:\n            bx,by=batch\n        else:\n            bx,=batch\n            by=torch.zeros(1)\n        with torch.no_grad():\n            embs = model(bx)\n            all_embs.append(embs.half())\n        all_ys.append(by)\n    all_embs = F.normalize(torch.cat(all_embs))\n    return all_embs, torch.cat(all_ys)\n\ndef get_targets_shape(train_df):\n    all_targets = add_target_groups(train_df).target.to_list()\n    all_targets_lens = [len(t) for t in all_targets]\n    targets_shape = []\n    for size in range(min(all_targets_lens), max(all_targets_lens)+1):\n        count = all_targets_lens.count(size) \/ len(all_targets)\n        targets_shape.append((size,count))\n    return targets_shape\n\ndef chisel(groups, groups_p, pos, target_count):\n    probs = []\n    groups_lens = [len(g)for g in groups]\n    current_count = groups_lens.count(pos)\n    if current_count >= target_count:\n\n        return\n    to_cut = target_count - current_count\n    for i in range(len(groups)):\n        if len(groups_p[i])>pos:\n            probs.append((i, groups_p[i][pos]))\n    probs.sort(key=lambda x:x[1])\n    for i in range(min(to_cut, len(probs))):\n        group_idx = probs[i][0] \n        groups[group_idx]=groups[group_idx][:pos]\n        groups_p[group_idx]=groups_p[group_idx][:pos]","7c197e09":"\ndef sorted_pairs(distances, indices):\n    triplets = []\n    n= len(distances)\n    for x in range(n):\n        used=set()\n        for ind, dist in zip(indices[x].tolist(), distances[x].tolist()):\n            if not ind in used:\n                triplets.append((x, ind, dist))\n                used.add(ind)\n    return sorted(triplets, key=lambda x: -x[2])\ndef do_chunk(embs):\n    step = 1000\n    for chunk_start in range(0, embs.shape[0], step):\n        chunk_end = min(chunk_start+step, len(embs))\n        yield embs[chunk_start:chunk_end]\ndef get_nearest(embs, emb_chunks, K=None, sorted=True):\n    if K is None:\n        K = min(51, len(embs))\n    distances = []\n    indices = []\n    for chunk in emb_chunks:\n        sim = embs @ chunk.T\n        top_vals, top_inds = sim.topk(K, dim=0, sorted=sorted)\n        distances.append(top_vals.T)\n        indices.append(top_inds.T)\n    return torch.cat(distances), torch.cat(indices)\n\ndef combined_distances(embs_list):\n    K = min(len(embs_list[0]), 51)\n    combined_inds =[get_nearest(embs, do_chunk(embs))[1] for embs in embs_list]\n    combined_inds = torch.cat(combined_inds, dim=1)\n    res_inds,res_dists = [],[]\n    for x in range(len(combined_inds)):\n        inds = combined_inds[x].unique()\n        Ds = [embs[None,x] @ embs[inds].T for embs in embs_list]\n        D = Ds[0] + Ds[1] - Ds[0] * Ds[1]\n        top_dists, top_inds = D.topk(K)\n        res_inds.append(inds[top_inds])\n        res_dists.append(top_dists)\n    return torch.cat(res_inds), torch.cat(res_dists)\n\ndef blend_embs(embs_list, threshold, m2_threshold, data_df):\n    combined_inds, combined_dists = combined_distances(embs_list)\n    check_measurements(combined_dists, combined_inds, data_df)\n    new_embs_list = L((torch.empty_like(embs) for embs in embs_list))\n    for x in range(len(embs_list[0])):\n        neighs = combined_dists[x] > threshold\n        if neighs.sum() == 1 and combined_dists[x][1]>m2_threshold:\n            neighs[1]=1\n        neigh_inds, neigh_ratios = combined_inds[x, neighs], combined_dists[x,neighs]\n        for embs, new_embs in zip(embs_list, new_embs_list):\n            new_embs[x] = (embs[neigh_inds] * neigh_ratios.view(-1,1)).sum(dim=0)\n    return new_embs_list.map(F.normalize)","9f1e80e0":"# Not used in solution, just for illustration purpose\ndef f1(tp, fp, num_tar):\n    return 2 * tp \/ (tp+fp+num_tar)\n\ndef build_from_pairs(pairs, target, display = True):\n    score =0\n    tp = [0]*len(target)\n    fp = [0]*len(target)\n    scores=[]\n    vs=[]\n    group_sizes = [len(x) for x in target]\n    for x, y, v in pairs:\n        group_size = group_sizes[x]\n        score -= f1(tp[x], fp[x], group_size)\n        if y in target[x]: tp[x] +=1\n        else: fp[x] +=1\n        score += f1(tp[x], fp[x], group_size) \n        scores.append(score \/ len(target))\n        vs.append(v)\n    if display:\n        plt.plot(scores)\n        am =torch.tensor(scores).argmax()\n        print(f'{scores[am]:.3f} at {am\/len(target)} pairs or {vs[am]:.3f} threshold')\n    return scores\n\n\ndef score_distances(dist, targets, display=False):\n    triplets = dist_to_edges(dist)[:len(dist)*10]\n    return max(build_from_pairs(triplets, targets, display))\n\ndef score_group(group, target):\n    tp = len(set(group).intersection(set(target)))\n    return 2 * tp \/ (len(group)+len(target))\ndef score_all_groups(groups, targets):\n    scores = [score_group(groups[i], targets[i]) for i in range(len(groups))]\n    return sum(scores)\/len(scores)\ndef show_groups(groups, targets):\n    groups_lens = [len(g)for g in groups]\n    targets_lens = [len(g) for g in targets]\n    plt.figure(figsize=(8,8)) \n    plt.hist((groups_lens,targets_lens) ,bins=list(range(1,52)), label=['preds', 'targets'])\n    plt.legend()\n    plt.title(f'score: {score_all_groups(groups, targets):.3f}')\n    plt.show()","b80abe3d":"measurements = {\n    'weight': [('mg',1), ('g', 1000), ('gr', 1000), ('gram', 1000), ('kg', 1000000)],\n    'length': [('mm',1), ('cm', 10), ('m',1000), ('meter', 1000)],\n    'pieces': [ ('pc',1)],\n    'memory': [('gb', 1)],\n    'volume': [('ml', 1), ('l', 1000), ('liter',1000)]\n}\n\ndef to_num(x, mult=1):\n    x = x.replace(',','.')\n    return int(float(x)*mult)\n\ndef extract_unit(tit, m):\n    pat = f'\\W(\\d+(?:[\\,\\.]\\d+)?) ?{m}s?\\W'\n    matches = regex.findall(pat, tit, overlapped=True)\n    return set(matches)\n\ndef extract(tit):\n\n    res =dict()\n    tit = ' '+tit.lower()+' '\n    for cat, units in measurements.items():\n        cat_values=set()\n        for unit_name, mult in units:\n            values = extract_unit(tit, unit_name)\n            values = {to_num(v, mult) for v in values}\n            cat_values = cat_values.union(values)\n        if cat_values:\n            res[cat] = cat_values\n    return res\n\ndef add_measurements(data):\n    data['measurement'] = data.title.map(extract)\n    return data\n\ndef match_measures(m1, m2):\n    k1,k2 = set(m1.keys()), set(m2.keys())\n    common = k1.intersection(k2)\n    if not common: return True\n    for key in common:\n        s1,s2 = m1[key], m2[key]\n        if s1.intersection(s2):\n            return True\n    return False\n\ndef check_measurements(combined_dists, combined_inds, data_df):\n    K = min(8, len(data_df)) * len(data_df)\n    _, inds_k = combined_dists.view(-1).topk(K)\n    removed = 0\n    inds_k = inds_k.tolist()\n    for idx in inds_k:\n        x = idx \/\/ combined_inds.shape[1]\n        y_idx = idx % combined_inds.shape[1]\n        y = combined_inds[x,y_idx] \n        if not match_measures(data_df.iloc[x].measurement, data_df.iloc[y.item()].measurement):\n            removed +=1\n            combined_dists[x][y_idx]=0\n    print('removed', removed, 'matches')","e790dfb9":"train_df = pd.read_csv(PATH\/'train.csv')\ntrain_df = add_splits(train_df)\n\nvalid_df = train_df[train_df.is_valid==True].reset_index()\nvalid_df=add_measurements(valid_df)","09d75317":"img_embs,ys = embs_from_model(load_image_model(image_model_file), get_image_dls(224,256).valid)\n\nbert_embs, ys = embs_from_model(load_bert_model(bert_model_file), get_text_dls().valid)","47d792c5":"set_size = len(img_embs)\ntarget_matrix= ys[:,None]==ys[None,:]\ntargets = [torch.where(t)[0].tolist() for t in target_matrix] \n\ncombined_inds, combined_dists = combined_distances([img_embs, bert_embs])\npairs = sorted_pairs(combined_dists, combined_inds)\n_=build_from_pairs(pairs[:10*len(combined_inds)], targets)","fba3851e":"new_embs = blend_embs([img_embs, bert_embs], RECIPROCAL_THRESHOLD, MIN_PAIR_THRESHOLD, valid_df)\n\ncombined_inds, combined_dists = combined_distances(new_embs)\npairs = sorted_pairs(combined_dists, combined_inds)\n_=build_from_pairs(pairs[:10*len(combined_inds)], targets)","d5ce60d2":"check_measurements(combined_dists, combined_inds, valid_df)\npairs = sorted_pairs(combined_dists, combined_inds)\n_=build_from_pairs(pairs[:10*len(combined_inds)], targets)","55a219ff":"groups = [[] for _ in range(set_size)]\ngroups_p = [[] for _ in range(set_size)]\nfor x,y,v in pairs:\n    groups[x].append(y)\n    groups_p[x].append(v)\nfor pos, size_pct in get_targets_shape(train_df):\n    chisel(groups, groups_p, pos, int(size_pct * len(groups)))\nshow_groups(groups, targets)","2fc7c682":"test_df = pd.read_csv(PATH\/'test.csv')\ntest_df = add_measurements(test_df)","d528167a":"TRIAL_RUN=False\n\nif TRIAL_RUN:\n    fake_test_df = train_df[['posting_id', 'image', 'image_phash', 'title', 'label_group']].copy()\n    fake_test_df = pd.concat([fake_test_df, fake_test_df])\n    fake_test_df = add_target_groups(fake_test_df)\n    test_df = fake_test_df","1f3313a8":"bert_embs,_ = embs_from_model(load_bert_model(bert_model_file), get_text_dls().test_dl(test_df))\n\nimg_embs,_ =embs_from_model(load_image_model(image_model_file), get_image_dls(224, 256).test_dl(test_df))","ad91f9c8":"set_size = len(img_embs)","0697a5c3":"%%time\nnew_embs = blend_embs([img_embs, bert_embs], RECIPROCAL_THRESHOLD, MIN_PAIR_THRESHOLD, test_df)\n","b90505de":"combined_inds, combined_dists = combined_distances(new_embs)","aaf7ba9b":"check_measurements(combined_dists, combined_inds, test_df)","a436579e":"pairs = sorted_pairs(combined_dists, combined_inds)","2ea10803":"groups = [[] for _ in range(set_size)]\ngroups_p = [[] for _ in range(set_size)]\nfor x,y,v in pairs:\n    groups[x].append(y)\n    groups_p[x].append(v)\nfor pos, size_pct in get_targets_shape(train_df):\n    chisel(groups, groups_p, pos, int(size_pct * len(groups)))","19f288cb":"matches = [' '.join(test_df.iloc[g].posting_id.to_list()) for g in groups]\ntest_df['matches'] = matches\n\ntest_df[['posting_id','matches']].to_csv('submission.csv',index=False)","c4be4235":"pd.read_csv('submission.csv').head()","e76829bf":"### Dataloader for Bert","3cda8cad":"### Helper code for units of measurement matching","8effa07b":"## UPDATE: I also incorporated embeddings blending from the 1st place solution described [here](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/238136) and extraction of units of measurements inspired by [this kernel](https:\/\/www.kaggle.com\/readoc\/extract-measurements-feature-engineering). All this together scored .772 (762 private) which would have placed it in the top 10 - still using just a single resnet18 + BERT (trained on 80% of the train data)","00c84a1a":"**This is the score we'd get by simply taking all matches above a fixed threshold:**","a1c271b8":"## Helper code","86ce8e11":"# This is a simplified version of my 18th place solution in the **Shopee - Price Match Guarantee** contest\n## I replaced my image models with resnet18 to showcase that even a very basic model could do well and was enough to score a silver medal in this competition","f6e874b3":"## Run on the test set\nsame as above, just without displaying stuff.","b7b787ff":"### Removing false positives based on measurements","03817840":"### Here we perform the chisel step to match the two distributions\nThis doesn't help here where we already have good matches, the effect is more pronounced on the test set where the initial grouping is further apart from the target","07dbc534":"### Generating embeddings from Resnet and BERT","c31d2681":"**This is the score after Neighborhood Blending:**","86593b9f":"## Bert Model","73540c81":"## IMAGE","48ceb69f":"## Check on validation set","24c77171":"# An outline of the (updated) approach\n\n### Step 1 training \nI used arcface for all my training, and fine tune pretrained models with it for a few epochs. For image I tried resnet, efficient net and nfnet. The nfnet worked the best for me so that\u2019s in my final solution. The resnet18 I used in the toy solution just to show a very basic model can work too. \n\nFor text I found that a language model pretrained on Indonesian language worked the best.\n\n### Step 2 Embeddings and similarities \nFor both models I first generate embedddings and then find the top 50 nearest neighbours for each row.\n\n### Step 3 Combine the model outputs\nI combine the neighbours from the previous step with formula D = D<sub>Resnet<\/sub> + D<sub>BERT<\/sub> - D<sub>Resnet<\/sub> * D<sub>BERT<\/sub>\n\nThis I found works much better than alternatives of taking mean or max.\n\n### Step 4 Blending\nI perform a single step of [Neighborhood Blending](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/238136)\nThis represented a jump in score from .749 to .770\n\n### Step 5 Unmatching based on measurements units\nI remove close matches that differ with measurement units in description ie 150ml vs 300ml.\nThis improved the score .770 -> .772\n\n### Step 6 force groups into a desired distribution \nThis was a largest single change I made, jumping my score from .739 to .759\nI nicknamed it \u201cchiseling\u201d the idea is: I first make an educated guess that the distribution of group targets in the test data is similar to the one in train. Then I remove matches until my submission has the same shape.\n\n\n\n\n"}}