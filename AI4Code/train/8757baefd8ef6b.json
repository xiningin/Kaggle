{"cell_type":{"9bcf3d17":"code","a5dd1481":"code","4235b900":"code","3a1fac6c":"code","e3aacbe3":"code","c874828c":"code","b06e462b":"code","733f28ef":"code","cf35ff71":"code","0e48c680":"code","92b55748":"code","6f5f1fae":"code","713313c8":"code","384f6198":"code","770cdb39":"code","8c8ea1fd":"code","a8b8800c":"code","6ea117df":"code","37c6ff86":"code","1c80eee2":"code","884ccfb3":"code","d1842eb0":"code","6ee4a883":"code","c175b651":"code","35e8ed1e":"code","c0620a41":"code","e4c3fbca":"code","30fd46c3":"code","d9737d0c":"code","5d64599b":"code","5dd90ee0":"code","e956f854":"code","74b058bf":"code","42760b6e":"code","1fe4ec16":"code","66c9c422":"code","da9d1528":"code","f98edd85":"code","79875322":"code","3e3c8496":"code","445d8615":"code","3375245c":"code","7699afef":"code","685dd2ed":"code","01a62644":"code","dacf32c0":"code","f3bc728f":"code","9db47caf":"code","c7bf384a":"code","24ed0ff8":"code","bb53a60c":"code","9d085040":"code","dec7b4fd":"code","521a5b42":"code","716fc425":"code","3ac3e168":"code","57bc8c3f":"code","4a435bba":"code","1c1528e3":"code","9d1f8d0c":"code","ecd39d16":"code","e874cb3a":"code","2d645f2f":"code","66d6ca5f":"code","50beee5e":"code","cc710b8f":"code","a2ccf1ac":"code","ee51af5f":"code","6468fab6":"code","8acb6d36":"code","78f1cd72":"code","129ef911":"code","bfcf0b0a":"code","f9dc0f37":"code","13e2620e":"code","9d26539d":"code","34c4b92f":"code","ca1766b1":"markdown","24dec0a6":"markdown","4b3fb111":"markdown","1b9795fa":"markdown","1df13464":"markdown","a8888666":"markdown","0793474f":"markdown","04e8648c":"markdown","47289a6c":"markdown","7b9ca68f":"markdown","4336bd6b":"markdown","d221211e":"markdown","c9660283":"markdown","6e44ed03":"markdown","d0ad744d":"markdown","3a902577":"markdown","edfaeb9e":"markdown","09de5a2a":"markdown","ca633ea8":"markdown","700732f5":"markdown","fa4aea42":"markdown","04b9122d":"markdown","51fa92df":"markdown","639853f3":"markdown","8d6ffab4":"markdown","0171530a":"markdown","cfa36ea2":"markdown","2ec3c820":"markdown","f8954cf6":"markdown","8f6b2370":"markdown","6df58156":"markdown","2396f202":"markdown","b738c233":"markdown","285fbf9a":"markdown","b0f2eeb0":"markdown","7104c356":"markdown","1214dcd2":"markdown","29295427":"markdown"},"source":{"9bcf3d17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5dd1481":"# Das CSV mit Kommaseperated Values wird durch Pandas in ein Dataframe eingelesen\ndf = pd.read_csv(\"\/kaggle\/input\/uci-online-news-popularity-data-set\/OnlineNewsPopularity.csv\")","4235b900":"# Anzahl der Zeilen und Spalten ausgeben\ndf.shape","3a1fac6c":"# Die Ersten 5 Zeilen des Datensatzes ausgeben, um einen \u00dcberblick zu gewinnen\ndf.head()","e3aacbe3":"# Beschrieb des Datensatzes, um den Inhalt absch\u00e4tzen zu k\u00f6nnen\ndf.describe()","c874828c":"# Check auf Null-Values\n# Datensatz hat keine Null-Values\n# Datensatz besteht aus 58\/60 float-Zahlen, 1 object (URL), 1 Integer (anz. Shares)\n\ndf.info()","b06e462b":"# Unn\u00f6tige Spalten l\u00f6schen per Slicing\n# URL und timedelta Spalten gel\u00f6scht, da diese non-predictable sind und damit f\u00fcr die prediction nicht relevant sind\n\n# Anmerkung warum Slicing und nicht Drop benutzt wurde:\n# Die Columns Names beginnen mit Leerzeichen, was durch das Projektteam erst zum Schluss des Projektes bemerkt wurde. \n\ndf_clean = df.iloc[:, 2:]\ndf_clean.info()","733f28ef":"# X = Traningsdaten\n# y = Validierungsdaten\n\nX = df_clean.iloc[:, :-1]\ny = df_clean.iloc[:, -1:]\n\n# Daten- \/ Achsenbeschriftungen\nclass_names = ['shares']\nfeature_names = list(X.columns)","cf35ff71":"# train_test_split training\n\nfrom sklearn.model_selection import train_test_split\n\n# Testdaten generieren\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","0e48c680":"# Vergleichsgrundlage mit Dummy Classifier schaffen\n# Als Prediction-Strategie wurde \"most-frequent\" gew\u00e4hlt, welche alle Daten mit dem h\u00e4ufigsten vorkommenden Wert voraussagt.\n\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_clf.score(X_test, y_test)","92b55748":"# KNN betrachtet die n\u00e4chsten Nachbarn (bsp. n_neighbours=3) und bestimmt danach die Zugeh\u00f6rigkeit vom jeweiligen Datenpunkt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=3)\n\nclf_knn.fit(X_train, y_train)","6f5f1fae":"# KNN anwenden auf Testdaten\ny_pred = clf_knn.predict(X_test)","713313c8":"from sklearn import metrics\n\n# Accuracy Score\nmetrics.accuracy_score(y_test, y_pred)","384f6198":"# Die Validation-Curve visualisiert, wie sich die predictions ver\u00e4ndern, wenn die Parameter angepasst werden\n\nfrom yellowbrick.model_selection import ValidationCurve","770cdb39":"# Validation Curve berechnen\n\nvc_viz = ValidationCurve(clf_knn, param_name='n_neighbors',\n         param_range=np.arange(1,5,1), cv=5, n_jobs=-1)\nvc_viz.fit(X,y)\nvc_viz.show()","8c8ea1fd":"# Der Decision Tree definiert Entscheidungsregeln, die hirarchisch aufeinandergefolgt Daten einteilen\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier(criterion='gini', max_depth=3)","a8b8800c":"# Trainieren\nclf_dt.fit(X_train, y_train)\n\n# DT anwenden auf Testdaten\ny_pred_dt = clf_dt.predict(X_test)","6ea117df":"# Accuracy Score\nmetrics.accuracy_score(y_test, y_pred)","37c6ff86":"# Validation Curve berechnen\n\nvc_viz = ValidationCurve(clf_dt, param_name='max_depth',\n         param_range=np.arange(1,10,1), cv=5, n_jobs=-1)\nvc_viz.fit(X,y)\nvc_viz.show()","1c80eee2":"y.nunique()","884ccfb3":"# Daten zweidimensional x = y visualisieren\nimport matplotlib.pyplot as plt\n# Daten plotten\nplt.scatter(y,y, color='r', s=100, edgecolor='k')\nplt.show()","d1842eb0":"# Da die Zielwerte viele verschiedene Auspr\u00e4gungen haben (1454), ist das korrekte Klassifizieren der Daten sehr schwierig\n# Um die Voraussagen zu vereinfachen, sollen die Zielwerte geclustert werden.\n\nfrom sklearn.cluster import KMeans","6ee4a883":"# Beste anzahl Clusters herausfinden (iterieren)\n\nsqds = []\nfor k in range(1,10):\n    kmeans = KMeans(n_clusters = k, random_state=21)\n    kmeans.fit(y)\n    sqds.append(kmeans.inertia_)\n    \nplt.plot(range(1,10), sqds)\nplt.title('Ellenbogen Methode')\nplt.xlabel('Anzahl der Cluster')\nplt.ylabel('Summe der Quadrierten Distanzen')\nplt.show()","c175b651":"# Clustering durchf\u00fchren\n\nk=4\nkmeans = KMeans(n_clusters=k, random_state=21)\nkmeans.fit(y)","35e8ed1e":"# Clustermittelpunkte\ncentroids = kmeans.cluster_centers_\ncentroids","c0620a41":"# Zuordnung der Punkte zu den Clustern\nclusters = kmeans.labels_\nclusters","e4c3fbca":"# Daten plotten inkl. F\u00e4rben nach Cluster\nplt.figure(figsize=(8,6))\nplt.scatter(y, y, c=clusters, cmap='jet', s=100, edgecolor='k')\n\n# Centroid-Punkte plotten\nplt.scatter(centroids, centroids, marker='*', color='black', s=400)\n\nplt.show()","30fd46c3":"# Centroiden Werte Runden, um die Daten zu vereinfachen\n\nfor i in range(centroids.size):\n    if centroids[i] > 10000:\n        centroids[i] = round(int(centroids[i]), -3)\n    if centroids[i] > 100000:\n        centroids[i] = round(int(centroids[i]), -4) \n    else:\n        centroids[i] = round(int(centroids[i]), -2)    \ncentroids","d9737d0c":"# Effektive Anzahl Shares durch die Clustermittelpunkte ersetzen.\n\nlist = []\nfor i in range(clusters.size):\n    for j in range(centroids.size):\n        if clusters[i] == j:\n            list.append(int(centroids[j]))\ny_cluster = list\nlist[0:30]","5d64599b":"# train_test_split durchf\u00fchren\n\nfrom sklearn.model_selection import train_test_split\n\n# Testdaten generieren\n\nX_train, X_test, y_train, y_test = train_test_split(X,y_cluster, test_size=0.3, random_state=42)","5dd90ee0":"# Vergleichsgrundlage mit Dummy Classifier schaffen\n# Als Prediction-Strategie wurde \"most-frequent\" gew\u00e4hlt, welche alle Daten mit dem h\u00e4ufigsten vorkommenden Wert voraussagt.\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\n\n# Anwenden auf Testdaten\ndummy_clf.fit(X_train, y_train)\n\n# Accuracy Score\ndummy_clf.score(X_test, y_test)","e956f854":"# KNN betrachtet die n\u00e4chsten Nachbarn (bsp. n_neighbours=3) und bestimmt danach die Zugeh\u00f6rigkeit vom jeweiligen Datenpunkt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=3)\n\nclf_knn.fit(X_train, y_train)\n\n# KNN anwenden auf Testdaten\ny_pred = clf_knn.predict(X_test)\n\n# Accuracy Score\nmetrics.accuracy_score(y_test, y_pred)","74b058bf":"# Trainieren\nclf_dt = DecisionTreeClassifier(criterion='gini', max_depth=4)\n\nclf_dt.fit(X_train, y_train)","42760b6e":"# DT anwenden auf Testdaten\ny_pred_dt = clf_dt.predict(X_test)","1fe4ec16":"# Classification Report ausgeben\nprint(metrics.classification_report(y_test, y_pred_dt, digits=3))\n\n# Die Accuracy ist sehr hoch, jedoch muss beachtet werden, dass sich rund 90% der Daten im ersten Cluster befinden.\n# Die hohe Accuracy zeugt daher nicht von einem guten Machine-Learning, sondern von einem Ungleichgewicht der Datenverteilung.","66c9c422":"# Validation Curve berechnen\n\nvc_viz = ValidationCurve(clf_dt, param_name='max_depth',\n         param_range=np.arange(1,10,1), cv=5, n_jobs=-1)\nvc_viz.fit(X,y_cluster)\nvc_viz.show()","da9d1528":"# Verschiedene Percentile ausgeben um zu Beurteilen wie die Daten verteilt sind\ny.describe(percentiles=[.01,.25,.50,.60,.70,.80,.90,.95,.99,.97])","f98edd85":"# Droppen der Sharezahlen kleiner 1 und gr\u00f6sser 99 Percentil, um Aussreisser zu verringern\ndf_clean_short = df_clean\nlist = []\nfor i in range(len(df_clean.index)):\n    if (df_clean_short.iloc[i][-1] < np.percentile(y, 1)) or (df_clean_short.iloc[i][-1] > np.percentile(y, 99)):\n        list.append(i)\ndf_clean_short = df_clean_short.drop(list)\nprint(len(list))\ndf_clean_short","79875322":"# y = Validierungsdaten f\u00fcr Clustering extrahieren\ny = df_clean_short.iloc[:, -1:]\n\n\nfrom sklearn.cluster import KMeans\n\n# Cluster definieren\nk=8\nkmeans = KMeans(n_clusters=k, random_state=21)\n\n# Fit Data\nkmeans.fit(y)\n\n# Clustermittelpunkte\ncentroids = kmeans.cluster_centers_\n\n# Zuordnung der Punkte zu den Clustern\nclusters = kmeans.labels_","3e3c8496":"centroids","445d8615":"# Daten plotten inkl. F\u00e4rben nach Cluster\nplt.figure(figsize=(8,6))\nplt.scatter(y, y, c=clusters, cmap='jet', s=100, edgecolor='k')\n\n# Centroid-Punkte plotten\nplt.scatter(centroids, centroids, marker='*', color='black', s=400)\n\nplt.show()","3375245c":"# Daten je nach gr\u00f6sse Runden\nfor i in range(centroids.size):\n    if centroids[i] > 10000:\n        centroids[i] = round(int(centroids[i]), -3)\n    if centroids[i] > 100000:\n        centroids[i] = round(int(centroids[i]), -4) \n    else:\n        centroids[i] = round(int(centroids[i]), -2)    \ncentroids","7699afef":"# die obersten 10% der Centroiden in einen Centroidenpunkt zusammenziehen\n# Der Clusterwert in centroid_highest repr\u00e4sentieren die obersten 10% der Daten, welche durch den kleinsten Wert angezeigt werden. \n\n# Centroiden > 90 Percentil auslesen\nlist = []\nfor i in range(centroids.size):\n    if int(centroids[i]) > np.percentile(y, 90):\n        list.append(int(centroids[i]))\n        \ncentroid_highest = min(list)\n\n# Centroiden > 90 Percentil mit dem kleinsten der ausgelesenen Werte ersetzen\nfor i in range(centroids.size):\n    if int(centroids[i]) > np.percentile(y, 90):\n        centroids[i] = min(list)","685dd2ed":"# Effektive Anzahl Shares durch die Clustermittelpunkte ersetzen.\nlist = []\nfor i in range(clusters.size):\n    for j in range(centroids.size):\n        if clusters[i] == j:\n                list.append(int(centroids[j]))\n\n# y_cluster = Neue Validierungsdaten\ny_cluster = list\ny_cluster[0:30]","01a62644":"# X = Traningsdaten\n\nX = df_clean_short.iloc[:, :-1]\n\n# Daten- \/ Achsenbeschriftungen\nclass_names = ['shares']\nfeature_names = X.columns","dacf32c0":"# train_test_split der Daten\n\nfrom sklearn.model_selection import train_test_split\n\n# Testdaten generieren\n\nX_train, X_test, y_train, y_test = train_test_split(X,y_cluster, test_size=0.3, random_state=42)","f3bc728f":"# KNN betrachtet die n\u00e4chsten Nachbarn (bsp. n_neighbours=3) und bestimmt danach die Zugeh\u00f6rigkeit vom jeweiligen Datenpunkt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=3)\n\nclf_knn.fit(X_train, y_train)\n\n# KNN anwenden auf Testdaten\ny_pred = clf_knn.predict(X_test)\n\n# Accuracy Score\nmetrics.accuracy_score(y_test, y_pred)","9db47caf":"# Trainieren mit Default-Parametern\nclf_dt = DecisionTreeClassifier()\n\nclf_dt.fit(X_train, y_train)\n\ny_pred_dt = clf_dt.predict(X_test)\n\n# Classification Report\nprint(metrics.classification_report(y_test, y_pred_dt, digits=3))","c7bf384a":"# Normalisierung der Testdaten mit PowerTransformer\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer()\nX_norm = pt.fit_transform(X)\nX_norm","24ed0ff8":"# train_test_split der Daten\n\nfrom sklearn.model_selection import train_test_split\n\n# Testdaten generieren\n\nX_train, X_test, y_train, y_test = train_test_split(X_norm,y_cluster, test_size=0.3, random_state=42)","bb53a60c":"# Vergleichsgrundlage mit Dummy Classifier schaffen\n# Als Prediction-Strategie wurde \"most-frequent\" gew\u00e4hlt, welche alle Daten mit dem h\u00e4ufigsten vorkommenden Wert voraussagt.\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\n\n# Anwenden auf Testdaten\ndummy_clf.fit(X_train, y_train)\n\n# Accuracy Score\ndummy_clf.score(X_test, y_test)","9d085040":"from sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=3)\n\nclf_knn.fit(X_train, y_train)\n\n# KNN anwenden auf Testdaten\ny_pred = clf_knn.predict(X_test)","dec7b4fd":"# Classification Report\nprint(metrics.classification_report(y_test, y_pred, digits=3))","521a5b42":"# Validation Curve --> Predictions-Parameter-Analyse anhand KNN\n\nvc_viz = ValidationCurve(clf_knn, param_name='n_neighbors',\n         param_range=np.arange(1,5,1), cv=5, n_jobs=-1)\nvc_viz.fit(X_norm,y_cluster)\nvc_viz.show()","716fc425":"# Trainieren mit Default-Parametern\nclf_dt = DecisionTreeClassifier()\n\nclf_dt.fit(X_train, y_train)\n\ny_pred_dt = clf_dt.predict(X_test)","3ac3e168":"# Classification Report\nprint(metrics.classification_report(y_test, y_pred_dt, digits=3))","57bc8c3f":"# Validation Curve\nvc_viz = ValidationCurve(clf_dt, param_name='max_depth',\n         param_range=np.arange(1,10,1), cv=5, n_jobs=-1)\nvc_viz.fit(X_norm,y_cluster)\nvc_viz.show()","4a435bba":"# Parameter als Dictionairy anlegen\n\nparams = {'max_depth':np.arange(1,11,1), 'criterion':['gini','entropy'], 'min_samples_leaf':np.arange(1,6,1)}\nparams","1c1528e3":"# Grid Search Cross Validation (CV)\n\nfrom sklearn.model_selection import GridSearchCV\n\n# GridSearch initialisiern\n# Verbose --> wieviel Daten sollen zur\u00fcckgeliefert werden\n\ngrid_clf = GridSearchCV(estimator=clf_dt, param_grid=params, scoring='accuracy', return_train_score=True, verbose=1, cv=5, n_jobs=-1)","9d1f8d0c":"# Lernprozess starten\n\ngrid_clf.fit(X_train, y_train)","ecd39d16":"# beste Parameter ausgeben\n\ngrid_clf.best_params_","e874cb3a":"# Bester erreichter Score\n\ngrid_clf.best_score_","2d645f2f":"# Visualisierung von overfitting und underfitting\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_cv_results = pd.DataFrame(grid_clf.cv_results_)\n\n# Wert x ist am besten (siehe auch Auswertung oben)\n\nsns.lineplot(x='param_max_depth', y='mean_test_score', hue='param_criterion', data=df_cv_results)","66d6ca5f":"# Trainingsdaten Verlauf angucken\n\nsns.lineplot(x='param_max_depth', y='mean_train_score', hue='param_criterion', data=df_cv_results)","50beee5e":"# Decision Tree Classifier Trainieren\n# \u00dcbernimmt direkt die besten Parameter aus dem Grid-Search\n\nclf_dt = DecisionTreeClassifier(**grid_clf.best_params_)\n\nclf_dt.fit(X_train, y_train)\n\ny_pred_dt = clf_dt.predict(X_test)","cc710b8f":"# Classification Report\nprint(metrics.classification_report(y_test, y_pred_dt, digits=3))","a2ccf1ac":"# Confusion Matrix rechnen\n\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix visualisieren\n\nlist = []\nfor i in range(centroids.size):\n    if int(centroids[i]) not in list:\n        list.append(int(centroids[i]))       \nlist.sort()\n\ndisp = metrics.ConfusionMatrixDisplay(cm, display_labels=list)\ndisp.plot(cmap='Blues')","ee51af5f":"# Validation Curve\nvc_viz = ValidationCurve(clf_dt, param_name='max_depth',\n         param_range=np.arange(1,10,1), cv=5, n_jobs=-1)\nvc_viz.fit(X_norm,y_cluster)\nvc_viz.show()","6468fab6":"# Random Forest Classifier\n# \u00dcbernimmt direkt die besten Parameter aus dem Grid-Search\nfrom sklearn.ensemble import RandomForestClassifier\nclf_RF = RandomForestClassifier(**grid_clf.best_params_)\nclf_RF.fit(X_train, y_train)","8acb6d36":"# Classification Report\n\ny_pred = clf_RF.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred, digits=3))","78f1cd72":"# Confusion Matrix rechnen\n\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix visualisieren\n\nlist = []\nfor i in range(centroids.size):\n    if int(centroids[i]) not in list:\n        list.append(int(centroids[i]))       \nlist.sort()\n\ndisp = metrics.ConfusionMatrixDisplay(cm, display_labels=list)\ndisp.plot(cmap='Blues')","129ef911":"from xgboost import XGBClassifier\nclf_bst = XGBClassifier()\nclf_bst.fit(X_train, y_train)\ny_pred = clf_bst.predict(X_test)","bfcf0b0a":"# Classification Report\nprint(metrics.classification_report(y_test, y_pred, digits=3))","f9dc0f37":"# Confusion Matrix rechnen\n\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix visualisieren\n\nlist = []\nfor i in range(centroids.size):\n    if int(centroids[i]) not in list:\n        list.append(int(centroids[i]))       \nlist.sort()\n\ndisp = metrics.ConfusionMatrixDisplay(cm, display_labels=list)\ndisp.plot(cmap='Blues')","13e2620e":"# Ausgeben der wichtigsten Features f\u00fcr den Boost-Classifier\n# Diese Information k\u00f6nnte sehr interessant f\u00fcr den Redaktionsleiter sein, der wissen m\u00f6chte, welche Artikel erfolgreich waren\n\nclf_bst.feature_importances_\n\nfrom yellowbrick.model_selection import FeatureImportances\nplt.figure(figsize=(10,10))\nfi_viz = FeatureImportances(clf_bst, labels=feature_names)\nfi_viz.fit(X_test, y_test)\nfi_viz.show()","9d26539d":"# Ada Boost Classifier von Sklearn\nfrom sklearn.ensemble import AdaBoostClassifier\nclf_abst = AdaBoostClassifier()\nclf_abst.fit(X_train, y_train)\ny_pred = clf_abst.predict(X_test)\n\n# Classification Report\nprint(metrics.classification_report(y_test, y_pred, digits=3))","34c4b92f":"# Confusion Matrix rechnen\n\ncm = metrics.confusion_matrix(y_test, y_pred)\n\n# Confusion Matrix visualisieren\n\nlist = []\nfor i in range(centroids.size):\n    if int(centroids[i]) not in list:\n        list.append(int(centroids[i]))       \nlist.sort()\n\ndisp = metrics.ConfusionMatrixDisplay(cm, display_labels=list)\ndisp.plot(cmap='Blues')","ca1766b1":"## K-N\u00e4chste Nachbarn Verfahren\n\nBei dem K-N\u00e4chste Nachbarn Klassifikator (KNN) wird das Label des betrachteten Datenpunktes anhand einer bestimmten Anzahl an Datenpunkten in seiner N\u00e4he bestimmt. Die benachbarten Punkte sollen also Aufschluss \u00fcber die Labelzugeh\u00f6rigkeit geben. \n\n#### Attribute:\n* n_neighbors --> wieviele Nachbarn sollen betrachtet werden (n_neighbors=3)","24dec0a6":"## KNN Classifier","4b3fb111":"# **Allgemeine Informationen**\n\n## Teilnehmende Studierende\n* Jakob Simonsmeier\n* Yanick Semmler\n\n## Verwendeter Datensatz\n* UCI Online News Popularity Data Set","1b9795fa":"# **Datenaufbereitung v3**\n\nAnhand der Erkenntnisse aus den verschiedenen obigen Tests, hat sich das Projektteam entschieden, bei den Validierungsdaten (y) das oberste und unterste eine Prozent zu l\u00f6schen. Es sollen damit extreme Ausreisser innerhalb der Daten eliminiert werden.\n\nDie obersten 10% der Centroiden werden in einem Centroidenpunkt zusammengefasst, um die Datenanzahl den anderen Centroiden anzugleichen. Die anzahl Centroiden wird beim Clustering daf\u00fcr auf 8 erh\u00f6ht.\n\nZudem sollen die Trainigsdaten (X) normalisiert werden, um die Bewertung der Daten gleich zu gewichten.\n\nMit diesen Anpassungen, soll die Accuracy nochmals verbessert werden.","1df13464":"## Analyse von fehlenden Daten\n\nMit \"df.info\" bekommt man die Datentypen und Null-Values der Spalten angezeigt, was f\u00fcr die weiterverarbeitung der Daten essenziell ist.\n\n***Nachtrag:***\n *Was man hier bei genauem hinschauen h\u00e4tte erkennen k\u00f6nnen, wir jedoch erst zum Schluss des Projektes herausgefunden haben, ist, dass alle Spaltennamen (ausser \"URL\") drei Leerschl\u00e4ge vor dem Spaltenamen haben.\nWir hatten deshalb Probleme, die Spalten z.B. beim Drop-Befehl direkt anzusteuern und mussten einen Umweg \u00fcber Slicing nehmen.*","a8888666":"## Decision Tree Classifier mit Hyperparameter-Optimierung","0793474f":"## Decision Tree Classifier","04e8648c":"### Bewertung der ausgew\u00e4hlten Modelle: Wie werden Ihre Modelle in der Praxis funktionieren?\n* Die in diesem Notebook errechneten Modelle haben unter Einbezug beziehungsweise nicht- Einbezug zahlreicher Aspekte, unterschiedlichste Ergebnisse produziert. Zu den verwendeten Methoden geh\u00f6rten:\n    * Bereinigung des Datensatzes\n    * Normalisierung der Daten\n    * Wahl der Klassifikatioren\n    * Clustering\n    * Hyperparameter Tuning\n    \n    \n* In der Praxis sind die Modelle anwendbar, auch wenn zahlreiche Aspekte unber\u00fccksichtigt blieben. \n* Einfl\u00fcsse auf die Populari\u00e4t durch Inhalte der Artikel (bspw. politische Orientierung) oder auch der Wahrheitsgehalt wurden nicht mit einbezogen.\n* Eine verl\u00e4ssliche Voraussage der Artikel-Popularit\u00e4t daher sehr schwierig zu treffen.\n","47289a6c":"# Voting Classifier\n\nVerschiedene Klassifikatoren bzw. deren Ergebnisse werden zusammengenommen und per Voting ein finales Ergebnis bestimmt.","7b9ca68f":"## Dummy Classifier","4336bd6b":"## Random Forest\nNach dem Bootstrapping-Verfahren werden n Stichproben aus der Datenmenge gezogen.\n\nIm Anschluss werden n Entscheidungsb\u00e4ume mit einer kleineren, zuf\u00e4lligen Teilmenge der m\u00f6glichen Attribute trainiert. Dabei wird immer der Split gew\u00e4hlt der die Unordnung am meisten verringert.\n\nAus den errechneten Ergebnissen wird ein Durchschnitt gebildet um ein repr\u00e4sentatives Ergebnis zu erhalten.\n\n#### Attribute:\n* grid_clf.bestparams --> \u00fcbernimmt die aus dem GridSearch erschlossenen Paramter","d221211e":"# **Modell Training 3**","c9660283":"## Boosting\n\nBeim Boosting werden eine Vielzahl von \"schwachen\" Klassifikatoren verwendet (z.B. kNN, DecisonTree). Bei dem Verfahren versucht der angewandte Klassifikator die Fehler des voraus gegangenen zu korrigieren.\n\nDie Klassifikatoren erhalten je nach ihrer Fehlerquote eine entsprechend gewichtete Stimme. Zum Schluss findet ein Voting statt, bei dem aus der Vielzahl von schwachen, ein starker Klassifikator entsteht.","6e44ed03":"### Validation Curve\n\nDie Validations Kurve dient dazu zu determinieren wie effektiv der Klassifikator mit den gew\u00e4hlten Parametern auf dem Datensatz performt.\n\nAnhand der Ergebnisse kann auf die st\u00e4rke jeweiliger Hyperparameter geschlossen werden. Beispielsweise kann beim Abfallen der Kurve bei zunehmender maximaler Tiefe eines Entscheidungsbaums, der Entwickler diesen Parameter entsprechend anpassen. Dadurch lassen sich Over- und Underfittin vermeiden.\n\n#### Attribute:\n* param_name --> welcher Parameter soll \u00fcberpr\u00fcft werden (param_name='n_neighbors')\n* param_range --> legt fest welche Bandbereite \u00fcberpr\u00fcft werden soll und in welchen Schritten diese durchlaufen wird (param_range=np.arange(1,5,1))\n* cv --> legt fest wie h\u00e4ufig die Kreuzvalidierung durchgef\u00fchrt werden soll (cv=5)\n* n_jobs --> definiert wieviele Prozesse parallel bearbeitet werden sollen \"-1\" verwendet alle zur Verf\u00fcgung stehenden Prozessorkerne (n_jobs=-1)","d0ad744d":"# **Modell Training 1**\n\nIn einer erste Runde des Machine-Learnings haben wir uns dazu entschieden die drei Classifier \"Dummy\", \"KNN\" und \"Decision Tree\" am bestehenden Datensatz zu testen und zu schauen, was dabei f\u00fcr Scores erreicht werden.","3a902577":"# **Benotungskriterien**\n\nDie Dauer Ihrer Pr\u00e4sentation sollte auf 15min plus 5-10min Diskussion begrenzt sein. Sie\nsollten\n\n1. Ihre Pr\u00e4sentation (PDF) und\n2. Ihr Notebook (als HTML und PDF) auf Moodle hochladen und\n3. den Link zu Ihrem Kaggle Notebook angeben und den Kaggle Nutzer rolandmueller zu Ihrem Notebook einladen.\n\nBenotungskriterien:\n* Ausf\u00fchrlichkeit und Korrektheit der Beschreibung des Vorgehens\n* Technische Raffinesse und Kreativit\u00e4t Ihrer L\u00f6sung\n* Verbindung von der technischen L\u00f6sung zum Unternehmensproblem bzw. Nutzen\n* Korrekte und transparente Angabe aller Hilfsmittel und Quellen\n* Vortrag und Qualit\u00e4t der Folien\n* Diskussion und Verst\u00e4ndnis","edfaeb9e":"\n## Clustering der Validierungsdaten mit K-Means\n\nDie Daten werden erneut geclustert, jedoch diesesmal mit 8 Centroiden, da am schluss die obersten 10% wieder zusammengezogen werden.","09de5a2a":"### Beschreibung der gew\u00e4hlten Vorgehensweise zur Optimierung der Parameter\n* Um beispielsweise die Parameter f\u00fcr den Entscheidungsbaum zu optimieren haben wir und dazu entschloosen ein GridSearch vorzunehmen. Daraus ergab sich in unserem Fall z.B. eine maximale Tiefe von 3\n* dar\u00fcber hinaus wurde der Datensatz fortw\u00e4hrend nach neu gewonnenen Erkenntnissen bearbeitet, sodass sich auch auch die anzahl der Cluster \u00e4nderte (vgl. Ellenbogen Methode)\n","ca633ea8":"# **Datenverst\u00e4ndnis**\n\nUm mit einem neuen Datensatz arbeiten zu k\u00f6nnen, muss man ihn zuerst verstehen. Das kann auf verschiedene Weisen geschehen.\n\nMit \"df.shape\", \"df.head\" und \"df.describe\" soll ein grober \u00fcberblick zu den Eckdaten des Datensatzes geschaffen werden, damit man als Analyst ein Gef\u00fchl f\u00fcr die Daten bekommt.\n\nDen Datensatz mit seanborn (bspw. pairplot) zu visualisieren ist aufgrund der Vielzahl an Attributen\/Diemensionen nicht m\u00f6glich bzw. nicht sinnvoll, da die CPU in Kaggle stark beschr\u00e4nkt ist.","700732f5":"# Modelltraining 3 vor Normalisierung","fa4aea42":"## Decision Tree Classifier ohne Hyperparameter-Optimierung","04b9122d":"### Haben Sie verschiedene Modelle verglichen? Wenn ja, welches ist besser? F\u00fcr welche Metrik?\n* In diesem Notebook wurden eine vielzahl verschiedener Modelle verglichen\n    * K-N\u00e4chste-Nachbarn (KNN)\n    * Entscheidungsbaum (Decision Tree)\n    * Random Forest\n    * Boosting\n    \n    \n* als \"besstes\" Modell hat sich das Boosting (sklearn - AdaBoostingClassifier) mit einer accuracy von 58,1% hervorgetan\n* jedoch unterscheiden sich die Ergebnisse nur marginal (vgl. Random Forest 57,6%)\n","51fa92df":"# Hyperparameter-Optimierung\n## Grid Search f\u00fcr Entscheidungsbaum\n\nGridSearch wird angewand um die Hyperparameter von einem gew\u00e4hlten Klassifikator zu optimieren. Als Parameter sind bei einem Entscheidungsbaum bspw. die maximale und minimale Tiefe, oder auch das gew\u00e4hlte Reinheitsma\u00df, sowie die Anzahl der Kreuzvalidierungen denkbar.\n\n#### Attribute:\n* param_grid --> \u00fcbergebt die zu optimierenden Parameter \n* scoring --> nach welchen Kriterium soll optimiert werden - bspw. Pr\u00e4zision, Recall etc. (scoring='accuracy')\n* verbose --> gibt Kontrolle dar\u00fcber, wieviele Informationen w\u00e4hrend der Kalkulation ausgegeben werden soll (verbose=1)","639853f3":"## Dummy Classifier","8d6ffab4":"# **Datenaufbereitung v2**\n\nDie Scores zeigen deutlich, dass die Daten f\u00fcr die Predictions noch ungeeignet oder nicht genug eindeutig zuordenbar sind.\n\nDie Analyse der Unique-Values von y zeigen, dass es rund 1500 verschiedene Werte gibt. Das macht die Vorhersage eines Wertes sehr schwierig.\n\nWir werden deshalb mit einem K-Means-Clustering die Unique-Values verringern, um damit vorhersagen einfacher zu machen. Dabei werden die effektive Anzahl Shares durch die Clustermittelpunkte (Centroiden) ersetzt.","0171530a":"# **Modell Training 2**\n\nDie geclusterten Daten werden nun am Decision Tree Classifier getestet.\n\nDie Accuracy ist nun sehr hoch, jedoch muss beachtet werden, dass sich rund 90% der Daten im ersten Cluster befinden.\nDie hohe Accuracy zeugt daher nicht von einem guten Machine-Learning, sondern von einem Ungleichgewicht der Datenverteilung.","cfa36ea2":"## Clustering der Validierungsdaten mit K-Means\nBeim Clustering werden \u00e4hnliche Objekte zu einer Menge bzw. einem Cluster zusammengefasst. Daten verschiedener Cluster sollen m\u00f6glichst un\u00e4hnlich sein.\n\nDie \u00c4hnlichkeit wird mithilfe einer Distanzfunktion errechnet, je kleiner die Distanz, desto gr\u00f6\u00dfer die \u00c4hnlichkeit.\n\nbesitzen die geclusterten Daten unterschiedliche Skalen, kann die Euklidische Distanz von einem bestimmten Attribut dominiert werden. Um dem entgegenzuwirken kommt Renormalisierung\/Reskalierung zum Einsatz.\n\nK-Means bezeichnet einen bestimmten Algorithmus beim Einsatz von Clustering. Hierbei lassen sich die Anzahl der Cluster explizit bestimmten, welche im durchlauf von 3 sich wiederholenden Schritten erzeugt werden.\n\n#### Attribute:\n* k_means --> Anzahl der zu erzeugenden Cluster (k_means=5)","2ec3c820":"## Decision Tree\n\nEin Entscheidungsbaum besteht aus einer Konten- und Kantenmenge.\n\nDabei repr\u00e4sentiert ein innerer Knoten ein Attribut, eine Kante einen Split auf dem Attribut des Vaterknotens.\n\nIn der Wurzel ist die Gesamtmenge der angewandten Daten vorhanden. Mit jeder Entscheidung wir die Datenmenge nach bestimmten Kriterien geteilt. Bspw. Jahreseinkommen > 30.000 - Jahreseinkommen < 30.0000\n\n#### Attribute:\n* criterion --> Ma\u00df f\u00fcr die Unreinheit\/Unordnung (criterion='gini')\n* max_depth --> Gibt die maximale Tiefe an, in die sich der Baum verzweigen soll (max_depth=3)","f8954cf6":"## Normalisierung mit PowerTransformer\n\nDie Normalisierung von Daten hilft vorallem Vektorbasierten Verfahren (wie z.B. KNN), da die verschiedenen Zahlenwerte und Metriken auf ein einheitliches Mass \/ eine einheitliche Gr\u00f6sse gestuft werden.\n\nPowerTransformer haben wir \u00fcber die Sklearn-API gefunden. Das n\u00fctzliche an PowerTransformer ist, dass dieser auch mit negativen Werten umgehen kann.","8f6b2370":"# Quellen\n\n* UCI Online News Popularity Data Set (www.mashable.com) || Januar 2015\n* Prof. Dr. Rohland M\u00fcller || Vorlesungsinhalte und zur Verf\u00fcgung gestellte Materialien\n* https:\/\/scikit-learn.org\/ (Scikit-learn)\n* https:\/\/www.scikit-yb.org\/ (Yellowbrick)\n* https:\/\/seaborn.pydata.org\/ (Seaborn)\n* https:\/\/numpy.org\/ (NumPy)\n* https:\/\/pandas.pydata.org\/ (Pandas)\n* https:\/\/matplotlib.org\/ (Matplotlib)\n* https:\/\/xgboost.readthedocs.io\/en\/latest\/ (XGBoost)\n","6df58156":"## KNN Classifier","2396f202":"## Dummy Classifier\n\nDer Dummy Classifier predictet die Label nach sehr simplen Regeln\/Strategien.\nAls Strategie kann beispielsweise das am h\u00e4ufigsten vorkommende Label, oder ein vom User \u00fcbergebenes Label gew\u00e4hlt werden. \n\nDa dieser Classifier keine eigenen Berechnungen anstellt, also durch seine Funktion keine Verbesserung der Ergebnisse erzeugt, dient er als Vergleichsgrundlage, um die St\u00e4rke des tats\u00e4chlich angewandten Klassifikators zu \u00fcberpr\u00fcfen.\n\n#### Attribute:\n* strategy --> welche Strategie soll gew\u00e4hlt werden (strategy=\"most_frequent\")","b738c233":"## Train Test Split","285fbf9a":"# **Datenaufbereitung v1**\n\nDer Datensatz ist in einem guten Zustand. Der Datensatz hat keine Null-Values, besteht aus 58\/60 float-Zahlen, 1 object (URL), 1 Integer (anz. Shares).\n\nDie Spalten \"URL\" und \"timedelta\" sind sogenannte non-predictive-Attribute. Das heisst, dass sie nicht aufgrund gewisser Parameter vorhergesagt werden k\u00f6nnen bzw. in keinem Zusammenhang mit den Zieldaten \"Shares\" stehen. Aus diesem Grund werden diese Spalten entfernt.\n\nDer Datensatz besteht damit nur noch aus Floats und Integers.\n\nZum Schluss wird der Datensatz in Validierungsdaten y (Shares) und Trainingsdaten X (alles ausser Shares) gesplitet, um danach mit dem Machine-Learning beginnen zu k\u00f6nnen.","b0f2eeb0":"# Import Datensatz \"Online News Popularity\"\n\n**Abstract**\n\nDieser Datensatz fasst einen heterogenen Satz von Merkmalen \u00fcber Artikel zusammen, die von Mashable in einem Zeitraum von zwei Jahren ver\u00f6ffentlicht wurden. Ziel ist es, die Anzahl der Shares in sozialen Netzwerken (Popularit\u00e4t) vorherzusagen.\n\n**Content**\n\nDie Artikel wurden von Mashable (www.mashable.com) ver\u00f6ffentlicht, und die Rechte zur Reproduktion der Inhalte liegen bei Mashable. Daher enth\u00e4lt dieser Datensatz nicht den Originalinhalt, sondern einige damit verbundene Statistiken. Der urspr\u00fcngliche Inhalt ist \u00f6ffentlich zug\u00e4nglich und kann \u00fcber die angegebenen URLs abgerufen werden.\nErfassungsdatum: 8. Januar 2015\nDie gesch\u00e4tzten relativen Leistungswerte wurden von den Autoren unter Verwendung eines Random Forest-Klassifikators und Rolling Windows als Bewertungsmethode gesch\u00e4tzt.\n\nAnzahl der Attribute: 61 (58 pr\u00e4diktive Attribute, 2 nicht-pr\u00e4diktive, 1 Zielfeld)","7104c356":"# Modellevaluierung\n","1214dcd2":"## Train Test Split\n\nTrain_test_split spaltet den Datensatz zuf\u00e4llig in einen Traings-Datensatz und einen Test-Datensatz.\n\nDer Trainings-Datensatz dient dazu, die gew\u00e4hlten Klassifikatoren zu trainieren. \n\nDa der Algorithmus in Folge auf den bereits bekannten Daten besser performt, wird f\u00fcr eine realistische Leistungspr\u00fcfung der bis dahin unbekannte Test-Datensatz verwendet.\n\n#### Attribute:\n* test_size --> wieviele Daten in Prozenz sollen f\u00fcr den Test-Datensatz verwendet werden (test_size=0.3)\n* random_state --> angewand wird ein zufallsgenerator, welcher sich \u00fcber den random_state reproduzieren l\u00e4sst (random_state=42)","29295427":"# **M\u00f6glicher Gesch\u00e4ftseinsatz**\n\n* Wie w\u00fcrde Ihr Modell dem Unternehmen helfen? Wie l\u00f6st es das Gesch\u00e4ftsproblem?\n    * Vorrauszusagen wie h\u00e4ufig ein bestimmter Artikel geteilt wird, kann vorteilhaft genutz werden um bewusst entsprechende Parameter f\u00fcr geschriebenes zu w\u00e4hlen\n    * Social Media hat einen unverkennbaren Einfluss auf den wirtschaftlichen Erfolg von Online Redaktionen\n    \n    \n    \n* Was ist die Bedeutung Ihres Modells und Ihrer L\u00f6sung aus der Gesch\u00e4ftsperspektive?\n    * Die angestellten Machine Learning Analysen zeigen eine Korrelation zwischen zahlreichen Parametern (wie z.B. Data-Channel \"Entertainment\" oder \"Published at Weekend\") und der Anzahl der \"shares\" in social media auf.\n    * Demnach kann Mashabel h\u00f6here Klickzahlen und somit h\u00f6hre Gewinne erzielen, indem sie sich an den aufgezeigten Mustern orientieren um die Popularit\u00e4t ihrer Artikel zu steigern"}}