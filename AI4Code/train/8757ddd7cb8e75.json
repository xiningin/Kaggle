{"cell_type":{"15a02772":"code","db6cf6e7":"code","fb89909f":"code","3fff13ce":"code","56aad993":"code","ce4dfeea":"code","f189a934":"code","904badbe":"code","4afe8d09":"code","5c4e2f02":"code","07ec62df":"code","95c0ffc6":"code","4399793c":"code","982527d7":"code","3abcce57":"code","4074946f":"code","42beade7":"code","91cffe8f":"code","b3b758b4":"code","e905f7c1":"code","63424dd7":"markdown","817f5ed3":"markdown","8872a983":"markdown"},"source":{"15a02772":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db6cf6e7":"path='\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\ntrain_df=pd.read_csv(path+'train.csv')\ntest_df=pd.read_csv(path+'test.csv')\nsample_submission_df=pd.read_csv(path+'sample_submission.csv')\n","fb89909f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Construction of a function to plot, and display the total number of the null values\ndef check_nulls(df):\n  sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') #Plot\n  null_columns=df.columns[df.isnull().any()]\n  df[null_columns].isnull().sum()\n  print(df[null_columns].isnull().sum()) #to display sum of nulls","3fff13ce":"#Displaying the columns having only NaNs\n#train_df.isnull().sum()\nnull_columns=train_df.columns[train_df.isnull().any()]\n#null_columns\ntrain_df[null_columns].isnull().sum()","56aad993":"#For training dataset\ncheck_nulls(train_df)","ce4dfeea":"#Similarly for testing dataset,\ncheck_nulls(test_df)","f189a934":"#construction of function to handle the null values\n\ndef null_data_management(train_df):\n  null_columns=train_df.columns[train_df.isnull().any()] #all the null columns \n  train_df_nans=train_df[null_columns] #(only NaN containing colums in train_df: these columns contains dtypes= cat\/int\/float)\n\n  #can be check by: train_df_nans.info()\n\n  train_df_nans_cat_features=train_df_nans.select_dtypes(include=['object']).copy()\n  train_df_nans_float_features=train_df_nans.select_dtypes(include=['float64']).copy()\n\n  #train_df_nans_cat_features.columns \n  #train_df_nans_float_features.columns\n  #train_df_nans_cat_features.isnull().sum() #shows number of NaNs presenting in each columns\n  #the meaning of train_df_nans_cat_features.isnull().sum() is same with,\n  # train_df[train_df_nans_cat_features.columns].isnull().sum() \n\n  for i, t in enumerate(train_df_nans_cat_features):            #to replace categorical\n    #print(t)\n    if train_df[t].isnull().sum()>=100:                         #if more than 100, dropout\n      train_df.drop([t],axis=1,inplace=True) \n    else:\n      train_df[t]=train_df[t].fillna(train_df[t].mode()[0])     #replace with mode\n\n  for i, t in enumerate(train_df_nans_float_features):          #to replace float\n    #print(t)\n    if train_df[t].isnull().sum()>=100:\n      train_df.drop([t],axis=1,inplace=True) \n    else:\n      train_df[t]=train_df[t].fillna(train_df[t].mean())        #replace with mean()\n  return train_df","904badbe":"#Lets replace\/remove nulls in training data\ntrain_df=null_data_management(train_df)\ncheck_nulls(train_df)","4afe8d09":"#Lets replace\/remove nulls for test data\ntest_df=null_data_management(test_df)\ncheck_nulls(test_df)","5c4e2f02":"#Lets remove 'id' columns from both train and testdataset\ntrain_df.drop(['Id'], axis=1, inplace=True)\ntest_df.drop(['Id'], axis=1, inplace=True)","07ec62df":"final_df=pd.concat([train_df, test_df], axis=0)\n#Finding the columns that consist the categorical variable\nfinal_cat_columns=final_df.select_dtypes(include=['object']).copy()\nmulticol=final_cat_columns.columns\n\ndef category_onehot_multcols(multicol):\n    df_final=final_df\n    i=0\n    for fields in multicol:\n        \n        #print(fields)\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:\n            \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final\n\ndf_final=category_onehot_multcols(multicol)","95c0ffc6":"#Many columns are duplicating, therefore, removing the duplicating columns\ndf_final=df_final.loc[:,~df_final.columns.duplicated()]\ndf_final.shape","4399793c":"train_df=df_final.iloc[:train_df.shape[0],:]\ntest_df=df_final.iloc[train_df.shape[0]:,:]\n\n#during concatination, the columns of test dataset may have NaNs, lets drop it.\ntest_df.drop(['SalePrice'], axis=1,inplace=True)\n","982527d7":"##Now, lets manage train\/test\nX_train=train_df.drop(['SalePrice'], axis=1)\ny_train=train_df['SalePrice']","3abcce57":"import xgboost\nregressor=xgboost.XGBRegressor()\nbooster=['gbtree','gblinear']\nbase_score=[0.25, 0.5, 0.75,1]\n\n## Hyper Parameter Optimization\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrandom_cv = RandomizedSearchCV(estimator=regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)\n\nrandom_cv.fit(X_train,y_train)","4074946f":"random_cv.best_estimator_","42beade7":"regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)","91cffe8f":"regressor.fit(X_train,y_train)","b3b758b4":"y_pred=regressor.predict(test_df)","e905f7c1":"#submission preparation\npred=pd.DataFrame(y_pred)\ndatasets=pd.concat([sample_submission_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample1_submission.csv', index=False)","63424dd7":"# Part 3: Datapreparation\n- The final dataset *train_df*, and *test_df* consist categorical variable as the independent variable, therefore we need to convert them into numerical independed variable (using 'get_dummy' function)\n- Very important thing: why we need to concat both train and test (can not apply same function for train and test) is:\n***- the level of categorical variables in train dataset and test dataset are different.***","817f5ed3":"# Part 4 Modeling and Prediction\n\n- 4.1 Hyper-parameter Tunning","8872a983":"1.2 Missing value handling rules,\nSince these NaNs are from numeric or categorical variables, we need to manage them accordingly.\n* Numeric missing: replaced with mean\/median values*\n* Cetegorical variables: replaced with mode values*"}}