{"cell_type":{"6631bacf":"code","58477d73":"code","f92c1b50":"code","9bfb3bbe":"code","944e9edf":"code","c38ff2ea":"code","e65c9e42":"code","df42b245":"code","8aa78f4c":"code","91e17886":"code","d63ff847":"code","d5c8661b":"code","b1d67b87":"code","dc6333d8":"code","94265031":"code","fad3a617":"code","4086a16f":"code","24a7a8c8":"code","c7e22e5f":"code","33bf867c":"code","75d53353":"code","80cd0918":"code","5fbd2eda":"code","3c1203e0":"markdown","a8e24e4c":"markdown","9d9f3644":"markdown","db5088b3":"markdown","621399da":"markdown","df809afa":"markdown","f742a317":"markdown","cda147b8":"markdown","a78dcb87":"markdown","18505402":"markdown","5ea73c2c":"markdown","e1f3b974":"markdown","4d2ce709":"markdown"},"source":{"6631bacf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\nfrom tqdm import tqdm\nimport random\nimport seaborn as sns\nimport math\n\nimport warnings\nwarnings.filterwarnings('ignore')","58477d73":"DATA_PATH = Path('..\/input\/ump-train-picklefile')\nSAMPLE_TEST_PATH = Path('..\/input\/ubiquant-market-prediction')\n!ls $SAMPLE_TEST_PATH","f92c1b50":"train = pd.read_pickle(DATA_PATH\/'train.pkl')","9bfb3bbe":"train.head()","944e9edf":"num_data_points = len(train)\nprint(f'We have {num_data_points} data points')","c38ff2ea":"num_investments = train['investment_id'].nunique()\nprint(f'We have {num_investments} unique investments')","e65c9e42":"num_time_intervals = train['time_id'].nunique()\nprint(f'Each investment has a maximum of {num_time_intervals} time intervals')","df42b245":"print(f'We have {train.isnull().sum().sum()} missing values')","8aa78f4c":"sns.set_theme()\nfig, ax =plt.subplots(1,2, figsize=(14, 4))\nsns.distplot(train['target'], ax=ax[0]).set_title('Target Distribution')\nsns.distplot(train['time_id'], ax=ax[1]).set_title('Time_id Distribution')\nfig.show()","91e17886":"sample_features = random.sample(range(299), 9)\nfig, ax = plt.subplots(3,3, figsize=(18, 18))\nfor i, sample in enumerate(sample_features):\n    sns.distplot(train[f'f_{sample}'], ax=ax[math.floor(i\/3),i%3]).set_title(f'f_{sample} Distribution')\nfig.show()","d63ff847":"sns.set_theme()\nfig, ax =plt.subplots(3,1, figsize=(16, 12))\nsns.lineplot(data=train[train['investment_id']==0]['target'], ax=ax[0]).set_title('Investment 0')\nsns.lineplot(data=train[train['investment_id']==1]['target'], ax=ax[1], color='r').set_title('Investment 1')\nsns.lineplot(data=train[train['investment_id']==2]['target'], ax=ax[2], color='g').set_title('Investment 2')\nfig.show()","d5c8661b":"investment_0 = train[train['investment_id']==0]\ndf = investment_0.drop(['row_id', 'investment_id'], axis=1)\ndel investment_0\ndf_30 = df.iloc[: , :32]\ndel df\ncorrMatrix = df_30.corr()\nplt.figure(figsize = (15,8))\nsns.heatmap(corrMatrix.to_numpy(), cmap=\"YlGnBu\")","b1d67b87":"# Due to low memory\n%reset -f","dc6333d8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport random\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","94265031":"DATA_PATH = Path('..\/input\/ump-train-picklefile')\nSAMPLE_TEST_PATH = Path('..\/input\/ubiquant-market-prediction')","fad3a617":"train = pd.read_pickle(DATA_PATH\/'train.pkl')\ntrain.drop(['row_id', 'time_id'], axis=1, inplace=True)\nX = train.drop(['target'], axis=1)\ny = train[\"target\"]\ndel train\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=False)\ndel X\ndel y","4086a16f":"model = LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        boosting_type=\"gbdt\",\n        n_estimators=1400,\n        min_child_samples = 1000,\n        num_leaves=100,\n        max_depth=10,\n        learning_rate=0.02,\n        subsample=0.8,\n        subsample_freq=1\n)\n\n\nmodel.fit(X_train, y_train,\n          eval_set=[(X_valid, y_valid)],\n          verbose=20,\n          eval_metric='rmse',\n          early_stopping_rounds=30)","24a7a8c8":"cat_model = CatBoostRegressor()\ncat_model.fit(X_train, y_train,\n          eval_set=[(X_valid, y_valid)],\n          verbose=20,\n          eval_metric='rmse',\n          early_stopping_rounds=30)","c7e22e5f":"def plotImp(model, X , num = 20, fig_size = (40, 20)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=fig_size)\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Features')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances-01.png')\n    plt.show()","33bf867c":"plotImp(model, X_valid)","75d53353":"#### Code for kfold split. This can't run on a kaggle kernel due to memory limits ####\n\n#n_splits = 4\n#kf = KFold(n_splits=n_splits, shuffle=True)\n#models = []\n#for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n#    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n#    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n#    \n#    model = LGBMRegressor(\n#        objective=\"regression\",\n#        metric=\"rmse\",\n#        boosting_type=\"gbdt\",\n#    )\n#    \n#    model.fit(X_train, y_train,eval_set=[(X_valid, y_valid)], \n#            early_stopping_rounds=100)\n#    models.append(model)\n#    \n#    print(f'Trained {i}\/{n_splits} models')","80cd0918":"example_test = pd.read_csv(SAMPLE_TEST_PATH\/'example_test.csv')\nsample_sub = pd.read_csv(SAMPLE_TEST_PATH\/'example_sample_submission.csv')\ndisplay(example_test.head(2))\ndisplay(sample_sub.head(2))","5fbd2eda":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop(['row_id'], axis=1, inplace=True)\n    pred = (model.predict(test_df) + cat_model(test_df))\/2\n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df) ","3c1203e0":"# EDA and LGMB Baseline \ud83d\udcda\ud83e\udd16\ud83d\udcca","a8e24e4c":"#### Look at sample submission and example test csvs\n\n","9d9f3644":"### Target and Time_id distribution","db5088b3":"#### Looking at correlation between the first 30 anonymous features, time_id and target for investment 0","621399da":"(Note this seems to disagree with the summary statistics in the data tab of the competition..)","df809afa":"### Basic EDA \ud83d\udcca","f742a317":"We can see each data point has:\n\n- `row_id` - A unique identifier for the row\n\n- `time_id` - ID for the time the data was collected. Not all investments have data for all the time IDs\n\n- `investment_id` - ID for each individual investment\n\n- `target` - The target\n\n- `[f_0:f_299]` - features generated from the investment data at that time ID","cda147b8":"#### Please give an upvote if you find this useful! (WIP)","a78dcb87":"### Load in the data \u23f3\nLoad in the pickle file from https:\/\/www.kaggle.com\/columbia2131\/speed-up-reading-csv-to-pickle :)","18505402":"### Target distribution of investment 0, 1 and 2\n","5ea73c2c":"### Imports \ud83d\uddc2","e1f3b974":" ### Looking at the distribution of 9 anonymous features","4d2ce709":"### Basic LightGBM model \ud83c\udf33"}}