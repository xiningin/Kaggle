{"cell_type":{"1be116ae":"code","3863e3ad":"code","2242bf62":"code","a9b7d9ab":"code","c738f9a7":"code","4652fa1f":"code","795ee981":"code","13d57594":"code","cdcd294e":"code","c18f3449":"code","b390135c":"code","9ba467ca":"code","a15b6d53":"code","7aa54c7f":"code","8ff5a3cc":"code","bc04356e":"code","e29fad7e":"code","23b1ca08":"code","c4468363":"code","8321fcc1":"code","6a2fd39f":"code","acb27249":"code","25127536":"code","0d27c51a":"code","dca3fe59":"code","f336a26b":"code","5633dc15":"code","363cd539":"code","d9fc3f5b":"code","59abe820":"code","49b3993c":"code","3b2a7df6":"code","7255678d":"code","56468d2f":"code","2602c6a1":"code","fa7ae7ec":"code","7684a404":"code","e2dec75e":"code","54cf52d7":"markdown","e703093e":"markdown","0b9117c4":"markdown","c078c70b":"markdown","58bb3938":"markdown","8ea08d1e":"markdown","d1362077":"markdown","06e76822":"markdown","5b399806":"markdown","4ffacf6f":"markdown","9bc50c7d":"markdown","c0aef537":"markdown","ece5fb2f":"markdown","81976d17":"markdown"},"source":{"1be116ae":"import pandas as pd\nimport numpy as np","3863e3ad":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nsns.set_palette('pastel')","2242bf62":"#The following line of code enables the automatic graph display in Jupyter notebook\n\n%matplotlib inline\n\n#The rest of modules used in the workbook are going to be loaded when they are needed","a9b7d9ab":"df = pd.read_csv(\"..\/input\/data-science-for-good-kiva-crowdfunding\/kiva_loans.csv\")","c738f9a7":"pd.set_option('display.max_rows', 30)\npd.set_option('display.max_columns', 30)","4652fa1f":"df.info()  ","795ee981":"df.head(3)","13d57594":"ax = sns.heatmap(df.drop(['id', 'partner_id'], axis = 1).corr(), cmap = 'coolwarm', annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.xticks(rotation=45)","cdcd294e":"# As we can see from the heatmap above, some of the factors, such as funded_amount and lender_count present a strong correlation.\n# This correlation can be interpreted in the following way: the bigger the sum of the loan, the more lenders it needs to be funded\ndf[['funded_amount', 'lender_count']].corr()","c18f3449":"#extracting all the unique country names in a single list\ncountries = list(df['country'].unique())","b390135c":"#Each country is introduced as a key of the dictionnary with a corresponding dataset as a value \ncountrydict = {elem : pd.DataFrame() for elem in countries}\nfor key in countrydict.keys():\n    countrydict[key] = df[:][df.country == key]","9ba467ca":"#Each dataset will have to drop the columns unnecessary to our future work\nfor key in countrydict:\n    countrydict[key].drop(['id','activity', 'use', 'country_code','region','currency','partner_id','posted_time','disbursed_time','funded_time','tags','date','loan_amount','country', 'loan_amount'], axis=  1, inplace = True)","a15b6d53":"#This function will rework the gender column. As it is a simplification it has to be taken with a grain of salt\ndef genalloc(x):\n    x = str(x)\n    #The following line transforms gender strings in lists 'female, female, female, male' -> ['female','female','female','male']\n    x = [x.strip() for x in x.split(',')]\n    \n    #monogender lists keep their value as a string\n    if len(x) == 1:\n        if x[0] == 'male':\n            return 'male'\n        elif x[0] == 'female':\n            return 'female'\n    #longer lists get a new string assigned based on their gender composition\n    if len(x) > 1:\n        if all(i in x for i in ['male', 'female']):\n            return 'mixed'\n        elif x[0] == 'male':\n            return 'men'\n        elif x[0] == 'female':\n            return 'women'","7aa54c7f":"#The fuctio is then applied to the 'genders' column\nfor key in countrydict:\n    countrydict[key].dropna(inplace = True)\n    countrydict[key]['borrower_genders'] = countrydict[key]['borrower_genders'].apply(lambda x: genalloc(x))","8ff5a3cc":"countrydict['Pakistan']['borrower_genders'].unique()","bc04356e":"#Creating data dummies for the categorical values of our datasets\nfor key in countrydict:\n    sex = pd.get_dummies(countrydict[key]['borrower_genders'],drop_first= True)\n    ints = pd.get_dummies(countrydict[key]['repayment_interval'],drop_first= True)\n    sec = pd.get_dummies(countrydict[key]['sector'],drop_first= True)\n    countrydict[key].drop(['borrower_genders','repayment_interval','sector'], axis = 1, inplace = True)\n    countrydict[key] = pd.concat([countrydict[key], sex, ints,sec],axis = 1)","e29fad7e":"#Unfortunately, not all datasets have the same column entries. \n#The following dict has countries as keys with their respective dataset shapes as values\ndfshapes = {}\nfor key in countrydict:\n    dfshapes[key] = pd.DataFrame(index = countrydict[key].columns.drop('funded_amount')).shape\n    \nprint(max(dfshapes, key=dfshapes.get))\nprint(dfshapes[max(dfshapes, key=dfshapes.get)])\nprint('\\n')\nprint(min(dfshapes, key=dfshapes.get))\nprint(dfshapes[min(dfshapes, key=dfshapes.get)])\n\n#We can see that the gap between the shapes of countries is rather big","23b1ca08":"#We will use Kenya's columns as standard for all the countries\n#The same number of columns\nfull_columns = countrydict['Kenya'].columns","c4468363":"#If the columns is absent in a dataframe it is added with empty values\nfor key in countrydict:\n    for x in full_columns:\n        if x not in countrydict[key]:\n            countrydict[key][x] = 0","8321fcc1":"#The shapes are now normalized\ndfshapes = {}\nfor key in countrydict:\n    dfshapes[key] = pd.DataFrame(index = countrydict[key].columns.drop('funded_amount')).shape\n\nprint('Kenya \\n',dfshapes['Kenya'],'\\n','Mauritania\\n',dfshapes['Mauritania'])","6a2fd39f":"#We will gather lengths of dataframes in order to weed out country datasets that have a small amount of entries\n#The following loops will remove datasets of countries that have less than a thousand entries \nshort = []\nfor k, v in countrydict.items():\n    if len(v) < 1000:\n        short.append(k)\nprint(f'There are {len(short)} countries that have fewer than 1000 entries')\nfor x in short:\n    countrydict.pop(x)\n","acb27249":"#Importing necessary modules from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","25127536":"#Initiating dataframes that will later hold the coefficients for every model as well as the different metrics measuring models' efficiency\ncoefficients = pd.DataFrame(index = full_columns[1:])\nerrors = pd.DataFrame(index = ['Mean Absolute Error', 'Mean Squared Error', 'Mean Squared Error Root','R2 Score'])","0d27c51a":"#The following loop iterates through each dataset in countrydict\n#Sklearn algorithms split data in test\/train sets\n#Linear regression model is initiated and fitted for the required information\n#Both coefficients and metrics for each country are later appended to the respective dataframes\n\nfor key in countrydict:\n    X = countrydict[key].drop('funded_amount', axis = 1)\n    y = countrydict[key]['funded_amount']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n    lm = LinearRegression()\n    lm.fit(X_train,y_train)\n    preds = lm.predict(X_test)\n    err_list = []\n    err_list.append(metrics.mean_absolute_error(y_test,preds))\n    err_list.append(metrics.mean_squared_error(y_test, preds))\n    err_list.append(np.sqrt(metrics.mean_squared_error(y_test, preds)))\n    err_list.append(metrics.r2_score(y_test, preds))\n    coefficients[str(key)] = lm.coef_\n    errors[str(key)] = err_list","dca3fe59":"#Below we can see a dataframe with metrics calculated for each country's regression model\nerrors = errors.transpose()\nerrors.nlargest(n = 75 ,columns = 'R2 Score')","f336a26b":"#The R2 scores range from around 0.55 to 0.98 with a sigificant concentration around 0.9\nsns.distplot(errors['R2 Score'], bins = 25, kde = False)","5633dc15":"#This function will add the entry count for each country\n#This will enable us to see the relation between the R2 Score and the number of entries the regression is based on\ndf_ccount = df.groupby('country').count()\ndef country_count(x):\n    return df_ccount.loc[str(x), 'id']","363cd539":"#Creating the Count column which will display the number of entries for each country\nerrors.reset_index(level = 0, inplace = True)\nerrors['Count'] = errors['index'].apply(lambda x: country_count(x))","d9fc3f5b":"errors.set_index('index', inplace = True)\nerrors.nlargest(n = 10 ,columns = 'R2 Score')","59abe820":"#As we can see, there is no appearent correlation between the number of entries and the R2 Score\nsns.jointplot(x = 'R2 Score', y = 'Count', data = errors, kind = 'hex')","49b3993c":"#We will also search for correlation of the R2 core with the standard deviation of the funded amount\n#It is possible to hypothesize that, in this case, a good R2 Score correlates with low standard deviation\ndef df_std(x):\n    return df[df['country'] == str(x)]['funded_amount'].std()","3b2a7df6":"errors.reset_index(level = 0, inplace = True)\nerrors['Standard Deviation'] = errors['index'].apply(lambda x: df_std(x))\nerrors.set_index('index', inplace = True)","7255678d":"#As we can see on the plot below,there is no apparent correlation between the R2 Score and the Standard Deviation\nsns.jointplot(x = 'R2 Score', y = 'Standard Deviation', data = errors)","56468d2f":"#Countries with highest and lowest R2 scores\ntop_R2 = errors.nlargest(n = 75 ,columns = 'R2 Score')['R2 Score'].reset_index(level=0, inplace=False)\nR2_compare = pd.concat([top_R2[:5], top_R2[-5:]])\nsns.catplot(x = 'index', y = 'R2 Score', data = R2_compare, aspect = 4, kind = 'bar')\nplt.title('Countries with highest and lowest R2 Scores',fontsize = 20)\nplt.xlabel('Country', fontsize = 15)\nplt.ylabel('R2 Score', fontsize = 15)","2602c6a1":"#The following Coefficients table show coefficients that went into constructing each regression for each country \ncoefficients = coefficients.transpose()\ncoefficients.head(5)","fa7ae7ec":"country_sectors = pd.DataFrame(index = coefficients.columns[9:], columns = ['Country Min', 'Min', 'Country Max','Max'])\n","7684a404":"for x in coefficients.columns[9:]:\n    country_sectors.loc[str(x)]['Country Min'] = coefficients[coefficients[str(x)] == coefficients[str(x)].min()].index[0]\n    country_sectors.loc[str(x)]['Min'] = coefficients[coefficients[str(x)] == coefficients[str(x)].min()][str(x)][0]\n    country_sectors.loc[str(x)]['Country Max'] = coefficients[coefficients[str(x)] == coefficients[str(x)].max()].index[0]\n    country_sectors.loc[str(x)]['Max'] = coefficients[coefficients[str(x)] == coefficients[str(x)].max()][str(x)][0]","e2dec75e":"country_sectors","54cf52d7":"### Foreword\n\nThis workbook was completed at the beginning of July 2020, more than two years after the original publishing date of this dataset on kaggle.com.\nThe main objective of this workbook is to effectively apply machine learning algorithms to the dataset in order to determine possible correlations between values.\n\nThis dataset also introduced me to kiva, an incredible crowdfunding community whose microcredits empower borrowers from around the world. You too can become a lender - kiva will help you find a purposeful endeavor that will inspire change and progress. Find additional information on kiva.org.\n****\n***\n#### Disclaimer\n\nThis workbook is fairly short and does not do justice to the data set in terms of data exploring. I advise you to first discover the nature of the data yourself or with the help of other kernels published on Kaggle. \n***\n***\nThe code below was heavily commented on in order to improve its readability.\nIf you have any remarks or suggestions, I would be more than glad to see them in the comments section - I am always a message away.\n\nThank you for your attention and I hope you will enjoy exploring this workbook.\n\nSincerely,\n<br>\nAlexander NINUA\n","e703093e":"Does this dataset present any correlations between its numerical factors?","0b9117c4":"## 1. Forming a hypothesis","c078c70b":"Our research shows that linear regression models, when fitted by country, demonstrate an impressive R2 Score. The coefficients are also interesting to analyze, however, it is important to take them with a grain of salt.\n\nThe data is there and it can be explored further and further. Feel free to download my code and work with the data on your own terms. \n\nI hope that my workbook helped you discover interesting details of this dataset. Again, all the feedback on my work is highly appreciated.\n\nThank you for your attention!","58bb3938":"Now we are going to prepare the data for the linear regression. \nBefore fitting the regression model, will need to make sure that : \n* Categorical values have dummy values assigned\n* All the dataframes are of the same shape\n* Countries with less than 1000 entries are out of the list","8ea08d1e":"## 3. Linear Regression by country","d1362077":"***\n\nNow we can say that the regression model is fitted to each country with more than a thousand entries. \n\nAt this point you can download the information above and do your own research on the subject of data coefficients by country and their possible interpretations. \n\nBelow you will find a brief visualization of metrics and coefficients\n\n***","06e76822":"In this section we will be dividing the data by country - this will facilitate further analysis. \nMy goal is to have a dictionnary with country names as keys and their respective datasets as values.","5b399806":"In this part we will apply linear regression to every country in countrydict. The end goal is to see whether we can predict the funded amount based on the auxiliary data, such as the sector of the project, gender(s) of the borrower(s) or the proposed repayment interval.\n\n___\n\n\nWe will later see which countries have the most precise regression models and what role do the different coefficients play there. ","4ffacf6f":"# Kiva loans and linear regression\n## Fitting the linear regression model by country","9bc50c7d":"However, the dataset does not only present numerical values: the columns 'term_in_months' or 'borrower_genders' present categorical values that can still be interpreted. \n","c0aef537":"## 2. Preparing datasets by country","ece5fb2f":"##### General information about the dataset","81976d17":"## **Hypothesis**\n\nEach and every loan described in the dataset comes with additional values. \nThe hypothesis of this research states that the additional values, when analyzed, can predict the loan amount.\nIn the following chapters we will try to study this relationship by country using linear regression.\n\n"}}