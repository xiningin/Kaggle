{"cell_type":{"40f505da":"code","a3168311":"code","f57eafc7":"code","787cc32f":"code","754e38e2":"code","c25273de":"code","48ee306b":"code","231af73a":"code","486a4f0c":"code","b7f304bb":"code","8f5363cc":"code","73957479":"code","9d060aae":"code","aa27575e":"code","2dcc0432":"code","5ecb3cc4":"code","3e82525c":"code","40b031e0":"code","87009c09":"code","cc61e802":"code","6ebc7f30":"code","c7c2756b":"code","a903d63b":"code","ec6b08d3":"code","fe9c9094":"code","e9fbc258":"code","7faa2741":"code","563ab35c":"code","8c949b73":"code","fe89151d":"code","e3c9b4d7":"code","b8cb2ee7":"code","2ce4e411":"code","58883bf4":"code","8c396f12":"code","fc8c6447":"code","dc85ac4a":"code","9104d1c2":"markdown","e48b85e4":"markdown","dd776115":"markdown","701025d9":"markdown","46772fbe":"markdown","0404d4aa":"markdown","19687b8d":"markdown","6eb7b684":"markdown","cc306611":"markdown","e10a31a1":"markdown","03520783":"markdown","9ae28733":"markdown","1d29930e":"markdown","52a2b5c2":"markdown","2a04cd6b":"markdown","3b125cb0":"markdown","2300b6d1":"markdown"},"source":{"40f505da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3168311":"df = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","f57eafc7":"df.head()","787cc32f":"df.shape","754e38e2":"#Check for the null value\ndf.isnull().sum()","c25273de":"bins = (2,6.5,8)\nlabels = ['bad','good']\ndf['quality'] = pd.cut(df['quality'],bins=bins,labels=labels)","48ee306b":"def unistats(df):\n    output_df = pd.DataFrame(columns=['Count','Missing','NUnique','Unique','Dtype', 'Numeric','Mode','Mean','Min','25%','Median','75%','Max','Std', 'Skew', 'Kurt'])\n\n\n    for col in df:\n        if pd.api.types.is_numeric_dtype(df[col]):\n            output_df.loc[col] = [df[col].count(), df[col].isnull().sum(), df[col].nunique(), df[col].unique(), df[col].dtype, pd.api.types.is_numeric_dtype(df[col]),\n                                  df[col].mode().values[0], df[col].mean(), df[col].min(), df[col].quantile(0.25), df[col].median(), df[col].quantile(0.75),\n                                  df[col].max(), df[col].std(),df[col].skew(), df[col].kurt()]\n        else:\n            output_df.loc[col] = [df[col].count(),df[col].isnull().sum(),df[col].nunique(), df[col].unique(), df[col].dtype, pd.api.types.is_numeric_dtype(df[col]),\n                                  df[col].mode().values[0],'','','','','','','','','']\n    return output_df.sort_values(by=['Numeric','Skew', 'NUnique'], ascending=False)","231af73a":"unistats(df)","486a4f0c":"def univaritePlot(df,col,vartype):\n    if vartype==0:\n        sns.set(style=\"darkgrid\")\n        fig, ax=plt.subplots(nrows =1,ncols=2,figsize=(20,8))\n        ax[0].set_title(col.upper() + \" DISTRIBUTION PLOT\")\n        sns.distplot(df[col],ax=ax[0])\n        ax[1].set_title(col.upper() + \" BOX PLOT\")\n        sns.boxplot(data =df, x=col,ax=ax[1],orient='v')\n        plt.show()\n    if vartype==1:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(len(df[col].unique())+10 , 7)\n        ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index) \n        for p in ax.patches:\n            percentage = '{:.1f}%'.format(100 * p.get_height()\/len(df))\n            x = p.get_x() + p.get_width() \/ 2 - 0.05\n            y = p.get_y() + p.get_height()\n            ax.annotate(percentage, (x, y), size = 12)","b7f304bb":"univaritePlot(df=df,col='quality',vartype=1)","8f5363cc":"univaritePlot(df=df,col='chlorides',vartype=0)","73957479":"univaritePlot(df=df,col='density',vartype=0)","9d060aae":"univaritePlot(df=df,col='residual sugar',vartype=0)","aa27575e":"# Find the correlation between variables\ndf.corr()","2dcc0432":"#sns.pairplot(df,hue='quality')\n#plt.show()","5ecb3cc4":"# Encode the class bad as 0 and good as a 1\ndf['quality'] = df['quality'].map({'bad':0, 'good':1})","3e82525c":"df.head()","40b031e0":"df['quality'].value_counts()","87009c09":"y = df['quality']\nX = df.drop(['quality'], axis=1)","cc61e802":"# Split the data into train and test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","6ebc7f30":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)","c7c2756b":"X_test = scaler.transform(X_test)","a903d63b":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape : \", X_test.shape)\n\ny_train_imb = (y_train != 0).sum()\/(y_train == 0).sum()\ny_test_imb = (y_test != 0).sum()\/(y_test == 0).sum()\nprint(\"Imbalance in Train Data : \", y_train_imb)\nprint(\"Imbalance in Test Data : \", y_test_imb)","ec6b08d3":"# Balancing DataSet\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_train,y_train = sm.fit_sample(X_train,y_train)","fe9c9094":"print(\"X_train Shape\", X_train.shape)\nprint(\"y_train Shape\", y_train.shape)\n\nimb = (y_train != 0).sum()\/(y_train == 0).sum()\nprint(\"Imbalance in Train Data : \",imb)","e9fbc258":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.metrics import sensitivity_specificity_support\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\n","7faa2741":"lr = LogisticRegression()\n\n\nlr.fit(X_train,y_train)\npreds = lr.predict(X_test)","563ab35c":"\nprint(\"Accuracy Score:\",accuracy_score(preds,y_test))\nprint(\"classification Report:\\n\",classification_report(preds,y_test))\nprint(\"confusion Matrix:\\n\",confusion_matrix(preds,y_test))","8c949b73":"sensitivity, specificity, _ = sensitivity_specificity_support(y_test, preds, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, preds),2))","fe89151d":"svc= SVC()\nsvc.fit(X_train,y_train)","e3c9b4d7":"preds1= svc.predict(X_test)","b8cb2ee7":"print(\"Accuracy Score:\",accuracy_score(preds1,y_test))\nprint(\"classification Report:\\n\",classification_report(preds1,y_test))\nprint(\"confusion Matrix:\\n\",confusion_matrix(preds1,y_test))","2ce4e411":"sensitivity, specificity, _ = sensitivity_specificity_support(y_test, preds1, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, preds1),2))","58883bf4":"# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\nforest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n\n# hyperparameter space\nparams = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nmodel = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","8c396f12":"# fit model\nmodel.fit(X_train, y_train)","fc8c6447":"print(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","dc85ac4a":"# predict churn on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy Score:\",accuracy_score(y_pred,y_test))\n\nprint(\"classification Report:\\n\",classification_report(y_pred,y_test))\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","9104d1c2":" # Exploratory Data Analysis (EDA)","e48b85e4":"## Random Forest","dd776115":"## Data Cleaning","701025d9":"#### Read CSV file","46772fbe":"Scaling the Data","0404d4aa":"As we see there is not null value in the dataset. So we can proceed with the same data","19687b8d":"As we see the data class is imbalance so we need to handle this with using SMOTE","6eb7b684":"# Bivariate Analysis","cc306611":"## Logistic Regression","e10a31a1":"### Univariate Analysis","03520783":"# Red Wine Quality","9ae28733":"## Support Vector Machine","1d29930e":"## Please Upvote ","52a2b5c2":"- quality > 6.5 => \"good\"\n- quality < 6.5 => \"bad\"\n","2a04cd6b":"Let visualize each column in deep for better understanding","3b125cb0":"# Model Building","2300b6d1":"### Univariate Plot"}}