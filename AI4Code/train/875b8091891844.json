{"cell_type":{"f85001dd":"code","b7aa75b0":"code","f1443fb2":"code","857158f3":"code","391fb23f":"code","cf4b72c7":"code","e64ff51c":"code","644d2ad9":"code","40faf2f7":"code","11ea3127":"code","9d42d2e1":"code","1f56a426":"code","6c5e27b1":"code","8820cb42":"code","a86ed62c":"code","8a62a628":"code","9334f0a4":"code","2ca3f67f":"code","0d049c46":"code","3a487337":"code","46cbf7d7":"code","4b4ac20e":"code","0c114476":"code","a2a5b798":"markdown","b53675c0":"markdown","e4b01dd5":"markdown","490c01f0":"markdown","7e7020fd":"markdown","705a5d62":"markdown","c03a5eb8":"markdown","ed8f7dec":"markdown","20bce673":"markdown","1abbe7b5":"markdown","9f72df62":"markdown","512f7d77":"markdown","4b415c81":"markdown","8a97398a":"markdown","add09a82":"markdown","ed024243":"markdown","8ab1af36":"markdown","3f195bac":"markdown","4442318a":"markdown","ba06cfa1":"markdown","d1d5bdf8":"markdown","cf9ef025":"markdown","d2edb1cb":"markdown","a8dfb0dc":"markdown","3a9b5d09":"markdown"},"source":{"f85001dd":"# !mkdir train\n!rsync -av \/kaggle\/input\/speaker-recognition-dataset\/16000_pcm_speeches train ","b7aa75b0":"import tensorflow as tf\nimport os\nfrom os.path import isfile, join\nimport numpy as np\nimport shutil\nfrom tensorflow import keras\nfrom pathlib import Path\nfrom IPython.display import display, Audio\nimport subprocess","f1443fb2":"DATASET_ROOT = os.path.join( \"train\/16000_pcm_speeches\")\n\nmy_audios_folder = 'audio'\nmy_noises_folder = 'noise'\n\nDATASET_AUDIOS_PATH = os.path.join(DATASET_ROOT, my_audios_folder)\nDATASET_NOISES_PATH = os.path.join(DATASET_ROOT, my_noises_folder)\n\n\n# Percentage of samples to use for validation\nVALID_SPLIT = 0.1\n\n# Seed to use when shuffling the dataset and the noise\nSHUFFLE_SEED = 43\n\n# The sampling rate to use.\n# This is the one used in all of the audio samples.\n# We will resample all of the noise to this sampling rate.\n# This will also be the output size of the audio wave samples\n# (since all samples are of 1 second long)\nSAMPLING_RATE = 16000\n\n# The factor to multiply the noise with according to:\n#   noisy_sample = sample + noise * prop * scale\n#      where prop = sample_amplitude \/ noise_amplitude\nSCALE = 0.5\n\nBATCH_SIZE = 128\nEPOCHS = 30","857158f3":"# If folder `audio`, does not exist, create it, otherwise do nothing\nif os.path.exists(DATASET_AUDIOS_PATH) is False:\n    os.makedirs(DATASET_AUDIOS_PATH)\n\n# If folder `noise`, does not exist, create it, otherwise do nothing\n# MY OWN NOTE : LET's SEE tf.io.gfile\nif tf.io.gfile.exists(DATASET_NOISES_PATH) is False:\n    tf.io.gfile.makedirs(DATASET_NOISES_PATH)\n\nfor folder in os.listdir(DATASET_ROOT):\n    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n        if folder in [my_audios_folder, my_noises_folder]:\n            # If folder is `audio` or `noise`, do nothing\n            continue\n        elif folder in [\"other\", \"_background_noise_\"]:\n            # If folder is one of the folders that contains noise samples,\n            # move it to the `noise` folder\n            shutil.move(\n                os.path.join(DATASET_ROOT, folder),\n                os.path.join(DATASET_NOISES_PATH, folder),\n            )\n        else:\n            # Otherwise, it should be a speaker folder, then move it to\n            # `audio` folder\n            shutil.move(\n                os.path.join(DATASET_ROOT, folder),\n                os.path.join(DATASET_AUDIOS_PATH, folder),\n            )","391fb23f":"# Get the list of all noise files\nnoise_paths = []\nfor subdir in tf.io.gfile.listdir(DATASET_NOISES_PATH):\n    subdir_path = Path(DATASET_NOISES_PATH) \/ subdir\n    if os.path.isdir(subdir_path):\n        noise_paths += [\n            os.path.join(subdir_path, filepath)\n            for filepath in os.listdir(subdir_path)\n            if filepath.endswith(\".wav\")\n        ]\n\nprint(\n    \"Found {} files belonging to {} directories\".format(\n        len(noise_paths), len(os.listdir(DATASET_NOISES_PATH))\n    )\n)","cf4b72c7":"command = (\n    \"for dir in `ls -1 \" + DATASET_NOISES_PATH + \"`; do \"\n    \"for file in `ls -1 \" + DATASET_NOISES_PATH + \"\/$dir\/*.wav`; do \"\n    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n    \"$file | grep sample_rate | cut -f2 -d=`; \"\n    \"if [ $sample_rate -ne 16000 ]; then \"\n    \"ffmpeg -hide_banner -loglevel panic -y \"\n    \"-i $file -ar 16000 temp.wav; \"\n    \"mv temp.wav $file; \"\n    \"fi; done; done\"\n)\n\nos.system(command)\n\n# Split noise into chunks of 16000 each\ndef load_noise_sample(path):\n    sample, sampling_rate = tf.audio.decode_wav(\n        tf.io.read_file(path), desired_channels=1\n    )\n    if sampling_rate == SAMPLING_RATE:\n        # Number of slices of 16000 each that can be generated from the noise sample\n        slices = int(sample.shape[0] \/ SAMPLING_RATE)\n        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n        return sample\n    else:\n        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n        return None\n\n\nnoises = []\nfor path in noise_paths:\n    sample = load_noise_sample(path)\n    if sample:\n        noises.extend(sample)\nnoises = tf.stack(noises)\n\nprint(\n    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n        len(noise_paths), noises.shape[0], noises.shape[1] \/\/ SAMPLING_RATE\n    )\n)","e64ff51c":"# see Full detail of a sound\n! ffprobe -i .\/train\/16000_pcm_speeches\/noise\/other\/pink_noise.wav -show_streams -select_streams a:0\n# ! ffprobe -i .\/train\/16000_pcm_speeches\/audio\/Benjamin_Netanyau\/100.wav -show_streams -select_streams a:0","644d2ad9":"def paths_and_labels_to_dataset(audio_paths, labels):\n    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n    # giving an array so we can create tf.data.Dataset.from_tensor_slice(list)\n    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n    audio_ds = path_ds.map(lambda x : path_to_audio(x))\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    return tf.data.Dataset.zip((audio_ds, label_ds))\n\n\ndef path_to_audio(path):\n    \"\"\"Reads and decodes an audio file.\"\"\"\n      # tf.audio.decode_wav(\n#     contents, desired_channels=-1, desired_samples=-1, name=None\n# )\n    audio = tf.io.read_file(path)\n    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n    return audio\n\n\n#TODO. Need to print each part for one sample\ndef add_noise(audio, noises=None, scale=0.5):\n    if noises is not None:\n        # Create a random tensor of the same size as audio ranging from\n        # 0 to the number of noise stream samples that we have.\n        tf_rnd = tf.random.uniform(\n            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n        )\n        # https:\/\/www.geeksforgeeks.org\/python-tensorflow-gather\/ \u0627\u06cc\u0646 \u0627\u06cc\u0646\u062f\u06a9\u0633 \u0647\u0627\u06cc\u06cc \u06a9\u0647 \u0645\u06cc\u06af\u0645\u0648 \u0627\u0632 \u0627\u06cc\u0646 \u0627\u0631\u0627\u06cc\u0647 \u0628\u0647 \u0645\u0646 \u0628\u062f\u0647\n        noise = tf.gather(noises, tf_rnd, axis=0)\n\n        # Get the amplitude proportion between the audio and the noise\n        prop = tf.math.reduce_max(audio, axis=1) \/ tf.math.reduce_max(noise, axis=1)\n        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n\n        # Adding the rescaled noise to audio\n        audio = audio + noise * prop * scale\n\n    return audio\n\n# TODO. shape of output.   each line output\ndef audio_to_fft(audio):\n    # Since tf.signal.fft applies FFT on the innermost dimension,\n    # we need to squeeze the dimensions and then expand them again\n    # after FFT\n    audio = tf.squeeze(audio, axis=-1)\n    # https:\/\/blog.faradars.org\/complex-numbers\/   \u0628\u0631\u0627\u06cc \u0645\u062d\u0627\u0633\u0628\u0647 \u0641\u0648\u0631\u06cc\u0647 \u0628\u0627\u06cc\u062f \u0648\u0631\u0648\u062f\u06cc \u0628\u0635\u0648\u0631\u062a \u0627\u0639\u062f\u0627\u062f \u0645\u062e\u062a\u0644\u0637 \u0628\u0627\u0634\u0646\n    fft = tf.signal.fft(\n        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n    )\n    fft = tf.expand_dims(fft, axis=-1)\n\n    # Return the absolute value of the first half of the FFT\n    # which represents the positive frequencies\n    return tf.math.abs(fft[:, : (audio.shape[1] \/\/ 2), :])\n\n\n# ========================================== ADDING MORE FEATURE EXTRACTIONS ===============================================\n","40faf2f7":"audio = tf.io.read_file('train\/16000_pcm_speeches\/audio\/Benjamin_Netanyau\/0.wav')\naudio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n\n# what is this fft doing ? \n# https:\/\/stackoverflow.com\/questions\/31662854\/why-should-i-discard-half-of-what-a-fft-returns\n# https:\/\/stackoverflow.com\/questions\/6740545\/understanding-fft-output\n\n\n# https:\/\/www.google.com\/search?q=what+is+first+half+of+FFT+result+site:stackoverflow.com&sxsrf=ALeKk03HyI7BG_P0_4dnCO3H5v5namISfg:1612258300719&sa=X&ved=2ahUKEwjdqLOa8sruAhVkoFwKHSQFA1cQrQIoBHoECAMQBQ&biw=1345&bih=667\n# https:\/\/www.google.com\/search?q=what+is+first+half+of+FFT+result+site:dsp.stackexchange.com&sxsrf=ALeKk03HyI7BG_P0_4dnCO3H5v5namISfg:1612258300719&sa=X&ved=2ahUKEwjdqLOa8sruAhVkoFwKHSQFA1cQrQIoBHoECBYQBQ\n\ndef audio_to_fft_example(audio):\n\n    audio = tf.squeeze(audio, axis=-1)\n    fft = tf.signal.fft(\n        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n    )\n    print(fft[2])\n    print(fft[15998])\n\n    # print(fft)\n    # print(audio.shape[0] \/\/ 2)\n    \n    fft = tf.expand_dims(fft, axis=-1)\n\n    return tf.math.abs(fft[:, : (audio.shape[0] \/\/ 2)])\n\n\n\n# test_audio_to_fft_example = audio_to_fft_example(audio)\n\n\n\n# How does this add noise works?\ndef add_noise_example(audio, noises=None, scale=0.5):\n    if noises is not None:\n        # Create a random tensor of the same size as audio ranging from\n        # 0 to the number of noise stream samples that we have.\n\n        tf_rnd = tf.random.uniform(\n            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n        )\n        # https:\/\/www.geeksforgeeks.org\/python-tensorflow-gather\/ \u0627\u06cc\u0646 \u0627\u06cc\u0646\u062f\u06a9\u0633 \u0647\u0627\u06cc\u06cc \u06a9\u0647 \u0645\u06cc\u06af\u0645\u0648 \u0627\u0632 \u0627\u06cc\u0646 \u0627\u0631\u0627\u06cc\u0647 \u0628\u0647 \u0645\u0646 \u0628\u062f\u0647\n        noise = tf.gather(noises, tf_rnd, axis=0)\n\n        # Get the amplitude proportion between the audio and the noise\n        prop = tf.math.reduce_max(audio, axis=1) \/ tf.math.reduce_max(noise, axis=1)\n        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n\n        # Adding the rescaled noise to audio\n        audio = audio + noise * prop * scale\n\n    return audio","11ea3127":"print(audio.shape) # (16000, 1)\nprint(noises.shape) # (354, 16000, 1)\n\n'''\ntf.random.uniform(\n    shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None\n)\n'''\n# Note it will create random 16000 numbers between 0 and 354   max(random_values_created) >> 353\nrandom_values_created = tf.random.uniform((tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32)  # 16000 , 1\n\n\n'''\ntf.gather(\n    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None\n)\naxis = 0 means row (if array is 1D, means index ), 1 means column\n'''\n# https:\/\/www.geeksforgeeks.org\/python-tensorflow-gather\/ \u0627\u06cc\u0646 \u0627\u06cc\u0646\u062f\u06a9\u0633 \u0647\u0627\u06cc\u06cc \u06a9\u0647 \u0645\u06cc\u06af\u0645\u0648 \u0627\u0632 \u0627\u06cc\u0646 \u0627\u0631\u0627\u06cc\u0647 \u0628\u0647 \u0645\u0646 \u0628\u062f\u0647\nnoise = tf.gather(noises, random_values_created, axis=0) # 16000, 16000, 1\n\n\n# tf.math.reduce_max([[1],[2],[3],[4]], axis=1)  # <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n# tf.math.reduce_max([[1],[2],[3],[4]], axis=0)  # <tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>\n\n\"\"\"\n tf.math.reduce_max(audio, axis=1).shape, tf.math.reduce_max(noise, axis=1).shape\n\n TensorShape([16000])), (TensorShape([16000, 1])\n\"\"\"\n# \u0627\u06cc\u0646 \u062e\u0637 \u0646\u0633\u0628\u062a \u0647\u0631 \u062f\u0627\u0645\u0646\u0647 \u0627\u0632 \u0635\u0648\u062a \u0628\u0647 \u062a\u0645\u0627\u0645 \u062f\u0627\u0645\u0646\u0647 \u0647\u0627\u06cc \u0646\u0648\u06cc\u0632\ntf.math.reduce_max(audio, axis=1) \/ tf.math.reduce_max(noise, axis=1) # 16000, 16000","9d42d2e1":"class_names = tf.io.gfile.listdir(DATASET_AUDIOS_PATH)\nprint(\"Our class names: {}\".format(class_names,))\n\naudio_paths = []\nlabels = []\n\nfor label, name in enumerate(class_names):\n    print(\"Processing speaker {}\".format(name,))\n    dir_path = Path(DATASET_AUDIOS_PATH) \/ name\n    speaker_sample_paths = [\n        os.path.join(dir_path, filepath)\n        for filepath in os.listdir(dir_path)\n        if filepath.endswith(\".wav\")\n    ]\n    audio_paths += speaker_sample_paths\n    labels += [label] * len(speaker_sample_paths)\n\nprint(labels)\n\nprint(\n    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n)\n\n# Shuffle\n\n'''\nwhy does it work? \narr = [1,2,3,4,5,6,7,8,9]\narr2 = [11,22,33,44,55,66,77,88,99]\n\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(arr)\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(arr2)\n\narr, arr2\n>> ([4, 9, 7, 8, 3, 6, 2, 1, 5], [44, 99, 77, 88, 33, 66, 22, 11, 55])\n'''\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(audio_paths)\nrng = np.random.RandomState(SHUFFLE_SEED)\nrng.shuffle(labels)\n\n\n# Split into training and validation\nnum_val_samples = int(VALID_SPLIT * len(audio_paths))\nprint(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\ntrain_audio_paths = audio_paths[:-num_val_samples]\ntrain_labels = labels[:-num_val_samples]\n\nprint(\"Using {} files for validation.\".format(num_val_samples))\nvalid_audio_paths = audio_paths[-num_val_samples:]\nvalid_labels = labels[-num_val_samples:]\n\n'''\nThis dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, \nreplacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, \nthen shuffle will initially select a random element from only the first 1,000 elements in the buffer.\n Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n\nreshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, \nthe idiomatic way to create epochs was through the repeat transformation:\n'''\n\n# Create 2 datasets, one for training and the other for validation\ntrain_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\ntrain_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n    BATCH_SIZE\n)\n\nvalid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\nvalid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)","1f56a426":"# Add noise to the training set\ntrain_ds = train_ds.map(\n    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n)\n\n# Transform audio wave to the frequency domain using `audio_to_fft`\ntrain_ds = train_ds.map(\n    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n)\n'''\nCreates a Dataset that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current\n element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs.\n batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2\n  elements (2 batches, of 20 examples each).\n'''\n\n\n'''\nThe tf.data API provides a software pipelining mechanism through the tf.data.Dataset.prefetch transformation, \nwhich can be used to decouple the time when data is produced from the time when data is consumed. In particular,\n the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of\n  the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of\n   batches consumed by a single training step. You could either manually tune this value, or set it \n   to tf.data.experimental.AUTOTUNE which will prompt the tf.data runtime to tune the value dynamically at runtime.\n'''\n\ntrain_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n\nvalid_ds = valid_ds.map(\n    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n)\nvalid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)","6c5e27b1":"# TODO : write this in model subclassing model.\n\n# TODO : 1D Convolutions\n# TODO : \u0646\u0642\u0627\u0634\u06cc \u0627\u06cc\u0646 \ndef residual_block(x, filters, conv_num=3, activation='relu'):\n  #shortcut\n  s = keras.layers.Conv1D(filters, 1, padding='same', )(x)\n  for i in range(conv_num -1):\n    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n    x = keras.layers.Activation(activation)(x)\n  x = keras.layers.Conv1D(filters, 3, padding='same')(x)\n  x = keras.layers.Add()([x, s])\n  x = keras.layers.Activation(activation)(x)\n  return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n\n\ndef build_model(input_shape, num_classes):\n  inputs = keras.layers.Input(shape=input_shape, name='audio_input')\n  x = residual_block(inputs, 16, 2)\n  x = residual_block(x, 32, 2)\n  x = residual_block(x, 64, 3)\n  x = residual_block(x, 128, 3)\n  x = residual_block(x, 128, 3)\n\n  x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n  # Flatten. I removed Flatten Layer. And saw next line error : \n\n  '''\n  assertion failed: [Condition x == y did not hold element-wise:]\n   [x (sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/Shape_1:0) = ] [128 1]\n    [y (sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/strided_slice:0) = ] [128 83]\n\t [[node sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\n    (defined at <ipython-input-39-90680b89c031>:5) ]] [Op:__inference_train_function_13032]\n  \n  '''\n  x = keras.layers.Flatten()(x)\n  x = keras.layers.Dense(256, activation=\"relu\")(x)\n  x = keras.layers.Dense(128, activation=\"relu\")(x)\n  outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n\n  return keras.models.Model(inputs=inputs, outputs=outputs)\n\n\nmodel = build_model((SAMPLING_RATE \/\/ 2, 1), len(class_names))\n\nmodel.summary()","8820cb42":"dot_img_file = '.\/train\/model_1.png'\ntf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=False)","a86ed62c":"# TODO : USE Categorical_crossentropy. for this type of loss \n'''\nsource : https:\/\/stats.stackexchange.com\/questions\/326065\/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other\nBoth, categorical cross entropy and sparse categorical cross entropy have the same loss function which you have mentioned above.\n The only difference is the format in which you mention Yi (i,e true labels).\n\nIf your Yi's are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\n\nBut if your Yi's are integers, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [1] , [2], [3]\n\nThe usage entirely depends on how you load your dataset. One advantage of using sparse categorical cross entropy is it saves\n time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n\n https:\/\/keras.io\/api\/losses\/probabilistic_losses\/#sparse_categorical_crossentropy-function\n https:\/\/keras.io\/api\/losses\/probabilistic_losses\/#categorical_crossentropy-function\n'''\n\ny_true = [[0, 1, 0], [0, 0, 1]]\ny_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\ncce = keras.losses.CategoricalCrossentropy()\ncce_output = cce(y_true, y_pred).numpy()\n\n\ny_true = [1, 2]\ny_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] \nscce = tf.keras.losses.SparseCategoricalCrossentropy()\nscce_output = scce(y_true, y_pred).numpy()\n\ncce_output, scce_output","8a62a628":"#TODO Metric and GradientTape : https:\/\/keras.io\/api\/metrics\/\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# CallBacks \ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\".\/logs\")  # https:\/\/keras.io\/api\/callbacks\/tensorboard\/\nmodel_save_filename = \"model.h5\"\nearlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nmdlcheckpoint_cb = keras.callbacks.ModelCheckpoint( model_save_filename, monitor='val_accuracy', save_best_only=True )","9334f0a4":"history = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    validation_data=valid_ds,\n    callbacks=[earlystopping_cb, mdlcheckpoint_cb, tensorboard_callback],\n)","2ca3f67f":"# let us see how is gradient of expression A with respect to a\na = tf.Variable(2.5)\nb = tf.Variable(3.0)\n\nwith tf.GradientTape() as tape:\n  tape.watch(a)\n  A = ( 3 * a**2 ) + b\n  mamd = tape.gradient(A, a)\n  print(mamd)\n","0d049c46":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_history = []","3a487337":"def train_step(audio, label):\n  with tf.GradientTape() as tape:\n    predicted = model(audio, training=True)\n    loss_value = loss_object(label, predicted)\n\n  loss_history.append(loss_value.numpy().mean())\n  grads = tape.gradient(loss_value, model.trainable_variables)\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))","46cbf7d7":"model = build_model((SAMPLING_RATE \/\/ 2, 1), len(class_names))\ndef train(epochs):\n  for epoch in range(epochs):\n    for (batch, (audios, labels)) in enumerate(train_ds):\n      \n      train_step(audios, labels)\n    print ('Epoch {} finished'.format(epoch))\n    \ntrain(10)","4b4ac20e":"import matplotlib.pyplot as plt\n \nplt.plot(loss_history) \nplt.xlabel('Batch #')\nplt.ylabel('Loss [entropy]')","0c114476":"class CustomModel(keras.Model):\n  def train_step(self, data):\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    print(data)\n    x, y = data\n    with tf.GradientTape() as tape:\n      y_pred = self(x, training=True)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss = self.compiled_loss(y, y_pred,\n                                regularization_losses=self.losses)\n    # Compute gradients\n    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n    # Update weights\n    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self.compiled_metrics.update_state(y, y_pred)\n    # Return a dict mapping metric names to current value\n    return {m.name: m.result() for m in self.metrics}\n\n# Construct and compile an instance of CustomModel\n# inputs = keras.Input(shape=(32,))\n# outputs = keras.layers.Dense(1)(inputs)\n# model = CustomModel(inputs, outputs)\nmodel = build_model((SAMPLING_RATE \/\/ 2, 1), len(class_names))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nearlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\n# Just use `fit` as usual\nmodel.fit(train_ds, epochs=3, callbacks=[earlystopping_cb])","a2a5b798":"Let's convert all noises to 16000HZ and slice them to 1 second audios","b53675c0":"our imports are here","e4b01dd5":"Let's see some of methods which are used.","490c01f0":" what is gradient tape ?\n\nSo first let's see what is this gradient tape then use it in our code.","7e7020fd":"what is the difference between this 2 type of loss function? <br>\nwe will see bellow","705a5d62":"# Second Approach : Class Base Approach ( gradienttape for engineers )\n\n[fchollet code](https:\/\/keras.io\/getting_started\/intro_to_keras_for_engineers\/#using-fit-with-a-custom-training-step)\n\nOk in previuos code we had everything we wanted except Callbacks :). So if you need them really mad. just use this approach. I don't run this code you can run it if you want. :)","c03a5eb8":"# Dataset Generation And Modeling\n","ed8f7dec":"Let's see one example to check if commands are working","20bce673":"![Tensorflow is beautifull](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmlfromscratch.com%2Ftensorflow-2%2F&psig=AOvVaw3e3hgnP0XLgUg3phi3TrTx&ust=1614263709030000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCMCihIzfgu8CFQAAAAAdAAAAABAI)","1abbe7b5":" Structure of data folders : \n\n\n\n```\nmain_directory\/\n...speaker_a\/\n...speaker_b\/\n...speaker_c\/\n...speaker_d\/\n...speaker_e\/\n...other\/\n..._background_noise_\/\nAfter sorting, we end up with the following structure:\n\nmain_directory\/\n...audio\/\n......speaker_a\/\n......speaker_b\/\n......speaker_c\/\n......speaker_d\/\n......speaker_e\/\n...noise\/\n......other\/\n......_background_noise_\/\n```","9f72df62":"# Sample code to see what are prepare dataset function doing?\n\n<br> \n\nso what are these functions doing? how they create our dataset? let's dive into them.","512f7d77":"let's create this directory structure","4b415c81":"# First approach : 2 Nested For approach ( gradienttape for researchers )","8a97398a":"get the list of all noise files","add09a82":"we have 2 model to use graident tape. \n1.  One is Create a class.\n2.  second one is to craete a function and use 2 nested for","ed024243":"our variables are defined here","8ab1af36":"# Functions to prepare data for feeding to model","3f195bac":"# Keras Doc Code Part ( I added some codes to it. )\n\n[Source](https:\/\/keras.io\/examples\/audio\/speaker_recognition_using_cnn\/)","4442318a":"# create model","ba06cfa1":"# feature Extraction","d1d5bdf8":"# categorical_cross_entory VS sparse_categorical_cross_entropy","cf9ef025":"# Compile model and fit","d2edb1cb":"# Same Model But Use Gradient_Tape for training","a8dfb0dc":"# create tf Dataset object","3a9b5d09":"Hi every One. In this kernel we will see how to use Tensorflow gradient tape and creating our custom training loops. <br>\n\nNOTE : I use Keras doc sample code for preparing audios and creating model **BUT I WILL ADD AS MUCH COMMENT AS I CAN** to make it simpler to understand. <br>\n\n\nWe have 2 type to use gradient tape we will wee them soon"}}