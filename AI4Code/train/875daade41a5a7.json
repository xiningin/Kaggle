{"cell_type":{"1eefcd07":"code","e8af9a95":"code","1265a5f7":"code","ecd0cf4e":"code","6538aa6e":"code","bff2f210":"code","dfe10e9f":"code","727b7e53":"code","d857f4ed":"code","8b8cef0b":"code","1846a534":"code","d64d5d08":"code","539e4e75":"code","88a40aa7":"code","b83a2d2b":"code","171c4be5":"code","0aca49c5":"code","86aec404":"code","10393452":"code","4d575be8":"code","e318455d":"code","fc200246":"code","a65e87a6":"code","8e2f5f8a":"code","80eaba88":"code","6920b514":"code","a6486c3a":"code","0db6e5d2":"markdown","2f445371":"markdown","1cb5fe0a":"markdown","830e52c3":"markdown","deb2f010":"markdown","fd556ed5":"markdown","5bd6390a":"markdown","5a84c8db":"markdown","8142bd62":"markdown","990ff229":"markdown"},"source":{"1eefcd07":"import fasttext, string, collections\nimport pandas as pd, numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","e8af9a95":"#full length of dataset is 101,230,332\nN_SAMPLES = 1000000\n\ntrain = pd.read_pickle('..\/input\/riiid-answer-correctness-prediction-files\/train.pkl')\ntrain = train[train['content_type_id'] == 0]\n\nquestions = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv').rename({'question_id':'content_id'},\n                                                                                        axis=1)\n\nprint(train.shape)\ntrain = train.sample(N_SAMPLES, random_state=34)\nprint(train.shape)\ntrain = pd.merge(train, questions, how='left', on='content_id')\nprint(train.shape)\nprint(train['user_id'].nunique())\ntrain.head()","1265a5f7":"print(f\"There are {train['content_id'].nunique()} unique content ids in train.csv\")\nprint(f\"On average, students get {round(train['answered_correctly'].mean(), 3)} of questions correct\")","ecd0cf4e":"train['content_id'].value_counts()","6538aa6e":"max_ = 0\niter_ = 0\nfor i, row in enumerate(train['tags'].values):\n    if len(row.split()) > max_: \n        max_ = len(row.split())\n        iter_ = i\ntrain.iloc[iter_]","bff2f210":"tags = []\n\nfor row in tqdm(train.index):\n    tags.append(train.iloc[row]['tags'].split())","dfe10e9f":"#check nunique tags\nlen(set([tag for sublist in tags for tag in sublist]))","727b7e53":"questions['part'].value_counts(normalize=True)","d857f4ed":"print(questions['bundle_id'].nunique())\nquestions['bundle_id'].value_counts()","8b8cef0b":"greek = '\\u03A9\\u0394\\u03BC\\u00B0\\u0302\\u03C0\\u03F4\\u03BB\\u03B1\\u03B3\\u03B4\\u03B5\\u03B6\\u03B7\\u03B8\\u03B9\\u03BA\\u03BD\\u03BE\\u03C1\\u03C2\\u03C3\\u03C4\\u03C5\\u03C6\\u03C7\\u03C8\\u03C9\\u0391\\u0395\\u0396\\u0397\\u0398\\u0399\\u039A\\u039B\\u039C\\u039D\\u039E\\u039F\\u03A0\\u03A1\\u03A3\\u03A4\\u03A5\\u03A6\\u03A7\\u03A8'\ngreek","1846a534":"cyrillic = '\\u0410\\u0430\\u0411\\u0413\\u0414\\u0415\\u0416\\u0417\\u0418\\u0419\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u041a\\u041b\\u041c\\u041d\\u041e\\u041f\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0420\\u0421\\u0422\\u0423\\u0424\\u0425\\u0426\\u0427\\u0428\\u0429'\ncyrillic","d64d5d08":"latin = '\\u00A1\\u00A2\\u00A3\\u00A4\\u00A5\\u00A6\\u00A7\\u00A8\\u00A9\\u00B0\\u00B1\\u00B2\\u00B3\\u00B4\\u00B5\\u00B6\\u00B7\\u00B8\\u00B9\\u00C0\\u00C1\\u00C3\\u00C5\\u00C6\\u00C7\\u00C8\\u00C9\\u00D0\\u00D1\\u00D2\\u00D4\\u00D5\\u00D6\\u00D7\\u00D8\\u00D9'\nlatin","539e4e75":"#not using 012 because they are reserved for labels\nchars = '3456789' + string.ascii_letters + greek + cyrillic + latin\nlen(chars)","88a40aa7":"most_common = [word for word, word_count in collections.Counter([tag for sublist in tags for tag in sublist]).most_common(len(chars))]\nleast_common = [word for word, word_count in collections.Counter([tag for sublist in tags for tag in sublist]).most_common()[len(chars):]]\nlen(most_common)","b83a2d2b":"char_dict = {}\n\nfor tag, char in zip(most_common, chars):\n    char_dict[tag] = char","171c4be5":"#remove low frequency tags and maps to unique character\ndef filter_tags(tag):\n    return_tag = []\n    \n    for tag in tag.split():\n        if tag not in least_common:\n            tag = char_dict.get(tag)\n            return_tag.append(tag)\n            \n    return \" \".join(return_tag)\n     \ntrain['tags'] = train['tags'].apply(filter_tags)","0aca49c5":"#I know I know, this is sloppy\ndef pad_tags(tag):\n    if len(tag.split()) == 1:\n        return \"\".join([tag]*6)\n    \n    if len(tag.split()) == 2:\n        tag1 = tag.split()[0]\n        tag2 = tag.split()[1]\n        return \"\".join([tag1, tag2]*3)\n    \n    if len(tag.split()) == 3:\n        tag1 = tag.split()[0]\n        tag2 = tag.split()[1]\n        tag3 = tag.split()[2]\n        return \"\".join([tag1, tag2, tag3]*2)\n    \n    if len(tag.split()) == 4:\n        tag1 = tag.split()[0]\n        tag2 = tag.split()[1]\n        tag3 = tag.split()[2]\n        tag4 = tag.split()[3]\n        return \"\".join([tag1, tag2, tag3, tag4, tag1, tag2])\n    \n    if len(tag.split()) == 5:\n        tag1 = tag.split()[0]\n        tag2 = tag.split()[1]\n        tag3 = tag.split()[2]\n        tag4 = tag.split()[3]\n        tag5 = tag.split()[4]\n        return \"\".join([tag1, tag2, tag3, tag4, tag5, tag1])\n    \n    else: return tag      \n    \ntrain['tags'] = train['tags'].apply(pad_tags)","86aec404":"train['answered_correctly'] = train['answered_correctly'].apply(str)\ntrain['answered_correctly'] = train['answered_correctly'].replace({'-1':'2'})\ntrain['interaction_enc'] = train['tags'] + train['answered_correctly']","10393452":"#debugging step\ni = 0\nfor i in range(100000):\n    if len([_ for _ in train.iloc[i]['interaction_enc']]) != 7: print('stop'); print(i); break","4d575be8":"with open('corpus.txt', 'w') as file:\n    for user in tqdm(train['user_id'].unique()):\n        user_df = train[train['user_id'] == user]\n        line=' '.join(user_df['interaction_enc'].values)\n        file.write(line+'\\n')","e318455d":"DIM = 200\nWINDOW = 6\nPERPLEXITY = 40\nN_ITER = 2500\nSEED = 34","fc200246":"%%time\n\ncbow = fasttext.train_unsupervised('corpus.txt', model='cbow',\n                                    dim=DIM, minn=1, maxn=1, ws=WINDOW)\nskipgram = fasttext.train_unsupervised('corpus.txt', model='skipgram',\n                                    dim=DIM, minn=1, maxn=1, ws=WINDOW)\nprint(f\"{cbow.get_output_matrix().mean()}\")                                                                                                                    \nprint(f\"{skipgram.get_output_matrix().mean()}\")","a65e87a6":"#sanity check\ncbow.get_subwords('Xy5Xy51')","8e2f5f8a":"# https:\/\/www.kaggle.com\/jeffd23\/visualizing-word-vectors-with-t-sne\/comments\ndef tsne_plot(model):\n    labels = []\n    tokens = []\n\n    for word in model.words:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=PERPLEXITY, n_components=2, init='pca', \n                      n_iter=N_ITER, n_jobs=-1, random_state=SEED)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(15, 15)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        \n        if i%50==0:\n            plt.annotate(labels[i],\n                         xy=(x[i], y[i]),\n                         xytext=(5, 2),\n                         textcoords='offset points',\n                         ha='right',\n                         va='bottom')\n    plt.show()","80eaba88":"tsne_plot(cbow)","6920b514":"tsne_plot(skipgram)","a6486c3a":"question_vectors = list(zip(train['tags'].values, [cbow.get_word_vector(word) for word in train['tags']]))","0db6e5d2":"**After mapping each tag to one of the characters in `string.printable`, we then 'pad' our tags such that even those with only 1 tag can be compared to those with 6. Now we can string concat the answer of the question so that the model learns it as a sort of word suffix.**","2f445371":"**As per the [data description tab](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/data), the `tags` of the questions found in the `questions.csv` of the competition are sufficient for clustering the questions together. In total, we have the following features to play around with to group questions together. We can of course engineer more clustering features, but for now we stick with these:**\n\n**Train**\n* `content_id` - (int16) ID code for the user interaction\n\n**Questions**\n* `tags` - one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n* `bundle_id` - code for which questions are served together.\n* `part` - the relevant section of the TOEIC test\n\n**Let's quickly explore these features. We begin with `content_id`. Bare in mind I am using a specific subsample of `train.csv` defined by the first block of the notebook.**","1cb5fe0a":"**Now we move onto the `tags` feature. Since there are multiple tags per questions, I mainly want to know what the maximum number of tags a given question can have and how many unique tags there are.**","830e52c3":"**We can train a skipgram or a continuous bag-of-words (CBOW) model. In general, CBOW learns better syntactic relationships between words while skipgram is better at capturing semantic relationships. Suppose we have the word `general`. CBOW would fetch morphologically similar words, like `generalize` or `generalized`, whereas the skipgram would find words like `universal` and `common`. As we don't really have words here, let's just try both.**\n\n**From the [fastText documentation](https:\/\/fasttext.cc\/docs\/en\/python-module.html), we can play with the following parameters. Values in `[]` are default values.**\n\n* **input** - training file path (required)\n* **model** - unsupervised fasttext model {cbow, skipgram} [skipgram]\n* **lr** - learning rate [0.05]\n* **dim** - size of word vectors [100]\n* **ws** - size of the context window [5]\n* **epoch** - number of epochs [5]\n* **minCount** - minimal number of word occurences [5]\n* **minn** - min length of char ngram [3]\n* **maxn** - max length of char ngram [6]\n* **neg** - number of negatives sampled [5]\n* **wordNgrams** - max length of word ngram [1]\n* **loss** - loss function {ns, hs, softmax, ova} [ns]\n* **bucket** - number of buckets [2000000]\n* **thread** - number of threads [number of cpus]\n* **lrUpdateRate** - change the rate of updates for the learning rate [100]\n* **t** - sampling threshold [0.0001]\n* **verbose** - verbose [2]\n    \n**This is why I formatted the encoded question\/answer pairs the way I did: we can just set `minn=1` and `maxn=1` to capture similarities between the tags associated to a particular problem and how the user answered it.** ","deb2f010":"**Disclaimer: I could be wildly off with my approach here, so I welcome thoughts and suggestions. If you see a mistake or have a question, just let me know.**","fd556ed5":"**Okay, maybe too many unique `bundle_ids` to use for clustering, but we can definitely use `tags` and potentially `part`, although the it is dominated by the value `5`.**\n\n**Now, it is important for the embedding strategy I wish to test that each unique question tag is mapped to a single character. You will see why shortly. If you run `string.printable`, it will only give you 100 characters to use, so we are going to need some other characters. Let's go to the Greek alphabet first and maybe some Cyrillic\/Latin letters if we need them. If anyone has a cleaner way of generating characters, do tell.**","5bd6390a":"**Let's also look at the distribution of `part` and `bundle_id`:**","5a84c8db":"**Great, seems that the CBOW model is forming character *n*-grams for each unique tag ID and question answer just as we wanted. Now let's visualize these embeddings with t-SNE to see what sort of structure there is.**","8142bd62":"# Embeddings for Knowledge Tracing\n\n**Motivation comes from [this paper](https:\/\/arxiv.org\/pdf\/2005.12442.pdf). I want to treat the questions as words in a corpus, so that I can embed them such that similar questions are clustered together in the same way that we use GloVe or Word2Vec vectors to cluster words with similar meaning together.** \n\n**Facebook's [fastText](https:\/\/github.com\/facebookresearch\/fastText) makes use of character level information. Each word is represented as a bag-of-character *n*-gram as well as the word itself. For example, the word `queen` with *n*-gram=3 is composed of `<qu`, `que`, `uee`, `een`, and `en>`. This can allow you to capture the meaning of suffixes\/prefixes, which motivates the encoding procedure I carry out in this notebook.**\n\n**fastText can also generate embeddings for words\/questions not in our corpus by adding the character n-gram of all n-gram representations. It essentially builds a representation for OOV tokens by stitching together character level n-grams that it has seen during training.**","990ff229":"**I will also changed the `answered_correctly = -1` value to `2`. I want to include this is a category on par with the other answers, so that we can cluster questions together that students did not answer. This makes sense if there is some logic behind the lack of an answer: was the question too hard or poorly worded? I do not know if any such logic underlies these `-1` values, but I will assume (for now) there is.**"}}