{"cell_type":{"4e65a018":"code","d3e192a9":"code","5ffce71b":"code","3a64bb95":"code","4a1fa582":"code","5e9dce10":"code","209f2eaf":"code","44ec696a":"code","504d9052":"code","1253a5db":"code","1f9a6d4a":"code","c96513a9":"code","fda732d0":"code","c72a034f":"code","95ad0dd5":"code","24831cf2":"code","18f9eec9":"code","b7b61145":"code","fc4f2ee6":"code","e516e5d2":"code","6886caa9":"code","fff06981":"code","500babc4":"code","c50f86dd":"code","c917380d":"code","3da38afb":"code","5d0f990f":"code","291f77b2":"code","dc1ffb1c":"code","3ebe15d6":"code","cf0937b9":"code","54e3cf66":"code","049ed593":"code","87540352":"code","4259f14d":"code","174ade37":"code","feb5813a":"code","ed4bd8dd":"code","92459f21":"code","36901869":"code","affb883f":"code","9c2bc0d5":"code","fe31e3c8":"code","12a984d1":"code","0fe41203":"code","ae1d982d":"code","95e6b121":"code","f1a4148f":"code","c6a16fc4":"code","621dbce9":"code","13019b1c":"code","bfe11a72":"code","a222af04":"code","f0b182d5":"code","2060f40e":"code","0ad3a807":"code","271120fc":"code","097c3272":"code","aab0352b":"code","0017a39b":"code","aa82dc95":"code","a7ab3d9c":"code","e2d91daf":"code","e390458a":"code","20b12f00":"code","d3a6ae8b":"code","fa206051":"code","6a80b0f3":"code","597816ee":"code","915e7514":"code","ad5824aa":"code","3dda8533":"code","ff023d43":"code","575337f7":"code","8628c852":"code","e84ade1b":"code","7300497e":"code","fb6b27f7":"code","c84f21e1":"markdown","a998011e":"markdown","fc35d468":"markdown","a95af110":"markdown","2684c95b":"markdown","e75c8c93":"markdown","e6ffd6e4":"markdown","cfa24aa3":"markdown","936b2c5f":"markdown","b7db9d8d":"markdown","2c98b392":"markdown","7604b921":"markdown","844bf513":"markdown","604efc93":"markdown","32ef42c8":"markdown","5e109919":"markdown","2089a9bb":"markdown","5053b2d0":"markdown","6c87a8e7":"markdown"},"source":{"4e65a018":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d3e192a9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.stats import normaltest\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')","5ffce71b":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error","3a64bb95":"#create class\nclass car_price_model:\n    \"\"\"  **car_price_model** is the class for exploratory data \n        analysis and machine learning in each data car. \n        This class have 10 attributes that are given important:\n\n    - multi_categorical_plot\n\n    - distplot_multi\n\n    - boxplot_multi\n\n    - correlation_plot\n\n    - VIF\n\n    - learner_selection\n\n    - training_evaluate\n    \"\"\"\n    \n    def __init__(self, data=None, cols=None, name='price'):\n        \n        self.name = name # target\n        self.data = data # feature\n        self.cols = cols # feature columns name\n        self.listof_model = {'LinearRegression': LinearRegression(), \n                'KNeighborsRegression':KNeighborsRegressor(),\n                'RandomForestRegression': RandomForestRegressor(),\n               'GradientBoostingRegression': GradientBoostingRegressor(),\n                'XGBoostRegression': XGBRegressor(),\n                'adaboost':AdaBoostRegressor()} # list of different learner\n    \n    #Read csv file\n    def read(self, file):\n        return pd.read_csv(file)\n    \n    def multi_categorical_plot(self, data):\n    \n        \"\"\" plot a categorical feature\n        \n            data: float64 array  n_observationxn_feature\n        \n        \"\"\"\n        # Find a feature that type is object\n        string = []\n        for i in data.columns:\n            if data[i].dtypes == \"object\":\n                string.append(i)\n    \n        fig = plt.figure(figsize=(20,5))\n        fig.subplots_adjust(wspace=0.2, hspace = 0.3)\n        for i in range(1,len(string)+1):\n            ax = fig.add_subplot(2,3,i)\n            sns.countplot(x=string[i-1], data=data, ax=ax)\n            ax.set_title(f\" {string[i-1]} countplot\")\n            \n    def distplot_multi(self, data):\n        \"\"\" plot multi distplot\"\"\"\n    \n        \n        from scipy.stats import norm\n        cols = []\n        \n        #Feature that is int64 or float64 type \n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n        \n        gp = plt.figure(figsize=(15,10))\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(2,3,i)\n            sns.distplot(data[cols[i-1]], fit=norm, kde=False)\n            ax.set_title('{} max. likelihood gaussian'.format(cols[i-1]))\n            \n    def boxplot_multi(self, data):\n        \n        \"\"\" plot multi box plot\n            hue for plotting categorical data\n        \"\"\"\n    \n        cols = []\n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n    \n        gp = plt.figure(figsize=(15,10))\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(2,3,i)\n            sns.boxplot(x = cols[i-1], data=data)\n            ax.set_title('Boxplot for {}'.format(cols[i-1]))\n            \n    def correlation_plot(self, data, vrs= 'price'):\n    \n        \"\"\"\n        This function plot only a variable that are correlated with a target  \n        \n            data: array m_observation x n_feature\n            vrs:  target feature (n_observation, )\n            cols: interested features\n        \"\"\"\n        \n        cols = []\n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n                \n        feat = list(set(cols) - set([vrs]))\n    \n        fig = plt.figure(figsize=(15,10))\n        fig.subplots_adjust(wspace = 0.3, hspace = 0.25)\n        for i in range(1,len(feat)+1):\n        \n            gp = data.groupby(feat[i-1]).agg('mean').reset_index()\n        \n            if len(feat) < 3:\n                ax = fig.add_subplot(1,3,i)\n            else:\n                n = len(feat)\/\/2 + 1\n                ax = fig.add_subplot(2,n,i)\n            \n            ax.scatter(data[feat[i-1]], data[vrs], alpha=.25)\n            ax.plot(gp[feat[i-1]], gp[vrs], 'r-', label='mean',  linewidth=1.5)\n            ax.set_xlabel(feat[i-1])\n            ax.set_ylabel(vrs)\n            ax.set_title('Plotting data {0} vs {1}'.format(vrs, feat[i-1]))\n            ax.legend(loc='best')\n            \n    # Standardize data\n    def standardize(self, data):\n        data = (data - data.mean())\/data.std()\n        return data\n            \n            \n    def VIF(self, data):\n        \"\"\" \n        This function compute variance inflation factor for data that all feature are multicolinear\n        \n        if the outcome is 1, it is okay\n        if it is between 1 and 5, it shows low to average colinearity, and above 5 generally means highly \n        redundant and variable should be dropped\n        \"\"\" \n        # Apply the standardize method to each feature and save it to a new data\n        std_data = data.apply(self.standardize, axis=0)\n    \n        from statsmodels.stats.outliers_influence import variance_inflation_factor\n    \n        vif = pd.DataFrame()\n        vif['VIF_FACTOR'] = [variance_inflation_factor(std_data.values, i) for i in range(std_data.shape[1])]\n    \n        vif['feature'] = std_data.columns\n    \n        return vif\n    \n    \n    def split_data(self):\n        \"\"\"\n        This function splits data to train set and target set\n        \n        data: matrix feature n_observation x n_feature dimension\n        name: target  (n_observation, )\n        cols: interested feature\n        \n        return xtrain, xtest, ytrain, ytest\n        \"\"\"\n    \n        train = self.data[self.cols]\n        target = self.data[self.name]\n    \n        return train_test_split(train, target, random_state=42, test_size=0.2, shuffle=True)\n    \n    def spearman_pearson_correlation(self, data):\n        \n        \n        gp = plt.figure(figsize=(15,5))\n        cols = ['pearson', 'spearman']\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(1,2,i)\n            sns.heatmap(data.corr(method=cols[i-1]), annot=True)\n            ax.set_title('{} correlation'.format(cols[i-1]))\n        \n        \n        plt.show()\n    \n    \n    def learner_selection(self):\n\n        \"\"\"\n            This function compute differents score measure like cross validation,\n            r2, root mean squared error and mean absolute error.\n            listof_model: dictionary type containing different model algorithm.     \n        \"\"\" \n    \n        result = {}\n        \n        x, _, y, _ = self.split_data() # take only xtrain and ytrain\n    \n        for cm in list(self.listof_model.items()):\n        \n            name = cm[0]\n            model = cm[1]\n        \n            cvs = cross_val_score(model, x, y, cv=10).mean()\n            ypred = cross_val_predict(model, x, y, cv=10)\n            r2 = r2_score(y, ypred)\n            mse = mean_squared_error(y, ypred)\n            mae = mean_absolute_error(y, ypred)\n            rmse = np.sqrt(mse)\n        \n            result[name] = {'cross_val_score': cvs, 'rmse': rmse, 'mae': mae, 'r2': r2}\n        \n            print('{} model done !!!'.format(name))\n        \n        \n        return pd.DataFrame(result)\n    \n    \n    def training_evaluate(self, algorithm):\n        \n        \"\"\"This function train and evaluate our model to find r2, rmse and mae\"\"\"\n        \n        result = {}\n        xtrain, xtest, ytrain, ytest = self.split_data()\n        \n        learner = self.listof_model[algorithm] # learner selected in model_selection function\n        \n        model = learner.fit(xtrain, ytrain)\n        ypred = model.predict(xtest)\n        \n        r2 = learner.score(xtest, ytest)\n        rmse =  np.sqrt(mean_squared_error(ytest, ypred))\n        mae = mean_absolute_error(ytest, ypred)\n        \n        result['car price measure'] = {'r2':round(r2, 3),  'rmse':round(rmse, 3), 'mae':round(mae, 3)}\n        \n        return  pd.DataFrame(result)\n        ","4a1fa582":"car1 = '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/focus.csv'\ncar2 = '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv'\ncar3 = '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv'\ncar4 = '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv'\ncar5 = '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv'","5e9dce10":"model = car_price_model()","209f2eaf":"focus = model.read(car1)","44ec696a":"focus.head()","504d9052":"focus.info()","1253a5db":"model.multi_categorical_plot(focus)","1f9a6d4a":"sns.countplot(x = 'model', hue='fuelType', data=focus)","c96513a9":"focus.describe()","fda732d0":"model.distplot_multi(focus)","c72a034f":"model.boxplot_multi(focus) # we see well that our maximun likelihood gaussian go with our boxplot. ","95ad0dd5":"model.spearman_pearson_correlation(focus)","24831cf2":"model.correlation_plot(focus)","18f9eec9":"focus_cols = ['mileage', 'year', 'engineSize'] #take columns","b7b61145":"model.VIF(focus[focus_cols])","fc4f2ee6":"focus_model = car_price_model(data=focus, cols=focus_cols) #select best algorithm","e516e5d2":"focus_model.learner_selection()","6886caa9":"focus_model.training_evaluate('GradientBoostingRegression')","fff06981":"audi = model.read(car2)","500babc4":"audi.head()","c50f86dd":"audi.info()","c917380d":"model.multi_categorical_plot(audi)","3da38afb":"audi.describe()","5d0f990f":"model.distplot_multi(audi)","291f77b2":"model.boxplot_multi(audi)","dc1ffb1c":"model.spearman_pearson_correlation(audi)","3ebe15d6":"model.correlation_plot(audi)","cf0937b9":"audi_cols = ['year', 'mileage', 'mpg', 'engineSize', 'tax'] #there is or not necessary to take tax feature","54e3cf66":"model.VIF(audi[audi_cols]) #all vif factor of feature are acceptable","049ed593":"#select learner\naudi_model = car_price_model(data=audi, cols=audi_cols)","87540352":"audi_model.learner_selection()","4259f14d":"audi_model.training_evaluate('XGBoostRegression')","174ade37":"ford = model.read(car3)","feb5813a":"ford.head()","ed4bd8dd":"ford.info()","92459f21":"ford = ford.replace(to_replace=2060, value=2016) #some errors","36901869":"model.multi_categorical_plot(ford) ","affb883f":"ford.describe()","9c2bc0d5":"model.distplot_multi(ford)","fe31e3c8":"model.boxplot_multi(ford)","12a984d1":"model.spearman_pearson_correlation(ford)","0fe41203":"model.correlation_plot(ford)","ae1d982d":"ford_cols = ['mileage', 'year', 'tax', 'engineSize', 'mpg']","95e6b121":"model.VIF(ford[ford_cols])","f1a4148f":"ford_model = car_price_model(data=ford, cols=ford_cols)","c6a16fc4":"ford_model.learner_selection()","621dbce9":"ford_model.training_evaluate('XGBoostRegression')","13019b1c":"toyota = model.read(car4)","bfe11a72":"toyota.head()","a222af04":"toyota.info()","f0b182d5":"model.multi_categorical_plot(toyota)","2060f40e":"toyota.describe()","0ad3a807":"model.distplot_multi(toyota)","271120fc":"model.boxplot_multi(toyota)","097c3272":"model.spearman_pearson_correlation(toyota)","aab0352b":"model.correlation_plot(toyota)","0017a39b":"toyota_cols = ['engineSize','year','tax', 'mileage', 'mpg']","aa82dc95":"model.VIF(toyota[toyota_cols])","a7ab3d9c":"toyota_model = car_price_model(data=toyota, cols=toyota_cols)","e2d91daf":"toyota_model.learner_selection()","e390458a":"toyota_model.training_evaluate('XGBoostRegression')","20b12f00":"skoda = model.read(car5)","d3a6ae8b":"skoda.head()","fa206051":"skoda.info()","6a80b0f3":"model.multi_categorical_plot(skoda)","597816ee":"skoda.describe()","915e7514":"model.distplot_multi(skoda)","ad5824aa":"model.boxplot_multi(skoda)","3dda8533":"model.spearman_pearson_correlation(skoda)","ff023d43":"model.correlation_plot(skoda)","575337f7":"skoda_cols = ['year', 'engineSize', 'mileage', 'tax', 'mpg']","8628c852":"model.VIF(skoda[skoda_cols])","e84ade1b":"skoda_model = car_price_model(data=skoda, cols=skoda_cols)","7300497e":"skoda_model.learner_selection()","fb6b27f7":"skoda_model.training_evaluate('XGBoostRegression')","c84f21e1":"**We obtain** $R^2 = 96.2\\%$ **for Toyota car.**","a998011e":"**We have** $R^2 = 94.5\\%$ **for Audi car**","fc35d468":"# Skoda car price","a95af110":"## Visualization, correlation, VIF, learner selection, training and evaluation","2684c95b":"## Visualization, correlation, VIF, learner selection, training and evaluation","e75c8c93":"# Ford car price","e6ffd6e4":"### Summarize\n\nFor this five cars, we obtain de $R^2$ score:\n\n> Skoda car $92.6\\%$\n\n> Toyota car $96.2\\%$\n\n> Ford car $91.6\\%$\n\n> Audi car $94.5\\%$\n\n> Focus car $92.1\\%$\n\n**For the rest of car, you can continue to find a score. If you do not understand my model or method be free to ask your question.**","cfa24aa3":"## Visualization, correlation, VIF, learner selection, training and evaluate","936b2c5f":"# Update","b7db9d8d":"with this two correlations we can see that **price is most correlated with mileage and year**. also year well correlated with mileage. We use VIF to see how this correlation are.","2c98b392":"## Visualization, correlation, VIF, learner selection, training and evaluate","7604b921":"**We get** $R^2 = 91.6\\%$ **for Ford car**","844bf513":"# Audi car price","604efc93":"# Focus car price","32ef42c8":"**We have** $R^2 = 92.1\\%$ **for Focus car**","5e109919":"# Toyota car price","2089a9bb":"### car_price_model class explaination\n\n**car_price_model** is the class that I use to do exploratory data analysis and machine learning in each data car. This class have 10 attributes that are:\n\n- multi_categorical_plot\n\n- distplot_multi\n\n- boxplot_multi\n\n- spearman_pearson_correlation\n\n- correlation_plot\n\n- VIF\n\n- learner_selection\n\n- training_evaluate\n\nThese are the function that we are going to use in this notebook. \n\n**N.B: We are making prediction price for 5 cars (focus, audi, ford, toyota, skoda)** ","5053b2d0":"## Visualization, correlation, VIF, learner selection, training and evaluate","6c87a8e7":"**We obtain** $R^2 = 92.6\\%$ **for Skoda car.**"}}