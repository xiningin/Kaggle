{"cell_type":{"c8ea01c2":"code","92039784":"code","020924af":"code","ab400f92":"code","a0587ead":"code","713c3bef":"code","5a163fc8":"code","9164bf48":"code","08f3fd5b":"code","1a5a2f4e":"code","99c76666":"code","c6981e0d":"code","cf9576c1":"code","6d0d4660":"code","3b5e6dfa":"code","1af63bd5":"code","9ffa2923":"code","86c47d19":"code","34b60e08":"code","eedc6725":"code","755b04fa":"code","f37989af":"code","2b254904":"code","6924d46c":"code","47654a77":"code","c027782e":"code","50663cc3":"code","b2d22a25":"code","80e07ca2":"code","5898e03e":"code","eb530d9c":"code","52729358":"code","5c24fdd3":"code","5812674e":"code","761756d3":"code","4260fcc7":"code","626062d5":"code","e87581b6":"code","4a43ab82":"code","4227f21c":"code","0c3c2080":"code","727056f3":"code","8d5701f9":"code","15c8d88f":"code","fb0018e2":"code","12105160":"code","52cff5fb":"code","36330ff6":"code","74ffd33f":"code","fc03e7e1":"code","41e4658b":"code","2c38e369":"code","23d64056":"code","223e5f4d":"markdown","8200eff7":"markdown","41425320":"markdown","15f5f5ec":"markdown","a523827d":"markdown","538713e4":"markdown","85f9ffd4":"markdown","68ce49c5":"markdown","088b60e7":"markdown","ba173ac7":"markdown","62d5145d":"markdown","1cc185eb":"markdown","f45adf10":"markdown","8c6469a1":"markdown","2b412f7d":"markdown","e1a86464":"markdown","3c27758f":"markdown","a265ddba":"markdown","ed3601a8":"markdown","676d196f":"markdown","1b2e075f":"markdown","c29169a6":"markdown"},"source":{"c8ea01c2":"import os\nimport numpy as np\nimport pandas as pd\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","92039784":"! pip install openpyxl","020924af":"description = pd.read_excel('\/kaggle\/input\/hr-analytics-case-study\/data_dictionary.xlsx')\ndescription.replace(to_replace=np.nan, value=' ', inplace=True)\ndescription","ab400f92":"df = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/general_data.csv')\nprint(df.columns)\nprint(df.shape)\ndf","a0587ead":"df_1 = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/manager_survey_data.csv')\nprint(df_1.columns)\nprint(df_1.shape)\ndf_1","713c3bef":"df_2 = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/employee_survey_data.csv')\nprint(df_2.columns)\nprint(df_2.shape)\ndf_2","5a163fc8":"df =  pd.concat([df.set_index('EmployeeID'), df_1.set_index('EmployeeID'),df_2.set_index('EmployeeID')], axis=1, join='inner').reset_index()\ndf","9164bf48":"weekends = ['2015-12-25', '2015-10-02', '2015-11-11', '2015-05-01',\n            '2015-01-14', '2015-11-10', '2015-03-05', '2015-07-17',\n            '2015-01-26', '2015-11-09', '2015-09-17', '2015-01-01']","08f3fd5b":"df_in = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/in_time.csv')\ndf_in.iloc[:,1:] = df_in.iloc[:,1:].astype('datetime64[ns]')\ndf_in.rename({'Unnamed: 0': 'EmployeeID'}, axis=1, inplace=True)\ndf_in.drop(weekends, axis=1, inplace=True)\n# for col in df_in.columns[1:]:\n#     df_in[col] = pd.to_datetime(df_in[col]).dt.time\nprint(df_in.shape)\ndf_in[:2]","1a5a2f4e":"df_in_hour = pd.DataFrame()\nfor col in df_in.columns[1:]:\n    time = pd.DatetimeIndex(df_in[col])\n    df_in_hour[col] = (time.hour * 60 + time.minute)\/60\n\ndf_in_hour[:2]","99c76666":"df_out = pd.read_csv('\/kaggle\/input\/hr-analytics-case-study\/out_time.csv')\ndf_out.iloc[:,1:] = df_out.iloc[:,1:].astype('datetime64[ns]')\ndf_out.rename({'Unnamed: 0': 'EmployeeID'}, axis=1, inplace=True)\ndf_out.drop(weekends, axis=1, inplace=True)\n# for col in df_out.columns[1:]:\n#     df_out[col] = pd.to_datetime(df_out[col]).dt.time\nprint(df_out.shape)\ndf_out[:2]","c6981e0d":"df_out_hour = pd.DataFrame()\nfor col in df_out.columns[1:]:\n    time = pd.DatetimeIndex(df_out[col])\n    df_out_hour[col] = (time.hour * 60 + time.minute)\/60\n\ndf_out_hour[:2]","cf9576c1":"time_df = pd.DataFrame()\ntime_df['EmployeeID_in'] = df_in['EmployeeID']\ntime_df['EmployeeID_out'] = df_out['EmployeeID']\ntime_df['EmployeeID'] = time_df['EmployeeID_in'] - time_df['EmployeeID_out']\nnot_zero = time_df.loc[time_df['EmployeeID'] != 0]\ntime_df['EmployeeID'] = df_in['EmployeeID']\nnot_zero","6d0d4660":"time_df.drop(['EmployeeID_in','EmployeeID_out'], axis=1, inplace=True)\ntime_df[:2]","3b5e6dfa":"df_in_avg = pd.DataFrame()\ntime_df['in_avg'] = df_in_hour.iloc[:,1:].mean(axis=1)\ntime_df[:2]","1af63bd5":"df_out_avg = pd.DataFrame()\ntime_df['out_avg'] = df_out_hour.iloc[:,1:].mean(axis=1)\ntime_df[:2]","9ffa2923":"df_out_avg = pd.DataFrame()\ntime_df['avg_work_day'] = time_df['out_avg'] - time_df['in_avg']\ntime_df[:2]","86c47d19":"time_df['num_day_off'] = df_in_hour.isnull().sum(axis=1)\ntime_df[:2]","34b60e08":"df =  pd.concat([df.set_index('EmployeeID'), time_df.set_index('EmployeeID')], axis=1, join='inner').reset_index()\ndf.reset_index(drop=True, inplace=True)\ndf","eedc6725":"for el in list(df.columns):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))\n    print('')","755b04fa":"# to drop\n# 'EmployeeCount', 'Over18', 'StandardHours'\ndf.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1, inplace=True)\n\n\n","f37989af":"df.info()","2b254904":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","6924d46c":"all_nan = list(null_view[-12:].index)\nall_nan","47654a77":"indexes = df.loc[pd.isnull(df[['JobSatisfaction', 'EnvironmentSatisfaction', 'WorkLifeBalance']]).any(1), :].index.values\nprint(f'Number indexes with missing data: {len(indexes)}')\nprint(indexes)","c027782e":"df = df.drop(df.index[indexes])\ntime_df = time_df.drop(time_df.index[indexes])\ndf.reset_index(drop=True, inplace=True)\ntime_df.reset_index(drop=True, inplace=True)\nprint(df.shape)\nprint(time_df.shape)","50663cc3":"def nan_predict(df,\n                skip_features_from_prediction_where_percent_missing_data_more_than = 100,\n                include_features_as_predictors_where_perc_missing_data_less_than = 50,\n                apply_fast_predictor_where_missing_data_less_than_percent = 100,\n                use_n_rows_for_train_not_more_than = 1000000000,    #  If your dataframe is large\n                randomizedSearchCV_iter_plus_perc_missing_data = 10,\n                n_estimators_parameter_for_LightGBM = 2000,\n                target_feature = None,   # For prediction at the end\n                ): \n    \n    import random\n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    from lightgbm import LGBMClassifier\n    from lightgbm import LGBMRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n    \n    \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    \n    \n\n    global counter_all_predicted_values\n    counter_all_predicted_values = 0\n    \n    global numeric_features\n    numeric_features = []\n    \n    global best_params\n    \n    \n    PARAMS  =  {'num_leaves': [12, 50, 120, 200, 300, 400, 500],   #np.arange(200, 600, step=100),\n                'max_depth': [4, 8, 12, 16],\n                'learning_rate': [0.001, 0.01, 0.1],\n                'n_estimators': [n_estimators_parameter_for_LightGBM],\n                'subsample': [0.1, 0.3, 0.5],\n                'feature_fraction': [0.1, 0.3, 0.5],\n                'bagging_fraction': [0.1, 0.3, 0.5],\n                'bagging_seed': np.arange(1, 3, step=1),\n                'lambda_l1': [0.2],\n                'lambda_l2': [0.1],\n                'min_child_samples': np.arange(2, 6, step=2),\n                'min_split_gain': [0.0001, 0.001]\n               }\n    \n    \n    CV = ShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n    \n    \n    \n\n    def NaN_info(df):\n        global null_view\n        try:\n            null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            null_view = pd.DataFrame(null_view, columns=['NANs'])\n            null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n            null_view[['TYPE']] = df.dtypes\n        except:\n            return null_view\n        return null_view\n    \n    \n    def numeric_features(df):\n        num_features = [feature for feature in df.columns if df[feature].dtype in ['int64', 'float64']]\n        return num_features\n    \n    \n    def integer_features(df):\n        global int_features\n        int_features = [feature for feature in df.columns if df[feature].dtype in ['int64']]\n        return int_features\n\n\n    def encoding(work_predictors, df):\n        feature_power = 0.5          # Skew handling\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, 'NoNoNo', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n            else:\n                df[j] = df[j]**feature_power\n        return df, work_predictors\n\n\n    def hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg, scoring):\n        global best_params\n        global pred_test_lgb\n\n        lgbm = alg(random_state = 0)\n        lgbm_randomized = RandomizedSearchCV(estimator=lgbm, \n                                            param_distributions=PARAMS, \n                                            n_iter=n_iter_for_RandomizedSearchCV, \n                                            scoring=scoring, \n                                            cv=CV, \n                                            verbose=0,\n                                            n_jobs = -1)\n\n        lgbm_randomized.fit(X_train, y_train)\n        \n        best_params = lgbm_randomized.best_params_\n        pred_test_lgb = lgbm_randomized.predict(X_test)\n        return best_params, pred_test_lgb\n\n    \n    def predict_regressor(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMRegressor(**best_params, n_jobs=-1, random_state=0)\n        lgbm = lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n\n\n    def predict_classifier(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMClassifier(**best_params, n_jobs=-1, random_state=0)\n        lgbm = lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n    \n    \n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n    \n    \n    \n    # Go)\n\n    plt.figure(figsize=(20, 5))\n    sns.heatmap(df.isnull(), cbar=False)\n    \n    \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    \n    all_features = list(df.columns)\n    df_indeces = list(df.index)\n    df.reset_index(drop=True, inplace=True)\n    \n    integer_features(df)\n\n    delete_miss_features = list(\n        (null_view.loc[null_view['PERCENT'] > skip_features_from_prediction_where_percent_missing_data_more_than]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_from_prediction_where_percent_missing_data_more_than}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n        \n    \n    if target_feature in all_miss_features:  # moving target_feature to end of the prediction\n        all_miss_features.append(all_miss_features.pop(all_miss_features.index(target_feature)))\n        \n    \n    for el in all_miss_features:\n        print('\\n\\n\\n\\n')\n        \n        # select features as predictors\n        NaN_info(df)\n        lot_of_miss_features = list(\n            (null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_perc_missing_data_less_than]).index)\n        now_predictors = list(set(all_features)-set(lot_of_miss_features))\n        work_predictors = list(set(now_predictors) - set([el]))\n\n        \n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        \n        # data without NaN rows (X data for train, evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        if len(work_indeces) > use_n_rows_for_train_not_more_than:\n            randomlist = random.sample(range(0, len(work_indeces)), use_n_rows_for_train_not_more_than)\n            work_indeces = [work_indeces[i] for i in randomlist]\n        \n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        \n        # Info\n        feature_type = df[el].dtypes\n        percent_missing_data = null_view['PERCENT'][el]\n        print(f'Feature: {el},   type: {feature_type},   missing values: {percent_missing_data}%\\n')    \n        print(f'Shape for train dataframe: {(X.shape)}')\n        print(f'Unused features as predictors, because missing data more than {include_features_as_predictors_where_perc_missing_data_less_than}% :')\n        print(lot_of_miss_features)\n        print('')\n        \n        \n        # PREDICTIONS\n        if percent_missing_data < apply_fast_predictor_where_missing_data_less_than_percent:\n            \n            # FAST Predictions without tuning hyperparameters\n            \n            print('FAST prediction without tuning hyperparameters\\n')\n            best_params = {}\n            if feature_type == 'object' or feature_type == 'bool':\n                print('FAST CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                lgbm = LGBMClassifier(n_jobs=-1, random_state=0)\n                lgbm = lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                accuracy = accuracy_score(y_test, pred_test_lgb_FAST)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb_FAST[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb_FAST, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('FAST REGRESSOR:')\n                \n                lgbm = LGBMRegressor(n_jobs=-1, random_state=0)\n                lgbm = lgbm.fit(X_train, np.log1p(y_train))\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                pred_test_lgb_FAST = np.expm1(pred_test_lgb_FAST)\n                MAE = mean_absolute_error(y_test,pred_test_lgb_FAST)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb_FAST[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n                \n                  \n        else:\n            \n            # ADVANCED Predictions with tuning hyperparameters\n            \n            n_iter_for_RandomizedSearchCV = int(randomizedSearchCV_iter_plus_perc_missing_data + percent_missing_data * 1)\n            print(f'Iteration for RandomizedSearchCV: {n_iter_for_RandomizedSearchCV}\\n')\n            \n            if feature_type == 'object' or feature_type == 'bool':\n                print('ADVANCED CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMClassifier, scoring='f1_weighted')\n                accuracy = accuracy_score(y_test, pred_test_lgb)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('ADVANCED REGRESSOR:')\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMRegressor, scoring='neg_mean_squared_error')\n                MAE = mean_absolute_error(y_test,pred_test_lgb)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n        \n        plt.figure(figsize=(20, 5))\n        sns.heatmap(df.isnull(), cbar=False)\n\n        \n    for feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')\n        \n    df.index = df_indeces\n\n    print('\\n\\n\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_from_prediction_where_percent_missing_data_more_than}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    print(f'{counter_all_predicted_values} values have been predicted and replaced')\n    print('\\n')\n    \n    return df","b2d22a25":"nan_predict(df)","80e07ca2":"integer  = ['TotalWorkingYears', 'EnvironmentSatisfaction', 'JobSatisfaction', \n            'WorkLifeBalance', 'NumCompaniesWorked']\nfor col in integer:\n    df[[col]] = df[[col]].astype('int64')\n","5898e03e":"for el in list(df.columns):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))\n    print('')","eb530d9c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nsns.set(font_scale=1.5)\nsns.set_style(style='white') \n\ntime_df[['Attrition']] = df[['Attrition']]\n\nfor el in time_df.columns:\n    plot_data = time_df[['Attrition', el]]\n    try:\n        g = sns.pairplot(plot_data, hue='Attrition', palette='Set1', height=10, aspect=2)\n        \n        handles = g._legend_data.values()\n        labels = g._legend_data.keys()\n        g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)\n    except:\n        pass","52729358":"def categorical_features(df):\n    global cat_features\n    cat_features = [feature for feature in df.columns if df[feature].dtype in ['object']]\n    return cat_features\n\ncategorical_features(df)","5c24fdd3":"# sns.set_palette('twilight_shifted')\nfor el in cat_features:\n    plot_data = df[['Attrition', el]]\n    try:\n        plt.figure(figsize=(20,10))\n        sns.countplot(x=plot_data[el], hue='Attrition', data=plot_data, palette='winter_r')\n    except:\n        pass","5812674e":"print('Impact of Performance Rating')\nprint('\\n\\n')\n\nprint('PerformanceRating == 4 (The Highest)')\nprint('')\na = df.loc[(df['Attrition'] == 'Yes') & (df['PerformanceRating'] == 4)]\nprint(f'Number of Dismissed: \\t\\t\\t{len(a)}')\nprint(f'Average Age: \\t\\t\\t\\t{a.Age.mean()}')\nprint(f'Average Monthly Income: \\t\\t{a.MonthlyIncome.mean()}')\nprint(f'Average Years at Company: \\t\\t{a.YearsAtCompany.mean()}')\nprint(f'Average Years with Current Manager:\\t{a.YearsWithCurrManager.mean()}')\nprint('\\n\\n')\n\n\nprint('PerformanceRating == 3 (The lowest)')\nprint('')\nb = df.loc[(df['Attrition'] == 'Yes') & (df['PerformanceRating'] == 3)]\nprint(f'Number of Dismissed: \\t\\t\\t{len(b)}')\nprint(f'Average Age: \\t\\t\\t\\t{b.Age.mean()}')\nprint(f'Average Monthly Income: \\t\\t{b.MonthlyIncome.mean()}')\nprint(f'Average Years at Company: \\t\\t{b.YearsAtCompany.mean()}')\nprint(f'Average Years with Current Manager:\\t{b.YearsWithCurrManager.mean()}')","761756d3":"print('Dismissals with Performance Rating and overtime (avg_work_day >= 10)')\nprint('\\n\\n')\n\nprint('PerformanceRating == 4 (The Highest)')\nprint('')\na = df.loc[(df['Attrition'] == 'Yes') & (df['avg_work_day'] >= 10) & (df['PerformanceRating'] == 4)]\nprint(f'Number of Dismissed: \\t\\t\\t{len(a)}')\nprint(f'Average Age: \\t\\t\\t\\t{a.Age.mean()}')\nprint(f'Average Monthly Income: \\t\\t{a.MonthlyIncome.mean()}')\nprint(f'Average Years at Company: \\t\\t{a.YearsAtCompany.mean()}')\nprint(f'Average Years with Current Manager:\\t{a.YearsWithCurrManager.mean()}')\nprint('\\n\\n')\n\n\nprint('PerformanceRating == 3 (The lowest)')\nprint('')\nb = df.loc[(df['Attrition'] == 'Yes') & (df['avg_work_day'] >= 10) & (df['PerformanceRating'] == 3)]\nprint(f'Number of Dismissed: \\t\\t\\t{len(b)}')\nprint(f'Average Age: \\t\\t\\t\\t{b.Age.mean()}')\nprint(f'Average Monthly Income: \\t\\t{b.MonthlyIncome.mean()}')\nprint(f'Average Years at Company: \\t\\t{b.YearsAtCompany.mean()}')\nprint(f'Average Years with Current Manager:\\t{b.YearsWithCurrManager.mean()}')","4260fcc7":"categorical_features(df)","626062d5":"for el in categorical_features(df):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))\n    print('')","e87581b6":"change = {\n    'No': 0,\n    'Yes': 1\n}\n\ndf['Attrition'] = df['Attrition'].map(change)\ndf['Attrition'] = df['Attrition'].astype('int64')\ndf['Attrition'].unique()","4a43ab82":"change = {\n    'Non-Travel': 0,\n    'Travel_Rarely': 1,\n    'Travel_Frequently': 2\n}\n\ndf['BusinessTravel'] = df['BusinessTravel'].map(change)\ndf['BusinessTravel'] = df['BusinessTravel'].astype('int64')\ndf['BusinessTravel'].unique()","4227f21c":"# change = {\n#     'Research & Development': 0,\n#     'Sales': 1,\n#     'Human Resources': 2\n# }\n\n# df['Department'] = df['Department'].map(change)\n# df['Department'] = df['Department'].astype('int64')\n# df['Department'].unique()\n\n\ndum_df = pd.get_dummies(df, columns=['Department'])\ndf =pd.concat([dum_df])\ndf[:2]","0c3c2080":"change = {\n    'Other': 0,\n    'Life Sciences': 1,\n    'Medical': 2,\n    'Marketing': 3,\n    'Technical Degree': 4,\n    'Human Resources': 5,\n}\n\ndf['EducationField'] = df['EducationField'].map(change)\ndf['EducationField'] = df['EducationField'].astype('int64')\ndf['EducationField'].unique()","727056f3":"# change = {\n#     'Sales Executive': 0,\n#     'Research Scientist': 1,\n#     'Laboratory Technician': 2,\n#     'Manufacturing Director': 3,\n#     'Healthcare Representative': 4,\n#     'Manager': 5,\n#     'Sales Representative': 6,\n#     'Research Director': 7,\n#     'Human Resources': 8,\n# }\n\n\n# df['JobRole'] = df['JobRole'].map(change)\n# df['JobRole'] = df['JobRole'].astype('int64')\n# df['JobRole'].unique()\n\n\ndum_df = pd.get_dummies(df, columns=['JobRole'])\ndf =pd.concat([dum_df])\ndf[:2]","8d5701f9":"change = {\n    'Male': 0,\n    'Female': 1,\n}\n\ndf['Gender'] = df['Gender'].map(change)\ndf['Gender'] = df['Gender'].astype('int64')\ndf['Gender'].unique()","15c8d88f":"change = {\n    'Single': 0,\n    'Divorced': 1,\n    'Married': 2\n}\n\ndf['MaritalStatus'] = df['MaritalStatus'].map(change)\ndf['MaritalStatus'] = df['MaritalStatus'].astype('int64')\ndf['MaritalStatus'].unique()","fb0018e2":"desc = pd.DataFrame()\na = list(df.columns)\nfor el in a:\n    uniq = df[el].unique()\n    desc.loc[el, \"values\"] = str(uniq)\n\ndesc","12105160":"general_dism = df.loc[df['Attrition'] == 1]\nundesirable_dism = df.loc[(df['Attrition'] == 1) & (df['JobInvolvement'] >= 3) & (df['PerformanceRating'] == 4)]\n\nfor col in df.columns:\n    plt.figure(figsize=(10, 5))\n    plt.title(col)\n    plt.scatter(general_dism.index, general_dism[col], label=\"general dismissed\", color='blue')\n    plt.scatter(undesirable_dism.index, undesirable_dism[col], label=\"undesirable dismissed\", color='red')\n    plt.legend()\n    plt.show()","52cff5fb":"df = df[:1600]","36330ff6":"still_working = df.loc[df['Attrition'] == 0]\ngeneral_dism = df.loc[df['Attrition'] == 1]\nundesirable_dism = df.loc[(df['Attrition'] == 1) & (df['JobInvolvement'] >= 3) & (df['PerformanceRating'] == 4)]\n\n\nprint('                                                company         still working       general dismissed   undesirable dismissed\\n')\nprint(f'Number                                           {df.shape[0]}                 {still_working.shape[0]}                  {general_dism.shape[0]}                   {undesirable_dism.shape[0]}\\n')\n\nfor col in df.columns:\n    comp = round(df[col].mean(), 2)\n    still = round(still_working[col].mean(), 2)\n    gen = round(general_dism[col].mean(), 2)\n    undes = round(undesirable_dism[col].mean(),2)\n    print(\"{:35s} {:20.2f} {:20.2f} {:20.2f} {:20.2f} \".format(col, comp, still, gen, undes))","74ffd33f":"import shap\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\n\nfeatures = [\n    # EDA for Managers\n    'PerformanceRating', 'JobSatisfaction', 'JobInvolvement',\n    'EnvironmentSatisfaction', 'Attrition', 'YearsAtCompany',\n    \n    # EDA for employees\n    'MonthlyIncome', 'StockOptionLevel', 'PercentSalaryHike', 'YearsWithCurrManager',\n    'Education', 'WorkLifeBalance', 'MaritalStatus', 'JobLevel', 'DistanceFromHome',\n    'TotalWorkingYears', 'TrainingTimesLastYear', 'BusinessTravel',\n    'EducationField', 'NumCompaniesWorked', 'YearsSinceLastPromotion',\n    'avg_work_day', 'num_day_off',\n\n    # EDA for fun\n    'in_avg', 'out_avg', 'Age', 'Gender', 'EmployeeID'\n    ]\n\n\nfor col in features[:]:\n    df_research = df.copy()\n\n    target = [col]\n    predictors = list(set(list(df_research.columns)) - set(target))\n\n    X = df_research[predictors]\n    y = df_research[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n    print(f'{target}')\n    model = LGBMRegressor(random_state=0).fit(X_train, y_train)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n    \n\n    plt.title(col)\n    plt.gcf().subplots_adjust()\n    shap.summary_plot(shap_values, X_test, max_display=X.shape[1], show=True, plot_size=(10, 12))\n    plt.savefig(\"shap_\"+col+\"_.png\")\n    plt.close()","fc03e7e1":"print(df['JobInvolvement'].unique())\nprint(df['PerformanceRating'].unique())","41e4658b":"undesirable = df.loc[(df['JobInvolvement'] >= 3) & (df['PerformanceRating'] == 4)]\nundesirable","2c38e369":"df_research = undesirable.copy()\n\ntarget = ['Attrition']\npredictors = list(set(list(df_research.columns)) - set(target))\n\nX = df_research[predictors]\ny = df_research[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nprint(f'{target}')\nmodel = LGBMRegressor(random_state=0).fit(X_train, y_train)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n\nplt.title('Attrition')\nplt.gcf().subplots_adjust()\n\nshap.summary_plot(shap_values, X_test, max_display=X.shape[1], show=True, plot_size=(10, 12))\n\n\n\n\nplt.savefig(\"shap_\"+col+\"_.png\")\nplt.close()","23d64056":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp\nfrom lightgbm import LGBMRegressor\n\n\ndf_permutation_1 = df.copy()\n\nfeatures = [\n    'PerformanceRating', 'JobSatisfaction', 'JobInvolvement',\n    'EnvironmentSatisfaction', 'YearsAtCompany',\n    \n    'MonthlyIncome', 'StockOptionLevel', 'PercentSalaryHike', 'YearsWithCurrManager',\n    'Education', 'WorkLifeBalance', 'MaritalStatus', 'JobLevel', 'DistanceFromHome',\n    'TotalWorkingYears', 'TrainingTimesLastYear', 'BusinessTravel',\n    'EducationField', 'NumCompaniesWorked', 'YearsSinceLastPromotion',\n    'avg_work_day', 'num_day_off',\n\n    'in_avg', 'out_avg', 'Age', 'Gender', 'EmployeeID'\n    ]\n\ntarget = ['Attrition']\npredictors = list(set(list(df_permutation_1.columns)) - set(target))\n\nX = df_permutation_1[predictors]\ny = df_permutation_1[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n\nmodel = LGBMRegressor(random_state=0).fit(X_train, y_train)\n\nfor feature in features:\n    pdp_dist = pdp.pdp_isolate(model=model,\n                               dataset=X_test,\n                               model_features=X_test.columns, \n                               feature=feature)\n\n    pdp.pdp_plot(pdp_dist, feature)\n    plt.title('Attrition')  #\n    plt.savefig(\"pdp_\"+feature+\"_.png\", dpi=100)\n    plt.show()\n    plt.close()  #","223e5f4d":"# Drop duplicate data => target leakage","8200eff7":"# Data cleaning","41425320":"# Statistic","15f5f5ec":"# Undesirable dismissions","a523827d":"# Concatenation main DF and Social Metrics DF","538713e4":"## EDA for Managers\n'PerformanceRating', 'JobSatisfaction', 'JobInvolvement',     \n'EnvironmentSatisfaction', 'Attrition'(Dismissals), 'YearsAtCompany',     \n\n## EDA for employees\n'MonthlyIncome', 'StockOptionLevel', 'PercentSalaryHike', 'YearsWithCurrManager',    \n'Education', 'WorkLifeBalance', 'MaritalStatus',  'JobLevel', 'DistanceFromHome',     \n'TotalWorkingYears', 'TrainingTimesLastYear', 'BusinessTravel',    \n'EducationField', 'NumCompaniesWorked',  'YearsSinceLastPromotion',    \n'avg_work_day', 'num_day_off', \n\n\n## EDA for fun))   \n'in_avg', 'out_avg', 'Age', 'Gender', 'EmployeeID'","85f9ffd4":"# Description of features","68ce49c5":"# Research of DateTime frames and data extraction","088b60e7":"# Encoding","ba173ac7":"![%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202021-03-04%20%D0%B2%2012.09.13.png](attachment:%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202021-03-04%20%D0%B2%2012.09.13.png)","62d5145d":"# NaN ML imputation","1cc185eb":"# Create DF for time parameters (later concat)","f45adf10":"# Concatenation main DF and time parameters DF","8c6469a1":"### !!! Let's chek inexes from data frames with time","2b412f7d":"# NaN cleaning","e1a86464":"# ML Research","3c27758f":"# Create main data frame","a265ddba":"# HR ANALYTICS ","ed3601a8":"## Problem Statement   \nA large company named \u0410\u0412\u0421, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons\n1. The former employees\u2019 projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n1. A sizeable department has to be maintained, for the purposes of recruiting new talent\n1. More often than not, the new employees have to be trained for the job and\/or given time to acclimatise themselves to the company\n","676d196f":"### Drop useless columns","1b2e075f":"# To int","c29169a6":"# Deep understanding with PDP Plots"}}