{"cell_type":{"28350870":"code","3bbc717a":"code","11f3e106":"code","787f05d1":"code","bfd2faaa":"code","b324f359":"code","0f6332d8":"code","2dbbd823":"code","de9eb313":"code","681e0197":"code","e0df0392":"code","581e4806":"code","432af645":"code","1ec236df":"code","9763699b":"code","ddd49879":"code","6cfa3166":"code","e057705a":"code","408c22b3":"code","f4d47b3d":"code","508ce287":"code","b7933e39":"code","de3d1bd7":"code","68fe9032":"code","6994b7d9":"code","ba6cd1aa":"code","cd08c13f":"code","45778c08":"code","44f37e8b":"code","237a84cd":"code","7bb9dce6":"code","b3aba2be":"code","2425fd6d":"code","3c545bab":"code","a744a9ed":"code","afa669cc":"code","56e46537":"code","8c7dabb2":"code","d5e9ac40":"code","1965e582":"code","2167ae1c":"code","50be7f33":"code","9f633d77":"code","afe03c81":"code","f0cba5c3":"code","73b8cd87":"markdown","e928dd23":"markdown","c38598ef":"markdown","0a03a84e":"markdown","2baac3b3":"markdown","a366c1ba":"markdown","c351e4cf":"markdown","45bd7bed":"markdown","a708aa07":"markdown","af509b8e":"markdown","28d69810":"markdown","0e17df8f":"markdown","cb5c1424":"markdown","a9c1e24c":"markdown","3227d3bc":"markdown","6af87fc4":"markdown","7c873315":"markdown","32f8a68f":"markdown","a1b660ba":"markdown","b053ea51":"markdown","56bb8593":"markdown","9965f87a":"markdown","5af40fbc":"markdown","dc3bcfc9":"markdown","9ec5f014":"markdown","d42150e3":"markdown","7f9479e2":"markdown","98833f8a":"markdown","c89e9acf":"markdown","97bb289b":"markdown","b833464d":"markdown","90b89c09":"markdown","bf34d0db":"markdown","cf15ee6c":"markdown","7946947a":"markdown","35ed7088":"markdown","63f99bd6":"markdown","2f939d13":"markdown","498d070b":"markdown","b6cec676":"markdown","e3f87325":"markdown","8a0d8673":"markdown","74e0a9e9":"markdown","fa372711":"markdown","e8fec032":"markdown","551c5151":"markdown","24e3b811":"markdown","084728b2":"markdown","d1dbd32d":"markdown","2e0a8901":"markdown","0e69a367":"markdown","6e0d4135":"markdown","d9c14a19":"markdown","0a0fa3bf":"markdown","42b95748":"markdown","c6e22a88":"markdown","4d00d5fc":"markdown","f6e0f348":"markdown","91444379":"markdown","7ef34e6d":"markdown","26985145":"markdown","728f01c2":"markdown","014962c3":"markdown","a966c80e":"markdown","b92ab241":"markdown","9c7f59ec":"markdown","4f4cdc37":"markdown","08c0b9b0":"markdown","0ad7ed4f":"markdown","f6346f82":"markdown","27b515f3":"markdown","0ce831cf":"markdown","be76973e":"markdown","2240ff2b":"markdown","afc83481":"markdown","9bd19f0f":"markdown","dc3bf289":"markdown","6d8644ec":"markdown","f62f0895":"markdown","9122c04a":"markdown","329036d0":"markdown","bd6605e8":"markdown","545f4101":"markdown","dba4e9ec":"markdown","72a77bf8":"markdown","51baae31":"markdown","0d66d84a":"markdown","09946027":"markdown","bfd1a098":"markdown"},"source":{"28350870":"# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","3bbc717a":"plt.figure(figsize=(6, 4))\nxx = np.linspace(0,1,50)\nplt.plot(xx, [2 * x * (1-x) for x in xx], label='gini')\nplt.plot(xx, [4 * x * (1-x) for x in xx], label='2*gini')\nplt.plot(xx, [-x * np.log2(x) - (1-x) * np.log2(1 - x)  for x in xx], label='entropy')\nplt.plot(xx, [1 - max(x, 1-x) for x in xx], label='missclass')\nplt.plot(xx, [2 - 2 * max(x, 1-x) for x in xx], label='2*missclass')\nplt.xlabel('p+')\nplt.ylabel('criterion')\nplt.title('Criteria of quality as a function of p+ (binary classification)')\nplt.legend();","11f3e106":"# first class\nnp.random.seed(17)\ntrain_data = np.random.normal(size=(100, 2))\ntrain_labels = np.zeros(100)\n\n# adding second class\ntrain_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\ntrain_labels = np.r_[train_labels, np.ones(100)]","787f05d1":"plt.figure(figsize=(10,8))\nplt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \ncmap='autumn', edgecolors='black', linewidth=1.5);\nplt.plot(range(-2,5), range(4,-3,-1));","bfd2faaa":"from sklearn.tree import DecisionTreeClassifier\n\n# Let\u2019s write an auxiliary function that will return grid for further visualization.\ndef get_grid(data):\n    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\nclf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, \n                                  random_state=17)\n\n# training the tree\nclf_tree.fit(train_data, train_labels)\n\n# some code to depict separating surface\nxx, yy = get_grid(train_data)\npredicted = clf_tree.predict(np.c_[xx.ravel(), \n                                   yy.ravel()]).reshape(xx.shape)\nplt.pcolormesh(xx, yy, predicted, cmap='autumn')\nplt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \ncmap='autumn', edgecolors='black', linewidth=1.5);","b324f359":"# use .dot format to visualize a tree\nfrom ipywidgets import Image\nfrom io import StringIO\nimport pydotplus #pip install pydotplus\nfrom sklearn.tree import export_graphviz\n\ndot_data = StringIO()\nexport_graphviz(clf_tree, feature_names=['x1', 'x2'], \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(value=graph.create_png())","0f6332d8":"data = pd.DataFrame({'Age': [17,64,18,20,38,49,55,25,29,31,33], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata","2dbbd823":"data.sort_values('Age')","de9eb313":"age_tree = DecisionTreeClassifier(random_state=17)\nage_tree.fit(data['Age'].values.reshape(-1, 1), data['Loan Default'].values)\n\ndot_data = StringIO()\nexport_graphviz(age_tree, feature_names=['Age'], \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","681e0197":"data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], \n                      'Salary': [25,80,22,36,37,59,74,70,33,102,88], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2","e0df0392":"data2.sort_values('Age')","581e4806":"age_sal_tree = DecisionTreeClassifier(random_state=17)\nage_sal_tree.fit(data2[['Age', 'Salary']].values, data2['Loan Default'].values);","432af645":"dot_data = StringIO()\nexport_graphviz(age_sal_tree, feature_names=['Age', 'Salary'], \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","1ec236df":"n_train = 150        \nn_test = 1000       \nnoise = 0.1\n\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\ndef generate(n_samples, noise):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X).ravel()\n    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \\\n    np.random.normal(0.0, noise, n_samples)\n    X = X.reshape((n_samples, 1))\n    return X, y\n\nX_train, y_train = generate(n_samples=n_train, noise=noise)\nX_test, y_test = generate(n_samples=n_test, noise=noise)\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nreg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)\n\nreg_tree.fit(X_train, y_train)\nreg_tree_pred = reg_tree.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=20)\nplt.plot(X_test, reg_tree_pred, \"g\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Decision tree regressor, MSE = %.2f\" % np.sum((y_test - reg_tree_pred) ** 2))\nplt.show()","9763699b":"df = pd.read_csv('..\/input\/telecom_churn.csv')\n\ndf['International plan'] = pd.factorize(df['International plan'])[0]\ndf['Voice mail plan'] = pd.factorize(df['Voice mail plan'])[0]\ndf['Churn'] = df['Churn'].astype('int')\nstates = df['State']\ny = df['Churn']\ndf.drop(['State', 'Churn'], axis=1, inplace=True)","ddd49879":"df.head()","6cfa3166":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_holdout, y_train, y_holdout = train_test_split(df.values, y, test_size=0.3,\nrandom_state=17)\n\ntree = DecisionTreeClassifier(max_depth=5, random_state=17)\nknn = KNeighborsClassifier(n_neighbors=10)\n\ntree.fit(X_train, y_train)\nknn.fit(X_train, y_train)","e057705a":"from sklearn.metrics import accuracy_score\n\ntree_pred = tree.predict(X_holdout)\naccuracy_score(y_holdout, tree_pred) # 0.94","408c22b3":"knn_pred = knn.predict(X_holdout)\naccuracy_score(y_holdout, knn_pred) # 0.88","f4d47b3d":"from sklearn.model_selection import GridSearchCV, cross_val_score\n\ntree_params = {'max_depth': range(1,11),\n               'max_features': range(4,19)}\n\ntree_grid = GridSearchCV(tree, tree_params,\ncv=5, n_jobs=-1,\nverbose=True)\n\ntree_grid.fit(X_train, y_train)\n","508ce287":"tree_grid.best_params_#{'max_depth': 6, 'max_features': 17}","b7933e39":"tree_grid.best_score_ #0.94256322331761677","de3d1bd7":"accuracy_score(y_holdout, tree_grid.predict(X_holdout)) #0.94599999999999995","68fe9032":"dot_data = StringIO()\nexport_graphviz(tree_grid.best_estimator_, feature_names=df.columns, \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","6994b7d9":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n\nknn_params = {'knn__n_neighbors': range(1, 10)}\n\nknn_grid = GridSearchCV(knn_pipe, knn_params,\ncv=5, n_jobs=-1,\nverbose=True)\n\nknn_grid.fit(X_train, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","ba6cd1aa":"accuracy_score(y_holdout, knn_grid.predict(X_holdout))","cd08c13f":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100, n_jobs=-1, \n                                random_state=17)\nprint(np.mean(cross_val_score(forest, X_train, y_train, cv=5))) # 0.949","45778c08":"forest_params = {'max_depth': range(1,11),\n'max_features': range(4,19)}\n\nforest_grid = GridSearchCV(forest, forest_params,\n                        cv=5, n_jobs=-1, verbose=True)\n\nforest_grid.fit(X_train, y_train)\n\nforest_grid.best_params_, forest_grid.best_score_ # ({'max_depth': 9, 'max_features': 6}, 0.951)","44f37e8b":"accuracy_score(y_holdout, forest_grid.predict(X_holdout)) # 0.953","237a84cd":"def form_linearly_separable_data(n=500, x1_min=0, x1_max=30, \n                                 x2_min=0, x2_max=30):\n    data, target = [], []\n    for i in range(n):\n        x1 = np.random.randint(x1_min, x1_max)\n        x2 = np.random.randint(x2_min, x2_max)\n        if np.abs(x1 - x2) > 0.5:\n            data.append([x1, x2])\n            target.append(np.sign(x1 - x2))\n    return np.array(data), np.array(target)\n\nX, y = form_linearly_separable_data()\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='black');","7bb9dce6":"tree = DecisionTreeClassifier(random_state=17).fit(X, y)\n\nxx, yy = get_grid(X)\npredicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\nplt.pcolormesh(xx, yy, predicted, cmap='autumn')\nplt.scatter(X[:, 0], X[:, 1], c=y, s=100, \ncmap='autumn', edgecolors='black', linewidth=1.5)\nplt.title('Easy task. Decision tree compexifies everything');","b3aba2be":"dot_data = StringIO()\nexport_graphviz(tree, feature_names=['x1', 'x2'], \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","2425fd6d":"knn = KNeighborsClassifier(n_neighbors=1).fit(X, y)\n\nxx, yy = get_grid(X)\npredicted = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\nplt.pcolormesh(xx, yy, predicted, cmap='autumn')\nplt.scatter(X[:, 0], X[:, 1], c=y, s=100, \ncmap='autumn', edgecolors='black', linewidth=1.5);\nplt.title('Easy task, kNN. Not bad');","3c545bab":"from sklearn.datasets import load_digits\n\ndata = load_digits()\nX, y = data.data, data.target\n\nX[0,:].reshape([8,8])","a744a9ed":"f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,6))\nfor i in range(4):\n    axes[i].imshow(X[i,:].reshape([8,8]), cmap='Greys');","afa669cc":"X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3,\nrandom_state=17)","56e46537":"tree = DecisionTreeClassifier(max_depth=5, random_state=17)\nknn = KNeighborsClassifier(n_neighbors=10)\n\ntree.fit(X_train, y_train)\nknn.fit(X_train, y_train)","8c7dabb2":"tree_pred = tree.predict(X_holdout)\nknn_pred = knn.predict(X_holdout)\naccuracy_score(y_holdout, knn_pred), accuracy_score(y_holdout, tree_pred) # (0.97, 0.666)","d5e9ac40":"tree_params = {'max_depth': [1, 2, 3, 5, 10, 20, 25, 30, 40, 50, 64],\n'max_features': [1, 2, 3, 5, 10, 20 ,30, 50, 64]}\n\ntree_grid = GridSearchCV(tree, tree_params,\ncv=5, n_jobs=-1,\nverbose=True)\n\ntree_grid.fit(X_train, y_train)","1965e582":"tree_grid.best_params_, tree_grid.best_score_ # ({'max_depth': 20, 'max_features': 64}, 0.844)","2167ae1c":"np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=1), X_train, y_train, cv=5)) # 0.987","50be7f33":"np.mean(cross_val_score(RandomForestClassifier(random_state=17), X_train, y_train, cv=5)) # 0.935","9f633d77":"def form_noisy_data(n_obj=1000, n_feat=100, random_seed=17):\n    np.seed = random_seed\n    y = np.random.choice([-1, 1], size=n_obj)\n\n    # first feature is proportional to target\n    x1 = 0.3 * y\n\n    # other features are noise\n\n    x_other = np.random.random(size=[n_obj, n_feat - 1])\n\n    return np.hstack([x1.reshape([n_obj, 1]), x_other]), y\n\nX, y = form_noisy_data()","afe03c81":"X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3,\nrandom_state=17)\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores, holdout_scores = [], []\nn_neighb = [1, 2, 3, 5] + list(range(50, 550, 50))\n\nfor k in n_neighb:\n\n    knn = KNeighborsClassifier(n_neighbors=k)\n    cv_scores.append(np.mean(cross_val_score(knn, X_train, y_train, cv=5)))\n    knn.fit(X_train, y_train)\n    holdout_scores.append(accuracy_score(y_holdout, knn.predict(X_holdout)))\n\nplt.plot(n_neighb, cv_scores, label='CV')\nplt.plot(n_neighb, holdout_scores, label='holdout')\nplt.title('Easy task. kNN fails')\nplt.legend();","f0cba5c3":"tree = DecisionTreeClassifier(random_state=17, max_depth=1)\ntree_cv_score = np.mean(cross_val_score(tree, X_train, y_train, cv=5))\ntree.fit(X_train, y_train)\ntree_holdout_score = accuracy_score(y_holdout, tree.predict(X_holdout))\nprint('Decision tree. CV: {}, holdout: {}'.format(tree_cv_score, tree_holdout_score))","73b8cd87":"Let\u2019s train a random forest on the same dataset, it works better than k-NN on the majority of datasets. But we here have an exception. \n\n------------\n\nEntrenemos un bosque aleatorio en el mismo conjunto de datos, funciona mejor que k-NN en la mayor\u00eda de los conjuntos de datos. Pero aqu\u00ed tenemos una excepci\u00f3n.","e928dd23":"However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the $30 \\times 30$ squares that frame the training set.\n\nSin embargo, el borde que construye el \u00e1rbol de decisi\u00f3n es demasiado complicado; Adem\u00e1s el \u00e1rbol en s\u00ed es muy profundo. Adem\u00e1s, imagine qu\u00e9 tan mal se generalizar\u00e1 el \u00e1rbol al espacio m\u00e1s all\u00e1 de los $ 30 \\times 30$ cuadrados que enmarcan el conjunto de entrenamiento.","c38598ef":"# <center>Topic 3. Classification, Decision Trees and k Nearest Neighbors","0a03a84e":"<img src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/topic3_eng_dtree_structure.png'>","2baac3b3":"## 4. Choosing Model Parameters and Cross-Validation \n\nThe main task of learning algorithms is to be able to *generalize* to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.\n\n## 4. Elegir los par\u00e1metros del modelo y la validaci\u00f3n cruzada\n\nLa tarea principal de los algoritmos de aprendizaje es poder * generalizar * a datos invisibles. Debido a que no podemos verificar de inmediato el rendimiento del modelo en los datos nuevos y entrantes (debido a que a\u00fan no conocemos los valores reales de la variable objetivo), es necesario sacrificar una peque\u00f1a parte de los datos para verificar la calidad del modelo.\n","a366c1ba":"The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is $S_1 = -\\frac{5}{13}\\log_2{\\frac{5}{13}}-\\frac{8}{13}\\log_2{\\frac{8}{13}} \\approx 0.96$. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is $S_2 = -\\frac{1}{7}\\log_2{\\frac{1}{7}}-\\frac{6}{7}\\log_2{\\frac{6}{7}} \\approx 0.6$. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable $Q$ (in this example it's a variable \"$x \\leq 12$\") is defined as\n\nEl grupo de la izquierda tiene 13 bolas, 8 azules y 5 amarillas. La entrop\u00eda de este grupo es $S_1 = -\\frac{5}{13}\\log_2{\\frac{5}{13}}-\\frac{8}{13}\\log_2{\\frac{8}{13}} \\approx 0.96$. El grupo de la derecha tiene 7 bolas, 1 azul y 6 amarillas. La entrop\u00eda del grupo de la derecha es $S_2 = -\\frac{1}{7}\\log_2{\\frac{1}{7}}-\\frac{6}{7}\\log_2{\\frac{6}{7}} \\approx 0.6$. Como puede ver, la entrop\u00eda ha disminuido en ambos grupos, m\u00e1s a\u00fan en el grupo correcto. Dado que la entrop\u00eda es, de hecho, el grado de caos (o incertidumbre) en el sistema, la reducci\u00f3n de la entrop\u00eda se denomina ganancia de informaci\u00f3n. Formalmente, la ganancia de informaci\u00f3n (IG) para una divisi\u00f3n basada en la variable $ Q $ (en este ejemplo es una variable \"$x \\leq 12$\") se define como","c351e4cf":"We can make sure that the tree built in the previous example is optimal: it took only 5 \"questions\" (conditioned on the variable $x$) to perfectly fit a decision tree to the training set. Under other split conditions, the resulting tree would be deeper, i.e. take more \"questions\" to reach an answer.\n \nAt the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for \"early stopping\" or \"cut-off\" to avoid constructing an overfitted tree. \n\n------------------\n\nPodemos asegurarnos de que el \u00e1rbol construido en el ejemplo anterior sea \u00f3ptimo: solo tom\u00f3 5 \"preguntas\" (condicionadas en la variable $x$) para ajustar perfectamente un \u00e1rbol de decisi\u00f3n al conjunto de capacitaci\u00f3n. Bajo otras condiciones de divisi\u00f3n, el \u00e1rbol resultante ser\u00eda m\u00e1s profundo, es decir, tomar\u00eda m\u00e1s \"preguntas\" para llegar a una respuesta.\n\u00a0\nEn el coraz\u00f3n de los algoritmos populares para la construcci\u00f3n del \u00e1rbol de decisi\u00f3n, como ID3 o C4.5, se encuentra el principio de la maximizaci\u00f3n codiciosa de la ganancia de informaci\u00f3n: en cada paso, el algoritmo elige la variable que proporciona la mayor ganancia de informaci\u00f3n al dividirse. Luego, el procedimiento se repite de forma recursiva hasta que la entrop\u00eda sea cero (o alg\u00fan valor peque\u00f1o para tener en cuenta el sobreajuste). Diferentes algoritmos utilizan diferentes heur\u00edsticas para \"detener temprano\" o \"cortar\" para evitar la construcci\u00f3n de un \u00e1rbol demasiado equipado.\n\n```python\ndef build(L):\n    create node t\n    if the stopping criterion is True:\n        assign a predictive model to t\n    else:\n        Find the best binary split L = L_left + L_right\n        t.left = build(L_left)\n        t.right = build(L_right)\n    return t     \n```","45bd7bed":"This is often done in one of two ways:\n- setting aside a part of the dataset (*held-out\/hold-out set*). Thus we reserve a fraction of the training set (typically from 20% to 40%), train the model on the remaining data (60-80% of the original set), and compute performance metrics for the model (e.g accuracy) on the hold-out set.\n- *cross-validation*. The most frequent case here is *k-fold cross-validation*.\n\n------\n\nEsto se hace a menudo de una de dos maneras:\n- dejar de lado una parte del conjunto de datos (* conjunto sostenido \/ retenido *). Por lo tanto, reservamos una fracci\u00f3n del conjunto de capacitaci\u00f3n (generalmente del 20% al 40%), entrenamos el modelo en los datos restantes (60-80% del conjunto original), y calculamos las m\u00e9tricas de rendimiento para el modelo (por ejemplo, precisi\u00f3n) en el conjunto de espera.\n- *validaci\u00f3n cruzada*. El caso m\u00e1s frecuente aqu\u00ed es la validaci\u00f3n cruzada * k-fold *.","a708aa07":"\nIf we sort by age, the target class ( \"loan default\") switches (from 1 to 0 or vice versa) 5 times. And if we sort by salary, it switches 7 times. How will the tree choose features now? Let's see.\n\n----------\n\nSi ordenamos por edad, la clase objetivo (\"incumplimiento del pr\u00e9stamo\") cambia (de 1 a 0 o viceversa) 5 veces. Y si lo ordenamos por salario, cambia 7 veces. \u00bfC\u00f3mo elegir\u00e1 el \u00e1rbol las caracter\u00edsticas ahora? Veamos.","af509b8e":"Now let\u2019s tune our model parameters using cross-validation as before, but now we\u2019ll take into account that we have more features than in the previous task: 64. \n\nAhora ajustemos los par\u00e1metros de nuestro modelo utilizando la validaci\u00f3n cruzada como antes, pero ahora tendremos en cuenta que tenemos m\u00e1s funciones que en la tarea anterior: 64.","28d69810":"It turns out that dividing the balls into two groups by splitting on \"coordinate is less than or equal to 12\" gave us a more ordered system. Let's continue to divide them into groups until the balls in each group are all of the same color.\n\nResulta que dividir las bolas en dos grupos al dividir \"la coordenada es menor o igual a 12\" nos dio un sistema m\u00e1s ordenado. Continuemos dividi\u00e9ndolos en grupos hasta que las bolas de cada grupo sean del mismo color.","0e17df8f":"\nLet's see the best parameters combination and the corresponding accuracy from cross-validation:\n\nVeamos la mejor combinaci\u00f3n de par\u00e1metros y la precisi\u00f3n correspondiente de la validaci\u00f3n cruzada:\n","cb5c1424":"Now, let's identify the parameters for the tree using cross-validation. We'll tune the maximum depth and the maximum number of features used at each split. Here is the essence of how the GridSearchCV works: for each unique pair of values of `max_depth` and `max_features`, compute model performance with 5-fold cross-validation, and then select the best combination of parameters.\n\nAhora, identifiquemos los par\u00e1metros para el \u00e1rbol usando validaci\u00f3n cruzada. Ajustaremos la profundidad m\u00e1xima y la cantidad m\u00e1xima de funciones utilizadas en cada divisi\u00f3n. Aqu\u00ed est\u00e1 la esencia de c\u00f3mo funciona GridSearchCV: para cada par \u00fanico de valores de `max_depth` y` max_features`, calcule el rendimiento del modelo con una validaci\u00f3n cruzada de 5 veces y luego seleccione la mejor combinaci\u00f3n de par\u00e1metros.","a9c1e24c":"### How a Decision Tree Works with Numerical Features\n\nSuppose we have a numeric feature \"Age\" that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as \"Age <17\", \"Age < 22.87\", and so on. But what if the age range is large? Or what if another quantitative variable, \"salary\", can also be \"cut\" in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.\n \nLet's consider an example. Suppose we have the following dataset:\n\n--------------------\n\n### C\u00f3mo funciona un \u00e1rbol de decisi\u00f3n con caracter\u00edsticas num\u00e9ricas\n\nSupongamos que tenemos una caracter\u00edstica num\u00e9rica \"Edad\" que tiene muchos valores \u00fanicos. Un \u00e1rbol de decisi\u00f3n buscar\u00e1 la mejor divisi\u00f3n (seg\u00fan alg\u00fan criterio de ganancia de informaci\u00f3n) al verificar los atributos binarios como \"Edad <17\", \"Edad <22.87\", etc. Pero \u00bfy si el rango de edad es grande? O \u00bfqu\u00e9 pasa si otra variable cuantitativa, el \"salario\", tambi\u00e9n puede ser \"recortada\" de muchas maneras? Habr\u00e1 demasiados atributos binarios para seleccionar en cada paso durante la construcci\u00f3n del \u00e1rbol. Para resolver este problema, las heur\u00edsticas se utilizan generalmente para limitar el n\u00famero de umbrales con los que comparamos la variable cuantitativa.\n\u00a0\nConsideremos un ejemplo. Supongamos que tenemos el siguiente conjunto de datos:","3227d3bc":"Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%\/96.6% accuracy for cross-validation and hold-out respectively. Decision trees perform very well, and even random forest (let's think of it for now as a bunch of trees that work better together) in this example cannot achieve better performance (95.1%\/95.3%) despite being trained for much longer. \n\n-----\n\nAqu\u00ed, el \u00e1rbol demostr\u00f3 ser mejor que el algoritmo de los vecinos m\u00e1s cercanos: 94.2%\/96.6% de precisi\u00f3n para validaci\u00f3n cruzada y retenci\u00f3n respectivamente. Los \u00e1rboles de decisi\u00f3n se desempe\u00f1an muy bien, e incluso los bosques aleatorios (pens\u00e9moslo por ahora como un grupo de \u00e1rboles que funcionan mejor juntos) en este ejemplo no se puede lograr un mejor rendimiento (95.1%\/95.3%) a pesar de haber estado entrenados por mucho m\u00e1s tiempo.","6af87fc4":"You would be right to point out that we have not tuned any `RandomForestClassifier` parameters here. Even with tuning, the training accuracy doesn\u2019t reach 98% as it did with one nearest neighbour.  \n\n-------------------\n\nEntrenemos un bosque aleatorio en el mismo conjunto de datos, funciona mejor que k-NN en la mayor\u00eda de los conjuntos de datos. Pero aqu\u00ed tenemos una excepci\u00f3n....","7c873315":"In the second example, the tree solved the problem perfectly while k-NN experienced difficulties. However, this is more of a disadvantage of using Euclidian distance than of the method. It did not allow us to reveal that one feature was much better than the others. \n\n----------------\n\nEn el segundo ejemplo, el \u00e1rbol resolvi\u00f3 el problema perfectamente mientras que k-NN experiment\u00f3 dificultades. Sin embargo, esto es m\u00e1s una desventaja de usar la distancia euclidiana que del m\u00e9todo. No nos permiti\u00f3 revelar que una caracter\u00edstica era mucho mejor que las otras.","32f8a68f":"Let\u2019s select 70% of the dataset for training (`X_train`, `y_train`) and 30% for holdout (`X_holdout`, `y_holdout`). The holdout set will not participate in model parameters tuning; we will use it at the end to check the quality of the resulting model.\n\nSeleccionemos el 70% del conjunto de datos para el entrenamiento (`X_train`,` y_train`) y el 30% para la retenci\u00f3n (`X_holdout`,` y_holdout`). El conjunto de exclusi\u00f3n no participar\u00e1 en el ajuste de par\u00e1metros del modelo; Lo usaremos al final para verificar la calidad del modelo resultante.","a1b660ba":"<img align='center' src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/topic3_hse_instruction.png'><br>","b053ea51":"Let's draw the resulting tree. Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can \"walk\" over the tree if you click on the picture.\n\nDibujemos el \u00e1rbol resultante. Debido a que no es completamente un ejemplo de juguete (su profundidad m\u00e1xima es 6), la imagen no es tan peque\u00f1a, pero puede \"caminar\" sobre el \u00e1rbol si hace clic en la imagen.","56bb8593":"### Crucial Tree Parameters\n\nTechnically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be *overfitted*, or too tuned to the training set, and will not predict labels for new data well. At the bottom of the tree, at some great depth, there will be partitions on less important features (e.g. whether a client came from Leeds or New York). We can exaggerate this story further and find that all four clients who came to the bank for a loan in green trousers did not return the loan. Even if that were true in training, we do not want our classification model to generate such specific rules.\n \nThere are two exceptions where the trees are built to the maximum depth:\n- Random Forest (a group of trees) averages the responses from individual trees that are built to the maximum depth (we will talk later on why you should do this)\n- *Pruning* trees. In this approach, the tree is first constructed to the maximum depth. Then, from the bottom up, some nodes of the tree are removed by comparing the quality of the tree with and without that partition (comparison is performed using *cross-validation*, more on this below).\n\nThe picture below is an example of a dividing border built in an overfitted tree. \n\n---------\n\n### Par\u00e1metros del \u00e1rbol crucial\n\nT\u00e9cnicamente, puede crear un \u00e1rbol de decisiones hasta que cada hoja tenga exactamente una instancia, pero esto no es com\u00fan en la pr\u00e1ctica cuando se construye un solo \u00e1rbol porque estar\u00e1 * demasiado ajustado *, o demasiado sintonizado con el conjunto de entrenamiento, y no predecir\u00e1 las etiquetas para Nuevos datos bien. En la parte inferior del \u00e1rbol, con una gran profundidad, habr\u00e1 particiones en caracter\u00edsticas menos importantes (por ejemplo, si un cliente vino de Leeds o Nueva York). Podemos exagerar m\u00e1s esta historia y encontrar que los cuatro clientes que acudieron al banco a pedir un pr\u00e9stamo con pantal\u00f3n verde no devolvieron el pr\u00e9stamo. Incluso si eso fuera cierto en el entrenamiento, no queremos que nuestro modelo de clasificaci\u00f3n genere tales reglas espec\u00edficas.\n\u00a0\nHay dos excepciones donde los \u00e1rboles se construyen a la profundidad m\u00e1xima:\n- El bosque aleatorio (un grupo de \u00e1rboles) promedia las respuestas de los \u00e1rboles individuales que se construyen a la profundidad m\u00e1xima (hablaremos m\u00e1s adelante sobre por qu\u00e9 debe hacerlo)\n- * Poda * arboles. En este enfoque, el \u00e1rbol se construye primero a la profundidad m\u00e1xima. Luego, de abajo hacia arriba, se eliminan algunos nodos del \u00e1rbol comparando la calidad del \u00e1rbol con y sin esa partici\u00f3n (la comparaci\u00f3n se realiza utilizando * validaci\u00f3n cruzada *, m\u00e1s sobre esto m\u00e1s adelante).\n\nLa siguiente imagen es un ejemplo de un borde divisorio construido en un \u00e1rbol sobre equipado.","9965f87a":"### Nearest Neighbors Method in Real Applications\n- k-NN can serve as a good starting point (baseline) in some cases;\n- In Kaggle competitions, k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking\/blending;\n- The nearest neighbors method extends to other tasks like recommendation systems. The initial decision could be a recommendation of a product (or service) that is popular among the *closest neighbors* of the person for whom we want to make a recommendation;\n- In practice, on large datasets, approximate methods of search are often used for nearest neighbors. There is a number of open source libraries that implement such algorithms; check out Spotify's library [Annoy](https:\/\/github.com\/spotify\/annoy).\n\n--------\n\n### M\u00e9todo de vecinos m\u00e1s cercanos en aplicaciones reales\n- k-NN puede servir como un buen punto de partida (l\u00ednea de base) en algunos casos;\n- En las competiciones Kaggle, k-NN se usa a menudo para la construcci\u00f3n de meta-caracter\u00edsticas (es decir, predicciones de k-NN como entrada para otros modelos) o para apilar \/ mezclar;\n- El m\u00e9todo de vecinos m\u00e1s cercanos se extiende a otras tareas como los sistemas de recomendaci\u00f3n. La decisi\u00f3n inicial podr\u00eda ser una recomendaci\u00f3n de un producto (o servicio) que es popular entre los \"vecinos m\u00e1s cercanos\" de la persona a quien queremos hacer una recomendaci\u00f3n;\n- En la pr\u00e1ctica, en grandes conjuntos de datos, los m\u00e9todos de b\u00fasqueda aproximados se utilizan a menudo para los vecinos m\u00e1s cercanos. Hay una serie de bibliotecas de c\u00f3digo abierto que implementan tales algoritmos; visita la biblioteca de Spotify [Annoy] (https:\/\/github.com\/spotify\/annoy).","5af40fbc":"There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability $p_1=\\frac{9}{20}$ and yellow with probability $p_2=\\frac{11}{20}$, which gives us an entropy $S_0 = -\\frac{9}{20}\\log_2{\\frac{9}{20}}-\\frac{11}{20}\\log_2{\\frac{11}{20}} \\approx 1$. This value by itself may not tell us much, but let's see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12.\n\nHay 9 bolas azules y 11 bolas amarillas. Si sacamos una bola al azar, ser\u00e1 azul con probabilidad $ p_1 =\\frac{9}{20}$ y amarillo con probabilidad $p_2 = \\frac{11}{20}$, lo que nos da una entrop\u00eda $S_0 = -\\frac{9}{20}\\log_2{\\frac{9}{20}}-\\frac{11}{20}\\log_2{\\frac{11}{20}}\\approx 1$. Es posible que este valor por s\u00ed solo no nos diga mucho, pero veamos c\u00f3mo cambia el valor si tuvi\u00e9ramos que dividir las bolas en dos grupos: con la posici\u00f3n menor o igual a 12 y mayor a 12.","dc3bcfc9":"Let's plot the data. Informally, the classification problem in this case is to build some \"good\" boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or at least a straight line or a hyperplane, would work well on new data.\n\n------------------------\n\nVamos a trazar los datos. De manera informal, el problema de clasificaci\u00f3n en este caso es construir un l\u00edmite \"bueno\" que separe las dos clases (los puntos rojos del amarillo). El aprendizaje autom\u00e1tico para este caso se reduce a elegir un buen borde de separaci\u00f3n. Una l\u00ednea recta ser\u00e1 demasiado simple, mientras que una curva compleja que serpentea por cada punto rojo ser\u00e1 demasiado compleja y nos llevar\u00e1 a cometer errores en nuevas muestras. Intuitivamente, alg\u00fan l\u00edmite suave, o al menos una l\u00ednea recta o un hiperplano, funcionar\u00eda bien en nuevos datos.","9ec5f014":"For another example, if you do not know how to tag a Bluetooth-headset on an online listing, you can find 5 similar headsets, and, if 4 of them are tagged as \"accessories\" and only 1 as \"Technology\", then you will also  label it under \"accessories\".\n \nTo classify each sample from the test set, one needs to perform the following operations in order:\n1. Calculate the distance to each of the samples in the training set.\n2. Select $k$ samples from the training set with the minimal distance to them.\n3. The class of the test sample will be the most frequent class among those $k$ nearest neighbors.\n\nThe method adapts quite easily for the regression problem: on step 3, it returns not the class, but the number \u2013 a mean (or median) of the target variable among neighbors.\n \nA notable feature of this approach is its laziness \u2013 calculations are only done during the prediction phase, when a test sample needs to be classified. No model is constructed from the training examples beforehand. In contrast, recall that for decision trees in the first half of this article the tree is constructed based on the training set, and the classification of test cases occurs relatively quickly by traversing through the tree.\n \nNearest neighbors is a well-studied approach. There exist many important theorems claiming that, on \"endless\" datasets, it is the optimal method of classification. The authors of the classic book \"The Elements of Statistical Learning\" consider k-NN to be a theoretically ideal algorithm which usage is only limited by computation power and the [curse of dimensionality](https:\/\/en.wikipedia.org\/wiki\/Curse_of_dimensionality). \n\nEn otro ejemplo, si no sabe c\u00f3mo etiquetar un auricular Bluetooth en una lista en l\u00ednea, puede encontrar 5 auriculares similares y, si 4 de ellos est\u00e1n etiquetados como \"accesorios\" y solo 1 como \"Tecnolog\u00eda\", entonces Tambi\u00e9n lo etiquetar\u00e1 bajo \"accesorios\".\n\u00a0\nPara clasificar cada muestra del conjunto de pruebas, se deben realizar las siguientes operaciones en orden:\n1. Calcule la distancia a cada una de las muestras en el conjunto de entrenamiento.\n2. Seleccione $ k $ muestras del conjunto de entrenamiento con la distancia m\u00ednima a ellos.\n3. La clase de la muestra de prueba ser\u00e1 la clase m\u00e1s frecuente entre los vecinos m\u00e1s cercanos a $ k $.\n\nEl m\u00e9todo se adapta con bastante facilidad para el problema de regresi\u00f3n: en el paso 3, no devuelve la clase, sino el n\u00famero, una media (o mediana) de la variable objetivo entre vecinos.\n\u00a0\nUna caracter\u00edstica notable de este enfoque es su pereza: los c\u00e1lculos solo se realizan durante la fase de predicci\u00f3n, cuando es necesario clasificar una muestra de prueba. Ning\u00fan modelo se construye a partir de los ejemplos de entrenamiento de antemano. En contraste, recuerde que para los \u00e1rboles de decisi\u00f3n en la primera mitad de este art\u00edculo, el \u00e1rbol se construye en funci\u00f3n del conjunto de entrenamiento, y la clasificaci\u00f3n de los casos de prueba se produce de manera relativamente r\u00e1pida al atravesar el \u00e1rbol.\n\u00a0\nVecinos m\u00e1s cercanos es un enfoque bien estudiado. Existen muchos teoremas importantes que afirman que, en conjuntos de datos \"sin fin\", es el m\u00e9todo \u00f3ptimo de clasificaci\u00f3n. Los autores del libro cl\u00e1sico \"Los elementos del aprendizaje estad\u00edstico\" consideran que k-NN es un algoritmo te\u00f3ricamente ideal cuyo uso solo est\u00e1 limitado por el poder de c\u00e1lculo y la [maldici\u00f3n de la dimensionalidad](https:\/\/en.wikipedia.org\/wiki \/Maldici\u00f3n de dimensionalidad).","d42150e3":"Now, let's tune the number of neighbors $k$ for k-NN:","7f9479e2":"## 7. Useful resources\n- Medium [\"story\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) based on this notebook\n- If you read Russian: an [article](https:\/\/habrahabr.ru\/company\/ods\/blog\/322534\/) on Habrahabr with ~ the same material. And a [lecture](https:\/\/youtu.be\/p9Hny3Cs6rk) on YouTube\n- Decision trees and k Nearest Neighbors are covered practically in every ML book. We recommend \u201cPattern Recognition and Machine Learning\u201d (C. Bishop) and \u201cMachine Learning: A Probabilistic Perspective\u201d (K. Murphy).\n- The book \u201cMachine Learning in Action\u201d (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) library. These guys work hard on writing really clear documentation.\n- Scipy 2017 [scikit-learn tutorial](https:\/\/github.com\/amueller\/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n- One more [ML course](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) with very good materials.\n- [Implementations](https:\/\/github.com\/rushter\/MLAlgorithms) of many ML algorithms. Good to search for decision trees and k-NN.\n\n-------------\n\n## 7. Recursos \u00fatiles\n- Medio [\"historia\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors- 8613c6b6d2cd) basado en este cuaderno\n- Si lees ruso: un [art\u00edculo](https:\/\/habrahabr.ru\/company\/ods\/blog\/322534\/) en Habrahabr con ~ el mismo material. Y una [conferencia](https:\/\/youtu.be\/p9Hny3Cs6rk) en YouTube\n- Los \u00e1rboles de decisi\u00f3n y los vecinos m\u00e1s cercanos est\u00e1n cubiertos pr\u00e1cticamente en todos los libros de ML. Recomendamos \u201cReconocimiento de patrones y aprendizaje autom\u00e1tico\u201d (C. Bishop) y \u201cAprendizaje autom\u00e1tico: una perspectiva probabil\u00edstica\u201d (K. Murphy).\n- El libro \u201cAprendizaje de m\u00e1quinas en acci\u00f3n\u201d (P. Harrington) lo guiar\u00e1 a trav\u00e9s de las implementaciones de algoritmos ML cl\u00e1sicos en Python puro.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) biblioteca. Estos chicos trabajan duro para escribir documentaci\u00f3n muy clara.\n- Scipy 2017 [scikit-learn tutorial](https:\/\/github.com\/amueller\/scipy-2017-sklearn) por Alex Gramfort y Andreas Mueller.\n- Uno m\u00e1s [curso de ML](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) con muy buenos materiales.\n- [Implementaciones](https:\/\/github.com\/rushter\/MLAlgorithms) de muchos algoritmos ML. Bueno para buscar \u00e1rboles de decisi\u00f3n y k-NN.","98833f8a":"The quality of classification\/regression with k-NN depends on several parameters:\n\n- The number of neighbors $k$.\n- The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the \"salary\" feature, which is on the order of thousands, to affect the distance more than \"age\", which is generally less than 100. \n- Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight).\n\n-------\n\nLa calidad de la clasificaci\u00f3n \/ regresi\u00f3n con k-NN depende de varios par\u00e1metros:\n\n- El n\u00famero de vecinos $k$.\n- La medida de la distancia entre las muestras (las m\u00e1s comunes incluyen las distancias de Hamming, Euclides, coseno y Minkowski). Tenga en cuenta que la mayor\u00eda de estas m\u00e9tricas requieren que los datos sean escalados. En pocas palabras, no queremos que la funci\u00f3n de \"salario\", que es del orden de miles, afecte la distancia m\u00e1s que la \"edad\", que generalmente es inferior a 100.\n- Pesos de los vecinos (cada vecino puede contribuir con pesos diferentes; por ejemplo, cuanto m\u00e1s lejos de la muestra, m\u00e1s bajo es el peso).","c89e9acf":"For the right group, we can easily see that we only need one extra partition using \"coordinate less than or equal to 18\". But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 ($\\log_2{1} = 0$).\n\nWe have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer \"questions\" or splits would be more accurate, even if it does not perfectly fit the training set. We will discuss the problem of overfitting later. \n\n-----------------------\n\nPara el grupo correcto, podemos ver f\u00e1cilmente que solo necesitamos una partici\u00f3n adicional que use \"coordenada menor o igual a 18\". Pero, para el grupo de la izquierda, necesitamos tres m\u00e1s. Tenga en cuenta que la entrop\u00eda de un grupo donde todas las bolas son del mismo color es igual a 0 ($\\log_2{1} = 0$).\n\nHemos construido con \u00e9xito un \u00e1rbol de decisi\u00f3n que predice el color de la bola seg\u00fan su posici\u00f3n. Es posible que este \u00e1rbol de decisi\u00f3n no funcione bien si agregamos bolas porque se ajustan perfectamente al conjunto de entrenamiento (iniciales 20 bolas). Si quisi\u00e9ramos hacerlo bien en ese caso, un \u00e1rbol con menos \"preguntas\" o divisiones ser\u00eda m\u00e1s preciso, incluso si no se ajusta perfectamente al conjunto de entrenamiento. Vamos a discutir el problema de sobrealimentaci\u00f3n m\u00e1s tarde.","97bb289b":"where $p_i$ is the probability of finding the system in the $i$-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize \"effective data splitting\", which we alluded to in the context of \"20 Questions\".\n\ndonde $p_i$ es la probabilidad de encontrar el sistema en el estado $i$ -th. Este es un concepto muy importante utilizado en f\u00edsica, teor\u00eda de la informaci\u00f3n y otras \u00e1reas. La entrop\u00eda se puede describir como el grado de caos en el sistema. Cuanto mayor sea la entrop\u00eda, menos ordenado estar\u00e1 el sistema y viceversa. Esto nos ayudar\u00e1 a formalizar la \"divisi\u00f3n efectiva de datos\", a la que aludimos en el contexto de las \"20 preguntas\".","b833464d":"<img align='center' src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/topic3_credit_scoring_entropy.png'><br>","90b89c09":"<img align='center' src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/cross_validation.png'><br>","bf34d0db":"Let's allocate 70% of the set for training (`X_train`, `y_train`) and 30% for the hold-out set (`X_holdout`, `y_holdout`). The hold-out set will not be involved in tuning the parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: decision tree and k-NN. We do not know what parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10.\n\n------\n\nAsignemos el 70% del conjunto para entrenamiento (`X_train`,` y_train`) y el 30% para el conjunto de retenci\u00f3n (`X_holdout`,` y_holdout`). El conjunto de retenci\u00f3n no estar\u00e1 involucrado en el ajuste de los par\u00e1metros de los modelos. Lo usaremos al final, despu\u00e9s de la optimizaci\u00f3n, para evaluar la calidad del modelo resultante. Entrenemos 2 modelos: \u00e1rbol de decisi\u00f3n y k-NN. No sabemos qu\u00e9 par\u00e1metros son buenos, por lo que asumiremos algunos aleatorios: una profundidad de \u00e1rbol de 5 y el n\u00famero de vecinos m\u00e1s cercanos es igual a 10.","cf15ee6c":"Next, let's do the same experiment as in the previous task, but, this time, let's change the ranges for tunable parameters.\n\nA continuaci\u00f3n, hagamos el mismo experimento que en la tarea anterior, pero, esta vez, cambiemos los rangos para los par\u00e1metros ajustables.","7946947a":"We discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exists others:\n\n- Gini uncertainty (Gini impurity): $G = 1 - \\sum\\limits_k (p_k)^2$. Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).\n- Misclassification error:  $E = 1 - \\max\\limits_k p_k$\n\nIn practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.\n \nFor binary classification, entropy and Gini uncertainty take the following form:\n\n$ S = -p_+ \\log_2{p_+} -p_- \\log_2{p_-} = -p_+ \\log_2{p_+} -(1 - p_{+}) \\log_2{(1 - p_{+})};$\n\n$ G = 1 - p_+^2 - p_-^2 = 1 - p_+^2 - (1 - p_+)^2 = 2p_+(1-p_+).$\n\nwhere ($p_+$ is the probability of an object having a label +).\n\nIf we plot these two functions against the argument $p_+$, we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.\n\n------------------------\n\nDiscutimos c\u00f3mo la entrop\u00eda nos permite formalizar particiones en un \u00e1rbol. Pero esto es solo una heur\u00edstica; existen otros:\n\n- Incertidumbre de Gini (impureza de Gini): $G = 1 - \\sum\\limits_k (p_k)^2$. Maximizar este criterio puede interpretarse como la maximizaci\u00f3n del n\u00famero de pares de objetos de la misma clase que est\u00e1n en el mismo sub\u00e1rbol (no debe confundirse con el \u00edndice de Gini).\n- Error de clasificaci\u00f3n err\u00f3nea: $E = 1 - \\max\\limits_k p_k$\n\nEn la pr\u00e1ctica, el error de clasificaci\u00f3n err\u00f3nea casi nunca se usa, y la incertidumbre y la informaci\u00f3n de Gini funcionan de manera similar.\n\u00a0\nPara la clasificaci\u00f3n binaria, la entrop\u00eda y la incertidumbre de Gini adoptan la siguiente forma:\n\n$ S = -p_+ \\log_2{p_+} -p_- \\log_2{p_-} = -p_+ \\log_2{p_+} -(1 - p_{+}) \\log_2{(1 - p_{+})};$\n\n$ G = 1 - p_+^2 - p_-^2 = 1 - p_+^2 - (1 - p_+)^2 = 2p_+(1-p_+).$\n\ndonde ($ p _ + $ es la probabilidad de que un objeto tenga una etiqueta +).\n\nSi trazamos estas dos funciones contra el argumento $p_+$, veremos que la trama de entrop\u00eda est\u00e1 muy cerca de la trama de incertidumbre de Gini, se duplic\u00f3. Por lo tanto, en la pr\u00e1ctica, estos dos criterios son casi id\u00e9nticos.","35ed7088":"#### Toy Example\nTo illustrate how entropy can help us identify good features for building a decision tree, let's look at a toy example. We will predict the color of the ball based on its position.\n\nPara ilustrar c\u00f3mo la entrop\u00eda puede ayudarnos a identificar buenas caracter\u00edsticas para construir un \u00e1rbol de decisiones, veamos un ejemplo de juguete. Predeciremos el color de la bola en funci\u00f3n de su posici\u00f3n.\n\n<img align='center' src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/decision_tree3.png'><br>","63f99bd6":"## 1. Introduction","2f939d13":"## 6. Pros and Cons of Decision Trees and the Nearest Neighbors Method\n\n### Decision trees\n\nPros:\n- Generation of clear human-understandable classification rules, e.g. \"if age <25 and is interested in motorcycles, deny the loan\". This property is called interpretability of the model.\n- Decision trees can be easily visualized, i.e. both the model itself (the tree) and prediction for a certain test object (a path in the tree) can \"be interpreted\".\n- Fast training and forecasting.\n- Small number of model parameters.\n- Supports both numerical and categorical features.\n\nCons:\n \n- The trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.\n- Separating border built by a decision tree has its limitations \u2013 it consists of hyperplanes perpendicular to one of the coordinate axes, which is inferior in quality to some other methods, in practice.\n- We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Note that overfitting is an issue for all machine learning methods.\n- Instability. Small changes to the data can significantly change the decision tree. This problem is tackled with decision tree ensembles (discussed next time).\n- The optimal decision tree search problem is NP-complete. Some heuristics are used in practice such as greedy search for a feature with maximum information gain, but it does not guarantee finding the globally optimal tree.\n- Difficulties to support missing values in the data. Friedman estimated that it took about 50% of the code to support gaps in data in CART (an improved version of this algorithm is implemented in `sklearn`).\n- The model can only interpolate but not extrapolate (the same is true for random forests and tree boosting). That is, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space. In our example with the yellow and blue balls, it would mean that the model gives the same predictions for all balls with positions >19 or <0.\n\n--------------------\n\n## 6. Pros y contras de los \u00e1rboles de decisi\u00f3n y el m\u00e9todo de vecinos m\u00e1s cercanos\n\n### \u00c1rboles de decisi\u00f3n\n\nPros:\n- Generaci\u00f3n de reglas de clasificaci\u00f3n claras y comprensibles para el hombre, por ejemplo, \"Si la edad es <25 y est\u00e1 interesada en las motocicletas, deniegue el pr\u00e9stamo\". Esta propiedad se llama interpretabilidad del modelo.\n- Los \u00e1rboles de decisi\u00f3n se pueden visualizar f\u00e1cilmente, es decir, tanto el modelo en s\u00ed (el \u00e1rbol) como la predicci\u00f3n de un determinado objeto de prueba (una ruta en el \u00e1rbol) se pueden \"interpretar\".\n- R\u00e1pido entrenamiento y previsi\u00f3n.\n- Peque\u00f1o n\u00famero de par\u00e1metros del modelo.\n- Soporta caracter\u00edsticas tanto num\u00e9ricas como categ\u00f3ricas.\n\nContras:\n\u00a0\n- Los \u00e1rboles son muy sensibles al ruido en los datos de entrada; todo el modelo podr\u00eda cambiar si el conjunto de entrenamiento se modifica ligeramente (por ejemplo, eliminar una caracter\u00edstica, agregar algunos objetos). Esto perjudica la interpretabilidad del modelo.\n- El borde de separaci\u00f3n construido por un \u00e1rbol de decisi\u00f3n tiene sus limitaciones: consiste en hiperplanos perpendiculares a uno de los ejes de coordenadas, que en la pr\u00e1ctica es inferior en calidad a otros m\u00e9todos.\n- Debemos evitar el ajuste excesivo mediante la poda, el establecimiento de un n\u00famero m\u00ednimo de muestras en cada hoja o la definici\u00f3n de una profundidad m\u00e1xima para el \u00e1rbol. Tenga en cuenta que el ajuste excesivo es un problema para todos los m\u00e9todos de aprendizaje autom\u00e1tico.\n- Inestabilidad. Peque\u00f1os cambios en los datos pueden cambiar significativamente el \u00e1rbol de decisi\u00f3n. Este problema se aborda con conjuntos de \u00e1rboles de decisi\u00f3n (que se discutir\u00e1n la pr\u00f3xima vez).\n- El problema de b\u00fasqueda del \u00e1rbol de decisi\u00f3n \u00f3ptimo es NP-completo. Algunas heur\u00edsticas se utilizan en la pr\u00e1ctica, como la b\u00fasqueda codiciosa de una funci\u00f3n con el m\u00e1ximo de ganancia de informaci\u00f3n, pero no garantiza encontrar el \u00e1rbol \u00f3ptimo a nivel mundial.\n- Dificultades para soportar valores perdidos en los datos. Friedman estim\u00f3 que tom\u00f3 aproximadamente el 50% del c\u00f3digo para admitir las brechas en los datos en CART (una versi\u00f3n mejorada de este algoritmo se implementa en `sklearn`).\n- El modelo solo puede interpolar pero no extrapolar (lo mismo ocurre con los bosques aleatorios y el reforzamiento de \u00e1rboles). Es decir, un \u00e1rbol de decisiones realiza una predicci\u00f3n constante para los objetos que se encuentran m\u00e1s all\u00e1 del cuadro delimitador establecido por el conjunto de entrenamiento en el espacio de caracter\u00edsticas. En nuestro ejemplo con las bolas amarillas y azules, esto significar\u00eda que el modelo ofrece las mismas predicciones para todas las bolas con posiciones> 19 o <0.","498d070b":"$$ \\Large IG(x \\leq 12) = S_0 - \\frac{13}{20}S_1 - \\frac{7}{20}S_2 \\approx 0.16.$$","b6cec676":"In k-fold cross-validation, the model is trained $K$ times on different ($K-1$) subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange).\nWe obtain $K$ model quality assessments that are usually averaged to give an overall average quality of classification\/regression.\n\nCross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.\n \nCross-validation is a very important technique in machine learning and can also be applied in statistics and econometrics. It helps with hyperparameter tuning, model comparison, feature evaluation, etc. More details can be found [here](https:\/\/sebastianraschka.com\/blog\/2016\/model-evaluation-selection-part1.html) (blog post by Sebastian Raschka) or in any classic textbook on machine (statistical) learning.\n\n--------\n\nEn la validaci\u00f3n cruzada K-fold, el modelo se entrena $ K $ veces en diferentes subconjuntos ($ K-1 $) del conjunto de datos original (en blanco) y se verifica en el subconjunto restante (cada vez uno diferente, que se muestra arriba en naranja).\nObtenemos evaluaciones de calidad del modelo de $ K $ que generalmente se promedian para dar una calidad promedio general de clasificaci\u00f3n \/ regresi\u00f3n.\n\nLa validaci\u00f3n cruzada proporciona una mejor evaluaci\u00f3n de la calidad del modelo en los nuevos datos en comparaci\u00f3n con el enfoque de conjunto de espera. Sin embargo, la validaci\u00f3n cruzada es computacionalmente costosa cuando tiene una gran cantidad de datos.\n\u00a0\nLa validaci\u00f3n cruzada es una t\u00e9cnica muy importante en el aprendizaje autom\u00e1tico y tambi\u00e9n se puede aplicar en estad\u00edstica y econometr\u00eda. Ayuda con la optimizaci\u00f3n del hiperpar\u00e1metro, la comparaci\u00f3n de modelos, la evaluaci\u00f3n de caracter\u00edsticas, etc. Se pueden encontrar m\u00e1s detalles [aqu\u00ed](https:\/\/sebastianraschka.com\/blog\/2016\/model-evaluation-selection-part1.html) (blog post por Sebastian Raschka) o en cualquier libro de texto cl\u00e1sico sobre aprendizaje autom\u00e1tico (estad\u00edstico).","e3f87325":"Let's assess prediction quality on our hold-out set with a simple metric, the proportion of correct answers (accuracy). The decision tree did better: the percentage of correct answers is about 94% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using random parameters.\n\nEvaluemos la calidad de la predicci\u00f3n en nuestro conjunto de espera con una m\u00e9trica simple, la proporci\u00f3n de respuestas correctas (precisi\u00f3n). El \u00e1rbol de decisi\u00f3n se desempe\u00f1\u00f3 mejor: el porcentaje de respuestas correctas es aproximadamente 94% (\u00e1rbol de decisi\u00f3n) versus 88% (k-NN). Tenga en cuenta que este rendimiento se logra mediante el uso de par\u00e1metros aleatorios.","8a0d8673":"<img align='center' src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/topic3_entropy_balls2.png'><br>\n","74e0a9e9":"## Article outline\n\n1. Introduction\n2. Decision Tree\n3. Nearest Neighbors Method\n4. Choosing Model Parameters and Cross-Validation\n5. Application Examples and Complex Cases\n6. Pros and Cons of Decision Trees and the Nearest Neighbors Method\n7. Useful resources\n\n## Resumen del art\u00edculo\n\n1. Introducci\u00f3n\n2. \u00c1rbol de decisiones\n3. M\u00e9todo de vecinos m\u00e1s cercanos\n4. Elegir los par\u00e1metros del modelo y la validaci\u00f3n cruzada\n5. Ejemplos de aplicaci\u00f3n y casos complejos\n6. Pros y contras de los \u00e1rboles de decisi\u00f3n y el m\u00e9todo de vecinos m\u00e1s cercanos\n7. Recursos \u00fatiles.","fa372711":"In our next case, we solve a binary classification problem (approve\/deny a loan) on the grounds of \"Age\", \"Home-ownership\", \"Income\" and \"Education\".\n \nThe decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form \"feature $a$ value is less than $x$ and feature $b$ value is less than $y$ ... => Category 1\" into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.\n\nAs we'll see later, many other models, although more accurate, do not have this property and can be regarded as more of a \"black box\" approach, where it is harder to interpret how the input data was transformed into the output. Due to this \"understandability\" and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (\"Top 10 Algorithms in Data Mining\", Knowledge and Information Systems, 2008. [PDF](http:\/\/www.cs.uvm.edu\/~icdm\/algorithms\/10Algorithms-08.pdf)).\n\n-----\n\nEn nuestro siguiente caso, resolvemos un problema de clasificaci\u00f3n binaria (aprobar \/ denegar un pr\u00e9stamo) por \"Edad\", \"Propiedad de la vivienda\", \"Ingresos\" y \"Educaci\u00f3n\".\n\u00a0\nEl \u00e1rbol de decisi\u00f3n como algoritmo de aprendizaje autom\u00e1tico es esencialmente el mismo que el diagrama que se muestra arriba; incorporamos un flujo de reglas l\u00f3gicas de la forma \"la caracter\u00edstica $a$ valor es menor que $x$ y la caracter\u00edstica $ b $ valor es menor que $y$ ... => Categor\u00eda 1\" en una estructura de datos similar a un \u00e1rbol. La ventaja de este algoritmo es que son f\u00e1cilmente interpretables. Por ejemplo, al usar el esquema anterior, el banco puede explicar al cliente por qu\u00e9 se les neg\u00f3 un pr\u00e9stamo: por ejemplo, el cliente no es propietario de una casa y su ingreso es inferior a 5,000.\n\nComo veremos m\u00e1s adelante, muchos otros modelos, aunque m\u00e1s precisos, no tienen esta propiedad y pueden considerarse m\u00e1s como un enfoque de \"caja negra\", donde es m\u00e1s dif\u00edcil interpretar c\u00f3mo se transformaron los datos de entrada en la salida. Debido a esta \"comprensibilidad\" y similitud con la toma de decisiones humana (puede explicar f\u00e1cilmente su modelo a su jefe), los \u00e1rboles de decisi\u00f3n han ganado una inmensa popularidad. C4.5, un representante de este grupo de m\u00e9todos de clasificaci\u00f3n, es incluso el primero en la lista de los 10 mejores algoritmos de miner\u00eda de datos (\"10 algoritmos principales en miner\u00eda de datos\", Sistemas de informaci\u00f3n y conocimiento, 2008. [PDF] (http: \/ \/www.cs.uvm.edu\/~icdm\/algorithms\/10Algorithms-08.pdf)).","e8fec032":"We see that the tree partitioned by both salary and age. Moreover, the thresholds for feature comparisons are 43.5 and 22.5 years of age and 95k and 30.5k per year. Again, we see that 95 is the average between 88 and 102; the individual with a salary of 88k proved to be \"bad\" while the one with 102k was \"good\". The same goes for 30.5k. That is, only a few values for comparisons by age and salary were searched. Why did the tree choose these features? Because they gave better partitioning (according to Gini uncertainty). \n\n------\n\nVemos que el \u00e1rbol se reparte por salario y edad. Adem\u00e1s, los umbrales para las comparaciones de caracter\u00edsticas son 43.5 y 22.5 a\u00f1os de edad y 95k y 30.5k por a\u00f1o. Nuevamente, vemos que 95 es el promedio entre 88 y 102; el individuo con un salario de 88k demostr\u00f3 ser \"malo\" mientras que el que ten\u00eda 102k fue \"bueno\". Lo mismo ocurre con 30.5k. Es decir, solo se buscaron algunos valores para las comparaciones por edad y salario. \u00bfPor qu\u00e9 el \u00e1rbol eligi\u00f3 estas caracter\u00edsticas? Porque dieron mejor partici\u00f3n (de acuerdo con la incertidumbre de Gini).","551c5151":"#### Example\nClassification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience *E* is the available training data: a set of *instances* (clients), a collection of *features* (such as age, salary, type of loan, past loan defaults, etc.) for each, and a *target variable* (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting *by how much time* the loan payment is overdue, this would become a regression problem.\n \nFinally, the third term used in the definition of machine learning is a **metric of the algorithm's performance evaluation *P*.** Such metrics differ for various problems and algorithms, and we'll discuss them as we study new algorithms. For now, we'll refer to a simple metric for classification algorithms, the proportion of correct answers \u2013 *accuracy* \u2013 on the test set.\n \nLet's take a look at two supervised learning problems: classification and regression.\n\n#### Ejemplo\nLa clasificaci\u00f3n y la regresi\u00f3n son problemas de aprendizaje supervisados. Por ejemplo, como instituci\u00f3n crediticia, es posible que deseamos predecir los incumplimientos de los pr\u00e9stamos en funci\u00f3n de los datos acumulados sobre nuestros clientes. Aqu\u00ed, la experiencia * E * son los datos de capacitaci\u00f3n disponibles: un conjunto de * instancias * (clientes), una colecci\u00f3n de * caracter\u00edsticas * (como la edad, el salario, el tipo de pr\u00e9stamo, los incumplimientos de pr\u00e9stamos pasados, etc.) para cada uno, y una * variable objetivo * (si fallaron en el pr\u00e9stamo). Esta variable objetivo es solo un hecho de incumplimiento de pr\u00e9stamo (1 o 0), as\u00ed que recuerde que este es un problema de clasificaci\u00f3n (binario). Si, en cambio, estuviera pronosticando * por cu\u00e1nto tiempo * est\u00e1 vencido el pago del pr\u00e9stamo, esto se convertir\u00eda en un problema de regresi\u00f3n.\n\u00a0\nFinalmente, el tercer t\u00e9rmino utilizado en la definici\u00f3n de aprendizaje autom\u00e1tico es una m\u00e9trica ** de la evaluaci\u00f3n de rendimiento del algoritmo * P *. ** Dichas m\u00e9tricas difieren para varios problemas y algoritmos, y los analizaremos a medida que estudiemos nuevos algoritmos. Por ahora, nos referiremos a una m\u00e9trica simple para los algoritmos de clasificaci\u00f3n, la proporci\u00f3n de respuestas correctas - * precisi\u00f3n * - en el conjunto de prueba.\n\u00a0\nEchemos un vistazo a dos problemas de aprendizaje supervisado: clasificaci\u00f3n y regresi\u00f3n.","24e3b811":"And how does the tree itself look? We see that the tree \"cuts\" the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it.\n\n\u00bfY c\u00f3mo se ve el propio \u00e1rbol? Vemos que el \u00e1rbol \"corta\" el espacio en 8 rect\u00e1ngulos, es decir, el \u00e1rbol tiene 8 hojas. Dentro de cada rect\u00e1ngulo, el \u00e1rbol har\u00e1 la predicci\u00f3n de acuerdo con la etiqueta de la mayor\u00eda de los objetos dentro de \u00e9l.","084728b2":"Results\n*(Legend: CV and Holdout are average shares of the correct answers on cross-model validation and hold-out sample. DT stands for a decision tree, k-NN stands for k-nearest neighbors, RF stands for random forest).*\n\n|     |   CV  | Holdout |  \n| **DT**  | 0.844 |  0.838  |  \n| **kNN** | 0.987 |  0.983  |\n| **RF**  | 0.935 |  0.941  | \n\n-------------------------\n\nResultados\n*(Leyenda: CV y Holdout son promedios compartidos de las respuestas correctas en la validaci\u00f3n de modelos cruzados y en la muestra de espera. DT representa un \u00e1rbol de decisi\u00f3n, k-NN significa k-vecinos m\u00e1s cercanos, RF significa bosque al azar).*\n\n|     |   CV  | Holdout |  \n| **DT**  | 0.844 |  0.838  |  \n| **kNN** | 0.987 |  0.983  |\n| **RF**  | 0.935 |  0.941  | ","d1dbd32d":"### Other Quality Criteria for Splits in Classification Problems","2e0a8901":"Let's try to separate these two classes by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.\n\nIntentemos separar estas dos clases entrenando un \u00e1rbol de decisi\u00f3n `Sklearn`. Usaremos el par\u00e1metro `max_depth` que limita la profundidad del \u00e1rbol. Vamos a visualizar el l\u00edmite de separaci\u00f3n resultante.","0e69a367":"#### How can we \"read\" such a tree?\n \nIn the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, $S=1$. Then, the first partition of the samples into 2 groups was made by comparing the value of $x_2$ with $1.211$ (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.\n\n------\n\n#### \u00bfC\u00f3mo podemos \"leer\" tal \u00e1rbol?\n\u00a0\nAl principio, hab\u00eda 200 muestras (instancias), 100 de cada clase. La entrop\u00eda del estado inicial fue m\u00e1xima, $ S = 1 $. Luego, la primera partici\u00f3n de las muestras en 2 grupos se realiz\u00f3 al comparar el valor de $ x_2 $ con $ 1.211 $ (encuentre esta parte del borde en la imagen de arriba). Con eso, la entrop\u00eda de los grupos de izquierda y derecha disminuy\u00f3. El proceso contin\u00faa hasta la profundidad 3. En esta visualizaci\u00f3n, cuantas m\u00e1s muestras de la primera clase, m\u00e1s oscuro es el color naranja del v\u00e9rtice; Cuantas m\u00e1s muestras de la segunda clase, m\u00e1s oscuro es el azul. Al principio, el n\u00famero de muestras de dos clases es igual, por lo que el nodo ra\u00edz del \u00e1rbol es blanco.","6e0d4135":"### Complex Case for the Nearest Neighbors Method\nLet's consider another simple example. In the classification problem, one of the features will just be proportional to the vector of responses, but this won't help for the nearest neighbors method.\n\n-------------\n\n### Caso complejo para el m\u00e9todo de vecinos m\u00e1s cercanos\nConsideremos otro ejemplo simple. En el problema de clasificaci\u00f3n, una de las caracter\u00edsticas ser\u00e1 proporcional al vector de respuestas, pero esto no ayudar\u00e1 al m\u00e9todo de vecinos m\u00e1s cercanos.","d9c14a19":"As always, we will look at the accuracy for cross-validation and the hold-out set. Let's construct curves reflecting the dependence of these quantities on the `n_neighbors` parameter in the method of nearest neighbors. These curves are called validation curves.\n\nOne can see that k-NN with the Euclidean distance does not work well on the problem, even when you vary the number of nearest neighbors over a wide range. In contrast, the decision tree easily \"detects\" hidden dependencies in the data despite a restriction on the maximum depth.\n\n------------\n\nComo siempre, veremos la precisi\u00f3n de la validaci\u00f3n cruzada y el conjunto de retenci\u00f3n. Construyamos curvas que reflejen la dependencia de estas cantidades en el par\u00e1metro `n_neighbors` en el m\u00e9todo de vecinos m\u00e1s cercanos. Estas curvas se llaman curvas de validaci\u00f3n.\n\nSe puede ver que k-NN con la distancia euclidiana no funciona bien en el problema, incluso cuando se var\u00eda el n\u00famero de vecinos m\u00e1s cercanos en un amplio rango. En contraste, el \u00e1rbol de decisi\u00f3n \"detecta\" f\u00e1cilmente las dependencias ocultas en los datos a pesar de una restricci\u00f3n en la profundidad m\u00e1xima.","0a0fa3bf":"### Class `KNeighborsClassifier` in Scikit-learn\nThe main parameters of the class `sklearn.neighbors.KNeighborsClassifier` are:\n- weights: `uniform` (all weights are equal), `distance` (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;\n- algorithm (optional): `brute`, `ball_tree `, `KD_tree`, or `auto`. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to `auto`, the right way to find the neighbors will be automatically chosen based on the training set.\n- leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;\n- metric: `minkowski`, `manhattan `, `euclidean`, `chebyshev`, or other.\n\n-------------\n\n### Clase `KNeighborsClassifier` en Scikit-learn\nLos par\u00e1metros principales de la clase `sklearn.neighbors.KNeighborsClassifier` son:\n- pesos: `uniforme` (todos los pesos son iguales),` distancia` (el peso es inversamente proporcional a la distancia de la muestra de prueba), o cualquier otra funci\u00f3n definida por el usuario;\n- algoritmo (opcional): `brute`,` ball_tree `,` KD_tree` o `auto`. En el primer caso, los vecinos m\u00e1s cercanos para cada caso de prueba se calculan mediante una b\u00fasqueda de cuadr\u00edcula sobre el conjunto de entrenamiento. En el segundo y tercer caso, las distancias entre los ejemplos se almacenan en un \u00e1rbol para acelerar la b\u00fasqueda de vecinos m\u00e1s cercanos. Si establece este par\u00e1metro en `auto`, la forma correcta de encontrar a los vecinos se elegir\u00e1 autom\u00e1ticamente en funci\u00f3n del conjunto de entrenamiento.\n- leaf_size (opcional): umbral para cambiar a la b\u00fasqueda de cuadr\u00edcula si el algoritmo para encontrar vecinos es BallTree o KDTree;\n- m\u00e9trico: `minkowski`,` manhattan `,` euclidean`, `chebyshev`, u otro.","42b95748":"\n## 2. Decision Tree \nWe begin our overview of classification and regression methods with one of the most popular ones \u2013 a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.\n\n## 2. \u00c1rbol de decisiones\nComenzamos nuestra descripci\u00f3n general de los m\u00e9todos de clasificaci\u00f3n y regresi\u00f3n con uno de los m\u00e1s populares: un \u00e1rbol de decisiones. Los \u00e1rboles de decisi\u00f3n se utilizan en las decisiones de la vida cotidiana, no solo en el aprendizaje autom\u00e1tico. Los diagramas de flujo son en realidad representaciones visuales de \u00e1rboles de decisi\u00f3n. Por ejemplo, la Escuela Superior de Econom\u00eda publica diagramas de informaci\u00f3n para facilitar la vida de sus empleados. Aqu\u00ed hay un fragmento de instrucciones para publicar un art\u00edculo en el portal de la instituci\u00f3n.","c6e22a88":"### The nearest neighbors method\n\nPros:\n- Simple implementation;\n- Well studied;\n- Typically, the method is a good first solution not only for classification or regression, but also recommendations;\n- It can be adapted to a certain problem by choosing the right metrics or kernel (in a nutshell, the kernel may set the similarity operation for complex objects such as graphs while keeping the k-NN approach the same). By the way, [Alexander Dyakonov](https:\/\/www.kaggle.com\/dyakonov), a former top-1 kaggler, loves the simplest k-NN but with the tuned object similarity metric;\n- Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates (\"We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset\").\n\nCons:\n- Method considered fast in comparison with compositions of algorithms, but the number of neighbors used for classification is usually large (100-150) in real life, in which case the algorithm will not operate as fast as a decision tree.\n- If a dataset has many variables, it is difficult to find the right weights and to determine which features are not important for classification\/regression.\n- Dependency on the selected distance metric between the objects. Selecting the Euclidean distance by default is often unfounded. You can find a good solution by grid searching over parameters, but this becomes very time consuming for large datasets.\n- There are no theoretical ways to choose the number of neighbors \u2013 only grid search (though this is often true for all hyperparameters of all models). In the case of a small number of neighbors, the method is sensitive to outliers, that is, it is inclined to overfit.\n- As a rule, it does not work well when there are a lot of features due to the \"curse of dimensionality\". Professor Pedro Domingos, a well-known member in the ML community, talks about it [here](https:\/\/homes.cs.washington.edu\/~pedrod\/papers\/cacm12.pdf) in his popular paper, \"A Few Useful Things to Know about Machine Learning\"; also \"the curse of dimensionality\" is described in the Deep Learning book in [this chapter](http:\/\/www.deeplearningbook.org\/contents\/ml.html).\n\nThis is a lot of information, but, hopefully, this article will be a great reference for you for a long time :)\n\n-----------------\n\n### El m\u00e9todo de vecinos m\u00e1s cercanos\n\nPros:\n- Implementaci\u00f3n simple;\n- bien estudiado;\n- T\u00edpicamente, el m\u00e9todo es una buena primera soluci\u00f3n no solo para la clasificaci\u00f3n o regresi\u00f3n, sino tambi\u00e9n para las recomendaciones;\n- Se puede adaptar a un determinado problema seleccionando las m\u00e9tricas o el kernel correctos (en pocas palabras, el kernel puede configurar la operaci\u00f3n de similitud para objetos complejos, como gr\u00e1ficos, mientras mantiene el enfoque de k-NN igual). Por cierto, [Alexander Dyakonov] (https:\/\/www.kaggle.com\/dyakonov), un antiguo kaggler top-1, ama el k-NN m\u00e1s simple pero con la m\u00e9trica de similitud de objetos sintonizados;\n- Buena interpretabilidad. Hay excepciones: si el n\u00famero de vecinos es grande, la capacidad de interpretaci\u00f3n se deteriora (\"No le otorgamos un pr\u00e9stamo, porque es similar a los 350 clientes, de los cuales 70 son malos, y eso es 12% m\u00e1s alto que el promedio para el conjunto de datos \").\n\nContras:\n- M\u00e9todo considerado r\u00e1pido en comparaci\u00f3n con las composiciones de algoritmos, pero el n\u00famero de vecinos utilizados para la clasificaci\u00f3n suele ser grande (100-150) en la vida real, en cuyo caso el algoritmo no funcionar\u00e1 tan r\u00e1pido como un \u00e1rbol de decisiones.\n- Si un conjunto de datos tiene muchas variables, es dif\u00edcil encontrar los pesos correctos y determinar qu\u00e9 caracter\u00edsticas no son importantes para la clasificaci\u00f3n \/ regresi\u00f3n.\n- Dependencia de la m\u00e9trica de distancia seleccionada entre los objetos. La selecci\u00f3n de la distancia euclidiana por defecto a menudo es infundada. Puede encontrar una buena soluci\u00f3n si la cuadr\u00edcula busca par\u00e1metros, pero esto requiere mucho tiempo para grandes conjuntos de datos.\n- No hay formas te\u00f3ricas de elegir el n\u00famero de vecinos, solo la b\u00fasqueda en cuadr\u00edcula (aunque esto es a menudo cierto para todos los hiperpar\u00e1metros de todos los modelos). En el caso de un peque\u00f1o n\u00famero de vecinos, el m\u00e9todo es sensible a los valores at\u00edpicos, es decir, est\u00e1 inclinado a adaptarse excesivamente.\n- Como regla general, no funciona bien cuando hay muchas caracter\u00edsticas debido a la \"maldici\u00f3n de la dimensionalidad\". El profesor Pedro Domingos, un conocido miembro de la comunidad de ML, habla sobre esto [aqu\u00ed](https:\/\/homes.cs.washington.edu\/~pedrod\/papers\/cacm12.pdf) en su popular art\u00edculo, \"A Few Cosas \u00fatiles para saber sobre el aprendizaje autom\u00e1tico \"; tambi\u00e9n \"la maldici\u00f3n de la dimensionalidad\" se describe en el libro Deep Learning en [este cap\u00edtulo](http:\/\/www.deeplearningbook.org\/contents\/ml.html).\n\nEsta es una gran cantidad de informaci\u00f3n, pero espero que este art\u00edculo sea una gran referencia para usted durante mucho tiempo :)","4d00d5fc":"### Decision Tree in a Regression Problem\n\nWhen predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes: \n\n- Variance around the mean: \n\n$$\\Large D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} y_i)^2, $$\n\nwhere $\\ell$ is the number of samples in a leaf, $y_i$ is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.\n\n------\n\n### \u00c1rbol de decisi\u00f3n en un problema de regresi\u00f3n\n\nAl predecir una variable num\u00e9rica, la idea de una construcci\u00f3n de \u00e1rbol sigue siendo la misma, pero los criterios de calidad cambian:\n\n- Varianza en torno a la media:\n\n$$\\Large D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} y_i)^2, $$\n\ndonde $\\ell$ es el n\u00famero de muestras en una hoja, $ y_i $ es el valor de la variable objetivo. En pocas palabras, al minimizar la variaci\u00f3n alrededor de la media, buscamos caracter\u00edsticas que dividan el conjunto de entrenamiento de tal manera que los valores de la caracter\u00edstica objetivo en cada hoja sean aproximadamente iguales.","f6e0f348":"The **conclusion** of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors (next time we will also add logistic regression to this list). It might be the case that these methods already work well enough.\n\n-------------\n\nLa **conclusi\u00f3n** de este experimento (y consejos generales): primero verifique modelos simples en sus datos: \u00e1rbol de decisiones y vecinos m\u00e1s cercanos (la pr\u00f3xima vez tambi\u00e9n agregaremos regresi\u00f3n log\u00edstica a esta lista). Puede ser que estos m\u00e9todos ya funcionen lo suficientemente bien.","91444379":"Now let\u2019s make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random parameters.  \n\nAhora vamos a hacer predicciones sobre nuestro conjunto de holdout. Podemos ver que k-NN lo hizo mucho mejor, pero tenga en cuenta que esto es con par\u00e1metros aleatorios.","7ef34e6d":"#### Entropy\nShannon's entropy is defined for a system with N possible states as follows:\n\nLa entrop\u00eda de Shannon se define para un sistema con N estados posibles de la siguiente manera:\n\n$$\\Large S = -\\sum_{i=1}^{N}p_i \\log_2{p_i},$$","26985145":"**Conclusion**: the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes.\n\n**Conclusi\u00f3n**: la heur\u00edstica m\u00e1s simple para manejar caracter\u00edsticas num\u00e9ricas en un \u00e1rbol de decisi\u00f3n es ordenar sus valores en orden ascendente y verificar solo aquellos umbrales donde el valor de la variable objetivo cambia.","728f01c2":"That has already passed 66% but not quite 97%. k-NN works better on this dataset. In the case of one nearest neighbour, we were able to reach 99% guesses on cross-validation.  \n\n-----------------\n\nEso ya ha pasado el 66% pero no del 97%. k-NN funciona mejor en este conjunto de datos. En el caso de un vecino m\u00e1s cercano, pudimos alcanzar un 99% de conjeturas en la validaci\u00f3n cruzada.","014962c3":"The most common ways to deal with overfitting in decision trees are as follows:\n- artificial limitation of the depth or a minimum number of samples in the leaves: the construction of a tree just stops at some point;\n- pruning the tree.\n\nLas formas m\u00e1s comunes de lidiar con el sobreajuste en los \u00e1rboles de decisi\u00f3n son las siguientes:\n- Limitaci\u00f3n artificial de la profundidad o un n\u00famero m\u00ednimo de muestras en las hojas: la construcci\u00f3n de un \u00e1rbol simplemente se detiene en alg\u00fan punto;\n- podando el arbol.","a966c80e":"where $q$ is the number of groups after the split, $N_i$ is number of objects from the sample in which variable $Q$ is equal to the $i$-th value. In our example, our split yielded two groups ($q = 2$), one with 13 elements ($N_1 = 13$), the other with 7 ($N_2 = 7$). Therefore, we can compute the information gain as\n\ndonde $q$ es el n\u00famero de grupos despu\u00e9s de la divisi\u00f3n, $ N_i $ es el n\u00famero de objetos de la muestra en la que la variable $Q$ es igual al valor de $i$ -th. En nuestro ejemplo, nuestra divisi\u00f3n produjo dos grupos ($q = 2$), uno con 13 elementos ($N_1 = 13$) y el otro con 7($N_2 = 7$). Por lo tanto, podemos calcular la ganancia de informaci\u00f3n como","b92ab241":"<img align='center' src='https:\/\/habrastorage.org\/files\/f9f\/3b5\/133\/f9f3b5133bae460ba96ab7e546155b1d.png'><br>","9c7f59ec":"Let's sort it by age in ascending order.  \/ Vamos a ordenarlo por edad en orden ascendente.","4f4cdc37":"## 5. Application Examples and Complex Cases\n\n### Decision trees and nearest neighbors method in a customer churn prediction task \n\nLet's read data into a `DataFrame` and preprocess it. Store *State* in a separate `Series` object for now and remove it from the dataframe. We will\n train the first model without the *State* feature, and then we will see if it helps. \n \n -----\n \n ## 5. Ejemplos de aplicaci\u00f3n y casos complejos\n\n### \u00c1rboles de decisi\u00f3n y m\u00e9todo de vecinos m\u00e1s cercanos en una tarea de predicci\u00f3n de rotaci\u00f3n de clientes\n\nVamos a leer los datos en un `DataFrame` y preprocesarlos. Almacene * State * en un objeto `Series` separado por ahora y elim\u00ednelo del marco de datos. Lo haremos\n\u00a0 entrene el primer modelo sin la funci\u00f3n * Estado *, y luego veremos si ayuda.","08c0b9b0":"We see that the decision tree approximates the data with a piecewise constant function.\n\nVemos que el \u00e1rbol de decisi\u00f3n se aproxima a los datos con una funci\u00f3n constante por partes.","0ad7ed4f":"###  Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition\n\nNow let's have a look at how these 2 algorithms perform on a real-world task. We will use the `sklearn` built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.\n \nPictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is \u200b\u200b\"unfolded\" into a vector of length 64, and we obtain a feature description of an object.\n \nLet's draw some handwritten digits. We see that they are distinguishable.\n\n---------------------------\n\n### \u00c1rboles de decisi\u00f3n y k-NN en una tarea de reconocimiento de d\u00edgitos manuscritos MNIST\n\nAhora veamos c\u00f3mo se comportan estos 2 algoritmos en una tarea del mundo real. Usaremos el conjunto de datos incorporado `sklearn` en d\u00edgitos escritos a mano. Esta tarea es un ejemplo en el que k-NN funciona sorprendentemente bien.\n\u00a0\nLas im\u00e1genes aqu\u00ed son matrices 8x8 (intensidad del color blanco para cada p\u00edxel). Luego, cada matriz de este tipo se \u200b\u200b\"despliega\" en un vector de longitud 64, y obtenemos una descripci\u00f3n de la caracter\u00edstica de un objeto.\n\u00a0\nVamos a dibujar algunos d\u00edgitos escritos a mano. Vemos que son distinguibles.","f6346f82":"The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our next topic).\n\nEl m\u00e9todo de un vecino m\u00e1s cercano funciona mejor que el \u00e1rbol, pero a\u00fan no es tan bueno como un clasificador lineal (nuestro siguiente tema).","27b515f3":"Let\u2019s train a decision tree and k-NN with our random parameters.\n\nEntrenemos un \u00e1rbol de decisi\u00f3n y k-NN con nuestros par\u00e1metros aleatorios.","0ce831cf":"Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).","be76973e":"According to the nearest neighbors method, the green ball would be classified as \"blue\" rather than \"red\".\n\nSeg\u00fan el m\u00e9todo de los vecinos m\u00e1s cercanos, la bola verde se clasificar\u00eda como \"azul\" en lugar de \"roja\".\n\n<img src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/kNN.png' align='center'><br>","2240ff2b":"$$\\Large IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{N_i}{N}S_i,$$","afc83481":"### Complex Case for Decision Trees\n\nTo continue the discussion of the pros and cons of the methods in question, let's consider a simple classification task, where a tree would perform well but does it in an \"overly complicated\" manner. Let's create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line. \n\n------\n\n### Caso complejo para \u00e1rboles de decisi\u00f3n\n\nPara continuar la discusi\u00f3n de los pros y los contras de los m\u00e9todos en cuesti\u00f3n, consideremos una tarea de clasificaci\u00f3n simple, donde un \u00e1rbol se desempe\u00f1ar\u00eda bien pero lo hace de una manera \"demasiado complicada\". Vamos a crear un conjunto de puntos en un plano (2 caracter\u00edsticas), cada punto ser\u00e1 una de las dos clases (+1 para el rojo, o -1 para el amarillo). Si lo ves como un problema de clasificaci\u00f3n, parece muy simple: las clases est\u00e1n separadas por una l\u00ednea.","9bd19f0f":"### How to Build a Decision Tree\n\nEarlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let's discuss a simple example where all the variables are binary.\n \nRecall the game of \"20 Questions\", which is often referenced when introducing decision trees. You've probably played this game -- one person thinks of a celebrity while the other tries to guess by asking only \"Yes\" or \"No\" questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking \"Is it Angelina Jolie?\" would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking \"Is the celebrity a woman?\" would reduce the possibilities to roughly half. That is to say, the \"gender\" feature separates the celebrity dataset much better than other features like \"Angelina Jolie\", \"Spanish\", or \"loves football.\" This reasoning corresponds to the concept of information gain based on entropy.\n\n-------------------------\n\n### C\u00f3mo construir un \u00e1rbol de decisi\u00f3n\n\nAnteriormente, vimos que la decisi\u00f3n de otorgar un pr\u00e9stamo se toma en funci\u00f3n de la edad, los activos, los ingresos y otras variables. \u00bfPero qu\u00e9 variable mirar primero? Vamos a discutir un ejemplo simple donde todas las variables son binarias.\n\u00a0\nRecuerde el juego de \"20 preguntas\", que a menudo se hace referencia cuando se introducen \u00e1rboles de decisi\u00f3n. Probablemente haya jugado a este juego: una persona piensa en una celebridad mientras que la otra intenta adivinar haciendo solo preguntas \"S\u00ed\" o \"No\". \u00bfQu\u00e9 pregunta har\u00e1 primero el adivinador? Por supuesto, le preguntar\u00e1n cu\u00e1l es el que m\u00e1s reduce el n\u00famero de las opciones restantes. Preguntando \"\u00bfEs Angelina Jolie?\" En el caso de una respuesta negativa, dejar\u00eda a todos menos a una celebridad en el \u00e1mbito de lo posible. En contraste, preguntando \"\u00bfEs la celebridad una mujer?\" Reducir\u00eda las posibilidades a aproximadamente la mitad. Es decir, la caracter\u00edstica \"g\u00e9nero\" separa al conjunto de datos de las celebridades mucho mejor que otras caracter\u00edsticas como \"Angelina Jolie\", \"espa\u00f1ola\" o \"ama el f\u00fatbol\". Este razonamiento corresponde al concepto de ganancia de informaci\u00f3n basado en la entrop\u00eda.","dc3bf289":"### Tree-building Algorithm  \/  Algoritmo de construcci\u00f3n de \u00e1rboles m\u00e1s tarde.","6d8644ec":"We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class \"switches\" from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for \"cutting\" a quantitative variable.\n \nGiven this information, why do you think it makes no sense here to consider a feature like \"Age <17.5\"?\n\n----------\n\nVemos que el \u00e1rbol utiliz\u00f3 los siguientes 5 valores para evaluar por edad: 43,5, 19, 22,5, 30 y 32 a\u00f1os. Si observa detenidamente, estos son exactamente los valores medios entre las edades en las que la clase objetivo \"cambia\" de 1 a 0 o de 0 a 1. Para ilustrar mejor, 43.5 es el promedio de 38 y 49 a\u00f1os; un cliente de 38 a\u00f1os no pudo devolver el pr\u00e9stamo, mientras que el cliente de 49 a\u00f1os lo hizo. El \u00e1rbol busca los valores en los que la clase objetivo cambia su valor como un umbral para \"cortar\" una variable cuantitativa.\n\u00a0\nDada esta informaci\u00f3n, \u00bfpor qu\u00e9 crees que no tiene sentido aqu\u00ed considerar una funci\u00f3n como \"Edad <17.5\"?","f62f0895":"Furthermore, when there are a lot of numeric features in a dataset, each with a lot of unique values, only the top-N of the thresholds described above are selected, i.e. only use the top-N that give maximum gain. The process is to construct a tree of depth 1, compute the entropy (or Gini uncertainty), and select the best thresholds for comparison.\n\nTo illustrate, if we split by \"Salary $\\leq$ 34.5\", the left subgroup will have an entropy of 0 (all clients are \"bad\"), and the right one will have an entropy of 0.954 (3 \"bad\" and 5 \"good\", you can check this yourself as it will be part of the assignment). The information gain is roughly 0.3.\nIf we split by \"Salary $\\leq$ 95\", the left subgroup will have the entropy of 0.97 (6 \"bad\" and 4 \"good\"), and the right one will have the entropy of 0 (a group containing only one object). The information gain is about 0.11.\nIf we calculate information gain for each partition in that manner, we can select the thresholds for comparison of each numeric feature before the construction of a large tree (using all features).\n\nMore examples of numeric feature discretization can be found in posts like [this](http:\/\/kevinmeurer.com\/a-simple-guide-to-entropy-based-discretization\/) or [this](http:\/\/clear-lines.com\/blog\/post\/Discretizing-a-continuous-variable-using-Entropy.aspx). One of the most prominent scientific papers on this subject is \"On the handling of continuous-valued attributes in decision tree generation\" (UM Fayyad. KB Irani, \"Machine Learning\", 1992).\n\n-----------\n\nAdem\u00e1s, cuando hay muchas caracter\u00edsticas num\u00e9ricas en un conjunto de datos, cada una con muchos valores \u00fanicos, solo se selecciona la N superior de los umbrales descritos anteriormente, es decir, solo se usa la N superior que proporciona la m\u00e1xima ganancia. El proceso consiste en construir un \u00e1rbol de profundidad 1, calcular la entrop\u00eda (o incertidumbre de Gini) y seleccionar los mejores umbrales para la comparaci\u00f3n.\n\nPara ilustrar, si dividimos por \"Salario $\\leq$ 34.5\", el subgrupo izquierdo tendr\u00e1 una entrop\u00eda de 0 (todos los clientes son \"malos\"), y el de la derecha tendr\u00e1 una entrop\u00eda de 0.954 (3 \"malos\" y 5 \"bueno\", puede comprobarlo usted mismo ya que formar\u00e1 parte de la tarea). La ganancia de informaci\u00f3n es de aproximadamente 0,3.\nSi dividimos por \"Salario $\\leq$ 95\", el subgrupo izquierdo tendr\u00e1 una entrop\u00eda de 0,97 (6 \"malo\" y 4 \"bueno\"), y el de la derecha tendr\u00e1 la entrop\u00eda de 0 (un grupo que contiene solo uno objeto). La ganancia de informaci\u00f3n es de alrededor de 0.11.\nSi calculamos la ganancia de informaci\u00f3n para cada partici\u00f3n de esa manera, podemos seleccionar los umbrales para la comparaci\u00f3n de cada caracter\u00edstica num\u00e9rica antes de la construcci\u00f3n de un \u00e1rbol grande (usando todas las caracter\u00edsticas).\n\nSe pueden encontrar m\u00e1s ejemplos de discretizaci\u00f3n de funciones num\u00e9ricas en publicaciones como [this] (http:\/\/kevinmeurer.com\/a-simple-guide-to-entropy-based-discretization\/) o [this] (http: \/\/ clear- lines.com\/blog\/post\/Discretizing-a-continuous-variable-using-Entropy.aspx). Uno de los art\u00edculos cient\u00edficos m\u00e1s destacados sobre este tema es \"Sobre el manejo de atributos de valor continuo en la generaci\u00f3n del \u00e1rbol de decisi\u00f3n\" (UM Fayyad. KB Irani, \"Machine Learning\", 1992).","9122c04a":"#### Example\nLet's consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.\n\n#### Ejemplo\nConsideremos ajustar un \u00e1rbol de decisi\u00f3n a algunos datos sint\u00e9ticos. Generaremos muestras de dos clases, ambas distribuciones normales pero con medios diferentes.","329036d0":"### Class DecisionTreeClassifier in Scikit-learn\nThe main parameters of the [`sklearn.tree.DecisionTreeClassifier`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) class are:\n\n- `max_depth` \u2013 the maximum depth of the tree;\n- `max_features` - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be \"expensive\" to search for partitions for *all* features);\n- `min_samples_leaf` \u2013 the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.\n\nThe parameters of the tree need to be set depending on input data, and it is usually done by means of *cross-validation*, more on this below.\n\n-----\n\n### Clase DecisionTreeClassifier en Scikit-learn\nLos par\u00e1metros principales de la clase [`sklearn.tree.DecisionTreeClassifier`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) son:\n\n- `max_depth` - la profundidad m\u00e1xima del \u00e1rbol;\n- `max_features` - el n\u00famero m\u00e1ximo de funciones con las que buscar la mejor partici\u00f3n (esto es necesario con una gran cantidad de funciones porque ser\u00eda\" costoso \"buscar particiones para * todas * las funciones);\n- `min_samples_leaf` - el n\u00famero m\u00ednimo de muestras en una hoja. Este par\u00e1metro evita la creaci\u00f3n de \u00e1rboles donde cualquier hoja tendr\u00eda solo unos pocos miembros.\n\nLos par\u00e1metros del \u00e1rbol deben configurarse en funci\u00f3n de los datos de entrada, y generalmente se realiza mediante * validaci\u00f3n cruzada *, m\u00e1s sobre esto m\u00e1s adelante.","bd6605e8":"Before we dive into the material for this week's article, let's talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell's book *Machine Learning* (1997) gives a classic, general definition of machine learning as follows: \n\n> A computer program is said to learn from experience *E* with respect to some class of tasks *T* and performance measure *P*, if its performance at tasks in *T*, as measured by *P*, improves with experience *E*.\n\nIn the various problem settings *T*, *P*, and *E* can refer to completely different things. Some of the most popular **tasks *T* in machine learning** are the following:\n\n- classification of an instance to one of the categories based on its features;\n- regression \u2013 prediction of a numerical target feature based on other features of an instance;\n- clustering \u2013 identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups;\n- anomaly detection \u2013 search for instances that are \"greatly dissimilar\" to the rest of the sample or to some group of instances;\n- and so many more.\n\nA good overview is provided in the \"Machine Learning basics\" chapter of [\"Deep Learning\"](http:\/\/www.deeplearningbook.org) (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).\n\n**Experience *E* ** refers to data (we can't go anywhere without it). Machine learning algorithms can be divided into those that are trained in *supervised* or *unsupervised* manner. In unsupervised learning tasks, one has a *set* consisting of *instances* described by a set of *features*. In supervised learning problems, there's also a *target variable*, which is what we would like to be able to predict, known for each instance in a *training set*. \n\n-----------------------------------------------------------------------------\n\nAntes de profundizar en el material del art\u00edculo de esta semana, hablemos sobre el tipo de problema que vamos a resolver y su lugar en el apasionante campo del aprendizaje autom\u00e1tico. El libro * Aprendizaje autom\u00e1tico * de T. Mitchell (1997) ofrece una definici\u00f3n general y cl\u00e1sica de aprendizaje autom\u00e1tico de la siguiente manera:\n\n> Se dice que un programa de computadora aprende de la experiencia * E * con respecto a alguna clase de tareas * T * y la medida de rendimiento * P *, si su desempe\u00f1o en las tareas en * T *, medido por * P *, mejora con la experiencia *MI*.\n\nEn las diversas configuraciones de problemas * T *, * P * y * E * pueden referirse a cosas completamente diferentes. Algunas de las m\u00e1s populares ** tareas * T * en aprendizaje autom\u00e1tico ** son las siguientes:\n\n- clasificaci\u00f3n de una instancia a una de las categor\u00edas seg\u00fan sus caracter\u00edsticas;\n- regresi\u00f3n: predicci\u00f3n de una caracter\u00edstica de destino num\u00e9rica basada en otras caracter\u00edsticas de una instancia;\n- agrupamiento - identificaci\u00f3n de particiones de instancias basadas en las caracter\u00edsticas de estas instancias para que los miembros dentro de los grupos sean m\u00e1s similares entre s\u00ed que aquellos en los otros grupos;\n- detecci\u00f3n de anomal\u00edas: busque instancias que sean \"muy diferentes\" del resto de la muestra o de alg\u00fan grupo de instancias;\n- y muchos m\u00e1s.\n\nSe proporciona una buena descripci\u00f3n general en el cap\u00edtulo \"Conceptos b\u00e1sicos del aprendizaje autom\u00e1tico\" de [\"Aprendizaje profundo\"] (http:\/\/www.deeplearningbook.org) (por Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).\n\n** Experiencia * E * ** se refiere a datos (no podemos ir a ning\u00fan lado sin ellos). Los algoritmos de aprendizaje autom\u00e1tico se pueden dividir en aquellos que est\u00e1n capacitados de manera * supervisada * o * no supervisada *. En las tareas de aprendizaje no supervisadas, uno tiene un * conjunto * que consta de * instancias * descrito por un conjunto de * caracter\u00edsticas *. En los problemas de aprendizaje supervisado, tambi\u00e9n hay una * variable objetivo *, que es lo que nos gustar\u00eda poder predecir, conocida para cada instancia en un * conjunto de entrenamiento *.","545f4101":"<center>\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/ods_stickers.jpg\" \/>\n     \n## [mlcourse.ai](mlcourse.ai), open Machine Learning course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), Gleb Filatov, and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","dba4e9ec":"## 3. Nearest Neighbors Method\n\n*The nearest neighbors method* (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.\n\n----------------\n\n## 3. M\u00e9todo de vecinos m\u00e1s cercanos\n\n* El m\u00e9todo de vecinos m\u00e1s cercanos * (k-Vecinos m\u00e1s cercanos, o k-NN) es otro m\u00e9todo de clasificaci\u00f3n muy popular que tambi\u00e9n se usa a veces en problemas de regresi\u00f3n. Este, al igual que los \u00e1rboles de decisi\u00f3n, es uno de los enfoques m\u00e1s comprensibles para la clasificaci\u00f3n. La intuici\u00f3n subyacente es que te pareces a tus vecinos. M\u00e1s formalmente, el m\u00e9todo sigue la hip\u00f3tesis de la compacidad: si la distancia entre los ejemplos se mide lo suficientemente bien, entonces es mucho m\u00e1s probable que ejemplos similares pertenezcan a la misma clase.","72a77bf8":"Let's list the best parameters and the corresponding mean accuracy from cross-validation.\n\nVamos a enumerar los mejores par\u00e1metros y la precisi\u00f3n media correspondiente de la validaci\u00f3n cruzada.","51baae31":"We got this overly complex construction, although the solution is just a straight line $x_1 = x_2$.\n\nObtuvimos esta construcci\u00f3n demasiado compleja, aunque la soluci\u00f3n es solo una l\u00ednea recta $x_1 = x_2$.","0d66d84a":"In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the \"Higher School of Economics and the Media\") based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.\n \nA decision tree is often a generalization of the experts' experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree. \n\n----------------\n\nEn t\u00e9rminos de aprendizaje autom\u00e1tico, uno puede verlo como un simple clasificador que determina la forma apropiada de publicaci\u00f3n (libro, art\u00edculo, cap\u00edtulo del libro, preimpresi\u00f3n, publicaci\u00f3n en la \"Escuela Superior de Econom\u00eda y Medios de Comunicaci\u00f3n\") seg\u00fan el contenido. (libro, folleto, papel), tipo de revista, tipo de publicaci\u00f3n original (revista cient\u00edfica, actas), etc.\n\u00a0\nUn \u00e1rbol de decisi\u00f3n es a menudo una generalizaci\u00f3n de la experiencia de los expertos, un medio para compartir el conocimiento de un proceso en particular. Por ejemplo, antes de la introducci\u00f3n de algoritmos escalables de aprendizaje autom\u00e1tico, los expertos resolvieron la tarea de calificaci\u00f3n crediticia en el sector bancario. La decisi\u00f3n de otorgar un pr\u00e9stamo se tom\u00f3 sobre la base de algunas reglas derivadas de manera intuitiva (o emp\u00edrica) que podr\u00edan representarse como un \u00e1rbol de decisi\u00f3n.","09946027":"<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/credit_scoring_toy_tree_english.png\" align='center'><br>","bfd1a098":"#### Example\nLet's generate some data distributed by the function $f(x) = e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$ with some noise. Then we will train a tree on it and show what predictions it makes.\n\n#### Ejemplo\nGeneremos algunos datos distribuidos por la funci\u00f3n $ f (x) = e ^ {- x ^ 2} + 1.5 * e ^ {- (x - 2) ^ 2} $ con algo de ruido. Luego entrenaremos un \u00e1rbol en \u00e9l y mostraremos las predicciones que hace."}}