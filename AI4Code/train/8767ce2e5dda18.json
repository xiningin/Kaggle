{"cell_type":{"784d16b0":"code","9f59bfbb":"code","4d805f3d":"code","e8dfb84e":"code","e59b3e6e":"code","8c9f9704":"code","c477ad50":"code","3201fdab":"code","fb869355":"code","8cb3df4e":"code","5a530b4b":"code","05675583":"code","8b425bcd":"code","18816663":"code","20b97286":"code","89cf9303":"code","4fd7c300":"code","0d11dc70":"code","9c37f61a":"code","35ba1611":"code","5b5f8bda":"code","ca166500":"code","a8db7edb":"code","ceca0331":"code","d7dd2f3f":"markdown","49038788":"markdown","e719883e":"markdown","969f2a2a":"markdown","2c761cc8":"markdown","265e908a":"markdown","f8bec950":"markdown","9c0a6a2d":"markdown","3fda9a57":"markdown","becf5b25":"markdown","f4781919":"markdown","c0a1a74e":"markdown","21d706af":"markdown","6cb63775":"markdown","5f563a83":"markdown","b797b0f2":"markdown","6186bffa":"markdown","383d6a23":"markdown","c765f2c0":"markdown","07a2e555":"markdown","4221a493":"markdown","7bfcae41":"markdown","f748baa0":"markdown","aab0b273":"markdown"},"source":{"784d16b0":"from IPython.display import Image\nImage(\"\/kaggle\/input\/online-retail-socgen\/workflow.PNG\")","9f59bfbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport plotly.express as px\nimport pandas_profiling as pp\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom plotly.offline import iplot\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","4d805f3d":"Data=pd.read_csv(\"\/kaggle\/input\/online-retail-socgen\/OnlineRetail.csv\", encoding='iso-8859-1' )\nprint(\"Count of Rows, Columns: \",Data.shape)","e8dfb84e":"report = pp.ProfileReport(Data)\nreport.to_file(\"report.html\")\n\nreport","e59b3e6e":"Data.isnull().sum()","8c9f9704":"Data['Date']=[item[0] for item in Data['InvoiceDate'].str.split()]\nData['Time']=[item[1] for item in Data['InvoiceDate'].str.split()]\nData['Month']=[item[1] for item in Data['Date'].str.split('-')]\nData['Year']=[item[2] for item in Data['Date'].str.split('-')]\nData['TotalCost']=Data['Quantity']*Data['UnitPrice']","c477ad50":"Month={'1':'Jan' , '2':'Feb' , '3':'Mar', '4':'Apr' ,'5':'May' , '6':'Jun' ,\n       '7':'Jul' , '8':'Aug' , '9':'Sep' , '10':'Oct', '11':'Nov' ,'12':'Dec',\n       '01':'Jan' , '02':'Feb' , '03':'Mar', '04':'Apr' ,'05':'May' , '06':'Jun' ,\n       '07':'Jul' , '08':'Aug' , '09':'Sep' }\n\nData=Data.replace({\"Month\": Month})\nData.head()","3201fdab":"Data['Description']=Data.groupby([\"Country\",\"UnitPrice\",\"Date\"])['Description'].transform(lambda x: x.fillna(x.mode()))\nData['Description']=Data['Description'].transform(lambda x: x.fillna(\"Others\"))","fb869355":"df_Non_Null=Data[Data['CustomerID'].isnull()==False].copy()\ndf_Null=Data[Data['CustomerID'].isnull()==True].copy()\n\nprint(\"df_Non_Null Shape : \",df_Non_Null.shape,\"\\ndf_Null Shape : \",df_Null.shape)","8cb3df4e":"Data_Join=df_Non_Null.merge(df_Null,on='InvoiceNo',how='inner')\nData_Join.head()","5a530b4b":"df_Null.head()","05675583":"print(\"min : \",min(df_Non_Null['CustomerID']))\nprint(\"max : \",max(df_Non_Null['CustomerID']))","8b425bcd":"df_Null['InvoiceNo'].nunique()","18816663":"np.random.seed( 30 )\n\ndf_Null['CustomerID']=df_Null.groupby([\"InvoiceNo\"])['CustomerID'].transform(lambda x: x.fillna(np.random.randint(18288,21998,1)[0]))\ndf_Null.head()","20b97286":"frames = [df_Non_Null, df_Null]\n \ndf = pd.concat(frames)\ndf.isnull().sum()","89cf9303":"df.head()","4fd7c300":"def reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            #Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n               NAlist.append(col)\n               df[col].fillna(-999,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] =df[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df,NAlist","0d11dc70":"df,df_Na=reduce_mem_usage(Data)","9c37f61a":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","35ba1611":"df = reduce_mem_usage(Data)","5b5f8bda":"print(\"Around \",np.around(len(df[df[\"Country\"] == 'United Kingdom'])\/len(df)*100),\"% of the data is from United Kingdom\")","ca166500":"print(\"Around \",np.around(sum(n < 0 for n in df.TotalCost.values.flatten())\/len(df)*100),\"% of data is with -ve values\")","a8db7edb":"df[df[\"TotalCost\"] < 0]","ceca0331":"df_list = ['Data', 'df_Non_Null', 'df_Null', 'Data_join', 'frames', 'df', 'df_Na']\ndel df_list\n\nimport gc \ngc.collect()","d7dd2f3f":"## 10.Explain the observations and **prove the assumptions**","49038788":"### An intution here is \"\u00cfdeally the number of distinct InvoiceNo and CustomerID would be same\".   \n\n### df_Non_Null Maximum value is 18287.\n### df_Null unique InvoiceNo  is  3710.\n### So the new random number generator can be in the range of 18288 to 21998. \n\n#### Note: We can take any range apart from  the df_Null's range(Positive Integers Only).","e719883e":"* ### Lets check the \"TotalCost\" to understand which transactions are having -ve values and why? ","969f2a2a":"### For this Dataset there is only 9.3 MB reduced. But when we use this step for large datasets the data size will be minimized to 30% of original size.","2c761cc8":"## 3.Explain the Variables and Make Assumptions\n\n**.** **InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction.  \n**.** **StockCode:** Item code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.  \n**.** **Description:** Item name. Nominal.   \n**.** **Quantity:** The quantities of each Item per transaction. Numeric.  \n**.** **InvoiceDate:** Invice Date and time. Numeric, the day and time when each transaction was generated.  \n**.** **UnitPrice:** Unit price. Numeric.  \n**.** **CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer  \n**.** **Country:** Country name. Nominal, the name of the country where each customer resides.  \n\n#### Assumptions\n\n* The company might be UK-based registered non-store online retail  \n* In **InvoiceNO** variable if the code starts with letter 'c', it indicates a cancellation of orders\n* Product price per unit is in pound sterling","265e908a":"### As there are no matching rows we have imputed NA's with **Random Number Generator** based on **InvoiceNo** groups","f8bec950":"## 1.  On your understanding of the data, what kind of business is this company in?\n\n### Data Set Information: \n   This is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a registered non-store online retail.The company mainly sells unique all-occasion gifts. Many transactions are in bulk and assorted. Many customers of the company are wholesalers. Majority sales share is from UK.\n   \n## Approach\n   1.Import the Data  \n     \n     \n   2.**Univariate Analysis** and Understand the **Data Significance**    \n     \n     \n   3.Explain the Variables and Make **Assumptions**  \n     \n     \n   4.Understand the **Missing Data** Nature  \n     \n     \n   5.Subset the **Null** and **Non-Null** Datasets   \n     \n     \n   6.For **Categorical Variables** - Try joining both the subsets with **InvoiceNo** if there is any lookup value found then fill the NA's with corresponding **CustomerID** value  \n     \n     \n   7.For **Continuous Variables** do localised imputation (based on Country, InvoiceNo, Date, UnitPrice)  \n     \n     \n   8.Else Impute Missing Values based on **data distribution**   \n     \n     \n   9.Data Minification  \n     \n     \n   10.Explain the observations and **prove the assumptions**\n     ","9c0a6a2d":"### CustomerID Imputation:","3fda9a57":"#### Description - has 1454 (0.3%) missing values\n#### CustomerID has 135080 (24.9%) missing values\n\n#### Description Imputation:\n   The better way to impute the Description is filling all the NA's with Localized Description based on (Country, UnitPrice, InvoiceDate)\n#### CustomerID Imputation:\n   Divide the subsets into Null_Set and Non_null set based on following conditions \n     \n   **df_Non_Null=Data[Data['CustomerID'].isnull()==False].copy()**  \n     \n   **df_Null=Data[Data['CustomerID'].isnull()==True].copy()**  \n     \n   Then take the **df_Null** and impute the NA's using **random number generator** aggregated at **InvoiceNo** level(because a single **InvoiceNo** can have only one **CustomerID**). Check the **Range** of **df_Non_Null \"CustomerID\"**. Make sure the randomly generated number should not be repeated and they should not be in the range of **df_Non_Null \"CustomerID\"** values.  ","becf5b25":"## 2.Univariate Analysis\n\nPandas Profiling Generates profile reports from a pandas dataframe. This is very helpful for quick data analysis.  \n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:  \n\n**.** **Essentials:** type, unique values, missing values  \n**.** **Quantile statistics** like minimum value, Q1, median, Q3, maximum, range, interquartile range  \n**.** **Descriptive statistics** like mean, mode, standard deviation, sum, median absolute deviation, coefficient of  variation, kurtosis, skewness  \n**.** **Most frequent values**  \n**.** **Histogram**  \n**.** **Correlations** highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices  \n**.** **Missing values** matrix, count, heatmap and dendrogram of missing values  \n\n\n\nFeel free to hover over the tabs. To understand the **Data Significance** click on **Toggle Details**.","f4781919":"### Understand the **Negative Unit Price** values","c0a1a74e":"### Find the range of **df_Non_Null**. The imputed values that are going to be generated should be exclusive and Non-Repeated.","21d706af":"### All missing values are treated. Now, Lets prove the assumptions by using deepdive analysis.","6cb63775":"### 91.0% of transaction are from UK. So it is highly possible that the base presence of this organisation is in UK.","5f563a83":"### All the -ve \"TotalCost\" are having a prefix \"C\" to the \"InvoiceNo\". We can consider them as Cancelled Orders.\n  \n  \n### Since the company is based on UK. The UnitPrice of the products should be Pound Sterling.","b797b0f2":"## 9. Data Minification\n\n### Reducing the memory of the data\nThe size of the dataset is pretty big, so we are trying to make the dataset smaller without losing information.\n\n** Reason behind memory Reduction: **\n\nInt16: 2 bytes\n\nInt32 and int: 4 bytes\n\nInt64 : 8 bytes\n\nThis is an example how different integer types are occupying the memory. In many cases it is not necessary to represent our integer as int64 and int32 it is just waste of memory. So I am trying to understand the necessaity of every numerical representation and try to convert the unnecessary higher numerical representation to lower one. In that, we can reduce the memory without losing the memory.","6186bffa":"### Description Imputation:","383d6a23":"## 5. Subsetting Data as df_Null and df_Non_Null","c765f2c0":"## 7.For **Continuous Variables** do localised imputation (based on Country, InvoiceNo, Date, UnitPrice)  \n     \n     \n## 8.Else Impute Missing Values based on **data distribution** \n\n### There are no missing values in Continuous Variables","07a2e555":"## 1.Import the Data ","4221a493":"### Join the Subsets to form its original structure.","7bfcae41":"## 6.For **Categorical Variables** - Try joining both the subsets with **InvoiceNo** if there is any lookup value found then fill the NA's with corresponding **CustomerID** value ","f748baa0":"## Feature Engineering\n\nCreate few extra features that supports for Localized data imputation(Time, Month, Year, TotalCost).","aab0b273":"## 4.Understand the **Missing Data** Nature\n\nAs we have already seen the missing values nature in pandas profiling lets check at column level."}}