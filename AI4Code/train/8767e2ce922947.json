{"cell_type":{"6c4f1417":"code","1c451171":"code","9d7af3f5":"code","78c1a0d7":"code","6500bea7":"code","697f02a2":"code","5cce0d32":"code","0e8d1f6c":"code","6c591ba7":"code","d031a673":"code","0434054a":"code","be8cbd1d":"code","cc3e7852":"code","0748e26e":"code","f852d241":"code","65333c3d":"code","49c5420d":"code","ee25847e":"code","c82a41d8":"code","d01e5005":"code","c6f6c9c6":"code","0aa309e2":"code","fdd156cb":"code","0ce4c7e5":"code","d93d254b":"code","ab6dd961":"code","1b652393":"code","2180dc43":"code","b0495682":"code","45b9946a":"code","cf3e141e":"code","6aefbab7":"code","de84b452":"code","9e8a2348":"code","8fd4a7ce":"code","b40438fb":"code","379428a9":"code","8a46c5bd":"code","6ea1a004":"code","fd165076":"code","58f2869d":"code","25d48a16":"markdown","4c7a189d":"markdown","a530733a":"markdown"},"source":{"6c4f1417":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport pickle\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","1c451171":"# constants\nIMG_SIZE = 28\nN_CHANNELS = 1 # because gray scale images","9d7af3f5":"train_df = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv')\npred_df = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/test.csv')","78c1a0d7":"train_df = train_df.append(test_df)","6500bea7":"train_df.head()","697f02a2":"print (f'Training set: {train_df.shape}')\nprint (f'To be Predicted: {pred_df.shape}')","5cce0d32":"X_train = train_df.drop(['label'], axis = 1)\nY_train = train_df['label']\nX_pred = pred_df.drop(['id'], axis = 1)","0e8d1f6c":"X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.15)","6c591ba7":"X_train, X_test, X_pred = X_train.apply(lambda x: x\/255), X_test.apply(lambda x: x\/255), X_pred.apply(lambda x: x\/255)","d031a673":"Y_train, Y_test = pd.get_dummies(Y_train), pd.get_dummies(Y_test)","0434054a":"X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\nX_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\nX_pred = X_pred.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)","be8cbd1d":"print (f'Training images: {X_train.shape}')\nprint (f'Testing images: {X_test.shape}')","cc3e7852":"Y_train = Y_train.to_numpy()","0748e26e":"fig, ax = plt.subplots(nrows=3, ncols=4)\ncount=0\nfor row in ax:\n    for col in row:\n        col.set_title(np.argmax(Y_train[count, :]))\n        col.imshow(X_train[count, :, :, 0])\n        count += 1\nplt.show()","f852d241":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n# This will just calculate parameters required to augment the given data. This won't perform any augmentations\ndatagen.fit(X_train)","65333c3d":"models = [0] * 4 # Model array to store different types of CNN architectures\n\nfor model_type in range(len(models)):\n    models[model_type] = Sequential()\n    models[model_type].add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(28, 28, 1)))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    if model_type > 0:\n        models[model_type].add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n        models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n        models[model_type].add(BatchNormalization(momentum=0.15))\n    elif model_type > 1:\n        models[model_type].add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n        models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n        models[model_type].add(BatchNormalization(momentum=0.15))\n    elif model_type > 2:\n        models[model_type].add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n        models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n        models[model_type].add(BatchNormalization(momentum=0.15))\n    models[model_type].add(Flatten())\n    models[model_type].add(Dense(128, activation = \"relu\"))\n    models[model_type].add(Dense(10, activation = \"softmax\"))","49c5420d":"# Compile all the models\nfor model_type in range(len(models)):\n    models[model_type].compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])","ee25847e":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","c82a41d8":"batch_size=32\nepochs = 25","d01e5005":"histories = [0] * len(models)\nfor model_type in range(len(models)):\n    # Fit the models\n    print (f'### Training model # {model_type}')\n    histories[model_type] = models[model_type].fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                                  epochs = epochs, validation_data = (X_test,Y_test),\n                                  steps_per_epoch=X_train.shape[0] \/\/ batch_size, \n                                  callbacks=[learning_rate_reduction])","c6f6c9c6":"for model_type in range(len(models)):\n    models[model_type].save(f'model_{model_type}.h5')","0aa309e2":"for i in range(len(models)):\n    with open(f'his{i}.pkl', 'wb') as output:\n        pickle.dump(histories[i], output, pickle.HIGHEST_PROTOCOL)","fdd156cb":"%matplotlib inline\ndef PlotLoss(histories, epochs, is_training):\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    for i in range(len(histories)):\n        if is_training:\n            plt.plot(np.arange(0, epochs), histories[i].history[\"loss\"], label=f\"train_loss_{i}\")\n            plt.title(\"Training Loss\")\n        else:\n            plt.plot(np.arange(0, epochs), histories[i].history[\"val_loss\"], label=f\"val_loss_{i}\")\n            plt.title(\"Validation Loss\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"upper right\")\n    plt.show()\n\ndef PlotAcc(histories, epochs, is_training):\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    for i in range(len(histories)):\n        if is_training:\n            plt.plot(np.arange(0, epochs), histories[i].history[\"accuracy\"], label=f\"train_acc_{i}\")\n            plt.title(\"Training Accuracy\")\n        else:\n            plt.plot(np.arange(0, epochs), histories[i].history[\"val_accuracy\"], label=f\"val_acc_{i}\")\n            plt.title(\"Validation Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"lower right\")\n    plt.show()","0ce4c7e5":"PlotLoss(histories, epochs, True)\nPlotLoss(histories, epochs, False)\nPlotAcc(histories, epochs, True)\nPlotAcc(histories, epochs, False)","d93d254b":"models = [0] * 4 # Model array to store different types of CNN architectures\n\nfor model_type in range(len(models)):\n    models[model_type] = Sequential()\n    \n    models[model_type].add(Conv2D(filters=2**(model_type + 3), kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(28, 28, 1)))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Conv2D(filters=2**(model_type + 4), kernel_size=(3, 3), padding='SAME', activation='relu'))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Conv2D(filters=2**(model_type + 5), kernel_size=(3, 3), padding='SAME', activation='relu'))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Flatten())\n    models[model_type].add(Dense(128, activation = \"relu\"))\n    \n    models[model_type].add(Dense(10, activation = \"softmax\"))","ab6dd961":"# Compile all the models\nfor model_type in range(len(models)):\n    models[model_type].compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])","1b652393":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","2180dc43":"batch_size=32\nepochs = 25","b0495682":"histories = [0] * len(models)\nfor model_type in range(len(models)):\n    # Fit the models\n    print (f'### Training model # {model_type}')\n    histories[model_type] = models[model_type].fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                                  epochs = epochs, validation_data = (X_test,Y_test),\n                                  steps_per_epoch=X_train.shape[0] \/\/ batch_size, \n                                  callbacks=[learning_rate_reduction])","45b9946a":"for model_type in range(len(models)):\n    models[model_type].save(f'model_{model_type}.h5')","cf3e141e":"for i in range(len(models)):\n    with open(f'his{i}.pkl', 'wb') as output:\n        pickle.dump(histories[i], output, pickle.HIGHEST_PROTOCOL)","6aefbab7":"PlotLoss(histories, epochs, True)\nPlotLoss(histories, epochs, False)\nPlotAcc(histories, epochs, True)\nPlotAcc(histories, epochs, False)","de84b452":"models = [0] * 6 # Model array to store different types of CNN architectures\n\nfor model_type in range(len(models)):\n    models[model_type] = Sequential()\n    \n    models[model_type].add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(28, 28, 1)))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    models[model_type].add(MaxPool2D(pool_size=(2, 2)))\n    models[model_type].add(BatchNormalization(momentum=0.15))\n    \n    models[model_type].add(Flatten())\n    models[model_type].add(Dense(2**(model_type + 5), activation = \"relu\"))\n    \n    models[model_type].add(Dense(10, activation = \"softmax\"))","9e8a2348":"# Compile all the models\nfor model_type in range(len(models)):\n    models[model_type].compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])","8fd4a7ce":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","b40438fb":"batch_size=32\nepochs = 25","379428a9":"histories = [0] * len(models)\nfor model_type in range(len(models)):\n    # Fit the models\n    print (f'### Training model # {model_type}')\n    histories[model_type] = models[model_type].fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                                  epochs = epochs, validation_data = (X_test,Y_test),\n                                  steps_per_epoch=X_train.shape[0] \/\/ batch_size, \n                                  callbacks=[learning_rate_reduction])","8a46c5bd":"for model_type in range(len(models)):\n    models[model_type].save(f'model_{model_type}.h5')\n    with open(f'his{model_type}.pkl', 'wb') as output:\n        pickle.dump(histories[model_type], output, pickle.HIGHEST_PROTOCOL)","6ea1a004":"PlotLoss(histories, epochs, True)\nPlotLoss(histories, epochs, False)\nPlotAcc(histories, epochs, True)\nPlotAcc(histories, epochs, False)","fd165076":"preds = models[3].predict(X_pred)","58f2869d":"pred_df['label'] = np.argmax(preds, axis=1)\npreds = pred_df[['id', 'label']]\npreds.to_csv('sub.csv', index=False)","25d48a16":"### Model #2 seems promising. Now search for number of filters in each convolutional layer","4c7a189d":"### Model #2 seems promising. Filter sizes: 32, 64 and 128. Now search for better size of fully connected layer","a530733a":"### 256 hidden layers looks promising, creating submission"}}