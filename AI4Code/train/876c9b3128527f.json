{"cell_type":{"ce2b74e2":"code","017d1955":"code","7c43302c":"code","8d0ae65e":"code","610602d5":"code","a5842b50":"code","0462ef7c":"code","d2c52ded":"code","5ed78459":"code","fb90df9a":"code","516d9ea7":"code","1cbade24":"code","a96bc422":"code","7322c6b3":"code","ea99c5ce":"code","c0e2346c":"code","ecd393dd":"code","9652e990":"code","475f029f":"code","108c4b5b":"code","771d4dbf":"code","78b01718":"code","df6be0e9":"code","4b95fabd":"code","ee29e7a6":"code","3993f240":"code","ffd0630e":"code","915e6c17":"code","8bcc8761":"code","185e0a5a":"code","a0e6ea5a":"code","6b6397bf":"code","c6d61503":"code","7cf67c17":"code","0a812c1f":"code","168bc541":"code","0b9dc1ed":"code","1d7b09be":"code","8ce75bfb":"code","2682b264":"code","270a3ab6":"code","40a528dc":"code","2020bafa":"markdown","b5b25603":"markdown"},"source":{"ce2b74e2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","017d1955":"# install packs\n!pip install xlrd\n!pip install openpyxl","7c43302c":"# import libs\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","8d0ae65e":"# import to dataframe\ndata = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\ndata.head()","610602d5":"data.describe()","a5842b50":"data['ICU']","0462ef7c":"data['ICU'].value_counts()","d2c52ded":"data.groupby('PATIENT_VISIT_IDENTIFIER', as_index=False).fillna('bfill').fillna('ffill')","5ed78459":"((data.groupby('PATIENT_VISIT_IDENTIFIER').fillna('bfill').fillna('ffill') ==\n  data.groupby('PATIENT_VISIT_IDENTIFIER', as_index=False).fillna('bfill').fillna('ffill')).any()==False).sum()","fb90df9a":"((data.groupby('PATIENT_VISIT_IDENTIFIER').fillna('bfill').fillna('ffill') ==\n  data.groupby('PATIENT_VISIT_IDENTIFIER', as_index=True).fillna('bfill').fillna('ffill')).any()==False).sum()","516d9ea7":"def preenche_tabela(dados):\n    features_continuas_colunas = dados.iloc[:,13:-2].columns\n    features_continuas = dados.groupby('PATIENT_VISIT_IDENTIFIER', as_index=False)[features_continuas_colunas].fillna(method='bfill').fillna(method='ffill')\n    features_categoricas = dados.iloc[:,:13]\n    saida = dados.iloc[:,-2:]\n    dados_finais = pd.concat([features_categoricas,features_continuas, saida], ignore_index = True, axis=1)\n    dados_finais.columns = dados.columns\n    return dados_finais\n    ","1cbade24":"dados_limpos = preenche_tabela(data)\na_remover = dados_limpos.query('WINDOW == \"0-2\" and ICU ==1')['PATIENT_VISIT_IDENTIFIER'].values\ndados_limpos = dados_limpos.query('PATIENT_VISIT_IDENTIFIER not in @a_remover')\ndados_limpos = dados_limpos.dropna()\ndados_limpos.describe()","a96bc422":"def prepare_window(rows):\n    if (np.any(rows['ICU'])):\n        rows.loc[rows['WINDOW'] == '0-2','ICU']=1\n    return rows.loc[rows['WINDOW']=='0-2']\n\ndados_limpos = dados_limpos.groupby('PATIENT_VISIT_IDENTIFIER').apply(prepare_window)\ndados_limpos.head()","7322c6b3":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn .metrics import accuracy_score\n\nnp.random.seed(73246)\n\nx_columns = data.describe().columns\ny = dados_limpos['ICU']\nx = dados_limpos[x_columns].drop('ICU', axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y)\nmodelo = DummyClassifier()\nmodelo.fit(x_train, y_train)\n\ny_prediction = modelo.predict(x_test)\naccuracy_score(y_test, y_prediction)","ea99c5ce":"y_test.describe()","c0e2346c":"modelo = LogisticRegression(max_iter=10000)\nmodelo.fit(x_train, y_train)\n\ny_prediction = modelo.predict(x_test)\naccuracy_score(y_test, y_prediction)","ecd393dd":"# verificar se existem colunas que n\u00e3o est\u00e3o no describe\ndata.columns.drop(data.describe().columns)","9652e990":"# Desafio 01: Transformar a coluna \"AGE_PERCENTIL\" em m\u00e9todos categ\u00f3ricos e \n# Desafio 02: verificar o impacto da muda\u00e7a nos resultados.\n# verificar dados em \"AGE_PERCENTIL\"\nkeys = data.AGE_PERCENTIL.sort_values().unique()\nvals = np.arange(110,10,-10)\n# montar dicion\u00e1rio\nages = dict(zip(keys,vals))\nages","475f029f":"np.random.seed(73246)\ndados_limpos_1 = dados_limpos.copy()\ndados_limpos_1['AGE_PERCENTIL'] = dados_limpos_1['AGE_PERCENTIL'].map(ages)\ndados_limpos_1 = dados_limpos_1.dropna()\n\nx_columns = data.columns\ny = dados_limpos_1['ICU']\nx = dados_limpos_1[x_columns].drop(['ICU','WINDOW'] , axis=1)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y)\n\nmodelo = LogisticRegression(max_iter=10000)\nmodelo.fit(x_train, y_train)\n\ny_prediction = modelo.predict(x_test)\naccuracy_score(y_test, y_prediction)","108c4b5b":"np.random.seed(73246)\ndados_limpos_1 = dados_limpos.copy()\ndados_limpos_1['AGE_PERCENTIL'] = dados_limpos_1['AGE_PERCENTIL'].astype('category').cat.codes\ndados_limpos_1 = dados_limpos_1.dropna()\n\nx_columns = data.columns\ny = dados_limpos_1['ICU']\nx = dados_limpos_1[x_columns].drop(['ICU','WINDOW'] , axis=1)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y)\n\nmodelo = LogisticRegression(max_iter=10000)\nmodelo.fit(x_train, y_train)\n\ny_prediction = modelo.predict(x_test)\naccuracy_score(y_test, y_prediction)","771d4dbf":"from sklearn.tree import DecisionTreeClassifier\n\nmodelo_arvore = DecisionTreeClassifier()\nmodelo_arvore.fit(x_train, y_train)\ny_predicao_arvore = modelo_arvore.predict(x_test)\naccuracy_score(y_test, y_predicao_arvore)","78b01718":"from sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt\n\nplot_confusion_matrix(modelo_arvore, x_test, y_test)","df6be0e9":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_predicao_arvore))","4b95fabd":"VP = 25\nVN = 32\nFP = 15\nFN = 16","ee29e7a6":"precision = VP\/(VP+FP)\nprecision","3993f240":"recall = VP\/(VP+FN)\nrecall","ffd0630e":"## Plotar decision tree\n# representa\u00e7\u00e3o de texto\n# ref.: https:\/\/mljar.com\/blog\/visualize-decision-tree\/\nfrom sklearn import tree\ntext_representation = tree.export_text(modelo_arvore)\nprint(text_representation)\n","915e6c17":"### Curva ROC e AUC\n# AUC - \u00c9 uma m\u00e9trica que calcula a \u00e1rea sob a curva.\n# AUC - \u00c9 a curva da taxa de verdadeiros positivos sobre falsos positivos\n# Quanto mais pr\u00f3ximo de 1 melhor \u00e9 o resultado\nfrom sklearn.metrics import roc_auc_score\n\n# probabilidade da classifica\u00e7\u00e3o de x_test\nprob_arvore = modelo_arvore.predict_proba(x_test)\nroc_auc_score(y_test.values, prob_arvore[:,1])","8bcc8761":"import  matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve\n\ndef roda_modelo(modelo,dados):\n    # definir dados que ser\u00e3o avaliados\n    x_columns = dados.columns\n    y = dados['ICU']\n    X = dados[x_columns].drop(['ICU', 'WINDOW'], axis=1)\n    \n    # separa\u00e7\u00e3o dos dados em treino e teste\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n    # treinar modelo\n    modelo.fit(X_train, y_train)\n    # realizar predi\u00e7\u00e3o\n    predicao = modelo.predict(X_test)\n    # verificar m\u00e9trica do modelo\n    proba = modelo.predict_proba(X_test)\n    auc = roc_auc_score(y_test, proba[:,1])\n    print(f'AUC {auc} \\n')\n    print('Classification Report')\n    print(classification_report(y_test, predicao))\n\n    plot_roc_curve(modelo, X_test, y_test)\n    plt.plot([0,1],[0,1], '--', color='gray')\n\n    \n    return predicao, auc, proba","185e0a5a":"predicao, auc, proba = roda_modelo(modelo_arvore, dados_limpos_1)","a0e6ea5a":"def roda_n_modelo(modelo,dados, n):\n    \n    # definir dados que ser\u00e3o avaliados\n    x_columns = dados.columns\n    y = dados['ICU']\n    X = dados[x_columns].drop(['ICU', 'WINDOW'], axis=1)\n    \n    auc_list = []\n    for _ in range(n):\n    # separa\u00e7\u00e3o dos dados em treino e teste\n        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n    # treinar modelo\n        modelo.fit(X_train, y_train)\n    # verificar m\u00e9trica do modelo\n        proba = modelo.predict_proba(X_test)\n        auc = roc_auc_score(y_test, proba[:,1])\n        \n        auc_list.append(auc)\n    \n    auc_mean = np.mean(auc_list)\n    auc_std = np.std(auc_list)\n    print(f'AUC -> {auc_mean-auc_std} - {auc_mean+auc_std}\\n')\n    return auc_mean, auc_std","6b6397bf":"roda_n_modelo(modelo_arvore, dados_limpos_1,100)","c6d61503":"roda_n_modelo(modelo_arvore, dados_limpos_1,1000)","7cf67c17":"auc_mean_hist = []\nauc_std_hist = []\nfor i in np.arange(1,200,10):\n    auc_mean, auc_std = roda_n_modelo(modelo_arvore, dados_limpos_1,i)\n    \n    auc_mean_hist.append(auc_mean)\n    auc_std_hist.append(auc_std)","0a812c1f":"plt.plot(np.arange(1,200,10),auc_mean_hist)","168bc541":"# testando com o modelo de regress\u00e3o logistica\n\nroda_n_modelo(modelo, dados_limpos_1,50)","0b9dc1ed":"from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold\n\ncv = StratifiedKFold(n_splits=5, shuffle=True)\ncross_validate(modelo,x,y,cv=cv)\n","1d7b09be":"from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\ncross_validate(modelo,x,y,cv=cv)","8ce75bfb":"def roda_modelo_cv(modelo,dados, n_splits, n_repeats):\n    \n    np.random.seed(3145)\n    #dados = dados.sample(frac=1).reset_index(drop=True)\n    # definir dados que ser\u00e3o avaliados\n    x_columns = dados.columns\n    y = dados['ICU']\n    X = dados[x_columns].drop(['ICU', 'WINDOW'], axis=1)\n\n    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n    resultados = cross_validate(modelo,x,y,cv=cv, scoring='roc_auc')\n\n    auc_mean = resultados['test_score'].mean()\n    auc_std = resultados['test_score'].std()\n    auc = roc_auc_score(y_test, proba[:,1])\n\n    \n    print(f'AUC_mean -> {auc_mean}\\n')\n    print(f'AUC -> {auc_mean - auc_std} -- {auc_mean + auc_std} \\n')","2682b264":"roda_modelo_cv(modelo, dados_limpos_1, n_splits=5, n_repeats=10)","270a3ab6":"roda_n_modelo(modelo, dados_limpos_1, 50)","40a528dc":"roda_modelo_cv(modelo_arvore, dados_limpos_1, n_splits=5, n_repeats=10)","2020bafa":"Verdadeiro positivo -> previ que era positivo e o resultado \u00e9 positivo\n\nVerdadeiro negativo -> previ que era negativo e o resultado \u00e9 negativo\n\nFalso positivo -> Previ que era positivo e o resultado era negativo\n\nFalso negativo -> Previ que era negativo e o resultado era positivo","b5b25603":"Precision: a capacidade de acertar o que \u00e9 positivo dentro dos palpites de valores positivos (o que eu acertei sobre os valores reais)\n\"De todas as minhas classifica\u00e7\u00f5es positivas, quantas s\u00e3o realmente positivas\"\n    \n    -> Precision = VP\/(VP+FP)\n    \n    O precision \u00e9 usado quando o falso positivo pode causar impactos maiores\/piores que o falso negativo\n    \nRecal: a capacidade de acertar os valores positivos dentro daquilo que foi previsto (o que eu acertei sobre o que eu previ)\n\"De todas as classes positivas, quantas eu realmente classifiquei corretamente\"\nrecall == 1 -> fui capaz de classificar todas as pessoas doentes como realmente doentes\n    \n    -> Recall = VP\/(VP+FN)\n    \n    O Reacal \u00e9 usado quando o falso negativo pode causar impactos maiores\/ piores que o falso positivo\n    \nF1: M\u00e9dia harm\u00f4nica entre Precision e Recall\n\n    -> F1 = 2 * (p * r)\/(p + r)\n    \nO F1 funciona como um alerta mostrando quando h\u00e1 um valor muito baixo de um dos valores anteriores (precision ou recall) \n\nler com mais detalhes:\nhttps:\/\/medium.com\/@shrutisaxena0617\/precision-vs-recall-386cf9f89488#:~:text=Precision%20and%20recall%20are%20two,correctly%20classified%20by%20your%20algorithm."}}