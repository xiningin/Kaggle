{"cell_type":{"9c05dd83":"code","dedb959d":"code","f12d8956":"code","aa24493e":"code","feb9ee67":"code","c904bdc1":"code","1b23a320":"code","38e80328":"code","15cd6f7f":"code","68f76531":"code","feed06e9":"code","412a96b6":"code","957d06f6":"code","377f1498":"code","35646ee3":"code","f78c5883":"code","7ab611be":"code","f11bca46":"code","85d0e0f8":"code","27a89c60":"code","524b5aa1":"code","c638bea1":"code","6203f0b7":"code","34058901":"code","cfc6a2d5":"code","75854181":"code","89ea7a63":"markdown","5a2238c4":"markdown","db918a75":"markdown","ecd15e7b":"markdown","6e158422":"markdown","2fb6407b":"markdown","71064e02":"markdown","082b340a":"markdown","9d144158":"markdown","215d10d6":"markdown","637f5813":"markdown","2556b6bd":"markdown","264c72a4":"markdown","349bbc01":"markdown","63bd4c24":"markdown","af22cca4":"markdown","ba5b49d6":"markdown","a8242ec7":"markdown","791f2924":"markdown","3f032bcf":"markdown","ea034f7b":"markdown","2794b02f":"markdown","dc764783":"markdown","f1431cc4":"markdown","6be66775":"markdown","40fcd9a8":"markdown","b0dd53aa":"markdown","edcdbf32":"markdown","42280583":"markdown","357a4abf":"markdown","25d57de5":"markdown","f7305893":"markdown","e4f0e959":"markdown","7885b645":"markdown","878eda20":"markdown","5d1f2757":"markdown"},"source":{"9c05dd83":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nsns.set_palette('Set2')\nsns.set_theme(style='darkgrid')","dedb959d":"sns.color_palette('Set2')","f12d8956":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","aa24493e":"train.head()","feb9ee67":"train.info()","c904bdc1":"train.describe()","1b23a320":"def check_missing(df):\n    print(bool(df.isnull))","38e80328":"check_missing(train)","15cd6f7f":"missing_values = pd.DataFrame(train.isna().sum())\nmissing_values.rename(columns={0:'missing_value'},inplace=True)\ndef train_missing_perecentage(idx):\n    return (idx\/len(train))*100\nmissing_values['missing_value'] = missing_values.apply(train_missing_perecentage)\nfeatures = list(train.columns)\npercentage = []\nfor i in features:\n    percentage.append(float(missing_values.loc[str(i)]))\nmissing_values = pd.DataFrame({'Feature':features,'Percentage':percentage})","68f76531":"px.scatter(data_frame=missing_values,x='Feature',y='Percentage',template='plotly_dark')","feed06e9":"sns.countplot(x=train.claim,palette='Set2')","412a96b6":"!pip install pandas-profiling\nfrom pandas_profiling import ProfileReport\nprof = ProfileReport(train,minimal=True) \nprof.to_file(output_file='train_output.html')","957d06f6":"features = list(train.columns)\nfeatures.remove('id')\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(train[features].corr(), annot=False, linewidths=.5, ax=ax,cmap=sns.color_palette('Set2',as_cmap=True))","377f1498":"test.head()","35646ee3":"test.info()","f78c5883":"test.describe()","7ab611be":"check_missing(test)","f11bca46":"missing_values = pd.DataFrame(test.isna().sum())\nmissing_values.rename(columns={0:'missing_value'},inplace=True)\ndef test_missing_perecentage(idx):\n    return (idx\/len(test))*100\nmissing_values['missing_value'] = missing_values.apply(test_missing_perecentage)\nfeatures = list(test.columns)\npercentage = []\nfor i in features:\n    percentage.append(float(missing_values.loc[str(i)]))\nmissing_values = pd.DataFrame({'Feature':features,'Percentage':percentage})","85d0e0f8":"px.scatter(data_frame=missing_values,x='Feature',y='Percentage',template='plotly_dark')","27a89c60":"prof = ProfileReport(test,minimal=True) \nprof.to_file(output_file='test_output.html')","524b5aa1":"features = list(test.columns)\nfeatures.remove('id')\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(test[features].corr(), annot=False, linewidths=.5, ax=ax,cmap=sns.color_palette('Set2',as_cmap=True))","c638bea1":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv',index_col='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv',index_col='id')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nfeatures = list(train.columns)\nfeatures.remove('claim')\ntarget = ['claim']","6203f0b7":"params = {\n    'task': 'train',    \n    'objective': 'binary',\n    'verbose':-1,\n    'num_leaves': 111, \n    'learning_rate': 0.016206997849237542, \n    'n_estimators': 2641, \n    'min_child_samples': 22,\n}\nfolds = KFold(n_splits=5,shuffle=True,random_state=42)\nfor fold, (train_idx,valid_idx) in enumerate(folds.split(train)):\n    print(f'fold {fold} starting...')\n    fold_train = train.iloc[train_idx]\n    train_x = fold_train[features]\n    train_y = fold_train[target]\n    dtrain = lgb.Dataset(train_x,label=train_y)\n    \n    fold_valid = train.iloc[valid_idx]\n    valid_x = fold_valid[features]\n    valid_y = fold_valid[target]\n    dvalid = lgb.Dataset(valid_x,valid_y)\n    \n    model = lgb.train(params,train_set=dtrain, \n               valid_sets=dvalid,\n              early_stopping_rounds =200,\n                     verbose_eval=100)\n    oof = model.predict(valid_x)\n    score = roc_auc_score(valid_y,oof)\n    print(f\"Valid score for {fold} is: {score}\")\n    oof = pd.DataFrame({'id':valid_x.index,'claim':oof})\n    oof.to_csv(f'{fold}_oof.csv',index=False)\n    model.save_model(f'lightgbm_{fold}.txt')\n    print(f' fold {fold} completed')","34058901":"for fold in tqdm(range(5)):\n    model = lgb.Booster(model_file=f'.\/lightgbm_{fold}.txt')\n    preds = model.predict(test)\n    submission = sample_submission.copy()\n    submission['claim'] = preds\n    submission.to_csv(f'submission_{fold}.csv',index=False)","cfc6a2d5":"sub0 = pd.read_csv('.\/submission_0.csv')\nsub1 = pd.read_csv('.\/submission_1.csv')\nsub2 = pd.read_csv('.\/submission_2.csv')\nsub3 = pd.read_csv('.\/submission_3.csv')\nsub4 = pd.read_csv('.\/submission_4.csv')\ntarget = (sub0.claim + sub1.claim + sub2.claim + sub3.claim + sub4.claim)\/5\nsub = sub0.copy()\nsub['claim'] = target\nsub.to_csv('submission.csv',index=False)","75854181":"feature_importance =  model.feature_importance()\nfeature_importance = (feature_importance - feature_importance.min())\/(feature_importance.max() - feature_importance.min())\nfeature_names = np.array(train_x.columns)\ndata={'feature_names':feature_names,'feature_importance':feature_importance}\ndf_plt = pd.DataFrame(data)\ndf_plt.sort_values(by=['feature_importance'], ascending=False,inplace=True)\nplt.figure(figsize=(20,40))\nsns.barplot(x=df_plt['feature_importance'], y=df_plt['feature_names'])\nplt.xlabel('FEATURE IMPORTANCE')\nplt.ylabel('FEATURE NAMES')\nplt.show()","89ea7a63":"# Blending","5a2238c4":"<h2><center>Work in Progress ... \u23f3<\/center><\/h2>","db918a75":"let's see how train data looks like","ecd15e7b":"# Importing Libraries","6e158422":"Let's take a look at our target variable:- \"claim\"","2fb6407b":"it's safe to say around 1.6% of data is missing in every feature exception are Id and claim","71064e02":"<div class=\"alert alert-block alert-info\" style=\"font-size:15px; font-family:verdana; line-height: 2.0em;\">\nThis month comptetion is a binary class classificaton in which we have to predict probiblities for   'claim' feature\n<\/div>","082b340a":"# Color Palette","9d144158":"<div class=\"alert alert-block alert-info\" style=\"font-size:15px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udcccHyper Parameter Optimization<br>\n    \ud83d\udcccUsing Catboost, XGboost and maybe Neural Nets<br>\n    \ud83d\udcccFeature Engineering<br>\n    \ud83d\udcccStacking<br>\n    \ud83d\udcccAdvanced Techniques like Psuedo Labelling, Gaussian optimization\n<\/div>","215d10d6":"# Training","637f5813":"Let's check missing value in test set","2556b6bd":"target is evenly distributed so we can simply use K-Fold","264c72a4":"<h2><center>If you learned something new or forked the notebook then please don't forget to upvote<br>Thank You<\/center>\n<\/h2>","349bbc01":"let's generate padnas profiling for test set","63bd4c24":"<div class=\"alert alert-block alert-warning\" style=\"font-size:15px; font-family:verdana; line-height: 2.0em;\">\nOur train set is very diverse so scaling is necessary\n<\/div>","af22cca4":"# EDA","ba5b49d6":"let's see how different features correlate to each other in case of test set","a8242ec7":"let's look at percentage of missing value in case of test set","791f2924":"# Modeling","3f032bcf":"# About the Competition\ud83d\udea9\n<p style=\"font-size:15px\">Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.<br><br>\n\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.<br><br>\n\nFor this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.<br><br>\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n<\/p>","ea034f7b":"# Possible Next Steps:-","2794b02f":"Now let's check our test set","dc764783":"Let's check if missing values are present","f1431cc4":"Let's train a simple LightGBM Classifier and setup our Baseline for rest of the competition ","6be66775":"# Data Description","40fcd9a8":"Hyperparameters are optimized using Optuna they can be further optimized by using better suggestions","b0dd53aa":"# Inference","edcdbf32":"<div class=\"alert alert-block alert-info\" style=\"font-size:15px; font-family:verdana; line-height: 2.0em;\">\nto quickly view detailed EDA on each features we can use pandas profiling\n<\/div>","42280583":"missing values are present we will need to deal with them before modeling","357a4abf":"it's safe to say around 1.6% of data is missing in every feature exception are Id","25d57de5":"Let's check how features correlate with each other","f7305893":"<div class=\"alert alert-block alert-info\" style=\"font-size:15px; font-family:verdana; line-height: 2.0em;\">\nUsually, we keep features that are highly correlated with our target variable and remove the reductant feature but in this case features are not highly correlated to each other\n<\/div>","e4f0e959":"<div style=\"font-size:15px\">\n We are given 3 csv files:-\n<ul>\n    <li><code>train.csv:<\/code> the training set<\/li>\n    <li><code>test.csv:<\/code> the test set<\/li>\n    <li><code>sample_submission.csv:<\/code> sample_submission file in submission format<\/li>\n<\/ul>    \n<\/div>","7885b645":"basic train set statistics","878eda20":"# Feature Importance","5d1f2757":"let's look perecentage of missing value for each feature"}}