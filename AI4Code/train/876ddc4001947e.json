{"cell_type":{"ba510265":"code","0b755507":"code","0f6d8a1a":"code","1eb1e6aa":"code","374988f7":"code","3b015702":"code","38b757f6":"code","745456ac":"code","ac8d1ba1":"code","2486e236":"code","36e50060":"code","c9c08863":"code","ca26fbd6":"code","64d597fc":"code","3ba0573d":"code","7dcd187b":"code","f790fe7a":"code","7119a3cc":"code","f83b7208":"code","f0608e4a":"code","5b0d35a4":"code","ee9f524f":"code","7f0352e6":"markdown"},"source":{"ba510265":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json, pickle\n\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nfrom sklearn.model_selection import KFold\n\nfrom tqdm.notebook import tqdm","0b755507":"from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification","0f6d8a1a":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","1eb1e6aa":"MAX_LENGTH = 300\nNUM_TARGETS = 1\n\nSEED = 321\n\nMODEL_NAME = \"roberta-base\"\n\nMODEL_ROOT = Path(\".\")","374988f7":"TRAIN_BATCH_SIZE = 16\nTRAIN_NUM_WORKERS = 2\n\nVAL_BATCH_SIZE = 20\nVAL_NUM_WORKERS = 2\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device:\", DEVICE)","3b015702":"seed_everything(SEED)\n\n\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\ndf[\"fold\"] = -1\nkf = KFold(n_splits=5, random_state=SEED, shuffle=True)\n\nfor fold, (_, val_set) in enumerate(kf.split(np.arange(len(df)))):\n    df.loc[val_set, \"fold\"] = fold\n\nprint(df.shape)\ndf.head()","38b757f6":"df.fold.value_counts()","745456ac":"def gen_data(model_name=MODEL_NAME):\n    X_input_ids = []\n    X_masks = []\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    with open(MODEL_ROOT\/f\"{MODEL_NAME}-tokenizer.pkl\", \"wb\") as f:\n        pickle.dump(tokenizer, f)\n\n    for excerpt in tqdm(df.excerpt):\n        inp = tokenizer(excerpt, add_special_tokens=True, return_tensors=\"pt\",\n                        max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n        X_input_ids.append(inp[\"input_ids\"])\n        X_masks.append(inp[\"attention_mask\"])\n\n    X_input_ids = torch.cat(X_input_ids)\n    X_masks = torch.cat(X_masks)\n    Y = torch.tensor(df.target.values, dtype=torch.float32)\n    \n    print(X_input_ids.shape, X_masks.shape, Y.shape)\n    \n    return X_input_ids, X_masks, Y","ac8d1ba1":"X_input_ids, X_masks, Y = gen_data()","2486e236":"class CRPDataset(Dataset):\n    def __init__(self, X_input_ids, X_masks, Y, is_train=True):\n        assert X_input_ids.shape == X_masks.shape\n        \n        self.X_input_ids = X_input_ids\n        self.X_masks = X_masks\n        self.Y = Y\n        \n    def __len__(self):\n        return len(self.X_input_ids)\n    \n    def __getitem__(self, idx):\n        return (self.X_input_ids[idx], self.X_masks[idx]), self.Y[[idx]] ","36e50060":"ds = CRPDataset(X_input_ids, X_masks, Y)\nlen(ds)","c9c08863":"(x, x_mask), y = ds[0]\nx.shape, x_mask.shape, y","ca26fbd6":"def get_model(model_name, task=\"token_classification\", num_targets=NUM_TARGETS):\n    task = task.lower()\n        \n    if \"token\" in task:\n        model_instance = AutoModelForTokenClassification\n    elif \"sequence\" in task:\n        model_instance = AutoModelForSequenceClassification\n        \n    model = model_instance.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    config = AutoConfig.from_pretrained(model_name)\n    \n    if hasattr(model, \"classifier\"):\n        model.classifier = nn.Linear(model.classifier.in_features, NUM_TARGETS)\n        \n    return config,tokenizer, model","64d597fc":"class AttentionBlock(nn.Module):\n  def __init__(self, in_features, middle_features, out_features):\n    super().__init__()\n    self.in_features = in_features\n    self.middle_features = middle_features\n    self.out_features = out_features\n\n    self.W = nn.Linear(in_features, middle_features)\n    self.V = nn.Linear(middle_features, out_features)\n\n  def forward(self, features):\n    att = torch.tanh(self.W(features))\n\n    score = self.V(att)\n\n    attention_weights = torch.softmax(score, dim=1)\n\n    context_vector = attention_weights * features\n    context_vector = torch.sum(context_vector, dim=1)\n\n    return context_vector","3ba0573d":"class CRPTokenModel(nn.Module):\n    def __init__(self, model_name=MODEL_NAME, num_targets=NUM_TARGETS, alpha=0.5, p=0.5):\n        super().__init__()\n        self.model_name = model_name\n        self.num_targets = num_targets\n        self.alpha = alpha\n        self.p = p\n        \n        \n        config,tokenizer, model = get_model(model_name, task=\"token_classification\", num_targets=1)\n        \n        self.in_features =  model.classifier.in_features\n        model.classifier = nn.Identity()\n        \n        self.config = config\n        self.tokenizer = tokenizer\n        self.model = model\n        \n        self.att = AttentionBlock(self.in_features, self.in_features, 1)\n        self.fc = nn.Linear(self.in_features, self.num_targets)\n        \n    def forward(self, *args, **kwargs):\n        \n        x = self.model(*args, **kwargs)[\"logits\"]\n        x = self.att(x)\n        \n        x = self.fc(x)\n        return x","7dcd187b":"def one_step( xb, yb, net, criterion, optimizer, scheduler=None):\n  xb, yb = (xb[0].to(DEVICE), xb[1].to(DEVICE)), yb.to(DEVICE)\n        \n  optimizer.zero_grad()\n  o = net(input_ids=xb[0], attention_mask=xb[1])\n  loss = criterion(o, yb)\n  loss.backward()\n  optimizer.step()\n  \n  with torch.no_grad():\n      l = loss.item()\n\n      r2 = r2_score(yb.cpu().numpy(), o.cpu().numpy())\n\n      rmse = torch.sqrt(torch.mean(torch.square(o - yb))).item()\n      mad = torch.mean(torch.abs(o - yb)).item()\n\n  if  scheduler is not None:\n    scheduler.step()\n\n  return l, rmse, mad, r2","f790fe7a":"@torch.no_grad()\ndef evaluate(net, criterion, val_laoder):\n    net.eval()\n\n    os, y = [], []\n    val_laoder = tqdm(val_laoder, leave = False, total=len(val_laoder))\n\n    for icount, (xb, yb) in  enumerate(val_laoder):\n\n        y.append(yb.to(DEVICE))\n\n        xb = (xb[0].to(DEVICE), xb[1].to(DEVICE))\n        o = net(input_ids=xb[0], attention_mask=xb[1])\n\n        os.append(o)\n\n    y = torch.cat(y)\n    o = torch.cat(os)\n\n    l = criterion(o, y).item()\n    \n    r2 = r2_score(y.cpu().numpy(), o.cpu().numpy())\n\n    rmse = torch.sqrt(torch.mean(torch.square(o - y))).item()\n    mad = torch.mean(torch.abs(o - y)).item()\n\n    return l, rmse, mad, r2","7119a3cc":"def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder):\n  net.train()\n  l, rmse, mad, r2, icount = 0.,0.,0.,0., 0\n  train_laoder = tqdm(train_laoder, leave = False)\n  epoch_bar = train_laoder\n  \n  for (xb, yb) in  epoch_bar:\n      _l, _rmse, _mad, _r2 = one_step(xb, yb, net, criterion, optimizer)\n      l += _l\n      rmse += _rmse\n      mad += _mad\n      r2 += _r2\n\n      icount += 1\n        \n      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n          epoch_bar.set_postfix(\n            loss=\"{:.3f}\".format(l\/icount),\n            rmse=\"{:.3f}\".format(rmse\/icount),\n            mad=\"{:.3f}\".format(mad\/icount),\n            r2=\"{:.3f}\".format(r2\/icount),\n          )\n  \n  scheduler.step()\n\n  l \/= icount\n  rmse \/= icount\n  mad \/= icount\n  r2 \/= icount\n  \n  l_val, rmse_val, mad_val, r2_val = evaluate(net, criterion, val_laoder)\n  \n  return (l, l_val), (rmse, rmse_val), (mad, mad_val), (r2, r2_val)","f83b7208":"class AutoSave:\n  def __init__(self, top_k=2, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n    self.top_k = top_k\n    self.logs = []\n    self.metric = metric\n    self.mode = mode\n    self.root = Path(root or MODEL_ROOT)\n    assert self.root.exists()\n    self.name = name\n\n    self.top_models = []\n    self.top_metrics = []\n\n  def log(self, model, metrics):\n    metric = metrics[self.metric]\n    rank = self.rank(metric)\n\n    self.top_metrics.insert(rank+1, metric)\n    if len(self.top_metrics) > self.top_k:\n      self.top_metrics.pop(0)\n\n    self.logs.append(metrics)\n    self.save(model, metric, rank, metrics[\"epoch\"])\n\n\n  def save(self, model, metric, rank, epoch):\n    t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n    name = re.sub(r\"[^\\w_\\-\\.]\", \"\", name) + \".pth\"\n    path = self.root.joinpath(name)\n\n    old_model = None\n    self.top_models.insert(rank+1, name)\n    if len(self.top_models) > self.top_k:\n      old_model = self.root.joinpath(self.top_models[0])\n      self.top_models.pop(0)      \n\n    torch.save(model.state_dict(), path.as_posix())\n\n    if old_model is not None:\n      old_model.unlink()\n\n    self.to_json()\n\n\n  def rank(self, val):\n    r = -1\n    for top_val in self.top_metrics:\n      if val <= top_val:\n        return r\n      r += 1\n\n    return r\n  \n  def to_json(self):\n    # t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_logs\".format(self.name)\n    name = re.sub(r\"[^\\w_\\-\\.]\", \"\", name) + \".json\"\n    path = self.root.joinpath(name)\n\n    with path.open(\"w\") as f:\n      json.dump(self.logs, f, indent=2)","f0608e4a":"def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):\n\n  save_root = Path(save_root) or MODEL_ROOT\n\n  saver = AutoSave(root=save_root, name=f\"crp_{model_name}_fold{fold}\", metric=\"rmse_val\")\n   \n  net = CRPTokenModel(model_name)\n  net = net.to(DEVICE)\n    \n  with open(MODEL_ROOT\/f\"{model_name}-config.pkl\", \"wb\") as f:\n    pickle.dump(net.config, f)\n    \n  with open(MODEL_ROOT\/f\"{model_name}-tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(net.tokenizer, f)\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(net.parameters(), lr=5e-5)\n  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n\n  train_data = CRPDataset(X_input_ids=X_input_ids[train_set] , X_masks=X_masks[train_set], Y=Y[train_set], is_train=True)\n  train_laoder = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True, pin_memory=True)\n\n  val_data = CRPDataset(X_input_ids=X_input_ids[val_set] , X_masks=X_masks[val_set], Y=Y[val_set], is_train=False)\n  val_laoder = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, num_workers=VAL_NUM_WORKERS, shuffle=False)\n\n  epochs_bar = tqdm(list(range(epochs)), leave=False)\n\n  for epoch  in epochs_bar:\n    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n    net.train()\n\n    (l, l_val), (rmse, rmse_val), (mad, mad_val), (r2, r2_val) = one_epoch(\n        net=net,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_laoder=train_laoder,\n        val_laoder=val_laoder,\n      )\n\n    epochs_bar.set_postfix(\n    loss=\"({:.3f}, {:.3f})\".format(l, l_val),\n    rmse=\"({:.3f}, {:.3f})\".format(rmse, rmse_val),\n    mad=\"({:.3f}, {:.3f})\".format(mad, mad_val),\n    r2=\"({:.3f}, {:.3f})\".format(r2, r2_val),\n    )\n\n    print(\n        \"[{epoch:02d}] loss: {loss} rmse: {rmse} mad: {mad} r2: {r2}\".format(\n            epoch=epoch,\n            loss=\"({:.3f}, {:.3f})\".format(l, l_val),\n            rmse=\"({:.3f}, {:.3f})\".format(rmse, rmse_val),\n            mad=\"({:.3f}, {:.3f})\".format(mad, mad_val),\n            r2=\"({:.3f}, {:.3f})\".format(r2, r2_val),\n        )\n    )\n\n    if save:\n      metrics = {\n          \"epoch\": epoch,\n          \"loss\": l, \"rmse\": -rmse, \"mad\": mad, \"r2\": r2,\n          \"loss_val\": l_val, \"rmse_val\": -rmse_val, \"mad_val\": mad_val, \"r2_val\": r2_val,\n      }\n\n      saver.log(net, metrics)","5b0d35a4":"def train(model_name, epochs=20, save=True, n_splits=5, seed=SEED, save_root=None, suffix=\"\", folds=None):\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  save_root = save_root or MODEL_ROOT\/f\"{model_name}{suffix}\"\n  save_root.mkdir(exist_ok=True, parents=True)\n\n  seed_everything(seed)\n  \n  fold_bar = tqdm(df.reset_index(drop=True).reset_index().groupby(\"fold\").index.apply(list).items(), total=df.fold.max()+1)\n  \n  for fold, val_set in fold_bar:\n      if folds and not fold in folds:\n        continue\n      \n      print(f\"\\n############################### [FOLD {fold}  SEED {seed}]\")\n      fold_bar.set_description(f\"[FOLD {fold}  SEED {seed}]\")\n      train_set = np.setdiff1d(df.index, val_set)\n        \n      one_fold(model_name, fold=fold, train_set=train_set , val_set=val_set , epochs=epochs, save=save, save_root=save_root)\n    \n      gc.collect()\n      torch.cuda.empty_cache()","ee9f524f":"for seed in [666]:\n    train(MODEL_NAME, epochs=2, suffix=f\"_maxlen{MAX_LENGTH}_seed{seed}\", folds=None, seed=seed)","7f0352e6":"# Training the model"}}