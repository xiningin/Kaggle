{"cell_type":{"e8d9f870":"code","a919b459":"code","4534c997":"code","bd2b2ebb":"code","b96c8bee":"code","43508c75":"code","1051e3af":"code","ddb91393":"code","d827cdb5":"code","c63fd08e":"code","20a550fa":"code","660757ac":"code","8a9bb1d9":"code","0a604eca":"code","ac4d0669":"code","f3d01417":"code","34bd6a63":"code","1745072a":"code","cae2dc72":"code","2d13a77b":"code","7ce4a9e2":"code","edde1cb9":"code","8378d784":"code","5a08774f":"code","c747ee67":"code","9a52ea86":"code","b5258725":"code","2c678c89":"code","4d955e81":"code","8246e66b":"code","d620921a":"code","c8013068":"code","7e3819bb":"code","660a6a75":"code","830382eb":"code","63c16c9f":"code","91b14e4b":"code","c1a2595a":"code","6d0aae45":"code","e84b804a":"code","e0706588":"code","6f4a5de1":"code","a7e1b9de":"code","a2039614":"code","873edecc":"code","fcf3e9f1":"code","01b662d7":"code","924152f3":"code","294ead74":"code","9be685ee":"code","d4b89989":"code","f11e4427":"markdown","4ce9eadc":"markdown","63882b28":"markdown","7d3c7396":"markdown","efd9b748":"markdown","b5f786c7":"markdown","f073d202":"markdown","c404604a":"markdown","c220e9cd":"markdown","32229748":"markdown","4ff4e506":"markdown","d2523ae1":"markdown","a94a680f":"markdown","cc5d7cde":"markdown","3cbe4652":"markdown","24234fa0":"markdown","c20e202e":"markdown","dbb1cc15":"markdown","1f54ad3c":"markdown","bf56cfd3":"markdown","1c74cbb4":"markdown","69efecd6":"markdown","c6380143":"markdown","0938c40d":"markdown","ec0f9520":"markdown","e0c780a5":"markdown","c42cb3df":"markdown","9d0792fd":"markdown","f80f1582":"markdown","ba67df94":"markdown","5457186e":"markdown","f15895fc":"markdown","65f9d74d":"markdown","8d9f1b5f":"markdown","203909b8":"markdown","421fb2e7":"markdown","5d7187b6":"markdown","dd3f733a":"markdown","5de64a8c":"markdown","bce5a193":"markdown","1c99e302":"markdown","89ee8a67":"markdown","b8ac51dc":"markdown","0c7f1d7b":"markdown","08bc7164":"markdown","45291c53":"markdown","c4e6b973":"markdown","20fdfd8c":"markdown","de1b51cb":"markdown","3187f8e3":"markdown","5e71f339":"markdown","1e56b025":"markdown","fb2676e7":"markdown","0e9eb67d":"markdown","90806581":"markdown","d071c9a1":"markdown","df2f7631":"markdown","564e5c9a":"markdown","7020d62d":"markdown","fdb11b50":"markdown","77e2af25":"markdown","873db229":"markdown","7934882a":"markdown","03919611":"markdown","0e61ace6":"markdown","cd6f4960":"markdown","d6ba6e6a":"markdown"},"source":{"e8d9f870":"# import libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a919b459":"# load the data\n\ndf = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","4534c997":"# get basic information on the data\n\ndf.info()","bd2b2ebb":"# get the number of unique customers\n\nlen(df.customerID.unique())","b96c8bee":"# checkout the first 5 rows of the data to get an impression of the data\n\ndf.head()","43508c75":"# drop customerID\n\ndf = df.drop(['customerID'], axis = 1)","1051e3af":"# convert TotalCharges to float\n\ndf['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')","ddb91393":"# checkout amount of missing values\n\ndf.isna().sum()","d827cdb5":"# drop instances with missing values\n\ndf = df.dropna()\n\n# check whether there remain missing values\n\ndf.isna().sum()","c63fd08e":"# import visualization libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# settings\n\nsns.set()\n%matplotlib inline","20a550fa":"# countplot of Churn\n\nsns.countplot(df['Churn'])\nplt.show()","660757ac":"# amount of customers who didn't churn and who churned\n\ndf.groupby('Churn').size()","8a9bb1d9":"# churn rate\n\nchurn_rate = df.groupby('Churn').size()[1]\/df.Churn.count()\nprint('Churn rate: %.2f%%' % (churn_rate * 100.0))","0a604eca":"# countplots of all categorical features\n\ndf_cat_features = df[['gender', \n                        'SeniorCitizen',\n                        'Partner', \n                        'Dependents', \n                        'PhoneService', \n                        'MultipleLines', \n                        'InternetService', \n                        'OnlineSecurity', \n                        'OnlineBackup', \n                        'DeviceProtection',\n                        'TechSupport',\n                        'StreamingTV', \n                        'StreamingMovies',\n                        'Contract',\n                        'PaperlessBilling',\n                        'PaymentMethod']].copy()\n\nplt.figure(figsize=(16,16))\nfor i in range(0,16):\n        plt.subplot(4,4,i+1)\n        sns.countplot(df_cat_features.iloc[:,i])\n        plt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","ac4d0669":"# countplots of all categorical features by Churn\n\nplt.figure(figsize=(16,16))\nfor i in range(0,16):\n        plt.subplot(4,4,i+1)\n        sns.countplot(df_cat_features.iloc[:,i], hue=df['Churn'])\n        plt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","f3d01417":"# summary statistics for numerical features\n\ndf.describe().iloc[:,1:4]","34bd6a63":"# heatmap of correlations of the numeric features\n\ncorr = df.drop('SeniorCitizen', axis=1).corr() # SeniorCitizen is again excluded here\nsns.heatmap(corr, annot=True)\nplt.show()","1745072a":"# boxplots of MonthlyCharges and TotalCharges\n\nplt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nsns.boxplot(df['MonthlyCharges'], orient='vertical', color='blue')\nplt.subplot(2,2,2)\nsns.boxplot(df['TotalCharges'], orient='vertical', color='orange')\nplt.show()","cae2dc72":"# histograms of MonthlyCharges and TotalCharges\n\nplt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nsns.distplot(df['MonthlyCharges'], kde=True, color='blue')\nplt.subplot(2,2,2)\nsns.distplot(df['TotalCharges'], kde=True, color='orange')\nplt.show()","2d13a77b":"# boxplot of tenure\n\nsns.boxplot(df['tenure'], orient='vertical', color='red')\nplt.show()","7ce4a9e2":"# histogram of tenure\n\nsns.distplot(df['tenure'], kde=True, color='red')\nplt.show()","edde1cb9":"# function to map values of tenure to different groups based on quartiles\n\ndef monthly_charges_group(row):\n    if row['MonthlyCharges'] <= df.MonthlyCharges.quantile(0.25):\n        return 'low'\n    elif row['MonthlyCharges'] <= df.MonthlyCharges.quantile(0.5):\n        return 'lower medium'\n    elif row['MonthlyCharges'] <= df.MonthlyCharges.quantile(0.75):\n        return 'upper medium'\n    else:\n        return 'high'\n\n# create new column containing the group information based on quartiles \n\ndf['monthly charges group'] = df.apply(monthly_charges_group, axis=1)\n\n# countplot of monthly charges groups by Churn\n\nsns.countplot(df['monthly charges group'], hue=df['Churn'], order=['low', 'lower medium', 'upper medium', 'high'])\nplt.show()","8378d784":"# function to map values of tenure to different groups based on quartiles\n\ndef total_charges_group(row):\n    if row['TotalCharges'] <= df.TotalCharges.quantile(0.25):\n        return 'low'\n    elif row['TotalCharges'] <= df.TotalCharges.quantile(0.5):\n        return 'lower medium'\n    elif row['TotalCharges'] <= df.TotalCharges.quantile(0.75):\n        return 'upper medium'\n    else:\n        return 'high'\n    \n# create new column containing the group information based on quartiles \n    \ndf['total charges group'] = df.apply(total_charges_group, axis=1)\n\n# countplot of total charges groups by Churn\n\nsns.countplot(df['total charges group'], hue=df['Churn'], order=['low', 'lower medium', 'upper medium', 'high'])\nplt.show()","5a08774f":"# function to map values of tenure to different groups based on quartiles\n\ndef tenure_group(row):\n    if row['tenure'] <= df.tenure.quantile(0.25):\n        return 'low'\n    elif row['tenure'] <= df.tenure.quantile(0.5):\n        return 'lower medium'\n    elif row['tenure'] <= df.tenure.quantile(0.75):\n        return 'upper medium'\n    else:\n        return 'high'\n    \n# create new column containing the group information based on quartiles \n\ndf['tenure group'] = df.apply(tenure_group, axis=1)\n\n# countplot of tenure groups by Churn\n\nsns.countplot(df['tenure group'], hue=df['Churn'], order=['low', 'lower medium', 'upper medium', 'high'])\nplt.show()","c747ee67":"# drop columns created for exploratory analysis and check df before proceeding to modeling\n\ndf = df.drop(['tenure group', 'monthly charges group', 'total charges group'], axis=1)\ndf.head()","9a52ea86":"# copy df to create df_enc to encode categorical features and the target\n\ndf_enc = df.copy()","b5258725":"# encode target\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df_enc['Churn'])\ndf_enc['Churn'] = le.transform(df_enc['Churn'])","2c678c89":"# encode categorical features\n\ncategorical_features = ['gender', \n                        'SeniorCitizen',\n                        'Partner', \n                        'Dependents', \n                        'PhoneService', \n                        'MultipleLines', \n                        'InternetService', \n                        'OnlineSecurity', \n                        'OnlineBackup', \n                        'DeviceProtection',\n                        'TechSupport',\n                        'StreamingTV', \n                        'StreamingMovies',\n                        'Contract',\n                        'PaperlessBilling',\n                        'PaymentMethod']\n\ndf_enc = pd.get_dummies(df_enc, columns=categorical_features, drop_first=True)","4d955e81":"# split encoded categorical features and encoded target into X and y\n\nX = df_enc.drop('Churn', axis=1)\ny = df_enc['Churn']","8246e66b":"# split X and y in training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","d620921a":"# copy numerical features from X_train and X_test to create X_train_scaled and X_test_scaled for scaled numerical features\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()","c8013068":"# scale numerical features\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']])\nX_train_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.transform(X_train_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']])\nX_test_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.transform(X_test_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']])","7e3819bb":"# encoded and scaled train features\n\nX_train_scaled.head()","660a6a75":"# encoded and scaled test features\n\nX_test_scaled.head()","830382eb":"from sklearn.linear_model import LogisticRegression\n\n# fit the model to the training set\n\nLR = LogisticRegression()\nLR.fit(X_train_scaled, y_train)\n\n# make predictions on the test set\n\ny_pred = LR.predict(X_test_scaled)","63c16c9f":"# evaluate model\n\nfrom sklearn.metrics import accuracy_score,roc_curve,auc,confusion_matrix,classification_report\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))\nprint('\\n')\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nprint('AUC: %.2f' % auc(fpr, tpr))\nprint('\\n')\nprint('Confusion matrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint('Classification report')\nprint(classification_report(y_test, y_pred))","91b14e4b":"# 10-fold cross validation\n\nfrom sklearn.model_selection import cross_val_score,KFold\n\ncv = KFold(n_splits=10, random_state=42)\ncv_results = cross_val_score(LR, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n\nprint('Accuracy of all 10 runs: ', cv_results)\nprint('Mean: %.2f%%' % cv_results.mean())","c1a2595a":"# grid search to find optimal regularization parameter C\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.0001, 0.001, 0.01, 1, 10, 100, 1000, 10000]}\nLR_grid = GridSearchCV(LR, param_grid, cv=cv)\n\n# fit model with optimized parameter C\n\nLR_grid.fit(X_train_scaled,y_train)","6d0aae45":"# best C\n\nprint('Optimal C found by grid search: C =', LR_grid.best_params_['C'])","e84b804a":"# make predictions with C=0.01\n\ny_pred_grid = LR_grid.predict(X_test_scaled)","e0706588":"# evaluate model\n\naccuracy_grid = accuracy_score(y_test, y_pred_grid)\nprint('Accuracy: %.2f%%' % (accuracy_grid * 100.0))\nprint('\\n')\nfpr_grid, tpr_grid, thresholds_grid = roc_curve(y_test, y_pred_grid)\nprint('AUC: %.2f' % auc(fpr_grid, tpr_grid))\nprint('\\n')\nprint('Confusion matrix')\nprint(confusion_matrix(y_test, y_pred_grid))\nprint('\\n')\nprint('Classification report')\nprint(classification_report(y_test, y_pred_grid))","6f4a5de1":"# set up list with different model thresholds and an empty cost variable\n\nthresh = np.arange(0.1,1.1,0.1).tolist()\ncost = [0] * 10","a7e1b9de":"# get probabilities of belonging to the 'churn' class\n\nprobs = LR_grid.predict_proba(X_test_scaled)[:,0]","a2039614":"# set up objects to store predictions for different thresholds\n\nthresh_1 = [0] * y_test.shape[0]\nthresh_2 = [0] * y_test.shape[0]\nthresh_3 = [0] * y_test.shape[0]\nthresh_4 = [0] * y_test.shape[0]\nthresh_5 = [0] * y_test.shape[0]\nthresh_6 = [0] * y_test.shape[0]\nthresh_7 = [0] * y_test.shape[0]\nthresh_8 = [0] * y_test.shape[0]\nthresh_9 = [0] * y_test.shape[0]\nthresh_10 = [0] * y_test.shape[0]","873edecc":"# convert probabilities to binary predictions for different thresholds and store them in the respective lists\n\nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.1:\n        thresh_1[i] = 1\n    else:\n        thresh_1[i] = 0\n\nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.2:\n        thresh_2[i] = 1\n    else:\n        thresh_2[i] = 0\n\nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.3:\n        thresh_3[i] = 1\n    else:\n        thresh_3[i] = 0\n\nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.4:\n        thresh_4[i] = 1\n    else:\n        thresh_4[i] = 0\n\nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.5:\n        thresh_5[i] = 1\n    else:\n        thresh_5[i] = 0\n        \nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.6:\n        thresh_6[i] = 1\n    else:\n        thresh_6[i] = 0\n        \nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.7:\n        thresh_7[i] = 1\n    else:\n        thresh_7[i] = 0\n        \nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.8:\n        thresh_8[i] = 1\n    else:\n        thresh_8[i] = 0\n        \nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 0.9:\n        thresh_9[i] = 1\n    else:\n        thresh_9[i] = 0\n        \nfor i in range(0,y_test.shape[0]):\n    if probs[i] > 1.0:\n        thresh_10[i] = 1\n    else:\n        thresh_10[i] = 0","fcf3e9f1":"# calculate hypothetical cost per customer depending on the model threshold and store it in the cost list\n\ncf = confusion_matrix(thresh_1,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[0] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_2,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[1] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_3,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[2] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_4,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[3] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_5,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[4] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_6,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[5] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_7,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[6] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_8,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[7] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_9,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[8] = FN*250 + TP*50 + FP*50 + TN*0\n\ncf = confusion_matrix(thresh_10,y_test)\nTP = cf[0][0]\/X_test_scaled.shape[0]\nFP = cf[1][0]\/X_test_scaled.shape[0]\nFN = cf[0][1]\/X_test_scaled.shape[0]\nTN = cf[1][1]\/X_test_scaled.shape[0]\ncost[9] = FN*250 + TP*50 + FP*50 + TN*0","01b662d7":"print('cost per customer')\ncost","924152f3":"# plot hypothetical cost for different thresholds\n\nimport matplotlib.pyplot as plt\n\nplt.plot(thresh,cost)\nplt.title('cost per customer for different model thresholds')\nplt.xlabel('threshold')\nplt.ylabel('cost per customer')\nplt.show()","294ead74":"# assume that model with threshold=0.5 is currently used, calculate associated cost\n\ncost_current_model = cost[5]\ncost_current_model","9be685ee":"# calculate hypothetical savings per customer as the difference between the currently used model and the optimal model\n\nsavings_per_customer = cost_current_model - min(cost)\nsavings_per_customer","d4b89989":"# assume a customer base of 350000, multiply by savings per customer to get total savings\ntotal_savings = 350000*savings_per_customer\ntotal_savings","f11e4427":"**<a id='3.1.'>3.1. Drop unnecessary columns<\/a>**","4ce9eadc":" ### **<a id='5'>5. Modeling<\/a>**","63882b28":"### **<a id='7'>7. Summary<\/a>**","7d3c7396":"Below is a heatmap of the numerical features. One observation is that 'tenure' is correlated with 'TotalCharges', which makes sense because the longer a contract exists, the higher the total amount paid for that contract.","efd9b748":"Let's plot the cost per customer for a more visual insight. The cost per customer appears to increas as the threshold increases.","b5f786c7":"Below is a histogram of 'tenure'. There appear to be many contracts that didn't last long and the amount of longer contracts appears to decrease with contract length. Interestingly, there appears to be a peak in the frequency of contracts that are around 70 years in length.","f073d202":"To calculate the total hypothetical cost savings we assume that we have a customer base of 350000 customers. The total hypothetical cost savings are then equal to about 15 mn dollars.","c404604a":"In general, a customer who churns is a customer who quits using a company's service or product. In the case of the telecommunication industry, a customer who churns is a customer who cancels or doesn't extend the existing contract with the telco company. The key assumption here is that acquiring a new customer is more expensive than retaining an existing one. Therefore, it might be a good idea to build a model that uses existing data to identify customers who are likely to churn. Once identified, those customers can be targeted with special offers, for instance, to increase the chances of them staying with their current telco provider. Retaining customer who would churn otherwise saves the cost of acquiring new customers.","c220e9cd":"**<a id='4.1.'>4.1. The target<\/a>**","32229748":"Let's now consider our numeric features in relation to our target, 'Churn'. First, for each numeric feature we have, 'MonthlyCharges', 'TotalCharges', and 'tenure', we create a function that maps the values of each numeric variable to a specific group (low, lower medium, upper medium or high) based on the first, second, third, and fourth quartiles of each numeric variable, respectively. We then, in turn, apply this function to each numeric variable to create a new column for each numeric variable in our data frame that represents to which group each instance in our data belongs, respectively. Finally, we create a countplot of the groups for each numeric variable and the amount of customers who did and who didn't churn.\n\n\n*Note: The groups in the following plots are based on quartiles. If one were to create different groups, based on different quantiles for instance, the plots could look considerably different.*\n\n\nFirst, let's consider 'MonthlyCharges'. We observe that most customers fall into the low group, followed by the lower medium group. The upper medium group contains less customers than the high group and both contain less customers than the low and the lower medium group. Interestingly, the amount of customers who churned appears to increase with monthly charges group and slightly falls off again when the high group is reached. This implies that the higher a customer's monthly charge, the less likely he is to churn. This fits to the observation we made before: the more engaged a customer, i.e. the more services are included in the contract, the less likely he is to churn. And in turn, the more services are included in the contract, the higher the monthly charge.","4ff4e506":"Now that we have cleaned our data, explored it and built and evaluated a churn prediction model it is time to assess the hypothetical business impact of our model. But first we need to clarify and prepare a couple things.\n\n\nThe Logistic Regression model calculates probabilites for the classes and then uses a specific threshold to predict whether an observation belongs to one class or the other. In other words, the Logistic Regression model calculates for each observation the probability of belonging to one or the other class, e.g. 0.73 'churn' and 0.27 'not churn'. Using a threshold of 0.5, the Logistic Regression model then assigns an observation to the 'churn' class if the probability of belonging to this class is greater than 0.5 for this observation. The default threshold for our Logistic Regression model is 0.5 and next we will vary this threshold to see how the predictions change.","d2523ae1":"Let's get an impression of what the data actually looks like.","a94a680f":"Then we evaluate our model using different metrics. \n\n- The accuracy of our model is 80.05%.\n- The AUC is 0.72, which is another measure of model performance - the bigger the better (max=1). More information on AUC can be found [here](https:\/\/de.wikipedia.org\/wiki\/Area_under_the_curve).\n\n\nAnother useful tool to assess the performance of a model is the confusion matrix. The confusion matrix below shows how many predictions were correct and how many were incorrect.\n\n- **True negatives**: In the upper left are the 'true negatives', i.e. the model predicted 'no churn' and the customer actually didn't churn. (1528)\n- **False negatives**: In the lower left are the 'false negatives', i.e. the model predicted 'no churn', but the customer actually churned. (280)\n- **True positives**: In the lower right are the 'true positives', i.e. the model predicted 'churn' and the customer actually churned. (330)\n- **False positives**: In the upper right are the 'false positives', i.e. the model predicted 'churn', but the customer actually didn't churn. (183)\n\n\nSimply looking at accuracy might not be sufficient to correctly assess a model's performance. Therefore, we calculate additional metrics called precision and recall.\n\n\n**Precision**: divide TP by the sum of all positive predictions, i.e. TP plus FP - in different words, how many customers did we correctly classify as churners in relation to how many customers we correctly classified overall\n\n\n- *precision = TP \/ (TP + FP) = 330 \/ ( 330 + 183) = 0.64*\n\n\n**Recall**: divide TP by the sum of correct positive predictions and false negative predictions, i.e. TP plus FN - in other words, how many customers did we classify as churners in relation to how many customers acctually churned\n\n\n- *recall = TP \/ (TP + FN) = 330 \/ (330 + 280) = 0.54*\n\n\nTo jointly assess precision and recall we can calculate the **f1-score**.\n\n\n- *f1-score = 2 * (precision * recall) \/ (precision + recall) = 2 * (0.64 * 0.54) \/ (0.64 + 0.54) = 0.58*\n\n\n\nPrecision, recall and the f1-score can all be found in the classification report below.\n\n\n\nFor more information on accuracy, precision, recall and the f1-score we recommend reading [this](https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c) article.","cc5d7cde":"More precisely, in the cleaned version of our data, there are 5163 customers who didn't churn and 1869 customers who churned. This implies that the classes of our target are imbalanced, i.e. there are much more cases of one class than of the other.","3cbe4652":"Now we split X and y in training and test sets. Our model will be trained on the training set and we will make predictions on the test set.","24234fa0":"## **Predicting Customer Churn using Telco Data + Estimating Hypothetical Business Impact**\n\n\nIn this kernel we explore customer churn in the telecommunication industry, using Logistic Regression to predict whether a customer will churn or not. We conclude the analysis by estimating the hypothetical business impact (i.e. potential cost savings in this case) that such a churn prediction model could achieve by correctly identifying those customers would potentially churn.\n\n\n**Overview**\n\n- <a href='#1'>1. Business problem<\/a>\n\n\n- <a href='#2'>2. Get the data<\/a>\n    - <a href='#2.1.'>2.1. Data overview<\/a>\n\n\n- <a href='#3'>3. Clean the data<\/a>\n    - <a href='#3.1.'>3.1. Drop unnecessary columns<\/a>\n    - <a href='#3.2.'>3.2. Convert data types<\/a>\n    - <a href='#3.3.'>3.3. Check missing values<\/a>\n\n\n- <a href='#4'>4. Explore the data<\/a>\n    - <a href='#4.1.'>4.1. The target<\/a>\n    - <a href='#4.2.'>4.2. Categorical  features<\/a>\n    - <a href='#4.3.'>4.3. Numerical features<\/a>\n\n\n- <a href='#5'>5. Modeling<\/a>\n    - <a href='#5.1.'>5.1. Preprocessing<\/a>\n    - <a href='#5.2.'>5.2. Logistic Regression<\/a>\n    - <a href='#5.3.'>5.3. Cross validation<\/a>\n    - <a href='#5.4.'>5.4. Parameter tuning<\/a>\n\n- <a href='#6'>6. Business impact<\/a>\n\n\n- <a href='#7'>7. Summary<\/a>\n\n\n**Inspiration** \n- [Data Optimal's blog post on: Create Better Data Science Projects With Business Impact](https:\/\/www.dataoptimal.com\/churn-prediction-with-r\/)\n- [Pavan Raj's kernel](https:\/\/www.kaggle.com\/pavanraj159\/telecom-customer-churn-prediction)\n- [Nilan's kernel](https:\/\/www.kaggle.com\/nilanml\/telecom-customer-churn-voting-80-1-accuracy)\n\n\n**Ideas for future work**\n- try more complex models such as tree-based models, ensembles or neural nets to improve predictive power and accuracy\n- feature engineering and feature selection\n- extract more insights from exploratory analysis\n- impute missing values\n- make code in business impact section cleaner and more efficient","c20e202e":"Since the numerical features in our data all have different scales it might make sense to transform them in a way that they all are on the same scale. First, we create copies of X_train and X_test and second, we use a standard scaler to scale the numerical features so that they have a mean of zero and a standard deviation of one.","dbb1cc15":"Let's clean the data before we proceed with the exploratory analysis and the modeling.","1f54ad3c":"To be able to properly work with the 'TotalCharges' column we have to convert it to a numerical data type.","bf56cfd3":"**<a id='4.3.'>4.3. Numerical features<\/a>**","1c74cbb4":"**<a id='3.2.'>3.2. Convert data types<\/a>**","69efecd6":"**<a id='5.3.'>5.3. Cross validation<\/a>**","c6380143":"The performance of our model is not too bad, but we could try to improve accuracy by tuning the hyperparameters of our model. One parameter we could try to optimize for our Logistic Regression is the regularization parameter C. In essence, C has an influence on the magnitude of the weights our model uses for each feature when calculating the loss function. Smaller C implies stronger regularization. Larger C implies less regularization. To learn more about regularization check out [these](http:\/\/statweb.stanford.edu\/~tibs\/sta305files\/Rudyregularization.pdf) slides.\n\n\nTo find the optimal C, we use a grid search, a method that tries out different values of C we provide to find the C that optimizes our model's performance. We then fit the model to the training set using the optimized value for C that has been found by the grid search.","0938c40d":"Next, we encode our categorical features using one hot encoding.","ec0f9520":"**<a id='3.3.'>3.3. Check missing values<\/a>**","e0c780a5":"**<a id='5.2.'>5.2. Logistic Regression<\/a>**","c42cb3df":"Second, let's consider 'TotalCharges'. Here, most customer falls into the high group. There is almost an equal amount of customers in the lower medium and upper medium groups, while the least customers are in the low group. The amount of customers who churned appears to decrease with the total charges group. This means that the higher a customer's total charges, the less likely he is to churn, i.e. a customer that has a high total charge has been with the telco provider for quite some time (and maybe also has high monthly charges, see above) and appears to be loyal.","9d0792fd":"**<a id='4.2.'>4.2. Categorical features<\/a>**","f80f1582":"Then, let's calculate the hypothetical savings per customer our optimal model could achieve as the difference between the cost per customer associated with the currently used model and the model with the optimal threshold. The savings per customer are about 44 dollars.","ba67df94":"First, let's explore the target, 'Churn'. The countplot below gives us a first impression of the amount of customers who didn't churn in relation to the amount of customers who actually churned. We observe that there are much more customer who didn't churn in comparison to the amount of customer who actually churned.","5457186e":"**<a id='2.1.'>2.1. Data overview<\/a>**","f15895fc":"Below are boxplots for 'MonthlyCharges' and 'TotalCharges'. They convey information on the distribution of the values of numerical variables, e.g. mean, quartiles and outliers. There are outliers on both ends for 'MonthlyCharges' and predominantely on the upper end for 'TotalCharges'. For more information on boxplots click [here](https:\/\/de.wikipedia.org\/wiki\/Box-Plot).","65f9d74d":"Furthermore, there are 7043 unique customers in the data.","8d9f1b5f":"**<a id='5.4.'>5.4. Parameter tuning<\/a>**","203909b8":"First, we import some basic libraries and then we load the data from a csv file into a data frame.","421fb2e7":"Let's now explore the categorical features in our data. First, we create a list of categorical features and we then create a countplot for each categorical feature that shows the amount of the different values of the respective categorical features.\n\n\nWe observe that \n\n- there is about an equal amount of female and male customers\n- there are much less senior citizens than there are citizens who are not senior\n- 'partner' is equally distributed\n- there are much more cases where 'Dependents' takes on the value 'No' compared to the amount of cases where 'Dependents' takes on the value 'Yes'\n- most customers have phone service included in their contracts\n- there is about an equal amount of customers who have only one line and customers who have multiple lines\n- many customers have internet service included in their contracts and more customers have fiber optic than DSL\n- of those customers who have internet service included, most don't have internet security\n- of those customers who have internet service included, more don't have online backup than have online backup\n- of those customers who have internet service included, more don't have device protection than have device protection\n- of those customers who have internet service included, more don't have tech support than have tech support\n- of those customers who have internet service included, the amount of those who have and have not streaming TV included is almost equal\n- of those customers who have internet service included, the amount of those who have and have not streaming movies included is almost equal\n- most customers have month-to-month contracts, while the amount of customers who have one-year or two-year contracts is about equal\n- there are more customers who have paperless billing\n- most customers use electronic check as their payment method, while the amount of customers who use mailed check, bank transfer or credit card is about equal","5d7187b6":"We previously split our data in training and test sets to avoid overfitting. Overfitting occurs when a model has too strong of a fit to your training data. It performs well on the training data, but it doesn't perform well when given new data that it hasn't seen so far. More information on overfitting can be found [here](https:\/\/en.wikipedia.org\/wiki\/Overfitting). Overfitting is also important in the context of the bias variance trad-off. To learn more about the bias variance trade-off check out [this](http:\/\/scott.fortmann-roe.com\/docs\/BiasVariance.html) article.\n\n\nAn even better approach to avoid overfitting than just splitting the data in training and test sets is to use cross validation to more rigorously avoid overfitting. Cross validation is a technique where the training and test data is randomly partitioned into a number of k folds, where usually k=10. We then run the model on all folds and average the evaluation metric over all runs. More information on cross validation can be found [here](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_%28statistics%29).\n\nBelow we perform a 10-fold cross validation and observe that each run yields a different accuracy ranging from 74% to 84%. The average accuracy over all runs is 80%, which is similar to the accuracy we observed before.","dd3f733a":"We have now predictions for 10 different thresholds ranging from 0.1 to 1.\n\n\nNext we calculate the hypothetical cost that we could save using our model for each of our 10 different thresholds. \n\n\nBut first, we need some more assumptions. There are 3 cases:\n\n\n- We assume that acquiring a customer in the telco industry costs 250 dollars. So if we make a prediction that a customer won't churn, but that customer actually would churn, we would need to acquire a new one which would cost 250 dollars.\n- Let's assume that acquiring a new customer costs 5 times more than retaining an existing one, i.e. 50 dollars. This means that, if we predict that a customer will churn (no matter whether that customer actually would churn or not), we would need to spend 50 dollars to retain that customer.\n- If we predict a customer won't churn and that customer actually wouldn't churn we would have to spend 0 dollars.\n\n\nLet's translate this to what we had in our confusion matrix earlier:\n\n\n- **False negative**: predict 'no churn', customer would actually churn -> would cost 250 dollars\n- **True positive**: predict 'churn', customer would actually churn -> would cost 50 dollars\n- **False positive**: predict 'churn', customer wouldn't actually churn -> would cost 50 dollars\n- **True negative**: predict 'no churn', customer wouldn't actually churn -> would cost 0 dollars\n\nLet's calculate the hypothetical cost per customer for each treshold based on our assumptions.","5de64a8c":"In this kernel we cleaned and explored data form the telecommunication industry, built a churn prediction model and showed the hypothetical business impact it could have by estimating the hypothetical cost savings that such a model could achieve.\n\n\nThe hypothetical cost savings may seem extraordinally high, the assumptions overly simplistic, and there are many elements of this work that could be improved (e.g. the model itself, feature enginering, feature selection etc.), but this kernel was simply meant to show what could be possibly done with quite simply measures and methods to actually have an impact on business through Data Science and Machine Learning. \n\n\nAny feedback is highly appreciated and warmly welcome. Thank you!","bce5a193":"Now it's time to build our prediction model. We will use a Logistic Regression since it is a linear model that can be explained more easily to executives rather than more complicated models like Random Forest, XGBoost or Neural Nets.\n\nFirst, we create an instance of the model, then we fit it to our features and target from the training set and lastly, we make predictions using the features in our test set.","1c99e302":"There are 3 numerical features in our data, namely 'tenure', 'MonthlyCharges', and 'TotalCharges'. Below are some summary statistics for these features. The average 'tenure' is about 34 years, the average 'MonthlyCharges' about 65 dollars, and the average 'TotalCharges' about 2283 dollars.","89ee8a67":"Before we can start building our prediction model, we need to do some preprocessing. First, we create a copy of our original data frame to encode the categorical features and our target', so that our model can process them later on.","b8ac51dc":"### **<a id='1'>1. Business problem<\/a>**","0c7f1d7b":"Let's get a first impression of what is in the data. There are 7043 instances and 21 features in the data and it contains different numeric and categorical features.","08bc7164":"Let's further assume that the model that is currently in use in the telco company applies a threshold of 0.5. The cost per customer associated with this model is about 81 dollars.","45291c53":"Below is a boxplot for 'tenure' that shows that there are some outliers on both ends.","c4e6b973":"Let's have a look at our training and test features after we have encoded the categorical features and scaled the numerical features.","20fdfd8c":"Lastly, we consider 'tenure'. The amount of customers per group steadily increases with the tenure group, while the amount of customers who churned decreases with the tenure group. There are only few customers who churned in the high group, but almost all customers in the low group churned. This implies that the longer a customer's tenure, the less likely he is to churn. Same argument as above: loyal customers who have been with the telco provider for quite some time tend to be less likely to churn.","de1b51cb":"For the purpose of simplicity, we just drop the rows containing missing values for now. Once could also impute them with the median or mean, for instance, but we don't do this for now since the amount os missing values is very low in relation to the total amount of data we have.","3187f8e3":"Then we split our features and our target into different objects called X for the features and y for the target.","5e71f339":"**<a id='5.1.'>5.1. Preprocessing<\/a>**","1e56b025":"Next, we explore whether there are differences in regard to whether a customer churned or not across the different values of all categorical features. We keep it to the most interesting insights, though.\n\n\nWe observe the following:\n\n\n- 'gender': there is about an equal amount of female and male customers who churned\n- 'SeniorCitizen': a high number of senior citizens churned\n- 'InternetService': among those customers who have fiber optic many more churned than among those who have DSL\n- 'OnlineSecurity': among those customer who have no online security many more churned than among those who have online security\n- 'OnlineBackup':  among those customer who don't have online backup many more churned than among those who have online backup\n- 'DeviceProtection':  among those customer who don't have device protection many more churned than among those who have device protection\n- 'TechSupport':  among those customer who don't have tech support many more churned than among those who have tech support\n- 'StreamingTV': the amount of customers who churned is about equally distributed across those customers who have and who don't have streaming TV\n- 'StreamingMovies': the amount of customers who churned is about equally distributed across those customers who have and who don't have streaming movies\n- 'Contract': most customers who churned have month-to-month contracts\n- 'PaperlessBilling': most customers who churned use paper billing\n- 'PaymentMethod': most customers who churned use electronic check \n\n\nFrom these observations one hypothesis we might conclude is that customers who are less engaged with their telco provider, i.e. have less services included in their contracts and have month-to-month contracts for instance, are more likely to churn. Not a really surprising observation, but that's what the data seem to tell us.","fb2676e7":"### **<a id='2'>2. Get the data<\/a>**","0e9eb67d":"Below is the list of the cost per customer associated with the different thresholds. It seems like the cost per customer rises with the treshold.","90806581":"Then we encode our target, 'Churn', using a binary label encoder.","d071c9a1":"Now we can make predictions using our tuned model with C=0.01.","df2f7631":"Before we proceed to the modeling part, we drop the newly created group columns and check our data frame to make sure the dropping has been successful.","564e5c9a":"### **<a id='6'>6. Business impact<\/a>**","7020d62d":"The grid search found the optimal value for C to be 0.01.","fdb11b50":"### **<a id='3'>3. Clean the data<\/a>**","77e2af25":"To express the class distribution differently, we can calculate the churn rate, i.e. the relation of customers who churned. It is about 27%.","873db229":"Below are histograms for 'MonthlyCharges' and 'TotalCharges'. They convey information on how the values of a variable are distributed in terms of frequency. We oberve that there are many 'MonthlyCharges' around 20 dollars and another large group between around 70 and 100 dollars. 'MonthlyCharges' between 20 and 70 dollars, however, seem to be less frequent.\n\n\nThe frequency of large 'TotalCharges' appears to decrease as 'TotalCharges' increases, which makes sense generally as for 'TotalCharges' to be large a customer has to have their contract for quite a long time which doesn't happen very often as we will see later. \n\n\nOverall we can conclude that neither 'MonthlyCharges' nor 'TotalCharges' appear to be normally distributed.","7934882a":"### **<a id='4'>4. Explore the data<\/a>**","03919611":"Since the 'customerID' column doesn't convey useful information for the exploratory analysis or the modeling we can drop it.","0e61ace6":"Then, we again evaluate the predictions of our tuned model. We acctually find that all metrics deteriorated in comparison to our original model - except for precision which improved slightly. Unofortunately, the grid search and tuning C didn't improve the performance of our model.","cd6f4960":"It is important to assess the amount and nature of missing values in the data to be able to conduct a concise exploratory analysis and build a good prediction model. Overall, there are only 11 missing values in the 'TotalCharges' column.","d6ba6e6a":"Now it's time to explore the data in more detail. For this purpose we load some visualization libraries."}}