{"cell_type":{"1d51cec3":"code","d38f50ab":"code","08befef7":"code","5a2cc319":"code","a0a9e67c":"code","280ae21c":"code","a0d26c7f":"code","090c396b":"code","148ebb7c":"code","042dfb8a":"code","c81e1e61":"code","5d93d495":"code","7bd5b82c":"code","3a968eac":"code","be3b6dd9":"code","82e6d35a":"code","158f96d4":"code","98a29ec3":"code","ce724356":"code","d088223b":"code","29c01dd7":"code","2d87ee7f":"code","ef1c0889":"code","40e2e93c":"code","a3aa889c":"markdown","6764b957":"markdown","3fc43621":"markdown","f1261955":"markdown","5272675f":"markdown","0d976fdf":"markdown","9f99e421":"markdown","497b93e5":"markdown"},"source":{"1d51cec3":"# !kaggle datasets download -d rhtsingh\/commonlitrobertaitptfit","d38f50ab":"# import torch\n# import pandas as pd\n# import numpy as np\n# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# class Dataset:\n#     def __init__(self, excerpt, tokenizer, max_len):\n#         self.excerpt = excerpt\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n\n#     def __len__(self):\n#         return len(self.excerpt)\n\n#     def __getitem__(self, item):\n#         text = str(self.excerpt[item])\n#         inputs = self.tokenizer(\n#             text, \n#             max_length=self.max_len, \n#             padding=\"max_length\", \n#             truncation=True\n#         )\n\n#         ids = inputs[\"input_ids\"]\n#         mask = inputs[\"attention_mask\"]\n\n#         return {\n#             \"input_ids\": torch.tensor(ids, dtype=torch.long),\n#             \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n#         }\n# def generate_predictions(model_path, max_len):\n#     model = AutoModelForSequenceClassification.from_pretrained(model_path)\n#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n#     model.to(\"cuda\")\n#     model.eval()\n    \n#     df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    \n#     dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n#     data_loader = torch.utils.data.DataLoader(\n#         dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n#     )\n\n#     final_output = []\n\n#     for b_idx, data in enumerate(data_loader):\n#         with torch.no_grad():\n#             for key, value in data.items():\n#                 data[key] = value.to(\"cuda\")\n#             output = model(**data)\n#             output = output.logits.detach().cpu().numpy().ravel().tolist()\n#             final_output.extend(output)\n    \n#     torch.cuda.empty_cache()\n#     return np.array(final_output)\n\n# preds1 = generate_predictions(\"..\/input\/a81653\/\", max_len=256)\n# preds2 = generate_predictions(\"..\/input\/a81656\/\", max_len=256)\n# preds3 = generate_predictions(\"..\/input\/a81657\/\", max_len=256)\n# preds4 = generate_predictions(\"..\/input\/a81660\/\", max_len=256)\n# preds5 = generate_predictions(\"..\/input\/a81675\/\", max_len=192)\n# preds6 = generate_predictions(\"..\/input\/a87832\/\", max_len=256)\n\n# preds7 = generate_predictions(\"..\/input\/modelf1\/\", max_len=256)\n# preds8 = generate_predictions(\"..\/input\/modelf2\/\", max_len=256)\n# preds9 = generate_predictions(\"..\/input\/modelf3\/\", max_len=256)\n# preds10 = generate_predictions(\"..\/input\/modelf4\/\", max_len=256)\n# preds11 = generate_predictions(\"..\/input\/modelf5\/\", max_len=256)\n\n# #1\n# # weights_pos = [2.29301865e-08 ,9.18492143e-02 ,3.56011564e-01, 5.34926853e-09,\n# #  8.52853500e-02 ,4.66853844e-01]\n# # weights = weights_pos\n# # preds1 = preds1*weights[0] + preds2*weights[1] + preds3*weights[2]+ preds4*weights[3] + preds5*weights[4]+ preds6*weights[5]\n# # preds2 = (preds7 + preds8+ preds9 + preds10+ preds11 + preds12)\/6\n# # preds = 0.6*preds1 + 0.4*preds2\n\n\n# #2 = 4.77\n# preds = (preds1 + preds2 + preds3 + preds4 + preds5 + preds6+ preds7 + preds8+ preds9 + preds10+ preds11) \/ 11\n# submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n# submission.target = preds\n# submission.to_csv(\"submission1.csv\", index=False)","08befef7":"\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n","5a2cc319":"# #\u8fd9\u4e2a\u662f5.11\u7684\u60c5\u51b5\n# import numpy as np \n# import pandas as pd \n\n# import os\n# from transformers import *\n\n# from sklearn.model_selection import KFold\n# from sklearn.metrics import mean_squared_error\n\n# import random\n\n# import torch\n# import torch.nn as nn\n\n# from torch.utils.data import DataLoader, Dataset\n\n# from tqdm import tqdm\n\n# if torch.cuda.is_available():\n#     device = torch.device(\"cuda\")\n#     print(\"GPU is available\")\n# else:\n#     device = torch.device(\"cpu\")\n#     print(\"GPU not available, CPU used\")\n    \n# test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\n# class Data(Dataset):\n#     def __init__(self, data):\n#         super().__init__()\n#         self.data = data\n\n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, idx):       \n#         excerpt = self.data.excerpt[idx]\n#         return excerpt\n    \n# test_data = Data(data = test_df) \n# test_loader = DataLoader(dataset = test_data, shuffle=False, batch_size = 32)\n\n# class ReadabilityModel(PreTrainedModel): \n#     def __init__(self, conf):\n#         super(ReadabilityModel, self).__init__(conf) \n#         self.roberta = RobertaModel(config=conf)\n#         self.drop_out = nn.Dropout(0.1)\n#         self.l1 = nn.Linear(768 * 1, 1)\n#         torch.nn.init.normal_(self.l1.weight, std=0.02)\n    \n#     def forward(self, ids, mask):\n#         out = self.roberta(\n#             input_ids=ids,\n#             attention_mask=mask\n#         )\n#         out = out['hidden_states']\n#         out = out[-1]\n#         out = self.drop_out(out)\n#         out = torch.mean(out, 1, True)\n        \n#         preds = self.l1(out)\n\n#         preds = preds.squeeze(-1).squeeze(-1)\n\n#         return preds\n    \n\n# tokenizer = RobertaTokenizerFast.from_pretrained('\/kaggle\/input\/robertabase', model_max_length=514) \n\n# model_config = RobertaConfig()\n# model_config.output_hidden_states = True\n# model_config.max_position_embeddings=514\n# model_config.vocab_size = 50265\n# model_config.type_vocab_size = 1\n\n# model = ReadabilityModel(model_config)\n# if torch.cuda.is_available():\n#     model.load_state_dict(torch.load(\"\/kaggle\/input\/rooobin\/rooberta_baseline.bin\"))\n# else: \n#     model.load_state_dict(torch.load(\"\/kaggle\/input\/rooobin\/rooberta_baseline.bin\", map_location=torch.device('cpu')))\n# model = model.to(device)\n\n# model.eval()\n# with torch.no_grad():\n#     for i, excerpts in enumerate(tqdm(test_loader)):\n#         batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=False)\n#         input_ids = batch['input_ids']\n#         input_ids = input_ids.to(device, dtype=torch.long)\n#         attention_mask = batch['attention_mask']\n#         attention_mask = attention_mask.to(device, dtype=torch.long)\n            \n#         preds = model(input_ids, attention_mask)       \n#         preds = preds.cpu().detach().numpy()\n\n#         if i==0:\n#             preds_test = preds\n#         else:\n#             preds_test = np.concatenate((preds_test,preds), axis=None)\n            \n# submission_df = pd.DataFrame({'id': test_df.id, 'target': preds_test})\n# submission_df.to_csv('submission2.csv', index=False)\n\n","a0a9e67c":"# import numpy as np \n# import pandas as pd \n# import os, random, sys, time, re\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torch.nn.functional as F\n# import torch.utils.data as D\n# from torch.nn.utils.rnn import pad_sequence\n\n# from sklearn.model_selection import StratifiedKFold, KFold\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# from transformers import *\n# DATA_PATH = \"..\/input\/commonlitreadabilityprize\/\"\n\n# # MODEL_PATH = '..\/input\/distilbertbaseuncased'\n# # MODEL_PATH = '..\/input\/pretrained-albert-pytorch\/albert-base-v2'\n# # MODEL_PATH = '..\/input\/camembertbasesquadfrfquadpiaf\/camembert-base-squadFR-fquad-piaf'\n# # MODEL_PATH = '..\/input\/roberta-transformers-pytorch\/distilroberta-base'\n# # MODEL_PATH = '..\/input\/roberta-transformers-pytorch\/roberta-base'\n# MODEL_PATH = '\/kaggle\/input\/bart-models-hugging-face-model-repository\/bart-base'\n# # MODEL_PATH = '..\/input\/electra-base'\n# VOCAB_PATH = '\/kaggle\/input\/robertabase' \n# # MODEL_PATH\n\n\n# N_FOLDS = 5\n# EPOCHES = 2\n# BATCH_SIZE = 12\n# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n# MAX_SEQUENCE_LENGTH = 220\n# LR = 2.5e-5\n# get_tokenizer = RobertaTokenizer\n# get_model = BartForSequenceClassification\n\n\n# # error log\n# sys.stderr = open('err.txt', 'w')\n\n# SEED = 7117\n# random.seed(SEED)\n# os.environ['PYTHONHASHSEED'] = str(SEED)\n# np.random.seed(SEED)\n# torch.manual_seed(SEED)\n# torch.cuda.manual_seed(SEED)\n# torch.cuda.manual_seed_all(SEED)\n# torch.backends.cudnn.deterministic = True\n\n\n# train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\n# test_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')\n\n# subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\n# y = (train_csv.target.values > 0).astype(int)\n# cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n\n# tokenizer = get_tokenizer.from_pretrained(VOCAB_PATH,\n#                    model_max_length=MAX_SEQUENCE_LENGTH)\n# train_csv['token'] = train_csv.excerpt.apply(tokenizer)\n# test_csv['token'] = test_csv.excerpt.apply(tokenizer)\n\n\n# class LitDataset(D.Dataset):\n    \n#     def __init__(self, token, target):\n#         self.token = token\n#         self.target = target\n        \n#     def __len__(self):\n#         return self.token.shape[0]\n\n#     def __getitem__(self, idx):\n#         return torch.tensor(self.token[idx].input_ids), \\\n#                 torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \n# def collate_fn(batch):\n#     ids, attns, targets = zip(*batch)\n#     ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n#     attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n#     targets = torch.tensor(targets).float().to(DEVICE)\n#     return ids, attns, targets\n\n# def collate_fn_test(batch):\n#     ids, attns, idxs = zip(*batch)\n#     ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n#     attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n#     return idxs, ids, attns\n\n# ds = LitDataset(train_csv.token, train_csv.target)\n# test_ds = LitDataset(test_csv.token, test_csv.index)\n\n# tloader = D.DataLoader(test_ds, batch_size=BATCH_SIZE,\n#                        shuffle=False, collate_fn = collate_fn_test, num_workers=0)\n\n# ### Table for results\n# header = r'''\n#             Train         Validation\n# Epoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n# '''\n# #          Epoch         metrics            time\n# raw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'\n\n# @torch.no_grad()\n# def validation_fn(model, loader, loss_fn):\n#     tloss = []\n#     for texts, attns, target in loader:\n#         outputs = model(texts, attention_mask=attns)\n#         loss = loss_fn(outputs.logits.squeeze(-1), target)\n#         tloss.append(loss.item())\n#     tloss = np.array(tloss).mean()\n#     return tloss\n\n# def oof_preds(ds, tloader, cv, y, epochs = EPOCHES):\n    \n#     loss_fn = torch.nn.MSELoss()\n    \n#     for train_idx, valid_idx in cv.split(range(len(ds)), y):\n        \n#         train_ds = D.Subset(ds, train_idx)\n#         loader = D.DataLoader(train_ds, batch_size=BATCH_SIZE,\n#                               shuffle=True, collate_fn = collate_fn,num_workers=0)\n        \n#         valid_ds = D.Subset(ds, valid_idx)\n#         vloader = D.DataLoader(valid_ds, batch_size=BATCH_SIZE,\n#                       shuffle=False, collate_fn = collate_fn,num_workers=0)\n        \n#         model = get_model.from_pretrained( \n#                           MODEL_PATH, num_labels=1).to(DEVICE);\n        \n#         optimizer = optim.AdamW(model.parameters(), LR,\n#                                 betas=(0.9, 0.999), weight_decay=1e-1)\n#         scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n#         print(header)\n#         for epoch in range(1, epochs+1):      \n#             start_time = time.time()\n#             tloss = []          \n#             model.train()\n            \n#             for texts, attns, target in loader:\n#                 optimizer.zero_grad()\n#                 outputs = model(texts, attention_mask=attns)\n#                 loss = loss_fn(outputs.logits.squeeze(-1), target)\n#                 tloss.append(loss.item())\n#                 loss.backward()\n#                 optimizer.step()\n#                 scheduler.step()\n#             tloss = np.array(tloss).mean()\n#             vloss = validation_fn(model, vloader, loss_fn)\n#             tmetric = tloss**.5\n#             vmetric = vloss**.5\n#             print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)\/60**1))\n#             del loss, outputs\n            \n#         model.eval();\n#         # Get prediction for test set\n#         ids, preds = [], [] \n#         with torch.no_grad():\n#             for batch_ids, texts, attn in tloader:\n#                 outputs = model(texts, attention_mask=attn)\n#                 ids += batch_ids\n#                 preds.append(outputs.logits.detach().squeeze(-1).cpu().numpy())\n            \n#         # Save prediction of test set\n#         preds = np.concatenate(preds)\n#         subm.loc[ids, 'target']  =  subm.loc[ids, 'target'].values + preds \/ N_FOLDS\n        \n#         del model, vloader, loader, train_ds, valid_ds\n#         torch.cuda.empty_cache()\n# oof_preds(ds, tloader, cv, y, epochs = EPOCHES)\n\n# subm.to_csv('submission3.csv')","280ae21c":"# import os\n# import gc\n# import sys\n# import cv2\n# import math\n# import time\n# import tqdm\n# import random\n# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# import optuna\n# import lightgbm as lgb\n# import xgboost as xgb\n# from sklearn.svm import SVR\n# from catboost import CatBoostRegressor, Pool, CatBoost\n\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import KFold,StratifiedKFold\n\n# import torch\n# import torchvision\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.nn import Parameter\n# import torch.nn.functional as F\n# from torch.optim import Adam, lr_scheduler\n# from torch.utils.data import Dataset, DataLoader\n# from torch.optim.optimizer import Optimizer\n# from torch.optim.lr_scheduler import _LRScheduler\n# from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n#                                       ReduceLROnPlateau)\n\n# from transformers import (AutoModel, AutoTokenizer, \n#                           AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\n\n# import plotly.express as px\n# import plotly.graph_objs as go\n# import plotly.figure_factory as ff\n\n\n# from colorama import Fore, Back, Style\n# y_ = Fore.YELLOW\n# r_ = Fore.RED\n# g_ = Fore.GREEN\n# b_ = Fore.BLUE\n# m_ = Fore.MAGENTA\n# c_ = Fore.CYAN\n# sr_ = Style.RESET_ALL","a0d26c7f":"# import os\n# import gc\n# import sys\n# import cv2\n# import math\n# import time\n# import tqdm\n# import random\n# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# import optuna\n# import lightgbm as lgb\n# import xgboost as xgb\n# from catboost import CatBoostRegressor, Pool, CatBoost\n\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import KFold,StratifiedKFold\n\n# import torch\n# import torchvision\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.nn import Parameter\n# import torch.nn.functional as F\n# from torch.optim import Adam, lr_scheduler\n# from torch.utils.data import Dataset, DataLoader\n# from torch.optim.optimizer import Optimizer\n# from torch.optim.lr_scheduler import _LRScheduler\n# from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n#                                       ReduceLROnPlateau)\n\n# from transformers import (AutoModel, AutoTokenizer, \n#                           AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\n\n# import plotly.express as px\n# import plotly.graph_objs as go\n# import plotly.figure_factory as ff\n\n\n# from colorama import Fore, Back, Style\n# y_ = Fore.YELLOW\n# r_ = Fore.RED\n# g_ = Fore.GREEN\n# b_ = Fore.BLUE\n# m_ = Fore.MAGENTA\n# c_ = Fore.CYAN\n# sr_ = Style.RESET_ALL\n\n# train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n# target = train_data['target'].to_numpy()\n\n# #for kfold  \n# num_bins = int(np.floor(1 + np.log2(len(train_data))))\n# train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\n# def rmse_score(y_true,y_pred):\n#     return np.sqrt(mean_squared_error(y_true,y_pred))\n# config = {\n#     'batch_size':128,\n#     'max_len':256,\n#     'seed':42,\n# }\n\n# def seed_everything(seed=42):\n#     random.seed(seed)\n#     os.environ['PYTHONASSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = True\n\n# seed_everything(seed=config['seed'])\n\n# class CLRPDataset(nn.Module):\n#     def __init__(self,df,tokenizer,max_len=128):\n#         self.excerpt = df['excerpt'].to_numpy()\n#         self.max_len = max_len\n#         self.tokenizer = tokenizer\n    \n#     def __getitem__(self,idx):\n#         encode = self.tokenizer(self.excerpt[idx],\n#                                 return_tensors='pt',\n#                                 max_length=self.max_len,\n#                                 padding='max_length',\n#                                 truncation=True)  \n#         return encode\n    \n#     def __len__(self):\n#         return len(self.excerpt)\n# def get_embeddings(df,path,plot_losses=True, verbose=True):\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"{device} is used\")\n            \n#     MODEL_PATH = path\n#     model = AutoModel.from_pretrained(MODEL_PATH)\n#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n#     model.to(device)\n#     model.eval()\n\n#     ds = CLRPDataset(df,tokenizer,config['max_len'])\n#     dl = DataLoader(ds,\n#                   batch_size = config[\"batch_size\"],\n#                   shuffle=False,\n#                   num_workers = 4,\n#                   pin_memory=True,\n#                   drop_last=False\n#                  )\n        \n#     embeddings = list()\n#     with torch.no_grad():\n#         for i, inputs in tqdm(enumerate(dl)):\n#             inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n#             outputs = model(**inputs)\n#             outputs = outputs[0][:,0].detach().cpu().numpy()\n#             embeddings.extend(outputs)\n#     return np.array(embeddings)\n# train_embeddings1 =  get_embeddings(train_data,'..\/input\/modelf1')\n# test_embeddings1 = get_embeddings(test_data,'..\/input\/modelf1')\n\n# train_embeddings2 =  get_embeddings(train_data,'..\/input\/modelf2')\n# test_embeddings2 = get_embeddings(test_data,'..\/input\/modelf2')\n\n# train_embeddings3 =  get_embeddings(train_data,'..\/input\/modelf3')\n# test_embeddings3 = get_embeddings(test_data,'..\/input\/modelf3')\n\n# train_embeddings4 =  get_embeddings(train_data,'..\/input\/modelf4')\n# test_embeddings4 = get_embeddings(test_data,'..\/input\/modelf4')\n\n# train_embeddings5 =  get_embeddings(train_data,'..\/input\/modelf5')\n# test_embeddings5 = get_embeddings(test_data,'..\/input\/modelf5')\n\n# train_embeddings6 =  get_embeddings(train_data,'..\/input\/a81653')\n# test_embeddings6 = get_embeddings(test_data,'..\/input\/a81653')\n\n# train_embeddings7 =  get_embeddings(train_data,'..\/input\/a81656')\n# test_embeddings7 = get_embeddings(test_data,'..\/input\/a81656')\n\n# train_embeddings8 =  get_embeddings(train_data,'..\/input\/a81660')\n# test_embeddings8 = get_embeddings(test_data,'..\/input\/a81660')\n\n# train_embeddings9 =  get_embeddings(train_data,'..\/input\/a81675')\n# test_embeddings9 = get_embeddings(test_data,'..\/input\/a81675')\n\n# train_embeddings10 =  get_embeddings(train_data,'..\/input\/a87832')\n# test_embeddings10 = get_embeddings(test_data,'..\/input\/a87832')\n\n# train_embeddings11 =  get_embeddings(train_data,'..\/input\/a81657')\n# test_embeddings11 = get_embeddings(test_data,'..\/input\/a81657')\n\n# params = {\n#  'reg_alpha': 6.147694913504962,\n#  'reg_lambda': 0.002457826062076097,\n#  'colsample_bytree': 0.3,\n#  'subsample': 0.8,\n#  'learning_rate': 1e-2,\n#  'max_depth': 20,\n#  'num_leaves': 111,\n#  'min_child_samples': 285,\n#  'random_state': 42,\n#  'verbose':-1,\n#  'n_estimators': 10000,\n#  'metric': 'rmse',\n#  'cat_smooth': 39}\n\n# def get_preds(params,train_embeddings,test_embeddings,nfolds=5):\n#     nfolds=5\n#     lgb_models = list()\n#     lgbm_preds = np.zeros(test_data.shape[0])\n#     kfold = StratifiedKFold(n_splits=nfolds)\n#     for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=train_data['bins'])):\n#         lgb_train = lgb.Dataset(train_embeddings[train_idx],target[train_idx].ravel())\n#         lgb_valid = lgb.Dataset(train_embeddings[valid_idx],target[valid_idx].ravel())\n\n#         lgb_model = lgb.train(params,\n#                           lgb_train, \n#                           valid_sets=[lgb_train,lgb_valid],\n#                           verbose_eval=0,\n#                           early_stopping_rounds=800,\n#                           )\n\n#         lgbm_preds += lgb_model.predict(test_embeddings)\/nfolds\n    \n#     return lgbm_preds","090c396b":"# lgbm_preds1 = get_preds(params,train_embeddings1,test_embeddings1)\n# lgbm_preds2 = get_preds(params,train_embeddings2,test_embeddings2)\n# lgbm_preds3 = get_preds(params,train_embeddings3,test_embeddings3)\n# lgbm_preds4 = get_preds(params,train_embeddings4,test_embeddings4)\n# lgbm_preds5 = get_preds(params,train_embeddings5,test_embeddings5)\n\n# lgbm_preds6 = get_preds(params,train_embeddings6,test_embeddings6)\n# lgbm_preds7 = get_preds(params,train_embeddings7,test_embeddings7)\n# lgbm_preds8 = get_preds(params,train_embeddings8,test_embeddings8)\n# lgbm_preds9 = get_preds(params,train_embeddings9,test_embeddings9)\n# lgbm_preds10 = get_preds(params,train_embeddings10,test_embeddings10)\n# lgbm_preds11 = get_preds(params,train_embeddings11,test_embeddings11)\n# lgbm_preds  = (lgbm_preds1 + lgbm_preds2 + lgbm_preds3 + lgbm_preds4 + lgbm_preds5\n#                  +lgbm_preds6 + lgbm_preds7 + lgbm_preds8 + lgbm_preds9 + lgbm_preds10+lgbm_preds11)\/11\n# #sample.target\n\n\n","148ebb7c":"\n# train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n# target = train_data['target'].to_numpy()\n\n# #for kfold  \n# num_bins = int(np.floor(1 + np.log2(len(train_data))))\n# train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n# bins = train_data.bins.to_numpy()\n\n# def rmse_score(y_true,y_pred):\n#     return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\n# def rmse_score(y_true,y_pred):\n#     return np.sqrt(mean_squared_error(y_true,y_pred))\n# def get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n#     kfold = StratifiedKFold(n_splits=nfolds)\n#     scores = list()\n#     preds = np.zeros((X_test.shape[0]))\n#     for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n#         model = SVR(C=C,kernel=kernel,gamma='auto')\n#         X_train,y_train = X[train_idx], y[train_idx]\n#         X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n#         model.fit(X_train,y_train)\n#         prediction = model.predict(X_valid)\n#         score = rmse_score(prediction,y_valid)\n#         print(f'Fold {k} , rmse score: {score}')\n#         scores.append(score)\n#         preds += model.predict(X_test)\n        \n#     print(\"mean rmse\",np.mean(scores))\n#     return np.array(preds)\/nfolds\n\n# svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n# svm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n# svm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\n# svm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\n# svm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\n\n# svm_preds6 = get_preds_svm(train_embeddings6,target,test_embeddings6)\n# svm_preds7 = get_preds_svm(train_embeddings7,target,test_embeddings7)\n# svm_preds8 = get_preds_svm(train_embeddings8,target,test_embeddings8)\n# svm_preds9 = get_preds_svm(train_embeddings9,target,test_embeddings9)\n# svm_preds10 = get_preds_svm(train_embeddings10,target,test_embeddings10)\n# svm_preds11 = get_preds_svm(train_embeddings11,target,test_embeddings11)\n\n# svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5 +svm_preds6+svm_preds7+svm_preds8+svm_preds9+svm_preds10+svm_preds11)\/11\n\n# sample.target = 0.75 * lgbm_preds + 0.25 * svm_preds\n# # sample.to_csv('submission.csv',index=False)\n# sample.to_csv('submission4.csv',index=False)","042dfb8a":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c81e1e61":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\n%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange\n\ndef convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\nclass CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output\ndef make_model(model_name='roberta-base', num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader\nclass Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\ndef config(fold):\n    torch.manual_seed(42)\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\n    \n    max_len = 250\n    batch_size = 64\n\n    model, tokenizer = make_model(\n        model_name='..\/input\/roberta-base\/', \n        num_labels=1\n    )\n    #\/kaggle\/input\/commonl-nfer-bin\/archive\/model3.bin\n    model.load_state_dict(\n        torch.load(f'\/kaggle\/input\/commonl-nfer-bin\/archive\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )\ndef run(fold=0):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds\npred_df = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df[f'fold{fold}'] = run(fold)\nsub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub['target'] = pred_df.mean(axis=1).values.tolist()\nsub.to_csv('submission5.csv', index=False)","5d93d495":"# ","7bd5b82c":"# import pandas as pd\n# import numpy as np\n\n# train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# %matplotlib inline\n# from glob import glob\n# import os\n# import matplotlib.pyplot as plt\n# import json\n# from collections import defaultdict\n# import gc\n# gc.enable()\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torch.optim.optimizer import Optimizer\n# import torch.optim.lr_scheduler as lr_scheduler\n# from torch.utils.data import (\n#     Dataset, DataLoader, \n#     SequentialSampler, RandomSampler\n# )\n# from transformers import RobertaConfig\n# from transformers import (\n#     get_cosine_schedule_with_warmup, \n#     get_cosine_with_hard_restarts_schedule_with_warmup\n# )\n# from transformers import RobertaTokenizer\n# from transformers import RobertaModel\n# from IPython.display import clear_output\n# from tqdm import tqdm, trange\n\n# def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n#     data = data.replace('\\n', '')\n#     tok = tokenizer.encode_plus(\n#         data, \n#         max_length=max_len, \n#         truncation=True,\n#         return_attention_mask=True,\n#         return_token_type_ids=True\n#     )\n#     curr_sent = {}\n#     padding_length = max_len - len(tok['input_ids'])\n#     curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n#     curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n#         ([0] * padding_length)\n#     curr_sent['attention_mask'] = tok['attention_mask'] + \\\n#         ([0] * padding_length)\n#     return curr_sent\n# class DatasetRetriever(Dataset):\n#     def __init__(self, data, tokenizer, max_len, is_test=False):\n#         self.data = data\n#         self.excerpts = self.data.excerpt.values.tolist()\n#         self.tokenizer = tokenizer\n#         self.is_test = is_test\n#         self.max_len = max_len\n    \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, item):\n#         if not self.is_test:\n#             excerpt, label = self.excerpts[item], self.targets[item]\n#             features = convert_examples_to_features(\n#                 excerpt, self.tokenizer, \n#                 self.max_len, self.is_test\n#             )\n#             return {\n#                 'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n#                 'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n#                 'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n#                 'label':torch.tensor(label, dtype=torch.double),\n#             }\n#         else:\n#             excerpt = self.excerpts[item]\n#             features = convert_examples_to_features(\n#                 excerpt, self.tokenizer, \n#                 self.max_len, self.is_test\n#             )\n#             return {\n#                 'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n#                 'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n#                 'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n#             }\n        \n        \n# class CommonLitModel(nn.Module):\n#     def __init__(\n#         self, \n#         model_name, \n#         config,  \n#         multisample_dropout=False,\n#         output_hidden_states=False\n#     ):\n#         super(CommonLitModel, self).__init__()\n#         self.config = config\n#         self.roberta = RobertaModel.from_pretrained(\n#             model_name, \n#             output_hidden_states=output_hidden_states\n#         )\n#         self.layer_norm = nn.LayerNorm(config.hidden_size)\n#         if multisample_dropout:\n#             self.dropouts = nn.ModuleList([\n#                 nn.Dropout(0.5) for _ in range(5)\n#             ])\n#         else:\n#             self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n#         self.regressor = nn.Linear(config.hidden_size, 1)\n#         self._init_weights(self.layer_norm)\n#         self._init_weights(self.regressor)\n \n#     def _init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.bias is not None:\n#                 module.bias.data.zero_()\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n#         elif isinstance(module, nn.LayerNorm):\n#             module.bias.data.zero_()\n#             module.weight.data.fill_(1.0)\n \n#     def forward(\n#         self, \n#         input_ids=None,\n#         attention_mask=None,\n#         token_type_ids=None,\n#         labels=None\n#     ):\n#         outputs = self.roberta(\n#             input_ids,\n#             attention_mask=attention_mask,\n#             token_type_ids=token_type_ids,\n#         )\n#         sequence_output = outputs[1]\n#         sequence_output = self.layer_norm(sequence_output)\n \n#         # multi-sample dropout\n#         for i, dropout in enumerate(self.dropouts):\n#             if i == 0:\n#                 logits = self.regressor(dropout(sequence_output))\n#             else:\n#                 logits += self.regressor(dropout(sequence_output))\n        \n#         logits \/= len(self.dropouts)\n \n#         # calculate loss\n#         loss = None\n#         if labels is not None:\n#             loss_fn = torch.nn.MSELoss()\n#             logits = logits.view(-1).to(labels.dtype)\n#             loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n#         output = (logits,) + outputs[1:]\n#         return ((loss,) + output) if loss is not None else output\n# def make_model(model_name, num_labels=1):\n#     tokenizer = RobertaTokenizer.from_pretrained(model_name)\n#     config = RobertaConfig.from_pretrained(model_name)\n#     config.update({'num_labels':num_labels})\n#     model = CommonLitModel(model_name, config=config)\n#     return model, tokenizer\n\n# def make_loader(\n#     data, \n#     tokenizer, \n#     max_len,\n#     batch_size,\n# ):\n    \n#     test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n#     test_sampler = SequentialSampler(test_dataset)\n#     test_loader = DataLoader(\n#         test_dataset, \n#         batch_size=batch_size \/\/ 2, \n#         sampler=test_sampler, \n#         pin_memory=False, \n#         drop_last=False, \n#         num_workers=0\n#     )\n\n#     return test_loader\n# class Evaluator:\n#     def __init__(self, model, scalar=None):\n#         self.model = model\n#         self.scalar = scalar\n\n#     def evaluate(self, data_loader, tokenizer):\n#         preds = []\n#         self.model.eval()\n#         total_loss = 0\n#         with torch.no_grad():\n#             for batch_idx, batch_data in enumerate(data_loader):\n#                 input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n#                     batch_data['attention_mask'], batch_data['token_type_ids']\n#                 input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n#                     attention_mask.cuda(), token_type_ids.cuda()\n                \n#                 if self.scalar is not None:\n#                     with torch.cuda.amp.autocast():\n#                         outputs = self.model(\n#                             input_ids=input_ids,\n#                             attention_mask=attention_mask,\n#                             token_type_ids=token_type_ids\n#                         )\n#                 else:\n#                     outputs = self.model(\n#                         input_ids=input_ids,\n#                         attention_mask=attention_mask,\n#                         token_type_ids=token_type_ids\n#                     )\n                \n#                 logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n#                 preds += logits\n#         return preds\n# def config(fold, model_name, load_model_path):\n#     torch.manual_seed(2021)\n#     torch.cuda.manual_seed(2021)\n#     torch.cuda.manual_seed_all(2021)\n    \n#     max_len = 250\n#     batch_size = 8\n\n#     model, tokenizer = make_model(\n#         model_name=model_name, \n#         num_labels=1\n#     )\n#     model.load_state_dict(\n#         torch.load(f'{load_model_path}\/model{fold}.bin')\n#     )\n#     test_loader = make_loader(\n#         test, tokenizer, max_len=max_len,\n#         batch_size=batch_size\n#     )\n\n#     if torch.cuda.device_count() >= 1:\n#         print('Model pushed to {} GPU(s), type {}.'.format(\n#             torch.cuda.device_count(), \n#             torch.cuda.get_device_name(0))\n#         )\n#         model = model.cuda() \n#     else:\n#         raise ValueError('CPU training is not supported')\n\n#     # scaler = torch.cuda.amp.GradScaler()\n#     scaler = None\n#     return (\n#         model, tokenizer, \n#         test_loader, scaler\n#     )\n# def run(fold=0, model_name=None, load_model_path=None):\n#     model, tokenizer, \\\n#         test_loader, scaler = config(fold, model_name, load_model_path)\n    \n#     import time\n\n#     evaluator = Evaluator(model, scaler)\n\n#     test_time_list = []\n\n#     torch.cuda.synchronize()\n#     tic1 = time.time()\n\n#     preds = evaluator.evaluate(test_loader, tokenizer)\n\n#     torch.cuda.synchronize()\n#     tic2 = time.time() \n#     test_time_list.append(tic2 - tic1)\n    \n#     del model, tokenizer, test_loader, scaler\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n#     return preds\n","3a968eac":"# %%time\n# pred_df1 = pd.DataFrame()\n# pred_df2 = pd.DataFrame()\n# pred_df3 = pd.DataFrame()\n# for fold in tqdm(range(5)):\n#     pred_df1[f'fold{fold}'] = run(fold, '..\/input\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n#     pred_df2[f'fold{fold+5}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/roberta-large-itptfit\/')\n#     pred_df3[f'fold{fold+10}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/commonlit-roberta-large-ii\/')","be3b6dd9":"# sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n# # sub['target'] = ((pred_df1.mean(axis=1) + pred_df2.mean(axis=1) + pred_df3.mean(axis=1))\/3).values.tolist()\n# sub['target'] = (pred_df2.mean(axis=1)*0.5) + (pred_df1.mean(axis=1)*0.3) + (pred_df3.mean(axis=1) * 0.2).values.tolist()\n# sub.to_csv('submission6.csv', index=False)","82e6d35a":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","158f96d4":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","98a29ec3":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\nconfig = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('..\/input\/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x\n    \ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\ntrain_embeddings1 =  get_embeddings(train_data,'..\/input\/clrprobertamodels\/model0\/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'..\/input\/clrprobertamodels\/model0\/model0.bin')\n\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/clrprobertamodels\/model1\/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/clrprobertamodels\/model1\/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/clrprobertamodels\/model2\/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/clrprobertamodels\/model2\/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/clrprobertamodels\/model3\/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/clrprobertamodels\/model3\/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/clrprobertamodels\/model4\/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'..\/input\/clrprobertamodels\/model4\/model4.bin')\n#!!!!!\n\n\n# train_embeddings6 =  get_embeddings(train_data,'..\/input\/modelf1\/pytorch_model.bin')\n# test_embeddings6 = get_embeddings(test_data,'..\/input\/modelf1\/pytorch_model.bin')\n\n# train_embeddings7 =  get_embeddings(train_data,'..\/input\/modelf2\/pytorch_model.bin')\n# test_embeddings7 = get_embeddings(test_data,'..\/input\/modelf2\/pytorch_model.bin')\n\n# train_embeddings8 =  get_embeddings(train_data,'..\/input\/modelf3\/pytorch_model.bin')\n# test_embeddings8 = get_embeddings(test_data,'..\/input\/modelf3\/pytorch_model.bin')\n\n# train_embeddings9 =  get_embeddings(train_data,'..\/input\/modelf4\/pytorch_model.bin')\n# test_embeddings9 = get_embeddings(test_data,'..\/input\/modelf4\/pytorch_model.bin')\n\n# train_embeddings10 =  get_embeddings(train_data,'..\/input\/modelf5\/pytorch_model.bin')\n# test_embeddings10 = get_embeddings(test_data,'..\/input\/modelf5\/pytorch_model.bin')\n\n# train_embeddings11 =  get_embeddings(train_data,'..\/input\/a81653\/pytorch_model.bin')\n# test_embeddings11 = get_embeddings(test_data,'..\/input\/a81653\/pytorch_model.bin')\n\n# train_embeddings12 =  get_embeddings(train_data,'..\/input\/a81656\/pytorch_model.bin')\n# test_embeddings12 = get_embeddings(test_data,'..\/input\/a81656\/pytorch_model.bin')\n\n# train_embeddings13 =  get_embeddings(train_data,'..\/input\/a81660\/pytorch_model.bin')\n# test_embeddings13 = get_embeddings(test_data,'..\/input\/a81660\/pytorch_model.bin')\n\n# train_embeddings14 =  get_embeddings(train_data,'..\/input\/a81675\/pytorch_model.bin')\n# test_embeddings14 = get_embeddings(test_data,'..\/input\/a81675\/pytorch_model.bin')\n\n# train_embeddings15 =  get_embeddings(train_data,'..\/input\/a87832\/pytorch_model.bin')\n# test_embeddings16 = get_embeddings(test_data,'..\/input\/a87832\/pytorch_model.bin')\n\n# train_embeddings16 =  get_embeddings(train_data,'..\/input\/a81657\/pytorch_model.bin')\n# test_embeddings16 = get_embeddings(test_data,'..\/input\/a81657\/pytorch_model.bin')\n\n\n\n#!!!!!!\n\nfrom sklearn.linear_model import Ridge\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=20,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = Ridge()\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds\nsvm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nsvm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\n\n# svm_preds6 = get_preds_svm(train_embeddings6,target,test_embeddings6)\n# svm_preds7 = get_preds_svm(train_embeddings7,target,test_embeddings7)\n# svm_preds8 = get_preds_svm(train_embeddings8,target,test_embeddings8)\n# svm_preds9 = get_preds_svm(train_embeddings9,target,test_embeddings9)\n# svm_preds10 = get_preds_svm(train_embeddings10,target,test_embeddings10)\n\n# svm_preds11 = get_preds_svm(train_embeddings11,target,test_embeddings11)\n# svm_preds12 = get_preds_svm(train_embeddings12,target,test_embeddings12)\n# svm_preds13 = get_preds_svm(train_embeddings13,target,test_embeddings13)\n# svm_preds14 = get_preds_svm(train_embeddings14,target,test_embeddings14)\n# svm_preds15 = get_preds_svm(train_embeddings15,target,test_embeddings15)\n# svm_preds16 = get_preds_svm(train_embeddings16,target,test_embeddings16)\n\n# svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5+svm_preds6+svm_preds7+svm_preds8+svm_preds9+svm_preds10+svm_preds11+svm_preds12+svm_preds13+svm_preds14+svm_preds15+svm_preds16)\/16\n\n\nsvm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)\/5\n\n\nsample.target = svm_preds\nsample.to_csv('submission1.csv',index=False)","ce724356":"# model2","d088223b":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\nconfig = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('..\/input\/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\ntrain_embeddings1 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model0\/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'..\/input\/clr-roberta\/model0\/model0.bin')\n\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model1\/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/clr-roberta\/model1\/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model2\/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/clr-roberta\/model2\/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model3\/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/clr-roberta\/model3\/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model4\/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'..\/input\/clr-roberta\/model4\/model4.bin')\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds\n\nsvm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nsvm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\nsvm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)\/5\n\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange\n\ndef convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n        \nclass CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output\n    \ndef make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader\nclass Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\n    \ndef config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )\ndef run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds\n\n\nimport pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\npred_df1 = pd.DataFrame()\npred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold, '..\/input\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n    pred_df2[f'fold{fold+5}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/roberta-large-itptfit\/')\n    pred_df3[f'fold{fold+10}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/commonlit-roberta-large-ii\/')\n    \nsample['target'] = (pred_df2.mean(axis=1)*0.35) + (pred_df1.mean(axis=1)*0.20) + (pred_df3.mean(axis=1) * 0.15) + (svm_preds * 0.30)\nsample.to_csv('submission2.csv', index=False)","29c01dd7":"s1 = pd.read_csv('submission1.csv')\ns2 = pd.read_csv('submission2.csv')\ns3 = pd.read_csv('submission5.csv')\n\ns1['target'] = 0.1*s1['target'] + 0.1*s3['target'] + 0.8*s2['target']\ns1.to_csv('submission.csv', index=False)","2d87ee7f":"\n# s4=pd.read_csv('submission4.csv')#best ,4,78\n# s5=pd.read_csv('submission5.csv')#4.77\n# s6=pd.read_csv('submission6.csv')\n# s7=pd.read_csv('submission7.csv')\n\n\n\n# s4['target'] =0.15*s4['target']+0.15*s5['target']+0.5*s6['target'] +0.2*s7['target']\n# s4.to_csv('submission.csv', index=False)","ef1c0889":"\n# s4=pd.read_csv('submission4.csv')#best ,4,78\n# s5=pd.read_csv('submission5.csv')#4.77\n# s6=pd.read_csv('submission6.csv')\n\n\n\n# s4['target'] =0.25*s4['target']+0.25*s5['target']+0.5*s6['target']\n# s4.to_csv('submission.csv', index=False)","40e2e93c":"# # s1=pd.read_csv('submission1.csv')#4.80\n# s2=pd.read_csv('submission2.csv')#5.11\n# s3=pd.read_csv('submission3.csv')#5.04\n# s4=pd.read_csv('submission4.csv')#best ,4,78\n# s5=pd.read_csv('submission4.csv')#4.77\n\n# # s1['target'] =0.8*s4['target']+ 0.05*s2['target']+0.15*s3['target']\n\n\n# #s1['target'] =0.3*(0.6*s1['target']+0.15*s2['target']+0.25*s3['target']) + 0.7*s4['target']#best\n\n# s2['target'] =0.05*s2['target']+0.15*s3['target'] + 0.4*s4['target']+0.4*s5['target']\n\n# s2.to_csv('submission.csv', index=False)","a3aa889c":"# model lgbm","6764b957":"# model10","3fc43621":"# model 7","f1261955":"# mode 5 - 4.77","5272675f":"# svm","0d976fdf":"# model3","9f99e421":"# model2","497b93e5":"# model6"}}