{"cell_type":{"60442a71":"code","726bf6b8":"code","9561395f":"code","47ff9f50":"code","3e567d7f":"code","302320ee":"code","334548e2":"code","68d4eb58":"code","9f87b638":"code","43b42a3b":"code","330abf89":"code","588fb95f":"code","271a7136":"code","378f7815":"code","539ddc44":"code","edced6a0":"code","5e6172c3":"code","7a043dcb":"code","61123422":"code","0081d37b":"code","cf9f844b":"code","f15a2a10":"code","5090e272":"code","2eb723fe":"code","f3cabfcd":"code","e5ac05dc":"code","a4f5d7f6":"code","8dba0110":"code","dd72c8d4":"code","95f285b2":"code","638689aa":"code","e429cea3":"code","9928d969":"code","532d469a":"code","09845b8f":"code","52fdf650":"code","340f28d0":"code","b90782bf":"code","89c0854a":"code","878c7d2e":"code","055c2e63":"code","628d4cb2":"code","6fc86382":"code","6f3e4548":"code","c8e13f9f":"code","a0e7be02":"code","30e5cb52":"code","8bd9c82b":"code","cf012607":"code","53faa397":"code","7b348405":"code","116534db":"code","73e0026b":"markdown","06b7d8be":"markdown","9a86e9c2":"markdown","3dcfc7c0":"markdown","37b5733a":"markdown","f5da0613":"markdown","452eceda":"markdown","a729b083":"markdown","9585e13e":"markdown","8281860f":"markdown","92ad8cb6":"markdown","3289aa01":"markdown","64ade269":"markdown","5c973efe":"markdown","1c3b0c2a":"markdown","6b420989":"markdown","370c0b1d":"markdown","abb78011":"markdown","e14589b4":"markdown","78f6232a":"markdown","e1a2ebb4":"markdown","969b728b":"markdown","42876b46":"markdown","875a9d48":"markdown","3ffb10b1":"markdown","ed42b268":"markdown","f79dd6bd":"markdown","2c9213e4":"markdown","13968cb0":"markdown","3e5a7b39":"markdown","9453496c":"markdown","758b8792":"markdown","566913db":"markdown","78fccedf":"markdown","fe8836be":"markdown","0825c292":"markdown"},"source":{"60442a71":"import numpy as np\nimport pandas as pd\nimport os\nimport shap\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport lime\nimport lime.lime_tabular\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, accuracy_score\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.impute import SimpleImputer\nshap.initjs()","726bf6b8":"data = pd.read_csv(\"\/kaggle\/input\/szeged-weather\/weatherHistory.csv\")\ndata.describe()\n# Note: The column named \"Loud Cover\" is not making any sense as it is only \"0\" I will drop it during Preprocessing.","9561395f":"data.info()\n# Note: The data does not require any imputing or interpolation as it has no null rows at all.\n# Note: Most of useful columns are numeric, no need to overthink about encoding as some task won't require any.","47ff9f50":"data.head(3)\n# Note: the Dataset is designed to be \"Hourly\". This is good in terms of details, but I rather something less complex. So, I will change it to \"Daily\" on next steps.","3e567d7f":"def simplify_summaries(base_summary):\n    base_split = base_summary.split(\" \")\n    removals_list = [\"Light\",\"Dangerously\",\"Partly\",\"Mostly\",\"and\"]\n    to_be_replaced_list = [\"Breezy\",\"Drizzle\",\"Overcast\"]\n    replacement_list = [\"Windy\",\"Rain\",\"Cloudy\"]\n    for removal in removals_list: \n        if removal in base_split:\n            base_split.remove(removal)\n            \n    for i in range(len(to_be_replaced_list)):\n        if to_be_replaced_list[i] in base_split:\n            base_split.remove(to_be_replaced_list[i])\n            base_split.append(replacement_list[i])\n        \n    base_split.sort()\n    return \" \".join(base_split)","302320ee":"data.Summary = data.Summary.apply(simplify_summaries)\ndata.head(3)\n# much better now as we reduced complexity of it dramatically.","334548e2":"# Dropping the column named \"Loud Cover\" on general dataset \"data\"\ndata.drop(columns=[\"Loud Cover\"], inplace=True)","68d4eb58":"# Changing the original \"Hourly\" dataset to new and simpler \"Daily\"\n# Fixing the Formatted Date for pandas usage.\ndata['Formatted Date'] = pd.to_datetime(data['Formatted Date'], utc=True)\ndata.sort_values(by=['Formatted Date'], inplace=True, ascending=True)","9f87b638":"data.head(4)","43b42a3b":"# Grouping by days to achieve \"Daily\" dataset on what's left as numerical columns for \"Sliding Windows to predict Temp\" task. \nswt_data = data.groupby([data['Formatted Date'].dt.date]).mean()\nswt_data[\"Summary\"] = data[\"Summary\"].groupby([data['Formatted Date'].dt.date]).agg(lambda x:x.value_counts().index[0])\nle = LabelEncoder()\nswt_data.Summary = le.fit_transform(swt_data.Summary)","330abf89":"# Results are sorted and daily.\nswt_data.head() ","588fb95f":"# Checking the results and it is clearly worked.\nswt_data.describe()","271a7136":"# Plotting approx. 2 years to have an idea about what we are working with.\nplt.figure(figsize=(24,8))\nplt.plot(swt_data[\"Temperature (C)\"][:740])\nplt.grid()\nplt.show()","378f7815":"ROLLING_MEAN_PARAMETER = 3\nswt_data[[\"Temperature (C)\",\"Apparent Temperature (C)\",\"Humidity\",\"Wind Speed (km\/h)\", \"Wind Bearing (degrees)\", \"Visibility (km)\", \"Pressure (millibars)\"]] = np.round(swt_data[[\"Temperature (C)\",\"Apparent Temperature (C)\",\"Humidity\",\"Wind Speed (km\/h)\", \"Wind Bearing (degrees)\", \"Visibility (km)\", \"Pressure (millibars)\"]].rolling(ROLLING_MEAN_PARAMETER).mean(),3)\nswt_data.dropna(inplace=True) # dropping the null days that are created by rolling mean","539ddc44":"# Plotting approx. 2 years to have an idea about what we are working with after rolling mean\nplt.figure(figsize=(24,8))\nplt.plot(swt_data[\"Temperature (C)\"][:740])\nplt.grid()\nplt.show()","edced6a0":"# Now I will design the dataset into more trainable sliding windows format.\nN_DAYS_BEFORE = 5\nswt_train = pd.DataFrame()\n\nfor day in range(N_DAYS_BEFORE-1,len(swt_data)):\n    for i in reversed(range(1,N_DAYS_BEFORE)):\n        for j in swt_data.columns:\n            col_name = str(j) + \" - \" + str(i)\n            swt_train.loc[day, col_name] = (swt_data[j][day-i])","5e6172c3":"# each row consist from previous 5 days with details.\nswt_train.head()","7a043dcb":"# first part of the shapes must be the same to labels.\nprint(swt_train.shape)","61123422":"# Prepearing the labels for SWT task\n# ignoring the first 4 days to match training data & only getting values so we won't have issues with date index later on.\nswt_labels = swt_data[\"Temperature (C)\"][N_DAYS_BEFORE-1:].values\n# first part of the shapes must be the same to train.\nprint(swt_labels.shape)","0081d37b":"# Temperature (C) - 1  of 22th feature should be equal to the value of 23th label (today = tomorrow of yesterday)\nprint(\" -- Features -- \\n\",swt_train.iloc[23])\nprint(\"\\n -- Label -- \\n\", swt_labels[22])","cf9f844b":"# Splitting train and test to be able to evaluate properly with some train test ratio.\nswt_train_x, swt_test_x, swt_train_y, swt_test_y = train_test_split(swt_train,swt_labels, test_size=0.1)","f15a2a10":"# Checking the shapes for safety\nprint(\"shape of training dataset features: \",swt_train_x.shape)\nprint(\"shape of training dataset labels: \",swt_train_y.shape)\nprint(\"shape of testing dataset features: \",swt_test_x.shape)\nprint(\"shape of testing dataset labels: \",swt_test_y.shape)","5090e272":"# Prepearing the labels for SWT task\n# ignoring the first 4 days to match training data & only getting values so we won't have issues with date index later on.\nsws_labels = swt_data[\"Summary\"][N_DAYS_BEFORE-1:].values\n# first part of the shapes must be the same to train.\nprint(sws_labels.shape)","2eb723fe":"# splitting (75\/25) as usual\nsws_train_x, sws_test_x, sws_train_y, sws_test_y = train_test_split(swt_train, sws_labels, random_state=41, test_size=0.25)","f3cabfcd":"sws_train_x","e5ac05dc":"# Checking the shapes for safety\nprint(\"shape of training dataset features: \",sws_train_x.shape)\nprint(\"shape of training dataset labels: \",sws_train_y.shape)\nprint(\"shape of testing dataset features: \",sws_test_x.shape)\nprint(\"shape of testing dataset labels: \",sws_test_y.shape)","a4f5d7f6":"# For this approach I will only use 1 column. this will be the \"Temperature (C)\"\nall_temps = swt_data[\"Temperature (C)\"].values\ntrain_temps = []\nlabel_temps = []\nfor i in range(len(all_temps)-30):\n    label_temps.append(all_temps[i+30])\n    train_temps.append(all_temps[i:i+30])\n    \ntrain_temps = np.array(train_temps)\nlabel_temps = np.array(label_temps)","8dba0110":"# last of the tomorrow's array should be same as the today's label \nprint(train_temps[45])\nprint(label_temps[44]) ","dd72c8d4":"# Splitting the train and test \nsdt_train_x = train_temps[:-400]\nsdt_test_x = train_temps[-400:]\nsdt_train_y = label_temps[:-400]\nsdt_test_y = label_temps[-400:]","95f285b2":"# Checking the shapes for safety\nprint(\"shape of training dataset features: \",sdt_train_x.shape)\nprint(\"shape of training dataset labels: \",sdt_train_y.shape)\nprint(\"shape of testing dataset features: \",sdt_test_x.shape)\nprint(\"shape of testing dataset labels: \",sdt_test_y.shape)","638689aa":"rf_model = RandomForestRegressor(max_depth=10)\nrf_model.fit(swt_train_x,swt_train_y)","e429cea3":"my_imputer = SimpleImputer()\nsws_train_x_imp = my_imputer.fit_transform(sws_train_x)\nsws_test_x_imp = my_imputer.transform(sws_test_x)\n\nmy_model = xgb.XGBClassifier(n_estimators=1000, \n                            max_depth=4, \n                            eta=0.05, \n                            base_score=sws_train_y.mean())\nhist = my_model.fit(sws_train_x_imp, sws_train_y, \n                    early_stopping_rounds=5, \n                    eval_set=[(sws_test_x_imp, sws_test_y)], eval_metric='mlogloss', \n                    verbose=10)","9928d969":"lr_model = Ridge()\nlr_model.fit(sdt_train_x,sdt_train_y)","532d469a":"swt_pred_y = rf_model.predict(swt_test_x)\nprint(\"r_square score of the RandomForestRegressor model : \",r2_score(swt_test_y,swt_pred_y))","09845b8f":"explainer = shap.TreeExplainer(rf_model)\nshap_values = explainer.shap_values(swt_train_x)\n\nshap.summary_plot(shap_values, swt_train_x, plot_type=\"bar\");","52fdf650":"shap.summary_plot(shap_values, swt_train_x)","340f28d0":"a = shap.force_plot(explainer.expected_value, shap_values[100,:], swt_train_x.iloc[100,:])\ndisplay(a)\n\nb = shap.force_plot(explainer.expected_value, shap_values[80,:], swt_train_x.iloc[80,:])\ndisplay(b)\n\nc = shap.force_plot(explainer.expected_value, shap_values[70,:], swt_train_x.iloc[70,:])\ndisplay(c)\n\nd = shap.force_plot(explainer.expected_value, shap_values[90,:], swt_train_x.iloc[90,:])\ndisplay(d)","b90782bf":"shap.force_plot(explainer.expected_value, shap_values, swt_train_x)","89c0854a":"print(\"prediction : \",rf_model.predict(swt_test_x.iloc[77].values.reshape(1,32)))\nprint(\"ground truth : \",swt_test_y[77])\n# very accurate prediction.","878c7d2e":"y_pred = my_model.predict(sws_test_x_imp)\naccuracy_score(y_pred, sws_test_y)","055c2e63":"predict_fn = lambda x: my_model.predict_proba(x).astype(float)\nexplainer = lime.lime_tabular.LimeTabularExplainer(sws_test_x_imp, feature_names=sws_test_x.columns, class_names=range(0,14), verbose=True, mode='classification')","628d4cb2":"print(le.inverse_transform(my_model.predict(sws_test_x_imp)[60].ravel()))\nprint(le.inverse_transform(my_model.predict(sws_test_x_imp)[0].ravel()))\nprint(le.inverse_transform(my_model.predict(sws_test_x_imp)[124].ravel()))\n# the indexes will be used later on.","6fc86382":"foggy_instance = sws_test_x.iloc[124].values\ncloudy_instance = sws_test_x.iloc[0].values\nclear_instance = sws_test_x.iloc[60].values","6f3e4548":"exp1 = explainer.explain_instance(foggy_instance, predict_fn, num_features=5, labels=range(0,6))\nexp2 = explainer.explain_instance(cloudy_instance, predict_fn, num_features=5, labels=range(0,6))\nexp3 = explainer.explain_instance(clear_instance, predict_fn, num_features=5, labels=range(0,6))","c8e13f9f":"exp1.show_in_notebook()","a0e7be02":"exp1.as_pyplot_figure(label=3)   # for class of 3, which is foggy\nplt.show()","30e5cb52":"exp1.as_list(label=3)","8bd9c82b":"print(exp1.as_map())","cf012607":"# label 0 is \"clear\"\nexp3.as_pyplot_figure(label=0)\nplt.show()","53faa397":"sdt_pred_y = lr_model.predict(sdt_test_x)\nprint(\"r_square score of the Ridge Regression model : \",r2_score(sdt_test_y,sdt_pred_y))   # the model performs really good.","7b348405":"plt.figure(figsize=(20,6))\nplt.plot(sdt_pred_y)\nplt.plot(sdt_test_y)\nplt.tight_layout()","116534db":"# efe erg\u00fcn","73e0026b":"## General Exploratory Data Analysis\n\n*Starting with importing the required libraries*","06b7d8be":"As we can see above, I displayed force plots of predictions for the 4 different days. <br \/>\nThe most important things to pay attention here are:\n* Model Output Value\n* Base Value\n* Forces that affect the Output value \n<br \/> <br \/>\nTo begin with, the base value is the **average** output value for entire dataset. But as we can see, with the forces (affects of the values of some columns) the Model Output Value moves away from the base value. <br \/>\nNow, we can take a look at the prediction instances above. <br \/> \nAt the last prediciton. We can see that the columns \"Apparent Temperature(C)-1\" and \"Temperature(C)-1\" columns increased the base value. Where the same columns for the day before applied forces to lower it. At the end prediction value become 15.83. <br \/> <br \/> <br \/> \n\nUp next, is an interactive cluster of all predictions. The best part about this is that you can change the x-axis selection to have different force plots for different purposes <br \/> \n\n### Clustered SHAP Values","9a86e9c2":"Now, we can start with the presentation of the explanations with LIME. <br \/> <br \/>\n\n### Showing In Notebook <br \/>\nThis function is designed especially for IPython notebooks such as my notebook that one can look at :) <br \/>\nWhat it shows are basically:\n* Prediction Probabilities of the given instance for the related classes\n* Class by Class opposite sided horizontal bar charts for each feature (Sorted By their affects on the prediciton) \n* An impractical table that shows values for related features. (Color coded by their affects)","3dcfc7c0":"The chart above explains the average impact per given column on predictions. <br \/>\nSo, if we look at it we can say the biggest effect on the prediction (today's temperature) is yesterday's Temperature. <br \/>\nThat is follwed by: Apparent Temperature of yesterday, Temperature of the day before yesterday, Apparent Temperature of the day before yesterday.\n<br \/> <br \/> <br \/>\nNow that we know which columns are more important when it comes to do predictions with our model, we can now see how they were actually affecting the outputs. <br \/>\nFor this, I will use a typical summary plot. Which combines feature importance with feature effects in very visible way.\n### SHAP Summary Plot","37b5733a":"As we see above, spikes are reduced dramatically. Therefore, it is much easier to learn a statistical model.","f5da0613":"* ### Ridge Regression","452eceda":"As we can see above, each point on the summary plot is a Shapley value for a feature and an instance. <br \/>\nThe position on the y-axis is determined by the feature and on the x-axis by the Shapley value. <br \/>\nThe color represents the value of the feature from low to high. <br \/>\nJust like the previous plot, here features are ordered according to their importance. <br \/> <br \/>\n### SHAP Force Plot","a729b083":"If we don't prefer graphs or plots, we can also receive the numerical values without any visualization. <br \/>\nMost common methods are \n### As_list\nand\n### As_map","9585e13e":"We see that there are a lot of spikes everywhere on the plot. This would increase complexity. <br>\nSo, I decided to apply rolling mean to reduce spikes.","8281860f":"* ### XGBoost","92ad8cb6":"# Table of Contents\n* ##  General Exploratory Data Analysis\n\n* ##  Preprocessing of the Tasks\n    - Sliding days as 1D Tempreture Arrays to predict next day's Temperature (SDT)\n    - Sliding windows with multiple columns to predict next day's Temperature [Done] (SWT)\n    - Sliding windows with multiple columns to predict next day's Daily Summary (SWS) [Done]\n\n\n* ##  Machine Learning Models\n    - Random Forest Regression [Done] (RF)\n    - Linear Regression (LR)\n    - XGBoost (XG) [Done-Bad]\n\n\n* ##  Evaluation and understanding predictions with XAI tools\n    - Lime\n    - Lime for Time\n    - SHAP [Done]","3289aa01":"## Machine Learning Models","64ade269":"* ### XG - SWS - LIME","5c973efe":"Now I will initialize the explainers for each instance we declared above. <br \/>\nThe important thing here is to provide how many \"possible labels\" and how many \"features\" you want to present. <br \/>\nFor the purposes of this notebook, I limited those to smaller numbers.","1c3b0c2a":"*Then continue by Exploring the dataset*","6b420989":"Here on this part of the notebook, the first explainable AI tool and it's use case will be demonstrated.\n\nThe tool is **SHAP**. <br \/>\nTo implement this tool for our time series explanation purposes, I will use my **RandomForestRegressor** model which was trained on the Sliding Windows styled dataframe with the Temperature label. <br \/>\nFirstly, we can start by checking if the model is worth to explain or does it require more development. <br \/>\nFor this, I will be using r^2 score. The better r^2 score means better performance. <br \/>\n","370c0b1d":"To make things more clear, I will store the instances above with better variable names.","abb78011":"Here on this part of the notebook, the second explainable AI tool and it's use case will be demonstrated.\n\nThe tool is **LIME**. the name stands for \"Local Interpretable Model-agnostic Explanations\" <br \/>\nTo implement this tool for our time series explanation purposes, I will use my **XGBoost** model which was trained on the Sliding Windows styled dataframe with the Simplified Summary Classes. <br \/>\nFirstly, we can start by checking if the model is worth to explain or does it require more development. <br \/>\nFor this, I will be using accuracy score. The reason of using accuracy is that I implemented the model as if it was a classification problem. <br \/>\n","e14589b4":"## Evaluation and understanding predictions with XAI tools","78f6232a":"* ### LR - SDT - SHAP","e1a2ebb4":"Now that we see reasons better, we can tell that prediction was foggy because on the day before the Visibility was lower than 9km. (Which is quite a good reason for such prediction) ","969b728b":"As one can imagine, for some implementations numerical values like above may need to be used as input. (e.g. specialized plotting methods) <br \/>\nTherefore, it is a nice addition that LIME can provide the output without any graph. <br \/> <br \/>\nNow that we find out how LIME works, let's do a small comparison between some different predictions to see reasoning of the xgboost model.","42876b46":"we need to simplify the summaries as there are too many details we don't need. I wil do it with a custom function.","875a9d48":"## Sliding days as 1D Tempreture Arrays to predict next day's Temperature (SDT)","3ffb10b1":"### As Pyplot Figure\nUp next, the alternative for the ones who are happy with less details. <br \/>\nIn my opinion, the label parameter is the most important part of this function. Becuase by giving this parameter you pick what label to get graph for. ","ed42b268":"## Sliding windows with multiple columns to predict next day's Temperature (SWT)","f79dd6bd":"## Preprocessing of the Tasks","2c9213e4":"* ### Random Forest Regressor","13968cb0":"The figures above may seem complex at first sight, I reckon the best way to look is to start from top left and follow:\n1. Check the class at the top of the progressbar styled graph named \"Prediction probabilities\". (it is 3 with 69% + yellow)\n2. Find the bar chart for it(\"NOT 3\" and \"3\") to look most effective features. (it is \"yesterday's Visibility\" which was lower than ?!?) <br \/>\n    2.1 As you can't see, we don't see it properly :\/ (Don't worry, we will see it soon :D)\n1. You can check rest of the figures to gain some information about other classes. ","3e5a7b39":"As we can see above, the model performs really good on the test data. <br \/> \nSo we shall continue with explaining.  <br \/>  <br \/> <br \/> \nTo begin our explanation on this model and the task I want to use a simple bar representation of importance. <br \/>\nBasically, this will sort the features by decreasing importance for our trained model and plot them.\n\n### SHAP Feature Importance","9453496c":"Now, we need to choose some instances to explain later on. For the purpose of making it different, I will pick 3 different predictions.","758b8792":"* ### RF - SWT - SHAP","566913db":"To simply describe what we see above, we can say \"a cummulative force plot graphs for all predictions\". (This is also the reason of slowness). <br \/>\nGo ahead and change the value on the left-hand side dropdown input if you want to experience interacitveness of the graph.","78fccedf":"## Sliding windows with multiple columns to predict next day's Daily Summary (SWS)","fe8836be":"As shown above, High amount of pressure caused xgboost model to predict as \"Clear\". <br \/> Which is good, because low pressure would make weather cloudy. Unlike low, high pressure would cause \"Dry\" conditions.   <br \/> On the second most effective feature, we see the pressure of the day before yesterday. Which is also nice because we understand that predicitons for being \"Clear\" is mostly based on related fields. ","0825c292":"As we can see above, model performs good enough to be considered as accurate. (Almost 90%.) <br \/> <br \/>\nNext, we need to define a lambda function called \" *predict_fn* \", this function will help us to get prediciton probabilities. <br \/>\nThen, we will set up our explainer. For this, I am using LimeTabularExplainer which explains predictions on tabular (i.e. matrix) data. <br \/> "}}