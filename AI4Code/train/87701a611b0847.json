{"cell_type":{"c2e7750a":"code","59410d9a":"code","0904d7df":"code","0ced2122":"code","d492cc74":"code","f8d9022f":"code","cecffa21":"code","541ea459":"code","582a32b4":"code","2b602528":"code","be4363e6":"code","e4918d29":"code","195e2186":"code","92cd9bab":"code","f39cac3f":"code","86fdb8d2":"code","f6d02af4":"code","4f993dc0":"code","db59b89a":"code","c9a48b96":"code","4388a44d":"code","30a3a50b":"code","4ac4d231":"code","930d008c":"code","fdd343ff":"code","dea44f37":"code","0834c5b0":"code","ffc8ed1a":"code","bfe12ff6":"code","16e8d268":"code","a9f20055":"code","ae69ed24":"code","e060cf8e":"code","77f87d32":"code","453bd9ce":"code","b48d238e":"code","1bbdd7bd":"code","bb663566":"code","69d0273c":"code","ed6a9090":"code","9eb1ba13":"code","bd4eaeef":"code","98bbff09":"code","51610883":"code","3899da37":"code","5260fda4":"code","614dc54f":"code","415e14dd":"code","2d88c4fb":"code","36f5e346":"code","c2518229":"code","2288f6dc":"code","9d3e30b6":"code","bbf918a6":"code","d3f71eaf":"code","36253628":"code","ba4ba459":"code","090b3992":"code","62ffa159":"code","05e7b1a8":"markdown","b8f956d4":"markdown","50ce6722":"markdown","49c82785":"markdown","c7fce9d6":"markdown","57431481":"markdown","ae66bd59":"markdown","8c94f5c8":"markdown","eff3c702":"markdown","b4547506":"markdown","913c656e":"markdown","fa3a8e36":"markdown","47e17096":"markdown","67b00e69":"markdown","5770762f":"markdown","d4e68f03":"markdown","4cb8d0b2":"markdown","ade331af":"markdown","c5ec159a":"markdown","ce40cd30":"markdown","ab145285":"markdown","3163781f":"markdown","e880a15e":"markdown","95623890":"markdown","780d9fb8":"markdown","1b265d63":"markdown","31a553c4":"markdown","6bfdf2d6":"markdown","23376f37":"markdown","2fc8f18f":"markdown","19b1618b":"markdown","d65a3766":"markdown","532e0317":"markdown","b261b8b8":"markdown","b09258d9":"markdown","522781ba":"markdown","649dbdd7":"markdown","b5eb3b6f":"markdown","522f55ff":"markdown","97c4834b":"markdown","2c60de1a":"markdown"},"source":{"c2e7750a":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport math, time, datetime\n# from math import sqrt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import colors\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, scale\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.metrics import (confusion_matrix, mean_squared_error, r2_score, classification_report,\n                            roc_auc_score, roc_curve, precision_recall_curve, auc, log_loss)\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import mutual_info_classif","59410d9a":"df_first = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_first = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_gender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_first.head()","0904d7df":"fig, ax = plt.subplots(figsize = (12, 2))\nax.barh(df_first['Survived'].unique(), df_first['Survived'].value_counts(), align='center', color=['red', 'green'])\nax.text(530, 0, df_first['Survived'].value_counts()[0], ha='center', va='center', color='w', size=20)\nax.text(320, 1, df_first['Survived'].value_counts()[1], ha='center', va='center', color='w', size=20)\nax.set_yticks(df_first['Survived'].unique())\nax.set_yticklabels(df_first['Survived'].unique())\nax.invert_yaxis()\nax.set_ylabel('Survived')\nax.set_title('How many people survived?')\n\nplt.show()","0ced2122":"df_first.info()\nprint(\"----------------------------\")\ndf_test_first.info()","d492cc74":"## plot graphic of missing values\nmsno.matrix(df_first, figsize=(12, 6))","f8d9022f":"# plot graphic of missing values\nmsno.matrix(df_test_first, figsize=(12, 6))","cecffa21":"# drop unnecessary columns, which won't be used in analysis and prediction\ndf = df_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)\ndf_test = df_test_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","541ea459":"# Checking for missing values in train data\ndf.isnull().sum()","582a32b4":"# Checking for missing values in test data\ndf_test.isnull().sum()","2b602528":"# getting the Pclass of the Fare missing value\nPclass_fare_mis = df_test[df_test['Fare'].isnull()]['Pclass'].values[0]\nprint(Pclass_fare_mis)\nprint(\"----------------------------\")\n# getting the mean of Fare for the Pclass 3\ndf_Pclass_mean = df_test.groupby(['Pclass']).mean().loc[Pclass_fare_mis,'Fare']\n\n# fill NaN values in Fare column with mean\ndf_test.loc[152, 'Fare'] = df_Pclass_mean\n\ndf_test.isnull().sum()","be4363e6":"# getting the Pclass of the Embarked missing value\nPclass_embarked_mis = df[df['Embarked'].isnull()]['Pclass'].values\nprint(Pclass_embarked_mis)\nprint(\"----------------------------\")\n# distribution in Embarked by their Pclass\nprint(df[df['Pclass'] == 1]['Embarked'].value_counts())\n\n# for Pclass = 1, the most occurring values are S and C. \n# Therefore, one is filled as 'S', and the other as 'C'. \ndf.loc[61, 'Embarked'] = 'S'\ndf.loc[829, 'Embarked'] = 'C'\n\ndf.isnull().sum()","e4918d29":"mis_val_female = df[df['Sex'] == 'female']['Age'].isna().sum()\nmis_val_male = df[df['Sex'] == 'male']['Age'].isna().sum()\n\nprint(f'Missing values in Age for female: {mis_val_female}')\nprint(f'Missing values in Age for male: {mis_val_male}')","195e2186":"def generate_random_numbers(df):\n    nan_count = df.isna().sum()\n    \n    ax = df.hist(bins=10, density=True, stacked=True, color='teal', alpha=0.6)\n    ax1 = df.plot(kind='kde', color='teal')\n    \n    # mean age\n    df_mean = df.mean(skipna=True)\n    # median age\n    df_median = df.median(skipna=True)\n    # std age\n    df_std = df.std(skipna=True)\n        \n    # getting density peak values\n    density = stats.gaussian_kde(df.dropna())\n    xs = np.linspace(df.min(),df.max(),200)\n    ys = density(xs)\n    index = np.argmax(ys)\n    max_y = ys[index]\n    max_x = xs[index]\n    \n    # density peak plot\n    ax.axvline(max_x, 0, 1, color='blue', label='peak(x): {:.2f}'.format(max_x))\n    \n    # once the median is closer than the mean to the x value of the density peak, median will be used.\n    # otherwise, the mean will be used to fill in the missing values by generated random numbers.\n    if abs(max_x - df_mean) <=  abs(max_x - df_median):\n        rand_numbers = np.random.randint(df_mean - df_std, df_mean + df_std, \n                                   size = nan_count)\n        ax.axvline(df_mean, 0, 1, color='red', label='mean: {:.2f}'.format(df_mean))\n        # we will generate random numbers using the mean {between (mean - std) & (mean + std)}\n        ax.axvspan(df_mean - df_std, df_mean + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n    else: \n        rand_numbers = np.random.randint(df_median - df_std, df_median + df_std, \n                                   size = nan_count)\n        ax.axvline(df_median, 0, 1, color='red', label='median: {:.2f}'.format(df_median))\n        \n        # we will generate random numbers using the median {between (median - std) & (median + std)}\n        ax.axvspan(df_median - df_std, df_median + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n\n    ax.set(xlabel='Age')\n    ax.legend(title='Female')\n    plt.show()\n\n    return (rand_numbers)","92cd9bab":"df_female_age = df[df['Sex'] == 'female']['Age']\ndf_female_rand_numbers = generate_random_numbers(df_female_age)\n\ndf_female_null = (df[df['Age'].isnull()]['Sex'] == 'female')\ndf_female_ind_null = df_female_null[df_female_null].index\ndf.loc[df_female_ind_null, 'Age'] = df_female_rand_numbers\n\nprint(df.isnull().sum())","f39cac3f":"df_male_age = df[df['Sex'] == 'male']['Age']\ndf_male_rand_numbers = generate_random_numbers(df_male_age)\n\ndf_male_null = (df[df['Age'].isnull()]['Sex'] == 'male')\ndf_male_ind_null = df_male_null[df_male_null].index\ndf.loc[df_male_ind_null, 'Age'] = df_male_rand_numbers\n\nprint(df.isnull().sum())","86fdb8d2":"df_test_female_age = df_test[df_test['Sex'] == 'female']['Age']\ndf_test_female_rand_numbers = generate_random_numbers(df_test_female_age)\n\ndf_test_female_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'female')\ndf_test_female_ind_null = df_test_female_null[df_test_female_null].index\ndf_test.loc[df_test_female_ind_null, 'Age'] = df_test_female_rand_numbers\n\nprint(df_test.isnull().sum())","f6d02af4":"df_test_male_age = df_test[df_test['Sex'] == 'male']['Age']\ndf_test_male_rand_numbers = generate_random_numbers(df_test_male_age)\n\ndf_test_male_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'male')\ndf_test_male_ind_null = df_test_male_null[df_test_male_null].index\ndf_test.loc[df_test_male_ind_null, 'Age'] = df_test_male_rand_numbers\n\nprint(df_test.isnull().sum())","4f993dc0":"## Create categorical variable for traveling alone\ndf['TravelAlone']=np.where((df[\"SibSp\"]+df[\"Parch\"])>0, 0, 1)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","db59b89a":"## Create categorical variable for traveling alone\ndf_test['TravelAlone']=np.where((df_test[\"SibSp\"]+df_test[\"Parch\"])>0, 0, 1)\ndf_test.drop('SibSp', axis=1, inplace=True)\ndf_test.drop('Parch', axis=1, inplace=True)\ndf_test.head()","c9a48b96":"col_order = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone', 'Survived']\ndf=df[col_order]\ndf.head()","4388a44d":"for col in df:\n    unique_vals = np.unique(df[col])\n    nr_values = len(unique_vals)\n    if nr_values < 10:\n        print(f'The number of values for feature {col} :{nr_values} -- {unique_vals}')\n    else:\n        print(f'The number of values for feature {col} :{nr_values}')","30a3a50b":"axes = pd.plotting.scatter_matrix(df, alpha=0.7, figsize=(14, 8), diagonal='kde')\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(0)\n    ax.yaxis.label.set_rotation(90)\n    ax.yaxis.label.set_ha('right')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","4ac4d231":"def subplot_graph(df, features):\n    f_count = len(features)\n    cols = f_count\n    if f_count < 4:\n        rows = 1\n    else:\n        rows = math.ceil(f_count\/3)\n        cols = 3\n                \n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n        \n    sub_rows = 0\n    sub_cols = 0    \n    for f in features:\n        if sub_cols > cols-1:\n            sub_cols = 0\n            sub_rows += 1\n            \n        unique_vals = len(np.unique(df[f]))\n        if unique_vals < 10:\n            df_sub = df.groupby(df.columns[-1])[f].value_counts().sort_index(level=0)\n            x_no = list(df_sub.xs(0, level=0).index)\n            y_no = list(df_sub.xs(0, level=0).values)\n            x_yes = list(df_sub.xs(1, level=0).index)\n            y_yes = list(df_sub.xs(1, level=0).values)\n            list_no = list(zip(x_no, y_no))\n            list_yes = list(zip(x_yes, y_yes))\n            df_no = pd.DataFrame(list_no)\n            df_yes = pd.DataFrame(list_yes)\n\n            df_plot = pd.merge(df_no, df_yes, on=df_no.columns[0], how=\"outer\")\n            df_plot = df_plot.fillna(0)\n            try:\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                   label='1', color='steelblue')\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n            except:\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                             label='1', color='steelblue')  \n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        else:    \n            df_3 = df[df[df.columns[-1]] == 0][f]\n            df_4 = df[df[df.columns[-1]] == 1][f]\n            try:\n                df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_xlim(xmin=0)\n            except:\n                df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n            \n        sub_cols += 1\n        \n    fig.tight_layout()\n    fig.show()\n\nfeatures = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone']\nsubplot_graph(df, features)","930d008c":"f_count = len(features)\ncols = f_count\nif f_count < 4:\n    rows = 1\nelse:\n    rows = math.ceil(f_count\/3)\n    cols = 3\n    \n# Set up the matplotlib figure\nfig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n\n\nsub_rows = 0\nsub_cols = 0    \nfor f in features:\n    if sub_cols > cols-1:\n        sub_cols = 0\n        sub_rows += 1\n        \n    \n    unique_vals = len(np.unique(df[f]))\n    if unique_vals < 10:\n        df_yy = pd.DataFrame()\n        df1 = df.groupby([f])[df.columns[-1]].value_counts().sort_index(level=0)\n        df_uniq = df1.index.levels[0]\n        f_uniqs = []\n        sur_rates = []\n        for uniq in df_uniq:\n            try:\n                df2 = df1.xs(uniq, level=0)\n                df_sum = df2.sum()\n                df_lev_1_each = df2[1]\n                sur_rate = round((df_lev_1_each \/ df_sum)*100, 1)\n                sur_rates.append(sur_rate)\n                f_uniqs.append(uniq)\n            except:\n                pass\n\n        df_xx = pd.DataFrame(sur_rates, index=f_uniqs, columns=[f])\n        df_yy = pd.concat([df_yy, df_xx], axis=1)\n        \n        df_sub = df_yy[f].dropna()\n#         print(df_sub)\n\n        try:\n            axes[sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        except:\n            axes[sub_rows, sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_rows, sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n    else:    \n        df_3 = df[df[df.columns[-1]] == 0][f]\n        df_4 = df[df[df.columns[-1]] == 1][f]\n        try:\n            df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].legend(title=df.columns[-1])\n            axes[sub_cols].set_xlim(xmin=0)\n        except:\n            df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n            axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n        \n    sub_cols += 1\n    \nfig.tight_layout()\nfig.show()","fdd343ff":"def create_heatmap(hm, figsize=(7, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    im = ax.imshow(hm, \n    #                vmin=0, vmax=10, \n                   cmap='viridis', aspect='auto')\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(hm.columns)))\n    ax.set_yticks(np.arange(len(hm.columns)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(hm.columns)\n    ax.set_yticklabels(hm.columns)\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n    ax.set_xticks(np.arange(hm.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(hm.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(hm.columns)):\n        for j in range(len(hm.columns)):\n            hm_val = round(hm.values[i, j], 2)\n            if hm_val > 0.85:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"black\", size=16)\n            else:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"w\", size=16)\n\n    fig.tight_layout()\n    plt.show()","dea44f37":"hm = df.corr()\ncreate_heatmap(hm)","0834c5b0":"X = df[df.columns[:-1]]\ny = df[df.columns[-1]]\nX_kaggle = df_test.copy()\nprint(X.shape)\nprint(X_kaggle.shape)","ffc8ed1a":"# There is no categorical columns in the dataframe\ncategorical_cols = list(set(X.columns) - set(X._get_numeric_data().columns))\ncategorical_cols","bfe12ff6":"numerical_cols = list(X._get_numeric_data().columns)\nnumerical_cols","16e8d268":"categorical_cols + ['Pclass', 'TravelAlone']","a9f20055":"from sklearn.compose import make_column_transformer\n\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols + ['Pclass', 'TravelAlone']),\n                                      remainder='passthrough')\nX_trans = column_trans.fit_transform(X)\nX_kaggle_trans = column_trans.fit_transform(X_kaggle)\n\ncols = []\nfor i in column_trans.transformers_[0][2]:\n    cols_enc = sorted(X[i].unique())\n    for col_enc in cols_enc:\n        col_name = str(i) + '_' + str(col_enc)\n        cols.append(col_name)\n        \nfor i in column_trans.transformers_[1][2]:\n    col_name = X.columns[i]\n    cols.append(col_name)\n    \nX_encoded = pd.DataFrame(X_trans, columns=cols)\nX_kaggle_encoded = pd.DataFrame(X_kaggle_trans, columns=cols)\nprint(X_encoded.shape)\nprint(X_kaggle_encoded.shape)\nX_encoded.head()","ae69ed24":"X_kaggle_encoded.head()","e060cf8e":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_encoded)\nX_encoded.columns[var_thres.get_support()]\n\nconstant_columns = [column for column in X_encoded.columns\n                    if column not in X_encoded.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","77f87d32":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, train_size = 0.8, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","453bd9ce":"hm_X_train = X_train.corr()\ncreate_heatmap(hm_X_train, figsize=(10, 6))","b48d238e":"#  to select highly correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                \n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","1bbdd7bd":"corr_features = correlation(X_train, 0.85)\ncorr_features","bb663566":"X_train = X_train.drop(corr_features, axis=1)\nX_test = X_test.drop(corr_features, axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(corr_features, axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","69d0273c":"# Run a Tree-based estimators (i.e. decision trees & random forests)\ndt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\ndt.fit(X_train, y_train)\n\nfi_col = []\nfi = []\nfor i,column in enumerate(X_train):\n#     print('The feature importance for {} is: {:.2%}'.format(column, dt.feature_importances_[i]))    \n    fi_col.append(column)\n    fi.append(dt.feature_importances_[i])\n\n# Creating a Dataframe\nfi_df = zip(fi_col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\n\n# Ordering the data\nfi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index(drop=True)\nfi_df","ed6a9090":"# determine the mutual information\nmutual_info = mutual_info_classif(X_train.values, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","9eb1ba13":"def model_evaluation(nb, cols):\n#     nb = BernoulliNB(binarize=0)\n#     nb = MultinomialNB()\n    nb.fit(X_train[cols], y_train)\n    y_pred_train = nb.predict(X_train[cols])\n    y_pred_test = nb.predict(X_test[cols])\n\n#     print(\"Test Accuracy: {:.1%}\".format(bnb.score(X_test[cols],y_test)))\n#     print(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\n#     print(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\n#     print('='*10)\n\n    nb_acc = nb.score(X_test[cols],y_test)\n    nb_mse = mean_squared_error(y_test, y_pred_test)\n    nb_r2 = r2_score(y_test, y_pred_test)\n    \n    return nb_acc, nb_mse, nb_r2","bd4eaeef":"import itertools\n\nnb_models = [BernoulliNB(), MultinomialNB(), GaussianNB()]\nstuff = X_train.columns\n\nbest_nb = []\nbest_cols = []\nbest_acc = []\nbest_mse = []\nbest_r2 = []\nfor nb in nb_models:\n    for L in range(0, len(X_train)+1):\n        for subset in itertools.combinations(stuff, L):\n            sub_list = list(subset)\n            if len(sub_list) != 0:\n                nb_acc, nb_mse, nb_r2 = model_evaluation(nb, sub_list)\n                best_cols.append(sub_list)\n                best_acc.append(nb_acc)\n                best_mse.append(nb_mse)\n                best_r2.append(nb_r2)\n                best_nb.append(nb)","98bbff09":"best_zip = zip(best_cols, best_nb, best_acc, best_mse, best_r2)\ndf_best_acc = pd.DataFrame(best_zip, columns=['columns', 'nb', 'accuracy', 'mse', 'r2'])\ndf_best_acc = df_best_acc.sort_values('accuracy', ascending = False).reset_index(drop=True)\npd.set_option('max_colwidth', -1)\ndf_best_acc.head(10)","51610883":"cols_final = ['Sex_female', 'Embarked_Q', 'Pclass_1', 'Pclass_2', 'TravelAlone_0', 'Age']\n\nX_train_final = X_train[cols_final]\nX_test_final = X_test[cols_final]\nX_kaggle_final = X_kaggle_encoded[cols_final]\n\nprint(X_train_final.shape)\nprint(X_test_final.shape)\nprint(X_kaggle_final.shape)","3899da37":"nb = MultinomialNB()\nnb.fit(X_train_final, y_train)\ny_pred_train = nb.predict(X_train_final)\npred_proba_train = nb.predict_proba(X_train_final)\n\nprint(\"Train Accuracy: {:.1%}\".format(nb.score(X_train_final,y_train)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_train, y_pred_train))\nprint('='*10)\nprint(classification_report(y_train, y_pred_train))","5260fda4":"y_pred_test = nb.predict(X_test_final)\npred_proba_test = nb.predict_proba(X_test_final)\n\nprint(\"Test Accuracy: {:.1%}\".format(nb.score(X_test_final,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","614dc54f":"def confusion_matrix_func(cm, cm_title):\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    # Plot the heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap='Reds', aspect='auto')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(cm.tolist())))\n    ax.set_yticks(np.arange(len(cm.tolist())))\n\n\n    thresh = cm.max() \/ 1.5\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(cm.tolist())):\n        for j in range(len(cm.tolist())):\n            text = ax.text(j, i, cm.tolist()[i][j],\n                           ha=\"center\", va=\"center\", size=16,\n                           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n\n    plt.xlabel('Actual value', size=16)\n    plt.ylabel('Predicted value', size=16)\n    plt.title(cm_title, size=20, x=0.2, y=1.2)\n    plt.show()\n# https:\/\/matplotlib.org\/stable\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html","415e14dd":"def Confusion_matrix_metrics(TP, FP, FN, TN):\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP \/ (TP + FN)\n    print('The True Positive Rate is: {:.2%}'.format(TPR))\n    # Specificity, selectivity or true negative rate (TNR)\n    TNR = TN \/ (TN + FP)\n    print('The True Negative Rate is: {:.2%}'.format(TNR))\n    print('='*10)\n\n    # accuracy (ACC)\n    ACC = (TP + TN) \/ (TP + TN + FP + FN)\n    print('The Accuracy is: {:.2%}'.format(ACC))\n    # balanced accuracy (BA)\n    BA = (TPR + TNR) \/ 2\n    print('The Balanced Accuracy is: {:.2%}'.format(BA))\n    print('='*10)\n\n    # Precision or positive predictive value\n    PPV = TP \/ (TP + FP)\n    print('The Precision is: {:.2%}'.format(PPV))\n    # negative predictive value (NPV)\n    NPV = TN \/ (TN + FN)\n    print('The Negative Predictive Value is: {:.2%}'.format(NPV))\n    # false discovery rate (FDR)\n    FDR = 1 - PPV\n    print('The False Discovery Rate is: {:.2%}'.format(FDR))\n    # false omission rate (FOR)\n    FOR = 1 - NPV\n    print('The False Omission Rate is: {:.2%}'.format(FOR))\n    print('='*10)\n\n    # prevalence threshold (PT)\n    PT = (math.sqrt(TPR*(1 - TNR)) + TNR - 1)\/(TPR + TNR - 1)\n    print('The Prevalence Threshold is: {:.2}'.format(PT))\n    # F1 score\n    F1 = 2*TP \/ (2*TP + FP + FN)\n    print('The F1 Score is: {:.2}'.format(F1))\n    # Matthews correlation coefficient (MCC) or phi coefficient\n    MCC = ((TP*TN) - (FP*FN)) \/ math.sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n    print('The Matthews Correlation Coefficient is: {:.2}'.format(MCC))\n    print('='*10)\n\n    # False positive rate or False alarm rate\n    FPR = FP \/ (FP + TN)\n    print('The False positive rate is: {:.2}'.format(FPR))\n    # False negative rate or Miss Rate\n    FNR = FN \/ (FN + TP)\n    print('The False Negative Rate is: {:.2%}'.format(FNR))","2d88c4fb":"cm = confusion_matrix(y_train, y_pred_train).T\nconfusion_matrix_func(cm, cm_title=\"Confusion Matrix_Train\")","36f5e346":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","c2518229":"cm_test = confusion_matrix(y_test, y_pred_test).T\nconfusion_matrix_func(cm_test, cm_title=\"Confusion Matrix_Test\")","2288f6dc":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm_test.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","9d3e30b6":"# TRAIN\n# calculate scores\nlr_auc = roc_auc_score(y_train, pred_proba_train[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, thresholds = roc_curve(y_train, pred_proba_train[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc = pd.DataFrame({\n    'False Positive Rate': lr_fpr,\n    'True Positive Rate': lr_tpr\n}, index=thresholds)\ndf_roc.index.name = \"Thresholds\"\ndf_roc.columns.name = \"Rate\"\n\n\n# TEST\n# calculate scores\nlr_auc_test = roc_auc_score(y_test, pred_proba_test[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc_test))\n# calculate roc curves\nlr_fpr_test, lr_tpr_test, thresholds_test = roc_curve(y_test, pred_proba_test[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc_test = pd.DataFrame({\n    'False Positive Rate': lr_fpr_test,\n    'True Positive Rate': lr_tpr_test\n}, index=thresholds_test)\ndf_roc_test.index.name = \"Thresholds\"\ndf_roc_test.columns.name = \"Rate\"\n\n\n# GRAPH\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# row=0, col=0\naxes[0, 0].plot(df_roc.iloc[:,0], df_roc.iloc[:,1], color='red', linewidth=2, \n                label=f'AUC={lr_auc:.2f}')\naxes[0, 0].fill_between(df_roc.iloc[:,0], df_roc.iloc[:,1], 0, color='red', alpha=0.3)\naxes[0, 0].plot(df_roc_test.iloc[:,0], df_roc_test.iloc[:,1], color='black', linewidth=2, \n                label=f'AUC_test={lr_auc_test:.2f}')\naxes[0, 0].plot([0, 1], [0, 1], color='green', linestyle='--', linewidth=1,\n                label='No Skill')\n\n# index of the first threshold for which the sensibility > 0.90\nidx = np.min(np.where(lr_tpr > 0.90))\naxes[0, 0].plot([0,lr_fpr[idx]], [lr_tpr[idx],lr_tpr[idx]], 'k--', color='blue')\naxes[0, 0].plot([lr_fpr[idx],lr_fpr[idx]], [0,lr_tpr[idx]], 'k--', color='blue')\n# Annotation\naxes[0, 0].annotate('(%.2f, %.2f)'%(lr_fpr[idx], lr_tpr[idx]),\n            (lr_fpr[idx], lr_tpr[idx]), \n            xytext =(-2 * 50, -30),\n            textcoords ='offset points',\n            bbox = dict(boxstyle =\"round\", fc =\"0.8\"), \n            arrowprops = dict(arrowstyle = \"->\"))\n\naxes[0, 0].set_xlabel('False Positive Rate', size=12)\naxes[0, 0].set_ylabel('True Positive Rate (recall)', size=12)\naxes[0, 0].legend(title='kNN')\naxes[0, 0].set_title('ROC curve', color='red', size=14)\n\n# row=0, col=1\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"True Positive Rate\"][1:], color='blue', linewidth=2, \n                label='TPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"True Positive Rate\"][1:], color='black', linewidth=2, \n                label='TPR_test')\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"False Positive Rate\"][1:], color='orange', linewidth=2, \n                label='FPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"False Positive Rate\"][1:], color='black', linewidth=2, \n                label='FPR_test')\n\naxes[0, 1].set_xlabel('Threshold', size=12)\naxes[0, 1].legend()\naxes[0, 1].set_title('TPR and FPR at every threshold', color='red', size=14)\n\n# row=1, col=0\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_proba_test[:, 1])\n\naxes[1, 0].plot(recall, precision, color='green', linewidth=2, \n                label=f'PR_Curve (AUC={auc(lr_fpr, lr_tpr):.2f})')\naxes[1, 0].fill_between(recall, precision, 0, color='green', alpha=0.3)\n\naxes[1, 0].set_xlabel('Recall', size=12)\naxes[1, 0].set_ylabel('Precision', size=12)\naxes[1, 0].legend()\naxes[1, 0].set_title('Precision-Recall Curve', color='red', size=14)\n\nfig.tight_layout()\nfig.show()","bbf918a6":"# Running Log loss on training\nprint('The Log Loss on Training is: {:.2}'.format(log_loss(y_train, pred_proba_train[:, 1])))\n\n# Running Log loss on testing\nprint('The Log Loss on Testing Dataset is: {:.2}'.format(log_loss(y_test, pred_proba_test[:, 1])))","d3f71eaf":"df_gender_sub.head()","36253628":"submission = df_gender_sub.drop('Survived', axis=1)\nX_kaggle_final = scale(X_kaggle_final)\ny_pred_kaggle = nb.predict(X_kaggle_final)\nsubmission['Survived'] = y_pred_kaggle\nsubmission.head()","ba4ba459":"# Are our test and submission dataframes the same length?\nif len(submission) == len(df_gender_sub):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","090b3992":"# Convert submisison dataframe to csv for submission to csv for Kaggle submisison\nsubmission.to_csv('titanic_submission_nb.csv', index=False)\nprint('Submission CSV is ready!')","62ffa159":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"titanic_submission_nb.csv\")\nsubmissions_check.head()","05e7b1a8":"<a id='461'><\/a>\n#### 4_6_1 Feature Selection - Drop Features Using Pearson Correlation","b8f956d4":"Mutual information (MI) measures the dependency between the variables. Higher values mean higher dependency.","50ce6722":"## Index\n\n[1 Importing packages](#1)<br>\n[2 Read CSV train\/test files into DataFrame](#2)<br>\n[3 Data Preprocessing](#3)<br>\n    <ul>\n        <li>[3_1 Missing Values](#31)<\/li>\n            <ul><li>[3_1_1 Fare - Missing Values](#311)<\/li>\n            <li>[3_1_2 Embarked - Missing Values](#12)<\/li>\n            <li>[3_1_3 Age - Missing Values](#313)<\/li><\/ul>\n        <li>[3_2 Combine SibSp and Parch for Simplicity](#32)<\/li>\n        <li>[3_3 Move the dependent column to the end](#33)<\/li>\n        <li>[3_4 Investigate all the elements within each Feature](#34)<\/li>\n        <li>[3_5 Exploratory Data Analysis](#35)<\/li>\n    <\/ul>\n[4 Regressions and Results](#4)<br>\n    <ul>\n        <li>[4_1 Separate the dataset](#41)<\/li>\n        <li>[4_2 Check categorical columns](#42)<\/li>\n        <li>[4_3 One_Hot_Encoding](#43)<\/li>\n        <li>[4_4 Check zero variance features](#44)<\/li>\n        <li>[4_5 Separate the dataset into train and test](#45)<\/li>\n        <li>[4_6 Feature Selection](#46)<\/li>\n            <ul><li>[4_6_1 Feature Selection - Drop Features Using Pearson Correlation](#461)<\/li>\n            <li>[4_6_2 Feature_Selection - Tree-based](#462)<\/li>\n            <li>[4_6_3 Feature_Selection - Mutual information](#463)<\/li>\n            <li>[4_6_4 Feature_Selection - Feature_Selection - Evaluation of all column combinations](#464)<\/li>\n            <li>[4_6_5 Feature_Selection - Final](#465)<\/li><\/ul>\n        <li>[4_7 Evaluating The Naive Bayes Model](#47)<\/li>\n        <li>[4_8 Confusion matrix](#48)<\/li>\n        <li>[4_9 roc curve and auc](#49)<\/li>\n        <li>[4_10 Logarithmic loss](#410)<\/li>\n[5. Submission](#5)<br>","49c82785":"<a id='2'><\/a>\n## 2 Read CSV train\/test files into DataFrame","c7fce9d6":"##### 3_1_2_1 Missing values in train data","57431481":"<a id='41'><\/a>\n### 4_1 Separate the dataset","ae66bd59":"<a id='321'><\/a>\n#### 3_1_1 Fare - Missing Values","8c94f5c8":"<a id='31'><\/a>\n### 3_1 Missing Values","eff3c702":"<a id='312'><\/a>\n#### 3_1_2 Embarked - Missing Values","b4547506":"<a id='5'><\/a>\n## 5. Submission","913c656e":"<a id='49'><\/a>\n### 4.9. roc curve and auc","fa3a8e36":"pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","47e17096":"<a id='463'><\/a>\n#### 4_6_3 Feature_Selection - Mutual information","67b00e69":"{'Sex_male'} {'Sex_female'} and {'TravelAlone_0'} {'TravelAlone_1'} are highly correlated features in the dataset.","5770762f":"**The goal** is to predict the target variable(Survived) using logistic regression.","d4e68f03":"<a id='44'><\/a>\n### 4_4 Check zero variance features","4cb8d0b2":"<a id='1'><\/a>\n## 1 Importing packages","ade331af":"##### 3_1_3_2 Missing values in test data","c5ec159a":"<a id='34'><\/a>\n### 3_4 Investigate all the elements within each Feature ","ce40cd30":"<a id='465'><\/a>\n#### 4_6_5 Feature_Selection - Final","ab145285":"<a id='42'><\/a>\n### 4_2 Check categorical columns","3163781f":"<a id='46'><\/a>\n### 4_6 Feature Selection","e880a15e":"<a id='48'><\/a>\n### 4.8. Confusion matrix","95623890":"<a id='32'><\/a>\n### 3_2 Combine SibSp and Parch for Simplicity","780d9fb8":"<a id='4'><\/a>\n## 4 Regressions and Results","1b265d63":"#### 3_2_2 Test data","31a553c4":"<a id='45'><\/a>\n### 4_5 Separate the dataset into train and test","6bfdf2d6":"##### 3_1_3_1 Missing values in train data","23376f37":"PLife -> Life is unfortunately not fair. If you're a 3rd class person in this world, it's a chance for you to survive.\n\nSex -> Positive discrimination has been made against women here as well.\n\nAge -> The age distribution for survivors and deceased is very similar except for children. The travelers strived for the survival of the children.\n\nFare -> Passengers who pay lower fare seem less likely to survive.\n\nEmbarked -> Although the number of boarders in Southhampton is higher than those in Cherbourg, the survival rate of Cherbourg is higher. This is probably related to the socioeconomic situation.","2fc8f18f":"<a id='3'><\/a>\n## 3 Data Preprocessing","19b1618b":"<a id='410'><\/a>\n### 4.10. Logarithmic loss","d65a3766":"#### 3_2_1 Train data","532e0317":"<a id='43'><\/a>\n### 4_3 One-Hot Encoding","b261b8b8":"<a id='35'><\/a>\n### 3_5 Exploratory Data Analysis","b09258d9":"![titanic_data_dict.png](attachment:titanic_data_dict.png)","522781ba":"<a id='47'><\/a>\n### 4.7. Evaluating The Naive Bayes Model","649dbdd7":"<a id='462'><\/a>\n#### 4_6_2 Feature_Selection - Tree-based","b5eb3b6f":"<a id='464'><\/a>\n#### 4_6_4 Feature_Selection - Evaluation of all column combinations","522f55ff":"<a id='33'><\/a>\n### 3_3 Move the dependent column to the end","97c4834b":"##### 3_1_1_1 Missing values in test data","2c60de1a":"<a id='313'><\/a>\n#### 3_1_3 Age - Missing Values"}}