{"cell_type":{"06f2424a":"code","5a32ef62":"code","6b002230":"code","a0cb0f17":"code","7880fe2f":"code","d2e7752f":"code","cf66d4eb":"code","2d334e28":"code","5d95e2fc":"code","9bdb4464":"code","37b03aac":"code","84bc185f":"code","879cdc58":"code","579a8675":"code","6de1edad":"code","fdd115b1":"code","ff7811fb":"code","ef337263":"code","3ebf546a":"code","9504a659":"code","5a252852":"code","7f1c298f":"code","47a894c8":"code","e7cea6ea":"code","7a019d7d":"code","e068e4a4":"code","3bafb237":"code","2494809d":"code","5e57cc3b":"code","705f1162":"code","4ec69db7":"code","d5460455":"code","b3d571cc":"code","978f1865":"code","5d7bcbe3":"code","c714b688":"code","bedc8a98":"code","ea9a17d9":"code","4e11356f":"code","4d79be95":"code","04a3aa75":"code","cc383005":"code","27dd7df3":"code","c1e1644a":"code","9d42efec":"code","626bbc11":"code","5c340bb4":"code","85ae5f3a":"code","e68a9e65":"code","c8c22643":"code","dd9b0c2a":"code","8ee226c9":"code","b37d049d":"code","086ca475":"code","6d4a2368":"code","b3943a8f":"code","22a7edd3":"code","7c354ec5":"code","2b4542a2":"code","6561ca7f":"code","dbc7bed2":"code","55e53721":"code","800520ad":"code","6c979152":"code","b0a3396c":"code","448bd23e":"code","35ebdbfa":"code","08e93463":"code","be596702":"code","c838c471":"code","fe36637c":"code","b416f8fb":"code","cd40ca75":"markdown","61b22a9d":"markdown","6c71bbe8":"markdown","1f1527cd":"markdown","d23523da":"markdown","616cd591":"markdown","793467a2":"markdown","18422604":"markdown","5b7cb5cc":"markdown","ad647a6a":"markdown","44ccdb5c":"markdown","b4c6d8bc":"markdown","af09990d":"markdown","edc29320":"markdown","d66bcc73":"markdown","334edb0a":"markdown","a1692bc8":"markdown","38cd9f24":"markdown","0d481f7f":"markdown","fd1af02a":"markdown","f9f8a444":"markdown","e715b507":"markdown","9f5eded7":"markdown","41b90d2a":"markdown","b6870dbb":"markdown","665ad94e":"markdown","53406143":"markdown","76e3c016":"markdown","f7939a62":"markdown","d4ad97f0":"markdown","634cfa0a":"markdown","f5a22196":"markdown","7b36ef5e":"markdown","21a4c279":"markdown","936286a1":"markdown","b363efdf":"markdown","a304374f":"markdown","8221eb67":"markdown","a9e27580":"markdown","6d372196":"markdown","0305047c":"markdown","c3d96a56":"markdown","adee1d75":"markdown","fb1e2f87":"markdown","4df4bbb8":"markdown","26adaaa2":"markdown","eb9ff165":"markdown","3ebecbd9":"markdown","3e7ce05f":"markdown","e4117014":"markdown","1c9a1457":"markdown","a063ba5f":"markdown","a88a5aca":"markdown","506f267a":"markdown","7c43e2a4":"markdown","fe989455":"markdown","aca8f702":"markdown","1900a1e7":"markdown","89fa4f9a":"markdown","40c79d35":"markdown","2f76ed5c":"markdown","68301d1d":"markdown","b88c15e5":"markdown","2fe26160":"markdown","1c89f5a4":"markdown","e03d2681":"markdown","5e3f91f7":"markdown","6fb0fbc8":"markdown","86f24b66":"markdown","edec0e82":"markdown","0b0ec47d":"markdown","25c1c594":"markdown","70bf3f9b":"markdown","6085b7ff":"markdown","8008ed96":"markdown","addf5cac":"markdown","5952b85b":"markdown","96228a29":"markdown","9f18b2bf":"markdown","5608ce83":"markdown"},"source":{"06f2424a":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport collections\nimport warnings\nfrom kaggle.competitions import twosigmanews\nfrom datetime import datetime\nfrom wordcloud import WordCloud\n\nwarnings.filterwarnings('ignore')\n#getting environment for accessing full data\nenv = twosigmanews.make_env()","5a32ef62":"market_train_full_df = env.get_training_data()[0]\nsample_market_df = pd.read_csv(\"..\/input\/marketdata_sample.csv\")\nsample_news_df = pd.read_csv(\"..\/input\/news_sample.csv\")","6b002230":"\"market_train_full_df dimention:{}\".format(market_train_full_df.shape)","a0cb0f17":"market_train_full_df.head(5)","7880fe2f":"market_train_full_df.columns","d2e7752f":"market_train_full_df.time.describe()","cf66d4eb":"# pd.to_datetime(market_train_full_df.time).apply(lambda x: pd.Series({\"daily\":datetime.date(x)}))[\"daily\"].value_counts()\nfig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(market_train_full_df.time.dt.date.value_counts().sort_index().index, market_train_full_df.time.dt.date.value_counts().sort_index().values)","2d334e28":"market_train_full_df.assetCode.describe()","5d95e2fc":"market_train_full_df.assetCode.value_counts().describe()","9bdb4464":"list(market_train_full_df.assetCode)[:5]","37b03aac":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9))\naxes.set_title(\"Daily assetCodes Violin\")\naxes.set_ylabel(\"Repetition\")\naxes.violinplot(list(market_train_full_df.assetCode.value_counts().values),showmeans=False,showmedians=True)","84bc185f":"market_train_full_df.assetName.describe()","879cdc58":"list(market_train_full_df.assetName)[:10]","579a8675":"from wordcloud import WordCloud\n# Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(\" \".join(market_train_full_df.assetName))\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(20,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","6de1edad":"market_train_full_df.universe.value_counts()","fdd115b1":"univers_df_dict = dict(collections.Counter(list(market_train_full_df.universe)))\npercent_univers_df_dict = {k: v \/ total for total in (sum(univers_df_dict.values()),) for k, v in univers_df_dict.items()}\nexplode=(0,0.1)\nlabels ='notUniverse','isUniverse'\nfig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"Univere Status\")\nax.pie(list(percent_univers_df_dict.values()), explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n\nmarket_train_full_df.universe.value_counts()","ff7811fb":"market_train_full_df.volume.describe()","ef337263":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9))\naxes.set_title(\"Volume Violin\")\naxes.set_ylabel(\"Volume\")\naxes.violinplot(list(market_train_full_df[\"volume\"].values),showmeans=False,showmedians=True)","3ebf546a":"fig, axes = plt.subplots(figsize=(20,10))\naxes.set_title(\"Volume\")\naxes.set_ylabel(\"volume\")\naxes.set_xlabel(\"records\")\naxes.plot(market_train_full_df[\"volume\"])","9504a659":"market_train_full_df.close.describe()","5a252852":"fig, axes = plt.subplots(figsize=(20,10))\n\naxes.set_title(\"Close Price\")\naxes.set_ylabel(\"close price\")\naxes.set_xlabel(\"records\")\naxes.plot(market_train_full_df[\"close\"])","7f1c298f":"fig, axes = plt.subplots(figsize=(20,10))\naxes.set_title(\"Open Price\")\naxes.set_ylabel(\"open price\")\naxes.set_xlabel(\"records\")\naxes.plot(market_train_full_df[\"open\"])","47a894c8":"market_returns_df = pd.concat(\n    [\n        market_train_full_df[\"returnsClosePrevRaw1\"].describe(),\n        market_train_full_df[\"returnsOpenPrevRaw1\"].describe(),\n        market_train_full_df[\"returnsClosePrevMktres1\"].describe(),\n        market_train_full_df[\"returnsOpenPrevMktres1\"].describe(),\n        market_train_full_df[\"returnsClosePrevRaw10\"].describe(),\n        market_train_full_df[\"returnsOpenPrevRaw10\"].describe(),\n        market_train_full_df[\"returnsClosePrevMktres10\"].describe(),\n        market_train_full_df[\"returnsOpenPrevMktres10\"].describe(),\n        market_train_full_df[\"returnsOpenNextMktres10\"].describe()\n        ],\n        axis=1\n    )\nmarket_returns_df","e7cea6ea":"market_returns_df.drop([\"returnsClosePrevMktres1\",\"returnsOpenPrevMktres1\",\"returnsClosePrevMktres10\",\"returnsOpenPrevMktres10\"],axis=1,inplace=True)\nmarket_returns_df","7a019d7d":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\naxes.set_title(\"Box Plot\")\naxes.set_ylabel(\"Returns\")\nmarket_train_full_df.boxplot(column=['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', \"returnsClosePrevRaw10\",\"returnsOpenPrevRaw10\",\"returnsOpenNextMktres10\"])    ","e068e4a4":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 8),sharey=True)\naxes[0].set_title(\"One-day difference in close and open returns\")\naxes[0].set_ylabel(\"difference\")\naxes[0].violinplot(list((market_train_full_df[\"returnsClosePrevRaw1\"] - market_train_full_df[\"returnsOpenPrevRaw1\"]).values),showmeans=False,showmedians=True,widths=0.9, showextrema=True)\naxes[1].set_title(\"10-day difference in close and open returns\")\naxes[1].set_ylabel(\"difference\")\naxes[1].violinplot(list((market_train_full_df[\"returnsClosePrevRaw10\"] - market_train_full_df[\"returnsOpenPrevRaw10\"]).values),showmeans=False,showmedians=True,widths=0.9, showextrema=True)","3bafb237":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9),sharey=True)\naxes.set_title(\"returnsOpenNextMktres10 violon\")\naxes.set_ylabel(\"\")\naxes.violinplot(list((market_train_full_df[\"returnsOpenNextMktres10\"]).values),showmeans=False,showmedians=True,widths=0.9, showextrema=True)","2494809d":"del axes\ndel market_train_full_df","5e57cc3b":"news_train_full_df = env.get_training_data()[1]","705f1162":"news_train_full_df.head()","4ec69db7":"news_train_full_df.columns","d5460455":"news_train_full_df.time.describe()","b3d571cc":"fig,axes = plt.subplots(1,1,figsize=(20,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(news_train_full_df.time.dt.date.value_counts().sort_index().index, news_train_full_df.time.dt.date.value_counts().sort_index().values)","978f1865":"news_train_full_df.sourceTimestamp.describe()","5d7bcbe3":"fig,axes = plt.subplots(1,1,figsize=(20,10))\naxes.set_title(\"sourceTimestamp Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(news_train_full_df.sourceTimestamp.dt.date.value_counts().sort_index().index, news_train_full_df.sourceTimestamp.dt.date.value_counts().sort_index().values)","c714b688":"news_train_full_df.firstCreated.describe()","bedc8a98":"fig,axes = plt.subplots(1,1,figsize=(20,10))\naxes.set_title(\"firstCreated Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(news_train_full_df.firstCreated.dt.date.value_counts().sort_index().index, news_train_full_df.firstCreated.dt.date.value_counts().sort_index().values)","ea9a17d9":"news_train_full_df.sourceId.describe()","4e11356f":"news_train_full_df.sourceId.value_counts().describe()","4d79be95":"news_train_full_df.headline.describe()","04a3aa75":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n \n\n# Create a list of word\n# text=(\"Python Python Python Matplotlib Matplotlib Seaborn Network Plot Violin Chart Pandas Datascience Wordcloud Spider Radar Parrallel Alpha Color Brewer Density Scatter Barplot Barplot Boxplot Violinplot Treemap Stacked Area Chart Chart Visualization Dataviz Donut Pie Time-Series Wordcloud Wordcloud Sankey Bubble\")\n \n# Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(( \" \".join(list(news_train_full_df.head(200000).headline))))\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","cc383005":"news_train_full_df.urgency.value_counts()","27dd7df3":"urgency_df_dict = dict(collections.Counter(list(news_train_full_df.urgency)))\npercent_urgency_df_dict = {k: v \/ total for total in (sum(urgency_df_dict.values()),) for k, v in urgency_df_dict.items()}\nexplode=(0,0.1,0.1)\nlabels =\"article\",\"alert\", \"unknown\"\nfig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"Urgency Status\")\nax.pie(list(percent_urgency_df_dict.values()), explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)","c1e1644a":"news_train_full_df.takeSequence.value_counts().head(20)","9d42efec":"fig,ax = plt.subplots(1,1,figsize=(15,10))\nax.set_xlabel(\"name\")\nax.set_ylabel(\"#\")\nnews_train_full_df.provider.value_counts().plot(kind=\"bar\",legend=\"provider\",color=\"tan\")","626bbc11":"news_train_full_df.head(5).subjects","5c340bb4":"from collections import Counter\ntmp_list = []\nfor i in news_train_full_df.head(200000).subjects:\n    tmp_list += i.replace(\"{\",\"\").replace(\"}\",\"\").replace(\" \",\"\").split(\",\")\n# Counter(tmp_list)\n# fig,ax = plt.subplots(1,1,figsize=(30,10))\n# ax.set_xticklabels(dict(Counter(tmp_list)).keys(),rotation=90)\n# font = {'family' : 'normal',\n#         'weight' : 'bold',\n#         'size'   : 10}\n# plt.rc('font', **font)\n# ax.bar(dict(Counter(tmp_list)).keys(),dict(Counter(tmp_list)).values())","85ae5f3a":"text =\" \".join(tmp_list).replace(\"'\",\"\")\n # Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(text)\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","e68a9e65":"news_train_full_df.head(5).audiences","c8c22643":"from collections import Counter\ntmp_list = []\nfor i in news_train_full_df.head(200000).audiences:\n    tmp_list += i.replace(\"{\",\"\").replace(\"}\",\"\").replace(\" \",\"\").split(\",\")\n# Counter(tmp_list)\n\n# fig,ax = plt.subplots(1,1,figsize=(30,10))\n# font = {'family' : 'normal',\n#         'weight' : 'bold',\n#         'size'   : 22}\n# plt.rc('font', **font)\n# ax.set_xticklabels(dict(Counter(tmp_list)).keys(),rotation=90)\n# ax.bar(dict(Counter(tmp_list)).keys(),dict(Counter(tmp_list)).values())","dd9b0c2a":"text =\" \".join(tmp_list).replace(\"'\",\"\")\n # Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(text)\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","8ee226c9":"pd.concat([news_train_full_df.bodySize.describe(),news_train_full_df.companyCount.describe(),news_train_full_df.sentenceCount.describe(),news_train_full_df.wordCount.describe()],axis=1)","b37d049d":"pd.concat([news_train_full_df.sentimentNegative.describe(),news_train_full_df.sentimentNeutral.describe(),news_train_full_df.sentimentPositive.describe()],axis=1)","086ca475":"fig , axes = plt.subplots(1,1,figsize=(20,8))\nnews_train_full_df.sentimentNegative.head(100).plot(kind=\"bar\",legend=\"Negative\",colormap=\"brg\")\nnews_train_full_df.sentimentPositive.head(100).plot(colormap=\"Set2\",linewidth=2,legend=\"Postive\")\nnews_train_full_df.sentimentNeutral.head(100).plot(colormap=\"RdGy\",linewidth=0.8,linestyle='dashed',legend=\"Neutral\")\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 10}\naxes.set_xticklabels(news_train_full_df.head(100).index,rotation=90)\nlegend = axes.legend(loc='upper left', shadow=True, fontsize='x-large')\nplt.rc('font', **font)","6d4a2368":"pd.concat([news_train_full_df.noveltyCount12H.describe(),news_train_full_df.noveltyCount24H.describe(), news_train_full_df.noveltyCount3D.describe(),\n          news_train_full_df.noveltyCount5D.describe(),news_train_full_df.noveltyCount7D.describe()],axis=1)","b3943a8f":"fig,axes = plt.subplots(3,2,figsize=(10,15))\n\nnoveltyCount12H_dict = dict(collections.Counter(list(news_train_full_df.noveltyCount12H)))\npercent_noveltyCount12H_dict = {k: v \/ total for total in (sum(noveltyCount12H_dict.values()),) for k, v in noveltyCount12H_dict.items()}\nsizes = list(percent_noveltyCount12H_dict.values())\naxes[0][0].set_title(\"noveltyCount12H\",loc=\"left\")\naxes[0][0].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\n\n\nnoveltyCount24H_dict = dict(collections.Counter(list(news_train_full_df.noveltyCount24H)))\npercent_noveltyCount24H_dict = {k: v \/ total for total in (sum(noveltyCount24H_dict.values()),) for k, v in noveltyCount24H_dict.items()}\nsizes = list(percent_noveltyCount24H_dict.values())\naxes[0][1].set_title(\"noveltyCount24H\",loc=\"left\")\naxes[0][1].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\n\nnoveltyCount3D_dict = dict(collections.Counter(list(news_train_full_df.noveltyCount3D)))\npercent_noveltyCount3D_dict = {k: v \/ total for total in (sum(noveltyCount3D_dict.values()),) for k, v in noveltyCount3D_dict.items()}\nsizes = list(percent_noveltyCount3D_dict.values())\naxes[1][0].set_title(\"noveltyCount3D\",loc=\"left\")\naxes[1][0].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\n\n\nnoveltyCount5D_dict = dict(collections.Counter(list(news_train_full_df.noveltyCount5D)))\npercent_noveltyCount5D_dict = {k: v \/ total for total in (sum(noveltyCount5D_dict.values()),) for k, v in noveltyCount5D_dict.items()}\nsizes = list(percent_noveltyCount5D_dict.values())\naxes[1][1].set_title(\"noveltyCount5D\",loc=\"left\")\naxes[1][1].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\n\nnoveltyCount7D_dict = dict(collections.Counter(list(news_train_full_df.noveltyCount7D)))\npercent_noveltyCount7D_dict = {k: v \/ total for total in (sum(noveltyCount7D_dict.values()),) for k, v in noveltyCount7D_dict.items()}\nsizes = list(percent_noveltyCount7D_dict.values())\naxes[2][0].set_title(\"noveltyCount7D\",loc=\"left\")\naxes[2][0].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\n\n\noveral_dict = pd.concat([news_train_full_df.noveltyCount12H,news_train_full_df.noveltyCount24H, news_train_full_df.noveltyCount3D,\n          news_train_full_df.noveltyCount5D,news_train_full_df.noveltyCount7D],axis=0)\nnoveltyOveral_dict = dict(collections.Counter(list(overal_dict)))\npercent_overal_dict = {k: v \/ total for total in (sum(noveltyOveral_dict.values()),) for k, v in noveltyOveral_dict.items()}\nsizes = list(percent_overal_dict.values())\naxes[2][1].set_title(\"overalNovelty\",loc=\"left\")\naxes[2][1].pie(sizes,  autopct='%1.1f%%',shadow=False, startangle=90)\nprint()","22a7edd3":"pd.concat([news_train_full_df.volumeCounts12H.describe(),news_train_full_df.volumeCounts24H.describe(), news_train_full_df.volumeCounts3D.describe(),\n          news_train_full_df.volumeCounts5D.describe(),news_train_full_df.volumeCounts7D.describe()],axis=1)","7c354ec5":"fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 15),squeeze=False)\naxes[0][0].set_title(\"volumeCounts12H\")\naxes[0][0].violinplot(list(news_train_full_df[\"volumeCounts12H\"].values))\n\naxes[0][1].set_title(\"volumeCounts24H\")\naxes[0][1].violinplot(list(news_train_full_df[\"volumeCounts24H\"].values))\n\naxes[1][0].set_title(\"volumeCounts3D\")\naxes[1][0].violinplot(list(news_train_full_df[\"volumeCounts3D\"].values))\n\naxes[1][1].set_title(\"volumeCounts5D\")\naxes[1][1].violinplot(list(news_train_full_df[\"volumeCounts5D\"].values))\n\naxes[2][0].set_title(\"volumeCounts7D\")\naxes[2][0].violinplot(list(news_train_full_df[\"volumeCounts7D\"].values))\n\nfig.delaxes(axes[2][1])","2b4542a2":"text=\"\"\ntext =\" \".join(list(news_train_full_df.headlineTag))\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(text)\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","6561ca7f":"news_train_full_df.marketCommentary.value_counts()","dbc7bed2":"market_commentary_df_dict = dict(collections.Counter(list(news_train_full_df.marketCommentary)))\npercent_commentary_df_dict = {k: v \/ total for total in (sum(market_commentary_df_dict.values()),) for k, v in market_commentary_df_dict.items()}\nexplode=(0,0.1)\nlabels ='False','True'\nfig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"Representing Genral Market Conditions\")\nax.pie(list(percent_commentary_df_dict.values()), explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)","55e53721":"news_train_full_df.sentimentWordCount.describe()","800520ad":"fig,ax = plt.subplots(1,1,figsize=(8,8))\nax.set_title(\"Hist(log sentimentWordCount)\")\nax.set_xlabel(\"Log(sentimentWordCount)\")\nnp.log10(news_train_full_df.sentimentWordCount).hist(ax=ax,)","6c979152":"news_train_full_df.assetName.head(5)","b0a3396c":"tmp_list = []\nfor i in news_train_full_df.head(200000).assetName:\n    tmp_list += i.replace(\"{\",\"\").replace(\"}\",\"\").replace(\" \",\"\").split(\",\")\ntext =\" \".join(tmp_list).replace(\"'\",\"\")\n # Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(text)\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","448bd23e":"news_train_full_df.assetCodes.head(10)","35ebdbfa":"news_train_full_df.sentimentClass.value_counts()","08e93463":"news_train_full_df.sentimentClass.value_counts()","be596702":"sentiment_df_dict = dict(collections.Counter(list(news_train_full_df.sentimentClass)))\npercent_univers_df_dict = {k: v \/ total for total in (sum(sentiment_df_dict.values()),) for k, v in sentiment_df_dict.items()}\nexplode=(0.0,0.025,0.05)\nlabels ='1','0',\"-1\"\nfig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"sentimentClass\")\nax.pie(list(percent_univers_df_dict.values()), explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)","c838c471":"news_train_full_df.relevance.describe()","fe36637c":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9))\naxes.set_title(\"Volume Violin\")\naxes.set_ylabel(\"Volume\")\naxes.violinplot(list(news_train_full_df[\"relevance\"].values),showmeans=False,showmedians=True)","b416f8fb":"news_train_full_df.firstMentionSentence.describe()","cd40ca75":" **Stock Analysis**","61b22a9d":"<a id=\"9\"><\/a> <br>\n* **G. OPEN**\n\n\nthe open price for the day (not adjusted for splits or dividends)","6c71bbe8":"assetNames with more repetition can be easily seen in the wordcloud.","1f1527cd":"<a id=\"30\"><\/a> <br>\n* **O. HEADLINE_TAG**\n\nThe Thomson Reuters headline tag for the news item","d23523da":"Now lets read the data description. \n\nAs it is mentioned in in description, the Market data (2007 to present) contains financial market information such as opening price, closing price, trading volume, calculated returns, etc.\nNow, lets take a glance to data description and understanding the attributes.","616cd591":"For handling probably memory heaps, we delete the market_train_full_df data and load the news_train_full_df. After finishing EDA on news_train_full_df, we will load both of them for analyzing compound features.","793467a2":"<a id=\"5\"><\/a> <br>\n* **C. ASSETNAME**\n\nOr category, the name that corresponds to a group of assetCodes. These may be \"Unknown\" if the corresponding assetCode does not have any rows in the news data.","18422604":"<a id=\"28\"><\/a> <br>\n* **M. NOVELTY_COUNT12H, NOVELTY_COUNT_24H, NOVELTY_COUNT_3D, NOVELTY_COUNT_5D, NOVELTY_COUNT_7D**\n\n\nNoveltyCounts represents the novelty of the content within a news item on a particular asset. It is calculated by comparing it with the asset-specific text over a cache of previous news items that contain the asset.","5b7cb5cc":"There is interesting point in the above chart. As you can see in december months (every year) there is a local minimum. And the reason is people was preparing themselves for christmas. They have ignored the stocks obviously.","ad647a6a":"<a id=\"15\"><\/a> <br>\n#  3-NEWS_DATA ANALYSIS\n","44ccdb5c":"<a id=\"8\"><\/a> <br>\n* **B. SOURCE_TIMESTAMP**\n\nsourceTimestamp represents the time when the news was created.\nLets do a studying like a previous feature on this attribute.\n","b4c6d8bc":"<a id=\"38\"><\/a> <br>\n#  4-COMPOUND FEATURES ANALYSIS","af09990d":"Similar visualization like previous feature can be done on the 'audiences' feature.","edc29320":"<a id=\"1\"><\/a> <br>\n#  1-INTRODUCTION AND ROADMAP\n\nIn this challenge we will deal with the stock data. There are two datasets. Marketdata_sample and News_sample. contains financial market information. Features like opening price and closing price and this sort of things are existed in dataset. News dataset contains information about news articles\/alerts published about assets. Attributes like asset details are located in this dataset.\nThe main goal of the competition is how we can use the content of news analytics to predict stock price performance.  \nNow, we will try to analyze both of datasets feature by feature.\n\n   **1. In first step we will do study on market data and try to visualize features and extract information from data.**\n    \n   **2. In the second step we will concentrate on news data and it's features.**\n    \n   **3. In the third section relation between two dataframes will be validated. Features correlations will be discussed there.**\n    \n   **4. Finally in forth step we will try to address the competition challenge goal and do submission based on the EDA we will have in this kernel. **\n   \nAny comment, idea or hint will be appreciated. \n\n**Your upvote will be motivation for me for continuing the kernel ;-)**","d66bcc73":"In next sections we will discussed more about market_df and its relation with news data.\nNow lets move on to News data and do similar studies.","334edb0a":"Reading the data and understanding the data.","a1692bc8":"<a id=\"10\"><\/a> <br>\n* **F. RETURNS**\n\nReturns are calculated based on different timespans. creating ","38cd9f24":"<a id=\"37\"><\/a> <br>\n* **V. FIRST_MENTION_SENTENCE**\n\nThe first sentence, starting with the headline, in which the scored asset is mentioned.\n        1: headline\n        2: first sentence of the story body\n        3: second sentence of the body, etc\n        0: the asset being scored was not found in the news item's headline or body text. As a result, the entire news item's text (headline + body) will be used to determine the sentiment score.","0d481f7f":"As it can be concluded that we have a missing values in last month of 2014. \n\nIt is Time dependent operation. So, timeseries analyzing is one of the possible action in this competition. We will to try to do it in next steps.","fd1af02a":"It is clear that by increasing the timestamp (going from 12H to 7D) the volumeCounts range is increased. By the way the most frequent values have belonged to the minimum percentiles in whole cases.","f9f8a444":"There are 3780 assetCodes.  the relation between these codes and the correspondence assetCode which is in news_df can bring considerable information. we will check it in next sections.\n\nAs it can be seen, there are assetsCode which repeated in 2498 records. The violion chart maybe represents more clear information about assetsCode distributions.","e715b507":"<a id=\"0\"><\/a> <br>\n## Kernel Headlines\n1. [Introduction and RoadMap](#1)\n2. [Market Data Analysis](#2)\n    1.  [time](#3)\n\t2.  [assetCode_violin](#4)\n\t3.  [assetName_wordcloud](#5)\n\t4.  [universe_pi-chart](#6)\n\t5.  [volume_violin](#7)\n\t6.  [close_line](#8)\n\t7.  [open_line](#9)\n\t8.  [returns_boxplot](#10)\n    \n3. [News Data Analysis](#15)\n     1.  [time_line](#16)\n\t 2.  [sourceTimestamp_line](#17)\n\t 3.  [firstCreated_line](#18)\n\t 4.  [sourceId_qpercentile](#19)\n\t 5.  [headline_wordcloud](#20)\n\t 6.  [urgency_pi_chart](#21)\n\t 7.  [takeSequence](#22)\n\t 8.  [provider_barchart](#23)\n\t 9.  [subjects_wordcloud](#24)\n\t 10.  [audience_wordcloud](#25)\n\t 11.  [word_sentences_describe](#26)\n\t 12.  [sentimentStatus_barchart](#27)\n\t 13.  [novelties_pichart](#28)\n\t 14.  [volumes_violin](#29)\n\t 15.  [headlineTag_wordcloud](#30)\n\t 16.  [marketCommentary_pichart](#31)\n\t 17.  [sentimentWordcount_histogram](#32)\n\t 18.  [assetName_wordcloud](#33)\n\t 19.  [assetCode_head](#34)\n\t 20.  [sentimentClass_pichart](#35)\n\t 21.  [relevance_violon](#36)\n\t 22.  [firstMentionSentence_describe](#37)\n     \n4. [COMPOUND FEATURES ANALYSIS](#38)\n","9f5eded7":"There is only one record which represents the alerting. The rest of 99 items are representing article record. You can easily draw the distribution using pi-chart.","41b90d2a":"So, in contrast to time-based features, sourceIds have repetitions. There are 9328750 sourceIds and most repetition is 43 times. Correlation between this features and other ones will be discussed on next steps.\n\n75% of news items have repetition less than one time.","b6870dbb":"Similar to sourceIds, there are repetitions. There are 5532379 unique headline. \nLets go deeper to headlines ;-)","665ad94e":"<a id=\"22\"><\/a> <br>\n* **G. TAKE_SEQUENCE**\n\nThe take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences.","53406143":"<a id=\"32\"><\/a> <br>\n* **Q. SENTIMEN_WORD_COUNT**\n\nThe number of lexical tokens in the sections of the item text that are deemed relevant to the asset. This can be used in conjunction with wordCount to determine the proportion of the news item discussing the asset.","76e3c016":"Another userful visualization for this feature can be wordcloud.","f7939a62":"Averagely, there are approximately 200 lexical tokens which are related to assets.","d4ad97f0":"<a id=\"20\"><\/a> <br>\n* **E. HEADLINE**\n\nThe item's headline","634cfa0a":"As it is cleared in the word_cloud some combination of texts are repetitive. For example, both of the 'ROUNDUP RESEARCH' and 'RESEARCH ROUNDUP' exist in headlineTag","f5a22196":"The distribution of universe in train data is provided above.","7b36ef5e":"<a id=\"3\"><\/a> <br>\n* **A. TIME**\n\ntime(datetime64[ns, UTC]) - the current time ","21a4c279":"Checking the dimentions of data.","936286a1":"<a id=\"25\"><\/a> <br>\n* **J. AUDIENCES**\n\nIdentifies which desktop news product(s) the news item belongs to. They are typically tailored to specific audiences. (e.g. \"M\" for Money International News Service and \"FB\" for French General News Service)","b363efdf":"<a id=\"4\"><\/a> <br>\n* **B. ASSETCODE**\n\n a unique id of an asset","a304374f":"Dropping NaN attributes for more clearly representation.","8221eb67":"Now, we have considerable information about all the attributes. So, we are ready to start deeper analysis on each dataset and also the relation between these two dataset.  In the next step, we will focus on relation between features.","a9e27580":"<a id=\"31\"><\/a> <br>\n* **P. MARKET_COMMUNITY**\n\nBoolean indicator that the item is discussing general market conditions, such as \"After the Bell\" summaries","6d372196":"Doing the similar analyze on returnsOpenNextMktres10, we have:","0305047c":"<a id=\"18\"><\/a> <br>\n* **C. FIRST_CREATED**\n\n\nfirstCreated(datetime64[ns, UTC]) - UTC timestamp for the first version of the item\nlast two previous time-based features, lets do similar study.","c3d96a56":"Drawing wordcloud ...\nUsing a section of data beacause of avoiding memory overflows...","adee1d75":"<a id=\"23\"><\/a> <br>\n* **H. PROVIDER**\n\n\ndentifier for the organization which provided the news item (e.g. RTRS for Reuters News, BSW for Business Wire)","fb1e2f87":"The distribution of sentimentClass is approximately uniform.","4df4bbb8":"<a id=\"6\"><\/a> <br>\n* **D. UNIVERSE**\n\n(float64) - a boolean indicating whether or not the instrument on that day will be included in scoring. This value is not provided outside of the training data time period. The trading universe on a given date is the set of instruments that are avilable for trading (the scoring function will not consider instruments that are not in the trading universe). The trading universe changes daily.","26adaaa2":"Two important hints can be concluded:\n\n**1- The difference of both of the periods is approximately concentrated on -8000 .**","eb9ff165":"**In progress ...**\n\n**Be in touch to get last commits ...**\n\n**I'll try to complete it as soon as possible**\n","3ebecbd9":"<a id=\"8\"><\/a> <br>\n* **F. CLOSE**\n\n(float64) - the close price for the day (not adjusted for splits or dividends)","3e7ce05f":"The most repetition of relevance is on One.","e4117014":"<a id=\"16\"><\/a> <br>\n* **A. TIME**\n\n\ntime(datetime64[ns, UTC]) - UTC timestamp showing when the data was available on the feed (second precision)\n\nlets define new temporary dataframe for studying on time parameters.","1c9a1457":"By discarding the two peaks which are obviously detectable and comparing two above diagrams (close and open price) the low difference between these two diagram has been revealed.. We will discuss in more details in next sections. But for now, it is enough to see the same rate in both of the diagrams.","a063ba5f":"Taking a glance to columns for undestranding the features...","a88a5aca":"Is there any relation between returnsClosePrevRaw1 and returnsOpenPrevRaw1 ?!","506f267a":"Similarity between the features \"sourceTimestamp\" and \"time\" can be concluded.","7c43e2a4":"<a id=\"7\"><\/a> <br>\n* **E. VOLUME**\n\n(float64) - trading volume in shares for the day","fe989455":"There is considerable std in the volume feature. On the other hand, the 75% percentile represents that the gap between the last quater of volume is very bigger than first quater. As a result, if we plot the violin diagram, It has considerable mass in first quater. Lets validate it...","aca8f702":"<a id=\"26\"><\/a> <br>\n* **K. BODY_SIZE, COMPANY_COUNT,SENTENCE_COUNT,WORD_COUNT**\n\nbodySize represents the size of the current version of the story body in characters\ncompanyCount represents the number of companies explicitly listed in the news item in the subjects field\nsentenceCount represents the total number of sentences in the news item. Can be used in conjunction with firstMentionSentence\nwordCount represents the total number of lexical tokens (words and punctuation) in the news item","1900a1e7":"<a id=\"27\"><\/a> <br>\n* **L. SENTIMENT_NEGATIVE, SENTIMENT_NEUTRAL, SENTIMENT_POSITIVE**\n\nsentimentNegative, sentimentNeutral and sentimentPositive respectively represents the probability that the sentiment of the news item was negative, neutral or positive for the asset.","89fa4f9a":"<a id=\"2\"><\/a> <br>\n#  2-MARKET_DATA ANALYSIS","40c79d35":"<a id=\"35\"><\/a> <br>\n* **T. SENTIMENT_CLASS**\n\nIndicates the predominant sentiment class for this news item with respect to the asset. The indicated class is the one with the highest probability.","2f76ed5c":"Only 5 percent of the dataset have information about general conditions.","68301d1d":"Our assumption has been approved. ;-)","b88c15e5":"Going deeper to market_df attributes and investigate feature by feature.","2fe26160":"<a id=\"29\"><\/a> <br>\n* **N. VOLUME_COUNT_12H, VOLUME_COUNT_24H, VOLUME_COUNT_3D, VOLUME_COUNT_15D, VOLUME_COUNT_7D**\n\nvolumeCounts represents the volume of news for each asset. A cache of previous news items is maintained and the number of news items that mention the asset within each of five historical periods is calculated.","1c89f5a4":"It seems that unknow is a miss in data. Except it's small occurences, it also doesnt mentioned in data describtion.","e03d2681":"news bodies have averagely 3082 character. Atleast one record have no body.\nin companyCount feature we can see there is (are) which referenced 9 companies in it's subject field.","5e3f91f7":"Most repetitive word have been seen above ;-).","6fb0fbc8":"<a id=\"21\"><\/a> <br>\n* **F. URGENCY**\n\ndifferentiates story types (1: alert, 3: article,2:unknown)","86f24b66":"![](https:\/\/st.depositphotos.com\/1760261\/1348\/i\/950\/depositphotos_13484306-stock-photo-data-analysis.jpg)","edec0e82":"<a id=\"19\"><\/a> <br>\n* **D. SOURCE_ID**\n\n\nAn Id for each news item.","0b0ec47d":"WordCloud can represnets more detailed information.","25c1c594":"<a id=\"34\"><\/a> <br>\n* **S. ASSET_CODE**\n\nlist of assets mentioned in the item","70bf3f9b":"The comparison between negative, positive and neutral probabilities represents the summation of these three probabilities is more than 1.0 in most cases and one of them is higher one. On the other hand there is no significant relation between these probabilities. For example in record number 83,84 and 85 the negativeEffect and PositiveEffects are low but the neutral effect is high. In contrast in records 53 and 54 the positiveEffect is major one.","6085b7ff":"There are some points can be concluded from the table:\n\n **1- 25% percentiles of whole the returnPeriods are negative.**\n \n **2- By increasing the period of returns, both of the open and close values are encoutered with larger std**\n \n **3- whole the periods have negative returns (min)**","8008ed96":"<a id=\"24\"><\/a> <br>\n* **I. SUBJECTS**\n\nTopic codes and company identifiers that relate to this news item. Topic codes describe the news item's subject matter. These can cover asset classes, geographies, events, industries\/sectors, and other types.","addf5cac":"The volume in some records has considerable peak in comparison to other ones. We will analyse it with more details in next sections.","5952b85b":"<a id=\"33\"><\/a> <br>\n* **R. ASSET_NAME**\n\nname of the asset","96228a29":"'Reuturs' has the most repetition in our dataset.","9f18b2bf":"<a id=\"36\"><\/a> <br>\n* **U. RELEVANCE**\n\nA decimal number indicating the relevance of the news item to the asset. It ranges from 0 to 1. If the asset is mentioned in the headline, the relevance is set to 1. When the item is an alert (urgency == 1), relevance should be gauged by firstMentionSentence instead.","5608ce83":"As you can see, the 'Unknown' assetName has 3511 unique values and  'Unknown' is the assetName that has most frequency in train data."}}