{"cell_type":{"b7304275":"code","e68325c2":"code","963a77e1":"code","4c8d536a":"code","7625c054":"code","d91a7c27":"code","0d103fd8":"code","687bba76":"code","f5845a61":"code","5f4fd5e4":"code","58fd459d":"code","941232cc":"markdown","7447905d":"markdown","55c4b4c1":"markdown","484e27c1":"markdown","8f2c1e8e":"markdown","9ab24958":"markdown","2fa4b790":"markdown","568dfcab":"markdown","0d787bd4":"markdown"},"source":{"b7304275":"import os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","e68325c2":"class Dataset:\n    \"\"\"\n    For comments_to_score.csv (the submission), get only one comment per row\n    \"\"\"\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n        }\n    \n    \nclass ValidationDataset:\n    \"\"\"\n    Goes through validation_data.csv, Loading and tokenizing both less_toxic and more_toxic\n    \n    Inspired by: https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n    \"\"\"\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def tokenize(self, text):\n        return self.tokenizer(text, max_length=self.max_len, \n                              padding=\"max_length\", truncation=True)\n    \n    def __getitem__(self, i):\n        more_toxic = self.df['more_toxic'].iloc[i]\n        less_toxic = self.df['less_toxic'].iloc[i]\n        \n        less_inputs = self.tokenize(less_toxic)\n        more_inputs = self.tokenize(more_toxic)\n\n        return {\n            \"less_input_ids\": torch.tensor(less_inputs[\"input_ids\"], dtype=torch.long),\n            \"less_attention_mask\": torch.tensor(less_inputs[\"attention_mask\"], dtype=torch.long),\n            \"more_input_ids\": torch.tensor(more_inputs[\"input_ids\"], dtype=torch.long),\n            \"more_attention_mask\": torch.tensor(more_inputs[\"attention_mask\"], dtype=torch.long),\n        }","963a77e1":"def validate(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\", nrows=VALIDATION_SIZE)\n    \n    dataset = ValidationDataset(df=df, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=64, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    n_samples = len(dataset)\n    hits = 0\n    \n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n                \n            less_output = model(input_ids=data['less_input_ids'], \n                                attention_mask=data['less_attention_mask'])\n            \n            more_output = model(input_ids=data['more_input_ids'], \n                                attention_mask=data['more_attention_mask'])\n            \n            if is_multioutput:\n                # Sum the logits of the 6 toxic labels\n                less_score = less_output.logits.sum(dim=1)\n                more_score = more_output.logits.sum(dim=1)\n                hits += (less_score < more_score).sum().item()\n            else:\n                less_score = less_output.logits[:, 1]\n                more_score = more_output.logits[:, 1]\n                hits += (less_score < more_score).sum().item()\n            \n            \n    \n    \n    accuracy = hits \/ n_samples\n    print(f\"Validation Accuracy: {accuracy:4.2f}\")\n    \n    torch.cuda.empty_cache()\n    return accuracy","4c8d536a":"# validate(model_chk, max_length, is_multioutput)","7625c054":"def generate_predictions(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            \n            if is_multioutput:\n                # Sum the logits for all the toxic labels\n                # One strategy out of various possible\n                output = output.logits.sum(dim=1)\n            else:\n                # Classifier. Get logits for \"toxic\"\n                output = output.logits[:, 1]\n            \n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","d91a7c27":"preds1 = generate_predictions(\"..\/input\/toxic-bert\", max_len=192, is_multioutput=True)\npreds2 = generate_predictions(\"..\/input\/roberta-base-toxicity\", max_len=192, is_multioutput=False)\npreds3 = generate_predictions(\"..\/input\/roberta-toxicity-classifier\", max_len=192, is_multioutput=False)","0d103fd8":"from sklearn.preprocessing import MinMaxScaler\n\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf_sub[\"score_bert\"] = preds1\ndf_sub[\"score_rob1\"] = preds2\ndf_sub[\"score_rob2\"] = preds3\n\ndf_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]] = MinMaxScaler().fit_transform(df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]])\n\ndf_sub[\"score\"] = df_sub[[\"score_bert\", \"score_rob1\", \"score_rob2\"]].sum(axis=1)\ndf_sub.head()","687bba76":"pd.set_option(\"display.max_colwidth\", 500)","f5845a61":"df_sub.sort_values(\"score\").head(3)[['score', 'text']]","5f4fd5e4":"df_sub.sort_values(\"score\").tail(3)[['score', 'text']]\n","58fd459d":" # Tie-break, if any\ndf_sub['score'] = df_sub['score'].rank(method='first')\n\ndf_sub = df_sub[[\"comment_id\", \"score\"]]\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","941232cc":"# Prediction\n\nAdapted from [AutoNLP for toxic ratings ;)](https:\/\/www.kaggle.com\/abhishek\/autonlp-for-toxic-ratings) by Abhishek.","7447905d":"# Submit","55c4b4c1":"# Please, _DO_ upvote if you find this useful or interesting!","484e27c1":"# Test and Validation Dataset","8f2c1e8e":"# Imports","9ab24958":"# Validation\n","2fa4b790":"# Ensemble\n\nLinear ensemble of the three models.\n\n\nSince their scales are off I first MinMaxScale the results (per model), and then I sum the scores","568dfcab":"# \u2623\ufe0f Jigsaw - HuggingFace Hub Baselines\n\nIn this notebook I will explore, without fine-tuning, various models from the huggingface hub.\n\nI am bringing them to kaggle as datasets:\n\n* [toxic-bert](https:\/\/www.kaggle.com\/julian3833\/toxic-bert)\n* [roberta-base-toxicity](https:\/\/www.kaggle.com\/julian3833\/roberta-base-toxicity)\n* [roberta-toxicity-classifier](https:\/\/www.kaggle.com\/julian3833\/roberta-toxicity-classifier)\n\n\n\n\n\n|Version | Model | Validation (first 5000 samples) | LB |\n|---| ---   | ---: | --- |\n|[V1](https:\/\/www.kaggle.com\/julian3833\/jigsaw-huggingface-hub-baselines?scriptVersionId=79545636) | [toxic-bert](https:\/\/www.kaggle.com\/julian3833\/toxic-bert) | `0.71` | `0.758` |\n|[V2](https:\/\/www.kaggle.com\/julian3833\/jigsaw-huggingface-hub-baselines?scriptVersionId=79547125) | [roberta-base-toxicity](https:\/\/www.kaggle.com\/julian3833\/roberta-base-toxicity) | `0.66` | `0.751` |\n|[V3](https:\/\/www.kaggle.com\/julian3833\/jigsaw-huggingface-hub-baselines?scriptVersionId=79547879) | [roberta-toxicity-classifier](https:\/\/www.kaggle.com\/julian3833\/roberta-toxicity-classifier) | `0.69` | `0.768` |\n|[V4](https:\/\/www.kaggle.com\/julian3833\/jigsaw-huggingface-hub-baselines) | Ensemble of the previous 3 | `--` | `0.782` |\n\n\n# Please, _DO_ upvote if you find this useful or interesting!","0d787bd4":"# View some results"}}