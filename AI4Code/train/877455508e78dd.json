{"cell_type":{"7489e318":"code","b290569e":"code","ba964cec":"code","9909dc15":"code","88a50ac0":"code","fe567ecf":"code","d31b621c":"code","7c0f8963":"markdown","ef4604c3":"markdown","4eb8501c":"markdown","7da5441a":"markdown","db687463":"markdown","2eeb3002":"markdown","93ce6179":"markdown","7f32a97c":"markdown","b405e4ac":"markdown","760cabdc":"markdown","ac086eb6":"markdown","bc325785":"markdown","d66a7362":"markdown","b0529663":"markdown"},"source":{"7489e318":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b290569e":"# install tesorflow bert package\n!pip install bert-for-tf2\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert\n\n#Loding pretrained bert layer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\n\n\n# Loading tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocab_file, do_lower_case)\n\nprint(\"done!\")","ba964cec":"text = 'Encoding will be clear with this example'\n# tokenize\ntokens_list = tokenizer.tokenize(text)\nprint('Text after tokenization')\nprint(tokens_list)\n\n# initilize dimension\nmax_len =12\ntext = tokens_list[:max_len-2]\ninput_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\nprint(\"After adding  flasges -[CLS] and [SEP]: \")\nprint(input_sequence)\n\n\ntokens = tokenizer.convert_tokens_to_ids(input_sequence )\nprint(\"tokens to id \")\nprint(tokens)\n\npad_len = max_len -len(input_sequence)\ntokens += [0] * pad_len\nprint(\"tokens: \")\nprint(tokens)\n\nprint(pad_len)\npad_masks = [1] * len(input_sequence) + [0] * pad_len\nprint(\"Pad Masking: \")\nprint(pad_masks)\n\nsegment_ids = [0] * max_len\nprint(\"Segment Ids: \")\nprint(segment_ids)","9909dc15":"import numpy as np\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\nMAX_LEN = 12\n\n# encode train set \ntrain_input = bert_encode([\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is amazing',\n    'this is a depressed text',\n    'sure is nice this weather',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'so good to be outside!',\n    'this is sad',\n    'this is good',\n    'I am crying a lot',\n    'good to see you!',\n    'this is sad',\n    'this is good',\n    'ah its raining again',\n    'this is a happy text',\n    'feeling a bit down',\n    'wow so amazing!',\n    'this is a depressed text',\n    'this looks really amazing',\n    'nobody likes me',\n    'I feel great today',\n    'lying in bed the whole day',\n    'this is a happy text',\n    'this is sad',\n    'this is so nice',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good',\n    'this is a depressed text',\n    'this is a happy text',\n    'this is sad',\n    'this is good'\n], tokenizer, max_len=MAX_LEN)\ntrain_labels = np.array([\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n    [0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],\n\n])\n\nprint(\"number of test samples:\", len(train_input[0]), \"labels:\", len(train_labels))\n","88a50ac0":"# first define input for token, mask and segment id  \nfrom tensorflow.keras.layers import  Input\ninput_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")\n\n#  output  \nfrom tensorflow.keras.layers import Dense\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])  \nclf_output = sequence_output[:, 0, :]\nout = Dense(2, activation='softmax')(clf_output)\n\n# intilize model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","fe567ecf":"# train\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=2,\n    batch_size=1\n)\n\nmodel.save('model.h5')\nprint(\"done and saved!\")","d31b621c":"test_input = bert_encode(['I feel sad', 'I am happy'], tokenizer, max_len= MAX_LEN )\ntest_pred = model.predict(test_input)\npreds = np.around(test_pred, 3)\npreds","7c0f8963":"## 4.1) Does the network learn to detect Depression?\n\n## 4.2) One thenth of the dataset is used as a validation set. Why do you think we need a validation dataset?","ef4604c3":"# 3) Create a new small (one layer) network on top of BERT base","4eb8501c":"# 2) Create a (fake) dataset","7da5441a":"## 3.1) What is the maximum number of input words in this network? Where is this defined?\n\n## 3.2) How many neurons does the output of BERT have?\n\n## 3.3) How many output neurons does our added 'dense' layer have?","db687463":"## 5.1) Does our network actually work?\n\n## 5.2) Are there examples the network is confused about?\n\n## 5.3) Does the network generalize to data it has never seen before?","2eeb3002":"## 1.1) Why can't neural networks read words directly?\n\n## 1.2) Which neuron is activated by the word 'Encoding'? And by 'example'?\n\n## 1.3) How many neurons are activated in total?\n\n## 1.4) What is the maximum number of words that can be encoded in this example?\n\n## 1.5) Change the maximum number of encoded words to 32.","93ce6179":"# 4) Train the network","7f32a97c":"## 2.1) What is the network's output for depressed text? And for happy text?\n\n## 2.2) Why is this actually not a good dataset to train the network on?","b405e4ac":"# load in BERT (enable INTERNET in SETTINGS for this to work)\n### (requires phone verification)","760cabdc":"# 5) Test the network","ac086eb6":"# 6) Create and train your own dataset\n\n## 6.1) What is something you would like to train BERT to recognize?\n\n## 6.2) Name 5 positive examples and 5 negative examples.\n\n## 6.3) Try to find 100 or more positive and 100 or more negative examples online (more is better) and create a new labeled dataset.\n\n## 6.4) Train and test the network.","bc325785":"Use a neural network (BERT) to analyse huge amounts of data automatically\n===","d66a7362":"In this lecture we will use an advanced deep neural network to analyze large amounts of text.\n\nSpecifically we will use BERT by google, which was google's response to an earlier network called ELMo.\n\nThis huge deep neural network can transform text into a vector, representing the meaning of the text.\n\nIf you would want to train a network the size of BERT you will need an expensive computer with a couple of state of the art GPUs, a big chunk of text, sometimes they use all the text on the internet for example. And then you need to wait a long time.\n\nThe latest Natural Language Processing network, GPT-3, cost in total 16 million dollars to train. Just an example of why here, we just download a pretrained BERT.\nWe can then cut of the top part of BERT, and with a headless network, as they say, or here that would mean a headless BERT, we can use the high level neurons BERT uses to make a prediction, without having to train a deep neural network from scratch.\n\n![high level features](https:\/\/informatics.sydney.edu.au\/blogs\/lecun.png)\n\nIn this image you can easily visualize what is happening, showing the features learned by a deep convolutional neural network used for vision. In the first layers of the network we can see very basic features. As we go deeper into the network, features start to resemble whole objects more in a vision network. In a language processing network the same happens, but that is more difficult to visualize.\n\nDeep neural networks need to be trained on millions of images or terabytes of text to work properly. What we can do after that however, is take a pretrained network, remove the top (classifier) layers, and attach our own classifier, to the 'high-level features'. We will then get a deep network with state of the art performance, without the need to train the whole network from scratch.\n\nIn this video some different layers of BERT are visualized.\n[https:\/\/vimeo.com\/358488181](https:\/\/vimeo.com\/358488181)\n\nIf you want to know even more, here is an article explaining how BERT sees the world in more detail.\n[https:\/\/towardsdatascience.com\/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1](https:\/\/towardsdatascience.com\/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)","b0529663":"# 1) Tokanization example\n\nHere we will transform words into tokens that a Neural Network can read.\n\nA neural network cannot read words, only values of input 'neurons'.\nHere we will map words to input neurons.\n"}}