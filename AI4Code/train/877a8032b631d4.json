{"cell_type":{"d00c6e38":"code","97c79e25":"code","9cc4820e":"code","3de41571":"code","3eb565ac":"code","6eee3d8b":"code","bb2957cd":"code","3f99ddd3":"code","e8662c33":"code","9d0e9f4e":"code","ebd48f02":"code","e84d3485":"code","16980a3d":"code","e1525f0b":"code","120a0ca5":"code","3f80a4b8":"code","45479fc5":"code","a80020dc":"code","5ef18d62":"code","0325aeb7":"code","4e0a314d":"code","0fd55bba":"markdown","4c8cbf2a":"markdown","2a024e63":"markdown","8e40c138":"markdown"},"source":{"d00c6e38":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport optuna","97c79e25":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","9cc4820e":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas().drop('id', axis=1)\ntrain = reduce_memory_usage(train)\ntest = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas().drop('id', axis=1)\ntest = reduce_memory_usage(test)\nss = dt.fread('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","3de41571":"bool_cols_train = []\nfor i, col in enumerate(train.columns):\n    if train[col].dtypes == bool:\n        bool_cols_train.append(i)\n    \nprint(bool_cols_train)","3eb565ac":"bool_cols_test = []\nfor i, col in enumerate(test.columns):\n    if train[col].dtypes == bool:\n        bool_cols_test.append(i)\n    \nprint(bool_cols_test)","6eee3d8b":"train.iloc[:, bool_cols_train] = train.iloc[:, bool_cols_train].astype(int)\ntest.iloc[:, bool_cols_test] = test.iloc[:, bool_cols_test].astype(int)","bb2957cd":"print(\"Train set shape\", train.shape, \"\\n\", \"Test set shape\", test.shape)","3f99ddd3":"train.head()","e8662c33":"cat_features = []\ncat_features_test = []\n\nfor col in train.columns:\n    if train[col].dtype=='int' and col not in ['f22']:\n        cat_features.append(col)\n\n\nfor col in test.columns:\n    if train[col].dtype=='int' and col not in ['f22']:\n        cat_features_test.append(col)    \n\n\nprint('categorical features', cat_features)\ndisplay(len(cat_features))\nprint('categorical features', cat_features_test)\ndisplay(len(cat_features_test))    ","9d0e9f4e":"X = train.drop(columns=cat_features).copy()\ny = train['target'].copy()\nX_test = test.drop(columns=cat_features_test).copy()\n\ndel train\ndel test","ebd48f02":"X['std'] = X.std(axis=1)\nX['min'] = X.min(axis=1)\nX['max'] = X.max(axis=1)\n\nX_test['std'] = X_test.std(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['max'] = X_test.max(axis=1)","e84d3485":"def objective(trial, data=X, target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 20000, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'predictor': \"gpu_predictor\",\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n         }\n    \n    model = XGBClassifier(**params, tree_method='gpu_hist', random_state=2021, use_label_encoder=False)\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    fpr, tpr, _ = roc_curve(test_y, preds)\n    score = auc(fpr, tpr)\n    \n    return score","16980a3d":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","e1525f0b":"optuna.visualization.plot_optimization_history(study)","120a0ca5":"optuna.visualization.plot_edf(study)","3f80a4b8":"optuna.visualization.plot_param_importances(study)","45479fc5":"optuna.visualization.plot_slice(study)","a80020dc":"optuna.visualization.plot_parallel_coordinate(study)","5ef18d62":"params=study.best_params\nprint(params)","0325aeb7":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","4e0a314d":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('.\/xgb.csv', index=False)\nss.head()","0fd55bba":"# Feature Engineering","4c8cbf2a":"Here I am dropping all the categorical features except **f22**. I have discussed the reason in this [https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/276053](http:\/\/) discussion thread.","2a024e63":"# Hyperparameter Tuning","8e40c138":"# Model Training"}}