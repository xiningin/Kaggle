{"cell_type":{"3136b235":"code","e0da1aef":"code","432b2873":"code","6cab364b":"code","ae6ec1df":"code","d429da42":"code","c0b333a2":"code","e4b8b62b":"code","c018adde":"code","0c9386a4":"code","88a1b906":"code","9791087a":"code","119b623d":"code","be634b99":"code","94f97bd2":"code","a4340601":"code","bcb9876c":"code","4d3891e3":"code","38947d97":"code","9b2fa169":"code","7ede9962":"code","b361e124":"code","5739069a":"code","85d86aea":"code","b41f2999":"code","baf02f6e":"code","88f2aaae":"code","33983b02":"code","9c994ad6":"code","657585ff":"code","351d66fe":"code","18140ba5":"code","b35d0467":"code","da9eb114":"code","e36aa29e":"code","02bcfb03":"code","27d12475":"code","86d44c70":"code","3bdf6d4f":"code","62b4e6fc":"code","3c6af4d7":"code","874415f1":"code","ff5d95f9":"code","bf66581f":"code","fa7056c0":"code","d34262c0":"code","2229b789":"code","4f69f287":"code","406ef9e5":"code","131837c4":"markdown","c467a5a4":"markdown","d11c0a11":"markdown","cde9231c":"markdown","e96d458e":"markdown","a75b7cb1":"markdown","e5164ea6":"markdown","c5552ef7":"markdown","394e6d98":"markdown","47e79719":"markdown","b65f10b3":"markdown","43d362c4":"markdown","822fcafa":"markdown","ff71068d":"markdown","4dbd9cd9":"markdown","44ddfdd5":"markdown","ddf8cbaa":"markdown","17cd9fad":"markdown","fd9adf90":"markdown","f9b5f195":"markdown"},"source":{"3136b235":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e0da1aef":"#Importing necessary packages and loading the train and test dataset\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rc('font',size=14)\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nimport scipy.stats as stats\ntrain= pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=  pd.read_csv(\"..\/input\/titanic\/test.csv\")\nimport re\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","432b2873":"train.info()","6cab364b":"test.info()","ae6ec1df":"train.head(10)","d429da42":"train['Title']=train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.',x).group(1))\ntest['Title']=test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))","c0b333a2":"#Countplot with Survived features\nplt.figure(figsize=(12,5))\nsns.countplot(x='Title',data=train,hue='Survived',palette='hls')\nplt.show()","e4b8b62b":"Title_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Countess\":   \"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Miss\",\n        \"Ms\":         \"Miss\",\n        \"Mrs\" :       \"Miss\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n# we map each title to correct category\ntrain['Title'] = train.Title.map(Title_Dictionary)\ntest['Title'] = test.Title.map(Title_Dictionary)\n","c018adde":"plt.figure(figsize=(12,5))\nsns.countplot(x='Title',data=train,hue='Survived',palette='hls')\nplt.show()","0c9386a4":"train['Surname']=train.Name.str.split(',').str[0]\ntest['Surname']=test.Name.str.split(',').str[0]\nprint(train[['Name','Surname']].head())\nprint(test[['Name','Surname']].head())","88a1b906":"train['Cabin_notnull']=np.where(train['Cabin'].isnull(),0,1)\ntest['Cabin_notnull']=np.where(test['Cabin'].isnull(),0,1)\npd.crosstab(train['Cabin_notnull'],train['Survived'])","9791087a":"plt.figure()\nsns.countplot(x=\"Embarked\",data=train,hue=\"Survived\",palette=\"hls\")\nplt.show()","119b623d":"train['Embarked'].fillna(train['Embarked'].value_counts().idxmax(),inplace=True)\ntest['Embarked'].fillna(test['Embarked'].value_counts().idxmax(),inplace=True)","be634b99":"train.groupby(['Embarked','Sex','Pclass','Title'])['Age'].median()","94f97bd2":"#Flling the null age values with respective group's median values\ntrain.Age.loc[train.Age.isnull()] = train.groupby(['Embarked','Sex','Pclass','Title']).Age.transform('median')\nprint(train[\"Age\"].isnull().sum())\ntest.Age.loc[test.Age.isnull()] = test.groupby(['Embarked','Sex','Pclass','Title']).Age.transform('median')","a4340601":"train.Age.describe()","bcb9876c":"#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 23, 36, 60, 120) \n\n#Seting the names that we want use to the categorys\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ntrain[\"Age_cat\"] = pd.cut(train.Age, interval, labels=cats)\n\n# Printing the new Category\ntrain[\"Age_cat\"].head()\ntest[\"Age_cat\"] = pd.cut(test.Age, interval, labels=cats)","4d3891e3":"#train.groupby(['Pclass','Title','Sex'])['Fair_Fare'].median()","38947d97":"train_ticket_freq=train.groupby('Ticket').Ticket.count()\ntest_ticket_freq=test.groupby('Ticket').Ticket.count()","9b2fa169":"train_ticket_count=pd.DataFrame({'Ticket':train_ticket_freq.index,\n                          'Count':train_ticket_freq.values})\ntest_ticket_count=pd.DataFrame({'Ticket':test_ticket_freq.index,\n                          'Count':test_ticket_freq.values})","7ede9962":"train_ticket_count.head(5)","b361e124":"train=pd.merge(train,\n                train_ticket_count,\n                on='Ticket',how='left')\ntest=pd.merge(test,\n                test_ticket_count,\n                on='Ticket',how='left')\ntrain.head()\ntest.head()","5739069a":"train['Fair_Fare']=train['Fare']\/train['Count']\ntrain.head()\ntest['Fair_Fare']=test['Fare']\/train['Count']","85d86aea":"test.loc[test.Fare.isnull()]","b41f2999":"train.Fair_Fare.loc[train.Fair_Fare.isnull()] = train.groupby(['Pclass','Title','Sex']).Fair_Fare.transform('median')\ntest.Fair_Fare.loc[test.Fair_Fare.isnull()] = test.groupby(['Pclass','Title','Sex']).Fair_Fare.transform('median')","baf02f6e":"train['Family']=train['SibSp']+train['Parch']+1\ntest['Family']=test['SibSp']+test['Parch']+1","88f2aaae":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Family\",y=\"Survived\",data=train, kind=\"bar\", size = 6,palette = \"hls\")\ng = g.set_ylabels(\"survival probability\")","33983b02":"train['IsAlone'] = 1 #initialize to yes\/1 is alone\ntrain['IsAlone'].loc[train['Family'] > 1] = 0 # now update to no\/0 if family size is greater than 1\ntest['IsAlone'] = 1 #initialize to yes\/1 is alone\ntest['IsAlone'].loc[test['Family'] > 1] = 0 # now update to no\/0 if family size is greater than 1","9c994ad6":"train.head()","657585ff":"plt.figure()\nsns.countplot(x='Family',data=train,hue='Survived',palette='hls')\nplt.show()","351d66fe":"a=[]\nts=test['Surname']\nfor x in ts:\n    if x not in a:\n        a.append(x)\nprint(len(a))","18140ba5":"import category_encoders as ce\ntarget_enc=ce.TargetEncoder(cols=['Embarked','Sex'])\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(train[['Embarked','Sex']],train['Survived'])\ntrain=train.join(target_enc.transform(train[['Embarked','Sex']]).\n                add_suffix('_target'))\ntrain.head()","b35d0467":"test['Sex_target']=0.742038\ntest.loc[test['Sex']=='female']['Sex_target']=0.188908\ntest.head()","da9eb114":"#Embarked Target Values to be imputed in Test Data\nprint(train[['Embarked','Embarked_target']].head(10))","e36aa29e":"test['Embarked_target']=0.339009\ntest.loc[test['Embarked']=='C']['Embarked_target']=0.553571\ntest.loc[test['Embarked']=='Q']['Embarked_target']=0.389610\ntest.head()","02bcfb03":"train = pd.get_dummies(train, columns=[\"Title\",\"Age_cat\"],\\\n                          prefix=[\"Prefix\",\"Age\"], drop_first=True)\n\ntest = pd.get_dummies(test, columns=[\"Title\",\"Age_cat\"],\\\n                          prefix=[\"Prefix\",\"Age\"], drop_first=True)","27d12475":"#Checking data shapes\nprint(train.shape)\nprint(test.shape)","86d44c70":"train.head()","3bdf6d4f":"#Label Encoding Surnames\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\ntrain_Surname=train.Surname\ntest_Surname=test.Surname\nse=train_Surname.append(test_Surname)\ntotal=pd.DataFrame({'Surname_Encoded':se.values})\n\n\ntotal=total[['Surname_Encoded']].apply(encoder.fit_transform)\ntest_se=total[891:]\ntrain_se=total[:891]\ntest['Surname_Encoded']=test_se\nfor i,x in enumerate(test_se.Surname_Encoded):\n    test.Surname_Encoded.iloc[i]=int(x)\ntrain['Surname_Encoded']=train_se.Surname_Encoded\ntest['Surname_Encoded']=pd.to_numeric(test['Surname_Encoded'], downcast='signed')","62b4e6fc":"del train['Name']\ndel train['Ticket']\ndel train['PassengerId']\ndel train['Age']\ndel train['Cabin']\ndel train['Parch']\ndel train['SibSp']\ndel train['Fare']\ndel train['Surname']\ndel test['Surname']\ndel test['Fare']\ndel test['PassengerId']\ndel test['Age']\ndel test['Ticket']\ndel test['Name']\ndel test['Cabin']\ndel test['Parch']\ndel test['SibSp']\ndel train['Embarked']\ndel test['Embarked']\ndel train['Sex']\ndel test['Sex']\n","3c6af4d7":"train.info()\n","874415f1":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(train.astype(float).corr(),vmax=1.0,annot=True)\nplt.show()\n","ff5d95f9":"train_=train['Survived']\nprint(train_.head())\ny=train_\nprint(y.head())\ny.shape\ntrained=train.drop(['Survived'],axis=1)\nX=trained\nprint(X.shape)\nprint(y.shape)\nprint(test.shape)\nprint(X.columns)\nprint(test.columns)","bf66581f":"imp_features=['Prefix_Mr','Sex_target','Prefix_Miss','Fair_Fare','Surname_Encoded','Pclass','Family','Count','Cabin_notnull','Embarked_target']\nX=X[imp_features]\ntest=test[imp_features]","fa7056c0":"'''from sklearn.ensemble import RandomForestClassifier\n#Decision Tree Algorithm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint \n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ntree = RandomForestClassifier(random_state=42) \n  \n# Instantiating RandomizedSearchCV object \ntree_cv = RandomizedSearchCV(tree, param_dist, cv = 10) \n  \ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_)) \nprint(\"Best score is {}\".format(tree_cv.best_score_)) '''","d34262c0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\ni=1\nk=10\nscores=[0]*k\n#Random Forest Best\nmodel=RandomForestClassifier(random_state=42,n_estimators= 200, min_samples_split= 2, min_samples_leaf= 4, \n                             max_features='auto' ,max_depth=50, bootstrap= False)\n#model=RandomForestClassifier()\n#Logistic Regression Best\n#model = LogisticRegression(random_state=1,C = 10)   \nkf = StratifiedKFold(n_splits=k,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(X,y):    \n    #print('\\n{} of kfold {}'.format(i,kf.n_splits))    \n    xtr,xvl = X.loc[train_index],X.loc[test_index]    \n    ytr,yvl = y[train_index],y[test_index]        \n    #print(xtr.head())\n    model.fit(xtr, ytr)    \n    pred_test = model.predict(xvl) \n    score = accuracy_score(yvl,pred_test)    \n    print('accuracy_score',score)    \n    scores[i-1]=score\n    i+=1\n    #pred_test = model.predict(test)\n    #pred=model.predict_proba(xvl)[:,1]\nprint(\"AVERAGE CROSS VALIDATION SCORE: \",sum(scores)\/len(scores))\n","2229b789":"feature_importances = pd.DataFrame(model.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)","4f69f287":"print(feature_importances)","406ef9e5":"pred_test = model.predict(test)\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\",index_col='PassengerId')\nsubmission['Survived'] = pred_test.astype(int)\nsubmission.to_csv('Titanic_RF_new.csv')","131837c4":"COMBINE Sibling and Parch to combine to Family_Size","c467a5a4":"MODELING","d11c0a11":"First off, removing the columns not necessary.","cde9231c":"CABIN FEATURE GENERATION","e96d458e":"Let us categorize this age feature from information obtained in above cell.","a75b7cb1":"One Hot encoding categorical columns.","e5164ea6":"Feature engineering is complete. Let us prepare our data for modeling.","c5552ef7":"Exploring 'EMBARKED' Feature..earlier we saw that this feature has 2 null values.\nLet us check the count of various embarking point to decide how should we impute the null values in this column.","394e6d98":"Moving on to Age Feature.","47e79719":"The cardinality of this feature>10. Now we will categorize these Titles into groups.","b65f10b3":"Age is a feature that can vary between different groups.. wrt Gender, Class. Also, the passengers on the ship can have different mean\/median age depending upon the Embarked station or the Title they carry. Let us look at the median ages as per these groups.[Feel free to comment what other feature can affect the expected age of a passenger :)]","43d362c4":"Hi Data Science Enthusiasts! Check out my kernel on titanic survival prediction. This kernel uses simple yet effective techniques to combine features and engineer them.\n\nDisclaimer: The following solution uses Random Forest but you can use logistic regression on the engineered 'train' and 'test' data as well. In my case, RF and Logistic give same test score but in training environment, RF leads LR by atleast 1%.\n\nPlease let me know if you any suggestions as this is my first kernel. Please upvote if you like the analysis. Happy predicting!\n","822fcafa":"Everything seems fine here.\n\nLet us check correlation of columns.","ff71068d":"Since Cabin feature has only 204 non null objects, let us generat a features that indicates whether or not it is NULL","4dbd9cd9":"* EMBARKED FILL NAs with Max ID","44ddfdd5":"Randomized Search to identify best parameter combination for random forest.\nI have commented this piece of code because this takes time.","ddf8cbaa":"\"S\" Station has roughly 4X vaues than the second most popular station. Simply imputing Embarked values with the max should work for now.","17cd9fad":"* As we did with Age Feature to fill NAs, FARE FEATURE null values will be filled with Class, Title, Sex Median Values","fd9adf90":"Let us extract Title from Name field using re","f9b5f195":"This categorization is much better."}}