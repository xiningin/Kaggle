{"cell_type":{"6cbfc112":"code","5b25c430":"code","6b5da6e5":"code","eda91572":"code","0097fc6b":"code","288a46c6":"code","b21a6985":"code","7fa272ee":"code","3ddf31e8":"code","a524b50a":"code","694caf8c":"code","5bd73991":"code","fd2bd987":"code","21e992df":"code","f498fee5":"code","7f870806":"code","a57d1d5c":"code","61193bda":"code","180bd450":"code","af41b7a1":"code","da06cac9":"code","cc09a0e7":"code","b96e9946":"code","eae9ba8d":"code","a0ee0805":"code","4fa53cc1":"code","3139a10e":"code","f9f24e1b":"code","5d62c80f":"code","94fd23ed":"code","826833f8":"code","9b6f2f97":"code","d8321489":"code","9b57ae13":"code","15afe529":"code","14ccbf75":"code","e52be588":"code","d63f8cc2":"code","2bd554ed":"code","ead66235":"code","897433fb":"code","59c74c04":"code","d78cf7be":"code","edb67955":"code","a6945f1a":"code","1056031c":"code","94c2ac35":"code","3e5e2309":"code","70ec4743":"markdown","f6083103":"markdown","2d3b09ce":"markdown","ddf5cec0":"markdown","66112700":"markdown","96dc2247":"markdown","22146436":"markdown","46a5c8f1":"markdown","3ae32cd5":"markdown","6126a759":"markdown","091e57f3":"markdown","75017f37":"markdown","a189ec31":"markdown","a7fbaea8":"markdown"},"source":{"6cbfc112":"%matplotlib inline\nimport os, optuna\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom pandas_profiling import ProfileReport\nfrom collections import Counter\n\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, RobustScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor, LocalOutlierFactor\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GroupKFold\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import ElasticNet\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","5b25c430":"df = pd.read_csv(\"..\/input\/now-you-are-playing-with-power\/train.csv\")\ndf = df.set_index('id')","6b5da6e5":"df.head()","eda91572":"# Plot the target variable to get a sense of it as a boxplot\nplt.figure(figsize=(16,8))\nplt.boxplot(df['output_gen'], vert=False)\nplt.title('Target Variable Distribution')\nplt.xlabel('Target Variable Values')\nplt.show()","0097fc6b":"# Plot the target variable to get a sense of it as a distplot\nsns.displot(data = df, x='output_gen', kde=True, height=8, aspect=2\/1)\nplt.title('Target Variable Distribution')\nplt.show()","288a46c6":"#Commenting this out for ease of saving the notebook; notebook was done on local computer\n\n#ProfileReport(df)","b21a6985":"# Some helper functions for different types of encodings of categorical data\n\ndef label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n\ndef one_hot_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X = X.join(pd.get_dummies(X[colname], prefix=colname))\n        X = X.drop(colname, axis=1)\n    return X","7fa272ee":"# Wrapper function to read in, encode and impute missing values for the data\n\ndef load_data():\n    df_train = pd.read_csv(\"..\/input\/now-you-are-playing-with-power\/train.csv\", index_col=\"id\")\n    df_test = pd.read_csv(\"..\/input\/now-you-are-playing-with-power\/test.csv\", index_col=\"id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","3ddf31e8":"# categorical feature encoding\n\nfeatures_nom = [\n    \"obs_day\"\n]\n\nordered_levels = {}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","a524b50a":"#Impute missing values\n\ndef impute(df):\n    # Imoute missing values for numerical data\n    imp = KNNImputer(n_neighbors=5)\n    #imp = SimpleImputer(strategy=\"median\")\n    numerical_df = df.select_dtypes(\"number\")\n    numerical_df = pd.DataFrame(data=imp.fit_transform(numerical_df), index=numerical_df.index, columns =numerical_df.columns)\n    \n    #imput missing values for categorical data\n    imp_cat = IterativeImputer(estimator=RandomForestClassifier(), \n                               initial_strategy='most_frequent',\n                               max_iter=10)\n    categorical_df = df.select_dtypes(\"category\")\n    categorical_df = label_encode(categorical_df)\n    categorical_df = pd.DataFrame(data=imp_cat.fit_transform(categorical_df), index=categorical_df.index, columns =categorical_df.columns, dtype=\"category\")\n    return categorical_df.join(numerical_df).reindex(columns= df.columns)","694caf8c":"#Now, load in the data\n\ndf_train, df_test = load_data()","5bd73991":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","fd2bd987":"df_train.to_csv(\"imputed_train.csv\")\ndf_test.to_csv(\"imputed_test.csv\")","21e992df":"def score_dataset(X, y, model=XGBRegressor(), l_encode=True):\n    # Label encoding for categoricals\n    if l_encode:\n        X = label_encode(X)\n    else:\n        X = one_hot_encode(X)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring='neg_mean_squared_error'\n    )\n    \n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    \n    return score","f498fee5":"X = df_train.copy()\ny = X.pop(\"output_gen\")\n\nscore_dataset(X, y)","7f870806":"X = df_train.copy()\ny = X.pop(\"output_gen\")\n\nscore_dataset(X, y, l_encode=False)","a57d1d5c":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","61193bda":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nmi_scores = make_mi_scores(X, y)\nmi_scores","180bd450":"# Try removing some of the uninformative features to see if that improves scores\nuninformative_features = [\n    'obs_minute',\n    'obs_day'\n]\n\nX = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.loc[:,~X.columns.isin(uninformative_features)]\n\nscore_dataset(X, y)","af41b7a1":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","da06cac9":"X = df_train.copy()\ny = X.pop(\"output_gen\")\n\npca, X_pca, loadings = apply_pca(X.select_dtypes(include=np.number))","cc09a0e7":"plot_variance(pca, width=10, dpi=100)","b96e9946":"loadings.iloc[:, :3]","eae9ba8d":"X = df_train.copy()\ny = X.pop(\"output_gen\")\n_, X_pca, _ = apply_pca(X.select_dtypes(include=np.number))\n#X = X.loc[:,~X.columns.isin(uninformative_features)]\n#X = X.join(X_pca.set_index(X.index).iloc[:, :6])\nX = X_pca.set_index(X.index).join(X[features_nom])\n\n\nscore_dataset(X, y)","a0ee0805":"def cluster_labels(df, features, n_clusters=10):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = one_hot_encode(X_scaled)\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ (X_scaled.std(axis=0)+0.000001)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n    X_new = pd.DataFrame(index=X.index)\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_new[\"Cluster\"] = X_new[\"Cluster\"].astype(\"category\")\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=10):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = one_hot_encode(X_scaled)\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ (X_scaled.std(axis=0)+0.000001)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])], index=X.index\n    )\n    return X_cd","4fa53cc1":"vape_with_hour = [\n    'obs_hour', 'vap_pressure', 'vap_enth', 'vap_motion', 'vap_temp'\n]\n\nfw_with_hour = [\n    'obs_hour', 'vap_enth', 'fw_enth', 'fw_motion'\n]\n\nvape = [\n    'vap_pressure', 'vap_enth', 'vap_motion', 'vap_temp'\n]\n\nfw = [\n    'vap_enth', 'fw_enth', 'fw_motion'\n]\n\nfw_and_vape = [\n    'vap_pressure', 'vap_enth', 'vap_motion', 'vap_temp', 'fw_enth', 'fw_motion'\n]","3139a10e":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_distance(X, vape_with_hour, n_clusters=10))\n\nscore_dataset(X, y)","f9f24e1b":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_labels(X, vape_with_hour, n_clusters=10))\n\nscore_dataset(X, y)","5d62c80f":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_distance(X, fw_with_hour, n_clusters=10))\n\nscore_dataset(X, y)","94fd23ed":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_labels(X, fw_with_hour, n_clusters=10))\n\nscore_dataset(X, y)","826833f8":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_distance(X, fw, n_clusters=10))\n\nscore_dataset(X, y)","9b6f2f97":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_labels(X, fw, n_clusters=10))\n\nscore_dataset(X, y)","d8321489":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_distance(X, vape, n_clusters=10))\n\nscore_dataset(X, y)","9b57ae13":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_labels(X, vape, n_clusters=10))\n\nscore_dataset(X, y)","15afe529":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_distance(X, fw_and_vape, n_clusters=10))\n\nscore_dataset(X, y)","14ccbf75":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(cluster_labels(X, fw_and_vape, n_clusters=10))\n\nscore_dataset(X, y)","e52be588":"def interactions(df):\n    df_new = pd.get_dummies(X['obs_hour'].astype(\"category\")).mul(X['vap_enth'], axis=0)\n    return df_new","d63f8cc2":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(interactions(X))\n\nscore_dataset(X, y)","2bd554ed":"def indicate_outliers(df):\n    X_new = pd.DataFrame(index=df.index)\n    lof = LocalOutlierFactor(n_neighbors=20)\n    X_new['outlier_scores'] = lof.fit(one_hot_encode(df)).negative_outlier_factor_\n    return X_new","ead66235":"indicate_outliers(X)","897433fb":"X = df_train.copy()\ny = X.pop(\"output_gen\")\nX = X.join(indicate_outliers(X))\n\nscore_dataset(X, y)","59c74c04":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"output_gen\")\n    \n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"output_gen\")\n        X = pd.concat([X, X_test])\n        \n    #Add cluster features\n    X = X.join(cluster_labels(X, vape_with_hour, n_clusters=10))\n        \n    #indicate outliers\n    X = X.join(indicate_outliers(X))\n        \n    #X = label_encode(X)\n    X= one_hot_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n    '''\n    #Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    cols_to_target_encode = [\"categoryA\", \"categoryE\"]\n    X = X.join(encoder.fit_transform(X, y, cols=cols_to_target_encode))\n    X = X.loc[:,~X.columns.isin(cols_to_target_encode)]\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n        X_test = X_test.loc[:,~X_test.columns.isin(cols_to_target_encode)]\n        '''\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X","d78cf7be":"#df_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"output_gen\"]\n\nscore_dataset(X_train, y_train)","edb67955":"def objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 5, 16),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=5)\nxgb_params = study.best_params","a6945f1a":"print(xgb_params)","1056031c":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"output_gen\"]\n\nfinal_model = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nfinal_model.fit(X_train, y_train)\npredictions = final_model.predict(X_test)\n\noutput = pd.DataFrame({'id': X_test.index, 'output_gen': predictions})\noutput.to_csv('third_submission.csv', index=False)","94c2ac35":"y_preds = final_model.predict(X_train)","3e5e2309":"sns.distplot(y)\nsns.distplot(y_preds)\nplt.show()","70ec4743":"## 1. Read in the Data and do Exploratory Data Analysis \n- look at the nature of the target variable (i.e. min and max values, skew, modality, etc.)\n- look at the other variables for any distribution skews, missing data, data types (i.e. numerical, categorical, etc.)\n- See if there are variable problems like high caridnality in the categorical data or different scales for the numerical data (i.e. values ranging from 0-1, 1-100, etc.)\n- look at any correlations present","f6083103":"## 6. Fit Final Model and Submit","2d3b09ce":"switching over to just PCA features is generally worse than baseline","ddf5cec0":"Adding in vapour pressure with hour clusters seems to get some traction. None of the other clusters really seem to help","66112700":"removing uninformative features did not help performance","96dc2247":"The target variable has a couple of different (3-4) regions of density; a multi-modal distirbution possibly related to ther variables","22146436":"The following code was my general plan. However, I had a power outage with an hour and half left and never finished running the code :(. Even still, I think I focused too much on feature engineering and did not do enough to address issues with missing data. I welcome any feedback!","46a5c8f1":"The plot of the actual versus predicted look pretty close. ","3ae32cd5":"## 5. Model Selection & Hyperparameter Tuning\n\nTry out a couple of models and optimize their hyperparameters on the feature set\n\nNote: it was at this point I suffered a power outage, so I jusyt focused in on the one algorithm, XGB Regression.","6126a759":"## Clean and read in the data\n\na lot of dirty data; around 11% of every column has missing data. missing data does not seem to be in a pattern. Missign data across categorical (days) and numerical values. I got my greatest performance boost in the last hour of the compeition by going from a simple imputer to something better.","091e57f3":"## Feature Engineering\n\n- Take a look at mutual information scores to see what variables work well with the target variable\n- Look at the PCA of the numerical features for inspiration\n- Use clustering to try and bin features for better performance\n- Try some interactions between some of the thermodynamic variables\n- Try flagging outlier data points","75017f37":"Indicating outliers really doesn't seem to help","a189ec31":"Lets try flagging outliers","a7fbaea8":"## Establish a baseline\n\nFor the baseline, I used XGBoost regressor with default settings"}}