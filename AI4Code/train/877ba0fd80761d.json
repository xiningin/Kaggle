{"cell_type":{"51429195":"code","6717c851":"code","851f5a09":"code","fce1e62f":"code","ed323ea9":"code","e2195b96":"code","8ec46128":"code","ec6019a2":"code","75fac951":"code","447ac87e":"code","8c21873f":"code","b8fcedfa":"code","97fc786f":"code","52cdd43f":"code","7b7ef807":"code","f2cecb8c":"code","cf3f2779":"code","3d2ad3c0":"code","a48e4041":"markdown"},"source":{"51429195":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport sklearn\nimport torchvision\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport PIL\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport seaborn as sns\nimport glob\nfrom pathlib import Path\ntorch.manual_seed(1)\nnp.random.seed(1)","6717c851":"train_files = []\ntrain_labels = []\ntest_files = []\ntest_labels = []\n\nfor classes in os.listdir('..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Training'):\n    class_path = os.path.join('..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Training', classes)\n    for file in os.listdir(class_path):\n        file = os.path.join(class_path, file)\n        train_files.append(file)\n        train_labels.append(classes)\n\nfor classes in os.listdir('..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Test'):\n    class_path = os.path.join('..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Test', classes)\n    for file in os.listdir(class_path):\n        file = os.path.join(class_path, file)\n        test_files.append(file)\n        test_labels.append(classes)","851f5a09":"train_files = pd.Series(train_files, name='file')\ntrain_labels = pd.Series(train_labels, name='label')\n\ntest_files = pd.Series(test_files, name='file')\ntest_labels = pd.Series(test_labels, name='label')\n\ntrain_df = pd.concat([train_files, train_labels], axis=1)\ntest_df = pd.concat([test_files, test_labels], axis=1)","fce1e62f":"lb = LabelEncoder()\ntrain_df['encoded_labels'] = lb.fit_transform(train_df['label'])\ntest_df['encoded_labels'] = lb.fit_transform(test_df['label'])","ed323ea9":"class FruitDetection(torch.utils.data.Dataset):\n    def __init__(self, df=train_df, transform=transforms.Compose([transforms.ToTensor()])):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        length = len(self.df)\n        return length\n    \n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx, 0]\n        label = self.df.iloc[idx, 2]\n        label = torch.tensor(label)\n        image = Image.open(img_path).convert('RGB')\n        img = np.array(image)\n        image = self.transform(image=img)[\"image\"]\n        return image, label","e2195b96":"train_transforms = A.Compose([\n    A.Resize(224, 224, 3),\n    A.HorizontalFlip(),\n    A.VerticalFlip(),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\ntest_transforms = A.Compose([\n    A.Resize(224, 224, 3),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","8ec46128":"train_dataset = FruitDetection(df=train_df, transform=train_transforms)\ntest_dataset = FruitDetection(df=test_df, transform=test_transforms)","ec6019a2":"train_df","75fac951":"train_df['encoded_labels'].value_counts()","447ac87e":"len(train_df['encoded_labels'].value_counts())","8c21873f":"train_labels = list(range(132))\ntrain_labels.remove(0)","b8fcedfa":"train_weights = []\nfor classes in train_labels:\n    total = 0\n    for row in train_df.itertuples():\n        if row[1] == classes:\n            total += 1\n    total \/= len(train_df)\n    total = 1 - total\n    train_weights.append(total)","97fc786f":"train_loading_weights = []\nfor img, label in train_dataset:\n    for i in range(131):\n        if label == torch.tensor(i):\n            train_loading_weights.append(train_weights[i])","52cdd43f":"batch_size = 64\n\ntrain_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=train_loading_weights, num_samples=len(train_dataset))\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=4)","7b7ef807":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","f2cecb8c":"model = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Sequential( \n                nn.Linear(2048, 131),\n                nn.LeakyReLU(),\n#                 nn.Dropout(0.2),\n#                 nn.BatchNorm1d(81),\n#                 nn.LeakyReLU(81, 7)\n)\nfor param in model.parameters():\n    param.requires_grad=True","cf3f2779":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\ncriterion = nn.CrossEntropyLoss()\n\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=4, cooldown=2, verbose=True)\n\nmodel = model.to(device)\ntrain_criterion = criterion.to(device)","3d2ad3c0":"epochs = 60\n\ntotal_train_loss = []\ntotal_valid_loss = []\nbest_valid_loss = np.Inf\n\nfor epoch in range(epochs): \n    print('Epoch: ', epoch + 1)\n    train_loss = []\n    valid_loss = []\n    train_correct = 0\n    train_total = 0\n    valid_correct = 0\n    valid_total = 0\n    for image, target in train_loader:\n        model.train()\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    for image, target in test_loader:\n        with torch.no_grad():\n            model.eval()\n            optimizer.zero_grad()\n            image, target = image.to(device), target.to(device)\n            output = model(image)\n            loss = criterion(output, target)\n            valid_loss.append(loss.item())\n            \n    epoch_train_loss = np.mean(train_loss)\n    epoch_valid_loss = np.mean(valid_loss)\n    print(f'Epoch {epoch + 1}, train loss: {epoch_train_loss:.4f}, valid loss: {epoch_valid_loss:.4f}')\n    if epoch_valid_loss < best_valid_loss:\n        torch.save(model.state_dict(), 'emotion_detection.pth')\n        print('Model improved. Saving model.')\n        best_valid_loss = epoch_valid_loss\n        \n    lr_scheduler.step(epoch_valid_loss)\n    total_train_loss.append(epoch_train_loss)\n    total_valid_loss.append(epoch_valid_loss)","a48e4041":"Training until epoch 60 gets a training loss of 0.0038 and validation loss of 0.1952. "}}