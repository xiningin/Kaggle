{"cell_type":{"a91e6be7":"code","fbc235cb":"code","6f0b5221":"code","66977ba9":"code","fb313880":"code","32a83bd5":"code","c0887a0c":"code","c5500fcd":"code","bab661de":"code","e92c431b":"code","f6df2197":"code","f4add648":"code","d24f40fe":"code","6032d965":"code","c8b8fde9":"code","9470c788":"code","23abff31":"code","af786915":"code","2abd4e87":"code","92e26819":"code","a4e592ed":"code","87736cfd":"code","fc861a97":"code","28208581":"code","2119479d":"code","982c6387":"code","02e3dff5":"code","21aaffe1":"code","86addae9":"code","7b78b596":"code","f5c9ef22":"code","eece8075":"code","f7378e4a":"code","a76d5a76":"code","1bd76cf6":"code","31edcc53":"code","702dbd7f":"code","e6a162b4":"code","62f28129":"code","c0f36ab4":"code","3025f3ba":"code","d503d01f":"code","5afb1ab1":"code","480b6777":"code","308645df":"code","fe40cbaf":"code","010d9b7a":"code","925e0e1b":"code","50225a27":"code","4affc72c":"code","c4de9f0f":"code","9618bdcc":"code","1fc4b45d":"code","2ce5cf72":"code","49b91ac3":"code","37300002":"code","b05ebe74":"code","b9ee5fbb":"code","43cb79ba":"code","687a3347":"code","5b502034":"code","8a6d140f":"code","9b3db480":"code","36ff1acd":"code","46af1575":"code","9be09a16":"code","5b0a4414":"code","760b5ba8":"code","8e66c0b9":"code","309424b6":"code","583c72cb":"code","e11ce124":"code","5c3558aa":"code","2fdd07f5":"code","78edd558":"markdown","781e3d69":"markdown","1186345d":"markdown","7e721840":"markdown","a8318409":"markdown","bef43658":"markdown","407ac775":"markdown","3eb5e637":"markdown","1414ef8a":"markdown","f9109565":"markdown","b183c1d7":"markdown","805cb39e":"markdown","abf0aa9b":"markdown","4589dadc":"markdown","99d810b5":"markdown","61d94184":"markdown","4993430d":"markdown","9a0fa847":"markdown","fd3014a1":"markdown","ae247806":"markdown","ad4ea654":"markdown","92a3ab09":"markdown","bc9cd9f4":"markdown","5c2bbceb":"markdown","bcc9de25":"markdown","86a90f9c":"markdown","9c9c8cdd":"markdown","1bd4df81":"markdown","4b93f59c":"markdown","81ab4ec2":"markdown","91500b43":"markdown","2c8b2422":"markdown","84c6e85f":"markdown","3a69094b":"markdown","4451764e":"markdown","0ef1f26e":"markdown","2ba967e9":"markdown","eea17290":"markdown","8e8ae6f6":"markdown"},"source":{"a91e6be7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbc235cb":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadspeptidecsv\/peptide.csv', delimiter=';', encoding = \"ISO-8859-1\", nrows = nRowsRead)\ndf.dataframeName = 'peptide.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","6f0b5221":"df.plot()","66977ba9":"fig = px.pie(df,\n             values=\"PepPosition\",\n             names=\"Prediction_ML_advanced_original\",\n             template=\"seaborn\")\nfig.update_traces(rotation=90, pull=0.05, textinfo=\"percent+label\")\nfig.show()","fb313880":"fig = px.pie(df,\n             values=\"NQE\",\n             names=\"Prediction_EL_Manzalawy_balanced\",\n             template=\"seaborn\")\nfig.update_traces(rotation=90, pull=0.05, textinfo=\"percent+label\")\nfig.show()","32a83bd5":"# Plotting boxplot for 'perc_premium_paid_by_cash_credit' column using the 'seaborn' library:\nsns.boxplot(df['Binding'],color='brown')\nplt.show()","c0887a0c":"sns.boxplot(df['PepPosition'],color='brown')\nplt.show()","c5500fcd":"#Nuclear Quantum Effects (NQE)\nsns.distplot(df['NQE'],color='green')\nplt.show()","bab661de":"sns.distplot(df['YWF'],color='purple')\nplt.show()","e92c431b":"# Now taking logarithm of the Income data and then plotting its distribution:\nsns.distplot(np.log(df[\"PepPosition\"]),color='red')\nplt.show()","f6df2197":"plt.hist(['PepPosition'],histtype='stepfilled',label=str,color='cyan',bins=10)\nplt.show()","f4add648":"#df['PepPosition'].value_counts().plot.bar(color='green')\n#plt.show()","d24f40fe":"#word cloud\nfrom wordcloud import WordCloud, ImageColorGenerator\ntext = \" \".join(str(each) for each in df.IVIG_sample)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_words=200,colormap='Set3', background_color=\"black\").generate(text)\nplt.figure(figsize=(10,6))\nplt.figure(figsize=(15,10))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","6032d965":"#word cloud\nfrom wordcloud import WordCloud, ImageColorGenerator\ntext = \" \".join(str(each) for each in df.Description)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_words=200,colormap='GnBu', background_color=\"white\").generate(text)\nplt.figure(figsize=(10,6))\nplt.figure(figsize=(15,10))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","c8b8fde9":"#Codes by Pooja Jain https:\/\/www.kaggle.com\/jainpooja\/av-guided-hackathon-predict-youtube-likes\/notebook\n\ntext_cols = ['PepSPID', 'Description', 'IVIG_sample']\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['|']), random_state = 42)\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(text_cols):\n  op = wc.generate(str(df[c]))\n  _ = axes[i].imshow(op)\n  _ = axes[i].set_title(c.upper(), fontsize=24)\n  _ = axes[i].axis('off')\n\n_ = fig.delaxes(axes[3])","9470c788":"#Nuclear Quantum Effects (NQE)\nsns.stripplot(x = \"NQE\", y = \"PepPosition\", data = df, jitter = True);","23abff31":"sns.catplot(\"NQE\", \"PepPosition\", data = df)\n#Nuclear Quantum Effects (NQE)","af786915":"sns.set_style('whitegrid')\nplt.figure(figsize=(30,30))\nsns.catplot(y='PepPosition',x='NQE',data=df,ci=None,col='Binding',sharey=False)","2abd4e87":"sns.set_style('whitegrid')\nplt.figure(figsize=(30,30))\nsns.catplot(y='PepPosition',x='YWF',data=df,ci=None,col='Binding',sharey=False)","92e26819":"# For this analysis I'll use scatter plot:\nsns.scatterplot(df['NQE'],df['PepPosition'])\nplt.show()","a4e592ed":"px.scatter(df, x='Prediction_EL_Manzalawy_balanced', y='PepPosition', color='Binding')","87736cfd":"px.scatter(df, x='Prediction_ML_advanced_original', y='PepPosition', color='Binding')","fc861a97":"px.scatter(df, x='Peptide', y='PepPosition', color='Binding')","28208581":"px.scatter(df, x='NQE', y='PepPosition', color='Binding', color_discrete_sequence=[\"red\", \"green\", \"blue\"])","2119479d":"num_vars=[x for x in df.columns if df[x].dtypes!='O']\nnum_vars","982c6387":"#Code by Pranay https:\/\/www.kaggle.com\/phuskisher\/epitope-prediction-bcell-auc-0-9\n\nfig, axes=plt.subplots(2,2, figsize=(20,20))\nfor i,j in enumerate(num_vars):\n    ax=axes[int(i\/2), i%2]\n    sns.kdeplot(df[j], ax=ax)","02e3dff5":"#Nuclear Quantum Effects (NQE)\n#sns.barplot(df['PepPosition'],df['NQE'],color='b',errcolor='c',errwidth='.26')\n#plt.xticks(rotation=45)\n#plt.show()\n#It's messy, though I save the snippet for a next time.","21aaffe1":"px.line(df,x=\"NQE\",y=\"PepPosition\",width = 500)\n#Nuclear Quantum Effects (NQE)","86addae9":"#Code by Chumajin https:\/\/www.kaggle.com\/chumajin\/eda-for-biginner\/notebook\n\npx.histogram(\n    df, \n    x='PepPosition',\n    nbins=50,\n    width = 500\n)","7b78b596":"fig = px.bar(df, x= \"NQE\", y= \"PepPosition\", color_discrete_sequence=['crimson'], title=\"Nuclear Quantum Effects & Peptides Position\")\nfig.show()","f5c9ef22":"fig = px.bar(df, x= \"YWF\", y= \"PepPosition\", color_discrete_sequence=['#2B3A67'], title=\"YWF Activator & Peptides Position\")\nfig.show()","eece8075":"#Code by Tushar Mishra https:\/\/www.kaggle.com\/tmchls\/covid-19-epitope-prediction-task-1-b-cell\/notebook\nfeatures=[\"Binding\",\"NQE\",\"YWF\",\"PepPosition\"]\nplt.figure(figsize=(20,20))\nplt.subplots_adjust(hspace=2.0)\nj=1\nfor i in features:\n    plt.subplot(4,5,j)\n    sns.distplot(df[i])\n    j+=1","f7378e4a":"df.isnull().sum()","a76d5a76":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)","1bd76cf6":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","31edcc53":"df[categorical_nan].isna().sum()","702dbd7f":"# Lets handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","e6a162b4":"df[numerical_nan].isna().sum()","62f28129":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","c0f36ab4":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","3025f3ba":"df = pd.get_dummies(df)","d503d01f":"df.head()","5afb1ab1":"#Code By Pranay  https:\/\/www.kaggle.com\/phuskisher\/epitope-prediction-bcell-auc-0-9\n\nfor i in num_vars:\n    fig=px.histogram(df, x=i, color='NQE')\n    fig.show()","480b6777":"for i in num_vars:\n    fig=px.box(df, y=i, color='YWF')\n    fig.show()","308645df":"corr= df.corr()\nmask=np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nfig,ax = plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corr, mask=mask, vmax=0.9, square=True, annot=True, cmap='YlGnBu')\nplt.show()","fe40cbaf":"x = df.drop('PepPosition',axis=1)","010d9b7a":"y = df['PepPosition']","925e0e1b":"from sklearn.model_selection import train_test_split","50225a27":"train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=11111, shuffle=True, train_size=None, test_size=None)","4affc72c":"# Creating dummies of both train_x and test_x sets:\ntrain_x = pd.get_dummies(train_x)\ntest_x = pd.get_dummies(test_x)","c4de9f0f":"train_y.value_counts()\/len(train_y)","9618bdcc":"test_y.value_counts()\/len(test_y)","1fc4b45d":"from sklearn.linear_model import LogisticRegression","2ce5cf72":"logr = LogisticRegression(n_jobs=1,max_iter=100,random_state=11111)","49b91ac3":"logr.fit(train_x,train_y)","37300002":"logr.score(train_x,train_y)","b05ebe74":"logr.score(test_x,test_y)","b9ee5fbb":"from sklearn.tree import DecisionTreeClassifier","43cb79ba":"dtc= DecisionTreeClassifier(max_depth=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0)","687a3347":"dtc.fit(train_x,train_y)","5b502034":"dtc.score(train_x,train_y)","8a6d140f":"dtc.score(test_x,test_y)","9b3db480":"from sklearn.ensemble import RandomForestClassifier","36ff1acd":"rfc = RandomForestClassifier(n_estimators = 100)","46af1575":"rfc.fit(train_x,train_y)","9be09a16":"rfc.score(train_x,train_y)","5b0a4414":"rfc.score(test_x,test_y)","760b5ba8":"rfc.predict(train_x)","8e66c0b9":"# Similarly getting predictions on test_x as follows:\nrfc.predict(test_x)","309424b6":"df.shape , train_x.shape","583c72cb":"#rfc.predict(test)","e11ce124":"#test_prediction = rfc.predict(test)","5c3558aa":"#submission = pd.DataFrame()\n#submission['target'] = test_prediction\n#submission.to_csv('customer_premium_on_time.csv', header=True, index=False)","2fdd07f5":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Mar\u00edlia Prata, @mpwolke was Here' )","78edd558":"#Check the proportion of 1s and 0s in the dependent variables of train & test that are just created.","781e3d69":"Compare the sizes of this and train_x set to see how different is test dataset currently from the train_x\n\nWe don't have test and train files, therefore test.shape Now is df.shape","1186345d":"#Evidence for Two Types of EAR (Epitope Antibody Reactivities)\n\nThe authors machine learning approaches initially identified two main classes of peptides, 1st degree classifiable and unclassifiable peptides. In order to directly relate this classification to information on the amino acid composition, the characteristics of respective peptides were visualized with the help of two ratio PWMs in separate scatter graphs for the \u201cbinding\u201d and \u201cnon-binding\u201d peptides. The visualization indicated that the 1st degree classifiable and unclassifiable peptides disperse into two distinguished groups for both, the \u201cbinding\u201d and \u201cnon-binding\u201d peptides, due to their opposite physico-chemical characteristics.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","7e721840":"#Nuclear Quantum Effects (NQE)","a8318409":"#Immunological Perspectives\n\nAntibodies from healthy human donors have been found by epitope profiling of IVIG preparations to bind to a vast number of peptides of human origin. \n\nIn absolute terms, the human immune system should not have any antibodies directed against their own proteome. However, due to the way how antibodies are generated, sophisticated mechanisms have to be in place either to prevent or to minimize B cells from generating autoreactive antibodies or to remove B cells (e.g. by autophagy) to eliminate the generation of highly reactive auto-antibodies.\n\nPossibly, the immunoglobulin locus including the machinery regulating humoral in conjunction with cellular immune responses has been modified over millions of years under constraints to select immunoglobulin structures harmless to their own organisms.\n\nAbout 25% of peptides in the authors training and test sets were scored as \u201cbinding\u201d to IVIG with high confidence. However, this percentage ignores the number of peptides tested in total and is probably overestimated.\n\nEAR (Epitope Antibody Reactivities) analysis leads to the hypothesis that under physiological conditions immunoglobulins possibly contribute to the homeostasis of the immune system by constantly capturing circulating peptides that originate from human proteins. The postulated scavenger function of eliminating self-peptides should not lead to inflammatory processes as exemplified by autoimmune diseases.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","bef43658":"![](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/bin\/pone.0078605.g007.jpg)\nDistribution of peptides initially scored as 1st degree classifiable and unclassifiable by ML-simple using PWM measures.\n\nPeptides of the training set were assigned to the groups 1st degree classifiable and unclassifiable by the authors \u201csimplified machine learning using human-understandable attributes\u201d (ML-simple) approach. They were further divided into peptides reacting with IVIG (\u201cbinding\u201d; panel A) or not reactive with IVIG (\u201cnon-binding\u201d; panel B). In a next step each peptide was assigned values using two ratio PWMs. The x-axis values derive from a PWM that was based on all peptides present in the training set. They are calculated by multiplying the ratios of the relative frequencies of each amino acid at each position in a peptide sequence for the group \u201cbinding\u201d (panel A) and \u201cnon-binding\u201d (panel B), respectively. The y-axis values were calculated in the same way, however, only the 1st degree unclassifiable peptides present in the training set were used as input of the PWM. Each peptide is represented by one dot. Peptides in red in panel A correspond to the type I EAR while those in black depict the type II EAR.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","407ac775":"#Type II EAR - Antibodies binding generated independently of specific MHC-peptide presentation to T cell receptors\n\n(MHC- Major Histocompatibility Complex)\n\nA second class of epitopes are represented by 1st degree unclassifiable peptides bound by IVIG (Type II EAR). Their properties are opposed to those of Type I EAR peptides, i.e. they are specifically enriched in polar amino acids asparagine, glutamine and glutamic acid and display low aromaticity. \n\nThe polarity in this group here would fit previous epitope descriptions . Antibodies binding to these peptides are suspected to have been generated independently of specific MHC-peptide (MHC: major histocompatibility complex) presentation to T cell receptors, ROC curve of 1st degree unclassifiable peptides.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","3eb5e637":"I just copy that explanation for the moment I have train\/test.\n\nThis means there's 58.1 % accuracy on my train dataset & 39,6 % accuracy on the test dataset. This also means that the test sample is really a representative of the train sample. However, a training score of 58,1 % is not that great for a good prediction of an unseen test dataset. The test set that I've been using till now was made out of the train dataset. But the test dataset that I've got as \"test.csv\" file has data of totally new customers. So it's like an unseen data for my model. For this reason, my LogisticRegression model will not give true predictions for the test.csv dataset.","1414ef8a":"#Handling Missing Values  ","f9109565":"The existence of two distinct EAR modes on the epitope level might have counterparts on the corresponding paratope level of binding antibodies. The question is whether these two EAR groups can be related to specific types of antibody species and whether paratope binding rules can be established in the future as well.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","b183c1d7":"#Logistic Regression Model","805cb39e":"#Above we can see what was made by the authors. From now, that's what I made.\n\nPlease, don't laugh.","abf0aa9b":"#Predicting linear B-cell epitopes using string kernels\n\nAuthors: Yasser El-Manzalawy , Drena Dobbs, Vasant Honavar - PMID: 18496882 PMCID: PMC2683948 DOI: 10.1002\/jmr.893\n\n\nThe identification and characterization of B-cell epitopes play an important role in vaccine design, immunodiagnostic tests, and antibody production.\n\nTherefore, computational tools for reliably predicting linear B-cell epitopes are highly desirable. El-Manzalawy, Dobbs and Honavar (authors above) evaluated Support Vector Machine (SVM) classifiers trained utilizing five different kernel methods using fivefold cross-validation on a homology-reduced data set of 701 linear B-cell epitopes, extracted from Bcipep database, and 701 non-epitopes, randomly extracted from SwissProt sequences.\n\nAnalysis of the data sets used and the results of this comparison show that conclusions about the relative performance of different B-cell epitope prediction methods drawn on the basis of experiments using data sets of unique B-cell epitopes are likely to yield overly optimistic estimates of performance of evaluated methods. \n\nThis argues for the use of carefully homology-reduced data sets in comparing B-cell epitope prediction methods to avoid misleading conclusions about how different methods compare to each other. https:\/\/pubmed.ncbi.nlm.nih.gov\/18496882\/","4589dadc":"I just copied and changed the numbers, since there is no train\/test \n\nSo there's 100 % accuracy on my train dataset and 49.6 % accuracy on the test dataset. This means my model is 100% accurate on the train dataset now. But still, a test accuracy of 49,6% is not that great. So I'll take up one more model which is called Random Forest","99d810b5":"#Create an object for this so that use the \"fit\" & the \"predict\" functions on it. Named it as logr","61d94184":"As Random Forest model has achieved significant score, Use this model to get the predictions. Start with getting predictions on train_x set as follows","4993430d":"#Epitope Prediction - Attibutes: High Aromaticity, Low Polarity and High Tyrosine Content\n\nClassifiers for epitope prediction were trained on the training set and applied to the test set. Both sets contained three times more non-binding than binding peptides, but this may not necessarily be the case for potential new data. \n\nAUC scores and accuracies characterized ML-advanced as slightly better compared to the machine learning according to El-Manzalawy when trained on both, the original and balanced training sets.The corresponding ROC curves are visualized  together with those derived from PWM classification of the training set. \n\nTo determine the attributes that enable the prediction of \u201cbinding\u201d, peptides of the training set were classified in a machine learning approach named ML-simple by using attributes reflecting human-readable rules.\n\nAttributes significantly associated with prediction of \u201cbinding\u201d were in particular high aromaticity, low polarity and high tyrosine content. However, the moderate percentages of correctly classified peptides for the individual attributes illustrate once more the advantage of using an ensemble approach for the most accurate prediction. https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","9a0fa847":"#I don't have train or test in this Dataset. I have previously made dummification above.","fd3014a1":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSiP_3UF8oo5O6nluFqxU7fQmYeZTHMEM9Gbw&usqp=CAU)onlinelibrary.wiley.com","ae247806":"It was suppose to be 0 and 1, but I don't have train and the result above doesn't explain much as expected.","ad4ea654":"#Simplified Machine Learning Using Human Understandable Attributes (ML-simple)\n\nA simplified machine learning approach was conducted using human-understandable attributes and rules (ML-simple). The principle advantage of the additional PWM approach was that it is more readily readable and facilitates to point to position effects and amino acid patterns that can be experimentally explored.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","92a3ab09":"We don't have test\/train. Save the lines below for another Dataset.","bc9cd9f4":"#Random Forest Model","5c2bbceb":"#Type I EAR (Epitope Antibody Reactivities) -  T cell activation\n\nThe typical classifiable epitopes bound by IVIG antibodies (Type I EAR) share several properties expected from the literature: high frequency of tyrosine, low hydrophobicity and high antigenicity.\n\nAntibody generation giving rise to Type I EAR (Epitope Antibody Reactivities) might directly be induced by T cell activation elicited by peptide binding to MHC class II complexes, ROC curve of 1st degree classifiable peptides that are predicted to be binding to MHC class II with a higher AUC score than all peptides of the training set. \nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","bcc9de25":"#Training the Model, by Rachit Scukla https:\/\/www.kaggle.com\/rax007\/insurance-company-premium\n\nFirst, Create a set of independent variables from the train dataset. Drop the 'target' variable from it using axis=1. This axis=1 specifies that the drop shall happen from the column. Store this set in an object called \"x\" as follows:","86a90f9c":"#Building classifier ensembles for B-cell epitope prediction\n\nAuthors:  EL-Manzalawy Y, Vasant Honavar\nMethods in Molecular Biology (Clifton, N.J.), 31 Dec 2013, 1184:285-294\nDOI: 10.1007\/978-1-4939-1115-8_15 PMID: 25048130 PMCID: PMC4385709\n\nIdentification of B-cell epitopes in target antigens is a critical step in epitope-driven vaccine design, immunodiagnostic tests, and antibody production.\n\nB-cell epitopes could be linear, i.e., a contiguous amino acid sequence fragment of an antigen, or conformational, i.e., amino acids that are often not contiguous in the primary sequence but appear in close proximity within the folded 3D antigen structure. \n\nNumerous computational methods have been proposed for predicting both types of B-cell epitopes. However, the development of tools for reliably predicting B-cell epitopes remains a major challenge in immunoinformatics.\n\nClassifier ensembles a promising approach for combining a set of classifiers such that the overall performance of the resulting ensemble is better than the predictive performance of the best individual classifier. \nhttp:\/\/europepmc.org\/article\/PMC\/4385709 ","9c9c8cdd":"#Dummification","1bd4df81":"#Limits of Epitope Prediction - Immune system classifies based on more specific characteristics using different modes of #antibody binding.\n\nThe design of a high-performance classifier for epitope prediction raises the principle question on the limits of such machine learning approaches. There may be representations of peptides, which could be used as attributes for machine learning, that capture all relevant epitope features for antibody binding and ignore all properties irrelevant for antibody binding. It appears that such a representation needs to include the information on whether a peptide belongs to the classifiable or unclassifiable group (at least 1st degree, if not also 2nd). \n\nHowever, the authors do not yet know how to obtain this information without knowing the class of the peptide (binding or non-binding).\n\nSome peptides belong to groups that have common characteristics, which can be learned by machine learning algorithms, so they are classified correctly \u2013 these are the 1st degree classifiable ones. The remaining peptides are not classified correctly because their broad characteristics point to the wrong classification, while the immune system classifies them based on more specific characteristics most likely by using different modes of antibody binding.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","4b93f59c":"![](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/bin\/pone.0078605.g001.jpg)\nData set preparation and computational workflow for the prediction of epitope-antibody-reactivities (EAR) determined for IVIG antibodies.\nRectangles represent groups of peptides (numbers in each group are indicated), boxes with rounded corners indicate the applied classification approaches. 1All peptides printed on the microarrays 2Removal of false positive (binding) peptides (e.g. those reactive with secondary antibodies) 3Separation of peptide set according to signal intensities of EAR into non-binders, binders and unassigned peptides 4Classification approach ML-advanced\u200a=\u200amachine learning with an ensemble classifier 5Number of peptides predicted to be non-binding\/binding, separated into those predicted correctly (underlined) and incorrectly 6Classification approach PWM\u200a=\u200aposition weight matrix 7Classification approach ML-simple\u200a=\u200asimplified machine learning using human-understandable attributes 8Capital letters A\u2013H indicate subsets of peptides assigned in supplementary information table S1 and explained there in the legend\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","81ab4ec2":"#Regulatory T cell (Treg) epitopes, now known as Tregitopes\n\nIn IgG, the main component of intravenous immunoglobulin therapy (IVIg). Tregitopes provide one explanation for the expansion and activation of Treg cells following IVIg treatment.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcR89lzYCi4Vxbt6i0tv2w27hvgR6QfFtEegLw&usqp=CAU)epivax.com","91500b43":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQ7ph5SZePZuB_DZazVHWwZIDyCVeaqujEQ6w&usqp=CAU)sciencedirect.com","2c8b2422":" Keep only the 'target' variable in an object y:","84c6e85f":"#Epitope Predictions Indicate the Presence of Two Distinct Types of Epitope-Antibody-Reactivities Determined by Epitope #Profiling of Intravenous Immunoglobulins\n\nAuthors: Mitja Lu\u0161trek,  ,  Peter Lorenz,  Michael Kreutzer,  Zilliang Qian,  ,  Felix Steinbeck,  Di Wu,  ,  Nadine Born,  Bjoern Ziems,  Michael Hecker,  Miri Blank,  Yehuda Shoenfeld,  Zhiwei Cao,  Michael O. Glocker,  Yixue Li,  Georg Fuellen,  and Hans-J\u00fcrgen Thiesen - Francesco Pappalardo, Editor\nPLoS One. 2013; 8(11): e78605. - Published online 2013 Nov 11. doi: 10.1371\/journal.pone.0078605\n\nComputational prediction of linear B cell epitopes was conducted using machine learning with an ensemble of classifiers in combination with position weight matrix (PWM) analysis. \n\nThe first attempts to predict continuous B cell epitopes were based on propensity scales. Current state-of-the-art epitope prediction generally uses machine learning approaches. The peptide chips were incubated with commercial intravenous immunoglobulin fractions (IVIG). \n\nThe peptides that bind antibodies are considered to contain at least one epitope, i.e., one antibody binding site. The input data set was split in half by random sampling to form a training and a test set.\n\nThe training and test sets contain roughly three times more non-binding than binding peptides. Such imbalanced data sets might reduce the performance of classifiers trained by machine learning algorithms. To handle imbalanced data sets, two methods were applied, random oversampling and undersampling.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","3a69094b":"#FastRNABindR: Fast and accurate prediction of protein-RNA interface residues\n\nAuthors: Yasser EL-Manzalawy, Mostafa Abbas, Qutaibah M. Malluhi, Vasant Honavar\nJuly 2016PLoS ONE 11(7):e0158445 -DOI: 10.1371\/journal.pone.0158445\n\nA wide range of biological processes, including regulation of gene expression, protein syn-thesis, and replication and assembly of many viruses are mediated by RNA-protein interac-tions. \n\nHowever, experimental determination of the structures of protein-RNA complexes isexpensive and technically challenging. Hence, a number of computational tools have been developed for predicting protein-RNA interfaces.\n\nThe computational efforts needed for generating PSSMs severely limits the practical utility of protein-RNA interface prediction servers. In this work, the authors experimented with two approaches, random sampling and sequence similarity reduction, for extracting a representative reference database of protein sequences from more than 50 million proteinsequences in UniRef100.\n\nTheir results suggest that random sampled databases produce bet-ter PSSM profiles (in terms of the number of hits used to generate the profile and the distance of the generated profile to the corresponding profile generated using the entire UniRef100 data as well as the accuracy of the machine learning classifier trained using these profiles). \n\nBased on their results, they developed FastRNABindR, an improved version of RNABindR for predicting protein-RNA interface residues using PSSM profiles generatedusing 1% of the UniRef100 sequences sampled uniformly at random.\n\nFastRNABindR is the only protein-RNA interface residue prediction onlineserver that requires generation of PSSM profiles for query sequences and accepts hundreds of protein sequences per submission.\n\nTheir approach for determining the optimal BLAST database for a protein-RNA interface residue classification task has the potential of substantially speeding up, and hence increasing the practical utility of, other amino acid sequence based predictors of protein-protein and protein-DNA interfaces. https:\/\/www.researchgate.net\/publication\/304993225_FastRNABindR_Fast_and_accurate_prediction_of_protein-RNA_interface_residues","4451764e":"#Coronavirus epitope prediction from highly conserved region of spike protein\n\nAuthor: Valentina Yurina - Clin Exp Vaccine Res 2020;9:169-173 -https:\/\/doi.org\/10.7774\/cevr.2020.9.2.169 - pISSN 2287-3651 \u2022 eISSN 2287-366X \n\nPurpose: The aim of this research was to predict the epitope for coronavirus family spike protein. Coronavirus family is highly evolved viruses which cause several outbreaks in the past decades. Therefore, it is crucial to design a global vaccine candidate to prevent the coronavirus outbreak in the future.\n\nMaterials and Methods: The spike protein amino acid sequences from nine coronavirus\nfamily were searched in the Uniprot database. The spike protein sequences were aligned using Clustal method. The highly conservatives amino acids were analyzed its B cell linear and\ncontinuous epitopes and T cell epitopes.\n\nResults: From the alignment results it was found that there is a highly conserved region in the extracellular domain of spike protein. With prediction methods from this highly conserved region, B cell and T cell epitopes from spike protein were derived.\n\nConclusion: From several different prediction results, B cell epitope and T cell epitope were identified in the highly conserved region thus it is promising to be developed as a coronavirus vaccine candidate.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQu4YuKHtelKCe6ignKtJZMDeHuQbMPvDNqOQ&usqp=CAU)\nhttps:\/\/ecevr.org\/Synapse\/Data\/PDFData\/9995CEVR\/cevr-9-169.pdf","0ef1f26e":"#Accuracy: expected and not expected\n\nWhile experimenting with various attributes and machine learning algorithms, the authors discovered that many of them can predict epitopes with an accuracy of around 80%. Not all classifiers misclassified the same peptides, which is why combining the classifiers into an ensemble improved the performance. However, it seemed that 15\u201320% of the peptides resisted correct first round classification irrespective of the method used.\n\nThe accuracy on the 1st degree classifiable peptides was close to 100%, which was also as expected. The value did not reach 100% because the classifier was exclusively trained on the 1st degree classifiable peptides, whereas the classifier that divided the peptides into classifiable and unclassifiable was trained on all peptides of the training set. However, the accuracy on the 1st degree unclassifiable peptides was also high (91.5%), which was not expected.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","2ba967e9":"#Decision Tree Classifier Model","eea17290":"I copied the lines below from Rachit Shukla  https:\/\/www.kaggle.com\/rax007\/insurance-company-premium\n\nNotice from the distribution plot that although the 'PepPosition' variable is considered float by python but it is actually an int variable as it has discrete values & not continuous! It reminds me of categorical variable. So now is the time I introduce the categorical variables & their analysis:\n\nThe categorical variables are discrete in nature & are stored as 'object' datatype. During the Univariate analysis of categorical variables, the task is to look for 'count' and 'count%'.","8e8ae6f6":"![](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/bin\/pone.0078605.g002.jpg)Performance comparison of the tested classifiers for IVIG binding prediction on the test set peptides.\n\nROC analysis for the authors machine learning approach with an ensemble classifier (ML-advanced), the machine learning method of El-Manzalawy et al. (2008), and a PWM approach using an PWM derived from the training set. Both machine learning approaches were trained on the original training set (\u201coriginal\u201d: three times more non-binding than binding peptides) and on the balanced training set (\u201cbalanced\u201d: equal number of binding and non-binding peptides) and finally applied on the test set. AUC values are indicated as well. Note that the curves based on the original and balanced training set of the authors ML-advanced method show almost complete overlap.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/"}}