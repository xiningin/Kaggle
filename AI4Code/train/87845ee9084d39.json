{"cell_type":{"10f5bbc1":"code","4b5cb16d":"code","8441c335":"code","8cd96865":"code","a373f749":"code","a0a0b60a":"code","9a649ca7":"code","cba16708":"code","f652b7ae":"code","38802634":"code","7445217c":"code","17b91efe":"code","75a13630":"code","3cf760bb":"code","1ac3c3ff":"code","1b87c1f4":"code","19997fec":"code","0efa851e":"code","c407bab7":"code","56272729":"code","e0eddb23":"code","ddbe8425":"markdown","932a7de4":"markdown"},"source":{"10f5bbc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nfrom pandas import * # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b5cb16d":"df = read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","8441c335":"df","8cd96865":"import scipy.cluster.hierarchy as sch\nfrom matplotlib.pyplot import *\n%matplotlib inline","a373f749":"x = df.iloc[:,:-1].values","a0a0b60a":"dendro = sch.dendrogram(sch.linkage(x,method = 'ward'))\ntitle('Dendrogram')\nylabel('Euclidean distance')\nxlabel('Clusters')","9a649ca7":"df['quality'].value_counts()","cba16708":"x = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","f652b7ae":"y","38802634":"for i in range(len(y)) :\n    if y[i]>=6.5 :\n        y[i] = 1\n    else:\n        y[i] = 0","7445217c":"y","17b91efe":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)","75a13630":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","3cf760bb":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)","1ac3c3ff":"from sklearn.model_selection import GridSearchCV","1b87c1f4":"parameters = [{'C':[0.25,0.5,0.75,1.0]}]","19997fec":"grid = GridSearchCV(estimator = reg , param_grid = parameters , scoring = 'accuracy',cv = 10)\ngrid.fit(x_train,y_train)\nbest_accuracy = grid.best_score_\nbest_parameters = grid.best_params_\nprint('BestAccuracy : {:.2f}%'.format(best_accuracy*100))\nprint('BestParameters : ',best_parameters)","0efa851e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","c407bab7":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier()","56272729":"models = [(dt,[{'criterion':['gini','entropy']}]),(knn,[{'n_neighbors':[5,10]}]),(rf,[{'n_estimators':[10,50,100,150,200],'criterion':['gini','entropy']}])]","e0eddb23":"for i,j in models :\n    grid = GridSearchCV(estimator = i , param_grid = j , scoring = 'accuracy',cv = 10)\n    grid.fit(x_train,y_train)\n    best_accuracy = grid.best_score_\n    best_parameters = grid.best_params_\n    print('{} BestAccuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('BestParameters : ',best_parameters)","ddbe8425":"**Therefore , we have got highest accuracy of 90.46% using RandomForest algorithm .**","932a7de4":"It is evident from the dendrogram that the data points in this dataset can be divided into two clusters ."}}