{"cell_type":{"c687f9ee":"code","6e420515":"code","8c3d3e08":"code","b2fe71e8":"code","e5d5ce70":"code","7f09fff3":"code","4bfacab1":"code","8f419525":"code","25510b57":"code","b373c1b5":"code","65d48eba":"code","68a327ab":"code","aec1dd5f":"code","8c0ec7dc":"code","261598e1":"code","f867d95e":"code","62a7aa05":"code","aa43001b":"code","5f9f2f03":"code","adfafbfb":"code","8de6efc9":"code","adec86af":"code","c92354c6":"code","6bc5c534":"code","4c66f63a":"code","a3b8f94e":"code","c01951a4":"code","30c8c216":"code","9ea8fa56":"code","d698cfee":"code","b7c17420":"code","b8a34dc3":"code","25f5360e":"code","d3b332e1":"code","99c22ec6":"code","167a6141":"code","231844cf":"code","7119b64b":"code","467d2dbf":"code","70119a3c":"code","f1605be3":"code","3d17fc6d":"code","d13d27d9":"code","d2d4b42c":"code","20defd64":"code","f9007a0e":"code","b2a27cd2":"code","2ef3c82e":"code","b386abc6":"code","d1ddbbc5":"code","8cbddade":"code","22f01f77":"code","443bcd30":"code","5013ccba":"code","5afb233e":"code","513df8c9":"code","ed3f6702":"code","0d569439":"code","c413a576":"code","bfcf1838":"code","a1f9b364":"code","6564f40d":"code","141185df":"code","c85cfc2d":"code","8ba8a63a":"code","7a3aeebb":"code","9ea61941":"code","f934ac54":"code","36984d55":"code","05d91b18":"code","f64195f1":"code","e2f3462d":"code","0107d57c":"code","79a0d1df":"code","e18b0365":"code","05149551":"code","f168ed27":"code","925cdf88":"code","c41f56cd":"code","87bd77f7":"code","4ce21f72":"code","725b3cb1":"code","fa0e87f0":"code","cd5593c8":"code","6849ea00":"code","30f42a12":"code","29fe8581":"code","43afbce1":"markdown","8d7f9794":"markdown","f1cacd8f":"markdown","d3a86a38":"markdown","830112fa":"markdown","aa6e3736":"markdown","d49cccaf":"markdown","19c4e5d7":"markdown","dba6daa7":"markdown","1af6dab0":"markdown","9062990d":"markdown","1bcb9900":"markdown","acba544b":"markdown","69ffc0cb":"markdown","a07c06d7":"markdown","9b73f5bd":"markdown","f9fb03a3":"markdown","3c7dea3b":"markdown","370a0e66":"markdown","54d86c86":"markdown","5db46e8b":"markdown","2f9795fb":"markdown","78cf4cf6":"markdown","ca562ce1":"markdown","3f4898bf":"markdown","6e186bb2":"markdown","8d404719":"markdown"},"source":{"c687f9ee":"#!pip install kaggle","6e420515":"#from google.colab import drive\n#drive.mount('\/content\/gdrive')","8c3d3e08":"#from google.colab import files\n#files.upload() ","b2fe71e8":"#!pip install -q kaggle\n#!mkdir -p ~\/.kaggle\n#!cp kaggle.json ~\/.kaggle\/\n#!ls ~\/.kaggle\n## we need to set permissions \n#!chmod 600 \/root\/.kaggle\/kaggle.json","e5d5ce70":"# Google Colab directory setting. Comment out after run this line\n\n#import os\n#os.chdir('\/content\/gdrive\/My Drive\/Competitions\/kaggle\/Kaggle-Titanic\/nbs')  #change dir","7f09fff3":"#!pwd","4bfacab1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport matplotlib.style as style\nstyle.available\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8f419525":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","25510b57":"df = pd.read_csv('..\/input\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})","b373c1b5":"df.head()","65d48eba":"test.head()","68a327ab":"df.groupby(by=['Pclass'])['Survived'].agg(['mean','count'])","aec1dd5f":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/2bc37b51-c9e4-402e-938e-70d3145815f2\/d787jna-1b3767d2-f297-4b73-a874-7cfa6d1e8a69.png\/v1\/fill\/w_1600,h_460,q_80,strp\/r_m_s__titanic_class_system_by_monroegerman_d787jna-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NDYwIiwicGF0aCI6IlwvZlwvMmJjMzdiNTEtYzllNC00MDJlLTkzOGUtNzBkMzE0NTgxNWYyXC9kNzg3am5hLTFiMzc2N2QyLWYyOTctNGI3My1hODc0LTdjZmE2ZDFlOGE2OS5wbmciLCJ3aWR0aCI6Ijw9MTYwMCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.6krQcPQvsfcQ_ZJ_CGvufi9MT-PJkkg1I8-grLy7Hiw\")","8c0ec7dc":"sex_survived= df.groupby(by=['Sex','Survived'])['Survived'].agg(['count']).reset_index()\nsex_survived","261598e1":"plt.figure(figsize=(10, 5))\nstyle.use('seaborn-notebook')\nsns.barplot(data=sex_survived, x='Sex',y='count', hue='Survived');","f867d95e":"# Plotly configuration function for Google Colab. We need to run this function for showing plotly graph in the Google colab\ndef configure_plotly_browser_state():\n    \n    import IPython\n    display(IPython.core.display.HTML('''\n        <script src=\"\/static\/components\/requirejs\/require.js\"><\/script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '\/static\/base',\n              plotly: 'https:\/\/cdn.plot.ly\/plotly-1.5.1.min.js?noext',\n            },\n          });\n        <\/script>\n        '''))","62a7aa05":"male_survived=pd.DataFrame(df['Age'][(df['Sex']=='male')& (df['Survived']==1)].value_counts().sort_index(ascending=False)).reset_index().rename(columns={'index':'Age','Age':'Number'})\nfemale_survived=pd.DataFrame(df['Age'][(df['Sex']=='female')& (df['Survived']==1)].value_counts().sort_index(ascending=False)).reset_index().rename(columns={'index':'Age','Age':'Number'})\nmale_not_survived=pd.DataFrame(df['Age'][(df['Sex']=='male') & (df['Survived']==0)].value_counts().sort_index(ascending=False)).reset_index().rename(columns={'index':'Age','Age':'Number'})\nfemale_not_survived=pd.DataFrame(df['Age'][(df['Sex']=='female') & (df['Survived']==0)].value_counts().sort_index(ascending=False)).reset_index().rename(columns={'index':'Age','Age':'Number'})\n\n","aa43001b":"from plotly import tools\n\n#Add function here\nconfigure_plotly_browser_state()\ninit_notebook_mode(connected=False)\n\ntrace1 = go.Scatter(\n    x = male_survived['Age'].sort_values(ascending=False),\n    y = male_survived['Number'],\n    name='Survived Male',\n    fill='tozeroy',\n    #connectgaps=True\n\n)\ntrace2 = go.Scatter(\n    x = female_survived['Age'].sort_values(ascending=False),\n    y = female_survived['Number'],\n    name='Survived Female',\n    fill='tozeroy',\n    #connectgaps=True\n\n)\ntrace3 = go.Scatter(\n    x = male_not_survived['Age'].sort_values(ascending=False),\n    y = male_not_survived['Number'],\n    fill='tozeroy',\n    name='Not Survived Male',\n    #connectgaps=True\n\n)\ntrace4 = go.Scatter(\n    x = female_not_survived['Age'].sort_values(ascending=False),\n    y = female_not_survived['Number'],\n    fill='tozeroy',\n    name = 'Not Survived Female',\n    \n\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Male', 'Female'))\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace3, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace4, 2, 1)\n\nfig['layout']['xaxis2'].update(title='Age')\nfig['layout'].update(height=700, width=1200,\n                     title='Age Gender Survive')\n\n\n\niplot(fig)","5f9f2f03":"# We need to make some data wrangling with both train and test data\ndf_all = [df,test]","adfafbfb":"for data in df_all:\n    print(f\"\\n -------- {data.index } ------- \\n\")\n    print(data.isnull().sum())","8de6efc9":"\nfor data in df_all:\n    data['isAlone']=1\n\n    data['Family_No'] = data['Parch'] + data['SibSp'] + 1\n        \n    data['isAlone'].loc[data['Family_No']>1]=0\n    \n    data['Age'].fillna(round(data['Age'].mean()), inplace=True)\n    \n    #``df.fillna(df.mode().iloc[0])`` If you want to impute missing values with the mode in a dataframe \n    data['Embarked'].fillna(data['Embarked'].mode().iloc[0], inplace=True)\n    \n    # mean of each Pclass\n    #data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n    data['Fare'] = df.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(x.mean()))\n    \n    ","adec86af":"df.head()","c92354c6":"test.isAlone.value_counts()","6bc5c534":"# Drop features that will not process\nfor data in df_all:\n    data.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch'],inplace=True,axis=1)","4c66f63a":"for data in df_all:\n    print(f\"\\n -------- {data.index } ------- \\n\")\n    print(data.isnull().sum())","a3b8f94e":"#get_dummies() function allows us to make a column for each categorical variable in features\ntest = pd.get_dummies(test,columns=['Sex','Embarked'])\ndf = pd.get_dummies(df,columns=['Sex','Embarked'])","c01951a4":"df.head()","30c8c216":"y=df['Survived']\nX=df.drop(columns=['Survived'],axis=1)","9ea8fa56":"#Split the data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","d698cfee":"# Original pararamaters\nDT= DecisionTreeClassifier()\nDT.fit(X_train, y_train)\nDT.score(X_test,y_test)","b7c17420":"# Checking the hyperparamates of decision tree classifier\nfrom IPython.display import HTML, IFrame\nIFrame(\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\", width=1100, height=500)","b8a34dc3":"# Grid CV\nparameters1 = [{'max_depth':np.linspace(1, 15, 15),'min_samples_split': np.linspace(0.1, 1.0, 5, endpoint=True)}]","25f5360e":"# Grid Search for Decision Treee\nGrid1 = GridSearchCV(DT, parameters1, cv=4,return_train_score=True,iid=True)\n\nGrid1.fit(X_train,y_train)","d3b332e1":"scores = Grid1.cv_results_","99c22ec6":"for param, mean_train in zip(scores['params'],scores['mean_train_score']):\n    print(f\"{param} accuracy on training data is {mean_train}\")","167a6141":"# best estimator for in Decision tree paramaters that we define. \nGrid1.best_estimator_","231844cf":"#Max score for above parameters\nmax(scores['mean_train_score'])","7119b64b":"XGB = XGBClassifier()","467d2dbf":"#parameters2 = [{'max_depth':np.linspace(1, 15, 15),'min_samples_split': np.linspace(0.1, 1.0, 5, endpoint=True),'n_estimators':[100]}]\n\nparameters3 =[{\"learning_rate\": [0.05, 0.10, 0.15, 0.20] ,\"max_depth\": [ 3, 4, 5, 6], \"min_child_weight\": [3,5,7],\"gamma\": [ 0.0, 0.1, 0.2 ,0.3],\"colsample_bytree\" : [ 0.4, 0.5]}]","70119a3c":"Grid1 = GridSearchCV(XGB, parameters3, cv=2,return_train_score=True)\n\nGrid1.fit(X_train,y_train)","f1605be3":"scores = Grid1.cv_results_","3d17fc6d":"# best estimator for in Decision tree paramaters that we define. \nGrid1.best_estimator_","d13d27d9":"max(scores['mean_train_score'])","d2d4b42c":"XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.5, gamma=0.0, learning_rate=0.1,\n       max_delta_step=0, max_depth=3, min_child_weight=5, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)","20defd64":"XGB.fit(X_train, y_train)","f9007a0e":"XGB.score(X_test,y_test)","b2a27cd2":"#pred = XGB.predict(test)","2ef3c82e":"#result = pd.DataFrame(pred,columns=['Survived'])","b386abc6":"#test1  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})","d1ddbbc5":"#submission = result.join(test1['PassengerId']).iloc[:,::-1]","8cbddade":"#submission.to_csv('submission.csv', index=False)","22f01f77":"df = pd.read_csv('..\/input\/train.csv' , header = 0,dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0,dtype={'Age': np.float64})","443bcd30":"df.info()","5013ccba":"\nfor data in [df,test]:\n    data['isAlone']=1\n\n    data['Family_No'] = data['Parch'] + data['SibSp'] + 1\n        \n    data['isAlone'].loc[data['Family_No']>1]=0\n    \n    data['Age'].fillna(data['Age'].mean(), inplace=True)\n    \n    #``df.fillna(df.mode().iloc[0])`` If you want to impute missing values with the mode in a dataframe \n    data['Embarked'].fillna(data['Embarked'].mode().iloc[0], inplace=True)\n    \n    # mean of each Pclass\n    #data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n    data['Fare'] = df.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(x.mean()))","5afb233e":"#import re\n# We have two types tickets first only number and the second one letter and number. We are going to have letters and create a feature.\n#trial_addFeature['Ticket_name'] =[]\n#test_addFeature['Ticket_name'] =[]\n# for data in df_all:\n#     for i,k in enumerate(data['Ticket']):\n#         try:\n#             x=k.split(\" \")[1]\n#             data['Ticket'].replace(data['Ticket'][i],k.split(\" \")[0],inplace=True)\n#         except IndexError:\n#             data['Ticket'].replace(data['Ticket'][i],\"No_letter\",inplace=True)\n\n\n#     data['Ticket'] =data['Ticket'].map(lambda x: re.sub('[.\/]', '', x))\n#     data['Ticket'] =data['Ticket'].map(lambda x: x.upper())\n\n\n#df['Ticket_name'] =df['Ticket_name'].map(lambda x: re.sub('[.\/]', '', x))\n#df['Ticket_name'] =df['Ticket_name'].map(lambda x: x.upper())\n\n#set(Ticket_name)            ","513df8c9":"#set(data['Ticket'])","ed3f6702":"#test['Ticket'] = test_addFeature['Ticket']\n#df['Ticket'] = df_addFeature['Ticket']\n","0d569439":"test.head()","c413a576":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\n#Upper limit is 100 but the oldest person is 80 years old\nfor data in [test,df]:\n    bins = [-1,0,5,10, 15, 25, 50,100]\n    labels = ['Unknown','Baby','Child','Young','Teen','Adult','Old']\n    data['Age'] = pd.cut(data['Age'], bins=bins,labels=labels)\n    data['Age'] = data['Age'].astype(str)\ntest['Age'] = LE.fit_transform(test['Age'])  \ndf['Age'] = LE.fit_transform(df['Age'])  \n\n#data['Age'] = data['Age'].astype(int)\n","bfcf1838":"for data in [test,df]:\n    for i,k in enumerate(data['Name']):\n        x=k.split(\",\")[1]\n        data['Name'].replace(data['Name'][i],x.split(\" \")[1],inplace=True)\n        ","a1f9b364":"df['Name'].value_counts()","6564f40d":"all_data = [df,test]\nKnown = ['Mr.','Miss.','Mrs.','Master.','Ms.','Mlle.','Mme.']\nfor k in (all_data):\n    for i,data in enumerate(k['Name']):\n        if (data) in Known:\n            if(data=='Mlle.'):\n                k['Name'] = k['Name'].replace('Mlle.','Miss.')\n            elif(data=='Ms.'):\n                k['Name'] = k['Name'].replace('Ms.','Miss.')\n            elif(data=='Mme.'):\n                k['Name'] = k['Name'].replace('Mme.','Mrs.')\n            else:\n                continue\n        else:\n            k['Name'] = k['Name'].replace(data,'not_known')\n        \n            \n            \n        \n        \n    ","141185df":"# Survived difference between people who had different title\ndf['Name'][df['Survived']==1].value_counts()\/df['Name'].value_counts()","c85cfc2d":"df.info()\n","8ba8a63a":"#columns = ['Embarked','Age','Sex','Name']\n#\n# for data in [df,test]:\n#     for i in columns:\n#         data[i] = data[i].astype(str)\n#         data[i] = LE.fit_transform(data[i])\n\n# Create feature for each categories\ntest=pd.get_dummies(test,columns=['Embarked','Name'])\ndf=pd.get_dummies(df,columns=['Embarked','Name'])\ntest['Sex'] = LE.fit_transform(test['Sex'])\ndf['Sex'] = LE.fit_transform(df['Sex'])","7a3aeebb":"for data in [df,test]:\n    data.drop(columns=['Ticket','Cabin','SibSp','Parch','PassengerId'], inplace=True, axis=1)","9ea61941":"df.drop(columns=['Embarked_Q'],axis=1,inplace=True)\ntest.drop(columns=['Embarked_Q'],axis=1,inplace=True)","f934ac54":"for data in [df,test]:\n\n    scale = StandardScaler().fit(data[['Fare']])\n    data[['Fare']] = scale.transform(data[['Fare']])\n","36984d55":"y=df['Survived']\nX=df.drop(columns=['Survived'],axis=1)\n","05d91b18":"#Split the data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","f64195f1":"\ndf.head()","e2f3462d":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,  VotingClassifier","0107d57c":"parameters_DC = [{'max_depth':[50,100],'min_samples_split': [0.1,0.2,0.5,0.8,0.9]}]\n\nparamaters_RF = [{'max_depth':[2,5,10,15,20,50],'min_samples_split': [0.1,0.2,0.5,0.8],'n_estimators':[100]}]\n\nparameters_XGB =[{\"learning_rate\": [0.2,0.5,0.8,0.9] ,\"max_depth\": [1, 3,5, 10], \"min_child_weight\": [3,5,7,10,20],\"gamma\": [0.1, 0.2 ,0.4,0.7],'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],'n_estimator':[100,1000,2000,4000]}]\n\nparameters_GBC =[{\"learning_rate\": [0.5, 0.25, 0.1, 0.05, 0.01] ,\"max_depth\": [ 3, 4, 5, 6], \"min_samples_leaf\" :[50,100,150],\"n_estimators\" : [16, 32, 64, 128]}]\n\nparameters_ADA =[{'algorithm':['SAMME'],\"base_estimator__criterion\" : [\"gini\"],\"base_estimator__splitter\" :   [\"best\", \"random\"],\"n_estimators\": [500,1000],\"learning_rate\":  [ 0.01, 0.1, 1.0]}] \n\n","79a0d1df":"DC = DecisionTreeClassifier()\n\n\n\nGrid_DC = GridSearchCV(DC, parameters_DC, cv=4,scoring=\"accuracy\", n_jobs= 4,return_train_score=True, verbose = 1)\n#Fit the model\nGrid_DC.fit(X_train,y_train)\n\n# Best estimator parameters\nDC_best = Grid_DC.best_estimator_\n\n# Best score for the model with the paramaters\nGrid_DC.best_score_","e18b0365":"RF = RandomForestClassifier()\n\nGrid_RF = GridSearchCV(RF, paramaters_RF, cv=4,scoring=\"accuracy\", n_jobs= 4,return_train_score=True, verbose = 1)\n#Fit the model\nGrid_RF.fit(X_train,y_train)\n\n# Best estimator parameters\nRF_best = Grid_RF.best_estimator_\n\n# Best score for the model with the paramaters\nGrid_RF.best_score_\n","05149551":"XGB = XGBClassifier()\n\nGrid_XGB = GridSearchCV(XGB, parameters_XGB, cv=4,scoring=\"accuracy\", n_jobs= 4,return_train_score=True, verbose = 1)\n#Fit the model\nGrid_XGB.fit(X_train,y_train)\n\n# Best estimator parameters\nXGB_best = Grid_XGB.best_estimator_\n\n# Best score for the model with the paramaters\nGrid_XGB.best_score_","f168ed27":"GBC = GradientBoostingClassifier()\n\n\nGrid_GBC = GridSearchCV(GBC,parameters_GBC, cv=4, scoring=\"accuracy\", n_jobs= 4, return_train_score=True,verbose = 1)\n\nGrid_GBC.fit(X_train,y_train)\n\nGBC_best = Grid_GBC.best_estimator_\n\n# Best score\nGrid_GBC.best_score_","925cdf88":"ADA = AdaBoostClassifier(DC_best)\n\n\nGrid_ADA = GridSearchCV(ADA,parameters_ADA, cv=4, scoring=\"accuracy\", n_jobs= 4, return_train_score=True,verbose = 1)\n\nGrid_ADA.fit(X_train,y_train)\n\nADA_best = Grid_ADA.best_estimator_\n\n# Best score\nGrid_ADA.best_score_","c41f56cd":"parameters_SVM = {'C': [0.1, 1, 10,50,100], 'gamma' : [0.001, 0.01, 0.1, 1,10]}\n","87bd77f7":"\nfrom sklearn.svm import SVC\nSVMC =SVC(probability=True)\nGrid_SVC = GridSearchCV(SVMC, parameters_SVM, scoring=\"accuracy\", return_train_score=True,verbose = 1,cv=2)\n\nGrid_SVC.fit(X_train, y_train)\n\nSVM_best = Grid_SVC.best_estimator_\n\n# Best score\nGrid_SVC.best_score_","4ce21f72":"voting = VotingClassifier(estimators=[('ADA', ADA_best),('DC', DC_best),('RF', RF_best),('GBC',GBC_best),('XGB',XGB_best),('SVC',SVM_best)],weights=[3,0,0,1,3,3], voting='hard', n_jobs=4)\n\nvoting_result = voting.fit(X_train, y_train)","725b3cb1":"voting.score(X_test,y_test)","fa0e87f0":"pred = voting.predict(test)","cd5593c8":"test_2 = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})","6849ea00":"result = pd.DataFrame(pred,columns=['Survived'])\nsubmission13 = result.join(test_2['PassengerId']).iloc[:,::-1]","30f42a12":"submission13.to_csv('submission13.csv', index=False)","29fe8581":"#!kaggle kernels push","43afbce1":"### Checking the result on XGB\nWe are going to submit only XGBoost result","8d7f9794":"### Ensemble Model","f1cacd8f":"<b>Sex:<\/b>","d3a86a38":"### Titanic Dataset","830112fa":"We are going to drop columns that we will not use in Machine learning process","aa6e3736":"### Name Title\nWe take the name and create a new features with Title of person","d49cccaf":"<b>Ticket<\/b>","19c4e5d7":"#### AdaBoostClassifier","dba6daa7":"### Age\nAs we imagine group of age is important than a single age (We think children women and the elderly had a chance for safe boats)","1af6dab0":"<b>Pclass:<\/b> Ticket class[](http:\/\/)\n- A proxy for socio-economic status","9062990d":"<b>Family:<\/b> \n> SibSpof: siblings-spouses aboard the Titanic  \n> Parchof: parents-children aboard the Titanic\n\n\n\nStep: Create a new features with _SibSpof_ and _Parchof_","1bcb9900":"### Voting Model","acba544b":"#### XGBoost","69ffc0cb":"<b>Target<\/b>: Our target is find which travels survied (Survived==1) or not (Survived==0)\n","a07c06d7":"We are going to predict travels who survived or not with features below","9b73f5bd":"# Introduction\n\nThis Kernel run on Google colab. First you need create a folder in your Google Drive and install datasets and kernels. Please check my blog and learn how to set up Kaggle with Colab.\n\nRun Kaggle kernels on Google Colab:\nhttps:\/\/medium.com\/@erdemalpkaya\/run-kaggle-kernel-on-google-colab-1a71803460a9\n\n\n### Improvement\n\nI used **pd.get_dummies()** for some of converting categorical columns to numerical columm. When you use the function an example for sex it creates male(0,1) and female(0,1) \n![get_dummies example](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*HfhgywtwXtxVcUmQuyu-_w.png)\n\nSo we do not need to keep both features and sometimes reducing the that features increase your accuracy.\n\nPreviously my submission was in top 16% after removing the dummy features my submission is in -> top 8%\n","f9fb03a3":"We are going to try one XGBoost with Grid Search and check the result","3c7dea3b":"#### Fare","370a0e66":"#### SVM","54d86c86":"#### Convert categorical data to Numerical data for process","5db46e8b":"The accuracy of the test data is <span style=\"color:red\">77.9<\/span> We need to try add feature or try ensemle models ","2f9795fb":"#### RandomForest","78cf4cf6":"#### GradientBoosting","ca562ce1":"## Introduction\n\nThis notebook included some EDA and predictions of survived Machine learning techniques with Scikit-learn","3f4898bf":"#### DecisionTree","6e186bb2":"### Feature Engineering\nNow we are going to try add more feature and try to change \n\n1. We are going to create some groups in with age column.\n\n2. Add a new features with Name titles like Mrs,Miss etc.. \n\n3. For some machine learning techniques we need to create features for each variables first we will try without and we will compare accuracy on the test data.\n\n4. Look close ensemble models and we will add multiple scikit learns models.\n","8d404719":"<b>Mlle: <\/b>The term Mademoiselle is a French familiar title, abbreviated Mlle, traditionally given to an unmarried woman.\n\n<b>Mme: <\/b>French abbreviation for Madame"}}