{"cell_type":{"0002744f":"code","b1e01122":"code","2d9f1686":"code","d36d3d63":"code","97ae5b82":"code","81b2b38a":"code","da5709cb":"code","1b07fb9c":"code","71d6e6e1":"code","7dd4b52c":"code","74f12505":"code","9195e8a0":"code","02a2c9ac":"code","1a7729f2":"code","45ff4b1b":"code","0b65e60d":"code","fe688776":"code","75280e56":"code","0a234c86":"code","ae0f1366":"code","e1333fa4":"code","f86e0ef8":"code","2542d28c":"code","e0860f41":"code","751bf4ff":"code","e1024f3c":"code","3c24b5f0":"code","0b6d1b50":"code","4b51d983":"code","49f55016":"code","d0e4e6b4":"code","0314e924":"code","673b17e0":"code","2468afa8":"code","37253172":"code","180de094":"code","8b3af255":"code","ebfb1565":"code","70e518d7":"code","04b43bef":"code","2e7bb014":"code","dfa700da":"code","8bdc0cef":"code","68a4715e":"code","0a4ab0ed":"code","865b705c":"code","cf3e3099":"code","5f4a59f6":"code","7db235d5":"code","6813b2f7":"code","5f873293":"code","872a9daa":"code","5418c8f6":"code","0311bc4a":"code","72993d99":"code","de1f474f":"code","716f0dca":"code","24cea38e":"code","f2bdbde4":"code","1b0951ef":"code","e423c13f":"code","1bc4d1b7":"code","93464b05":"code","d85b90cd":"code","9fa1eac5":"code","bd93265e":"code","f30545ef":"code","460c458c":"code","ae205f85":"code","b1979df7":"code","d00a088a":"code","fbea1d71":"code","dd7c9cff":"markdown","32fb2227":"markdown","f506a226":"markdown","5587fffd":"markdown","431c8e94":"markdown","87a0df49":"markdown","6c7ea626":"markdown","f2122235":"markdown","7c70c326":"markdown","c4f47592":"markdown","4d717e1c":"markdown","0492ad32":"markdown","8282cacb":"markdown","76df45cc":"markdown","2a2de0ac":"markdown","19c300cc":"markdown","3c1645b7":"markdown","aa3e158a":"markdown","3ed7db5d":"markdown","a031aa6b":"markdown","184a4155":"markdown","fb27211b":"markdown","12c668f8":"markdown","92a92a30":"markdown","a1618637":"markdown","bafb5157":"markdown","34bf2952":"markdown","e3a27567":"markdown","b928c484":"markdown","0b3c44d8":"markdown","9f8c27ab":"markdown","ded3d4b5":"markdown","750bb755":"markdown","4c368c16":"markdown","6e042255":"markdown","1cf5ea1d":"markdown","dffeafb6":"markdown","9c61fed3":"markdown","d77fe357":"markdown","3e0923a8":"markdown","5bd8b6b6":"markdown","dd16c947":"markdown","7d38123d":"markdown","1cb8f359":"markdown","3e0c4efd":"markdown","1d3f4d47":"markdown"},"source":{"0002744f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\nimport itertools\nfrom datetime import datetime\nfrom scipy import interp\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\n\npd.set_option('max_columns', 100)","b1e01122":"# Training data\ntrain = pd.read_csv('\/kaggle\/input\/risco-de-credito\/treino.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","2d9f1686":"# Test data\ntest = pd.read_csv('\/kaggle\/input\/risco-de-credito\/teste.csv')\nprint('Test data shape: ', test.shape)\ntest.head()","d36d3d63":"df_lgb_ = train.copy()\ntarget = train['inadimplente']\ndf_lgb = train.drop(['inadimplente'], axis=1)\ntrain_df = train.copy()\nx = train.copy()","97ae5b82":"x=train['inadimplente'].value_counts().values\nsns.barplot([0,1],x)\nplt.title('Target variable count')\n\nprint(\"There are {}% target values with 1\".format(100 * train['inadimplente'].value_counts()[1]\/train.shape[0]))","81b2b38a":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head(10)","da5709cb":"train.dtypes.value_counts()","1b07fb9c":"features = train.columns.values[1:11]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])\n","71d6e6e1":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(10))","7dd4b52c":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(10))","74f12505":"# Find correlations with the target and sort\ncorrelations = train.corr()['inadimplente'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(5))\nprint('\\nMost Negative Correlations:\\n', correlations.head(5))","9195e8a0":"corr_train = train.corr()\nplt.figure(figsize = (14, 10))\n# Heatmap of correlations\nsns.heatmap(corr_train, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","02a2c9ac":"plt.style.use('fivethirtyeight')\n# Plot the distribution of ages in years\nplt.hist(train['idade'], edgecolor = 'k')\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\nprint('min age {} max age {}'.format(train['idade'].min(), train['idade'].max()))\nprint('age <20 {}, age >99 {}'.format(len(train[train['idade']>99]),len(train[train['idade']<20])))","1a7729f2":"plt.figure(figsize = (10, 8))\n# KDE plot of loans that were repaid on time\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'idade'], label = 'target == 0')\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'idade'], label = 'target == 1')\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","45ff4b1b":"# Age information into a separate dataframe\nage_data = train[['inadimplente', 'idade']]\n# Bin the age data\nage_data['age_binned'] = pd.cut(age_data['idade'], bins = np.linspace(20, 60, num = 6))\nage_data.head(10)","0b65e60d":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('age_binned').mean()\nage_groups","fe688776":"plt.figure(figsize = (8, 8))\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['inadimplente'])\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","75280e56":"plt.figure(figsize = (10, 8))\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'numero_linhas_crdto_aberto'], label = 'target == 0')\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'numero_linhas_crdto_aberto'], label = 'target == 1')\nplt.xlabel('number of open credit lines'); plt.ylabel('Density'); plt.title('Distribution of number of open credit lines');","0a234c86":"plt.figure(figsize = (10, 8))\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'numero_emprestimos_imobiliarios'], label = 'target == 0')\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'numero_emprestimos_imobiliarios'], label = 'target == 1')\nplt.xlabel('number real estate loans'); plt.ylabel('Density'); plt.title('Distribution of number real estate loans');","ae0f1366":"def plot_dist_col(column, train, test):\n    '''plot dist curves for train and test  data for the given column name'''\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.distplot(train[column].dropna(), color='green', ax=ax).set_title(column, fontsize=10)\n    sns.distplot(test[column].dropna(), color='purple', ax=ax).set_title(column, fontsize=10)\n    plt.xlabel(column, fontsize=12)\n    plt.legend(['train', 'test'])\n    plt.show()","e1333fa4":"plot_dist_col('util_linhas_inseguras', train, test)    ","f86e0ef8":"plot_dist_col('idade', train, test) ","2542d28c":"test.dtypes","e0860f41":"plot_dist_col('razao_debito', train, test) ","751bf4ff":"\nplot_dist_col('salario_mensal', train, test)","e1024f3c":"plot_dist_col('numero_emprestimos_imobiliarios', train, test)","3c24b5f0":"plot_dist_col('salario_mensal', train, test)","0b6d1b50":"plot_dist_col('numero_de_dependentes', train, test) ","4b51d983":"train = train.drop(columns = ['inadimplente'])\n# Feature names\nfeatures = list(train.columns)\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","49f55016":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d0e4e6b4":"def auc_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Area Under ROC Curve (AUC)\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)\ndef plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n    \"\"\"\n    Plots the ROC Curve given predictions and labels\n    \"\"\"\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n    plt.figure(figsize=(8, 8))\n    plt.plot(fpr_train, tpr_train, color='black',\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n    plt.plot(fpr_val, tpr_val, color='darkorange',\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)\ndef plot_pre_curve(y_test,probs):\n    precision, recall, thresholds = precision_recall_curve(y_test, probs)\n    plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n    # plot the precision-recall curve for the model\n    plt.plot(recall, precision, marker='.')\n    plt.title(\"precision recall curve\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the plot\n    plt.show()","0314e924":"X_train, X_val, y_train, y_val = train_test_split(train, target,\n                                                  test_size=0.30, \n                                                  random_state=2020, \n                                                  stratify=target)","673b17e0":"# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n# Train on the training data\nlog_reg.fit(X_train, y_train)","2468afa8":"# Get score on training set and validation set for Logistic Regression\ntrain_preds = log_reg.predict_proba(X_train)[:, 1]\nval_preds = log_reg.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","37253172":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"Logistic Regression Baseline\")","180de094":"plot_pre_curve(y_val ,val_preds)","8b3af255":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","ebfb1565":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","70e518d7":"\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 2020, verbose = 1, n_jobs = -1)\n# Train on the training data\nrandom_forest.fit(X_train,y_train)\n    # Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Get score on training set and validation set for random forest\ntrain_preds = random_forest.predict_proba(X_train)[:, 1]\nval_preds = random_forest.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","04b43bef":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"Random Forest Baseline\")","2e7bb014":"plot_pre_curve(y_val ,val_preds)","dfa700da":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","8bdc0cef":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","68a4715e":"# Lgbm\nimport lightgbm as lgb","0a4ab0ed":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(df_lgb.shape[1])\ntrain_weight = 1-y_train.replace(y_train.value_counts()\/len(y_train))\npositive_weight = train_weight[y_train==1].values[0]\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 5000, class_weight = 'balanced', scale_pos_weight= positive_weight)\n    \n# Train using early stopping\nmodel.fit(X_train, y_train, early_stopping_rounds=100, eval_set = [(X_val, y_val)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\nfeature_importances += model.feature_importances_","865b705c":"# Get score on training set and validation set for random forest\ntrain_preds = model.predict_proba(X_train)[:, 1]\nval_preds = model.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","cf3e3099":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"LGBM Classifier\")","5f4a59f6":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","7db235d5":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","6813b2f7":"# Make sure to average feature importances! \nfeature_importances = feature_importances \/ 2\nfeature_importances = pd.DataFrame({'feature': list(df_lgb.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\nfeature_importances.head()","5f873293":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","872a9daa":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 10 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 15\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:10]))), \n            df['importance_normalized'].head(10), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:10]))))\n    ax.set_yticklabels(df['feature'].head(10))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","5418c8f6":"norm_feature_importances = plot_feature_importances(feature_importances)","0311bc4a":"import itertools\nfrom scipy import interp\ndef gradient_boosting_model(params, folds, test_df, model='LGB',stack = False):    \n    print(str(model)+' modeling...')\n    start_time = timer(None)\n    plt.rcParams[\"axes.grid\"] = True\n    nfold = folds\n    skf = StratifiedKFold(n_splits=nfold, shuffle=False, random_state=44400)\n\n    oof = np.zeros(len(train_df))\n    mean_fpr = np.linspace(0,1,100)\n    cms= []\n    tprs = []\n    aucs = []\n    y_real = []\n    y_proba = []\n    recalls = []\n    roc_aucs = []\n    f1_scores = []\n    accuracies = []\n    precisions = []\n    feature_importance_df = pd.DataFrame()\n    predictions = np.zeros(len(test_df))\n\n    i = 1\n    for train_idx, valid_idx in skf.split(train_df, train_df['inadimplente'].values):\n        print(\"\\nfold {}\".format(i))\n        \n        if model == 'LGB':\n        \n            trn_data = lgb.Dataset(train_df.iloc[train_idx][features].values,\n                                   label=train_df.iloc[train_idx]['inadimplente'].values\n                                   )\n            val_data = lgb.Dataset(train_df.iloc[valid_idx][features].values,\n                                   label=train_df.iloc[valid_idx]['inadimplente'].values\n                                   )   \n\n            clf = lgb.train(param_lgb, trn_data, num_boost_round=1000,  valid_sets = [trn_data, val_data], verbose_eval=800, early_stopping_rounds = 10000)\n            oof[valid_idx] = clf.predict(train_df.iloc[valid_idx][features].values) \n  \n            predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ skf.n_splits\n    \n        if model == 'XGB':\n\n            trn_data = xgb.DMatrix(train_df.iloc[train_idx][features], \n                                   label=train_df.iloc[train_idx]['inadimplente'].values)\n            val_data = xgb.DMatrix(train_df.iloc[valid_idx][features], \n                                   label=train_df.iloc[valid_idx]['inadimplente'].values)\n\n            watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n\n            clf = xgb.train(params, dtrain = trn_data, evals=watchlist, early_stopping_rounds=1000, maximize=True, verbose_eval=800)\n            oof[valid_idx] = clf.predict(val_data, ntree_limit=clf.best_ntree_limit)\n            \n            test_xgb = xgb.DMatrix(test_df[features])\n            predictions += clf.predict(test_xgb, ntree_limit=clf.best_ntree_limit) \/ skf.n_splits\n        \n        # Scores \n        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx]))\n        accuracies.append(accuracy_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        recalls.append(recall_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        precisions.append(precision_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        f1_scores.append(f1_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n\n        # Roc curve by folds\n        f = plt.figure(1)\n        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n\n        # Precion recall by folds\n        g = plt.figure(2)\n        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx])\n        y_real.append(train_df.iloc[valid_idx]['inadimplente'].values)\n        y_proba.append(oof[valid_idx])\n        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n\n        i= i+1\n        \n        # Confusion matrix by folds\n        cms.append(confusion_matrix(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        \n        # Features imp\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"Feature\"] = features\n        if model == 'LGB':\n            fold_importance_df[\"importance\"] = clf.feature_importance()\n        fold_importance_df[\"fold\"] = nfold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)    \n\n    # Metrics\n    print(\n            '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n            '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n            '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n            '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n            '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n    )\n    \n    # Roc plt\n    f = plt.figure(1)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(str(model)+' ROC curve by folds')\n    plt.legend(loc=\"lower right\")\n    \n    # PR plt\n    g = plt.figure(2)\n    plt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\n    y_real = np.concatenate(y_real)\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n    plt.plot(recall, precision, color='blue',\n             label=r'Mean P|R')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(str(model)+' P|R curve by folds')\n    plt.legend(loc=\"lower left\")\n\n    # Confusion maxtrix\n    plt.rcParams[\"axes.grid\"] = False\n    cm = np.average(cms, axis=0)\n    class_names = [0,1]\n    plt.figure()\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title= str(model).title()+' Confusion matrix [averaged\/folds]')\n    \n    # Feat imp plt\n    if model != 'XGB':\n        cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:30].index)\n        best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n        plt.figure(figsize=(10,10))\n        sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n                edgecolor=('white'), linewidth=2, palette=\"rocket\")\n        plt.title(str(model)+' Features importance (averaged\/folds)', fontsize=18)\n        plt.tight_layout()\n        \n    # Timer end    \n    timer(start_time)\n    \n    return predictions\n    \n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","72993d99":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nparam_xgb = {\n            'n_jobs' : -1, 'n_estimators' : 500, 'seed' : 4040,\n            'random_state':404, 'eval_metric':'auc' }\n# Test data\nsub_df = pd.read_csv('\/kaggle\/input\/risco-de-credito\/teste.csv')\npreds_xgb = gradient_boosting_model(param_xgb, 10, sub_df, 'XGB')\n\n","de1f474f":"param_lgb = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'class_weight':'unbalanced',\n    'scale_pos_weight':positive_weight,    \n    'objective': 'binary', \n    'verbosity': 1\n}\n\n# Test data\nsub_df = pd.read_csv('\/kaggle\/input\/risco-de-credito\/teste.csv')\n\npredictions = gradient_boosting_model(param_lgb, 10, sub_df, 'LGB')\nsub_df[\"inadimplente\"] = predictions.round()\nsub_df[\"inadimplente score\"] = predictions","716f0dca":"sub_df.to_csv(\"final_test.csv\", index=False)\nsub_df.head()","24cea38e":"def severity_validation(df):\n    df['defaulting-score'] = \"None\"\n    for i, row in df.iterrows():\n        if row['inadimplente'] <0.5:\n            df['defaulting-score'][i] = \"low-defaulting-score\"\n        elif row['inadimplente'] <=0.75:\n            df['defaulting-score'][i] = \"medium-defaulting-score\" \n        else:\n            df['defaulting-score'][i] = \"high-defaulting-score\" \n    return df\n\ncustomer_df= pd.DataFrame(predictions, columns=['inadimplente'])\n\ncustomer_score=severity_validation(customer_df)\ncustomer_score['inadimplente']\nq50, q25 = np.percentile(customer_score['inadimplente'], [50 ,25])\nq75, q25 = np.percentile(customer_score['inadimplente'], [75 ,25])\nq100, q75 = np.percentile(customer_score['inadimplente'], [100 ,75])\n\niqr_50 = q50 - q25\niqr_75 = q75 - q50\niqr_100 = q100 - q75\niqr_75_25 = q75 - q25","f2bdbde4":"customer_score['inadimplente result'] = predictions.round()\nprint(\"minimum defaulting prob \",customer_score[customer_score['defaulting-score']=='high-defaulting-score']['inadimplente'].min())\nprint(\"maximum defaulting prob \",customer_score[customer_score['defaulting-score']=='high-defaulting-score']['inadimplente'].max())\n\ncustomer_score[customer_score['defaulting-score']=='high-defaulting-score'].head(10)","1b0951ef":"# Install library \n!pip install factor_analyzer\n# import factor analyzer library\nfrom factor_analyzer import FactorAnalyzer","e423c13f":"fa = FactorAnalyzer()\nfa.fit(sub_df, 10)\n\nev, v = fa.get_eigenvalues()\n\n# Create scree plot using matplotlib\nplt.figure(figsize=(25,10))\nplt.scatter(range(1,sub_df.shape[1]+1),ev)\nplt.plot(range(1,sub_df.shape[1]+1),ev)\nplt.hlines(1, 0, sub_df.shape[1], colors='r')\nplt.title('Scree Plot')\nplt.xlabel('Factors')\nplt.ylabel('Eigenvalue')\nplt.grid()\nplt.show()","1bc4d1b7":"# Perform Factor Analysis\nfa = FactorAnalyzer(n_factors=3, rotation='varimax')\nfa.fit(sub_df)\nloads = fa.loadings_\nloads = pd.DataFrame(loads, index=sub_df.T.index)","93464b05":"#Heatmap of loadings\nplt.figure(figsize=(30,25))\nsns.heatmap(loads, annot=True, cmap=\"YlGnBu\")","d85b90cd":"test","9fa1eac5":"def score_factor1(df, df_factor_analysis, target, name_target):\n    df['score_factor1_target'] = df[df['inadimplente']==target]['vezes_passou_de_30_59_dias'] * df_factor_analysis.T['vezes_passou_de_30_59_dias'][0] \\\n    + df[df[name_target]==target]['numero_vezes_passou_90_dias'] * df_factor_analysis.T['numero_vezes_passou_90_dias'][0] \\\n    + df[df[name_target]==target]['numero_de_vezes_que_passou_60_89_dias'] * df_factor_analysis.T['numero_de_vezes_que_passou_60_89_dias'][0]        \n\nscore_factor1(sub_df, loads, 0, 'inadimplente')       \nscore_factor1(sub_df, loads, 1, 'inadimplente')\n","bd93265e":"def score_factor2(df, df_factor_analysis, target, name_target):\n    df['score_factor2_target'] = df[df['inadimplente']==target]['numero_linhas_crdto_aberto'] * df_factor_analysis.T['numero_linhas_crdto_aberto'][1] \\\n    + df[df[name_target]==target]['numero_emprestimos_imobiliarios'] * df_factor_analysis.T['numero_emprestimos_imobiliarios'][1]  \n\nscore_factor2(sub_df, loads, 0, 'inadimplente')       \nscore_factor2(sub_df, loads, 1, 'inadimplente')\n","f30545ef":"def score_factor3(df, df_factor_analysis, target, name_target):\n    df['score_factor3_target'] = df[df['inadimplente']==target]['idade'] * df_factor_analysis.T['idade'][2] \\\n    + df[df[name_target]==target]['numero_de_dependentes'] * df_factor_analysis.T['numero_de_dependentes'][2]   \n\nscore_factor3(sub_df, loads, 0, 'inadimplente')       \nscore_factor3(sub_df, loads, 1, 'inadimplente')\n","460c458c":"sub_df","ae205f85":"sub_df.columns","b1979df7":"sub_df[['score_factor1_target', 'inadimplente score' ]].sort_values(by=['score_factor1_target', 'inadimplente score'],ascending=False).head(10)","d00a088a":"sub_df[['score_factor2_target', 'inadimplente score' ]].sort_values(by=['score_factor2_target', 'inadimplente score'],ascending=False).head(10)","fbea1d71":"sub_df[['score_factor3_target', 'inadimplente score' ]].sort_values(by=['score_factor3_target', 'inadimplente score'],ascending=False).head(10)","dd7c9cff":"### Factors \nThe factors represents high correlated variables. We just considered factor loadings >35 in each factor. \nLet's analyze defaulting customers and group by the highest scores(factor loadings) for each factor.\n\n- factor 1 (**customer delaying payment.**) - vezes_passou_de_30_59_dias, numero_vezes_passou_90_dias, numero_de_vezes_que_passou_60_89_dias.\n- factor 2 (**customer with many open loans**) - numero_linhas_crdto_aberto,numero_emprestimos_imobiliarios.\n- factor 3 (**young customer with few dependents**) - idade, numero_de_dependentes.\n","32fb2227":"We have some inconsisent values. age equal 0 for example. We need drop these rows.","f506a226":"# Feature Selection","5587fffd":"3 factors","431c8e94":"Now we have how to identify high-defaulting customers, the customer can be assigned a \"defaulting-score\" based on the predicted label such that:\n\n- Low-defaulting-score for Customers with label < 0.50\n- Medium-defaulting-score Score for Customers with label between 0.5 and 0.75\n- High-defaulting-score Score for Customers with label > 0.75","87a0df49":"### Correlations\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n- .00-.19 \u201cvery weak\u201d\n- .20-.39 \u201cweak\u201d\n- .40-.59 \u201cmoderate\u201d\n- .60-.79 \u201cstrong\u201d\n- .80-1.0 \u201cvery strong\u201d","6c7ea626":"# Modeling","f2122235":"### Train Test Distribution Analysis","7c70c326":"#### Classification report train","c4f47592":"### Defaulting-Score for customers","4d717e1c":"The data is unbalanced with respect with target value.","0492ad32":"#### Creation final test dataset","8282cacb":"# Exploratory Data Analysis","76df45cc":"# End Notebook","2a2de0ac":"### Improved Model: Random Forest","19c300cc":"customers with high 'inadimplente score' and high score for the Factor 3.","3c1645b7":"### Loading Required libraries","aa3e158a":"```numero_vezes_passou_90_dias```, ```vezes_passou_de_30_59_dias```, ````numero_de_vezes_que_passou_60_89_dias``` are very correlated. expected correlation ;)\n\n```numero_emprestimos_imobiliarios``` has medium correlation with ```salario_mensal:``` indicate that people with a high salary have more loans.","3ed7db5d":"### Column Types","a031aa6b":"more significant correlations: the **vezes_passou_de_30_59_dias** is the most positive correlation;\n","184a4155":"**razao_debito** and **util_linhas_inseguras** are the most important features for our model !","fb27211b":"#### Classification report train","12c668f8":"Acknowledgments\n\nLigthGBM Simple fe by [@caesarlupum](https:\/\/www.kaggle.com\/caesarlupum\/ashrae-ligthgbm-simple-fe), Brazil against the advance of Covid-19 by [@caesarlupum](https:\/\/www.kaggle.com\/caesarlupum\/brazil-against-the-advance-of-covid-19), eda and prediction by [@gpreda Introduction](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction).","92a92a30":"##### Evaluation Logistic Regression ROC_AUC","a1618637":"# Baseline","bafb5157":"Same columns in train and test set have very close number of duplicates of same or very close values. \nThis is an interesting pattern that we might be able to use in the future","34bf2952":"customers with high 'inadimplente score' and high score for the Factor 2.","e3a27567":"##### Age informative plots","b928c484":"customers with high 'inadimplente score' and high score for the Factor 1.","0b3c44d8":"### Logistic Regression","9f8c27ab":"missing values: salario_mensal\t```19.78%``` and numero_de_dependentes ```2.61%```","ded3d4b5":"### Factory Analysis in defaulting customers\n","750bb755":"we not have features that have zero importance. Nice !\n","4c368c16":"# Predictive model for credit approval\nCredit score models calculate the probability of default and are one of the most main tools used by several companies to approve or deny credit.\n\nDescription:\n\nEach row represents a customer and the columns represent the data (information) for those customers.\nThe response variable is the defaulting column, which indicates whether the customer has become defaulting (1) or not (0).\nThe variables are described below:\n\n\n- ```\u00ecdade```: The age of the customer\n- ```numero_de_dependentes```: The number of people dependent on the customer.\n- ```salario_mensal```: Monthly salary of the client.\n- ```numero_emprestimos_imobiliarios```: Number of real estate loans that the customer has open.\n- ```numero_vezes_passou_90_dias```: Number of times the policyholder spent more 90 days overdue.\n- ```util_linhas_inseguras```: How much the customer is using in relation to their credit limit, on lines that are not secured by personal assets, such as real estate and cars.\n- ```vezes_passou_de_30_59_dias```:  Number of times the customer delayed the payment of a loan, (between 30 and 59 days).\n- ```razao_debito```: Ratio between debts and the borrower's equity. debt ratio = Debts \/ Equity\n- ```numero_linhas_crdto_aberto```: Number of loans outstanding by the customer.\n- ```number_of_ numero_de_vezes_que_passou_60_89_dias```: Number of times the customer delayed the payment of a loan, (between 60 and 89 days).\n","6e042255":"classification report","1cf5ea1d":"Find the features with zero importance","dffeafb6":"### Duplicate values\nLet's now check how many duplicate values exists per columns.","9c61fed3":"In general we have similar distribution in dataset, but **util_linhas_inseguras**, **numero_emprestimos_imobiliarios**,**salario_mensal**  have higher values in test data.","d77fe357":"### Examine the Distribution of the Target Column","3e0923a8":"## Gradient Boosting Model function","5bd8b6b6":"This is great, the model is accurate! just 5 FP of 71863, 48 FN of 5084.","dd16c947":"numeric variables 7 ```int64``` and 4 ```float64``` (which can be either discrete or continuous).","7d38123d":"Average feature importances!","1cb8f359":"### Checking missing data in train\n\nnumber and percentage of missing values in each column.","3e0c4efd":"#### Average failure to repay loans by age bracket.","1d3f4d47":"There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest two age groups 20-28, 28-36.\n\n\nThis is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."}}