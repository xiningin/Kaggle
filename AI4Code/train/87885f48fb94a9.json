{"cell_type":{"968e7455":"code","667e5a99":"code","555ffde6":"code","cc83dc96":"code","a2a11dd0":"code","445c81fd":"code","83ddec30":"code","ef9b3ab1":"code","3c42a5b0":"code","3b8be753":"code","d35f12d2":"code","b58baf3c":"code","6b833202":"code","faacc736":"code","05abd9fe":"code","048b33f3":"code","88bb8e12":"code","d73e2237":"code","36e521be":"code","ab2db6e0":"code","0abb20d8":"code","b0435405":"code","cfc85ccc":"code","f81c2813":"code","43a912ff":"code","d55d63fc":"code","cebba4ee":"code","f15f85c7":"code","113a1fdb":"markdown","ed3770d4":"markdown","de6d3300":"markdown","ba7214a6":"markdown","a23daa07":"markdown","1a7a8b9a":"markdown","34b8c472":"markdown","21c65f91":"markdown","fcff47bf":"markdown","8bc5fded":"markdown","ff54446c":"markdown","0dbd3162":"markdown","934e22cd":"markdown","aff4aba8":"markdown","8f249f40":"markdown","b540a62a":"markdown","d951e8c7":"markdown"},"source":{"968e7455":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","667e5a99":"import matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm, tree\nprint ('Setup Complete')","555ffde6":"# load the data\nlabeled_images = pd.read_csv('..\/input\/train.csv')\nimages = labeled_images.iloc[0:5000,1:]\nlabels = labeled_images.iloc[0:5000,:1]\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\nprint(images)","cc83dc96":"# now we gonna load the second image, reshape it as matrix than display it\ni=9\nimg=train_images.iloc[i].values\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train_labels.iloc[i,0])\nplt.show\nprint(train_labels.head(123))","a2a11dd0":"index = [20, 5, 121, 2, 42, 9, 44, 0, 120, 7]\n\nfor l in index:\n    plt.figure(l)\n    img=train_images.iloc[l].values\n    img=img.reshape((28,28))\n    plt.imshow(img,cmap='gray')\n    plt.title(train_labels.iloc[l,0])","445c81fd":"i=4\ntrain_images.iloc[i].describe()\nprint(type(train_images.iloc[i]))\nplt.hist(train_images.iloc[i])\nplt.title(i)\nplt.xlabel(\"long of pixels\")\nplt.ylabel(\"number of columns\")","83ddec30":"data0 = train_images.iloc[20]\nplt.hist(data0)","ef9b3ab1":"data1 = train_images.iloc[5]\nplt.hist(data1)","3c42a5b0":"data2 = train_images.iloc[121]\nplt.hist(data2)","3b8be753":"data3 = train_images.iloc[2]\nplt.hist(data3)","d35f12d2":"data4 = train_images.iloc[42]\nplt.hist(data4)","b58baf3c":"data5 = train_images.iloc[9]\nplt.hist(data5)","6b833202":"data6 = train_images.iloc[44]\nplt.hist(data6)","faacc736":"data7 = train_images.iloc[0]\nplt.hist(data7)","05abd9fe":"data8 = train_images.iloc[120]\nplt.hist(data8)","048b33f3":"data9 = train_images.iloc[7]\nplt.hist(data9)","88bb8e12":"data = np.append(train_images.iloc[index[0]], (train_images.iloc[index[1:]]))\nplt.figure()\nplt.hist(data)\nplt.show()\nplt.close()","d73e2237":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","36e521be":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(random_state = 0)\ntree.fit(train_images, train_labels)\ntest_predict = tree.predict(test_images)\nprint(mean_absolute_error(test_labels, test_predict))","ab2db6e0":"print(train_labels.values.ravel())\nprint(np.unique(test_labels)) # to see class number","0abb20d8":"test_images[test_images>0]=1\ntrain_images[train_images>0]=1\n\nimg=train_images.iloc[i].values.reshape((28,28))\nplt.imshow(img,cmap='binary')\nplt.title(train_labels.iloc[i])","b0435405":"# now plot again the histogram\nplt.hist(train_images.iloc[i])","cfc85ccc":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","f81c2813":"# Test again to data test\ntest_data=pd.read_csv('..\/input\/test.csv')\ntest_data[test_data>0]=1\nresults=clf.predict(test_data[0:5000])\nprint('result complete')","43a912ff":"# separate code section to view the results\nprint(results)\nprint(len(results))","d55d63fc":"# dump the results to 'results.csv'\ndf = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","cebba4ee":"#check if the file created successfully\nprint(os.listdir(\".\"))","f15f85c7":"# from https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","113a1fdb":"# Data Download\nWe have the file, can listed it but how we are take it from sever. Thus we also need to code the download link.","ed3770d4":"that histogram means if the pixel 0 is more then 700 in index 4 or pixel 100 is less than 100 in index 4 etc.","de6d3300":"Now plot the histogram within img","ba7214a6":"# Q5\nBased on this finding, Can you explain why if the value is capped into [0,1] it improve significantly?. Perharps you need to do several self designed test to see why.","a23daa07":"show the number of the data on the label","1a7a8b9a":"\n# Q2\nNow plot an image for each image class","34b8c472":"# Q6\nAlhamdulillah, we have completed our experiment. Here's things to do for your next task:\n\nWhat are the overfitting factor of SVM algorithm?. Previously on decision tree regression, the factor was max_leaf nodes. Do similar expriment using SVM!\nApply Decision Tree Classifier on this dataset, seek the best overfitting factor, then compare it with results of SVM.\nApply Decision Tree Regressor on this dataset, seek the best overfitting factor, then compare it with results of SVM & Decision Tree Classifier. Provides the results in table\/chart. I suspect they are basically the same thing.\nApply Decision Tree Classifier on the same dataset, use the best overfitting factor & value. But do not use unnormalized dataset, before the value normalized to [0,1]","21c65f91":"#### Prediction labelling\nIn Kaggle competition, we don't usually submit the end test data performance on Kaggle. But what to be submitted is CSV of the prediction label stored in a file.","fcff47bf":"#### Retrain the model\nUsing the now adjusted data, let's retrain our model to see the improvement","8bc5fded":"# Q3\nCan you check in what class does this histogram represent?. How many class are there in total for this digit data?. How about the histogram for other classes","ff54446c":"# Load Data","0dbd3162":"# Import Library:\n","934e22cd":"### Improving Performance\nDid you noticed, that the performance is so miniscule in range of ~0.1. Before doing any improvement, we need to analyze what are causes of the problem?. But allow me to reveal one such factor. It was due to pixel length in [0, 255]. Let's see if we capped it into [0,1] how the performance are going to improved.","aff4aba8":"# Q4\nIn above, did you see score() function?, open SVM.score() dokumentation at SKLearn, what does it's role?. Does it the same as MAE discussed in class previously?. Ascertain it through running the MAE. Now does score() and mae() prooduce the same results?.","8f249f40":"## Train the model\nNow we are ready to train the model, for starter let's use SVM. For the learning most model in SKLearn adopt the usual fit() and predict().","b540a62a":"# Q1\n  Notice in the above we used _images.iloc?, can you confirm on the documentation? what is the role?","d951e8c7":".iloc is use for select rows or columns by its index, which is start from 0  "}}