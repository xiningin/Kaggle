{"cell_type":{"68a3ce50":"code","b9ee8f3a":"code","f584f32e":"code","c9260d8a":"code","d8fff976":"code","f3d2d913":"code","84e0daa6":"code","c8fe181f":"code","09341d25":"code","e7ce84b3":"code","2121a0a2":"code","dc325829":"code","02ffef5a":"code","eb9246e7":"code","b85b8ad1":"code","20e2d8a4":"code","2256dd7d":"code","d11b717d":"code","75bd5676":"code","4624967d":"code","1d9abef7":"code","93f937c1":"code","5b43e281":"code","a53db81a":"code","e1843b01":"code","e9d44ad5":"code","8f1beacf":"code","d8e5c650":"code","59ce1d85":"code","eab9e795":"code","c5e64969":"code","1cd21918":"code","ee4f451e":"code","19b5084a":"code","879fdc7e":"code","21414cd3":"code","5fa5c226":"code","ecdb0315":"code","ec445ac3":"code","b8dc412d":"code","bcd6d3d8":"code","8aa539de":"code","2899d4f8":"code","a54a519f":"code","8e8d41fc":"code","5a04e458":"code","f116e58d":"code","84ff6569":"code","7a542c77":"code","c25ec727":"code","def313d0":"code","cde1f4dd":"code","b2f72284":"code","b7a63e4a":"code","6167c320":"code","4086b667":"code","655213f3":"code","d994f1bd":"code","afdac06d":"code","040e00a9":"code","caaf94cd":"code","64145f82":"code","31f1a6ad":"code","3df5df1a":"code","2c86bb7b":"code","205ec705":"markdown","1debfbb1":"markdown","1935f56d":"markdown","fcb54884":"markdown","c933b8ef":"markdown","b0c9d962":"markdown","88213811":"markdown","52f37afb":"markdown","7232db0d":"markdown","673a0f41":"markdown","4c63111a":"markdown","2f5e4fe5":"markdown","50aa3138":"markdown","53f4337c":"markdown","0b4a61ab":"markdown","16767138":"markdown","3f96d3ce":"markdown","f41474e0":"markdown","221d68bd":"markdown","4acc0dde":"markdown","2099c56d":"markdown","bba2c2d5":"markdown","600298d4":"markdown","d625d741":"markdown","3da7c053":"markdown","ce1e2a13":"markdown"},"source":{"68a3ce50":"import numpy as np         # package for numerical computations\nimport pandas as pd        # package for data analysis and management\n\n# libraries for data visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# for preprocessing, ML models and evaluation\nfrom sklearn.preprocessing import StandardScaler           # for standardizing the data\nfrom sklearn.model_selection import train_test_split       # to split the data into train and test data set\nfrom sklearn.feature_selection import SelectFromModel      # to select the best features from the model\n\nfrom sklearn.metrics import accuracy_score,mean_absolute_error,mean_squared_error #evaluation metrics for regression model\n\nfrom sklearn.tree import DecisionTreeRegressor             # decision tree regression model\nfrom sklearn.linear_model import LinearRegression          #linear regression model\nfrom sklearn.ensemble import RandomForestRegressor         # random forest regression model\nimport xgboost as xgb                                      #xgboost model\nfrom xgboost import plot_importance                        # to plot the feature importance plot from xgboost model\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV # to select the best params for the model\n\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","b9ee8f3a":"cancer_train_data  = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Training_set_label.csv\" )\ncancer_test_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Testing_set_label.csv')","f584f32e":"pd.set_option('display.max_columns',45) \n#to display the top 5 rows\ncancer_train_data.head()","c9260d8a":"cancer_test_data.head()","d8fff976":"cancer_train_data.info()","f3d2d913":"cancer_test_data.info()","84e0daa6":"# descriptive statistics\ncancer_train_data.describe()","c8fe181f":"cancer_test_data.describe()","09341d25":"#checking whether the data contains any duplicate records\ndup_data = cancer_train_data[cancer_train_data.duplicated()] ","e7ce84b3":"dup_data.shape","2121a0a2":"#dropping the duplicates from train dataset\ncancer_train_data.drop_duplicates(inplace=True)","dc325829":"cancer_train_data.shape","02ffef5a":"#dropping this column as it has more null values\ncancer_train_data.drop(columns=['PctSomeCol18_24'],axis=1,inplace=True)\ncancer_test_data.drop(columns=['PctSomeCol18_24'],axis=1,inplace=True)","eb9246e7":"#checking the value counts of two categorical columns\nfor i in cancer_train_data.select_dtypes(include=object):\n    print(cancer_train_data[i].value_counts(ascending=False))\n    print(\"-------------------------------------------------------------------------------\")","b85b8ad1":"#dropping Geography column as it as more only one count in each unique value\n#dropping the binnedInc column as we already have Medianincome of a country column\ncancer_train_data.drop(columns=['binnedInc', 'Geography'],axis=1,inplace=True)\ncancer_test_data.drop(columns=['binnedInc', 'Geography'],axis=1,inplace=True)","20e2d8a4":"fig, axes = plt.subplots(6, 5, figsize=(18, 14))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(cancer_train_data.columns[:-1]):\n  plot = sns.distplot(cancer_train_data[c] ,ax=axes[i])\nplt.tight_layout()","2256dd7d":"fig, axes = plt.subplots(10, 3, figsize=(20, 40))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(cancer_train_data.columns[:-1]):\n  plot = sns.scatterplot(x=\"TARGET_deathRate\",y=c,data=cancer_train_data,ax=axes[i])\nplt.tight_layout()","d11b717d":"# sns.pairplot(cancer_train_data);","75bd5676":"f,ax=plt.subplots(figsize=(25,25));\nsns.heatmap(cancer_train_data.corr(),annot=True);","4624967d":"fig, axes = plt.subplots(6, 5, figsize=(18, 14))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(cancer_train_data.columns[:-1]):\n  plot = sns.boxplot(data=cancer_train_data, x=c, ax=axes[i])\nplt.tight_layout()","1d9abef7":"X=cancer_train_data.drop('TARGET_deathRate',axis=1) #dropping the target variable \ny=cancer_train_data['TARGET_deathRate'] # target variable","93f937c1":"# splitting the train and test dataset with the test_size of 0.1 and random_state=21\nX_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.1,random_state=21)","5b43e281":"#filling the null values in these respective columns with the mean values\nX_train['PctPrivateCoverageAlone'].fillna(X_train['PctPrivateCoverageAlone'].mean(),inplace=True)\nX_train['PctEmployed16_Over'].fillna(X_train['PctEmployed16_Over'].mean(),inplace=True)\nX_test['PctPrivateCoverageAlone'].fillna(X_test['PctPrivateCoverageAlone'].mean(),inplace=True)\nX_test['PctEmployed16_Over'].fillna(X_test['PctEmployed16_Over'].mean(),inplace=True)","a53db81a":"X_train.isnull().sum() #no null values in all the columns","e1843b01":"# finding all the outliers in all the columns and replacing the same with the upper_whisker value of each column\nfor i in X_train.select_dtypes(include=['float64','int64']):\n    q1 = X_train[i].quantile(0.25)                 \n    q3 = X_train[i].quantile(0.75)\n    iqr = q3 - q1\n    whisker_width = 1.5\n    lower_whisker = q1 - whisker_width*iqr\n    upper_whisker = q3 + whisker_width*iqr\n    outlier_train = X_train[(X_train[i] < q1 - whisker_width*iqr) | (X_train[i] > q3 + whisker_width*iqr)]\n    X_train.loc[X_train[i]>upper_whisker,i] = upper_whisker\n","e9d44ad5":"# finding all the outliers in all the columns and replacing the same with the upper_whisker value of each column\nfor i in X_test.select_dtypes(include=['float64','int64']):\n    q1 = X_test[i].quantile(0.25)                 \n    q3 = X_test[i].quantile(0.75)\n    iqr = q3 - q1\n    whisker_width = 1.5\n    lower_whisker = q1 - whisker_width*iqr\n    upper_whisker = q3 + whisker_width*iqr\n    outlier_test = X_test[(X_test[i] < q1 - whisker_width*iqr) | (X_test[i] > q3 + whisker_width*iqr)]\n    X_test.loc[X_test[i]>upper_whisker,i] = upper_whisker\n\n ","8f1beacf":"# finding all the outliers in all the columns and replacing the same with the upper_whisker value of each column\nfor i in cancer_test_data.select_dtypes(include=['float64','int64']):\n    q1 = cancer_test_data[i].quantile(0.25)                 \n    q3 = cancer_test_data[i].quantile(0.75)\n    iqr = q3 - q1\n    whisker_width = 1.5\n    lower_whisker = q1 - whisker_width*iqr\n    upper_whisker = q3 + whisker_width*iqr\n    outlier_test = cancer_test_data[(cancer_test_data[i] < q1 - whisker_width*iqr) | (cancer_test_data[i] > q3 + whisker_width*iqr)]\n    cancer_test_data.loc[cancer_test_data[i]>upper_whisker,i] = upper_whisker","d8e5c650":"#there are no outliers in the data now\nfig, axes = plt.subplots(6, 5, figsize=(18, 14))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(X_train.columns):\n  plot = sns.boxplot(data=X_train, x=c, ax=axes[i])\nplt.tight_layout()","59ce1d85":"#Building the baseline model using the linearRegression algorithm\nlr=LinearRegression()\nlr.fit(X_train,Y_train) #training the model\nprediction=lr.predict(X_test) #predicting the target variable for the unseen data\n","eab9e795":"#evaluating the model with the following metrics\nmse=mean_squared_error(prediction,Y_test)          #mean squared error\nmae=mean_absolute_error(prediction,Y_test)         #mean absolute error\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))#root mean squared error","c5e64969":"#creating the dataframe with these error metrics values\nlrmodel=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['LinearRegression'])","1cd21918":"dt=DecisionTreeRegressor() #decisiontree regression algorithm with the default parameters\ndt.fit(X_train,Y_train)\nprediction=dt.predict(X_test)","ee4f451e":"mse=mean_squared_error(prediction,Y_test)\nmae=mean_absolute_error(prediction,Y_test)\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))","19b5084a":"dtmodel=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['DecisionTree'])\n","879fdc7e":"rf=RandomForestRegressor() #Random Forest Regression algorithm with the default parameters\nrf.fit(X_train,Y_train)\nprediction=rf.predict(X_test)","21414cd3":"mse=mean_squared_error(prediction,Y_test)\nmae=mean_absolute_error(prediction,Y_test)\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))","5fa5c226":"rfmodel=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['RandomForest'])\n","ecdb0315":"xg=xgb.XGBRegressor() #Xgboost regression algorithm with the default parameters\nxg.fit(X_train,Y_train)\nprediction=xg.predict(X_test)","ec445ac3":"mse=mean_squared_error(prediction,Y_test)\nmae=mean_absolute_error(prediction,Y_test)\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))","b8dc412d":"xgmodel=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['XGBoost'])\n","bcd6d3d8":"models = pd.concat([lrmodel, dtmodel,rfmodel,xgmodel]) \n","8aa539de":"models","2899d4f8":"X_train.shape","a54a519f":"X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.1,random_state=21)","8e8d41fc":"xgb1 = xgb.XGBRegressor()\nparameters = {\n    'n_estimators': [80, 90, 100, 125, 150,200,1000,2000,2050],\n    'max_depth': [2,3,4,5,8,16,None],\n    'learning_rate': [0.15, 0.1, 0.3, 0.5]\n}\ncv = RandomizedSearchCV(xgb1, parameters, cv=5)\ncv.fit(X_train, Y_train)","5a04e458":"cv.best_params_","f116e58d":"xgb2 = xgb.XGBRegressor()\nparameters = {\n    'n_estimators': [1000,3000,3500],\n    'max_depth': [2,3,None],\n    'learning_rate': [0.15,0.155,0.159]\n}\ncv = GridSearchCV(xgb2, parameters, cv=5)\ncv.fit(X_train, Y_train)","84ff6569":"cv.best_params_","7a542c77":"xg=xgb.XGBRegressor(n_estimators=3500,max_depth=2,learning_rate=0.159,n_jobs=-1,random_state=42)\nxg.fit(X_train,Y_train)\nprediction=xg.predict(X_test)","c25ec727":"mse=mean_squared_error(prediction,Y_test)\nmae=mean_absolute_error(prediction,Y_test)\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))","def313d0":"xgmodel_after_tuning=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['Tuned_XGBoost'])\n","cde1f4dd":"models = pd.concat([lrmodel, dtmodel,rfmodel,xgmodel,xgmodel_after_tuning]) \n","b2f72284":"models # as we can see that, this model has improved further by tuning it","b7a63e4a":"f,ax=plt.subplots(figsize=(9,9));\n#it shows the feature score value from the xgboost model\nplot_importance(xg,ax=ax);\nplt.show()","6167c320":"#to see the feature and its importance value\nfeat_labels=X_train.columns.to_list()\nfor feature in zip(feat_labels, xg.feature_importances_):\n    print(feature)","4086b667":"#selecting only the features which is greater than or equal to that threshold value\nselection = SelectFromModel(xg,threshold=0.0134)\nselection.fit(X_train, Y_train)\n\n# Transform the train and test features\nselect_X_train = selection.transform(X_train)\nselect_X_test = selection.transform(X_test) \n\n# train model\nselection_model = xgb.XGBRegressor(n_estimators=3500,max_depth=2,learning_rate=0.15,n_jobs=-1,random_state=21)\nselection_model.fit(select_X_train, Y_train)","655213f3":"final_features=[]\nfor feature_list_index in selection.get_support(indices=True):\n    final_features.append(feat_labels[feature_list_index])","d994f1bd":"#final features after the transformation\n#There are around 19 features\nfor feature in zip(final_features, selection_model.feature_importances_):\n    print(feature)","afdac06d":"prediction=selection_model.predict(select_X_test) #predicting the model with the selected features\nmean_squared_error(prediction,Y_test) ","040e00a9":"mse=mean_squared_error(prediction,Y_test)\nmae=mean_absolute_error(prediction,Y_test)\nrmse=np.sqrt(mean_squared_error(prediction,Y_test))","caaf94cd":"xgmodel_after_feature_selected=pd.DataFrame({\"mse\":mse,\"mae\":mae,\"rmse\":rmse},index=['Feature_Selected_XGBoost'])","64145f82":"models = pd.concat([lrmodel, dtmodel,rfmodel,xgmodel,xgmodel_after_tuning,xgmodel_after_feature_selected]) ","31f1a6ad":"models","3df5df1a":"#transform the test features and predict the target\nselect_test = selection.transform(cancer_test_data)\npred_test=selection_model.predict(select_test)","2c86bb7b":"#creating the dataframe with the predicted value of the test dataset \ndf=pd.DataFrame({\"prediction\":pred_test})\ndf.to_csv(\"xg5.csv\",index=False) #converting the dataframe to csv for submission\n","205ec705":"## Task 5\n### Feature Selection - XGBoost\n- Selecting only the important features to improve the model performance further","1debfbb1":"- We can clearly see some of the inferences from these scatterplot\n    - incident rate, povertyPercent, PctHS25_over, PctHS18_24, PctUnemployed16_Over, PctPublicCoverage, PctPublicCoverageAlone are positively correlated with the target variable\n    - medianIncome, PercentMarried, PctEmpPrivCoverage,PctPrivateCoverage, PctPrivateCoverageAlone, PctBatchDeg25_Over,\n      PctEmplyed16_Over, pctMarriedHousehold are negatively correlated with the target variable\n    - Seems like avgAnnCount, avgDeathPerYear, IncidentRate, MedianAge,studyPerCap are having outliers with respect to the target variable","1935f56d":"## Task 4\n### Hyperparameter Tuning\n\n- From the above models, XGBRegressor gave the less mean squared error value.\n- So, choosing this model to fine-tune further to achieve even more lesser mse value.\n- Choosing the main three parameters of XGBoost Regressor:\n    - n_estimators\n    - max_depth\n    - learning_rate\n- First, finding the best parameters value using RandomizedSearchCV as it takes less time to search and give the best values. Using that threshold values, creating another set of parameters and finding out the best from that using GridSearchCV","fcb54884":"### Model Evaluation","c933b8ef":"### Loading the data and displaying the first 5 rows","b0c9d962":"- As we inferred from the scatter plots, this heatmap shows that:\n    - incident rate, povertyPercent, PctHS25_over, PctHS18_24, PctUnemployed16_Over, PctPublicCoverage, PctPublicCoverageAlone, are highly positively correlated with the target variable\n    - Added to that, PctBlack is also positively correlated with the target variable\n    - medianIncome, PercentMarried, PctBatchDeg25_Over,PctEmpPrivCoverage, PctPrivateCoverage, PctPrivateCoverageAlone, PctEmplyed16_Over, pctMarriedHousehold are negatively correlated with the target variable\n    - Added to that PctBatchDeg18_24,PctWhite, PctAsian, PctOtherRace are also negatively correlated with the target variable","88213811":"### Split the data into Train and Test Sets","52f37afb":"#### ii) Decision Tree Model","7232db0d":"### SUMMARY OF THE ABOVE MODELS","673a0f41":"- There are 34 columns including the target variable. \n- There are null values in the three columns (PctSomeCol18_24, PctEmployed16_Over, PctPrivateCoverageAlone).\n- There are two columns of type Object (Geography,binnedInc)  and rest columns of type int64 or float64.","4c63111a":"#### iv) XGBoost Model","2f5e4fe5":"- As we suspected from the scatter plot, there are some outliers in these columnss\n    - avgAnnCount, avgDeathPerYear, IncidentRate, MedianAge,studyPerCap ","50aa3138":"- Most of the features follow the normal distribution\n- PctWhite feature is left skewed\n- PctBlack, PctAsian, PctOtherRace, AvgAnnount, avgDeathsPerYear, popEst2015, studyPerCap, MedianAge features are right skewed","53f4337c":"#### iii) RandomForest Model","0b4a61ab":"## Cancer Death Rate Prediction\n### Agenda\n- Problem Statement\n    - Objective\n    - Dataset & Data Description\n    \n- Solution Steps:\n    - Task 1\n        - Import Libraries\n        - Load data\n        - Understand your data: Data Analysis and Visualizations (EDA)\n        - Separate input and target features of the data\n        - Prepare train and test datasets\n    - Task 2\n        - Pre-process data\n    - Task 3 \n        - Choose any regression algorithm and build model\n        - Train & Evaluate model \n        - Try with different machine learning models and evaluate them\n    - Task 4\n        - Hyperparameter tuning for the best model obtained from task 3\n        - Optimize it with the best parameters\n    - Task 5\n        - Feature Selection\n        - Train and Evaluate the model with the selected features\/\n        \n    - Load the new test data\n    - Fill missing values if any\n    - Preprocessing and cleaning the data\n    - Predict the target values\n- Conclusion\n","16767138":"### Other Models","3f96d3ce":"- We can see that the model has improved further with the less mse value\n- Load the test data (name it as test_data). You can load the data using the below command.\n\n    - test_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Testing_set_label.csv')\n- Loading the data, filling the missing values and replacing the outliers with the upper whisker values are done in the previous steps in the respective tasks","f41474e0":"## Task 3\n### Building Machine Learning Model \n#### i)Linear Regression Model","221d68bd":"## Problem Statement\n### Objective\nMany aspects of the behaviour of cancer disease are highly unpredictable. Even with the huge number of studies that have been done on the DNA mutation responsible for the disease, we are still unable to use these information at clinical level. \n\nHowever, it is important that we understand the effects and impacts of this disease from the past information as much as we possibly can. Now, You are required to build a machine learning  model that would predict the cancer death rate for the given year.\n\n### Dataset & Data Description\nThis dataset contains the data collected from cancer.gov and the US Census American Community Survey. \n\nTo load the training data in your jupyter notebook, use the below command:\n\n    import pandas as pd\n    cancer_data  = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Training_set_label.csv\" )\n    \n#### Dataset Description: There are 34 columns including the target column\n- TARGET_deathRate: Dependent variable. Mean per capita (100,000) cancer mortalities(a)\n- avgAnnCount: Mean number of reported cases of cancer diagnosed annually(a)\n- avgDeathsPerYear: Mean number of reported mortalities due to cancer(a)\n- incidenceRate: Mean per capita (100,000) cancer diagoses(a)\n- medianIncome: Median income per county (b)\n- popEst2015: Population of county (b)\n- povertyPercent: Percent of populace in poverty (b)\n- studyPerCap: Per capita number of cancer-related clinical trials per county (a)\n- binnedInc: Median income per capita binned by decile (b)\n- MedianAge: Median age of county residents (b)\n- MedianAgeMale: Median age of male county residents (b)\n- MedianAgeFemale: Median age of female county residents (b)\n- Geography: County name (b)\n- AvgHouseholdSize: Mean household size of county (b)\n- PercentMarried: Percent of county residents who are married (b)\n- PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school (b)\n- PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma (b)\n- PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college (b)\n- PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree (b)\n- PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma (b)\n- PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree (b)\n- PctEmployed16_Over: Percent of county residents ages 16 and over employed (b)\n- PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed (b)\n- PctPrivateCoverage: Percent of county residents with private health coverage (b)\n- PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance) (b)\n- PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage (b)\n- PctPublicCoverage: Percent of county residents with government-provided health coverage (b)\n- PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone (b)\n- PctWhite: Percent of county residents who identify as White (b)\n- PctBlack: Percent of county residents who identify as Black (b)\n- PctAsian: Percent of county residents who identify as Asian (b)\n- PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian (b)\n- PctMarriedHouseholds: Percent of married households (b)\n- BirthRate: Number of live births relative to number of women in county (b)\n    - (a): years 2010-2016\n    - (b): 2013 Census Estimates\n\n","4acc0dde":"## Conclusion:\n- XGBoost model performed better than LinearRegression, Decisiontree and Randomforest model.\n- Decisiontree Regressor gave the highest mse compare to all the other models and showed the poor performance.\n- It is clearly observable that how the mse value decresed from the baseline XGBoost model to the tunedXGBoost model and further reduced the mse value with the featureselected XGBoost model.\n- We can see that most of the selected features by the XGBoost model are positively or negatively correlated with the target variable as we already inferred from the heatmap and scatterplot.\n- Data standardization reduced the model performance. So here data is neither standardized nor normalised. Used the data as it is.\n- Feature selection by Boruta didn't work so well with this dataset.\n- Also, random_state plays the major role.\n- Replacing the outliers with the median value might improve the model further. You can try it out.\n- Try changing the threshold value for the feature selection in the XGBoost model and see if the model is improving further.\n- Also, you can try with the Tuned RandomForest model or Tuned DecisionTree model to see how the mse value changes.\n","2099c56d":"### Separate the Input and Target Features of the data","bba2c2d5":"##### From all the above models, XGBRegressor performs well by giving the less mse value","600298d4":"### Performing Exploratory Data Analysis (EDA)","d625d741":"- Building and fitting the model with the best parameters values {'learning_rate': 0.159, 'max_depth': 2, 'n_estimators': 3500} provided by GridsearchCV","3da7c053":"## Task 1\n### Import Libraries\n\n\n\n","ce1e2a13":"## Task 2\n### Performing Data Preparation Steps"}}