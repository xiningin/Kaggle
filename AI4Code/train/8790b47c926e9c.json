{"cell_type":{"6b6aa813":"code","f8aaf53f":"code","86ca7a39":"code","75befa67":"code","70494cd1":"code","d559d2e0":"code","ef09c958":"code","383dabd7":"code","03f4db65":"code","8ed860c4":"code","058fcf54":"code","41fdb02f":"code","e88b43b1":"code","77751cb0":"markdown","5a92c1d4":"markdown","e23deeae":"markdown","d55898db":"markdown","7c640863":"markdown","f0e6975a":"markdown","9b058ad3":"markdown","a5043561":"markdown","3cc6c145":"markdown","9a717aa4":"markdown","46340ff4":"markdown","c5ff58f2":"markdown","b4ca1191":"markdown","b285a9c5":"markdown","1bd89488":"markdown","aeb00b2c":"markdown","f4dcee35":"markdown","ade2da4a":"markdown","2994185d":"markdown"},"source":{"6b6aa813":"import gym\nimport numpy as np\nimport pickle, os\nenv = gym.make(\"Taxi-v3\")\nstate = env.reset()","f8aaf53f":"state","86ca7a39":"env.render()","75befa67":"n_states = env.observation_space.n\nn_actions = env.action_space.n\nn_actions\nn_states\nenv.env.s = 254\nenv.render()\nenv.step(3)\nenv.render()","70494cd1":"state = env.reset()\ncounter = 0\ng = 0\nreward = None\nenv.render()\nwhile reward != 20:\n    state, reward, done, info = env.step(env.action_space.sample())\n    counter += 1 \n    g += reward","d559d2e0":"print(\"Solved in {} steps with a total reward of {}\".format(counter,g))\nQ = np.zeros([n_states,n_actions])\nepisodes = 1\nalpha = 0.618","ef09c958":"for episode in range(1,episodes+1):\n    done = False\n    reward = 0\n    state = env.reset()\n    firststate = state\n    print(\"Initial State = {}\".format(state))\n    while reward != 20:\n        action = np.argmax(Q[state])\n        state2, reward, done, info = env.step(action)\n        Q[state,action] = Q[state,action] + alpha*(reward+np.max(Q[state2])-Q[state,action])\n        state = state2","383dabd7":"firststate\nfinalstate = state\nfinalstate\nfirststate\nfinalstate\nQ","03f4db65":"episodes = 500\nrewardTracker = []\nG = 0\nalpha = 0.618\nfor episode in range(1, episodes+1):\n    done = False\n    G, reward = 0,0\n    state = env.reset()\n    while done != True:\n        action = np.argmax(Q[state])\n        state2, reward, done, info = env.step(action)\n        Q[state,action] += alpha*((reward+np.max(Q[state2]))-Q[state,action])\n        G += reward\n        state = state2\n    if episode % 100 == 0:\n        print('Episode {} Total Reward:{}'.format(episode,G))","8ed860c4":"Q","058fcf54":"state = env.reset()\ndone = None\nenv.render()\ncounter = 0\nstate = env.reset()\ndone = False\nwhile done != True:\n    action = np.argmax(Q[state])\n    state, reward, done, info = env.step(action)\n    counter += 1\ncounter","41fdb02f":"with open(\"Uber_qTable.pkl\",'wb') as f:\n    pickle.dump(Q,f)","e88b43b1":"with open(\"Uber_qTable.pkl\",'rb') as f:\n    Qtest = pickle.load(f)","77751cb0":"<a id = '2.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >2. Mathematics behind Reinforcement Learning<\/h2>\n<br>","5a92c1d4":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Save\/Load the Q matrix as pickle file<\/strong><\/p>","e23deeae":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px; padding: 5px;text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n* [1. Introduction](#1.0)\n    * [1.1 What is reinforcement learning](#1.1)\n    * [1.2 How is reinforcement learning different from supervised and unsupervised learning](#1.2)\n \n* [2. Mathematics behind reinforcement learning](#2.0)\n    * [2.1 What is a Q table](#2.1)\n    * [2.2 What is SARSA algorithm](#2.2)\n\n* [3. Use case: Program Uber to pickup passenger](#3.0)\n    * [3.1 Approch 1: Random Walk ](#3.1)\n    * [3.2 Approch 2: Greedy algorithm: Q table](#3.2)\n    * [3.3 Approch 3: Epsilon Greedy algorithm: SARSA ](#3.3)\n   \n\n* [4. Summary](#4)\n\n* [5. References](#5)\n\n","d55898db":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks....Do checkout if you find any of my other work useful....Happy Reading :)<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/saurabhbagchi\/sarcasm-detection-wip\" >Sarcasm Detection<\/a><\/li>\n<li> <a href =\"https:\/\/www.kaggle.com\/saurabhbagchi\/multinomial-logistic-regression-for-beginners\" >Multinomial Logistic Regression for beginners<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/saurabhbagchi\/efficientnet-with-undersampling-optimizer-tuning\">Efficientnet with undersampling optimizer<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/saurabhbagchi\/ensemble-of-best-public-notebooks\">Ensemble of best public notebooks<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/saurabhbagchi\/hubmap-pytorch-with-changed-parameters\">Hubmap Pytorch with changed parameters<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/saurabhbagchi\/fmst-semiconductor-manufacturing-project\">FMST Semiconductor Manufacturing Project<\/a><\/li>\n    \n<\/ol>","7c640863":"<a id = '5.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 5. References<\/h2>\n<br>\n\n<ol>\n<li><a href =\"https:\/\/deepsense.ai\/what-is-reinforcement-learning-the-complete-guide\/#:~:text=Reinforcement%20learning%20is%20the%20training,faces%20a%20game%2Dlike%20situation.&text=Its%20goal%20is%20to%20maximize%20the%20total%20reward.\" >Complete Guide to reinforcement learning<\/a><\/li>\n<li> <a href =\"https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning\" >Reinforcement Learning Wikipedia page<\/a><\/li>\n<li> <a href = \"https:\/\/www.geeksforgeeks.org\/what-is-reinforcement-learning\/\">Reinforcement Learning: An Easy Tutorial<\/a><\/li>\n<li> <a href = \"https:\/\/www.kdnuggets.com\/2018\/03\/5-things-reinforcement-learning.html\">5 things to know about reinforcement learning<\/a><\/li>\n<li><a href =\"https:\/\/searchenterpriseai.techtarget.com\/definition\/reinforcement-learning\" >Definition of reinforcement learning<\/a><\/li>\n<li> <a href =\"https:\/\/wiki.pathmind.com\/deep-reinforcement-learning\" >A beginner's guide to reinforcement learning<\/a><\/li>\n<li> <a href = \"https:\/\/gym.openai.com\/\">Open AI Gym: Official Documentation + Github resource<\/a><\/li>\n   \n\n    ","f0e6975a":"Everything turn out to be excellent, we were able to get the taxi to navigate the 5 x 5 grid to maximize the rewards. We will see later that there is a better approach which is called $\\epsilon$ greedy approach implemented through SARSA algorithm, build up on existing Q algorithm \n\n\n<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What happend so far?<\/strong><\/p>\n\n<ol>\n    <li>Understood what is reinforcement learning<\/li>\n    <li>How reinforcement learning is different from supervised and unsupervised learning<\/li>\n    <li>What is policy, state, action and rewards with respect to reinforcement learning<\/li>\n    <li>Explored why random walk does not give optimized results <\/li>\n    <li>Demonstrated Q algorithm implementation on a toy 5x5 grid for Uber Taxi<\/li>\n<\/ol>\n\n\n<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Well, This a lengthy notebook and Best of my work as far I know, I hope this is helpful..... Thank you so much for reading all the way here...!!<\/strong><\/p>\n","9b058ad3":"The yellow position above shows that the uber cab is currently empty, it will turn green once passenger gets in the car","a5043561":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Random Walk<\/strong><\/p>","3cc6c145":"<a id = '3.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >3. Program Uber to pickup passenger<\/h2>\n<br>","9a717aa4":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What is Q Learning<\/strong><\/p>\n\nQ-Learning is a model free learning method. It is also know as off policy learning method. The agent's action can be random and still it can find an optimal policy, there is no requirement to follow a specific policy, that is the meaning of off policy\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Algorithm<\/strong><\/p>\n\nFor an environment, we build a table called Q-table which has dimensions $S$ x $A$ where $S$ and $A$ are the numbers of state and actions respectively.\n\nFor every state, there are actions and the likeness of choosing a particular action depends on the values in Q-table called state action value. Initially the values of the Q table are 0.An action is chosen for a state. Q-value is increased for the state action, if that action gives a good reward for next state action detected.\n\nQ values are updated using this equation\n\n$Q(s_{t},a_{t}) <--   Q(s_{t},a_{t}) +\\alpha*(r_{t+1} + \\gamma*max(Q(s_{t+1},a)-Q(s_{t+1},a_{t}))$\n\nwhere the previous state and the action are $s_{t}$ and $a_{t}$ and $s_{t+1}$ and $r_{t+1}$ are the current state and reward. The learned value is the target and the old value is the prediction and the difference between them is the error. We then fix the old value using error with the learning rate.\n\nMain steps of the algorithm\n\n1. Initialize Q for all states and actions\n2. For N episodes, follow steps 3-9\n3. Initialize state S\n4. Follow the steps from 5-8 for each step of the episode\n5. Choose an action A, from state S, with some policy derived from Q\n6. Now take action, A and get new state S' and reward R\n7. For S&A, update the Q value from the above equation\n8. Set S=S', as the current state\n9. Terminate if the state S is the terminal\n\nThe key thing in Q-learning is the learned value or the target. The optimal future value estimate is calculated as the max of all the actions in State S'. Suppose for a state S, you have two possible actions A1 and A2, then the action with a greater Q-value is chosen. Action is chosen on a random basis if both have the same Q values.\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Driverless cab<\/strong><\/p>\n\nGoal is to pick and drop the passenger from one location to the another. We would like our automated cab to\n\n* Drop the passenger at the right location\n* Take minimum time possible to drop off\n* Ensure passenger safety and abide by traffic rules\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Driverless cab: Rewards<\/strong><\/p>\n\nHere an imaginary cab driver can be considered as an agent whose focus is on maximizing the rewards and it will learn about the environment and will gain control on the cab through trial and error. So, we can decide the magnitude of rewards and penalties. Some points on that\n\n* High rewards for dropping off the passenger successfully\n* Penalty on dropping off on wrong locations\n* Magnitude of reward and penalty\n* As we would prefer our agent to reach with a few minute delay rather thn making the wrong moves in order to reach the destination faster, the penalty will be \"slight\" negative\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Driverless cab: States<\/strong><\/p>\n\nAgent takes action according to current state it has encountered. The set of all the circumstances the taxi can inhabit is referred to as the state space. Agent makes the actions based on the information in the state. So state needs to have all the necessary information. For example: Let us train a cab for a particular training area where we train it to reach four parking points A, B, C and D.\n\n* Say only our driverless cab is in the parking lot\n* The parking lot can be assumed to be a 5 x 5  grid, giving 25 possible locations. The current location of our cab is (3,1)\n* There are 4 locations where we can pick up or drop passenger A,B,C and D\n* Our passenger is in location D and needs to go to A\n* Passenger states = 5 location with passenger state of being inside the taxi\n* Total possible states = 5 * 5 * 5 * 4 = 500\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Driverless cab: Actions<\/strong><\/p>\n\n* Agent considers one of the 500 states and takes actions\n* Action in this case\n* Move in a direction or decide to pick up or drop off a passenger\n* It can move in either of the 4 directions (north, east, west and south)\n* 6 possible actions\n* Action Space - All possible actions that an agent can take \n* Some actions cannot be performed because of the walls, that is penalised in the environment code by -1\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Driverless cab: Utilized Functions<\/strong><\/p>\n\n* env.reset: Returns a random initial state and also resets the environment\n* env.step(action): Increases time step in the environment\n* env.render: This is helpful in visualization as it renders one frame of an environment\n\n**Environment returns**\n\n* Observation: Environment specific object representing your observation of the environment\n* Reward: Rewards achieved from the action done previously\n* Done: Tells if environment needs to be reset\n* Info: Used for debugging ","46340ff4":"<br>\n<h2 style = \"font-size:50px; font-family:Garamond ; font-weight : normal; background-color:#FA4659; color :  #F0FFF3; text-align: center; border-radius: 5px 5px; padding: 5px\"> Reinforcement Learning: How Uber works<\/h2> \n<br>\n<div class = 'image'> <img style=\"float:center; width:80%;border:10px solid #FA4659;\" align=center src = https:\/\/www.businessmodelsinc.com\/wp-content\/uploads\/2018\/11\/Uber-1024x576.jpg> \n<\/div>\n<br>\n<br>","c5ff58f2":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >1. Introduction<\/h2>\n<br>","b4ca1191":"Using the Q algorithm we were able to achieve better rewards over the number of episodes","b285a9c5":"<a id = '4.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #FA4659 ; color : #F0FFF3; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" > 4. Summary<\/h2>\n<br><\/h2>","1bd89488":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What are we trying to solve? - What is reinforcement learning?<\/strong><\/p>\n\nReinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. To get the machine to do what the programmer wants, the artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\nAlthough the designer sets the reward policy\u2013that is, the rules of the game\u2013he gives the model no hints or suggestions for how to solve the game. It\u2019s up to the model to figure out how to perform the task to maximize the reward, starting from totally random trials and finishing with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, reinforcement learning is currently the most effective way to hint machine\u2019s creativity. \n<br>\nIn contrast to human beings, artificial intelligence can gather experience from thousands of parallel gameplays if a reinforcement learning algorithm is run on a sufficiently powerful computer infrastructure.\nhttps:\/\/deepsense.ai\/what-is-reinforcement-learning-the-complete-guide\/\n<br>\n<br>\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>What are examples of reinforcement learning?<\/strong><\/p>\n\nReinforcement learning is heavily inspired by real life examples of how humans or animals strive towards a certain goal if we associate rewards with a positive experience and penalty with a negative experience\n\n* Training a dog to fetch a stick or a ball (rewards is either cuddle\/fondle or food and penalty is no reaction or scolding)\n* Training a baby to walk (rewards is either cuddle\/fondle or chocolate and penalty is no reaction or scolding)\n* Training a cat to do new tricks (rewards is again either cuddle\/fondle or food and penalty is no reaction or scolding)\n\nLet us discuss the different types of training done in Machine Learning, which will help us understand the nuances involved here\n\n**Types of Learning:**\n\n* Supervised Learning\n* Unsupervised Learning\n* Reinforcement Learning\n\n**Supervised Learning:**\n\n* Data is present in (x,y) format where x is feature space and y is target\n* Goal is to learn a mathematical function to map x to y\n* Few techniques used are as below\n1. * Classification\n2. * Regression\n3. * Object detection\n\n**Unsupervised Learning:**\n\n* Data is present in x format which is the feature space, just feature space, no labels\n* Goal is to understand the data and understand hidden or latent structures\n* Few techniques used are as below\n1. * Dimensionality reduction aka PCA (Principal component analysis)\n2. * Clustering\n3. * Feature learning\n\n<br>\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Reinforcement Learning<\/strong><\/p>\n\n* It is more general than supervised and unsupervised learning\n* Agents get some reward or penalty as an outcome of interacting with the environment\n* The goal is to maximize the reward and take only those actions that will help achieve that\n\nReinforcement learning can be considered as the science of decision making which is semi supervised learning. In other words it is an experience based decision making technique. The process involves below steps\n\n* Observe -> Environment of the agent\n* Decide -> Decision as per the observation\n* Act -> Action on the decision\n* Receive -> Getting rewarded or penalized as per the action\n* Learn -> From previous action and improve\n* Iterate -> Repeat the entire process till success\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Rewards: Main component in reinforcement learning process<\/strong><\/p>\n\nReward is a scalar feedback signal (single number). Basically, it conveys the overall progress of the agent. The task of the agent is to maximize the reward as follows\n\n* Drop\/pickup a passenger from a driverless car\n1. * Positive or negative reward for correct\/wrong location\n* Fly stunt maneuvers in a fighter plane\n1. * Positive reward for going on the desired trajectory\n2. * Negative reward on crashing\n* Defeat a world class chess player\n1. * Positive or negative reward for victory\/defeat\n* Manage a bank portfolio\n1. * Positive or negative reward for profit\/loss\n* Create a walking robot\n1. * Positive reward if it walks in the right direction\n2. * Negative reward if it trips and falls\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Goals and Actions: Main component in reinforcement learning process<\/strong><\/p>\n\nGoal is to maximize the rewards by taking actions according to the policy. Goal depends upon the time. RL algorithm's goal can be to get instant rewards or it can leave some reward to achieve long term goal.\n\nExample: In chess an agent can sacrifice some pawns for long term goal of doing check mate to the opponent\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>State: Main component in reinforcement learning process<\/strong><\/p>\n\nState contains all the information about the environment that can be helpful in deciding the next steps. For example, a frame in a game of chess defines the opponent's situation, your own situation and location of possible rewards\n\n**Agent state and environment state:**\n\n* Agent state is private to that particular agent. Algorithms use the agent's state for choosing actions\n* Agent does same action in the environment and gets rewards or penalties depending upon the action. Agent may or may not be aware of the environment state\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Policy: Main component in reinforcement learning process<\/strong><\/p>\n\nPolicy is the rules followed by the agent to get maximum rewards. It can also be decided as behavior of agent. The policy can be deterministic or stochastic (probability of taking some action for a given state)\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Model: Main component in reinforcement learning process<\/strong><\/p>\n\nAgent represents the environment in the form of a model.It helps in figuring out a plan for the next steps by learning about the environment. It can be divided into two states as below\n\n* Transition: It is used to predict the future state, example if the position or velocity of an object is given it can predict what will the environment do next\n\n$P_{ss'}^{a} = P[s_{t+1} = s'|s_{t}=s,A_{t}=a]$\n\nThe above equation tells us that given $s$ and $a$ what would be $s'$. Where $s$, $a$ and $s'$ are the current state, action and the probability of being in the next state respectively.\n\n* Reward: It predicts the immediate reward. Following an action, it predicts the reward the object would get\n\n$R_{s}^{a} = E[r_{t+1}|s_{t} = s, A_{t}=a]$\n\nThe above equation tells us that given $s$ and$a$ what would be $R$. Where $s$, $a$ and $R$ are the current state, action and expected reward respectively.\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>RL Framework<\/strong><\/p>\n\nReinforcement learning can be applied to a very large set of problems ranging from teaching a computer to play a game to understanding certain chemical reactions. But the major problem is to get started as there is a serious lack of simulated learning environments available for experimentation\n\n<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>OpenAIGym<\/strong><\/p>\n\nIt is a python package containing several reinforcement learning environments, ranging from basic to advanced. Also includes video games and robotics simulated environments. The aim of developing this was to have a standard environment and benchmark for research in RL","aeb00b2c":"We can see that the Q matrix is now fairly populated so the cab knows which action to take from a particular state through the entries in the matrix to maximize the rewards (greedy approach)","f4dcee35":"<p style = \"font-size:20px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>Q algorithm: A greedy approach<\/strong><\/p>","ade2da4a":"As we can see from above a random walk leads us to highly undesirable results, with total negative rewards","2994185d":"<p style = \"font-size:30px; color:#FA4659 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>I would like to thanks @bhuvanchennoju for creating such wonderful notebook formats and entire credit to him for the format in this notebook<\/strong><\/p>"}}