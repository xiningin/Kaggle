{"cell_type":{"3c04e45e":"code","7d105521":"code","9327997a":"code","091899d6":"code","30ab3c3a":"code","fcbccf98":"code","d0013a5f":"code","f33aa706":"code","7f2cc7ac":"code","54e23135":"code","61ddc12d":"code","7d282900":"code","459c2904":"code","9837a674":"code","055a588f":"code","a1f000d2":"code","67e403c2":"code","91b4aaa2":"code","12943329":"code","26817085":"code","a1dae3df":"code","6d3a97ff":"code","90b9d27c":"markdown","22fc2063":"markdown","8a6df2ab":"markdown","3826d8f1":"markdown","f38d7f42":"markdown","5753e1e9":"markdown","755f502f":"markdown","3f82531e":"markdown","6e14b191":"markdown","69f71892":"markdown","95ac3404":"markdown","2071ad33":"markdown","ddc5377b":"markdown","93e49981":"markdown","7d6e263d":"markdown","5dd56f52":"markdown","0435ca8c":"markdown","12568657":"markdown","8cecac03":"markdown","f653384b":"markdown","fd9fc46e":"markdown","6de1964f":"markdown","6aa4061a":"markdown","8ceb661d":"markdown","f618098d":"markdown","b5280a41":"markdown","4b8a2603":"markdown","b70666c4":"markdown","01a06451":"markdown"},"source":{"3c04e45e":"import warnings\n\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\n\n# %matplotlib inline\nwarnings.filterwarnings('ignore')","7d105521":"###############################################################\n# Load the data                                               #\n###############################################################\ntrain_data_path = 'input\/train.csv'\ntest_data_path = 'input\/test.csv'\n\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\n\ntrain_data_size = train_data.shape[0]\ntest_data_size = test_data.shape[0]\n\nall_data = pd.concat((train_data, test_data)).reset_index(drop=True)","9327997a":"missing_data_threshold = 80\nnone_category_threshold = 2\npredominant_value_threshold = 98\nfeature_correlation_threshold = 0.80\nprediction_correlation_threshold = 0.60\nskewness_threshold = 0.75\nboxcox_lambda = 0.15\nn_folds = 7\n#blending weights\nstacked_pred_weight = 0.4\ngbr_pred_weight = 0.1\nxgb_pred_weight = 0.2\nridge_pred_weight = 0.1\nelastic_pred_weight = 0.05\nlasso_pred_weight = 0.05\nlr_pred_weight = 0.1","091899d6":"predicted_feature = 'SalePrice'\n\ny = train_data[predicted_feature]","30ab3c3a":"###############################################################\n# Organize features into collections                          #\n###############################################################\n\n# Some of the numerical features are really categorical features that encode the different categories with numbers.\n# Converting them to 'str' will help later in classifying them as categorical.\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['YrSold'] = all_data['YrSold'].apply(str)\nall_data['YearBuilt'] = all_data['YearBuilt'].apply(str)\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].apply(str)\nall_data = all_data.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})\n\nnumeric_features = [f for f in all_data.columns if all_data.dtypes[f] != 'object']\nnumeric_features_filtered = numeric_features\n\ncategorical_features = [f for f in all_data.columns if all_data.dtypes[f] == 'object']","fcbccf98":"# I define the set of features that will be used for training. This allows me to make any changes I need easily\n# and at the same time, leave the original information unchanged.\nfeatures = numeric_features_filtered + categorical_features\nfeatures.remove('Id')\nfeatures.remove('SalePrice')","d0013a5f":"###############################################################\n# Analyze and handle missing data                             #\n###############################################################\n# Calculate the amount of missing values in each feature\nmissing_perc = (all_data.isnull().sum() \/ len(all_data)) * 100\nmissing_perc = missing_perc[(missing_perc.index != predicted_feature) & (missing_perc > 0)]\nmissing_perc.sort_values(inplace=True, ascending=False)\n\nmissing_abs = all_data.isnull().sum()\nmissing_abs = missing_abs[(missing_abs.index != predicted_feature) & (missing_abs > 0)]\nmissing_abs.sort_values(inplace=True, ascending=False)\n\nmissing_data = pd.DataFrame({'Missing_Ratio': missing_perc, 'Missing_Total': missing_abs})\nprint(\"\\nFeatures with missing values:\")\nprint('{}'.format(missing_data))","f33aa706":"# Handle variable by variable\n# We discard variables with more than 80% percent of data missing\nfor f in missing_data[missing_data.Missing_Ratio > missing_data_threshold].index:\n    features.remove(f)\n\n# For categorical values, if the number of na represent more than 2%, we fill it with \"None\",\n# but if it is less than 2%, we fill it with the most common value in the dataset\nfor f in missing_data[(missing_data.Missing_Ratio <= missing_data_threshold) & (missing_data.Missing_Ratio > none_category_threshold)].index:\n    if f in categorical_features:\n        all_data[f] = all_data[f].fillna(\"None\")\n\nfor f in missing_data[(missing_data.Missing_Ratio <= none_category_threshold)].index:\n    if f in categorical_features:\n        all_data[f] = all_data[f].fillna(all_data[f].mode()[0])\n\n\n# Some features are domain specific and I decide how to fill them manually\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nall_data[\"GarageYrBlt\"] = all_data[\"GarageYrBlt\"].fillna(0)\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data[\"BsmtFullBath\"] = all_data[\"BsmtFullBath\"].fillna(0)\nall_data[\"BsmtHalfBath\"] = all_data[\"BsmtHalfBath\"].fillna(0)\nall_data[\"BsmtUnfSF\"] = all_data[\"BsmtUnfSF\"].fillna(0)\nall_data[\"TotalBsmtSF\"] = all_data[\"TotalBsmtSF\"].fillna(0)\nall_data[\"GarageArea\"] = all_data[\"GarageArea\"].fillna(0)\nall_data[\"GarageCars\"] = all_data[\"GarageCars\"].fillna(0)\nall_data[\"BsmtFinSF2\"] = all_data[\"BsmtFinSF2\"].fillna(0)\nall_data[\"BsmtFinSF1\"] = all_data[\"BsmtFinSF1\"].fillna(0)","7f2cc7ac":"###############################################################\n# Generate new variables from the existing ones               #\n###############################################################\n# I generate new features by combining existing features in the dataset.\n# I saw no way to automatize this. It is completely domain dependant.\nall_data[\"TotalSF\"] = all_data[\"TotalBsmtSF\"] + all_data[\"GrLivArea\"]\nall_data[\"TotalSF_HQ\"] = all_data[\"TotalBsmtSF\"] + all_data[\"GrLivArea\"] - all_data[\"LowQualFinSF\"] - all_data[\"BsmtUnfSF\"]\nall_data[\"TotalBath\"] = all_data[\"BsmtFullBath\"] + all_data[\"FullBath\"] + (all_data[\"BsmtHalfBath\"]\/2) + (all_data[\"HalfBath\"]\/2)\nall_data[\"TotalPorchSF\"] = all_data[\"OpenPorchSF\"] + all_data[\"EnclosedPorch\"] + all_data[\"3SsnPorch\"] + all_data[\"ScreenPorch\"]\nall_data[\"OverallRating\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\n\nfeatures.append('TotalSF')\nfeatures.append('TotalSF_HQ')\nfeatures.append('TotalBath')\nfeatures.append('TotalPorchSF')\nfeatures.append('OverallRating')\n\nall_data[\"HasPool\"] = all_data[\"PoolArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"Has2ndFloor\"] = all_data[\"2ndFlrSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasGarage\"] = all_data[\"GarageArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasBsmt\"] = all_data[\"TotalBsmtSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasFireplace\"] = all_data[\"Fireplaces\"].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures.append('HasPool')\nfeatures.append('Has2ndFloor')\nfeatures.append('HasGarage')\nfeatures.append('HasBsmt')\nfeatures.append('HasFireplace')","54e23135":"###############################################################\n# Select and discard the most relevant\/useless features       #\n###############################################################\n# Automatically drop features that have a value present in more than 98% of the examples\n# These features are not very informative\nprint('\\nVariables that have a very predominant value. Check if could be candidates to drop')\nfor i in all_data[features].columns:\n    most_frequent_value = all_data[i].mode().iloc[0]\n    most_frequent_value_repetitions = all_data[i].value_counts().iloc[0]\n    most_frequent_value_perc = most_frequent_value_repetitions \/ len(all_data) * 100\n    if most_frequent_value_perc > predominant_value_threshold:\n        print('Variable: {}: value \"{}\" appears {} times, {}% of the examples'.format(i, most_frequent_value,\n                                                                                      most_frequent_value_repetitions,\n                                                                                      most_frequent_value_perc))\n        features.remove(i)","61ddc12d":"# Encode the categorical features.\n# I assign the mean of the SalePrice for houses with each value in that category\ndef encode(frame, feature):\n    numeric_values = pd.DataFrame()\n    numeric_values['categories'] = frame[feature].unique()\n    numeric_values.index = numeric_values.categories\n    category_value = frame[[feature, predicted_feature]].groupby(feature).mean()[predicted_feature]\n    if np.isnan(category_value).any():\n        category_value[np.isnan(category_value) == True] = frame[predicted_feature].mean()\n    numeric_values['spmean'] = category_value\n    numeric_values = numeric_values['spmean'].to_dict()\n    for cat, value in numeric_values.items():\n        frame.loc[frame[feature] == cat, feature + '_E'] = value\n\nnew_features = []\nfeatures_to_delete = []\nfor q in features:\n    if q in categorical_features:\n        encode(all_data, q)\n        features_to_delete.append(q)\n        new_features.append(q + '_E')\n\n# I substitute the categorical features by their encoded versions\nfor x in features_to_delete:\n    features.remove(x)\nfeatures = features + new_features","7d282900":"#\n# Detect correlations between training features\n#\ncorr_features = pd.DataFrame({'Feature_A': [], 'Feature_B': [], 'corr_score': []})\n\ncorrmat = all_data[features].corr()\nfor i in range(2, len(corrmat.columns)):\n    for j in range(1, i):\n        feature_1 = corrmat.columns[i]\n        feature_2 = corrmat.columns[j]\n        if corrmat[feature_1][feature_2] > feature_correlation_threshold:\n            corr_features.loc[len(corr_features)] = [feature_1, feature_2, corrmat[feature_1][feature_2]]\n\ncorr_learning_features = corr_features.loc[(corr_features.Feature_A != predicted_feature) & (corr_features.Feature_B != predicted_feature)]\nprint(\"\\nThere are {} pairs of training features potentially correlated\".format(len(corr_learning_features)))\nfor index, row in corr_learning_features.sort_values('corr_score', ascending=False).iterrows():\n    print('{} y {} : {}'.format(row['Feature_A'], row['Feature_B'], row['corr_score']))","459c2904":"# I drop only some of the features, that are clearly overlapping\nfeatures.remove('GarageCond_E')\nfeatures.remove('GarageArea')\nfeatures.remove('GarageQual_E')","9837a674":"#\n# And now detect correlations between training features and the predicted feature\n#\ncorr_features = pd.DataFrame({'Feature_A': [], 'Feature_B': [], 'corr_score': []})\n\ncorrmat = all_data[:train_data.shape[0]][features + [predicted_feature]].corr('spearman')\nfor i in range(1, len(corrmat.columns)):\n    feature_1 = corrmat.columns[i]\n    feature_2 = predicted_feature\n    if corrmat[feature_1][feature_2] > prediction_correlation_threshold:\n        corr_features.loc[len(corr_features)] = [feature_1, feature_2, corrmat[feature_1][feature_2]]\n\nprediction_correlated_features = []\nprint(\"\\nThere are {} features potentially correlated with {}\".format(len(corr_features), predicted_feature))\nfor index, row in corr_features.sort_values('corr_score', ascending=False).iterrows():\n    if row['Feature_A'] != predicted_feature:\n        print('{}: {}'.format(row['Feature_A'], row['corr_score']))\n        prediction_correlated_features.append(row['Feature_A'])","055a588f":"#\n# Using the numeric features that are more correlated with the feature to predict, we generate new polinomial features\n#\nfor f in prediction_correlated_features:\n    new_feature_name = \"{}_2\".format(f)\n    all_data[new_feature_name] = all_data[f] ** 2\n    features.append(new_feature_name)\n\n    new_feature_name = \"{}_3\".format(f)\n    all_data[new_feature_name] = all_data[f] ** 3\n    features.append(new_feature_name)\n\n    new_feature_name = \"{}_sqrt\".format(f)\n    all_data[new_feature_name] = np.sqrt(all_data[f])\n    features.append(new_feature_name)","a1f000d2":"###############################################################\n# Transform numeric features                                  #\n###############################################################\n#Deleting outliers\n# Outlier detection is also domain specific\nfor i in all_data[(all_data['GrLivArea']>4000) & (all_data['SalePrice']<300000)].index:\n    all_data = all_data.drop(i)\n    train_data = train_data.drop(i)\n    y = y.drop(i)\n    train_data_size -= 1","67e403c2":"# Check the skew of all numerical features\n# For variable that show a high skewness, I aplly boxcox to normalize the distribution\nnumeric_feats = all_data[features].dtypes[all_data[features].dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' : skewed_feats})\nskewness = skewness[abs(skewness) > skewness_threshold]\nlam = boxcox_lambda\nfor feat in skewness.index:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","91b4aaa2":"###############################################################\n# Build the model                                             #\n###############################################################\n# Regenerate the training and test datasets, using the set of features developed\ntrain = all_data[['Id'] + features][:train_data_size].as_matrix()\ntest = all_data[['Id'] + features][-test_data_size:].as_matrix()","12943329":"# Normalize the feature to predict\ny = np.log1p(y)","26817085":"# First of all, I have to calculate the parameters for the models I will use to predict.\n# I will use cross validation and measure the results using the same score as in the competition\n\nkf = KFold(n_folds, shuffle=True, random_state=42)\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\n# LogisticRegression\nlr_model = make_pipeline(StandardScaler(), LinearRegression())\n# score = np.sqrt(-cross_val_score(lr_model, train, y, scoring=scorer, cv = kf))\n# print(\"\\nLinear Regression Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n\n# Lasso Regression\n# alphas = [0.00000001, 0.000003, 0.000009, 0.00001, 0.00003, 0.00009, 0.0001, 0.0003, 0.0009, 0.001, 0.003, 0.009, 0.01]\n# for a in alphas:\n# for a in np.arange(0.0005, 0.003, 0.00001):\n#     lasso_model = Lasso(alpha=a, random_state=1)\n#     score = np.sqrt(-cross_val_score(lasso_model, train, y, scoring=scorer, cv=kf))\n#     if score.mean() < 1.1174:\n#         print(\"Lasso Score with alpha {}: {} ({})\".format(a, score.mean(), score.std()))\n\nlasso_model = make_pipeline(StandardScaler(), Lasso(alpha=0.00096, random_state=1))\n# score = np.sqrt(-cross_val_score(lasso_model, train, y, scoring=scorer, cv=kf))\n# print(\"Lasso Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n\n#ElasticNet\n# l1_ratios = [0.0001, 0.0003, 0.0009, 0.001, 0.003, 0.009, 0.01, 0.03, 0.09, 0.1, 0.3, 0.9, 1, 3, 9, 10, 30, 90]\n# # for a in alphas:\n# for a in np.arange(0.001, 0.0095, 0.0005):\n#     for l1 in l1_ratios:\n#         ENet = ElasticNet(alpha=a, l1_ratio=l1, random_state=3)\n#         score = np.sqrt(-cross_val_score(ENet, train, y, scoring=scorer, cv = kf))\n#         if score.mean() < 0.1175:\n#             print(\"ENet Score with alpha {} and l1_ratio {}: {} ({})\".format(a, l1, score.mean(), score.std()))\n\nelastic_model = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0033, l1_ratio=0.3, random_state=3))\n# score = np.sqrt(-cross_val_score(elastic_model, train, y, scoring=scorer, cv = kf))\n# print(\"ENet Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n\n# Ridge Regression\n# En primera ronda los mejores resultados fueron 6, 2, 6  y 3,2,3\n# alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30]\n# degree = [1,2,3]\n# coef = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6]\n# for a in np.arange(2.0, 8.0, 0.2):\n#     for d in [2]:\n#         for c in np.arange(3.0, 4.0, 0.1):\n#             Ridge = KernelRidge(alpha=a, kernel='polynomial', degree=d, coef0=c)\n#             score = np.sqrt(-cross_val_score(Ridge, train, y, scoring=scorer, cv = kf))\n#             if score.mean() < 0.11417:\n#                 print(\"Ridge Score with alpha {}, degree {} and coef {}: {} ({})\".format(a, d, c, score.mean(), score.std()))\n\nridge_model = make_pipeline(StandardScaler(), KernelRidge(alpha=4.3, kernel='polynomial', degree=2, coef0=3.9))\n# score = np.sqrt(-cross_val_score(ridge_model, train, y, scoring=scorer, cv = kf))\n# print(\"Ridge Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n\n# XGBRegressor\n# l_rates = [0.0005, 0.001, 0.003, 0.009, 0.01, 0.03, 0.09, 0.1, 0.3, 0.9]\n# # for l in l_rates:\n# for l in np.arange(0.001, 0.009, 0.0005):\n#     incrementing_steps = 0\n#     last_score = 999999999\n#     for est in range(3000, 10000, 250):\n#         XGB = XGBRegressor(n_estimators=est, learning_rate=l, n_jobs=-1, nthread=-1)\n#         score = np.sqrt(-cross_val_score(XGB, train, y, scoring=scorer, cv = kf))\n#         if score.mean() < 0.12:\n#             print(\"XGB Score with learning rate {} and num estimators {}: {:.6f} ({:.6f})\".format(l, est, score.mean(), score.std()))\n#         if score.mean() > last_score:\n#             incrementing_steps += 1\n#             print(\"This try was worse than the previous one. incrementing_steps = {}\".format(incrementing_steps))\n#         else:\n#             incrementing_steps = 0\n#         last_score = score.mean()\n#         if incrementing_steps == 3:\n#             print(\"Stop exploring more estimators, because it has gone worse for the last three\")\n#             break\n#     print(\"Finished for learning rate {}\".format(l))\n\nxgb_model = XGBRegressor(learning_rate=0.0085, n_estimators=3500, n_jobs=-1, nthread=-1)\n# score = np.sqrt(-cross_val_score(xgb_model, train, y, scoring=scorer, cv=kf))\n# print(\"XGB Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n\n# GradientBoostingRegressor\n# for est in range(400, 2000, 200):\n#     for a in np.arange(0.05, 1, 0.05):\n#         for d in range(2, 6, 1):\n#             for sub in np.arange(0.5, 1, 0.1):\n#                 # skgbm_model = GradientBoostingRegressor(loss='huber', n_estimators=700, alpha=.5, max_depth=3, subsample=.6)\n#                 skgbm_model = GradientBoostingRegressor(loss='huber', n_estimators=est, alpha=a, max_depth=d, subsample=sub)\n#                 score = np.sqrt(-cross_val_score(skgbm_model, train, y, scoring=scorer, cv=kf))\n#                 print(\"skgbm_model with est {}, alpha {:.3f}, d {} and sub {:.1f}: Score: {:.4f} ({:.4f})\".format(est, a, d, sub, score.mean(), score.std()))\n\ngbr_model = GradientBoostingRegressor(loss='huber', n_estimators=600, alpha=0.5, max_depth=2, subsample=0.6)\n# score = np.sqrt(-cross_val_score(gbr_model, train, y, scoring=scorer, cv=kf))\n# print(\"GradientBoostingRegressor Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n# Stacked model\nstacked_model = StackingCVRegressor(regressors=(lr_model, lasso_model, elastic_model, ridge_model, gbr_model), meta_regressor=xgb_model, use_features_in_secondary=True)\n# score = np.sqrt(-cross_val_score(stacked_model, train, y, scoring=scorer, cv=kf))\n# print(\"Stacked Score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","a1dae3df":"# And finally, use the parameterized models to predict the results and blend them\nlr_model.fit(train, y)\nlr_pred = lr_model.predict(test)\n\nlasso_model.fit(train, y)\nlasso_pred = lasso_model.predict(test)\n\nelastic_model.fit(train, y)\nelastic_pred = elastic_model.predict(test)\n\nridge_model.fit(train, y)\nridge_pred = ridge_model.predict(test)\n\nxgb_model.fit(train, y)\nxgb_pred = xgb_model.predict(test)\n\ngbr_model.fit(train, y)\ngbr_pred = gbr_model.predict(test)\n\nstacked_model.fit(train, y)\nstacked_pred = stacked_model.predict(test)\n\n\nensemble = stacked_pred * stacked_pred_weight + \\ \n           gbr_pred * gbr_pred_weight + \\\n           xgb_pred * xgb_pred_weight + \\\n           ridge_pred * ridge_pred_weight + \\\n           elastic_pred * elastic_pred_weight + \\\n           lasso_pred * lasso_pred_weight + \\\n           lr_pred * lr_pred_weight\n\nfinal_ensemble = np.floor(np.expm1(ensemble))","6d3a97ff":"# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\noutput = pd.DataFrame({'Id': all_data['Id'][-test_data_size:], 'SalePrice': final_ensemble})\noutput.to_csv('submission.csv', index=False)","90b9d27c":"***Missing data***\n\nFirst of all, I will handle missing data. There are many features that have missing values, and even some of them which miss almost all of the values. First, tell me which features miss values and how many:","22fc2063":"This is the result:\n\n                    Missing_Ratio  Missing_Total\n    PoolQC            99.657417           2909\n    MiscFeature       96.402878           2814\n    Alley             93.216855           2721\n    Fence             80.438506           2348\n    FireplaceQu       48.646797           1420\n    LotFrontage       16.649538            486\n    GarageQual         5.447071            159\n    GarageCond         5.447071            159\n    GarageFinish       5.447071            159\n    GarageType         5.378554            157\n    BsmtExposure       2.809181             82\n    BsmtCond           2.809181             82\n    BsmtQual           2.774923             81\n    BsmtFinType2       2.740665             80\n    BsmtFinType1       2.706406             79\n    MasVnrType         0.822199             24\n    MasVnrArea         0.787941             23\n    MSZoning           0.137033              4\n    BsmtFullBath       0.068517              2\n    BsmtHalfBath       0.068517              2\n    Utilities          0.068517              2\n    Functional         0.068517              2\n    Electrical         0.034258              1\n    BsmtUnfSF          0.034258              1\n    Exterior1st        0.034258              1\n    Exterior2nd        0.034258              1\n    TotalBsmtSF        0.034258              1\n    GarageArea         0.034258              1\n    GarageCars         0.034258              1\n    BsmtFinSF2         0.034258              1\n    BsmtFinSF1         0.034258              1\n    KitchenQual        0.034258              1\n    SaleType           0.034258              1\n\nThere are several different situations. There are features that miss most of the values, so they are not very representative to do the training, and there are other features that just miss some of the values and some handling of these missed values should be implemented to improve the training accuracy. \n\nFirst, I try to implement some generic decisions, that will let me handle most of the cases:\n1. I will drop features that miss more than 80% of the values\n2. For categorical features, if they miss more than 2% of the values, I fill them with 'None', making it a new category by itsef, while if the missing values represent less than 2% percent of the values, I just fill them with the most common value in the category.\n\nThere are some other features that are not covered by these criteria, and I decided to fill them manually. Maybe these cases could be generalized to find a domain agnostic criteria too.","8a6df2ab":"Variable 'predicted_feature' will help in keeping the code independent from the exact feature we want to predict. This will not made the code generic, but will help if in the future I want to resuse this code for another problem.","3826d8f1":"And now I define the \"features\" collection, that initially has all the categorical and numerical features in the dataset, but will be heavily modified during the process. This way, I will use *all_data[features]* along the code to refer to the dataset I am working with, but avoiding the modification of the original datasets, so that I can check the original data at any time of the process (and I had to do it quite a few times).\n\nAs a first step, I drop the 'Id' and 'SalePrice' columns, since I will not need them during the data analysis\/feature engineering phase.","f38d7f42":"And, to wrap up, generate the submission information for the competition.","5753e1e9":"**Model building**\n\nFirst, I split the all_data set back into the training and testing sets.","755f502f":"So, the data is ready and now it is time to train the models. It is time to build them.","3f82531e":"***Outliers management***\n\nThe set of features is ready, but I will do a couple last steps analysing their values. \n\nThe first step is regarding outliers. There are two houses in the training set that have a huge amount of square feet that does not correspond to their sale price. Most of the kernels I studied decided to drop them, and so did I.","6e14b191":"***Detect correlated features***\n\nNow that all the features are numeric, it is possible to study the correlations between the features, and between the features and the SalePrice.\n\nI study first the correlation between features. Highly correlated features should be avoided since they negatively affect the predictions. ","69f71892":"**Feature engineering**\n\n***'Categorize' some numerical features***\n\nI want to organize the features in two collections, one for numerical features and another one for categorical features. However, there are some features that even though they contain numbers, and would be automatically classified as numerical, in essence they are representing categories with those numbers. I transform the numbers into strings so that those features are detected as categorical from now on.","95ac3404":"**Conclusions\/Next steps**\n\nThere are a bunch of things that would be worth trying and maybe could help in improving the results. To name a few:\n* I could try using different models and see if any of them provides better results, or complements the ones I already used.\n* I could repeat the model parameter tuning phase, as I changed some small details in the feature engineering phase after I had all the models already parameterized. Maybe this could result is slightly better tuned models. \n* I could split the train set into a train and test set, and use it to fine tune the blending weights.\n* There are wonderful visualizations that are very helpful in the feature engineering phase. As my objective was to create a kernel as automatic as possible, I preferred to use raw data, but adding them would improve the decisions that had to be taken manually.\n* Retry the Johnson function in the SalePrice feature, as it should provide better results than the log.\n\nThere is a big score difference between the score that this kernel gets if I execute it inside kaggle, or if I execute the code in my laptop and I upload the submission file. I have to look for the reasons for that gap.\n\nAs I said at the beginning, any feedback will be highly appreciated.\n","2071ad33":"***Detect correlation between features and the SalePrice***\n\nAnd next, I detect correlation between the remaining features and the SalePrice. This will point out which of the features in the data set are more representative to decide the price of a house, and I will use this information to add some new very important features to the collection. ","ddc5377b":"I have some doubts wether the *HasPool*\/*PoolArea* features could help predicting the price of the most expensive houses. I need to check the data. In case it seems so, it would be enough to re-append them to the *features* collection so that the following steps still take them into acount.","93e49981":"Imports of all the libraries used along the code","7d6e263d":"***Encode categorical features***\n\nThen I encode the categorical values. This converts categorical features into numerical so that the regression algorithms can use them in the calculations. \n\nI used a piece of code that assigned to the different values of a feature the ordinal number according to the average sale price of the houses with that value. This is, if a feature has values A, B and C, and the average sale prices for houses for these values are 10000, 15000 and 12000, respectively, the encoding algorithm would substitute A with 3, B with 1 and C with 2, like this:\n\n* A --> Average SalePrice=10000 --> 3\n* B --> Average SalePrice=15000 --> 1\n* C --> Average SalePrice=12000 --> 2\n\nHowever, I decided to modify this substitution and instead of the ordinal, I assign directly the average sale price of the category value. In my impression, this keeps the ordering information while also including information about the proportion in which each category value affects the final sale price. The tests I did seem to give slightly better results. There is a caveat here (represented by an 'if' clause in the code), that is the fact that you can find a value in the test set that does not appear in the train set, so there is no information about the SalePrice for that category value. In this case, I decided to assign the mean SalePrice for all houses in the train set. In the Ames dataset this happens only once, with the value 150 of feature MsSubClass.","5dd56f52":"***Model parameter tuning***\n\nI have chosen seven different models. A simple logistic regression, Lasso, ElasticNet, Ridge, XGBRegressor, GradientBoostingRegressor and a stacked combination of all of them. But as a newbie that I am, I have no experience to decide how to wisely choose the parameters to apply in each of them. So I have to rely on something that allows me to measure objectively how well they perform. I will go one by one, and use cross validation on each of the models. I start with a for loop with coarse grained values (or several nested for loops if I want to tune several parameters), and once I see which values give better results, I repeat the steps, but with values in the range of the ones that gave the best scores in the first round, more fine grained. This is a very time consuming and resource intensive method, and only allows to fit a few parameters, but works reasonably well if you do not have any other heuristic to apply. \n\nI leave the commented code showing the parameter tuning loops, although it is not necessary to make the final predictions.","0435ca8c":"***Generate polinomial features***\n\nCorrelation are detected and now it is time to generate the new features. I will get each of the features that have a correlation factor above the threshold and generate three new features: the square, the cube and the square root of each of them. These are called polinomial features and will let the prediction function adapt better to the SalePrice distribution.","12568657":"I arrange here all the variables in the code that help me take decisions, to make changes easier.","8cecac03":"The results are the following:\n\nThere are 14 pairs of training features potentially correlated\n* SaleType_E y SaleCondition_E : 0.9210433865680396\n* Exterior2nd_E y Exterior1st_E : 0.9116839166782535\n* Has2ndFloor y 2ndFlrSF : 0.9064688408171122\n* HasFireplace y Fireplaces : 0.8996251260698431\n* GarageCars y GarageArea : 0.8898902241956981\n* TotalSF y GrLivArea : 0.8717698431418889\n* FireplaceQu_E y HasFireplace : 0.8693916712558053\n* YearBuilt_E y GarageYrBlt_E : 0.8499587140172883\n* TotalSF_HQ y TotalSF : 0.8408043871234457\n* BsmtFinType2_E y HasBsmt : 0.8291587577005163\n* TotalSF y TotalBsmtSF : 0.8271178158193125\n* GarageCond_E y HasGarage : 0.8206178473298337\n* GarageQual_E y GarageCond_E : 0.8108106519177948\n* TotRmsAbvGrd y GrLivArea : 0.8083544205418542\n","f653384b":"***Make predictions and blend them***\n\nUp to this point, the models are defined, so I just have to fit them using the training set and make the predictions of the data in the test set. I blend the results of the different models. I did not find a way to fine tune the weights to apply to each of the predictions of each of the models. I have chosen them based on what I read in other kernels and the results I measured for each model in the previous step. There could probably be better combinations. ","fd9fc46e":"It is also important to make sure that the variable we want to predict is normally distributed. I apply the log to it. Some kernels show that the Johnson distribution adapts better to the initial distribution of the SalePrice, and should make a better job normalizing it, but I got a bit better results using the log.","6de1964f":"***Normalize features distributions***\n\nThe second step regarding the features values affects their statistical distribution. The more the distributions of the features look like a normal\/Gaussian distribution, the better the predictions will be, so I check the skewness of all the final features, and those above a threshold are normalized using the boxcox function. Logarithm is an alternative used in many kernels, but the boxcox function seemed to provide slighlty better results.","6aa4061a":"First of all, I have to say that I am a newbie. I am learning (or re-learning, since I studied a bit of it at the University... 20 years ago) Machine Learning, and enjoying very much kaggle while doing so.\n\nMy kernel has very few original things. It is essentially a work of going through a lot of interesting kernels and trying to get the best from each of them and put it all together. Most things are a compendium of all the techniques I have seen while studying available kernels on the competition. I have extracted things, and owe them almost all my result, from:\n* [https:\/\/www.kaggle.com\/gunesevitan\/in-depth-eda-and-stacking-with-house-prices](http:\/\/)\n* [https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso](http:\/\/)\n* [https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard](http:\/\/)\n* [https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset](http:\/\/)\n* [https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models](http:\/\/)\n* [https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python](http:\/\/)\n* [https:\/\/www.kaggle.com\/yw6916\/top-9-eda-stacking-ensemble](http:\/\/)\n* [https:\/\/www.kaggle.com\/nwhidden\/ames-house-prices-regression-practice](http:\/\/)\n* [https:\/\/www.kaggle.com\/chiranjeevbit\/complete-eda-and-predict-house-price](http:\/\/)\n\nAnd I also upvoted every one of them, of course!\n\nI tried to achieve two things with the kernel:\n1. To combine every interesting technique I learnt. This is dangerous, since it could happen that mathematically combining some of them may not make much sense. I tried to validate the result by measuring the scores on every change, since I still do not understand the implications of every parameter out there.\n2. To generalize, where possible, the techniques to make them domain independent. Getting a fully generic algorithm is quite hard, but the kernel could serve as a good code base for any other regression problem.\n\nPlease, any feedback in the comments is more than welcome. ","8ceb661d":"***Generate new features based on existing ones***\n\nNow I generate some additional features based on the existing features. As the comment in the code says, this is another of the domain specific parts, that needs a careful study of the feature set, some domain knowledge and some common sense. \n\nI have generated two sets of new features. The first set, refers to summaries of the most important features of the houses (total square feet, total square feet with good quality, total baths, total porch area and overall rating), something that every buyer would like to know about any house. The second set is a set of flags that denote if the house has some \"complementary\" features such as pool, second floor, garage, etc. Somehow this information is already present in the data (i.e., PoolArea = 0 means no pool), but this way I make it explicit.","f618098d":"I suppose it would be possible to automatically decide which features to drop, but in this case I decided to do it manually. I chose the minimum amount of features that would eliminate the maximum amount of the highest correlated features, and only for those features that clearly representing the same concept, such GarageCArs and GarageArea. If unsure, I decided to maintain the features in the dataset.","b5280a41":"**First steps**\n\nFirst of all, load the data sets, both the train and the test. I create one joint collection, called *all_data*, so that all the modifications to the features I will do will be applied to both the train and the test set at the same time.","4b8a2603":"With the threshold I chose (0.60), these are the features I used to create the polinomical features.\n\nThere are 16 features potentially correlated with SalePrice\n* TotalSF: 0.8149837586936304\n* OverallQual: 0.8098285862017292\n* Neighborhood_E: 0.7557789170655119\n* GrLivArea: 0.7313095834659141\n* TotalSF_HQ: 0.7071092354480142\n* TotalBath: 0.7037310128090049\n* GarageCars: 0.6907109670497433\n* ExterQual_E: 0.6840137963904297\n* BsmtQual_E: 0.678026253071665\n* KitchenQual_E: 0.6728485475386916\n* YearBuilt: 0.6526815462850586\n* FullBath: 0.6359570562496957\n* GarageYrBlt: 0.6340952202911952\n* GarageFinish_E: 0.6339736230180727\n* TotalBsmtSF: 0.6027254448924095\n\n","b70666c4":"**Introduction**","01a06451":"***Drop features that are not informative***\n\nIt is necessary to keep working on the data set with the features. \n\nFirst, I drop the features that have a predominant value (over 98% of the cases), because they are of little use to predict any price. It results in dropping the following features:\n\nVariables that have a very predominant value. Check if could be candidates to drop:\n* Variable: 3SsnPorch: value \"0\" appears 2882 times, 98.7324426173347% of the examples\n* Variable: LowQualFinSF: value \"0\" appears 2879 times, 98.6296676944159% of the examples\n* Variable: PoolArea: value \"0\" appears 2906 times, 99.55464200068516% of the examples\n* Variable: Condition2: value \"Norm\" appears 2889 times, 98.97225077081193% of the examples\n* Variable: Heating: value \"GasA\" appears 2874 times, 98.45837615621788% of the examples\n* Variable: RoofMatl: value \"CompShg\" appears 2876 times, 98.52689277149709% of the examples\n* Variable: Street: value \"Pave\" appears 2907 times, 99.58890030832477% of the examples\n* Variable: Utilities: value \"AllPub\" appears 2918 times, 99.9657416923604% of the examples\n* Variable: HasPool: value \"0\" appears 2906 times, 99.55464200068516% of the examples"}}