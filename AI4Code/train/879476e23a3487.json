{"cell_type":{"a10d05b1":"code","c8fc0175":"code","b7178267":"code","8ea6c7d7":"code","c62b206f":"code","127edf68":"code","44405dd2":"code","83e73dc4":"code","f4b55fc5":"code","0b566cc8":"code","ffa2a04b":"code","627d1a5a":"code","15332c76":"code","256620a0":"code","0ca0dfa5":"code","f0fb8d4f":"code","42a4b725":"code","fbd4d726":"code","9b7466b1":"code","c5ebc4a8":"markdown","8954a3ad":"markdown","013b4bfb":"markdown","4d8d5cd1":"markdown","2542a16a":"markdown","90c391c4":"markdown","45aa6beb":"markdown"},"source":{"a10d05b1":"import warnings\nwarnings.simplefilter(action=\"ignore\")\n\nimport datetime\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\"ggplot\")\n\n# Seed Everything\nseed = 123\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)","c8fc0175":"def read_data(train_path, test_path):\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    # Set date to the index\n    train['date'] = pd.to_datetime(train['date'])\n    train.set_index([\"date\"], inplace=True)\n\n    test['date'] = pd.to_datetime(test['date'])\n    test.set_index([\"date\"], inplace=True)\n    \n    return train, test","b7178267":"def get_date_features(df):\n    \n    # Add date-specific features\n    df['dayofweek'] = df.index.dayofweek\n    df['is_weekend'] = df.index.dayofweek \/\/ 5\n    df['day'] = df.index.day\n    df['month'] = df.index.month\n    df['year'] = df.index.year\n    df['dayofyear'] = df.index.dayofyear\n    df['weekofyear'] = df.index.weekofyear\n\n    return df\n\n\ndef generate_features(train, test):\n\n    # Get the time delta between last test and last train observations\n    model_delta = max(test.index) - max(train.index)\n\n    # Initialize variables\n    lags = [model_delta.days]\n    window = 7\n    lag_features = []\n\n    # Concatenate train and test together\n    data = pd.concat([train, test], sort=False)\n\n    for lag in lags:\n        feat = data.groupby([\"store\", \"item\"])[[\"sales\"]].rolling(window=window).mean().shift(lag)\n        feat.columns = ['sales_mean_lag_{}'.format(lag)]\n        lag_features.append(feat)\n\n        feat = data.groupby([\"store\", \"item\"])[[\"sales\"]].rolling(window=window).std().shift(lag)\n        feat.columns = ['sales_std_lag_{}'.format(lag)]\n        lag_features.append(feat)\n\n    # Concatenate all features together\n    lag_features = pd.concat(lag_features, axis=1)\n\n    # Add date features to the train and test\n    train = get_date_features(train)\n    test = get_date_features(test)\n\n    # Join lag features to the train and test by (store, item, date)\n    train.set_index([\"store\", \"item\"], append=True, inplace=True)\n    train = train.reorder_levels([\"store\", \"item\", \"date\"])\n    test.set_index([\"store\", \"item\"], append=True, inplace=True)\n    test = test.reorder_levels([\"store\", \"item\", \"date\"])\n\n    train = train.join(lag_features)\n    test = test.join(lag_features)\n\n    # Resetting index back\n    train.reset_index(level=[0, 1], inplace=True)\n    test.reset_index(level=[0, 1], inplace=True)\n    \n    return train, test, model_delta, window","8ea6c7d7":"# Read data\ntrain, test = read_data(train_path=\"..\/input\/demand-forecasting-kernels-only\/train.csv\",\n                        test_path=\"..\/input\/demand-forecasting-kernels-only\/test.csv\")","c62b206f":"train.head()","127edf68":"test.head()","44405dd2":"# Look at the data\nplt.figure(figsize=(12, 8))\nplt.plot(train[(train.store==1) & (train.item==1)][\"sales\"].diff())\nplt.title(\"Train data for store 1; item 1\")\nplt.show()","83e73dc4":"# There are multiple approaches for multi-step time series predictions: https:\/\/arxiv.org\/pdf\/1108.3259.pdf\n\n# Generate some features\ntrain, test, model_delta, window = generate_features(train, test)","f4b55fc5":"train.head()","0b566cc8":"# Train start date\nstart = min(train.index) + model_delta + datetime.timedelta(days=window)\n\n# Validation start date\nsplit_date = max(train.index) - model_delta\n\n# Keep only non-NA values in the train data\ntrain = train[train.index >= start]\n\n# Train-Validation split\nval_train = train[train.index < split_date]\nvalidation = train[train.index > split_date]","ffa2a04b":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 2\n    diff = 100 * np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    \n    return np.nanmean(diff)\n\n\ndef smape_lightgbm(y_pred, y_true):\n    return \"smape\", smape(y_true, y_pred), False","627d1a5a":"# Select all the features except for the target variable\nfeatures = [x for x in train.columns if x != \"sales\"]\n\nparams = {\n    \"learning_rate\": 0.1,\n    \"objective\": \"regression\",\n    \"metric\": \"None\",\n    \"n_estimators\": 1000,\n    \"colsample_bytree\": 0.9,\n    \"num_leaves\": 32,\n    \"subsample\": 0.8,\n    \"subsample_freq\": 1,\n    \"lambda_l2\": 1,\n}\n\n# Train LightGBM model\nreg = lgb.LGBMRegressor(**params)\nreg = reg.fit(\n    val_train[features],\n    val_train[\"sales\"],\n    eval_set=[(validation[features], validation[\"sales\"])],\n    verbose=100,\n    early_stopping_rounds=200,\n    eval_metric=smape_lightgbm,\n)","15332c76":"predictions = reg.predict(validation[features])\nvalidation['pred'] = predictions\nprint(\"Validation SMAPE: {:.3f}\".format(smape(validation[\"sales\"], validation[\"pred\"])))","256620a0":"plt.figure(figsize=(12, 8))\n\nto_plot = validation[(validation.store==1) & (validation.item==1)]\n\nplt.plot(to_plot[\"sales\"], alpha=0.8)\nplt.plot(to_plot[\"pred\"], linestyle=\"--\")\nplt.title(\"True values vs Predicted values for store 1; item 1\")\nplt.legend([\"TRUE VALUES\", \"PREDICTED VALUES\"])\nplt.xticks(None)\nplt.show()","0ca0dfa5":"# Re-train the model on the whole train data\nparams['n_estimators'] = reg.best_iteration_\n\n# Train LightGBM model\nreg = lgb.LGBMRegressor(**params)\nreg = reg.fit(\n    train[features],\n    train[\"sales\"],\n)","f0fb8d4f":"feature_importances = pd.DataFrame(\n    {\"feature\": val_train[features].columns,\n     \"importance\": reg.feature_importances_}\n)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(\n    data=feature_importances.sort_values(\"importance\", ascending=False).head(10),\n    x=\"importance\",\n    y=\"feature\",\n)\nplt.yticks(fontsize=14)\nplt.title(\"Top 10 features\")\nplt.show()","42a4b725":"test[\"sales\"] = reg.predict(test[features])","fbd4d726":"plt.figure(figsize=(12, 8))\nplt.plot(train[(train.store==1) & (train.item==1)][[\"sales\"]])\nplt.plot(test[(test.store==1) & (test.item==1)][[\"sales\"]])\nplt.title(\"Train data and Test predictions for store 1; item 1\")\nplt.legend([\"TRAIN\", \"TEST\"])\nplt.show()","9b7466b1":"test[[\"id\", \"sales\"]].to_csv(\"submission.csv\", index=False)","c5ebc4a8":"### Make test predictions","8954a3ad":"### Read data and preprocess date feature","013b4bfb":"### Evaluate model","4d8d5cd1":"### Machine Learning pipeline","2542a16a":"### SMAPE metric","90c391c4":"### Train a model","45aa6beb":"![smape](https:\/\/im0-tub-ru.yandex.net\/i?id=2933600500955e25964ec47bc5aebcf0&n=13)"}}