{"cell_type":{"4079d719":"code","2f34f4bb":"code","92d48aee":"code","69c3e09a":"code","c1847fc4":"code","cad4b9b1":"code","a0d3a27c":"code","4f194032":"code","413e1cbd":"code","b54f75da":"code","97951cf4":"code","c2812255":"code","14fe7d7c":"code","472d63f8":"code","cda1e3d9":"code","f297d6a7":"code","081b6f87":"code","de2adcd3":"code","b6a40c44":"code","03f63da6":"code","1fe0b843":"code","6df76306":"code","80bec717":"code","31678e31":"code","9a3d5311":"code","957c7348":"code","8ad7faa1":"code","ba62161b":"code","ed1a719b":"code","f4870826":"code","748fd012":"code","ae99e1f6":"code","affb41e7":"code","a6f4382d":"code","16dec13b":"code","d798a181":"code","0f067ea8":"code","7d2e8c33":"code","cb7f95c6":"code","224a3e3d":"code","1b9f7f01":"code","d6f96e9c":"code","f391c3a8":"code","c915c75d":"code","ad0ec448":"code","b49cbb4c":"code","04007053":"code","dd1d650c":"code","e8d3f7de":"code","48749fb7":"code","901b1865":"code","b29afa25":"code","3ac63643":"code","7196080e":"markdown","0e2bcf97":"markdown","25446533":"markdown","8ef6ea6a":"markdown","2e33de98":"markdown","34658575":"markdown","5b87715b":"markdown","9642ad96":"markdown","02ce0c37":"markdown","35202296":"markdown","7194b3f1":"markdown","de6fa891":"markdown","fca89dc8":"markdown","0e86bd3f":"markdown","cb238341":"markdown","b8ef404e":"markdown","65ee6383":"markdown","5e5a52f1":"markdown","dd507413":"markdown","b1c1c7ff":"markdown","d331d8bd":"markdown","f1266688":"markdown","2e344591":"markdown","c77be377":"markdown","29adbaf4":"markdown","74d5feef":"markdown"},"source":{"4079d719":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom statsmodels.regression.linear_model import OLS\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nzomato = pd.read_csv(\"..\/input\/zomato.csv\", na_values = [\"-\", \"\"])\n# Making a copy of the data to work on\ndata = zomato.copy()","2f34f4bb":"data.shape\n# The dataset has 51717 rows and 17 columns","92d48aee":"data.info()\n# Each row represents a restaurant and each column is a feature of the restaurant","69c3e09a":"data.head(3)","c1847fc4":"data.tail(3)\n","cad4b9b1":"data[\"menu_item\"].value_counts()[:1]","a0d3a27c":"data.isnull().sum()","4f194032":"data.address[1]","413e1cbd":"# Renaming and removing commas in the cost column \ndata = data.rename({\"approx_cost(for two people)\": \"cost\"}, axis=1)\ndata[\"cost\"] = data[\"cost\"].replace(\",\", \"\", regex = True)","b54f75da":"# Converting numeric columns to their appropriate dtypes\ndata[[\"votes\", \"cost\"]] = data[[\"votes\", \"cost\"]].apply(pd.to_numeric)","97951cf4":"# Examining restaurant types in the column \"listed_in(type)\"\ndata[\"listed_in(type)\"].value_counts()","c2812255":"# Examining the top 20 restaurant types in the column \"rest_type\"\ndata[\"rest_type\"].value_counts()[:10]","14fe7d7c":"# Group and aggregate duplicate restaurants that are listed under multiple types in listed_in(type)\ngrouped = data.groupby([\"name\", \"address\"]).agg({\"listed_in(type)\" : list})\nnewdata = pd.merge(grouped, data, on = ([\"name\", \"address\"]))","472d63f8":"# Examine the duplicates\nnewdata.head(3)\n# The duplicates can be seen in column \"listed_in(type)_x\"","cda1e3d9":"# Drop rows which have duplicate information in \"name\", \"address\" and \"listed_in(type)_x\"\nnewdata[\"listed_in(type)_x\"] = newdata[\"listed_in(type)_x\"].astype(str) # converting unhashable list to a hashable type\nnewdata.drop_duplicates(subset = [\"name\", \"address\", \"listed_in(type)_x\"], inplace = True)\n","f297d6a7":"newdata.shape","081b6f87":"newdata.describe(include = \"all\")","de2adcd3":"# Converting the restaurant names to rownames \nnewdata.index = newdata[\"name\"]\n","b6a40c44":"# Identifying the top 10 cuisines in Bangalore?\npd.DataFrame(newdata.groupby([\"cuisines\"])[\"cuisines\"].agg(['count']).sort_values(\"count\", ascending = False)).head(10)\n","03f63da6":"# Dropping unnecessary columns\nnewdata.drop([\"name\", \"url\", \"phone\", \"listed_in(city)\", \"listed_in(type)_x\", \"address\", \"dish_liked\",  \"listed_in(type)_y\", \"menu_item\", \"cuisines\", \"reviews_list\"], axis = 1, inplace = True)\n","1fe0b843":"newdata.head(3)","6df76306":"# Converting restaurant ratings to a numeric variable\nnewdata[\"rating\"] = newdata[\"rate\"].str[:3] # Extracting the first three characters of each string in \"rate\"\nnewdata.drop(\"rate\", axis = 1, inplace = True)","80bec717":"# Recreating dataset without NEW restaurants\nnewdata = newdata[newdata.rating != \"NEW\"] ","31678e31":"newdata.isnull().sum()","9a3d5311":"newdata = newdata.dropna(subset = [\"rating\"])","957c7348":"newdata[\"rating\"] = pd.to_numeric(newdata[\"rating\"])","8ad7faa1":"# Plotting the distribution of restaurant ratings\nplt.figure(figsize = (10, 5))\nplt.hist(newdata.rating, bins = 20, color = \"r\")\nplt.show()\n","ba62161b":"# Plotting the distribution of locations\nplt.figure(figsize = (30, 20))\nax = sns.barplot(data = newdata, x = newdata.location.value_counts().index, y = newdata.location.value_counts())\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\") # to make the labels more readable\nplt.show()","ed1a719b":"# Printing restaurant value counts for the top 25 locations\nnewdata[\"location\"].value_counts()[:25]","f4870826":"# Top 5 locations with the highest ratings\n(pd.DataFrame(newdata.groupby(\"location\")[\"rating\"].mean())).sort_values(\"rating\", ascending = False).head(5)\n","748fd012":"# Top 5 most expensive locations (cost = cost for two)\n(pd.DataFrame(newdata.groupby(\"location\")[\"cost\"].mean())).sort_values(\"cost\", ascending = False).head(5)","ae99e1f6":"# Identifying the high rated fancy restaurants on Sankey Road\nnewdata[(newdata[\"location\"] == \"Sankey Road\") & (newdata[\"rating\"] >= 4 )]","affb41e7":"newdata[(newdata[\"location\"] == \"Lavelle Road\") & (newdata[\"rating\"] >= 4 )][:10]","a6f4382d":"# Visualizing the relationship between rating and cost\nplt.figure(figsize = (10, 5))\nplt.scatter(newdata.rating, newdata.cost)\nplt.show()","16dec13b":"# Separating the predictors and target\npredictors = newdata.drop(\"rating\", axis = 1)\ntarget = newdata[\"rating\"]","d798a181":"# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, random_state = 0, test_size = 0.3)","0f067ea8":"# Preprocessing the predictors\nnum_cols = [\"votes\", \"cost\"]\ncat_cols = [\"location\", \"rest_type\", \"online_order\", \"book_table\"]\n\nnum_imputer = SimpleImputer(strategy = \"median\") \n# Imputing numeric columns with the median (not mean because of the high variance)\nnum_imputed = num_imputer.fit_transform(X_train[num_cols])\nscaler = StandardScaler()\n# Scaling the numeric columns to have a mean of 0 and standard deviation of 1\nnum_preprocessed = pd.DataFrame(scaler.fit_transform(num_imputed), columns = num_cols)\n\ncat_imputer = SimpleImputer(strategy = \"most_frequent\")\n# Imputing categorical columns with the mode\ncat_imputed = pd.DataFrame(cat_imputer.fit_transform(X_train[cat_cols]), columns = cat_cols)\n# Dummifying the categorical columns\ncat_preprocessed = pd.DataFrame(pd.get_dummies(cat_imputed, prefix = cat_cols, drop_first = True))\n","7d2e8c33":"# Joining the numeric and categorical columns and checking their shape\npredictors = pd.concat([num_preprocessed, cat_preprocessed], axis=1)","cb7f95c6":"# Dropping the feature with a high VIF \npredictors.drop(\"rest_type_Quick Bites\", axis = 1, inplace = True)\npredictors.shape","224a3e3d":"Y = list(y_train)","1b9f7f01":"# Building an Ordinary Least Squares regression model\nimport statsmodels.api as sm\nX = sm.add_constant(predictors)\nols = sm.OLS(Y, X).fit()","d6f96e9c":"# Predicting on the train data\npred_train = np.around(ols.predict(X), 1)\npred_train[:5] # checking the first 5 predictions","f391c3a8":"# Preprocessing the test data and predicting on it\ntest_num_imputed = num_imputer.transform(X_test[num_cols])\ntest_num_preprocessed = pd.DataFrame(scaler.transform(test_num_imputed), columns = num_cols)\n\ntest_cat_imputed = pd.DataFrame(cat_imputer.transform(X_test[cat_cols]), columns = cat_cols)\ntest_cat_preprocessed = pd.DataFrame(pd.get_dummies(test_cat_imputed, prefix = cat_cols))\n\ntest_predictors = pd.concat([test_num_preprocessed, test_cat_preprocessed], axis=1)\ntest_predictors.drop(\"rest_type_Quick Bites\", axis = 1, inplace = True)\n\n# Accounting for missing columns in the test set caused by dummification\nmissing_cols = set(predictors) - set(test_predictors)\n# Adding missing columns to test set with default value equal to 0\nfor c in missing_cols:\n    test_predictors[c] = 0\n# Ensuring the order of column in the test set is in the same order than in train set\ntest_predictors = test_predictors[predictors.columns]\n\ntest_X = sm.add_constant(test_predictors)\ntest_Y = list(y_train)\n\n# Prediction\npred_test = np.around(ols.predict(test_X), 1)\npred_test[:5] # first five rating predictions","c915c75d":"mean_squared_error(y_train, pred_train)","ad0ec448":"mean_squared_error(y_test, pred_test)","b49cbb4c":"# Finding the Mean Absolute Percentage Error\ndef mape(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\nmape(y_train, pred_train)","04007053":"mape(y_test, pred_test)","dd1d650c":"# Printing the model summary\nols.summary()","e8d3f7de":"# Regression with XGBoost\nxgb = XGBRegressor(n_estimators = 100, max_depth = 8, gamma = 0.5, colsample_bytree = 0.8, random_state = 0)\nxgb.fit(predictors, y_train)\n\npred_train = xgb.predict(predictors)\npred_test = xgb.predict(test_predictors)","48749fb7":"mean_squared_error(y_train, pred_train)","901b1865":"mean_squared_error(y_test, pred_test)","b29afa25":"mape(y_train, pred_train)","3ac63643":"mape(y_test, pred_test)","7196080e":"Instead of representing a rating as 3.5\/5, we are now representing it as just 3.5.\n\nNext we will remove the \"NEW\" level from ratings as it is not predictable.","0e2bcf97":"These are the features we'll use to build our regression model.","25446533":"It appears both menu_item and dish_liked have over 50% of their data missing.","8ef6ea6a":"### Observations\n\n- After Whitefield, the maximum number of restaurants are in BTM, HSR, Marathahalli and Electronic City\n- Koramangala has been split blockwise or it would be at the top with the others\n\n\nLet's see which locations have the **highest rated** restaurants.","2e33de98":"### Observations\nInterestingly, restaurants rated between 4.5 and 5.0 are **cheaper** than those rated between 4.0 and 4.5.\n\nWe have explored and cleaned the dataset and can now apply preprocessing steps that are necessary for model-building.\n","34658575":"### Observations\n- The dataset contains missing values\n- Location information is captured more accurately by \"location\" than by \"listed_in(city)\"\n- There is some information overlap between rest_type and listed_in(type) \n- There's something strange about the menu_item column - let's take a closer look","5b87715b":"## Data preprocessing","9642ad96":"### Observations\n- The top two locations with high ratings are also the two most expensive locations (Sankey Road and Lavelle Road)\n- In general we can see that restaurants around the MG Road area are more expensive\n","02ce0c37":"Which locations are the **most expensive** to dine in?","35202296":"## Model Building","7194b3f1":"### Evaluation","de6fa891":"### Observation\n\nAdjusted R-squared is very low.\n\nLet's try Decision Tree-based regression with boosting.","fca89dc8":"39617 entries in the column are empty lists [] ","0e86bd3f":"## Observations\n- There are 8792 unique restaurant names, of which **Cafe Coffee Day** has the highest occurrence (54)\n- There are 93 unique locations in Bangalore of which **Whitefield** has the highest number of restaurants (885). Note that this is different from the original dataset's \"top\" location, BTM, which shows the importance of removing duplicates\n- The most common restaurant type is \"Quick Bites\" (5024 occurrences)\n- The most common listed type is Delivery (8715) \n- Biryani is the most popular dish, but we can't be sure about this as dish_liked is missing over half its data\n- There are 2609 unique levels in the cuisines column, this is because restaurants are categorised under many different combinations of cuisines\n- Average cost for two at Bangalore restaurants is Rs 487 and there is very **high variance** (standard deviation Rs 390)\n- Average number of votes per restaurant is 180 and here too there is **high variance**\n- Majority of restaurants allow online ordering but don't allow online table booking\n- NEW is the most common entry in the rating column - this represents unrated new restaurants. We will look at the ratings more closely later\n- Like menu_item, reviews_list also contains many empty lists (2511)\n","cb238341":"After building one regression model I had found that one feature (\"rest_type_Quick Bites\") had a high VIF of 12, indicating multicollinearity. We will drop this feature from our predictors.","b8ef404e":"We will drop rows that have missing values in the target variable. The remaining missing values in other features will be imputed later.","65ee6383":"The reduced dataset has 12499 restaurants - **a substantial reduction from 51717 !**","5e5a52f1":"Now we can convert ratings to a numeric column.","dd507413":"Almost all of them are located in one 5 star hotel!\n\nWhat about Lavelle Road?","b1c1c7ff":"This kernel explores the Zomato Bangalore dataset uploaded by Himanshu Poddar and attempts to predict restaurant ratings with regression. This is Part One of my three-part analysis, and uses only the numeric features and categorical features with <100 levels.\n\nThe kernel consists of:\n\n* Data cleaning (identifying and dropping duplicates, reformatting features)\n* Exploratory Data Analysis and observations\n* Data visualizations\n* Preprocessing and prediction with regression models\n* Model evaluation (MSE, MAPE, R^2) \n* Results summary","d331d8bd":"## Data visualizations","f1266688":"Zip codes are not included in the addresses. So they may not be useful for analysis but can be used to identify duplicate data.","2e344591":"### Results summary\n\nOLS linear regression predicted with approximately 8% Mean Absolute Percentage Error on the train and test sets, after checking for multicollinearity and dropping the feature with a high VIF (rest_type_Quick Bites). Adjusted R-squared was low (0.30), indicating that the model does not explain the variance in restaurant ratings, i.e. it is underfitting. \n\nWe then tried regression with XGBoost and experimented with hyperparameters. Here the train MAPE was 6.5% and test MAPE was 7%. \n\nOther types of models and feature transformation may improve performance. In Part Two of my analysis we will transform the ratings into a multi-class categorical feature and see if classification models are able to do better.","c77be377":"There is an information overlap between these two features and we can see that rest_type is more informative. Additionally, the uploader has mentioned there is duplication of data because many restaurants are categorised under multiple types in listed_in(type). We will handle this duplication before proceeding further.","29adbaf4":"Despite being a southern city, Bangalore has **more North Indian restaurants than South Indian**. Bangaloreans also really seem to love their biryani as no other dish has an entire cuisine category to itself.","74d5feef":"### Observations\n- **3.7 is the most common rating**, i.e. most Bangaloreans have above-average dining experiences when they go out. \n- There are very few ratings between 2 to 2.5 and 4.5 to 5, and hardly any under 2."}}