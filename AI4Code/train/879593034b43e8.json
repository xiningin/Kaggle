{"cell_type":{"bce8fb0e":"code","bd81c67e":"code","0d31b5f4":"code","0809c51a":"code","c452a63c":"code","5fcf225a":"code","779d8a28":"code","be6eaf42":"code","a0983a6c":"code","07ee76ce":"code","feca7c68":"code","bbe254dd":"code","21c061ed":"code","0c3c3eac":"code","fe06fff6":"code","ae9361e9":"code","0e90c957":"code","7efe951b":"code","d177c002":"code","84fac610":"code","b30706ed":"code","44ece8e5":"code","22f2db4b":"code","e131d645":"code","ca38aaa4":"code","e2799376":"code","406a4ba4":"code","7da7a1eb":"code","97db7ec5":"code","8330829f":"code","e886385b":"code","1e928b5c":"code","1d7081cf":"code","82051673":"code","c7e4d20f":"code","b597dbab":"code","83c66f09":"code","eda27725":"code","dc444abb":"code","7add8460":"code","ceb9e624":"code","0ade7e2b":"code","49f01689":"code","f09e9aaf":"code","e23d572e":"markdown","1b859844":"markdown","c9311f24":"markdown","c3e1b429":"markdown","7d631b28":"markdown","e8e1a4c1":"markdown","f2f24f9e":"markdown"},"source":{"bce8fb0e":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string\n# import tensorflow_hub as hub\n# import tensorflow_text\nfrom tqdm import tqdm","bd81c67e":"df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf.head()","0d31b5f4":"df.info()","0809c51a":"df.drop(['keyword','location','id'],axis='columns',inplace=True)","c452a63c":"sns.countplot(df['target'], palette=\"plasma\")\nfig = plt.gcf()\nfig.set_size_inches(5,5)\nplt.title('target')","5fcf225a":"df.shape","779d8a28":"df.head()","be6eaf42":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n     \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","a0983a6c":"def text_cleaner(text):\n    temp = text.lower()\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    temp = url_pattern.sub(r'', temp)\n    html=re.compile(r'<[^>]+>')\n    temp = html.sub(r'',temp)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    temp = emoji_pattern.sub(r'', temp)\n    \n    table=str.maketrans('','',string.punctuation)\n    temp = temp.translate(table)\n    \n    tem=[]\n    for word in temp.split():\n        if word in abbreviations.keys():\n            tem.append(abbreviations[word])\n        else:\n            tem.append(word)\n    temp=tem\n    \n    stop_words = set(stopwords.words('english'))\n    le=WordNetLemmatizer()\n    tokens = [le.lemmatize(word) for word in temp if not word in stop_words]\n    \n    long_words=[]\n    for i in tokens:\n        if len(i)>1:                                            \n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()","07ee76ce":"df['text'] = df['text'].apply(lambda text: text_cleaner(text))\ndf.head()","feca7c68":"df.replace('', np.nan, inplace=True)\ndf.dropna(axis='rows',inplace=True)","bbe254dd":"def leng(text):\n    return len(text.split())","21c061ed":"df['text_length']=df['text'].apply(lambda text: leng(text))\nplt.boxplot(df['text_length'])\nplt.show()","0c3c3eac":"maximum = df['text_length'].max()","fe06fff6":"df_mislabeled = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nindex_mislabeled = df_mislabeled.index.tolist()\n\nlength = len(index_mislabeled)\nprint(length)","ae9361e9":"df_mistext_check_before = df[df['text'].isin(index_mislabeled)].sort_values(by = 'text')\ndf_mistext_check_before.head(10)","0e90c957":"majority_df = df_mistext_check_before.groupby(['text'])['target'].mean()#apply(lambda x: x.mode())","7efe951b":"majority_df","d177c002":"def relabel(row, majority_index):\n    ind = ''\n    if row['text'] in majority_index:\n        ind = row['text']\n        if majority_df[ind] < 0.5:\n            return 0\n        else:\n            return 1\n    else:\n        return row['target']\n","84fac610":"# df['target'] = df[df['text'].isin(majority_df.index.tolist())].apply(lambda row['target']: majority_df[row['text']]) \ndf['target'] = df.apply( lambda row: relabel(row, majority_df.index), axis = 1)\n# df['target']=df['target'].apply(lambda row : int(row))","b30706ed":"df_mislabeled = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nindex_mislabeled = df_mislabeled.index.tolist()\n\nlength = len(index_mislabeled)\nprint(length)","44ece8e5":"df.info()","22f2db4b":"x_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>') \nx_tokenizer.fit_on_texts(list(df['text']))","e131d645":"df['sequences']=x_tokenizer.texts_to_sequences(df['text'])","ca38aaa4":"df['sequences'].iloc[0]","e2799376":"from sklearn.model_selection import train_test_split\nx_train,x_valid,y_train,y_valid=train_test_split(df['sequences'],df['target'].values,test_size=0.2,shuffle=True) ","406a4ba4":"x_train   =   tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=maximum, padding='post')\nx_valid   =   tf.keras.preprocessing.sequence.pad_sequences(x_valid,maxlen=maximum, padding='post')\n\nx_voc   =  len(x_tokenizer.word_index) + 1 ","7da7a1eb":"x_word_index = x_tokenizer.word_index","97db7ec5":"len(x_word_index)","8330829f":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\n    -O \/tmp\/glove.6B.100d.txt","e886385b":"embeddings_index = {};\nwith open('\/tmp\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((x_voc+1, 100));\nfor word, i in x_word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","1e928b5c":"embeddings_matrix.shape","1d7081cf":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(x_voc+1, 100, input_length=maximum, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences = False)),\n    tf.keras.layers.Dense(20, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 20\n\nhistory = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_valid, y_valid), verbose=2)","82051673":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.show()","c7e4d20f":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Test'])\nplt.show()","b597dbab":"test_df.drop(['location','id','keyword'],axis='columns',inplace=True)","83c66f09":"test_df.head()","eda27725":"test_df['text'] = test_df['text'].apply(lambda text: text_cleaner(text))","dc444abb":"test_df.head()","7add8460":"test_seq = x_tokenizer.texts_to_sequences(test_df['text'])\ntest_seq = tf.keras.preprocessing.sequence.pad_sequences(test_seq,maxlen=maximum, padding='post')","ceb9e624":"preds=model.predict(test_seq)\npreds=[int(round(x[0])) for x in preds]\nsamp=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsamp[\"target\"]=preds\nsamp[\"target\"].value_counts()","0ade7e2b":"samp.to_csv(\"outputGlove.csv\",index=False)","49f01689":"print(test_df.iloc[-15])","f09e9aaf":"samp['target'].iloc[-15]","e23d572e":"## Text Preprocessing","1b859844":"## Relabeling mislabeled tweets","c9311f24":"We will relabel them according to mean of the labels of the same tweet.","c3e1b429":"There are 73 texts that are mislabeled.","7d631b28":"After relabeling, we can see that there are no mislabeled tweets.","e8e1a4c1":"#### Drop empty texts","f2f24f9e":"## Glove Vector Embeddings"}}