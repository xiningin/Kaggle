{"cell_type":{"3cae093e":"code","2cdfa3aa":"code","fd55bea0":"code","34867089":"code","74b17ea3":"code","b89ae3ea":"code","7e815660":"code","8835ea2c":"code","7c504207":"code","53896d59":"code","c6b7cb64":"code","0d143e3c":"code","e0c832d8":"code","645c01da":"code","f29e287c":"code","5c6f94cf":"code","8cef4635":"code","32719c10":"code","9768f33d":"code","4f8a4d3a":"code","ef064dd6":"code","a60959e8":"code","da6deb59":"code","a05972f9":"code","21fbfa49":"code","ee506d8e":"code","dadbacb8":"code","0a0ec9f1":"code","95f0e345":"code","de16078e":"code","2f7b2247":"code","da3fa54d":"code","9e3c2187":"code","81bd573d":"code","a0a26996":"code","dfb33bdf":"code","1764b214":"code","4ce81ac3":"code","1e3f3a98":"markdown","cb68c2b2":"markdown","1cb69bfe":"markdown","870fdd65":"markdown","cb25134c":"markdown","4b2d3272":"markdown","e6a55645":"markdown","f30a2772":"markdown","5d7277f0":"markdown","a30c87a3":"markdown","903db1ce":"markdown","ac1a3898":"markdown","d3e3a965":"markdown","afd2af41":"markdown","c3be5a5f":"markdown","c310e973":"markdown","594f29ff":"markdown","8f960f53":"markdown","a4e4be38":"markdown","4142dcf0":"markdown","e63072e3":"markdown","e9262c95":"markdown","e1ee403e":"markdown","0d334ca5":"markdown","07e69511":"markdown","59e4459c":"markdown","8b0aa771":"markdown"},"source":{"3cae093e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2cdfa3aa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import recall_score, precision_score, classification_report,accuracy_score,confusion_matrix, roc_curve, auc, roc_curve,accuracy_score,plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom scipy import ndimage\nimport seaborn as sns","fd55bea0":"#Load dataset\nTrain_Data = pd.read_csv(\"..\/input\/gene-expression\/data_set_ALL_AML_train.csv\")\nTest_Data = pd.read_csv(\"..\/input\/gene-expression\/data_set_ALL_AML_independent.csv\")\nlabels = pd.read_csv(\"..\/input\/gene-expression\/actual.csv\", index_col = 'patient')","34867089":"Train_Data.head()","74b17ea3":"#check for nulls\nprint(Train_Data.isna().sum().max())\nprint(Test_Data.isna().sum().max())","b89ae3ea":"#drop 'call' columns\ncols = [col for col in Test_Data.columns if 'call' in col]\ntest = Test_Data.drop(cols, 1)\ncols = [col for col in Train_Data.columns if 'call' in col]\ntrain = Train_Data.drop(cols, 1)","7e815660":"#Join all the data\npatients = [str(i) for i in range(1, 73, 1)]\ndf_all = pd.concat([train, test], axis = 1)[patients]","8835ea2c":"#transpose rows and columns\ndf_all = df_all.T","7c504207":"df_all[\"patient\"] = pd.to_numeric(patients)\nlabels[\"cancer\"]= pd.get_dummies(labels.cancer, drop_first=True)\n# add the cancer column to train data\n\nData = pd.merge(df_all, labels, on=\"patient\")","53896d59":"Data.head()","c6b7cb64":"Data['cancer'].value_counts()","0d143e3c":"plt.figure(figsize=(4,8))\ncolors = [\"AML\", \"ALL\"]\nsns.countplot('cancer', data=Data, palette = \"Set1\")\nplt.title('Class Distributions \\n (0: AML || 1: ALL)', fontsize=14)","e0c832d8":"#X -> matrix of independent variable\n#y -> vector of dependent variable\nX, y = Data.drop(columns=[\"cancer\"]), Data[\"cancer\"]","645c01da":"X","f29e287c":"y","5c6f94cf":"#split the dataset\nX_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.25, random_state= 0)","8cef4635":"#before feature scaling\n#X_train = pd.DataFrame(X_train)\n#X_train.plot(kind=\"kde\", legend=None)","32719c10":"#feature scaling\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","9768f33d":"X_train.shape","4f8a4d3a":"#after feature scaling\n#X_train = pd.DataFrame(X_train)\n#X_train.plot(kind=\"kde\", legend=None)","ef064dd6":"pca = PCA()\npca.fit_transform(X_train)\n\ntotal = sum(pca.explained_variance_)\nk = 0\ncurrent_variance = 0\nwhile current_variance\/total < 0.90:\n    current_variance += pca.explained_variance_[k]\n    k = k + 1\n    \nprint(k, \" features explain around 90% of the variance. From 7129 features to \", k, \", not too bad.\", sep='')\n\npca = PCA(n_components=k)\nX_train_pca = pca.fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nvar_exp = pca.explained_variance_ratio_.cumsum()\nvar_exp = var_exp*100\nplt.bar(range(k), var_exp,color = 'r')","a60959e8":"pca.n_components_ ","da6deb59":"from mpl_toolkits.mplot3d import Axes3D\n\npca3 = PCA(n_components=3).fit(X_train)\nX_train_reduced = pca3.transform(X_train)\n\nplt.clf()\nfig = plt.figure(1, figsize=(10,6))\nax = Axes3D(fig, elev=-150, azim=110,)\nax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:, 2], c = y_train, cmap='coolwarm', linewidths=10)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])","a05972f9":"from sklearn.utils import resample\nfrom collections import Counter\n\nprint(\"Before Upsampling:-\")\nprint(Counter(y_train))\n\n\n# Let's use SMOTE to oversample\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train_ov, y_train_ov = oversample.fit_resample(X_train_pca,y_train)\n\nprint(\"After Upsampling:-\")\nprint(Counter(y_train_ov))","21fbfa49":"# do a grid search\nsvc_params = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n\nsearch = GridSearchCV(SVC(), svc_params, n_jobs=-1, verbose=1)\nsearch.fit(X_train_ov, y_train_ov)\n\nbest_accuracy = search.best_score_ #to get best score\nbest_parameters = search.best_params_ #to get best parameters\n# select best svc\nbest_svc = search.best_estimator_\nbest_svc","ee506d8e":"#build SVM model with best parameters\nsvc_model = SVC(C=1, kernel='linear',probability=True)\n\nsvc_model.fit(X_train_ov, y_train_ov)\n\nprediction=svc_model.predict(X_test_pca)\n\nacc_svc = accuracy_score(prediction,y_test)\nprint('The accuracy of SVM is', acc_svc)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True, cmap='Greens', fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = svc_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","dadbacb8":"knn_param = {\n    \"n_neighbors\": [i for i in range(1,30,5)],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"leaf_size\": [1, 10, 30],\n    \"p\": [1,2]\n}\nsearch = GridSearchCV(KNeighborsClassifier(), knn_param, n_jobs=-1, verbose=1)\nsearch.fit(X_train_ov, y_train_ov)\n\nbest_accuracy = search.best_score_ #to get best score\nbest_parameters = search.best_params_ #to get best parameters\n# select best svc\nbest_knn = search.best_estimator_\nbest_knn","0a0ec9f1":"knn_model = KNeighborsClassifier(algorithm='ball_tree', leaf_size=1, n_neighbors=6,\n                     weights='distance')\n\nknn_model.fit(X_train_ov,y_train_ov)\nprediction=knn_model.predict(X_test_pca)\n\nacc_knn = accuracy_score(prediction,y_test)\nprint('The accuracy of K-NN is', acc_knn)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True, cmap='Greens', fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = knn_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","95f0e345":"log_grid = {'C': [1e-03, 1e-2, 1e-1, 1, 10], \n                 'penalty': ['l1', 'l2']}\n\nlog_model = GridSearchCV(estimator=LogisticRegression(solver='liblinear'), \n                  param_grid=log_grid, \n                  cv=3,\n                  scoring='accuracy')\nlog_model.fit(X_train_ov, y_train_ov)\n\n\nbest_accuracy = log_model.best_score_ #to get best score\nbest_parameters = log_model.best_params_ #to get best parameters\n# select best svc\nbest_lr = log_model.best_estimator_\nbest_lr","de16078e":"#Logistic Regression\nlr_model = LogisticRegression(C=0.001, solver='liblinear')\n\nlr_model.fit(X_train_ov,y_train_ov)\n\nprediction=lr_model.predict(X_test_pca)\n\nacc_log = accuracy_score(prediction,y_test)\nprint('Validation accuracy of Logistic Regression is', acc_log)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"Greens\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = lr_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","2f7b2247":"params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4, 5, 6], 'max_depth':[3,4,5,6,7,8]}\ndecision_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n\ndecision_search.fit(X_train_ov, y_train_ov)\n\n\nbest_accuracy = decision_search.best_score_ #to get best score\nbest_parameters = decision_search.best_params_ #to get best parameters\n# select best svc\nbest_ds = decision_search.best_estimator_\nbest_ds","da3fa54d":"#Decision Tree\nds_model = DecisionTreeClassifier(max_depth=3, max_leaf_nodes=3, random_state=42)\n\nds_model.fit(X_train_ov,y_train_ov)\n\nprediction=ds_model.predict(X_test_pca)\n\nacc_decision_tree = accuracy_score(prediction,y_test)\nprint('Validation accuracy of Decision Tree is', acc_decision_tree)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"Greens\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = ds_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","9e3c2187":"# Hyperparameters search grid \nrf_param_grid = {'bootstrap': [False, True],\n         'n_estimators': [60, 70, 80, 90, 100],\n         'max_features': [0.6, 0.65, 0.7, 0.75, 0.8],\n         'min_samples_leaf': [8, 10, 12, 14],\n         'min_samples_split': [3, 5, 7]\n        }\n\n# Create the GridSearchCV object\nrf_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_param_grid, cv=3, scoring='accuracy')\nrf_search.fit(X_train_ov, y_train_ov)\n\nbest_accuracy = rf_search.best_score_ #to get best score\nbest_parameters = rf_search.best_params_ #to get best parameters\n# select best svc\nbest_rf = rf_search.best_estimator_\nbest_rf","81bd573d":"#Random forest\nrf_model = RandomForestClassifier(bootstrap=False, max_features=0.6, min_samples_leaf=8,\n                       min_samples_split=3, n_estimators=70)\n\nrf_model.fit(X_train_ov,y_train_ov)\n\nprediction=rf_model.predict(X_test_pca)\n\nacc_random_forest = accuracy_score(prediction,y_test)\nprint('Validation accuracy of RandomForest Classifier is', acc_random_forest)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"Greens\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = rf_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","a0a26996":"xgb_grid_params = {'max_depth': [3, 4, 5, 6, 7, 8, 10, 12],\n               'min_child_weight': [1, 2, 4, 6, 8, 10, 12, 15],\n               'n_estimators': [40, 50, 60, 70, 80, 90, 100, 110, 120, 130],\n               'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]}\n\n# Create the GridSearchCV object\nxgb_search = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=xgb_grid_params, cv=3, scoring='accuracy')\nxgb_search.fit(X_train_ov, y_train_ov)\n\nbest_accuracy = xgb_search.best_score_ #to get best score\nbest_parameters = xgb_search.best_params_ #to get best parameters\n# select best svc\nbest_xgb = xgb_search.best_estimator_\nbest_xgb","dfb33bdf":"#XB Boost\nxgb_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.001, max_delta_step=0, max_depth=3,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=40, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\nxgb_model.fit(X_train_ov,y_train_ov)\n\nprediction=xgb_model.predict(X_test_pca)\n\nacc_xgb = accuracy_score(prediction,y_test)\nprint('Validation accuracy of XG Boost is', acc_xgb)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"Greens\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = xgb_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","1764b214":"from sklearn.naive_bayes import GaussianNB\n#In case of naive Bayes, there isn't a hyper-parameter to tune, so you have nothing to grid search over.\nnb_model = GaussianNB()\n\nnb_model.fit(X_train_ov,y_train_ov)\n\nprediction=nb_model.predict(X_test_pca)\n\nacc_nb = accuracy_score(prediction,y_test)\nprint('Validation accuracy of Naive Bayes is', acc_nb)\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"Greens\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = nb_model.predict_proba(X_test_pca)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","4ce81ac3":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Decision Tree',\n              'Random Forest', 'XG Boost', 'Naive Bayes'],\n\n    'Score': [acc_svc, acc_knn, acc_log, acc_decision_tree, \n              acc_random_forest, acc_xgb, acc_nb]})\nmodels.sort_values(by='Score', ascending=False)","1e3f3a98":"### Drop 'call'","cb68c2b2":"# Classification of Cancer by Gene Expression","1cb69bfe":"### Hyperprameter optimization for Random forest","870fdd65":"### Hyperprameter optimization for Logistic regression","cb25134c":"### Check for nulls","4b2d3272":"### Dimentionality reduction using Principal Component Analysis(PCA)","e6a55645":"### Hyperprameter optimization for KNN","f30a2772":"### Balacing the lables using SMOTE","5d7277f0":"### Hyperprameter optimization for SVM","a30c87a3":"We can now rank our evaluation of all the models to choose the best one for our problem. ","903db1ce":"### Load all the required libraries","ac1a3898":"### Split the dataset","d3e3a965":"### Before feature scaling","afd2af41":"### 3D plot of first 3 principal components","c3be5a5f":"### Hyperprameter optimization for XGBoost","c310e973":"![1800ss_science_source_rm_aml_cancer_cells.jpg](attachment:1800ss_science_source_rm_aml_cancer_cells.jpg)","594f29ff":"### Encode the categorical columns","8f960f53":"We can clearly see that the labels are not balanced and we need to balance them before building our classification models.","a4e4be38":"### Hyperprameter optimization for Decision trees","4142dcf0":"https:\/\/www.kaggle.com\/crawford\/gene-expression","e63072e3":"### Transpose rows and columns","e9262c95":"### After feature scaling","e1ee403e":"### Model evaluation","0d334ca5":"### Feature scaling","07e69511":"The dataset showes how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).","59e4459c":"We can observe here Navive Bayes and Logistic regression gave us the maximum accuracy of 94.4% and 83.3% respectively while that of SVM, Descision tree, Random forest and XG Boost remains 72.2%. You can fine tune these models even further to get even better accuracy.","8b0aa771":"### Load dataset"}}