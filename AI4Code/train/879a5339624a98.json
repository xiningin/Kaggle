{"cell_type":{"bff52de4":"code","f379c8b2":"code","76b1cffd":"code","056016af":"code","7e9125cd":"code","92591862":"code","a4dda1db":"code","c85ab433":"code","ab00563a":"code","a0ba7fb4":"code","0eeef4c3":"code","9ace11bb":"code","2ec7a0a5":"code","033fcf7f":"code","4e8b477a":"code","ab93a1dc":"code","2b085331":"code","efd0d97e":"code","df4c8211":"code","a67a77a6":"code","84913e49":"code","390b84d6":"code","ddfdf6fa":"code","371ceb94":"code","cb23be07":"code","69b468cc":"code","8145935c":"code","775816d5":"code","e2f113d5":"code","af2d6bbe":"code","c7f8c1b6":"code","efada6f6":"code","3253d11f":"code","fd2fd7f3":"code","fa3eb6e6":"code","1cda9152":"code","04278f10":"markdown","7e65f731":"markdown","4632da98":"markdown","7fd0522e":"markdown","2d2bac41":"markdown","f54b0a8e":"markdown","6d636d6d":"markdown","83bf3d5a":"markdown","4a73aa3e":"markdown","9725c9af":"markdown","7c972189":"markdown","026d323f":"markdown","53aacac2":"markdown"},"source":{"bff52de4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plot\nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\n    \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\npath=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f379c8b2":"print(path)","76b1cffd":"data=pd.read_csv(path[1]) # Reserved df for other usage\ndata1=pd.read_csv(path[0]) \ndata2=pd.read_csv(path[1])","056016af":"print(data1.columns)\nprint(data1.shape)","7e9125cd":"print(data2.columns)\nprint(data2.shape)","92591862":"data2.info()","a4dda1db":"data2.describe(include=['O'])","c85ab433":"data2.describe()","ab00563a":"data2.head()","a0ba7fb4":"data2= data2.drop('Address',axis=1)","0eeef4c3":"data2.head()","9ace11bb":"data2['Rooms'].value_counts()","2ec7a0a5":"data2=data2[data2['Rooms']<=8]","033fcf7f":"sns.countplot(data2['Rooms'])","4e8b477a":"data2['Date']=pd.to_datetime(data2.Date)\ndata2['Month']=data2['Date'].dt.month\ndata2=data2.drop('Date',axis=1)\nsns.countplot(data2['Month'])","ab93a1dc":"data2['Type'].value_counts()","2b085331":"one_hot = pd.get_dummies(data2['Type'])\nsns.countplot(data2['Type'])\none_hot.columns= ['Type(h)','Type(t)','Type(u)']\ndata2 = data2.drop('Type',axis = 1)\ndata2 = data2.join(one_hot)","efd0d97e":"data2['Suburb'].value_counts()","df4c8211":"sns.countplot(data2['Regionname'])","a67a77a6":"data2['Postcode'].value_counts()","84913e49":"sns.countplot(data2['CouncilArea'])","390b84d6":"data2=data2.drop(['Suburb','CouncilArea', 'Postcode'],axis =1)","ddfdf6fa":"one_hot = pd.get_dummies(data2['Regionname'])\ndata2['Regionname'].unique()\none_hot.columns= ['Northern Metropolitan', 'Western Metropolitan',\n       'Southern Metropolitan', 'South-Eastern Metropolitan',\n       'Eastern Metropolitan', 'Northern Victoria', 'Western Victoria',\n       'Eastern Victoria']\ndata2 = data2.drop('Regionname',axis = 1)\ndata2 = data2.join(one_hot)","371ceb94":"one_hot = pd.get_dummies(data2['Method'])\nsns.countplot(data2['Method'])\none_hot.columns= ['Method(S)','Method(SP)','Method(PI)','Method(SN)','Method(VB)','Method(PN)','Method(SA)','Method(W)','Method(SS)']\ndata2 = data2.drop('Method',axis = 1)\ndata2 = data2.join(one_hot)","cb23be07":"seller=data2['SellerG'].value_counts()\nseller.unique()","69b468cc":"big=seller[seller.values>1000]\nsmall=seller[seller.values<=50]\nmedium=seller.loc[(seller.values<=1000) & (seller.values>50)]","8145935c":"big.index","775816d5":"soldby=[]\nfor each_seller in data2['SellerG']:\n    if each_seller in big.index:\n        soldby.append('big')\n    elif each_seller in small.index:\n        soldby.append('small')\n    else:\n        soldby.append('medium')\ndata2['Soldby']=soldby","e2f113d5":"one_hot = pd.get_dummies(data2['Soldby'])\nsns.countplot(data2['Soldby'])\none_hot.columns= ['big','medium','small']\ndata2 = data2.drop('SellerG',axis = 1)\ndata2 = data2.drop('Soldby',axis = 1)\ndata2 = data2.join(one_hot)","af2d6bbe":"data2.head()","c7f8c1b6":"data_dummy=data2\ndata_dummy.dropna(subset = [\"Price\"], inplace=True)\nX=data_dummy[[ 'Rooms', 'Propertycount', 'Distance']]\nY=data_dummy[['Price']]\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y)","efada6f6":"reg_model= LinearRegression()\ndummy_reg=reg_model.fit(X_train,Y_train)\nreg_model.score(X_test,Y_test)","3253d11f":"data_reg=data2\ndata_reg.dropna(subset = [\"Price\"], inplace=True)\nY_reg=data_reg[['Price']]\nX_reg=data_reg.drop('Price',axis=1)\nX_train,X_test,Y_train,Y_test=train_test_split(X_reg,Y_reg)","fd2fd7f3":"reg_LM= LinearRegression()\nreg=reg_LM.fit(X_train,Y_train)\nreg_LM.score(X_test,Y_test)","fa3eb6e6":"data2.shape","1cda9152":"for num in range (1,28):\n    pca_method=PCA(n_components=num)\n    X_PCA_train=pca_method.fit_transform(X_train)\n    X_PCA_test=pca_method.fit_transform(X_test)\n    reg_LM= LinearRegression()\n    reg=reg_LM.fit(X_PCA_train,Y_train)\n    score=reg_LM.score(X_PCA_test,Y_test)\n    print('R_Square for PCA at n_components= '+str(num)+' is '+str(score))","04278f10":"# Feature: Type\n* Since there are only 3 types of house (**h** - house,cottage,villa,semi,terrace; **u** - unit, duplex; **t** - townhouse) that is commonly sold, I choose to one hot encode this feauture directly without any need for preprocess.","7e65f731":"# Feature: Rooms\nWe can assume house that have more than 8 rooms as outliers and eliminate those.","4632da98":"# Feature: SellerG\n* This feature named the real estate agent that sold the house, which I feel is quite important to the price of the house since they are the one who makes the deal with the buyers.\n* Since there are sellers who could sold around 7000 houses a year and others who only sold 1. I think it is best to devide this feature into 3 groups: Big Seller(who sold more than 1000) , Medium( sold <1000 but more than 50) and Small ( less than 50)","7fd0522e":"# Feature: Suburb, Regionname, CouncilArea, Postcode\n* These 4 features indicate the location of the house. \n        * Suburb: This feature has too many unique values(380) and this might be not good for a linear regression model\n        * Regionname: Since it only has 8 distinct values, I belive we can encode this feature and use it for modeling\n        * CouncilArea: I also has 34 unique values which could make up 34 more weighted value in the model so I decide to eliminate this feature.\n        * Postcode: Same as Suburb, since it has too many unique values, I will drop this feature for the linear regression model.","2d2bac41":"# About the data:\n* There are 63,023 instances in the dataset.\n* There are 13 features : two are float, three are int, and the remaining eight are object.\n* There appear to be no missing values for all attributes except Price (the target attribute), 14590 instances of the data is missing.","f54b0a8e":"# Feature: Date\n* Since Date seems to have a lot of unique value, I choose to group them up by month so we could have a better view and easier to train model.\n* Since all the data are collected in the same year, I shouldn't consider the annual factor.\n* After that, I also eliminate Date feature.","6d636d6d":"# Linear Regression:\n* Linear regression on the table that we just feature engineer\n* Using LM with only onehotencode feature, our model R_squared improve to 0.533","83bf3d5a":"# Input Into Dataframe: \n* I choose to use dataframe 2 since it has more data to use for model training.","4a73aa3e":"# Feature: Method\n* This feature also have only 9 unique method to sell the price, which could possibly affect the price to sold the house. Thus I will onehotencode this feature.","9725c9af":"# PCA then Linear Regression Modeling:\n* Since we have 28 features, which may increase our training time, I choose to use PCA with hypeparameter tuning for n_components to find the best PCA n_components that give us the best model.\n* At n_components=11, we can have a model with R_Square=0.506 which is a little bit less accuracy than the model with full 28 features. However, if we consider 11 to 28 features training, the time of training might decrease significantly.\n","7c972189":"# Conclusion\n* PCA might not give us the best model. However, PCA can reduce traning time significantly ","026d323f":"# Benchmark modeling:\n* Apply Linear Regression model on all the features that we have and on the data that eliminate the missing one.\n* We can see that R_squared score is 0.32 which is quite good for a dummy model.","53aacac2":"# Eliminate features:\nAddress should be eliminate since they have too many unique value and probably cause more error as well as run time of the model."}}