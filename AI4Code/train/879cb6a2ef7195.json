{"cell_type":{"37249f07":"code","f68f3d4c":"code","55e60437":"code","cc99f71b":"code","f08a12d9":"code","8328af01":"code","39ddb7e7":"code","63bf0c6b":"code","85aac460":"code","00638f0f":"code","0f6fdf68":"code","2aa0bd5c":"code","dd1064c0":"code","627f0749":"code","d43cb956":"code","d4052a84":"code","db168609":"code","5a931175":"code","26f35909":"code","7bc3e7aa":"code","a7e55064":"code","ecbd89bf":"code","0a811dec":"code","0628f74b":"code","c3cf88d3":"code","f8d8d228":"code","970c4fb5":"code","1b920fe2":"code","e3f3b8b4":"code","90a65eb0":"code","4fa224b7":"code","3616d005":"code","54f61582":"code","97c2d0e1":"code","0f0da557":"code","61d7acf3":"code","f2c692a1":"code","e3f0dd74":"code","d2af69ec":"code","ab27af2f":"code","6b3f3d89":"code","63d97b8b":"code","721d337e":"code","6b0b8564":"code","5ceb057e":"code","bb053eea":"markdown","f4dab529":"markdown","c54bd19b":"markdown","a3066e86":"markdown","32a2631f":"markdown","8d8446de":"markdown","6bcbcb98":"markdown","4020d1e9":"markdown","d1228df5":"markdown","0011ff8f":"markdown","6de693c6":"markdown","35d2cc78":"markdown","944e2cb2":"markdown","6b9f81a5":"markdown","d5d9b736":"markdown","9028a6b3":"markdown","c97e04f1":"markdown","19f02b7f":"markdown","2dbf2819":"markdown","81d48f21":"markdown","58135edc":"markdown","3254fced":"markdown","da6caf14":"markdown","53a26634":"markdown","22cbf6b4":"markdown","65771004":"markdown","12a3cf69":"markdown","847f5ade":"markdown","019f92bf":"markdown","46688944":"markdown","23a281ff":"markdown","83ebf334":"markdown","4b891568":"markdown","34439969":"markdown"},"source":{"37249f07":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost.sklearn import XGBClassifier\n\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import Pipeline\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f68f3d4c":"#creditcard = pd.read_csv(\"creditcard.csv\")\ncreditcard = pd.read_csv(\"..\/input\/creditcard.csv\")","55e60437":"creditcard.info()","cc99f71b":"creditcard.describe()","f08a12d9":"sns.distplot(creditcard[\"Time\"].astype(float))","8328af01":"sns.countplot(data = creditcard, x = 'Class')","39ddb7e7":"creditcard['Class'].unique()","63bf0c6b":"len(creditcard[creditcard['Class']==1])","85aac460":"len(creditcard[creditcard['Class']==0])","00638f0f":"492\/(284315+492)*100","0f6fdf68":"amount = creditcard[creditcard['Class']==1]['Amount']","2aa0bd5c":"sns.distplot(amount)","dd1064c0":"creditcard.hist(figsize=(30,30))","627f0749":"# RobustScaler is less prone to outliers.\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html\n\nrobust = RobustScaler()\n\ncreditcard['scaled_amount'] = robust.fit_transform(creditcard['Amount'].values.reshape(-1,1))\ncreditcard['scaled_time'] = robust.fit_transform(creditcard['Time'].values.reshape(-1,1))\n\ncreditcard.drop(['Time','Amount'], axis=1, inplace=True)","d43cb956":"creditcard.info()","d4052a84":"X = creditcard.drop('Class', axis=1)\ny = creditcard['Class']","db168609":"X.head()","5a931175":"y.head()","26f35909":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10, stratify=y)\n\n\"\"\"sss1 = StratifiedShuffleSplit(n_splits=5, random_state=10, test_size=0.2)\nrest, test = sss1.split(X, y)\nprint(\"Train:\", rest, \"Test:\", test)\n#X_test, y_test = X.iloc[test], y.iloc[test]\"\"\"","7bc3e7aa":"sss = StratifiedShuffleSplit(n_splits=3, random_state=10, test_size=0.2)","a7e55064":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef plot_roc(y_test, y_pred, label):\n    log_fpr, log_tpr, log_thresold = roc_curve(y_test, y_pred)\n    plt.plot(log_fpr, log_tpr, label=label+'{:.4f}'.format(roc_auc_score(y_test, y_pred)))\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.legend()","ecbd89bf":"clf = DummyClassifier()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\nplot_roc(y_test, y_pred, 'Dummy AUC ')","0a811dec":"%%time\nclf = LogisticRegression(random_state=10)\nparam_grid={'C': [0.01, 0.1, 1, 10, 100]}\nscoring = 'f1_micro'\n\nGS = GridSearchCV(clf, param_grid, scoring=scoring, cv=sss.split(x_train, y_train), n_jobs=-1)\nLR_GS = GS.fit(x_train, y_train)\n\n\nprint(LR_GS.best_score_)\ny_pred = LR_GS.best_estimator_.predict(x_test)","0628f74b":"LR_GS.best_estimator_","c3cf88d3":"plot_roc(y_test, y_pred, 'Logistic Regression AUC ')","f8d8d228":"%%time\nclf = LogisticRegression(random_state=10, penalty='l1')\nparam_grid={'C': [0.01, 0.1, 1, 10, 100],\n            'class_weight':[{0:.1,1:.9}, {0:.0017,1:0.9983}, {0:.3,1:.7}]}\nscoring = 'roc_auc'\n\nRS = RandomizedSearchCV(clf, param_grid, scoring=scoring, cv=sss.split(x_train, y_train), n_jobs=-1)\nLR_RS = RS.fit(x_train, y_train)\n\nprint(LR_RS.best_score_)\ny_pred = LR_RS.best_estimator_.predict(x_test)","970c4fb5":"LR_RS.best_estimator_","1b920fe2":"plot_roc(y_test, y_pred, 'Logistic Regression AUC ')","e3f3b8b4":"params = {\n        'min_child_weight': [0.5, 1, 2],\n        'gamma': [1, 1.5, 2, 5],\n        'subsample': [0.5, 0.8, 1.0],\n        'max_depth': [3, 5, 7],\n        'scale_pos_weight':[10, 100, 300]\n        }\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=500, objective='binary:logistic',\n                    verbosity=2, nthread=-1)\n\n#disable_default_eval_metric=1, eval_metric = 'auc',\n\nrs_xgb = RandomizedSearchCV(xgb, param_distributions=params, \n                                   scoring='roc_auc', cv=3,\n                                   verbose=2, n_jobs=-1,\n                                   random_state=10)\n\n#sss.split(x_train, y_train)","90a65eb0":"%%time\nrs_xgb.fit(x_train, y_train)","4fa224b7":"rs_xgb.grid_scores_","3616d005":"print(rs_xgb.best_score_)\nprint(rs_xgb.best_params_)\ny_pred = rs_xgb.best_estimator_.predict(x_test)","54f61582":"plot_roc(y_test, y_pred, 'XGBoost AUC ')","97c2d0e1":"%%time\nsm = SMOTEENN(random_state=10)\nx_train_res, y_train_res = sm.fit_sample(x_train, y_train)","0f0da557":"print(len(x_train_res))\nprint(len(y_train_res))","61d7acf3":"np.unique(y_train_res)","f2c692a1":"x_train_res.shape","e3f0dd74":"model = Sequential()\nmodel.add(Dense(32, input_dim=30, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","d2af69ec":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","ab27af2f":"%%time\nmodel.fit(x_train_res, y_train_res, epochs=20, batch_size=5000)","6b3f3d89":"y_pred = model.predict(x_test)","63d97b8b":"plot_roc(y_test, y_pred, 'Keras model_1 AUC ')","721d337e":"my_submission = pd.DataFrame(y_pred)\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","6b0b8564":"clf = LogisticRegression(random_state=10)\nparam_grid={'C': [0.01, 0.1, 1, 10, 100]}\nscoring = 'f1_micro'\n\nGS = GridSearchCV(clf, param_grid, scoring=scoring, cv=3, n_jobs=-1)\nLR_GS_new = GS.fit(x_train_res, y_train_res)\n\nprint(LR_GS_new.best_score_)\ny_pred = LR_GS_new.best_estimator_.predict(x_test)","5ceb057e":"plot_roc(y_test, y_pred, 'Log_Reg (sampled data) AUC ')","bb053eea":"The data in this dataset is extremely imbalanced so my approach to solve this problem can be divided into 2 parts:\n* Trying out varying models on the original data with varying class_weights\n* Resampling the data to have more data for the minority class and then try models","f4dab529":"___\n___","c54bd19b":"___","a3066e86":"As we can see from the table, if we have an imbalanced dataset we have to:\n* Give weights for each class, so that the model penalises the misclassification of the class we care about (1 in this case)\n* Or Resample the data to increase or decrease the data of the classes in the dataset","32a2631f":"| Model \t| with class weights \t| with Resampled Data \t| HyperParameter tuned \t| AUC on Testset \t|\n|:----------------------:\t|:------------------:\t|:-------------------:\t|:---------------------------------------------------------------------------:\t|:--------------:\t|\n| DummyClassifier \t| No \t| No \t| No \t| 0.4991 \t|\n| LogisticRegression \t| No \t| No \t| 'C' \t| 0.7805 \t|\n| LogisticRegression \t| Yes \t| No \t| 'C', 'class_weight' \t| 0.9029 \t|\n| XGBClassifier \t| Yes \t| No \t| 'min_child_weight', 'gamma',  'subsample', 'max_depth',  'scale_pos_weight' \t| 0.9275 \t|\n| Keras Sequential Model \t| No \t| Yes \t| No \t| 0.9634 \t|\n| LogisticRegression \t| No \t| No \t| 'C' \t| 0.9482 \t|","8d8446de":"**Model 3 :**\n\nXGBoost","6bcbcb98":"___","4020d1e9":"___","d1228df5":"#TestSet has:\n\nX_test, y_test","0011ff8f":"___","6de693c6":"Now that data values are taken care of, we need to worry about how the dataset will be divided into train and test set.\n\nWe will use StratifiedShuffleSplit with n_split=1 so as to preserve the percentages of original classes in the split as well.\n\nFor more info see this link:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html","35d2cc78":"___","944e2cb2":"We will now divide the dataset to 80\/20 train\/test with stratified splitting.\n\nFurther, we will divide the train test from above to 80\/20 for training and cross-validation","6b9f81a5":"___","d5d9b736":"___\n___","9028a6b3":"**Read Data**","c97e04f1":"**Model 5**\n\nLets see if the data sampling improves the original logistic regression model that we used","19f02b7f":"**Model 2 :**\n\nLogistic Regression with parameter tuning of 'C' and 'class_weight'.\n\nThe 'class_weight' parameter helps in a imbalanced dataset by penalising the wrong classification of smaller classes more.\n\nAlso scoring metric is 'roc_auc'","2dbf2819":"From the histogram above we can see that most of the data columns are scaled (apart from amount and time)\n\nSo next we will try to scale the values of amount and time columns\n\n___","81d48f21":"**Function :**","58135edc":"___","3254fced":"___","da6caf14":"**Model 4 :**\n\nKeras model to fit and validate train and test set respectively","53a26634":"___","22cbf6b4":"**Model 1 :**\n\nLogistic Regression with parameter tuning of 'C' and scoring as 'f1_micro' to get proper f1 values for imbalanced dataset classes","65771004":"We can see that the data here is very imbalanced, with only 0.172% of the data as class 1 (i.e 1 for fraudulent transactions)","12a3cf69":"**EDA**","847f5ade":"**Data Sampling**\n\nHere we will transform the original imbalanced dataset with imblearn python package - https:\/\/imbalanced-learn.readthedocs.io\/","019f92bf":"### Conclusion","46688944":"___","23a281ff":"**Import Packages**","83ebf334":"**Model 0 :**\n\nDummyClassifier","4b891568":"___","34439969":"We can see that the majority of the fraud data transaction is less than 500"}}