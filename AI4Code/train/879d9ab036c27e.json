{"cell_type":{"2757f462":"code","66097f31":"code","9f9812af":"code","8978c220":"code","4a27ece8":"code","b26b4c81":"code","12ed984d":"code","46945cd0":"code","0a38115a":"code","ecc42284":"code","97722d66":"code","e45eeeb0":"code","0098b383":"code","9b34fa4b":"code","31493886":"code","c207acb6":"code","f7c5ab97":"code","9cac9d69":"code","4a94bff3":"markdown","3e00aa5c":"markdown","6da67322":"markdown","fed74713":"markdown","a323b9d5":"markdown","eca7787e":"markdown","80395460":"markdown","2b278355":"markdown","8ef7f8e3":"markdown","764edb57":"markdown","812d2da3":"markdown","edcbb586":"markdown","2fdf59e2":"markdown","3ccf1d76":"markdown","e3b401d1":"markdown","739864ff":"markdown","43fe00ca":"markdown","1a993da0":"markdown","0a0cb5a6":"markdown"},"source":{"2757f462":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Data Handling:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\n# Matplotlib and Seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Other:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning:\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","66097f31":"# First load the CSV to a Pandas dataframe\ndata_DF = pd.read_csv('..\/input\/data.csv')\n\n# Next examine the Dataframe\nprint(data_DF.shape)\nprint(data_DF.keys())\nprint(data_DF.dtypes)","9f9812af":"# view the top 5 rows:\ndata_DF.head()","8978c220":"# Look at some basic stats of all numeric data:\ndata_DF.describe()","4a27ece8":"data_DF.isnull().sum()","b26b4c81":"data_DF = data_DF.drop(labels = ['Unnamed: 32'], axis = 1)","12ed984d":"sns.countplot(data_DF.diagnosis);","46945cd0":"data_DF.diagnosis.value_counts()","0a38115a":"vars = data_DF.keys().drop(['id','diagnosis'])\nplot_cols = 5\nplot_rows = math.ceil(len(vars)\/plot_cols)\n\nplt.figure(figsize = (5*plot_cols,5*plot_rows))\n\nfor idx, var in enumerate(vars):\n    plt.subplot(plot_rows, plot_cols, idx+1)\n    sns.boxplot(x = 'diagnosis', y = var, data = data_DF)","ecc42284":"fig, (ax) = plt.subplots(1, 1, figsize=(20,10))\n\nhm = sns.heatmap(data_DF.corr(), \n                 ax=ax, # Axes in which to draw the plot\n                 cmap=\"coolwarm\", # color-scheme\n                 annot=True, \n                 fmt='.2f',       # formatting  to use when adding annotations.\n                 linewidths=.05)\n\nfig.suptitle('Breast Cancer Correlations Heatmap', \n              fontsize=14, \n              fontweight='bold');","97722d66":"vars_to_drop = ['id', 'radius_mean', 'perimeter_mean', 'radius_worst', 'area_worst', 'perimeter_worst', 'radius_se', 'perimeter_se',\n               'concave points_mean', 'compactness_mean', 'compactness_worst', 'concavity_worst', 'concavity_mean', 'concavity_se',\n               'texture_worst', 'smoothness_worst', 'texture_se']","e45eeeb0":"g = sns.pairplot(data_DF.drop(vars_to_drop, axis = 1), hue='diagnosis', height=3)\ng.map_lower(sns.kdeplot)","0098b383":"y = data_DF.diagnosis\nX = data_DF.drop(vars_to_drop, axis = 1)\nX = X.drop('diagnosis', axis = 1)\nprint(y.shape)\nprint(X.shape)\nX.head()","9b34fa4b":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1, test_size = 0.25)","31493886":"XGBC_model = XGBClassifier(random_state=1, objective = 'multi:softprob', num_class=2)\nXGBC_model.fit(train_X, train_y)\n\n# make predictions\nXGBC_predictions = XGBC_model.predict(val_X)\n\n# Print Accuracy for initial RF model\nXGBC_accuracy = accuracy_score(val_y, XGBC_predictions)\nprint(\"Accuracy score for XGBoost Classifier model : \" + str(XGBC_accuracy))","c207acb6":"# Create dataframe of feature name and importance \nfeature_imp_DF = pd.DataFrame({'feature': X.keys().tolist(), 'importance': XGBC_model.feature_importances_})\n\n# Print the sorted values form the dataframe:\nprint(\"Feature Importance:\\n\")\nprint(feature_imp_DF.sort_values(by=['importance'], ascending=False))","f7c5ab97":"# Plot feature importance from the dataframe..","9cac9d69":"# feature_imp_DF.sort_values(by=['importance'],ascending=False).plot(kind='bar');\ng = sns.barplot(x=\"feature\", y=\"importance\", data=feature_imp_DF.sort_values(by=['importance'],ascending=False))\ng.set_xticklabels(feature_imp_DF.sort_values(by=['importance'],ascending=False)['feature'],rotation=90);","4a94bff3":"**Create Simple XGBoost Classifier Model to Predict Diagnosis:**","3e00aa5c":"# Benign or Malignant?\n\nIn this notebook, I will attempt to predict whether cancer is benign or malignant based on features that are computed from a digitized image of a fine needle aspirate of a breast mass. Data is from the Breast Cancer Wisconsin data set. I wil aslo use seaborn to visualize the data and help select the best features for my model. \n\n**Outline:**\n1.  Imports\n2. Load\/Preview data using Pandas\n3. Explore The Dependant Variable (diagnosis)\n4. Explore The Independant\/Predictor Variables and Select Features\n5. Machine Learning To Predict Diagnosis (XGBoost Classifier)\n","6da67322":"# Imports:","fed74713":"From this correlation heat map, we have a bit of an idea which variables may be a little redundant, and could be removed because of too much covariance...\n\n* radius_mean, perimeter_mean, area_mean are all highly correlated (r= 0.99 - 1.00). This is intuitive. \n* The above 3 vars are also highly correlated with radius_worst, perimeter_worst, area_worst...\n\nmaybe I'll keep the 'area_worst' or 'area_mean' and drop the rest...\n\n* We can also see that compactness_worst, concave_points_worst, concavity_worst also are highly correlated with compactness_mean, concave_mean, and Concavity_mean\n    - concave_points_mean and concave_points_worst both seems to have distinct differences between benign and malignant, with relatively lower outliers. Maybe we should choose one of these to keep and drop the rest?\n* There are a few more vars with correlations > 0.80. We'll remove some of those too...    ","a323b9d5":"**First, Define X, y**","eca7787e":"# Machine Learning to Predict the Diagnosis:","80395460":"**How each variable differs by diagnosis:**","2b278355":"**Next, Split into Training and validation data:**","8ef7f8e3":"Alright, That's a lot of graphs. We can see that for a lot of the predictor variables, values are higher (on average) in the malignant group. However, there are plenty of outliers amongst the benign data, which could cause those values to look like those of the malignant group... next, it is probably interesting to see if\/how any of these variables are correlated with eachother...","764edb57":"Here we quickly check for missing values. We can see that the variable named 'Unnamed: 32' is missing for everyone... We will go ahead and drop that variable from the dataframe right away. ","812d2da3":"Random Forest Classifier models tend to do really well with these types of predictions (and are also less sensitive to covariance in the predictor variables, so we will start there. I will use a simple XGBoost classifier model to perform these predictions.","edcbb586":"First, I'll just quickly look at how each variable differs between benign an malignant diagnoses. I prefer to do this with boxplots to help vizualize the spread\/outliers of each variable, but this could also be done with violin plots or other options.","2fdf59e2":"# Import\/Preview Data:","3ccf1d76":"# Explore The Independant\/Predictor Variables ","e3b401d1":"**Correlations Heat Map:**","739864ff":"**Now that we've dropped some variables, we can further visualize it... **\n\n(this is admittedly too many variables for this type of visualization, but if you have a decent sized monitor, it's still doable)","43fe00ca":"Without any tuning, we are able to predict if the biopsied tumor is benign or malignant with ~97% accuracy using an XGBoosted Random Forest Classifier Model... Not too Bad...\n\n**Next, let's examine the importance of the predictive variables**","1a993da0":"# Let's Quickly Explore The Dependant Variable (diagnosis)","0a0cb5a6":"Here we can see that there are ~50% more Benign ('B') diagnoses than Malignant ('M'). Let's see what those actual numbers are, though..."}}