{"cell_type":{"a6c46e02":"code","8cd84d8e":"code","842bf3d3":"code","62d8ee47":"code","77e5ec3c":"code","eb9afe60":"code","facb121d":"code","d64cc9a2":"code","236f2400":"code","e74b7a8d":"code","1c2bfa6e":"code","846ff96d":"code","619104e4":"code","8bea6aba":"code","2ae6224d":"code","8c95dda3":"code","70bce263":"code","b116ad27":"code","0b4e381d":"code","98c31828":"code","c1e88f60":"code","1f87bf96":"code","dff171f4":"code","04a4e1ed":"code","94850d61":"code","f517ddfa":"code","3cfff98c":"code","f52873ca":"code","db798fc8":"markdown","da54cf3c":"markdown","cc63272a":"markdown","e1f1a2cf":"markdown","67269e3d":"markdown","2af5f391":"markdown","a00151c4":"markdown","0c252fe2":"markdown","a270b55f":"markdown","d808b75c":"markdown","c37834d7":"markdown","e4f7fb84":"markdown","f29d74fe":"markdown","321faa68":"markdown","1c7a75b2":"markdown","6e1254f0":"markdown","cfcf2ab7":"markdown"},"source":{"a6c46e02":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import  Counter\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC","8cd84d8e":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","842bf3d3":"train.head()","62d8ee47":"test.head()","77e5ec3c":"train.shape, test.shape","eb9afe60":"# tokenizer = nltk.tokenize.WhitespaceTokenizer()\n# tokenizer = nltk.tokenize.TreebankWordTokenizer()\n# tokenizer = nltk.tokenize.WordPunctTokenizer()\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nnormalization = None\n# normalization = 'stemmer'\n# normalization = 'lemmatizer'\n\nvectorizer = 'countvectorizer'\n# vectorizer = 'tfidfvectorizer'","facb121d":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ntrain['text'] = train['text'].apply(remove_URL)\ntest['text'] = test['text'].apply(remove_URL)","d64cc9a2":"train.head()","236f2400":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntrain['text'] = train['text'].apply(remove_html)\ntest['text'] = test['text'].apply(remove_html)","e74b7a8d":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain['text'] = train['text'].apply(remove_emoji)\ntest['text'] = test['text'].apply(remove_emoji)","1c2bfa6e":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text'] = train['text'].apply(remove_punct)\ntest['text'] = test['text'].apply(remove_punct)","846ff96d":"# Before Tokenization\ntrain.head(3)","619104e4":"train['text'] = train['text'].apply(tokenizer.tokenize)\ntest['text'] = test['text'].apply(tokenizer.tokenize)","8bea6aba":"# After Tokenization\ntrain.head()","2ae6224d":"def stem_tokens(tokens):\n    stemmer = nltk.stem.PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]\n    return tokens\n\ndef lemmatize_tokens(tokens):\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    return tokens\n\ndef normalize_tokens(normalization):\n    if normalization is not None:\n        if normalization == 'stemmer':\n            train['text'] = train['text'].apply(stem_tokens)\n        elif normalization == 'lemmatizer':\n            train['text'] = train['text'].apply(lemmatize_tokens)\n        \nnormalize_tokens(normalization)","8c95dda3":"train.head()","70bce263":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\ntrain['text'] = train['text'].apply(remove_stopwords)\ntest['text'] = test['text'].apply(remove_stopwords)","b116ad27":"train.head()","0b4e381d":"def combine_tokens(text):\n    combined_text = ' '.join(text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(combine_tokens)\ntest['text'] = test['text'].apply(combine_tokens)","98c31828":"train.head()","c1e88f60":"# Vectorization\ndef vectorize(vectorizer):\n    if vectorizer == 'countvectorizer':\n        print('countvectorizer')\n        vectorizer = CountVectorizer()\n        train_vectors = vectorizer.fit_transform(train['text'])\n        test_vectors = vectorizer.transform(test['text'])\n    elif vectorizer == 'tfidfvectorizer':\n        print('tfidfvectorizer')\n        vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n        train_vectors = vectorizer.fit_transform(train['text'])\n        test_vectors = vectorizer.transform(test['text'])\n    return train_vectors, test_vectors\n\ntrain_vectors, test_vectors = vectorize(vectorizer)","1f87bf96":"train_vectors[0].todense()","dff171f4":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","04a4e1ed":"# clf_NB.fit(train_vectors, train['target'])","94850d61":"svc = SVC(kernel='rbf', C=70, gamma='auto', probability=True, random_state=41)\nrfc = RandomForestClassifier(n_estimators=200, random_state=41)\ngbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.2, random_state=41)","f517ddfa":"vcf = VotingClassifier(estimators=[('svc', svc), ('rfc', rfc), ('gbc', gbc)], voting='soft')\nvcf.fit(train_vectors, train['target'])","3cfff98c":"def make_submission(submission_file, model, test_vectors):\n    sample_submission = pd.read_csv(submission_file)\n    sample_submission['target'] = model.predict(test_vectors)\n    sample_submission.to_csv('my_submission.csv', index=False)","f52873ca":"submission_file = '\/kaggle\/input\/nlp-getting-started\/sample_submission.csv'\ntest_vectors = test_vectors\nmake_submission(submission_file, vcf, test_vectors)","db798fc8":"### Removing Emojis","da54cf3c":"# Modeling","cc63272a":"## Combine text together","e1f1a2cf":"# Bag of Words (BOW)","67269e3d":"You can choose hyper-parameter (tokenizer, normalization, vetorcizer)\n\nI choose RegexpTokenizer, countvectorizer.","2af5f391":"## Removing useless words (url, html, emoji, punctuation)","a00151c4":"### Removing HTMLS Tags","0c252fe2":"This is my first Kernel in active competition. I refered the following kernels.\nReferences:\n- https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp\n- https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n- https:\/\/www.kaggle.com\/vtech6\/learning-nlp-with-disaster-tweets\n\nIf you like my kernel, please cheer me up with your vote ^^ Thank you!","a270b55f":"## Normalization","d808b75c":"# Data Cleaning\nI got data cleaning ideas from https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","c37834d7":"### Revmoing Punctuations","e4f7fb84":"## Removing Stopwords\nI refered from https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp\/","f29d74fe":"## Tokenization\nI refered from https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp","321faa68":"## Countvectorizer or TF-IDF","1c7a75b2":"### Removing URLs","6e1254f0":"- if normalization == **'stemmer'**:  \ndeeds -> deed, residents -> resid, receive -> receiv, evacuation -> evacu\n\n- if normalization == **'lemmatizer'**:  \nresidents -> residents, wildfires -> wildfire\n\n- if normalization == **None**:   \nNothing happends","cfcf2ab7":"### Hyper Parameter"}}