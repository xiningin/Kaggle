{"cell_type":{"6d539ea2":"code","b2f10571":"code","1e8956ba":"code","692bb239":"code","a28e9775":"code","c90a1f14":"code","a9855e97":"code","0b11de3b":"code","d747e091":"code","69299552":"code","d188c691":"code","6e6ed1b7":"code","76d33805":"code","bb029943":"code","2645f547":"code","3520a5ec":"code","1d7b926e":"code","2ecc0beb":"code","2ce09c97":"code","51d45f40":"code","14ce4e15":"code","c30d6dd6":"code","88cb0e3f":"code","117e5f7a":"code","0f8c9e63":"code","4e104188":"code","c3d5d9c9":"code","9508c115":"code","96276033":"code","7eefa928":"code","145ba7c7":"code","e1efa2fa":"code","f0ad5709":"code","dcb5240c":"code","a00fc9ce":"code","cffc7946":"code","7de25198":"code","b03197e5":"code","b7f17b8a":"code","a34c49da":"code","a4ca297a":"code","eb6de541":"code","1636cfe8":"code","53ecf4ca":"markdown","31466ba1":"markdown","8b4ae8bd":"markdown","b6b7ed27":"markdown","9efbbe80":"markdown","69a99ebe":"markdown","86f55096":"markdown","0c97cdf9":"markdown","b756d411":"markdown","3c025e2f":"markdown","37fc450d":"markdown","a310c34a":"markdown","2164b8e6":"markdown","ed57169f":"markdown","83690a6a":"markdown","584ce96a":"markdown","9934aa69":"markdown","06688a0c":"markdown","bd513dca":"markdown","9f6b94a4":"markdown","773f9ba5":"markdown","556c0962":"markdown","cc85620d":"markdown","06e43d1e":"markdown","106cc414":"markdown","86b81116":"markdown","cca2cd15":"markdown","17ca8cd0":"markdown","0adb3694":"markdown","0ab56f0b":"markdown","34ec2cda":"markdown","c47aa82a":"markdown","993753dc":"markdown","c0dfa8a4":"markdown","e361a762":"markdown","92f1204a":"markdown","9499a97f":"markdown","e51ad58c":"markdown","f0f518dc":"markdown","4dff25a0":"markdown","fcf9dbb4":"markdown"},"source":{"6d539ea2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b2f10571":"# Import packages\nimport time\nimport gc\n\n## Basic data processing\nimport numpy as np\nimport pandas as pd\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n## Modelling\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, plot_importance\n\n## Model Explanatory\nimport shap  # package used to calculate Shap values\nimport eli5\n\n## Settings\npd.set_option('display.max_columns', 500) # Able to display more columns.\npd.set_option('display.max_info_columns', 150) # Able to display more columns in info().","1e8956ba":"# Load the dataset\ndata_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ndata_df.info() # show entries, dtypes, memory useage.","692bb239":"# Have a look\ndata_df.head(5)","a28e9775":"# Distinguish numerical and categorical features\nnumerical_features = data_df.select_dtypes(include=\"float64\").columns\ncategorical_features = data_df.select_dtypes(include=\"int64\").columns\nlen(numerical_features), len(categorical_features)","c90a1f14":"# Check the range of chosen datatype\nprint(np.iinfo(np.int8))\nprint(np.finfo(np.float16))","a9855e97":"# Shrink the data type to save memory usage\ndata_df[numerical_features] = data_df[numerical_features].astype(\"float16\")\ndata_df[categorical_features] = data_df[categorical_features].astype(\"int8\")\ndata_df.info()","0b11de3b":"# Exclude id and label\ncategorical_features = categorical_features[~categorical_features.isin([\"id\", \"target\"])]","d747e091":"# Basic statistic on labels\ndata_df[\"target\"].astype(\"object\").describe() # All the Nominal data can be treated as \"object\" type for simplicity.","69299552":"# Basic statistic on numerical features\ndata_df.loc[:, numerical_features].describe()","d188c691":"# Basic statistic on categorical features\ndata_df.loc[:, categorical_features].astype(\"object\").describe()","6e6ed1b7":"# Irrelevant columns\n'''\nid: id is useless for analysis and modeling.\n'''\nirrelevant_columns = ['id']\ndata_preprocessed_df = data_df.drop(irrelevant_columns, axis=1)","76d33805":"# Replace the empty data with NaN\ndata_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ndata_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = data_preprocessed_df.isna().sum() \/ data_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])\ncount_missing_value_df.sum()","bb029943":"'''\nDescription: Generate new feature by several statistic methods\nArgs:\n    dataset: The chosen dataset\n    numerical_features: The numerical features in a list\n    categorical_features: The categorical features in a list\nReturn: None\n'''\ndef feature_generator(dataset, numerical_features, categorical_features):\n    # Numerical feature\n    dataset['n_min'] = dataset[numerical_features].min(axis=1)\n    dataset['n_max'] = dataset[numerical_features].max(axis=1)\n    dataset['n_std'] = dataset[numerical_features].std(axis=1)\n    dataset['n_mean'] = dataset[numerical_features].mean(axis=1)\n    # Categorical feature\n    dataset['c_sum'] = dataset[categorical_features].sum(axis=1)\n    dataset['c_mode'] = dataset[categorical_features].mode(axis=1)","2645f547":"# Generate new feature by several statistic methods\nnew_features = ['n_min', 'n_max', 'n_std', 'n_mean', 'c_sum', 'c_mode']\nfeature_generator(data_preprocessed_df, numerical_features, categorical_features)","3520a5ec":"# Copy a new dataframe for following phase\ndata_best_df = data_preprocessed_df.copy()","1d7b926e":"# Count the number of target(0\/1), transform the result to pandas dataframe\ntarget_counts = data_best_df[\"target\"].value_counts()\ntarget_counts_df = pd.DataFrame(target_counts)\n\n# Visualize the distribution of the target(label)\ntarget_fig = make_subplots(\n    rows=1, cols=2, \n    specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}]])\n\ntarget_fig.add_trace(go.Bar(x=target_counts_df.index, \n                           y=target_counts_df[\"target\"],\n                           text=target_counts_df[\"target\"],\n                           textposition='outside',\n                           showlegend=False),\n                           1, 1)\n\ntarget_fig.add_trace(go.Pie(labels=target_counts_df.index, \n                           values=target_counts_df[\"target\"],\n                           showlegend=True),\n                           1, 2)\n\ntarget_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The distribution of target\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  xaxis1_title = 'target', \n                  yaxis1_title = 'Counts',\n                  legend_title_text=\"target\"\n                 )\ntarget_fig.update_xaxes(type='category')\ntarget_fig.show()","2ecc0beb":"# Set up the matplotlib figure\nf, axes = plt.subplots(40, 6, figsize=(30, 200))\nfor feature,number in zip(numerical_features, range(240)):\n    yaxix_name = feature\n    r_pos = number \/\/ 6\n    c_pos = number % 6\n    sns.boxplot(x='target', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","2ce09c97":"# Plot features we created\nf, axes = plt.subplots(3, 2, figsize=(15, 15)) #suitable for two line with 6 graph.\nfor feature,number in zip(new_features, range(6)):\n    yaxix_name = feature\n    r_pos = number \/\/ 2\n    c_pos = number % 2\n    sns.boxplot(x='target', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","51d45f40":"# Set up the matplotlib figure\nf, axes = plt.subplots(8, 6, figsize=(30, 40))\nfor feature,number in zip(categorical_features, range(45)):\n    yaxix_name = feature\n    r_pos = number \/\/ 6\n    c_pos = number % 6\n    sns.countplot(x=\"target\", hue=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","14ce4e15":"# Check the new feature: c_mode\nsns.countplot(x=\"target\", hue=\"c_mode\", data=data_best_df).set_title(\"c_mode\")","c30d6dd6":"#signi_feature = ['f1', 'f3', 'f7', 'f8', 'f19', 'f24', 'f40', 'f53', 'f54', 'f65', 'f92', 'f93', 'f112', 'n_mean', 'c_sum', 'f22', 'f44', 'f56', 'f58', 'f69', 'f139', 'f146', 'f150', 'f179', 'f181','target']","88cb0e3f":"# Copy a new dataframe for following phase\ndata_modelling_df = data_best_df.copy()","117e5f7a":"# Release big variables that are not used in the following. Save Memory\ndel data_df, data_preprocessed_df, data_best_df\ngc.collect()","0f8c9e63":"# Train\/Test Split\nX = data_modelling_df.drop(\"target\", axis=1)\nY = data_modelling_df.target\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)","4e104188":"# Release big variables that are not used in the following. Save Memory\ndel data_modelling_df\ngc.collect()","c3d5d9c9":"# Start time\nstart_time = time.time()\n\nxgb_params = {\n    \"random_state\": 0,\n    \"n_estimators\": 10000,\n    \"learning_rate\":0.008,\n    \"eval_metric\": \"auc\",\n    \"objective\":\"binary:logistic\",\n    \"use_label_encoder\": False,\n    \"booster\": \"gbtree\",\n    # GPU\n    \"gpu_id\": 0,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\"\n}\n\nxgbc = XGBClassifier(**xgb_params)\nxgbc.fit(x_train, y_train, verbose=False)\n\n# Calculate the training time\nxgbc_time = time.time() - start_time\n\n# xgbc.evals_result() #Return the evaluation results of eval_sets\npredictions = xgbc.predict_proba(x_test)[:,1]\nauc_xgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_xgbc}')","9508c115":"# Start time\nstart_time = time.time()\n\ncatb_params = {\n    \"random_seed\": 0,\n    \"iterations\": 10000,\n    \"learning_rate\":0.008,\n    \"eval_metric\" : \"AUC\",\n    \"verbose\": 0,\n    # GPU\n    \"task_type\" : \"GPU\",\n    \"devices\" : \"0\",\n}\n\ncatbc = CatBoostClassifier(**catb_params)\ncatbc.fit(x_train, y_train, verbose=False)\n\n# Calculate the training time\ncatbc_time = time.time() - start_time\npredictions = catbc.predict_proba(x_test)[:,1]\nauc_catbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_catbc}')","96276033":"# Start time\nstart_time = time.time()\n\nlgbc_params = {\n    \"n_estimators\":10000, \n    \"learning_rate\":0.008, \n    \"objective\":'binary',                      \n    \"metric\":'auc',                       \n    \"reg_alpha\":10,\n    \"reg_lambda\":0.1,                     \n    \"num_leaves\":31,\n    \"max_depth\":-1,\n    \"subsample\":0.6,\n    \"subsample_freq\":1, \n    \"colsample_bytree\":0.4,\n    \"min_child_weight\":256,\n    \"min_child_samples\":20, \n    \"random_state\":0,\n    # GPU\n    \"device\": \"gpu\"\n}\n\nlgbc = LGBMClassifier(**lgbc_params)\n\nlgbc.fit(x_train, y_train, eval_metric='auc', verbose=-1)\n\n# Calculate the training time\nlgbc_time = time.time() - start_time\npredictions = lgbc.predict_proba(x_test)[:,1]\nauc_lgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_lgbc}')","7eefa928":"# Collect all the model performance\nmodel_comparison = pd.DataFrame(data = [(auc_xgbc, xgbc_time), (auc_catbc, catbc_time), (auc_lgbc, lgbc_time)], \n                                index = [\"XGboost\", \"CatBoost\", \"LGBM\"],\n                                columns=['AUC', 'Time'])\\\n                     .sort_values(by = \"AUC\", ascending=False)\nmodel_comparison","145ba7c7":"import lightgbm as lgb\nlgb.plot_importance(lgbc, max_num_features=20, figsize=(10, 8))","e1efa2fa":"# Release big variables that are not used in the following. Save Memory\ndel x_train, x_test, y_train, y_test\ngc.collect()","f0ad5709":"# Make Prediction by classifiers\nvoting_clas = VotingClassifier(estimators=[('CatBoost', catbc), ('LGBoost',lgbc)], voting='soft', n_jobs=-1)\nvotingC = voting_clas.fit(X, Y)","dcb5240c":"# Release big variables that are not used in the following. Save Memory\ndel X, Y\ngc.collect()","a00fc9ce":"# Load the dataset\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\ntest_df.info() # show entries, dtypes, memory useage.","cffc7946":"test_df[numerical_features] = test_df[numerical_features].astype(\"float16\")\ntest_df[categorical_features] = test_df[categorical_features].astype(\"int8\")\ntest_df.info()","7de25198":"# Have a look\ntest_df.head()","b03197e5":"# Drop Irrelevant columns\ntest_preprocessed_df = test_df.drop(irrelevant_columns, axis=1)","b7f17b8a":"# Save the \"id\" column\nid_df = test_df['id']","a34c49da":"# Release big variables that are not used in the following. Save Memory\ndel test_df\ngc.collect()","a4ca297a":"feature_generator(test_preprocessed_df, numerical_features, categorical_features)","eb6de541":"# Use trained model(best) to make predictions\npredictions = votingC.predict_proba(test_preprocessed_df)[:,1]\npredictions_df = pd.DataFrame(predictions, columns=['target'])\nsubmission_df = pd.concat([id_df, predictions_df], axis=1)","1636cfe8":"# Save aggregated predictions to .csv for project submission\nsubmission_df.to_csv('submission.csv', index=False)","53ecf4ca":"# Thanks for reading, Have a good day ~","31466ba1":"The result partially matchs results from section3: EDA","8b4ae8bd":"<a id=\"1\"><\/a>\n# 1. Data Overview","b6b7ed27":"<a id=\"4.2.3\"><\/a>\n### 4.2.3 LightGBM","9efbbe80":"<a id=\"4.2\"><\/a>\n## 4.2. Train Models\n> Let's use three state-of-art ensembled models to make prediction\n\n* [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/)\n* [CatBoost](https:\/\/catboost.ai\/)\n* [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Booster.html)\n\nThanks for the following Kernel(Author) to support the Parameter sets.\n* https:\/\/www.kaggle.com\/mehrankazeminia\/1-tps-oct-21-lgbm-auc-evaluation\/notebook#notebook-container\n* https:\/\/www.kaggle.com\/stevenrferrer\/tps-oct-2021-baseline-lgbm-xgb-cb","69a99ebe":"It seems that **f22** is an important feature.","86f55096":"<a id=\"4.2.1\"><\/a>\n### 4.2.1 XGboost","0c97cdf9":"<a id=\"1.2\"><\/a>\n## 1.2. Data Type\n\n> [NOIR](https:\/\/www.questionpro.com\/blog\/nominal-ordinal-interval-ratio\/): Nominal, Ordinal, Interval, Ratio.  \n\nAs the features in this dataset have been anonymized, we just assume that the data type of each feature is what it looks like.\n* numerical --> Ratio\n* categorical -> Norminal\/Ordinal","b756d411":"<a id=\"2.2\"><\/a>\n## 2.2. Missing Value Detection","3c025e2f":"<a id=\"3.2\"><\/a>\n## 3.2. What is the distribution of numerical features on target?","37fc450d":"<a id=\"4.1\"><\/a>\n## 4.1. Train Test Split","a310c34a":"<a id=\"5.4\"><\/a>\n## 5.4. Make Prediction","2164b8e6":"<a id=\"2.3\"><\/a>\n## 2.3. New Feature Generation\n* Numerical feature: min,max,mean,std\n* Categorical feature: sum, mode","ed57169f":"<a id=\"5.3\"><\/a>\n## 5.3. New Feature Generation","83690a6a":"<a id=\"5.1\"><\/a>\n## 5.1. Load Data","584ce96a":"<a id=\"1.3\"><\/a>\n## 1.3. Statistical View ","9934aa69":"# October 2021 Tabular Playground\nThis notebook aims to show an entire workflow of Data Science by using the dataset from October 2021 Tabular Playground. A competitive performance is ensured.\n* [**Author**](https:\/\/www.linkedin.com\/in\/chi-wang-22a337207\/)\n* [**Dataset**](https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/data)","06688a0c":"It seems that **n_mean** and **c_sum** are important features.","bd513dca":"<a id=\"3.3\"><\/a>\n## 3.3. What is the distribution of categorical features on target?","9f6b94a4":"As the declaration in this playground, all the features are scaled **[0, 1]**.","773f9ba5":"<a id=\"2.1\"><\/a>\n## 2.1. Drop Irrelevant Columns","556c0962":"<a id=\"5.2\"><\/a>\n## 5.2. Drop Irrelevant Columns","cc85620d":"<a id=\"4.3\"><\/a>\n## 4.3. Model Comparison","06e43d1e":"The label is quite balanced.","106cc414":"<a id=\"1.1\"><\/a>\n## 1.1. Load Data","86b81116":"# Tips\n* It's a good strategy to compress the dataset by data type transformation. Eg. **Float64 --> Float16**\n* Release variables that occupy huge memory. **del xxx; gc.collect()**\n* Generate new features base on **(min\/max\/mean\/std\/mode)** could help to improve the results. ","cca2cd15":"<a id=\"2\"><\/a>\n# 2. Data Preprocessing","17ca8cd0":"Overall:   \nimportant features:  f1, f3, f7, f8, f19, f24, f40, f53, f54, f65, f92, f93, f112, n_mean  \nsignificant features: **c_sum, f22, f44, f56, f58, f69, f139, f146, f150, f179, f181**","0adb3694":"<a id=\"3.1\"><\/a>\n## 3.1. What is the distribution of the label?","0ab56f0b":"<a id=\"5\"><\/a>\n# 5. Prediction","34ec2cda":"All the categorical feature only contain two values (**binary**). Most of them are quite **unbalanced**.","c47aa82a":"<a id=\"4.4\"><\/a>\n## 4.4. Best Model Explaination","993753dc":"# Table of Content\n1. [Data Overview](#1)\n    * [1. Load Data](#1.1)\n    * [2. Data Type](#1.2)\n    * [3. Statistical View](#1.3)\n2. [Data Preprocessing](#2)\n    * [1. Drop Irrelevant Columns](#2.1)\n    * [2. Missing Value Detection](#2.2)\n    * [3. New Feature Generation](#2.3)\n3. [Data Analysis](#3)\n    * [1. What is the distribution of label? ](#3.1)\n    * [2. What is the distribution of numerical features on target? ](#3.2)\n    * [3. What is the distribution of categorical features on target? ](#3.3)\n4. [Modelling](#4)\n    * [1. Train Test Split ](#4.1)\n    * [2. Train Models ](#4.2)\n        * [1. XGboost ](#4.2.1)\n        * [2. CatBoost ](#4.2.2)\n        * [3. LightGBM ](#4.2.3)\n    * [3. Model Comparison ](#4.3)\n    * [4. Best Model Explaination ](#4.4)\n    * [5. Parameter\/Feature Tuning ](#4.5)\n5. [Prediction](#5)\n    * [1. Load Data](#5.1)\n    * [2. Drop Irrelevant Columns](#5.2)\n    * [3. New Feature Generation](#5.3)\n    * [4. Make Prediction](#5.4)\n    * [5. Save the Prediction to CSV file](#5.5)","c0dfa8a4":"There is **no missing value** in this dataset.","e361a762":"<a id=\"4.2.2\"><\/a>\n### 4.2.2 CatBoost","92f1204a":"<a id=\"4.5\"><\/a>\n## 4.5. Parameter\/Feature Tuning  ","9499a97f":"It seems the potential **important features** are: f1, f3, f7, f8, f19, f24, f40, f53, f54, f65, f92, f93, f112  \nIt seems the potential ***significant features*** are: **f44, f56, f58, f69, f139, f146, f150, f179, f181**","e51ad58c":"<a id=\"5.5\"><\/a>\n## 5.5. Save the Prediction to CSV file","f0f518dc":"<a id=\"3\"><\/a>\n# 3. Data Analysis","4dff25a0":"<a id=\"4\"><\/a>\n# 4. Modelling","fcf9dbb4":"# Issues\n* Could try dimensional reduction(PCA, correlation Analysis) to speed-up.\n* Stacking tech is worth to try."}}