{"cell_type":{"1c46945f":"code","30d0e884":"code","ef84d165":"code","7c8ba055":"code","b94ebfda":"code","3cc054c0":"code","01938346":"code","83e7dbf9":"code","4bad618d":"code","ec8b2c26":"code","16999d66":"code","a3c43e6f":"code","a93d3876":"code","3f7299d0":"code","f79b2af6":"code","2d0bfbf9":"code","27b9dd40":"code","480e52f4":"code","ae29d178":"code","6e8db7b9":"code","61a99312":"code","6a770e3a":"code","5ec26449":"code","515547fa":"code","9975c7ff":"code","9760984a":"code","1a7e1dfb":"code","b3d60bd3":"code","78fa6e63":"code","af1bf710":"code","737df006":"code","fc4cf859":"code","764687fb":"code","7260eddc":"code","90e04bb5":"code","df9be2c1":"code","d6e53418":"code","e72ff6c9":"code","d7d02566":"code","bdbfabdd":"code","f155c1c9":"markdown","16120c0b":"markdown","482c97c9":"markdown","be1f8c09":"markdown","67840c37":"markdown","fba6a275":"markdown","93c8d5ef":"markdown","89ed6a1f":"markdown","9a9a6d37":"markdown","f55d3c6f":"markdown","a38488d9":"markdown","9a870247":"markdown","6b912fb9":"markdown","fc96bf55":"markdown","1d5794fc":"markdown","3091fa40":"markdown","1a2b60ca":"markdown","3d23b670":"markdown","eb80774a":"markdown","a984165e":"markdown","d24b4137":"markdown","9b4a2b37":"markdown","679066a2":"markdown","01bdd242":"markdown","bba90e9c":"markdown","e560af4f":"markdown","bb73528a":"markdown","ef56d16f":"markdown","11b345b7":"markdown","2b6ea381":"markdown","8997b3b1":"markdown","cbb7462b":"markdown","92121784":"markdown","fe784070":"markdown","95f7f821":"markdown","066805e0":"markdown","815deaaa":"markdown","ea97ebfe":"markdown","f169bcaa":"markdown","e7dcbbd7":"markdown"},"source":{"1c46945f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport geopy","30d0e884":"df_train = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col=['Id'])\n\ndf_test = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", index_col=['Id'])","ef84d165":"print('Formato do Dataset:', df_train.shape)","7c8ba055":"df_train.head()","b94ebfda":"df_train.describe()","3cc054c0":"df_train.info()","01938346":"df_train.head()","83e7dbf9":"from sklearn import preprocessing\nLabelEncoder = preprocessing.LabelEncoder()\n\ncorrelationMap = df_train.apply(preprocessing.LabelEncoder().fit_transform).corr()\ncorrelationMap.head()","4bad618d":"plt.figure(figsize=(10,7))\nsns.heatmap(correlationMap, vmin=-0.4, vmax=0.4, cmap = 'autumn')","ec8b2c26":"sns.set()\nsns.pairplot(df_train, palette='mako', height=2)","16999d66":"import plotly.express as px\n\nfig = px.scatter_mapbox(df_train, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n                  color_continuous_scale=px.colors.sequential.solar, size_max=15, zoom=4,\n                  mapbox_style=\"carto-positron\")\nfig.show()","a3c43e6f":"df_train.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.4, s=df_train[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","a93d3876":"df_train.plot(kind=\"scatter\", x=\"total_rooms\", y=\"median_house_value\", alpha=0.4, s=df_train[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(8,6),\n             cmap=plt.get_cmap(\"viridis\"), colorbar=False)","3f7299d0":"df_train.plot(kind=\"scatter\", x=\"total_bedrooms\", y=\"median_house_value\", alpha=0.4, s=df_train[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(8,6),\n             cmap=plt.get_cmap(\"magma\"), colorbar=False)","f79b2af6":"from geopy.distance import geodesic\n\ncoord_LA = (34.0536909, -118.2427666)\ncoord_SD = (32.7174209, -117.1627714)\ncoord_SF = (37.7790262, -122.4199061)\n\ndef dist_cidades(df_train):\n    coord_casa = (df_train[\"latitude\"], df_train[\"longitude\"])\n    df_train[\"dist_to_city\"] = min(geodesic(coord_casa, coord_SF).km, geodesic(coord_casa, coord_LA).km, geodesic(coord_casa, coord_SD).km)\n    \n    return df_train","2d0bfbf9":"df_train = df_train.apply(dist_cidades, axis = 1)","27b9dd40":"df_train.head()","480e52f4":"df_train.plot(kind=\"scatter\", x=\"dist_to_city\", y=\"median_house_value\", alpha=0.4, s=df_train[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(8,6),\n             cmap=plt.get_cmap(\"mako\"), colorbar=False)","ae29d178":"df_train['rooms_per_household'] = df_train['total_rooms']\/df_train['households']\ndf_train['people_per_bedroom'] = df_train['population']\/df_train['total_bedrooms']","6e8db7b9":"df_train.head()","61a99312":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","6a770e3a":"Y = df_train.pop('median_house_value').values.astype(float)\n\nX = df_train","5ec26449":"X = scaler.fit_transform(X)","515547fa":"from sklearn.model_selection import cross_val_score\n\ndef rmsle(y_test,y_pred):\n    return np.sqrt(np.mean((np.log(np.abs(y_pred)+1)-np.log(np.abs(y_test)+1))**2))\n\n# Aqui vamos definir um avaliador simples para os regressores usando valida\u00e7\u00e3o cruzada\ndef scorer(model, X_train, Y_train):\n    score = np.mean(cross_val_score(model, X_train, Y_train, cv = 10))\n    return score","9975c7ff":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Fazendo um holdout dentro da pr\u00f3pria base de treino apenas para avaliar o erro e achar o melhor regressor\nX_train, xTEST, Y_train, yTEST = train_test_split(X, Y, test_size=0.3) \n\nLR = LinearRegression()\nLR.fit(X_train,Y_train)\n\npred = LR.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"O RMSLE para o Regressor Linear \u00e9 de: \", RMSLE)","9760984a":"print(\"A acur\u00e1cia do Regressor Linear \u00e9 de: \", scorer(LR, X_train, Y_train))","1a7e1dfb":"from sklearn.linear_model import Lasso\n\n# Fazendo um holdout dentro da pr\u00f3pria base de treino apenas para avaliar o erro e achar o melhor regressor\nX_train, xTEST, Y_train, yTEST = train_test_split(X, Y, test_size=0.3) \n\nL = Lasso()\nL.fit(X_train,Y_train)\n\npred = L.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"O RMSLE para o Regressor Lasso \u00e9 de: \", RMSLE)","b3d60bd3":"print(\"A acur\u00e1cia do Regressor Lasso \u00e9 de: \", scorer(L, X_train, Y_train))","78fa6e63":"from sklearn.linear_model import Ridge\n\n# Fazendo um holdout dentro da pr\u00f3pria base de treino apenas para avaliar o erro e achar o melhor regressor\nX_train, xTEST, Y_train, yTEST = train_test_split(X, Y, test_size=0.3) \n\nR = Ridge()\nR.fit(X_train,Y_train)\n\npred = R.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"O RMSLE para o Regressor Ridge \u00e9 de: \", RMSLE)","af1bf710":"print(\"A acur\u00e1cia do Regressor Ridge \u00e9 de: \", scorer(R, X_train, Y_train))","737df006":"from sklearn.neighbors import KNeighborsRegressor\n\n# Fazendo um holdout dentro da pr\u00f3pria base de treino apenas para avaliar o erro e achar o melhor regressor\nX_train, xTEST, Y_train, yTEST = train_test_split(X, Y, test_size=0.3) \n\nKNN = KNeighborsRegressor()\nKNN.fit(X_train,Y_train)\n\npred = KNN.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"O RMSLE para o Regressor KNN \u00e9 de: \",RMSLE)","fc4cf859":"print(\"A acur\u00e1cia do Regressor KNN \u00e9 de: \", scorer(KNN, X_train, Y_train))","764687fb":"from sklearn.ensemble import RandomForestRegressor\n\n# Fazendo um holdout dentro da pr\u00f3pria base de treino apenas para avaliar o erro e achar o melhor regressor\nX_train, xTEST, Y_train, yTEST = train_test_split(X, Y, test_size=0.3) \n\nRFR = RandomForestRegressor()\nRFR.fit(X_train,Y_train)\n\npred = RFR.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"O RMSLE para o Regressor Random Forest \u00e9 de: \",RMSLE)","7260eddc":"print(\"A acur\u00e1cia do Regressor Random Forest \u00e9 de: \", scorer(RFR, X_train, Y_train))","90e04bb5":"# Dist\u00e2ncia at\u00e9 uma grande cidade\ndef dist_cidades(df_test):\n    coord_casa = (df_test[\"latitude\"], df_test[\"longitude\"])\n    df_test[\"dist_to_city\"] = min(geodesic(coord_casa, coord_SF).km, geodesic(coord_casa, coord_LA).km, geodesic(coord_casa, coord_SD).km)\n    \n    return df_test\n\ndf_test = df_test.apply(dist_cidades, axis = 1)\n\n# Features extras\ndf_test['rooms_per_household'] = df_test['total_rooms']\/df_test['households']\ndf_test['people_per_bedroom'] = df_test['population']\/df_test['total_bedrooms']\n\n# Normaliza\u00e7\u00e3o dos dados\nX_test = scaler.fit_transform(df_test)","df9be2c1":"RFR.fit(X,Y)\npredict = RFR.predict(X_test)\n\nprint(\"O nosso Regressor Random Forest preveu os seguintes pre\u00e7os m\u00e9dios para as casas:\", predict)","d6e53418":"submission = pd.DataFrame(predict)","e72ff6c9":"submission[0] = df_test.index\nsubmission[1] = predict\nsubmission.columns = ['Id','median_house_value']","d7d02566":"submission.head()","bdbfabdd":"submission.to_csv('submission.csv',index = False)","f155c1c9":"Como podemos perceber, **n\u00e3o existem dados faltantes nesse dataset**, o que torna a prepara\u00e7\u00e3o de dados mais simples.","16120c0b":"## 4. Pr\u00e9-processamento dos dados\nFinalmente, tendo criado todas as nossas features a analisado a base de dados, podemos fazer um pr\u00e9-processamento nos dados, que ir\u00e1 auxiliar no nosso regressor.\n\nIremos aplicar uma normaliza\u00e7\u00e3o linear nos valores do dataset, j\u00e1 que nossas features apresentam intervalos muito diferentes de valores. O objetivo da normaliza\u00e7\u00e3o \u00e9 alterar os valores das colunas num\u00e9ricas no dataset para uma escala comum, sem distorcer as diferen\u00e7as nos intervalos de valores. Vamos precisar do m\u00f3dulo de pr\u00e9-processamento da biblioteca scikit-learn e do utilit\u00e1rio MinMaxScaler.","482c97c9":"Vamos separar o nosso dataset em atributos e r\u00f3tulo:","be1f8c09":"O jeito mais simples de enxergar essas correla\u00e7\u00f5es \u00e9 plotando um **Heat Map** da biblioteca seaborn.\nNele, como podemos ver na legenda da direita, quanto mais clara \u00e9 a c\u00e9lula, maior \u00e9 a correla\u00e7\u00e3o, e quanto mais escura \u00e9 a c\u00e9lula, menor \u00e9 a correla\u00e7\u00e3o entre as vari\u00e1veis.","67840c37":"Percebemos que quanto maior a dist\u00e2ncia at\u00e9 uma grande cidade, menor tende a ser o pre\u00e7o m\u00e9dio da casa","fba6a275":"Com o comando *describe* podemos ter uma vis\u00e3o estat\u00edstica simples do nosso dataset:","93c8d5ef":"Podemos tirar algumas conclus\u00f5es interessantes dessa correla\u00e7\u00e3o representada no Heat Map:\n- As vari\u00e1veis **total_rooms**, **total_bedrooms**, **population** e **households** est\u00e3o fortemente relacionadas entre si, porque, logicamente, quanto mais pessoas ou fam\u00edlias na casa, ela dever\u00e1 ser maior (ter mais salas) e ter mais quartos.\n- Todas as demais vari\u00e1veis apresentam certa correla\u00e7\u00e3o entre si, mas nada t\u00e3o relevante ao nosso modelo\n- A terceira conclus\u00e3o e, provavelmente, a mais importante para o modelo \u00e9 que as vari\u00e1veis que apresentam maior rela\u00e7\u00e3o com o pre\u00e7o m\u00e9dio da casa (*median_house_value*) s\u00e3o, principalmente, **median_income**, **total_rooms**, **total_bedrooms** e **median_age**, o que mostra coer\u00eancia na base de dados, j\u00e1 que retrata um resultado bem intuitivo.","89ed6a1f":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\n#### $\\textit{- An\u00e1lise do dataset California Housing e aplica\u00e7\u00e3o de alguns tipos de regress\u00e3o para predizer o pre\u00e7o m\u00e9dio de uma casa }$\n#### $\\textit{Autor: Victor Rocha da Silva - PMR3508-2020-177}$\n\n## 1. Preparando os dados (Data Prep)\n### 1.1 Importando bibliotecas e dados\n- Primeiramente, vamos importar algumas bibliotecas que ser\u00e3o \u00fateis ao trabalhar com esses dados e, logo em seguida, o dataset.","9a9a6d37":"Agora \u00e9 s\u00f3 obter a predi\u00e7\u00e3o:","f55d3c6f":"### 1.2 Lidando com dados faltantes (*missing data*)\n- Aqui faremos uma an\u00e1lise acerca dos dados faltantes, caso existam, verificando qual a melhor maneira de lidar com eles.","a38488d9":"Feita essa an\u00e1lise de correla\u00e7\u00e3o, podemos partir agora para uma an\u00e1lise explorat\u00f3ria da base de dados.","9a870247":"## 7. Submiss\u00e3o da predi\u00e7\u00e3o","6b912fb9":"### 5.3 Regress\u00e3o Ridge","fc96bf55":"### 5.4 Regress\u00e3o KNN","1d5794fc":"### 5.2 Regress\u00e3o Lasso","3091fa40":"- Aqui temos uma representa\u00e7\u00e3o de um trecho do dataset e de como ele se estrutura:","1a2b60ca":"## 5. Avalia\u00e7\u00e3o de regressores\nVamos testar alguns tipos de regressor e ver aquele que obt\u00e9m o melhor desempenho, o que ser\u00e1 avaliado pelo Root Mean Squared Log Error (RMSLE), que definiremos abaixo. O melhor regressor \u00e9 aquele que apresentar o menor RMSLE, que ser\u00e1 usado para realizarmos a predi\u00e7\u00e3o.","3d23b670":"### 5.1 Regress\u00e3o linear","eb80774a":"Com o comando *info* podemos ver a quantidade de valores n\u00e3o nulos (n\u00e3o faltantes) no dataset:","a984165e":"Agora, s\u00f3 precisamos exportar o nosso data frame com a predi\u00e7\u00e3o para o formato .csv:","d24b4137":"Podemos perceber que as coordenadas geogr\u00e1ficas das casas da base de dados formam uma faixa cont\u00ednua e apresentam valores relativamente pr\u00f3ximos. Vemos ainda que predominam no dataset as pessoas que moram na resid\u00eancia com renda m\u00e9dia inferior a $100000; que quanto maior o n\u00famero de c\u00f4modos na casa, maior tende a ser o n\u00famero de quartos; e, por motivos \u00f3bvios, quanto maior a popula\u00e7\u00e3o da casa, maior tende a ser o n\u00famero de quartos. Al\u00e9m disso, refor\u00e7ando o que j\u00e1 haviamos conclu\u00eddo anteriormente na matriz de correla\u00e7\u00e3o, vemos que h\u00e1 uma grande tend\u00eancia de quanto maior \u00e9 a renda m\u00e9dia da casa, maior \u00e9 o pre\u00e7o da casa (o que \u00e9 algo natural).","9b4a2b37":"Vemos que o n\u00famero de c\u00f4modos e de quartos t\u00eam sim influ\u00eancia sobre o pre\u00e7o m\u00e9dio das casas, mas isso n\u00e3o fica t\u00e3o evidente nos gr\u00e1ficos acima, pois devemos lembrar que cada observa\u00e7\u00e3o do dataset corresponde a uma certa regi\u00e3o da California, em que foi feito um agrupamento de casas e, consequentemente, ao calcular o pre\u00e7o m\u00e9dio, pode ser que a rela\u00e7\u00e3o c\u00f4modos\/quartos-pre\u00e7o fique omitida\/atenuada. Muitas vezes a m\u00e9dia esconde mais do que mostra, principalmente quando se trata de conceitos geogr\u00e1ficos e sociais.","679066a2":"## 3. Feature Engineering\n\nNessa etapa, vamos aproveitar as conclus\u00f5es obtidas na an\u00e1lise explorat\u00f3ria para criar algumas vari\u00e1veis que nos ajudem a predizer o pre\u00e7o da casa.","01bdd242":"## 2. Interpreta\u00e7\u00e3o e an\u00e1lise do data frame\nNessa etapa, temos como objetivo fazer uma an\u00e1lise geral do nosso dataframe, analisando a correla\u00e7\u00e3o entre as vari\u00e1veis e interpretando os dados que temos. Os gr\u00e1ficos ser\u00e3o uma ferramenta de grande import\u00e2ncia para fazer tal an\u00e1lise.\n\n### 2.1 Vis\u00e3o geral da base de dados\nCome\u00e7aremos nossa an\u00e1lise com alguma observa\u00e7\u00f5es simples e relevantes sobre a base de dados.","bba90e9c":"### 3.2 Features extras\nAqui, vamos criar uma feature para o n\u00famero de c\u00f4modos por casa e outra para o n\u00famero de pessoas por quarto em cada casa:","e560af4f":"Mais alguns gr\u00e1ficos relevantes:","bb73528a":"Podemos perceber que as casas mais caras encontram-se, no geral, pr\u00f3ximas ao litoral e pr\u00f3ximas das grandes cidades da Calif\u00f3rnia, como San Francisco, San Diego e, destacadamente, Los Angeles.\nIsso ser\u00e1 muito \u00fatil no momento da predi\u00e7\u00e3o, pois podemos criar algumas vari\u00e1veis extras que auxiliem nosso regressor a prever o pre\u00e7o m\u00e9dio das casas.","ef56d16f":"## 6. Predi\u00e7\u00e3o\nFeita toda a prepara\u00e7\u00e3o de dados e sele\u00e7\u00e3o do melhor regressor, podemos finalmente prever o pre\u00e7o m\u00e9dio das casas do set de teste. \n\nAntes, vamos aplicar os mesmos procedimentos feitos no set de treino no set de teste:","11b345b7":"Vamos come\u00e7ar com um pairplot acerca de todas as vari\u00e1veis:","2b6ea381":"Vamos agora fazer alguns plots interessantes acerca das conclus\u00f5es que tiramos acima e tentar identificar mais alguma informa\u00e7\u00e3o \u00fatil.\n\nCome\u00e7aremos fazendo uma an\u00e1lise mais geogr\u00e1fica do nosso dataset, analisando onde se concentram as casas de maior valor m\u00e9dio e por que, o que nos ajudar\u00e1 a entender melhor a disposi\u00e7\u00e3o das casas pela Calif\u00f3rnia. Uma \u00f3tima maneira de plotar os mapas j\u00e1 com as coordenadas do dataset \u00e9 usar a biblioteca Plotly:","8997b3b1":"### 3.1 Dist\u00e2ncia at\u00e9 uma grande\/importante cidade\n\nPara esse atributo, iremos utilizar a biblioteca Geopy para medir as dist\u00e2ncias entre as regi\u00f5es e as grandes cidades. Vamos utilizar as tr\u00eas principais cidades da Calif\u00f3rnia: Los Angeles, San Francisco e San Diego (pelo fato de serem grandes centros de influ\u00eancia), sendo que consideraremos como dist\u00e2ncia at\u00e9 uma grande cidade sempre a menor dist\u00e2ncia entre as tr\u00eas calculadas.\n\nPrimeiro, precisamos definir uma fun\u00e7\u00e3o que calcula essas dist\u00e2ncias para cada regi\u00e3o do dataset.","cbb7462b":"### 5.5 Regress\u00e3o Random Forest","92121784":"Agora vamos aplicar a fun\u00e7\u00e3o no nosso dataset, criando essa nova feature:","fe784070":"Usando como m\u00e9trica para escolher o melhor regressor o Root Mean Squared Log Error (RMSLE), vemos claramente que o **Regressor Random Forest** \u00e9 o que se destaca. Portanto, vamos us\u00e1-lo para fazer a nossa predi\u00e7\u00e3o.","95f7f821":"Precisamos agora transformar esse array com as predi\u00e7\u00f5es em um data frame e em seguida export\u00e1-lo para o formato .csv:","066805e0":"Vamos analisar agora a rela\u00e7\u00e3o entre a renda m\u00e9dia na casa com o pre\u00e7o da casa em si, j\u00e1 que essa foi uma importante correla\u00e7\u00e3o encontrada:","815deaaa":"### 2.3 An\u00e1lise explorat\u00f3ria dos dados\nIremos explorar os dados da base California Housing e obter algumas conclus\u00f5es acerca dos dados, principalmente usando gr\u00e1ficos. Espera-se que tanto essa etapa, como a busca por correla\u00e7\u00e3o seja \u00fatil na escolha dos atributos e cria\u00e7\u00e3o do nosso modelo.\nPara os gr\u00e1ficos, usaremos as bibliotecas Matplotlib e Seaborn, j\u00e1 importadas.","ea97ebfe":"Vamos tamb\u00e9m acrescentar a coluna Id e renomear a coluna correspondente do data frame para *median_house_value*, que \u00e9 o que estamos prevendo com o regressor:","f169bcaa":"A nossa base de dados apresenta 9 *features*, sendo uma delas a vari\u00e1vel de classe *median_house_value* que queremos prever.","e7dcbbd7":"### 2.2 Busca por correla\u00e7\u00f5es entre as vari\u00e1veis\nPrecisamos buscar **correla\u00e7\u00f5es** entre as vari\u00e1veis, verificando quais influem significativamente no pre\u00e7o da casa e quais n\u00e3o, para montar um modelo adequado e eficiente. Al\u00e9m disso, \u00e9 valido enxergar tamb\u00e9m quais vari\u00e1veis estao ligadas entre si na base. \n\nPara fazer isso, vamos ter que aplicar o LabelEncoder nos dados, da biblioteca scikit-learn."}}