{"cell_type":{"ae0f3df0":"code","b56803b2":"code","f4e71f10":"code","46dcef33":"code","bed20ef8":"code","ec7adec0":"code","0b570cb6":"code","a5ce098e":"code","9b770f31":"code","7df02488":"code","01df9e5f":"code","0e00cd82":"code","6f84526e":"code","729cdee0":"code","94502bfd":"code","ca6a9fd8":"code","b739bc8d":"code","2673101e":"code","eff231cb":"code","94cf72fe":"code","daa40cc1":"code","c6097ad1":"code","f92334cd":"code","51c12d25":"code","01c12b82":"code","2b8bcb8d":"code","f0c17179":"code","87571fda":"code","2fc9cfa4":"code","d2df0228":"code","62852033":"code","153b35fb":"code","9dea3e26":"code","5cf99720":"code","e73620a6":"code","fc67f60d":"code","5c85f3e8":"code","2ae1a2b4":"code","3a840229":"code","5ba9df3d":"code","0f802e8e":"markdown","8d76c280":"markdown","e7042412":"markdown","b8b7ff23":"markdown","bac6e357":"markdown","199fa30a":"markdown","30fcbf3b":"markdown","a1ce1bd2":"markdown","e290df0d":"markdown","7cb647ea":"markdown","e2766bf0":"markdown","88238628":"markdown","8412eb0e":"markdown","63be660f":"markdown","bceffffc":"markdown","a6a0ae4b":"markdown","fbd0b5d4":"markdown","d22feee3":"markdown","df5b3b8e":"markdown","02da40ef":"markdown","3a84a8e4":"markdown","fef89300":"markdown","ea07ce6e":"markdown","da7087a6":"markdown","4e29d9f4":"markdown","c0276694":"markdown","9f36eccb":"markdown","d4301250":"markdown","4fb7c049":"markdown","cf7d7881":"markdown","2089ea31":"markdown","39cd7191":"markdown","9040d0aa":"markdown","cab14c65":"markdown","434570a5":"markdown","8b1d1305":"markdown","917f0261":"markdown","8750520b":"markdown","5ab4294b":"markdown","960d3917":"markdown","dc5e6848":"markdown","ede2529c":"markdown","52983e21":"markdown","7759187a":"markdown","7efbac71":"markdown"},"source":{"ae0f3df0":"#!pip install --ignore-installed --upgrade tensorflow==2.4.1 --user","b56803b2":"import time\nimport numpy as np\nimport os\nimport path\nimport pydot\nfrom typing import List, Tuple\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport PIL.Image\nimport pathlib\nimport shutil\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.models import Model, load_model\n\nfrom tensorflow.python.keras.utils import layer_utils\n#from tensorflow.keras.utils.vis_utils import model_to_dot\nfrom tensorflow.keras.utils import model_to_dot\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\n\nfrom IPython.display import SVG\n\nimport scipy.misc\n\nimport tensorflow.keras.backend as K\nK.set_image_data_format('channels_last') # can be channels_first or channels_last. \nK.set_learning_phase(1) # 1 stands for learning phase","f4e71f10":"def identity_block(X: tf.Tensor, level: int, block: int, filters: List[int]) -> tf.Tensor:\n    \"\"\"\n    Creates an identity block (see figure 3.1 from readme)\n\n    Input:\n        X - input tensor of shape (m, height_prev, width_prev, chan_prev)\n        level - integer, one of the 5 levels that our networks is conceptually divided into (see figure 3.1 in the readme file)\n              - level names have the form: conv2_x, conv3_x ... conv5_x\n        block - each conceptual level has multiple blocks (1 identity and several convolutional blocks)\n                block is the number of this block within its conceptual layer\n                i.e. first block from level 2 will be named conv2_1\n        filters - a list on integers, each of them defining the number of filters in each convolutional layer\n\n    Output:\n        X - tensor (m, height, width, chan)\n    \"\"\"\n\n    # layers will be called conv{level}_iden{block}_{convlayer_number_within_block}'\n    conv_name = f'conv{level}_{block}' + '_{layer}_{type}'\n\n    # unpack number of filters to be used for each conv layer\n    f1, f2, f3 = filters\n\n    # the shortcut branch of the identity block\n    # takes the value of the block input\n    X_shortcut = X\n\n    # first convolutional layer (plus batch norm & relu activation, of course)\n    X = Conv2D(filters=f1, kernel_size=(1, 1), strides=(1, 1),\n               padding='valid', name=conv_name.format(layer=1, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=1, type='bn'))(X)\n    X = Activation('relu', name=conv_name.format(layer=1, type='relu'))(X)\n\n    # second convolutional layer\n    X = Conv2D(filters=f2, kernel_size=(3, 3), strides=(1, 1),\n               padding='same', name=conv_name.format(layer=2, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=2, type='bn'))(X)\n    X = Activation('relu')(X)\n\n    # third convolutional layer\n    X = Conv2D(filters=f3, kernel_size=(1, 1), strides=(1, 1),\n               padding='valid', name=conv_name.format(layer=3, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=3, type='bn'))(X)\n\n    # add shortcut branch to main path\n    X = Add()([X, X_shortcut])\n\n    # relu activation at the end of the block\n    X = Activation('relu', name=conv_name.format(layer=3, type='relu'))(X)\n\n    return X","46dcef33":"def convolutional_block(X: tf.Tensor, level: int, block: int, filters: List[int], s: Tuple[int,int,int]=(2, 2)) -> tf.Tensor:\n    \"\"\"\n    Creates a convolutional block (see figure 3.1 from readme)\n\n    Input:\n        X - input tensor of shape (m, height_prev, width_prev, chan_prev)\n        level - integer, one of the 5 levels that our networks is conceptually divided into (see figure 3.1 in the readme file)\n              - level names have the form: conv2_x, conv3_x ... conv5_x\n        block - each conceptual level has multiple blocks (1 identity and several convolutional blocks)\n                block is the number of this block within its conceptual layer\n                i.e. first block from level 2 will be named conv2_1\n        filters - a list on integers, each of them defining the number of filters in each convolutional layer\n        s   - stride of the first layer;\n            - a conv layer with a filter that has a stride of 2 will reduce the width and height of its input by half\n\n    Output:\n        X - tensor (m, height, width, chan)\n    \"\"\"\n\n    # layers will be called conv{level}_{block}_{convlayer_number_within_block}'\n    conv_name = f'conv{level}_{block}' + '_{layer}_{type}'\n\n    # unpack number of filters to be used for each conv layer\n    f1, f2, f3 = filters\n\n    # the shortcut branch of the convolutional block\n    X_shortcut = X\n\n    # first convolutional layer\n    X = Conv2D(filters=f1, kernel_size=(1, 1), strides=s, padding='valid',\n               name=conv_name.format(layer=1, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=1, type='bn'))(X)\n    X = Activation('relu', name=conv_name.format(layer=1, type='relu'))(X)\n\n    # second convolutional layer\n    X = Conv2D(filters=f2, kernel_size=(3, 3), strides=(1, 1), padding='same',\n               name=conv_name.format(layer=2, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=2, type='bn'))(X)\n    X = Activation('relu', name=conv_name.format(layer=2, type='relu'))(X)\n\n    # third convolutional layer\n    X = Conv2D(filters=f3, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n               name=conv_name.format(layer=3, type='conv'),\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=conv_name.format(layer=3, type='bn'))(X)\n\n    # shortcut path\n    X_shortcut = Conv2D(filters=f3, kernel_size=(1, 1), strides=s, padding='valid',\n                        name=conv_name.format(layer='short', type='conv'),\n                        kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3, name=conv_name.format(layer='short', type='bn'))(X_shortcut)\n\n    # add shortcut branch to main path\n    X = Add()([X, X_shortcut])\n\n    # nonlinearity\n    X = Activation('relu', name=conv_name.format(layer=3, type='relu'))(X)\n\n    return X","bed20ef8":"def ResNet50(input_size: Tuple[int,int,int], classes: int) -> Model:\n    \"\"\"\n        Builds the ResNet50 model (see figure 4.2 from readme)\n\n        Input:\n            - input_size - a (height, width, chan) tuple, the shape of the input images\n            - classes - number of classes the model must learn\n\n        Output:\n            model - a Keras Model() instance\n    \"\"\"\n\n    # tensor placeholder for the model's input\n    X_input = Input(input_size)\n\n    ### Level 1 ###\n\n    # padding\n    X = ZeroPadding2D((3, 3))(X_input)\n\n    # convolutional layer, followed by batch normalization and relu activation\n    X = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2),\n               name='conv1_1_1_conv',\n               kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name='conv1_1_1_nb')(X)\n    X = Activation('relu')(X)\n\n    ### Level 2 ###\n\n    # max pooling layer to halve the size coming from the previous layer\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # 1x convolutional block\n    X = convolutional_block(X, level=2, block=1, filters=[64, 64, 256], s=(1, 1))\n\n    # 2x identity blocks\n    X = identity_block(X, level=2, block=2, filters=[64, 64, 256])\n    X = identity_block(X, level=2, block=3, filters=[64, 64, 256])\n\n    ### Level 3 ###\n\n    # 1x convolutional block\n    X = convolutional_block(X, level=3, block=1, filters=[128, 128, 512], s=(2, 2))\n\n    # 3x identity blocks\n    X = identity_block(X, level=3, block=2, filters=[128, 128, 512])\n    X = identity_block(X, level=3, block=3, filters=[128, 128, 512])\n    X = identity_block(X, level=3, block=4, filters=[128, 128, 512])\n\n    ### Level 4 ###\n    # 1x convolutional block\n    X = convolutional_block(X, level=4, block=1, filters=[256, 256, 1024], s=(2, 2))\n    # 5x identity blocks\n    X = identity_block(X, level=4, block=2, filters=[256, 256, 1024])\n    X = identity_block(X, level=4, block=3, filters=[256, 256, 1024])\n    X = identity_block(X, level=4, block=4, filters=[256, 256, 1024])\n    X = identity_block(X, level=4, block=5, filters=[256, 256, 1024])\n    X = identity_block(X, level=4, block=6, filters=[256, 256, 1024])\n\n    ### Level 5 ###\n    # 1x convolutional block\n    X = convolutional_block(X, level=5, block=1, filters=[512, 512, 2048], s=(2, 2))\n    # 2x identity blocks\n    X = identity_block(X, level=5, block=2, filters=[512, 512, 2048])\n    X = identity_block(X, level=5, block=3, filters=[512, 512, 2048])\n\n    # Pooling layers\n    X = AveragePooling2D(pool_size=(2, 2), name='avg_pool')(X)\n\n    # Output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', name='fc_' + str(classes),\n              kernel_initializer=glorot_uniform(seed=0))(X)\n\n    # Create model\n    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n\n    return model","ec7adec0":"# set input image parameters\nimage_size = (64, 64) \nchannels = 3\nnum_classes = 10","0b570cb6":"model = ResNet50(input_size = (image_size[1], image_size[0], channels), classes = num_classes)","a5ce098e":"model.summary()","9b770f31":"# path to desired image set, relative to current working dir\nin_folder = os.path.join('..', 'input', 'animals10', 'raw-img')\n\nfile_count = []\n\n# get number of images in each folder (images per class)\nfor fld in os.listdir(in_folder):\n    crt = os.path.join(in_folder, fld)\n    \n    image_count = len(os.listdir(crt))\n    \n    file_count.append(image_count)\n    \n    print(f'{crt} contains {image_count} images')","7df02488":"print(f'Total number of images: {sum(file_count)}')","01df9e5f":"os.listdir(os.path.join(in_folder, 'elefante'))[:10]","0e00cd82":"data_dir = pathlib.Path(in_folder)\n\nelefante = list(data_dir.glob('elefante\/*'))\n\nPIL.Image.open(str(elefante[0]))","6f84526e":"PIL.Image.open(str(elefante[0])).size","729cdee0":"PIL.Image.open(str(elefante[10]))","94502bfd":"PIL.Image.open(str(elefante[10])).size","ca6a9fd8":"# path to output folder, relative to current working dir\nout_folder = os.path.join('..', 'output', 'animals10', 'processed')","b739bc8d":"def square_crop_image(im: PIL.Image) -> PIL.Image:\n    width, height = im.size\n    new_size = min(width, height)\n\n    # center crop\n    left = (width - new_size) \/ 2\n    top = (height - new_size) \/ 2\n    right = (width + new_size) \/ 2\n    bottom = (height + new_size) \/ 2\n\n    crop_im = im.crop((left, top, right, bottom))\n    crop_im = crop_im.convert('RGB')\n\n    return crop_im","2673101e":"def make_dataset(in_folder, im_per_class):\n    # iterate through all folders (there should be one folder per object class)\n    for fld in os.listdir(in_folder):\n        # create the output folder for processed images for current class\n        # delete folder and contents if there is one already\n        out = os.path.join(out_folder, fld)\n        if os.path.exists(out):\n            shutil.rmtree(out)\n        os.makedirs(out)\n\n        fld_path = pathlib.Path(os.path.join(in_folder, fld))\n        num_images = 0\n        for file in list(fld_path.glob('*')):\n            # open image, center crop to a square\n            # save to the output folder\n            with PIL.Image.open(file) as im:\n                crop_im = square_crop_image(im)\n                crop_im.save(os.path.join(out, str(num_images) + '.jpg'))\n                im.close()\n            # break when desired number of images\n            # has been processed (to keep classes balance)\n            num_images = num_images + 1\n            if (num_images > im_per_class):\n                break","eff231cb":"# get the number of images that will make our classes balanced\nim_per_class = min(file_count)\n\n# process input images \nmake_dataset(in_folder, im_per_class)","94cf72fe":"img_height = image_size[1]\nimg_width = image_size[0]\nbatch_size = 32","daa40cc1":"data_dir = pathlib.Path(out_folder)","c6097ad1":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split = 0.2,\n    subset=\"training\",\n    label_mode='categorical', # default mode is 'int' label, but we want one-hot encoded labels (e.g. for categorical_crossentropy loss)\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    label_mode='categorical',\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","f92334cd":"class_names = train_ds.class_names\nprint(class_names)","51c12d25":"plt.figure(figsize=(10, 10))\n\ni = 1\n\nfor images, labels in train_ds.take(1):\n    for (image, label) in zip(images, labels): \n        ax = plt.subplot(4, 4, i)\n        plt.imshow(image.numpy().astype(\"uint8\"))\n        plt.title(class_names[tf.argmax(label, axis=0)])\n        plt.axis(\"off\")\n        i = i + 1\n        if i == 17:\n            break\nplt.show()","01c12b82":"# use keras functionality for adding a rescaling layer\nnormalization_layer = layers.experimental.preprocessing.Rescaling(1.\/255)\n\n# rescale training and validation sets\nnorm_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nnorm_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))","2b8bcb8d":"image_batch, labels_batch = next(iter(norm_train_ds))\n\n# get one image\nfirst_image = image_batch[0]\n\n# confirm pixel values are now in the [0,1] range\nprint(np.min(first_image), np.max(first_image))","f0c17179":"model.compile(\n    optimizer='adam', # optimizer\n    loss='categorical_crossentropy', # loss function to optimize \n    metrics=['accuracy'] # metrics to monitor\n)","87571fda":"AUTOTUNE = tf.data.AUTOTUNE\n\nnorm_train_ds = norm_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nnorm_val_ds = norm_val_ds.cache().prefetch(buffer_size=AUTOTUNE)","2fc9cfa4":"import time\n\nstart = time.time()\n\nmodel.fit(\n    norm_train_ds, \n    validation_data=norm_val_ds,\n    epochs = 2)\n\nstop = time.time()\n\nprint(f'Training took: {(stop-start)\/60} minutes')","d2df0228":"model_on_gpu = ResNet50(input_size = (image_size[1], image_size[0], channels), classes = num_classes)\nmodel_on_gpu.compile(\n    optimizer='adam', # optimizer\n    loss='categorical_crossentropy', # loss function to optimize\n    metrics=['accuracy'] # metrics to monitor\n)","62852033":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","153b35fb":"callbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", # monitor validation loss (that is, the loss computed for the validation holdout)\n        min_delta=1e-2, # \"no longer improving\" being defined as \"an improvement lower than 1e-2\"\n        patience=10, # \"no longer improving\" being further defined as \"for at least 10 consecutive epochs\"\n        verbose=1\n    )\n]","9dea3e26":"start = time.time()\nwith tf.device('\/gpu:0'):\n    history = model_on_gpu.fit(\n        norm_train_ds, \n        validation_data=norm_val_ds,\n        epochs=40,\n        callbacks=callbacks,\n    )\nstop = time.time()\nprint(f'Training on GPU took: {(stop-start)\/60} minutes')","5cf99720":"print(history.history.keys())\n\nplt.figure(figsize=(12, 10))\n\n# summarize history for accuracy\nplt.subplot(2, 1, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(2, 1, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","e73620a6":"#preds = model_on_gpu.evaluate(X_test, Y_test)\npreds = model_on_gpu.evaluate(norm_val_ds)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","fc67f60d":"import cv2\n\ntest_img_path = '..\/input\/just-one-cat\/cat_1.jpg'\n\n# read image\nimg = cv2.imread(test_img_path)\n\n# reorder RGB channels (opencv reads as BGR by default)\nRGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# resize to the same input image size that we used when defining the model architecture\nx = cv2.resize(RGB_img, image_size, interpolation = cv2.INTER_AREA)\n\nprint('Input image shape:', x.shape) # sanity check\n\nplt.imshow(RGB_img)","5c85f3e8":"x = np.expand_dims(x, axis=0) # fake batch size dimension\nx = x\/255.0 # normalize pixel values to [0,1] interval\n\npreds = model_on_gpu.predict(x) # get model predictions\n\nprint(\"Model's prediction has this format: [p(0), p(1), ... , p(5)] = \")\nprint(preds)\n\n\n\nprint(\"The prediction for this image is: \", class_names[np.argmax(preds)])","2ae1a2b4":"len(preds[0])\n\nfor i, p in enumerate(preds[0]):\n    print(f'p:{p:.05f}\\t{class_names[i]}')","3a840229":"model_on_gpu_40e = ResNet50(input_size = (image_size[1], image_size[0], channels), classes = len(class_names))\n\nmodel_on_gpu_40e.compile(\n    optimizer='adam', # optimizer\n    loss='categorical_crossentropy', # loss function to optimize\n    metrics=['accuracy'] # metrics to monitor\n)\n\nstart = time.time()\nwith tf.device('\/gpu:0'):\n    history = model_on_gpu_40e.fit(\n        norm_train_ds, \n        validation_data=norm_val_ds,\n        epochs=40,\n        #batch_size=64,\n    )\nstop = time.time()\n\nprint(f'Training for 40 epochs on GPU took: {(stop-start)\/60} minutes')","5ba9df3d":"print(history.history.keys())\n\nplt.figure(figsize=(12, 10))\n\n# summarize history for accuracy\nplt.subplot(2, 1, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(2, 1, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","0f802e8e":"# 6. Training the ResNet model on Kaggle GPU","8d76c280":"In the previous section I talked about the **identity block**. The one above skipped over two convolution layers. A more powerful version of the residual block is one that skips over three convolutional layers. In this notebook I will implement the latter.\n\nI will also use a batch normalization layer after each convolutional layer, which helps by speeding up the training. If you want to read more about this, check out this article on <a href=\"https:\/\/machinelearningmastery.com\/batch-normalization-for-training-of-deep-neural-networks\/\">Machine Learning Mastery<\/a>.\n\nThe **identity block** is used when the input x has the same dimension (width and height) as the output of the third layer. \nWhen this condition is not met, we use a **convolution block**. \n\nThe only difference between the identity block and the convolution block is that the second has another convolution layer (plus a batch normalization) on the skip conection path. The convolution layer on the skip connection path has the purpose of resizing x so that its dimension matches the output and thus we would be able to add those two together.\n\n![image.png](attachment:image.png)\n\nI will use 3x3 and 1x1 convolutional layers, like in the original ResNet article.","e7042412":"In this notebook I'll code a <strong>50-layer ResNet<\/strong> model from scrath using Keras.<br\/><br\/>\nResNets were introduced by <a href=\"https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He et al. 2016<\/a> to bypass the problem or <a href=\"https:\/\/www.youtube.com\/watch?v=qhXZsFVxGKo\">vanishing and exploding gradients in very deep neural networks<\/a>. \n<br\/><br\/>\nI want to implement this model myself instead of using an existing library because this will give me a deeper understanding of ResNet and because it's a nice ooportunity to get familiar with <a href=\"https:\/\/keras.io\/\">Keras<\/a>.\n<br\/><br\/> \nFor this project I'll use the <a href=\"https:\/\/www.kaggle.com\/alessiocorrado99\/animals10\">10 Animals dataset available on Kaggle<\/a>. \n<br\/><br\/> \nIf you want to jump right to using a ResNet, have a look at <a href='https:\/\/keras.io\/api\/applications\/'>Keras' pre-trained models<\/a>. In this Notebook I will code my ResNet from scratch not out of need, as implementations already exist, but as a valuable learning process.\n<br\/><br\/>\nTo see the duration of training on a Kaggle CPU, don't activate the GPU accelerator for this Notebook until you reach section 6. When I publish this Notebook, I can either have the GPU activated from the start for the whole Notebook or not at all, I can't have half-half, for demo purposes.\n<br\/><br\/>\nSo, in order to observe the snail speed of section 5, you should manually deactivate GPU and reactivate once you reach section 6 (at which point, you'll have to re-run everything, but will be fast from then on).  \n\nFor a Python project instead of a Jupyter notebook, see <a href='https:\/\/github.com\/mihaelagrigore\/ResNet-Keras-code-from-scratch-train-on-GPU'>my github repo: Deep Learning with Tensorflow & Keras<\/a>","b8b7ff23":"On my laptop, training for two epochs takes a huge amount of time. Even after half an hour of training, the accuracy on the training set is 24%, which is of course not acceptable in a real world scenario. \n\nOn Kaggle, on a CPU, training takes even longer than on my personal laptop. Kaggle CPU is very very slow.\n\nSo we'll have to activate Kaggle's GPU and retrain. In the next section we'll see how.","bac6e357":"Now that we defined our model, it's time to train it.  \n\n### 5.1 Loading and preparing the image dataset.","199fa30a":"# 9. Wrapping up","30fcbf3b":"### 7.1 Let's evaluate our model on the test set.","a1ce1bd2":"If you're a beginner with TensorFlow, like me, you might find this <a href=\"https:\/\/www.tensorflow.org\/guide\/intro_to_graphs\">introduction to TensorFLow graphs<\/a> useful. The alternative to graphs is running the model in eager mode, which you can read a little bit about <a href=\"https:\/\/www.tensorflow.org\/guide\/eager\">here<\/a>.\n\nLet's instantiate the model.  ","e290df0d":"Let's examine the structure of our model by running the following code:","7cb647ea":"We use a callback to stop the training process when the accuracy is no longer improving.","e2766bf0":"Helper function for cropping images into a square.","88238628":"First, choose an optimizer schedule, a loss function and metrics to track during training.","8412eb0e":"In the previous section, we used Keras Callbacks to stop training at the optimum number of epochs: when the loss for the validation set was no longer improving.  \n\nThe stopping happened very early in the training. Why do you think that happened ? Have a look at the images we used from 10 different classes, take a look at our model's structure and try to compute the number of parameters the network has to learn through training. Please leave your comments below regarding what can be the cause of our model not learning anymore after only a a couple of epochs. I believe this discussion is quite beneficial to our learning process.  \n\nNow, what happens if I decide not to stop training at epoch < 10 ? I want to see what I end up with if I keep training for 100 epochs. ","63be660f":"First thing to notice is that classes are imbalanced. There are two options so we can have the same number of training images from all classes:\n- downsampling (remove the extra images from classes that have more than the min number of images across all classes)\n- image augmentation (artificially create more images for some classes)\n\nFor this project I will choose the first option, as image augmentation is not the main goal today.","bceffffc":"Here I implemented a 50-layer ResNet using Keras, following the architecture published by <a href=\"https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He et al. 2016<\/a> (with slight changes).  \n\nI first tried training on CPU, which was slow. Then I switched on Kaggle's GPU and started training fast.  \n\nI used Kers Callbacks to stop training at the optimum number of epoch, when loss for the validation set was no longer improving. The stopping happened very early in the training when the validation error was still pretty high.  \n\nThen I insisted on training for the whole 40 epochs and noticed no further improvement (no surprise though, I just wanted to see what the learning curve would look like).  \n\nHere's what happened:\n- the model has 23,555,082 trainable parameters\n- we ask it to learn to classify 10 different classes, after seeing 1446 of each, so 14460 images  \n- at the same time, the dataset chosen is a difficult one: it sometimes contains multiple instances of the same class in one image and it shows one class from the front, as well as from the side. \n\nUsually, compumter vision models are trained on millions of images, training data is also 'augmented' (through various processing techniques like rotations, change in brightness, hue etc), Training from scratch on a handful of data is a nice exercise, but this approach is only to be used as learning experience. The way to go with CNN is to take a pre-trained model, adapt the architecture to the current problem (by replacing the last layer(s)) and fine-tune on the current data. \n\nThe above would explain the low accuracy on training and validation set. The other problem I noticed was the unstable accuracy, jumping up and down abruptly from one epoch to the next. I would experiment with the batch size and\/or the learning rate to fix this problem. For example, this was the training setup used in the [He et al Resnet paper from 2016](https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n![image.png](attachment:e25c59ad-4526-4161-b367-476771913701.png)\n\nBut I will stop here. The goal was to implement the model architecture from scratch using Tensorflow \/ Keras, not to get a good performance. For good performance I would certainly fine-tune a pre-trained model from the Tensorflow Model Zoo ^^","a6a0ae4b":"Let's have a look at how our metrics progressed across epochs for training and validation sets.  \nWe examine both accuracy and loss to see if we can draw any interesting conclusions out of it.","fbd0b5d4":"I will just hardcode the image and filter dimensions to make it easier to read. In a real developing environment, most values below should look like INPUT_WIDTH, 2 * INPUT_WIDTH and so on.","d22feee3":"Confirm rescaling results look as expected.","df5b3b8e":"Now we use Keras preprocessing utilities to prepare our dataset. The method I am using below for loading images from separate folders is described in <a href='https:\/\/www.tensorflow.org\/tutorials\/load_data\/images'>TensforFlow official documentation here<\/a>.\n\nI will choose labels to be categorical (one-hot encoded). You can read about how one-hot encoding is implemented in TensorFlow <a href='https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/one_hot'>here<\/a>.  \n\nFor a discussion on why one hot encoding is preferred, check out section 4 in <a href='https:\/\/machinelearningmastery.com\/how-to-prepare-categorical-data-for-deep-learning-in-python\/'>this tutorial<\/a>.","02da40ef":"# 4. Defining the ResNet model\n\nHere is what my 50 layers ResNet will look like:\n\n![image.png](attachment:image.png)","3a84a8e4":"Make sure we have enabled the GPU.","fef89300":"# 7. Making predictions","ea07ce6e":"Examine a few processed images before proceesing to the training.","da7087a6":"Cropping was not the best solution, but I'll work with that now.\n\n \nWe convert our labels (e.g. right now we have a label of 1 which stands for the class 'cavallo' for example) through one-hot-encoding (our label of 1 from my example thus becomes the array: 0 1 0 0 0 0 0 0 0 0).  \n","4e29d9f4":"# 1. Setting up our environment\n\nKeras is a high level API on top of TensorFlow. More information, as well as documentation, is available <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\">here, on TensorFlow.org<\/a>. But probably a better introduction is the one on <a href=\"https:\/\/keras.io\/about\/\">Keras' website<\/a>.\n\nIt used to be the case that Keras was a high level API that you could configure to use one of three possible underlying Neural Networks API:  \n- TensorFlow\n- Theano\n- CNTK\n\nLater, Keras became integrated with TensorFlow and it comes bundled in latest versions of it. So I no longer have to install it as a separate library, like it was the case in the past. \n\nTherefore, in order to install Keras, I need to get TensorFlow.\n\nIf you don't have Tensorflow installed and if you're using Anaconda, like me (I created this notebook in my local Jupyter Notebook), check <a href=\"https:\/\/anaconda.org\/search?q=tensorflow\">Anaconda official website<\/a> to see which version is available for installation through Anaconda. \n\nAt the moment, the latest available through Anaconda is 2.3.0\n\n![image.png](attachment:image.png)\n\nBut I want the global latest, which is 2.4. So I used pip to install it. ","c0276694":"Let's inspect the contents of an image folder.  ","9f36eccb":"### 7.2 Let's try a new, unseen image","d4301250":"## Table of contents\n\n1. [Setting up our environment](#1.-Setting-up-our-environment)\n2. [What are ResNets and how they work](#2.-What-are-ResNets-and-how-they-work)\n3. [ResNet building blocks](#3.-The-building-blocks)\n4. [Defining the ResNet model](#4.-Defining-the-ResNet-model)\n5. [Training the ResNet model](#5.-Training-the-ResNet-model)\n6. [Training the ResNet model on Kaggle GPU](#6.-Training-the-ResNet-model-on-Kaggle-GPU)\n7. [Making predictions](#7.-Making-predictions)\n8. [Overfitting when training our Residual Neural Network](#8.-Overfitting-when-training-our-Residual-Neural-Network)\n9. [Wrapping up](#9.-Wrapping-up)","4fb7c049":"# 5. Training the ResNet model","cf7d7881":"Configure the model like before.","2089ea31":"Configure the dataset for performance by enabling prefetching. Read more <a href=\"https:\/\/www.tensorflow.org\/guide\/data_performance\">here<\/a>.","39cd7191":"Have a look at the contents of the data folder and see how many images we have.","9040d0aa":"# 8. Overfitting when training our Residual Neural Network","cab14c65":"Let's import the necessary libraries for this project.  ","434570a5":"Train the model.","8b1d1305":"Parameters for creating the tf datasets: \n- size of the input images\n- batch size for training ","917f0261":"Run the following cell to train your model on 2 epochs with a batch size of 32. We also use this to see how long it takes to train for one epoch.","8750520b":"The building block above is called a residual block or the identity block. The \"weight layer\" is a typical convolutional layer** , the classical type for image recognition networks. Normally, we would have a convolutional layer, then a non-linearity (here, the ReLU function), a convolutional layer, a ReLU and so on.  \n\nResNets introduce a **direct connection** (a bypass) that jumps over two convolutional layers and the ReLU in between (it can be more than two though). This way the second ReLU in this picture will have as input, instead of F(x) (like a typical CNN), an F(x) + x.\n\nIt was found empirically that a ResNet did not behave like a traditional very deep NN. Can be seen in the image below, from the same article - better than expressed through words.\n\n![image.png](attachment:image.png)\n\n\\* For an in-depth description of the backpropagation mechanism and also to understand the Mathematics behind it, I recommend <a href=\"http:\/\/neuralnetworksanddeeplearning.com\/chap2.html\">Chapter 2 from Michael Nielsen's free book on Neural Networks and Deep Learning: Introduction to the core principles<\/a>. I found this to be the best resource on this topic.  \n\n** For an intro to Convolutional Neural Networks, see this overview from the freely available <a href=\"https:\/\/cs231n.github.io\/convolutional-networks\/\">CS231n: Convolutional Neural Networks for Visual Recognition<\/a> from Stanford. ","5ab4294b":"# 2. What are ResNets and how they work\n\nCurrently, there is a trend in Machine Learning, especially in computer vision and speech recognition, to use deeper and deeper neural networks. The reason behind this is the current consensus that deeper networks can learn more complex problems. \n\nDeep Neural Networks suffer from one pervasive problem: <a href=\"https:\/\/www.youtube.com\/watch?v=qhXZsFVxGKo\">vanishing gradients <\/a>. There's also the less common problem of <a href=\"https:\/\/www.youtube.com\/watch?v=qhXZsFVxGKo\">exploding gradients.<\/a>\n\nVanishing gradients make the network learn very slow or may completely stop it from continuing to learn. Why ? The network learns (improves its weights and biases) through backpropagation. During backpropagation*, I computes the partial derivative of the cost function with respect to each weight and bias in each layer, progressively going from the final layers towards the front layers. \n\nIn a very deep network, as we progress from last layers towards the first layers and computing partial derivatives, these values become smaller and smaller. Which means we will do near zero modifications to weights and biases in front layers. This basically means those layers are hardly doing any learning.  \n\nWhile working on very deep computer vision networks, <a href=\"https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He et al. 2016<\/a> found a way to bypass the vanishing gradients problem.  \n\nIt doesn't sound so intuitive to me how He's approach solves this problem, but it works. Their solution is to take the output of one layer and to jump over a few layers and input this deeper into the neural network. Here is how the authors illustrate this mechanism in their <a href=\"https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">article.<\/a>\n\n![image.png](attachment:image.png)\nReproduced from <a href=\"https:\/\/openaccess.thecvf.com\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\">He et al 2016.<\/a>","960d3917":"# <center>Coding a customized ResNet50 in Keras","dc5e6848":"First thing I notice: the images are not squared. When I defined the ResNet50 network architecture, I set the input to 64x64 RGB images.   \n\nSo, for this particular dataset, I'll just crop the images to a square the size of their respective height. I can also resize directly, in which case the images would look vertically stretched. But since the target object looks almost centered, I think I can get away with cropping. \n\nOur images have different sizes too. It's not a problem. After cropping them into squares, I will resize to (64, 64).\n\nAt the same time, I'll delete the extra images. As mentioned earlier, I want to have equal amounts of data for all target classes.  \n\nEarlier we made a list of number of images we have for each class. We will keep min(file_count) files in each animal category and delete the rest.","ede2529c":"# 3. ResNet building blocks","52983e21":"See a few images.","7759187a":"### 5.2 Model training","7efbac71":"Helper functions that processes all input images, squares them and saves the processed images to a new folder.  \nIt also makes sures out processed folders will have an equal amount of images for all classes."}}