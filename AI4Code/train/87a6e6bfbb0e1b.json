{"cell_type":{"16ab527b":"code","d2fd8f7c":"code","f62674c4":"code","c7536299":"code","3443edf5":"code","d365aca6":"code","6c500445":"code","a6d3dbf3":"code","8ba5fe0f":"code","03aaf87c":"code","e044411a":"code","0bb3459f":"code","389de423":"code","134400a4":"code","5f87e931":"code","8f69797a":"code","72f0ef75":"code","09e2b4dc":"code","db6dc4fd":"code","59708fbf":"code","807c4976":"code","292bda7b":"code","467526b3":"code","45c96706":"code","e06e7a65":"code","99556ea0":"code","62fbd7d7":"code","79c9c4ea":"code","30a76d77":"code","33e04aa2":"code","82729b97":"code","e9118b52":"markdown"},"source":{"16ab527b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2fd8f7c":"data = pd.read_csv(\"..\/input\/abalone-dataset\/abalone.csv\")","f62674c4":"data.Sex.value_counts()","c7536299":"data","3443edf5":"data[\"Sex\"].replace({\"M\": 0, \"F\": 1, \"I\":2}, inplace=True)","d365aca6":"X = data.drop('Sex',axis=1,inplace = False)\nY = pd.DataFrame(data['Sex'])\nX.head(2)","6c500445":"Y","a6d3dbf3":"import matplotlib.pyplot as plt\nfrom scipy import ndimage as ndi\nimport random\nimport os\n#from IPython.core.debugger import set_trace\n#from IPython.display import display, Image\nimport math\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn import datasets\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8ba5fe0f":"def gaussian(x, mu, sig):\n    return np.exp(-np.power(x - mu, 2.) \/ (2 * np.power(sig, 2.)))\n\ndef gaussian_plot(arr,step):\n    mu = arr.mean()\n    sig = arr.std()\n    x = np.arange(arr.min()-1, arr.max()+1, step)\n    print('Average = %.2f sd = %.2f min = %.2f max = %.2f' %(mu,sig,arr.min(),arr.max()))\n    plt.plot(x, gaussian(x, mu, sig))\n    \ndef plot_confusion_matrix(y_true, y_pred,classes,normalize=False,title=None,cmap=plt.cm.Blues):\n    \n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n","03aaf87c":"X2 = X[Y['Sex']==0]\nX1 = X[Y['Sex']==1]\nX0 = X[Y['Sex']==2]\n\nY2 = Y[Y['Sex']==0]\nY1 = Y[Y['Sex']==1]\nY0 = Y[Y['Sex']==2]\n\nX1.shape","e044411a":"for i in range(8):\n  plt.subplot(8, 1, i+1)\n  print('Feature :',X0.columns[i])\n  gaussian_plot(X2.iloc[:,i],0.1)\n  gaussian_plot(X1.iloc[:,i],0.1)\n  gaussian_plot(X0.iloc[:,i],0.1)\n  \nplt.show()","0bb3459f":"from sklearn.metrics import f1_score, roc_auc_score,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)","389de423":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, Y_train).predict(X_test)","134400a4":"classes_name = np.asarray(['M','F','I'])\naccuracy = accuracy_score(Y_test, y_pred)\nprint(\" - Accuracy = \" + str(accuracy*100))\nplot_confusion_matrix( Y_test, y_pred ,classes = classes_name  )","5f87e931":"def maximum_absolute_scaling(df, df_train):\n    # copy the dataframe\n    df_scaled = df.copy()\n    # apply maximum absolute scaling\n    for column in df_scaled.columns:\n        df_scaled[column] = df_scaled[column]  \/ df_train[column].abs().max()\n    return df_scaled\n    ","8f69797a":"# call the maximum_absolute_scaling function\nX_train_scaled = maximum_absolute_scaling(X_train,X_train)\nX_train_scaled ","72f0ef75":"X_test_scaled = maximum_absolute_scaling(X_test,X_train)\nX_test_scaled ","09e2b4dc":"y_pred = gnb.fit(X_train_scaled , Y_train).predict(X_test_scaled )\n\nclasses_name = np.asarray(['M','F','I'])\naccuracy = accuracy_score(Y_test, y_pred)\nprint(\" - Accuracy = \" + str(accuracy*100))\nplot_confusion_matrix( Y_test, y_pred ,classes = classes_name  )","db6dc4fd":"import os\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nimport warnings\nwarnings.filterwarnings(\"ignore\")","59708fbf":"data = pd.read_csv(\"..\/input\/abalone-dataset\/abalone.csv\")","807c4976":"data.sample(7)","292bda7b":"data.isna().sum()","467526b3":"new_col = pd.get_dummies(data.Sex)\nnew_col","45c96706":"data[new_col.columns] = new_col\nsns.countplot(data.Sex) \n#","e06e7a65":"sns.pairplot(data.drop(['F','I', 'M'], axis=1))","99556ea0":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\nX = data.drop(['Sex', 'Rings'], axis = 1)\nY = data.Rings.astype('float')[:]\nX = scaler.fit_transform(X)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n(X_test.shape,Y_test.shape)","62fbd7d7":"Y_train","79c9c4ea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, mean_absolute_error,mean_squared_error\n\n# lgr = LogisticRegression(random_state=23)\nlgr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nlgr.fit(X_train,Y_train)\ny_predict = lgr.predict(X_train)\n\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_train, y_predict))\nprint(\"Mean absolute error: %.2f\" % mean_absolute_error(Y_train, y_predict))\n","30a76d77":"# Plot outputs\nplt.scatter(y_predict ,Y_train,  color='blue')\n#plt.plot(Y_train_pred ,Y_train, color='blue', linewidth=3)\nplt.xticks(())\nplt.yticks(())\nplt.show()","33e04aa2":"y_test_predict = lgr.predict(X_test)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_test, y_test_predict))\nprint(\"Mean absolute error: %.2f\" % mean_absolute_error(Y_test, y_test_predict))\n","82729b97":"# Plot outputs\nplt.scatter(y_test_predict ,Y_test,  color='blue')\n#plt.plot(Y_train_pred ,Y_train, color='blue', linewidth=3)\nplt.xticks(())\nplt.yticks(())\nplt.show()","e9118b52":"# Regression"}}