{"cell_type":{"4e2d77e6":"code","f0aae7ae":"code","b1c4a3ab":"code","82491fca":"code","c54b553a":"code","c07fc32e":"code","4b4d43cc":"code","328622ce":"code","44834900":"code","3ddd29fe":"code","5a03cb8d":"code","bc9bb58f":"markdown","f7b5f7dc":"markdown","d43beeca":"markdown","7a2407a3":"markdown","9fa58a71":"markdown","b315f17f":"markdown","a65cdccd":"markdown","c2ee35d5":"markdown","dc8e3ca1":"markdown","185a93e3":"markdown","b9b0f783":"markdown","677a199b":"markdown","ddf00cba":"markdown"},"source":{"4e2d77e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f0aae7ae":"df = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head()","b1c4a3ab":"X = np.array(df.iloc[:,[3,4]])","82491fca":"import matplotlib.pyplot as plt\n\nplt.scatter(X[:,0], X[:,1], s = 25)\nplt.title('Raw Data')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.show()","c54b553a":"from sklearn.cluster import KMeans\n\nwcss = []\niterations = 500\nnum_centroid_seeds = 10\nrand_state = 0\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, max_iter = iterations, n_init = num_centroid_seeds, random_state = rand_state)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","c07fc32e":"kmeans = KMeans(n_clusters = 5, max_iter = iterations, n_init = num_centroid_seeds, random_state = rand_state)\nkmeans_preds = kmeans.fit_predict(X)","4b4d43cc":"point_size = 25\ncolors = ['red', 'blue', 'green', 'cyan', 'magenta']\nlabels = ['Careful', 'Standard', 'Target', 'Careless', 'Sensible']\n\nplt.figure(figsize = (10,7))\nfor i in range(5):\n    plt.scatter(X[kmeans_preds == i,0], X[kmeans_preds == i,1], s = point_size, c = colors[i], label = labels[i])\n    \nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 100, c = 'orange', label = 'Centroids')\nplt.title('Clusters of Clients')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(loc = 'best')\nplt.show()","328622ce":"import scipy.cluster.hierarchy as sch\n\nplt.figure(figsize = (25,10))\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean Distances')\nplt.show()","44834900":"from sklearn.cluster import AgglomerativeClustering\n\nagg_clustering = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\nagg_preds = agg_clustering.fit_predict(X)","3ddd29fe":"point_size = 25\ncolors = ['red', 'blue', 'green', 'cyan', 'magenta']\nlabels = ['Careful', 'Standard', 'Target', 'Careless', 'Sensible']\n\nplt.figure(figsize = (10,7))\nfor i in range(5):\n    plt.scatter(X[agg_preds == i,0], X[agg_preds == i,1], s = point_size, c = colors[i], label = labels[i])\n\nplt.title('Clusters of Clients')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(loc = 'best')\nplt.show()","5a03cb8d":"point_size = 25\ncolors = ['red', 'blue', 'green', 'cyan', 'magenta']\nlabels = ['Careful', 'Standard', 'Target', 'Careless', 'Sensible']\n\nplt.figure(figsize = (25,7))\n\nplt.subplot(1,2,1)\nfor i in range(5):\n    plt.scatter(X[kmeans_preds == i,0], X[kmeans_preds == i,1], s = point_size, c = colors[i], label = labels[i])\n    \nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 100, c = 'orange', label = 'Centroids')\nplt.title('Clusters of Clients (K-Means)')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(loc = 'best')\n\nplt.subplot(1,2,2)\nfor i in range(5):\n    plt.scatter(X[agg_preds == i,0], X[agg_preds == i,1], s = point_size, c = colors[i], label = labels[i])\n    \nplt.title('Clusters of Clients (Agglomerative)')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(loc = 'best')\n\nplt.show()","bc9bb58f":"Plot our clusters","f7b5f7dc":"Create our dendrogram","d43beeca":"### **In conclusion we can see that Hierarchical Clustering seems to produce better results than K-Means Clustering**","7a2407a3":"### **K-Means Algorithm**","9fa58a71":"Let's use the 'Elbow Method' to determine the appropriate number of clusters to use","b315f17f":"Visualize the raw data","a65cdccd":"Examine our dataset","c2ee35d5":"### **Agglomerative Hierarchical Clustering Algorithm**","dc8e3ca1":"## **Our goal is to cluster our customers into buying groups based off of their Annual Income and Spending Scores**","185a93e3":"From the above we can see that the optimal number of clusters is 5","b9b0f783":"Plot our clusters","677a199b":"### **Compare KMeans to Agglomerative Clusters**","ddf00cba":"Perfrom Agglomerative Clustering"}}