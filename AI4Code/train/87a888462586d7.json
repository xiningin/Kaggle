{"cell_type":{"5ede511d":"code","4e175e17":"code","a7c8c56d":"code","9910fdf0":"code","d318e58d":"code","c8207b03":"code","06653bae":"code","8104b79d":"code","df099285":"code","23b96d6e":"code","3835265e":"code","2a92893d":"code","6e95a45f":"code","4f9a8c85":"code","f77063f3":"code","90e5484e":"code","aa32863d":"code","b15f2dd4":"code","bfa05f0d":"code","e8187e4b":"code","f4b6fd92":"code","4c4b8a9a":"code","e8c5a311":"code","3abea4e4":"code","0aa07f5f":"code","b23d07ec":"code","be50b4b9":"code","aecd8601":"code","9b8ff8cb":"code","b2d83e3e":"code","a28f9cf3":"code","122c4812":"code","b47d8c84":"code","feadddc4":"code","975348ae":"code","809b9b26":"code","7979ca44":"code","e211f7a8":"code","a17fa7f8":"code","013d4de9":"code","d93fa6b3":"code","adb2a8d6":"code","8f4a0ad5":"code","f5d022cf":"code","e5010ee9":"code","2811fb5a":"code","b614949a":"code","22d1c66b":"code","5de31228":"code","8d7d34e6":"code","03271f71":"code","5a742265":"code","19168f8a":"markdown","b5ac0903":"markdown","8a37682d":"markdown","f2f372c0":"markdown","f0e4f6db":"markdown","7aa8349f":"markdown","0ff8bc9d":"markdown","b36e3c8e":"markdown","9cb531a4":"markdown","5905edcd":"markdown","26387f8f":"markdown","477e4eec":"markdown","4dc8a419":"markdown","62d2e83f":"markdown","f1688604":"markdown"},"source":{"5ede511d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import CatBoostEncoder, TargetEncoder\n\nfrom sklearn.linear_model import SGDClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nfrom time import time\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,roc_curve,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","4e175e17":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n","a7c8c56d":"train.head()","9910fdf0":"test.head()","d318e58d":"train.info()","c8207b03":"%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(18,9))\nsns.heatmap(train.isnull(), cbar= False)","06653bae":"print(100*train['Age'].isnull().sum()\/train.shape[0])\nprint(100*train['Cabin'].isnull().sum()\/train.shape[0])\nprint(100*train['Embarked'].isnull().sum()\/train.shape[0])","8104b79d":"%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(18,9))\nsns.heatmap(test.isnull(), cbar= False)","df099285":"print(100*test['Age'].isnull().sum()\/train.shape[0])\nprint(100*test['Cabin'].isnull().sum()\/train.shape[0])\nprint(100*test['Fare'].isnull().sum()\/train.shape[0])","23b96d6e":"%matplotlib notebook\n%matplotlib inline\ndef cor_heat(df):\n    cor=df.corr()\n    plt.figure(figsize=(10,5),dpi=100)\n    sns.heatmap(data=cor,annot=True,square=True,linewidths=0.1,cmap='YlGnBu')\n    plt.title(\"Pearson Co-relation: Heat Map\")\ncor_heat(train)","3835265e":"cat_columns= [col for col in train.columns if train[col].dtype==object]\ncat_columns","2a92893d":"def id_relation(col):\n    print(train.groupby(col)['Survived'].value_counts(normalize=True).unstack().dropna()[1].sort_values(ascending=False))\nfor col in cat_columns:\n    id_relation(col)\n","6e95a45f":"train_titanic = train.drop(['PassengerId','Age','Cabin'],axis = 1)\ntest_titanic = test.drop(['PassengerId','Age','Cabin'],axis = 1)","4f9a8c85":"train_titanic.Embarked.mode()","f77063f3":"train_titanic.Embarked.fillna('S',inplace=True)","90e5484e":"train_titanic.isnull().sum().sum()","aa32863d":"test_titanic.Fare.median()","b15f2dd4":"test_titanic.Fare.fillna(14.4542,inplace=True)","bfa05f0d":"test_titanic.isnull().sum().sum()","e8187e4b":"train_titanic.columns","f4b6fd92":"test_titanic.columns","4c4b8a9a":"cat_cols = [col for col in train_titanic.columns if train_titanic[col].dtype=='object']\ncat_cols","e8c5a311":"train_titanic.drop('Name',inplace=True,axis=1)\ntest_titanic.drop('Name',inplace=True,axis=1)","3abea4e4":"train_titanic.columns","0aa07f5f":"test_titanic.columns","b23d07ec":"cat_cols = [col for col in train_titanic.columns if train_titanic[col].dtype=='object']\ncat_cols","be50b4b9":"for cols in cat_cols:\n    print(train_titanic[cols].nunique())","aecd8601":"#balanced\/imbalanced?\n%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(10,5))\nsns.countplot(x='Survived',data=train_titanic)","9b8ff8cb":"cbe=CatBoostEncoder(cols=cat_cols)\n    # X= df_tran_tr.drop(['isFraud'],axis=1)\n    # y= df_tran_tr[['isFraud']]\ncbe.fit(train_titanic[cat_cols],train_titanic[['Survived']])\n\n    # #Train & Test Set transforming\ntrain_titanic=train_titanic.join(cbe.transform(train_titanic[cat_cols]).add_suffix('_target'))\ntrain_titanic.drop(['Sex', 'Ticket', 'Embarked'],axis=1,inplace=True)\n\ntest_titanic=test_titanic.join(cbe.transform(test_titanic[cat_cols]).add_suffix('_target'))\ntest_titanic.drop(['Sex', 'Ticket', 'Embarked'],axis=1,inplace=True)\n","b2d83e3e":"train_titanic.head()","a28f9cf3":"test_titanic","122c4812":"#don't do this before category encoding\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","b47d8c84":"#Reducing memory without any data loss\ntrain_titanic=reduce_mem_usage(train_titanic)\ntest_titanic=reduce_mem_usage(test_titanic)","feadddc4":"train_set = lgb.Dataset(train_titanic.drop(\"Survived\",axis=1), label = train_titanic[\"Survived\"])","975348ae":"\nimport csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\nN_FOLDS=5\n\ndef objective(params, n_folds = N_FOLDS):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n    \n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # Retrieve the subsample if present otherwise set to 1.0\n    subsample = params['boosting_type'].get('subsample', 1.0)\n    \n    # Extract the boosting type\n    params['boosting_type'] = params['boosting_type']['boosting_type']\n    params['subsample'] = subsample\n    \n    # Make sure parameters that need to be integers are integers\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        params[parameter_name] = int(params[parameter_name])\n    \n    start = timer()\n    \n    # Perform n_folds cross validation\n    cv_results = lgb.cv(params, train_set, num_boost_round = 1000, nfold = n_folds, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n    \n    run_time = timer() - start\n    \n    # Extract the best score\n    best_score = np.max(cv_results['auc-mean'])\n    \n    # Loss must be minimized\n    loss = 1 - best_score\n    \n    # Boosting rounds that returned the highest cv score\n    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n\n    # Write to the csv file ('a' means append)\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n            'estimators': n_estimators, \n            'train_time': run_time, 'status': STATUS_OK}","809b9b26":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample","7979ca44":"# Create the learning rate\nlearning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}","e211f7a8":"learning_rate_dist = []\n\n# Draw 10000 samples from the learning rate domain\nfor _ in range(10000):\n    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\nplt.title('Learning Rate Distribution', size = 18); \nplt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);","a17fa7f8":"# Discrete uniform distribution\nnum_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\nnum_leaves_dist = []\n\n# Sample 10000 times from the number of leaves distribution\nfor _ in range(10000):\n    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n    \n# kdeplot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\nplt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);","013d4de9":"# boosting type domain \nboosting_type = {'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n\n# Draw a sample\nparams = sample(boosting_type)\nparams","d93fa6b3":"# Retrieve the subsample if present otherwise set to 1.0\nsubsample = params['boosting_type'].get('subsample', 1.0)\n\n# Extract the boosting type\nparams['boosting_type'] = params['boosting_type']['boosting_type']\nparams['subsample'] = subsample\n\nparams","adb2a8d6":"# Define the search space\nspace = {\n   'class_weight': hp.choice('class_weight', [None, 'balanced']),\n    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n}","8f4a0ad5":"# Sample from the full space\nx = sample(space)\n\n# Conditional logic to assign top-level keys\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\n\nx","f5d022cf":"x = sample(space)\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\nx","e5010ee9":"from hyperopt import tpe\n\n# optimization algorithm\ntpe_algorithm = tpe.suggest","2811fb5a":"from hyperopt import Trials\n\n# Keep track of results\nbayes_trials = Trials()","b614949a":"# File to save first results\nimport pandas as pd\ndf = pd.DataFrame(list())\ndf.to_csv('gbm_trials_titanic.csv')\n","22d1c66b":"out_file = 'gbm_trials_titanic.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\nof_connection.close()","5de31228":"from hyperopt import fmin","8d7d34e6":"%%capture\n\n# Global variable\nglobal  ITERATION\n\nITERATION = 0\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, \n            max_evals = 100, trials = bayes_trials, rstate = np.random.RandomState(50))","03271f71":"\nresults = pd.read_csv('gbm_trials_titanic.csv')\n\n# Sort with best scores on top and reset index for slicing\nresults.sort_values('loss', ascending = True, inplace = True)\nresults.reset_index(inplace = True, drop = True)\nprint(results.params[0])\nprint(results.head())","5a742265":"def model_output(your_model):\n    #training the model with inp and out df\n    your_model.fit(X_train,y_train)\n    your_model_pred= your_model.predict(test_titanic)\n    your_model_df= pd.DataFrame({'PassengerId':test['PassengerId'],'Survived': your_model_pred})\n    your_model_df.to_csv('submission_titanic_lgbmc_bayesian_prev_cat_21num_leaves.csv',index=False)\n\nrdf_model=RandomForestClassifier(warm_start=True)\n#xgb_model=XGBClassifier()\nnbg_model=GaussianNB()\nmplc_model=MLPClassifier()\nadb_model= AdaBoostClassifier()\ngbb_model=GradientBoostingClassifier()\nsvc_model= SVC()\nknn_model=KNeighborsClassifier()\nsgd_model=SGDClassifier()\nlgbmc_model= LGBMClassifier(boosting_type=  'gbdt',objective='binary',random_state=42, max_depth=4,class_weight= None, colsample_bytree= 0.8175679964858448,\n                            learning_rate= 0.05088656537291398, min_child_samples= 25, num_leaves= 21, \n                            reg_alpha= 0.5804082563063744, reg_lambda=  0.192747845878722, \n                            subsample_for_bin= 40000, subsample= 0.8293119621855407)\n\n\nmodel_output(lgbmc_model)","19168f8a":"# let us now look at our dataset","b5ac0903":" from the gaph it can be seen that 'cabin' column has a lot of missing values ( 77.10437710437711%)so we should extratct this column. Anothe column 'age' has some missing values(19.865319865319865%) . And embark column has only few(0.2244668911335578%) missing values. Let us see the correlation of diffrent columns to see which columns should we take","8a37682d":"# co relating the features with heat maps","f2f372c0":"# we do not need the column \"NAME\" as the name of a person will never affect tthe survival state","f0e4f6db":"There are no missing values in the data now...Now let us take a look at the columns and then appply categorical encoding to the cateorical columns as all the algtorithm do not support categorical variables. So by categorical encoding we have to convert the categorical column to numerical column. We also have to make sure that the train dataset and test dataset have the same columns","7aa8349f":"# See the mising values with the help of graphs","0ff8bc9d":"# **Hyperparameter Tuning Using Bayesian Optimization**","b36e3c8e":"# importing all libraries","9cb531a4":"# **We Submit prediction for various model on kaggle and found that lgbmc model perform better than any of the model with an accuracy of 78.947%.. So we will  tune the hyperparameter on this model to get more ood result**","5905edcd":"# Let us decrease the usage of the memory by the dataset","26387f8f":"# importing all The data","477e4eec":"# now we are ready for categorical encoding:\nWe can do categorical encoding in various process like 'LabelEncoding','OneHtEncoding',\"TargetEncoding\",\"CatBoostEncoding\".. depending n the dataset we choose various encoding type. Like for imbalanced data tree based algorithm should be coosen. And for tree basd algorithm 'TargetEncoding' and 'CatBoostEncoding' is better choice. But for balaced data we can use \"LabelEncoding\" and \"OneHotEncoding\". \"LabelEncoding\" may be used when the unique values in the caterogical columns are less than 10-15.. otherise we can use \"OneHotEncoding\".Though there is no particular set of rules for using the encoding tyype. One should try various encoding type and should go for the best one.","4dc8a419":"# imputing missing vaalues\nthere have missing values in 'embarked' column so we need to impute the missing values","62d2e83f":" from the correlation we can see that the columns 'passenger id','sib sp' has lower coreltion. Though the 'age' column has quite  good corelation but has a good amount of missing values too. So we will extract these columns from out tarin and test dataset","f1688604":"# Exploratory Data Analysis"}}