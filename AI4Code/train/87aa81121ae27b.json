{"cell_type":{"c9e3ad8f":"code","88e91446":"code","94fc1979":"code","52ec622f":"code","f6a55089":"code","47024f26":"code","4ab61672":"code","0597baff":"code","4edc782f":"code","7495e6dc":"code","1d0d8cb2":"code","0b18da17":"code","89c44c1b":"code","83e66a87":"code","7c4320b0":"code","4fcd9e62":"code","128f8730":"code","f4cef74e":"code","ec563936":"code","5afeac1d":"code","267edd03":"code","d703e33b":"code","afd9cb54":"code","d2e4c85b":"code","44ba8138":"code","d1317acf":"code","f8f6b0bc":"code","5ad85e70":"code","f56c2aef":"code","14a8ff05":"code","ee9194c5":"code","1d8a4f36":"code","8b59e5f9":"code","ac1973e6":"code","2b7b775e":"code","67c305e8":"code","ed4122a4":"code","8594c3b4":"code","08ab4a46":"code","d1a13e3d":"code","9511c5d4":"code","b63456b7":"code","ca8c8f58":"code","fb6458e6":"code","18aef6f7":"code","15ece0ca":"code","5f5f7ead":"code","9af4d22d":"code","623d8bed":"code","7144b2e5":"code","7f4a6aea":"code","2461ce4c":"code","2a6d5126":"code","a37b93d6":"code","4aeccef0":"code","f0db86d0":"code","84a24e68":"code","e0c62aa9":"code","664120a7":"code","2f63e21e":"code","db8e9b61":"code","aca1aa5d":"code","84c442c1":"code","48d557ad":"code","c6936db4":"code","fd3aeee8":"code","92a8d0bc":"code","7e3eb0a1":"code","04a939a4":"code","0960bd90":"code","a213da48":"code","412af1c9":"code","3a03a5c6":"code","d717c8bb":"code","4701b193":"code","ced12ca5":"code","eca54847":"code","d92e043e":"code","0de91d3b":"code","1d78cecb":"code","47cffdd4":"code","b5f40352":"markdown","915b7c1c":"markdown","95ec2bc1":"markdown","9db71f52":"markdown","6c6cad53":"markdown","f5352751":"markdown","0145dc77":"markdown","8760b7a1":"markdown","9fcba5e1":"markdown","b51d649a":"markdown","0811995e":"markdown","3ddc7fbf":"markdown","b16d07c4":"markdown","385c3e29":"markdown","254b5e03":"markdown","ab05ffc9":"markdown","407cd171":"markdown","7c660dfa":"markdown","4994d3da":"markdown","5377729f":"markdown","db080039":"markdown","0fbca96a":"markdown","fad5875d":"markdown","7f9de039":"markdown","5b939e3a":"markdown","e1c862b1":"markdown","5cf7502c":"markdown","f0478274":"markdown"},"source":{"c9e3ad8f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows',None)\npd.set_option('display.max_column',None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","88e91446":"df = pd.read_csv('\/kaggle\/input\/hrattritioneda\/Attrition-EDA.csv')\ndf.head()","94fc1979":"X=df.drop('Attrition',axis=1)\ny=df['Attrition']","52ec622f":"X.shape","f6a55089":"cov_matirx=np.cov(X.T)\neig_vals,eig_vectors=np.linalg.eig(cov_matirx)","47024f26":"eig_vals  # The values are not in order , we need to sort the values ","4ab61672":"tot=sum(eig_vals)\nvar_exp=[(i\/tot)*100 for i in sorted(eig_vals,reverse=True)]\ncum_var_exp=np.cumsum(var_exp)\nprint('Cumulative variance Explained:',cum_var_exp)","0597baff":"plt.figure(figsize=(15,4))\nplt.bar(range(X.shape[1]),var_exp,alpha=0.5,align='center',label='Individual explained variance')\nplt.step(range(X.shape[1]),cum_var_exp,where='mid',label='cummulative explained variance')\nplt.ylabel(\"explained variance ratio\")\nplt.xlabel(\"principal components\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","4edc782f":"eigen_pairs=[(np.abs(eig_vals[i]),eig_vectors[:,i]) for i in range(len(eig_vals))]\n\neig_val_sort=[eigen_pairs[index][0] for index in range(len(eig_vals))]\neig_vec_sort=[eigen_pairs[index][1] for index in range(len(eig_vals))]","7495e6dc":"eig_val_sort.sort(reverse=True)\n","1d0d8cb2":"P_reduce=np.array(eig_vec_sort[0:37]).T","0b18da17":"projected_data=np.dot(X,P_reduce)","89c44c1b":"projected_data_df=pd.DataFrame(projected_data)\nprojected_data_df.head()","83e66a87":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3, stratify=y)","7c4320b0":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X.shape)\nprint(y.shape)","4fcd9e62":"# Applying PCA function on training \n# and testing set of X component \nfrom sklearn.decomposition import PCA\n\n\npca = PCA(n_components = 37)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)                          # LOGISITC REGRESSION WITH PCA\n\nfrom sklearn.linear_model import LogisticRegression\nalgo= LogisticRegression(random_state = 3)\n\nalgo.fit(X_train_pca , y_train)\ny_train_pred = algo.predict(X_train_pca)\ny_train_prob = algo.predict_proba(X_train_pca)\n\n#overall acc of train model\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score,roc_curve\nfrom sklearn.model_selection import cross_val_score\n\nprint('Confusion matrix - Train :', '\\n',confusion_matrix(y_train , y_train_pred))\nprint('Overall Accuracy - Train :',accuracy_score(y_train , y_train_pred))\nprint('AUC - Train:', roc_auc_score(y_train , y_train_prob[:,1]))\n\ny_test_pred = algo.predict(X_test_pca)\ny_test_prob = algo.predict_proba(X_test_pca)[:,1]\nprint('*'*50)\nprint('Confusion matrix - Test :', '\\n',confusion_matrix(y_test , y_test_pred))\nprint('Overall Accuracy - Test :',accuracy_score(y_test , y_test_pred))\nprint('AUC - Test:', roc_auc_score(y_test , y_test_prob))\n\nprint('*'*50)\nscores=cross_val_score(algo,X,y,cv=3,scoring='roc_auc')\nprint('Cross Val Scores')\nprint(scores)\nprint('Bias Error    :',100-scores.mean()*100)\nprint('Variance Error:',scores.std()*100)\n\n\n\nfpr , tpr , threshold = roc_curve(y_test , y_test_prob)\nplt.plot(fpr , tpr)\nplt.plot(fpr , fpr , 'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n","128f8730":"from sklearn.metrics import confusion_matrix , accuracy_score , roc_auc_score , roc_curve\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,KFold\n\n# Importing all the predictive models.\n\n\nlr = LogisticRegression(fit_intercept=True)\nbagged_lr = BaggingClassifier(base_estimator = lr, n_estimators = 25, random_state = 3)\ngnb= GaussianNB()\nbnb= BernoulliNB()\nmnb= MultinomialNB()\nknn = KNeighborsClassifier()\ndtc = DecisionTreeClassifier(ccp_alpha=0.01) # to increase pruning and avoid overfitting\nrfc= RandomForestClassifier()\nsvm= SVC(probability=True)\n\n# Declaring various classification models for the predictive model building.","f4cef74e":"clf=DummyClassifier(strategy='stratified')\nclf.fit(X_train,y_train)","ec563936":"clf.predict(X_test)","5afeac1d":"print('Base Score on Train Data Set: ',clf.score(X_train,y_train)) \nprint('Base Score on Test Data Set : ',clf.score(X_test,y_test)) ","267edd03":"from sklearn.metrics import classification_report","d703e33b":"def model_eval(algo , X_train , y_train , X_test , y_test):\n\n    algo.fit(X_train , y_train)\n    y_pred = algo.predict(X_train)\n\n    y_train_pred = algo.predict(X_train)               # Finding the positives and negatives \n    y_train_prob = algo.predict_proba(X_train)[:,1]    #we are intersted only in the second column\n\n\n    #overall acc of train model\n    print('Confusion matrix - Train :', '\\n',confusion_matrix(y_train , y_train_pred))\n    print('Overall Accuracy - Train :',accuracy_score(y_train , y_train_pred))\n    print('AUC - Train:', roc_auc_score(y_train , y_train_prob))\n\n    y_test_pred = algo.predict(X_test)\n    y_test_prob = algo.predict_proba(X_test)[:,1]\n    print('*'*50)\n    print('Confusion matrix - Test :', '\\n',confusion_matrix(y_test , y_test_pred))\n    print('Overall Accuracy - Test :',accuracy_score(y_test , y_test_pred))\n    print('AUC - Test:', roc_auc_score(y_test , y_test_prob))\n    \n    print('*'*50)\n    scores=cross_val_score(algo,X,y,cv=3,scoring='roc_auc')\n    print('Cross Val Scores')\n    print(scores)\n    print('Bias Error    :',100-scores.mean()*100)\n    print('Variance Error:',scores.std()*100)\n    \n    print('\\n')\n    print('Classification Report:\\n', classification_report(y_test, y_test_pred))\n    \n    \n\n    fpr , tpr , threshold = roc_curve(y_test , y_test_prob)\n    plt.plot(fpr , tpr)\n    plt.plot(fpr , fpr , 'r-')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')","afd9cb54":"model_eval(bagged_lr , X_train , y_train , X_test , y_test)","d2e4c85b":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score, cross_val_predict","44ba8138":"bagged_lr=BaggingClassifier(base_estimator=lr,n_estimators=15,random_state=3)\nadaboost_lr=AdaBoostClassifier(base_estimator=lr,n_estimators=50,random_state=3)   #default decision tree\ngb=GradientBoostingClassifier(n_estimators=55,random_state=3)                   # Cannot have base_estimator\n\nmodels=[]\nmodels.append(('Bagged_Logisitc_Regression',bagged_lr))\nmodels.append(('Ada_Boost_Logistic_Regression',adaboost_lr))\nmodels.append(('Gradient_Boost',gb))\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","d1317acf":"model_eval(knn , X_train , y_train , X_test , y_test)","f8f6b0bc":"bagged_knn=BaggingClassifier(base_estimator=knn,n_estimators=15,random_state=3) # default DT, cannot use RandomForest\nadaboost=AdaBoostClassifier(n_estimators=50,random_state=3)                    # default decision tree, cannot use KNN\n\n\nmodels=[]\nmodels.append(('Bagged_KNN',bagged_knn))\nmodels.append(('Ada_Boost',adaboost))\n\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","5ad85e70":"model_eval(gnb , X_train , y_train , X_test , y_test)","f56c2aef":"gnb= GaussianNB()\nbnb= BernoulliNB()\n\ngaussian_bag=BaggingClassifier(base_estimator=gnb,n_estimators=10,random_state=3)\ngaussian_adaboost=AdaBoostClassifier(base_estimator=gnb,n_estimators=30,random_state=3)\nbernoulli_bag=BaggingClassifier(base_estimator=bnb,n_estimators=10,random_state=3)\nbernoulli_adaboost=AdaBoostClassifier(base_estimator=bnb,n_estimators=30,random_state=3)\n\n\n\nmodels=[]\nmodels.append(('Naive_Bayes_Gaussian',gnb))\nmodels.append(('Naive_Bayes_Bernoulli',bnb))\nmodels.append(('Gaussian_bagged',gaussian_bag))\nmodels.append(('Bernoulli_bagged',bernoulli_bag))\nmodels.append(('Adaboost_Gaussian',gaussian_adaboost))\nmodels.append(('Adaboost_Bernoulli',bernoulli_adaboost))\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","14a8ff05":"model_eval(dtc , X_train , y_train , X_test , y_test)","ee9194c5":"bagged_dtc=BaggingClassifier(n_estimators=15,random_state=3)       # default decision tree, cannot use RandomF \nadaboost_dtc=AdaBoostClassifier(n_estimators=50,random_state=3)       # default decision tree, cannot use KNN\n\n\nmodels=[]\nmodels.append(('Bagged_DTC',bagged_dtc))\nmodels.append(('Adaboost_DTC',adaboost_dtc))\n\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","1d8a4f36":"model_eval(rfc , X_train , y_train , X_test , y_test)","8b59e5f9":"bagged_rfc=BaggingClassifier(base_estimator=rfc,n_estimators=15,random_state=3)       # default decision tree, cannot use RandomF \nadaboost_rfc=AdaBoostClassifier(base_estimator=rfc,n_estimators=50,random_state=3)   # default decision tree, cannot use KNN\n             \n\nmodels=[]\nmodels.append(('Bagged_RFC',bagged_rfc))\nmodels.append(('Adaboost_RFC',adaboost_rfc))\n\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","ac1973e6":"from scipy.stats import randint as sp_randint\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrfc = RandomForestClassifier(random_state=3)\nparams = { 'n_estimators' : sp_randint(50 , 200) , \n           'max_features' : sp_randint(1,26) ,\n           'max_depth' : sp_randint(2,10) , \n           'min_samples_split' : sp_randint(2,10) ,\n           'min_samples_leaf' : sp_randint(1,10) ,\n           'criterion' : ['gini' , 'entropy']\n    \n}\n\nrsearch_rfc = RandomizedSearchCV(rfc , param_distributions= params , n_iter= 200 , cv = 3 , scoring='roc_auc' , random_state= 3 , return_train_score=True , n_jobs=-1)\n\nrsearch_rfc.fit(X,y)","2b7b775e":"rsearch_rfc.best_params_    ","67c305e8":"rfc= RandomForestClassifier(**rsearch_rfc.best_params_,random_state=3)\n\nrfc.fit(X_train,y_train)\n\ny_train_pred=rfc.predict(X_train)                 # Finding the Positives and Negatives \ny_train_prob=rfc.predict_proba(X_train)[:,1]      # We are interested only in the 2nd column\n\n\n\nprint('Confusion Matrix - Train:','\\n' ,confusion_matrix(y_train,y_train_pred))\nprint('Overall Accuracy - Train:', accuracy_score(y_train,y_train_pred))             #Train\nprint('AUC- Train',roc_auc_score(y_train,y_train_prob))\n\ny_test_pred=rfc.predict(X_test)\ny_test_prob=rfc.predict_proba(X_test)[:,1]\n\n\nprint('\\n')\nprint('Confusion Matrix - Test:','\\n' ,confusion_matrix(y_test,y_test_pred))\nprint('Overall Accuracy - Test:', accuracy_score(y_test,y_test_pred))               #Test\nprint('AUC- Test',roc_auc_score(y_test,y_test_prob))\n\nprint('\\n')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')","ed4122a4":"col_sorted_by_importance=rfc.feature_importances_.argsort()\nfeat_imp=pd.DataFrame({\n    'cols':X.columns[col_sorted_by_importance],\n    'imps':rfc.feature_importances_[col_sorted_by_importance]\n})\n\nfeat_imp.sort_values(by='imps',ascending=False)[:10]","8594c3b4":"model_eval(svm, X_train , y_train , X_test , y_test)","08ab4a46":"from sklearn.svm import SVC\nsvm=SVC(probability=True)\n\nkernel=['linear','poly','rbf','sigmoid']\n\nfor i in kernel:\n    svm=SVC(kernel=i,C=1.0)\n    svm.fit(X_train,y_train)\n    print('For kernel i,',i)\n    print('accuracy is' ,svm.score(X_test,y_test))","d1a13e3d":"from sklearn.model_selection import GridSearchCV \nsvm=SVC(probability=True,class_weight='balanced',random_state=3)\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'coef0':[0.001,10,0.5],\n              'kernel': ['rbf','poly', 'sigmoid']}  \n  \ngrid_search_svm = GridSearchCV(svm, param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngrid_search_svm.fit(X_train,y_train) ","9511c5d4":"# print best parameter after tuning \nprint(grid_search_svm.best_params_) \n  \n# print how our model looks after hyper-parameter tuning \nprint(grid_search_svm.best_estimator_) ","b63456b7":"svm= SVC(probability=True,**grid_search_svm.best_params_,random_state=3)\n\nsvm.fit(X_train,y_train)\n\ny_train_pred=svm.predict(X_train)                 # Finding the Positives and Negatives \ny_train_prob=svm.predict_proba(X_train)[:,1]      # We are interested only in the 2nd column\n\n\n\nprint('Confusion Matrix - Train:','\\n' ,confusion_matrix(y_train,y_train_pred))\nprint('Overall Accuracy - Train:', accuracy_score(y_train,y_train_pred))             #Train\nprint('AUC- Train',roc_auc_score(y_train,y_train_prob))\n\ny_test_pred=svm.predict(X_test)\ny_test_prob=svm.predict_proba(X_test)[:,1]\n\n\nprint('\\n')\nprint('Confusion Matrix - Test:','\\n' ,confusion_matrix(y_test,y_test_pred))\nprint('Overall Accuracy - Test:', accuracy_score(y_test,y_test_pred))               #Test\nprint('AUC- Test',roc_auc_score(y_test,y_test_prob))\n\nprint('\\n')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')","ca8c8f58":"bagged_svm=BaggingClassifier(base_estimator=svm,n_estimators=15,random_state=3)       # default decision tree, cannot use RandomF \nadaboost_svm=AdaBoostClassifier(base_estimator=svm,n_estimators=15,random_state=3)   # default decision tree, cannot use KNN\ngb_lr=GradientBoostingClassifier(n_estimators=55,random_state=3)                # Does not have base_estimator, uses DT as stump\n\nmodels=[]\nmodels.append(('Bagged_SVM',bagged_svm))\nmodels.append(('Adaboost_SVM',adaboost_svm))\nmodels.append(('Gradient_Boost',gb_lr))\n\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X_train,y_train,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,fontsize=8)\nplt.show()","fb6458e6":"Xytrain=pd.concat([X_train,y_train],axis=1)\n\nprint('Before Oversampling:','\\n',Xytrain['Attrition'].value_counts())\n\nXytrain0=Xytrain[Xytrain['Attrition']==0]\nXytrain1=Xytrain[Xytrain['Attrition']==1]\n\nlen0=len(Xytrain0)\nlen1=len(Xytrain1)\n\nXytrain1_os=Xytrain1.sample(len0,replace=True,random_state=3) # To duplicate the values when over sampling [replace=True]\nXytrain_os=pd.concat([Xytrain0,Xytrain1_os],axis=0)           # Axis 0 because it is appending and not merging \n\nprint('\\n')\nprint('After Oversampling:','\\n',Xytrain_os['Attrition'].value_counts())","18aef6f7":"X_os=Xytrain_os.drop('Attrition',axis=1)\ny_os=Xytrain_os['Attrition']","15ece0ca":"from sklearn.model_selection import train_test_split\nX_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_os, y_os, test_size=0.3, random_state=3)","5f5f7ead":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nX_train_os_scaled = ss.fit_transform(X_train_os)\nX_test_os_scaled = ss.transform(X_test_os)","9af4d22d":"svm_os= SVC(probability=True)\nmodel_eval(svm_os, X_train_os_scaled , y_train_os , X_test_os_scaled , y_test_os)","623d8bed":"from imblearn.over_sampling import SMOTE,SVMSMOTE","7144b2e5":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30,random_state = 7)\n\nsmote=SVMSMOTE(sampling_strategy='minority',random_state=3)\nX_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)","7f4a6aea":"#smote = SMOTE(sampling_strategy = 'minority', random_state = 3)\n#X_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)","2461ce4c":"svm= SVC(probability=True)\n\nsvm.fit(X_train_sm,y_train_sm)\n\ny_train_pred=svm.predict(X_train_sm)                 # Finding the Positives and Negatives \ny_train_prob=svm.predict_proba(X_train_sm)[:,1]      # We are interested only in the 2nd column\n\n\n\nprint('Confusion Matrix - Train:','\\n' ,confusion_matrix(y_train_sm,y_train_pred))\nprint('Overall Accuracy - Train:', accuracy_score(y_train_sm,y_train_pred))             #Train\nprint('AUC- Train',roc_auc_score(y_train_sm,y_train_prob))\n\ny_test_pred=svm.predict(X_test)\ny_test_prob=svm.predict_proba(X_test)[:,1]\n\n\nprint('\\n')\nprint('Confusion Matrix - Test:','\\n' ,confusion_matrix(y_test,y_test_pred))\nprint('Overall Accuracy - Test:', accuracy_score(y_test,y_test_pred))               #Test\nprint('AUC- Test',roc_auc_score(y_test,y_test_prob))\n\nprint('\\n')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')","2a6d5126":"from imblearn.over_sampling import ADASYN \nadasyn = ADASYN(sampling_strategy='auto')","a37b93d6":"X_train_adasyn,y_train_adasyn=adasyn.fit_sample(X_train,y_train)\ny_train_adasyn.value_counts()","4aeccef0":"model_eval(svm, X_train_adasyn , y_train_adasyn , X_test , y_test)","f0db86d0":"import lightgbm as lgb\nlgbm = lgb.LGBMClassifier()\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nparams = {\n    'n_estimators' : sp_randint(50,200) , \n    'max_depth' : sp_randint(2,15) ,\n    'learning_rate' : sp_uniform(0.001 , 0.5 ) ,\n    'num_leaves' : sp_randint(20 , 50) \n} \n\n\nrsearch = RandomizedSearchCV(lgbm , param_distributions= params , cv = 3 , n_iter= 200 , n_jobs=-1 ,random_state= 3)\n\nrsearch.fit(X , y)","84a24e68":"rsearch.best_estimator_","e0c62aa9":"lgbm= lgb.LGBMClassifier(**rsearch.best_params_)\n\nlgbm.fit(X_train,y_train)\n\ny_train_pred=lgbm.predict(X_train)                 # Finding the Positives and Negatives \ny_train_prob=lgbm.predict_proba(X_train)[:,1]      # We are interested only in the 2nd column\n\nprint('Confusion Matrix - Train:','\\n' ,confusion_matrix(y_train,y_train_pred))\nprint('Overall Accuracy - Train:', accuracy_score(y_train,y_train_pred))             #Train\nprint('AUC- Train',roc_auc_score(y_train,y_train_prob))\n\ny_test_pred=lgbm.predict(X_test)\ny_test_prob=lgbm.predict_proba(X_test)[:,1]\n\n\nprint('\\n')\nprint('Confusion Matrix - Test:','\\n' ,confusion_matrix(y_test,y_test_pred))\nprint('Overall Accuracy - Test:', accuracy_score(y_test,y_test_pred))               #Test\nprint('AUC- Test',roc_auc_score(y_test,y_test_prob))\n\nprint('\\n')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')","664120a7":"from sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nparams = {\n        'min_child_weight': [1,2,3,4,5,6,7,8,9,10],\n        'gamma': [0.5, 1,1,1.25,1.35,1.45, 1.5,1.75, 2, 5],\n        'subsample': [0.6,0.7 ,0.8,0.9, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0,1.1,1.2],\n        'max_depth': [3, 4, 5,6,7]\n        }\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=1000, objective='binary:logistic',\n                    silent=True, nthread=1)\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 3)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=3 )\n\nrandom_search.fit(X, y)","2f63e21e":"xgb= XGBClassifier(**random_search.best_params_,random_state=3)\n\nxgb.fit(X_train,y_train)\n\ny_train_pred=xgb.predict(X_train)                 # Finding the Positives and Negatives \ny_train_prob=xgb.predict_proba(X_train)[:,1]      # We are interested only in the 2nd column\n\n\n\nprint('Confusion Matrix - Train:','\\n' ,confusion_matrix(y_train,y_train_pred))\nprint('Overall Accuracy - Train:', accuracy_score(y_train,y_train_pred))             #Train\nprint('AUC- Train',roc_auc_score(y_train,y_train_prob))\n\ny_test_pred=xgb.predict(X_test)\ny_test_prob=xgb.predict_proba(X_test)[:,1]\n\n\nprint('\\n')\nprint('Confusion Matrix - Test:','\\n' ,confusion_matrix(y_test,y_test_pred))\nprint('Overall Accuracy - Test:', accuracy_score(y_test,y_test_pred))               #Test\nprint('AUC- Test',roc_auc_score(y_test,y_test_prob))\n\nprint('\\n')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')","db8e9b61":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nsvm=SVC(probability=True)","aca1aa5d":"stacked=VotingClassifier(estimators=[('Bagged_Logistic_Regression',bagged_lr),('Adaboost_Bernoulli_Naive_Bayes',bernoulli_adaboost),('GBOOst',gb),('Bagged_RandomForest',bagged_rfc),('Support_Vector_Machines',svm)],voting='soft')","84c442c1":"models=[]\nmodels.append(('Bagged_Logistic_Regression',bagged_lr))\nmodels.append(('Adaboost_Bernoulli_Naive_Bayes',bernoulli_adaboost))\nmodels.append(('GBOOst',gb))\nmodels.append(('Bagged_RandomForest',bagged_rfc))\nmodels.append(('Support_Vector_Machines',svm))\nmodels.append(('Stacked',stacked))\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X,y,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","48d557ad":"stacked11=VotingClassifier(estimators=[('Bagged_Logistic_Regression',bagged_lr),('GBOOst',gb),('Support_Vector_Machines',svm)],voting='soft')","c6936db4":"models=[]\nmodels.append(('Bagged_Logistic_Regression',bagged_lr))\nmodels.append(('GBOOst',gb))\nmodels.append(('Support_Vector_Machines',svm))\nmodels.append(('Stacked',stacked))\n\n\nresults=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(n_splits=5,shuffle=True,random_state=0)\n    cv_result=cross_val_score(model,X,y,cv=kfold,scoring='roc_auc')\n    results.append(cv_result)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name,np.mean(cv_result),np.var(cv_result,ddof=1)))\nfig=plt.figure(figsize=(15,8))\nfig.suptitle(\"Algorithm comparision\")\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","fd3aeee8":"data=pd.read_csv('\/kaggle\/input\/hrattritioneda\/Attrition-EDA.csv')\ndata.head()","92a8d0bc":"from pycaret.classification import *\nclf=setup(data,target='Attrition')","7e3eb0a1":"compare_models()","04a939a4":"from pycaret.classification import *\nclf=setup(data,target='Attrition',normalize=True,\n    normalize_method='zscore',\n    transformation=True,\n    transformation_method='yeo-johnson')","0960bd90":"compare_models(sort='AUC')","a213da48":"lr.classes_ = np.array([-1, 1])","412af1c9":"tuned_lr= tune_model(lr,optimize='AUC')","3a03a5c6":"evaluate_model(tuned_lr)","d717c8bb":"final_lr_model=finalize_model(tuned_lr)","4701b193":"print(final_lr_model)","ced12ca5":"predictions=predict_model(final_lr_model,data=data)\npredictions.head()","eca54847":"columns=[column for column in predictions.columns if (column!='Attrition') & (column!='Label') &(column!='Score')]\ncolumns= columns + ['Attrition','Label','Score']\npredictions=predictions[columns]","d92e043e":"predictions['Attrition'].unique()","0de91d3b":"predictions['Label'].unique()","1d78cecb":"predictions.head()","47cffdd4":"from PIL import Image\nImage.open('\/kaggle\/input\/employee-retention\/employee-retention-strategy-slide13.png')","b5f40352":"## Extract the independent variable X and dependent variable Y","915b7c1c":"## Splitting the data set to train and test ","95ec2bc1":"## * XGBoost","9db71f52":"## *KNN","6c6cad53":"### PCA model","f5352751":"## PCA ","0145dc77":"## Some of the proven employee retention statergies.","8760b7a1":"##  Building Machine Learning Models","9fcba5e1":"## SMOTE (Synthetic Minority Oversampling TEchnique)","b51d649a":"Below is a link to article in site questionpro explaining some of the retention statergies.","0811995e":"##### PCA is a method used to reduce number of variables in the data by extracting important one from a large pool. It reduces the dimension of data with the aim of retaining as much information as possible.","3ddc7fbf":"## *Decision Tree Classifier","b16d07c4":"## The END ","385c3e29":"Link for HR attrition analysis part one EDA: https:\/\/www.kaggle.com\/winterbreeze\/ibm-hr-attrition-analysis-part-1-eda","254b5e03":"## *Logistic Regression ","ab05ffc9":"## *LightGBM","407cd171":"## ** Using Pycaret to assess which model is best  ","7c660dfa":"## *Random Forest Classifier","4994d3da":"## *Support Vector Machines ","5377729f":"### Creating a dummy classifier to know the base models predictions","db080039":"## Using Randomised Search Cross Validation to Search for the best parameters ","0fbca96a":"## *ADASUN (Adaptive Synthetic Sampling)","fad5875d":"[https:\/\/www.questionpro.com\/blog\/employee-retention-strategies\/](http:\/\/)","7f9de039":"## *Naive Bayes","5b939e3a":"## Over Sampling Minority Class","e1c862b1":"### This notebook contains only the model building, comaprision of models using sklearn and pycaret modules.","5cf7502c":"##### So we conclude that the best model we are getting is Logistic Regression model and now we can fit this model with the required predictor variables and predict the attrition of an employee. This is a versatile model and can be implemented in any organization to analyse their previous employee base and check the attrition.","f0478274":"* now we know that our Supervised Learning Models have to perform better than the above mentioned score"}}