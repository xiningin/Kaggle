{"cell_type":{"1cbb0978":"code","c5182a9a":"code","5866d5f7":"code","778d4122":"code","cf58e54c":"code","2d327636":"code","4ba67d55":"code","309224ed":"code","dd02fc61":"code","6b34d672":"code","0d5aa5d6":"code","a3a84834":"code","cdf64eb4":"code","a8190428":"code","03411d6f":"code","bc4b60c3":"code","b146e540":"code","6c59cebd":"code","318b17b3":"code","9b6ecb43":"code","42e07386":"code","715b0707":"code","acb389a1":"code","b55ddcbe":"code","3ac58516":"code","31875403":"code","5d5bc8fa":"code","fe454fd6":"code","0117e8dc":"code","4216c504":"code","e3d579c5":"code","c86c7b57":"code","b4498ec2":"code","00bbc3ad":"code","f6f8fd3f":"code","cc66eaf6":"code","eb4bef47":"code","ca438c79":"code","8f32eaa7":"code","80605084":"code","844df22d":"code","23b75ef9":"markdown","d0ec03c4":"markdown","aa4b311c":"markdown","233bac2a":"markdown"},"source":{"1cbb0978":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c5182a9a":"## import library\nfrom sklearn import preprocessing\nfrom sklearn import ensemble, linear_model, naive_bayes, neighbors, svm, \\\ndiscriminant_analysis, tree, gaussian_process, model_selection\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport copy\nfrom scipy.spatial.distance import pdist,squareform\nimport time","5866d5f7":"# load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","778d4122":"passenger_id = test['PassengerId'].copy()","cf58e54c":"# combining train + test\ntraintest = pd.concat([train, test], axis=0, sort=False)","2d327636":"## Check NaN values\nisnull = traintest.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","4ba67d55":"## Add title column\ntraintest['Title'] = traintest['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\nreplace = {\n    'Ms': 'Miss',\n    'Dona': 'Miss',\n    'Mlle': 'Miss',\n    'Mme': 'Miss',\n    'Don': 'Mr',\n    'Sir': 'Mr'\n}\n\ntraintest.replace({'Title': replace}, inplace=True)","309224ed":"## TODO 1: Filling missing value\n\n## Age: filling with mean value of title \ndf_nan_ages = traintest['Age'].isnull()\ntitle_ages = traintest[['Title', 'Age']].groupby('Title').mean().to_dict()['Age']\ntraintest['Age'][df_nan_ages] = traintest[df_nan_ages]['Title'].apply(lambda x: title_ages[x])\n\n## Fare: filling with mode value\n#traintest['Fare'].fillna(traintest['Fare'].mode(), inplace=True)\n\n## Embarked: filling with mode value\n#traintest['Fare'] = traintest['Embarked'].fillna(traintest['Embarked'].mode(), inplace=True)\n\n\n## NOTE: inplace=True when using fillna above is not working. For working using dict instead of values: \n###: df.fillna({'x':0, 'y':0}, inplace=True)\n## More detail: https:\/\/stackoverflow.com\/questions\/21998354\/pandas-wont-fillna-inplace\ntraintest.fillna({\n    'Fare': traintest['Fare'].mode()[0],\n    'Embarked': traintest['Embarked'].mode()[0]\n}, inplace=True)\n\n## Cabin: dropping cabin column as there are many NaN rows\ntraintest.drop(columns='Cabin', inplace=True)\ntraintest.drop(columns='PassengerId', inplace=True)","dd02fc61":"## TODO: Creating new features\n\n## Family size = SibSp + Parch + 1 (self)\ntraintest['FamilySize'] = traintest.SibSp + traintest.Parch + 1\n\n## Add Family Surname\n#traintest['Surname'] = traintest['Name'].str.split(\",\", expand=True)[0]\n\n## Add IsAlone column\ntraintest['IsAlone'] = (traintest['FamilySize'] == 1)*1\n\n## Add age range\ntraintest['AgeStage'] = traintest['Age']\ntraintest['AgeStage'][traintest['Age'] <= 11] = 'Child'\ntraintest['AgeStage'][(traintest['Age'] <= 20) & (traintest['Age'] > 11)] = 'Young'\ntraintest['AgeStage'][(traintest['Age'] <= 40) & (traintest['Age'] > 20)] = 'Adult'\ntraintest['AgeStage'][traintest['Age'] > 40] = 'Old'\n\n## Drop the Name column\ntraintest.drop(columns='Name', inplace=True)","6b34d672":"## TODO: Check null again\n## Check NaN values\nisnull = traintest.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","0d5aa5d6":"traintest.head(2)","a3a84834":"## TODO: Handle categorical variables\n\"\"\"\nCategorical variables: Pclass, Sex, Ticket, 'Embarked' and Surname\n\"\"\"\ncategorical_vars = [\"Pclass\", \"Sex\", \"Ticket\", 'Embarked', 'Title', 'AgeStage']\n\n## TODO: Normalize numerical features\nnumerical_vars = ['Fare', 'Age', 'FamilySize', 'Parch', 'SibSp']\n\ntraintest_set = []\n## using label encoding\ntraintest[categorical_vars] = traintest[categorical_vars].astype('category')\ntraintest_labelEncoding = traintest.copy()\nlabel_encoder = preprocessing.LabelEncoder()\nfor var in categorical_vars:\n    print(var)\n    traintest_labelEncoding[var] = label_encoder.fit_transform(traintest_labelEncoding[var])\n\ntraintest_set.append(traintest_labelEncoding)\n## using one hot encoding\ntraintest_onehot = pd.get_dummies(traintest, columns=categorical_vars)\ntraintest_onehot[numerical_vars] = (traintest_onehot[numerical_vars] - traintest_onehot[numerical_vars].mean())\/(traintest_onehot[numerical_vars].max() - traintest_onehot[numerical_vars].min())\n\n\ntraintest_set.append(traintest_onehot)\n## using embedding: UPDATING","cdf64eb4":"traintest_labelEncoding.head(2)","a8190428":"traintest_onehot.head(2)","03411d6f":"## Pearson Correlation of Features \ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\n","bc4b60c3":"correlation_heatmap(traintest_set[0])","b146e540":"## TODO: split train\/val\/test set\nX_trains, y_train, X_tests = [], None,[]\n\nfor traintest in traintest_set:\n    train = traintest[traintest['Survived'].notnull()]\n    y_train = train['Survived']\n    X_train = train.drop(columns='Survived')\n    \n    print(\"X train shape: \", X_train.shape)\n    print(\"Y train shape: \", y_train.shape)\n\n    X_trains.append(X_train)\n    \n    test = traintest[traintest['Survived'].isnull()]\n    X_test = test.drop(columns='Survived')\n    \n    print(\"X test shape: \", X_test.shape)\n    X_tests.append(X_test)","6c59cebd":"## TODO: Models - Machine Learning Algorithm (MLA) Selection and Initialization\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    #discriminant_analysis.LinearDiscriminantAnalysis(),\n    #discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()\n    \n]","318b17b3":"## TODO: split dataset in cross-validation \ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 )","9b6ecb43":"## TODO: train models\nMLAs = [copy.deepcopy(MLA), copy.deepcopy(MLA)]\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n\nresult_table = []\n## loop over features\nfor i in range(len(X_trains)):\n    \n    #create table to compare MLA metrics\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n    MLA_predict = y_train.copy()\n    print(\"Y train shape: \", y_train.shape)\n    \n    row_index = 0\n    for alg in MLAs[i]:\n        target = y_train.copy()\n        #set name and parameters\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n        \n        ## score model with cross validations\n        print(\"Y train shape: \", y_train.shape)\n        print(MLA_name)\n        cv_results = model_selection.cross_validate(alg, X_trains[i], target, cv  = cv_split)\n        \n        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n        #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n        \n        #save MLA predictions - see section 6 for usage\n        alg.fit(X_trains[i], y_train)\n        MLA_predict[MLA_name] = alg.predict(X_trains[i])\n\n        row_index+=1\n    \n    result_table.append((MLA_compare, MLA_predict))","42e07386":"## TODO: Show results\n## Result 1:\nMLA_compare = result_table[0][0]\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","715b0707":"## TODO: Show results\n## Result 2:\nMLA_compare = result_table[1][0]\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","acb389a1":"## Using top-2 of each features\nprediction_df = pd.DataFrame()\n\n## TODO: submit predictions\nalg_name = 'XGBClassifier'\nfeature_index = 0\nalg_index = 19\nprediction = MLAs[feature_index][alg_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n\n\n## TODO: submit predictions\nalg_name = 'GradientBoostingClassifier'\nfeature_index = 0\nalg_index = 3\nprediction = MLAs[feature_index][alg_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n\n\n## TODO: submit predictions\nalg_name = 'RidgeClassifierCV'\nfeature_index = 1\nalg_index = 8\nprediction = MLAs[feature_index][alg_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n\n\n## TODO: submit predictions\nalg_name = 'LogisticRegressionCV'\nfeature_index = 1\nalg_index = 6\nprediction = MLAs[feature_index][alg_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n","b55ddcbe":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity(prediction_df.T)","3ac58516":"## TODO: Check how different output of the models \n\ncm = cosine_similarity(prediction_df.T)\n#print(distance_matrix)\nax = plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, xticklabels=prediction_df.columns, yticklabels=prediction_df.columns)\nax.set_title(\"different output of the models\")\nplt.show()","31875403":"for i in range(len(result_table)):\n    for alg in MLAs[i]:\n        labels = [0, 1]\n        predict = alg.predict(X_trains[i])\n        cm = confusion_matrix(y_train, predict, labels)\n        #print(cm.shape)\n        ax= plt.subplot()\n        sns.heatmap(cm, annot=True, ax = ax, fmt=\"d\")\n        ax.set_title(alg.__class__.__name__)\n        plt.show()","5d5bc8fa":"## TODO: Using Neural Networks\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping\nfrom sklearn import preprocessing\nfrom keras import regularizers","fe454fd6":"'''\nX_train = X_trains[1]\n\nmodel_neuron = Sequential() \nmodel_neuron.add(Dense(output_dim=512, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=256, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=128, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=64, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=65, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=32, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=1, input_shape=(X_train.shape[1],), activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\nmodel_neuron.compile(loss='mse', optimizer=Adam(lr=1e-5), metrics=['accuracy'])\nmodel_neuron.summary()\n\nhistory = model_neuron.fit(X_train, y_train, nb_epoch=1000, validation_split=0.2, callbacks=[EarlyStopping(patience=10)])\n'''","0117e8dc":"'''\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n'''","4216c504":"'''\nlabels = [0, 1]\npredict = model_neuron.predict_classes(X_trains[i])\ncm = confusion_matrix(y_train, predict, labels)\n#print(cm.shape)\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt=\"d\")\nax.set_title(\"Predictions of Neural Network model\")\nplt.show()\n'''","e3d579c5":"'''\n## TODO: submit predictions\nalg_name = 'NeuralNetwork'\nfeature_index = 1\nprediction = model_neuron.predict_classes(X_tests[feature_index])\nprediction = np.array([x[0] for x in prediction])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n'''","c86c7b57":"## TODO: Combining Models\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    ## liear models:\n    #('rcv', linear_model.RidgeClassifierCV()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\"\"\"\nfor i in range(len(X_trains)):\n    print(\"___________ Feature %d ____________\"%(i+1))\n    #Hard Vote or majority rules\n    vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n    vote_hard_cv = model_selection.cross_validate(vote_hard, X_trains[i], y_train, cv  = cv_split)\n    vote_hard.fit(X_trains[i], y_train)\n\n    print(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n    print(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n    print(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n    print('-'*10)\n\n\n    #Soft Vote or weighted probabilities\n    vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n    vote_soft_cv = model_selection.cross_validate(vote_soft, X_trains[i], y_train, cv  = cv_split)\n    vote_soft.fit(X_trains[i], y_train)\n\n    print(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n    print(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n    print(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n    print('-'*10)\n\"\"\"","b4498ec2":"vote_ests = [vote_est, vote_est]","00bbc3ad":"\"\"\"\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\n\nfor i in range(len(vote_ests)):\n    start_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\n    for clf, param in zip (vote_ests[i], grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n        #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n        #print(param)\n\n\n        start = time.perf_counter()        \n        best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n        best_search.fit(X_trains[i], y_train)\n        run = time.perf_counter() - start\n\n        best_param = best_search.best_params_\n        print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n        clf[1].set_params(**best_param) \n\n\n    run_total = time.perf_counter() - start_total\n    print('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\n    print('-'*10)\n\"\"\"","f6f8fd3f":"\"\"\"\nThe best parameter for AdaBoostClassifier is {'learning_rate': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.53 seconds.\nThe best parameter for BaggingClassifier is {'max_samples': 0.5, 'n_estimators': 300, 'random_state': 0} with a runtime of 37.23 seconds.\nThe best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 50, 'random_state': 0} with a runtime of 60.98 seconds.\nThe best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 38.43 seconds.\nThe best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 75.40 seconds.\nThe best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 8.46 seconds.\nThe best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'newton-cg'} with a runtime of 33.00 seconds.\nThe best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.33 seconds.\nThe best parameter for GaussianNB is {} with a runtime of 0.06 seconds.\nThe best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 5.85 seconds.\nThe best parameter for SVC is {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 54.53 seconds.\nThe best parameter for XGBClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'seed': 0} with a runtime of 130.39 seconds.\nTotal optimization time was 7.97 minutes.\n----------\nThe best parameter for AdaBoostClassifier is {'learning_rate': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 84.49 seconds.\nThe best parameter for BaggingClassifier is {'max_samples': 1.0, 'n_estimators': 300, 'random_state': 0} with a runtime of 168.08 seconds.\nThe best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 50, 'random_state': 0} with a runtime of 88.23 seconds.\nThe best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300, 'random_state': 0} with a runtime of 185.25 seconds.\nThe best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 50, 'oob_score': True, 'random_state': 0} with a runtime of 107.36 seconds.\nThe best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 33.33 seconds.\nThe best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 414.12 seconds.\nThe best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 1.23 seconds.\nThe best parameter for GaussianNB is {} with a runtime of 0.22 seconds.\nThe best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 294.33 seconds.\nThe best parameter for SVC is {'C': 5, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 1571.60 seconds.\nThe best parameter for XGBClassifier is {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 100, 'seed': 0} with a runtime of 3347.66 seconds.\n\n\"\"\"","cc66eaf6":"best_param = [[\n            [\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            {'learning_rate': 0.25, 'n_estimators': 300, 'random_state': 0}\n            ],\n       \n    \n            [\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            {'max_samples': 0.5, 'n_estimators': 300, 'random_state': 0}\n            ],\n\n    \n            [\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 50, 'random_state': 0}\n            ],\n\n\n            [\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0}\n            ],\n\n    \n            [\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0}\n            ],\n    \n            [ \n            #GaussianProcessClassifier\n            {'max_iter_predict': 10, 'random_state': 0}\n            ],\n        \n    \n            [\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            {'fit_intercept': True, 'random_state': 0, 'solver': 'newton-cg'}],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': 0.1, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': 7, #default: 5\n            'weights': 'distance', #default = \u2018uniform\u2019\n            'algorithm':'brute'\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': 1, #default=1.0\n            'gamma': 0.1, #edfault: auto\n            'decision_function_shape': 'ovo', #default:ovr\n            'probability': True,\n            'random_state': 0\n             }],\n\n    \n            [\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n             {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'seed': 0}\n            ]   \n        ],\n    [\n            [\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            {'learning_rate': 0.25, 'n_estimators': 300, 'random_state': 0}\n            ],\n       \n    \n            [\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            {'max_samples': 1.0, 'n_estimators': 300, 'random_state': 0}\n            ],\n\n    \n            [\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 50, 'random_state': 0}\n            ],\n\n\n            [\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300, 'random_state': 0}\n            ],\n\n    \n            [\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 50, 'oob_score': True, 'random_state': 0}\n            ],\n    \n            [ \n            #GaussianProcessClassifier\n            {'max_iter_predict': 10, 'random_state': 0}\n            ],\n        \n    \n            [\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'}],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': 0.1, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': 7, #default: 5\n            'weights': 'distance', #default = \u2018uniform\u2019\n            'algorithm':'brute'\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': 1, #default=1.0\n            'gamma': 0.1, #edfault: auto\n            'decision_function_shape': 'ovo', #default:ovr\n            'probability': True,\n            'random_state': 0\n             }],\n\n    \n            [\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n             {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'seed': 0}\n            ]   \n        ]\n]","eb4bef47":"for i in range(len(vote_ests)):\n    \n    for clf, param in zip (vote_ests[i], best_param[i]): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n        #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n        #print(param)\n\n        print('The best parameter for {} is {}'.format(clf[1].__class__.__name__, param[0]))\n        clf[1].set_params(**param[0]) ","ca438c79":"grid_hards = []\nfor i in range(len(vote_ests)):\n    grid_hard = ensemble.VotingClassifier(estimators = vote_ests[i], voting = 'hard')\n    grid_hard_cv = model_selection.cross_validate(grid_hard, X_trains[i], y_train, cv  = cv_split)\n    grid_hard.fit(X_trains[i], y_train)\n    \n    grid_hards.append(grid_hard)\n\n    print(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n    print(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n    print(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n    print('-'*10)\n\n    #Soft Vote or weighted probabilities w\/Tuned Hyperparameters\n    #grid_soft = ensemble.VotingClassifier(estimators = vote_ests[i] , voting = 'soft')\n    #grid_soft_cv = model_selection.cross_validate(grid_soft, X_trains[i], y_train, cv  = cv_split)\n    #grid_soft.fit(X_trains[i], y_train)\n\n    #print(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n    #print(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n    #print(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n    #print('-'*10)","8f32eaa7":"for i in range(len(vote_ests)):\n    labels = [0, 1]\n    predict = grid_hards[i].predict(X_trains[i])\n    cm = confusion_matrix(y_train, predict, labels)\n    #print(cm.shape)\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt=\"d\")\n    ax.set_title(\"Vote hard ensemble\")\n    plt.show()","80605084":"## TODO: submit predictions\nalg_name = 'GridHardVoting'\nfeature_index = 0\nprediction = grid_hards[feature_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction\n\n\n## TODO: submit predictions\nalg_name = 'GridHardVoting'\nfeature_index = 1\nprediction = grid_hards[feature_index].predict(X_tests[feature_index])\ntemp = {'PassengerID': passenger_id, 'Survived': prediction.astype(int)}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_%s_feature%s.csv'%(alg_name, feature_index), index=False)\nprediction_df[alg_name] = prediction","844df22d":"## TODO: Check how different output of the models  (Contain Grid Hard)\n\ncm = cosine_similarity(prediction_df.T)\n#print(distance_matrix)\nax = plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, xticklabels=prediction_df.columns, yticklabels=prediction_df.columns)\nax.set_title(\"different output of the models\")\nplt.show()","23b75ef9":"## TODO:\n* Filling missing values\n* Dealing with category variables:\n    * Label encoding\n    * Embedding\n* Creating new features from original features:\n    * Age: -> split by range to get new features: child, young, aldult or old man\n    * Famili: -> how many man\/woman\/child\/old people in the family?\n* Model:\n    * AdaBoostClassifier\n    * BaggingClassifier\n    * ExtraTreesClassifier\n    * GradientBoostingClassifier\n    * RandomForestClassifier\n    \n    * GaussianProcessClassifier\n    \n    * LogisticRegressionCV\n    * PassiveAggressiveClassifier\n    * RidgeClassifierCV\n    * SGDClassifier\n    \n    * BernoulliNB\n    * GaussianNB\n    \n    * KNeighborsClassifier\n    \n    * SVM\n    \n    * LinearDiscriminantAnalysis\n    * QuadraticDiscriminantAnalysis\n    \n    * xgboost\n    \n    * Neural Network\n\n* Combining models?\n   ","d0ec03c4":"# Test Area","aa4b311c":"As can be seen the prediction of models with same feature are similar to each others. And not much similar to models with different feature.","233bac2a":"## TODO: Check how different output of the models  (Contain Neural Network)\n\ncm = cosine_similarity(prediction_df.T)\n#print(distance_matrix)\nax = plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, xticklabels=prediction_df.columns, yticklabels=prediction_df.columns)\nax.set_title(\"different output of the models\")\nplt.show()"}}