{"cell_type":{"6618b6db":"code","a08f1e28":"code","fcd853a1":"code","e1e74a0a":"code","3691f439":"code","2da17a40":"code","f5337ade":"code","68175aba":"code","dff2e67f":"code","5872e5ce":"code","d8a0e3c5":"code","759d8a0a":"code","34f21cad":"code","ba46e295":"code","1e91d24d":"code","c7e73245":"code","516a2a74":"code","daff6042":"code","8a47b791":"code","f02eb51e":"markdown","efb2cea0":"markdown","bbd96cea":"markdown","94e79a42":"markdown","fae63a1d":"markdown","f3f22cef":"markdown","c2386a2f":"markdown","920da385":"markdown","b5c11a86":"markdown","6faee64c":"markdown","a3fd1a34":"markdown","a933338b":"markdown","a2e45b24":"markdown","ed1afbe4":"markdown","47da7e36":"markdown","9ee91bf5":"markdown","f6d92f01":"markdown","081ac7db":"markdown","1cec68f0":"markdown","7b9fca79":"markdown"},"source":{"6618b6db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a08f1e28":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fcd853a1":"columns = ['CRIM', 'ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nhouse_df = pd.read_csv('..\/input\/boston-house-prices\/housing.csv', header=None, delimiter='\\s+', names=columns)\nhouse_df.head()","e1e74a0a":"# To check whether there is missing number and the data type for each column\nhouse_df.isna().sum()","3691f439":"# From the house data, we expect that the datatype for each column should be either integer or float.\nhouse_df.info()","2da17a40":"# We will observe general statistical information from the datasets\nhouse_df.describe()","f5337ade":"# Let use see whether MEDV follow normal distribution or not\nsns.displot(house_df['MEDV'])","68175aba":"# In general, bigger room leads to a higher house cost. Let us see the relation between number of room (RM) with house price (MEDV)\nsns.lmplot(data=house_df, x='RM', y='MEDV')","dff2e67f":"# it is interesting that some houses have MEDV of 50.00 regardless of room size. Let's take a look on these houses.\nhouse_df[house_df['MEDV'] == 50.00]","5872e5ce":"# It seems that for house with MEDV of 50.00 has relatively low LSTAT (percentage of lower status of the population). Let us check whether it is true.\nsns.lmplot(data=house_df, x='LSTAT', y='MEDV')","d8a0e3c5":"# Let us compare the correlation between each features, especially compared to the price of the house\nplt.figure(figsize=(20,15))\nsns.heatmap(house_df.corr(), annot=True)","759d8a0a":"# Let start to predict the house price based on existing features. House Price (MEDV) will be dependent variables (y), and the rest of the features are independent variables. \nX = house_df.iloc[:,:-1].values\ny = house_df.iloc[:,-1].values","34f21cad":"# The data will also be split into train set and test set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","ba46e295":"# Let's predict house price with linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nlinreg = LinearRegression()\nlinreg.fit(X_train,y_train)\ny_pred_linreg = linreg.predict(X_test)\nr2_score(y_test, y_pred_linreg)","1e91d24d":"from sklearn.preprocessing import PolynomialFeatures\npoly_feat = PolynomialFeatures(degree=2)\nX_poly = poly_feat.fit_transform(X_train)\npolyreg = LinearRegression()\npolyreg.fit(X_poly,y_train)\ny_pred_poly = polyreg.predict(poly_feat.transform(X_test))\nr2_score(y_test, y_pred_poly)","c7e73245":"from sklearn.svm import SVR\nsvreg = SVR(kernel='rbf')\nsvreg.fit(X_train, y_train)\ny_pred_svr = svreg.predict(X_test)\nr2_score(y_test, y_pred_svr)","516a2a74":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(X_train, y_train)\ny_pred_tree = tree_reg.predict(X_test)\nr2_score(y_test, y_pred_tree)","daff6042":"from sklearn.ensemble import RandomForestRegressor\nforest_regressor = RandomForestRegressor(n_estimators=100)\nforest_regressor.fit(X_train, y_train)\ny_pred_forest = forest_regressor.predict(X_test)\nr2_score(y_test, y_pred_forest)","8a47b791":"from sklearn.model_selection import cross_val_score\nml_model = [polyreg, tree_reg, forest_regressor]\nfor i in ml_model:\n    accuracy = cross_val_score(estimator=i, X=X_train, y=y_train, cv=10)\n    print('accuracy score of {} is {}'.format(i,accuracy.mean()))","f02eb51e":"## Polynomial Regression","efb2cea0":"As the dataset already contain correct data types, there is no need to change it.","bbd96cea":"First, we will import necessary libraries","94e79a42":"## Random Forest Regression","fae63a1d":"Let's import our datasets and check basic information. We also knows from the datasets infor that there are 14 columns, describing details as follow:\n\n- CRIM - per capita crime rate by town\n- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n- INDUS - proportion of non-retail business acres per town.\n- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n- NOX - nitric oxides concentration (parts per 10 million)\n- RM - average number of rooms per dwelling\n- AGE - proportion of owner-occupied units built prior to 1940\n- DIS - weighted distances to five Boston employment centres\n- RAD - index of accessibility to radial highways\n- TAX - full-value property-tax rate per \\$10,000\n- PTRATIO - pupil-teacher ratio by town\n- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n- LSTAT - percentage of lower status of the population\n- MEDV - Median value of owner-occupied homes in $1000's","f3f22cef":"### We can see that number of room (RM), pupil to teacher ratio(PTRATIO), and percentage of lower status of the population (LSTAT) played the biggest impact to house price (MEDV) compared with other features.","c2386a2f":"#### Seems that non-linear regression is more suitable for this dataset. Let use other models (SVR, Decision Tress, and Random Forest) as comparison","920da385":"### As the features value is not significantly different, I will not do standard scaling here. If results shows low accuracy, I will get back and retest it with standard scaler ","b5c11a86":"We observed that there is no missing data","6faee64c":"Seems SVR is not a suitable model. Even SVR with 'linear' kernel performs better than 'rbf' kernel","a3fd1a34":"## Linear Regression","a933338b":"## Decision Trees Regression","a2e45b24":"#### the accuracy is decent, but let's see whether we can get a higher score using non-linear regression","ed1afbe4":"As the dataset already contain correct data types, there is no need to change it.","47da7e36":"### Between all models, Random Forest Regression has the highest mean accuracy of 86%. Therefore, Random Forest regression is the most suitable model to predict Boston house price based on provided features.","9ee91bf5":"## K-fold Cross Validation","f6d92f01":"### We see that houses price increases exponentially with increase of LSTAT.It is even more obvious when LSTAT is below 10. It is interesting to see that some people are willing to pay high price to own a house surrounded by owners with higher status, regardless the size of the houses","081ac7db":"The results between Polynomial regression, Decision Trees, and Random Forest are similar. Let us do k-fold cross validation to select the best model for this dataset to account for different data used for train and test","1cec68f0":"## Support Vector Regression (SVR)","7b9fca79":"### The distribution is slightly skewed to the right, with additional peal appearing at the right most graph"}}