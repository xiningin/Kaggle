{"cell_type":{"bf8a93fa":"code","cfb512f3":"code","9429b2ff":"code","bb1d0d89":"code","1ab61f29":"code","a7da6a49":"code","82047b64":"code","f772c08d":"code","acfbef4b":"code","d275e8d3":"code","0719570e":"code","61963f23":"code","e6ee775c":"code","3221fc92":"code","86663644":"code","7a2e7a57":"code","9fd2105a":"code","96d940be":"code","3f677e7f":"code","c8a6be11":"code","b2f59b7e":"code","42634423":"code","0bb20ae4":"code","e823c472":"code","cf1ebe95":"code","b89589c9":"code","7f16b343":"code","859f07a2":"code","83f12cfa":"code","ef185501":"code","69a864b2":"code","35384612":"code","b373b5b2":"code","1bd618e2":"code","37547d6c":"code","6fcfe594":"code","7d069df2":"code","ba8c9ad2":"markdown","d477c4cf":"markdown","d95464e4":"markdown","fa4e24db":"markdown","5bbce564":"markdown","4348c7b6":"markdown","5316633a":"markdown","ae191df2":"markdown","55fd6320":"markdown","14f09cd5":"markdown","72337980":"markdown","40312ca8":"markdown","d8a1954a":"markdown","34bc1198":"markdown","f12891a3":"markdown","16d0d4be":"markdown","4f4425b7":"markdown","4cb28ad6":"markdown","2dc37df2":"markdown","f065f261":"markdown","24fecdbf":"markdown","7b839958":"markdown","60ec47dd":"markdown","69b5a4a9":"markdown","3872c6cc":"markdown","b3bd1645":"markdown","bd5290c0":"markdown","aef05504":"markdown","71f8882a":"markdown"},"source":{"bf8a93fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfb512f3":"import sklearn \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","9429b2ff":"data = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')\ndata.head()","bb1d0d89":"data.shape","1ab61f29":"data.info()","a7da6a49":"data.describe()","82047b64":"data.Exited.value_counts()","f772c08d":"sns.countplot(x = 'Exited', data=data)","acfbef4b":"sns.countplot(x = 'Gender', data=data)","d275e8d3":"sns.countplot(x = 'Geography', data=data)","0719570e":"plt.figure(figsize = (12,8))\nsns.heatmap(data.corr(),annot=True, cmap='viridis')","61963f23":"plt.figure(figsize = (12,6))\nsns.scatterplot(x=data['Age'], y = data['Exited'])","e6ee775c":"plt.figure(figsize = (12,8))\nsns.scatterplot(x = data['Balance'], y = data['EstimatedSalary'], hue = data['Exited'])","3221fc92":"plt.figure(figsize = (10,8))\nsns.boxplot(data=data, x = 'Exited', y = 'Age')","86663644":"plt.figure(figsize = (10,8))\nsns.boxplot(data=data, x = 'Exited', y = 'Balance')","7a2e7a57":"data.groupby('IsActiveMember')['Exited'].value_counts()","9fd2105a":"data.groupby('IsActiveMember')['Balance'].mean()","96d940be":"data=pd.concat([data, pd.get_dummies(data.Geography)], axis=1)\ndata.drop('Geography', axis=1, inplace=True)","3f677e7f":"data['Gender'] = data['Gender'].apply(lambda x : 1 if x=='Female' else 0)","c8a6be11":"data.drop(columns=['RowNumber', 'CustomerId','Surname'], axis=1, inplace=True)","b2f59b7e":"target = data['Exited']\ndata.drop(columns=['Exited'], axis=1, inplace=True)","42634423":"data.head()","0bb20ae4":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndata = ss.fit_transform(data)","e823c472":"from imblearn.over_sampling import SMOTE\nk = 1\nsm = SMOTE(sampling_strategy='auto', k_neighbors=k, random_state=42)\ndata_res, target_res = sm.fit_resample(data, target)","cf1ebe95":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_res, target_res, test_size = 0.2, random_state = 0)","b89589c9":"from sklearn.metrics import confusion_matrix, accuracy_score,f1_score, roc_curve, classification_report,roc_auc_score\ndef evaluation(X_test, clf, y_test):\n  y_pred = clf.predict(X_test)\n  print('CLASSIFICATION REPORT')\n  print(classification_report(y_test, y_pred))\n      \n  print('F1-Score')\n  print(np.round(f1_score(y_test, y_pred)*100,2))\n    \n  print('Accuracy')\n  accuracy = accuracy_score(y_test, y_pred)\n  print(np.round(accuracy*100, 2), '%')\n\ndef plot_loss(model):\n  prob=model.predict_proba(X_test)[:,1]\n  fpr, tpr, thresholds=roc_curve(y_test, prob)\n  plt.plot(fpr, tpr, linewidth=2)\n  plt.plot([0,1], [0,1], 'k--')\n  #plt.title('Logistic Regression ROC curve')\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  print('AUC-ROC')\n  print(np.round(roc_auc_score(y_test, prob)*100,2))","7f16b343":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nevaluation(X_test, lr, y_test)\nplot_loss(lr)","859f07a2":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\nevaluation(X_test, dtc, y_test)\nplot_loss(dtc)","83f12cfa":"from sklearn.svm import SVC\nsvc_model=SVC(probability=True)\nsvc_model.fit(X_train, y_train)\nevaluation(X_test, svc_model, y_test)\nplot_loss(svc_model)","ef185501":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\nevaluation(X_test, gbc, y_test)\nplot_loss(gbc)","69a864b2":"from xgboost import XGBClassifier\nxgb_clf = XGBClassifier(learning_rate = 0.3, n_estimators = 180, max_depth = 3)\nxgb_clf.fit(X_train, y_train)\nevaluation(X_test, xgb_clf, y_test)\nplot_loss(xgb_clf)","35384612":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nevaluation(X_test, rfc, y_test)\nplot_loss(rfc)","b373b5b2":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\n\nmodel = Sequential()\nmodel.add(Dense(20, activation = 'relu'))\nmodel.add(Dense(15, activation = 'relu'))\nmodel.add(Dense(10, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))","1bd618e2":"model.compile(optimizer='adam', loss = 'binary_crossentropy',metrics=['accuracy'])","37547d6c":"model.fit(X_train, y_train, batch_size = 128,epochs = 50)","6fcfe594":"y_pred = model.predict(X_test)\nfor i in range(0, y_pred.size):\n    if y_pred[i] > 0.5:\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\nprint(accuracy)","7d069df2":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nsns.heatmap(cm, annot = True, fmt = '.0f', linewidths = .1, square = True, cmap='viridis')\nplt.xlabel('Prediction')\nplt.title('Accuracy: {0}'.format(round(accuracy, 2)))\nplt.ylabel('Actual')\nplt.show()","ba8c9ad2":"Using the describe function always provides you with interesting insights in your data. As you can see we have the mean values for tenure which could help us determine that for how long does the customer stay with the bank. There's also additional information such as the minimum and maximum balance, estimated salary etc.","d477c4cf":"Using SVC we got accuracy and roc-auc score of 79.69. This is better than Logistic Regression but Decison Tree is best amongst all three.","d95464e4":"From the boxplot we can see that the median age of people who exit is 45 compared to 36 of the people who stay. We see a large number of outliers in the segment of people who stay.","fa4e24db":"# Exploratory Data Analysis","5bbce564":"Gradient Boosting gives an accuracy of 85.37% and auc roc score of 93. ","4348c7b6":"Random Forest Classifier gives an accuracy of 90.5 and auc roc score of 96.6. The performance is comparative to XGB Classifier but slightly better.","5316633a":"# Data Cleaning","ae191df2":"Amongst all the Machine Learning algorithms we employed, Random Forest Classifier outperformed all the algorithms and gave us the best performance. Let's also try employing a deep neural network and see how it performs ","55fd6320":"There's not much difference in the proportion of male and female genders as inferred from the countplot.","14f09cd5":"We standardize the data before running our model on it","72337980":"# Modelling","40312ca8":"Not a major difference in the balance of active and inactive members.","d8a1954a":"Using Decision Tree Classifier we got an accuracy and auc roc score of 84.21. This is a lot better than Logistic Regression","34bc1198":"**This is the first proper notebook that I've written and if it helped you in some way please upvote it as it would motivate me to write and post more. Thanks for reading.**","f12891a3":"Our target variable is **Exited** and from the correlation matrix we see that there are no strong correlations with any of the features corresponding to the target variable. **Age** is slightly correlated.","16d0d4be":"Most of our customers(approx 50%) live in **France** while Germany and Spain have almost the same number of customers.","4f4425b7":"We now convert the Geography feature into three separate features on the basis of country and drop the Geometry feature.","4cb28ad6":"From scatterplot you need to observe that people above the age of 73 have not exited. This means people who get old usually don't exit from the organization. There's an outlier around the age of 83 though.","2dc37df2":"We see that people who exit are having a slightly larger median balance compared to those who stay. ","f065f261":"**45%** customers are inactive and have not exited from the organization. We can safely assume that these people either have forgotten about their account or else have kept their money in savings. The cause of concern is that **36%** customers who were active in using the services have **exited**.","24fecdbf":"We got an accuracy of 71% and auc roc score of 77.4 using Logistic Regression. Let's see if other algorithm gives us better results.","7b839958":"We'll allocate **80%** of our data for training and **20%** of the remaining data as test set.","60ec47dd":"Here we observe that we have 3 categorical features namely Surname,Geography and Gender. Rest all features are numerical ie either int or float","69b5a4a9":"We convert the Gender from categorical to numerical feature by assigning the tag of 1 to Female and 0 to Male. No sexism intended","3872c6cc":"You may wonder that why are we converting all categorical features into numerical values. The reason for doing this is that most Machine Learning algorithms **expect numerical values** as the input.\nIn the final step of cleaning the data we'll drop the unnecessary columns in our dataframe.","b3bd1645":"The class imbalance problem is addressed by creating synthetic samples using the SMOTE (Synthetic Minority Over-sampling Technique). We could've upsampled the minority class or downsampled the majority class but that could still result in overfitting\/underfitting issues. ","bd5290c0":"XGB Classifier gives an accuracy of 90.2 and auc roc score of 96.1.The performance is decent and we get a superlative balance between accuracy and auc roc score","aef05504":"We observe that amongst 10000 people, **2037** customers have stopped dealing with the organization. This makes up about **20%** of total customers. This is also a problem of class imbalance that needs to be dealt properly so that we can make sure that our data doesn't overfit on the condition that the customer stays. More on this later","71f8882a":"Deep neural network gave an accuracy of 79% and was unable to outperform Random Forest with a huge margin. "}}