{"cell_type":{"1926920e":"code","4c5cd1f0":"code","bb860dae":"code","3cba09bb":"code","5af15fd9":"code","a18c541f":"code","51771576":"code","0681e6d3":"code","692eaf32":"code","6a7159c5":"code","2606447a":"code","e0a3e90e":"code","8a783c8c":"code","6cbc98ec":"code","7d9c9b8d":"code","eb304ab9":"code","69369110":"code","9045bb3c":"code","d0ea0edf":"code","fcfb06d1":"code","41da9a51":"code","a3abf12a":"code","dd875f2a":"code","23e3adc7":"code","6da65ee3":"code","78e5279a":"code","10a401e2":"code","13f4e3b7":"code","9823c1de":"code","8ec826b3":"code","6be0dffb":"code","0d2e78d8":"code","0414e5cb":"code","c80565df":"code","52c75aa3":"code","e83d7f24":"code","9e22df4f":"code","3e13f001":"code","4136948b":"code","fc7b2f48":"code","0c26b361":"code","238d60a5":"code","25611afe":"code","f7e62cdc":"code","4845153e":"code","6df224b9":"code","58976753":"code","c6f5a0a9":"code","82c62151":"markdown","1e7080c3":"markdown","d05635f4":"markdown","a645c5af":"markdown","c918cc9d":"markdown","0ed9b03d":"markdown","dc370507":"markdown","a3bb01b0":"markdown","44bb61fa":"markdown","8f81572d":"markdown","263cce22":"markdown","f0672316":"markdown","b1892100":"markdown","7f40cd22":"markdown","e570c49a":"markdown","e6e5ee92":"markdown","e7cfcd4c":"markdown","91f86007":"markdown","b8f74da2":"markdown","f01bf5bb":"markdown","2d98e04d":"markdown","07de7622":"markdown","c70fb7d2":"markdown","f0c6971a":"markdown","57359e83":"markdown","a443ea37":"markdown","cddf850f":"markdown","47cb4aad":"markdown","adccdc07":"markdown","77bad79a":"markdown","ca1b4307":"markdown","b02597b0":"markdown","da8380dc":"markdown","c62948dd":"markdown","223b1a6e":"markdown","bba0f005":"markdown","b7507440":"markdown","c28666d6":"markdown","8c64b6ab":"markdown","9c08b51f":"markdown","976d67a0":"markdown","39cb6ab6":"markdown","c2cc6318":"markdown","994e8c15":"markdown","c06b455d":"markdown","67a9b752":"markdown","961c9760":"markdown","97d40792":"markdown","38e01dae":"markdown","231a9d76":"markdown"},"source":{"1926920e":"# comandos m\u00e1gicos que n\u00e3o se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# come\u00e7am com %\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","4c5cd1f0":"# importando os principais m\u00f3dulos que usaremos ao longo da aula\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nimport yellowbrick.cluster\n\nimport sklearn.cluster\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.preprocessing\n\nimport scipy.stats\nimport scipy.optimize\nimport scipy.integrate\n\nimport copy\nimport datetime","bb860dae":"print(np.__version__)\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(sns.__version__)\nprint(yellowbrick.__version__)\nprint(sklearn.__version__)\nprint(scipy.__version__)","3cba09bb":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"1XS8Sw8jeuI\")","5af15fd9":"blob_centers = np.array([[ 0.2,  2.3],[-1.5 ,  2.3],\n                         [-2.8,  1.8],[-2.8,  2.8],[-2.8,  1.3]])\n\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\nX, y = sklearn.datasets.make_blobs(n_samples=2000, centers=blob_centers,\n                                   cluster_std=blob_std, random_state=0)","a18c541f":"plt.scatter(X[:,0],X[:,1], s=4, c='k');","51771576":"m = sklearn.cluster.KMeans(n_clusters = 5)\ny_pred = m.fit_predict(X)","0681e6d3":"sklearn.metrics.homogeneity_completeness_v_measure(y, y_pred)","692eaf32":"# retiradas de github.com\/ageron\/handson-ml2\/blob\/master\/09_unsupervised_learning.ipynb\n\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=4)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() \/ 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=30, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=50, linewidths=50,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=False, show_ylabels=False):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\", fontsize=14)\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n    else:\n        plt.tick_params(labelleft=False)","6a7159c5":"plot_decision_boundaries(m, X)","2606447a":"T = m.transform(X)\nT.shape","e0a3e90e":"m.inertia_","8a783c8c":"def plot_inercia_per_k (X, k_list, ax = None):\n\n    ax = ax or plt.gca()\n    \n    kmeans_per_k = [sklearn.cluster.KMeans(n_clusters=k, random_state=0).fit(X)\n                    for k in k_list]\n\n    inercias = [m.inertia_ for m in kmeans_per_k]\n\n    ax.plot(k_list, inercias,ls='-',marker='*')\n\n    ax.set_xlabel('$k$')\n    ax.set_ylabel('in\u00e9rcia')\n    \n    return kmeans_per_k","6cbc98ec":"kmeans_per_k = plot_inercia_per_k(X, range(1,10))","7d9c9b8d":"plot_decision_boundaries(kmeans_per_k[3], X)","eb304ab9":"def plot_silhueta_per_k(X, k_list, ax = None):\n\n    ax = ax or plt.gca()\n    \n    kmeans_per_k = [sklearn.cluster.KMeans(n_clusters=k, random_state=0).fit(X)\n                    for k in k_list]\n\n    silhuetas = [sklearn.metrics.silhouette_score(X, m.labels_) for m in kmeans_per_k]\n\n    ax.plot(k_list, silhuetas,ls='-',marker='*')\n\n    ax.set_xlabel('$k$')\n    ax.set_ylabel('silhueta m\u00e9dia')\n    \n    return kmeans_per_k","69369110":"plot_silhueta_per_k(X,range(2,10));","9045bb3c":"def plot_silhueta(X, k_list):\n        \n    n_lines = int(np.ceil(len(k_list)\/2))    \n    \n    fig, ax = plt.subplots(n_lines, 2, figsize = (16,4*n_lines))\n\n    for i in range(len(k_list)):\n\n        m = sklearn.cluster.KMeans(k_list[i])\n        yellowbrick.cluster.silhouette_visualizer(m, X, show=False, ax=ax.ravel()[i]);\n\n    fig.tight_layout()","d0ea0edf":"plot_silhueta(X, [2,3,4,5,6,7])","fcfb06d1":"df_raw = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv',\n                     encoding=\"ISO-8859-1\", low_memory=False, \n                     parse_dates=[\"InvoiceDate\"])","41da9a51":"df_raw","a3abf12a":"df_raw.dtypes","dd875f2a":"missingno.bar(df_raw)\nmissingno.matrix(df_raw);","23e3adc7":"df_tmp = df_raw.dropna(axis = 0, subset = ['CustomerID'])","6da65ee3":"df_tmp[\"TotalSum\"] = df_tmp[\"Quantity\"] * df_tmp[\"UnitPrice\"]","78e5279a":"snapshot_date = max(df_tmp.InvoiceDate) + datetime.timedelta(days=1)\n\ndf_rfm_raw = df_tmp.groupby(['CustomerID']).agg({\n      'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n      'InvoiceNo': 'count',\n      'TotalSum': 'sum'})\n\ndf_rfm_raw.rename(columns = {'InvoiceDate': 'Recency',\n                            'InvoiceNo': 'Frequency',\n                            'TotalSum': 'MonetaryValue'}, inplace=True)\n\ndf_rfm_raw","10a401e2":"print(df_rfm_raw.skew())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,3))\n\nfor i in range(3):\n    sns.distplot(df_rfm_raw.iloc[:,i], ax=ax[i])","13f4e3b7":"arr_rfm = sklearn.preprocessing.power_transform(df_rfm_raw)\n\ndf_rfm = pd.DataFrame(arr_rfm,\n                      index = df_rfm_raw.index,\n                      columns = df_rfm_raw.columns)","9823c1de":"print(df_rfm.skew())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,3))\n\nfor i in range(3):\n    sns.distplot(df_rfm.iloc[:,i], ax=ax[i])","8ec826b3":"df_rfm['MonetaryValue'] = np.cbrt(df_rfm_raw['MonetaryValue'])","6be0dffb":"fig, ax = plt.subplots(figsize=(5,3))\n\nsns.distplot(df_rfm.iloc[:,i],ax=ax)\nplt.title(f'skew: {df_rfm.skew()[-1]:.3}');","0d2e78d8":"df_rfm = (df_rfm-df_rfm.mean())\/df_rfm.std()","0414e5cb":"fig, ax = plt.subplots(2,1, figsize = (8,6), sharex=True)\n\nkmeans_per_k = plot_inercia_per_k(df_rfm, range(1,11), ax=ax[0])\nplot_silhueta_per_k(df_rfm, range(2,11), ax=ax[1]);","c80565df":"plot_silhueta(df_rfm, [2,3,4,5,6,7])","52c75aa3":"fig, ax = plt.subplots(1, 2, subplot_kw={'projection':'3d'}, figsize=(16,6))\n\nk = [2,3]\n\nfor i in range(len(k)):\n\n    ax[i].scatter(df_rfm.iloc[:,0], df_rfm.iloc[:,1], df_rfm.iloc[:,2],\n                  c=kmeans_per_k[k[i]].labels_, cmap='viridis')\n\n    ax[i].set_title(f'$k={k[i]+1}$')\n    \n    ax[i].set_xlabel('R')\n    ax[i].set_ylabel('F')\n    ax[i].set_zlabel('M')","e83d7f24":"fig, ax = plt.subplots(1,2,figsize=(15,5))\n\nk = [2,3]\n\nfor i in range(len(k)):\n    \n    df_tmp = copy.deepcopy(df_rfm)\n    df_tmp['ID'] = df_rfm.index\n    df_tmp['Cluster'] = kmeans_per_k[k[i]].labels_\n    df_tmp_melt = pd.melt(df_tmp.reset_index(),\n                          id_vars=['ID', 'Cluster'],\n                          value_vars=['Recency','Frequency','MonetaryValue'],\n                          var_name='Attribute',\n                          value_name='Value')\n\n    sns.lineplot('Attribute', 'Value', hue='Cluster', data=df_tmp_melt, \n                 ax=ax[i], palette = sns.color_palette(\"husl\", k[i]+1))\n    \n    ax[i].set_title(f'$k={k[i]+1}$')","9e22df4f":"def create_df_with_noise (array, noise_frac,\n                          max_index_for_noise, \n                          columns = None,\n                          start = '2020-01-01 00:00:00', \n                          freq = '2T'):\n    \n    df = pd.DataFrame(array, columns = columns)\n\n    sigma_noise = noise_frac*(np.amax(array[:max_index_for_noise,:], \n                                      axis=0)-\n                              np.amin(array[:max_index_for_noise,:], \n                                      axis=0))\n\n    for i in range(df.shape[1]):\n        df.iloc[:,i] = (df.iloc[:,i] + \n                        sigma_noise[i]*np.random.randn(df.shape[0]))\n        \n        \n    df.index = pd.date_range(start = start, \n                             periods = df.shape[0],\n                             freq=freq)\n\n    return df","3e13f001":"class CSTR():\n    \n    def __init__ (self, Vj = 3.85, rho = 50.0, rhoj = 62.3, k0 = 7.08e10,\n                   E = 30000.0, R = 1.99, DH = -30000.0,\n                   Cp = 0.75, U = 150.0, A = 250.0, Cpj = 1.0,\n                   nk =  2880, t_sampling = 2, Pv =  -10.0, Pt = -4.,\n                   mu_u = {'Fi': 40,'Cai': 0.5, 'Ti': 530, 'Tji': 530}, \n                   sigma_u = {'Fi': 1.5, 'Cai': 2e-2, 'Ti': 1, 'Tji': 1},\n                   ws_u = 100, sp = {'Vs': 48, 'Fs': 40, 'Fjs': 49.9}, \n                   T_guesses = [500,600,700], ee0 = 2):\n        '''\n        Construtor: inicializa o objeto, deixando-o pronto para a execu\u00e7\u00e3o \n                    com os m\u00e9todos `run` ou `simulate`.\n                    \n        Para as unidades dos argumentos, consulte doi.org\/10.1002\/aic.13953\n        '''\n        \n        # par\u00e2metros do modelo:\n        \n        # volum\u00e9tricos\n        self.Vj, self.rho, self.rhoj = Vj, rho, rhoj \n\n        # de rea\u00e7\u00e3o\n        self.k0, self.E, self.R, self.DH = k0, E, R, DH\n\n        # de energia\n        self.Cp, self.U, self.A, self.Cpj  = Cp, U, A, Cpj\n                        \n        # c\u00e1lculo da constante de rea\u00e7\u00e3o\n        self.k_fun = lambda k0, T: k0*np.exp(-self.E\/(self.R*T))\n        \n        # par\u00e2metros da simula\u00e7\u00e3o:\n        \n        # n\u00famero de observa\u00e7\u00f5es\n        self.nk = nk\n\n        # instantes de amostragem\n        self.t = np.arange(0,t_sampling*(nk)\/60,t_sampling\/60)\n        \n        # ganhos do controlador\n        self.Pv, self.Pt = Pv, Pt\n        \n        # vari\u00e1veis:\n        \n        # vari\u00e1veis de estado (x)\n        self.x_labels = ['V','Ca','T','Tj']\n        self.x_units  = ['ft$^3$','mol\/ft$^3$','R','R']\n        \n        # vari\u00e1veis de entrada (u)\n        self.u_labels = ['Fi','Cai','Ti','Tji']\n        self.u_units  = ['ft$^3$\/h','mol\/ft$^3$','R','R']\n        \n        # vari\u00e1veis manipuladas (m)\n        self.m_labels = ['F','Fj']\n        self.m_units  = ['ft$^3$\/h','ft$^3$\/h']\n\n        # definindo vari\u00e1veis u, x e m\n            \n        u_labels = ['Fi', 'Cai', 'Ti', 'Tji']\n        x_labels = ['V', 'Ca', 'T', 'Tj']\n        m_labels = ['F', 'Fj']\n\n        # gerando vari\u00e1veis de entrada com perturba\u00e7\u00f5es de processo\n        \n        mu_u = [mu_u.get(k) for k in u_labels if k in mu_u]\n        sigma_u = [sigma_u.get(k) for k in u_labels if k in sigma_u]\n\n        self.set_input(mu_u, sigma_u, ws_u)\n\n        # definindo set-points\n        \n        [setattr(self, name, value) for name, value in sp.items()]\n        \n        # calculando estados estacion\u00e1rios\n        \n        self.calc_EE(T_guesses)\n        \n        # definindo set-point da temperatura\n\n        self.Ts = self.EE[ee0,1]\n        \n        # definindo condi\u00e7\u00f5es iniciais\n        \n        V0   = self.Vs\n        Ca0  = self.EE[ee0,0]\n        T0   = self.EE[ee0,1]\n        Tj0  = self.EE[ee0,2]\n\n        F0  = self.Fs\n        Fj0 = self.Fjs\n\n        self.set_initial_conditions (V0, Ca0, T0, Tj0, F0, Fj0)\n        \n    #########################\n\n    def set_input (self, u_ref, sigma, WS):\n        '''\n        Gera vari\u00e1veis de entrada com variabilidade que simula \n        perturba\u00e7\u00f5es de processo.\n        '''        \n        \n        from sklearn.preprocessing import scale\n        \n        self.u = np.zeros((self.nk, len(u_ref)))\n        self.mu_u = np.zeros((self.nk, len(u_ref)))\n                 \n        for i in range(len(u_ref)):\n\n            u = np.random.randn(self.nk+WS-1)\n            u = pd.Series(u).rolling(window=WS).mean().iloc[WS-1:].values\n            self.mu_u[:,i] = u_ref[i]\n            self.u[:,i] = self.mu_u[:,i]+sigma[i]*scale(u)\n            \n    #########################\n    \n    def set_set_points (self,Vs,Ts,Fs,Fjs):\n        '''\n        Atribui\u00e7\u00e3o dos set-points.\n        '''  \n        \n        self.Vs, self.Ts, = Vs, Ts\n        self.Fs, self.Fjs = Fs, Fjs\n\n    #########################\n    \n    def calc_EE (self, T_guesses): \n        '''\n        C\u00e1lculo dos estados estacion\u00e1rios.\n        '''  \n        \n        Vo = self.Vs\n        Fo = self.Fs\n        Fjo = self.Fjs\n        \n        Cao = self.mu_u[0,1]\n        To  = self.mu_u[0,2]\n        Tjo = self.mu_u[0,3]\n        \n        def F1(T):\n            k = self.k_fun(self.k0,T)\n            return -self.DH*k*Fo*Cao\/(Fo\/Vo + k)\n\n        def F2(T):\n            Tj  = (Fjo*Tjo\/self.Vj + \n                   self.U*self.A*T\/(self.rhoj*self.Vj*self.Cpj))\/(Fjo\/self.Vj+\\\n                                    self.U*self.A\/(self.rhoj*self.Vj*self.Cpj))\n            return Fo*self.rho*self.Cp*(T - To) + self.U*self.A*(T - Tj)\n        \n        def F12(T):\n            k = self.k_fun(self.k0,T)\n            Tj  = (Fjo*Tjo\/self.Vj + \n                   self.U*self.A*T\/(self.rhoj*self.Vj*self.Cpj))\/(Fjo\/self.Vj+\\\n                                    self.U*self.A\/(self.rhoj*self.Vj*self.Cpj))\n            return Fo*self.rho*self.Cp*(T - To) + self.U*self.A*(T - Tj) + \\\n                   self.DH*k*Fo*Cao\/(Fo\/Vo + k)\n        \n        def Ca(T):\n            k = self.k_fun(self.k0,T)\n            return Cao\/(1 + Vo*k\/Fo)\n\n        def Tjac(T):\n            tjac=(Fjo*Tjo\/self.Vj + \n                  self.U*self.A*T\/(self.rhoj*self.Vj*self.Cpj))\/(Fjo\/self.Vj+\\\n                                   self.U*self.A\/(self.rhoj*self.Vj*self.Cpj))\n\n            return tjac\n\n        self.t_ee = []; self.f_ee = []; \n        self.g_ee = []; self.ca_ee = []; \n        self.tj_ee = []\n\n        for T in np.arange(min(T_guesses),max(T_guesses)):\n            self.t_ee.append(T)\n            self.f_ee.append(F1(T))\n            self.g_ee.append(F2(T))\n            self.ca_ee.append(Ca(T))\n            self.tj_ee.append(Tjac(T))\n\n        T1 = scipy.optimize.fsolve(F12,T_guesses[0])\n        Ca1 = Ca(T1);\n        Tj1 = Tjac(T1)\n        \n        T2 = scipy.optimize.fsolve(F12,T_guesses[1])\n        Ca2 = Ca(T2)\n        Tj2 = Tjac(T2)\n        \n        T3 = scipy.optimize.fsolve(F12,T_guesses[2])\n        Ca3 = Ca(T3)\n        Tj3 = Tjac(T3)\n\n        self.EE  = np.array([[Ca1[0], T1[0], Tj1[0]], \n                             [Ca2[0], T2[0], Tj2[0]], \n                             [Ca3[0], T3[0], Tj3[0]]])\n                \n    #########################\n     \n    def model (self,t,x,u,m):\n        '''\n        Sistema de equa\u00e7\u00f5es diferenciais que constitui\n        o modelo din\u00e2mico propriamente dito do processo.\n        '''  \n        \n        # vari\u00e1veis de estado\n        V, Ca, T, Tj = x\n\n        # vari\u00e1veis de entrada  \n        Fi, Cai, Ti, Tji  = u\n\n        # vari\u00e1veis manipuladas\n        F, Fj  = m\n\n        k = self.k_fun(self.k0,T)\n\n        dx    = np.zeros(4)\n        dx[0] = Fi - F\n        dVC   = Fi*Cai - F*Ca - V*k*Ca\n        dx[1] = (dVC - Ca*dx[0])\/V\n        dVT   = Fi*Ti - F*T + \\\n                V*k*Ca*(-self.DH)\/(self.rho*self.Cp) - \\\n                self.U*self.A*(T-Tj)\/(self.rho*self.Cp)\n        dx[2] = (dVT - T*dx[0])\/V\n        dx[3] = (Tji-Tj)*Fj\/self.Vj + \\\n                self.U*self.A*(T-Tj)\/(self.Vj*self.rhoj*self.Cpj)\n\n        return dx  \n\n    #########################  \n    \n    def control (self,V,T):\n        '''\n        Atua\u00e7\u00e3o de controle proporcional.\n        '''  \n\n        # estruturas de controle\n        F  = self.Fs  + self.Pv*(self.Vs - V)\n        Fj = self.Fjs + self.Pt*(self.Ts - T) \n\n        # restri\u00e7\u00f5es\n        if Fj > 100: Fj = 100\n        if Fj <   0: Fj =   0\n        if F  > 100: F  = 100\n        if F  <   0: F  =   0\n\n        return np.array([F, Fj])\n    \n    #########################\n    \n    def set_initial_conditions (self, V0, Ca0, T0, Tj0, F0, Fj0):\n        '''\n        Atribui\u00e7\u00e3o de condi\u00e7\u00f5es iniciais.\n        '''  \n        \n        self.x0 = np.array([V0, Ca0, T0, Tj0])\n        self.m0 = np.array([F0, Fj0])\n        \n    #########################\n    \n    def simulate (self, control = True, sp = None, devs = None):\n        '''\n        Integra\u00e7\u00e3o das equa\u00e7\u00f5es diferenciais que comp\u00f5em o modelo.\n        ''' \n        \n        # matriz que armazenar\u00e1 as vari\u00e1veis de estado\n        self.x = np.zeros((self.nk,self.x0.shape[0]))\n        \n        # matriz que armazenar\u00e1 as vari\u00e1veis manipuladas\n        self.m = np.zeros((self.nk,self.m0.shape[0]))\n\n        # definindo estado inicial\n        self.x[0,:] = self.x0\n        self.m[0,:] = self.m0\n        \n        # matriz que armazenar\u00e1 os valores nominais de todas as vari\u00e1veis\n        self.nom = np.zeros((self.nk, \n                             self.x.shape[1]+self.u.shape[1]+self.m.shape[1]))\n        self.nom[0,:] = np.hstack((self.x0, self.mu_u[0,:], self.m0))\n\n        # armazenando set-points                \n        if sp is not None:\n            self.sp = sp\n        else:\n            self.sp = np.zeros(self.nk)+2\n\n        # armazenando valores normais das vari\u00e1veis que ter\u00e3o desvio        \n        if devs is not None:\n            normais = {}\n            for key, value in devs.items():\n                normais[key] = getattr(self, key)\n                \n        # loop de integra\u00e7\u00e3o\n                    \n        for j in range(1,self.nk):\n            \n            # introduzindo desvios nas vari\u00e1veis pertinentes            \n            if devs is not None:\n                for key, value in devs.items():\n                    setattr(self, key, normais[key]*(1+value[j]))\n\n            # integrando!            \n            res = scipy.integrate.solve_ivp(lambda t,x:self.model(t,\n                                                                  x,\n                                                                  self.u[j,:],\n                                                                  self.m[j-1,:]\n                                                                  ),\n                                            [self.t[j-1],self.t[j]],\n                                            self.x[j-1,:],\n                                            rtol=1e-12,\n                                            atol=1e-12) \n            self.x[j,:] = res.y[:,-1]\n            \n            if control:\n            \n                # controlando!\n                self.m[j,:] = self.control(*self.x[j,[0,2]])\n                \n                # setando o set-point de Ts para definir o EE\n                self.Ts = self.EE[int(self.sp[j]),1] \n                self.nom[j,:] =  np.array([self.x0[0]] +\n                                          self.EE[int(self.sp[j]),:].tolist() +\n                                          self.mu_u[j,:].tolist() + \n                                          [self.Fs]+[self.Fjs])\n                    \n            else:\n                \n                self.m[j,:] = self.m[j-1,:]\n\n    #########################\n    \n    def run (self, ee_sp = None, \n             meas_noise_frac = 0.1, max_index_noise_frac = -1, \n             faulty = False, faulty_variable = 'k0',\n             devs_fault = None, index_fault = None):\n        '''\n        Integra\u00e7\u00e3o do modelo, chamando o m\u00e9todo `simulate` e retornando\n        um DataFrame com os dados de processo.\n        ''' \n        \n        if faulty:\n            if devs_fault is None:\n                devs_fault = np.zeros(self.nk)\n                j_f0 = int(self.nk\/2)\n                self.index_fault = j_f0\n                j_fn = self.nk\n                for j in range(j_f0, j_fn):\n                    if j >= j_f0 and j<=j_fn:\n                        devs_fault[j] = (j_f0-j)\/j_fn\n                    else:\n                        devs_fault[j] = (j_f0-j_fn)\/j_fn \n            else:\n                self.index_fault = index_fault\n            self.simulate(sp= ee_sp, devs = {faulty_variable:devs_fault})\n        else:\n            self.simulate(sp = ee_sp)\n\n        array = np.hstack((self.x, self.u, self.m))\n\n        df = create_df_with_noise(array, meas_noise_frac, \n                                  max_index_noise_frac,\n                                  columns = (self.x_labels + \n                                             self.u_labels + \n                                             self.m_labels))\n\n        return df\n        \n    #########################        \n              \n    def plot (self, kind = 'x'):\n        '''\n        Plot das vari\u00e1veis $x$, $u$ ou $m$.\n        ''' \n        \n        if kind == 'x':\n            y = self.x\n            y_labels = self.x_labels\n            y_units  = self.x_units\n            nom = self.nom[2:,:4]\n            title='State'\n        elif kind == 'u':\n            y = self.u\n            y_labels = self.u_labels\n            y_units  = self.u_units\n            title='Input'\n        elif kind == 'm':\n            y = self.m\n            y_labels = self.m_labels\n            y_units  = self.m_units\n            nom = self.nom[2:,8:]\n            title='Manipulated'\n            \n        fig, ax = plt.subplots(1, y.shape[1], figsize=(15,4))\n            \n        for i in range(y.shape[1]):\n            ax.ravel()[i].plot(self.t, y[:,i])\n            ax.ravel()[i].set_title(y_labels[i]+' ('+y_units[i]+')')\n            if kind == 'x' or kind == 'm':\n                ax.ravel()[i].plot(self.t[2:],\n                                   nom[:,i],':k')\n                \n        fig.suptitle(title+' Variables x time (h) ');\n        \n        fig.tight_layout()\n        \n    #########################    \n    \n    def plot_van_heerden (self, ax = None):\n        '''\n        Plot do diagrama de van Heerden.\n        ''' \n        \n        ax = ax or plt.gca()\n        ax.plot(self.t_ee,self.f_ee,label='Generated heat')\n        ax.plot(self.t_ee,self.g_ee,label='Removed heat')\n        ax.legend()\n        ax.set_xlabel('T (R)')\n        ax.set_ylabel('Q (BTU\/h)')","4136948b":"c = CSTR()","fc7b2f48":"c.plot_van_heerden()","0c26b361":"ee_sp = np.zeros(c.nk)+2\nee_sp[c.nk\/\/3:2*c.nk\/\/3] = 1\nee_sp[2*c.nk\/\/3:] = 0","238d60a5":"%time cstr_df = c.run(ee_sp = ee_sp)","25611afe":"cstr_df","f7e62cdc":"c.plot('x')\nc.plot('u')\nc.plot('m')","4845153e":"fig, ax = plt.subplots(2,1, figsize = (8,6), sharex=True)\n\nplot_inercia_per_k(cstr_df, range(1,11), ax=ax[0])\nplot_silhueta_per_k(cstr_df, range(2,11), ax=ax[1]);","6df224b9":"plot_silhueta(cstr_df, [2,3,4,5,6,7])","58976753":"m = sklearn.cluster.KMeans(n_clusters=3)\nm.fit(cstr_df)\n\nfig, ax = plt.subplots(1, 2, subplot_kw={'projection':'3d'}, figsize=(16,6))\n\ncolors = [ee_sp, m.labels_]\ntitles = ['Labels verdadeiros', 'Labels $k$-means']\n\nfor i in range(len(colors)):\n\n    ax[i].scatter(cstr_df.loc[:,'Ca'], cstr_df.loc[:,'T'], cstr_df.loc[:,'Tj'],\n                  c=colors[i], cmap='viridis')\n\n    ax[i].set_title(titles[i])\n    \n    ax[i].set_xlabel('Ca')\n    ax[i].set_ylabel('T')\n    ax[i].set_zlabel('Tj')","c6f5a0a9":"sklearn.metrics.homogeneity_completeness_v_measure(m.labels_, ee_sp)","82c62151":"# CSTR\n\nO [reator tanque agitado cont\u00ednuo](https:\/\/en.wikipedia.org\/wiki\/Continuous_stirred-tank_reactor) (CSTR, da sigla em ingl\u00eas), esquematizado a seguir, \u00e9 um dos modelos de reator mais utilizados na engenharia qu\u00edmica.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/be\/Agitated_vessel.svg\" width=\"300\" height=\"300\"\/>\n\nNa modelagem de reatores CSTR, uma hip\u00f3tese muito utilizada \u00e9 a da [mistura perfeita](https:\/\/en.wikipedia.org\/wiki\/Perfect_mixing), em que considera-se que cada elemento de fluido da entrada se mistura instant\u00e2nea e perfeitamente ao conte\u00fado uniforme do reator. Nesse caso, o modelo \u00e9 chamado de *CSTR ideal*. \n\nA partir da rela\u00e7\u00e3o b\u00e1sica de balan\u00e7o:\n\n$$\\verb! Acumulo = Entrada - Saida + Geracao!,$$\n\nas seguintes equa\u00e7\u00f5es diferenciais ordin\u00e1rias podem ser obtidas para um CSTR ideal em que ocorra a rea\u00e7\u00e3o irrevers\u00edvel $A \\rightarrow B$:\n\n* **Balan\u00e7o de massa global**:\n\n$$\\frac{dV}{dt} = F_i - F$$\n\n* **Balan\u00e7o de massa por componente**:\n\n$$ \\frac{dVC_A}{dt} = F_i C_{Ai} - FC_A - VrC_A $$\n\n* **Balan\u00e7o de energia**:\n\n$$ \\frac{dVT}{dt} = F_i T_i - FT - \\frac{\\Delta H}{\\rho c_p} V_r C_A - \\frac{UA}{\\rho c_p} (T-T_j) $$\n\n* **Balan\u00e7o de energia na jaqueta de resfriamento**:\n\n$$ \\frac{dT_j}{dt} = \\frac{F_j (T_{ji}-T_j)}{V_j} + \\frac{UA}{\\rho_j c_{pj} V_j} (T-T_j) $$\n\nO modelo fica completo com a adi\u00e7\u00e3o da [taxa de rea\u00e7\u00e3o](https:\/\/en.wikipedia.org\/wiki\/Reaction_rate):\n\n* **Taxa de rea\u00e7\u00e3o ([equa\u00e7\u00e3o de Arrhenius](https:\/\/en.wikipedia.org\/wiki\/Arrhenius_equation))**:\n\n$$ r = r_0 e^{-E\/RT}$$\n\nAl\u00e9m do mais, podem ser adicionadas equa\u00e7\u00f5es de [controle](https:\/\/en.wikipedia.org\/wiki\/Control_system) (no caso, do tipo [proporcional](https:\/\/en.wikipedia.org\/wiki\/Proportional_control)):\n\n* **Equa\u00e7\u00f5es de controle**:\n\n$$ F = F_{set} +P_V (V_{set}-V)$$\n\n$$ F_j = F_{j, set} +P_T (T_{set}-T) $$\n\nSeguem as descri\u00e7\u00f5es das vari\u00e1veis:\n\n* **Vari\u00e1veis de estado**: \n    * $V$: volume de rea\u00e7\u00e3o;\n    * $C_A$: concentra\u00e7\u00e3o do reagente $A$ no reator;\n    * $T$: temperatura do reator;\n    * $T_j$: temperatura da jaqueta.\n* **Vari\u00e1veis de entrada**: \n    * $F_i$: vaz\u00e3o de entrada do reator;\n    * $C_{Ai}$: concentra\u00e7\u00e3o de $A$ na entrada do reator;\n    * $T_i$: temperatura da entrada do reator;\n    * $T_{ji}$: temperatura de entrada da jaqueta.\n* **Vari\u00e1veis manipuladas**: \n    * $F$: vaz\u00e3o de sa\u00edda do reator;\n    * $F_{j}$: vaz\u00e3o de \u00e1gua de resfriamento da jaqueta.\n* **Vari\u00e1veis controladas**: $V$ e $T$.\n* **Par\u00e2metros**: \n    * $r_0$: fator pr\u00e9-exponencial da equa\u00e7\u00e3o de Arrhenius;\n    * $E$: energia de ativa\u00e7\u00e3o da equa\u00e7\u00e3o de Arrhenius;\n    * $\\Delta H$: entalpia de rea\u00e7\u00e3o;\n    * $\\rho$: densidade da mistura; \n    * $c_p$: capacidade calor\u00edfica da mistura; \n    * $U$: coeficiente global de transfer\u00eancia de calor;\n    * $A$: \u00e1rea de troca t\u00e9rmica;\n    * $V_j$: volume da jaqueta; \n    * $\\rho_j$: densidade do fluido de resfriamento;\n    * $c_{pj}$ : capacidade calor\u00edfica do fluido de resfriamento;\n    * $P_V$: ganho do controlador de vaz\u00e3o de sa\u00edda;\n    * $P_T$: ganho do controlador de vaz\u00e3o de fluido de resfriamento.\n    \nO modelo acima \u00e9 baseado no estudo de caso proposto por [FEITAL *et al.* (2013)](https:\/\/aiche.onlinelibrary.wiley.com\/doi\/abs\/10.1002\/aic.13953).\n   \n## Implementa\u00e7\u00e3o\n    \nComo os resultados do modelo s\u00e3o totalmente determin\u00edsticos, \u00e9 \u00fatil adicionar ru\u00eddo de medi\u00e7\u00e3o para tornar os dados mais realistas. Isso ser\u00e1 feito pela fun\u00e7\u00e3o `create_df_with_noise`, definida abaixo, que aceita uma matriz de dados no argumento `array`, a fra\u00e7\u00e3o da variabilidade total que vai corresponder \u00e0 variabilidade do ru\u00eddo no argumento `noise_frac` e o \u00edndice m\u00e1ximo para o c\u00e1lculo dessa fra\u00e7\u00e3o no argumento `max_index_for_noise`:","1e7080c3":"Analisando o padr\u00e3o de dados faltantes:","d05635f4":"Perceba que foi necess\u00e1rio fornecer a priori o n\u00famero de clusters. Essa \u00e9 uma caracter\u00edstica do modelo $k$-means.\n\nPara verificar o desempenho do modelo, n\u00e3o \u00e9 apropriado utilizar a fun\u00e7\u00e3o `accuracy_score`, como no problema de classifica\u00e7\u00e3o, j\u00e1 que o r\u00f3tulo de uma classe verdadeira no vetor `y` n\u00e3o necessariamente \u00e9 o mesmo r\u00f3tulo do cluster correspondente em `y_pred`.\n\nTr\u00eas m\u00e9tricas de clusteriza\u00e7\u00e3o muito utilizadas s\u00e3o:\n\n- **homogeneidade**: maior quanto mais cada cluster cont\u00e9m apenas amostras de uma \u00fanica classe verdadeira.\n- **completude**: maior quanto mais amostras de cada classe verdadeira s\u00e3o atribu\u00eddas a um \u00fanico cluster.\n- **medida V**: a m\u00e9dia harm\u00f4nica entre homogeneidade e completude.\n\nAs tr\u00eas m\u00e9tricas variam entre 0 e 1 e podem ser calculadas de uma s\u00f3 vez com a fun\u00e7\u00e3o [homogeneity_completeness_v_measure](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.homogeneity_completeness_v_measure.html):","a645c5af":"Os seis grupos s\u00e3o facilmente discriminados visualmente. \n\nPara discrimin\u00e1-los computacionalmente, utilizaremos o modelo [$k$-means](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering), cujo funcionamento ser\u00e1 detalhado mais adiante.","c918cc9d":"Checando:","0ed9b03d":"Alguns pontos s\u00e3o enganosos e classificados incorretamente pelo $k$-means. Provavelmente correspondem ao transiente da troca de estados estacion\u00e1rios.\n\nComo temos os labels verdadeiros para esse caso - que s\u00e3o os set-points especificados na simula\u00e7\u00e3o - \u00e9 poss\u00edvel calcular as m\u00e9tricas homogeneidade, completude e medida V:","dc370507":"## Escola Piloto Virtual - PEQ\/COPPE\/UFRJ\n\n## Data Science e Machine Learning na Pr\u00e1tica - Introdu\u00e7\u00e3o e Aplica\u00e7\u00f5es na Ind\u00fastria de Processos\n\nEste notebook \u00e9 referente \u00e0 Aula 4 do curso, que trata do problema de [clusteriza\u00e7\u00e3o](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis) utilizando o modelo [$k$-means](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering). Ser\u00e3o dois estudos de caso:\n\n* [segmenta\u00e7\u00e3o de mercado](https:\/\/pt.wikipedia.org\/wiki\/Segmenta%C3%A7%C3%A3o_de_mercado);\n* identifica\u00e7\u00e3o autom\u00e1tica de [modos de opera\u00e7\u00e3o em um reator qu\u00edmico](http:\/\/www.learncheme.com\/simulations\/kinetics-reactor-design\/multiple-steady-states-in-cstr-with-heat-exchange).","a3bb01b0":"No caso acima, ocorreu aumento de dimensionalidade (j\u00e1 que h\u00e1 mais centr\u00f3ides do que dimens\u00e3o original do problema, 2). Mas no caso de conjuntos com muitas vari\u00e1veis, a transforma\u00e7\u00e3o poderia ser usada para fins de redu\u00e7\u00e3o de dimensionalidade.\n\nAgora vamos apresentar o modelo formalmente e entender como funciona o c\u00e1lculo dos centr\u00f3ides.","44bb61fa":"Nota-se que h\u00e1 um ponto de inflex\u00e3o em $k=4$, a partir do qual n\u00e3o h\u00e1 uma diminui\u00e7\u00e3o substancial da in\u00e9rcia conforme se aumenta $k$. Portanto, o valor de $k$ sugerido pelo m\u00e9todo seria $4$. N\u00e3o \u00e9 a melhor solu\u00e7\u00e3o (sabemos que o \u00f3timo \u00e9 $k=5$), apesar de levar a um resultado razo\u00e1vel: ","8f81572d":"# Segmenta\u00e7\u00e3o de mercado\n\n[Segmenta\u00e7\u00e3o de mercado](https:\/\/pt.wikipedia.org\/wiki\/Segmenta%C3%A7%C3%A3o_de_mercado) \u00e9 o processo de identificar, em um conjunto heterog\u00eaneo de consumidores, grupos com caracter\u00edsticas e comportamentos semelhantes. \u00c9 uma importante ferramenta para entender o mercado e direcionar a\u00e7\u00f5es espec\u00edficas para cada tipo de cliente.\n\nExistem v\u00e1rias metodologias para efetuar a segmenta\u00e7\u00e3o. Uma muito conhecida \u00e9 a [RFM](https:\/\/en.wikipedia.org\/wiki\/RFM_(market_research)), que visa dividir grupos de acordo com os seguintes crit\u00e9rios:\n\n* **Recency**: quando a \u00faltima compra foi efetuada;\n\n* **Frequency**: com que frequ\u00eancia compras s\u00e3o efetuadas;\n\n* **Monetary Value**: quanto o cliente costuma gastar.\n\nO [conjunto de dados](https:\/\/www.kaggle.com\/carrie1\/ecommerce-data) utilizado refere-se a transa\u00e7\u00f5es efetuadas entre 01\/12\/2010 e 09\/12\/2011 por uma loja online de varejo do setor de presentes baseada no Reino Unido. \n\nA an\u00e1lise aqui efetuada foi baseada [neste curso](https:\/\/www.datacamp.com\/courses\/customer-segmentation-in-python), em particular [nestes](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/production\/course_10628\/slides\/chapter2.pdf), [nestes](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/production\/course_10628\/slides\/chapter3.pdf) e [nestes](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/production\/course_10628\/slides\/chapter4.pdf) slides. Para um estudo mais aprofundado aplicado ao mesmo conjunto de dados, recomendo [este paper](https:\/\/link.springer.com\/article\/10.1057\/dbm.2012.17) ou [este notebook](https:\/\/www.kaggle.com\/fabiendaniel\/customer-segmentation).\n\n## Importando e analisando dados\n\nUtilizando a fun\u00e7\u00e3o `read_csv` para ler os dados:","263cce22":"## $k$-means - Formaliza\u00e7\u00e3o\n\nEnunciaremos as hip\u00f3teses relativas ao modelo:\n\n* **Hip\u00f3tese 1 (n\u00famero de clusters)**: o n\u00famero $k$ de clusters \u00e9 conhecido a priori.\n\nSeja um conjunto de dados $\\mathbf{X} \\in \\mathbb{R}^{n\\times m}$, em que cada linha corresponde a uma observa\u00e7\u00e3o e cada coluna corresponde a uma vari\u00e1vel. Sendo v\u00e1lida a Hip\u00f3tese 1, o modelo pode definir os $k$ clusters dividindo o espa\u00e7o $\\mathbb{R}^{m}$ em $k$ regi\u00f5es disjuntas associadas a cada um dos clusters. \n\n* **Hip\u00f3tese 2 (caracteriza\u00e7\u00e3o dos clusters)**: um cluster $j$ pode ser caracterizado por meio de um par\u00e2metro de centralidade.\n\nEm particular, o par\u00e2metro utilizado no $k$-means \u00e9 o *centr\u00f3ide*, a m\u00e9dia $\\mu_j$ das observa\u00e7\u00f5es contidas no cluster.\n\n* **Hip\u00f3tese 3 (composi\u00e7\u00e3o dos clusters)**: cada cluster \u00e9 composto pelas observa\u00e7\u00f5es que se encontram mais pr\u00f3ximas de seu centr\u00f3ide do que dos demais centr\u00f3ides.\n\nDo ponto de vista geom\u00e9trico, a hip\u00f3tese 3 implica que os clusters devem ser [convexos](https:\/\/pt.wikipedia.org\/wiki\/Convexo) e [isotr\u00f3picos](https:\/\/en.wikipedia.org\/wiki\/Isotropy). Al\u00e9m do mais, como tudo o que importa para a atribui\u00e7\u00e3o de uma observa\u00e7\u00e3o a um cluster \u00e9 a dist\u00e2ncia para o centr\u00f3ide, o modelo pode n\u00e3o funcionar bem quando os clusters t\u00eam diferentes vari\u00e2ncias. Para uma demonstra\u00e7\u00e3o pr\u00e1tica do mal funcionamento do modelo no caso de viola\u00e7\u00e3o das hip\u00f3teses, recomendo [esta p\u00e1gina](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py).\n\nV\u00e1lidas as hip\u00f3teses, \u00e9 poss\u00edvel definir uma fun\u00e7\u00e3o objetivo para minimiza\u00e7\u00e3o, denominada *in\u00e9rcia*, a soma dos quadrados das dist\u00e2ncias de cada observa\u00e7\u00e3o ao centr\u00f3ide de seu cluster:\n\n$$\n\\sum_{j=1}^k \\sum_{i=1}^{n_j}||x_i-\\mu_j||^2,\n$$\n\nem que $j = 1,...,k$ s\u00e3o os v\u00e1rios clusters e $i = 1, ..., {n_j}$ s\u00e3o as observa\u00e7\u00f5es em um cluster $j$. As vari\u00e1veis de decis\u00e3o do espa\u00e7o de otimiza\u00e7\u00e3o s\u00e3o os centr\u00f3ides $\\mu_j$. \n\nPodemos acessar a in\u00e9rcia de um modelo do `scikit-learn` por meio do atributo `inertia_`:","f0672316":"Em todos os gr\u00e1ficos, \u00e9 evidente a maior adequa\u00e7\u00e3o da escolha $k=3$.\n\nVisualizando o espa\u00e7o $C_A,T,T_j$, tanto com os r\u00f3tulos verdadeiros quanto com os r\u00f3tulos atribu\u00eddos pelo $k$-means:","b1892100":"Visualizando as distribui\u00e7\u00f5es resultantes:","7f40cd22":"Poder\u00edamos aplicar a clusteriza\u00e7\u00e3o neste momento, mas as distribui\u00e7\u00f5es das vari\u00e1veis s\u00e3o muito [assim\u00e9tricas](https:\/\/pt.wikipedia.org\/wiki\/Assimetria_(estat%C3%ADstica)), como mostrado nos gr\u00e1ficos abaixo:","e570c49a":"As curvas n\u00e3o s\u00e3o muito informativas. Em particular, na curva de in\u00e9rcia, fora o ponto $k=2$, as demais inflex\u00f5es s\u00e3o muito sutis. Isso dificulta a escolha visual em que se baseia o m\u00e9todo do cotovelo.\n\nChecando os diagramas de silhueta:","e6e5ee92":"O gr\u00e1fico acima sugere, como o m\u00e9todo do cotovelo, a escolha $k=4$. Apesar disso, fica claro que o valor $k=5$ tamb\u00e9m \u00e9 bom e consideravelmente melhor do que os subsequentes.\n\nUm gr\u00e1fico muito mais informativo \u00e9 o *diagrama de silhueta*. Para plot\u00e1-lo, utilizaremos a biblioteca [Yellowbrick](https:\/\/www.scikit-yb.org\/), que cont\u00e9m rotinas para visualiza\u00e7\u00e3o de resultados de aprendizado de m\u00e1quina. Em particular, utilizaremos o m\u00e9todo [silhouette_visualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/silhouette.html#yellowbrick.cluster.silhouette.silhouette_visualizer) do m\u00f3dulo [yellowbrick.cluster](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/), que aceita um modelo e uma matriz de dados e plota o diagrama de silhueta:","e7cfcd4c":"\u00c9 \u00fatil calcular a soma total gasta em cada compra:","91f86007":"Acima da figura est\u00e3o impressos os valores de [skew](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.skew.html) de cada vari\u00e1vel, uma medida de assimetria; quanto mais distantes de zero, mais assim\u00e9tricas s\u00e3o as distribui\u00e7\u00f5es.\n\nPodemos aplicar [transforma\u00e7\u00f5es de pot\u00eancia](https:\/\/en.wikipedia.org\/wiki\/Power_transform) para aumentar a simetria das distribui\u00e7\u00f5es, tornando-as mais \"gaussianas\". Para isso, utilizaremos a fun\u00e7\u00e3o [power_transform](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.power_transform.html) do m\u00f3dulo [sklearn.preprocessing](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing):","b8f74da2":"* Foram fornecidos como par\u00e2metros \u00e0 fun\u00e7\u00e3o `make_blobs` o n\u00famero de observa\u00e7\u00f5es e os centros e desvios-padr\u00e3o de cada cluster. O n\u00famero de clusters gerado foi 5, de acordo com os formatos das arrays `blob_centers` e `blob_std`.\n\n* Apesar de a fun\u00e7\u00e3o retornar os r\u00f3tulos verdadeiros `y`, eles n\u00e3o ser\u00e3o usados no treinamento dos modelos.\n\nVisualizando os dados `X`:","f01bf5bb":"## Selecionando o n\u00famero de clusters\n\nDuas t\u00e9cnicas ser\u00e3o apresentadas para decidir o n\u00famero de clusters: os m\u00e9todos [do cotovelo](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)) e da [silhueta](https:\/\/en.wikipedia.org\/wiki\/Silhouette_(clustering)).\n\n### M\u00e9todo do cotovelo\n\nNo m\u00e9todo do cotovelo, a in\u00e9rcia \u00e9 plotada em fun\u00e7\u00e3o do n\u00famero de clusters, como abaixo:","2d98e04d":"Um algoritmo muito usado para efetuar essa minimiza\u00e7\u00e3o de maneira eficiente \u00e9 o [algoritmo de Lloyd](https:\/\/en.wikipedia.org\/wiki\/Lloyd%27s_algorithm):\n\n1. **ESPECIFIQUE** um n\u00famero de clusters $k$.\n2. **INICIALIZE** os centr\u00f3ides $\\mu_j$ utilizando $k$ observa\u00e7\u00f5es aleat\u00f3rias.\n3. **ATRIBUA** a cada observa\u00e7\u00e3o o cluster de centr\u00f3ide mais pr\u00f3ximo.\n4. **RECALCULE** os centr\u00f3ides $\\mu_j$ de acordo com os clusters rec\u00e9m-atribu\u00eddos.\n5. **VOLTE** ao passo 3 e **REPITA** at\u00e9 converg\u00eancia.\n\nO algoritmo acima tem converg\u00eancia garantida, apesar de facilmente resultar em m\u00ednimos locais que podem corresponder a solu\u00e7\u00f5es esp\u00farias. Na pr\u00e1tica, executa-se o algoritmo v\u00e1rias vezes com diferentes inicializa\u00e7\u00f5es e escolhe-se a que resulta em menor in\u00e9rcia. No caso do `scikit-learn`, por default, o algoritmo \u00e9 executado 10 vezes.\n\nPode-se mostrar que, geometricamente, a etapa 3 do algoritmo resulta nas parti\u00e7\u00f5es de Voronoi anteriormente apresentadas em gr\u00e1fico.\n\nA maneira como se efetua a inicializa\u00e7\u00e3o aleat\u00f3ria dos centr\u00f3ides, no passo 2, \u00e9 um importante fator para converg\u00eancia. [ARTHUR e VASSILVITSKII (2007)](https:\/\/theory.stanford.edu\/~sergei\/papers\/kMeansPP-soda.pdf), por exemplo, propuseram o [$k$-Means++](https:\/\/en.wikipedia.org\/wiki\/K-means%2B%2B), em que for\u00e7a-se os centr\u00f3ides iniciais a resultarem afastados uns dos outros, aumentando a probabilidade de converg\u00eancia para a solu\u00e7\u00e3o \u00f3tima. Essa \u00e9 a inicializa\u00e7\u00e3o utilizada por default no `scikit-learn`.\n\nH\u00e1 diversas propostas para melhoria do algoritmo b\u00e1sico acima apresentado. [ELKAN (2003)](https:\/\/www.aaai.org\/Papers\/ICML\/2003\/ICML03-022.pdf) sugeriu uma metodologia em que muitos c\u00e1lculos de dist\u00e2ncia s\u00e3o evitados, utilizando o conceito de [desigualdade triangular](https:\/\/pt.wikipedia.org\/wiki\/Desigualdade_triangular) e por meio do rastreamento de limites superiores e inferiores para dist\u00e2ncias entre observa\u00e7\u00f5es e centr\u00f3ides. \u00c9 o algoritmo utilizado por default no `scikit-learn`.\n\n[HUANG (1997)](https:\/\/link.springer.com\/article\/10.1023\/A:1009769707641) prop\u00f4s o $k$-modes, designado para lidar com vari\u00e1veis categ\u00f3ricas, por meio da defini\u00e7\u00e3o dos clusters com base no n\u00famero de categorias coincidentes entre as observa\u00e7\u00f5es. [HUANG (1997)](https:\/\/grid.cs.gsu.edu\/~wkim\/index_files\/papers\/kprototype.pdf), em outro paper do mesmo ano, prop\u00f4s ainda o $k$-prototypes, em que se combina o tratamento de vari\u00e1veis cont\u00ednuas e categ\u00f3ricas. Ambas as implementa\u00e7\u00f5es n\u00e3o est\u00e3o dispon\u00edveis no `scikit-learn`, mas podem ser aplicadas por meio da biblioteca [kmodes](https:\/\/pypi.org\/project\/kmodes\/).\n\nOutro avan\u00e7o importante foi a proposta de [SCULLEY (2010)](https:\/\/www.eecs.tufts.edu\/~dsculley\/papers\/fastkmeans.pdf) para treino do $k$-means utilizando minilotes, de maneira parecida com o que \u00e9 feito em redes neurais, o que acelera o algoritmo e possibilita o seu uso em conjuntos de dados que n\u00e3o caibam na mem\u00f3ria. No `scikit-learn`, essa metodologia est\u00e1 dispon\u00edvel na classe [MiniBatchKMeans](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.MiniBatchKMeans.html).\n\n","07de7622":"# Conclus\u00e3o\n\nNesta aula aprendemos como resolver problemas de [clusteriza\u00e7\u00e3o](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis) utilizando o modelo [$k$-means](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering), aplicando aos casos de [segmenta\u00e7\u00e3o de mercado](https:\/\/pt.wikipedia.org\/wiki\/Segmenta%C3%A7%C3%A3o_de_mercado) e identifica\u00e7\u00e3o de [modos de opera\u00e7\u00e3o em um reator qu\u00edmico](http:\/\/www.learncheme.com\/simulations\/kinetics-reactor-design\/multiple-steady-states-in-cstr-with-heat-exchange).\n\n\u00c9 importante ressaltar que, apesar de a maior parte das aplica\u00e7\u00f5es de aprendizado de m\u00e1quina ser de algoritmos supervisionados, a maioria dos dados dispon\u00edveis *n\u00e3o* possui r\u00f3tulo. Da\u00ed a import\u00e2ncia de dominar tamb\u00e9m metodologias n\u00e3o-supervisionadas, j\u00e1 que elas possibilitam a captura de padr\u00f5es a partir da estrutura dos dados $X$ sem a necessidade da utiliza\u00e7\u00e3o de r\u00f3tulos (conjuntos de sa\u00edda $y$) que guiem o aprendizado.\n\n\u00c9 isso. At\u00e9 a pr\u00f3xima!","c70fb7d2":"# Videoaula\n\nEste notebook \u00e9 explicado em detalhes ao longo da seguinte videoaula:","f0c6971a":"* Nos gr\u00e1ficos acima, cada figura em forma de faca corresponde a um cluster e cont\u00e9m os coeficientes de silhueta de suas observa\u00e7\u00f5es.\n\n* A altura de cada figura indica o tamanho (quantidade de observa\u00e7\u00f5es) de cada cluster.\n\n* Quanto mais comprida for uma figura, mais os coeficientes de silhueta do cluster correspondente aproximam-se de $1$, o valor \u00f3timo.\n\n* A an\u00e1lise dos gr\u00e1ficos indicaria que $k=4$ ou $5$ seriam escolhas razo\u00e1veis, j\u00e1 que em todos os clusters as figuras estendem-se al\u00e9m da linha vertical que representa a silhueta m\u00e9dia. \n\n* Na pr\u00e1tica, a escolha por $k=5$ poderia ser feita de modo a gerar clusters de tamanhos semelhantes.\n\nAgora vamos analisar uma aplica\u00e7\u00e3o pr\u00e1tica de clusteriza\u00e7\u00e3o, a segmenta\u00e7\u00e3o de mercado.","57359e83":"Para a playlist do curso completo, clique [aqui](https:\/\/www.youtube.com\/playlist?list=PLvr45Arc0UpzsRhzq3q4_KmZcm0utwvvB).","a443ea37":"Fica claro dos gr\u00e1ficos anteriores que os clusters obtidos n\u00e3o refletem uma estrutura de agrupamento intr\u00ednseca dos dados. Isso n\u00e3o \u00e9 necessariamente um problema: se o modelo conseguir dividir o espa\u00e7o em clusters de clientes com caracter\u00edsticas espec\u00edficas, o resultado pode ser satisfat\u00f3rio.\n\nPara entender melhor as diferen\u00e7as entre os clusters, \u00e9 \u00fatil tra\u00e7armos um [snake plot](https:\/\/www.marketingprofs.com\/tutorials\/snakeplot.asp), em que as m\u00e9dias de rec\u00eancia, frequ\u00eancia e valor monet\u00e1rio de cada um dos clusters s\u00e3o interligados por segmentos de reta:","cddf850f":"Descri\u00e7\u00f5es das colunas, de acordo com o [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail):\n\n* **InvoiceNo**: N\u00famero da fatura. Nominal, um n\u00famero integral de 6 d\u00edgitos atribu\u00eddo exclusivamente a cada transa\u00e7\u00e3o. Se este c\u00f3digo come\u00e7ar com a letra 'c', isso indica um cancelamento.\n* **StockCode**: c\u00f3digo do produto (item). Nominal, um n\u00famero integral de 5 d\u00edgitos atribu\u00eddo exclusivamente a cada produto distinto.\n* **Description**: Nome do produto (item). Nominal.\n* **Quantity**: as quantidades de cada produto (item) por transa\u00e7\u00e3o. Num\u00e9rico.\n* **InvoiceDate**: Data e hora do invent\u00e1rio. Num\u00e9rico, o dia e a hora em que cada transa\u00e7\u00e3o foi gerada.\n* **UnitPrice**: pre\u00e7o unit\u00e1rio. Num\u00e9rico, pre\u00e7o do produto por unidade em libras esterlinas.\n* **CustomerID**: N\u00famero do cliente. Nominal, um n\u00famero integral de 5 d\u00edgitos atribu\u00eddo exclusivamente a cada cliente.\n* **Contry**: nome do pa\u00eds. Nominal, o nome do pa\u00eds onde cada cliente reside.\n\nAnalisando os tipos das vari\u00e1veis no DataFrame:","47cb4aad":"Visualizando o `DataFrame` resultante:","adccdc07":"O modelo CSTR est\u00e1 implementado na classe `CSTR`, definida a seguir.","77bad79a":"* A cada cluster $j$ \u00e9 atribu\u00eddo um ponto especial $\\mu_j$ chamado de *centr\u00f3ide*, correspondente \u00e0 m\u00e9dia dos pontos contidos no cluster. Na figura, os centr\u00f3ides est\u00e3o marcados com `x`.\n\n* Gr\u00e1ficos em que o espa\u00e7o \u00e9 particionado em formatos geom\u00e9tricos disjuntos (sem sobreposi\u00e7\u00f5es) s\u00e3o chamados de [tessela\u00e7\u00f5es](https:\/\/en.wikipedia.org\/wiki\/Tessellation). Em particular, o gr\u00e1fico acima, em que cada parti\u00e7\u00e3o $j$ \u00e9 definida como o lugar geom\u00e9trico dos pontos mais pr\u00f3ximos a um certo ponto \u00e9 chamado de [tessela\u00e7\u00e3o de Voronoi](https:\/\/en.wikipedia.org\/wiki\/Voronoi_diagram).\n\n* No modelo $k$-means, as regi\u00f5es que separam os $k$ clusters s\u00e3o definidas como as parti\u00e7\u00f5es de Voronoi dos respectivos centr\u00f3ides $\\mu_j$. \n\n* As fronteiras de decis\u00e3o s\u00e3o lineares. \n\n* A maior fonte de erros parece estar na atribui\u00e7\u00e3o de observa\u00e7\u00f5es de um cluster mais espalhado a um cluster mais compacto. De fato, como veremos, um dos pontos fracos do modelo $k$-means \u00e9 n\u00e3o lidar bem com clusters de vari\u00e2ncias diferentes.\n\nPodemos expressar os dados `X` em um novo espa\u00e7o, em que as coordenadas de cada observa\u00e7\u00e3o s\u00e3o as dist\u00e2ncias a cada centr\u00f3ide. Essa transformac\u00e3o, conhecida como [quantiza\u00e7\u00e3o vetorial](https:\/\/en.wikipedia.org\/wiki\/Vector_quantization), \u00e9 efetuada com o m\u00e9todo `transform` do modelo:","ca1b4307":"Os sombreados definem intervalos de confian\u00e7a de 95%.\n\nDo primeiro gr\u00e1fico acima (em que $k=3$), percebe-se que os clusters representam tr\u00eas tipos distintos de clientes:\n\n* $0$: aparecem com m\u00e9dia frequ\u00eancia, n\u00e3o gastam muito e compraram h\u00e1 n\u00e3o muito tempo.\n* $1$: aparecem com baixa frequ\u00eancia, gastam pouco e compraram h\u00e1 muito tempo. Em tese, \u00e9 o cluster de clientes que agregam menos valor ao neg\u00f3cio.\n* $2$: aparecem com bastante frequ\u00eancia, gastam muito e compraram h\u00e1 pouco tempo. Em tese, \u00e9 o cluster de clientes mais valiosos.\n\nA introdu\u00e7\u00e3o de mais um cluster no modelo divide o cluster $0$ do gr\u00e1fico da esquerda em dois, os clusters $0$ e $3$ no gr\u00e1fico da direita. A escolha por efetuar ou n\u00e3o essa divis\u00e3o adicional pode ser uma decis\u00e3o estrat\u00e9gica da companhia, baseada nas a\u00e7\u00f5es que ser\u00e3o tomadas a partir da informa\u00e7\u00e3o fornecida pelo modelo.\n\nAgora vamos iniciar nosso segundo estudo de caso, a identifica\u00e7\u00e3o de modos de opera\u00e7\u00e3o em um reator qu\u00edmico.","b02597b0":"# Clusteriza\u00e7\u00e3o\n\n* Na clusteriza\u00e7\u00e3o, o objetivo \u00e9 agrupar observa\u00e7\u00f5es similares em grupos chamados de *clusters*.\n\n* \u00c9 um procedimento an\u00e1logo \u00e0 classifica\u00e7\u00e3o, por\u00e9m n\u00e3o-supervisionado. N\u00e3o h\u00e1 um conjunto de dados `y` com os r\u00f3tulos de cada cluster: os algoritmos devem agrupar as observa\u00e7\u00f5es apenas utilizando a estrutura da matriz de dados `X`.\n\nVamos utilizar a fun\u00e7\u00e3o [make_blobs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_blobs.html) do `scikit-learn` de modo a gerar um conjunto de dados bidimensional simples para ver a metodologia em a\u00e7\u00e3o:","da8380dc":"# O modelo $k$-means\n\nPara come\u00e7armos a entender o modelo, \u00e9 interessante visualizar seu resultado em gr\u00e1fico. A fun\u00e7\u00e3o `plot_decision_boundaries`, definida abaixo, aceita um modelo e um conjunto de dados e exibe as fronteiras de decis\u00e3o de cada cluster:","c62948dd":"Os diagramas de silhueta s\u00e3o bem mais informativos. Todas as configura\u00e7\u00f5es resultam em clusters com boa quantidade de silhuetas acima da m\u00e9dia (que n\u00e3o \u00e9 muito alta, entretanto). A escolha de 4 clusters parece ser particularmente adequada por resultar em conjuntos de tamanhos semelhantes.\n\n\u00c9 poss\u00edvel visualizar o espa\u00e7o RFM em gr\u00e1ficos 3d. Abaixo s\u00e3o comparadas duas escolhas, $k=3$ e $k=4$:","223b1a6e":"Visualizando o DataFrame:","bba0f005":"Parece bem melhor!\n\nComo \u00faltima etapa de pr\u00e9-processamento, normalizemos as vari\u00e1veis para que todas tenham m\u00e9dia $0$ e desvio-padr\u00e3o $1$:","b7507440":"**M\u00e3o na massa!**\n\n* Adicione o conjunto de dados [Mall Customer Segmentation Data](https:\/\/www.kaggle.com\/vjchoudhary7\/customer-segmentation-tutorial-in-python) ao notebook. \n\n* Efetue uma explora\u00e7\u00e3o inicial do conjunto (uma boa biblioteca para isso \u00e9 a [pandas-profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling), vale a pena us\u00e1-la).\n\n* Aplique o modelo $k$-means para obter clusters de clientes de diferentes caracter\u00edsticas. Identifique que tipos de clientes os clusters conseguem discriminar. \n\nDica: nem todas as vari\u00e1veis precisam ser utilizadas para que se atinja uma separa\u00e7\u00e3o adequada e interpret\u00e1vel.","c28666d6":"Essa normaliza\u00e7\u00e3o faz com que as vari\u00e1veis se distribuam em torno do mesmo ponto e com o mesmo grau de espalhamento, o que pode ajudar bastante a aplica\u00e7\u00e3o do algoritmo.\n\n## Modelagem\n\nO primeiro passo \u00e9 calcular as curvas de in\u00e9rcia e silhueta em fun\u00e7\u00e3o de $k$ de modo a tentar encontrar uma pista para um bom n\u00famero de clusters:","8c64b6ab":"Percebe-se que h\u00e1 muitas compras em que n\u00e3o se identificam os consumidores.\n\n## Pr\u00e9-processamento dos dados\n\nPodemos retirar as observa\u00e7\u00f5es em que os consumidores n\u00e3o s\u00e3o identificados:","9c08b51f":"As m\u00e9tricas confirmam o bom desempenho verificado com as visualiza\u00e7\u00f5es anteriores.\n\nA metodologia apresentada pode ser utilizada, por exemplo, para gerar um cluster de dados para cada modo de opera\u00e7\u00e3o no contexto do treinamento de modelos de detec\u00e7\u00e3o de falhas. O PCA \u00e9 um caso de modelo que n\u00e3o conseguiria capturar a estrutura multimodal de dados contendo v\u00e1rios modos de opera\u00e7\u00e3o. A estrat\u00e9gia poderia ser, portanto, gerar um modelo PCA para cada cluster e utilizar o $k$-means para determinar automaticamente, durante a opera\u00e7\u00e3o, o estado estacion\u00e1rio em que o processo se encontra no momento. Voc\u00ea pode inclusive tentar implementar essa estrat\u00e9gia utilizando o modelo PCA da Aula 2 (para rodar uma simula\u00e7\u00e3o `CSTR` com desvios em vari\u00e1veis de processo, especifique o argumento `faulty` do m\u00e9todo `run` como `True`).","976d67a0":"O primeiro ter\u00e7o dos instantes foi atribu\u00eddo ao EE 2, o segundo ter\u00e7o ao EE 1 e o \u00faltimo ter\u00e7o ao EE 0.\n\nRodando a simula\u00e7\u00e3o:","39cb6ab6":"Visualizando a evolu\u00e7\u00e3o temporal das vari\u00e1veis de estado, de entrada e manipuladas, respectivamente:","c2cc6318":"A separa\u00e7\u00e3o entre os estados estacion\u00e1rios fica evidente nos gr\u00e1ficos de $C_A$, $T$ e $T_j$.\n\n## Aplica\u00e7\u00e3o do $k$-means\n\nComo no exemplo anterior, come\u00e7emos gerando os gr\u00e1ficos de in\u00e9rcia e silhueta:","994e8c15":"## Integra\u00e7\u00e3o do modelo\n\nInicializando um objeto `CSTR`:","c06b455d":"Na c\u00e9lula abaixo, os dados s\u00e3o transformados para o espa\u00e7o RFM:  ","67a9b752":"O que ocorre \u00e9 a interpreta\u00e7\u00e3o dos dois clusters do canto inferior direito, que s\u00e3o bem pr\u00f3ximos, como um cluster s\u00f3.\n\nObs: o m\u00e9todo do cotovelo tamb\u00e9m \u00e9 usado em outros casos, como para selecionar o n\u00famero de componentes principais no PCA, por exemplo.\n\n### M\u00e9todo da silhueta\n\nO coeficiente de silhueta de uma observa\u00e7\u00e3o $x$ \u00e9 definido como:\n\n$$\ns = \\frac{b-a}{\\max(a,b)},\n$$\n\nsendo:\n\n* $a$ a dist\u00e2ncia m\u00e9dia intra-cluster, ou seja, a m\u00e9dia das dist\u00e2ncias de $x$ \u00e0s observa\u00e7\u00f5es do mesmo cluster;\n* $b$ a dist\u00e2ncia m\u00e9dia para o cluster mais pr\u00f3ximo, ou seja, a m\u00e9dia das dist\u00e2ncias de $x$ \u00e0s observa\u00e7\u00f5es do cluster mais pr\u00f3ximo.\n\n$s$ varia entre $-1$ e $1$:\n\n* Um valor perto de $1$ sugere que $x$ est\u00e1 afastado dos demais clusters e contido em um cluster de amostras similares.\n* Um valor perto de $0$ sugere que a amostra est\u00e1 pr\u00f3xima de uma fronteira entre clusters.\n* Um valor perto de $-1$ sugere que a amostra est\u00e1 no cluster errado.\n\nA fun\u00e7\u00e3o [silhouette_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html) retorna uma m\u00e9trica definida como a m\u00e9dia das silhuetas de todas as observa\u00e7\u00f5es. Abaixo, visualizamos um gr\u00e1fico dessa m\u00e9trica em fun\u00e7\u00e3o do n\u00famero de clusters:","961c9760":"A figura acima ilustra que h\u00e1 tr\u00eas estados estacion\u00e1rios no processo, j\u00e1 que s\u00e3o tr\u00eas os pontos em que o calor gerado \u00e9 igual ao calor removido.\n\nNa classe `CSTR`, os set-points dispon\u00edveis para a malha de controle s\u00e3o os tr\u00eas estados estacion\u00e1rios (EE) apresentados acima (0, 1 ou 2). Devemos especific\u00e1-los por meio de uma array que cont\u00e9m os set-points de cada um dos instantes de amostragem, como definido abaixo:","97d40792":"Essas m\u00e9tricas s\u00e3o interessantes, mas na maioria dos problemas pr\u00e1ticos n\u00e3o podemos aplic\u00e1-las, j\u00e1 que os r\u00f3tulos verdadeiros `y` n\u00e3o est\u00e3o dispon\u00edveis. Nos casos em que est\u00e3o dispon\u00edveis, o mais apropriado em geral \u00e9 utilizar modelos de classifica\u00e7\u00e3o.","38e01dae":"Visualizando o [diagrama de van Heerden](https:\/\/pubs.acs.org\/doi\/abs\/10.1021\/ie50522a030):","231a9d76":"As duas primeiras distribui\u00e7\u00f5es parecem razo\u00e1veis, mas a \u00faltima ainda tem um valor de skew alto. Podemos tentar outra transforma\u00e7\u00e3o, aplicando a raiz c\u00fabica:"}}