{"cell_type":{"d9e436ce":"code","17016626":"code","ef07ef68":"code","b37ee3c4":"code","9a93b4b8":"code","cd8903e5":"code","e1238ce0":"code","cc3e1e48":"code","91f091d7":"code","d97c770b":"code","eacbd3da":"code","d58d6b7a":"code","f22eddcc":"code","bc94d91a":"code","1bb6c9c6":"code","28385e68":"code","e11462ac":"code","a764ea7a":"code","3d744b31":"code","6b8117d7":"code","7ede677c":"code","a67cf28e":"code","8b6f6565":"code","e6c97b72":"code","01eff6d9":"code","db1d964a":"code","1c967cf6":"code","afb6059f":"code","9ed88ee2":"code","95489533":"code","2d29d82c":"code","fc8a6130":"code","b2adb02f":"code","c4680ecf":"code","7fe0f009":"code","4779a6cc":"code","ebaf7c81":"code","9085e3ea":"code","a55c673a":"code","e1729ace":"markdown","3eb2af38":"markdown","c98a0deb":"markdown","8b19a8be":"markdown","b1db6977":"markdown","a80ae211":"markdown","c71a6184":"markdown","9a1912df":"markdown","073178b2":"markdown","96942592":"markdown","86781f19":"markdown","fdfd71b1":"markdown","02591298":"markdown","bb8481b6":"markdown","4d60e847":"markdown","8fc39af4":"markdown","960c94a8":"markdown","95e0db28":"markdown","ac8ffee7":"markdown","dc44e470":"markdown","f48c39ac":"markdown"},"source":{"d9e436ce":"%%capture\n# install missing packages\n!pip install networkx\n!pip install urllib3\n!pip install pytorch_lightning\n!pip install dgl ","17016626":"import os\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport dgl","ef07ef68":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport networkx as nx\n\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nfrom nilearn.image import index_img\nfrom nilearn import image\nfrom nilearn import plotting\nimport nibabel as nib\nimport h5py\nimport matplotlib.pyplot as plt\n\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nimport seaborn as sns\nimport datetime\n","b37ee3c4":"%%capture\n!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","9a93b4b8":"# assign paths to variables\n\nmask_filepath = '..\/input\/trends-assessment-prediction\/fMRI_mask.nii'\nexample_subject_filepath = '..\/input\/trends-assessment-prediction\/fMRI_train\/10004.mat'\nfmri_filepath = '..\/input\/trends-assessment-prediction\/fMRI_train\/'\nscores_filepath = '..\/input\/trends-assessment-prediction\/train_scores.csv'\nsmri_filepath = 'ch2better.nii'\nfnc_filepath = \"..\/input\/trends-assessment-prediction\/fnc.csv\"\nicn_numbers_filepath = \"..\/input\/trends-assessment-prediction\/ICN_numbers.csv\"","cd8903e5":"def load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with the version 7.3 flag. Return the subject\n    niimg, using a mask niimg as a template for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\n\n\nmask_niimg = nl.image.load_img(mask_filepath)\nsubject_niimg = load_subject(example_subject_filepath, mask_niimg)\n\nprint(f\"Image type is {type(subject_niimg)}\")\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","e1238ce0":"# nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filepath, view_type='filled_contours', draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')","cc3e1e48":"# grid_size = int(np.ceil(np.sqrt(num_components)))\n# fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n# [axi.set_axis_off() for axi in axes.ravel()]\n# row = -1\n# for i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n#     col = i % grid_size\n#     if col == 0:\n#         row += 1\n#     nlplt.plot_stat_map(cur_img, bg_img=smri_filepath, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)","91f091d7":"# data = subject_niimg.get_fdata()\n# m,n = data.shape[::2]\n# data_new = data.transpose(0,3,1,2).reshape(m,-1,n)\n# data_new.shape","d97c770b":"class fmri_dataset_generator(Dataset):\n    \"\"\"fmri dataset.\"\"\"\n    \n    def __init__(self, subject_image_paths, mask_file):\n        \"\"\"\n        Args:\n            subject_image_paths (string): Subject images\n            mask_file (string): The mask niimg object used for nifti headers.\n        \"\"\"\n        \n        self.subject_image_paths = subject_image_paths\n        self.mask_file = mask_file\n        \n    def __len__(self):\n        return len(self.subject_image_paths)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = self.subject_image_paths[idx]\n        img_data = load_subject(img_name, mask_niimg)\n        \n        np_data = []\n        for img in image.iter_img(img_data):\n            np_data.append(img.get_fdata())\n\n        img_data_conv = torch.unsqueeze(torch.FloatTensor(np_data),1)\n\n        return img_data_conv\n\ndef my_collate_fn(batch):\n    data=[]\n    for i in batch:\n        data.append(i)\n    data_ = torch.cat(data, dim=0)\n    return data_, data_\n  ","eacbd3da":"root_dir = \"..\/input\/trends-assessment-prediction\/fMRI_train\/\"\nall_mat_files = [\"\".join([root_dir, f]) for f in listdir(root_dir)]","d58d6b7a":"len(all_mat_files)","f22eddcc":"validation_split = 0.1\n\nk = round(len(all_mat_files) * validation_split)\nindicies = list(range(0,len(all_mat_files)))\n\nvalidation_ind = random.sample(indicies, k)\ntrain_ind = list(set(indicies) - set(validation_ind))","bc94d91a":"print(f\"Train dataset has {len(train_ind)} examples.\")\nprint(f\"Validation dataset has {len(validation_ind)} examples.\")","1bb6c9c6":"# select file paths for these datasets\nvalidation_datafiles = [all_mat_files[i] for i in validation_ind]\ntrain_datafiles = [all_mat_files[i] for i in train_ind]","28385e68":"train_dataset = fmri_dataset_generator(train_datafiles, mask_file=mask_niimg)\nval_dataset = fmri_dataset_generator(validation_datafiles, mask_file=mask_niimg)","e11462ac":"train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=my_collate_fn)","a764ea7a":"# A single batch\niter(train_dataloader).next()[0].size()","3d744b31":"IMAGE_WIDTH = 53\nIMAGE_HEIGHT = 63\nIMAGE_DEPTH = 52\n\ndebug_model = False\n\nclass AutoEncoder(nn.Module):\n   \n    def __init__(self):\n        super().__init__()\n       \n        # Encoder specification\n        self.enc_cnn_1 = nn.Conv3d(1, 2, kernel_size=3)\n        self.enc_cnn_2 = nn.Conv3d(2, 4 , kernel_size=3)\n        \n        self.enc_pool = nn.MaxPool3d(2, stride=2, return_indices=True)\n\n        self.enc_linear_1 = nn.Linear(6776, 25)\n        self.enc_linear_2 = nn.Linear(25, 10)\n        \n        self.dec_cnn_1 = nn.Conv3d(2, 1, kernel_size=3, padding = [2,2,2])\n        self.dec_cnn_2 = nn.Conv3d(4, 2 , kernel_size=3, padding = [2,2,2])\n        \n        self.dec_unpool = nn.MaxUnpool3d(2, stride=2)\n\n        self.dec_linear_1 = nn.Linear(10, 25)\n        self.dec_linear_2 = nn.Linear(25, 6776)\n       \n    def forward(self, images):\n        code, pool_par = self.encode(images)\n        if debug_model == True: print(\"encode complete\") \n        out = self.decode(code, pool_par)\n        if debug_model == True: print(\"decode complete\")\n        return out, \n   \n    def encode(self, images):\n        \n        if debug_model == True: print(f\"start encode: {images.shape}\")\n        \n        code = self.enc_cnn_1(images)\n        if debug_model == True: print(f\"after cnn_1: {code.shape}\")\n        \n        pool1, indices1 = self.enc_pool(code)\n        if debug_model == True: print(f\"after 1st max_pool3d: {pool1.shape}\")\n        code = F.selu(pool1)\n        \n        code = self.enc_cnn_2(code)\n        if debug_model == True: print(f\"after cnn_2: {code.shape}\")\n        \n        pool2, indices2 = self.enc_pool(code)\n        if debug_model == True: print(f\"after 2nd max_pool3d: {pool2.shape}\")\n        code = F.selu(pool2)\n        \n        code = pool2.view([images.size(0), -1])\n        if debug_model == True: print(f\"after view: {code.shape}\")\n        \n        code = F.selu(self.enc_linear_1(code))\n        if debug_model == True: print(f\"after linear_1: {code.shape}\")\n        \n        code = self.enc_linear_2(code)\n        if debug_model == True: print(f\"after linear_2: {code.shape}\")\n        \n        #required for unpool\n        pool_par = {\"P1\": [indices1], \"P2\": [indices2]}\n        \n        return code, pool_par\n   \n    def decode(self, code, pool_par):\n        \n        if debug_model == True: print(f\"start decode:{code.shape}\") \n        \n        out = self.dec_linear_1(code)\n        if debug_model == True: print(f\"after dec_linear_1:{out.shape}\")\n        \n        out = F.selu(self.dec_linear_2(out))\n        if debug_model == True: print(f\"after dec_linear_2:{out.shape}\")\n        \n        out = out.view([out.size(0), 4, 11, 14, 11]) \n        if debug_model == True: print(f\"after view:{out.shape}\")\n           \n        out = self.dec_unpool(out, pool_par['P2'][0], output_size = [53, 4, 23, 28, 23])\n        out = F.selu(out)\n        if debug_model == True: print(f\"after 1st unpool:{out.shape}\")\n        \n        out = self.dec_cnn_2(out)\n        if debug_model == True: print(f\"after dec_cnn_2:{out.shape}\")\n        \n        out = self.dec_unpool(out, pool_par['P1'][0], output_size = [53, 2, 51, 61, 50])\n        out = F.selu(out)\n        if debug_model == True: print(f\"after 2nd unpool:{out.shape}\")\n        \n        out = self.dec_cnn_1(out)\n        if debug_model == True: print(f\"after dec_cnn_1:{out.shape}\")\n        out = F.selu(out)\n        \n        return out","6b8117d7":"USE_GPU = True\n    \nif USE_GPU and torch.cuda.is_available():\n  device = 'cuda'\nelse:\n  device = 'cpu'\n\nprint(device)","7ede677c":"debug_train = True\ndebug_sample = 100\n\n# Hyperparameters\nnum_epochs = 5\nbs = 1\nlr = 0.002\noptimizer_cls = optim.Adam\n\ntrain_loss_values = []\nval_loss_values = []\n\n# Load data\ntrain_dataloader = DataLoader(train_dataset, batch_size = bs, collate_fn=my_collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=1, collate_fn=my_collate_fn)\n\n# Instantiate model\nautoencoder = AutoEncoder()\nautoencoder.to(device)\nloss_fn = nn.MSELoss()\noptimizer = optimizer_cls(autoencoder.parameters(), lr=lr)\n\nbegin_time = datetime.datetime.now()\n\n# Training loop\nfor epoch in range(num_epochs):\n    \n    train_loss=0.0\n    val_loss=0.0\n    print(\"Epoch %d \\n\" % epoch)\n   \n    for i, train_images in enumerate(train_dataloader):\n        \n        if i==debug_sample:\n            break\n        \n        print(f\"train_subject:{i}\")\n        \n        train_out = autoencoder(Variable(train_images[0]))\n        \n        optimizer.zero_grad()\n        t_loss = loss_fn(train_out[0], Variable(train_images[0]))\n        t_loss.backward()\n        optimizer.step()\n        \n        train_loss += t_loss.item()\n        \n    with torch.no_grad():\n        for i, val_images in enumerate(val_dataloader):\n            \n            if i==debug_sample:\n                break\n                \n            print(f\"val_subject:{i}\")\n            \n            autoencoder.eval()\n            \n            val_out = autoencoder(Variable(val_images[0]))\n            v_loss = loss_fn(val_out[0], Variable(val_images[0]))\n            val_loss += v_loss.item()\n            \n#     epoch_loss = running_loss \/ len(dataloaders['train'])\n    epoch_train_loss = train_loss \/ debug_sample\n    epoch_val_loss = val_loss \/ debug_sample\n    train_loss_values.append(epoch_train_loss)\n    val_loss_values.append(epoch_val_loss)\n    \n    plt.plot(np.array(train_loss_values), 'r', np.array(val_loss_values), 'b')\n#     plt.show()\n        \n    ModelCheckpoint(filepath='{epoch:02d}-{val_loss:.2f}.hdf5', monitor='valloss', verbose = 1)\n#     print(f\"\\nTrain Loss = %.3f\\n\\n\" % loss.data.item())\n#     print(f\"\\nVal Loss = %.3f\\n\\n\" % val_loss.data.item())\n\nprint(datetime.datetime.now() - begin_time)\n","a67cf28e":"loss_values","8b6f6565":"embeddings = []\nfor i, images in enumerate(dl):\n    if i == 1:\n        break\n    out, _ = autoencoder.encode(Variable(images[0]))\n    embeddings.append(out)\n    \nprint(embeddings)\n    ","e6c97b72":"embeddings[0].shape","01eff6d9":"fnc_df = pd.read_csv(fnc_filepath)\nicn_numbers_df = pd.read_csv(icn_numbers_filepath)\n\ntitles = list(fnc_df.head(0))[1:]\ntitles[:5]","db1d964a":"temp_i=[]\ntemp_j=[]\nfor t in titles:\n    i = t[t.find(\"(\")+1:t.find(\")\")]\n    j = t[t.find(\"(\", -5)+1:t.find(\")\", -1)]\n    temp_i.append(int(i))\n    temp_j.append(int(j))\ntemp_i[:5]\n\n# this is the lookup table\nicn_numbers = icn_numbers_df.to_dict()['ICN_number']\nicn_numbers.items()\nrev_icn_numbers = {v:k for k,v in icn_numbers.items()}\nprint(rev_icn_numbers)\n\ntemp_i = [rev_icn_numbers.get(item,item) for item in temp_i]\ntemp_j = [rev_icn_numbers.get(item, item) for item in temp_j]\ncorr_structure = list(zip(temp_i, temp_j))","1c967cf6":"# every subject has 1378 edges\nlen(corr_structure)","afb6059f":"# create a graph for the example subject\nG=nx.Graph()\nG.add_edges_from(corr_structure)\n\nfor i in G.nodes():\n    img = index_img(subject_niimg,i)\n    G.nodes[i]['image']=img\n    \nnx.draw(G)","9ed88ee2":"all_fmri_files = [f for f in listdir(fmri_filepath) if isfile(join(fmri_filepath, f))]","95489533":"# how many subject files do we have\nlen(all_fmri_files)","2d29d82c":"# for cnt, f in enumerate(all_mat_files):\n#     if cnt == 1:\n#         break\n#     subject_path =my_path+f\n#     subject_niimg = load_subject(subject_path, mask_niimg)\n\n# e = index_img(subject_niimg,2)\n# d = index_img(subject_niimg,2).get_fdata()\n","fc8a6130":"# re-initiate the embedding dataloader \nmy_dataset = fmri_dataset(fmri_filepath, mask_file=mask_niimg)\ndl = DataLoader(my_dataset, batch_size=1, collate_fn=my_collate_fn)","b2adb02f":"# replace nan in targets with mean values \nscores_df = pd.read_csv(scores_filepath)\nscores_df_cleaned = scores_df.fillna(scores_df.mean())\nscores_df_cleaned.to_csv(\"scores_df_cleaned\", index=False)","c4680ecf":"# create a data generator\n# 1. Get input            : input_path -> image\n# 2. Get output           : input_path -> label\n# 3. Pre-process input    : image -> pre-processing step -> image\n# 4. Get generator output : ( batch_input, batch_labels )\n\ndef get_input(input_path, mask_niimg):\n    subject_niimg = load_subject(input_path, mask_niimg)\n    return(subject_niimg)\n\ndef get_output(input_path, label_file = None ):\n    subject_id = input_path.split('\/')[-1].split('.')[0]\n    labels = label_file[label_file['Id']==int(subject_id)]\n    labels = labels.values[0][1:] # remove ID\n    return(labels)\n\ndef collate_fn(batch):\n    data=[]\n    for i in batch:\n        data.append(i)\n    data_ = torch.cat(data, dim=0)\n    return data_\n\ndef get_image_embedding(subject_niimg, mask_niimg):\n    np_data = []\n    for img in image.iter_img(subject_niimg):\n        np_data.append(img.get_fdata())\n    img_data_conv = torch.unsqueeze(torch.FloatTensor(np_data),1)\n    emb, _ = autoencoder.encode(Variable(img_data_conv))\n    return emb\n\ndef preprocess_input(input_path, image_emb, fnc_file, corr_structure):\n    subject_id = input_path.split('\/')[-1].split('.')[0]\n    correlations = fnc_file[fnc_file['Id']==int(subject_id)]\n#     edge_attr = [{'attr1': i} for i in correlations.values[0]]\n    edge_attr = [i for i in correlations.values[0]]\n    edge_attr.pop(0)\n#     edge_attrs = dict(zip(corr_structure, edge_attr))\n    \n#     G = nx.Graph()\n#     G.add_edges_from(corr_structure)\n#     nx.set_edge_attributes(G, edge_attrs)\n#     for i in G.nodes:\n#         img = image_emb[i]\n#         G.nodes[i]['image']=img\n    \n    jj = dgl.DGLGraph()\n    jj.add_nodes(53)\n    jj.add_edges(temp_i, temp_j)\n    jj.edata['attr1'] = torch.as_tensor(edge_attr)\n    jj.ndata['image'] = image_emb\n    \n    return jj\n\ndef image_generator(files, mask_file, fnc_file, label_file, batch_size = 1):\n    \n    while True:\n          # Select files (paths\/indices) for the batch\n          batch_paths  = np.random.choice(a = files, size = batch_size)\n          batch_input  = []\n          batch_output = [] \n          \n          # Read in each input, perform preprocessing and get labels\n          for input_path in batch_paths:\n              input_img = get_input(input_path, mask_niimg)\n              output = get_output(input_path, label_file)\n              emb = get_image_embedding(input_img, label_file)\n            \n              input = preprocess_input(input_path, image_emb=emb, fnc_file = fnc_df, corr_structure=corr_structure)\n              batch_input += [ input ]\n              batch_output += [ output ]\n                \n          # Return a tuple of (input, output) to feed the network\n          yield( batch_input, batch_output )\n            \n\nsubject_paths = [fmri_filepath+i for i in all_fmri_files]\nlabel_df = pd.read_csv(\".\/scores_df_cleaned\")\n\nimg_gen = image_generator(files = subject_paths, mask_file = mask_niimg, fnc_file = fnc_df, label_file = label_df, batch_size = 1)","7fe0f009":"samp=next(img_gen)\n\n# graph with embeddings as node attributes\nnx.draw(samp[0][0].to_networkx())","4779a6cc":"samp[1][0]","ebaf7c81":"from dgl.nn.pytorch import GraphConv","9085e3ea":"import torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\ndebug_dgl_model = True\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_classes):\n        super(Classifier, self).__init__()\n        self.conv1 = GraphConv(20, 16)\n        self.conv2 = GraphConv(16, 8)\n        self.classify = nn.Linear(8, 5)\n\n    def forward(self, g):\n        \n        \n        # Use node embedding vector featue to initialise hidden state.\n        h = g[0].ndata['image']\n        \n#         if debug_dgl_model == True: print(f\"feat vectors: {h.shape}\")\n        # Perform graph convolution and activation function.\n        h = F.relu(self.conv1(g[0], h))\n        \n#         if debug_dgl_model == True: print(f\"after first covn: {h.shape}\")\n        \n        h = F.relu(self.conv2(g[0], h))\n        \n#         if debug_dgl_model == True: print(f\"after second covn: {h.shape}\")\n                \n        g[0].ndata['h'] = h\n        \n#         if debug_dgl_model == True: print(f\"hidden state shape: {g[0].ndata['h'].shape}\")\n        \n        # Calculate graph representation by averaging all the node representations.\n        hg = dgl.mean_nodes(g[0], 'h')\n        \n#         if debug_dgl_model == True: print(f\"av graph nodes: {hg}\")\n            \n        lin_out = self.classify(hg)\n#         if debug_dgl_model == True: print(f\"lin out: {lin_out}\")\n        \n        return lin_out","a55c673a":"\n# Create model\nmodel = Classifier(1, 20, 5)\nloss_func = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\n\nepoch_losses = []\nfor epoch in range(5):\n    epoch_loss = 0\n    for iter, (bg, label) in enumerate(img_gen):\n        prediction = model(bg)\n#         print(prediction)\n#         print(label)\n        loss = loss_func(prediction,  torch.FloatTensor(label))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n    epoch_loss \/= (iter + 1)\n    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n    epoch_losses.append(epoch_loss)","e1729ace":"# Tying it all Together","3eb2af38":"# TReNDS Neuroimaging\n\nHuman brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.\n\n\nIn this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.\n","c98a0deb":"Each of these components can be plotted seperately, creating 53 images per subject. ","8b19a8be":"# Graph Neural Network Model","b1db6977":"Below we create a lookup table for the region mapping.","a80ae211":"## Model Training","c71a6184":"I want a model with a couple of convolution layers and a multiclass output.","9a1912df":"For one subject, we should have 53 vectors of length 20. ","073178b2":"Independent Component Analysis (ICA) is a data-driven method to analyze fMRI data. In particular, it's a method for separating a multivariate signal into additive subcomponents, much like indentifying a single conversation amoungst a crowd at a cocktail party. The output from this analysis is a set of spatial maps identifying the main regions of brain activity. Since the brain is highly inter-connected, activity in each of these regions may be more or less correlated overtime, these correlations are saved in fMRI_train. Each subject has a set of maps. Below we plot all 53 spatial maps from the example subject onto a single template. ","96942592":"Heren we construct an example of a correlation graph. The column headers in the fnc dataset list pairwise brain areas and the values give the correlation strength. So we need to process the headers and their values to construct the graph. The icn numbers help us map between the regions in the fnc dataset and the subject maps.\n","86781f19":"# Methodology\n\nThe problem appeared to be well suited to Graph Neural Networks (GNN), which are appropriate for problems with rich relational structure (Peter W. Battaglia et al. (2018)). To be able to apply GNN's to this problem the data has to be constructed as a graph. The key steps in this are: 1) to train a 3d convolutional embedding using the brain images, 2) to extract the correlation structure from fnc.csv using ICN_numbers as reference, 3) to construct a graph from the correlations and assign image embeddings as node attributes. 4) finally, to create a data generator that takes a subject graph as input and maps it to the output features (train_scores)\n","fdfd71b1":"# Code","02591298":"So far I've created a model for embeddings, and I have demonstrated how to create a graph from correlations between the ICA maps. Now I want to build a dataloader which combines these into a single data object for all subjects. Furthermore, we need to associate these graphs with their target vector of subject scores.","bb8481b6":"## Model","4d60e847":"## Embeddings  \n\nNow we have a trained model, we can use just the embedding method of our AutoEncoder model to calculate embeddings for images. Here we test if we get the expected ouput.","8fc39af4":"# 3D Convolutional Autoencoder\n\nThe first task is to train an embedding model so that the images can be represented by a more convient embedding vector for the downstream GNN model. ","960c94a8":"## Constructing Correlation Graps ","95e0db28":"Here the batch comes in multiples of 53 images, that is the number of images per subject. And each image has dimensions (pixels) of width: 53, height: 63, depth: 52. ","ac8ffee7":"For illistration purposes for each subject I can add an fmri image to each node and the fnc correlation as edge weight. Later we will replace the image with the image embeddings. Also for consideration are the subject loadings, which have a many-to-one relationship to the fmri images. If I want to include these later I will need to create a map manually as best I can.","dc44e470":"### Section in Progress","f48c39ac":"## Model Data loader\n\nPytorch requires us to rewrite __len__ and __getitem__ methods to create a dataloader. In addition, due to the specifics of this dataset, I have also defined a custom collate function to priduce the correct output format. "}}