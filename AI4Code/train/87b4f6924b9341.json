{"cell_type":{"e59b8175":"code","adda8f44":"code","902d47b4":"code","8cc9eec0":"code","b15683fc":"code","a41126e3":"code","f6dabc32":"code","43aef816":"code","12e554e9":"code","8d124e22":"code","81be5cae":"code","3c8ec1e5":"code","d650b87f":"code","561bca13":"code","a5f74e30":"code","9ab768ee":"code","bbd91dce":"code","f74d5afc":"code","f71d4894":"code","93c1caf9":"code","19d7aade":"code","6324442e":"code","7b68f8bd":"code","7b6a79af":"code","ebc2fae5":"code","7422a196":"code","cd207aa7":"code","939f994c":"code","a6f04305":"code","aedc0744":"code","eb842acc":"code","473da6a3":"code","e120b293":"code","151c542f":"code","93c36613":"code","5d2d17ae":"code","4695a34d":"code","eb37bde2":"code","9cdf622d":"code","7bef8aaf":"code","835ac96d":"code","7f2c0ac3":"code","07662a69":"code","13e49e04":"code","72ae9343":"code","ce8e687e":"code","2c62cd58":"code","b5b19eb5":"code","a0a85dd1":"code","07d96ff1":"code","219f68ff":"markdown"},"source":{"e59b8175":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","adda8f44":"train = pd.read_csv('\/kaggle\/input\/tcdml1920-income-ind\/tcd ml 2019-20 income prediction training (with labels).csv') \ntest = pd.read_csv('\/kaggle\/input\/tcdml1920-income-ind\/tcd ml 2019-20 income prediction test (without labels).csv')\n\n# train = train.drop(['Wears Glasses', 'Hair Color', 'Body Height [cm]'], axis=1)\n# test = test.drop(['Wears Glasses', 'Hair Color', 'Body Height [cm]'], axis=1)","902d47b4":"train.head()","8cc9eec0":"# # when BACHELOR income is more than 5000000 \n# train = train[train['Income in EUR'] < 5000000 ]\n\n# # when AGE is 103 and income is more than 3000000(outlier in plot)\n# train = train[train['Instance'] != 54704 ]\n# # when AGE is more than 112, no more relevant data\n# train = train[train['Age'] < 112]","b15683fc":"all_data = pd.concat((train.loc[:,'Year of Record':'University Degree'],\n                      test.loc[:,'Year of Record':'University Degree']))\nall_data.columns","a41126e3":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nincome = pd.DataFrame({\"Income\":train[\"Income in EUR\"], \"log(Income + 1)\":np.log1p(train[\"Income in EUR\"])})\nincome.hist()","f6dabc32":"#log transform the target:\ntrain[\"Income in EUR\"] = np.log1p(train[\"Income in EUR\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","43aef816":"train['Gender'] = train['Gender'].fillna('UNKNOWN')\ntrain['Gender'] = train['Gender'].replace('0', 'UNKNOWN')\ntrain['Gender'] = train['Gender'].replace('unknown', 'UNKNOWN')\n\ntest['Gender'] = test['Gender'].fillna('UNKNOWN')\ntest['Gender'] = test['Gender'].replace('0', 'UNKNOWN')\ntest['Gender'] = test['Gender'].replace('unknown', 'UNKNOWN')\n\n\ntrain['University Degree'] = train['University Degree'].replace('0', 'Unknown University')\ntrain['University Degree'] = train['University Degree'].fillna('Unknown University')\n\ntest['University Degree'] = test['University Degree'].replace('0', 'Unknown University')\ntest['University Degree'] = test['University Degree'].fillna('Unknown University')\n\n\ntrain['Profession'] = train['Profession'].fillna('Unknown Profession')\ntest['Profession'] = test['Profession'].fillna('Unknown Profession')\n\ntrain['Income in EUR'] = train['Income in EUR'].fillna(int(train['Income in EUR'].mean()))\n\nall_data = pd.get_dummies(all_data)\n\n\n# # Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['University Degree'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"University Degree\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()\n","12e554e9":"len(train)","8d124e22":"\n# dum = train[train['Income in EUR'] < 5000000]\n# len(dum)","81be5cae":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Gender'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Gender\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()\n","3c8ec1e5":"# dum = train[train['Income in EUR'] > 3000000]\n# dum","d650b87f":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Age'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Age\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()\n","561bca13":"# dum = train[train['Age'] > 100]\n# dum\n# # Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(dum['Age'], dum['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Age more than 100\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()\n","a5f74e30":"# len(dum)","9ab768ee":"# du = train[train['Instance'] != 54704]\n# du","bbd91dce":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Gender'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Gender\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()","f74d5afc":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Year of Record'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Year of Record\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()","f71d4894":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Profession'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Profession\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()","93c1caf9":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\n# plt.scatter(train['Country'], train['Income in EUR'], c = \"blue\", marker = \"s\")\n# plt.title(\"Looking for outliers\")\n# plt.xlabel(\"Country\")\n# plt.ylabel(\"Income in EUR\")\n# plt.show()","19d7aade":"# print(\"Find most important features relative to target\")\n# train.columns\n# corr = all_data.corr()\n# corr.sort_values([\"Income in EUR\"], ascending = False, inplace = True)\n# print(corr[\"Income in EUR\"])","6324442e":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","7b68f8bd":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\n\nX_test = all_data[train.shape[0]:]\ny = train['Income in EUR']\nX_train.isnull().sum().sum()\ny.isnull().sum()","7b6a79af":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","ebc2fae5":"model_ridge = Ridge()","7422a196":"# alphas = [0.05, 0.1, 0.3, 1, 2, 2.5, 3, 3.5, 4, 5]\n# cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n#             for alpha in alphas]\n\n# cv_ridge = pd.Series(cv_ridge, index = alphas)\n# cv_ridge.plot(title = \"Validation - Just Do It\")\n# plt.xlabel(\"alpha\")\n# plt.ylabel(\"rmse\")","cd207aa7":"# cv_ridge.min()","939f994c":"# model_cv_ridge = RidgeCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)","a6f04305":"# model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)","aedc0744":"# rmse_cv(model_lasso).mean()","eb842acc":"# coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n","473da6a3":"# print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n","e120b293":"# imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n","151c542f":"# matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n# imp_coef.plot(kind = \"barh\")\n# plt.title(\"Coefficients in the Lasso Model\")\n","93c36613":"# #let's look at the residuals as well:\n# matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\n# preds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n# preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n# preds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","5d2d17ae":"# from sklearn.linear_model import LinearRegression\n\n# model_linear = LinearRegression().fit(X_train, y)\n","4695a34d":"# from sklearn.linear_model import SGDRegressor\n# from sklearn.metrics import mean_squared_error, r2_score\n# sgd_model = SGDRegressor().fit(X_train, y)","eb37bde2":"from keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers.core import Dense\nfrom keras.models import Model\nfrom keras.layers.core import Activation\nfrom keras.layers import Dropout\n\ndef create_model(dim, regress=False):\n    model = Sequential()\n    model.add(Dropout(0.2,input_shape=(dim,)))\n    model.add(Dense(64, input_dim=dim, activation=\"relu\"))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dense(24, activation=\"relu\"))\n    model.add(Dense(24, activation=\"relu\"))\n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dense(8, activation=\"relu\"))\n    model.add(Dense(1, activation=\"linear\"))\n    return model","9cdf622d":"model = create_model(X_train.shape[1], regress=True)\nopt = Adam(lr=1e-2, decay=1e-3 \/ 200)\nmodel.compile(loss=\"mean_squared_error\", optimizer=opt)\n\n# train the model\nprint(\"training model...\")\nmodel.fit(X_train, y, epochs=30, batch_size=100)","7bef8aaf":"# import xgboost as xgb","835ac96d":"# dtrain = xgb.DMatrix(X_train, label = y)\n# dtest = xgb.DMatrix(X_test)\n\n# params = {\"max_depth\":2, \"eta\":0.1}\n# model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","7f2c0ac3":"# model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","07662a69":"# model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n# model_xgb.fit(X_train, y)","13e49e04":"# xgb_preds = np.expm1(model_xgb.predict(X_test))\n# cv_ridge_preds = np.expm1(model_cv_ridge.predict(X_test))\n# lasso_preds = np.expm1(model_lasso.predict(X_test))\n# linear_preds = np.expm1(model_linear.predict(X_test))\n# sgd_preds = np.expm1(sgd_model.predict(X_test))\ncnn_preds = np.expm1(model.predict(X_test))\n","72ae9343":"# predictions = pd.DataFrame({\"cvRidge\":cv_ridge_preds, \"lasso\":lasso_preds})\n# predictions.plot(x = \"cvRidge\", y = \"lasso\", kind = \"scatter\")","ce8e687e":"# preds = 0.7*lasso_preds + 0.3*cv_ridge_preds\n# preds = 1.0*cv_ridge_preds\n# preds = 1.0*linear_preds\n# preds = 1.0*sgd_preds\npreds = 1.0*cnn_preds","2c62cd58":"preds = preds[:,0]","b5b19eb5":"preds.shape","a0a85dd1":"solution = pd.DataFrame({\"Instance\":test.Instance, \"Income\":preds})\nsolution\n","07d96ff1":"solution.to_csv(\"tcd ml 2019-20 income prediction submission file.csv\", index = False)","219f68ff":"ridge cv comment"}}