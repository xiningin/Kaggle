{"cell_type":{"67e1ece4":"code","b2e293a2":"code","bad5adde":"code","24adc2a0":"code","a81886ec":"code","acdc032a":"code","61107ac7":"code","2a244cee":"code","bda529d9":"code","5bff6e04":"code","119b4669":"code","dc33818d":"code","63919907":"code","b0e6a6e7":"code","1f7be041":"code","676686c2":"code","d8787fab":"code","dc2c135b":"code","33d20321":"code","da32a84c":"code","3fc8e37b":"code","a65884bd":"code","9ec06367":"code","c0871476":"code","0b3e884a":"code","2b5f31f9":"code","ec2677fe":"code","7fa41a75":"code","a133d8d8":"code","3ea95ab1":"code","ca5fa784":"code","6620204e":"code","56c7b0cc":"code","655a3dfc":"code","986a0120":"code","f30bdf91":"code","02f0de6f":"code","029692ce":"markdown","8ca7a1bd":"markdown","f3394822":"markdown","1926c3b5":"markdown","474591f5":"markdown","b18f33b6":"markdown","2a3bf7e7":"markdown","4b005fda":"markdown","4772c3aa":"markdown","93c3a808":"markdown","fcd406d9":"markdown","c31f57ea":"markdown","a174ebcf":"markdown","44f3e95a":"markdown","d0d102fe":"markdown"},"source":{"67e1ece4":"# importing libraries\nimport os, time\nos.environ['PYTHONHASHSEED']=str(1)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# %load_ext tensorboard\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport random\nseed = 12","b2e293a2":"import tensorflow as tf\nfrom tensorflow import keras as k\nprint(tf.__version__)\nfrom keras import backend as K\nimport os, shutil, re, string\nimport matplotlib.pyplot as plt\nimport spacy\nseed=100","bad5adde":"from skimage.transform import resize\nfrom skimage import img_as_ubyte\nfrom imageio import imread\nimport datetime\n\ndef set_seed():\n    global seed\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed()\n\n# use_tpu = True #@param {type:\"boolean\"}\n\n# if use_tpu:\n#     assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n\n# if 'COLAB_TPU_ADDR' in os.environ:\n#   TF_MASTER = 'grpc:\/\/{}'.format(os.environ['COLAB_TPU_ADDR'])\n#   print('Found TPU at: {}'.format(TF_MASTER))\n# else:\n#   TF_MASTER=''\n\n  \n## Checking the GPU configuration\n!nvidia-smi","24adc2a0":"import json\nfolder_main = '\/kaggle\/input\/food41'\nfolder_images = os.path.join(folder_main, 'images')\ntrainjson_fp = folder_main+'\/meta\/meta\/train.json'\ntestjson_fp = folder_main+'\/meta\/meta\/test.json'\nwith open(trainjson_fp, 'r') as f:\n    trainjson = json.load(f)\nwith open(testjson_fp, 'r') as f:\n    testjson = json.load(f)\nlabels = list(trainjson.keys())\nlabels[:5]","a81886ec":"# helper function\n\ndef readImg(img):\n    global folder_images\n    return imread(os.path.join(folder_images, img+'.jpg'))\n\ndef showImg(img):\n    plt.figure(figsize=(5,5))\n    plt.imshow(img)\n    plt.show()","acdc032a":"# create training data\nset_seed()\ntrain_samples = list(np.random.permutation(list(pd.core.common.flatten(trainjson.values()))))\nprint(\"Total number of samples for train\",len(train_samples))\nprint(\"Some samples are\",train_samples[:5])\nsample_img = readImg(train_samples[0])\nprint(\"Sample Image Shape\",sample_img.shape)\nshowImg(sample_img)","61107ac7":"# create testing data\nset_seed()\ntest_samples = list(np.random.permutation(list(pd.core.common.flatten(testjson.values()))))\nprint(\"Total number of samples for test\",len(test_samples))\nprint(\"Some samples are\",test_samples[:5])\nsample_img = readImg(test_samples[0])\nprint(\"Sample Image Shape\",sample_img.shape)\nshowImg(sample_img)","2a244cee":"# # get image with min size\n# shapeX, shapeY = [1000, 1000], [1000, 1000]\n# for i, img in enumerate(train_samples):\n#     print(\"\\rReading\",i+1,\"of total\",len(train_samples),\"images\", end='')\n#     imgShape = readImg(img).shape\n#     if imgShape[0] < shapeX[0]:\n#         shapeX = imgShape[:2]\n#     if imgShape[1] < shapeY[1]:\n#         shapeY = imgShape[:2]\n# print(\"\\nLowest training image shape - shapeX:\", shapeX,\", shapeY:\",shapeY)","bda529d9":"# # remove images with one channel\n# train_sample_one = []\n# for i, x in enumerate(train_samples):\n#     print(\"\\rReading\",i+1,\"of total\",len(train_samples),\"images\", end='')\n#     imgShape = readImg(x)\n#     if len(imgShape.shape) != 3:\n#         train_sample_one.append(x)\n# print(len(train_sample_one),\"images are of single channel\")\nshowImg(readImg('lasagna\/3787908'))","5bff6e04":"train_sample_one = ['lasagna\/3787908', 'bread_pudding\/1375816', 'steak\/1340977']\ntrain_samples = [x for x in train_samples if x not in train_sample_one]\nprint(\"Remaining samples for train are\",len(train_samples))","119b4669":"# # remove images with one channel\n# test_sample_one = []\n# for i, x in enumerate(test_samples):\n#     print(\"\\rReading\",i+1,\"of total\",len(test_samples),\"images\", end='')\n#     imgShape = readImg(x).shape\n#     if len(imgShape) != 3:\n#       test_sample_one.append(x)\n# print(len(test_sample_one),\"images are of single channel\")","dc33818d":"# create training data\ntrain_samples_subset_list = []\nfor x in trainjson.keys():\n    train_samples_subset_list.extend(trainjson[x][:50])\nset_seed()\ntrain_samples_subset = list(np.random.permutation(train_samples_subset_list))\nprint(\"Total number of samples for train\",len(train_samples_subset))\nprint(\"Some samples are\",train_samples_subset[:5])\nsample_img = readImg(train_samples_subset[0])\nprint(\"Sample Image Shape\",sample_img.shape)\nshowImg(sample_img)","63919907":"# remove single channel image\ntrain_sample_one = ['lasagna\/3787908', 'bread_pudding\/1375816', 'steak\/1340977']\ntrain_samples_subset = [x for x in train_samples_subset if x not in train_sample_one]\nprint(\"Remaining samples for train are\",len(train_samples_subset))","b0e6a6e7":"# create testing data\ntest_samples_subset_list = []\nfor x in testjson.keys():\n    test_samples_subset_list.extend(testjson[x][:20])\nset_seed()\ntest_samples_subset = list(np.random.permutation(test_samples_subset_list))\nprint(\"Total number of samples for test\",len(test_samples_subset))\nprint(\"Some samples are\",test_samples_subset[:5])\nsample_img = readImg(test_samples_subset[0])\nprint(\"Sample Image Shape\",sample_img.shape)\nshowImg(sample_img)","1f7be041":"# map labels to index\nlabel_index = dict()\nindex_label = dict()\nfor i, x in enumerate(labels):\n    label_index[x] = i\n    index_label[i] = x\nprint(label_index)\nprint(index_label)","676686c2":"# hyper-parameters\n\nimg_tensor = [128, 128, 3]","d8787fab":"def cropResize(image, y, z):\n    return img_as_ubyte(resize(image, (y,z)))\n\ndef normalizeImage(image):\n    # applying normalization\n    return image\/255.0\n\ndef preprocessImage(image, y, z):\n    return normalizeImage(cropResize(image, y, z))","dc2c135b":"# display sample preprocessed image \nsample_img_r = sample_img[:, :, 0]\nsample_img_r_pre = preprocessImage(sample_img_r, 128, 128)\nshowImg(sample_img_r_pre)","33d20321":"# flip\ndef flipImage(img):\n    return np.flip(img)\n","da32a84c":"# display sample preprocessed image \nsample_img_r = sample_img[:, :, 0]\nsample_img_r_pre = preprocessImage(flipImage(sample_img_r), 128, 128)\nshowImg(sample_img_r_pre)","3fc8e37b":"def getBatchData(t, batch, batch_size, flip):\n    global img_tensor, labels\n    [h, w, c] = [img_tensor[0], img_tensor[1], img_tensor[2]]\n    batch_data = np.zeros((batch_size,h,w,c)) # generating null image representations\n    batch_labels = np.zeros((batch_size, len(labels))) # batch_labels is the one hot representation of output\n    if flip:\n        batch_data_flip = np.zeros((batch_size,h,w,c))\n        batch_labels_flip = np.zeros((batch_size, len(labels)))\n    # batch_labels = []\n    for idx in range(batch_size): # iterating over the batch_size\n        imgPath = t[idx + (batch*batch_size)]\n        imgLabel = imgPath.strip().split('\/')[0]\n        image = readImg(imgPath)\n\n        # separate preprocessImage function is defined for cropping, resizing and normalizing images\n        batch_data[idx,:,:,0] = preprocessImage(image[:, :, 0], h, w)\n        batch_data[idx,:,:,1] = preprocessImage(image[:, :, 1], h, w)\n        batch_data[idx,:,:,2] = preprocessImage(image[:, :, 2], h, w)\n\n        batch_labels[idx, label_index[imgLabel]] = 1\n\n        if flip:\n            batch_data_flip[idx,:,:,0] = preprocessImage(flipImage(image[:, :, 0]), h, w)\n            batch_data_flip[idx,:,:,1] = preprocessImage(flipImage(image[:, :, 1]), h, w)\n            batch_data_flip[idx,:,:,2] = preprocessImage(flipImage(image[:, :, 2]), h, w)\n\n            batch_labels_flip[idx, label_index[imgLabel]] = 1\n    if flip:\n        batch_data = np.concatenate((batch_data, batch_data_flip))\n        batch_labels = np.concatenate((batch_labels, batch_labels_flip))\n\n    return batch_data, batch_labels","a65884bd":"def generator(folder_list, batch_size, flip=False):\n    print('\\nLoading from', len(folder_list), 'images; batch size =', batch_size)\n    while True:\n        num_batches = int(len(folder_list)\/batch_size)\n        for batch in range(num_batches): # we iterate over the number of batches\n#             print(\"\\rReading batch\",str(batch+1),\"of total\",str(num_batches), end='')\n            yield getBatchData(folder_list, batch, batch_size, flip)\n        \n        # checking if any remaining batches are there or not\n        if len(folder_list)%batch_size != 0:\n            # updated the batch size and yield\n            batch_size_rem = len(folder_list)%batch_size\n            yield getBatchData(folder_list, batch, batch_size_rem, flip)","9ec06367":"# check complete batch shape\nsample_generator = generator(train_samples_subset, batch_size=16, flip=True)\nsample_batch_data, sample_batch_labels = next(sample_generator)\nprint(\"\\nSample Train batch data shape\", sample_batch_data.shape)\nprint(\"Train Batch labels\", sample_batch_labels[0])\nprint(\"Sample train batch image\")\nshowImg(sample_batch_data[0])\n\n# validation batch sample\nsample_test_generator = generator(test_samples_subset, batch_size=8)\nsample_test_batch_data, sample_test_batch_labels = next(sample_test_generator)\nprint(\"\\nSample Test batch data shape\", sample_test_batch_data.shape)\nprint(\"Test Batch labels\", sample_test_batch_labels[0])\nprint(\"Sample test batch image\")\nshowImg(sample_test_batch_data[-2])","c0871476":"def plotModelHistory(h):\n    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n    ax[0].plot(h.history['loss'])   \n    ax[0].plot(h.history['val_loss'])\n    ax[0].legend(['loss','val_loss'])\n    ax[0].title.set_text(\"Train loss vs Validation loss\")\n\n    ax[1].plot(h.history['categorical_accuracy'])   \n    ax[1].plot(h.history['val_categorical_accuracy'])\n    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n\n    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))","0b3e884a":"logPath = '.\/logs\/'\nif not os.path.exists(logPath):\n    os.mkdir(logPath)\n# %load_ext tensorboard\n# %tensorboard --logdir logPath\n\n# selected set for training and prediction\ntrain_set = train_samples\ntest_set = test_samples\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        ACCURACY_THRESHOLD = 0.90\n        if(logs.get('val_categorical_accuracy') > ACCURACY_THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached %2.2f%% accuracy!\" %(ACCURACY_THRESHOLD*100))   \n            self.model.stop_training = True\n\n\ndef trainModel(model, epochs, optimizer, vb=1, modelName='model'):\n    global train_set, test_set\n    !rm -rf .\/logs\/\n    logs = logPath + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    batch_size = 128\n    bestModelPath = '.\/'+modelName+'_model.hdf5'\n    callback = myCallback()\n    cbs = [callback,\n           k.callbacks.TensorBoard(log_dir=logs, histogram_freq=1),\n           k.callbacks.ReduceLROnPlateau(monitor = 'val_categorical_accuracy',patience = 5, verbose = 1),\n           k.callbacks.EarlyStopping(monitor = 'val_categorical_accuracy',patience = 5,verbose = 1,restore_best_weights = True),\n           k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)]\n\n    # setup generators for training\n    \n    train_generator = generator(train_set, batch_size, flip=True)\n    test_generator = generator(test_set, batch_size, flip=True)\n    \n    num_train_sequences = len(train_set)\n    print('# training sequences =', num_train_sequences)\n    num_test_sequences = len(test_set)\n    print('# test sequences =', num_test_sequences)\n\n    if (num_train_sequences%batch_size) == 0:\n        steps_per_epoch = int(num_train_sequences\/batch_size)\n    else:\n        steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\n\n    if (num_test_sequences%batch_size) == 0:\n        validation_steps = int(num_test_sequences\/batch_size)\n    else:\n        validation_steps = (num_test_sequences\/\/batch_size) + 1\n\n    model.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                  metrics=[k.metrics.CategoricalAccuracy(), k.metrics.Precision(), k.metrics.Recall()]\n    )\n    return model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n                               validation_data=test_generator, validation_steps=validation_steps,\n                               verbose=vb, batch_size=batch_size, callbacks=cbs)","2b5f31f9":"def trainModelSingle(model, epochs, optimizer, vb=1, modelName='model'):\n    global train_set, test_set\n#     logs = logPath + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    batch_size = 128\n    bestModelPath = '.\/'+modelName+'_model.hdf5'\n    callback = myCallback()\n    cbs = [callback,\n#            k.callbacks.TensorBoard(log_dir=logs, histogram_freq=1),\n           k.callbacks.ReduceLROnPlateau(monitor = 'val_categorical_accuracy',patience = 5, verbose = 1),\n           k.callbacks.EarlyStopping(monitor = 'val_categorical_accuracy',patience = 5,verbose = 1,restore_best_weights = True),\n           k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)]\n\n    # setup generators for training\n    train_generator = generator(train_set, batch_size, flip=True)\n    test_generator = generator(test_set, batch_size, flip=True)\n\n    if (num_train_sequences%batch_size) == 0:\n        steps_per_epoch = int(num_train_sequences\/batch_size)\n    else:\n        steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\n\n    if (num_test_sequences%batch_size) == 0:\n        validation_steps = int(num_test_sequences\/batch_size)\n    else:\n        validation_steps = (num_test_sequences\/\/batch_size) + 1\n\n    return model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n                               validation_data=test_generator, validation_steps=validation_steps,\n                               verbose=vb, batch_size=batch_size, callbacks=cbs)","ec2677fe":"# evaluate model with time\ndef evaluate(model):\n    global test_set\n    batch_size = 128\n    num_train_sequences = len(test_set)\n    steps_per_epoch = 0\n    if (num_train_sequences%batch_size) == 0:\n        steps_per_epoch = int(num_train_sequences\/batch_size)\n    else:\n        steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\n\n    test_generator = generator(test_set, batch_size=batch_size)\n    t1 = time.time()\n    model = k.models.load_model(model)\n    eval_results = model.evaluate_generator(test_generator, steps=steps_per_epoch)\n    t2 = time.time()\n    print(f'\\nAccuracy: {eval_results[1]}, Loss: {eval_results[0]}')\n    print(f'Total Prediction Time: {t2-t1}')\n    print(f'FPS Prediction Time: {len(test_set)\/(t2-t1)}')\n    print(f'Prediction Time per Image: {(t2-t1)\/len(test_samples_subset)}')","7fa41a75":"mobilenet = k.applications.mobilenet_v2.MobileNetV2(weights='imagenet', input_shape=img_tensor, include_top=False)\nmobilenet.trainable = False\n\nmodel = k.models.Sequential([\n                             mobilenet,\n                             tf.keras.layers.GlobalAveragePooling2D(),\n                             k.layers.Dropout(0.2),\n                             k.layers.Dense(256, activation='relu'),\n                             k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.1),\n#                              k.layers.Dense(128, activation='relu'),\n#                              k.layers.BatchNormalization(),\n#                              k.layers.Dropout(0.1),\n                             k.layers.Dense(len(index_label), activation='softmax')\n])\nprint(model.summary())","a133d8d8":"trainModel(model, 10, 'adam', modelName='mobilenet')","3ea95ab1":"# trainModelSingle(model, 10, 'adam', modelName='mobilenet')","ca5fa784":"evaluate('\/kaggle\/working\/mobilenet_model.hdf5')","6620204e":"resnet = k.applications.ResNet50(weights='imagenet', input_shape=img_tensor, include_top=False)\nresnet.trainable = False\n\nmodel_2 = k.models.Sequential([\n                             resnet,\n                             tf.keras.layers.GlobalAveragePooling2D(),\n                             k.layers.Dropout(0.2),\n                             k.layers.Dense(256, activation='relu'),\n                             k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.1),\n                             k.layers.Dense(128, activation='relu'),\n                             k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.1),\n                             k.layers.Dense(len(index_label), activation='softmax')\n])\nprint(model_2.summary())","56c7b0cc":"trainModel(model_2, 10, 'adam', modelName='resnet')","655a3dfc":"evaluate('.\/resnet_model.hdf5')","986a0120":"vgg16 = k.applications.VGG16(weights='imagenet', input_shape=img_tensor, include_top=False)\nvgg16.trainable = False\n\nmodel_3 = k.models.Sequential([\n                             vgg16,\n                             tf.keras.layers.GlobalAveragePooling2D(),\n                             k.layers.Dropout(0.2),\n                             k.layers.Dense(512, activation='relu'),\n                             k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.1),\n                             k.layers.Dense(256, activation='relu'),\n                             k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.1),\n                             k.layers.Dense(len(index_label), activation='softmax')\n])\nprint(model_3.summary())","f30bdf91":"trainModel(model_3, 10, 'adam', modelName='vgg16')","02f0de6f":"evaluate('.\/vgg16_model.hdf5')","029692ce":"['lasagna\/3787908', 'bread_pudding\/1375816', 'steak\/1340977'] are the single channel images in train set.\n\nNo single channel images are there in test set.","8ca7a1bd":"## Train heavy model - VGG16","f3394822":"### Helper Functions","1926c3b5":"### Pick Sample Dataset\nA small part of the dataset to build the initial models as the data is really huge and will take days to properly train all the models on GPU.","474591f5":"## Train Medium model - ResNet50","b18f33b6":"# Generator function for all models","2a3bf7e7":"# Food Classification\nThis involves a food classification dataset which has 101 classes. You need to analyze and preprocess the dataset as well as build deep learning models for performing food classification. \n<br>\nThree models are trained for this task, mainly light, medium, and heavy model. <br>\n- Light model - MobileNetV2 <br>\n- Medium model - ResNet50 <br>\n- Heavy model - VGG16 <br>\n<br>","4b005fda":"# Training different models\n## Objectives\n1. Obtain good accuracy in all the models trained. \n2. Implement any techniques for traning such as transfer learning, knowledge transfer, etc. \n3. The models should not overfit the training dataset. \n4. Measure the performance in terms of accuracy and speed of each model. ","4772c3aa":"# Analyze the dataset\n## Objectives\n1. Extract the dataset. \n2. Re-arrange dataset into training and testing folders.\n3. List number of samples in training and testing folders. \n4. Plot sample images from training and testing datasets. ","93c3a808":"**Loading json files**","fcd406d9":"# Pre-process Images\n## Objectives\n1. Implement preprocessing codes for each model. \n2. Augment the dataset. \n3. Preview the preprocessed dataset. ","c31f57ea":"### Setup Labels Encoding","a174ebcf":"Found lowest image dimension as (122, 512) and (512, 193).","44f3e95a":"These models will take a very long time to build so no prediction output is available, you can probably try building them.","d0d102fe":"## Train Light model - MobileNetV2"}}