{"cell_type":{"acbfaa67":"code","f91bc547":"code","37b6ef05":"code","4b57f702":"code","b45bda5b":"code","7370cb6d":"code","a8bd5b04":"code","4c254b46":"code","bb586286":"code","683c1035":"code","81ee0c0f":"code","2dc4efc2":"code","40431334":"code","1613a963":"code","63e8aac3":"code","585b121f":"code","de78640e":"code","936532ac":"code","9c17770a":"code","5115d555":"code","59b716fe":"code","62032ef1":"code","d58cecb7":"code","87cd0232":"code","8e44cdb6":"code","cb4ded55":"code","09ac242b":"code","0fedb6e4":"code","15f0cf17":"code","14f8b701":"code","9b6fde01":"code","c674147d":"code","90d74a83":"code","b196bd9c":"code","8ee5516c":"code","591b18fe":"code","6e4dc7fd":"code","a21eb7f4":"code","80c455dd":"code","5e52a265":"code","2525dac0":"code","ec80e149":"code","101140f4":"code","e732275a":"code","72bd678f":"code","8a345472":"code","9341fe67":"code","cc192d55":"code","14bc0251":"code","3b838ee2":"code","f567cb24":"code","85c4062d":"code","b8e3080a":"code","38784799":"code","52bfa60c":"markdown","cb35887b":"markdown","bc0d8717":"markdown","b1386630":"markdown","d88d12f8":"markdown","ecd129d9":"markdown","3966b9a2":"markdown","aba1e2af":"markdown","696d1912":"markdown","8ee9aa05":"markdown"},"source":{"acbfaa67":"# Data Preprocessing\nimport pandas as pd \nimport numpy as np \n\n# Data Visualization \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# ML Models \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Miscellanous \nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline ","f91bc547":"# Loading Data \ntrain = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ntest = pd.read_csv('..\/input\/mobile-price-classification\/test.csv')","37b6ef05":"train.head()","4b57f702":"test.head()","b45bda5b":"# Let's make a copy of train and test data so that even if we have to make any changes in these datasets we would not lose the original datasets.\n\ntrain_original=train.copy()\ntest_original =test.copy()","7370cb6d":"train.columns","a8bd5b04":"test.drop(['id'],axis=1, inplace=True)","4c254b46":"test.columns","bb586286":"# Print data types for each variable \ntrain.dtypes","683c1035":"# shape of the dataset.\ntrain.shape, test.shape","81ee0c0f":"# unique values in label\ntrain['price_range'].nunique()    ","2dc4efc2":"#frequency table\ntrain['price_range'].value_counts()","40431334":"# Normalize can be set to True to print proportions instead of number ( percentage distribution )\ntrain['price_range'].value_counts(normalize=True)    ","1613a963":"train['price_range'].value_counts().plot.bar()","63e8aac3":"plt.figure(1)\nplt.subplot(121)\ntrain['battery_power'].value_counts(normalize=True).plot.bar(figsize=(30,20), title='battery_power');","585b121f":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['battery_power']);\nplt.subplot(122)\ntrain['battery_power'].plot.box(figsize=(16, 5))\nplt.show()","de78640e":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['four_g']);\nplt.subplot(122)\ntrain['four_g'].plot.box(figsize=(16, 5))\nplt.show()","936532ac":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['int_memory']);\nplt.subplot(122)\ntrain['int_memory'].plot.box(figsize=(16, 5))\nplt.show()","9c17770a":"blue = pd.crosstab(train['blue'], train['price_range'])\ndual_sim = pd.crosstab(train['dual_sim'], train['price_range'])\nfc = pd.crosstab(train['fc'], train['price_range'])\nfour_g = pd.crosstab(train['four_g'], train['price_range'])\nn_cores = pd.crosstab(train['n_cores'], train['price_range'])\nsc_h = pd.crosstab(train['sc_h'], train['price_range'])\nthree_g = pd.crosstab(train['three_g'], train['price_range'])\nwifi = pd.crosstab(train['wifi'], train['price_range'])\ntouch_screen = pd.crosstab(train['touch_screen'], train['price_range'])\n\nblue.div(blue.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\ndual_sim.div(dual_sim.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nfc.div(fc.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nfour_g.div(four_g.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nn_cores.div(n_cores.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nsc_h.div(sc_h.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nthree_g.div(three_g.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\nwifi.div(wifi.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\ntouch_screen.div(touch_screen.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(4, 4))\n\n","5115d555":"train.groupby('price_range')['ram'].mean().plot.bar()","59b716fe":"train.groupby('price_range')['battery_power'].mean().plot.bar()","62032ef1":"train.groupby('price_range')['int_memory'].mean().plot.bar()","d58cecb7":"# heat map to visualize the correlation between all the numerical variables.\n\nmatrix = train.corr()\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(matrix, vmax=.8, square=True, cmap='BuPu')","87cd0232":"train.isnull().sum()","8e44cdb6":"test.isnull().sum()","cb4ded55":"X = train.drop('price_range', 1)\ny=train.price_range","09ac242b":"class0=train_original[train_original['price_range']==0]\nclass0.describe()","0fedb6e4":"class1=train_original[train_original['price_range']==1]\nclass1.describe()","15f0cf17":"class2=train_original[train_original['price_range']==2]\nclass2.describe()","14f8b701":"class3=train_original[train_original['price_range']==3]\nclass3.describe()","9b6fde01":"class0.shape,class1.shape,class2.shape,class3.shape","c674147d":"i = 1\nl=0\nkf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n\nfor train_index, test_index in kf.split(X, y):\n    print('\\n{} of kfold {}'.format(i, kf.n_splits))\n    xtr, xvl = X.loc[train_index], X.loc[test_index]\n    ytr, yvl = y[train_index], y[test_index]\n    \n    model = tree.DecisionTreeClassifier(random_state=1)\n    model.fit(xtr, ytr)\n    pred_test = model.predict(xvl)\n    score=accuracy_score(yvl, pred_test)\n    print('accuracy_score', score)\n    i+=1\n    l+=score\npred_test = model.predict(test)\npred= model.predict_proba(xvl)[:, 1] \nprint('\\n accuracy :', l\/5)","90d74a83":"\nfpr = {}\ntpr = {}\nthresh ={}\nn_classes = 4\nfor i in range(n_classes):\n    fpr[i], tpr[i] , thresh[i]= roc_curve(yvl,  pred, pos_label=i) \n\nplt.figure(figsize=(12,8)) \n\nplt.plot(fpr[0],tpr[0],label=\"Class 0\") \nplt.plot(fpr[1],tpr[1],label=\"Class 1\") \nplt.plot(fpr[2],tpr[2],label=\"Class 2\") \nplt.plot(fpr[3],tpr[3],label=\"Class 3\") \n\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","b196bd9c":"i =1 \nl=0\nkf = StratifiedKFold(n_splits=5, random_state = 1, shuffle=True)\nfor train_index, test_index in kf.split(X, y):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]   \n    \n    model = RandomForestClassifier(random_state=1, max_depth=10)\n    model.fit(xtr, ytr)\n    pred_test = model.predict(xvl)\n    score=accuracy_score(yvl, pred_test)\n    print('accuracy_score',score)     \n    i+=1 \n    l += score\n\npred_test = model.predict(test)\npred = model.predict_proba(xvl)[:, 1]\nprint('\\n accuracy :', l\/5)","8ee5516c":"\nfpr = {}\ntpr = {}\nthresh ={}\nn_classes = 4\nfor i in range(n_classes):\n    fpr[i], tpr[i] , thresh[i]= roc_curve(yvl,  pred, pos_label=i) \n\nplt.figure(figsize=(12,8)) \n\nplt.plot(fpr[0],tpr[0],label=\"Class 0\") \nplt.plot(fpr[1],tpr[1],label=\"Class 1\") \nplt.plot(fpr[2],tpr[2],label=\"Class 2\") \nplt.plot(fpr[3],tpr[3],label=\"Class 3\") \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","591b18fe":"paramgrid = {'max_depth':list(range(1, 20, 2)), 'n_estimators':list(range(1, 200, 20))}","6e4dc7fd":"grid_search = GridSearchCV(RandomForestClassifier(random_state=1), paramgrid)","a21eb7f4":"from sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(X, y, test_size=0.3, random_state=1)","80c455dd":"grid_search.fit(x_train, y_train)","5e52a265":"grid_search.best_estimator_","2525dac0":"#  Now let\u2019s build the model using these optimized values.","ec80e149":"i=1 \nl=0\nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = RandomForestClassifier(random_state=1, max_depth=17, n_estimators=161)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)  \n    i+=1 \n    l +=score\n    \npred_test = model.predict(test) \npred2=model.predict_proba(xvl)[:,1]\nprint('\\n score',l\/5)","101140f4":"fpr = {}\ntpr = {}\nthresh ={}\nn_classes = 4\nfor i in range(n_classes):\n    fpr[i], tpr[i] , thresh[i]= roc_curve(yvl,  pred2, pos_label=i) \n\nplt.figure(figsize=(12,8)) \n\nplt.plot(fpr[0],tpr[0],label=\"Class 0\") \nplt.plot(fpr[1],tpr[1],label=\"Class 1\") \nplt.plot(fpr[2],tpr[2],label=\"Class 2\") \nplt.plot(fpr[3],tpr[3],label=\"Class 3\") \n\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","e732275a":"i =0\nl=0\nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X,y):     \n    print('\\n{} of kfold {}'.format(i+1,kf.n_splits))     \n    xtr,xvl = X.loc[train_index],X.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = XGBClassifier(n_estimators=50, max_depth=4)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1\n    l +=score\n    \npred_test = model.predict(test) \npred3=model.predict_proba(xvl)[:,1]\nprint('\\n score',l\/5)","72bd678f":"fpr = {}\ntpr = {}\nthresh ={}\nn_classes = 4\nfor i in range(n_classes):\n    fpr[i], tpr[i] , thresh[i]= roc_curve(yvl,  pred3, pos_label=i) \n\nplt.figure(figsize=(12,8)) \n\nplt.plot(fpr[0],tpr[0],label=\"Class 0\") \nplt.plot(fpr[1],tpr[1],label=\"Class 1\") \nplt.plot(fpr[2],tpr[2],label=\"Class 2\") \nplt.plot(fpr[3],tpr[3],label=\"Class 3\") \n\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","8a345472":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","9341fe67":"clf = SVC(gamma= 'scale')\nclf.fit(x_train, y_train)\nclf.score(x_train, y_train), clf.score(x_test, y_test)","cc192d55":"results={}\nfor name, col in x_train.items():\n    temp_x_train = x_train.copy()\n    temp_x_train[name] = np.random.permutation(col)\n    results[name] = clf.score(temp_x_train, y_train)\n","14bc0251":"feature_imp = pd.Series(results).sort_values()\nfeature_imp","3b838ee2":"feature_imp.plot.barh()","f567cb24":"X_after_drop = X.drop(['ram'], axis=1)\ntest_after_drop = test.drop(['ram'], axis= 1)","85c4062d":"l=0\ni=0\nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True) \nfor train_index,test_index in kf.split(X_after_drop,y):     \n    print('\\n{} of kfold {}'.format(i+1,kf.n_splits))     \n    xtr,xvl = X_after_drop.loc[train_index],X_after_drop.loc[test_index]     \n    ytr,yvl = y[train_index],y[test_index]         \n    model = XGBClassifier(n_estimators=50, max_depth=4)     \n    model.fit(xtr, ytr)     \n    pred_test = model.predict(xvl)     \n    score = accuracy_score(yvl,pred_test)     \n    print('accuracy_score',score)     \n    i+=1\n    l +=score\n    \npred_test = model.predict(test_after_drop) \npred3=model.predict_proba(xvl)[:,1]\nprint('\\n score',l\/5)","b8e3080a":"test_after_drop","38784799":"X_after_drop","52bfa60c":"# Feature importance","cb35887b":"# Hyperparameter tunning","bc0d8717":"# Conclusion\n\n-  After performing Decision tree, Random forest, Hyperparameter tunning and XGBOOST we found that accuracy score of XGBOOST  is more compare to all of the other models.\n- Applying feature importance technique we can see that performance of our model is reduced, so without reducing features XGBOOST gives best accuracy.\n\n\n# If you like this notebook please do upvote!!","b1386630":"# Visualize features","d88d12f8":"# Stratified k-fold cross validation Random forest model","ecd129d9":"# Stratified k-fold cross validation Decision tree model","3966b9a2":"# Load libraries and datasets\n\nLoading all the required libraries and datasets for the classification of mobile price range.","aba1e2af":"#  Stratified k-fold cross validation XGBOOST model","696d1912":"# Finding missing values","8ee9aa05":" # Independent Variable v\/s Target Variable"}}