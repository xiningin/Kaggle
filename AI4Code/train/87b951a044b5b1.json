{"cell_type":{"0fe71940":"code","795d8c4d":"code","757db4c0":"code","3fed2e2d":"code","38576b36":"code","3fce87ca":"code","355bac4d":"code","542d536a":"code","144d459e":"code","88b81191":"code","51b15cf9":"code","7b7afa9a":"code","a3b03c64":"code","aeef40cd":"code","e4214b29":"code","6bd16c51":"code","01428ffa":"code","704968cf":"code","0e66ee5c":"code","baa7fdfc":"code","cc095716":"code","902609f3":"code","753b2f13":"code","97736d18":"code","1738f0a9":"code","2563c11a":"code","861562f1":"markdown","9903369f":"markdown","8a4da759":"markdown","cd9487be":"markdown","c1c9d35b":"markdown","bf24f8c0":"markdown","af3d5bf8":"markdown","1e4ca4fa":"markdown","d4455f3d":"markdown"},"source":{"0fe71940":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport sklearn.tree as tree\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nfrom sklearn import metrics\nfrom subprocess import call\nfrom IPython.display import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')","795d8c4d":"#Reading file\ntitanic=pd.read_csv('..\/input\/train.csv')","757db4c0":"#Inspecting data\nprint(titanic.describe())\nprint(titanic.info())","3fed2e2d":"#Filling nans in age column with mean age of people from the same sex and Pclass\ngrouped=titanic.groupby(['Sex', 'Pclass'])\ndef cleaning(group):\n    return group.fillna(group.mean())\ntitanic['Age']=grouped['Age'].transform(cleaning)","38576b36":"#Filling nans values in embarked column with the most frequent value\nprint(titanic['Embarked'].value_counts())\ntitanic['Embarked']=titanic['Embarked'].fillna('S')","3fce87ca":"#Dropping unnecessary columns - PassengerID, ticket and cabin\ntitanic.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1, inplace=True)","355bac4d":"#Adding column with information about family size\ntitanic['FamilySize']= titanic['SibSp']+titanic['Parch']+1","542d536a":"#Adding column IsAlone: 0 - travelling with family, 1 - travelling alone\ntitanic['IsAlone']=1\ntitanic['IsAlone'].loc[titanic['FamilySize']>1]=0","144d459e":"#Extracting titles from name column and creating column Title\ntitanic['Title']=titanic['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitanic['Title'].replace({'Ms':'Miss', 'Mlle':'Miss', 'Mme':'Mrs'}, inplace=True)\ntitanic['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Jonkheer', 'Countess', 'Lady', 'Don', 'Capt', 'Sir'], 'Special', inplace=True)","88b81191":"#Creating column with age group\ntitanic['GroupAge']=pd.cut(titanic['Age'], 4, labels=False)","51b15cf9":"#Creating column with fare group\ntitanic['GroupFare']=pd.qcut(titanic['Fare'],4, labels=False)","7b7afa9a":"#Dropping unnecessary columns\ntitanic.drop(['FamilySize', 'Name', 'Age', 'SibSp', 'Parch', 'Fare'], axis=1, inplace=True)","a3b03c64":"#Creating dummy variables for categorical columns\ntitanic=pd.get_dummies(titanic, columns=['Pclass', 'Sex', 'Embarked', 'Title'], drop_first=True)","aeef40cd":"#Inspecting data\nprint(titanic.head())\nprint(titanic.describe())\nprint(titanic.info())","e4214b29":"#Extract feature columns and target column\nX=titanic.drop('Survived', axis=1)\ny=titanic['Survived']","6bd16c51":"#split dataset to train and test sample\nX_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=42)","01428ffa":"#function which makes prediction on given model and return auc score\ndef prediction_auc(X, y_true):\n    y_pred=decisiontree.predict(X)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_true, y_pred)\n    result = auc(false_positive_rate, true_positive_rate)\n    return result\n\n#plotting results for train and test data depending on teste variable\ndef plotting(x, x_name, list_auc_train, list_auc_test):\n    plt.plot(x, list_auc_train, label='train')    \n    plt.plot(x, list_auc_test, label='test')\n    plt.legend()\n    plt.xlabel(x_name)\n    plt.ylabel('AUC score')\n    plt.show()\n\n#function which return image of tree's structure\ndef visualizing_tree(tree, name):\n    export_graphviz(tree, out_file='tree.dot',feature_names = X_train.columns.tolist(), class_names=['Not Survived', 'Survived'], rounded = True, proportion = False, precision = 2, filled = True)\n    call(['dot', '-Tpng', 'tree.dot', '-o', name, '-Gdpi=600'])\n    return Image(filename = name)","704968cf":"max_depths=range(1,31) #creating list of values to check\n\nlist_auc_train=[] #list to store results for train dataset \nlist_auc_test=[] #list to store results for test dataset\n\n#creating model and check result for each value of max depth\nfor max_depth in max_depths:\n    decisiontree=DecisionTreeClassifier(max_depth=max_depth)\n    decisiontree.fit(X_train,y_train)\n    \n    auc_train=prediction_auc(X_train,y_train)\n    list_auc_train.append(auc_train)\n    \n    auc_test=prediction_auc(X_test,y_test)\n    list_auc_test.append(auc_test)\n\n#plotting results    \nplotting(max_depths, 'max depth', list_auc_train, list_auc_test)","0e66ee5c":"#buliding tree with max depth=4\ndecisiontree_depth_4=DecisionTreeClassifier(max_depth=4)\ndecisiontree_depth_4.fit(X_train,y_train)\ny_pred=decisiontree_depth_4.predict(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nresult = auc(false_positive_rate, true_positive_rate)\nprint('AUC score: ', result)","baa7fdfc":"#visualizing tree structure\nvisualizing_tree(decisiontree_depth_4, 'tree_depth_4.png')","cc095716":"min_samples_splits=np.linspace(0.05,0.4,8) #creating list of values to check\nlist_auc_train=[] #list to store results for train dataset \nlist_auc_test=[] #list to store results for test dataset\n\n#creating model and check result for each value of min samples split\nfor min_samples_split in min_samples_splits:\n    decisiontree=DecisionTreeClassifier(min_samples_split=min_samples_split)\n    decisiontree.fit(X_train,y_train)\n    auc_train=prediction_auc(X_train,y_train)\n    list_auc_train.append(auc_train)\n    \n    auc_test=prediction_auc(X_test,y_test)\n    list_auc_test.append(auc_test)\n\n#plotting results    \nplotting(min_samples_splits, 'min samples split', list_auc_train, list_auc_test)","902609f3":"#building tree with min samples split=0.1 and visualizing its structure\ndecisiontree_split_01=DecisionTreeClassifier(min_samples_split=0.1)\ndecisiontree_split_01.fit(X_train,y_train)\ny_pred=decisiontree_split_01.predict(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nresult = auc(false_positive_rate, true_positive_rate)\nprint('AUC score: ', result)\nvisualizing_tree(decisiontree_split_01, 'tree_split_01.png')","753b2f13":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True) #creating list of values to check\nlist_auc_train=[] #list to store results for train dataset \nlist_auc_test=[] #list to store results for test dataset\n\n#creating model and check result for each value of min samples leaf\nfor min_samples_leaf in min_samples_leafs:\n    decisiontree=DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n    decisiontree.fit(X_train,y_train)\n    auc_train=prediction_auc(X_train,y_train)\n    list_auc_train.append(auc_train)\n    \n    auc_test=prediction_auc(X_test,y_test)\n    list_auc_test.append(auc_test)\n\n#plotting results    \nplotting(min_samples_leafs, 'min samples leaf', list_auc_train, list_auc_test)","97736d18":"#building tree with min samples leaf=0.2 and visualizing its structure\ndecisiontree_leaf_02=DecisionTreeClassifier(min_samples_leaf=0.2)\ndecisiontree_leaf_02.fit(X_train,y_train)\ny_pred=decisiontree_leaf_02.predict(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nresult = auc(false_positive_rate, true_positive_rate)\nprint('AUC score: ', result)\nvisualizing_tree(decisiontree_leaf_02, 'tree_leaf_02.png')","1738f0a9":"#Random Forest\nrandomforest=RandomForestClassifier() #initializing model\nne = np.arange(1,20) \nparam_grid = {'n_estimators' : ne} #parameters grid\n\nrf_cv = GridSearchCV(randomforest, param_grid=param_grid, cv=5) #performing gridsearchcv\nrf_cv.fit(X, y)\nprint('Best estimator', rf_cv.best_params_)","2563c11a":"#Random Forest\nrandomforest=RandomForestClassifier() #initializing model\nne = np.arange(1,20) \nparam_grid = {'n_estimators' : ne} #parameters grid\n\nrf_cv = GridSearchCV(randomforest, param_grid=param_grid, cv=5) #performing gridsearchcv\nrf_cv.fit(X, y)\nprint('Best estimator', rf_cv.best_params_)","861562f1":"* TUNING MAX DEPTH","9903369f":"* TUNING MIN SAMPLES SPLIT","8a4da759":"**DATA CLEANING**","cd9487be":"**FEATURE ENGINEERING**","c1c9d35b":"* TUNING MIN SAMPLES LEAF","bf24f8c0":"**BUILDING AND EVALUATING RANDOM FOREST**","af3d5bf8":"**IMPORTING LIBRARIES AND LOADING DATASET**","1e4ca4fa":"**SPLITTING DATASET INTO TRAIN AND TEST**","d4455f3d":"**BUILDING AND EVALUATING DECISION TREES**"}}