{"cell_type":{"affb4fdb":"code","b8b799b2":"code","25a69fb6":"code","a4446603":"code","654ff6f2":"code","3e32e60a":"code","d39a4f74":"code","8882fd8a":"code","385dd24a":"code","ee3164bb":"code","344dc6ba":"code","80a0a2d7":"code","1a69dbff":"code","1871ecf1":"code","2cdbc890":"code","fb60f94d":"code","8076f0c0":"code","0bb4c7fe":"code","f477e7fb":"code","64b8594f":"code","528751b0":"code","d7aa1d9c":"markdown","8d4c364b":"markdown","602ffbb6":"markdown","4f55e973":"markdown","4d986d0d":"markdown","29e9bad1":"markdown","25f6dda4":"markdown","e82ea440":"markdown","3ee1f85f":"markdown"},"source":{"affb4fdb":"import pandas as pd\nimport numpy as np\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nnp.random.seed(203)","b8b799b2":"from sklearn.decomposition import PCA","25a69fb6":"train = pd.read_csv('..\/input\/diabetes-data-set\/diabetes-dataset.csv')","a4446603":"train.head()","654ff6f2":"target = train.Outcome\ndata = train.drop('Outcome', axis=1)","3e32e60a":"X = data.values\n\n# Invoke the PCA method. Since this is a binary classification problem\n# let's call n_components = 2\npca = PCA(n_components=2)\npca_2d = pca.fit_transform(X)\n\n# Invoke the TSNE method\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000)\ntsne_results = tsne.fit_transform(X)","d39a4f74":"import seaborn as sns\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18,6), dpi=100)\nsns.scatterplot(pca_2d[:,0],pca_2d[:,1], hue=target, ax=axes[0])\naxes[0].set_title('PCA_PLOT')\nsns.scatterplot(tsne_results[:,0],tsne_results[:,1], hue=target, ax=axes[1])\naxes[1].set_title('Tsne_plot')","8882fd8a":"# Calling Sklearn scaling method\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","385dd24a":"pca = PCA(n_components=2)\npca_2d_std = pca.fit_transform(X_std)\n\n# Invoke the TSNE method\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000)\ntsne_results_std = tsne.fit_transform(X_std)","ee3164bb":"fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18,6), dpi=100)\nsns.scatterplot(pca_2d_std[:,0],pca_2d_std[:,1], hue=target, ax=axes[0])\naxes[0].set_title('Normalize_PCA_PLOT')\nsns.scatterplot(tsne_results_std[:,0],tsne_results_std[:,1], hue=target, ax=axes[1])\naxes[1].set_title('Normalize_Tsne_plot')","344dc6ba":"pca = PCA().fit(X_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,7,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","80a0a2d7":"pca = PCA(n_components=5)\nX_pca_model = pca.fit_transform(X_std)\n","1a69dbff":"train_pca_x, val_pca_x, train_pca_y, val_pca_y = train_test_split(X_pca_model, y, stratify=y ,shuffle =True , test_size=0.25)\nclf = SVC(kernel='rbf').fit(train_pca_x, train_pca_y)\npred_y = clf.predict(val_pca_x)\n\nprint (classification_report(val_pca_y, pred_y))\nprint (accuracy_score(val_pca_y, pred_y))","1871ecf1":"input_layer = Input(shape=(X.shape[1],))\nencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50, activation='relu')(encoded)\ndecoded = Dense(50, activation='tanh')(encoded)\ndecoded = Dense(100, activation='relu')(decoded)\noutput_layer = Dense(X.shape[1], activation ='relu')(decoded)\n\nautoencoder = Model(input_layer,output_layer)\nautoencoder.compile(optimizer='adam', loss='mse')","2cdbc890":"y=target.values","fb60f94d":"scaler = preprocessing.MinMaxScaler()\nscaler.fit(X)\nX_scale = scaler.transform(X)\n\n\nx_perished, x_survived = X_scale[y == 0], X_scale[y == 1]\nautoencoder.fit(x_perished, x_perished, epochs = 20, shuffle = True, validation_split = 0.25)","8076f0c0":"autoencoder.layers[0]","0bb4c7fe":"hidden_repr = Sequential()\nhidden_repr.add(autoencoder.layers[0])\nhidden_repr.add(autoencoder.layers[1])\nhidden_repr.add(autoencoder.layers[2])\n","f477e7fb":"No_diabetes_hid_rep = hidden_repr.predict(x_perished)\ndiabetes_hid_rep = hidden_repr.predict(x_survived)\n\nrep_x = np.append(No_diabetes_hid_rep, diabetes_hid_rep, axis = 0)\ny_n = np.zeros(No_diabetes_hid_rep.shape[0])\ny_f = np.ones(diabetes_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)","64b8594f":"from sklearn.svm import SVC","528751b0":"train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, stratify=rep_y ,shuffle =True , test_size=0.25)\nclf = SVC(kernel='rbf').fit(train_x, train_y)\npred_y = clf.predict(val_x)\n\nprint (classification_report(val_y, pred_y))\nprint (accuracy_score(val_y, pred_y))","d7aa1d9c":"# Autoencoder","8d4c364b":"By scaling (or standardising) our features we get even more obvious\/intuitive clusters in our plots.","602ffbb6":"# Let's Normalize the data","4f55e973":"# Trainig with PCA","4d986d0d":"# Lets see how our dataset looks when embedded in 2d ","29e9bad1":"What are Autoencoders? - Autoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.","25f6dda4":"90 percent of variance is explained ","e82ea440":"Take aways\n1. PCA worked better than autoencoder in this dataset \n2. we can improve our autoencoder by increasing number of layers\n3. Autoencoder needs more data to learn\n","3ee1f85f":"## let's Obtain the Hidden Representation"}}