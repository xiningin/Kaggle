{"cell_type":{"9e4bc561":"code","6b223e2e":"code","9fc61f3c":"code","b35b78ff":"code","6a269a0a":"code","4fa0d1fa":"code","90491cff":"code","27a7aae7":"code","c886f4eb":"code","e8ab03df":"code","7b4eaf67":"code","26de6049":"code","8635ab4c":"code","8d5f5dec":"code","24defb16":"code","b9368367":"code","4ee19bc7":"code","9040ea4c":"code","00a33236":"code","738ccd51":"code","9c4b8fd7":"code","37efc1de":"code","3a162799":"code","b37f569e":"code","7b19ff40":"code","924ac244":"code","d95b932e":"code","f65b9645":"code","2b31da74":"code","590589a9":"code","f56316ee":"code","cd9e71fa":"code","e3fbbfe9":"code","844849cc":"code","b124f7a5":"code","9ca69490":"code","09aa0eaf":"code","eb0b3e4f":"code","e16d7623":"markdown","93b72d6c":"markdown","0de0a5a6":"markdown","a393eef1":"markdown","b883a6ff":"markdown","0ce3365a":"markdown","4cae5667":"markdown","15140a4c":"markdown","b07e9918":"markdown","b425b17a":"markdown","e74ce386":"markdown","898acc27":"markdown","4fb54edf":"markdown","896031b3":"markdown","0d534ee7":"markdown","d0a4622a":"markdown","9ac5afaa":"markdown","9a423605":"markdown"},"source":{"9e4bc561":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=FutureWarning) # ignoring Future Warning to clean up notebook","6b223e2e":"sns.set_style('whitegrid')\n\n# to print summary of cross validation score\ndef score_summary(scores):\n    print ('cv_scores: ',scores)\n    print ('mean: ',np.mean(scores))\n    print ('std. deviation: ',np.std(scores))\n\n# to save visualisations as png images\ndef save_figure(fig_id):\n    path = os.path.join('iris_images_' + fig_id + '.png')\n    plt.savefig(path, dpi=300, format='png')","9fc61f3c":"df = pd.read_csv('..\/input\/iris\/Iris.csv')","b35b78ff":"df.head()","6a269a0a":"df.info()","4fa0d1fa":"df.describe()","90491cff":"df['Species'].value_counts()","27a7aae7":"sns.pairplot(df, height=4, hue = 'Species')\nsave_figure('1')","c886f4eb":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='SepalLengthCm', y='SepalWidthCm', hue='Species')\nsave_figure('2')","e8ab03df":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='PetalLengthCm', y='PetalWidthCm', hue='Species')\nsave_figure('3')","7b4eaf67":"# raw correlation between features\ndf.corr()","26de6049":"# plotting heatmap to visualise correlation among features\nplt.figure(figsize=(10,6))\nax = sns.heatmap(df.corr(), annot=True, cmap='Greens')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)  # to correct glitch in seaborn that clips the matrix from top and bottom\nsave_figure('4')","8635ab4c":"df = df.drop('Id', axis=1)\n\nX = df.drop('Species', axis=1)\ny = df['Species']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['Species'])","8d5f5dec":"from sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\nscaler.fit_transform(X_train)\nscaler.transform(X_test)","24defb16":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_reg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\nlog_reg_clf.fit(X_train, y_train)\nlog_reg_clf_scores = cross_val_score(log_reg_clf, X_train, y_train, cv=5)\nscore_summary(log_reg_clf_scores)","b9368367":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier()\nsgd_clf.fit(X_train, y_train)\nsgd_clf_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5)\nscore_summary(sgd_clf_scores)","4ee19bc7":"from sklearn.svm import SVC\nsvc_clf = SVC(gamma='auto')\nsvc_clf.fit(X_train, y_train)\nsvc_clf_scores = cross_val_score(svc_clf, X_train, y_train, cv=5)\nscore_summary(svc_clf_scores)","9040ea4c":"from sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\ndt_clf_scores = cross_val_score(dt_clf, X_train, y_train, cv=5)\nscore_summary(dt_clf_scores)","00a33236":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=100)\nrf_clf.fit(X_train, y_train)\nrf_clf_scores = cross_val_score(rf_clf, X_train, y_train, cv=5)\nscore_summary(rf_clf_scores)","738ccd51":"from sklearn.ensemble import BaggingClassifier\nbag_clf = BaggingClassifier(n_estimators=100, max_samples=0.8)\nbag_clf.fit(X_train, y_train)\nbag_clf_scores = cross_val_score(bag_clf, X_train, y_train, cv=5)\nscore_summary(bag_clf_scores)","9c4b8fd7":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train, y_train)\ngb_clf_scores = cross_val_score(gb_clf, X_train, y_train, cv=5)\nscore_summary(gb_clf_scores)","37efc1de":"from xgboost import XGBClassifier\nxgbc_clf  = XGBClassifier(n_estimators=500, objective='multi:softmax')\nxgbc_clf.fit(X_train, y_train)\nxgbc_clf_scores = cross_val_score(xgbc_clf, X_train, y_train, cv=5)\nscore_summary(xgbc_clf_scores)","3a162799":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C':[8,9,10, 11, 12, 15 ],\n    'gamma':[ .01,.02, 0.05, .001]\n}\n\ngrid = GridSearchCV(svc_clf, param_grid, cv=30)\ngrid.fit(X_train, y_train)","b37f569e":"grid.best_params_","7b19ff40":"best_model = grid.best_estimator_","924ac244":"from sklearn.metrics import classification_report, confusion_matrix\nbest_pred = best_model.predict(X_test)\nprint(confusion_matrix(y_test, best_pred))\nprint(classification_report(y_test, best_pred))","d95b932e":"# changing categorical feature (species) to numerical for ANN \nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_y_train = encoder.transform(y_train)\ndummy_y_train = np_utils.to_categorical(encoded_y_train)\n\nencoder.fit(y_test)\nencoded_y_test = encoder.transform(y_test)\ndummy_y_test = np_utils.to_categorical(encoded_y_test)","f65b9645":"dummy_y_train.shape","2b31da74":"dummy_y_test.shape","590589a9":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.activations import elu, relu, sigmoid, softmax\n\ndl_model = Sequential()\n\ndl_model.add(Dense(units=4, activation='elu'))\ndl_model.add(Dense(units=8, activation='elu'))\ndl_model.add(Dense(units=3, activation='softmax'))\n\ndl_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","f56316ee":"dl_model.fit(x=X_train.values, y=dummy_y_train, epochs=1000, verbose=0)","cd9e71fa":"dl_model_loss = pd.DataFrame(dl_model.history.history)\ndl_model_loss.plot(figsize=(10,6))","e3fbbfe9":"from sklearn.metrics import classification_report, confusion_matrix\ndl_pred = dl_model.predict(X_test.values)\ndl_pred.shape","844849cc":"confusion_matrix(np.argmax(dummy_y_test,axis=1), np.argmax(dl_pred,axis=1))","b124f7a5":"print(classification_report(np.argmax(dummy_y_test,axis=1), np.argmax(dl_pred,axis=1)))","9ca69490":"from sklearn.pipeline import Pipeline\n\nsome_data = [[3.1, 3.1, 1.4, 0.8]]\n\npipeline = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('svc', grid.best_estimator_)\n])\n\nmodel = pipeline.fit(X_train, y_train)\npipeline.predict(some_data)","09aa0eaf":"import joblib\nfrom keras.models import load_model\n\njoblib.dump(model, 'iris_model.pkl')\n#loaded_model = joblib.load('iris_model.pkl')\n\ndl_model.save('iris_dl_model.h5')\n#loaded_dl_model = load_model('iris_dl_model.h5')","eb0b3e4f":"clfs_pipeline = Pipeline([\n    ('normalizer', MinMaxScaler()),\n    ('clf', LogisticRegression())\n])\n\nclfs = [LogisticRegression(), SGDClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(),\n       BaggingClassifier(), GradientBoostingClassifier(), XGBClassifier()]\nfor classifier in clfs:\n    clfs_pipeline.set_params(clf=classifier)\n    clfs_score = cross_val_score(clfs_pipeline, X_train, y_train, cv=5)\n    print('--------------------------------------------------------------------')\n    print(str(classifier))\n    score_summary(clfs_score)","e16d7623":"# 10. Using ANN","93b72d6c":"# 6. Pre-processing : Normalization","0de0a5a6":"## 2. Defining Basic Functions","a393eef1":"# 7. Choosing a better performing model","b883a6ff":"# Iris Dataset - Multiclass Classification Task\n\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\nId\n\nSepalLengthCm\n\nSepalWidthCm\n\nPetalLengthCm\n\nPetalWidthCm\n\nSpecies - Target Variable\n\n## Solution involves following steps:\n\n### 1. Importing the Basic Libraries required \n### 2. Defining basic functions -  to print score summary of cross validation and to save visualizations\n### 3. Importing data from csv file and evaluation of summary statistics\n### 4. Exploratory Data Analysis to see patterns in data\n### 5. Splitting Dataset into Training and Testing set \n### 6. Normalization of data through MinMaxScaler\n### 7. Selecting a model through cross_val_score \n### 8. Fine tuning best performing model through Grid Search CV\n### 9. Testing of fine tuned model on Test set - 100% Accuracy\n### 10. Tackling the same problem through Artificial Neural Network using Keras\n### 11. Validating ANN through Test set - 100% Accuracy\n### 12. Saving models using Joblib\n### 13. Creating a Pipleine to get predicition of Species class on new data\n### 14. Streamlining process to test various Classifiers using Cross Validation Score through a Pipeline\n\n### Kindly provide suggestions to improve the model.\n\n\n## 1. Importing Basic Libraries","0ce3365a":"## SVC has performed best.\n\n# 8. Fine Tuning SVC Model","4cae5667":"## 3. Importing Data and Summary statistics","15140a4c":"# 9. Testing on Test set","b07e9918":"# 13. Model Persistence using Joblib","b425b17a":"# 12. Creating Pipeline to Make Predictions on New Data Using SVC","e74ce386":"### Virsicolor (orange) and Virginica (green) do not seem linearly separable from each other.","898acc27":"# 14. Pipeline to Test Various Classifiers using Cross Validation","4fb54edf":"### Balanced dataset.","896031b3":"## 4. Exploratory Data Analysis","0d534ee7":"### Strong correlation of sepal_length with sepal_width and petal_length\n\n# 5. Splitting data into Train and Test set to prevent Snooping Bias \n","d0a4622a":"### No null values.Target feature is categorical.","9ac5afaa":"### Setosa (blue) is linearly separable from the rest.","9a423605":"# 11. Testing on Test set"}}