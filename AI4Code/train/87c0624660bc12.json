{"cell_type":{"a6a7b7c8":"code","e477e33e":"code","ce0f4958":"code","098de457":"code","f1809978":"code","493e3f28":"code","1d3e8f2c":"code","c0cc6077":"code","58596594":"code","143ed032":"code","40f7c29e":"code","4dd8fda0":"markdown","4a63efaf":"markdown","f5a7a4df":"markdown","22cfe1ca":"markdown","5953cb2f":"markdown","2abef8bd":"markdown","9813561b":"markdown","0bc11d41":"markdown","dea371d4":"markdown","da16053e":"markdown","b6a56474":"markdown","6b399e27":"markdown"},"source":{"a6a7b7c8":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n\ndset_train = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\n\ndset_test = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(dset_test, batch_size=4,\n                                         shuffle=False, num_workers=2)","e477e33e":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","ce0f4958":"criterion = nn.CrossEntropyLoss()","098de457":"%cd ..\/input\/","f1809978":"from fastai_minima.optimizer import OptimWrapper\nfrom fastai_minima.learner import Learner, DataLoaders\nfrom fastai_minima.callback.training import CudaCallback, ProgressCallback","493e3f28":"from torch import optim","1d3e8f2c":"def opt_func(params, **kwargs): return OptimWrapper(optim.Adam(params, **kwargs))","c0cc6077":"dls = DataLoaders(trainloader, testloader)","58596594":"learn = Learner(dls, Net(), loss_func=criterion, opt_func=opt_func, cbs=[CudaCallback()])","143ed032":"learn.fit(1, lr=0.001)","40f7c29e":"from fastai_minima.metrics import accuracy","4dd8fda0":"And that's it! We can also utilize fastai's Metric class and I've included all of them available, simply do:","4a63efaf":"## fastai_minima\nfastai_minima is designed around encouraging you to utilize raw pytorch with minimal fastai, while still gaining access to their training loop. It's only true requirements are `fastprogress` and `fastcore`, and it will forever be pinned around what's going on in fastai version 2.2.6.\n\nWe'll walk through a minimal use example below:","f5a7a4df":"To use any torch optimizer we need to make a very basic `opt_func` function that utilizes `OptimWrapper`. We'll use `optim.Adam` in our example:\n\n> Note: you don't ever need to pass in the model parameters directly when using it, `Learner` will take care of this","22cfe1ca":"## Usage Example\n\nWe're going to follow an example from my blog [Pytorch to fastai, Bridging the Gap](https:\/\/muellerzr.github.io\/fastblog\/2021\/02\/14\/Pytorchtofastai.html) and use the CIFAR10 dataset:\n","5953cb2f":"You can view all available metrics in the fastai [metrics](https:\/\/docs.fast.ai\/metrics) documentation.\n\nI hope some of you find this library useful!","2abef8bd":"Next we can use the `DataLoaders` to tie those together:","9813561b":"Before finally creating our fastai `Learner`. Since we also want to use the GPU, we'll pass in our `CUDACallback` which will set our data and model to the GPU during training:","0bc11d41":"Next we neeed to change directories to the library:","dea371d4":"And now we can import all we need, which includes:\n\n* `OptimWrapper` - which wraps any Pytorch optimizer we want and let's us then use fastai `Learner`'s `splitter` to declare our param groups in case we want to perform any differential learning rates\n* `DataLoaders` - a wrapper class for combining two or more Pytorch `DataLoaders` together\n* `Learner` - fastai's base `Learner` class, which enables access to its entire [Callback](https:\/\/docs.fast.ai\/callback.core) system. I've also recorded a video explaining just what the Callback system and how you can use it [here](https:\/\/www.youtube.com\/watch?v=DIYWATB4B0I&lc=UgyBJX1F2KvnKasfXfZ4AaABAg.9JzGO4586dj9JzXWXDzSYi)\n* `CudaCallback` and `ProgressCallback` - As the name implies, the first is needed to move our data to the GPU, and the second is to enable a progress bar. We'll import them now:","da16053e":"Followed by our loss function:","b6a56474":"From there we can just call `Learner.fit`:","6b399e27":"Next we'll make our model:\n"}}