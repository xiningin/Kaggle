{"cell_type":{"c2246c9d":"code","f6ba8871":"code","edc89fff":"code","28b4e0ea":"code","864026e7":"code","fcf46b33":"code","6cbcd010":"code","2b4552b1":"code","4b6b59f6":"code","bec9fad9":"code","bc4532d8":"code","fb62883d":"code","367a7c0e":"code","7a71094e":"code","730b319a":"code","36338af4":"code","8feb4e93":"code","9c33777c":"code","db23d2e8":"markdown","747c979a":"markdown","83400ce7":"markdown","d573d998":"markdown","bb23d54a":"markdown","1dd96ac0":"markdown","23a925c0":"markdown","0cbbf10c":"markdown","c845488e":"markdown","04f63d6d":"markdown","1ee050ad":"markdown","c89d5530":"markdown","07c4a1dd":"markdown","8e1a2cb1":"markdown","6c25d069":"markdown"},"source":{"c2246c9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Conv2D, activations, Dropout,Flatten, MaxPooling2D, Dense, GlobalAveragePooling2D\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport cv2\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","f6ba8871":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain_data.head()","edc89fff":"labels = train_data[\"label\"]\ntrain_data = train_data.drop([\"label\"], axis = 1)\ntrain_data.head()","28b4e0ea":"f, axr = plt.subplots(2,2)\nimg1 = np.asarray(train_data.iloc[3]).reshape(28,28)\nimg2 = np.asarray(train_data.iloc[4]).reshape(28,28)\nimg3 = np.asarray(train_data.iloc[6]).reshape(28,28)\nimg4 = np.asarray(train_data.iloc[9]).reshape(28,28)\n\naxr[0,0].imshow(img1, cmap = plt.get_cmap(\"gray\"))\naxr[0,1].imshow(img2, cmap = plt.get_cmap(\"gray\"))\naxr[1,0].imshow(img3, cmap = plt.get_cmap(\"gray\"))\naxr[1,1].imshow(img4, cmap = plt.get_cmap(\"gray\"))","864026e7":"train_data = np.asarray(train_data)\nprint(\"Shape of training data = \", train_data.shape)\ntest_data = np.asarray(test_data)\nprint(\"Shape of testing data = \", test_data.shape)","fcf46b33":"# pros_train_data = train_data \/ train_data.mean()\npros_train_data = train_data - np.mean(train_data) \/ np.std(train_data)\npros_test_data = test_data - np.mean(test_data) \/ np.std(test_data)\n                                   ","6cbcd010":"train_x, val_x, train_y, val_y = train_test_split(pros_train_data, labels, shuffle = True, test_size = 0.1)","2b4552b1":"print(\"Shape of training data = \",train_x.shape, train_y.shape)\nprint(\"Shape of validation data = \", val_x.shape, val_y.shape)","4b6b59f6":"x_train = (train_x).reshape(-1, 28, 28, 1)\nx_val = (val_x).reshape(-1, 28, 28, 1)\n\nprint(x_train.shape, x_val.shape)","bec9fad9":"train_y = np_utils.to_categorical(train_y)\nval_y = np_utils.to_categorical(val_y)\nprint(train_y.shape, val_y.shape)\n","bc4532d8":"clf = Sequential()\nclf.add(Conv2D(32, kernel_size = (3,3), activation = \"relu\", input_shape = (28,28,1), padding = \"same\"))\nclf.add(MaxPooling2D((3,3), padding = \"same\"))\nclf.add(Conv2D(32, kernel_size = (3,3), activation= \"relu\", padding=\"same\"))\nclf.add(MaxPooling2D((3,3), padding = \"same\"))\nclf.add(Conv2D(64, kernel_size = (3,3), activation= \"relu\", padding=\"same\"))\nclf.add(Conv2D(128, kernel_size = (3,3), activation= \"relu\", padding=\"same\"))\nclf.add(MaxPooling2D((3,3), padding = \"same\"))\nclf.add(Flatten())\nclf.add(Dense(64, activation = \"relu\"))\nclf.add(BatchNormalization())\nclf.add(Dropout(0.5))\nclf.add(Dense(10, activation = \"softmax\"))\nclf.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\nclf.summary()\n\n\n","fb62883d":"history = clf.fit(x_train, train_y, batch_size=32,epochs=50,verbose=1,validation_data=(x_val, val_y))","367a7c0e":"x_test = np.asarray(pros_test_data).reshape(-1, 28,28, 1)\npred = clf.predict(x_test)\n\npredictions = np.argmax(pred,axis=1)\npredictions.shape\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"my_attempt.csv\", index=False, header=True)","7a71094e":"history.history.keys()\n\nval_loss = history.history[\"val_loss\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nacc = history.history[\"acc\"]\n\nepochs = [i for i in range(len(loss))]\n\nplt.plot(val_acc)\nplt.plot(acc)\nplt.title(\"model accuracy\")\nplt.xlabel(\"accuracy\")\nplt.ylabel(\"epochs\")\nplt.legend([\"val_acc\",\"acc\"])\nplt.show()\n\n","730b319a":"plt.plot(val_loss)\nplt.plot(loss)\nplt.title(\"model loss\")\nplt.xlabel(\"loss\")\nplt.ylabel(\"epochs\")\nplt.legend([\"val_loss\",\"loss\"])\nplt.show()\n\n","36338af4":"datagen =ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n                               height_shift_range=0.08, zoom_range=0.08)\n\nbatches = datagen.flow(x_train, train_y, batch_size = 32)\nval_batches = datagen.flow(x_val, val_y, batch_size = 32)\n","8feb4e93":"history=clf.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)\n","9c33777c":"pred = clf.predict(x_test)\nprediction = np.argmax(pred,axis=1)\nprediction.shape\n\nsubmission=pd.DataFrame({\"ImageId\": list(range(1,len(prediction)+1)),\n                         \"Label\": prediction})\nsubmission.to_csv(\"my_third_best_submission2.csv\", index=False, header=True)","db23d2e8":"## Seperating features and labels\n","747c979a":"## A walkthrough to understand data\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.","83400ce7":"## Visualizing losses and accuracy","d573d998":"## Exploring CNN By keras\n-------------------------------------------------------------------------------------------------------------------------------\nThis notebook presents my exploration of Convolutional Nueral Networks by using keras. I tried different combination of hyperparameters and techniques for better results, until now I achieved 99.31%  test accuract in it. I tried to create this notebook which gives you a complete walkthrough of CNN and how to implement it in Keras. \n\nFor that here I am gonna use the standard MNIST dataset. So let me first give you a brief about this dataset and then we will begin our journey of CNN.\n\n![cnn.png](attachment:cnn.png)","bb23d54a":"### Applying one-hot encoding","1dd96ac0":"## Visualizing the data","23a925c0":"## About dataset\n-------------------------------------------------------------------------------------------------------------------------------\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. ","0cbbf10c":"## Convolutional Neural Networks\n---------------------------------------------------------------------------------------------------------------------------------\n\nConvolutional Nueral Networks is a special kind of neural networks that is used in image processing and image recognition, specifically designed to work on pixel data.\n\nConvolutional Neural Networks are inspired by the brain. Research in the 1950s and 1960s by D.H Hubel and T.N Wiesel on the brain of mammals suggested a new model for how mammals perceive the world visually.  Based on this concept they were introduced in a paper by Bengio, Le Cun, Bottou and Haffner. Their first Convolutional Neural Network was called [LeNet-5](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf ) and was able to classify digits from hand-written numbers. \n\nConvolutional Neural Networks are a bit different than Nueral Networks. First of all, the layers are organised in 3 dimensions: width, height and depth. Further, the neurons in one layer do not connect to all the neurons in the next layer but only to a small region of it. Lastly, the final output will be reduced to a single vector of probability scores, organized along the depth dimension.\n\n![1*U8huw63urvRLUwJe89VXpA.png](attachment:1*U8huw63urvRLUwJe89VXpA.png)\n\nThis is the components of CNN :-\n\n![1*NQQiyYqJJj4PSYAeWvxutg.png](attachment:1*NQQiyYqJJj4PSYAeWvxutg.png)\n\n\nConvolution is one of the main building blocks of a CNN. The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information.\n\nIn the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.\n\nWe execute a convolution by sliding the filter over the input. At every location, a matrix multiplication is performed and sums the result onto the feature map.\n\nIn the animation below, you can see the convolution operation. You can see the filter (the green square) is sliding over our input (the blue square) and the sum of the convolution goes into the feature map (the red square).\n\n![1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif](attachment:1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif)\n\n\n\n","c845488e":"## Building our model","04f63d6d":"## Normalization\n---------------------------------------------------------------------------------------------------------------------------------\n\nThis step is one of the important steps in building any model. So let me give you an introduction to **Normalization**.\n\n### What is Data Normalization?\nData Normalization is a method used to standardize the range of independent variables or features of data. It is one of the cruicial stage of data preprocessing step. \n\n### Why data normalization?\nWe have to normalize our data because our features do not have uniform scale. Let me explain you by an example:\n\nSay you have data of some peoples their age, income and sex:\n\n![1*IV63jT3z4aJOGyHjEMdiGg.png](attachment:1*IV63jT3z4aJOGyHjEMdiGg.png)\n\nNow as we can see here the data about age and income both are of different scale and when we use this data without normalization or scaling it will cause us a huge problem. So, that's where data normalization comes to aid.\n\n### How to normalize your data?\n\nThere are many techniques that we can use, some of them are:-\n\n   ### 1.  Min-max normalization\n![1*m-c4ARwLehrvsBW84w8jcA.png](attachment:1*m-c4ARwLehrvsBW84w8jcA.png)\n\n### 2.  Z-Score normalization\n![1*4pGbXvZ_kUZB1bZZIimNaw.png](attachment:1*4pGbXvZ_kUZB1bZZIimNaw.png)\n\nI used this technique in my notebook.\n\n### 3.  Constant normalization\nTake your value and divide by a constant. In case of images we will divide by 255 because it is largest value of pixle.\n\n\n\n","1ee050ad":"### Splitting data into training and validation set","c89d5530":"## Importing all the necessary modules","07c4a1dd":"### Reshaping data according to our model","8e1a2cb1":"## Conclusion\n\nSo, here ends our exploration journey. I hope this notebook will became helpful fou yo","6c25d069":"## Start predictions"}}