{"cell_type":{"76178c0a":"code","2c709bac":"code","32a24cc9":"code","b74e3c2b":"code","59a43608":"code","8e74bc28":"code","53940449":"code","61ce570d":"code","c7f643a3":"code","25ff605c":"code","a8f77db0":"code","cf6036c6":"code","3c1959c9":"code","73444413":"code","30adcaa6":"code","da73b09a":"code","e294ef19":"code","8d70f80a":"code","755564df":"code","21ac8fa4":"code","f8d944fe":"code","fd050eda":"code","c53cdba7":"code","0540cff1":"code","978743f6":"code","3d7b96a0":"code","6285c1a4":"code","1ad9f9f1":"code","0b93ca78":"code","ca596e08":"code","5a7b7f5d":"code","958e9035":"code","422eee70":"code","94e7a24d":"code","3575138d":"code","a8bf7e81":"code","1bdbc2f7":"markdown","5596aea2":"markdown","12184604":"markdown","d70dfecb":"markdown","ffd41b0e":"markdown","a26499b8":"markdown","b77791ed":"markdown","62d94778":"markdown","4b4cd9fc":"markdown","27e386c5":"markdown","cf7b70ed":"markdown","a77f150c":"markdown","6d151f19":"markdown","2d61ea75":"markdown","5d5d29d9":"markdown","9e99dcaa":"markdown","c9b10e69":"markdown","2124f038":"markdown","e6d6af47":"markdown","88683218":"markdown","b060b26d":"markdown","cf387002":"markdown","9f0d657d":"markdown","36abb5bd":"markdown"},"source":{"76178c0a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","2c709bac":"df= pd.read_csv(\"commercial_vedio_data.csv\")","32a24cc9":"print(df.info())","b74e3c2b":"df.info","59a43608":"df.head()","8e74bc28":"df.tail()","53940449":"df.describe()","61ce570d":"df.isnull()","c7f643a3":"df.isnull().values.any()","25ff605c":"df.isnull().sum(axis=0)","a8f77db0":"df = df.fillna(df.mean())\n","cf6036c6":"df.isnull()","3c1959c9":"df = df.fillna(0)","73444413":"df.isnull()","30adcaa6":"import seaborn as sns\nimport matplotlib.pyplot as plt","da73b09a":"plt.figure(figsize=(15,8))\nbarplot=sns.distplot(df['labels'], kde=False, rug=True)","e294ef19":"plt.figure(figsize=(15,8))\ncorr_plot = sns.heatmap(df.corr(),cmap=\"RdYlGn\",annot=False)","8d70f80a":"df_corr=df.corr()\nx=[]\nfor i in df_corr['labels']:\n    x.append(i)","755564df":"plt.figure(figsize=(15,45))\nax=sns.barplot(x =x , y = df_corr['labels'].index)\nax.set(xlabel='correlation values', ylabel='columns names')\nplt.show()","21ac8fa4":"from sklearn.neural_network import MLPClassifier","f8d944fe":"import numpy as np","fd050eda":"df[\"labels\"]","c53cdba7":"# to see unique val\nset(df['labels'].values)","0540cff1":"df['labels'] = df['labels'].apply(lambda x:0 if x == -1 else x)","978743f6":"# Import `train_test_split` from `sklearn.model_selection`\nfrom sklearn.model_selection import train_test_split\n\n# Specify the data \nX=df.drop(\"labels\",axis=1)\n\n# Specify the target labels and flatten the array \ny=df[\"labels\"]\n\n# Split the data up in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","3d7b96a0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)","6285c1a4":"from sklearn.metrics import accuracy_score","1ad9f9f1":"from sklearn.ensemble import RandomForestClassifier","0b93ca78":"clf=RandomForestClassifier()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_tst = accuracy_score(y_test, pred)\nprint(\"testing accuracy\", accuracy_tst)","ca596e08":"clf = MLPClassifier(alpha=1e-3,max_iter=500, hidden_layer_sizes = (16,32,64,32,16))\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_tst = accuracy_score(y_test, pred)\nprint(\"testing accuracy\", accuracy_tst)","5a7b7f5d":"print(X_train.shape)","958e9035":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, ReLU, Activation, Dropout","422eee70":"model = Sequential()\nmodel.add(Dense(64, input_dim=231,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(64,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(64,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(256,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(256,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512,kernel_initializer='normal'))\n# model.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1 ,activation=\"sigmoid\"))\nopt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel.summary()","94e7a24d":"model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=500, batch_size=128)","3575138d":"model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)","a8bf7e81":"model = Sequential()\nmodel.add(Dense(256, input_dim=231, activation='relu',kernel_initializer='normal'))\nmodel.add(Dense(128, activation='relu',kernel_initializer='normal'))\nmodel.add(Dense(64, activation='relu',kernel_initializer='normal'))\nmodel.add(Dense(32, activation='relu',kernel_initializer='normal'))\n#model.add(Dense(16, activation='relu',kernel_initializer='normal'))\nmodel.add(Dense(1 ,activation=\"sigmoid\"))\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel.summary()","1bdbc2f7":"### Random Forest","5596aea2":"### Visualizing The Data","12184604":"### Advantages\nThe random forest algorithm works well when the data sets  have both categorical and numerical features.\nThe random forest algorithm also works well when data has missing values or it has not been scaled \n\n### Disadvantages\nA major disadvantage of random forests lies in their complexity. Random Forest required much more computational resources, owing to the large number of decision trees joined together. Due to their complexity, they require much more time to train than other comparable algorithms.\n","d70dfecb":"To visualize the data and see the distribution of the labels, I plotted the distribution plot using seborn. We have more data points of positive class than the negative class.","ffd41b0e":"### The advantages of using Multi-layer Perceptron are:\n\nCapability to learn non-linear models.\n\nCapability to learn models in real-time (on-line learning) using partial_fit.\n\n### The disadvantages of Multi-layer Perceptron (MLP) include:\n\nMLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n\nMLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n\nMLP is sensitive to feature scaling.","a26499b8":"Using df.head() Returns the first 5 rows of the dataframe.","b77791ed":"### Summary of Findings\nThe model aims to predict the accuracy of 0.99. For the prediction, We tried Random Forest as the base model to see how good it is performed. it performed quite well and resulted in 95% accuracy on the test set.  \n\n\nWe also use keras to build our neural network. We started with a base network with 6 layers. To make the model more complex; we included dropout, more layers, changing the number of neurons and using different activation functions- relu\nThis model also performed well with an accuracy of 0.9362\n\n","62d94778":"We changed the label of -1 to 0 to make it easy for us to feed the data to the models.","4b4cd9fc":"Now we plot the correlation matrix to see how variables are correlated with the target variable. We can see some of features are dependent on each other.","27e386c5":"### Multi-Layer Perceptron (MLP) Classifier Algorithm\n\nI used MLPClassifier to implement a multi-layer perceptron (MLP) algorithm that trains the model using  Backpropagation.      \n","cf7b70ed":"### Data Wrangling and Cleaning\nWe used isnull() to find the missing values in the dataset and use fillna to fill in the null values.","a77f150c":"### Using The Scaling Method to Standardize The Data","6d151f19":"We tried Random Forest as the base model to see how good it is performed. it performed quite well and resulted in 95% accuracy on the test set.","2d61ea75":"### To evaluate whether any value is missing in a Series","5d5d29d9":"### Loading In The Data","9e99dcaa":"I used the describe() function to get a basic statistical summary about the data such as: Percentile, Mean, Median, Variance, Std and to determine anomaly in the data.","c9b10e69":"### Evaluating Model Performance\n","2124f038":"Now we are trying the keras to build our own neural network. We started with a base network with 6 layers. Now we are trying adding dropout, adding more layers, changing the number of neurons to make the model more complex. We are also trying different activation functions- relu. ","e6d6af47":"Using df.tail() Returns the first 5 rows of the dataframe.","88683218":"### Data Exploration\nusing the info function to look at the indexes, Datatype and Memory information","b060b26d":"### TikTok commercial video advertisements detect! Who can make the accuracy 0.99?\n\n### Introduction\n\nThe purpose of the project is to predict the label and make the accuracy 0.99?\n\nFor the prediction, we used Random Forest as the base model to see how good it will perform. Including Neural network  (sklearn's MLPClassifier) with basic 5 layers and this setting of neurons: (16,32,64,32,16) and the keras to build our models. We started with a base network with 6 layers.we added dropout, more layers, changing the number of neurons to make the model more complex. We are also trying different activation functions- relu.\n\nFor the Split test, the dataset has been separated into training and Testing data so that prediction can be done on the testing data. I also import Tableau that can make the visualization more feasible.\nThe technical aspects used for visualization are Matplolib, seaborn, including data manipulation: pandas, NumPy, modeling-Sklearn, Neural network & Keras.\n\n\n","cf387002":"Replace all null values with the mean","9f0d657d":"### Important Library","36abb5bd":"This graph shows a relationship between correlation value and column name."}}