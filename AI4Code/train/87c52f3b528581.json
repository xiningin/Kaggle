{"cell_type":{"5d34498e":"code","d47f9125":"code","9e32f72e":"code","2a1bacfc":"code","5c7843bc":"code","68586f84":"code","faf4f375":"code","bec64cf6":"code","43452ae5":"code","ad420712":"code","b046aee3":"code","bf63f5a4":"code","2da8610e":"code","29a41a0e":"code","2c70a302":"code","d35a7be4":"code","c6ec113c":"code","26ac0070":"code","bc6114bf":"code","8abec6d8":"code","2a4aa5be":"code","7f2cee1c":"code","b8a5355d":"code","75adf8c0":"code","34cbf009":"code","f2c84dd6":"code","8757384b":"code","454b433f":"code","ba346ca5":"code","ab83a8d8":"code","c1769481":"code","3aad421f":"code","c628be60":"code","4e025837":"code","4e90589c":"code","f53b66b7":"code","1a6aa9eb":"code","68ac7e6d":"code","9b0a3559":"code","03e7e243":"code","f24e7d3a":"code","9304f18d":"code","bbdf2fe9":"code","4155af11":"code","70742d32":"code","ce3b5e11":"code","f5088979":"code","94f5db11":"code","3900f8b3":"code","2edbb5fd":"code","1058e3b9":"code","878e1639":"code","d6434e1d":"code","b6f43ce6":"code","48fcebbe":"code","0960d373":"code","6e18c5b2":"code","40535540":"code","526ba367":"code","7e49ba66":"code","ef29bbf1":"code","f2b119a9":"code","eb5cabb9":"code","d4f40bf6":"markdown","419adbb0":"markdown","f2a5a460":"markdown","0617fb79":"markdown","7f4d920f":"markdown","df994198":"markdown","dce56af6":"markdown","ac8a147b":"markdown","99aeb017":"markdown","684e8464":"markdown","931e9e2d":"markdown","524297b9":"markdown","011b5686":"markdown","9d0ce2e3":"markdown","5cfbb41c":"markdown","0fb1439b":"markdown","501a419b":"markdown","27916670":"markdown","9ca769a9":"markdown","a1723563":"markdown","dcab938e":"markdown","13b57b8f":"markdown","b2605b5d":"markdown","ba74a26c":"markdown","b637fc6d":"markdown","fad2f6fc":"markdown","b9a744f1":"markdown","2530ce7e":"markdown","47ac2766":"markdown","1f0ffffa":"markdown","af80ed71":"markdown","8ce1f4cc":"markdown","1ee27e46":"markdown","8d7e44b9":"markdown","976d71b9":"markdown","69a65578":"markdown","19de0eac":"markdown","b57f0d84":"markdown","a669094b":"markdown","324c19e0":"markdown","42350cda":"markdown","d3c2b863":"markdown","e3c4de8c":"markdown","9423bcd0":"markdown","e819a499":"markdown","e48028e9":"markdown","ffb592ad":"markdown","1b4cd60e":"markdown","cfd099e5":"markdown","2fcad1f4":"markdown","895b37a8":"markdown","b4024e5c":"markdown","37bd6606":"markdown"},"source":{"5d34498e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d47f9125":"# Import data to dataframe\ndata = pd.read_csv('\/kaggle\/input\/asteroid-dataset\/dataset.csv')","9e32f72e":"pd.set_option('display.max_columns', 500)\ndata.head()","2a1bacfc":"data.columns","5c7843bc":"data.describe()","68586f84":"data.shape","faf4f375":"#1. id and spkid\nprint(data['id'].nunique())\nprint(data['spkid'].nunique())\nprint(data['full_name'].nunique())\nprint(data['pdes'].nunique())","bec64cf6":"# Potentially hazardous asteroids\ndata['pha'].value_counts(normalize=True)","43452ae5":"# Near Earth Object\ndata['neo'].value_counts(normalize=True)","ad420712":"# Asteroid orbit ID\nprint(data['orbit_id'].unique())\nprint(data['orbit_id'].nunique())","b046aee3":"# Comet Designation prefix\nprint(data['prefix'].unique())\nprint(data['prefix'].nunique())","bf63f5a4":"# Equinox reference\nprint(data['equinox'].unique())\nprint(data['equinox'].nunique())","2da8610e":"# Orbit classification\nprint(data['class'].unique())\nprint(data['class'].nunique())","29a41a0e":"data1 = data.drop(['id', 'pdes', 'name', 'prefix', 'equinox'], axis='columns', inplace=False)","2c70a302":"asteroid_df = data1[data1['pha'].notna()]\nasteroid_df = asteroid_df.drop(['diameter', 'albedo', 'diameter_sigma'], axis= 'columns')","d35a7be4":"asteroid_df = asteroid_df[asteroid_df['H'].notna()]","c6ec113c":"asteroid_df = asteroid_df[asteroid_df['sigma_ad'].notna()]\nasteroid_df = asteroid_df[asteroid_df['ma'].notna()] # Remove row with the one missing value for 'ma'","26ac0070":"asteroid_df['neo'] = asteroid_df['neo'].astype('category')\nasteroid_df['pha'] = asteroid_df['pha'].astype('category')\nasteroid_df['class'] = asteroid_df['class'].astype('category')","bc6114bf":"# What percent of asteroids are near earth objects?\n\nasteroid_df['neo'].value_counts(normalize=True)*100","8abec6d8":"# Of the near earth objects, what percent of them are potentially hazardous asteroids?\n\nasteroid_df[asteroid_df['neo']=='Y']['pha'].value_counts(normalize=True)*100","2a4aa5be":"# How many asteroids of the dataset are potentially hazardous asteroids?\n\nasteroid_df['pha'].value_counts(normalize=True)*100","7f2cee1c":"# Of the potentially hazardous asteroids, what percent of them are near earth objects?\n\nasteroid_df[asteroid_df['pha']=='Y']['neo'].value_counts(normalize=True)*100","b8a5355d":"# What is the distribution of the orbit classification?\n\nasteroid_df['class'].value_counts(normalize=True)*100","75adf8c0":"# How many orbit IDs exist?\n\nasteroid_df['orbit_id'].nunique()","34cbf009":"# Number of orbit_id that have less than 10 occurances\norbits = asteroid_df['orbit_id'].value_counts().loc[lambda x: x<10].index.to_list()","f2c84dd6":"len(orbits)","8757384b":"asteroid_df.loc[asteroid_df['orbit_id'].isin(orbits), 'orbit_id'] = 'other'","454b433f":"# Reset the index\nasteroid_df = asteroid_df.reset_index(drop=True)","ba346ca5":"# Create a subset of only numerical columns to scale\nsubset_df = asteroid_df[asteroid_df.columns[~asteroid_df.columns.isin(['spkid', 'full_name', 'neo', 'pha', 'orbit_id', 'class'])]]","ab83a8d8":"from sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nscaled_df = scaler.fit_transform(subset_df)\nscaled_df = pd.DataFrame(scaled_df, columns=subset_df.columns)\nasteroid_df = pd.concat([asteroid_df[['spkid', 'full_name', 'neo', 'pha', 'orbit_id', 'class']],scaled_df], axis=1)\nscaled_df.head()","c1769481":"# 1. Create one-hot encoding columns using get_dummies\nasteroid_df1 = pd.get_dummies(asteroid_df, columns=['neo', 'class', 'orbit_id'])\nasteroid_df1.head()","3aad421f":"from sklearn.model_selection import train_test_split\n\nX = asteroid_df1.drop(['spkid', 'full_name', 'pha'], axis=1)\ny = asteroid_df1.iloc[:]['pha']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1501)","c628be60":"print(\"Before OverSampling, counts of label 'N': {}\".format(sum(y_train == 'N'))) \nprint(\"Before OverSampling, counts of label 'Y': {} \\n\".format(sum(y_train == 'Y'))) \n  \n# import SMOTE module from imblearn library \nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state = 12) \nx_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel()) \n  \nprint(\"After OverSampling, counts of label 'N': {}\".format(sum(y_train_res == 'N'))) \nprint(\"After OverSampling, counts of label 'Y': {}\".format(sum(y_train_res == 'Y'))) ","4e025837":"def metricCalculation(y_test, pred):\n    \n    precision_metric = metrics.precision_score(y_test, pred, average = \"macro\")\n    recall_metric = metrics.recall_score(y_test, pred, average = \"macro\")\n    accuracy_metric = metrics.accuracy_score(y_test, pred)\n    f1_metric = metrics.f1_score(y_test, pred, average = \"macro\")\n    print('Precision metric:',round(precision_metric, 2))\n    print('Recall Metric:',round(recall_metric, 2))\n    print('Accuracy Metric:',round(accuracy_metric, 4))\n    print('F1 score:',round(f1_metric, 2))","4e90589c":"# Import the model\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate the model\nlogisticRegr = LogisticRegression(max_iter= 10000) # create object for the class\n\n# Fit to train model with features and labels\nlogisticRegr.fit(x_train_res, y_train_res)\n\n# Predict for test set\nlr_pred = logisticRegr.predict(x_test)","f53b66b7":"# Calculate metrics\nmetricCalculation(y_test, lr_pred)","1a6aa9eb":"# Print confusion matrix\nprint(metrics.confusion_matrix(y_test, lr_pred))","68ac7e6d":"# Import the model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate model with 150 decision trees\nrf = RandomForestClassifier(n_estimators = 150, random_state = 1551)\n\n# Train the model on training data\nrf.fit(x_train_res, y_train_res)\n\n# Predict for test set\nrf_pred = rf.predict(x_test)","9b0a3559":"# Calculate metrics\nmetricCalculation(y_test, rf_pred)","03e7e243":"# Confusion matrix\nprint(metrics.confusion_matrix(y_test, rf_pred))","f24e7d3a":"feature_imp = pd.DataFrame(rf.feature_importances_,index=x_train_res.columns, columns = ['Importance']).sort_values(by='Importance', ascending=False)","9304f18d":"# Top 10 important variables\nfeature_imp[0:10]","bbdf2fe9":"# 10 least important features\nfeature_imp[-10:]","4155af11":"feature_imp[-50:].index","70742d32":"asteroid_df2 = pd.get_dummies(asteroid_df, columns=['neo', 'class'])\nasteroid_df2.drop(['orbit_id','sigma_ma', 'sigma_tp'], axis='columns', inplace=True)","ce3b5e11":"# Create train test splits \n\nX1 = asteroid_df2.drop(['spkid', 'full_name', 'pha'], axis=1)\ny1 = asteroid_df2.iloc[:]['pha']\n\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.30, random_state=1501)","f5088979":"# Create equal balance of classes using SMOTE\n\nsm = SMOTE(random_state = 12) \nx_train_res1, y_train_res1 = sm.fit_sample(x_train1, y_train1.ravel()) \n  \nprint(\"After OverSampling, counts of label 'N': {}\".format(sum(y_train_res1 == 'N'))) \nprint(\"After OverSampling, counts of label 'Y': {}\".format(sum(y_train_res1 == 'Y'))) ","94f5db11":"# Instantiate model with 150 decision trees\nrf = RandomForestClassifier(n_estimators = 150, random_state = 1551)\n\n# Train the model on training data\nrf.fit(x_train_res1, y_train_res1)\n\n# Predict for test set\nrf_pred1 = rf.predict(x_test1)","3900f8b3":"# Calculate metrics\nmetricCalculation(y_test, rf_pred1)","2edbb5fd":"print(metrics.confusion_matrix(y_test, rf_pred))","1058e3b9":"# Duplicate the training data sets for the label\ny_train_res_2 = y_train_res","878e1639":"# Encode labels\n\nfor n,i in enumerate(y_train_res_2):\n    if i=='Y':\n      y_train_res_2[n] = 1\n    else:\n        y_train_res_2[n] = 0","d6434e1d":"# Use label encoding to encode test labels \ny_test_2 = y_test.cat.codes","b6f43ce6":"# Load the training dataset along with the label to LGBM\nimport lightgbm as lgb \n\ntrain_data=lgb.Dataset(x_train_res,label=y_train_res_2)","48fcebbe":"#setting parameters for lightgbm\n\nparam = {'num_leaves': 150, # number of leaves per tree\n         'nrounds': 350,\n         'max_depth': 25, # depth of tree\n         'learning_rate': 0.01, # learning rate\n         'max_bin': 500 # max number of bins to bucket the feature values.\n        }","0960d373":"# Train the model \n\nlgbm = lgb.train(param, train_data)\nlgbm_pred = lgbm.predict(x_test)\n\n# Convert the predicted probabilities to 0 or 1\nfor i in range(0,len(y_test_2)):\n    if lgbm_pred[i]>=.5:       # setting threshold to .5\n       lgbm_pred[i]=1\n    else:  \n       lgbm_pred[i]=0","6e18c5b2":"# Calculate metrics\nmetricCalculation(y_test_2, lgbm_pred)","40535540":"# Confusion Matrix\nprint(metrics.confusion_matrix(y_test_2, lgbm_pred))","526ba367":"# Duplicate the training data sets for the label\ny_train_res_3 = y_train_res1\n\n# Encode labels\n\nfor n,i in enumerate(y_train_res_3):\n    if i=='Y':\n      y_train_res_3[n] = 1\n    else:\n        y_train_res_3[n] = 0\n        \n# Use label encoding to encode test labels \ny_test_3 = y_test1.cat.codes","7e49ba66":"# Load the training dataset along with the label to LGBM\nimport lightgbm as lgb \n\ntrain_data_1=lgb.Dataset(x_train_res1,label=y_train_res_3)","ef29bbf1":"# Train the model \n\nlgbm_1 = lgb.train(param, train_data_1)\nlgbm_pred_1 = lgbm_1.predict(x_test1)\n\n# Convert the predicted probabilities to 0 or 1\nfor i in range(0,len(y_test_3)):\n    if lgbm_pred_1[i]>=.5:       # setting threshold to .5\n       lgbm_pred_1[i]=1\n    else:  \n       lgbm_pred_1[i]=0","f2b119a9":"# Calculate metrics\nmetricCalculation(y_test_3, lgbm_pred_1)","eb5cabb9":"# Confusion Matrix\nprint(metrics.confusion_matrix(y_test_3, lgbm_pred_1))","d4f40bf6":"Before developing the models, we need to create the train and test sets. Remove 'spkid' and 'full_name' since it will not be required in the data modelling. The feature 'pha' will be used as label alone.","419adbb0":"### 1. Logistic Regression","f2a5a460":"Remove the remaining missing row values using column 'sigma_ad' since it seems to have the most number of missing values.","0617fb79":"Based on the Random Forest model, the most important feature is the Earth Minimum Orbit Intersection Distance (moid_id) followed by identifying if the object in question is a near earth object (neo) or not. ","7f4d920f":"Of the data set of asteroids provided, 99.7% of the asteroids are non-hazardous. All the potentially hazardous asteroids are near earth objects (neo). On the other hand, only 9% of the near earth objects are hazardous. \n\nOur focus is to predict if an asteroid is potentially hazardous. ","df994198":"# 5. Conclusion","dce56af6":"1. Logistic Regression\n2. Random Forest\n3. Light Gradient Boosting","ac8a147b":"There are 331 orbit ids that occur less than 10 times. We can replace these orbit ids by renaming them as 'others' so there is no loss of data.","99aeb017":"For the sake of LGBM, convert the labels into numeric values by substituting Y with 1 and N with 0","684e8464":"Set paramateres before fitting the model. After experimenting with a few learning rates, it was found that a rate of 0.01 yeilded the highest value for precision although all other metrics remained almost the same. ","931e9e2d":"Although the accuracy and recall of the model is high, the precision metric and F1 score paint a different picture. The low F1 score and precision prove that the model doesn't classify well and has a poor balance between the two classes. Based on the confusion matrix alone, we notice a high number of false positives. The power of the logistic regression model isn't strong enough to predict the nature of the asteroids.","524297b9":"These categories can be further analysed to understand their distribution by answering questions pertinent to their features.","011b5686":"# 2. Data Wrangling","9d0ce2e3":"Before creating machine learning models, it is imperative to make sure the data being provided isn't cumbersome. For example, the 'orbit_id' feature has 525 unique catergories to identify the asteroid's orbit. We can reduce this number by analysing the less occuring orbit IDs.","5cfbb41c":"### Basic Column Definition from the [JPL website](https:\/\/ssd.jpl.nasa.gov\/sbdb_query.cgi)\n* SPK-ID: Object primary SPK-ID\n* Object ID: Object internal database ID\n* Object fullname: Object full name\/designation\n* pdes: Object primary designation\n* name: Object IAU name\n* NEO: Near-Earth Object (NEO) flag\n* PHA: Potentially Hazardous Asteroid (PHA) flag\n* H: Absolute magnitude parameter\n* Diameter: object diameter (from equivalent sphere) km Unit\n* Albedo: Geometric albedo\n* Diameter_sigma: 1-sigma uncertainty in object diameter km Unit\n* Orbit_id: Orbit solution ID\n* Epoch: Epoch of osculation in modified Julian day form\n* Equinox: Equinox of reference frame\n* e: Eccentricity\n* a: Semi-major axis au Unit\n* q: perihelion distance au Unit\n* i: inclination; angle with respect to x-y ecliptic plane\n* tp: Time of perihelion passage TDB Unit\n* moid_ld: Earth Minimum Orbit Intersection Distance au Unit","0fb1439b":"# 3. Data Preprocessing","501a419b":"No missing values exist in the ateroid identifying columns.","27916670":"We use the same parameters for the model.","9ca769a9":"The model with the trimmed data is a definite improvement although it isn't as good as the prediciton by Random Forest, since it still has more mislabeled asteoroids. LGBM ranks second in model performance. ","a1723563":"### Analyse missing values","dcab938e":"### Analyse columns\n\nBased on the description of the data above, it can be noticed that many features have missing values. Before imputing or eliminating them, we need to first understand the kind of data each feature holds\n","13b57b8f":"The Random Forest model with only important features trumps the other models in perfomance metrics. Light Gradient Boosting had a good performance as well, but not as good as Random Forest, even with the model tuned for different paramters. Logistic Regression was used a baseline model to match the other models with, and although it had good accuracy, it still was weak in performance. <br>\n\nThus a tuned Random Forest model would be best to predict the hazardous nature of the asteroids. ","b2605b5d":"Logistic Regression will be the baseline model for the dataset. Using the metrics from this model, we can compare metrics from the other models and tune them to achieve better values.","ba74a26c":"### Columns Data Types","b637fc6d":"Most columns have almost no missing values. The 'sigma' columns seems to have missing values for the same number of rows. Although the 'name' column has 97% missing values, it is paired with 'pdes' to make a full name. \n\nColumns 'diameter', 'albedo' and 'diameter_sigma' have 85% missing values. Since these values cannot be measured or derived, these columns can be removed.\n\nColumns 'pha', 'moid' and those with the 'sigma' prefix columns have missing values for the same rows where 'pha' is missing data. Since its only 2% of the data, we can keep remove these entries.","fad2f6fc":"Convert the labels into numeric values by substituting Y with 1 and N with 0","b9a744f1":"Upon further exploring the least important features, it can be seen that the orbit IDs do not contribute much to the model. The dataset can be modified by eliminating the orbit_id feature completely. Similarly we can also elimate the features 'sigma_ma' and 'sigma_tp' that have 0 importance.","2530ce7e":"The random forest classifier has a higher F-score and precision than the logistic regression, proving that its a better model for identifying the nature of an asteroid. Using this model, we can identify the most important features that help in determining the type of asteroid.","47ac2766":"This notebook explores the [NASA JPL Asteroid Dataset](https:\/\/www.kaggle.com\/sakhawat18\/asteroid-dataset) using machine learning techniques to create a model to predict whether asteoirds are potentially hazardous or not. The notebook covers the following aspects of machine learning:\n1. Data Exploration\n2. Data Wrangling\n3. Data Preprocessing\n4. ML Model Developing\n5. Conclusion","1f0ffffa":"### 3.1 LGBM with entire data set","af80ed71":"### 2. Random Forest","8ce1f4cc":"The Light Gradient Boosting Model with the entire dataset has higher metric values as compared to Logistic Regression but a little lower than Random Forest. The confusion matrix shows that the LGBM has more mislabled values than the Random Forest model. Now we try training the model on the trimmed data set based on the importance of the Random Forest model.","1ee27e46":"Columns 'id', 'spkid' and 'full_name' are unique for each row. The 'full_name' column values are split into columns 'pdes' and 'name'. These columns can be removed since they will not facilitate in the analysis. The 'id' column has alphanumeric values whereas column 'spkid' doesn't. So column 'id' can be removed as well. \n\nColumns 'prefix' and 'equinox' have only one value so they can be eliminated as well.","8d7e44b9":"The data is highly imbalanced with over 99% of the data belonging to the negative class. This could sway the models and predict only the negative class for any input. For this reason, its best to oversample the positive class and create an equal sample numbers for both classes. This is achieved by usig the library [SMOTE](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html).","976d71b9":"### 3. Light Gradient Boosting","69a65578":"There are a few values missing in column 'H' - absolute magnitude. This can be determined using albedo and diameter but since those columns no longer exist, we can remove the rows with missing 'H' values.","19de0eac":"Certain column types will need to be changed for the machine learning models to use. Machine Learning models will not be able to process <br> \nConvert columns 'neo', 'pha' and 'class' to categorical variables.","b57f0d84":"Now analyse all columns of the 'object' datatype","a669094b":"The best performing model can then be selected as a winner to conduct reliable predictions.","324c19e0":"Random Forest is known to elimiate the chance of overfitting and with the help of its ensamble method, it could be a better classifier than logisitic regression. ","42350cda":"Create a function to calculate the metrics of each model.","d3c2b863":"Convert the categorical columns 'neo' and 'class' and object column 'orbit_id' into one-hot encoding variables.","e3c4de8c":"As can be seen, the model performance has improved with fewer false positives and false negatives. Thus, Random Forest can be used as a reliable model to predict the nature of the asteroid.","9423bcd0":"To do this, create a new dataset with one-hot encoding and dropping the orbit_id column.","e819a499":"Following is the table of performance evaluation for the models created.","e48028e9":"\n| Model| Accuracy  | Precision    | Recall   | F-1 Score   |\n|---:|:-------------|:-----------|:------|:------|\n| Random Forest with importance | 99.99%  | 0.98    | 1.0   | 0.99     |\n| Random Forest | 99.98%  | 0.97   | 0.99   | 0.98     |\n| Light Gradient Boosting with importance | 99.98%  | 0.96    | 0.99   | 0.98     |\n| Light Gradient Boosting | 99.98%  |  0.96  |  0.99  |   0.98   |\n| Logistic Regression |  99.11% | 0.6       | 0.98   | 0.67     |","ffb592ad":"This algorithm will be used on 2 datasets - one with all the features and one without the important features as identified by the Random Forest model.","1b4cd60e":"# 1. Data Exploration","cfd099e5":"# 4. ML Model Developing\n\nNow that the data is ready to be modeled, there are a wide range of algorithms that can be put to use. The goal is to predict if an asteroid is potentially hazardous or not. For this classification problem, we can use the following algorithms.","2fcad1f4":"The data needs to be normalised before using it to train models, so all the numeric features need to be on the same scale. For this we use min-max scaler.","895b37a8":"### 3.2 LGBM with trimmed data","b4024e5c":"Gradient Boosting Model (GBM) is a model better preferred for prediction since it combines the principles of gradient decsent and the randomness of decision trees. We can expect a better performing model with GBM, as compared to Random Forest. Due to the massive size of the data, we can choose Light Gradient Boosting model which is known for its quick performance when compared to XGBoosting.","37bd6606":"In this section we shall explore the columns of the dataframe and analyse them accordingly."}}