{"cell_type":{"1d9ecc03":"code","03e125c5":"code","09ee4343":"code","c7899793":"code","34e8fbfd":"code","05a2c65d":"code","76f45523":"code","e233c6d3":"code","4c148dee":"code","5e249249":"code","bbb3b91e":"code","239e6a7a":"code","96323e1c":"code","109c5fda":"code","2c25e99b":"code","ddc2f08f":"code","4e16fcbf":"code","3dbb91a1":"code","a3574f78":"code","bc1dc763":"code","ce7f9838":"code","72148926":"code","b9e59594":"code","d6d0e85a":"code","dcac2916":"code","54047a8e":"code","7a81d39c":"code","5a6ea289":"code","1e693995":"code","916fb501":"code","a1c4096d":"code","e4eeaa88":"code","05f2b932":"code","9d489c62":"code","74ddf9f5":"code","c66225fa":"code","d249529f":"code","7960428a":"code","3f810dea":"code","97610a62":"code","a2ac0f79":"code","8c61008f":"code","b1fcf45a":"markdown","efd4d428":"markdown","abdce8b8":"markdown","110c8d5c":"markdown","ec2bb2ce":"markdown","f70827fe":"markdown","3bace42a":"markdown","7d1d16de":"markdown","678af7e2":"markdown","852ee22f":"markdown","02328a59":"markdown","c59080bc":"markdown","22616c13":"markdown","9fcfad02":"markdown","b14d1178":"markdown","59c6a398":"markdown","3a3acb28":"markdown","4285d3f8":"markdown","a28226e5":"markdown","42f4cf2e":"markdown","01dc1e54":"markdown","92ef0959":"markdown","90de1df5":"markdown","f8f48d7d":"markdown","1ec8bdf2":"markdown"},"source":{"1d9ecc03":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","03e125c5":"#import train and test CSV files\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\n#take a look at the training data\ntrain_df.describe(include=\"all\")\n#test_df.describe(include=\"all\")","09ee4343":"print(train_df.shape,test_df.shape)","c7899793":"print(train_df.columns)","34e8fbfd":"train_df.sample(5)","05a2c65d":"train_df.info()","76f45523":"#check for any other unusable values\nprint(pd.isnull(train_df).sum())","e233c6d3":"#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_df)","4c148dee":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)","5e249249":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df)","bbb3b91e":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_df)\nplt.show()","239e6a7a":"#sort the ages into logical categories\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)","96323e1c":"train_df[\"CabinBool\"] = train_df[\"Cabin\"].notnull().astype('int')\ntest_df[\"CabinBool\"] = test_df[\"Cabin\"].notnull().astype('int')\n\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train_df)","109c5fda":"test_df.describe(include=\"all\")","2c25e99b":"train = train_df\ntest = test_df","ddc2f08f":"#we'll start off by dropping the Cabin feature since not a lot more useful information can be extracted from it.\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\n\n#we can also drop the Ticket feature since it's unlikely to yield any useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","4e16fcbf":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","3dbb91a1":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","a3574f78":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","bc1dc763":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ce7f9838":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","72148926":"# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n#I tried to get this code to work with using .map(), but couldn't.\n#I've put down a less elegant, temporary solution for now.\n#train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n#test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","b9e59594":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head()\n\n#dropping the Age feature for now, might change\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","d6d0e85a":"#drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","dcac2916":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","54047a8e":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","7a81d39c":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","5a6ea289":"#check train data\ntrain.head()","1e693995":"#check test data\ntest.head()","916fb501":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","a1c4096d":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","e4eeaa88":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","05f2b932":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","9d489c62":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","74ddf9f5":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","c66225fa":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","d249529f":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","7960428a":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","3f810dea":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","97610a62":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","a2ac0f79":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","8c61008f":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","b1fcf45a":"### Age Feature","efd4d428":"## 4) Data Visualization\nIt's time to visualize our data so we can see whether our predictions were accurate! ","abdce8b8":"## Sources:\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Scikit-Learn ML from Start to Finish](https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish?scriptVersionId=320209)\n\nAny and all feedback is welcome! ","110c8d5c":"### Name Feature\nWe can drop the name feature now that we've extracted the titles.","ec2bb2ce":"* **Numerical Features:** Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n* **Categorical Features:** Survived, Sex, Embarked, Pclass\n* **Alphanumeric Features:** Ticket, Cabin\n\n#### What are the data types for each feature?\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string\n","f70827fe":"### Sex Feature","3bace42a":"### Fare Feature\nIt's time separate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","7d1d16de":"## 7) Creating Submission File\nIt's time to create a submission.csv file to upload to the Kaggle competition!","678af7e2":"### Splitting the Training Data\nWe will use part of our training data (22% in this case) to test the accuracy of our different models.","852ee22f":"It's clear that the majority of people embarked in Southampton (S). Let's go ahead and fill in the missing values with S.","02328a59":"#### Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps. \n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","c59080bc":"I decided to use the Gradient Boosting Classifier model for the testing data.","22616c13":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages. ","9fcfad02":"### Sex Feature","b14d1178":"We can see that except for the abovementioned missing values, no NaN values exist.","59c6a398":"If you've come this far, congratulations and thank you for reading! \n\n*If you use any part of this notebook in a published kernel, credit (you can simply link back here) would be greatly appreciated. :)*","3a3acb28":"### Testing Different Models\nI will be testing the following models with my training data (got the list from [here](http:\/\/https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)):\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","4285d3f8":"## 6) Choosing the Best Model","a28226e5":"Now that we've filled in the missing values at least *somewhat* accurately (I will work on a better way for predicting missing age values), it's time to map each age group to a numerical value.","42f4cf2e":"### Embarked Feature","01dc1e54":"### Cabin Feature","92ef0959":"### Embarked Feature","90de1df5":"The code I used above is from [here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions). Next, we'll try to predict the missing Age values from the most common age for their Title.","f8f48d7d":"### Some Predictions:\n* Sex: Females are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: People of higher socioeconomic class are more likely to survive.","1ec8bdf2":"Let's compare the accuracies of each model!"}}