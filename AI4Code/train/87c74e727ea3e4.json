{"cell_type":{"d80decfc":"code","0c33281f":"code","b94bb280":"code","3f0d490d":"code","2139ab08":"code","28c84e8e":"code","a23a4f0b":"code","1fe6f667":"code","99813c34":"code","878e29f3":"code","4b04ac89":"code","de647625":"code","72e70ac2":"code","fcc0ac62":"code","c6a0c953":"markdown","0972634c":"markdown","b848335e":"markdown","c011dbcc":"markdown","d177c216":"markdown","42c945eb":"markdown","237f1948":"markdown","c2b1dfb5":"markdown","0e956fe4":"markdown"},"source":{"d80decfc":"%reset -f\n\n# install segmentation models\n! pip install segmentation_models -q","0c33281f":"%matplotlib inline\nimport gc\nimport os\nimport segmentation_models as sm\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import models, layers\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nimport logging\nlogging.getLogger('tensorflow').disabled = True\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","b94bb280":"# Set all random seeds in order to reproduce the result\n\nfrom tensorflow import random\nimport random as randm\nfrom numpy.random import seed\n\ndef seed_all_inclusive(seeding):\n    random.set_seed(seeding)\n    seed(seeding)\n    os.environ['PYTHONHASHSEED'] = str(seeding)\n    randm.seed(seeding)\n\nseeding = 1\nseed_all_inclusive(seeding) \n","3f0d490d":"img_size=512\nrepeat_epoch=10\nbackbone = 'efficientnetb2' \nzoom_pixel_fixed = img_size\/\/4\nglobal TL_FLAG\n \nradiopedia_FLAG=0\nbatch_size = 8\nprint('batch size', batch_size)\nprint('zoom_pixel_fixed.........', zoom_pixel_fixed)","2139ab08":"# use 5-fold validation\n\nn_split=5\nkfold_index=n_split\nskf = KFold(n_splits=n_split, shuffle=True, random_state=seeding)\n ","28c84e8e":"# load the data\n\nprefix = '\/kaggle\/input\/covid-segmentation\/'\nif radiopedia_FLAG:\n    images_radiopedia = np.load(os.path.join(prefix, 'images_radiopedia.npy')).astype(np.float32)\n    masks_radiopedia = np.load(os.path.join(prefix, 'masks_radiopedia.npy')).astype(np.int8)\n    \nimages_medseg = np.load(os.path.join(prefix, 'images_medseg.npy')).astype(np.float32)\nmasks_medseg = np.load(os.path.join(prefix, 'masks_medseg.npy')).astype(np.int8)","a23a4f0b":"# normalize the data between 0 and 255\ndef preprocess_images(images_arr, all_equal=True):\n    global max_val, min_val\n    if all_equal:\n        images_arr = (images_arr-min_val)\/(max_val-min_val)\n\n    images_arr = images_arr*255\n    mins = images_arr.min()\n    maxs = images_arr.max()\n    print(f'After normalization mins {mins}, maxs {maxs}')\n    return images_arr, (mins, maxs)\n\n# initalize max and min values\nglobal max_val\nglobal min_val\nmax_val = images_medseg.max()\nmin_val = images_medseg.min()\nprint('images_medseg_max {}, images_medseg_min {}'.format(images_medseg.max(), images_medseg.min()))\nprint('mask_medseg_max {}, mask_medseg_min {}'.format(masks_medseg.max(), masks_medseg.min()))\nprint('image_medseg_shape {}, mask_medseg_shape {}'.format(images_medseg.shape, masks_medseg.shape))\n\n# normalize them now\nif radiopedia_FLAG:\n    images_radiopedia, _ = preprocess_images(images_radiopedia) \nimages_medseg, _ = preprocess_images(images_medseg)","1fe6f667":"# plot with only 3 columns\n\ndef plot_images(data, col = 3, colour_map = 'gray', img_size=8, title = 'Medseg_dataset',\n                sub_title = ['Image', 'Ground_glass_mask', 'Pulmonary_consolidation_mask']):\n    if len(data)%col ==0:\n        rows = (len(data)\/\/col)\n    else:\n        rows = (len(data)\/\/col)+1\n    fig = plt.figure(figsize=(col*img_size, rows*img_size))\n    plt.suptitle(title, fontsize = 30)        \n    for i, image_ in enumerate(data):\n        fig.add_subplot(rows, col, i+1)\n        plt.imshow(data[i], cmap=colour_map)\n        plt.title(sub_title[i%col])\n    plt.show()\n    \n\n# plot few Medseg_images and corresponding Masks\n    \nplot_list = []    \nfor i in range(2):\n    plot_list.append(images_medseg[i].reshape(512,512))\n    plot_list.append((masks_medseg[i][..., 0]*255).reshape(512,512))  # scale images between 0 and 255\n    plot_list.append((masks_medseg[i][..., 1]*255).reshape(512,512))   # scale images between 0 and 255  \n\nplot_images(plot_list)","99813c34":"# Consoltidated F1 score\n\ndef fscore_glass_and_consolidation(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., :2], \n                               y_pred[..., :2])  \n    \n\n# from keras\ndef history_plot(history):\n    plt.rcParams.update({'font.size': 16})\n    hist = pd.DataFrame(history.history)\n    fig, (ax1) = plt.subplots(figsize=(12,12),nrows=1, ncols=1)\n    hist['loss'].plot(ax=ax1,c='k',label='training loss')\n    hist['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')\n    ax1.legend()\n    plt.ylabel('Categorical crossentropy loss',size=14)\n    plt.xlabel('Epoch',size=14)\n\n    plt.show()  \n\n\n","878e29f3":"def get_model_trainable(img_size, kfold_index):\n    input_main = tf.keras.Input(shape=((img_size,img_size, 1)))\n    input_concatenated= tf.keras.layers.concatenate([input_main, input_main, input_main], axis =3)\n    input_scaled = sm.get_preprocessing(backbone)(input_concatenated)\n\n    model = sm.FPN(backbone_name=backbone,\n                      encoder_weights='imagenet',\n                      classes=4, \n                      activation='softmax')\n\n    output = model(input_scaled)\n\n    model = tf.keras.Model(inputs=input_main, outputs=output, name = 'fpn4')\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4), loss = 'categorical_crossentropy', \n                  metrics=[fscore_glass_and_consolidation])   \n    return model\n\n# Since we are using transfer learninso, so we kept Adam's inital learning rate at 1e-4","4b04ac89":"# function to reduce learning rate and for earlystopping\n\ndef callback_func_finetuner(model_path):\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path,\n                                                    verbose=1,monitor='val_fscore_glass_and_consolidation',\n                                                    mode='max',save_best_only=True)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_fscore_glass_and_consolidation',mode = 'max', \n                                                  patience=40, restore_best_weights=True)\n    reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                                                  patience = 4, min_lr=0.00001, \n                                                  mode = 'min', verbose = 1)\n    return checkpoint, early_stop, reduce\n\n","de647625":"radiopedia_FLAG=0\n\nseed_all_inclusive(seeding)  \nseeding = seeding+1  \n\ndef train_aug(image, mask, augment = True):\n    global seeding \n    global img_size\n    global TL_FLAG\n    seeding = seeding+1\n    \n    mask = tf.cast(mask, tf.float32) \n    image = tf.cast(image, tf.float32) \n    if augment: \n        rand_num = tf.random.uniform((), seed = seeding+1)       \n        if  rand_num > 0.5: #  random cropping\n            x_offset = tf.random.uniform([],  minval=0, maxval=zoom_pixel_fixed, dtype=tf.dtypes.int32, seed = seeding+2)\n            y_offset = tf.random.uniform([],  minval=0, maxval=zoom_pixel_fixed, dtype=tf.dtypes.int32, seed = seeding+3)\n\n            image = tf.image.crop_to_bounding_box(image, y_offset, x_offset, img_size-zoom_pixel_fixed, img_size-zoom_pixel_fixed)\n            mask = tf.image.crop_to_bounding_box(mask, y_offset, x_offset, img_size-zoom_pixel_fixed, img_size-zoom_pixel_fixed)\n            \n        if tf.random.uniform((), seed = seeding) > 0.5: \n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n\n        if tf.random.uniform((), seed = seeding+1) > 0.5:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n\n        if tf.random.uniform((), seed = seeding+2) > 0.5:\n            image = tf.image.rot90(image, k=1)\n            mask = tf.image.rot90(mask, k=1) \n                           \n        image = tf.image.random_brightness(image, 0.2, seed = seeding+4)\n        image = tf.image.random_contrast(image, 0.8, 1.2, seed = seeding+5)  \n        \n        mask= tf.image.resize(mask, [img_size,img_size])    \n        image= tf.image.resize(image, [img_size,img_size])  \n        \n      \n    mask = tf.reshape(mask, [img_size,img_size, 4])\n    image = tf.reshape(image, [img_size,img_size, 1])\n    \n    if mask.shape[2]!=4:\n        print(mask.shape, 'error mask')\n    if image.shape[2]!=1:\n        print(image.shape, 'error  image')\n \n    return image,  mask\n\n\ndef val_aug(name_images, label):\n    image,  mask = train_aug(name_images, label, augment = False)\n    return image,  mask\n\ndef to_train_tfdata(train_paths, train_labels, bsize, repeat_num= 30, seeds = seeding):\n    AUTO = tf.data.experimental.AUTOTUNE\n    dset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n    dset = dset.shuffle(buffer_size=49321, seed=seeding)\n    dset = dset.map(train_aug, num_parallel_calls=AUTO)\n    dset = dset.batch(bsize, drop_remainder=True).repeat(repeat_num).prefetch(AUTO) # overlaps data preprocessing and model execution while training\n    return dset\n\ndef val_tfdata( valid_paths, valid_labels, bsize):\n    AUTO = tf.data.experimental.AUTOTUNE\n    dset = tf.data.Dataset.from_tensor_slices((valid_paths, valid_labels))\n    dset = dset.map(val_aug, num_parallel_calls=AUTO)\n    dset = dset.repeat(1) \n    dset = dset.batch(bsize).prefetch(AUTO) # overlaps data preprocessing and model execution while training\n    return dset\n\n","72e70ac2":"# plot original image and mask without augmentation    \ndvalid = val_tfdata(images_medseg[:1], masks_medseg[:1], bsize= 1)\nimage, label = next(iter(dvalid))    \nplot_list = []    \nfor i in range(1):\n    plot_list.append(image.numpy().reshape(512,512))\n    plot_list.append((label.numpy()[..., 0]*255).reshape(512,512))  # scale images between 0 and 255\n    plot_list.append((label.numpy()[..., 1]*255).reshape(512,512))   # scale images between 0 and 255  \nplot_images(plot_list, title = 'Original Image and corresponding mask')\ndel dvalid\n\n\nprint('')\n# plot different augmentation of above image and mask   \nfor i in range(5):\n    plot_list = []    \n    dtrain = to_train_tfdata(images_medseg[:1], masks_medseg[:1], bsize= 1, repeat_num=1,  seeds = seeding+i)\n    image, label = next(iter(dtrain)) \n    plot_list.append(image.numpy().reshape(512,512))\n    plot_list.append((label.numpy()[..., 0]*255).reshape(512,512))  # scale images between 0 and 255\n    plot_list.append((label.numpy()[..., 1]*255).reshape(512,512))   # scale images between 0 and 255  \n    plot_images(plot_list, title = f'Augmentation {i+1}')\n    print('')\n","fcc0ac62":"kfold_index_temp=kfold_index\nfor train_indexes, val_indexes in skf.split(range(100)):\n    print('kfold_index_temp............', kfold_index_temp)\n    kfold_index_temp=kfold_index_temp-1\n    #   Split data in train and validation with kfold \n        # Add radiopedia data also if radiopedia_FLAG is true\n    if radiopedia_FLAG:\n        train_images = np.concatenate((images_medseg[train_indexes], images_radiopedia))\n        train_masks = np.concatenate((masks_medseg[train_indexes], masks_radiopedia))\n    else:\n        train_images = images_medseg[train_indexes]\n        train_masks = masks_medseg[train_indexes]\n    val_images = images_medseg[val_indexes]\n    val_masks = masks_medseg[val_indexes]\n    \n    #   initialize model checkpoints\n    checkpoint, early_stop, reduce= callback_func_finetuner('covid_model_' + str(kfold_index_temp)+ '.h5')\n  \n    #   prepare data for training and testing pipline\n    TL_FLAG='supervised'\n    print('.........TL_FLAG...........', TL_FLAG)\n    dtrain = to_train_tfdata(train_images, train_masks, bsize= batch_size, repeat_num=repeat_epoch,  seeds = seeding)\n    dvalid = val_tfdata(val_images, val_masks, bsize= batch_size)\n    \n    #  Initialize the FPN model with imagenet weights\n    model = get_model_trainable(img_size, kfold_index)  \n    model.summary() \n    seeding = seeding+1\n\n    # Flush the memory and model training\n    K.clear_session()\n    seed_all_inclusive(seeding)  \n    gc.collect()\n\n    history = model.fit(\n        dtrain,\n        epochs = repeat_epoch,\n        steps_per_epoch = train_masks.shape[0]\/\/batch_size,\n        callbacks = [checkpoint, reduce,early_stop],\n        validation_data = dvalid,\n        verbose = 1,\n        validation_steps = val_masks.shape[0] \/\/ batch_size,\n    )     \n    history_plot(history)  \n    \n#     free up the memory before next fold training\n\n    del model\n    del dtrain, dvalid, train_images, train_masks, val_images, val_masks\n    del checkpoint, reduce,early_stop\n    K.clear_session()\n    seed_all_inclusive(seeding)  \n    gc.collect()","c6a0c953":"# Image Segmentation model 'FPN'\n","0972634c":"# Import essential libraries","b848335e":"# Data Augmentation and data pipline","c011dbcc":"# Data augmentation verification","d177c216":"# Output five segmentation models as we did 5-Fold Validation","42c945eb":"# Initialize all necessary global variables\n","237f1948":"# Data plotting","c2b1dfb5":"# Model fit","0e956fe4":"# Data Normalization"}}