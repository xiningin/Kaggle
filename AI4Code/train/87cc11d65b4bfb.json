{"cell_type":{"3d764970":"code","031e0012":"code","b3e145e3":"code","ecb2420f":"code","1e0fb3b7":"code","b2b4f307":"code","f0bcd2de":"code","3576d9a3":"code","938bdb17":"code","1b6b91e0":"code","9e7d8b2c":"code","60b91204":"code","0d4c38de":"code","b83e58a2":"code","f3f15b92":"code","0ada3f73":"code","b624ef8c":"code","143578a4":"code","d3b64f32":"code","cf205990":"code","97933d98":"code","2b08691e":"code","9ea5be62":"code","be7dedb2":"code","091813ef":"code","5944938f":"code","51b0ff46":"code","3d6d5550":"code","24257bad":"code","081ceb97":"code","c72c72a2":"code","bc1c6214":"code","7f191445":"code","5972f796":"code","a5d55029":"code","17f90495":"code","4ae6114f":"code","b6d3f038":"code","861bdddc":"code","a07c10ea":"code","94641b25":"code","8dddebb3":"code","11ba94c5":"code","a345c26d":"markdown","2d4594f4":"markdown","7b31dd14":"markdown","ae768a95":"markdown","55241fe5":"markdown","071de89e":"markdown","6875eecf":"markdown","daf840a2":"markdown","8262ed56":"markdown","7de0a73c":"markdown","e454bf0b":"markdown"},"source":{"3d764970":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom tqdm.notebook import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","031e0012":"df = pd.read_csv('\/kaggle\/input\/smile-tweeter\/smileannotationsfinal.csv',\n                names=['id', 'tweet', 'category'])\ndf.set_index('id', inplace=True)\ndf.info()","b3e145e3":"df.head(10)","ecb2420f":"df.shape","1e0fb3b7":"df.tweet.iloc[0]","b2b4f307":"df.category.value_counts()","f0bcd2de":"def label_cleaning(df):\n    df = df[~df.category.str.contains('\\|')]\n    df = df[df.category != 'nocode']\n    return df","3576d9a3":"df = label_cleaning(df)\ndf.category.value_counts()","938bdb17":"df.shape","1b6b91e0":"possible_labels = df.category.unique()\nlabel_dict = {}\nfor index, label in enumerate(possible_labels):\n    label_dict[label] = index","9e7d8b2c":"label_dict","60b91204":"df['label'] = df.category.replace(label_dict)","0d4c38de":"df.head(10)","b83e58a2":"from sklearn.model_selection import train_test_split","f3f15b92":"X_train, X_val, y_train, y_val = train_test_split(\n        df.index.values,\n        df.label.values,\n        test_size=0.15,\n        random_state=17,\n        stratify=df.label.values\n)","0ada3f73":"df['data_type'] = ['not_set']*df.shape[0]","b624ef8c":"df.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'","143578a4":"df.groupby(['category','label','data_type']).count()","d3b64f32":"from transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset","cf205990":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","97933d98":"encoded_data_train = tokenizer.batch_encode_plus(\n    df[df.data_type=='train'].tweet.values,\n    add_special_token=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df.data_type=='val'].tweet.values,\n    add_special_token=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\n","2b08691e":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df[df.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type=='val'].label.values)","9ea5be62":"dataset_train = TensorDataset(input_ids_train,attention_masks_train,labels_train)\ndataset_val = TensorDataset(input_ids_val,attention_masks_val,labels_val)","be7dedb2":"len(dataset_train)","091813ef":"len(dataset_val)","5944938f":"from transformers import BertForSequenceClassification","51b0ff46":"model = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(label_dict),\n    output_attentions=False,\n    output_hidden_states=False\n)","3d6d5550":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler","24257bad":"batch_size = 4 #should be 32\n\ndataloader_train = DataLoader(\n    dataset_train,\n    sampler=RandomSampler(dataset_train),\n    batch_size = batch_size\n)\n\ndataloader_val = DataLoader(\n    dataset_val,\n    sampler=RandomSampler(dataset_val),\n    batch_size = 32\n)","081ceb97":"from transformers import AdamW, get_linear_schedule_with_warmup","c72c72a2":"optimizer = AdamW(\n    model.parameters(),\n    lr=1e-5, #2e-5 -> 5e-5\n    eps=1e-8\n)","bc1c6214":"epochs = 10\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(dataloader_train)*epochs\n)","7f191445":"from sklearn.metrics import f1_score","5972f796":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","a5d55029":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')","17f90495":"import random\n\n#for using gpu\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","4ae6114f":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(device)","b6d3f038":"def evaluate(dataloader_val):\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n                    'input_ids': batch[0],\n                    'attention_mask': batch[1],\n                    'labels': batch[2]\n                 }\n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val)\n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n    \n    return loss_val_avg, predictions, true_vals","861bdddc":"for epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    \n    loss_train_total = 0\n    \n    progress_bar = tqdm(\n                        dataloader_train,\n                        desc='Epoch {:1d}'.format(epoch),\n                        leave=False,\n                        disable=False\n                        )\n    for batch in progress_bar:\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n                    'input_ids': batch[0],\n                    'attention_mask': batch[1],\n                    'labels': batch[2]\n                 }\n        outputs = model(**inputs)\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n   \n    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n    \n    tqdm.write('\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)\n    \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    \n    val_f1 = f1_score_func(predictions, true_vals)\n    \n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 score (weighted): {val_f1}')","a07c10ea":"model.to(device)\npass","94641b25":"model.load_state_dict(\n                        torch.load('.\/BERT_ft_epoch3.model',\n                        map_location=torch.device('cpu'))\n)","8dddebb3":"_,predictions,true_vals = evaluate(dataloader_val)","11ba94c5":"accuracy_per_class(predictions, true_vals)","a345c26d":"**Loading tokenizer and encoding our data**","2d4594f4":"**Optimizer and Scheduler**","7b31dd14":"**Training\/Valiation split**","ae768a95":"Assign numbers to labels","55241fe5":"**Modeling**","071de89e":"**Performance metrics**","6875eecf":"**Create Data loader**","daf840a2":"**Create training loop**","8262ed56":"**Prepare train-val dataset**","7de0a73c":"# **Here is the code of another [course](https:\/\/www.coursera.org\/projects\/sentiment-analysis-bert) I took from Coursera**","e454bf0b":"Clean the label"}}