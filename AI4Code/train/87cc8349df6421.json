{"cell_type":{"1f50ad18":"code","67c104f4":"code","121917c8":"code","eabfebac":"code","5d01a547":"code","c48cfde8":"code","0b1f6a4b":"code","4dc81040":"code","cb0c64ce":"code","7543c4b0":"markdown","549dbcdf":"markdown","4b28b391":"markdown","34a8c116":"markdown","4077fa84":"markdown"},"source":{"1f50ad18":"import json\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport re\nfrom tqdm import tqdm\nimport nltk\nimport random\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n\ntrain_example_paths = glob.glob('..\/input\/coleridgeinitiative-show-us-the-data\/train\/*.json')\ntrain_example_names = [fn.split('.')[0] for fn in os.listdir('..\/input\/coleridgeinitiative-show-us-the-data\/train')]\n\nmetadata = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ndocIdx = train_example_names.copy()","67c104f4":"def load_train_example_by_name(name):\n    doc_path = os.path.join('..\/input\/coleridgeinitiative-show-us-the-data\/train', name + '.json')\n    with open(doc_path) as f:\n        data = json.load(f)\n    return data\n\ndef text_cleaning(text):\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n\n    text = re.sub(r'\\[[0-9]+]', ' specialreference ', text)\n\n    # Remove years\n    text = re.sub(r'(19|20)[0-9][0-9]', ' specialyear ', text)\n\n    # remove other digits\n    text = re.sub(r'\\d+', ' ', text)\n\n    # remove extra spaces\n    text = re.sub(\"\\s+\",\" \", text)\n\n    # Remove websites\n    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n\n    return text.lower()","121917c8":"import string\n\ntemp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\ntemp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\ntemp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n\nexisting_labels = temp_1 + temp_2 + temp_3\nexisting_labels = [l.lower() for l in existing_labels]\nexisting_labels = list(set(existing_labels))\n\n# Sort labels by length in descending order\nexisting_labels = sorted(existing_labels, key = len, reverse= True)","eabfebac":"pos_sentences = []\nneg_sentences = []\ndoc_label_section_idx = []\ndoc_label_sentence_idx = []\ndoc_label_list = []\nfirst_label_sec_name = []\nfirst_labels = []\nn_secs = []\n\ndef process_doc(doc_id):\n    doc_json = load_train_example_by_name(doc_id)\n    this_doc_label_section_idx = []\n    this_doc_label_sentence_idx = []\n    this_doc_label_list = []\n    i_doc_sent = -1\n    doc_first_label = True\n    n_secs.append(len(doc_json))\n\n    for i_sec, section in enumerate(doc_json):\n        \n        sentences = sent_tokenize(section['text'])\n\n        adni_count = 0\n        for sentence in sentences:\n            i_doc_sent += 1\n            clean_sentence = text_cleaning(sentence)\n\n            has_label = False\n            label_is_adni = False\n            for clean_label in existing_labels:\n                if clean_label in clean_sentence:\n                    if doc_first_label:\n                        first_label_sec_name.append(section['section_title'])\n                        first_labels.append(clean_label)\n                        doc_first_label = False\n\n                    has_label = True\n                    this_doc_label_section_idx.append(i_sec)\n                    this_doc_label_sentence_idx.append(i_doc_sent)\n                    this_doc_label_list.append(clean_label)\n                    clean_sentence = clean_sentence.replace(clean_label, '')\n\n    doc_label_section_idx.append(this_doc_label_section_idx)\n    doc_label_sentence_idx.append(this_doc_label_sentence_idx)\n    doc_label_list.append(this_doc_label_list)\n    if doc_first_label:\n        first_label_sec_name.append('NOT FOUND')\n        first_labels.append('NOT FOUND')","5d01a547":"for doc_id in tqdm(docIdx):\n    process_doc(doc_id)","c48cfde8":"def process_sec_name(text):\n    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n\n    # remove extra spaces\n    text = re.sub(\"\\s+\",\" \", text)\n\n    text = ' '.join([t for t in text.split(' ') if len(t) > 1])\n\n    return text.lower()\n\nsection_order = pd.Series(first_label_sec_name).value_counts().to_frame().reset_index()\nsection_order.columns = ['sec_name', 'cnt']\n\nsection_order.sec_name = section_order.sec_name.apply(lambda x: process_sec_name(x))\nsection_order = section_order.groupby('sec_name')['cnt'].sum().to_frame().reset_index()\nsection_order.columns = ['sec_name', 'cnt']\nsection_order = section_order.loc[section_order.sec_name.str.len() > 0]\n\n# Consolidate rows that contain 'data'\nsec_cons = section_order.sec_name.str.contains('data') | section_order.sec_name.str.contains('sample')\ncount_sum_data = section_order.loc[sec_cons, 'cnt'].sum()\nsection_order = section_order.loc[~sec_cons].reset_index(drop = True)\nsection_order.loc[len(section_order)] = ['data', count_sum_data]\n\n# Consolidate rows that contain 'study'\nsec_cons = section_order.sec_name.str.contains('study')\ncount_sum_data = section_order.loc[sec_cons, 'cnt'].sum()\nsection_order = section_order.loc[~sec_cons].reset_index(drop = True)\nsection_order.loc[len(section_order)] = ['study', count_sum_data]\n\nsection_order = section_order.loc[section_order.cnt > 10]\nsection_order = section_order.sort_values(by = 'cnt', ascending= False).reset_index(drop = True)\n\nsection_order.to_csv('section_order.csv', index = False)","0b1f6a4b":"section_order","4dc81040":"def sort_doc_sections(doc_secs):\n    # doc_secs must be a list of dicts with field'section_title'\n    for sec in doc_secs:\n        section_title = process_sec_name(sec['section_title'])\n        if len(section_title) < 4:\n            sec['score'] = 0\n        else:\n            sec_scores = section_order.loc[section_order.sec_name.str.contains(section_title) |\\\n                                           section_order.sec_name.apply(lambda x: x in section_title), 'cnt']\n            # sum scores of all matches\n            result_score = sec_scores.sum() if len(sec_scores) > 0 else 0\n\n            sec['score'] = result_score\n\n    return sorted(doc_secs, key = lambda x: x['score'], reverse = True)","cb0c64ce":"ex_i = 100\n\ndoc_json = load_train_example_by_name(train_example_names[ex_i])\ndoc_json = [{'section_title': s['section_title']} for s in doc_json]\n\nprint(f'Section: {first_label_sec_name[ex_i]}')\nprint(f'Dataset Name: {first_labels[ex_i]}')\nsort_doc_sections(doc_json)","7543c4b0":"## Sort Sections","549dbcdf":"## Example","4b28b391":"### In this notebook, I sorted sections by their title score.\n\n- Each title is scored by how many times sections with that title contained the first occurence of a dataset name.\n\n- The idea is, when you sort sections like this, uppermost sections have a higher chance of having a dataset name in them.\n\n- existing_labels: code from https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/discussion\/232964","34a8c116":"## Get Section Title Info","4077fa84":"## Extract Information"}}