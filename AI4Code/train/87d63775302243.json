{"cell_type":{"5e9f416e":"code","f83b5d4a":"code","fe0f8d93":"code","b5b6fe8b":"code","fb8c0f91":"code","47f2c0a4":"code","35f57781":"code","dffe7fd0":"code","8c427915":"code","c6a4d2ff":"code","bfc1daea":"code","7710a629":"code","1b486074":"code","162f1ff4":"code","c7f7340e":"code","a06adb31":"code","67efa6be":"code","6822c4bb":"code","589c3192":"code","bb41f08a":"code","84d4af4f":"code","3fdb7e4e":"code","07b05e05":"code","eeb240f4":"code","83a8ddc2":"code","4b28632d":"code","7679b256":"code","dfd50d19":"code","bd7f4ac9":"code","bd94ba72":"code","92aef61d":"code","b83759f5":"code","ab6ddf40":"code","c258c8c9":"code","8b638c8b":"code","b0e95815":"code","39dcc5f2":"code","a516702c":"code","1359ddbe":"code","9542ed32":"code","1f308c45":"code","c5f26799":"code","e593e8be":"code","31019d42":"code","51eca4cc":"code","08ef02e1":"code","54f6bea2":"code","9b170e75":"code","5cf5f5d7":"code","de58f516":"code","311f8d14":"code","a9f8c4e5":"code","95176221":"code","3a56e533":"code","05120cd8":"code","7f1b3ae8":"code","508d94f3":"code","ae4fca2b":"code","b7dd4c94":"code","a7258108":"code","269d591a":"code","a084c542":"code","5c624562":"code","d0559166":"markdown","210ba922":"markdown","d28ed92d":"markdown","e1048ecc":"markdown","73840e2b":"markdown","03cc909a":"markdown","aa794d90":"markdown","fa48c14e":"markdown","1ff96e26":"markdown","b01fb046":"markdown","8a3a1b11":"markdown","c8a954d1":"markdown","319b88ec":"markdown","923419a8":"markdown","736a40da":"markdown","c078e5d8":"markdown","99fe0a33":"markdown","98a9f862":"markdown","de56340a":"markdown","f295f9ad":"markdown","f51d8502":"markdown","73b36337":"markdown","7b016336":"markdown","31d551e4":"markdown","9cd117ea":"markdown","447c9bce":"markdown","57e64d81":"markdown","793825ca":"markdown","1ad9d5ab":"markdown","6b2de33d":"markdown","746419ba":"markdown","74f8b4c7":"markdown","a9403b60":"markdown","9fcc2df0":"markdown","023cd50a":"markdown","42ba5711":"markdown","8fd874eb":"markdown","a9a0596b":"markdown","372f4b66":"markdown","c2157582":"markdown","661f6eb5":"markdown","d91c8764":"markdown","71403eb4":"markdown","631e9ce3":"markdown","ba9c5b7e":"markdown"},"source":{"5e9f416e":"from IPython.display import YouTubeVideo\nYouTubeVideo('Slq2zeofV0w')","f83b5d4a":"!ls ..\/input","fe0f8d93":"# Install deps\n\n!pip  install -q tensor2tensor\n!pip  install -q  tensorflow matplotlib\n!pip install -q bert-tensorflow","b5b6fe8b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport random\nimport warnings\nwarnings.filterwarnings('ignore',category=FutureWarning)\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n\nfrom bert.tokenization import FullTokenizer\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras import backend as K\n","fb8c0f91":"# setting multiple seeds because randomness can occur at multiple places\n# still the Keras and Bert results are not reproducible\n# you should run the experiments multiple times in order to get reliable results\n# see also https:\/\/machinelearningmastery.com\/reproducible-results-neural-networks-keras\/\n\nrandom.seed(1)\nnp.random.seed(1)\n\ntf.random.set_random_seed(seed = 2)\n\n","47f2c0a4":"path = '..\/input\/imdb-movie-reviews-dataset\/aclimdb\/'","35f57781":"def shuffle(X, y):\n    perm = np.random.permutation(len(X))\n    X = X[perm]\n    y = y[perm]\n    return X, y","dffe7fd0":"def load_imdb_dataset(path):\n    imdb_path = os.path.join(path, 'aclImdb')\n\n    # Load the dataset\n    train_texts = []\n    train_labels = []\n    test_texts = []\n    test_labels = []\n    for dset in ['train', 'test']:\n        for cat in ['pos', 'neg']:\n            dset_path = os.path.join(imdb_path, dset, cat)\n            for fname in sorted(os.listdir(dset_path)):\n                if fname.endswith('.txt'):\n                    with open(os.path.join(dset_path, fname)) as f:\n                        if dset == 'train': train_texts.append(f.read())\n                        else: test_texts.append(f.read())\n                    label = 0 if cat == 'neg' else 1\n                    if dset == 'train': train_labels.append(label)\n                    else: test_labels.append(label)\n\n    # Converting to np.array\n    train_texts = np.array(train_texts)\n    train_labels = np.array(train_labels)\n    test_texts = np.array(test_texts)\n    test_labels = np.array(test_labels)\n\n    # Shuffle the dataset\n    train_texts, train_labels = shuffle(train_texts, train_labels)\n    test_texts, test_labels = shuffle(test_texts, test_labels)\n\n    # Return the dataset\n    return train_texts, train_labels, test_texts, test_labels","8c427915":"trainX, trainY, testX, testY = load_imdb_dataset(path)\n\nprint ('Train samples shape :', trainX.shape)\nprint ('Train labels shape  :', trainY.shape)\nprint ('Test samples shape  :', testX.shape)\nprint ('Test labels shape   :', testY.shape)","c6a4d2ff":"total = 0\nfor text, num_label in zip(trainX[:10], trainY[:10]):\n    print(num_label, text[:300].split('\\n')[0])\n    total = total + len(text.split(\" \"))\n\nprint(\"Average: \"+str(total\/10))","bfc1daea":"# count the words\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\ndocs = pd.DataFrame([['This is a text.'],['This is a longer text.'],['This text is longer.']],columns=['Text'])","7710a629":"countVec =  CountVectorizer(stop_words={})\ncounts = countVec.fit_transform(docs[\"Text\"])\n\n\nx_traincv_df = pd.DataFrame(counts.toarray(),index = docs[\"Text\"],columns=list(countVec.get_feature_names()))\nx_traincv_df","1b486074":"tfidfVec = TfidfVectorizer()\ntfidf = tfidfVec.fit_transform(docs[\"Text\"])\n\nx_traincv_df = pd.DataFrame(tfidf.toarray(),index = docs[\"Text\"],columns=list(tfidfVec.get_feature_names()))\nx_traincv_df","162f1ff4":"tfidfVec = TfidfVectorizer(ngram_range=[1,2],stop_words=stopwords.words('english'))\ntfidf = tfidfVec.fit_transform(docs[\"Text\"])\nx_traincv_df = pd.DataFrame(tfidf.toarray(),index = docs[\"Text\"],columns=list(tfidfVec.get_feature_names()))\nx_traincv_df","c7f7340e":"results = pd.DataFrame(index = ['NB-count','NB-tfidf','NB-tfidf stopwords','NB-tfidf stopwords bigrams','CNN-LSTM','BERT'],\n                       columns=['Accuracy'])","a06adb31":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\ndef train(classifier, X, y, X_test, y_test):\n    ### provide classifier, train and test set\n    ### get train\/val split\n    ### fit on val\n    ### test on test\n    ### return accuracy score for test\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=33)\n    print(\"validation results:\")\n    classifier.fit(X_train, y_train)\n    print(classification_report(y_val, classifier.predict(X_val)))\n    \n    print(\"test results:\")\n    X_test_preds = classifier.predict(X_test)\n    print(classification_report(y_test, X_test_preds) )\n    return accuracy_score(y_test,X_test_preds)","67efa6be":"\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\n\n\n\ntrial1 = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('classifier', MultinomialNB()),\n])\n \nacc = train(trial1, trainX, trainY, testX, testY)\n\nresults.at['NB-count','Accuracy'] = acc\n","6822c4bb":"\n\ntrial2 = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', MultinomialNB()),\n])\n \nacc = train(trial2, trainX, trainY, testX, testY)\n\n\nresults.at['NB-tfidf','Accuracy'] = acc","589c3192":"\n \ntrial3 = Pipeline([\n    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),\n    ('classifier', MultinomialNB()),\n])\n \nacc = train(trial3, trainX, trainY, testX, testY)\n\n\nresults.at['NB-tfidf stopwords','Accuracy'] = acc","bb41f08a":"\n\ntrial4 = Pipeline([\n    ('vectorizer', TfidfVectorizer(ngram_range =[1,2],stop_words=stopwords.words('english'))),\n    ('classifier', MultinomialNB(alpha=0.05)),\n])\n\nacc = train(trial4, trainX, trainY, testX, testY)\n\n\nresults.at['NB-tfidf stopwords bigrams','Accuracy'] = acc","84d4af4f":"trial5 = Pipeline([\n    ('vectorizer', TfidfVectorizer(ngram_range =[1,2],stop_words=stopwords.words('english'))),\n    ('classifier', LogisticRegression()),\n])\n\nacc = train(trial5, trainX, trainY, testX, testY)\n\nresults.at['LR stopwords bigrams','Accuracy'] = acc","3fdb7e4e":"results","07b05e05":"ax = sns.barplot(x=results.index, y=results[\"Accuracy\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","eeb240f4":"# import numpy as np\n# import pandas as pd\n# from sklearn.manifold import TSNE\n# from bokeh.io import push_notebook, show, output_notebook\n# from bokeh.plotting import figure\n# from bokeh.models import ColumnDataSource, LabelSet\n# output_notebook()\n \n# X = []\n# for word in embeddings_index.keys():\n#     X.append(embeddings_index[word])\n\n# X = np.array(X)\n# print(\"Computed X: \", X.shape)\n# X_embedded = TSNE(n_components=2, n_iter=250, verbose=2).fit_transform(X)\n# print(\"Computed t-SNE\", X_embedded.shape)\n \n# df = pd.DataFrame(columns=['x', 'y', 'word'])\n# df['x'], df['y'], df['word'] = X_embedded[:,0], X_embedded[:,1], w2v_model2.wv.vocab\n \n# source = ColumnDataSource(ColumnDataSource.from_df(df))\n# labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n#                   text_font_size=\"8pt\", text_color=\"#555555\",\n#                   source=source, text_align='center')\n \n# plot = figure(plot_width=600, plot_height=600)\n# plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n# plot.add_layout(labels)\n# show(plot, notebook_handle=True)\n \n","83a8ddc2":"GLOVE_DIR = '..\/input\/glove-global-vectors-for-word-representation\/'\nMAX_SEQUENCE_LENGTH = 256\nMAX_NUM_WORDS = 1000000\nEMBEDDING_DIM = 200\n","4b28632d":"\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Bidirectional\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten, LSTM\nfrom keras.models import Model\nfrom keras.initializers import Constant\n\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(trainX)\n\nX_train, X_val, y_train, y_val = train_test_split(trainX, trainY, test_size=0.25, random_state=33)\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_val = tokenizer.texts_to_sequences(X_val)\nsequences_test = tokenizer.texts_to_sequences(testX)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\nX_val_seq = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH)\nX_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n\ny_train_labels = to_categorical(np.asarray(y_train))\ny_val_labels = to_categorical(np.asarray(y_val))\ny_test_labels = to_categorical(np.asarray(testY))\n\nprint('Shape of data tensor:', X_train_seq.shape)\nprint('Shape of label tensor:', y_train_labels.shape)\n\n","7679b256":"# Initialize session\n#sess = tf.Session()\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","dfd50d19":"embeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt')) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors.' % len(embeddings_index))","bd7f4ac9":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","bd94ba72":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","92aef61d":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = Dropout(0.2)(x)\nx = MaxPooling1D(5)(x)\nx = Bidirectional(LSTM(64, activation='relu'))(x)\nx = Dropout(0.2)(x)\n# x = MaxPooling1D(5)(x)\n# x = Conv1D(128, 5, activation='relu')(x)\n# x = MaxPooling1D(5)(x)  # global max pooling\n# x = Flatten()(x)\n# x = Dense(64, activation='relu')(x)\npreds = Dense(2, activation='softmax')(x)\n\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\n# happy learning!\nhistory= model.fit(X_train_seq, y_train_labels, validation_data=(X_val_seq, y_val_labels),\n         epochs=4, batch_size=128)","b83759f5":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ab6ddf40":"val_preds = model.predict(X_val_seq, batch_size = 128)\ntest_preds = model.predict(X_test_seq, batch_size = 128)","c258c8c9":"label_val_preds = []\nlabel_test_preds = []\nfor pred in val_preds:\n    label_val_preds.append(pred.argmax(axis=0))\nfor pred in test_preds:\n    label_test_preds.append(pred.argmax(axis=0))","8b638c8b":"print(\"validation results:\")\nprint(classification_report(y_val, label_val_preds))\nprint(\"test results:\")\nprint(classification_report(testY, label_test_preds))","b0e95815":"\nacc = accuracy_score(testY,label_test_preds)\nprint(acc)\nresults.at['CNN-LSTM','Accuracy'] = acc\n","39dcc5f2":"import bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization","a516702c":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom datetime import datetime\n# This is a path to an uncased (all lowercase) version of BERT\nBERT_MODEL_HUB = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"\n#https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-24_H-1024_A-16.zip\ndef create_tokenizer_from_hub_module():\n  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT_MODEL_HUB)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    with tf.Session() as sess:\n      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                    tokenization_info[\"do_lower_case\"]])\n      \n  return bert.tokenization.FullTokenizer(\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntokenizer = create_tokenizer_from_hub_module()","1359ddbe":"tokenizer.tokenize(\"I like strawberries.\")","9542ed32":"#Input: the man went to the [MASK1] . he bought a [MASK2] of milk.\n\n#Labels: [MASK1] = store; [MASK2] = gallon","1f308c45":"\n\n\n# Params for bert model and tokenization\nbert_path = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"\nmax_seq_length = 256","c5f26799":"# Create datasets (Only take up to max_seq_length words for memory)\ntrain_text = trainX\ntrain_text = [' '.join(t.lower().split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = trainY\n\ntest_text = testX\ntest_text = [' '.join(t.lower().split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]\ntest_label = testY","e593e8be":"trainX[0:10]","31019d42":"train = pd.DataFrame(columns=['review','polarity'])\ntrain['review'] = trainX\ntrain['polarity'] = trainY\n\ntest = pd.DataFrame(columns=['review','polarity'])\ntest['review'] = testX\ntest['polarity'] = testY","51eca4cc":"DATA_COLUMN = 'review'\nLABEL_COLUMN = 'polarity'\n# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\nlabel_list = [0, 1]","08ef02e1":"train.head(10)","54f6bea2":"# train = train.sample(1000)\n# test = test.sample(1000)","9b170e75":"# Use the InputExample class from BERT's run_classifier code to create examples from the data\ntrain_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\ntest_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","5cf5f5d7":"# We'll set sequences to be at most 256 tokens long.\nMAX_SEQ_LENGTH = 256\n# Convert our train and test features to InputFeatures that BERT understands.\ntrain_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\ntest_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","de58f516":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n  \"\"\"Creates a classification model.\"\"\"\n\n  bert_module = hub.Module(\n      BERT_MODEL_HUB,\n      trainable=True)\n  bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n  bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n  # Use \"pooled_output\" for classification tasks on an entire sentence.\n  # Use \"sequence_outputs\" for token-level output.\n  output_layer = bert_outputs[\"pooled_output\"]\n\n  hidden_size = output_layer.shape[-1].value\n\n  # Create our own layer to tune for politeness data.\n  output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(\"loss\"):\n\n    # Dropout helps prevent overfitting\n    output_layer = tf.nn.dropout(output_layer, rate=0.1)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # Convert labels into one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n    # If we're predicting, we want predicted labels and the probabiltiies.\n    if is_predicting:\n      return (predicted_labels, log_probs)\n\n    # If we're train\/eval, compute loss between predicted and actual label\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, predicted_labels, log_probs)","311f8d14":"# model_fn_builder actually creates our model function\n# using the passed parameters for num_labels, learning_rate, etc.\ndef model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]\n\n    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n    \n    # TRAIN and EVAL\n    if not is_predicting:\n\n      (loss, predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      train_op = bert.optimization.create_optimizer(\n          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n      # Calculate evaluation metrics. \n      def metric_fn(label_ids, predicted_labels):\n        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n        f1_score = tf.contrib.metrics.f1_score(\n            label_ids,\n            predicted_labels)\n        auc = tf.metrics.auc(\n            label_ids,\n            predicted_labels)\n        recall = tf.metrics.recall(\n            label_ids,\n            predicted_labels)\n        precision = tf.metrics.precision(\n            label_ids,\n            predicted_labels) \n        true_pos = tf.metrics.true_positives(\n            label_ids,\n            predicted_labels)\n        true_neg = tf.metrics.true_negatives(\n            label_ids,\n            predicted_labels)   \n        false_pos = tf.metrics.false_positives(\n            label_ids,\n            predicted_labels)  \n        false_neg = tf.metrics.false_negatives(\n            label_ids,\n            predicted_labels)\n        return {\n            \"eval_accuracy\": accuracy,\n            \"f1_score\": f1_score,\n            \"auc\": auc,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"true_positives\": true_pos,\n            \"true_negatives\": true_neg,\n            \"false_positives\": false_pos,\n            \"false_negatives\": false_neg\n        }\n\n      eval_metrics = metric_fn(label_ids, predicted_labels)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(mode=mode,\n          loss=loss,\n          train_op=train_op)\n      else:\n          return tf.estimator.EstimatorSpec(mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metrics)\n    else:\n      (predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      predictions = {\n          'probabilities': log_probs,\n          'labels': predicted_labels\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n  # Return the actual model function in the closure\n  return model_fn","a9f8c4e5":"# Compute train and warmup steps from batch size\n# These hyperparameters are copied from this colab notebook (https:\/\/colab.sandbox.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/bert_finetuning_with_cloud_tpus.ipynb)\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 4.0\n# Warmup is a period of time where hte learning rate \n# is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 10000\nSAVE_SUMMARY_STEPS = 1000\nOUTPUT_DIR = \"output\/\"","95176221":"# Compute # train and warmup steps from batch size\nnum_train_steps = int(len(train_features) \/ BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)","3a56e533":"# Specify outpit directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)","05120cd8":"model_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","7f1b3ae8":"# Create an input function for training. drop_remainder = True for using TPUs.\ntrain_input_fn = bert.run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)","508d94f3":"print(f'Beginning Training!')\ncurrent_time = datetime.now()\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint(\"Training took time \", datetime.now() - current_time)","ae4fca2b":"test_input_fn = run_classifier.input_fn_builder(\n    features=test_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","b7dd4c94":"metrics = estimator.evaluate(input_fn=test_input_fn, steps=None)","a7258108":"metrics","269d591a":"results.at[\"BERT\",\"Accuracy\"] = metrics['eval_accuracy']","a084c542":"results","5c624562":"ax = sns.barplot(x=results.index, y=results[\"Accuracy\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","d0559166":"# Word Embeddings","210ba922":"https:\/\/colab.research.google.com\/drive\/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB#scrollTo=cSq2DR-Sr9DX&forceEdit=true&offline=true&sandboxMode=true","d28ed92d":"# Loading the data","e1048ecc":"## Sources\n\nThere are great resources I relied upon. Please let me know if I missed any reference I used in this kernel:\n- Tf-IDF:  https:\/\/towardsdatascience.com\/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n- Vector Space model: http:\/\/blog.christianperone.com\/2013\/09\/machine-learning-cosine-similarity-for-vector-space-models-part-iii\/\n- Word2Vec: \n    - https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/\n    - https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n- GloVe: https:\/\/nlp.stanford.edu\/projects\/glove\/\n- using GloVe in Keras: https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n- FastText: https:\/\/fasttext.cc\/\n- CNN for NLP: https:\/\/medium.com\/cityai\/deep-learning-for-natural-language-processing-part-iii-96cfc6acfcc3\n- Transformers: \n    - https:\/\/ai.googleblog.com\/2017\/08\/transformer-novel-neural-network.html\n    - Attention is all you need: https:\/\/arxiv.org\/abs\/1706.03762\n- BERT:\n    - https:\/\/towardsdatascience.com\/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec\n    - embedding layers: https:\/\/medium.com\/@_init_\/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n    - NYT: https:\/\/www.nytimes.com\/2018\/11\/18\/technology\/artificial-intelligence-language.html\n    - hugging face: https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\n    - https:\/\/github.com\/strongio\/keras-bert\n","73840e2b":"RNN encoder-decoder approaches provides important advances capturing some long distance dependencies, but one vector represents the meaning of longer phrases.\n\nAttention mechanisms can weight the importance of each input vector and improve RNN-based architectures even further.","03cc909a":"https:\/\/projector.tensorflow.org","aa794d90":"On stage, a woman takes a seat at the piano. She\n\na) sits on a bench as her sister plays with the doll.\n\nb) smiles with someone as the music plays.\n\nc) is in the crowd, watching the dancers.\n\nd) nervously sets her fingers on the keys.","fa48c14e":"# Bert","1ff96e26":"## FastText\n\nFastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures","b01fb046":"https:\/\/www.nytimes.com\/2018\/11\/18\/technology\/artificial-intelligence-language.html","8a3a1b11":"## BERT tokenization\n\nAnother innovation of BERT is the way text is tokenized. THe tokenizer developed by Sennrich, Haddow Birch (2016) breaks down longer words into basic words resulting into a very small lexicon.  The technique they creted is  called [BPE](https:\/\/arxiv.org\/pdf\/1508.07909.pdf) and is based on WordPiece tokenisation.","c8a954d1":"Finally, there is a third layer for BERT: a position layer. Note that transformers actually do not encode the sequence of words. Sequential information of words is captured by producing an embedding for each of the words in the vocabulary for each position (with a maximum of 512). For example the *I* in the sentences mentioned earlier are all examples for *I* in position 1, whereas the *I* in *dogs I liked* is an example for position 2.\n\n## summing up\n\nWe end up with three embedding layers of shape $(1,n,768)$. All three vectors are summed element-wise so that we end up with a vector of shape $(1,n,768)$ where is $n$ is the length of the respective input sequence.","319b88ec":"\tAccuracy\n* NB-count 0.81136 \n* NB-tfidf\t0.82872\n* NB-tfidf stopwords\t0.83324\n* NB-tfidf stopwords bigrams\t0.84868\n* CNN-LSTM\t0.84 - 0.88\n* BERT\t~ 0.92\n* LR stopwords bigrams\t0.87504","923419a8":"## TF-IDF","736a40da":"*  TF = (Number of times term t appears in a document)\/(Number of terms in the document)\n* IDF = log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.","c078e5d8":"# Word Representations\n\nBefore we go into the details of BERT, let's look at how words are represented in order to be fed into a machine learning algorithm in order to do classification tasks such as sentiment analysis, pronoun resolution or parsing.\n\nIn general, words are mapped to higher dimensional vectors. They are used as feature vectors to train traditional ML algorithms such as Random Forests or being the first layer of a neural network.\n","99fe0a33":"### training BERT","98a9f862":"![segment_embedding](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*7p1kuvoafkCziIWYZSG94w.png)","de56340a":"## training BERT\n\nHow does BERT actually train the different embeddings and recive a representation of the different tokens? Word embeddings use CBOW representations either predicting a word in context or given the word predict the context. BERT uses a similar approach by masking words in the input strings that are then predicted by the model.","f295f9ad":"![attention_grid](http:\/\/mogren.one\/graphics\/illustrations\/2016-08-08\/bahdanau-etal-alignment.png)","f51d8502":"In order to weigh the different words by their frequency in the document and by the frequency of how often they occur in the entire  corpus, you can use the tf-idf (term frequency-inverse document frequency) for each word in the one-hot vector.\n","73b36337":"The encoder creates a representation of the input sequence for multiple layers comparing the input sequence word by word.\n\nThe decoder generates one word at a time taking into account not only the representation of the previously generated word, but from the entire top layer from the encoder.\n\nUtilizing the context encoded via multiple layers allow to represent complex semantic relations. Coreference resolution for example can be sentitive to a single word as in *The animal didn't cross the street because it was too tired\/wide*","7b016336":"## Transformers\n\nThe next step was to use only attention and to not use an RNN architecture at all. The transformer approach is build as a self-attention from each word and the surrounding context.\n","31d551e4":"## GloVe\n\nGloVe embeddings are utilizing the co-occuring matrix used distributional embeddings and deploy neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors. ","9cd117ea":"We define a simple pipeline in sklearn to train a model using different methods for vectorizing the text:\n","447c9bce":"## Other embeddings and further readings\n\n- Elmo: https:\/\/allennlp.org\/elmo\n- GPT-2: https:\/\/openai.com\/blog\/better-language-models\/\n- Grover: https:\/\/grover.allenai.org\n- XLnet: https:\/\/github.com\/zihangdai\/xlnet\n- CTRL model: https:\/\/blog.einstein.ai\/introducing-a-conditional-transformer-language-model-for-controllable-generation\/\n\nBertology:\n- https:\/\/huggingface.co\/pytorch-transformers\/bertology.html\n- the clever Hans moment: https:\/\/bheinzerling.github.io\/post\/clever-hans\/","57e64d81":"https:\/\/paperswithcode.com\/sota\/sentiment-analysis-on-imdb","793825ca":"Additional preprocessing can be added by (a) removing so-called stop words and (b) allowing for n-grams.","1ad9d5ab":"![transformer](https:\/\/3.bp.blogspot.com\/-aZ3zvPiCoXM\/WaiKQO7KRnI\/AAAAAAAAB_8\/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs\/s1600\/transform20fps.gif)","6b2de33d":"## Word2Vec\n\nWord2Vec embeddings take the idea of representing words based on their context to the next level. The multi-dimensional representation can be trained via the two algorithms:\n\n- Continuous bag-of-words (CBOW)\u200a\u2014\u200aThe order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.\n- Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context.\n\n![CBOW](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*TY9nYgPpwJloevhp.png)","746419ba":"acc: 0.84636","74f8b4c7":"# From Embeddings to Transformers: Language Models with BERT \n\nWord embeddings such as GloVe and FastText have been used for solving various NLP problems for a few years now. Recently, more complex embeddings such as BERT have shown to beat most of the best-performing systems for question answering, textual entailment and question continuation tasks. In this kernel, I will provide some background on word embeddings and how BERT differs from previous approaches and how it can be utilized to solve for example a sentiment analysis task for movie reviews.\n\nStarting with a couple of baseline approaches (Naive Bayes, Logistic Regression) and the simple encoding of words in a text via word counts and tf-idf counts, I will motivate the more powerful approach of word embeddings that capture more semantic relationships in their vector representations going beyond a simple one-hot vector.\n\nGloVe embeddings are used for a CNN-LSTM network will show comparable performance to the classical approaches, but finetuning the sentiment task via BERT shows much better performance. \n\nAt the end, I will also list more recent developments such as XLnet that are outperforming BERT in many NLP tasks.\n\n\n\n","a9403b60":"![title](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/08\/Example-of-a-CNN-Filter-and-Polling-Architecture-for-Natural-Language-Processing.png)","9fcc2df0":"Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n\n- Lowercase our text (if we're using a BERT lowercase model)\n- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n- Map our words to indexes using a vocab file that BERT provides\n- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n- Append \"index\" and \"segment\" tokens to each input (see the BERT paper)","023cd50a":"A so-called one-hot vector can be derived via a CountVectorizer. ","42ba5711":"![attention](https:\/\/skymind.ai\/images\/wiki\/attention_model.png)","8fd874eb":"![t-SNE](https:\/\/nlpforhackers.io\/wp-content\/uploads\/2018\/04\/Screen-Shot-2018-04-05-at-23.18.25.png)","a9a0596b":"## BERT embeddings\n\nBert is using the idea of an attention-based transformer and learns the context embeddings for every word. Similar approaches like ELMo and GPT have similar embedding strategies:\n![sesame](https:\/\/1.bp.blogspot.com\/-RLAbr6kPNUo\/W9is5FwUXmI\/AAAAAAAADeU\/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs\/s1600\/image3.png)\n\nELMo does not use transformer but captures the context via 2 independently LSTMs - one trained from left-to-right and the other one trained from right-to-left -, whereas GPT uses a left-to-right transformer. Only BERT uses transformers in both directions.","372f4b66":"## BERT embedding layers\n\nLet's look at the different embedding layers that BERT provides. Simialar to other word embeddings, there is a token layer. Because the WordPiece tokenization, we only have 30,522 different tokens and the dimension of the token embedding for each token is 768. \n![token_embedding](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*BXjLEnX89ftmFO_l91sP-A.png)","c2157582":"A quick look at the data...","661f6eb5":"![title](http:\/\/d3kbpzbmcynnmx.cloudfront.net\/wp-content\/uploads\/2015\/09\/Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png)","d91c8764":"## Attention based embeddings\n\nWord embeddings have been used as the input layer for various neural network architectures such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). ","71403eb4":"[](http:\/\/)BERT in particular excels in word continuation problems. The task is defined as given a sentence, what is the most likely continuation sequence. The BERT representation has an additional embedding layer that captures information regarding this task called the Segment embedding. \n> On stage, a woman takes a seat at the piano. She\n1. sits on a bench as her sister plays with the doll.\n2. smiles with someone as the music plays.\n3. is in the crowd, watching the dancers.\n4. nervously sets her fingers on the keys.","631e9ce3":"![coreference](https:\/\/1.bp.blogspot.com\/-AVGK0ApREtk\/WaiAuzddKVI\/AAAAAAAAB_A\/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs\/s1600\/image2.png)","ba9c5b7e":"## Bag of words\nThere are several well-known methods for representing words, starting with a simple bag of words (BOW) approach"}}