{"cell_type":{"2ddb6061":"code","68822879":"code","5d39f9ab":"code","ef5fa17d":"code","0cf4a221":"code","fdc8398a":"code","8953ee73":"code","2d0094d3":"code","49f54ce2":"code","5c1cddfc":"code","dcfd255d":"code","0fff94a6":"code","36ac86dc":"code","4fa69023":"code","ab143e32":"code","83e10714":"code","afb6bf82":"code","f84e94ca":"code","d65031af":"code","4424714a":"code","a63499ce":"code","01d87138":"code","eda5c27c":"code","4c189661":"code","aa381fee":"code","dda16af0":"code","b8f9ae43":"code","87244dee":"code","2fa1d3a8":"code","b9478124":"code","e6a91d9e":"code","eb44c9a2":"code","90602d77":"code","493588c2":"code","239753cc":"code","d80dfdce":"code","3b899659":"code","ac737e5d":"code","11bbfd8a":"code","1be6fe1e":"code","2333a6a7":"code","ca563557":"code","ee84e86e":"code","61a8268e":"code","250b86c0":"code","b320d41e":"code","9035898f":"code","1e6acf77":"code","6ac33380":"code","90e997e3":"code","0b5d539c":"code","42481b54":"code","e6b1eec2":"code","e96af065":"code","9981a9b7":"code","5cdd4518":"code","3e9d89ff":"code","19fb7699":"code","e2e3f69c":"code","e15c1a0d":"code","823bf276":"code","dd5f2095":"code","f667dfce":"code","35f70563":"code","28943042":"code","7deb6676":"markdown","964e162f":"markdown","300de940":"markdown","684f1ed3":"markdown","83818f08":"markdown","266ceea1":"markdown","80a414a5":"markdown","c0ef700e":"markdown","42f172c1":"markdown","b1824735":"markdown","0179bf8f":"markdown","ad8cd728":"markdown","38fa14ab":"markdown","076b00b7":"markdown","df2071ee":"markdown","75fa33bb":"markdown","3b0f22cb":"markdown","c23d996a":"markdown","76ee857e":"markdown","f4ead636":"markdown","04899ab5":"markdown","95c836e3":"markdown","71cb531e":"markdown","0c15b7c0":"markdown","018a86c0":"markdown","45581deb":"markdown","84fdf7f6":"markdown","14a6c3b7":"markdown","c92b8618":"markdown","b9dc40d8":"markdown"},"source":{"2ddb6061":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom IPython.display import clear_output\nimport gc\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nfrom scipy.stats import moment","68822879":"train_files = os.listdir(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\")\ntest_files = os.listdir(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/test\")","5d39f9ab":"len(train_files)","ef5fa17d":"len(test_files)","0cf4a221":"cols = []\nrows = []\nfor i,fname in enumerate(train_files):\n    train = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\",fname))\n    cols.append(train.shape[0])\n    rows.append(train.shape[1])\n    print(f'{i+1} \/ {len(train_files)}')\n    clear_output(wait=True)","fdc8398a":"print(f\"Rows of all train files: {pd.Series(rows).unique()}\\nColumns of all train files: {pd.Series(cols).unique()}\")","8953ee73":"cols = []\nrows = []\nfor i,fname in enumerate(test_files):\n    test = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/test\",fname))\n    cols.append(test.shape[0])\n    rows.append(test.shape[1])\n    print(f'{i+1} \/ {len(test_files)}')\n    clear_output(wait=True)","2d0094d3":"print(f\"Rows of all test files: {pd.Series(rows).unique()}\\nColumns of all test files: {pd.Series(cols).unique()}\")","49f54ce2":"train = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\",train_files[2]))\ntrain.head()","5c1cddfc":"train.count()","dcfd255d":"(train==0).sum()","0fff94a6":"missing_tracker = pd.DataFrame()\nfor i,fname in enumerate(train_files):\n    train = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\",fname))\n    missing_tracker = missing_tracker.append(pd.DataFrame(train.count()).T)\n    print(f'{i+1} \/ {len(train_files)}')\n    clear_output(wait=True)","36ac86dc":"missing_tracker","4fa69023":"missing_tracker.nunique()","ab143e32":"for col in missing_tracker.columns:\n    print(f\"{col}\\n{sorted(missing_tracker[col].unique())}\\n\\n\")","83e10714":"plt.figure(figsize=(15,20))\nsns.heatmap(missing_tracker);","afb6bf82":"mi = ((missing_tracker==60001).sum()\/4431)*100\nprint(f'% of files with complete data per sensor\\n\\n{mi}')\nmi.plot(kind=\"bar\")\nplt.axhline(y=100,color=\"red\")\nplt.title(\"Sensors with complete readings\")\nplt.ylabel(\"% of files with complete data\");","f84e94ca":"mi = ((missing_tracker==0).sum()\/4431)*100\nprint(f'% of files with no data at all per sensor\\n\\n{mi}')\nmi.plot(kind=\"bar\")\nplt.title(\"Sensors with no readings at all\")\nplt.ylabel(\"% of files with no data at all\");","d65031af":"train_1 = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\",train_files[0]))\ntrain_1.head()","4424714a":"train_1.shape","a63499ce":"fig,ax=plt.subplots(5,2,figsize=(12,17))\nr=0\nc=0\nfor i,col in enumerate(train_1.columns):\n    ax[r,c].plot(train_1[col])\n    ax[r,c].set_title(col)\n    c+=1\n    if (i+1)%2==0:\n        r+=1\n        c=0\nplt.show();","01d87138":"train_1.plot(figsize=(15,8))\nplt.legend(loc=\"upper left\", ncol=5);","eda5c27c":"fig = plt.figure(figsize=(10,7))\nsns.heatmap(train_1.corr(),annot=True,fmt=\".2f\",cbar=False);","4c189661":"sns.pairplot(train_1,diag_kind=\"kde\");","aa381fee":"def find_stats(x):\n    '''\n    Function to apply describe on monthly data\n    '''\n    return x.describe()","dda16af0":"import random\nrandom.seed(11)\nrand_sample = random.sample(range(len(train_files)),int(np.floor(len(train_files)*0.7)))\nprint(len(rand_sample))\ntrain_files = list(pd.Series(train_files)[rand_sample])","b8f9ae43":"train = pd.DataFrame()\nfor n,i in enumerate(train_files):\n    df = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\",i))\n    df.index=pd.date_range(\"2000-01-01\",periods=len(df),freq=\"10T\") #adding date range with 10 minute interval\n    df = df.resample(rule=\"M\").apply(find_stats).values.reshape(1,-1) #resampling the time series data and calculating stats on monthly data\n    df = pd.DataFrame(df)\n    df[\"segment_id\"] = int(i.replace(\".csv\",\"\"))\n    train = train.append(df)\n    print(f'{n}')\n    gc.collect()\n    clear_output(wait=True)","87244dee":"train_labels = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv\")","2fa1d3a8":"train = train.merge(train_labels,on=\"segment_id\",how=\"left\")","b9478124":"train=train.drop(columns=\"segment_id\")","e6a91d9e":"train.fillna(0,inplace=True)","eb44c9a2":"X = train.iloc[:,:-1].copy()","90602d77":"y = train.iloc[:,-1].copy()","493588c2":"X.head()","239753cc":"y.isnull().sum()","d80dfdce":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.decomposition import PCA\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold,cross_val_score","3b899659":"#pca = PCA(n_components=10)\n#X_pca = pca.fit_transform(X)\n#np.cumsum(pca.explained_variance_ratio_)","ac737e5d":"del df\ndel train\ngc.collect()","11bbfd8a":"X_pca = X.copy()","1be6fe1e":"rf = RandomForestRegressor(random_state=11)\nrf.fit(X_pca,y)\nprint(f\"No of Features with Importance > 0: {sum(rf.feature_importances_>0)}\")","2333a6a7":"feat_imp = pd.Series(rf.feature_importances_).reset_index(drop=True)\nthreshold = feat_imp.quantile(0.75)","ca563557":"final_feat = list(feat_imp[feat_imp>threshold].index)\nlen(final_feat)","ee84e86e":"X_pca = X_pca.iloc[:,final_feat].copy()\nX_pca.shape","61a8268e":"del rf\ngc.collect()","250b86c0":"MAX_DEPTH = 10\nN_ESTIMATORS = 1000\nMIN_SAMPLES_LEAF = 300\nL1 = 50000\n\n\ngscv = LGBMRegressor(n_estimators=N_ESTIMATORS,\n                     max_depth=MAX_DEPTH,\n                     num_leaves=2**MAX_DEPTH,\n                     min_data_in_leaf=MIN_SAMPLES_LEAF,\n                     lambda_l1 = L1,\n                     random_state=11,\n                     n_jobs=-1)\n","b320d41e":"cv = KFold(n_splits=3,random_state=11,shuffle=True)\ncv_score = cross_val_score(gscv,X_pca,y,cv=cv,scoring=\"neg_mean_absolute_error\",n_jobs=-1)","9035898f":"-1 * (cv_score.astype(\"int\"))","1e6acf77":"np.mean(-1*(cv_score.astype(\"int\")))","6ac33380":"gscv.fit(X_pca,y)","90e997e3":"del X\ngc.collect()","0b5d539c":"mean_absolute_error(gscv.predict(X_pca),y)","42481b54":"r2_score(gscv.predict(X_pca),y)","e6b1eec2":"del cv, cv_score\ngc.collect()","e96af065":"del X_pca\ngc.collect()","9981a9b7":"test = pd.DataFrame()\nfor n,i in enumerate(test_files):\n    df = pd.read_csv(os.path.join(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/test\",i))\n    df.index=pd.date_range(\"2000-01-01\",periods=len(df),freq=\"10T\")\n    df = df.resample(rule=\"M\").apply(find_stats).values.reshape(1,-1)\n    df = pd.DataFrame(df)\n    df = df.iloc[:,final_feat].copy()\n    df[\"segment_id\"] = int(i.replace(\".csv\",\"\"))\n    test = test.append(df)\n    print(f'{n}')\n    gc.collect()\n    clear_output(wait=True)","5cdd4518":"test_segment_ids = test[\"segment_id\"]\ntest = test.drop(columns=\"segment_id\")","3e9d89ff":"test.fillna(0,inplace=True)","19fb7699":"#X_test_pca = pca.transform(test)","e2e3f69c":"X_test_pca = test.copy()","e15c1a0d":"del test\ngc.collect()","823bf276":"#X_test_pca = X_test_pca.iloc[:,final_feat].copy()","dd5f2095":"pred = gscv.predict(X_test_pca)","f667dfce":"pred","35f70563":"submission = pd.DataFrame({\"segment_id\":test_segment_ids,\"time_to_eruption\":pred})","28943042":"submission.to_csv(\"ingv_submission.csv\",index=False)","7deb6676":"<h3>Checking if the shape of data is uniform across all the train files","964e162f":"<h2>Model is overfit to the training set. Need further hyperparameter tuning.","300de940":"<h3>As seen earlier in the heatmap, sensors 2, 3, 5, 8, and 9 have no readings in many files.","684f1ed3":"<h3>Whoa!! NaNs","83818f08":"<h3>There are few sensors with no data at all in some files. Let us also examine if there are sensors with intermittent missing values (NaN) across files","266ceea1":"<h3>The heatmap shows sensors 2, 3, 5, 8, and 9 have no readings in many files. Sensors 4 and 6 have atleast 1 reading in every file.","80a414a5":"<h3>Though PCA helped me reduce the dimensionality with 99.99 explained variance ratio. But the performance of the model was better without PCA being applied since it ignores the target variable","c0ef700e":"<h3>All the train and test files have same shape. Hence, we'll inspect the first few rows of first 10 train files to get an idea of data","42f172c1":"<h3>CV scores (MAE)","b1824735":"<h3>Let's see the distribution of sensors with complete readings in the files","0179bf8f":"<h1>INGV - Volcanic Eruption Prediction","ad8cd728":"<h3><ol>\n<li>sensor_1: Has moderate no. of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>sensor_2: Has lots of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>sensor_3: Has moderate no. of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>Sensor_4: Looks quite stable. It's having very less number of intermittent NaNs and there are no cases where it hasn't recorded any readings.<\/li>\n<li>sensor_5: Has lots of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>sensor_6: Has moderate no. of intermittent missing readings (few NaNs) and no cases with no readings (all NaNs)<\/li>\n<li>sensor_7: Has less no. of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>sensor_8: Has very less no. of intermittent missing readings (few NaNs) and cases with no readings (all NaNs)<\/li>\n<li>Sensor_9:  Seems to be highly unstable with varying intermittent missing readings \/ NaNs<\/li>","38fa14ab":"<h3>Using Random Forest for feature importance","076b00b7":"<h3>Let's plot the readings of each sensor","df2071ee":"<h3>Not only NaNs there are also few 0s","75fa33bb":"<h1>Reducing the dimensionality of train and test sets by computing important stats (using describe) on monthly data<\/h1>\n\n<h3>There are 4,000+ train files\/locations with 60,000 readings of 10 sensors. Each loacation\/file has a single target variable i.e. 60,000 rows of data has only 1 target\/label. Hence, we need to reshape the data into single row per location. Doing this for huge data can be tedious. The readings are recorded for every 10 minutes. Hence, I've applied the describe function on monthly data and reshaped it into a single row.","3b0f22cb":"<h3>In the above sample, sensors 1 and 2 have many peaks and troughs in the readings, especially sensor 2","c23d996a":"<h3>Let's look at the data in a single training file","76ee857e":"<h3>Selecting threshold for feature importance. Importance above 75 percentile","f4ead636":"<h3>All the sensors don't have the readings in the same range","04899ab5":"<h3>Manually tuning the hyperparameters, since GridSearchCV is taking too long","95c836e3":"<h3>There is no correlation between the sensor readings","71cb531e":"<h3>Checking if the shape of data is uniform across all the test files","0c15b7c0":"<h3>There are sensors with intermittent missing values","018a86c0":"<h3>Selecting 70% of train files due to memory constraints","45581deb":"<h3>Since the dataset is huge, RAM utilization reached to its maximum, for better memory management I've manually removed unwanted variables and performed garbage collection frequently","84fdf7f6":"<h3>MAE on train set","14a6c3b7":"<h3>The above bar chart shows sensors 1,4,6, and 7 have almost all the readings in all the files<\/h3>\n\n<h3>Let's see the distribution of sensors with no readings at all across the files","c92b8618":"<h3>Are the sensor readings correlated. How's the distribution of sensor readings?<\/h3>","b9dc40d8":"<h3>R<sup>2<\/sup> on Training set"}}