{"cell_type":{"9f883398":"code","cceb0a6b":"code","63a3060e":"code","2ffa1dbe":"code","29db68e6":"code","c0fd5110":"code","948257f4":"code","036b15dd":"code","01b2ae8d":"code","6cb15292":"code","8b085701":"code","907522a6":"code","390d797f":"code","2e3a0257":"code","28191a8b":"code","cf3e1c0c":"code","c61e2cba":"markdown","0b3a045a":"markdown","4cf0ae8b":"markdown","48521874":"markdown","3ad729e3":"markdown","791d637f":"markdown","b54c03df":"markdown","bde2112b":"markdown","51cd92e4":"markdown","75a7f931":"markdown","f52fec1f":"markdown","677df32b":"markdown","896f8b7f":"markdown","47bf47da":"markdown","3d41b139":"markdown","969f33cd":"markdown","9fe48439":"markdown","6d705076":"markdown","918fdff9":"markdown"},"source":{"9f883398":"import pandas as pd\nimport numpy\nfrom datetime import datetime\nfrom sklearn.metrics import accuracy_score\n\ngender_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","cceb0a6b":"print(f\"Training set dimension: {train.shape}\")\nprint(f\"Testing set dimension: {test.shape}\")","63a3060e":"import pandas as pd\nimport numpy as np\nimport pickle\nimport os\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"Fill na and set index: FillSet\")\ndef FillNA(X, ind, cat, num, col='Cabin'):\n\n    X = X.set_index(ind)\n    X[cat] = X[cat].fillna('Missing')\n    X[num] = X[num].fillna(X[num].median())\n    X[col] = X[col].apply(lambda x: x[0])\n\n    return X\n\nprint(\"Title function: Title\")\ndef Title(df, col):\n    title = []\n    for i in df[col]:\n        if 'Miss.' in i:\n            title.append('Miss')\n        elif 'Mrs.' in i:\n            title.append('Mrs')\n        elif 'Mr.' in i:\n            title.append('Mr')\n        else:\n            title.append('No')\n\n    df[col] = title\n\n    return df\n\n\nprint(\"One hot decoder imported: Decoder\")\nclass Decoder(TransformerMixin, BaseEstimator):\n    def __init__(self, columns):\n        super().__init__()\n\n        self.columns = columns\n        self.onehot = OneHotEncoder(drop='first')\n\n    def fit(self, X, y=None):\n        return self\n\n    def fit_transform(self, X):\n        one = self.onehot.fit_transform(X[self.columns]).toarray()\n        col_names = self.onehot.get_feature_names()\n\n        return pd.concat([X.drop(self.columns, axis=1), pd.DataFrame(one, index=X.index, columns=col_names)], axis=1)\n\n\nprint(\"Select imported: SelectCol\")\ndef SelectCol(X, drop_cols, target, col_str):\n    if col_str is not None:\n        for i in col_str:\n            X.apply(lambda i: str(i))\n\n    X = X.drop(drop_cols, axis=1)\n\n    train = X[~X.Survived.isna()]\n    test = X[X.Survived.isna()]\n\n    return train.drop(target, axis=1), train[target], test.drop(target, axis=1)\n\n\n\nprint(\"Grid Search train: GSCV\")\ndef GSCV(pipe, params, X, y, test, submission, m, scoring, cv=5):\n    grid = GridSearchCV(estimator=pipe,\n                        param_grid=params,\n                        cv=cv,\n                        iid=False,\n                        return_train_score=False,\n                        refit=True,\n                        scoring=scoring\n                       )\n    grid.fit(X, y)\n    pd.DataFrame(grid.predict(test), index=submission.index, columns=['Survived']).to_csv(\"\/kaggle\/working\/\" + m + \"_submission.csv\")\n    return grid.best_score_, grid.best_params_, grid.best_estimator_.predict(test)\n\n\n\nprint(\"Ensemble function: EnsemblePropensity\")\ndef EnsemblePropensity(directory, folder):\n    d = directory + folder\n    count = 0\n    df = pd.DataFrame()\n    for i in os.listdir(d):\n        if 'csv' in i:\n            score = pd.read_csv(d + i)\n            df = pd.concat([df, score.iloc[:, 1]], axis=1)\n            count += 1\n            index = score.iloc[:, 0]\n\n    df = pd.DataFrame(np.sum(df, axis=1), columns=['Probability'])\n    df['Survived'] = [1 if i >= 2 else 0 for i in df.Probability]\n    df = df.iloc[:, 1]\n    df.to_csv(d + \"Ensemble_\" + str(count) + \".csv\")\n\n    print(\"Ensemble complete\")\n","2ffa1dbe":"test.insert(test.shape[1], \"Survived\", [np.nan]*len(test))\nprint(f\"Train dim: {train.shape}, Test dim: {test.shape}\")\n\ndf = pd.concat([train, test], sort=True)\nprint(f\"DF dimension: {df.shape}\")","29db68e6":"print(\"<<<<< Original dataset >>>>>>\")\nprint(f\"Missing: {df.isna().sum()}\")\ndf = FillNA(df, 'PassengerId',['Cabin', 'Embarked'], ['Age','Fare'])\nprint(\"<<<<< Filled NA >>>>>>\")\nprint(f\"Missing: {df.isna().sum()}\")","c0fd5110":"df = Title(df, 'Name')\ndf.head()","948257f4":"df = Decoder(['Cabin', 'Embarked', 'Pclass', 'Sex', 'Name']).fit_transform(df)\nprint(f\"New dimension: {df.shape}\")","036b15dd":"X, y, test_data = SelectCol(df, ['Ticket'], 'Survived', None)\nX.shape, y.shape, test_data.shape","01b2ae8d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb\n\nmodel = []\naccuracy = []\nbest_params = []\nruntime = []\nscores = []\n\nscoring = 'f1_micro'\nparams = [ {'criterion': ['gini','entropy'],\n            'max_depth':np.arange(1,20).tolist()},\n          \n          {'n_estimators': np.arange(5,30).tolist(),\n            'criterion': ['gini','entropy'],\n            'max_depth':np.arange(1,10).tolist()},\n          \n          {'learning_rate': [0.001, 0.01, 0.03, 0.05, 0.1, 0.2, 0.3],\n            'n_estimator': np.arange(5,25).tolist(),\n            'max_depth':np.arange(3,15).tolist(),\n             'alpha': [1, 2, 3]},\n          \n          {'learning_rate': [0.001, 0.01, 0.03, 0.05, 0.1, 0.2, 0.3],\n            'n_estimator': np.arange(5,25).tolist(),\n            'num_leaves':np.arange(3,15).tolist(),\n             'reg_alpha': [0.01, 0.02, 0.03]},\n           \n           {'depth': np.arange(5,25).tolist(),\n             'learning_rate' : [0.01, 0.03, 0.1, 0.15, 0.3],\n             'l2_leaf_reg': [1,3,4,9],\n             'iterations': [100]} ]\n\nmodels = [\"dt\", \"rf\", \"xgb\", \"lgbm\", \"cb\"]\n\nestimators = [DecisionTreeClassifier(random_state=234),\n              RandomForestClassifier(random_state=123),\n              xgb.XGBClassifier(n_jobs=3),\n              lgbm.LGBMClassifier(objective = 'binary'),\n              cb.CatBoostClassifier(silent=True)]\n\nprint(\"Parameters and estimators are created\")","6cb15292":"# for i in range(len(models)):\n#     est = estimators[i]\n#     param = params[i]\n#     m = models[i]\n#     start = datetime.now()\n    \n#     score, parm, estimation = GSCV(pipe=est, \n#                                   params=param, \n#                                   X=X, \n#                                   y=y, \n#                                   test=test_data,\n#                                   submission=gender_submission,\n#                                   m = m,\n#                                   scoring=scoring\n#                                   )\n#     end = datetime.now()\n    \n#     model.append(m + \"_\" + scoring)\n#     accuracy.append(score)\n#     best_params.append(parm)\n#     runtime.append(end-start)\n#     scores.append(scoring)\n    \n#     print(f\"<<<< Model: {m} >>>>\")\n#     print(f\"Train score: {score}\")\n#     print(f\"Best parameters: {parm}\")\n#     print(f\"Train Runtime: {end-start}\")","8b085701":"# pd.DataFrame({'model': model,\n#             'score': accuracy,\n#             'best parameters': best_params,\n#             'runtime': runtime,\n#             'metric': scores})","907522a6":"estimators = [DecisionTreeClassifier(random_state=234, criterion='entropy', max_depth=4),\n              RandomForestClassifier(random_state=123, criterion='entropy', max_depth=6, n_estimators=5),\n              xgb.XGBClassifier(n_jobs=3, alpha=1, learning_rate=0.03, max_depth=8, n_estimators=5),\n              lgbm.LGBMClassifier(objective = 'binary', learning_rate=0.1, n_estimator=5, num_leaves=11, reg_alpha=0.01)]\n\nmodels = [\"dt\", \"rf\",\"xgb\",\"lgbm\"]\naccuracy=[]\n\nfor i in range(len(estimators)):\n    est = estimators[i]\n    \n    est.fit(X, y)\n    y_pred = est.predict(X)\n    \n    accuracy.append(accuracy_score(y, y_pred))\n    pd.DataFrame(est.predict(test_data), columns=['Survived']).to_csv(\"\/kaggle\/working\/\" + \"1_\" + models[i] + \"_submission.csv\")\n    \n    print(f\"{models[i]} done\")\n    print(f\"Accuracy {accuracy_score(y, y_pred)}\")","390d797f":"pd.DataFrame({'model': models,\n              \"accuracy\":accuracy\n            })","2e3a0257":"import torch\nfrom torch.autograd import Variable\nfrom keras.utils import to_categorical\n\nX = Variable(torch.from_numpy(X.values))\ny = Variable(torch.from_numpy(y.values))\ntest_data = Variable(torch.from_numpy(test_data.values))","28191a8b":"from torch import nn\n\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        \n        self.fc1 = nn.Linear(21, 12)\n        self.fc2 = nn.Linear(12, 6)\n        self.fc3 = nn.Linear(6, 2)\n        \n        self.dropout = nn.Dropout(p=0.02)\n        \n        self.relu = nn.ReLU()\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self, x):\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.dropout(self.relu(self.fc2(x)))\n        x = self.logsoftmax(self.fc3(x))\n        \n        return x\n\nmodel = MLP()\nmodel","cf3e1c0c":"from torch import optim\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.NLLLoss()\nepoch = 100\n\npermutation = torch.randperm(X.size()[0])\nbatch_size = 33\n\nfor e in range(epoch):\n    train_loss = 0\n    train_accuracy = 0\n    \n    for i in range(0,X.size()[0],batch_size):\n        indices = permutation[i:i+batch_size]\n        batch_x, batch_y = X[indices], y[indices]\n        \n        optimizer.zero_grad()\n\n        logps = model(batch_x.float())\n        loss = criterion(logps, batch_y.long())\n        train_loss = loss\n\n        ps = torch.exp(logps)\n        top_ps, top_class = ps.topk(1, dim=1)\n        equals = top_class == batch_y.view(*top_class.shape)\n\n        train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n\n        loss.backward()\n        optimizer.step()\n    \n    else:\n        \n        with torch.no_grad():\n\n            model.eval()\n\n            logps = model(test_data.float())\n            ps = torch.exp(logps)\n            top_ps, top_class = ps.topk(1, dim=1)\n\n            pd.DataFrame(np.array(top_class), columns=['Survived']).to_csv(\"\/kaggle\/working\/\" + str((e+1)) + \"_mlp_submission.csv\")\n\n            model.train()\n    \n    print(\"<<< Epoch: {} >>>\".format(e+1), \"Train loss: {:3f}\".format(train_loss), \"Train accuracy: {:3f}\".format(train_accuracy*100\/(len(X)\/batch_size)))","c61e2cba":"Create batch of 33 rows and train on each batch then predict test labels.","0b3a045a":"Extracting titles from the names.","4cf0ae8b":"Creating train and test dataset","48521874":"## 1. Import packages and dataset","3ad729e3":"Handling categorical variables and creating dummy variables","791d637f":"Set hyperparameters for the grid search.","b54c03df":"The end of the notebook","bde2112b":"Tuned models (GridSearch takes long time)","51cd92e4":"Convert the data to tensor","75a7f931":"Print the results","f52fec1f":"[](http:\/\/)Filling NA or missing values","677df32b":"## 2. Functions and classes built for implementation","896f8b7f":"## 4. Train and test the model","47bf47da":"The followings are the functions and classes that are used to preprocess and train the data.\n- `FullNA` function fills NA values of categorical values as \"Missing\", numberical as the median. For `Cabin` column, the first letter was extracted as a variable.\n- `Title` function extracts 'Mr', 'Miss', and 'Mrs' titles from the passenger names.\n- `Decoder` function uses one-hot-encoder on categorical variables to create dummy variables. \n- `SelectCol` function selects certain columns from the dataset and splits the data into training and testing data\n- `GSCV` function trains `GridSearchCV` on the trainin data and predicts labels for the testing data and saves the result to the output folder.\n- `Ensemble Propensity` function ensembles the results.","3d41b139":"In order to process the data, the train and test datasets were combined.","969f33cd":"## 5. Multi-layer Perceptron","9fe48439":"Train and test the models","6d705076":"Create tht model: Multi-layer perceptron with 4 layers","918fdff9":"## 3. Prepocessing the dataset"}}