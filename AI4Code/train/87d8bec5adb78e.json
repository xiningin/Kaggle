{"cell_type":{"9b68fd50":"code","cf7bf116":"code","d08e3a57":"code","64f1aa14":"code","9187623e":"code","ad1ee618":"code","c0f7e48c":"code","fa586c38":"code","1b8cad1e":"code","4dac690e":"code","ab373494":"code","b7aba459":"code","599f95a7":"code","9bdb8da9":"code","74ef302d":"code","41d78591":"code","195ec9bf":"code","17b0c3ba":"code","223ae98f":"code","b8e0adfe":"code","e801ac76":"code","d9d0a324":"code","695fa427":"code","ac05a768":"markdown","437f8437":"markdown","aea184d0":"markdown","2d4db625":"markdown","45d2012f":"markdown","52bd6c85":"markdown","461628d8":"markdown","dda08d44":"markdown","3cc034bd":"markdown","bc7703ac":"markdown","dbb6a0f2":"markdown","4b40d737":"markdown","e6d7f42f":"markdown","97cae1a7":"markdown","17da2774":"markdown","c388813e":"markdown","5b995598":"markdown","eddbfef1":"markdown"},"source":{"9b68fd50":"# Import common libraries\nimport sys # access to system parameters\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd # functions for data processing and analysis modeled after R dataframes with SQL like features\nimport pandas_profiling\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport numpy as np # foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp # collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \nimport scipy.stats as ss\n\nimport sklearn # collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n\n#misc libraries\nimport random\nimport time\nimport datetime\nimport os\nimport glob\nimport math\n\n\n# Visualisation\nimport matplotlib #collection of functions for scientific and publication-ready visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\npd.plotting.register_matplotlib_converters()\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport plotly\nprint(\"plotly version: {}\". format(plotly.__version__))\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot # Offline mode\ninit_notebook_mode(connected=True)\nimport seaborn as sns\nfrom xgboost import plot_importance\n\n\n# Import common MLA libraries\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection, model_selection, metrics\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, roc_curve, auc, confusion_matrix, plot_confusion_matrix\n\n# Default Global settings\npd.set_option('max_columns', None)\n#os.chdir(\"C:\/Users\/wturner\/OneDrive - Macadamia Processing Co Limited\/\")\n\nprint(\"Setup Successful\")","cf7bf116":"df = pd.read_csv('..\/input\/noshowappointments\/KaggleV2-May-2016.csv')","d08e3a57":"# Work smarter not harder\n# pandas_profiling.ProfileReport(df)","64f1aa14":"df.dtypes","9187623e":"# Get the list of columns\ndf.columns","ad1ee618":"# Change columns to correct data types\ncol_int = ['AppointmentID', 'PatientId'] # create a list of column names to convert to integer\ncol_float = ['Age'] # create a list of column names to convert to float\ncol_string = [] # create a list of column names to convert to string\ncol_ordinal = ['Scholarship',  'Hipertension',  'Diabetes',  'Alcoholism',  'Handcap', 'SMS_received',] # create a list of column names to convert to ordinal\ncol_nominal = ['Gender', 'Neighbourhood'] # create a list of column names to convert to nominal\ncol_date = ['ScheduledDay', 'AppointmentDay'] # create a list of column names to convert to date\n\n\ndef change_dtypes(col_int, col_float, col_string, col_ordinal, col_nominal, col_date, df): \n    '''\n    AIM    -> Changing dtypes to save memory\n    INPUT  -> List of int column names, float column names, df\n    OUTPUT -> updated df with smaller memory  \n    '''\n    df[col_int] = df[col_int].apply(pd.to_numeric)\n    df[col_float] = df[col_float].astype('float32')\n    df[col_string] = str(df[col_string])\n    df[col_ordinal] = df[col_ordinal].astype('object')\n    df[col_nominal] = df[col_nominal].astype('object')\n    for col in col_date:\n        df[col] = pd.to_datetime(df[col])\n    \nchange_dtypes(col_int, col_float, col_string, col_ordinal, col_nominal, col_date, df)","c0f7e48c":"# Select the target variable and drop it from the df for data wrangling\ndf['No-show'] = df['No-show'].map({'Yes': 1, 'No': 0})\ntarget = df['No-show']\ndf.drop(columns=['No-show'], inplace=True)","fa586c38":"# Normalising - binary classification unlikely to need normalising but its still good practice to examine the distribution\n\n# Plot of a histogram of the Target variable\nfig = go.Figure(data=[go.Histogram(x=target)])\nfig.update_layout(title=\"Histogram of the target variable\")\nfig.show()","1b8cad1e":"# Null Values\n\n# Dropping observations with too many NA, and surfacing the observations possibly requiring some imputation\n\n# If a row has at least 25% of its values as NA, then drop the row\nthreshold = len(df.columns) * .8\ndf = df.dropna(thresh=threshold)\n\nnrows = df.shape[0]\nnull_cat_col = []\nnull_int_col = []\n\ndef nullimputer(df):\n    '''\n    AIM    -> Impute null values where required, drop columns with too many nulls\n    INPUT  -> dataframe\n    OUTPUT -> dataframe with no null values\n    '''\n    ### Possible improvement: columns with null between 15-30% surfaced with recommendation to feature engineer instead of dropping\n    \n    for col in df.columns:\n        x = df[col].isnull().sum()\n        numnull = x \/ nrows\n\n        if numnull >= .15:\n            print(\"Dropped column(s):\",col)\n            df.drop([col],axis=1, inplace=True) # If a column has at least 15% of its values as NA, then drop the column\n        elif 0 < numnull < .15:\n            if col in col_int:\n                null_int_col.append(col)\n            elif col in col_ordinal:\n                null_cat_col.append(col)\n            elif col in col_nominal:\n                null_cat_col.append(col)\n            else:\n                print(\"Column with 0 to 15% null:\",col) # If a column has 0 to 15% of its values as NA, display the name of the column\n        else:\n            continue #If a column has no null then continue\n\n\n    print(\"Categorical columns with nulls:\", null_cat_col)\n    print(\"Numerical columns with nulls:\", null_int_col)\n    \n    print(\"Imputing values where required...\")\n    for col in null_int_col:\n        df[col] = df[col].fillna(df[col].median()) # Fill integer NA with median\n\n    for col in null_cat_col:\n        df[col] = df[col].fillna(df[col].mode()[0]) # Fill categorical NA with mode\n    \n    print(\"{} null values remain\".format(df.isna().sum().sum()))\n    \nnullimputer(df)","4dac690e":"# Number of days between the date of booking and the appointment date\ndf['days_until_appt'] = (df['ScheduledDay'] - df['AppointmentDay']).astype('timedelta64[D]')\n\n# Drop the appt date columns now, unused as not doing time series analysis\ndf.drop(columns=['ScheduledDay', 'AppointmentDay'], inplace=True)","ab373494":"# Encoding ordinal variables\nlabel = LabelEncoder()\n\nfor col in col_ordinal:\n   df[col] = label.fit(df[col].values).transform(df[col].values)","b7aba459":"# Encoding nominal variables\nfor col in col_nominal:\n    df_dummies = pd.get_dummies(df[col])\n    df = pd.concat([df, df_dummies], axis=1)\n    df.drop(col, inplace=True, axis=1)","599f95a7":"# Examining correlations with the target variable with the absolute value of pearson R correlation greater than or equal to:\ncorr_threshold = 0.1\n\ncorrdata = pd.concat([df,target],axis=1)\ncorr_matrix = abs(corrdata.corr())\ncorr_matrix_target = pd.DataFrame(corr_matrix[\"No-show\"])\n\ncorr_matrix_target = corr_matrix_target[corr_matrix_target[\"No-show\"] >= corr_threshold]\ncorr_matrix_target = corr_matrix_target[corr_matrix_target.index != \"No-show\"]\n\n\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_matrix_target, cmap=\"Blues\", vmax=corr_matrix_target.max(), vmin=corr_matrix_target.min(),\n            square=True, linewidths=.2, cbar_kws={\"shrink\": .7})","9bdb8da9":"# The strongest correlations between other variables\nall_corr = df.corr().abs().unstack().sort_values(kind=\"quicksort\")\n\nall_corr[(all_corr > .3) & (all_corr != 1)].tail(10)","74ef302d":"MLA = [\n    # Ensemble Methods\n    #ensemble.GradientBoostingClassifier(),\n    #ensemble.RandomForestClassifier(),\n    \n    # Linear Models\n    linear_model.LogisticRegressionCV(),\n    \n    # Decision Trees    \n    tree.DecisionTreeClassifier(),\n\n    # XGBoost\n    XGBClassifier()  \n    \n    ]","41d78591":"# Create a dataframe for the model results\nresult_table = pd.DataFrame(columns=[\"fit_time\",\"score_time\",\"test_score\",\"train_score\",\"MLA\"])\n\n# Cross validate through the list of MLAs\nfor alg in MLA:\n    result = cross_validate(alg, df, target, cv=5, return_train_score = True)\n    result[\"MLA\"] = alg\n    resultdf = pd.DataFrame.from_dict(result)\n    result_table = pd.concat([result_table, resultdf])","195ec9bf":"# Rearrange to put MLA column first and group MLAs by mean\ncols = result_table.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nresult_table = result_table[cols]","17b0c3ba":"result_table_copy = result_table\nresult_table_copy[\"MLA\"] = result_table[\"MLA\"].astype(\"string\")","223ae98f":"result_summary = result_table_copy.groupby(['MLA']).mean()\n\n# Display the table\nresult_summary","b8e0adfe":"result_summary = result_summary.set_index([pd.Index([\"DecisionTree\", \"LogisticRegression\", \"XGBoost\"])])","e801ac76":"# Bar plot of the scoring results\nsns.barplot(x=result_summary.index, y=\"test_score\", data=result_summary)","d9d0a324":"xgb = XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(df, target, random_state = 0)\nxgb = xgb.fit(X_train, y_train)","695fa427":"plt.subplots(figsize=(20, 25))\nplt.barh(df.columns, xgb.feature_importances_)","ac05a768":"# Considerations, Limitations & Next Steps","437f8437":"## Modelling","aea184d0":"**Considerations for production:**\n* Whether all of these datapoints always be avaialble at the time the appointment is booked\n* New neighbourhoods will need a new model\n\n\n**Limitations**:\n* Imbalanced target variable\n* Not production ready\n\n\n**Next steps**\n* Add neighbourhood demographics from external data source\n* Feature engineer whether the patient had previously no-showed","2d4db625":"## Importing Libraries","45d2012f":"Most of the variables collected do not have correlation with the target variable. This may impact model performance.\n\nMy hypotheses about these three strongest (albeit very weak) correlations...\n\na) the trend of no-show appointments may be changing over time\n\nb) the SMS notification program may have been implemented on a date after data collection began, and therefore only newer appointments can have sms_received = 1 values\n\nc) appointments booked a long time out from the appointment day may be more likely to be cancelled as the patient forgets, and these cases wouldn't include urgent appointments","52bd6c85":"## Looking at the data","461628d8":"# Pre-Processing","dda08d44":"The most accurate algorithms were Logistic Regression and XGBoost at 79.8% and 78.9% respectively. As XGBoost had a much higher fit time we would continue with Logistic Regression to validate its performance compared to a baseline. This would determine if the model performs well enough to productionalise.","3cc034bd":"### Cleaning","bc7703ac":"## Data Analysis","dbb6a0f2":"### Correcting","4b40d737":"### Data Dictionary\n* 01 - PatientId - Identification of a patient\n* 02 - AppointmentID - Identification of each appointment\n* 03 - Gender - Male or Female\n* 04 - ScheduledDay - The day of the appointment\n* 05 - AppointmentDay - The day the appointment was booked\n* 06 - Age - How old the patient is in years\n* 07 - Neighbourhood - Where the appointment takes place\n* 08 - Scholarship - True of False; whether the patient has government medical support\n* 09 - Hipertension - True or False\n* 10 - Diabetes - True or False\n* 11 - Alcoholism - True or False\n* 12 - Handcap - True or False\n* 13 - SMS_received - 1 or more messages sent to the patient.\n* 14 - No-show - True or False. The target variable","e6d7f42f":"### Feature Importances with XGBoost","97cae1a7":"As Age is the only numeric variable, and we want to handle all possible age ranges, we do not investigate the presence of outliers.","17da2774":"### Preparing the data for modelling and analysis","c388813e":"### Completing","5b995598":"## Feature Engineering","eddbfef1":"# Introduction\n\nOne significant cost in the medical industry is appointment no-shows: when a patient boooks an appointment but does not attend and does not notify the clinic ahead of time. This is different to a cancellation because the clinic does not have the opportunity to book another patient in the same time slot. No-shows are costly because it is lost income. Many clinics implement processes to avoid no-shows such as SMS & email reminders and cancellation fees. \n\nThis notebook attempts to predict whether a patient will no-show or not. This prediction is valuable as the clinics could then implement new methods to handle these specific cases. Additionally, it would assist clinics to identify which features are likely to lead to no-shows. \n\nThe dataset used was provided on kaggle by username JoniHoppen in 2017 (https:\/\/www.kaggle.com\/joniarroba\/noshowappointments)."}}