{"cell_type":{"980639e8":"code","79625350":"code","9e2e134d":"code","03b94b2e":"code","2b8f33df":"code","485d61cd":"code","d47a4514":"code","b88267da":"code","dacf9794":"code","e8f29c2b":"code","06f01476":"code","f455ddb9":"code","572337d6":"code","35bb1e17":"code","aef57984":"code","71c0155b":"code","29845316":"code","863127fd":"code","46920e04":"code","f4746809":"code","8322857e":"code","783d97e9":"code","1a74b6eb":"code","23f84625":"code","1363744c":"code","56492f87":"code","b005d043":"code","7fde59d0":"code","97dbfd31":"code","847659ed":"code","6310e588":"code","0287599c":"code","e4ebe467":"code","f61bba51":"code","91877825":"code","4a0f3c92":"code","b88f11ee":"code","1c30ed0d":"code","e574a027":"code","8d14360d":"code","323cfe11":"code","747d044d":"code","5df5bda7":"code","1f2d2192":"code","50166a80":"markdown","3daf5a10":"markdown","ef00e5ac":"markdown","2bdbba97":"markdown"},"source":{"980639e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79625350":"# Packages used to view data analysis graphs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Metrics packages to analyze the efficiency of the algorithm \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n \n# Packages to standardize, normalize data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Package to share traning and test data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score \n\n# Package to generating the predective model report\nfrom sklearn.metrics import classification_report\n\n# Pacotes de Modelos preditivos\n# Predective models packages\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# Attribute selection and dimensioning reduction packages \nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\n\n# package to balance classes\nfrom imblearn.over_sampling import SMOTE\n\n# Pipeline package\nfrom sklearn.pipeline import Pipeline\n\n# Package to not show the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9e2e134d":"# importing the training file with the target variable\ntrain = pd.read_csv(\"\/kaggle\/input\/santander-customer-satisfaction\/train.csv\")","03b94b2e":"train.describe()","2b8f33df":"test = pd.read_csv(\"\/kaggle\/input\/santander-customer-satisfaction\/test.csv\")","485d61cd":"submission = pd.read_csv(\"\/kaggle\/input\/santander-customer-satisfaction\/sample_submission.csv\")","d47a4514":"train.drop(['ID'], axis=1, inplace=True)\ntest.drop(['ID'], axis=1, inplace=True)","b88267da":"test.head()","dacf9794":"submission.head()","e8f29c2b":"train.isna().sum()","06f01476":"# Veriying the the shape of the data train and data test\ntrain.shape, test.shape","f455ddb9":"# Loading training data and test data in one Dataset\nX_data = pd.concat((train.loc[:,'var3':'var38'], \n                      test.loc[:,'var3':'var38']))","572337d6":"X_data","35bb1e17":"# Checking and deleting duplicate columns:\n# checking the shape, decreased\ncol_duplicates = []\ncolumns = X_data.columns\nfor i in range(len(columns)-1):\n    s = X_data[columns[i]].values\n    for j in range(i+1, len(columns)):\n        if np.array_equal(s, X_data[columns[j]].values):\n            col_duplicates.append(columns[j])\n        \ntrain.drop(col_duplicates, axis=1, inplace=True)\ntest.drop(col_duplicates, axis=1, inplace=True)\nX_data.drop(col_duplicates, axis=1, inplace=True)\n\n# Checking shape\ntrain.shape, test.shape, X_data.shape","aef57984":"# Removes columns with constant values\n\ncols_Remove = []\nfor col in X_data.columns:\n    if X_data[col].std() == 0:\n        cols_Remove.append(col)\n        \ntrain.drop(cols_Remove, axis=1, inplace=True)\ntest.drop(cols_Remove, axis=1, inplace=True)\nX_data.drop(cols_Remove, axis=1, inplace=True)\n\ntrain.shape, test.shape, X_data.shape","71c0155b":"# top 5 values more communs\n# Note that the negative value -999999 is incompatible to enter the predictive model algorithm\ntrain.var3.value_counts()[:5], test.var3.value_counts()[:5], X_data.var3.value_counts()[:5]","29845316":"# 116 values of -999999 were found in column var3 which is suspected to be the client's nationality and\n# the value -999999 is can be said to be the nationality unknown to the client or has not been placed\n\n\ntrain.loc[train.var3==-999999].shape, test.loc[test.var3==-999999].shape, X_data.loc[X_data.var3==-999999].shape ","863127fd":"# So we are going to replace the value -999999 by the value 2 fashion, which is the most repeated value  \n\ntrain.var3 = train.var3.replace(-999999,2)\ntest.var3 = test.var3.replace(-999999,2)\nX_data.var3 = X_data.var3.replace(-999999,2)\n\ntrain.loc[train.var3==999999].shape, test.loc[test.var3==999999].shape, X_data.loc[X_data.var3==999999].shape","46920e04":"train.shape, test.shape, X_data.shape","f4746809":"# Calculating the standard deviation of each column and removing columns with a standard deviation less than or equal to 0.07\n# then 52 columns remain.\n# Based on the standard deviation of the VAR3 variable, \n# any variable that has a standard deviation less than 0.041125 will be excluded.\n\nXNR = X_data.copy()\ncols_Remove = []\n\nfor col in XNR.columns:\n    if XNR[col].std() < 0.07:\n        cols_Remove.append(col)\n        \nXNR.drop(cols_Remove, axis=1, inplace=True)\ntrain.drop(cols_Remove, axis=1, inplace=True)\ntest.drop(cols_Remove, axis=1, inplace=True)\n\nCol_Excluidas = len(X_data.columns) - len(XNR.columns)\n\nmsg = '%s columns were excluded  \\nand %s colunms left' % (len(cols_Remove), len(XNR.columns))\n\nprint(msg)","8322857e":"train.shape, test.shape, XNR.shape","783d97e9":"# Happy customers have TARGET==0, unhappy custormers have TARGET==1.\n# The most customers are classified satisfied customers, almost 4% are just dissatisfied customers.\n# The TARGET variable is not balanced.\ndf = pd.DataFrame(train.TARGET.value_counts())\ndf['Porcentagem'] = 100*df['TARGET']\/train.shape[0]\ndf","1a74b6eb":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE","23f84625":"X = train.drop(['TARGET'], axis=1)\ny = train['TARGET']","1363744c":"smote= SMOTE()\nx_smote, y_smote= smote.fit_resample(X, y)\nprint('Original dataset shape:', Counter(y))\nprint('Resample dataset shape:', Counter(y_smote))","56492f87":"df = pd.DataFrame(y_smote.value_counts())\ndf['Porcentagem'] = 100*y_smote\/x_smote.shape[0]\ndf","b005d043":"x_smote_ID = x_smote.index\nx_smote.head()","7fde59d0":"x_smote_columns = x_smote.columns","97dbfd31":"test_ID = test.index\ntest.head()","847659ed":"test_columns = test.columns","6310e588":"from sklearn.preprocessing import StandardScaler","0287599c":"#%config Completer.use_jedi = False","e4ebe467":"sc = StandardScaler()","f61bba51":"test = sc.fit_transform(test)\ntest = pd.DataFrame(test,columns=test_columns,index=test_ID)\ntest.head()","91877825":"x_smote = sc.fit_transform(x_smote)\nx_smote = pd.DataFrame(x_smote,columns=x_smote_columns,index=x_smote_ID)\nx_smote.head()","4a0f3c92":"# Data Normalization:\n# x_smote = x_smote.apply(lambda x: (x - x.min(axis=0)) \/ (x.max(axis=0) - x.min(axis=0)))\n# test = test.apply(lambda x: (x - x.min(axis=0)) \/ (x.max(axis=0) - x.min(axis=0)))\n","b88f11ee":"from sklearn.tree import DecisionTreeClassifier","1c30ed0d":"# X_train = x_smote.copy()\n# y_train = train['TARGET']\n# X_test = test.copy()\n# y_test = submission['TARGET']\n","e574a027":"train.head()","8d14360d":"x_smote.head()","323cfe11":"train.shape, x_smote.shape","747d044d":"%%time\n\n# The CART algorithm showed the best precision among the others\narray = x_smote\nlabels = array.columns\n\nX = array[labels]\ny = train['TARGET']\n    \nX_resampled, y_resampled = SMOTE(sampling_strategy=0.2).fit_resample(X, y)   \n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, \n                                                    y_resampled, \n                                                    test_size=0.2, \n                                                    random_state=1)\n\nmodel = DecisionTreeClassifier()\nmodelo = model.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nmsg = \" Accuracy: %.2f%%\" % ((accuracy * 100.0))    \n\n\nprint(msg)\n# Making predictions and building the report\nreport = classification_report(y_test, y_pred)\n\n# Printing the report\nprint(report)","5df5bda7":"%%time\n# Confusion Matrix\n# Now let's check the accuracy in a table format with DecisionTreeClassifier (CART)\n\n# loading and share data in predictive variables and the target variable\narray = XNR\nlabels = array.columns\n\nX = array[labels]\ny = Y_dados\n\n\n# Splitting data into training and testing\nmodel = SMOTE()\nX_resampled, y_resampled = model.fit_resample(X, y)   \n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, \n                                                    y_resampled, \n                                                    test_size=0.2, \n                                                    random_state=1)\n\n# Creating a model\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Making predictions and building the Confusion Matrix\nprevisoes = model.predict(X_test)\nmatrix = confusion_matrix(y_test, previsoes)\n\n# Printing Confusion Matrix\nprint(matrix)\nprint(previsoes)\nprint(model.feature_importances_)","1f2d2192":"%%time\n\n# Loading datas\n\narray = XNR\nlabels = array.columns\n\nX = array[labels]\ny = Y_dados\n    \n# Separating training data and test data\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=1)\n\n# Division of training data into training data and validation data\nx_train_res, x_val, y_train_res, y_val = train_test_split(X_train, y_train,\n                                                  test_size = .1,\n                                                  random_state=12)\n\n# Applying SMOTE for class balancing\nmodeloSMOTE = SMOTE(sampling_strategy='all', k_neighbors=5)\nX_resampled, y_resampled = modeloSMOTE.fit_sample(x_train_res, y_train_res)\n        \n# Creating the model\nmodel = DecisionTreeClassifier()\nmodelo = model.fit(X_resampled, y_resampled)\ny_pred = modelo.predict(X_test)\n          \n# Evaluating the model and updating the accuracy list\nscore = model.score(x_val, y_val)\nprint(\"Accuracy is = %.2f%%\" % ( score * 100))\n\n# Making predictions and building the report\nreport = classification_report(y_test, y_pred)\n\n# Printing the report\nprint(report)","50166a80":"# Confusion Matrix \nMetric used to check the target balancing of target classes","3daf5a10":"# Classification Report for CART","ef00e5ac":"In this project, was developed a predictive model to identify unhappy customers at the beginning of the relationship with Santander Bank. \nIt was based on historical information collected from the past and current customers, making it possible to predict the satisfaction of new customers.","2bdbba97":"# Project Predicting the Satisfaction Level of Santander Bank Customers"}}