{"cell_type":{"062c68e5":"code","4ae6e184":"code","374c42b9":"code","f4a2c1db":"code","bfdc1942":"code","303116e2":"code","464468d0":"code","595bc4c8":"code","b90c7c4a":"code","326f6b60":"code","a5e60493":"code","2c132ba9":"code","a5982987":"code","7cbb8f45":"code","94b8ccc9":"code","491aa892":"code","b9f5ec81":"code","4005b082":"code","0e610e61":"code","cf2fee3c":"code","583f17ca":"code","785d3798":"code","d504a361":"code","e1f6f014":"code","51625606":"code","30428f56":"code","dc8b927c":"code","976108c3":"code","1e5a11cc":"code","c57f6625":"code","cdaa705b":"code","11553bfb":"code","7c4e9d07":"code","bb0e8ec9":"code","7eb5aba6":"code","fa5cc66f":"code","88df1fac":"code","56f6b0bc":"code","c4fd135e":"code","8e9ec510":"code","7a24e94a":"code","d2f2d615":"code","389535f0":"code","dbcc6151":"code","6107aab3":"code","14d92bdb":"code","fcd52452":"code","f78a058f":"code","c8c8a19c":"code","e2fae1dd":"code","dd981996":"code","78ff9854":"code","e6955bb9":"code","0fb9c7e9":"code","681dde53":"code","720f9671":"code","d3e9e1bf":"code","1a3af041":"code","a304b911":"code","2d3d8fa3":"code","2e54ba82":"code","28f98432":"code","4a403c6e":"code","67c22095":"code","89d808c1":"code","b2557b4f":"code","96300f45":"code","a87a0a47":"code","e20c2412":"code","7576b2d6":"code","735709e7":"code","62fd1f7c":"code","b43dde3d":"code","74ce63b3":"code","c5b23fd7":"code","1be311e3":"code","9585a6c3":"code","be7bf934":"code","53ed3c8d":"code","9f480f80":"code","bb68cf31":"code","11a3d200":"code","7e8dc2b5":"code","27ef1791":"code","b02e0e4c":"code","263069a2":"code","481d2a6f":"code","5cbd0199":"code","66994efc":"code","6a1db20f":"code","18929fa7":"code","f1e60053":"code","bc1e35c7":"code","12ea8149":"code","69ee218f":"code","a12e6ba3":"code","5cbd7f9c":"code","8f22934f":"code","b12e583e":"code","baaef3a8":"code","7d22037c":"code","4fd91ec5":"code","c44de84c":"code","3e148007":"code","082543b2":"code","4cc7434c":"code","7ab7a314":"code","8f6da3e4":"code","fc19cd44":"code","6ff85ed1":"code","282e210a":"code","f060f212":"code","4036bd01":"code","db132b77":"code","33454467":"code","5052fbde":"code","53a23216":"code","cbb8e7e0":"code","eae0edd1":"code","b4128d72":"code","d7b34424":"code","7c6b6b93":"code","761f9ba7":"code","4eec72f3":"code","65a653bb":"code","3400801c":"code","efcf4656":"code","a061222d":"code","8be1c0b9":"code","019785a9":"code","25308edd":"code","01065962":"code","533f67b3":"code","aa75bcc6":"code","d14e4a95":"code","f5329cff":"code","ed5d0988":"code","2b8cbaa2":"code","51a9cac4":"code","c42a96d0":"code","6a566907":"code","5ae8db5d":"code","882fce60":"code","30e24cec":"code","2758dd35":"code","c2622e3a":"code","efffff51":"code","8e64eff4":"code","cefd79bb":"code","188801a3":"code","7c470e99":"code","0b225eb2":"code","e0121030":"code","95bbb378":"code","2fdd69b6":"code","f817b230":"code","52887d0c":"code","004463a7":"code","4566a052":"code","9fa8f31e":"code","e7448858":"code","2084736f":"code","4a03a12e":"code","32c2eb8f":"code","bea272d7":"markdown","c8613f9f":"markdown","e6c6b92e":"markdown","d3d026c3":"markdown","09b00afd":"markdown","ceced6a5":"markdown","beb567e2":"markdown","89ead69c":"markdown","1641b7dd":"markdown","21fa8812":"markdown","bdec9c79":"markdown","12298f5d":"markdown","4f27f5ed":"markdown","a1c10361":"markdown","80a285df":"markdown","ccfcf798":"markdown","41da91af":"markdown","5c39f855":"markdown","0a581ecc":"markdown","70b80620":"markdown","320751b6":"markdown","fff9b0c2":"markdown","7e86c023":"markdown","ee9c47d0":"markdown","20c6d394":"markdown","373f6fa0":"markdown","a0e33322":"markdown","e9497f8f":"markdown","74e95336":"markdown","6f28200a":"markdown","c5d74297":"markdown","6442ddab":"markdown","0d134cdf":"markdown","edc3bf83":"markdown","bcd9bcbc":"markdown","06da5a9c":"markdown","801cff50":"markdown","7575edb4":"markdown","082e507e":"markdown","b031f096":"markdown","2f546aaf":"markdown","cd18c171":"markdown","f431e9b2":"markdown","f1c33d09":"markdown","0bf6758e":"markdown","781ea28c":"markdown","27905b30":"markdown","8528ba73":"markdown","269fc498":"markdown","85594d4e":"markdown","4ffabdb8":"markdown","83c4f7fc":"markdown","fe766274":"markdown","ce9f00b7":"markdown","62bbdf06":"markdown","707ad67f":"markdown","a6131066":"markdown","f4ad3bbd":"markdown","001625d3":"markdown","518e859a":"markdown","0ce0d2ff":"markdown","caa0c25c":"markdown","e53b2e0a":"markdown","bb7dc7a6":"markdown","db83f01a":"markdown","11360431":"markdown","d40b3db3":"markdown","31aa579f":"markdown","77d27e6d":"markdown","dc6601e1":"markdown","1542b76b":"markdown","bab8f9e5":"markdown","57307154":"markdown","7331a6c8":"markdown","b7ed8513":"markdown","00dfe9af":"markdown","5a4c2229":"markdown","ebda2d62":"markdown","8d69d833":"markdown","b466ece3":"markdown","06215b67":"markdown","48a20d85":"markdown","73a1bcb9":"markdown","f92027b7":"markdown","171595cb":"markdown","b8343d65":"markdown","5911491a":"markdown","6b2b5ce7":"markdown","49344d76":"markdown","159885c3":"markdown","6caf8361":"markdown","d15d4597":"markdown","1a322d3f":"markdown","e99ad74b":"markdown","4adbaee1":"markdown","75a36cfb":"markdown"},"source":{"062c68e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","4ae6e184":"pd.set_option('display.max_columns',999)","374c42b9":"import statsmodels.api as sm","f4a2c1db":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","bfdc1942":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","303116e2":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","464468d0":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","595bc4c8":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","b90c7c4a":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip","326f6b60":"from statsmodels.stats import diagnostic as diag\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","a5e60493":"from scipy.stats import normaltest,f_oneway\nfrom scipy.stats import ttest_ind","2c132ba9":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs","a5982987":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor","7cbb8f45":"dt = DecisionTreeRegressor()\net = ExtraTreeRegressor()","94b8ccc9":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","491aa892":"abr = AdaBoostRegressor()\nbr = BaggingRegressor()\netr = ExtraTreesRegressor()\ngbr = GradientBoostingRegressor()\nrfr = RandomForestRegressor()","b9f5ec81":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","4005b082":"data=pd.read_csv('\/kaggle\/input\/ibm-watson-marketing-customer-value-data\/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\ndata.head()","0e610e61":"data.isnull().sum()","cf2fee3c":"data[['Customer Lifetime Value','Income','Monthly Premium Auto','Total Claim Amount']].describe()","583f17ca":"data.shape","785d3798":"sns.boxplot(data['Income'])\nplt.show()","d504a361":"sns.boxplot(data['Monthly Premium Auto'])\nplt.show()","e1f6f014":"sns.boxplot(data['Total Claim Amount'])\nplt.show()","51625606":"sns.distplot(data['Income'])\nplt.show()","30428f56":"sns.distplot(data['Monthly Premium Auto'])\nplt.show()","dc8b927c":"sns.distplot(data['Total Claim Amount'])\nplt.show()","976108c3":"sns.distplot(data['Income']**2)\nplt.show()","1e5a11cc":"sns.distplot(data['Income']**(1\/2))\nplt.show()","c57f6625":"sns.distplot(data['Monthly Premium Auto']**(2))\nplt.show()","cdaa705b":"sns.distplot(data['Total Claim Amount']**2)\nplt.show()","11553bfb":"sns.barplot(x = 'Location Code',y='Customer Lifetime Value',data = data)\nplt.show()","7c4e9d07":"sns.barplot(x = 'State',y='Customer Lifetime Value',data = data)\nplt.show()","bb0e8ec9":"sns.barplot(x = 'Response',y='Customer Lifetime Value',data = data)\nplt.show()","7eb5aba6":"sns.barplot(x = 'Gender',y='Customer Lifetime Value',data = data)\nplt.show()","fa5cc66f":"sns.barplot(x = 'Education',y='Customer Lifetime Value',data = data)\nplt.xticks(rotation=45)\nplt.show()","88df1fac":"sns.barplot(x = 'Number of Policies',y='Customer Lifetime Value',data = data)\nplt.show()","56f6b0bc":"sns.barplot(x = 'Policy Type',y='Customer Lifetime Value',data = data)\nplt.xticks(rotation = 90)\nplt.show()","c4fd135e":"sns.barplot(x = 'Coverage',y='Customer Lifetime Value',data = data)\nplt.show()","8e9ec510":"sns.barplot(x = 'Number of Open Complaints',y='Customer Lifetime Value',data = data)\nplt.show()","7a24e94a":"sns.pairplot(y_vars='Customer Lifetime Value',x_vars=['Income','Monthly Premium Auto','Total Claim Amount'],data = data)\nplt.show()","d2f2d615":"sns.heatmap(data[['Customer Lifetime Value','Monthly Premium Auto','Income','Total Claim Amount']].corr(),annot = True)\nplt.show()","389535f0":"cols = data.select_dtypes(object).columns\nfor i in cols:\n    data[i] = le.fit_transform(data[i])","dbcc6151":"X = data.drop('Customer Lifetime Value',axis=1)\ny = data['Customer Lifetime Value']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)","6107aab3":"from sklearn.model_selection import train_test_split\n# train data - 70% and test data - 30%\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(y_train.shape)","14d92bdb":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","fcd52452":"X_sm = X\nX_sm = sm.add_constant(X_sm)\nlm = sm.OLS(y,X_sm).fit()\nlm.summary()","f78a058f":"sns.pairplot(x_vars=['Monthly Premium Auto','Total Claim Amount','Income'],y_vars =['Customer Lifetime Value'],data = data)\nplt.show()","c8c8a19c":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100)","e2fae1dd":"lr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nresiduals = y_pred-y_test\nmean_of_residuals = np.mean(residuals)\nprint(f\"The mean of the residuals is {mean_of_residuals}\")","dd981996":"name = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals,X_test)\nlzip(name, test)","78ff9854":"p = sns.distplot(residuals,kde=True)\np = plt.title('Normality of error terms\/residuals')","e6955bb9":"min(diag.acorr_ljungbox(residuals , lags = 40)[1])","0fb9c7e9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_sm.values, i) for i in range(X_sm.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","681dde53":"data=pd.read_csv('\/kaggle\/input\/ibm-watson-marketing-customer-value-data\/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')","720f9671":"State = data.groupby('State')\nWashington = State.get_group('Washington')['Customer Lifetime Value']\nArizona = State.get_group('Arizona')['Customer Lifetime Value']\nNevada = State.get_group('Nevada')['Customer Lifetime Value']\nCalifornia = State.get_group('California')['Customer Lifetime Value']\nOregon = State.get_group('Oregon')['Customer Lifetime Value']","d3e9e1bf":"for i in [Washington,Arizona,Nevada,California,Oregon]:\n    print(normaltest(i),'\\n')","1a3af041":"f_oneway(Washington,Arizona,Nevada,California,Oregon)","a304b911":"Response = data[['Customer Lifetime Value','Response']].groupby('Response')\nNo = Response['Customer Lifetime Value'].get_group('No')\nYes = Response['Customer Lifetime Value'].get_group('Yes')","2d3d8fa3":"for i in [No,Yes]:\n    print(normaltest(i),'\\n')","2e54ba82":"ttest_ind(No,Yes)","28f98432":"Coverage = data[['Customer Lifetime Value','Coverage']].groupby('Coverage')\nbasic = Coverage['Customer Lifetime Value'].get_group('Basic')\nextended = Coverage['Customer Lifetime Value'].get_group('Extended')\npremium = Coverage['Customer Lifetime Value'].get_group('Premium')","4a403c6e":"for i in [basic,extended,premium]:\n    print(normaltest(i),'\\n')","67c22095":"f_oneway(basic,extended,premium)","89d808c1":"Education = data[['Customer Lifetime Value','Education']].groupby('Education')\nbachelor = Education['Customer Lifetime Value'].get_group('Bachelor')\ncollege = Education['Customer Lifetime Value'].get_group('College')\nhighschool = Education['Customer Lifetime Value'].get_group('High School or Below')\nmaster = Education['Customer Lifetime Value'].get_group('Master')\ndoctor = Education['Customer Lifetime Value'].get_group('Doctor')","b2557b4f":"for i in [basic,college,highschool,master,doctor]:\n    print(normaltest(i),'\\n')","96300f45":"f_oneway(bachelor,college,highschool,master,doctor)","a87a0a47":"es = data[['Customer Lifetime Value','EmploymentStatus']].groupby('EmploymentStatus')\nemployed = es['Customer Lifetime Value'].get_group('Employed')\nunemployed = es['Customer Lifetime Value'].get_group('Unemployed')\nmedleave = es['Customer Lifetime Value'].get_group('Medical Leave')\ndisabled = es['Customer Lifetime Value'].get_group('Disabled')\nretired = es['Customer Lifetime Value'].get_group('Retired')","e20c2412":"for i in [employed,unemployed,medleave,disabled,retired]:\n    print(normaltest(i),'\\n')","7576b2d6":"f_oneway(employed,unemployed,medleave,disabled,retired)","735709e7":"g = data[['Customer Lifetime Value','Gender']].groupby('Gender')\nf = g['Customer Lifetime Value'].get_group('F')\nm = g['Customer Lifetime Value'].get_group('M')","62fd1f7c":"for i in [f,m]:\n    print(normaltest(i),'\\n')","b43dde3d":"ttest_ind(f,m)","74ce63b3":"location = data[['Customer Lifetime Value','Location Code']].groupby('Location Code')\nsub = location['Customer Lifetime Value'].get_group('Suburban')\nurban = location['Customer Lifetime Value'].get_group('Urban')\nrural = location['Customer Lifetime Value'].get_group('Rural')","c5b23fd7":"for i in [sub,urban,rural]:\n    print(normaltest(i),'\\n')","1be311e3":"f_oneway(sub,urban,rural)","9585a6c3":"MaritalStatus = data[['Customer Lifetime Value','Marital Status']].groupby('Marital Status')\nMarried = MaritalStatus['Customer Lifetime Value'].get_group('Married')\nSingle = MaritalStatus['Customer Lifetime Value'].get_group('Single')\nDivorced = MaritalStatus['Customer Lifetime Value'].get_group('Divorced')","be7bf934":"for i in [Married,Single,Divorced]:\n    print(normaltest(i),'\\n')","53ed3c8d":"f_oneway(Married,Single,Divorced)","9f480f80":"Policy  = data[['Customer Lifetime Value','Policy']].groupby('Policy')\np3 = Policy['Customer Lifetime Value'].get_group('Personal L3')\np2 = Policy['Customer Lifetime Value'].get_group('Personal L2')\np1 = Policy['Customer Lifetime Value'].get_group('Personal L1')\nc3 = Policy['Customer Lifetime Value'].get_group('Corporate L3')\nc2 = Policy['Customer Lifetime Value'].get_group('Corporate L2')\nc1 = Policy['Customer Lifetime Value'].get_group('Corporate L1')\ns3 = Policy['Customer Lifetime Value'].get_group('Special L3')\ns2 = Policy['Customer Lifetime Value'].get_group('Special L2')\ns1 = Policy['Customer Lifetime Value'].get_group('Special L1')","bb68cf31":"for i in [p3,p2,p1,c3,c2,c1,s3,s2,s1]:\n    print(normaltest(i),'\\n')","11a3d200":"f_oneway(p3,p2,p1,c3,c2,c1,s3,s2,s1)","7e8dc2b5":"R  = data[['Customer Lifetime Value','Renew Offer Type']].groupby('Renew Offer Type')\no1 = R['Customer Lifetime Value'].get_group('Offer1')\no2 = R['Customer Lifetime Value'].get_group('Offer2')\no3 = R['Customer Lifetime Value'].get_group('Offer3')\no4 = R['Customer Lifetime Value'].get_group('Offer4')","27ef1791":"for i in [o1,o2,o3,o4]:\n    print(normaltest(i),'\\n')","b02e0e4c":"f_oneway(o1,o2,o3,o4)","263069a2":"Sales  = data[['Customer Lifetime Value','Sales Channel']].groupby('Sales Channel')\nagent = Sales['Customer Lifetime Value'].get_group('Agent')\nbranch = Sales['Customer Lifetime Value'].get_group('Branch')\ncall = Sales['Customer Lifetime Value'].get_group('Call Center')\nweb = Sales['Customer Lifetime Value'].get_group('Web')","481d2a6f":"for i in [agent,branch,call,web]:\n    print(normaltest(i),'\\n')","5cbd0199":"f_oneway(agent,branch,call,web)","66994efc":"VC  = data[['Customer Lifetime Value','Vehicle Class']].groupby('Vehicle Class')\nfd = VC['Customer Lifetime Value'].get_group('Four-Door Car')\ntd = VC['Customer Lifetime Value'].get_group('Two-Door Car')\nsuv = VC['Customer Lifetime Value'].get_group('SUV')\nsc = VC['Customer Lifetime Value'].get_group('Sports Car')\nls = VC['Customer Lifetime Value'].get_group('Luxury SUV')\nlc = VC['Customer Lifetime Value'].get_group('Luxury Car')","6a1db20f":"for i in [fd,td,suv,sc,ls,lc]:\n    print(normaltest(i),'\\n')","18929fa7":"f_oneway(fd,td,suv,sc,ls,lc)","f1e60053":"VS  = data[['Customer Lifetime Value','Vehicle Size']].groupby('Vehicle Size')\nm = VS['Customer Lifetime Value'].get_group('Medsize')\ns = VS['Customer Lifetime Value'].get_group('Small')\nl = VS['Customer Lifetime Value'].get_group('Large')","bc1e35c7":"for i in [m,s,l]:\n    print(normaltest(i),'\\n')","12ea8149":"f_oneway(m,s,l)","69ee218f":"data.drop(['State','Customer','Response','EmploymentStatus','Gender','Location Code','Vehicle Size','Policy','Policy Type','Sales Channel','Income','Effective To Date','Education'],axis=1,inplace = True)","a12e6ba3":"data.head()","5cbd7f9c":"data['Number of Policies'] = np.where(data['Number of Policies']>2,3,data['Number of Policies'])","8f22934f":"new = pd.get_dummies(data,columns=['Coverage','Marital Status','Number of Policies','Renew Offer Type','Vehicle Class'],drop_first=True)","b12e583e":"new.head()","baaef3a8":"X = new.drop('Customer Lifetime Value',axis=1)\ny = new['Customer Lifetime Value']","7d22037c":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100)","4fd91ec5":"lr.fit(X_train,y_train)","c44de84c":"lr.score(X_test,y_test)","3e148007":"lr.score(X_train,y_train)","082543b2":"sfs = SFS(lr, k_features='best', forward=True, floating=False, \n          scoring='neg_mean_squared_error', cv=20)\nmodel = sfs.fit(new.drop('Customer Lifetime Value', axis=1),new['Customer Lifetime Value'])\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","4cc7434c":"print('Selected features:', sfs.k_feature_idx_)","7ab7a314":"sfs = SFS(lr, k_features='best', forward=False, floating=False, \n          scoring='neg_mean_squared_error', cv=20)\nmodel = sfs.fit(new.drop('Customer Lifetime Value', axis=1).values,new['Customer Lifetime Value'])\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Backward Selection (w. StdErr)')\nplt.grid()\nplt.show()","8f6da3e4":"print('Selected features:', sfs.k_feature_idx_)","fc19cd44":"X.columns","6ff85ed1":"test_X = X[['Monthly Premium Auto','Number of Open Complaints','Total Claim Amount','Coverage_Premium',\n            'Marital Status_Single','Number of Policies_2','Number of Policies_3',\n            'Renew Offer Type_Offer2','Vehicle Class_SUV','Vehicle Class_Sports Car']]","282e210a":"train = []\ntest = []","f060f212":"X_train,X_test,y_train,y_test = train_test_split(test_X,y,test_size=0.3,random_state=100)","4036bd01":"lr.fit(X_train,y_train)","db132b77":"test.append(lr.score(X_test,y_test))","33454467":"train.append(lr.score(X_train,y_train))","5052fbde":"metrics = [r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error]","53a23216":"y_pred = lr.predict(X_test)","cbb8e7e0":"r2 = []\nmae = []\nmape = []\nmse = []","eae0edd1":"for i in metrics:\n    print(i(y_test,y_pred))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred))\n    else:\n        mse.append(i(y_test,y_pred))","b4128d72":"algo = [abr,gbr,dt,et,etr,br,rfr]","d7b34424":"for i in algo:\n    temp = 0\n    print(f\"New Model{i}\")\n    for j in range(1,300,1):\n        NXT,NXt,NYT,NYt = train_test_split(X,y,test_size=0.3,random_state=j)\n        i.fit(NXT,NYT)\n        test_score = i.score(NXt,NYt)\n        train_score = i.score(NXT,NYT)\n        if test_score>temp:\n            temp = test_score\n            print(j,train_score,temp)","7c6b6b93":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=159)","761f9ba7":"dt.fit(X_train,y_train)","4eec72f3":"y_pred_dt = dt.predict(X_test)","65a653bb":"for i in metrics:\n    print(i(y_test,y_pred_dt))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_dt))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_dt))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_dt))\n    else:\n        mse.append(i(y_test,y_pred_dt))","3400801c":"train.append(dt.score(X_train,y_train))\ntest.append(dt.score(X_test,y_test))","efcf4656":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=69)","a061222d":"et.fit(X_train,y_train)","8be1c0b9":"y_pred_et = et.predict(X_test)","019785a9":"for i in metrics:\n    print(i(y_test,y_pred_et))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_et))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_et))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_et))\n    else:\n        mse.append(i(y_test,y_pred_et))","25308edd":"train.append(et.score(X_train,y_train))\ntest.append(et.score(X_test,y_test))","01065962":"pd.DataFrame({'Model':['Linear Regression','Decision Tree','Extra Tree'],'R2_Score':r2,'MAE':mae,'MAPE':mape,'MSE':mse})","533f67b3":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","aa75bcc6":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=83)","d14e4a95":"abr.fit(X_train,y_train)","f5329cff":"y_pred_abr = abr.predict(X_test)","ed5d0988":"for i in metrics:\n    print(i(y_test,y_pred_abr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_abr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_abr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_abr))\n    else:\n        mse.append(i(y_test,y_pred_abr))","2b8cbaa2":"train.append(abr.score(X_train,y_train))\ntest.append(abr.score(X_test,y_test))","51a9cac4":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=292)\nbr.fit(X_train,y_train)\ny_pred_br = br.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_br))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_br))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_br))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_br))\n    else:\n        mse.append(i(y_test,y_pred_br))","c42a96d0":"train.append(br.score(X_train,y_train))\ntest.append(br.score(X_test,y_test))","6a566907":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=69)\netr.fit(X_train,y_train)\ny_pred_etr = etr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_etr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_etr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_etr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_etr))\n    else:\n        mse.append(i(y_test,y_pred_etr))","5ae8db5d":"train.append(etr.score(X_train,y_train))\ntest.append(etr.score(X_test,y_test))","882fce60":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=181)\ngbr.fit(X_train,y_train)\ny_pred_gbr = gbr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_gbr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_gbr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_gbr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_gbr))\n    else:\n        mse.append(i(y_test,y_pred_gbr))","30e24cec":"train.append(gbr.score(X_train,y_train))\ntest.append(gbr.score(X_test,y_test))","2758dd35":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=35)\nrfr.fit(X_train,y_train)\ny_pred_rfr = rfr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_rfr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_rfr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_rfr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_rfr))\n    else:\n        mse.append(i(y_test,y_pred_rfr))","c2622e3a":"train.append(rfr.score(X_train,y_train))\ntest.append(rfr.score(X_test,y_test))","efffff51":"hyper_params_gbr = {'loss':['ls','lad','huber'],'learning_rate':[0.1,0.01,1],'n_estimators':[100,150]}","8e64eff4":"gbr2 = GradientBoostingRegressor()","cefd79bb":"model = GridSearchCV(gbr2,param_grid=hyper_params_gbr)","188801a3":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=181)","7c470e99":"model.fit(X_train,y_train)","0b225eb2":"model.score(X_test,y_test)","e0121030":"model.score(X_train,y_train)","95bbb378":"model.best_params_","2fdd69b6":"gbr2 = GradientBoostingRegressor()","f817b230":"br2 = BaggingRegressor(gbr2)","52887d0c":"temp = 0\nfor j in range(1,300,1):\n    NXT,NXt,NYT,NYt = train_test_split(X,y,test_size=0.3,random_state=j)\n    br2.fit(NXT,NYT)\n    test_score = br2.score(NXt,NYt)\n    train_score = br2.score(NXT,NYT)\n    if test_score>temp:\n        temp = test_score\n        print(j,train_score,temp)","004463a7":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=87)\nbr2.fit(X_train,y_train)\ny_pred_br2 = br2.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_br2))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_br2))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_br2))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_br2))\n    else:\n        mse.append(i(y_test,y_pred_br2))","4566a052":"train.append(br2.score(X_train,y_train))\ntest.append(br2.score(X_test,y_test))","9fa8f31e":"test_scores = []\ntrain_scores = []\ncv = KFold(n_splits=10,random_state=42, shuffle=False)\nfor train_index,test_index in cv.split(X):\n    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n    br2.fit(X_train,y_train)\n    test_scores.append(br.score(X_test, y_test))\n    train_scores.append(br.score(X_train, y_train))","e7448858":"np.mean(train_scores)","2084736f":"np.mean(test_scores)","4a03a12e":"ALL_SCORES = pd.DataFrame({'Model':['Linear Regression','Decision Tree','Extra Tree','AdaBoost','Bagging',\n                                    'Extra Trees','GradientBoosting','Random Forest','Final_Model'],\n                           'Training_Score':train,'Testing_Score':test,'R2_Score':r2,'MAE':mae,'MAPE':mape,'MSE':mse})","32c2eb8f":"ALL_SCORES","bea272d7":"### Spliting the data into train(70) and test(30)","c8613f9f":"### STATISTICAL ANALYSIS","e6c6b92e":"The monthly premium auto has multiple peaks,so to remove those peaks we can apply any of the power transformation (SQUARE \/ CUBE) but as we can see that after the square transformation the data is getting heavily skewed, so we stick with the actual distribution again.","d3d026c3":"MEANS ARE NOT SAME FOR EDUCATION\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Education' which means 'Education' feature can be a significant for predicting 'Customer Lifetime Value'","09b00afd":"- As we can see that there are outliers in the total claim amount and also in monthly premium auto , usually we remove the outliers for a better model.\n- since our dataset is related to insurance and banking industry, we must be accept the outliers,as they can be our potential customers.\n- And there are no outliers in the income.\n- Conclusion: No outlier treatment required.","ceced6a5":"ALL STATES HAVE SAME MEAN VALUE FOR CLV\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable which means 'State' feature is not significant for predicting 'Customer Lifetime Value'","beb567e2":"#### TRANSFORMATION OF THE NUMERICAL VARIABLES","89ead69c":"H0: Error terms are homoscedastic\n\nHa: Error terms are not homoscedastic\n\np-value < 0.05 reject null hypothesis, error terms are not homoscedastic","1641b7dd":"### Gradient Boosting","21fa8812":"### AdaBoost","bdec9c79":"MEANS ARE NOT SAME FOR Employment Status\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Employment Status' which means 'Employment Status' feature can be a significant for predicting 'Customer Lifetime Value'","12298f5d":"### Extra Tree Regressor","4f27f5ed":"We can see that the average customer lifetime value is same for both male and female.","a1c10361":"CLV of all the 'States' follow a normal distribution. Hence, we can perform ANOVA test.","80a285df":"After Looking at the base model and the p-value of the feature's, we know that the Hypothesis for the feature's is\n\nH0: Feature is not significant\nHa: Feature is significant\nBut we just cant conclude the significance of the feature's just by base model and also without using any of the feature engineering technique's we have at our disposal. So we will first try to do the statistical test's of the feature for the feature selection, we can also use the forward selection and backward elimination , we will use the Variance inflation factor","ccfcf798":"CLV of all the categories of 'Location Code' follow a normal distribution. Hence, we can perform ANOVA test.","41da91af":"### Extra Tree","5c39f855":"### Mean Of Residuals","0a581ecc":"#### Gender vs Customer Lifetime Value","70b80620":"The average customer lifetime value of the customer who stay in different location code is the same so while creating the model we can drop this.","320751b6":"#### Vehicle Size vs Customer Lifetime Value","fff9b0c2":"### Final Model(Boosting+Bagging)","7e86c023":"#### Backward","ee9c47d0":"#### Policy vs Customer Lifetime Value","20c6d394":"MEANS ARE SAME FOR LOCATION CODE\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Location Code' which means 'Location Code' feature is not significant for predicting 'Customer Lifetime Value'","373f6fa0":"### ENSEMBLE METHODS","a0e33322":"CLV of all the categories of 'Location Code' follow a normal distribution. Hence, we can perform ANOVA test.","e9497f8f":"### Feature Selection-Forward, Backward\n#### Forward","74e95336":"### Bagging","6f28200a":"Of all the models we decided to choose gradient boosting as the next model, and furthur tweak the hyper parameter's of the model and also put this boosting model into bagging regressor and check for the model accuracy.","c5d74297":"So we can clearly see that the features removed didn't contribute to tell us the differing variance in the data, so it was a good decision to remove those features","6442ddab":"### Furthur Modelling:\n\n#### So we did the EDA and also the Statistical Analysis, so now we can just dis regard the features which we are not significant  for our model.","0d134cdf":"MEANS ARE NOT SAME Marital Status\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for at least on Group of 'Marital Status' which means 'Marital Status' feature can be significant for predicting 'Customer Lifetime Value'\n","edc3bf83":"So, multicollinearity exists.","bcd9bcbc":"Again for the total claim amount after applying the transformation's the data is getting skewed, and hence we stick to the actual distibution of the data.\n\nConclusion: No matter what power transformation we are applying to the numerical variables, it is still not getting normally distributed, and moreover the data is getting skewed, so rather we will just stick with the actual distribution ","06da5a9c":"### Random Forest","801cff50":"#### Education vs Customer Lifetime Value","7575edb4":"#### Renew Offer Type vs Customer Lifetime Value","082e507e":"Ho: Autocorrelation is absent\n\nHa: Autocorrelation is present\n\nThe P-value is >0.05 ,we fail to reject the null hypothesis, autocorrelation is absent.","b031f096":"### IMPORTING LIBRARIES","2f546aaf":"We will consider the r2_Score and the Mean absolute percentage error as the metrics we are going to use to measure the model.","cd18c171":"CLV of all the 'Response' follow a normal distribution. Hence, we can perform ANOVA test or test of mean for independent categories.","f431e9b2":"CLV of all the categories of 'Gender' follow a normal distribution. Hence, we can perform ANOVA test or test of mean for independent features.","f1c33d09":"### Finding the best sample by random state for each model.","0bf6758e":"#### Customer Response to marketing calls vs Customer Lifetime Value","781ea28c":"Secondly, when we convert the numerical features to categorical, our normal practice is label encoding for ordinal data and one hot for nominal data, but we can also use one hot encoding for ordinal data if there isn't any curse of dimensionality, so we will convert the categorical to numerical with one-hot encoding \/ dummification.","27905b30":"Number of open complaints also show kind of similar trend, where people who have complaints 2 or lesser have a similar pattern but where as >3 do not show any pattern we will have to do statistical test to understand if this feature is really significant or not","8528ba73":"MEANS ARE SAME FOR GENDER\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Gender' which means 'Gender' feature is not significant for predicting 'Customer Lifetime Value'","269fc498":"### Homoscedasticity_test","85594d4e":"Though the features, months since policy inception, months since last claim, number of open complaints and number of policies are all numerical, but they are discrete numbers and we will consider them as categorical features while preparing the model.\n\nFirstly, according to our EDA, we saw that the number of policies >= 3 have similar trend so we will group all of them as 3","4ffabdb8":"#### Vehicle Class vs Customer Lifetime Value","83c4f7fc":"MEANS ARE NOT SAME FOR COVERAGE\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Coverage' which means 'Coverage' feature can be a significant for predicting 'Customer Lifetime Value'","fe766274":"### No Autocorrelation","ce9f00b7":"Customer Lifetime Value is different for different types of coverage.","62bbdf06":"#### State vs Customer Lifetime Value","707ad67f":"### NULL VALUE'S CHECK","a6131066":"### Test of normality of residuals","f4ad3bbd":"#### Sales Channel vs Customer Lifetime Value","001625d3":"### MEASURES OF CENTRAL TENDENCY","518e859a":"We can see the best sample for each model, and we can also see which model is a good fit and which model is overfitting\/underfitting model.\n\n- AdaBoost is good model, but we will have to check for the metrics\n- Gradient Boosting is the best model comparitively, again we will check for the metrics\n- DecisionTree and the Extra Tree regressor models are not working better for this data set ,as the model is overfitting\n- Bagging and random forest regressor models are again overfitting model.","0ce0d2ff":"### NEXT MODEL","caa0c25c":"Suprisingly Both the forward and backward selection gave us the same features to select for our model, so we will be sticking to the same features.","e53b2e0a":"The average customer lifetime value of the customer who stay in different state is same and we can drop this also","bb7dc7a6":"### NO MULTI COLLINEARITY","db83f01a":"We can see a pattern here, customers who have taken only 1 policy have lower customer lifetime value, and customers who have taken 3 or greater show a similar trend, so we can combine all of them into one bin, and we can also see that the customers who have taken 2 policies have very high customer lifetime value comparitively.","11360431":"### SHAPE OFTHE DATA SET","d40b3db3":"### Using KFold Validation","31aa579f":"So after removing the unnessary feature's our model is giving is an accuracy of about 60%, we would like to take it 70% in the furthur models.","77d27e6d":"#### Location Code vs Customer Lifetime Value","dc6601e1":"We can also see that education is not a significant feature for assessing the lifetime value of the customer.","1542b76b":"RESPONE HAVE SAME MEAN VALUE\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable which means 'Response' feature is not significant for predicting 'Customer Lifetime Value'","bab8f9e5":"### Decision Tree Regressor","57307154":"Considering CLV (Customer Lifetime Value) as the target variable, we shall try to understand how each of the independent variables are contributing towards the target variable.\n\nSince our target variable is a continuous variable, we will have to perform ANOVA to understand how significant are the independent variables towards target variable.\n\nFor ANOVA,\n\nNull hypothesis is that there is no significant difference among the groups\nAlternative hypothesis is that there is at least one significant difference among the groups","7331a6c8":"CLV of all the 'Coverage' follow a normal distribution. Hence, we can perform ANOVA test.","b7ed8513":"These are the default parameters of the model so no hyper parameter tuning for this model is required, let us try and put this model into the bagging regressor.","00dfe9af":"There isn't much difference in the customer lifetime value w.r.t what policy type he has taken, all we need is how much revenue a customer can bring to the company, so it doesnt matter what type of policy he\/she has chosen.","5a4c2229":"### ASSUMPTIONS OF LINEAR REGRESSION.\n### Linearity","ebda2d62":"CLV of all the categories of 'Education' follow a normal distribution. Hence, we can perform ANOVA test.","8d69d833":"### DATA VISUALIZATION AND INFERENCES\n\n#### UNIVARIATE ANALYSIS","b466ece3":"#### Employment Status vs Customer Lifetime Value ","06215b67":"The average customer lifetime value for both of them is same.","48a20d85":"\nAnd we can clearly see in the correlation map, that customer lifetime value has a better correlation with monthly premium auto and acceptable co relation with total claim amount, but it show's no relationship with income, so again with all the visualization's we can come to the conclusion that we can dis regard the INCOME feature.","73a1bcb9":"As we can see that none of the continuous variables are normally distributed.\nSo in our case , we want to make the distributions normal, we can apply some transformations to the data and see if we can achieve a normally distributed variable.","f92027b7":"### READING THE DATA SET","171595cb":"We can see that after using KFold validation we got our average training accuracy to be 86.45 and average testing accuracy to be 86.30, OUR final model is bagging regressor with base estimator gradient boosting regressor.","b8343d65":"#### Marital Status vs Customer Lifetime Value","5911491a":"The 2nd assumption is that the mean of the residuals must be close to zero, which again fails.","6b2b5ce7":"We can clearly see that the Linear Regression model is having the best r2_Score, and decision tree and extra tree regressor have no better accuracy that so we will have to build our model using ensemlle technique's, boosting and bagging.","49344d76":"We don't see any linear relationship between the variables and the Y varible , which fails the first assumption of linear regression.","159885c3":"As we can see that while we are trying to transform the data to make it normal,rather the distribution is getting skewed, or is having multiple peaks which again is a problem to our model, hence we just stick with the same distribution of the variable.","6caf8361":"#### Coverage Type vs Customer Lifetime Value","d15d4597":"CLV of all the categories of 'Employment Status' follow a normal distribution. Hence, we can perform ANOVA test.","1a322d3f":"#### Finding the best sample","e99ad74b":"### BASE MODEL USING OLS\n\n##### Using label encoding just for the purpose of looking at the base model, encoding technique's may change furthur(one-hot encoding is used)","4adbaee1":"The distribution clearly show's that the residuals are not normally distributed, and the third assumption also fails.","75a36cfb":"\nWe can clearly see that there is a linear relationship between Customer lifetime value and monthly premium auto, but we do not see any relationship between income and the total claim amount."}}