{"cell_type":{"7c85c7ba":"code","1a78b2eb":"code","261c5a20":"code","630a2ce7":"code","f1885a89":"code","791dae6b":"code","8520b058":"code","f2f84e10":"code","db0c8457":"code","cff78ce4":"code","0c65b896":"code","32e8126b":"code","aba8bcec":"code","0c3e4f76":"code","defc65ae":"code","3f366f7c":"code","7775d585":"code","cba61611":"code","2825f2fb":"code","cf27a9bd":"code","6f113b25":"code","249e15e3":"markdown","e000efa8":"markdown","38ffadd8":"markdown","4c126505":"markdown","1c135e24":"markdown","41aa28ef":"markdown","bf5e9d1f":"markdown","fb3488a2":"markdown","f06840a4":"markdown","2d43fdfd":"markdown","af11fc6f":"markdown","3f90431a":"markdown","6c734c72":"markdown","b30027e4":"markdown","255e0c2a":"markdown","f5ee108d":"markdown","574e3c01":"markdown","38aab512":"markdown","5ae32460":"markdown","46ef16fe":"markdown","adafda46":"markdown","3a410ad8":"markdown","fb5e72e7":"markdown"},"source":{"7c85c7ba":"#import necessary packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(tf.__version__)\nfrom keras.regularizers import l2\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n","1a78b2eb":"# Input data files are available in the \"..\/input\/\" directory.","261c5a20":"\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","630a2ce7":"file_path =\"..\/input\/heart.csv\"\ndata = pd.read_csv(file_path) \nprint(data.shape)\nd=data.transpose()\nprint(data.describe())\n\nprint(data.head())\nprint(data.columns)","f1885a89":"plotdata=data[['age','chol','thalach','trestbps','target']]\n\nplotdata['target'][plotdata['target']==0]='no heart disease present (0)'\nplotdata['target'][plotdata['target']==1]='heart disease present (1)'\n\nsns.pairplot(plotdata, hue='target')","791dae6b":"sns.relplot(x='trestbps', y=\"thalach\", size=\"chol\", sizes=(15, 200),data=data)","8520b058":"sns.factorplot('cp','age',data=data,\n                   hue='target',\n                   size=6,\n                   aspect=0.8,\n                   palette='magma',\n                   join=False,\n              )","f2f84e10":"\nsns.barplot(x=data.age.value_counts()[:15].index,y=data.age.value_counts()[:15].values)\nplt.xlabel('Age')\nplt.ylabel('Number of people')\nplt.show()","db0c8457":"plt.figure(figsize=(9,9))\nsns.heatmap(data.corr(),annot=True,fmt='.1f',cmap=\"YlGnBu\",annot_kws = {'size':10})\nplt.show()","cff78ce4":"#separate Data versus target\ny = data.target\ny = y.astype('float64') \n#We drop the sex as it is a binary categorical feature that interferes with the training of the model\nX=data.drop(['target','sex'], axis=1)\n# 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n#        'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n#Are there missing values?\nprint('Is there any missing data: %s' %data.isnull().values.any())\n\n\n#Separate training data from test data\nX_train, X_test, y_train, y_test =train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle=True,\n                                                      random_state=0)\nX_stats=X_train.describe()\nX_stats=X_stats.transpose()\n\n\n\n#I would like to start with a simple neural network, so lets renormalize the data we will feed to it. \ndef norm(x):\n    return ((x-X_stats['mean'])\/X_stats['std'])\nn_X_train=norm(X_train)\nn_X_test=norm(X_test)\n","0c65b896":"#We use l2 regularization to prevent overfitting\ndef Build_Model():\n    model=keras.Sequential([\n        layers.Dense(64,activation=tf.nn.relu,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), input_shape=[len(n_X_train.keys())]),\n        layers.Dense(64,activation=tf.nn.relu,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n        layers.Dense(1,activation=tf.nn.sigmoid)\n    ])\n\n\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n    return model\n","32e8126b":"model=Build_Model()\nmodel.summary()","aba8bcec":"EPOCHS=500\nhistory=model.fit(n_X_train,y_train,epochs=EPOCHS,validation_split=0.2,verbose=0)","0c3e4f76":"\ndef plot_history(history):\n    hist=pd.DataFrame(history.history)\n    hist['epoch']=history.epoch\n    \n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('cross_entropy')\n    plt.plot(hist['epoch'],hist['loss'],label='Train Error')\n    plt.plot(hist['epoch'],hist['val_loss'], label='Val Error')\n    plt.legend()\n    \n    plt.figure()\n    \nplot_history(history)\n    ","defc65ae":"model=Build_Model()\nearly_stop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\nhistory=model.fit(n_X_train,y_train,epochs=EPOCHS,validation_split=0.2,verbose=0,callbacks=[early_stop])\nplot_history(history)","3f366f7c":"loss, accu=model.evaluate(n_X_test,y_test,verbose=1)\nprint(accu)\nprint('Testing accuracy:%0.2f'%(accu))\n","7775d585":"test_predictions=model.predict_classes(n_X_test)\ntest_predictions_probability=model.predict(n_X_test)\n","cba61611":"\n\nprint('Confusion matrix:')\nprint(metrics.confusion_matrix(y_test, test_predictions))\nconfusion=metrics.confusion_matrix(y_test, test_predictions)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\n#accuracy\nprint('Accuracy:')\nprint((TP + TN) \/ float(TP + TN + FP + FN))\n#print(metrics.accuracy_score(y_test, test_predictions))","2825f2fb":"fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predictions_probability)\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for heart disease')\nplt.xlabel('False Positive Rate ')\nplt.ylabel('True Positive Rate ')\nplt.grid(True)\n","cf27a9bd":"print(metrics.roc_auc_score(y_test, test_predictions_probability))","6f113b25":"\n\ndef Build_Model_and_ROC(model,name,  X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_test)\n    test_predictions_probability=model.predict_proba(X_test)[:,1]\n    \n    #Get the confusion matrix of the classifier\n    confusion=metrics.confusion_matrix(y_test, y_pred)\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    print('Confusion Matrix for '+ name +':')\n    print(confusion)\n   \n    #ROC curve\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predictions_probability)\n\n    #Plotting ROC curve\n  \n    plt.plot(fpr, tpr, label=name)\n    \n\n    print(name+\" Number of mislabeled points out of a total %d points  : %d\" %(X_test.shape[0],(y_test != y_pred).sum()))\n    print( 'Accuracy of '+ name+ ' : %f'%(metrics.accuracy_score(y_test, y_pred)))\n    print('auc score of '+ name+ ' : %f'%( metrics.roc_auc_score(y_test, test_predictions_probability)))\n    print('\\n')\n  \n   \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for heart disease')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)\n    plt.legend(loc=4) \n\nmodels={'XG Boost': XGBClassifier(), 'Logistic Regression': LogisticRegression(), 'Naive Bayes': GaussianNB(),\n        'Random Forest':RandomForestClassifier(max_depth=5), 'Decision Tree Bagging' : BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100 ,\n                         bootstrap=True, n_jobs=-1,oob_score=True)} \nplt.figure(figsize=(20,10))\nfor name, model in models.items():\n    Build_Model_and_ROC( model, name,  X_train, X_test, y_train, y_test)    \n\n    ","249e15e3":" Lets do a factor plot between age and chest pain type.\n","e000efa8":"All in all these models give similar results  to those of a simple neural network.\n\nThank you for taking the time of reading my Kernel, any comments are very welcome !","38ffadd8":"We continue with a relative plot of resting blood pressure (Trestbps), maximum heart rate achieved (Thalach), and serum cholestoral (Chol)","4c126505":"We train the model","1c135e24":"It seems like chest pain of types 1 and 2 are the best at differentiating the target with respect to age.","41aa28ef":"The columns represent the following information.\n\n**age**: age in years \n\n**sex**: (1 = male; 0 = female) \n\n**cp**: chest pain type (0-4)\n\n**trestbps**: resting blood pressure (in mm Hg on admission to the hospital) \n\n**chol**: serum cholestoral in mg\/dl \n\n**fbs**: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n\n**restecg**: resting electrocardiographic results \n\n**thalach**: maximum heart rate achieved \n\n**exang**: exercise induced angina (1 = yes; 0 = no) \n\n**oldpeak**: ST depression induced by exercise relative to rest \n\n**slope**: Slope of the peak exercise ST segment \n\n**ca**: number of major vessels (0-3) colored by flourosopy \n\n**thal3** = normal; 6 = fixed defect; 7 = reversable defect \n\n**target**: 1 or 0 (presence or absence of heart disease)","bf5e9d1f":"We find the area under the curve for this model","fb3488a2":"Neural network  model","f06840a4":"Finally let's do a correlation heat map","2d43fdfd":"Lets use Seaborn's pair plot to get an idea of how certain columns are correlated and distributed against each other","af11fc6f":"Recall that the confusion matrix is given by \n\\begin{pmatrix} \n\\textbf{True Positives} & \\textbf{False Positives} \\\\\n\\textbf{False Negatives} & \\textbf{True Negatives}\n\\end{pmatrix} ","3f90431a":"Pretty Good! Now lets make some predictions and get a confusion matrix.","6c734c72":"It is very close to 1 which is a good indicative.","b30027e4":"**Plotting some aspects of data and comparing classification models on UCI Heart Disease Dataset**","255e0c2a":"We import the data and explore it by making some graphics using seaborn","f5ee108d":"Data preprocessing","574e3c01":"Finally we compare the following models with our neural network: XG Boost, Logistic Regression, Naive Bayes,\nRandom Forest and Decision Tree Bagging.","38aab512":"Let's check how the age is distributed in the data. we will only consider the 15 highest ranking positions.","5ae32460":"It appears that no two different features are highly correlated, so lets just proceed with building the models.","46ef16fe":"We plot an ROC curve to evaluate the model. Recall that $$\\textbf{False positive rate}=\\frac{FP}{FP+TN}$$ and $$\\textbf{True positive rate}=\\frac{TP}{TP+FN}$$","adafda46":"Plotting the history of training","3a410ad8":"Reduce Overfitting with early stopping","fb5e72e7":"Model Evaluation"}}