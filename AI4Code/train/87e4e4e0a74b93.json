{"cell_type":{"2a710882":"code","7257d04f":"code","88157eb3":"code","07d890ef":"code","0d72bcf2":"code","bddcf774":"code","303840d1":"code","82594021":"code","146828a4":"code","2dc5304d":"code","384b327d":"code","de8c40bb":"code","5fc03f88":"code","97c8c45c":"code","571a35f2":"code","95fede9d":"code","5a69a5d3":"code","6dcb022e":"code","33e5811d":"code","582c8b35":"code","395e7bfd":"code","7e932d15":"code","64d9c824":"code","ff205992":"code","9bfdb043":"code","3317edc1":"code","640b8cad":"code","c2de69f8":"code","48a51e2c":"code","f021d0af":"code","c5f9463e":"code","35a5736c":"code","c9c21250":"code","31ecae09":"code","31361369":"code","195af7cb":"code","4baae364":"code","722f4d61":"markdown","9ef99022":"markdown","2ef07c63":"markdown","4be8be85":"markdown","1c39380a":"markdown","9cebbb2d":"markdown","c03fa812":"markdown","72b9d1ba":"markdown","dbf3349b":"markdown","580413a3":"markdown","d5113710":"markdown","479af09c":"markdown","36c3bf95":"markdown","317a7ee5":"markdown","52537145":"markdown","4cdcb64d":"markdown","a5317afa":"markdown","c6ed58e9":"markdown","da64f1b8":"markdown","ef1c7c84":"markdown","03190e04":"markdown","79d442c0":"markdown","2f1b8829":"markdown","89799f2f":"markdown","c000ff58":"markdown","575397cb":"markdown","31c741be":"markdown","ece6967d":"markdown","e95dc8d0":"markdown","5c07ed2a":"markdown","3325fdd5":"markdown","2b792889":"markdown","25089bcd":"markdown"},"source":{"2a710882":"# https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n# Helpful Resource\n# https:\/\/machinelearningmastery.com\/overfitting-machine-learning-models\/","7257d04f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport shap\nfrom sklearn import tree\nfrom sklearn.metrics import precision_recall_curve , plot_precision_recall_curve , accuracy_score , precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz\n\n\n# age\n# sex\n# chest pain type (4 values)\n# resting blood pressure\n# serum cholestoral in mg\/dl\n# fasting blood sugar > 120 mg\/dl\n# resting electrocardiographic results (values 0,1,2)\n# thalach = maximum heart rate achieved\n# exercise induced angina\n# oldpeak = ST depression induced by exercise relative to rest\n# the slope of the peak exercise ST segment\n# ca = number of major vessels (0-3) colored by flourosopy (i.e major vessels that can be seen)\n# thal: (thalassemia) 3 = normal; 6 = fixed defect; 7 = reversable defect\n","88157eb3":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata.head(20)","07d890ef":"corr = data.drop(\"target\" , 1).corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n_=sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","0d72bcf2":"X_train, X_test, y_train, y_test = train_test_split(data.drop(\"target\" , 1) , data.target, test_size=.2)","bddcf774":"plt.figure(figsize=(50,50))\nmodel = DecisionTreeClassifier(criterion='entropy')\n_ = model.fit(X_train, y_train)\n_ = tree.plot_tree(model , feature_names=[x for x in data.columns])","303840d1":"print('10 fold')\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"accuracy\") # cv=10\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nprint('\\n')\n\nprint('5 fold')\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"accuracy\")# cv=5\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))","82594021":"y_pred_single_decision_tree=model.predict(X_test)\n# cm = confusion_matrix(y_train , y_pred_single_decision_tree)\nprint(accuracy_score(y_test , y_pred_single_decision_tree))\nprint(precision_score(y_test , y_pred_single_decision_tree))\n_ = plot_confusion_matrix(model, X_test , y_test)","146828a4":"# Precision = TruePositives \/ (TruePositives + FalsePositives)\n# Recall = TruePositives \/ (TruePositives + FalseNegatives)\n_ = plot_precision_recall_curve(model, X_test, y_test)\nprint(classification_report(y_test, y_pred_single_decision_tree))","2dc5304d":"train_scores, test_scores = list(), list()\nvalues = [i for i in range(1, 31)]\nfor i in values:\n  model = DecisionTreeClassifier(criterion='entropy',max_depth=i)\n  model.fit(X_train, y_train)\n  train_yhat = model.predict(X_train)\n  train_acc = accuracy_score(y_train , train_yhat)\n  train_scores.append(train_acc)\n  test_yhat = model.predict(X_test)\n  test_acc = accuracy_score(y_test, test_yhat)\n  test_scores.append(test_acc)\n  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\nplt.plot(values, train_scores, '-o', label='Train')\nplt.plot(values, test_scores, '-o', label='Test')\nplt.legend()\nplt.show()","384b327d":"shap.initjs()\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test , y_test)\nshap.summary_plot(shap_values, X_test)\nshap.summary_plot(shap_values[0], X_test)\nshap.summary_plot(shap_values[1], X_test)","de8c40bb":"shap_values_ind = explainer.shap_values(X_test)","5fc03f88":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[1] , X_test)","97c8c45c":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[0] , X_test)","571a35f2":"model = RandomForestClassifier(criterion='entropy' , n_estimators=500)\n_ = model.fit(X_train, y_train)","95fede9d":"print('10 fold')\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"accuracy\") # cv=10\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nprint('\\n')\n\nprint('5 fold')\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"accuracy\")# cv=5\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))","5a69a5d3":"y_pred_random_forest_tree=model.predict(X_test)\ncm = confusion_matrix(y_test , y_pred_random_forest_tree)\nprint(accuracy_score(y_test , y_pred_random_forest_tree))\nprint(precision_score(y_test , y_pred_random_forest_tree))\n_ = plot_confusion_matrix(model, X_test , y_test)","6dcb022e":"# Precision = TruePositives \/ (TruePositives + FalsePositives)\n# Recall = TruePositives \/ (TruePositives + FalseNegatives)\n_ = plot_precision_recall_curve(model, X_test, y_test)\nprint(classification_report(y_test, y_pred_random_forest_tree))","33e5811d":"shap.initjs()\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test , y_test)\nshap.summary_plot(shap_values, X_test)\nshap.summary_plot(shap_values[0], X_test)\nshap.summary_plot(shap_values[1], X_test)","582c8b35":"shap_values_ind = explainer.shap_values(X_test)","395e7bfd":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[1] , X_test)","7e932d15":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[0] , X_test)","64d9c824":"for col in data.columns.drop(['target' , 'sex'] , 1):\n  X = data[col].values.reshape(-1,1)\n  Y = data['target'].values.reshape(-1,1)\n  model = LogisticRegression(max_iter=1000).fit(X, np.ravel(Y.astype(float)))\n  x_pred_proba = model.predict_proba(X)\n  plt.xlabel(col)\n  plt.ylabel('target')\n  plt.scatter(X , Y)\n  plt.scatter(X , x_pred_proba[:,0]) # plot each col probability of them NOT having heart disease \n  plt.show()","ff205992":"model = LogisticRegression(max_iter=1000).fit(X_train, y_train)","9bfdb043":"print('10 fold')\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"accuracy\") # cv=10\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=10 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nprint('\\n')\n\nprint('5 fold')\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"accuracy\")# cv=5\nprint(\"%0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\nscores = cross_val_score(model , X_test,y_test , cv=5 , scoring=\"recall\")\nprint(\"%0.3f recall with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))","3317edc1":"y_pred_log_regression=model.predict(X_test)\ncm = confusion_matrix(y_test , y_pred_log_regression)\nprint(accuracy_score(y_test , y_pred_log_regression))\nprint(precision_score(y_test , y_pred_log_regression))\n_ = plot_confusion_matrix(model , X_test , y_test)","640b8cad":"_ = plot_precision_recall_curve(model, X_test, y_test)\nprint(classification_report(y_test, y_pred_log_regression))","c2de69f8":"shap.initjs()\nmodel.fit(X_train , y_train)\nexplainer = shap.KernelExplainer(model.predict , X_test)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","48a51e2c":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[1] , X_test)","f021d0af":"for col in X_train.columns:\n  shap.dependence_plot(col , shap_values_ind[1] , X_test)","c5f9463e":"X_train, X_test, y_train, y_test = train_test_split(data.drop(\"target\" , 1) , data.target, test_size=.2)\npd.set_option(\"max_rows\", None)","35a5736c":"decision_tree_model = DecisionTreeClassifier(criterion='entropy')\n_=decision_tree_model.fit(X_train , y_train)\ndecision_tree_y_predictions = decision_tree_model.predict(X_test)\ndecision_tree_y_predictions = pd.DataFrame(decision_tree_y_predictions)\ndecision_tree_y_predictions = decision_tree_y_predictions.values","c9c21250":"df = pd.DataFrame(X_test , copy=True)\ndf['actual'] = y_test\ndf['predicted'] = decision_tree_y_predictions.astype(int)\ndf","31ecae09":"random_forest_model = RandomForestClassifier(criterion='entropy' , n_estimators=500)\n_ = random_forest_model.fit(X_train, y_train)\nrandom_forest_model = random_forest_model.predict(X_test)\nrandom_forest_model = pd.DataFrame(random_forest_model)\nrandom_forest_model = random_forest_model.values","31361369":"df = pd.DataFrame(X_test , copy=True)\ndf['actual'] = y_test\ndf['predicted'] = random_forest_model.astype(int)\ndf","195af7cb":"logistic_regression_model = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n_ = logistic_regression_model.fit(X_train, y_train)\nlogistic_regression_model = logistic_regression_model.predict(X_test)\nlogistic_regression_model = pd.DataFrame(logistic_regression_model)\nlogistic_regression_model = logistic_regression_model.values","4baae364":"df = pd.DataFrame(X_test , copy=True)\ndf['actual'] = y_test\ndf['predicted'] = logistic_regression_model.astype(int)\ndf","722f4d61":"#### Dependence Plot for predicted No Heart Disease","9ef99022":"#### *Conclusion* \n\n---\n\nThe most impactful features using a Random Forest Model are (like a Decision Tree Model), thal, ca, and cp. However, the recall, accuracy, and cross validation scores of a Random Forest Model tend to be moderately better than a Decision Tree Model.\n","2ef07c63":"### ***Using a Decision Tree Model***","4be8be85":"### Logistic Regression","1c39380a":"#### Dependence Plot for Predicted Heart Disease","9cebbb2d":"### A look at the data","c03fa812":"#### Dependence Plot for Predicted No Heart Disease","72b9d1ba":"#### *Conclusion*\n\n---\n\n\nFor a Decision Tree Model, the number of major vessels colored by flourosopy (i.e 'ca' feature), chest pain level (i.e, 'cp' feature) and thalassemia (i.e 'thal' feature) tend to cause the greatest impact to predicting whether a patient is diagnosed with heart disease or not. Accuracy for a Decision Tree is not the best on either number of folds (5 or 10).","dbf3349b":"#### Explanation","580413a3":"#### Explanation","d5113710":"#### Confusion Matrix","479af09c":"#### Explanation","36c3bf95":"#### Cross Validation","317a7ee5":"#### *Conclusion* \n\n---\n\n","52537145":"#### Confusion Matrix Reports","4cdcb64d":"#### Will it overfit?","a5317afa":"### ***Using a Logistic Regression Model***","c6ed58e9":"### Random Forest","da64f1b8":"#### Dependence Plot for Predicted Heart Disease","ef1c7c84":"### Conclusion\n\n---\n\nRandom forests and logistic regression consistently outperform Decision Trees in terms of correct predicitions. Given the dataset is so small, the Decision tree model did not tend to overfit for this dataset (due to the lack of noise), but analysis was done anyway to showcase the plots and values of training and testing. \n\nAcross all three algorithms, the most impactful features tend to be the same, with 'ca'(i.e fluoroscopy), 'thal' (i.e thalassemia) and 'cp' (i.e chest pain), consistently ranking among the top 3 most impactful features.\n\nIn a realistic setting, a Random Forest seems most appropriate due to the slighlty better and consistent precision, recall and accuracy values, as well as substantial faster explanation runtimes. \n\nThe purpose of the notebook is just to compare multiple algorithms in terms of performance and predictions, in a production setting I would have skipped the single Decision Tree model entirely and opted for a Random Forest (a random forest is really just multiple decision trees with their average taken).\n\nIn any case, I (wrongfully) thought cholesterol and age would be the two most signficant factors in properly classifying heart disease. However, given the other features I was clearly wrong in my original hypothesis, so I did learn something. ","03190e04":"## Comparisons between different models","79d442c0":"#### Dependence Plot for predicted No Heart Disease","2f1b8829":"#### Confusion Matrix Reports","89799f2f":"#### Confusion Matrix","c000ff58":"### Correlation Matrix","575397cb":"#### Confusion Matrix","31c741be":"#### Confusion Matrix Reports","ece6967d":"#### Cross Validation","e95dc8d0":"### ***Using a Random Forest Model***","5c07ed2a":"## Final predictions and conclusions across all models","3325fdd5":"### Decision Tree","2b792889":"#### Cross Validation","25089bcd":"#### Dependence Plot for Predicted Heart Disease"}}