{"cell_type":{"31c7af4e":"code","130ffc67":"code","712f6b2c":"code","6574b4db":"code","0129fc30":"code","870a030e":"code","17d5eb92":"code","74ffc753":"code","3fecb53f":"code","bac9f57b":"code","af8a8565":"code","3565b0af":"code","82635ec0":"code","4114b5a5":"code","996b6f1a":"code","a634b250":"code","64affc6d":"code","83fe15ef":"code","783b12ba":"code","d084606f":"code","63d5e980":"code","6ac9b722":"code","f07800eb":"code","6c0093c4":"code","fa41524d":"code","f5cf35cb":"code","ae01f913":"code","319a259f":"code","2eae8e7f":"code","42cb47ef":"code","5436c072":"code","65fffe82":"code","be3c8240":"code","429f7abe":"code","09663e50":"code","56136f64":"code","a4ec7cda":"code","03288574":"code","64f41dee":"code","6569aa88":"code","0157fb25":"code","491105c5":"code","b50e70f1":"code","406c0554":"code","e7f94286":"code","c76bbd0f":"markdown","defc3dbe":"markdown","17afc3d7":"markdown","22a4b3b3":"markdown","b7cc4a76":"markdown","37571d68":"markdown","7ba03733":"markdown","d72a68ba":"markdown","3f1750b2":"markdown","6b3e3859":"markdown","9a51dee3":"markdown","94d1f7ab":"markdown","06bc668c":"markdown","f82731c2":"markdown","928a4639":"markdown","c23e6e7f":"markdown","00b31446":"markdown","dc3f0275":"markdown","ae34c944":"markdown","678ead6a":"markdown","b35c798b":"markdown","2217ec8b":"markdown","b7596d75":"markdown","d412d772":"markdown","61c9c225":"markdown","14d26a63":"markdown","37e4050f":"markdown","bb816da8":"markdown","67aa646a":"markdown","c9b7799a":"markdown","75480b5c":"markdown","d426f5d0":"markdown","19ded992":"markdown","93ccc8e9":"markdown"},"source":{"31c7af4e":"# EDA using Pandas Profiling\n!pip install pandas_profiling","130ffc67":"# EDA using Sweetviz\n# !pip install sweetviz","712f6b2c":"# Load libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\n# import sweetviz as sv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import classification_report, balanced_accuracy_score\n\n\n\n# Color Palette\n\ncustom_colors = [\"#85CEDA\",\"#D2A7D8\", \"#A67BC5\", \"#BB1C8B\", \"#05A4C0\"]\ncustomPalette = sns.set_palette(sns.color_palette(custom_colors))\n\n# Set size\n\nsns.palplot(sns.color_palette(custom_colors),size=1)\nplt.tick_params(axis='both', labelsize=0, length = 0)\n","6574b4db":"df = pd.read_csv('..\/input\/connectionist-bench-sonar-mines-vs-rocks-uci\/sonar.all-data-uci.csv')","0129fc30":"# shape\ndf.shape","870a030e":"# info\ndf.info()","17d5eb92":"# type\n# pd.set_option('display.max_row', 500)\ndf.dtypes","74ffc753":"# peek at data\n# pd.set_option('display.width', 100)\ndf.head(20)","3fecb53f":"# describe data\ndf.describe()","bac9f57b":"# class distribution\ndf.groupby(by='Label').size()","af8a8565":"pp.ProfileReport(df,title= 'Pandas Profile report of \"Sonar\" dataset', html= {'style':{'full_width': True}})","3565b0af":"# For show result uncomment below lines\n# report = sv.analyze(df)\n# report.show_html(\"Sonar_EDA_report.html\") # specify a name for the report\n# report","82635ec0":"# Histogram\ndf.hist(sharex= False, sharey= False, xlabelsize= 1, ylabelsize= 1, figsize=(12,12))\nplt.show()","4114b5a5":"# density\ndf.plot(kind='density', subplots=True, layout=(8,8), sharex=False, legend=False, fontsize=1, figsize=(12,12))\nplt.show()","996b6f1a":"# correlation matrix\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(df.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\nfig.set_size_inches(10,10)\nplt.show()","a634b250":"X = df.drop('Label', axis= 1)\ny = df['Label']","64affc6d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","83fe15ef":"# test options\nnum_folds = 10\nseed = 101\nscoring = 'accuracy'","783b12ba":"scaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)","d084606f":"knn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train, y_train)","63d5e980":"y_pred= knn_model.predict(scaled_X_test)","6ac9b722":"#A comparison between predicted Value vs Actual Values\n\npd.DataFrame({'Y_Test':y_test, 'Y_Pred': y_pred})","f07800eb":"accuracy_score(y_test, y_pred)","6c0093c4":"confusion_matrix(y_test, y_pred)","fa41524d":"print(classification_report(y_test, y_pred))","f5cf35cb":"test_error_rate= []\n\n\nfor k in range (1, 30):\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    knn_model.fit(scaled_X_train, y_train)\n    \n    y_pred_test = knn_model.predict(scaled_X_test)\n    \n    test_error=1- accuracy_score(y_test, y_pred_test)\n    test_error_rate.append(test_error)\n    \ntest_error_rate","ae01f913":"plt.figure(figsize=(6, 4), dpi = 200)\nplt.plot(range(1, 30), test_error_rate, label='Test Error')\nplt.legend()\nplt.ylabel('Error Rate')\nplt.xlabel('K Value')","319a259f":"scaler= StandardScaler()\nknn= KNeighborsClassifier()","2eae8e7f":"operations= [('scaler', scaler), ('knn', knn)]\npipe= Pipeline(operations)\nk_values= list(range(1, 20))\nparam_grid= {'knn__n_neighbors': k_values}\nfull_cv_classifier= GridSearchCV(pipe, param_grid, cv=5, scoring= scoring)\nfull_cv_classifier.fit(X_train, y_train)","42cb47ef":"full_cv_classifier.best_estimator_.get_params()","5436c072":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))","65fffe82":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","be3c8240":"# compare algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\nplt.show()","429f7abe":"# standardized the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))","09663e50":"results = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle= True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","56136f64":"# compare scaled algorithms\nfig = plt.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\nplt.show()","a4ec7cda":"# KNN algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nneighbors = [1,2,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle= True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)","03288574":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","64f41dee":"plt.figure(figsize=(6, 4), dpi = 200)\nplt.plot(neighbors, means, label='Test Error')\nplt.legend()\nplt.ylabel('Error Rate')\nplt.xlabel('K Value')","6569aa88":"# ensembles\nensembles = []\n# Boosting methods\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))","0157fb25":"results = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle= True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","491105c5":"# compare ensemble algorithms\nfig = plt.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\nplt.show()","b50e70f1":"scaler= StandardScaler()\nknn1= KNeighborsClassifier(n_neighbors= 1)\noperations= [('scaler', scaler), ('knn1', knn1)]","406c0554":"pipe= Pipeline(operations)\npipe.fit(X_train, y_train)","e7f94286":"# estimate accuracy on validation set\npredictions = pipe.predict(X_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","c76bbd0f":"__K nearset neighbors (KNN)__ assigns a label to new data according to the __distance between the old data and the new data.__","defc3dbe":"* #### __1st Method: Elbow Method__","17afc3d7":"## __5. Determining the Features and the Target Variable__","22a4b3b3":"#### __Dataset information (Pandas Profiling & Sweetviz Report)__","b7cc4a76":"__It should be mentioned that feature scaling is compulsory in the KNN algorithm.__","37571d68":"* #### __2nd Method: Grid Search Cross Validation ( Pipeline application)__","7ba03733":"## __10. Evaluating the Model__","d72a68ba":"__KNN's best of 84.9%. (But what about variance? KNN seemed to indicate a tighter variance during spot checking).__\n\nLet's try some ensemble methods. No standardization on data this time. Because apparantly all four ensembles we are using are based on decision trees and thus are less sensitive to data distributions. (Ok. Nice tip!)","3f1750b2":"# __Create a machine learning model capable of detecting Rock or Mine__\n\n## based on the response of the 60 separate sonar frequencies.\n","6b3e3859":"## __7. Feature Scaling__","9a51dee3":"* #### Sweetviz Report","94d1f7ab":"# **Part 2 - Building and Training the Classification model**","06bc668c":"* #### Pandas Profile","f82731c2":"## __12. Spot check some algorithms__","928a4639":"#### **Data Description**\n\n---\n\nThe file **\"sonar.mines\"** contains **111 patterns** obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file **\"sonar.rocks\"** contains **97 patterns** obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n\n**Each pattern** is a **set of 60 numbers** in the **range 0.0 to 1.0**. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n\nThe label associated with each record contains the letter **\"R\"** if the object is a **rock** and **\"M\"** if it is a **mine** (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n","c23e6e7f":"## __11. Selecting the best K__","00b31446":"## __14. Ensembles__","dc3f0275":"## **3. Exploration Data Analysis (EDA)**","ae34c944":"__The sonar dataset has 111 mines and 97 rocks.__ ","678ead6a":"#### **Abstract**\n\n|Info                      | Answer        |\n|--------------------------|---------------|\n|Data Set Characteristics: | Multivariate  | \n|Attribute Characteristics:| Real          |\n|Associated Tasks:         | Classification| \n|Number of Web Hits:       | 213249        |\n|Number of Instances:      | 208           |\n|Area:                     | Physical      |\n|Number of Attributes:     | 60            |\n|Date Donated              | N\/A           |\n|Missing Values?           | N\/A           |","b35c798b":"## __13. Algorith Tuning: KNN show as the most promising options__","2217ec8b":"## __8. Training the Model__","b7596d75":"## __15. Finilizing the model__","d412d772":"### __Data Source:__\n[https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)](http:\/\/https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))","61c9c225":"## **Table of content**\n\n* ### **Part 1 - Data Preprocessing** \n    1. Importing Libraries\n    2. Importing Datasets\n    3. Exploration Data Analysis (EDA)\n    4. Data visualization\n    5. Determining the Features and the Target Variable\n    6. Spliting the Data to Train & Test\n    7. Feature Scaling\n    \n    \n* ### **Part 2 - Building and Training the Classification model**\n \n    8. Training the model\n    9. Prediction\n    10. Evaluating the Model\n    11. Selecting the best K\n    12. Spot check some algorithms\n    13. Algorith Tuning: KNN show as the most promising options\n    14. Ensembles\n    15. Finilizing the model\n","14d26a63":"## __9. Prediction__","37e4050f":"## __4. Data Visualization\ud83d\udcca\ud83d\udcc8__","bb816da8":"# __Part 1 - Data Preprocessing__","67aa646a":"## **2. Import Datasets**","c9b7799a":"## __6. Spliting the Data to Train & Test__","75480b5c":"## __1. Importing Libraries__","d426f5d0":"__The dataset has 208 samples and 60 features + the target variable (Label).__ All features are float and target is object. ","19ded992":"Installation __*'pandas-profiling'*__ and __*'sweetviz'*__ for generates reports to help analyze EDA","93ccc8e9":"ET might be worthy of further study."}}