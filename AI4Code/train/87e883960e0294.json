{"cell_type":{"833b5a22":"code","1cb60326":"code","7f44d9b9":"code","15bc6fc6":"code","acfcaed7":"code","00478c27":"code","12809a54":"code","501dd09c":"code","c78e1ff7":"code","e71d4380":"code","4c704a0c":"code","d30924a9":"code","1ddeed46":"code","41a31b8f":"code","a3e38264":"code","950bdd11":"code","6a1bc38f":"code","514872bd":"code","49c607bf":"code","23451c17":"code","c7740ffd":"code","a1a56fd3":"code","4e83e629":"markdown","cae72eca":"markdown"},"source":{"833b5a22":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport pickle\nimport gc\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nimport torch.nn.functional as F\nimport os\nimport random\nimport time\nimport pickle\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder","1cb60326":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","7f44d9b9":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\ntrain_csv_path       = '..\/input\/google-quest-challenge\/train.csv'\ntest_csv_path        = '..\/input\/google-quest-challenge\/test.csv'\nseed                 = 0\nepochs               = 50\nseed_everything(seed)","15bc6fc6":"train = pd.read_csv(train_csv_path)\ntest  = pd.read_csv(test_csv_path)","acfcaed7":"train.columns","00478c27":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr","12809a54":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= len(word_index) + 1:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n                        \n    return embedding_matrix, unknown_words","501dd09c":"X_train_question = train['question_body']\nX_train_title    = train['question_title']\nX_train_answer   = train['answer']\n\nX_test_question  = test['question_body']\nX_test_title     = test['question_title']\nX_test_answer    = test['answer']","c78e1ff7":"tokenizer = text.Tokenizer()","e71d4380":"tokenizer.fit_on_texts(list(X_train_question) + \\\n                       list(X_train_answer) + \\\n                       list(X_train_title) + \\\n                       list(X_test_question) + \\\n                       list(X_test_answer) + \\\n                       list(X_test_title)\n                      )","4c704a0c":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","d30924a9":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","1ddeed46":"X_train_question = tokenizer.texts_to_sequences(X_train_question)\nX_train_title    = tokenizer.texts_to_sequences(X_train_title)\nX_train_answer   = tokenizer.texts_to_sequences(X_train_answer)\n\n\nX_test_question  = tokenizer.texts_to_sequences(X_test_question)\nX_test_title     = tokenizer.texts_to_sequences(X_test_title)\nX_test_answer    = tokenizer.texts_to_sequences(X_test_answer)","41a31b8f":"question_length_max = np.array([len(x) for x in X_train_question]).max()\nanswer_length_max   = np.array([len(x) for x in X_train_answer]).max()\nprint(question_length_max, answer_length_max)\nquestion_length_max = np.array([len(x) for x in X_test_question]).max()\nanswer_length_max   = np.array([len(x) for x in X_test_answer]).max()\nprint(question_length_max, answer_length_max)","a3e38264":"X_train_question = pad_sequences(X_train_question, maxlen=300)\nX_train_answer   = pad_sequences(X_train_answer, maxlen=300)\nX_train_title    = pad_sequences(X_train_title,  maxlen=50)\n\nle_cat = LabelEncoder()\nle_host = LabelEncoder()\ntrain_cat_enc = le_cat.fit_transform(train.category)\ntrain_host_enc = le_host.fit_transform(train.host)","950bdd11":"train.category = train_cat_enc\ntrain.host = train_host_enc","6a1bc38f":"class QuestDataset(Dataset):\n    \n    def __init__(self, df, questions, answers, titles):\n        \n        self.df         = df\n        self.questions  = questions\n        self.answers    = answers\n        self.titles     = titles\n        self.categories = self.df.category.values\n        self.hosts      = self.df.host.values\n        self.question_cols = ['question_asker_intent_understanding',\n                              'question_body_critical', 'question_conversational',\n                              'question_expect_short_answer', 'question_fact_seeking',\n                              'question_has_commonly_accepted_answer',\n                              'question_interestingness_others', 'question_interestingness_self',\n                              'question_multi_intent', 'question_not_really_a_question',\n                              'question_opinion_seeking', 'question_type_choice',\n                              'question_type_compare', 'question_type_consequence',\n                              'question_type_definition', 'question_type_entity',\n                              'question_type_instructions', 'question_type_procedure',\n                              'question_type_reason_explanation', 'question_type_spelling',\n                              'question_well_written']\n        self.answer_cols   = ['answer_helpful',\n                              'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n                              'answer_satisfaction', 'answer_type_instructions',\n                              'answer_type_procedure', 'answer_type_reason_explanation',\n                              'answer_well_written']\n        \n        \n        self.label = self.df[self.question_cols + self.answer_cols].values\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \n        question = self.questions[idx]\n        answer   = self.answers[idx]\n        title    = self.titles[idx]\n        category = self.categories[idx]\n        host     = self.hosts[idx]\n        \n        \n        labels = self.label[idx]\n        \n        return [question, answer, title], labels","514872bd":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x","49c607bf":"class QuestModel(nn.Module):\n    \n    def __init__(self, embedding_matrix):\n        super().__init__()\n        \n        LSTM_UNITS = 128\n        embed_size = embedding_matrix.shape[1]\n        DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n        max_features = 30000\n        \n        self.embedding        = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        \n        ###########################################################\n        #LSTM \n        ##########################################################\n        self.lstm_q_1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm_q_2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.lstm_a_1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm_a_2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.lstm_t_1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm_t_2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n    \n        self.linear1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS * 2, DENSE_HIDDEN_UNITS),\n                                     nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                     nn.ReLU(inplace=True),\n                                     nn.Dropout(0.5))\n            \n        self.bilinear = nn.Bilinear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2  = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS*3, DENSE_HIDDEN_UNITS),\n                                      nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                      nn.ReLU(inplace=True),\n                                      nn.Dropout(0.5))\n        \n        self.linear_q_out  = nn.Linear(DENSE_HIDDEN_UNITS, 21)\n        self.linear_aq_out = nn.Linear(DENSE_HIDDEN_UNITS, 9)\n        \n    def forward(self, question, answer, title):\n        \n        #######################################\n        #Question\n        #######################################\n        \n        question_embedding = self.embedding(question.long())\n        question_embedding = self.embedding_dropout(question_embedding)\n        \n        q_lstm1, _ = self.lstm_q_1(question_embedding)\n        q_lstm2, _ = self.lstm_q_2(q_lstm1)\n        \n        q_avg_pool    = torch.mean(q_lstm2, 1)\n        q_max_pool, _ = torch.max(q_lstm2, 1)\n        \n        #######################################\n        #answer\n        #######################################\n        answer_embedding   = self.embedding(answer.long())\n        answer_embedding   = self.embedding_dropout(answer_embedding)\n        \n        a_lstm1, _ = self.lstm_a_1(answer_embedding)\n        a_lstm2, _ = self.lstm_a_2(a_lstm1)\n        \n        a_avg_pool    = torch.mean(a_lstm2, 1)\n        a_max_pool, _ = torch.max(a_lstm2, 1)\n        \n        #######################################\n        #title\n        #######################################\n        \n        title_embedding   = self.embedding(title.long())\n        title_embedding   = self.embedding_dropout(title_embedding)\n        \n        t_lstm1, _ = self.lstm_t_1(title_embedding)\n        t_lstm2, _ = self.lstm_t_2(t_lstm1)\n        \n        t_avg_pool    = torch.mean(t_lstm2, 1)\n        t_max_pool, _ = torch.max(t_lstm2, 1)\n        \n        q_features = torch.cat((q_max_pool, q_avg_pool), 1) #LSTM_UNIT * 4\n        a_features = torch.cat((a_max_pool, a_avg_pool), 1) #LSTM_UNIT * 4\n        t_features = torch.cat((t_max_pool, t_avg_pool), 1) #LSTM_UNIT * 4\n        \n        hidden_q  = self.linear1(torch.cat((q_features, t_features), 1)) #LSTM * 8\n        bil_sim   = self.bilinear(q_features, a_features)      \n        hidden_aq = self.linear2(torch.cat((q_features, a_features, bil_sim), 1))\n        \n        q_result  = self.linear_q_out(hidden_q)\n        aq_result = self.linear_aq_out(hidden_aq)\n        out = torch.cat([q_result, aq_result], 1)\n        \n        return out","23451c17":"def train_model(train_loader, optimizer, criterion):\n    \n    model.train()\n    avg_loss = 0.\n    \n    for idx, (inputs, labels) in enumerate(train_loader):\n        questions, answers, title = inputs\n        questions, answers, title = questions.cuda(), answers.cuda(), title.cuda()\n        labels = labels.float().cuda()\n        \n        optimizer.zero_grad()\n        output_train = model(questions, answers, title)\n        loss = criterion(output_train,labels)\n        loss.backward() \n        optimizer.step()\n        avg_loss += loss.item() \/ len(train_loader)\n        \n    return avg_loss\n\ndef val_model(val_loader):\n    avg_val_loss = 0.\n    model.eval() #\u5b9f\u884c\u30e2\u30fc\u30c9\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(val_loader):\n            questions, answers, title = inputs\n            questions, answers, title = questions.cuda(), answers.cuda(), title.cuda()\n            labels = labels.float().cuda()\n            output_val = model(questions, answers, title)\n            avg_val_loss += criterion(output_val, labels).item() \/ len(val_loader)\n        \n    return avg_val_loss","c7740ffd":"kf = KFold(n_splits=5, shuffle=True, random_state=seed)\nfor fold, (train_index, val_index) in enumerate(kf.split(range(len(train)))):\n    print(\"fold:\", fold)\n    train_df = train.iloc[train_index]\n    val_df   = train.iloc[val_index]\n    \n    train_set    = QuestDataset(train_df, X_train_question[train_index], \n                                X_train_answer[train_index],\n                                X_train_title[train_index])\n    train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n    \n    val_set      = QuestDataset(val_df, \n                                X_train_question[val_index], \n                                X_train_answer[val_index],\n                                X_train_title[val_index],)\n    val_loader   = DataLoader(val_set, batch_size=128, shuffle=False)\n    \n    model = QuestModel(embedding_matrix)\n    model.cuda()\n    \n    best_avg_loss   = 100.0\n    best_param_loss = None\n    i = 0\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n    criterion = nn.BCEWithLogitsLoss()\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    \n    for epoch in range(epochs):\n        \n        if i == 5: break\n        #print(optimizer.param_groups[0]['lr'])\n        start_time   = time.time()\n        avg_loss     = train_model(train_loader, optimizer, criterion)\n        avg_val_loss = val_model(val_loader)\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format( epoch + 1, epochs, avg_loss, avg_val_loss, elapsed_time))\n    \n        if best_avg_loss > avg_val_loss:\n            i = 0\n            best_avg_loss = avg_val_loss \n            best_param_loss = model.state_dict()\n        else:\n            i += 1\n            \n        torch.save(best_param_loss, 'weight_loss_best_{}.pt'.format(fold))","a1a56fd3":"with open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","4e83e629":"## Preparing Train","cae72eca":"## Useful Function"}}