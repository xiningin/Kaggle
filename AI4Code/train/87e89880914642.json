{"cell_type":{"2d2a79ce":"code","2d25e9d2":"code","932ced25":"code","04f997f5":"code","98295882":"code","f08cc9e8":"code","bb631ac6":"code","7c7f9668":"code","a27efbb9":"code","1e1164d2":"code","6b522e29":"code","7852876c":"code","e4cf73b2":"code","abd4e37a":"code","065cef90":"code","9d02cc54":"code","c1dff5b9":"code","cf2eb4f6":"code","70df57d1":"code","16393b29":"code","7a49c09a":"code","35ab25fd":"code","939a3b4b":"code","795c0881":"code","23249300":"code","0268e9ac":"code","d8d00551":"code","efe55d88":"code","f6763d17":"code","dcd7a4b8":"code","0276419a":"markdown","ef4af3c6":"markdown","521b3064":"markdown","6485ff74":"markdown","c5b3be54":"markdown","79c1832d":"markdown","83296795":"markdown","49a51eac":"markdown","4ce48dc6":"markdown","4c759264":"markdown","7d31f2b6":"markdown","5ff7178e":"markdown","d7c7e015":"markdown","b8ec39f6":"markdown","9855cafb":"markdown","e3116904":"markdown","abbfd7b8":"markdown","299fd6f5":"markdown","bcf2ecbe":"markdown","67361966":"markdown"},"source":{"2d2a79ce":"!pip install sacrebleu","2d25e9d2":"# product_title_translation_eval_script.py\n\"\"\"Sample evaluation script for product title translation.\"\"\"\n\nimport re\nfrom typing import List\n\nimport regex\nfrom sacrebleu import corpus_bleu\n\nOTHERS_PATTERN: re.Pattern = regex.compile(r'\\p{So}')\n\n\ndef eval(preds: List[str], refs: List[str]) -> float:\n    \"\"\"BLEU score computation.\n\n    Strips all characters belonging to the unicode category \"So\".\n    Tokenize with standard WMT \"13a\" tokenizer.\n    Compute 4-BLEU.\n\n    Args:\n        preds (List[str]): List of translated texts.\n        refs (List[str]): List of target reference texts.\n    \"\"\"\n    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n    return corpus_bleu(\n        preds, [refs],\n        lowercase=True,\n        tokenize='13a',\n        use_effective_order= False\n    ).score","932ced25":"# Better eval function that can take single strings\ndef better_eval(preds, refs):\n    if not isinstance(preds, list):\n        preds = [preds]\n    if not isinstance(refs, list):\n        refs = [refs]\n    return eval(preds, refs)","04f997f5":"import pandas as pd\nimport re","98295882":"!ls ..\/input\/student-shopee-code-league-product-translation","f08cc9e8":"dev_en_df = pd.read_csv('..\/input\/student-shopee-code-league-product-translation\/dev_en.csv')\ndev_tcn_df = pd.read_csv('..\/input\/student-shopee-code-league-product-translation\/dev_tcn.csv')","bb631ac6":"dev_df = pd.DataFrame({'text': dev_tcn_df['text'], 'translation_output': dev_en_df['translation_output']})\nprint(dev_df.shape)\ndev_df.head()","7c7f9668":"better_eval(dev_df['text'].to_list(), dev_df['translation_output'].to_list())","a27efbb9":"# https:\/\/stackoverflow.com\/questions\/2718196\/find-all-chinese-text-in-a-string-using-python-and-regex\nzh_pattern = re.compile(u'[\u2e80-\u2e99\u2e9b-\u2ef3\u2f00-\u2fd5\u3005\u3007\u3021-\u3029\u3038-\u303a\u303b\u3400-\u4db5\u4e00-\u9fc3\u8c48-\u9db4\u4fae-\u983b\u4e26-\u9f8e]', re.UNICODE)","1e1164d2":"# text charset\n\"\".join(sorted(list(set(\"\".join(dev_df['text'].to_list())))))","6b522e29":"# translation_output charset\n\"\".join(sorted(list(set(\"\".join(dev_df['translation_output'].to_list())))))","7852876c":"OTHERS_PATTERN.sub('', \"\u2018\u2019\u25e2\u25e4\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\ud83c\udf80\")","e4cf73b2":"def test_symbol(symbol):\n    with_symbol = f'The quick fox jumped {symbol} over the lazy dog'\n    without_symbol =  f'The quick fox jumped over the lazy dog'\n    return symbol, better_eval(without_symbol, with_symbol)","abd4e37a":"for symbol in \"\u2018\u2019\u25e2\u25e4\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\ud83c\udf80\":\n    print(test_symbol(symbol))","065cef90":"better_eval('The quick fox jumped over the lazy lazy lazy dog', 'The quick fox jumped over the lazy dog')","9d02cc54":"masked_character_pattern= re.compile(r'[^ -~\u00d7\u00e8\u2018\u2019\u25e2\u25e4\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\ud83c\udf80]')","c1dff5b9":"dev_df['text_reduced_chars'] = dev_df['text'].str.replace(masked_character_pattern, ' ').str.replace(' +', ' ')\ndev_df.head()","cf2eb4f6":"better_eval(dev_df['text_reduced_chars'].to_list(), dev_df['translation_output'].to_list())","70df57d1":"dev_df.loc[dev_df['text'].str.contains('\u3010')]","16393b29":"# Note the missing spacing                    !     !                                   !    !\nbetter_eval(\"IFairies zircon tassel earrings \u301033531\u3011\", \"IFairies zircon tassel earrings \u3010 33531 \u3011\")","7a49c09a":"funny_punctuation_pattern = re.compile(r'([\u00d7\u00e8\u2018\u2019\u25e2\u25e4\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\ud83c\udf80])')","35ab25fd":"dev_df['text_spaced_punctuation'] = dev_df['text_reduced_chars'].str.replace(funny_punctuation_pattern, lambda m: f' {m.groups(1)[0]} ').str.replace(' +', ' ')\ndev_df.head()","939a3b4b":"dev_df.loc[dev_df['text'].str.contains('\u3010')]","795c0881":"better_eval(dev_df['text_spaced_punctuation'].to_list(), dev_df['translation_output'].to_list())","23249300":"from itertools import chain","0268e9ac":"# Add some padding to punctuation\nraw_replaces = [\n    ('#', ' # '),\n    ('\uff06', ' & '),\n    (r'\\(', ' ('),\n    (r'\\)', ') '),\n    (r'\\*', ' * '),\n    (r'\\+', ' + '),\n    ('-', ' - '),\n    ('\/', ' \/ '),\n    (':', ': '),\n    ('<', ' < '),\n    ('>', ' > '),\n    ('@', ' @ '),\n    ('_', ' _ '),\n    ('~', ' ~ '),\n    ('\u00e8', ' \u00e8 '),\n    ('=', ' = '), \n    (r'\\\\', r' \\ '),\n    ('\u2161', ' Ii '),\n    ('\u3002', '. '),\n    ('\u30fb', ' \u30fb '),\n    (' \u339d ', ' Cm '),\n    ('\u25e2', ' \u25e2 '),\n    ('\u25e4', ' \u25e4 '),\n    ('\u3010', ' \u3010 '),\n    ('\u3011', ' \u3011 '),\n    ('\u2162', ' Iii '),\n    ('\u2163', ' Iv '),\n    ('\uff08', ' ('),\n    ('\uff09', ') '),\n    ('\uff0a', ' * '),\n    ('\uff0c', ', '),\n    ('\uff0d', ' - '),\n    ('\uff0e', '. '),\n    ('\uff0f', ' \/ '),\n    ('\uff1a', ': '),\n    ('\ufe31', ' | '),\n    ('\uff01', '! '),\n    ('\uff02', '\"'),\n    ('\uff03', ' # '),\n    ('\uff05', '% '),\n    (' \uff06 ', ' & '),\n    ('\uff0b', ' + '),\n    ('\uff1c', ' < '),\n    ('\uff1e', ' > '),\n    ('\uff1f', ' ? '),\n    ('\uff5e', ' ~ '),\n    ('\uff06', ' & '),\n]\n\n# Convert fullwidth characters to english\nfullwidth_offset = ord('\uff21') - ord('A')\nfullwidth_replaces = [(chr(i + fullwidth_offset), chr(i)) for i in chain(range(ord('0'), ord('9') + 1), range(ord('A'), ord('Z') + 1), range(ord('a'), ord('z') + 1))]\n\n# Allow only ASCII and chinese characters\nblacklist_pattern = re.compile(u'[^ -:<-\\]_a-z|~\u00d7\u00e8\u25e2\u25e4\u3008\u3009\u3010\u3011\u2e80-\u2e99\u2e9b-\u2ef3\u2f00-\u2fd5\u3005\u3007\u3021-\u3029\u3038-\u303a\u303b\u3400-\u4db5\u4e00-\u9fc3\u8c48-\u9db4\u4fae-\u983b\u4e26-\u9f8e]', re.UNICODE)\nblacklist_replaces = [(blacklist_pattern, ' ')]","d8d00551":"cleaning_text = dev_df['text']\nfor pat, sub in chain(raw_replaces, fullwidth_replaces, blacklist_replaces):\n    cleaning_text = cleaning_text.str.replace(pat, sub)\ndev_df['text_cleaned'] = cleaning_text\ndev_df.head()","efe55d88":"dev_df['text_cleaned_reduced_chars'] = dev_df['text_cleaned'].str.replace(masked_character_pattern, ' ').str.replace(' +', ' ')\ndev_df.head()","f6763d17":"better_eval(dev_df['text_cleaned'].to_list(), dev_df['translation_output'].to_list())","dcd7a4b8":"better_eval(dev_df['text_cleaned_reduced_chars'].to_list(), dev_df['translation_output'].to_list())","0276419a":"## Let's take a closer look at the character space","ef4af3c6":"Hmm, the spacing around the english text looks a bit different.\n\nShopee's translator tends to insert spaces between the punctuation and any alphanumeric data. Will that affect score?","521b3064":"Oh wow, bleu thinks `\u301033531\u3011` and `\u3010 33531 \u3011` are almost totally different!\n\nLet's add back the spacing and see if that changes anything","6485ff74":"You immediately get 13.0, because there's alot of serial numbers and what not that will count towards the final result","c5b3be54":"Some symbols are still preserved!","79c1832d":"## Symbols and BLEU\n\nIf we remove the symbols during cleaning, will that affect score?","83296795":"## What happens if we just submit chinese?\n\nThere is some english text inside the input data, right?","49a51eac":"Sacrebleu thinks some of these characters are meaningful glyphs!","4ce48dc6":"Looks better, how does that do?","4c759264":"# Closing\n\n![image.png](attachment:image.png)\n\nSubmission 9 was created by:\n- Jieba to segment the chinese text \n- Using fasttext word vectors aligned for zh-en use to generate a word to word mapping from chinese to english\n\nSubmission 8 added in this punctuation cleaning and it jumped 9+ bleu\n\nFinal submissions gained score by training on validation data. Turns out shopee's model solution is **very** esoteric with some weird text that other trained from scratch models could not pick up.","7d31f2b6":"## Why's it worse??\n\nLet's narrow down our checks to rows with the funny brackets","5ff7178e":"# Look at data","d7c7e015":"Other than ASCII characters and CJK glyphs, there seems to be some symbols in the input data:\n\n```\n\u2018\u2019\u2027\u2161\u25c6\u25e2\u25e4\u2605\u2606\u2666\u266a\u2764\u3001\u3002\u3008\u3009\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\n```\n\nBut Shopee's reference trims down the extra symbols to\n\n```\n\u2018\u2019\u25e2\u25e4\u300a\u300b\u300e\u300f\u3010\u3011\u30fb\ud83c\udf80\n```\n\nThey did say the text will be cleaned of emoji right?","b8ec39f6":"# Ideas","9855cafb":"## After more eyeballing...","e3116904":"## What happens if we prune the chinese input to just that character space?\n\nBLEU penalizes if extra text is found in a prediction, so if we remove extra tokens the score should improve","abbfd7b8":"13 to 18 BLEU just by sorting out punctuation!","299fd6f5":"\ud83e\udd14","bcf2ecbe":"## New and improved punctuation!","67361966":"# Grading Script"}}