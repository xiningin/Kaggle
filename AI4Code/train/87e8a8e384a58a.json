{"cell_type":{"bfbf768d":"code","09bb9465":"code","28632eee":"code","a24d27b1":"code","a2532e4d":"code","f5dc9a88":"code","c152655b":"code","6eff0d0a":"code","9d881051":"code","cb7ac558":"code","264c380b":"code","ba3960c8":"markdown","2fca1b7b":"markdown","e8a93a3e":"markdown","ffb22f9a":"markdown","5241e4e9":"markdown","a7336ab6":"markdown","1413c631":"markdown","335c4092":"markdown","7dc66f67":"markdown","483ec701":"markdown"},"source":{"bfbf768d":"import numpy as np","09bb9465":"data = open(\"..\/input\/speech\/Speech.txt\", \"r\")","28632eee":"reading= [x.strip() for x in data.readlines()]","a24d27b1":"text = ''\nfor i in reading:\n    text = text + i\nprint(text)","a2532e4d":"def generateTable(data,k=4):\n    \n    T = {}\n    for i in range(len(data)-k):\n        X = data[i:i+k]\n        Y = data[i+k]\n        #print(\"X  %s and Y %s  \"%(X,Y))\n        \n        if T.get(X) is None:\n            T[X] = {}\n            T[X][Y] = 1\n        else:\n            if T[X].get(Y) is None:\n                T[X][Y] = 1\n            else:\n                T[X][Y] += 1\n    \n    return T","f5dc9a88":"def convertFreqIntoProb(T):     \n    for kx in T.keys():\n        s = float(sum(T[kx].values()))\n        for k in T[kx].keys():\n            T[kx][k] = T[kx][k]\/s\n                \n    return T","c152655b":"def trainMarkovChain(text,k=4):\n    \n    T = generateTable(text,k)\n    T = convertFreqIntoProb(T)\n    \n    return T","6eff0d0a":"model = trainMarkovChain(text)","9d881051":"def sample_next(ctx,T,k):\n    ctx = ctx[-k:]\n    if T.get(ctx) is None:\n        return \" \"\n    possible_Chars = list(T[ctx].keys())\n    possible_values = list(T[ctx].values())\n    \n    #print(possible_Chars)\n    #print(possible_values)\n    \n    return np.random.choice(possible_Chars,p=possible_values)","cb7ac558":"def generateText(starting_sent,k=4,maxLen=1000):\n    \n    sentence = starting_sent\n    ctx = starting_sent[-k:]\n    \n    for ix in range(maxLen):\n        next_prediction = sample_next(ctx,model,k)\n        sentence += next_prediction\n        ctx = sentence[-k:]\n    return sentence","264c380b":"text = generateText(\"dear\",k=4,maxLen=2000)\nprint(text)","ba3960c8":"**This functions gives the list of all the possible next characters along with the probability of each of those**","2fca1b7b":"**It shows the probability of each combination and returns the letter which has the maximum probability**","e8a93a3e":"# In this notebook we'll be working on generating text, that is, lyrics, speeches, any form of text synthetically.The concept of Markov Chains is used for here. Do check out [this link for more information about Markov Chains](https:\/\/en.wikipedia.org\/wiki\/Markov_chain)","ffb22f9a":"**Thus, this is what the input data looks like**","5241e4e9":"**The input data is a experts of a speech made by the Honourable Prime Minister during the occassion of Independence Day. Using this data combined with the concept of Markov Chains, we'll be generating a similar speech**","a7336ab6":"# Thus, in this way, given an input speech, we generated a programatically generated speech using basics of Python and Markov Chains, This technique has immense uses and can be used to synthetically generate texts\/lyrics\/speeches anything.Multiple speeches or songs or text files can be given as input for training which can help generate completely new text for us. There are advanced way of doing this as well. This is a beginner's guide to the application of this concept. Thank you for reading this! Do leave your feedbacks and views to help me improve further. Thanks :)\n","1413c631":"**Given the starting word as the input, and the length of the expected synthetically generated text, this functions calls the function sample next continousuly till the expected text is generated**\n- starting_sent is what you expect the start to be\n- k is the length of words after starting sent upto which one wants markov chains to be formed\n- maxlen is the length of the expected output","335c4092":"**Reading the data set**","7dc66f67":"Here, we get the frequency of occurence of 2 consecutive letters\n- For example, If letter is 'd', the possibility of 'e' following 'd' is very high since the word 'dear' is repreated multiple times. So the frequency of all such possible combinations is computed here.","483ec701":"**Importing the necessary libraries**"}}