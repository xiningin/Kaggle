{"cell_type":{"937e68ef":"code","e3c06125":"code","4c3c4c13":"code","6efd79e2":"code","b366e181":"markdown","13eb4add":"markdown","2ba0c9fe":"markdown","c59ac995":"markdown","de5dc2ca":"markdown","098799ee":"markdown","d3cf93e5":"markdown","d7cbb7fb":"markdown","b68da07a":"markdown"},"source":{"937e68ef":"import numpy as np","e3c06125":"def entropy(y):\n    cnt = np.bincount(y)\n    # creates an array cnt, cnt[i] = count of entries of number i.\n    \n    probabilities = cnt \/ len(y)\n    # array of probabilities for each state\n        \n    res = -np.sum([p * np.log(p) for p in probabilities if p > 0])\n    # calculating entropy\n    return res\n\n\nclass ClassifyNode:\n    \n    # ClassidyNode class contains child nodes, best threshold and best feature for splitting\n    \n    def __init__(self, feature=None, threshold=None, l_node=None, r_node=None, mark=None):\n        \n        self.feature = feature     \n        # idx of feature for splitting\n        \n        self.threshold = threshold\n        # threshold value for splitting\n        \n        self.l_node = l_node       \n        # left ClassifyNode object \n        \n        self.r_node = r_node       \n        # right ClassifyNode object \n        \n        self.mark = mark           \n        # if it is a leaf node, it gets a class mark \n        \n    def is_leaf_node(self):\n        return self.mark is not None\n        \nclass MyDecisionTreeClassifier:\n    \n    def __init__(self, min_samples_split=2, max_depth=\"inf\", n_feats=None):\n        self.min_samples_split = min_samples_split \n        # min number of samples in node to allow splitting\n        \n        self.max_depth = max_depth\n        # max depth of tree\n        \n        self.n_feats = n_feats\n        # if n_feats < X.shape[1] then choose features randomly\n        \n        self.root = None\n        # root ClassifyNode object\n        \n    def fit(self, X, y):\n        \n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        # avoid situation when n_feats > real number of features \n        \n        self.root = self._build_node(X, y)\n        # The first node for splitting\n        \n    \n        \n        \n    def predict(self, X):\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n        # see _traverse_tree function\n    \n    \n    def _traverse_tree(self, x, node):\n        if node.is_leaf_node():\n        # if current node is terminal then return class with biggest occurrence level\n        \n            return node.mark\n        \n        \n        if x[node.feature] <= node.threshold:  \n        # chek feature in this node and compare node threshold with feature value in x \n            \n            return self._traverse_tree(x, node.l_node)\n            # send x to child node until it gets the leaf node\n            \n        return self._traverse_tree(x, node.r_node)\n    \n        \n    \n    \n    \n    def _build_node(self, X, y, depth=0):\n        \n        n_samples, n_features = X.shape \n        # in arrays and Dataframes shapes are in format (n_samples, n_features)\n        \n        n_labels = len(np.unique(y))\n        # array of unique classes\n        \n        # check stopping criteria (max_depth & min_samples_split)\n        \n        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n            leaf_mark = self._most_encountered_mark(y)\n            return ClassifyNode(mark=leaf_mark)\n        \n        \n        feature_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n        # if we decided to choose a subset of features\n        \n        best_feature, best_thresh = self._best_split(X, y, feature_idxs)\n        # see _best_split()\n        \n        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n        # see _split()\n        \n        left = self._build_node(X[left_idxs, :], y[left_idxs], depth+1)\n        right = self._build_node(X[right_idxs, :], y[right_idxs], depth+1)\n        # after we find best split, we give left data and right data to child nodes\n        \n        return ClassifyNode(best_feature, best_thresh, left, right)\n        \n        \n    def _best_split(self, X, y, feature_idxs):\n        \n        best_information_gain = -1\n        \n        split_idx, split_thresh = None, None\n        \n        for idx in feature_idxs:\n            # for each feature\n            \n            X_column=X[:, idx]\n            # take all values in this features\n\n            thresholds = np.unique(X_column)\n            # drop similar values\n            \n            for threshold in thresholds:\n                # for each unique value\n                \n                gain = self._information_gain(y, X_column, threshold)\n                # see _information_gain()\n                \n                if gain > best_information_gain:\n                    best_information_gain = gain\n                    split_idx = idx\n                    split_thresh = threshold\n                # save information gain in case it better than previous\n                    \n        return split_idx, split_thresh\n    \n    \n    def _information_gain(self, y, X_column, split_thresh):\n        \n        parent_entropy = entropy(y)\n        # see entropy()\n        \n        left_idxs, right_idxs = self._split(X_column, split_thresh)\n        # see _split()\n        \n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 0\n        # check if we splitted nothing\n        \n        n = len(y)\n        n_l, n_r = len(left_idxs), len(right_idxs)\n        # number of samples in left and right nodes\n        \n        entropy_l, entropy_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n        \n        child_entropy = (n_l\/n)*entropy_l + (n_r\/n)*entropy_r\n        # weighted sum of left and right child\n        \n        info_gain = parent_entropy - child_entropy\n        # finally calculated information gain \n        \n        return info_gain\n    \n    \n    def _split(self, X_column, split_thresh):\n        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n        \n        return left_idxs, right_idxs\n        \n    \n    def _most_encountered_mark(self, y):\n        most_encountered = np.argmax(np.bincount(y))\n        return most_encountered","4c3c4c13":"\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred)\/len(y_true)\n    return accuracy\n\n\ndata = datasets.load_breast_cancer()\n\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(\"Train shape:\", X_train.shape)\n\nsk_clf = DecisionTreeClassifier(max_depth=100, criterion='entropy', random_state=0)\nmy_clf = MyDecisionTreeClassifier(max_depth=100)\n\nmy_clf.fit(X_train, y_train)\nsk_clf.fit(X_train, y_train)\n\nmy_y_pred = my_clf.predict(X_train)\nsk_y_pred = sk_clf.predict(X_train)\n\nmy_acc = accuracy(y_train, my_y_pred)\nsk_acc = accuracy(y_train, sk_y_pred)\n\nprint(\"Working test on train data\", my_acc)\nprint(\"Working test on train data sklearn\", sk_acc)\n\nmy_y_pred = my_clf.predict(X_test)\nsk_y_pred = sk_clf.predict(X_test)\n\nmy_acc = accuracy(y_test, my_y_pred)\nsk_acc = accuracy(y_test, sk_y_pred)\n\nprint(\"Working test on test data\", my_acc)\nprint(\"Working test on test data sklearn\", sk_acc)","6efd79e2":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred)\/len(y_true)\n    return accuracy\n\n\ndata = datasets.load_breast_cancer()\n\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(\"Train shape:\", X_train.shape)\n\nsk_clf = DecisionTreeClassifier(max_depth=100, criterion='entropy', random_state=0)\nmy_clf = MyDecisionTreeClassifier(max_depth=100)\n\n%time my_clf.fit(X_train, y_train)\n%time sk_clf.fit(X_train, y_train)\n","b366e181":"<font size=\"4\"><b>Our algorithm was written without any optimizing so it works much longer<br>\nthan built-in Decision Tree, but it's enough for understanding what's going on<\/b><\/font>","13eb4add":"# 2. Information gain \n\n<font size=\"4\">After we can calculate entropy we can also calculate information gain<\/font><br><br>\n\n<p style=\"text-align: center;\"><font size=\"5\">I(node) = E(node) - ($\\frac{left}{n}$ * E(left_node) + $\\frac{right}{n}$ * E(right_node))<\/font><\/p><br>\n\n<font size=\"4\">Information gain is a indicator of how much chaos can be reduced by particular split.<br><br>We want to reduce chaos as much as possible, so we will search for split that gives us largest information gain.<br><br>Why we take weighted sum of left and right node? Because if you watch carefully you can observe that not weighted sum will give us situation when samples are splitted one by one<\/font>\n\n\n\n","2ba0c9fe":"# Compare time of fitting","c59ac995":"# Comparing\n\n## Let's compare our algorithm with built-in DecisionTree in sklearn","de5dc2ca":"# 3. Recursive approach in the algorithm\n\n<font size=\"4\">We will apply greedy algorithm and the main steps are:<\/font>\n<br>\n1. Take our data\n2. Iterate through all features\n3. In each feature iterate through all values\n4. For each pair feature\/value calculate the entropy and information gain\n5. Find the best pair that give us largest information gain\n6. Split our data by that pair\n7. Give splitted data to left and right node of the tree\n8. Repeat\n","098799ee":"# Decision tree implementation with numpy only\n\n### This notebook is dedicated to understanding Decision Tree algorithm because always there is a group of people who use it without understanding things under the hood\n![tree-img](https:\/\/dodskypict.com\/D\/Dark-Forest-Wallpaper-On-Wallpaper-Hd-17.jpg)","d3cf93e5":"# Theory\n\n## In this notebook you will see implementation of classification decision tree. At first, few words about idea that lies under that algorithm\n\n1. <font size=\"3\"><i>During the algorithm work train data will be splitted based on some measurement of how good our split is.\n   For classification problem there are two main indicators: \"gini index\" and \"information entropy\".\n   In this notebook <b>\"information entropy\"<\/b> will be used.<\/i><\/font>\n2. <font size=\"3\"><i>Decision tree is a structure that uses recursive algorithm of fitting, so you should repeat at least <b>what recursion is<\/b>.<\/i><\/font>\n3. <font size=\"3\"><i>If you don't want to be lost every couple of strings you should repeat basics of <b>numpy.array<\/b> creation and <b>basics operations with them<\/b>.<\/i><\/font>\n","d7cbb7fb":"# Sources\n1. Infornmation entropy (rus) - https:\/\/www.youtube.com\/watch?v=KMEqfb6KO0c\n2. Information entropy (eng) - https:\/\/www.youtube.com\/watch?v=2s3aJfRr9gE\n","b68da07a":"# 1. Information Entropy\n\n<font size=\"3\"><b>I will tell you nothing about origin of formula because this notebook is not about math. All need resource you will find at the end of notebook. Just take it for granted, there is one interesting formula that shows how chaotic our system is<\/b><\/font>\n<br><br>\n\n<p style=\"text-align: center;\"><font size=\"8\"><b>-$\\sum_{i=1}^{N} P_i log(P_i)$<\/b><\/font><\/p>\n<br><br>\n\n<font size=\"3\">For example suppose we have a system with 4 balls where <b>all balls are green<\/b>. In that case we have only one possible state in our system and our formula would be like this:<\/font>\n<br><br>\n\n<p style=\"text-align: center;\"><font size=\"8\"><b>-$\\sum_{i=1}^{4} 1* log(1)$<\/b><\/font><\/p>\n<br><br>\n\n<font size=\"3\">Because the probability of random pulled ball of being green(our state) is one, our formula will give us zero (this system doesn't give as any information and we certainly know the color of our ball)<\/font> \n<br><br>\n\n<font size=\"3\">Now let's consider system that will give as the largest entropy. Suppose we have a system with 4 balls but now each of them has <b>different color<\/b> (green, red, black and white). Our formula would be like this<\/font> \n<br><br>\n\n<p style=\"text-align: center;\"><font size=\"8\"><b>-$\\sum_{i=1}^{4}$$^1\/_4$ $log$($^1\/_4$)<\/b><\/font><\/p>\n<br><br>\n\n<font size=\"3\">Because of our logarithm has small number and there is a minus at the start of the formula our entropy becomes large<\/font> \n<br><br>\n\n<font size=\"3\">Well, it's enough for understanding. Just remember that our formula give <b>larger<b> value when there are many classes and vice versa when we have a little amount of classes<\/font> "}}