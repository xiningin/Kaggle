{"cell_type":{"ae1a39e2":"code","00ee45b5":"code","44d98762":"code","ea4d4643":"code","6f1003d0":"code","43eb9e89":"code","f6da7388":"code","27c37e44":"code","47a6713e":"code","d5cef2d5":"code","ef60a8ea":"code","250aa3cd":"code","aa247149":"code","254abf66":"code","3be047c5":"code","25febc47":"code","20ae8865":"markdown","a215dee4":"markdown","e2817658":"markdown","bccff5c9":"markdown","6c3076b6":"markdown","59f91514":"markdown","212d6406":"markdown","26af1937":"markdown","2dae07e9":"markdown"},"source":{"ae1a39e2":"import keras\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Activation, Dense, Dropout\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.core import Dense, Activation\nimport keras.utils as kutils","00ee45b5":"#import nltk\n#nltk.corpus.gutenberg.fileids()\n","44d98762":"#raw_txt = nltk.corpus.gutenberg.raw(\"shakespeare-hamlet.txt\") + \\\n#nltk.corpus.gutenberg.raw(\"shakespeare-caesar.txt\") + \\\n#nltk.corpus.gutenberg.raw(\"shakespeare-macbeth.txt\")\n\n# C\u00f3digo adaptado de https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms\nimport pandas as pd\nimport os\n\ncurr_dir = '..\/input\/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\n\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","ea4d4643":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nimport string\n\nall_sents = [[w.lower() for w in word_tokenize(sen) if not w in string.punctuation] \\\n             for sen in all_headlines]\n\nx = []\ny = []\n\nprint(all_sents[:10])\n\nfor sen in all_sents:\n    for i in range(1, len(sen)):\n        x.append(sen[:i])\n        y.append(sen[i])\n        \n\nprint(x[:10])\nprint(y[:10])\n\n","6f1003d0":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\nall_text = [c for sen in x for c in sen]\nall_text += [c for c in y]\n\nall_text.append('UNK') # Palavra desconhecida\n\nwords = list(set(all_text))\n        \nword_indexes = {word: index for index, word in enumerate(words)}      \n\nmax_features = len(word_indexes)\n\nx = [[word_indexes[c] for c in sen] for sen in x]\ny = [word_indexes[c] for c in y]\n\nprint(x[:10])\nprint(y[:10])\n\ny = kutils.to_categorical(y, num_classes=max_features)\n\nmaxlen = max([len(sen) for sen in x])\n\nprint(maxlen)\n","43eb9e89":"x = pad_sequences(x, maxlen=maxlen)\nx = pad_sequences(x, maxlen=maxlen)\n\nprint(x[:10,-10:])\nprint(y[:10,-10:])","f6da7388":"print(x[:10,-10:])\n\nfor y_ in y:\n    for i in range(len(y_)):\n        if y_[i] != 0:\n            print(i)","27c37e44":"embedding_size = 10\n\nmodel = Sequential()\n    \n# Add Input Embedding Layer\nmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\n    \n# Add Hidden Layer 1 - LSTM Layer\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\n    \n# Add Output Layer\nmodel.add(Dense(max_features, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","47a6713e":"model.summary()","d5cef2d5":"model.fit(x, y, epochs=20, verbose=5)","ef60a8ea":"import pickle\n\nprint(\"Saving model...\")\nmodel.save('shak-nlg.h5')\n\nwith open('shak-nlg-dict.pkl', 'wb') as handle:\n    pickle.dump(word_indexes, handle)\n\nwith open('shak-nlg-maxlen.pkl', 'wb') as handle:\n    pickle.dump(maxlen, handle)\nprint(\"Model Saved!\")","250aa3cd":"import pickle\n\nmodel = keras.models.load_model('shak-nlg.h5')\nmaxlen = pickle.load(open('shak-nlg-maxlen.pkl', 'rb'))\nword_indexes = pickle.load(open('shak-nlg-dict.pkl', 'rb'))","aa247149":"#sample_seed = input()\nsample_seed = \"california\"\nsample_seed_vect = np.array([[word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK'] \\\n                    for c in word_tokenize(sample_seed)]])\n\nprint(sample_seed_vect)\n\nsample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\n\nprint(sample_seed_vect)\n\npredicted = model.predict_classes(sample_seed_vect, verbose=0)\n\nprint(predicted)\n\ndef get_word_by_index(index, word_indexes):\n    for w, i in word_indexes.items():\n        if index == i:\n            return w\n        \n    return None\n\n\nfor p in predicted:    \n    print(get_word_by_index(p, word_indexes))","254abf66":"sample_seed = \"barack\"","3be047c5":"def HundredPredictedWords(sample_seed):    \n    i = 0\n    r = \"\"\n    while i < 10: \n        sample_seed_vect = np.array([[word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK'] \\\n                            for c in word_tokenize(sample_seed)]])\n\n        #print(sample_seed_vect)\n\n        sample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\n\n        #print(sample_seed_vect)\n\n        predicted = model.predict_classes(sample_seed_vect, verbose=0)\n\n        #print(predicted)\n\n        def get_word_by_index(index, word_indexes):\n            for w, i in word_indexes.items():\n                if index == i:\n                    return w\n\n            return None\n\n        \n        for p in predicted:  \n            r = (get_word_by_index(p, word_indexes))\n        #print(get_word_by_index(p, word_indexes))\n        \n        sample_seed = sample_seed + \" \" + r\n        \n        i = i + 1\n        \n    \n    return sample_seed\n    \n    ","25febc47":"sample_seed = \"\"\nprint(HundredPredictedWords(sample_seed))\n\n","20ae8865":"<h2>4. Usando LSTM para gera\u00e7\u00e3o de texto<\/h2>\n\n<p> Vamos nos basear no exemplo dispon\u00edvel em https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms, e vamos usar um LSTM para gera\u00e7\u00e3o de texto.\n    <\/p>","a215dee4":"<h2>3. Redes Neurais Recorrentes<\/h2>\n\n<p>Uma rede neural recorrente \u00e9 uma rede com um \"loop\", ou seja, uma rede que aproveita o aprendizado de uma amostra anterior para amostras seguintes. A figura abaixo, retirada de http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/ ilustra sua arquitetura.\n    <\/p>\n    \n![](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/RNN-unrolled.png)    \n\n<p>Perceba que essa arquitetura de redes neurais \u00e9 especialmente aplic\u00e1vel para problemas com sequ\u00eancias, uma vez que as amostras anteriores influenciam na aprendizagem.<\/p>\n\n<p>Por\u00e9m, as redes recorrentes convencionais tem limita\u00e7\u00f5es para utilizar informa\u00e7\u00f5es aprendidas em etapas mais distantes (que acabam sendo atualizadas durante o treino). As redes neurais LSTM vieram para tentar sanar esse problema, como veremos \u00e0 seguir.<\/p>","e2817658":"<p>Agora vejamos como o algoritmo prev\u00ea uma palavra simples.<\/p>","bccff5c9":"<h2> 1. Introdu\u00e7\u00e3o<\/h2>\n\n<p>Enquanto na Extra\u00e7\u00e3o de Informa\u00e7\u00e3o a tarefa principal estava relacionada em compreender a escrita e tratar ambiguidades ou similaridades no texto, na gera\u00e7\u00e3o autom\u00e1tica de texto o desafio est\u00e1 em decidir como expressar uma dada informa\u00e7\u00e3o em palavras.\n    <\/p>\n    \n  <p>H\u00e1 diversas aplica\u00e7\u00f5es pr\u00e1ticas para NLG (do ingl\u00eas, Natural Language Generation), como:<\/p>\n  \n  * Sumariza\u00e7\u00e3o de documentos;\n  * Gera\u00e7\u00e3o de not\u00edcias;\n  * Gera\u00e7\u00e3o de texto para chatbots.\n  \n  <p>Nesta aula vamos abordar algumas t\u00e9cnicas para gera\u00e7\u00e3o autom\u00e1tica de texto, usando redes neurais recorrentes.<\/p>","6c3076b6":"<h3>Redes LSTM<\/h3>\n\n<p>As redes LSTM (Long-Short Term Memory, ou de Mem\u00f3ria de curto e longo prazo, numa tradu\u00e7\u00e3o livre), ao contr\u00e1rio das redes neurais recorrentes convencionais, que apenas atualizam o estado aprendido em etapas anteriores, trazem o conceito de mem\u00f3ria, que \u00e9 controlada pelo modelo.\n    <\/p>\n    <p>Desta forma, esse modelo n\u00e3o apenas armazena os estados aprendidos em etapas anteriores, mas decide o que deve ser aproveitado, descartado ou atualizado. Vejamos a ilustra\u00e7\u00e3o abaixo, tamb\u00e9m extra\u00edda de http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/.\n    <\/p>\n    \n![](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)    \n\n<p>As etapas ilustradas s\u00e3o: <\/p>\n1. *Qual estado de mem\u00f3ria deve ser lembrado*: A primeira camada usa uma fun\u00e7\u00e3o sigmoid que retorna um valor entre 0 e 1 para decidir se o estado de mem\u00f3ria vindo da itera\u00e7\u00e3o anterior deve ser mantido (1 para completamente mantido) ou esquecido (0 para completamente esquecido);\n2. *Qual estado de mem\u00f3ria deve ser atualizado*: Primeiramente usa-se uma fun\u00e7\u00e3o sigmoid para decidir quais valores ser\u00e3o atualizados com os novos dados da amostra atual, depois usa-se uma fun\u00e7\u00e3o tanh (que retorna um valor entre -1 e 1) para criar um novo vetor de estados da mem\u00f3ria(denominado Ct) que poder\u00e1 ser utilizado na itera\u00e7\u00e3o seguinte;\n3. *Atualizar as c\u00e9lulas de mem\u00f3ria antigas*: Combina os estados que devem ser lembrados, os estados que devem ser atualizados e os novos estados para atualizar as c\u00e9lulas de mem\u00f3ria, que ser\u00e3o utilizadas na itera\u00e7\u00e3o seguinte;\n4. *Decide a sa\u00edda*: Por fim, a sa\u00edda da itera\u00e7\u00e3o \u00e9 feita atrav\u00e9s da combina\u00e7\u00e3o de uma fun\u00e7\u00e3o sigmoid e uma fun\u00e7\u00e3o tanh, considerando o estado atual da mem\u00f3ria.\n\n    ","59f91514":"<p>Agora vamos criar nosso modelo de LSTM usando o Keras.<\/p>","212d6406":"<h2>2. T\u00e9cnicas para Gera\u00e7\u00e3o de Texto<\/h2>\n\n<p> O objetivo da t\u00e9cnica de gera\u00e7\u00e3o autom\u00e1tica de texto \u00e9, dada uma sequ\u00eancia de n-gramas (ou caracteres, ou senten\u00e7as), prever qual ser\u00e1 o pr\u00f3ximo n-grama (ou caractere, ou senten\u00e7a).\n    <\/p>","26af1937":"<p><b>Exerc\u00edcio 9<\/b>: Escreva um algoritmo que gera palavras at\u00e9 100 palavras, com base em um texto curto de entrada (no m\u00e1ximo 5 palavras).<\/p>","2dae07e9":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 09 - Gera\u00e7\u00e3o Autom\u00e1tica de Texto <\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>"}}