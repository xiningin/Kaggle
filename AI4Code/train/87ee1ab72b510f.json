{"cell_type":{"5f006cc0":"code","b8801c32":"code","b44604cd":"code","d48bb73c":"code","3deecd71":"code","4bbb5e4c":"code","97ae6b06":"code","1656b0f0":"code","4553dd2a":"code","9dcf6864":"code","56d64ccd":"code","985a065c":"code","d929442b":"code","83ee88ea":"markdown","b634d427":"markdown","336ff26f":"markdown","9dd6dc7d":"markdown"},"source":{"5f006cc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8801c32":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport random\nfrom sklearn import preprocessing\nimport torch.nn as nn\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset","b44604cd":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nif device == 'cuda':\n  torch.cuda.manual_seed_all(111)\n\ntrain = pd.read_csv('..\/input\/ai-tomato\/training_set.csv', header=None, skiprows=1)\n\ntrain","d48bb73c":"learning_rate = 0.1\ntraining_epoch = 1000\nbatch_size = 50","3deecd71":"train[0] = train[0] % 10000 \/100\ntrain.drop(4, axis=1,inplace=True) #rainfall \uc0ad\uc81c\n\nxtrain = train.loc[:,[i for i in train.keys()[:-1]]]\nytrain = train[train.keys()[-1]]\n\nxtrain = np.array(xtrain)\nxtrain = torch.FloatTensor(xtrain).to(device)\n\nytrain = np.array(ytrain)\nytrain = torch.FloatTensor(ytrain).view(-1,1).to(device)\n\ntrain","4bbb5e4c":"train_dataset = torch.utils.data.TensorDataset(xtrain,ytrain)\n\ndata_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size = batch_size,\n                                          shuffle=True,\n                                          drop_last=True)","97ae6b06":"xtrain.shape","1656b0f0":"lin1 = nn.Linear(6,12)\nlin2 = nn.Linear(12,24)\nlin3 = nn.Linear(24,12)\nlin4 = nn.Linear(12,6)\nlin5 = nn.Linear(6,1)\n\nnn.init.kaiming_uniform_(lin1.weight)\nnn.init.kaiming_uniform_(lin2.weight)\nnn.init.kaiming_uniform_(lin3.weight)\nnn.init.kaiming_uniform_(lin4.weight)\nnn.init.kaiming_uniform_(lin5.weight)\n\nrelu = nn.ReLU()\n\nmodel = nn.Sequential(lin1,relu,\n                      lin2,relu,\n                      lin3,relu,\n                      lin4,relu,\n                      lin5\n                      ).to(device)","4553dd2a":"optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss = nn.MSELoss().to(device)","9dcf6864":"total_batch = len(data_loader)\n\nfor epoch in range(training_epoch):\n    avg_cost = 0\n    for X,Y in data_loader:\n        X = X.to(device)\n        Y = Y.to(device)\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = loss(hypothesis,Y)\n        cost.backward()\n        optimizer.step()\n\n\n        avg_cost += cost\/total_batch\n    \n    if epoch % 10 == 0:  \n        print('Epoch:', '%d' % (epoch ), 'Cost =', '{:.9f}'.format(avg_cost))\nprint('Learning Finished')","56d64ccd":"test = pd.read_csv('..\/input\/ai-tomato\/test_set.csv')\ntest","985a065c":"test=test.dropna(axis=1)","d929442b":"test['date'] = test['date'] % 10000 \/100\ntest.drop('rain fall', axis=1,inplace=True) #rainfall \uc0ad\uc81c\n\nxtest = test.loc[:,[i for i in test.keys()[:]]]\nxtest = np.array(xtest)\nxtest = torch.from_numpy(xtest).float().to(device)\n\nH = model(xtest)\n\nH = H.cpu().detach().numpy().reshape(-1,1)\n\nsubmit = pd.read_csv('..\/input\/ai-tomato\/submit_sample.csv')\n\nfor i in range(len(submit)):\n  submit['expected'][i] = H[i]\n\nsubmit.to_csv('submission.csv', index = None, header=True)","83ee88ea":"## optimizer & loss \uc124\uc815","b634d427":"## \ud559\uc2b5","336ff26f":"# \ud30c\ub77c\ubbf8\ud130 \uc124\uc815","9dd6dc7d":"## \ubaa8\ub378 \uc124\uacc4\n\n- x_train\uc758 shape\uc774 [2120,6] \uc784\uc744 \ud655\uc778\ud558\uc5ec linear model\ub85c \ub2e4\uc74c\uacfc \uac19\uc774 layer \uc744 \uad6c\uc131. \uc608\uce21 \ubb38\uc81c\uc774\uae30\ub54c\ubb38\uc5d0 \ub9c8\uc9c0\ub9c9 output\uc740 1\ub85c \uc124\uc815\n- \ucd08\uae30\ud654\ubc29\ubc95\uc740 kaiming_uniform \uc744 \uc0ac\uc6a9\n- \ud65c\uc131\ud568\uc218\ub294 relu \uc0ac\uc6a9\n- overfitting\uc758 \uacbd\ud5a5\uc774 \ubcf4\uc774\uc9c0\uc54a\uc544, dropout\uc740 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc74c"}}