{"cell_type":{"7169c3a9":"code","4d1695b8":"code","887e4365":"code","fedaf742":"code","db990d06":"code","475cc536":"code","0a6a1f5d":"code","d96e0a02":"markdown","58744f44":"markdown","c30052ed":"markdown","e55391f2":"markdown","e97dea0e":"markdown","51592ab9":"markdown","6d05c808":"markdown","31ca99e2":"markdown"},"source":{"7169c3a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nRootDir = \"\/kaggle\/input\/sentimental-analysis-for-tweets\"\n\n\nfilename = RootDir + \"\/sentiment_tweets3.csv\"\ndf = pd.read_csv(filename)\nprint (df.shape)","4d1695b8":"tweets = df.values[:,1]\nlabels = df.values[:,2].astype(float)\nprint (tweets[40], labels[40])\nprint (tweets[8002], labels[8002])","887e4365":"!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nbert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')","fedaf742":"embeddings = bert_model.encode(tweets, show_progress_bar=True)\nprint (embeddings.shape)","db990d06":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(embeddings, labels, \n                                          test_size=0.2, random_state=42)\nprint (\"Training set shapes:\", X_train.shape, y_train.shape)\nprint (\"Test set shapes:\", X_test.shape, y_test.shape)","475cc536":"from tensorflow.keras import Sequential, layers\n\nclassifier = Sequential()\nclassifier.add (layers.Dense(256, activation='relu', input_shape=(768,)))\nclassifier.add (layers.Dense(1, activation='sigmoid'))\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  \n    \nhist = classifier.fit (X_train, y_train, epochs=10, batch_size=16, \n                      validation_data=(X_test, y_test))","0a6a1f5d":"from matplotlib import pyplot\n\npyplot.figure(figsize=(15,5))\npyplot.subplot(1, 2, 1)\npyplot.plot(hist.history['loss'], 'r', label='Training loss')\npyplot.plot(hist.history['val_loss'], 'g', label='Validation loss')\npyplot.legend()\npyplot.subplot(1, 2, 2)\npyplot.plot(hist.history['accuracy'], 'r', label='Training accuracy')\npyplot.plot(hist.history['val_accuracy'], 'g', label='Validation accuracy')\npyplot.legend()\npyplot.show()","d96e0a02":"Next, we load BERT:","58744f44":"We can now run the BERT model on all tweets to get their encoding","c30052ed":"Plot the loss and accuracy:","e55391f2":"Start by some imports and reading the data","e97dea0e":"There are 768 features in the embedding vector for every tweet. Now build a simple classification model to work on them","51592ab9":"The embeddings will be our features to train a classifier, but first we need tp split the data into training and test sets:","6d05c808":"It seems we reach a very good prediction accuracy (>98%) immediately, with almost no improvement by additional epochs","31ca99e2":"We have 10,314 tweets in our dataset. Each one has a label: 0=not depressed, 1=depressed. Let's get the tweets (input) and labels (output), and print a sample of each ttype of tweet:"}}