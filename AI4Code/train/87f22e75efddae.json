{"cell_type":{"21945a59":"code","674f88af":"code","22785653":"code","74800e69":"code","4a7a51d5":"code","1f0f4412":"code","684c7739":"code","665491c7":"code","c90b39e1":"code","eda2e8b7":"code","936c254a":"code","8db7f585":"code","2c361b8c":"code","9cd21de7":"code","06aa7b78":"code","04b38ca7":"code","ede5b5c8":"code","557c4ea5":"code","8957ef0c":"code","a7b43f75":"code","996c91cf":"code","62379f99":"code","dddcea9a":"code","68cf1586":"code","489f24cc":"code","fd710183":"code","0e8014fe":"code","59fdbe13":"code","33cb97b0":"code","37d6eb85":"code","b50351af":"code","361f38f8":"code","11284ae5":"code","a6802584":"code","808147cb":"code","76a2b422":"code","313535c7":"code","0019a6de":"code","6423051a":"code","c48070ed":"code","ca3acd49":"code","bac701db":"code","bb69476b":"code","e4490e4a":"code","2854fa50":"code","8eaf78d8":"code","89a56c69":"code","11e6dcd7":"code","a283f593":"code","921d7dde":"code","c70204a2":"code","bd973dc2":"code","f0483c8b":"code","a4341833":"code","2938fcd3":"code","3ffdbca1":"code","cee45698":"code","6360b2ae":"code","833231aa":"code","94746016":"code","ec425fee":"code","9792857f":"code","a6f32fd5":"code","a939a37f":"code","514f30e0":"code","77ad100c":"code","84da3218":"code","8cefe109":"markdown","b8c67943":"markdown","21607c8c":"markdown","46d2d062":"markdown","b80454f3":"markdown","99479e16":"markdown","e2f63dac":"markdown","19589bf9":"markdown","39879e6c":"markdown","145c815f":"markdown","3005b0ce":"markdown","fb875154":"markdown","c82c88e6":"markdown","7f348adc":"markdown","32a2dcd7":"markdown","d530cc79":"markdown","c919f388":"markdown","df0ffa31":"markdown","96ff75d6":"markdown","2b576eec":"markdown","27fb2629":"markdown","6d1fcafb":"markdown","abc0de45":"markdown","18f9ca85":"markdown","36491381":"markdown","d337ff3c":"markdown","797b6c24":"markdown","0ce8ee10":"markdown"},"source":{"21945a59":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly\nimport os","674f88af":"df = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","22785653":"df.info()","74800e69":"pd.set_option('display.max_columns', 130)\n#since its 'date' lets change its type to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\ndf.head()","4a7a51d5":"df.describe()","1f0f4412":"plt.subplots(figsize=(10,10))\ncorr_array = df.corr()\nsns.heatmap(corr_array)\nplt.show()","684c7739":"plt.subplots(figsize=(15,15))\n\nplt.scatter(x=df['long'], y=df['lat'], c=df['price'])\nplt.show()","665491c7":"cols = df.columns.tolist()\n#put the sub target next to the original target\ncols = cols[:3] + ['price_bin'] + cols[3:]\n\ndf['price_bin'] = (df['price'] > 1e6).astype(int)\n\n#set the columns in the right order\ndf = df[cols]\n","c90b39e1":"df['price_bin'].value_counts()","eda2e8b7":"\n\nplt.subplots(figsize=(15,15))\nplt.scatter(x=df['long'], y=df['lat'], c=df['price_bin'])\nplt.show()","936c254a":"N = df.count()[0]\nval_split_ratio = 0.1\ntest_train_split_ratio = 0.2\nval_size = int(N*val_split_ratio)\ntest_size = int((N-val_size)*test_train_split_ratio)\ntrain_size = N -val_size - test_size\ntrain_test_val_indexes = np.hstack([np.ones(test_size),np.zeros(train_size), -np.ones(val_size)])\nnp.random.shuffle(train_test_val_indexes)\ntrain_test_val_indexes = train_test_val_indexes.astype(int)\n\ntest_indexes = train_test_val_indexes == 1\ntrain_indexes = train_test_val_indexes == 0\nval_indexes = train_test_val_indexes == -1","8db7f585":"df['price_per_sqrft_living'] = df['price']\/df['sqft_living']\ndf['price_per_sqrft_lot'] = df['price']\/df['sqft_lot']\n# I have make sure that these variables are only used when infered from observations outside the data point!","2c361b8c":"df['age_sold'] = df['date'].dt.year - df['yr_built']\n\ndf['was_renovated'] = df['yr_renovated'] != 0\n#replace 0's in non renovated houses with the year of the built\ndf.loc[df['yr_renovated'] == 0, 'yr_renovated'] = df['yr_built']","9cd21de7":"from category_encoders.leave_one_out import LeaveOneOutEncoder","06aa7b78":"zipcode_encoder = LeaveOneOutEncoder()\n\nzipcode_encoder.fit(\n                X=df.iloc[test_indexes | train_indexes]['zipcode'].astype(str),\n                y=df.iloc[test_indexes | train_indexes]['price_per_sqrft_living']\n            )\ndf['zipcode_encoded_prices_per_sqrft_living'] = zipcode_encoder.transform(\n                                                    X=df['zipcode'].astype(str),\n                                                    y=df['price_per_sqrft_living']\n                                                )\n\nzipcode_encoder.fit(\n                X=df.iloc[test_indexes | train_indexes]['zipcode'].astype(str),\n                y=df.iloc[test_indexes | train_indexes]['price_per_sqrft_lot']\n            )\ndf['zipcode_encoded_prices_per_sqrft_living'] = zipcode_encoder.transform(\n                                                    X=df['zipcode'].astype(str),\n                                                    y=df['price_per_sqrft_lot']\n                                                )\n","04b38ca7":"from statsmodels.tsa.stattools import adfuller","ede5b5c8":"df.groupby('date').mean().sort_values('date')['price_per_sqrft_lot'].plot()\nplt.show()","557c4ea5":"df.groupby('date').mean().sort_values('date')['price_per_sqrft_living'].plot()\nplt.show()","8957ef0c":"_, p_val, _, _, _,_ = adfuller(df.groupby('date').mean().sort_values('date')['price_per_sqrft_lot'])\nprint(f'p-value: {p_val} for H0=Data is stationery')","a7b43f75":"_, p_val, _, _, _,_ = adfuller(df.groupby('date').mean().sort_values('date')['price_per_sqrft_living'])\nprint(f'p-value: {p_val} for H0=Data is stationery')","996c91cf":"print(f\"repeated coordinates: {df.iloc[:,0].count() - (df['long'].astype(str)+'\/'+df['lat'].astype(str)).nunique()}\")","62379f99":"print(\"Non unique coordinate pairs: {}\".format(\ndf.iloc[:,0].count() - (df['long'].astype(str)+'\/'+df['lat'].astype(str)).nunique()\n))","dddcea9a":"df['long'] += np.random.random(size = df.iloc[:,0].count())\/100\ndf['lat'] += np.random.random(size = df.iloc[:,0].count())\/100","68cf1586":"print(\"Non unique coordinate pairs after random noise: {}\".format(\ndf.iloc[:,0].count() - (df['long'].astype(str)+'\/'+df['lat'].astype(str)).nunique()\n))","489f24cc":"from sklearn.neighbors import KNeighborsTransformer","fd710183":"def measure_neighbours(input_df, n_neighbors, target, encoding_indexes, return_distances=False):\n    src_indexes = input_df.iloc[encoding_indexes].index\n    knn = KNeighborsTransformer(n_neighbors=n_neighbors)\n    \n    knn.fit(input_df.iloc[encoding_indexes][['lat', 'long']], input_df[target])\n\n    distances, indexes = knn.kneighbors(input_df[['lat', 'long']], return_distance=True)\n    means = []\n    stds = []\n    distance_means = []\n    distance_stds = []\n    \n    for distance, point_neighbors in zip(distances, indexes):\n        # distance != 0.0 not to consider the point itself for a huge target leak\n        means.append(input_df[target].take(src_indexes[point_neighbors[distance != 0.0]]).mean())\n        stds.append(input_df[target].take(src_indexes[point_neighbors[distance != 0.0]]).std())\n        \n        if return_distances is True:\n            distance_means.append(distance[distance != 0.0].mean())\n            distance_stds.append(distance[distance != 0.0].std())\n    \n    if return_distances is True:\n        return distance_means, distance_stds, means, stds\n    else:\n        return means, stds","0e8014fe":"n_neighbors = 15\ntarget = 'price'\n\n\nmeans, stds = measure_neighbours(df, n_neighbors, target, test_indexes | train_indexes)\ndf[f'{n_neighbors}_NN_{target}_mean'] = means\ndf[f'{n_neighbors}_NN_{target}_std'] = stds","59fdbe13":"sns.scatterplot(x=df[target], y=df[f'{n_neighbors}_NN_{target}_mean'])\ncoef = np.corrcoef(x=df[target], y=df[f'{n_neighbors}_NN_{target}_mean'])[0,1]\nprint(f'Pearson coefficience: {coef}')\nplt.show()","33cb97b0":"sns.scatterplot(x=df[target], y=df[f'{n_neighbors}_NN_{target}_std'])\ncoef = np.corrcoef(x=df[target], y=df[f'{n_neighbors}_NN_{target}_std'])[0,1]\nprint(f'Pearson coefficience: {coef}')\nplt.show()","37d6eb85":"# this can take up to a couple of minutes\nn_neighbors_list = [7, 15, 27, 39]\ntargets = ['price', 'price_bin', 'price_per_sqrft_living', 'price_per_sqrft_lot', 'sqft_living', 'sqft_above', 'sqft_lot', 'yr_built', 'condition', 'grade']\n\nfor n_neighbors in n_neighbors_list:\n    distances_taken = False\n    for target in targets:\n        if distances_taken is False:\n            dist_means, dist_stds, means, stds = measure_neighbours(df, n_neighbors, target, test_indexes | train_indexes, return_distances=True)\n            df[f'{n_neighbors}_NN_distance_mean'] = dist_means\n            df[f'{n_neighbors}_NN_distance_std'] = dist_stds\n            distances_taken = True\n        else:\n            means, stds = measure_neighbours(df, n_neighbors, target, test_indexes | train_indexes)\n        df[f'{n_neighbors}_NN_{target}_mean'] = means\n        df[f'{n_neighbors}_NN_{target}_std'] = stds\n ","b50351af":"df","361f38f8":"X_cols = df.columns[4:]\nX_cols = X_cols[np.where(X_cols != 'price_per_sqrft_living')]\nX_cols = X_cols[np.where(X_cols != 'price_per_sqrft_lot')]\nX_cols = X_cols[np.where(X_cols != 'zipcode')]\nX_cols = X_cols[np.where(X_cols != 'lat')]\nX_cols = X_cols[np.where(X_cols != 'long')]\n","11284ae5":"X_train = df.iloc[train_indexes][X_cols]\nX_test = df.iloc[test_indexes][X_cols]\nX_val = df.iloc[val_indexes][X_cols]","a6802584":"from sklearn.preprocessing import PolynomialFeatures","808147cb":"poly = PolynomialFeatures(2)\nn_features_poly = 36\nX_train_poly_base = np.hstack([poly.fit_transform(X_train.iloc[:,:n_features_poly]), X_train.iloc[:,n_features_poly:]])\nX_test_poly_base = np.hstack([poly.fit_transform(X_test.iloc[:,:n_features_poly]), X_test.iloc[:,n_features_poly:]])\nX_val_poly_base = np.hstack([poly.fit_transform(X_val.iloc[:,:n_features_poly]), X_val.iloc[:,n_features_poly:]])","76a2b422":"X_train_poly_base.shape","313535c7":"from sklearn.linear_model import Ridge, LinearRegression, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","0019a6de":"y_cols = 'price'\ny_test = df.iloc[test_indexes][y_cols]\ny_train = df.iloc[train_indexes][y_cols]\ny_val = df.iloc[val_indexes][y_cols]","6423051a":"estimators = {\n    'ridge' : Ridge(),\n    'lasso' : Lasso(),\n    'svm' : SVR(),\n    'rf' : RandomForestRegressor(),\n    'adab' : AdaBoostRegressor(),\n    'knn' : KNeighborsRegressor()\n}\n\n\ndummy_pipeline = Pipeline([('preproc', None),\n                           ('preproc_step2', None),\n                           ('dim_redu', None),\n                           ('reg', LinearRegression())]) ","c48070ed":"est_parameters = [\n    {\n        'preproc': (PowerTransformer(), None),\n        'preproc_step2': (StandardScaler(), MinMaxScaler(), None), \n        'dim_redu': (PCA(), PCA(20), None,),\n        'reg' : (estimators['ridge'],),\n        'reg__alpha' : 10.0**np.array([5,6,7,8,9])\n    },\n    {\n        'preproc': (PowerTransformer(), None),\n        'preproc_step2': (StandardScaler(), MinMaxScaler(), None), \n        'dim_redu': (PCA(), PCA(20), None,),\n        'reg' : (estimators['svm'],),\n        'reg__degree' : (1, 2, 3,),\n        'reg__C' : 10.0**np.array([-1, -2, -3,])\n    },\n    {\n        'preproc': (PowerTransformer(), None),\n        'dim_redu': (PCA(), PCA(20), None,),\n        'reg' : (estimators['rf'],),\n        'reg__n_estimators' : (50, 100, 200),\n        'reg__max_depth' : (5, 10, 25),        \n    },\n    {\n        'preproc': (PowerTransformer(), None),\n        'dim_redu': (PCA(), PCA(20), None,),\n        'reg' : (estimators['adab'],),\n        'reg__n_estimators' : (30, 100, 300),\n        'reg__learning_rate' : (0.01, 0.1, 1),\n    },\n    {\n        'preproc': (PowerTransformer(), None),\n        'preproc_step2': (StandardScaler(), MinMaxScaler(), None), \n        'dim_redu': (PCA(), PCA(20), None,),\n        'reg' : (estimators['knn'],),\n        'reg__n_neighbors' : (3, 10, 15)        \n    },\n]","ca3acd49":"#Do NOT rerun this cell unless you want to spend a couple of hours frying eggs on your processor\n\n# grid = GridSearchCV(estimator=dummy_pipeline, param_grid=est_parameters, scoring='neg_root_mean_squared_error')\n# grid.fit(X=X_train_poly_base, y=y_train)\n# print(grid.best_params_)\n# print(\"Root Mean Squared Error: {:.3f}\".format(-grid.best_score_))","bac701db":"import xgboost as xgb","bb69476b":"# X_transformed = PowerTransformer().fit_transform(X=X_train_poly_base,y=y_train)\n# X_transformed = X_train_poly_base\n\n# the non-transformed features proved to be the best here\nX_transformed = X_train\n\ndata_dmatrix = xgb.DMatrix(data=X_transformed,label=y_train)","e4490e4a":"# best params after GridSearch \n\nparams = {\n          'objective':'reg:squarederror',\n          'max_depth': 5, \n          'subsample': 0.7,\n          'colsample_bytree': 0.7,\n          'learning_rate': 0.1,\n          'gamma': 10\n          }\n\ncv_results = xgb.cv(dtrain=data_dmatrix,\n                    params=params,\n                    nfold=3,\n                    num_boost_round=450,\n                    early_stopping_rounds=50,\n                    metrics=\"rmse\",\n                    as_pandas=True)","2854fa50":"cv_results","8eaf78d8":"xgboost_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n                               gamma=10,\n                               learning_rate = 0.1,\n                               colsample_bytree = 0.7,\n                               subsample=0.7,\n                               max_depth = 5,\n                               )","89a56c69":"xgboost_reg.fit(X=X_train, y=y_train)","11e6dcd7":"print(f\"R^2: {xgboost_reg.score(X=X_test, y=y_test)}\")\ny_pred = xgboost_reg.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))}\")\nprint(f\"MAE: {mean_absolute_error(y_true=y_test, y_pred=y_pred)}\")","a283f593":"feature_importances = pd.DataFrame(data=xgboost_reg.feature_importances_, index=X_cols).reset_index().rename({\"index\":\"feature\",0:\"importance\"}, axis=1)","921d7dde":"fig, ax = plt.subplots(figsize=(10,20))\nsns.barplot(data=feature_importances.sort_values(by='importance', ascending=False).iloc[:40,:], y='feature', x='importance', ax=ax)","c70204a2":"print(f\"R^2: {xgboost_reg.score(X=X_val, y=y_val)}\")\ny_pred = xgboost_reg.predict(X_val)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_true=y_val, y_pred=y_pred))}\")\nprint(f\"MAE: {mean_absolute_error(y_true=y_val, y_pred=y_pred)}\")","bd973dc2":"linear_model = Ridge(alpha=10e7)","f0483c8b":"linear_model.fit(X=X_train_poly_base, y=y_train)","a4341833":"print(f\"R^2: {linear_model.score(X=X_test_poly_base, y=y_test)}\")\ny_pred = linear_model.predict(X_test_poly_base)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))}\")\nprint(f\"MAE: {mean_absolute_error(y_true=y_test, y_pred=y_pred)}\")","2938fcd3":"## task 1 solution: TOP regressor (or a draw with XGBoost, but faster to train and make predictions)\n\nprint(f\"R^2: {linear_model.score(X=X_val_poly_base, y=y_val)}\")\ny_pred = linear_model.predict(X_val_poly_base)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_true=y_val, y_pred=y_pred))}\")\nprint(f\"MAE: {mean_absolute_error(y_true=y_val, y_pred=y_pred)}\")","3ffdbca1":"n_features_poly_noft = 15\nX_train.iloc[:,:n_features_poly_noft]","cee45698":"X_train_poly_base_noft = poly.fit_transform(X_train.iloc[:,:n_features_poly_noft])\nX_test_poly_base_noft = poly.fit_transform(X_test.iloc[:,:n_features_poly_noft])\n\nlinear_model = Ridge(alpha=10e2)\nlinear_model.fit(X=X_train_poly_base_noft, y=y_train)\nlinear_model.score(X=X_test_poly_base_noft, y=y_test)","6360b2ae":"y_cols = 'price_bin'\ny_test = df.iloc[test_indexes][y_cols]\ny_train = df.iloc[train_indexes][y_cols]\ny_val = df.iloc[val_indexes][y_cols]","833231aa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score\n","94746016":"#this can take up to a few minutes \nlinear_model = LogisticRegression(C=10e-7, max_iter=1e6)\n\nlinear_model.fit(X=X_train_poly_base, y=y_train)","ec425fee":"y_pred = linear_model.predict(X_test_poly_base)\nprint(classification_report(y_true=y_test, y_pred=y_pred))","9792857f":"y_pred = linear_model.predict(X_val_poly_base)\nprint(classification_report(y_true=y_val, y_pred=y_pred))","a6f32fd5":"y_scores = linear_model.predict_proba(X_test_poly_base)\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_scores[:,1])\nauc = roc_auc_score(y_true=y_test, y_score=y_scores[:,1])\nsns.lineplot(fpr, tpr)\nplt.title(f\"ROC curve, AUC score = {auc}\")","a939a37f":"xgboost_bin = xgb.XGBRegressor(objective ='binary:logistic', \n                               gamma=4,\n                               learning_rate = 0.1,\n                               colsample_bytree = 0.7,\n                               subsample=0.7,\n                               max_depth = 5,\n                               )\nxgboost_bin.fit(X=X_train, y=y_train)","514f30e0":"y_scores = xgboost_bin.predict(X_test)\ny_pred = y_scores > 0.5\nprint(classification_report(y_true=y_test, y_pred=y_pred))","77ad100c":"y_scores_val = xgboost_bin.predict(X_val)\ny_pred = y_scores_val > 0.5\nprint(classification_report(y_true=y_val, y_pred=y_pred))","84da3218":"## task 2 solution: TOP binary classificator\n\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_scores)\nauc = roc_auc_score(y_true=y_test, y_score=y_scores)\nsns.lineplot(fpr, tpr)\nplt.title(f\"ROC curve, AUC score = {auc}\")","8cefe109":"It seems like the new features were more than worth it!","b8c67943":"We are also creating a subtask, which will be a prediction whether the price is going to exceed 1 million","21607c8c":"# Modeling price prediction\n## Regression","46d2d062":"First, it seams like we have some repetition in the coordinates. This is going to be a problem for the KNN algrithm since I want to exclude only the point from which I am measuring the neighbors.\n\nI can fix that by adding some small noise to the coordinates. Since the data has the precision to the third place, I can just append the noise after that.","b80454f3":"## Benchmarking\nFinally, let's check if our features made the positive change","99479e16":"## Polynomial features\n\nWe can also try X with polynomial features, but to keep the amount of features from escaping into tens of thousands, I will only transform some of the features.\n\nUsing them I can introduce a non-linearity to our models - whether they are simple linear regression or even tree-based models.","e2f63dac":"That is way to close to choose a clear winner - but perhaps after some more fine-tuning and more features XGBoost would take over in a more visible way.\n\nInteresing observation we can make here is: MAE is significanly smaller than RMSE - which can imply that the prediction were heavily influenced by **outliers**. Perhaps guessing the price of really expensive houses can be very unpredictable. If that is the case though - we can probably expect that our classification model are going to work way better because they will only have to guess that the house is expensive.\n\nAnother thing is the score on the validation set - those were houses non-exisant to our model even when creating features, so because its score is very close to the score of test set - we can safely say that the Target Leak has been avoided succesfully!\n\nThere still remain many things to try out like a ton of new features or different transformations, but that will have to be left for a real projects when I will have more time and a team to do so :) \n\nTo boost the score itself we could try things like stacking or using the Deep Learning techniques but that is not what I wanted to show in this work since for the amount of data and level of functionality for comercial projects - those techniques would not fit this project very well. They are simply too quarky, unnecesarily difficult in deployment and maintanance and can be simply to slow, expensive and resource-heavy.","19589bf9":"As we can see - those are actually some of the **highest** coefficiences so far!\n\nWe can now automate the process to create many features like that.","39879e6c":"Most of them can be used straight up, yet some need some more work to serve us a a solid source of information. \n\nFirst, lets visualize the data geographically.","145c815f":"### Date-sold trend normalization","3005b0ce":"The Augmented Dicky-Fuller test fails to reject the H0, hence I do not have to worry about the dates and I can just skip that column.","fb875154":"# Data Visualization and analysis\nFirst lets see the information I can exract directly from the data","c82c88e6":"So let's try some estimators","7f348adc":"Iteresingly, it does not seem to have any visible trend.","32a2dcd7":"### Nearest neighbours (literally)\nNext, I **could** (and I probably would if i had more time :) ) use some outside sources to locate the malls, beaches, parking lots, subway stations, restaurants, shools, kindergartens etc. but for the sake of this quick work I will have to use the fact that those factors will, or at least should be **correlated** with the **house price of the N-nearest neighbours** from each point within our data range. This simplification not explain all the variance that I could achieve by collecting all of them, but it should explain a significant part of it as you will see in a second. Based on that neighbourhood I will create some features to use alongside the ones in the dataset and with all that - hopefully make some solid predictions.\n","d530cc79":"## Classification\n\nThe cool thing about this task and dataset is - the classification task is nothing else but a regression task activated by a 0\/1 threshold. And for that reason - I do not necessarily have to go trough the process of finding the right model again - I can just reuse the model that we have and only change its target column and loss function to a ex. Sigmoid(output), and the optimal parameters for regression should be very near to optimal parameters for classification!\n\nWithout getting into the hiperparametrization again - the results confirmed that the parameters were best when left the same.","c919f388":"As expected, optimal parameters seem to be exactly the same","df0ffa31":"### Target encoder\nI can encode the zipcode with the average price in the zipcode to make it yet another usefull feature. Then I will use the LeaveOneOut option to not consider a datapoint's target value and prevent a TargetLeak","96ff75d6":"Unfortunately, due to a hardware issues i couldn't draw it here on a geographicall map, but I did that outside of this notebook and the houses seem to be all located in one City of Seattle. \n\nSince their locaction is consistent across all data - I can treat them as near neighbours and thus I can use **interpolation** techniques to get an information about each of the houses neighbourhood. \n\nThe location definitely seems to be an important factor in the pricing, since the houses with simmilar prices  seem to cluster in the near proximity (also that is what I expect from the real-world housing data since the price depends from the neighbourhood and the disance to city centers\/beaches\/malls etc.).\n\n## WARNING Target Leak!\nWhenever I am using a target column to create features I should get a big red flag for a Target Leak. Yet if I remain carefull and make sure that I only include informations outside the data point's target value - I will be fine.\n\nFor further more carefull evaluation, I will separate out a validation set with a small portion of data (10%) that will not be used for encoding the variables (so I will basically treat it as a non existant houses). The test set can be now interpretable as a set that the model was not trained on but when making the prediction for the house from test set - the other houses from the test set are treated as existing source of data. From now on - I will be creating aggragation-based features only on the df.iloc[test_indexes | train_indexes] as seen further in the notebook. The difference between scores on test and validation set will show if I have succesfully avoided the target leak!\n","2b576eec":"Let's examine feature importances","27fb2629":"Out of the standard approaches the winner (by quite a large margin) was a regularized Linear model with degree 2 polynomial features and the value of alpha = 10e7. Surprisingly though, any proprocessing would make it worse (even if by a very small margin) and trying the dimentionality reduction has worsen it significantly.\n\nBefore we settle on this, since AdaBoost did only slighlty worse - lets try out some more sophisticated state-of-the-art model called the XGBoost (although if the linear model came out on top - that may suggest the general linear nature of the data and even algorithm so robust as XGBoost may struggle to beat the linear model)","6d1fcafb":"# Feature engeneering\n\n### Simple column manipulation\nLets first fix the date columns to the more digastable form and add some usefull ones as well.\n\n","abc0de45":"It was close, but now I will take the both winner of the GridSearch and the best XGBoost model and test them both on our test set to get the final winner.","18f9ca85":"Success! The imporvement I have made from that R^2 is more than significant!","36491381":"This above is really just a sample, i did not really run it all at once, the parameters were chosen sort of by looking in which direction i should tweak them and then I would rerun the grid search, tried out many things in the process, switched between default and poly-transformed features etc.\n\nP.S. I would normally try to implement a Baessian Search instead of standard Grid Search but i couldn't get any off-the-shelf solution to work as I would like it to, so because that was basically a weekend project anyway I just had to settle on doing this workaround described above","d337ff3c":"## Train test split\nNow that we have our features, I can select the feature columns and separate the training data from the test data","797b6c24":"Due to the fact like inflation and general trend for housing prices - they may differ based on the time when they were sold.","0ce8ee10":"In summary: The best model for the regression turned out to be an LinearRegression with polynomial features with an XGBoost very close to it, and for binary classification the XGBoost took over significantly with an amazing score.\n\nWhat was left to do: more features, more features and once again more features, then maybe some more tuning of the models. I could also try some feature selection based on the feature importances and perhaps choose the best model based on the AIC or BIC criterion."}}