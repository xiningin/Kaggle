{"cell_type":{"db51c365":"code","03a8eaa6":"code","13c43c13":"code","18dd1f96":"code","451735ae":"code","5dda228e":"code","7efed843":"code","ba884a17":"code","57a4e12a":"code","8b45e6ba":"code","5fd443d4":"code","b2235d38":"code","cb61d012":"code","00cbf864":"code","1c8fbccb":"code","3a5ee500":"code","63c73566":"code","5d99e21d":"code","b5b1b813":"code","5f427aed":"code","805d1f35":"code","f3f5b0ce":"code","4eaab3cf":"code","d58056a0":"code","c7b3be0a":"markdown","e8c62633":"markdown","1f23667d":"markdown","cacb4587":"markdown","2ef5acb6":"markdown","8f7ffdb6":"markdown","7bde0ccd":"markdown","3ad71cdb":"markdown","bfe2da61":"markdown","662272df":"markdown","9df3f152":"markdown","7bcf5521":"markdown","1becaf53":"markdown","bf08cb0f":"markdown","62fddb61":"markdown","17ea1e65":"markdown","39ef50a3":"markdown","7b2a4670":"markdown","2ff731c4":"markdown","3b2a5552":"markdown"},"source":{"db51c365":"!pip install datefinder\n!pip install geotext\n!pip install langdetect","03a8eaa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datefinder\nimport pycountry\nfrom geotext import GeoText\nimport covid19_tools as cv19\nimport re\nfrom collections import Counter\nimport geopandas as gpd\nimport json\nimport matplotlib.pyplot as plt\nfrom langdetect import detect\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","13c43c13":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","18dd1f96":"print('Loading full text')\nfull_text_repr = cv19.load_full_text(meta_df,\n                                     '..\/input\/CORD-19-research-challenge')","451735ae":"def get_body_text(full_text_repr):\n    body_text = []\n    for article in full_text_repr:\n        text = [body_text['text'] for body_text in article['body_text']]\n        body_text.append(''.join(text))\n    return body_text\n\nbody_text_repr = get_body_text(full_text_repr)\n\nfull_text_ids = [article['paper_id'] for article in full_text_repr]\nmeta_df['full_text'] = None\nmeta_df['full_text'] = meta_df['sha'].apply(lambda x: full_text_ids.index(x) if x in full_text_ids else -1)\nmeta_df['full_text'] = meta_df['full_text'].apply(lambda x: body_text_repr[x] if x != -1 else None)","5dda228e":"print(meta_df.shape)\nmeta_df.head()","7efed843":"def find_country(text):\n    '''\n    Extracts countries using pycountry.\n    '''\n    entities = []\n    \n    for country in pycountry.countries:\n        if country.name in text and country not in entities:\n            entities.append(country.name)\n    return entities\n\n#Adds a new column in the df containing the countries found in the titles.\nmeta_df[\"country_title\"] = np.nan\nmeta_df['country_title'] = meta_df[meta_df['title'].notna()]['title'].apply(lambda x: find_country(x))\n\n#Adds a new column in the df containing the countries found in the abstracts.\nmeta_df[\"country_abstract\"] = np.nan\nmeta_df['country_abstract'] = meta_df[meta_df['abstract'].notna()]['abstract'].apply(lambda x: find_country(x))\n\n#Adds a new column in the df containing the countries found in the full text.\nmeta_df[\"country_text\"] = np.nan\nmeta_df['country_text'] = meta_df[meta_df['full_text'].notna()]['full_text'].apply(lambda x: find_country(x))\nmeta_df.head()","ba884a17":"def list_entities_no_duplicates(df, column):\n    '''\n    Extracts a list of unique entities (i.e. countries or cities).\n    '''\n    entities_list = df[df[column].notna()][column].values.tolist()\n    entities_list_flat = [item for sublist in entities_list for item in sublist]\n    entities_list_flat_no_dupli = list(set(entities_list_flat))\n    return entities_list_flat_no_dupli\n\n#Convert title countries into a list to validate them\ncountries_title_list = list_entities_no_duplicates(meta_df, 'country_title')\nprint(countries_title_list)\n\n#Convert abstract countries into a list to validate them\ncountries_abstract_list = list_entities_no_duplicates(meta_df, 'country_abstract')\nprint(countries_abstract_list)\n\n#Convert text countries into a list to validate them\ncountries_text_list = list_entities_no_duplicates(meta_df, 'country_text')\nprint(countries_text_list)","57a4e12a":"def find_city(text):\n    '''\n    Extracts cities using geotext.\n    '''\n    places = GeoText(text)\n    \n    return places.cities\n\n#Adds a new column in the df containing the country found in the titles.\nmeta_df[\"city_title\"] = np.nan\nmeta_df['city_title'] = meta_df[meta_df['title'].notna()]['title'].apply(lambda x: find_city(x))\n\n#Adds a new column in the df containing the country found in the titles.\nmeta_df[\"city_abstract\"] = np.nan\nmeta_df['city_abstract'] = meta_df[meta_df['abstract'].notna()]['abstract'].apply(lambda x: find_city(x))\n\n#Adds a new column in the df containing the country found in the titles.\nmeta_df[\"city_text\"] = np.nan\nmeta_df['city_text'] = meta_df[meta_df['full_text'].notna()]['full_text'].apply(lambda x: find_city(x))\nmeta_df.head()","8b45e6ba":"#Convert title cities into a list to validate them\ncities_title_list = list_entities_no_duplicates(meta_df, 'city_title')\nprint(cities_title_list)\n\n#Convert title locations into a list to validate them\ncities_abstract_list = list_entities_no_duplicates(meta_df, 'city_abstract')\nprint(cities_abstract_list)\n\n#Convert text locations into a list to validate them\ncities_abstract_list = list_entities_no_duplicates(meta_df, 'city_text')\nprint(cities_abstract_list)","5fd443d4":"def find_location(df, id_to_search):\n    '''\n    Given a df and an id it returns the countries and cities found in the title and abstract of the article the id belongs to.\n    '''\n    country_title = df[df['cord_uid'] == id_to_search]['country_title']\n    country_abstract = df[df['cord_uid'] == id_to_search]['country_abstract']\n    city_title = df[df['cord_uid'] == id_to_search]['city_title']\n    city_abstract = df[df['cord_uid'] == id_to_search]['city_abstract']\n        \n    return country_title, country_abstract, city_title, city_abstract\n\n# example\nxqhn0vbp = find_location(meta_df, 'xqhn0vbp')\nprint(xqhn0vbp)","b2235d38":"meta_df.to_csv('\/kaggle\/working\/df_locations.csv', sep = ';', index = False)","cb61d012":"meta_df.columns","00cbf864":"meta_df.shape","1c8fbccb":"# Load the countries shapefile\nshapefile = '\/kaggle\/input\/worldmapshapes\/ne_110m_admin_0_countries.shp'\n\n#Read shapefile using Geopandas\ngdf = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\n\n#Rename columns\ngdf.columns = ['country', 'country_code', 'geometry']\ngdf[\"country\"] = gdf[\"country\"].replace('United States of America', 'United States') \ngdf.head()","3a5ee500":"print(gdf[gdf['country'] == 'Antarctica'])\n#Drop row corresponding to 'Antarctica'\ngdf = gdf.drop(gdf.index[159])","63c73566":"def count_entities(df, column):\n    '''\n    Counts entities (i.e., countries or cities) and returns a dataframe with entities and counts.\n    '''\n    entities_list = df[df[column].notna()][column].values.tolist()\n    entities_list_flat = [item for sublist in entities_list for item in sublist]\n    count = Counter(entities_list_flat)\n    \n    df_counts = pd.DataFrame(count.items(), columns=[column, 'count'])\n    \n    return df_counts\n\ncount_countries_title = count_entities(meta_df, 'country_title')\nprint(count_countries_title)\n\ncount_countries_abstract = count_entities(meta_df, 'country_abstract')\nprint(count_countries_abstract)\n\ncount_countries_text = count_entities(meta_df, 'country_text')\nprint(count_countries_text)","5d99e21d":"#Perform left merge to preserve every row in gdf with countries extracted from titles\nmerged_country_title = gdf.merge(count_countries_title, left_on = 'country', right_on = 'country_title', how = 'left')\nmerged_country_title.to_csv('\/kaggle\/working\/merged_country_title.csv', sep = ';', index = False)\nprint(merged_country_title.head())\n\n#Perform left merge to preserve every row in gdf with countries extracted from abstracts\nmerged_country_abstract = gdf.merge(count_countries_abstract, left_on = 'country', right_on = 'country_abstract', how = 'left')\nmerged_country_abstract.to_csv('\/kaggle\/working\/merged_country_abstract.csv', sep = ';', index = False)\nprint(merged_country_abstract.head())\n\n#Perform left merge to preserve every row in gdf with countries extracted from full text\nmerged_country_text = gdf.merge(count_countries_text, left_on = 'country', right_on = 'country_text', how = 'left')\nmerged_country_text.to_csv('\/kaggle\/working\/merged_country_text.csv', sep = ';', index = False)\nprint(merged_country_text.head())","b5b1b813":"# set a variable that will call whatever column we want to visualise on the map\nvariable = 'count'\n\n# set the range for the choropleth\nvmin, vmax = 0, 1400\n\n# create figure and axes for Matplotlib\nfig, ax = plt.subplots(1, figsize=(100, 60))\n\n# create map\nmerged_country_title.plot(column=variable, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n\n# remove the axis\nax.axis('off')\n\n# add a title\nax.set_title('Choropleth map of articles published by country, countries extracted from titles', fontdict={'fontsize': '100', 'fontweight' : '3'})\n\n# Create colorbar as a legend\n\nnorm = plt.Normalize(vmin=vmin, vmax=vmax)\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=norm)\n\n# empty array for the data range\nsm._A = []\n\n# add the colorbar to the figure\ncbar = fig.colorbar(sm, orientation=\"horizontal\", shrink=0.3);\ncbar.ax.tick_params(labelsize=50)","5f427aed":"# set a variable that will call whatever column we want to visualise on the map\nvariable = 'count'\n\n# set the range for the choropleth\nvmin, vmax = 0, 3000\n\n# create figure and axes for Matplotlib\nfig, ax = plt.subplots(1, figsize=(100, 60))\n\n# create map\nmerged_country_abstract.plot(column=variable, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n\n# remove the axis\nax.axis('off')\n\n# add a title\nax.set_title('Choropleth map of articles published by country, countries extracted from abstracts', fontdict={'fontsize': '100', 'fontweight' : '3'})\n\n# Create colorbar as a legend\n\nnorm = plt.Normalize(vmin=vmin, vmax=vmax)\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=norm)\n\n# empty array for the data range\nsm._A = []\n\n# add the colorbar to the figure\ncbar = fig.colorbar(sm, orientation=\"horizontal\", shrink=0.3);\ncbar.ax.tick_params(labelsize=50)","805d1f35":"# set a variable that will call whatever column we want to visualise on the map\nvariable = 'count'\n\n# set the range for the choropleth\nvmin, vmax = 0, 10000\n\n# create figure and axes for Matplotlib\nfig, ax = plt.subplots(1, figsize=(100, 60))\n\n# create map\nmerged_country_text.plot(column=variable, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n\n# remove the axis\nax.axis('off')\n\n# add a title\nax.set_title('Choropleth map of articles published by country, countries extracted from full text', fontdict={'fontsize': '100', 'fontweight' : '3'})\n\n# Create colorbar as a legend\n\nnorm = plt.Normalize(vmin=vmin, vmax=vmax)\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=norm)\n\n# empty array for the data range\nsm._A = []\n\n# add the colorbar to the figure\ncbar = fig.colorbar(sm, orientation=\"horizontal\", shrink=0.3);\ncbar.ax.tick_params(labelsize=50)","f3f5b0ce":"def extract_date(text):\n    new_text = re.sub(r'\\(.*?\\)', \"\", text) # Exclude text in parenthesis\n    search = re.findall(r'\\D(\\d{4})\\D', new_text)\n  \n    years = [search for year in search if year <= '2020']\n    \n    return years\n\n#Adds a new column in the df containing the years found in the titles.\nmeta_df[\"year_title\"] = np.nan\nmeta_df['year_title'] = meta_df[meta_df['title'].notna()]['title'].apply(lambda x: extract_date(x))\nmeta_df.head()","4eaab3cf":"#Adds a new column in the df containing the language the title is written\nmeta_df[\"lang\"] = np.nan\nmeta_df['lang'] = meta_df[meta_df['title'].notna()]['title'].apply(lambda x: detect(x))\nmeta_df.head()","d58056a0":"meta_df.to_csv('\/kaggle\/working\/df_updated.csv', sep = ';', index = False)","c7b3be0a":"They seem correct. We will now focus on the cities:","e8c62633":"# Add new variables\nThe main objective of this notebook is to create new variables for the CORD-19 dataset. By doing so, it may help the researchers apply more filters to extract the specific articles they would like to work on. \nThe created variables are the following:\n1. Locations (both countries and cities)\n2. Dates\n3. Language\n\nWe will start by installing the necessary packages.","1f23667d":"And importing them.","cacb4587":"We then prepare the dataframes with the count of articles per country. Those will be based on results on the titles, the absracts and the full text.","2ef5acb6":"# 1. Extract locations\nNow that we have our dataframe ready we will continue by extracting locations from each title and abstract. \n* We first tried spaCy's pretrained NER model. Unfortunatelly, there were many diseases and medical terms recognised as locations. \n* We then used geograpy library that extracts places and categorises them into countries, regions, cities and other. The result was better but still, we detected a lot of noise, especially in countries and regions.\n* We finally decided to use pycountry for the countries. The result was satisfactory.\n* To extract the cities, we used the Cities class of GeoText. We still detected some noise but the result is better than the previous options.\n\nWe will first focus on the countries:","8f7ffdb6":"We will now merge the dataframes with the shapefile:","7bde0ccd":"# 2. Extract the dates","3ad71cdb":"We will now extract all the years from the full text, excluding those referring to a citation:","bfe2da61":"We can drop the row for \u2018Antarctica\u2019 as it unnecessarily occupies a large space in our map and is not required in our current analysis:","662272df":"The merged file is a GeoDataframe object that can be rendered using geopandas module. We are now ready to render our choropleth maps, based on data extracted from the titles, abstract and full text of the corpus:","9df3f152":"Most of the elements extracted from the corpus are cities but there is also some noise detected. We suggest the use of the cities extracted by the titles werewe detect less noise.","7bcf5521":"We will then add the full text to the articles:","1becaf53":"And we will now validate them:","bf08cb0f":"We create a function to automatically extract the locations given an id of an article:","62fddb61":"# 3. Detect abstract's language","17ea1e65":"## 1.1 Visualisation per countries\nWe load a shapefile with all countries to prepare a map of articles per country:","39ef50a3":"We will then validate the results:","7b2a4670":"And we finally save the df in our working folder for future use:","2ff731c4":"We will now save the updated dataframe including all added variables:","3b2a5552":"We will finally extract the abstracts language. This will help to determine whether we have non-english articles in our corpus. Those could be translated in the future."}}