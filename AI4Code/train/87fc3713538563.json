{"cell_type":{"dec62979":"code","ee796c05":"code","46dc5e9d":"code","aa95677f":"code","4290b981":"code","fe484a47":"code","d6184c01":"code","742eb7de":"code","0d62e4fe":"code","c0d923d0":"code","5afade79":"code","fe477d06":"code","99235fa8":"code","07fd3503":"code","381ab4b8":"code","59688da9":"code","29d37686":"code","245a2c02":"code","b8cd7f96":"code","060c584d":"code","430d8d30":"code","9f5f32c3":"code","af9dcdde":"code","5a840a3a":"code","13cfef55":"code","802dc4c3":"code","77859225":"code","c0611eb5":"code","df0fa128":"code","005c2cba":"code","66bc10f4":"code","f8faf42c":"code","10555be0":"code","dbd64790":"code","a2c3ca20":"code","ffd43a6d":"code","bccf975d":"code","97d81796":"code","99d670d8":"code","499c132f":"markdown","c67ae575":"markdown","63020c5b":"markdown","8da6deef":"markdown","cd844013":"markdown","18b02db8":"markdown","66c85d65":"markdown","41699e33":"markdown","d2d6f653":"markdown","b8e7beca":"markdown","59c2f9e0":"markdown","c8314ee5":"markdown","b068ecca":"markdown","02a18b6d":"markdown","285fdbed":"markdown"},"source":{"dec62979":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # basic visualizations \nimport seaborn as sns # advanced visualizations\n\nimport random\nrandom.seed(42) #We are setting the seed to assure you get the same answers\n\nimport warnings\nwarnings.filterwarnings('ignore')","ee796c05":"#read the data and display the top 5 rows\ndf = pd.read_csv(\"..\/input\/ecommerce-ab-testing\/ab_test.csv\")\ndf.head()","46dc5e9d":"# change column names \ndf.columns = [\"user_id\", \"timestamp\", \"group\", \"landing_page\", \"converted\"]\ndf.head()","aa95677f":"#numer of rows and unique users\nprint(f'Number of rows: {df.shape[0]}')\nprint(f'Number of unique users: {df.user_id.nunique()}')","4290b981":"#general info\ndf.info()","fe484a47":"#missing values\ndf.isna().sum()","d6184c01":"#Does the number of new_page and treatment match?\nn_treat = df[df[\"group\"] == \"treatment\"].shape[0]\nn_new_page = df[df[\"landing_page\"] == \"new_page\"].shape[0]\ndifference = n_treat - n_new_page\n\npd.DataFrame({\n    'N treatment': [n_treat],\n    'N new_page': [n_new_page],\n    'Difference': [difference]\n})","742eb7de":"# lets see those rows \ndf[(df[\"group\"] == \"treatment\") & (df[\"landing_page\"] == \"old_page\")]","0d62e4fe":"df_mismatch = df[(df[\"group\"] == \"treatment\") & (df[\"landing_page\"] == \"old_page\")\n               |(df[\"group\"] == \"control\") & (df[\"landing_page\"] == \"new_page\")]\n\nn_mismatch = df_mismatch.shape[0]\n\npercent_mismatch = round(n_mismatch \/ len(df) * 100, 2)\nprint(f'Number of mismatched rows: {n_mismatch} rows')\nprint(f'Percent of mismatched rows: {percent_mismatch} percent')","c0d923d0":"df2 = df[(df[\"group\"] == \"treatment\") & (df[\"landing_page\"] == \"new_page\")\n        |(df[\"group\"] == \"control\") & (df[\"landing_page\"] == \"old_page\")]\n\nlen(df2)","5afade79":"df2.head()","fe477d06":"# Double Check all of the correct rows were removed - this should be 0\ndf2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]","99235fa8":"# Another way to double Check all of the correct rows were removed \ndf_mismatch = df2[(df2[\"group\"] == \"treatment\") & (df2[\"landing_page\"] == \"old_page\")\n               |(df2[\"group\"] == \"control\") & (df2[\"landing_page\"] == \"new_page\")]\n\nn_mismatch = df_mismatch.shape[0]\n\npercent_mismatch = round(n_mismatch \/ len(df2) * 100, 2)\nprint(f'Number of mismatched rows: {n_mismatch} rows')\nprint(f'Percent of mismatched rows: {percent_mismatch} percent')","07fd3503":"# unique user id in df2 \ndf2.user_id.nunique()","381ab4b8":"# number of repeated ids in df2\nlen(df2) - df2.user_id.nunique()","59688da9":"# Display the duplicated row \ndf2[df2.duplicated(\"user_id\") == True]","29d37686":"#drop the duplicated row\ndf2 = df2.drop_duplicates(\"user_id\") ","245a2c02":"# Douple Check that it is actually dropped\nlen(df2) - df2.user_id.nunique()","b8cd7f96":"# Percent of convergance\n# The probability of an individual converting regardless of the page they receive\ndf2.converted.mean() * 100","060c584d":"#Given that an individual was in the control group, what is the probability they converted?\n#Given that an individual was in the treatment group, what is the probability they converted?\ndf2.user_id = df2.user_id.astype(str)\ndf2.groupby(\"group\").mean() * 100","430d8d30":"#What is the probability that an individual received the new page?\npd.DataFrame(df2.landing_page.value_counts(normalize = True) * 100)","9f5f32c3":"# Creating the sampling distribution of difference in means \nmeans_diff = []\nsize = df.shape[0]\nfor _ in range(10000):\n    sample = df2.sample(size, replace = True)\n    control_mean = sample[sample[\"group\"] == \"control\"][\"converted\"].mean()\n    treat_mean = sample[sample[\"group\"] == \"treatment\"][\"converted\"].mean()\n    means_diff.append(treat_mean - control_mean)","af9dcdde":"# Plotting the sampling distribution \nplt.figure(figsize = (8,4), dpi = 100)\nplt.hist(means_diff, bins = 25)\nplt.show()","5a840a3a":"# Simulate distribution under the null hypothesis\nmeans_diff = np.array(means_diff)\nnull_vals = np.random.normal(0, means_diff.std(), means_diff.size)","13cfef55":"# Plot the null distribution\nplt.figure(figsize = (8,4), dpi = 100)\nplt.hist(null_vals, bins = 25)\nplt.show()","802dc4c3":"# Plot observed statistic with the null distibution\ncontrol_mean = df2[df2[\"group\"] == \"control\"][\"converted\"].mean()\ntreat_mean = df2[df2[\"group\"] == \"treatment\"][\"converted\"].mean()\nobs_diff = treat_mean - control_mean\n\nplt.figure(figsize = (8,4), dpi = 100)\nplt.hist(null_vals, bins = 25)\nplt.axvline(obs_diff, c='red')\nplt.show()","77859225":"# calculating the p value \n(null_vals > obs_diff).mean()","c0611eb5":"import statsmodels.api as sm\n\nconvert_old = df2[(df2[\"converted\"] == 1) & (df2[\"landing_page\"] == \"old_page\")]['user_id'].nunique()\nconvert_new = df2[(df2[\"converted\"] == 1) & (df2[\"landing_page\"] == \"new_page\")]['user_id'].nunique()\nn_old = df2[df[\"landing_page\"] == \"old_page\"]['user_id'].nunique()\nn_new = df2[df[\"landing_page\"] == \"new_page\"]['user_id'].nunique()","df0fa128":"#Compute test statistic and p-value\nz_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger')","005c2cba":"# Print Z Score and P_Value\nz_score, p_value ","66bc10f4":"# Creat the intercept \ndf2[\"intercept\"] = 1\ndf2.head()","f8faf42c":"# Create ab_page column, which is 1 when an individual receives the treatment and 0 if control.\ndf2[\"ab_page\"] = df2.group.apply(lambda x: 1 if (x == \"treatment\") else 0)\ndf2.head()","10555be0":"# Instantiate and fit the regression model\nmodel = sm.Logit(df2['converted'], df2[['intercept','ab_page']])\nresult = model.fit()\nresult.summary()","dbd64790":"# Read the country data\ncountries = pd.read_csv(\"..\/input\/ecommerce-ab-testing\/countries_ab.csv\")\ncountries.head()","a2c3ca20":"# Merge the countries dataframe with df2 \ncountries.columns = [\"user_id\", \"country\"]\ncountries[\"user_id\"] = countries[\"user_id\"].astype(str)\ndf3 = df2.merge(countries, on = \"user_id\", how = \"left\")\ndf3.head()","ffd43a6d":"# creating dummies for country and landing_page columns \ndf3[['CA','UK','US']] = pd.get_dummies(df3['country'])\ndf3[['new_page','old_page']] = pd.get_dummies(df3['landing_page'])\ndf3.head()","bccf975d":"# lest see if there is a relation between country and conversion\npd.pivot_table(data = df3, index = \"country\", values = \"converted\").sort_values(by = \"converted\", ascending = False) * 100","97d81796":"# Instantiate and fit the regression model with country as an additional variable: 'CA' is a baseline\nmodel = sm.Logit(df3['converted'], df3[['intercept','ab_page', 'UK','US']])\nresult = model.fit()\nresult.summary()","99d670d8":"# exponentiate the parameters to inteprete the result\nnp.exp(result.params)","499c132f":"#### Summary:\nIn this notebook we conducted a detailed A\/B testing using 3 main methods:\n1. Sampling distribution\n2. Z test\n3. Logestic regression \n\nAll three methods resulted in the same conclusion: the treatment has no impact.","c67ae575":"<a id='probability'><\/a>\n#### Part II - Probability","63020c5b":"\n## Table of Contents\n- [Introduction](#intro)\n- [Part I - Data Cleaning](#Cleaning)\n- [Part II - Probability](#probability)\n- [Part III - A\/B Test](#ab_test)\n- [Part IV - Regression](#regression)\n\n\n\n\n\n\n<a id='intro'><\/a>\n### Introduction\n\nA\/B tests are very commonly performed by data analysts and data scientists.  It is important that we get some practice working with the difficulties of these.\n\nFor this project, we will be working to understand the results of an A\/B test run by an e-commerce website.  Our goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n\n","8da6deef":"<a id='Cleaning'><\/a>\n#### Part I - Data Cleaning \n\nTo get started, let's import our libraries.","cd844013":"<a id='regression'><\/a>\n### Part IV - Regression\n\nIn this final part, you will see that the result we achieved in the A\/B test Part above can also be achieved by performing regression. Since each row is either a conversion or no conversion, we will use logestic regression to see if there is a significant difference in conversion based on which page a customer receives. However, we first need to create in df2 a column for the intercept, and create a dummy variable column for which page each user received.","18b02db8":"As you can see, there are 3893 rows where treatment does not match with new_page or control does not match with old_page, we cannot be sure if this row truly received the new or old page. How we will deal with those rows?","66c85d65":"Using test statistic and p-value, we reach the same coclusion: we can not reject the null ","41699e33":"It seems that coutry has a very minor impact on convergance. we will see its true impact along with ather features in the regression# Instantiate and fit the regression model\nmodel = sm.Logit(df2['converted'], df2[['intercept','ab_page']])\nresult = model.fit()\nresult.summary()# Instantiate and fit the regression model\nmodel = sm.Logit(df2['converted'], df2[['intercept','ab_page']])\nresult = model.fit()\nresult.summary()","d2d6f653":"There is mismatch between number of users assigned to treatment and the number of those landed on treatment page. This might indicate a problem with the data and needs further exploration.","b8e7beca":"#### Concluding remarks for this section so far:\n\n1. The p_value (0.9) is greater than alpha, therefore we fail to reject the null.\n2. This emphasizes of initial conclusion that there is no significant impact for the new page.\n\nWe could also use a built-in to achieve similar results. Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance.","59c2f9e0":"For now, consider you need to make the decision just based on all the data provided. If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%\n\n**$H_{0}$**: **$p_{old}$** - **$p_{new}$** **$>= 0$**\n\n**$H_{1}$**: **$p_{old}$** - **$p_{new}$** **$ < 0$**","c8314ee5":"#### Is there a sufficient evidence to conclude that the new treatment page leads to more conversions?\n\n1. The probability that an individual received the new page is 50%\n2. The probability of an individual converting regardless of the page they receive is 11.96%\n3. Given that an individual was in the control group, the probability they converted is 12.04%\n4. Given that an individual was in the treatment group, the probability they converted is 11.88%\n\n1 to 4 suggests that there is no significant difference in convergence between treatment and control groups. Therefore we may conclude that the new treatment page has no impact and does not lead to more conversions.","b068ecca":"All the coefficients are statistically insignificant except the intercept. This comes inline with the initial conclusions that we have just made.","02a18b6d":"<a id='ab_test'><\/a>\n### Part III - A\/B Test\n\nNotice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.\n\nHowever, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time? How long do you run to render a decision that neither page is better than another?\n\nThese questions are the difficult parts associated with A\/B tests in general. ","285fdbed":"The P-Value is 0.190, It is different from the one we obtained from the previous analysis because the null hypothesis is different in both cases.\n\nWe might add additional features to our model such timestamp, the reason for that conversion might differ according to the time at which the user visits the website. But this might come with a disadvantage, the model will become more complicated and less interpretable. It might be also  susceptible to overfitting.\n\n\nWe will leave timestamp for now. Instead, along with testing if the conversion rate changes for different pages, we will also add an effect based on which country a user lives in. we will need to read in the countries.csv dataset and merge together your datasets on the appropriate rows."}}