{"cell_type":{"4e675673":"code","9029d0b0":"code","a63b3794":"code","74e3b5a9":"code","d8dcfb30":"code","343f5244":"code","b51c9804":"code","9957258e":"code","0b62a95f":"code","7ced1ca4":"code","055d8d70":"code","68bd175c":"code","dcbfc9d8":"code","b94c4095":"code","acb4994e":"code","80da1019":"code","9b0d38b8":"code","5cbc7317":"code","69aa0cc7":"code","ca960bc3":"code","53b41c2d":"code","93e3e78f":"code","877bcb04":"code","aa1f96bc":"markdown","6a8f76ad":"markdown","aa2cf1f4":"markdown","765817e4":"markdown","11faf9d3":"markdown","43031ea4":"markdown","2ae2c12f":"markdown"},"source":{"4e675673":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9029d0b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a63b3794":"import os\nprint(os.listdir(\"..\/input\"))","74e3b5a9":"data=pd.read_csv(\"..\/input\/camera_dataset.csv\")\ndata.head()","d8dcfb30":"df_corr=data.corr(method='pearson')\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(df_corr, annot=True, ax=ax)\nplt.title('Correlation for camera attributes')\nplt.show()","343f5244":"fig= plt.figure(figsize=(15,15))\nax2=fig.add_subplot(331)\nplt.scatter(data['Low resolution'], data['Max resolution'])\nplt.title('Low ressolution & High Resolution')\nplt.xlabel('Low Resolution')\nplt.ylabel('Max Resolution')\nax2=fig.add_subplot(332)\nplt.scatter(data[\"Effective pixels\"], data['Max resolution'])\nplt.title('Effective pixels & Max resolution ')\nplt.xlabel('Effective pixels')\nplt.ylabel('Max resolution')\nax2=fig.add_subplot(333)\nplt.scatter(data['Release date'],data['Effective pixels'])\nplt.title('Release date & Effective pixels')\nplt.xlabel('Release date')\nplt.ylabel('Effective pixels')\nplt.show()","b51c9804":"data.fillna(0, inplace=True)","9957258e":"from sklearn.cluster import KMeans\nimport sklearn.metrics as sm","0b62a95f":"chosen=['Release date','Max resolution','Low resolution','Effective pixels','Zoom wide (W)','Zoom tele (T)','Normal focus range','Macro focus range','Storage included','Weight (inc. batteries)','Dimensions','Price']\nX=data[chosen].values\nmodel = KMeans(n_clusters=2)\nmodel.fit(X)","7ced1ca4":"colormap = np.array(['pink','purple'])\nplt.scatter(data['Zoom wide (W)'], data['Low resolution'],c=colormap[model.labels_], s=40)\nplt.title('K Mean Classification')\nplt.show()","055d8d70":"plt.scatter(data['Zoom wide (W)'], data['Weight (inc. batteries)'],c=colormap[model.labels_], s=40)\nplt.title('K Mean Classification')\nplt.show()","68bd175c":"fig2 = plt.figure(figsize=(15, 15))\nax6 = fig2.add_subplot(331)\nplt.scatter(data['Effective pixels'], data['Max resolution'],c=colormap[model.labels_], s=40)\nplt.title('effective pixels and max resolution')\nplt.xlabel('Effective pixels')\nplt.ylabel('Max resolution')\nax6 = fig2.add_subplot(332)\nplt.scatter(data['Effective pixels'], data['Low resolution'],c=colormap[model.labels_], s=40)\nplt.title('effective pixels and low resolution')\nplt.xlabel('Effective pixels')\nplt.ylabel('Low resolution')\nax6=fig2.add_subplot(333)\nplt.scatter(data['Max resolution'], data['Low resolution'],c=colormap[model.labels_], s=40)\nplt.title('max resolution and low resolution')\nplt.xlabel('Max resolution')\nplt.ylabel('Low resolution')\nax6=fig2.add_subplot(334)\nplt.scatter(data['Zoom wide (W)'], data['Weight (inc. batteries)'],c=colormap[model.labels_], s=40)\nplt.title('Zoom and weight')\nplt.xlabel('Zoom')\nplt.ylabel('weight')\nax6=fig2.add_subplot(335)\nplt.scatter(data['Effective pixels'], data['Release date'],c=colormap[model.labels_], s=40)\nplt.title('Effective pixels and release date')\nplt.xlabel('Effective pixels')\nplt.ylabel('Release date')\nax6=fig2.add_subplot(336)\nplt.scatter(data['Max resolution'], data['Release date'],c=colormap[model.labels_], s=40)\nplt.xlabel('Max resolution')\nplt.ylabel('Release date')\nax6=fig2.add_subplot(337)\nplt.scatter(data['Low resolution'], data['Release date'],c=colormap[model.labels_], s=40)\nplt.xlabel('Low resolution')\nplt.ylabel('Release date')\nax6=fig2.add_subplot(337)\nplt.scatter(data['Weight (inc. batteries)'], data['Dimensions'],c=colormap[model.labels_], s=40)\nplt.xlabel('Weight')\nplt.ylabel('Dimensions')\nplt.show()\n","dcbfc9d8":"data1=data.drop(['Model','Release date'],axis=1)","b94c4095":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(data1)\n    data1[\"clusters\"] = kmeans.labels_\n    #print(data[\"clusters\"])\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"Distortion\")\nplt.show()","acb4994e":"from sklearn.preprocessing import StandardScaler","80da1019":"x = StandardScaler().fit_transform(X)\nprint(x)","9b0d38b8":"print('NumPy covariance matrix: \\n%s' %np.cov(x.T))","5cbc7317":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\nprincipalDf","69aa0cc7":"pca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","ca960bc3":"print(pca.explained_variance_ratio_)","53b41c2d":"\nplt.scatter(principalDf['principal component 1'],principalDf['principal component 2'],s=30,c='goldenrod',alpha=0.5)\nplt.title('plotting both variables')\nplt.xlabel('Principal component 1')\nplt.ylabel('Principal component 2')\nplt.show()","93e3e78f":"model = KMeans(n_clusters=3)\nmodel.fit(principalDf)","877bcb04":"colormap = np.array(['blue','red','yellow','orange','purple'])\nplt.scatter(principalDf['principal component 1'], principalDf['principal component 2'],c=colormap[model.labels_], s=40)\nplt.title('K Mean Classification')\nplt.show()","aa1f96bc":"covariance matrix  (similar to correlation matrix)","6a8f76ad":"**Above we find, that the first component contains about 62.3% of the variance and second about 29.2%, together about 91.5% of the information.**","aa2cf1f4":"**We plot the mean distance to the centroid as a function of K, leading to an elbow point, where the rate of decreasesharply shifts, giving a rough estimate to determine K **","765817e4":"**Next, we find the explained variance ratio. This is tells how much information can be attributed to each of the principal component. Since, we are converting a 13 dimensional data to 2 dimensions, we will loose some of the variance by doing so.**","11faf9d3":"Here, we start by the correlation heat map, to find the correlation between attributes. The matrix used here, uses Pearson's correlation matrix, where the value for correlation lies between -1 and 1 , negative values suggesting a negative correlation between the attributes ( indirectly proportional) and positive values suggesting a positive corelation between attributes (directly proportional). ","43031ea4":"**Since PCA is effected by scale, we need to scale features  in the data before applying PCA.**","2ae2c12f":"**Since, we observe that only 2 components account for over 95% of the data, it means, that we will be able to recover most of the essential characteristics with only 2 components.**"}}