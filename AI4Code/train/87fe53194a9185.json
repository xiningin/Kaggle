{"cell_type":{"c484e1ec":"code","4770a85c":"code","ca66615e":"code","01657c0c":"code","ba024254":"code","48dd8d14":"code","2cae5936":"code","bd50d032":"code","161697d2":"code","751ae6ab":"code","41ebc31e":"code","0bfcc6d2":"code","33cefee9":"code","dd258682":"code","2556413e":"code","9590a7e6":"code","41f1b618":"code","5ae62eb9":"code","75084923":"code","8a4ea01d":"code","7e480abe":"code","d8b68874":"code","65e6badc":"code","b9f6caf7":"code","19c93d02":"code","ab7dcc75":"code","da1a318a":"markdown","21180ee2":"markdown","3ecc95e1":"markdown","fd350c1f":"markdown","b0e746be":"markdown","8f97cbec":"markdown","c36a702b":"markdown","2d8936bd":"markdown","80c3ea8d":"markdown","1f5b527d":"markdown","7cee0988":"markdown","24679576":"markdown","88e4a589":"markdown","1c3e5520":"markdown","91046942":"markdown","53ca28c2":"markdown","4102aba2":"markdown","814ad0d6":"markdown","6ed4de66":"markdown","a0bc1f1a":"markdown"},"source":{"c484e1ec":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()","4770a85c":"data = pd.read_csv(\"..\/input\/boom-bikes-linear-regression\/day.csv\")\ndata.shape","ca66615e":"data.head()","01657c0c":"# instant is just index numbers, so we set instant to indexes\ndata = data. set_index('instant')","ba024254":"data.info()","48dd8d14":"# let us derive the day(1-30\/31) from the dteday column and drop the dteday colum\n# we also need to drop registered and casual columns as the sum of these 2 columns equals our target column which is cnt\n# lets cerate dummy varibales for weathersit and season column, right now our data for these columns is label encoded - lets check if creating dummies will have any effect of not\nfrom datetime import date\ndata.dteday = pd.to_datetime(data.dteday)\ndata[\"day\"] = [i.day for i in data.dteday]\ndata.drop(['dteday','registered','casual'],axis=1, inplace=True)\ndata.weathersit = data.weathersit.map({1:'Clear',2:'Mist',3:'Light_Snow_Rain',4:'Heavy_Snow_Rain'})\ndata.season = data.season.map({1:'Spring',2:'Summer',3:'Fall',4:'Winter'})\ndata.mnth = data.mnth.replace((1,2,3,4,5,6,7,8,9,10,11,12),('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'))\ndata.weekday = data.weekday.map({0:'Sun',1:'Mon',2:'Tue',3:'Wed',4:'Thu',5:'Fri',6:'Sat'})","2cae5936":"data.head(5)","bd50d032":"#let us look at the distributions of count of bikes rented as per the season\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18,12))\nsns.boxplot(x=\"season\", y=\"cnt\", data=data, ax=ax1)\nsns.boxplot(x=\"weathersit\", y=\"cnt\", data=data, ax=ax2)\nsns.boxplot(x=\"holiday\", y=\"cnt\", data=data, ax=ax3)\nsns.boxplot(x=\"workingday\", y=\"cnt\", data=data, ax=ax4)","161697d2":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,6))\nsns.scatterplot(x=\"temp\", y=\"cnt\", data=data, ax=ax1)\nsns.scatterplot(x=\"hum\", y=\"cnt\", data=data, ax=ax2)\nsns.scatterplot(x=\"windspeed\", y=\"cnt\", data=data, ax=ax3)\nfig.show()","751ae6ab":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\nsns.barplot(x=\"mnth\", y=\"cnt\", data=data, ax=ax1)\nsns.lineplot(x=\"day\", y=\"cnt\", data=data, ax=ax2)\nfig.show()\nsales = [str(i)+'-'+str(j)+'-'+str(k) for i,j,k in zip(data.day, data.mnth, data.yr.map({0:'2018',1:'2019'}))]\nfig, (ax1) = plt.subplots(1, 1, figsize=(18,6))\nsns.lineplot(x=pd.to_datetime(sales), y=data.cnt, ax=ax1)","41ebc31e":"# creating dummies \nseason_dummy = pd.get_dummies(data.season,drop_first=True)\nweather_dummy = pd.get_dummies(data.weathersit,drop_first=True)\nmonth_dummy = pd.get_dummies(data.mnth,drop_first=True)\nweekday_dummy = pd.get_dummies(data.weekday,drop_first=True)\nfinal_df = data.join(season_dummy)\nfinal_df = final_df.join(weather_dummy)\nfinal_df = final_df.join(month_dummy)\nfinal_df = final_df.join(weekday_dummy)\nfinal_df.drop(['season','weathersit','mnth','weekday'], axis=1, inplace=True)\nprint(final_df.shape)\nfinal_df.head()","0bfcc6d2":"# let us look at the correation matrix of the data\ncorr = data[['temp','atemp','hum','windspeed','cnt']].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,square=True, linewidths=.5, annot=True)\nf.show()","33cefee9":"final_df.describe()","dd258682":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","2556413e":"# creating a method for linear regression so that we can re use it again with different features\ndef lin_reg(X_train, y_train, X_test, y_test):\n  X_train = sm.add_constant(X_train) # adding a constant\n  model = sm.OLS(y_train, X_train).fit()\n  X_test = sm.add_constant(X_test)\n  y_pred_train = model.predict(X_train)\n  y_pred_test = model.predict(X_test)\n  print(\"R2 Score of the training data:\",r2_score(y_pred = y_pred_train, y_true = y_train))\n  print(\"R2 Score of the testing data:\",r2_score(y_pred = y_pred_test, y_true = y_test))\n  print(model.summary())\n  sns.regplot(y_test,y_pred_test)\n  return model","9590a7e6":"# Train test split ######################################################################\nX = final_df.drop('cnt', axis=1)\ny = final_df.cnt\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n# Sacling the numerical variables #######################################################\nnum_cols = ['temp','atemp','hum','windspeed']\nscaler = MinMaxScaler()\nscaler.fit(X_train[num_cols])\ntrain_scaled = scaler.transform(X_train[num_cols])\ntest_scaled = scaler.transform(X_test[num_cols])\n# Scaling training data\nX_train[num_cols] = train_scaled\n# Scaling test data\nX_test[num_cols] = test_scaled\n# Applying Linear Regression ############################################################\n# Creating a base model to check the initial results\nmodel_1 = lin_reg(X_train, y_train, X_test, y_test)","41f1b618":"''' \nalthough we have a function created above for linear regression, we have created here another one as we do not need\nto print summary and the chart everytime we create a model to check the p values.\n'''\ndef lin_reg_2(X_train, y_train, X_test, y_test):\n  X_train = sm.add_constant(X_train) # adding a constant\n  model = sm.OLS(y_train, X_train).fit()\n  X_test = sm.add_constant(X_test)\n  y_pred_train = model.predict(X_train)\n  y_pred_test = model.predict(X_test)\n  print(\"R2 Score of the training data:\",r2_score(y_pred = y_pred_train, y_true = y_train))\n  print(\"R2 Score of the testing data:\",r2_score(y_pred = y_pred_test, y_true = y_test))\n  return model","5ae62eb9":"# this method will get the p-values from the model, sort it in descending order and return the sorted data frame. \n# (this will help us pick up the variable with highest pvalue and drop it)\ndef get_pvalues(model):\n  pval = pd.DataFrame(data= model.pvalues, columns=['Pvalue'])\n  pval['feature'] = pval.index\n  pval.reset_index(drop=True, inplace=True)\n  pval['Pvalue'] = [round(i,3) for i in pval['Pvalue']]\n  pval = pval[['feature','Pvalue']].sort_values('Pvalue', ascending=False)\n  return pval","75084923":"# this method will compute VIFs for all the variables, sort it in descending order and return the sorted data frame. \n# (this will help us pick up the variable with highest VIF and drop it)\ndef get_VIF(X_train):\n  columns = X_train.columns\n  # VIF DataFrame\n  vif_data = pd.DataFrame()\n  vif_data[\"feature\"] = X_train.columns\n  # calculating VIF for each feature\n  vif_data[\"VIF\"] = [variance_inflation_factor(X_train[columns].values, i) for i in range(len(columns))]\n  vif_data = vif_data.sort_values('VIF', ascending=False)\n  return vif_data","8a4ea01d":"'''\nThis Method will Iterate over our list of variables calculating P values and VIF's for each variable,\nuntil we eliminate all the varibales with high VIFs and high P-values\nwe drop variables one at a time, and after every time we drop a variable we compute the VIF and P-values again for all the variables.\n'''\ndef pval_vif_feature_selection(X_train, X_test):\n  flag_1 = False # the flag is set to True when we have eliminated all the variables with high VIF\n  flag_2 = False # the flag is set to True when we have eliminated all the variables with high P-Values\n  X_train_VIF = X_train\n  cols = X_train_VIF.columns # inital list of our columns (contains all variables)\n  # the below loop will run until we have eliminated all the variables with high collinearity and high p values.\n  while True:\n    # creating a model in stats model to look at the p values and drop columns which are not significant\n    X_train_VIF = X_train[cols]\n    X_test_VIF = X_test[cols]\n    model = lin_reg_2(X_train_VIF, y_train, X_test_VIF, y_test)\n    pvalues = get_pvalues(model) \n    # dropping a variable with the highest p-value\n    if pvalues['Pvalue'].tolist()[0]>0.05:\n      print(\"Dropping\",pvalues['feature'].tolist()[0],\"with Pvalue\",pvalues['Pvalue'].tolist()[0])\n      cols = cols.drop(pvalues['feature'].tolist()[0])\n    else:\n      if not flag_2:\n        print(\"\\n----------------------------------------------\\nDropped columns with high pvalues - None Left\\n----------------------------------------------\")\n      flag_2 = True\n    X_train_VIF = X_train[cols]\n    # calling the VIF method\n    vif_data = get_VIF(X_train_VIF)\n    # dropping a variable with the highest VIF\n    if vif_data['VIF'].tolist()[0]>5:\n      print(\"Dropping\",vif_data['feature'].tolist()[0],\"with VIF\",vif_data['VIF'].tolist()[0])\n      cols = cols.drop(vif_data['feature'].tolist()[0])\n    else:\n      if not flag_1:\n        print(\"\\n----------------------------------------------------\\nDropped All columns with high collinearity - None Left\\n----------------------------------------------------\")\n      flag_1 = True\n    # when both flags are set to True we exit the loop\n    if flag_1 and flag_2:\n      print(vif_data,\"\\n\")\n      print(pvalues)\n      return cols\n    print()","7e480abe":"# running the above explained method, \n# please have a look at the output below as it explains which variables we are dropping and how it is affecting our R2 score\nX_train_VIF = X_train\nX_test_VIF = X_test\ncolumns = pval_vif_feature_selection(X_train_VIF, X_test_VIF)","d8b68874":"### let us now create a model with the columns we have and do a detailed analysis on the model to check if our model can explain the variance\nX_train_VIF = X_train[columns]\nX_test_VIF = X_test[columns]\nmodel_2 = lin_reg(X_train_VIF, y_train, X_test_VIF, y_test)","65e6badc":"from sklearn.feature_selection import RFE\nlm = LinearRegression()\n#Use RFE to remove not significant features from the initial model.\nrfe = RFE(lm, step=1)\nrfe = rfe.fit(X_train, y_train)\n#Test new model\n#New features dataframe containing only selected features through RFE\nX_RFE = X_train[X_train.columns[rfe.support_]]\nX_RFE_test = X_test[X_test.columns[rfe.support_]]\nmodel_3 = lin_reg(X_RFE, y_train, X_RFE_test, y_test)","b9f6caf7":"# method for cheking LR assumptions\ndef assumptions(model, X_train, X_test, y_train, y_test):\n  # 0. checking R2 score on test and training set\n  r2_test = r2_score(y_test,model.predict(sm.add_constant(X_test)))\n  r2_train = r2_score(y_train,model.predict(sm.add_constant(X_train)))\n  print(\"R2 for training set:\",r2_train,\"\\nR2 for test set:\",r2_test)\n  # 1. checking if the residual mean is 0\n  y_pred = model.predict(sm.add_constant(X_train))\n  residuals = y_train.values - y_pred\n  print(\"Residual Mean Error is:\",round(residuals.mean(),3))\n  # 2. check for homoscedasticity\n  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28,6))\n  sns.scatterplot(y_pred,residuals, ax=ax1)\n  ax1.set_xlabel(\"Residuals\")\n  ax1.set_ylabel(\"Predictions\")\n  ax1.set_title(\"Homoscedasticity Check\")\n  sns.lineplot([0,8000],[0,0],color='red', ax=ax1)\n  # 3. check for normality of error terms\/residuals\n  sns.distplot(residuals, ax=ax2)\n  ax2.set_xlabel(\"Residuals\")\n  ax2.set_title(\"Normality of Error Terms Check\")\n  # 4. check for multi collinearity\n  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n  sns.heatmap(X_train.corr(), cmap=cmap, ax=ax3)\n  ax3.set_title(\"Multi-Collinearity Check\")","19c93d02":"# Model 3 Residual Analysis\nassumptions(model_3,X_RFE, X_RFE_test,y_train,y_test)","ab7dcc75":"# Model 2 Residual Analysis\nassumptions(model_2,X_train[columns], X_test[columns],y_train,y_test)","da1a318a":"After getting the intial results we see that although our R<sup>2<\/sup> and adj.R<sup>2<\/sup> is 85% and 84%, our F statistic is low and also we see that the p value for lot of our variables is very high. we also see a problem of multi-collinearity.<br>\n<b>Let us try to solve the problem of multi-collinearity using VIF and also we will iteratively drop variables which has high p-value<\/b> (high p-value implies that the variable is not very significant for predictig the results & high VIF signifies that a variable is highly correlated with other variables).","21180ee2":"### Let us start with data preperation first","3ecc95e1":"### <b>Model 1: Selecting all the available features for processing -> BASE<\/b>","fd350c1f":"### EDA","b0e746be":"<b>Conclusion<\/b>\n>Consedering the adj.R<sup>2<\/sup>, F-statistic and the number of significant variables present we can safely conclude that model 3 which we created using RFE is our best model and can best explain the variance of the data.<br>\n> So to predict future sales for boom bikes we can go ahead and deploy our model 3 to get a good estimate of the sales per day.","8f97cbec":"### Modeling","c36a702b":"Checking if there is any Linear relationship between our continous variables and our target variable.\n> we see some linear relation between count and the temp variable.<br>\n> for the other 2 variables we cannot say the same.(no pattern)","2d8936bd":"<b>Observation:<\/b>\n>After creating our 3<sup>rd<\/sup> model using RFE feature selection technique we see that our R<sup>2<\/sup> and adj.R<sup>2<\/sup> have increased compared to our 2<sup>nd<\/sup> model and our F-statictic has also significantly increased suggesting a good fit. also, RFE took care of the multi collinearity problem for us and we do not see high P-values for any of the variable<br>","80c3ea8d":"### <b>Model 2: Feature Selection Using VIF and P-Values<\/b>\n","1f5b527d":"### Residual Analysis & Linear Assumptions Checks\n\n---\n\n","7cee0988":"#### Model 3: Using RFE for Feature Selection","24679576":">we see that both our models <b><i>satisfies all the assumptions of linear regression.<\/i><\/b> also the r2 on training and test set is pretty close. we can say we have achieved a generalised model.<br>\nalthough our adj. R2 for model 3 is higher than model 2 so we go ahead and select model 3.<br>\n<b>Note: we have not performed residual analysis on model 1 as there we have multi collinearity problem and lot of out variables are not significant for the model.<\/b>","88e4a589":"just for better understanding of the data we will paste our data dictionary here\n  - instant: record index\n\t- dteday : date\n\t- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n\t- yr : year (0: 2018, 1:2019)\n\t- mnth : month ( 1 to 12)\n\t- holiday : weather day is a holiday or not \n\t- weekday : day of the week\n\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n\t+ weathersit : \n\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy - Clear\n\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist - Mist\n\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds - Light_Snow_Rain\n\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog - Heavy_Rain\n\t- temp : temperature in Celsius\n\t- atemp: feeling temperature in Celsius\n\t- hum: humidity\n\t- windspeed: wind speed\n\t- casual: count of casual users\n\t- registered: count of registered users\n\t- cnt: count of total rental bikes including both casual and registered\n","1c3e5520":"> Our plan here is to look at the P-Values first and drop a variable with the highest P-Value (Least Signifianct variable) and then caluculate the VIF for all the avriables and drop the one with highest VIF value (which is highly correlated with other variables).<br>\n><b>Note: we perform the dropping of the variables one at a time.<\/b><br>\n>After creating the dummy variables our data set now has lot of columns, so rather than checking p values and VIFs manually, I've created functions which will keep on iterating over our training data set until it has removed all the variables with VIF > 5 and variables with P-Value > 0.05","91046942":"chart 1: is the test of homoscedasticity where we check if the variance is constant<br>\nchart 2: is the test we check id the residuals are normallu distributed.<br>\nchart 3: is there multi collinearity present<br>","53ca28c2":"### Data Reading and Understanding","4102aba2":"as we per the EDA performed above we saw a linear pattern between temp and the target variable and also we see that temp and atemp are highly correlated.","814ad0d6":"<b>Observation:<\/b>\n>After creating our 2<sup>nd<\/sup> model we see that although our R<sup>2<\/sup> and adj.R<sup>2<\/sup> have been reduced by 2\/3 percent but our F-statictic has significantly increased suggesting a good fit. also, we took care of the multi collinearity problem by dropping the variables with high VIF. the variables we have now are also significant with p-values < 0.05.<br>\n>now that we have made some progress, but it was a manual approach. so let us use another feature selection technique RFE and check if we get any different results than this.","6ed4de66":"Above we have plotted all the categorial variables distribution with respect to the count\/sales variable.\n><li> we do not see any outliers in any of our variable.<br>\n><li> we can also conclude that we can have more number of riders in Summer and Fall compared to other seasons.<br>\n><li> we can expect more sales when we have a clear weather.<br>\n><li> we do not see if any difference in sales when it comes to working or non-working day.","a0bc1f1a":"Looking at the sales of the company as per the time series data we have\n> <li> we see a rise number of riders in the month of may to october\n> <li> and also if we look at avergae number of riders on a day in a month we see number of riders rising in between 5th and 10th day of the month\n> <li> looking at our 3rd chart of time series we can expect an increase in the number riders every year."}}