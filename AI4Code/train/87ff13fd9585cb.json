{"cell_type":{"5fca1f01":"code","9c67a158":"code","ad1f17f2":"code","83049201":"code","fa3fe4e6":"code","989c40d9":"code","ad392a7c":"code","f4069be3":"code","7b02a2e6":"code","c8f63bb2":"code","7999ab14":"code","d86750a0":"code","2c2cf081":"code","633e351d":"code","cba0e9a9":"code","754824a0":"code","de810c11":"code","f899f3b6":"code","76134319":"code","276baf37":"code","5b781c49":"code","c76a86fb":"code","08d6082c":"code","f47d97ab":"code","8a628b75":"code","508cf078":"code","7ff628dd":"code","d9400eb4":"code","fce6672e":"code","a1becdef":"code","c90cd7d6":"code","223153b0":"code","ecb77da6":"code","6f79341d":"markdown","56d4d49a":"markdown","96669992":"markdown","f3683150":"markdown","2b2640b9":"markdown","ce5b89ab":"markdown","9b94b970":"markdown","8afffb55":"markdown","b82b5a45":"markdown","1c93a575":"markdown","02f174c6":"markdown","3c2f6904":"markdown","1bb3f68d":"markdown","5e0c43d8":"markdown","5891e068":"markdown","62d3cc82":"markdown","d19906c0":"markdown","f8b81ee0":"markdown","791b937e":"markdown","a12afdda":"markdown","41eabe46":"markdown","7f999d96":"markdown","a770027f":"markdown","812187c8":"markdown","42d9ee8d":"markdown","9c3527d6":"markdown","5716472d":"markdown","3e281ac8":"markdown","ba993811":"markdown","169e26aa":"markdown","e1486cb0":"markdown","9f72a906":"markdown","80e73dff":"markdown","40190bfd":"markdown","11eb7a21":"markdown","feb303ad":"markdown","ad0f6ffe":"markdown"},"source":{"5fca1f01":"pip install scikit-learn==0.24.1","9c67a158":"# Essentials\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport missingno as miss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\n\n# Stats\nfrom scipy.stats import skew\n\n# Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, FunctionTransformer\n\n# Models\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\n\n# Hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n# Misc\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import set_config","ad1f17f2":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","83049201":"display(train.head(5))\nprint()\ndisplay(train.info())","fa3fe4e6":"display(X_test.head(5))\nprint()\ndisplay(X_test.info())","989c40d9":"def missing_values_table(data):\n    \n    missing_cout = data.isna().sum()\n    \n    missing_percent = (missing_cout \/ len(data)).apply('{0:.2%}'.format)\n    \n    data_types = data.dtypes\n    \n    miss_table = pd.concat([missing_cout, missing_percent, data_types], axis=1)\n    miss_table = miss_table.loc[miss_table[0] > 0]\n    miss_table.columns = ['Missing Values', '% of Total Values', 'Data Types']\n    \n    print(miss_table.shape[0], 'columns out of total',\n          data.shape[1], 'columns in selected data have missing values.')\n    print()\n    display(miss_table.sort_values(by=['Data Types', 'Missing Values'], ascending=False))","ad392a7c":"missing_values_table(train)\nmiss.matrix(train)","f4069be3":"missing_values_table(X_test)\nmiss.matrix(X_test)","7b02a2e6":"def na_compare_prices(feature):\n    not_na_price = train[train[feature].notna()].groupby(by=feature).SalePrice.mean()\n    na_price = train[train[feature].isna()].SalePrice.mean()\n    \n    colors = ['MediumSeaGreen',] * len(not_na_price)\n    colors.append('crimson')\n\n    fig = go.Figure([go.Bar(\n        x=[*not_na_price.index, 'NaN'],\n        y=[*not_na_price, na_price],\n        marker_color=colors,\n    )])\n    \n    fig.update_layout(\n        title={'text': 'SalePrice by ' + feature, 'x':0.45},\n        xaxis_title_text=feature, \n        yaxis_title_text='SalePrice',\n        width=800, height=400,\n    )\n    fig.show()","c8f63bb2":"for feature in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType']:  \n    na_compare_prices(feature)","7999ab14":"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle('Target distribution: raw vs logarithmic')\nsns.histplot(ax=axes[0], data=train.SalePrice, kde=True)\naxes[0].set_title('SalePrice')\naxes[0].text(520000, 150,\n    'Skewness: {:.3f}\\nKurtosis: {:>8.3f}'.format(train.SalePrice.skew(), train.SalePrice.kurt()),\n    fontsize=12)\nsns.histplot(ax=axes[1], data=np.log1p(train.SalePrice), kde=True)\naxes[1].set_title('log(SalePrice + 1)')\naxes[1].text(12.5, 128,\n    'Skewness: {:.3f}\\nKurtosis: {:>8.3f}'.format(np.log1p(train.SalePrice).skew(), np.log1p(train.SalePrice).kurt()),\n    fontsize=12)\nfig.show()","d86750a0":"numerical_cols = [cname for cname in train.columns if \n                train[cname].dtype in ['int64', 'float64']]\n\nfig, axes = plt.subplots(len(numerical_cols)\/\/3, 3, figsize=(20, 60))\nfor column, ax in zip(numerical_cols, axes.flat):\n    sns.histplot(ax=ax, data=train[column], kde=True)\n    ax.annotate(\n        'Skewness: {:.3f}\\nKurtosis: {:>8.3f}'.format(train[column].skew(), train[column].kurt()),\n        xy=(0.65, 0.8), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"))\n\nplt.show()","2c2cf081":"y_train = np.log1p(train.SalePrice)\nX_train = train.drop(['SalePrice'], axis=1)","633e351d":"categorical_cols = [cname for cname in X_train.columns if\n                        X_train[cname].nunique() < 15 and\n                        X_train[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\nX_train = X_train[numerical_cols + categorical_cols]\nX_test = X_test[numerical_cols + categorical_cols]\n\nskewed_cols = X_train[numerical_cols].apply(lambda x: skew(x.dropna()))\nskewed_cols = skewed_cols[skewed_cols > 0.75].index.tolist()","cba0e9a9":"set_config(display='diagram')","754824a0":"categorical_transformer = Pipeline(steps=[\n    ('SimpleImputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore')),\n])\ndisplay(categorical_transformer)","de810c11":"numerical_transformer = Pipeline(steps=[\n    ('Regularization', ColumnTransformer(\n        transformers=[\n            ('Skewed: np.log1p', FunctionTransformer(np.log1p), skewed_cols),\n    ], remainder='passthrough')),\n    ('KNNImputer', KNNImputer(n_neighbors=2, add_indicator=False)),\n])\n\ndisplay(numerical_transformer)","f899f3b6":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical', numerical_transformer, numerical_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n])\ndisplay(preprocessor)","76134319":"def rmse_cv(pipe, X_train, y_train):\n    return np.sqrt(-cross_val_score(pipe, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 3, n_jobs=-1).mean())    ","276baf37":"pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', Ridge())\n])\n\nrmse_cv(pipe, X_train, y_train)","5b781c49":"def hyperopt_search(fn, space):\n    trials = Trials()\n\n    best_hyperparams=fmin(\n        fn=fn, # function to optimize\n        space=space, \n        algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n        max_evals=100, # maximum number of iterations\n        trials=trials, # logging\n        rstate=np.random.RandomState(123) # fixing random state for the reproducibility\n    )\n\n    print(\"The best hyperparameters are : \",\"\\n\")\n    print(best_hyperparams)","c76a86fb":"def ridge_rmse_cv(params, preprocessor=preprocessor, X_train=X_train, y_train=y_train, random_state=123):\n    \n    # the function gets a set of variable parameters in \"params\"\n    params = {'alpha': params['alpha']}\n    \n    # we use this params to create a new pipeline\n    model = Ridge(**params)    \n    pipe = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # and then conduct the cross validation\n    return rmse_cv(pipe, X_train, y_train)","08d6082c":"space={'alpha': hp.loguniform('alpha', -10, 4)}\n\nhyperopt_search(ridge_rmse_cv, space)","f47d97ab":"pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', Lasso())\n])\n\nrmse_cv(pipe, X_train, y_train)","8a628b75":"def lasso_rmse_cv(params, preprocessor=preprocessor, X_train=X_train, y_train=y_train, random_state=123):\n    \n    params = {'alpha': params['alpha']}\n\n    model = Lasso(**params)    \n    pipe = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n\n    return rmse_cv(pipe, X_train, y_train)","508cf078":"space={'alpha': hp.loguniform('alpha', -10, 4)}\n\nhyperopt_search(lasso_rmse_cv, space)","7ff628dd":"pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', XGBRegressor(random_state=123))\n])\n\nrmse_cv(pipe, X_train, y_train)","d9400eb4":"ridge_pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('Ridge',Ridge(alpha=3.72))\n])\n\nlasso_pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('Lasso', Lasso(alpha=0.00034))\n])\n\nxdbregressor_pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('XGBRegressor', XGBRegressor(random_state=123, learning_rate=0.0578, max_depth=2, n_estimators=765))\n])\n\nestimators = [\n    ('Ridge', ridge_pipe),\n    ('Lasso', lasso_pipe),\n    ('Gradient Boosting', xdbregressor_pipe),\n]","fce6672e":"stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=0.189))\ndisplay(stacking_regressor)","a1becdef":"rmse_cv(stacking_regressor, X_train, y_train)","c90cd7d6":"fig = go.Figure(go.Scatter(\n    x=['Ridge', 'Lasso', 'XGBRegressor', 'StackingRegressor'], \n    y=[0.13304, 0.13014 ,0.12645 , 0.12387],\n    mode='lines+markers'))\n\nfig.update_layout(\n    title={'text': 'Model performance', 'x':0.5},\n    xaxis_title_text='Model', \n    yaxis_title_text='RMSE',\n    width=700, height=400,\n)\n    \nfig.show()","223153b0":"stacking_regressor.fit(X_train, y_train)\npreds_test = np.expm1(stacking_regressor.predict(X_test))","ecb77da6":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\ndisplay(output.head())\noutput.to_csv('submission.csv', index=False)","6f79341d":"`Ridge()` without hyperparameter tuning.","56d4d49a":"# Ridge","96669992":"The idea behind stacking models is rather simple: we take predictions of several uncorrelated models and use final model to add weights to those and make our final predictions. Usually this approach results in better predictions.\n\nLet's combine our preprocessor and models with tuned hyperparameters into pipelines.","f3683150":"```python\ndef gb_rmse_cv(params, preprocessor=preprocessor, X_train=X_train, y_train=y_train, random_state=123):\n    \n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth']), \n              'learning_rate': params['learning_rate']}\n\n    model = XGBRegressor(random_state=random_state, **params)\n    \n    pipe = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n\n    return rmse_cv(pipe, X_train, y_train)\n\nspace={\n    'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n    'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n    'learning_rate': hp.loguniform('learning_rate', -5, 0)\n}\n\nhyperopt_search(gb_rmse_cv, space)\n```\n\n---\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100\/100 [23:42<00:00, 14.23s\/it, best loss: 0.12645916421481454]\nThe best hyperparameters are :  \n\n{'learning_rate': 0.05779730159845562, 'max_depth': 2.0, 'n_estimators': 765.0}","2b2640b9":"And combine them into sklearn's `StackingRegressor`, using `Ridge()` model as our final estimator. (I tunned `alpha` in Colab)","ce5b89ab":"In oreder to invert logarithmic transformation of our target (`SalePrice`), we'll use `numpy.expm1` on final model's predictions.","9b94b970":"# Making submissions","8afffb55":"`Lasso()` performed much worse out of the box. Let's see if we can make model better by tuning `alpha` in similar way.","b82b5a45":"# Model performance","1c93a575":"# Pipeline\n\nLet's start building our pipeline.\n\nFirst, let's splt training dataset into features and target and perform the `numpy.log1p` transformation for target.","02f174c6":"As we can see from the left plot, the SalePrice distribution is skewd to the right. Most ML models perform much better with normally distributed data. So, in order to enhance the performance of our models, we cam make target feature and other numeric features more normal by taking $log(feature + 1)$ - look at the plot on the right.\n\nLet's plot distributions of othen numerical features as well.","3c2f6904":"`Lasso()` without hyperparameter tuning.","1bb3f68d":"# House Prices: Data preprocessing and StackingRegressor in a single pipeline.\n\nThe goal of this notebook, as you might have guessed by the title, is to construct a single pipeline containing data preprocessing and several stacking models (Ridge, Lasso, Gradient Boosting). This project is very light on future engineering and there is still a lot of further work to be done here.\n\n**Why did I used a single pipeline?**\n\n* Learning and practicing construction of sklearn pipelines, obviously.\n\n* Doing preprocessing inside the pipeline prevents **train-test contamination**, which might be critical for cross-validation methods. For example, if we are imputing missing numerical values in both training and test data at the same time with mean value, our training data becomes corrupted (now it contains information from test data). Our model may give very good scores on testing or validation, but it's performance will become worse when we deploy it to make decisions on new data. When using cross-validation, it becomes very difficult to prevent train-test contamination, because the train-test split is happening inside the cv function. The solution is using pipelines: they exclude the validation data from any type of fitting, including the fitting of preprocessing steps.\n\n**Credit to:**\n\n* [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) - log transformation of skewed numeric variables and linear models\n\n* [Hyperparameters tunning with Hyperopt](https:\/\/www.kaggle.com\/ilialar\/hyperparameters-tunning-with-hyperopt#Hyperopt)","5e0c43d8":"Let's make a simple line graph to compare RMSE of our models.","5891e068":"Let's tune `alpha` hyperparameter using Tree-structured Parzen Estimator algorithm from **Hyperopt** library. It uses Bayesian approach for optimization, which is much more handy than random search and grid search, especially for gradient boosting model.\n\nWe'll use hyperopt's `fmin` to find best hyperparametrs combination which minimises loss function of our pipline from set space of hyperparametrs.","62d3cc82":"For categorical features, first we'll impute NA values with a constant marker value, and then encode them using One-Hot-Encoding.","d19906c0":"# Lasso","f8b81ee0":"Now we'll combine numerical and categorical transformers with `ColumnTransformer` for parallel preprocessing.","791b937e":"After hyperparametrs tuning gradient boosting showed very good results.","a12afdda":"Set config so sklearn's Pipelines are displayed as neat diagrams.","41eabe46":"# StackingRegressor","7f999d96":"# Exploring missing values\n\nAs we can see in results of `info()` method, there are missing values in dataset. Let's explore them.\n\nFirst, we'll write a function to calculate how many missing values there are in each column. The function will display that in a dataframe  along with % of values missing and data type of that column.","a770027f":"Let's try sklearn's regularized linear regression models: `Ridge()` -  l_2 and `Lasso()` - l_1 regularisation.\n\nFirst, we'll define cross-validation function, which will return mean RMSE (root mean squared error) over 3 k-folds.","812187c8":"## Data distribution and skewness.\n\n\nFist, let's plot distribution of our target - SalePrice.","42d9ee8d":"For numerical features, we'll first apply `numpy.log1p` only to skewed columns using `ColumnTransformer`, with remaining unskewed passing through unchanged, and then impute NA values using `KNNImputer`.","9c3527d6":"# Preprocessing pipeline","5716472d":"And also we'll add a missing value matrix from `missingno` library.","3e281ac8":"I think it would be reasonable to handle missing values thusly:\n\n* For categorical features - impute them with marker, like 'missing'.\n\n* For numerical features - impute them using the mean value from k-Nearest Neighbors.","ba993811":"Let's tune `n_estimators`, `max_depth` and `learning_rate` prameters of `XGBRegressor` model. (I did this step in Google Colab)","169e26aa":"There is a slight enhancement in model's performance after hyperparametrs tuning.","e1486cb0":"# Loading and exploring data\n\nTo begin, let's load some essential libraries and data from  [House Prices competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques).","9f72a906":"Now it performs even better than `Ridge()`","80e73dff":"Then we'll make three lists of feature names for our preprocessing pipeline:\n\n* categorical features with low cardinality\n\n* numerical features\n\n* skewed numerical features: features with `scipy.stats.skew` > 0.75","40190bfd":"# XGBRegressor\n\nNow let's try some gradient boosting: we'll use `XGBRegressor` model.","11eb7a21":"We have 1460 rows in training dataset and 1459 in test.\n\n* Each row in the dataset describes the characteristics of a house - 79 features in total.\n\n* Our goal is to predict the SalePrice, given these features.\n","feb303ad":"Then we'll define function to optimise - RMSE of `Ridge()` model on cross-validation.","ad0f6ffe":"There are a lot of missing values, mostly in categorical features. For most of them NA values means abscense of that feature for the house. For example:\n\n* `Alley`: Type of alley access to property. NA - No alley access.\n\n* `PoolQC`: Pool quality. NA - No Pool.\n\nLet's compare prices of houses by top-6 features with most missing values."}}