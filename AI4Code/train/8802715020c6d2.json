{"cell_type":{"aa80140b":"code","3ea189e4":"code","56a70fff":"code","e25debf0":"code","91865cab":"code","5ae461f0":"code","dd258b98":"code","50be3535":"code","0b1aff43":"code","8de6481d":"code","e5586252":"code","cf07ebdf":"code","66476aad":"code","f135e080":"code","2cd045bf":"code","3fcf29a1":"code","c9362818":"code","44b85f6e":"code","6d735e16":"code","a18cf513":"code","913fd557":"code","c08938b1":"code","f77febfd":"code","5ebd0564":"code","c37d705e":"code","b31ad8f1":"code","d75bf810":"code","fd082113":"code","b9387997":"code","4919fd46":"code","bf5986cc":"code","f048d111":"code","999016af":"code","03a1a060":"code","df657623":"code","c36ca8ab":"code","eb4d3af5":"code","2cbcfd41":"code","a8eb7494":"markdown","feefd2f8":"markdown","99f0a3e2":"markdown","9df3109a":"markdown","a3d55ab1":"markdown","9baf882b":"markdown","e4f1617e":"markdown","ceacd5aa":"markdown","6ea55fde":"markdown","ed49c50f":"markdown","ea5318e6":"markdown","bc53a420":"markdown","9e7c131a":"markdown","107dbc19":"markdown","334568e8":"markdown","8eec55e4":"markdown","853a9515":"markdown","fb22fc6e":"markdown","a6b9a33e":"markdown"},"source":{"aa80140b":"! pip install nlplot","3ea189e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nlplot\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 300)\npd.options.display.float_format = '{:.3f}'.format\npd.set_option('display.max_colwidth', 5000)","56a70fff":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')","e25debf0":"# In this case, we're going to sample and visualize 10000 pieces of data\ntrain = train.sample(n=10000, random_state=0)","91865cab":"# Convert text to lowercase\ntrain['text'] = train['text'].apply(lambda x: x.lower())","5ae461f0":"display(train.head(), train.shape)","dd258b98":"df = train.groupby('sentiment').size().reset_index(name='count')\nfig = px.bar(df, y='count', x='sentiment', text='count')\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(\n    title=str('sentiment counts'),\n    xaxis_title=str('sentiment'),\n    width=700,\n    height=500,\n    )\nfig.show()","50be3535":"# initialize\nnpt = nlplot.NLPlot(train, target_col='text')\nnpt_negative = nlplot.NLPlot(train.query('sentiment == \"negative\"'), target_col='text')\nnpt_neutral = nlplot.NLPlot(train.query('sentiment == \"neutral\"'), target_col='text')\nnpt_positive = nlplot.NLPlot(train.query('sentiment == \"positive\"'), target_col='text')","0b1aff43":"stopwords = npt.get_stopword(top_n=30, min_freq=0)\nprint(stopwords)","8de6481d":"# uni-gram\nnpt.bar_ngram(\n    title='uni-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=1,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)","e5586252":"# bi-gram\nnpt.bar_ngram(\n    title='bi-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=2,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)","cf07ebdf":"# tri-gram\nnpt.bar_ngram(\n    title='tri-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=3,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)","66476aad":"# positive\/neutral\/negative\nfig_unigram_positive = npt_positive.bar_ngram(\n    title='uni-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=1,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)\n\nfig_unigram_neutral = npt_neutral.bar_ngram(\n    title='uni-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=1,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)\n\nfig_unigram_negative = npt_negative.bar_ngram(\n    title='uni-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=1,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)","f135e080":"# subplot\ntrace1 = fig_unigram_positive['data'][0]\ntrace2 = fig_unigram_neutral['data'][0]\ntrace3 = fig_unigram_negative['data'][0]\n\nfig = make_subplots(rows=1, cols=3, subplot_titles=('positive', 'neutral', 'negative'), shared_xaxes=False)\nfig.update_xaxes(title_text='word count', row=1, col=1)\nfig.update_xaxes(title_text='word count', row=1, col=2)\nfig.update_xaxes(title_text='word count', row=1, col=3)\n\nfig.update_layout(height=1100, width=1000, title_text='unigram positive vs neutral vs negative')\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=1, col=3)\n\nfig.show()","2cd045bf":"# positive\/neutral\/negative\nfig_bigram_positive = npt_positive.bar_ngram(\n    title='bi-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=2,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)\n\nfig_bigram_neutral = npt_neutral.bar_ngram(\n    title='bi-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=2,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)\n\nfig_bigram_negative = npt_negative.bar_ngram(\n    title='bi-gram',\n    xaxis_label='word_count',\n    yaxis_label='word',\n    ngram=2,\n    top_n=50,\n    width=800,\n    height=1100,\n    stopwords=stopwords,\n)","3fcf29a1":"# subplot\ntrace1 = fig_bigram_positive['data'][0]\ntrace2 = fig_bigram_neutral['data'][0]\ntrace3 = fig_bigram_negative['data'][0]\n\nfig = make_subplots(rows=1, cols=3, subplot_titles=('positive', 'neutral', 'negative'), shared_xaxes=False)\nfig.update_xaxes(title_text='word count', row=1, col=1)\nfig.update_xaxes(title_text='word count', row=1, col=2)\nfig.update_xaxes(title_text='word count', row=1, col=3)\n\nfig.update_layout(height=1100, width=1000, title_text='bigram positive vs neutral vs negative')\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=1, col=3)\n\nfig.show()","c9362818":"npt.treemap(\n    title='All sentiment Tree of Most Common Words',\n    ngram=1,\n    stopwords=stopwords,\n)","44b85f6e":"npt_positive.treemap(\n    title='Positive Tree of Most Common Words',\n    ngram=1,\n    stopwords=stopwords,\n)","6d735e16":"npt_neutral.treemap(\n    title='Neutral Tree of Most Common Words',\n    ngram=1,\n    stopwords=stopwords,\n)","a18cf513":"npt_negative.treemap(\n    title='Negative Tree of Most Common Words',\n    ngram=1,\n    stopwords=stopwords,\n)","913fd557":"npt.word_distribution(\n    title='number of words distribution'\n)","c08938b1":"fig_wd_positive = npt_positive.word_distribution()\nfig_wd_neutral = npt_neutral.word_distribution()\nfig_wd_negative = npt_negative.word_distribution()","f77febfd":"trace1 = fig_wd_positive['data'][0]\ntrace2 = fig_wd_neutral['data'][0]\ntrace3 = fig_wd_negative['data'][0]\n\nfig = make_subplots(rows=3, cols=1, subplot_titles=('positive', 'neutral', 'negative'), shared_xaxes=True)\n\nfig.update_layout(height=1200, width=900, title_text='words distribution positive vs neutral vs negative')\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=2, col=1)\nfig.add_trace(trace3, row=3, col=1)\n\nfig.show()","5ebd0564":"# All sentiment\nnpt.wordcloud(\n    stopwords=stopwords,\n    colormap='tab20_r',\n)","c37d705e":"# positive\nnpt_positive.wordcloud(\n    stopwords=stopwords,\n    colormap='tab20_r',\n)","b31ad8f1":"# neutral\nnpt_neutral.wordcloud(\n    stopwords=stopwords,\n    colormap='tab20_r',\n)","d75bf810":"# negative\nnpt_negative.wordcloud(\n    stopwords=stopwords,\n    colormap='tab20_r',\n)","fd082113":"npt.build_graph(stopwords=stopwords, min_edge_frequency=25)\nnpt_positive.build_graph(stopwords=stopwords, min_edge_frequency=10)\nnpt_neutral.build_graph(stopwords=stopwords, min_edge_frequency=10)\nnpt_negative.build_graph(stopwords=stopwords, min_edge_frequency=10)","b9387997":"# graph data\ndisplay(\n    npt.node_df.head(),\n    npt.edge_df.head(),\n)","4919fd46":"# all data\nnpt.co_network(\n    title='All sentiment Co-occurrence network',\n    color_palette='hls',\n    width=1000,\n    height=1200,\n)","bf5986cc":"npt_positive.co_network(\n    title='Positive Co-occurrence network',\n    color_palette='hls',\n    width=1000,\n    height=1200,\n)","f048d111":"npt_neutral.co_network(\n    title='Neutral Co-occurrence network',\n    color_palette='hls',\n    width=1000,\n    height=1200,\n)","999016af":"npt_negative.co_network(\n    title='Negative Co-occurrence network',\n    color_palette='hls',\n    width=1000,\n    height=1200,\n)","03a1a060":"npt.sunburst(\n    title='All sentiment sunburst chart',\n    colorscale=True,\n    color_continuous_scale='Oryel',\n    width=1000,\n    height=800,\n)","df657623":"npt_positive.sunburst(\n    title='Positive sunburst chart',\n    colorscale=True,\n    color_continuous_scale='Oryel',\n    width=1000,\n    height=800,\n)","c36ca8ab":"npt_neutral.sunburst(\n    title='Neutral sunburst chart',\n    colorscale=True,\n    color_continuous_scale='Oryel',\n    width=1000,\n    height=800,\n)","eb4d3af5":"npt_negative.sunburst(\n    title='Negative sunburst chart',\n    colorscale=True,\n    color_continuous_scale='Oryel',\n    width=1000,\n    height=800,\n)","2cbcfd41":"npt.ldavis(num_topics=3, passes=5, save=False)","a8eb7494":"Stopword calculations can be performed.","feefd2f8":"### Comparison of each sentiment","99f0a3e2":"### positive\/neutral\/negative","9df3109a":"# import","a3d55ab1":"### positive\/neutral\/negative","9baf882b":"## Histogram of the word count","e4f1617e":"# Introduction nlplot\n\nnlplot is a package developed for the visualization of natural languages.\nIn this kernel, we will show you how to use nlplot and analyze Tweets.\n\n\n## \"nlplot\" Description\nFacilitates the visualization of natural language processing and provides quicker analysis\n\nYou can draw the following graph\n\n1. N-gram bar chart\n2. N-gram tree Map\n3. Histogram of the word count\n4. wordcloud\n5. co-occurrence networks\n6. sunburst chart\n7. pyLDAvis\n\n\uff08Tested in English and Japanese\uff09\n\n## Contributions Welcome!\n### Github \u2192 https:\/\/github.com\/takapy0210\/nlplot\n","ceacd5aa":"# Word cloud","6ea55fde":"# pyLDAvis","ed49c50f":"# Co-occurrence network","ea5318e6":"### positive\/neutral\/negative","bc53a420":"# Install","9e7c131a":"# sunburst chart","107dbc19":"# Data Loading","334568e8":"# Using nlplot","8eec55e4":"Create instances with all data and positive\/neutral\/negative","853a9515":"## N-gram bar chart","fb22fc6e":"## Tree Map","a6b9a33e":"### positive\/neutral\/negative"}}