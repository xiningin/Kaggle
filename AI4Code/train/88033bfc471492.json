{"cell_type":{"d8d8a880":"code","b01950d0":"code","15eeb251":"code","c937bfc6":"code","13ce8355":"code","df7440d3":"code","bcb4a356":"code","09b1e754":"code","c9b1e714":"code","bf730d87":"code","6fc0fe19":"code","9a898dc1":"code","f951c1a1":"code","9b975ab8":"code","95661f9e":"code","32609f50":"code","c35633bf":"code","b7fae66f":"code","916ac959":"code","7ce4fdea":"code","be6d5bb6":"code","62e921d5":"code","b6073226":"code","781ed5c6":"code","12cf234e":"code","e6337684":"code","2e189738":"code","4876547c":"code","f772b973":"code","cd4e11f6":"code","0b7ceb56":"markdown","1b14f4e9":"markdown","58afe45d":"markdown","c1310e1d":"markdown","aa42e51c":"markdown","eb4d1040":"markdown","88c98aa4":"markdown","a2308a6f":"markdown","abec649a":"markdown","69bf6665":"markdown","53733e8d":"markdown","700b9e50":"markdown"},"source":{"d8d8a880":"import datetime\nimport re,string\nimport nltk\nfrom nltk.corpus import reuters\nimport numpy as np\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import LazyCorpusLoader, CategorizedPlaintextCorpusReader\nfrom collections import Counter\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow import keras\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN,RNN, LSTM\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Flatten, Input, Dense\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n    CountVectorizer, HashingVectorizer\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport multiprocessing","b01950d0":"# https:\/\/www.kaggle.com\/alvations\/testing-1000-files-datasets-from-nltk\nreuters = LazyCorpusLoader('reuters', CategorizedPlaintextCorpusReader, \n                           '(training|test).*', cat_file='cats.txt', encoding='ISO-8859-2',\n                          nltk_data_subdir='\/kaggle\/input\/reuters\/reuters\/reuters\/')\n# https:\/\/miguelmalvarez.com\/2015\/03\/20\/classifying-reuters-21578-collection-with-python-representing-the-data\/\nreuters.words()","15eeb251":"#https:\/\/www.kaggle.com\/rblcoder\/learning-nltk-reuters-21578-benchmark-corpus\ntrain_docs = list(filter(lambda doc: doc.startswith(\"train\"),\n                        reuters.fileids()))\ntest_docs = list(filter(lambda doc: doc.startswith(\"test\"),\n                        reuters.fileids()))\ntrain_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training\/')])\ntest_documents, test_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('test\/')])","c937bfc6":"print(f\"There are {len(train_documents)} Reuters News Articles with {len(train_categories)} labels.\") \nprint(f\"The first ten labels: {train_categories[:10]}\")","13ce8355":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_categories)\ntest_labels = mlb.transform(test_categories)","df7440d3":"print(train_documents[0])","bcb4a356":"print(train_labels[0])","09b1e754":"train_labels.shape","c9b1e714":"stoplist = nltk.corpus.stopwords.words('english')\nDROP_STOPWORDS = False\nSTEMMING = False  # judgment call, parsed documents more readable if False\n\nMAX_NGRAM_LENGTH = 1  # try 1 for unigrams... 2 for bigrams... and so on\nVECTOR_LENGTH = 1000  # set vector length for TF-IDF and Doc2Vec\nSET_RANDOM = 9999\n\ncodelist = ['\\r', '\\n', '\\t']    \n\n# text parsing function for entire document string\ndef parse_doc(text):\n    text = text.lower()\n    text = re.sub(r'&(.)+', \"\", text)  # no & references  \n    text = re.sub(r'pct', 'percent', text)  # replace pct abreviation  \n    text = re.sub(r\"[^\\w\\d'\\s]+\", '', text)  # no punct except single quote \n    text = re.sub(r'[^\\x00-\\x7f]',r'', text)  # no non-ASCII strings    \n    if text.isdigit(): text = \"\"  # omit words that are all digits    \n    for code in codelist:\n        text = re.sub(code, ' ', text)  # get rid of escape codes  \n    # replace multiple spacess with one space\n    text = re.sub('\\s+', ' ', text)        \n    return text\n\ndef parse_words(text): \n    # split document into individual words\n    tokens=text.split()\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n    # remove punctuation from each word\n    tokens = [re_punc.sub('', w) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out tokens that are one or two characters long\n    tokens = [word for word in tokens if len(word) > 2]\n    # filter out tokens that are more than twenty characters long\n    tokens = [word for word in tokens if len(word) < 21]\n    # filter out stop words if requested\n    if DROP_STOPWORDS:\n        tokens = [w for w in tokens if not w in stoplist]         \n    # perform word stemming if requested\n    if STEMMING:\n        ps = PorterStemmer()\n        tokens = [ps.stem(word) for word in tokens]\n    # recreate the document string from parsed words\n    text = ''\n    for token in tokens:\n        text = text + ' ' + token\n    return tokens, text","bf730d87":"train_tokens = []  # list of token lists for gensim Doc2Vec\ntrain_text = [] # list of document strings for sklearn TF-IDF\nfor doc in train_documents:\n    text_string = doc\n    # parse the entire document string\n    text_string = parse_doc(text_string)\n    # parse words one at a time in document string\n    tokens, text_string = parse_words(text_string)\n    train_tokens.append(tokens)\n    train_text.append(text_string)","6fc0fe19":"test_tokens = []  # list of token lists for gensim Doc2Vec\ntest_text = [] # list of document strings for sklearn TF-IDF\nfor doc in test_documents:\n    text_string = doc\n    # parse the entire document string\n    text_string = parse_doc(text_string)\n    # parse words one at a time in document string\n    tokens, text_string = parse_words(text_string)\n    test_tokens.append(tokens)\n    test_text.append(text_string)\n","9a898dc1":"print('\\nNumber of training documents:',\n\tlen(train_text))\t\nprint('\\nFirst item after text preprocessing, train_text[0]\\n', \n\ttrain_text[0])\nprint('\\nNumber of training token lists:',\n\tlen(train_tokens))\t\nprint('\\nFirst list of tokens after text preprocessing, train_tokens[0]\\n', \n\ttrain_tokens[0])","f951c1a1":"indexes = [word for doc in train_tokens for word in doc]  \nprint(f\"There are {len(indexes)} words in the data corpus\")","9b975ab8":"corpus_freq = Counter(indexes).most_common()\ncorpus_freq[:25]","95661f9e":"uniques = [word for word,freq in Counter(indexes).items() if freq == 1]\nprint(f\"There are {len(uniques)} words that occur just once in the data corpus totaling 1638886 words\")","32609f50":"corpus_freq[:-26:-1]","c35633bf":"unique_indexes = list(set(indexes))\nprint(f\"There are {len(unique_indexes)} words in the data corpus that represent the vocabulary\")","b7fae66f":"vocabulary_df = pd.DataFrame(Counter(Counter(indexes).values()).most_common(),columns=['freq', 'num of words'])\nvocabulary_df.sort_values(by='freq',).head(10)","916ac959":"doc_sizes = ([len(lst) for lst in train_tokens]) # num of words in each document\nprint(f\"Number of words in Rueters News Articles: {min(doc_sizes)} to {max(doc_sizes)}\")\nprint(f\"There are a total of {sum(doc_sizes)} words in the data corpus\")","7ce4fdea":"plt.figure(figsize=[15,9])\nplt.hist(doc_sizes, bins=20,range = (0,800))\nplt.xlabel(\"Words Per Document\")\nplt.ylabel(\"Number of Reuters News Articles\")","be6d5bb6":"print('\\nBegin Doc2Vec Work')\ncores = multiprocessing.cpu_count()\nprint(\"\\nNumber of processor cores:\", cores)\n\ntrain_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_tokens)]","62e921d5":"print(\"\\nWorking on Doc2Vec vectorization, dimension 200\")\nmodel_200 = Doc2Vec(train_corpus, vector_size = 200, window = 4, \n\tmin_count = 2, workers = cores, epochs = 40)\n\nmodel_200.train(train_corpus, total_examples = model_200.corpus_count, \n\tepochs = model_200.epochs)  # build vectorization model on training set\n\n# vectorization for the training set\ndoc2vec_200_vectors = np.zeros((len(train_tokens), 200)) # initialize numpy array\nfor i in range(0, len(train_tokens)):\n    doc2vec_200_vectors[i,] = model_200.infer_vector(train_tokens[i]).transpose()\nprint('\\nTraining doc2vec_200_vectors.shape:', doc2vec_200_vectors.shape)\n# print('doc2vec_200_vectors[:2]:', doc2vec_200_vectors[:2])\n\n# vectorization for the test set\ndoc2vec_200_vectors_test = np.zeros((len(test_tokens), 200)) # initialize numpy array\nfor i in range(0, len(test_tokens)):\n    doc2vec_200_vectors_test[i,] = model_200.infer_vector(test_tokens[i]).transpose()\nprint('\\nTest doc2vec_200_vectors_test.shape:', doc2vec_200_vectors_test.shape)","b6073226":"doc2vec_200_vectors[0]","781ed5c6":"model = Sequential()\nmodel.add(Dense(512, input_shape=(200,)))\nmodel.add(Dense(units=256,activation='tanh',use_bias=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=128,activation='tanh',use_bias=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=90, activation='softmax'))","12cf234e":"model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","e6337684":"class TrainRuntimeCallback(keras.callbacks.Callback):\n\n  def on_train_begin(self,logs={}):\n    self.start = datetime.datetime.now()\n\n  def on_train_end(self,logs={}):\n    self.process_time = (datetime.datetime.now() - self.start).total_seconds()","2e189738":"train_rt = TrainRuntimeCallback()\nhistory = model.fit(doc2vec_200_vectors,\n                    train_labels,\n                    callbacks = [train_rt],\n                    epochs=5,\n                    batch_size=100,\n                    validation_split = 0.15)","4876547c":"train_time = (train_rt.process_time\/60)\nprint(f\"\\nTraining time: {train_time}\\n \")","f772b973":"loss, accuracy = model.evaluate(doc2vec_200_vectors_test, test_labels)\nprint('test set accuracy: ', accuracy * 100, \"\\n\")","cd4e11f6":"history_dict = history.history\nlosses = history.history['loss']\naccs = history.history['accuracy']\nval_losses = history.history['val_loss']\nval_accs = history.history['val_accuracy']\nepochs = len(losses)\n\nplt.figure(figsize=(12, 4))\nfor i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n    plt.subplot(1, 2, i + 1)\n    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n    plt.legend()\nplt.show()","0b7ceb56":"## Classification: Dense Neural Network","1b14f4e9":"## Further EDA","58afe45d":"# Reuters Document Classification Using Doc2Vec and Dense Neural Networks","c1310e1d":"### How many words are in each document?","aa42e51c":"### How often does each word occur?","eb4d1040":"## Doc2Vec\n\nDoc2vec is an NLP tool for representing documents as a vector and is a generalizing of the word2vec method. In order to understand doc2vec, it is advisable to understand [word2vec](https:\/\/arxiv.org\/abs\/1301.3781) approach.  A good explanation of Doc2Vec can be found [here](https:\/\/medium.com\/wisio\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e).    ","88c98aa4":"### Visualize the first document vector","a2308a6f":"## EDA","abec649a":"## Pre Processing\n\nBreak the documents into tokens, removing abbreviations, digits, escape codes, multiple spaces. Exclude tokens that are one or two characters long or more than 20 characters long, can also filter out stopwords and perform stemming if desired. ","69bf6665":"## Load Data","53733e8d":"### The top 25 most common words:","700b9e50":"### How many words occur just once in the data set?"}}