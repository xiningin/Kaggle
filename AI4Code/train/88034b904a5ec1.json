{"cell_type":{"44e068ff":"code","2d57a9c7":"code","68b4cb49":"code","a1b5028b":"code","afd6f878":"code","05e44e80":"code","d7bdc284":"code","f0015d94":"code","75fdccca":"code","337d1a37":"code","bc6b00c7":"code","124502c8":"code","b7c86a3a":"code","f5b81140":"code","c063b438":"code","5321fc5e":"code","508c5d5e":"code","b4ac5846":"code","784e50ad":"code","e1be5c3c":"code","186eeb73":"code","4b456d71":"code","dccb780a":"code","4686ed59":"code","15add8c6":"code","a9660a13":"code","df83ae45":"code","7cb03799":"markdown","b46429cc":"markdown","3b4e809f":"markdown","1da350e0":"markdown","15559bb5":"markdown","a63e23c2":"markdown","7b4d323f":"markdown","af766f9d":"markdown"},"source":{"44e068ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d57a9c7":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","68b4cb49":"pd.set_option('display.max_rows', None)","a1b5028b":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\ndef process_tweet(tweet):\n    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n    tweet2 = re.sub('https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n    tweet2 = re.sub(r'#', '', tweet2)\n    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n    tweet_tokens = tokenizer.tokenize(tweet2)\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords.words('english') and word not in string.punctuation):\n            tweets_clean.append(word)\n    stemmer = PorterStemmer()\n    tweets_stem = []\n    for word in tweets_clean:\n        stem_word = stemmer.stem(word)\n        tweets_stem.append(stem_word)\n    return \" \".join(tweets_stem)","afd6f878":"print(process_tweet('forest%20fire'))","05e44e80":"#Cleans keyword.\ndef process_keyword(keyword):\n    keyword_arr = []\n    keywords = keyword.split('%20')\n    stemmer = PorterStemmer()\n    for word in keywords:\n        word = word.lower()\n        if word not in stopwords.words('english') and word not in string.punctuation:\n            keyword_arr.append(stemmer.stem(word))\n    return \" \".join(keyword_arr)","d7bdc284":"print(process_keyword('evacuation'))","f0015d94":"#Cleans location.\ndef process_location(location):\n    #Replace short-hand\n    dictionary = {'United States': 'US', 'New York': 'NYC', 'Los Angeles': 'LA', 'D.C.': 'DC', \n                  'United Kingdom': 'UK', 'USA': 'US', 'Planet': '', 'California': 'CA', \n                  'New York City': 'NYC', 'Texas': 'TX', 'San Diego': 'SanDiego', 'South Africa': 'SouthAfrica', \n                  'Tennessee': 'TN', 'New Jersey': 'NJ'}\n    for i in dictionary.keys():\n        location = location.replace(i, dictionary[i])\n    locations = location.replace(',', '').split()\n    ret_arr = []\n    for loc in locations:\n        loc2 = loc.lower()\n        ret_arr.append(loc2)\n    return \" \".join(ret_arr)","75fdccca":"print(process_location('Los Angeles, CA'))","337d1a37":"train = train.fillna('nan')\ntest = test.fillna('nan')","bc6b00c7":"for i in range(train.shape[0]):\n    train.loc[i, 'keyword'] = process_keyword(train.loc[i, 'keyword'])\n    train.loc[i, 'location'] = process_location(train.loc[i, 'location'])\n    train.loc[i, 'text'] = process_tweet(train.loc[i, 'text'])","124502c8":"for i in range(test.shape[0]):\n    test.loc[i, 'keyword'] = process_keyword(test.loc[i, 'keyword'])\n    test.loc[i, 'location'] = process_location(test.loc[i, 'location'])\n    test.loc[i, 'text'] = process_tweet(test.loc[i, 'text'])","b7c86a3a":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,3)) # one,two and three gram vectorization\ndf_train, df_val = train_test_split(train)\nfreqs_train = tf_idf_vect.fit_transform(df_train['text'].values)\nfreqs_val = tf_idf_vect.transform(df_val['text'].values)","f5b81140":"tf_idf_key = TfidfVectorizer()\ntf_idf_loc = TfidfVectorizer()\nkey_train = tf_idf_key.fit_transform(df_train['keyword'].values)\nloc_train = tf_idf_loc.fit_transform(df_train['location'].values)\nkey_val = tf_idf_key.transform(df_val['keyword'].values)\nloc_val = tf_idf_loc.transform(df_val['location'].values)","c063b438":"print(freqs_train)","5321fc5e":"from sklearn.naive_bayes import MultinomialNB\ny_train = df_train['target'].values\nnb_text = MultinomialNB().fit(freqs_train, y_train)\nnb_key = MultinomialNB().fit(key_train, y_train)\nnb_loc = MultinomialNB().fit(loc_train, y_train)","508c5d5e":"from sklearn.metrics import classification_report\ndef printreport(exp, pred):\n    print(pd.crosstab(exp, pred, rownames=['Actual'], colnames=['Predicted']))\n    print('\\n \\n')\n    print(classification_report(exp, pred))","b4ac5846":"from sklearn.metrics import f1_score\ny_val = df_val['target'].values\ntext_weighting_arr = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.1, 1.2, 1.3, 1.4, 1.5]\nkey_weighting_arr = [0, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8]\nresult = np.zeros((len(text_weighting_arr), len(key_weighting_arr)))\nfor i in range(len(text_weighting_arr)):\n    for j in range(len(key_weighting_arr)):\n        if (text_weighting_arr[i] + key_weighting_arr[j] <= 1):\n            predicted_proba = text_weighting_arr[i] * nb_text.predict_log_proba(freqs_val) + key_weighting_arr[j] * nb_key.predict_log_proba(key_val) + (1-text_weighting_arr[i]-key_weighting_arr[j]) * nb_loc.predict_log_proba(loc_val)\n            predicted = np.argmax(predicted_proba, axis = 1)\n            result[i,j] = f1_score(y_val, predicted)\n#print(result)","784e50ad":"predicted_proba = .6 * nb_text.predict_log_proba(freqs_val) + .25 * nb_key.predict_log_proba(key_val) + .15 * nb_loc.predict_log_proba(loc_val)\npredicted = np.argmax(predicted_proba, axis = 1)\ny_val = df_val['target']\n#printreport(y_val, predicted)\n#print(f1_score(y_val, predicted))","e1be5c3c":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(ngram_range=(1,3))\ndf_train_2, df_val_2 = train_test_split(train)\nfreqs_train_2 = count_vect.fit_transform(df_train_2['text'].values)\nfreqs_val_2 = count_vect.transform(df_val_2['text'].values)","186eeb73":"from sklearn.naive_bayes import MultinomialNB\ny_train_2 = df_train_2['target'].values\nnb_2 = MultinomialNB().fit(freqs_train_2, y_train_2)","4b456d71":"predicted_2 = nb_2.predict(freqs_val_2)\ny_val_2 = df_val_2['target'].values\nprintreport(y_val_2, predicted_2)","dccb780a":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,3)) # one,two and three gram vectorization\ntf_idf_key = TfidfVectorizer()\ntf_idf_loc = TfidfVectorizer()\nfreqs_train = tf_idf_vect.fit_transform(train['text'].values)\nkey_train = tf_idf_key.fit_transform(train['keyword'].values)\nloc_train = tf_idf_loc.fit_transform(train['location'].values)\nfreqs_test = tf_idf_vect.transform(test['text'].values)\nkey_test = tf_idf_key.transform(test['keyword'].values)\nloc_test = tf_idf_loc.transform(test['location'].values)","4686ed59":"y_train = train['target'].values\nnb_text = MultinomialNB().fit(freqs_train, y_train)\nnb_key = MultinomialNB().fit(key_train, y_train)\nnb_loc = MultinomialNB().fit(loc_train, y_train)","15add8c6":"#predicted_proba = .6 * nb_text.predict_log_proba(freqs_test) + .25 * nb_key.predict_log_proba(key_test) + .15 * nb_loc.predict_log_proba(loc_test)\n#predicted = np.argmax(predicted_proba, axis = 1)\npredicted = nb_text.predict(freqs_test)","a9660a13":"#Make submission file\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = test['id']\nsubmission_df['target'] = predicted\nprint(submission_df.head())","df83ae45":"submission_df.to_csv('submission.csv',index=False)","7cb03799":"Here is a function for transforming the tweet to a more accessible format.","b46429cc":"We fill the NA data and treat it as a keyword or location, since there are quite a lot of NAs out there.","3b4e809f":"Now we train the Naive Bayes model on the whole training set.","1da350e0":"We first try tf_idf to vectorise the data.","15559bb5":"And these functions deal with the keyword and location. I have surveyed these data fields to see what changes need to be made (eg relating 'US' to 'United States').","a63e23c2":"Essentially we calculate the log probabilities for each classifier and try to combine them. We do a grid search to find the best parameters for each naive bayes classifier.","7b4d323f":"Part of the code is taken from https:\/\/www.kaggle.com\/mohitsital\/0-80777-simplest-model-naive-bayes\/data.","af766f9d":"Now we build a naive bayes classifier on each indicator: text, key and location."}}