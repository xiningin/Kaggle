{"cell_type":{"2dffaf0d":"code","75d148dd":"code","f88b3c7c":"code","e78c08df":"code","cd5d70e4":"code","25e803dc":"code","fb7bca16":"code","32af5315":"code","322aaa22":"code","d1d5ab4f":"code","d3c07e03":"code","e36f9b0e":"code","df1c7e3a":"code","3967edb6":"code","6592b277":"code","ceeceeb9":"code","5a8d4c0d":"code","a7e0543f":"code","54cd5216":"code","44a7a389":"code","7ff807df":"code","91d23280":"code","7440c973":"code","2a6e1bad":"code","30fb8340":"code","64a4e09f":"code","6dc8c126":"code","520a5177":"code","cd999f9a":"code","2bd0b3f0":"code","b39ce887":"markdown","7c505954":"markdown","0bfc22a7":"markdown","eabb0b85":"markdown","ac255eba":"markdown","d2ff3cca":"markdown","0c1aad5b":"markdown","18d5be34":"markdown","ee45b49c":"markdown","c3eb7600":"markdown","c68aff94":"markdown","fdfde84f":"markdown","ecb02b6c":"markdown","9c4b6695":"markdown","a7834f64":"markdown"},"source":{"2dffaf0d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","75d148dd":"from sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest","f88b3c7c":"ieq_data = pd.read_csv(\"ashrae_thermal_comfort_database_2.csv\", index_col='Unnamed: 0')","e78c08df":"ieq_data.head()","cd5d70e4":"ieq_data.info()","25e803dc":"ieq_data[\"ThermalSensation_rounded\"].value_counts()","fb7bca16":"ieq_data.head()","32af5315":"list(ieq_data.columns)","322aaa22":"feature_columns = [\n 'Year',\n 'Season',\n 'Climate',\n 'City',\n 'Country',\n 'Building type',\n 'Cooling startegy_building level',\n 'Sex',\n 'Clo',\n 'Met',\n 'Air temperature (C)',\n 'Relative humidity (%)',\n 'Air velocity (m\/s)']","d1d5ab4f":"features = ieq_data[feature_columns]","d3c07e03":"features.info()","e36f9b0e":"target = ieq_data['ThermalSensation_rounded']","df1c7e3a":"target.head()","3967edb6":"features_withdummies = pd.get_dummies(features)\n","6592b277":"features_withdummies.head()","ceeceeb9":"features_train, features_test, target_train, target_test = train_test_split(features_withdummies, target, test_size=0.3, random_state=2)\n","5a8d4c0d":"features_train.head()","a7e0543f":"features_train.info()","54cd5216":"features_test.info()","44a7a389":"model_rf = RandomForestClassifier(oob_score = True, max_features = 'auto', n_estimators = 100, min_samples_leaf = 2, random_state = 2)","7ff807df":"model_rf.fit(features_train, target_train)\n","91d23280":"mean_model_accuracy = model_rf.oob_score_\n\nprint(\"Model accuracy: \"+str(mean_model_accuracy))","7440c973":"#Dummy Classifier model to get a baseline\nbaseline_rf = DummyClassifier(strategy='stratified',random_state=0)\nbaseline_rf.fit(features_train, target_train)\n#DummyClassifier(constant=None, random_state=1, strategy='most_frequent')\nbaseline_model_accuracy = baseline_rf.score(features_test, target_test)\nprint(\"Model accuracy: \"+str(baseline_model_accuracy))","2a6e1bad":"y_pred = model_rf.predict(features_test)\ny_true = np.array(target_test)\ncategories = np.array(target.sort_values().unique())\nprint(classification_report(y_true, y_pred))","30fb8340":"importances = model_rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]","64a4e09f":"# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(features_withdummies.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, features_withdummies.columns[indices[f]], importances[indices[f]]))","6dc8c126":"# Plot the feature importances of the forest\nplt.figure(figsize=(15,6))\nplt.title(\"Feature Importances\")\nplt.barh(range(15), importances[indices][:15], align=\"center\")\nplt.yticks(range(15), features_withdummies.columns[indices][:15])#\nplt.gca().invert_yaxis()\nplt.tight_layout(pad=0.4)\nplt.show()\n","520a5177":"def plot_confusion_matrix(cm, categories, title='Confusion matrix', cmap='Reds'):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(categories))\n    plt.xticks(tick_marks,categories, rotation=90)\n    plt.yticks(tick_marks,categories)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","cd999f9a":"# Compute confusion matrix: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\nsns.set(font_scale=1.4)\ncm = confusion_matrix(y_true, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(cm, categories)","2bd0b3f0":"# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.figure(figsize=(12,10))\nplot_confusion_matrix(cm_normalized, categories, title='Normalized Classification Error Matrix')\nplt.show()","b39ce887":"The **target** variable is the column that we want to predict - in this case, thermal sensation. We will use the \"rounded\" version to minimize the number of categories","7c505954":"## Create dummy variables for the categories\n\nOnce again, we need to convert the categorical variables to dummy variables in order as that is the input the model expects","0bfc22a7":"## Load the IEQ Data and find a classification objective\n\nWe can constrain the data to be able to predict a certain attribute","eabb0b85":"According to the feature importance analysis, it seems that the conventional environmental metrics are the best predictors of comfort followed by the personal factors\n\n## Plot Feature Importance\n\nWe can also plot the feature importance in a line chart of the top features to get a better visual sense","ac255eba":"## Create the Train and Test Split using SK Learn\n\nNow we will create a function that will divide the data set into a random train\/test combination.","d2ff3cca":"The baseline model is only 28%, therefore our model is almost twice the accuracy at predicting the right value\n\n## Classification Report\n\nClassification is often evaluated by more than just accuracy -- there are several other metrics that are calculated to understand the success to classification. We can report that outlines the `precision`, `recall`, `f1-score`, and `support` metrics for each of the classes being predicted.","0c1aad5b":"## Classification Confusion Matrix Visualization\n\nA confusion matrix is a visualization that helps a user understand which classes are being misclassified \n\nIn this case we will look at absolute numbers of misclassifications and a normalized version of misclassification.","18d5be34":"# Introduction to Machine Learning for the Built Environment - Supervised Classification Models\n\n- Created by Clayton Miller - clayton@nus.edu.sg - miller.clayton@gmail.com\n\nThis notebook is an introduction to the machine learning concepts of classification. We will use the ASHRAE Thermal Comfort Database II data set to predict what makes a person feel comfortable","ee45b49c":"Let's use the following columns as input features for the classification model. These features will be used by the model to try to predict `ThermalSensation_rounded`.\n\nSeveral of the features are related to the building context (i.e.: `Country`, `City`), the environmental conditions (i.e.: `Air Temperature (C)`, `Relative humidity (%)`) and personal factors (i.e.: `Sex`, `Clo`, etc.)\n","c3eb7600":"## Classification Objective -- Predict Thermal Sensation using a Random Forest Model\n\nLet's use many of the other variables to predict thermal sensation as classification objective.\n\nTo do this we can use the Random Forest Classification Model. This model is a good all-purpose model that is able to ingest input features of various types. It is a form of a [decision-tree model](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).\n","c68aff94":"## Train the Random Forest Model and make the classification prediction\n\nWe now can call the Random Forest model from sklearn that was loaded before and specify various input features (or parameters) that influence the way the model is constructed.\n\nThese parameters can be optimized in order to achieve the best accuracy.","fdfde84f":"## Creating Feature and Target Data Sets\nThe first thing we need to do is create the the feature data set and the target variable.","ecb02b6c":"## Feature Importance\n\nWith Random Forest models, there is the built-in capability to calculate the **Feature Importance**. This value is calculated based on which features most contribute to accurate predictions.\n\n","9c4b6695":"The model is accurate about half the time in predicting if someone is comfortable. That seems low, but let's find where the baseline is.\n\n## Create a Baseline Model to compare the accuracy of the model\n\nSci-kit learn allows you to create a baseline which is the accuracy in just random guessing\n\n","a7834f64":"## Out-of-Bag (OOB) Error Calculation\n\nOOB is a metric to measure the accuracy of the classification to predict the right class. The fact that we have six classes to predict makes this classification a bit of challenge.\n"}}