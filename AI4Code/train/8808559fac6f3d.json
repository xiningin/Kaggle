{"cell_type":{"9722819a":"code","f955b97a":"code","b53919e6":"code","57be518a":"code","87c114ad":"code","9bee6126":"code","95a2f3e8":"code","f82bd705":"code","e88ff247":"code","6774e4f7":"code","a6409d32":"code","6416b7b7":"code","133091f1":"code","e9196f02":"code","4ccc4499":"code","09759a78":"code","095509c9":"code","07b22c98":"code","3abaface":"code","ae83718b":"code","9655f722":"code","6c1e4e29":"code","103b2336":"code","6f6a8a6f":"code","792e9660":"code","2208e0c2":"code","a07317f5":"code","fbc5fbb1":"code","8bf499ca":"code","50298810":"code","26f9b2f3":"code","a92c9b26":"code","2865d7f6":"code","8c5cc212":"code","e236e753":"code","7278de15":"code","ce69bfc2":"code","ff6ddc40":"code","e14fbfc5":"code","2254a5b5":"code","6cead8a5":"code","dc3478f9":"code","d8656475":"code","35112e2c":"code","7703b00b":"code","ab9cd2e2":"code","9f91b67a":"code","1cd7bc25":"code","8baebae4":"code","077651db":"code","e4ad943d":"code","e0265780":"code","3b11383d":"code","d1fc9e5b":"code","752698a9":"code","59650ab6":"code","73706d4b":"code","2371d4a5":"code","345d784c":"code","805b553c":"code","6b2e0b48":"code","9d59bc6a":"code","6ee98ae4":"code","30984097":"code","220053ee":"code","bc7ed20b":"code","1db841f7":"code","03eabc00":"code","9fe5d9d1":"code","9d24c4f4":"code","022ffc9d":"code","3cf3b1e6":"code","16c0723e":"code","205e75be":"code","46197955":"code","b659ff78":"code","43f573a9":"code","06f95276":"code","5e1a2d5e":"code","d736de0c":"code","a0bd8263":"code","19f61689":"code","83f63184":"code","0f3f34ca":"code","20a2f380":"code","6df39d93":"code","dd9f4294":"code","b1e93e9e":"code","c05bd51c":"code","c46cb3e8":"code","5f9f8c94":"code","2010d6bb":"code","8430062e":"code","ccbbf74f":"code","62c66525":"code","d4f9333e":"code","455efce0":"code","fbb366f5":"code","84e0927d":"code","35e9a619":"code","bf7d1207":"code","7411272a":"code","730a3f87":"code","1964ee60":"code","8e1688ba":"code","240f78ee":"code","0ffb4c87":"code","23db6be7":"code","6b1281b0":"code","cacc6f33":"markdown","0d815e22":"markdown","1eb177b8":"markdown","09108e20":"markdown","6a6c7ecf":"markdown","f0028b13":"markdown","0b20ac2c":"markdown","f7937c89":"markdown","2646bde7":"markdown","6c8e5038":"markdown","56ce26be":"markdown","9078af9d":"markdown","ecada56d":"markdown","d322cfe3":"markdown","9066836a":"markdown","63fc0189":"markdown","c124ca62":"markdown","79038209":"markdown","e345957c":"markdown","74b3e990":"markdown","74e5535d":"markdown"},"source":{"9722819a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","f955b97a":"# A code to display all the columns on your jupyter notebook\npd.set_option('display.max_columns',None)","b53919e6":"#loading my excel file\ndf = pd.read_excel('..\/input\/amcats-salary-prediction-dataset\/Salary.xlsx')\ndf = df.drop('Unnamed: 0',1)\ndf.head()","57be518a":"#understanding the shape of the data\ndf.shape","87c114ad":"# Let's check the type of columns\ndf.info()","9bee6126":"#Let's check the no. of unique values in all the columns\nprint('Unique values:')\nfor i in df.columns:\n    print(i,df[i].nunique())\n    print()","95a2f3e8":"#Dropping College and college city ID' -- they are same (around 1350 unique id's)\n#dropping columns which are unnecessary or may not be known prior to receiving a job offer.\ndf = df.drop(['ID','CollegeID','CollegeCityID','DOL','Designation','DOJ'],1)","f82bd705":"# Changing the data type as required\n\ndf['CollegeTier'] = df['CollegeTier'].astype('object')\ndf['CollegeCityTier'] = df['CollegeCityTier'].astype('object')","e88ff247":"# separating all the numerical \/ date-type features in the data\nnum_df = df.select_dtypes(exclude='object')\nnum_df.head()","6774e4f7":"#If a student hasn't opted\/attempted an optional test, the same has been mentioned as -1. I will be replacing the\n#same with 0.\n\nnum_df.loc[:,['Domain','ComputerProgramming','ElectronicsAndSemicon','ComputerScience','MechanicalEngg','ElectricalEngg','TelecomEngg','CivilEngg']]=\\\nnum_df.loc[:,['Domain','ComputerProgramming','ElectronicsAndSemicon','ComputerScience','MechanicalEngg','ElectricalEngg','TelecomEngg','CivilEngg']].replace(-1,0)","a6409d32":"#on observing the data, some graduation years were mentioned as 0\n#Replacing graduation year 0 with the modal graduation year\n\nnum_df.loc[num_df['GraduationYear']<1960,'GraduationYear'] = num_df['GraduationYear'].mode()[0]","6416b7b7":"# separating categorical features\ncat_col = df.select_dtypes(include='object')\ncat_col.head()","133091f1":"cat_col.describe()","e9196f02":"#Analysing categorical columns\n\ncat_col['10board'].value_counts(normalize=True)","4ccc4499":"board_10 = []\nfor i in cat_col['10board']:\n    if i == 'cbse' or i == 'state board':\n        board_10.append(i)\n        \n    else:\n        board_10.append('state board')\n        \ncat_col['10board'] = board_10","09759a78":"cat_col['10board'].value_counts(normalize=True)","095509c9":"cat_col['12board'].value_counts(normalize=True)","07b22c98":"board_12 = []\nfor i in cat_col['12board']:\n    if i == 'cbse' or i == 'state board':\n        board_12.append(i)\n    else:\n        board_12.append('state board')\n        \ncat_col['12board'] = board_12","3abaface":"cat_col['12board'].value_counts(normalize=True)","ae83718b":"cat_col['CollegeTier'].value_counts(normalize=True)","9655f722":"cat_col['Degree'].value_counts(normalize=True)","6c1e4e29":"cat_col['Specialization'].value_counts(normalize=True)","103b2336":"#creating categories for specialization\n\ncategory = []\nfor i in cat_col['Specialization']:\n    if i.find('electronic') != -1 or i.find('electrical') != -1 or i=='telecommunication engineering':\n        category.append('Electrical\/Electronics')\n        \n    elif i.find('computer') != -1:\n        category.append('Computers')\n    elif i == 'information technology':\n        category.append(i)\n        \n    elif i.find('mech') != -1 or i.find('auto') !=-1: #automobile\/automotive\n        category.append('Mechanical')\n        \n    elif i.find('civil') != -1 :\n        category.append('Civil Engineering')\n    else:\n        category.append('others')\n        \n        \ncat_col.loc[:,'Specialization'] = category\n\ncat_col['Specialization'].value_counts(normalize=True)","6f6a8a6f":"cat_col['JobCity'] = cat_col['JobCity'].str.lower()\ncat_col['JobCity'].value_counts(normalize=True)","792e9660":"# checking the number of unique cities\ncat_col['JobCity'].nunique()","2208e0c2":"# Cleaning the data\ncat_col['JobCity'] = cat_col['JobCity'].str.strip(' ')\ncat_col['JobCity'].nunique()","a07317f5":"job_city= []\nfor i in cat_col['JobCity']:\n    i = str(i)\n    if i.find('delhi')!= -1 or i.find('ncr')!= -1 or i.find('dehli')!= -1:\n        job_city.append('delhi')\n        \n    elif i.find('mumbai')!= -1:\n        job_city.append('mumbai')\n        \n    else:\n        job_city.append(i)\n        \ncat_col['JobCity']= job_city\ncat_col['JobCity'].nunique()","fbc5fbb1":"# In the above codes, I have brought down the no. of unique values from 261 to 231","8bf499ca":"# Creating 9 categories for city based on most frequent cities\nfinal_city = []\nfor i in cat_col['JobCity']:\n    if i in cat_col['JobCity'].value_counts().head(10).index.drop('nan'):\n        final_city.append(i)\n    else:\n        final_city.append('others') ","50298810":"cat_col['JobCity']= final_city\ncat_col['JobCity'].value_counts()","26f9b2f3":"cat_col['CollegeCityTier'].value_counts()","a92c9b26":"cat_col['CollegeState'].value_counts(normalize=True)","2865d7f6":"plt.figure(figsize = (12,8))\nnum_df.groupby('GraduationYear')['Salary'].mean().plot(color='r',marker= 'o')\nplt.title('Time series plot for mean salary')\nplt.ylabel('Mean Salary')\nplt.show()","8c5cc212":"#Feature Extraction\nnum_df['Birth Year'] = pd.to_datetime(num_df['DOB']).dt.year\nnum_df.drop('DOB',1,inplace=True)","e236e753":"num_df['Grad Age'] = num_df['GraduationYear'] - num_df['Birth Year']\nnum_df['GradAge_12th'] = num_df['12graduation'] - num_df['Birth Year']\nnum_df.drop(['12graduation','Birth Year','GraduationYear'],axis=1,inplace=True)","7278de15":"clean_df = pd.concat([cat_col,num_df],1)\nclean_df.head()","ce69bfc2":"#checking collge GPA since some students have entered a score out of 100, while others have added a score out of 10\n# new_df['collegeGPA'].describe()","ff6ddc40":"#converting the CGPA's out of 10 to % scale\nclean_df.loc[clean_df['collegeGPA'] <= 10,'collegeGPA']= \\\n(clean_df.loc[clean_df['collegeGPA'] <= 10,'collegeGPA']\/10)*100","e14fbfc5":"# outlier Analysis \n\nfor i in num_df.columns:\n    sns.boxplot(clean_df[i])\n    plt.title(i)\n    plt.show()","2254a5b5":"# These columns are optional subject columns, and if the subject has not been opted, the same has been replaced with\n# 0. Therefore, I am not considering such columns for outlier analysis since all the points other than 0 will be\n# considered as an outlier\ncols_for_outlier = clean_df[num_df.columns].columns.drop(['ComputerProgramming',\n       'ElectronicsAndSemicon', 'ComputerScience', 'MechanicalEngg',\n       'ElectricalEngg', 'TelecomEngg', 'CivilEngg'])","6cead8a5":"q1 = clean_df[cols_for_outlier].quantile(0.25)\nq3 = clean_df[cols_for_outlier].quantile(0.75)\niqr = q3-q1\nub= q3 + 1.5*iqr\nlb = q1 - 1.5*iqr\n\nnew_df = clean_df[~((clean_df[cols_for_outlier] < lb) | (clean_df[cols_for_outlier] > ub)).any(axis=1)]\n# Very neglegible decrease in the number of records\n# Removing outliers except in the following columns \n#'ComputerProgramming','ElectronicsAndSemicon', 'ComputerScience', 'MechanicalEngg','ElectricalEngg',\n#'TelecomEngg', 'CivilEngg'\n#since skew is merely because of absence of that test (which has been replaced with 0 value)","dc3478f9":"new_df.shape","d8656475":"# Storing category and numerical column names in separte variables\ncats = cat_col.columns\nnums = num_df.columns","35112e2c":"#checking the distribution post outlier analysis\nfor i in cols_for_outlier:\n    sns.boxplot(new_df[i])\n    plt.title(i)\n    plt.show()","7703b00b":"#The Outliers have been treated and the data looks more normal","ab9cd2e2":"nums","9f91b67a":"# checking the shape of numeric data\n\nfor i in nums:\n    try:\n        sns.distplot(new_df[i])\n        plt.title('{} (skewness {})'.format(i,new_df[i].skew()))\n        plt.show()\n        \n    except:\n        pass","1cd7bc25":"import plotly.graph_objects as go","8baebae4":"for i in cats:\n    \n    r = new_df[i].value_counts()\n    label = r.index\n    value = r.values\n\n    colors = ['gold', 'mediumturquoise']\n    fig = go.Figure(data=go.Pie(labels=label,values=value,marker=dict(colors=colors)))\n\n    fig.update_traces(hoverinfo='label+percent', textfont_size=20,\n                      marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n\n    fig.update_layout(font_color=\"black\",font=dict(size=18),title={\n            'text': i,\n            'y':0.9,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'})\n    fig.show()","077651db":"# Most of the engineeringg students in the data are males\n# Majority of the students in 10th and 12th standard are from the state board followed central board.\n# Tier 2 colleges dominate the data just like they dominate in the real world meaning that only a few colleges are tier 1\n# B Tech\/ B.E clearly dominates other degrees\n# The specialization has been manually categorized into 5 buckets with majority of students specializing in computer science,\n# followed by Electronics\/Electrical, IT, Mechanical and others\n# Majority of the data belongs to students who went to a college in Uttar Pradesh","e4ad943d":"for i in nums.drop('Salary'):\n    plt.figure(figsize=(7,5))\n    sns.scatterplot(i,'Salary',data=new_df,color='c')\n    plt.show()\n    \n#Data appears to be highly non linear in nature","e0265780":"#let's check the correlation heatmap for understanding the degree of linear relationship\n\nmask = np.zeros_like(new_df.corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nplt.figure(figsize=(18,12))\nsns.heatmap(new_df.corr(),annot=True,annot_kws={'color':'black'},cmap='PRGn_r',mask=mask)\nplt.show()","3b11383d":"# None of the features is showing a very strong linear relationship with the target variable salary.\n# Out of all the given variables, it appears that quant skills are the most important in terms of pearson's correlation coefficient.\n# 10th percentage is positively correlated with 12th percentage\n# Scores in AMCAT's Logical aptitude test is having a positive correlation (moderate) with English and Quant\n# In the AMCAT's personality test, it appears that openess to experience in a personality is having a 44% positive correlation with agreeableness in the candidates ","d1fc9e5b":"#Analysing Target variable with categorical features\n\nfor i in cats:\n    plt.figure(figsize=(15,5))\n    plt.title('{} vs. Salary'.format(i))\n    sns.barplot(i,'Salary',data=new_df,palette='Spectral')\n    plt.xticks(rotation=30)\n    plt.grid()\n    plt.show()\n    \n#insights - \n\n# Mumbai,Bangalore,Pune and Hyderabad have the highest entry level mean salary while it is surprising to note that cities like Delhi and kolkata have the lowest entry level salaries for engineering graduates.\n# Mean salary of males appear slighly more than females. However,this needs to confirmed statstically using t-test\n# Board appears to be having significant difference in the salary of students with CBSE having higher mean salary. The same needs to be confirmed statistically\n# Mean Salary of tier 1 collegees appear to be significantly higher than tier 2 colleges\n# Mean salary of Mtech \/ M.E and Btech \/ B.E students appear to be slighly higher than other degrees.\n# Average Starting salary of students having Specialization in Civil Engineering is higher relative to other fields","752698a9":"plt.figure(figsize=(15,5))\nplt.title('Specialization vs. Salary')\nsns.boxplot('Specialization','Salary',data=new_df,palette='Set3')\nplt.xticks()\nplt.grid()\nplt.show()","59650ab6":"import scipy.stats as stats","73706d4b":"# Statistical tests\n\n#performing t-tests\n\nfor i in cats:\n    if new_df[i].nunique()==2:\n        s1 = new_df[new_df[i]==new_df[i].unique()[0]]['Salary']\n        s2 = new_df[new_df[i]==new_df[i].unique()[1]]['Salary']\n        print(i)\n        print(stats.ttest_ind(s1,s2))\n        print()","2371d4a5":"# Inference -\n# As per t-test independent there appears to be a significant difference between Salary of Males and Females\n# there appears to be a significant difference between Salary of Students from tier 1 and tier 2\n# College City tier is also turning out to be a significant contributer to difference between Salary of Students \n# Similarly 10th and 12th board also play a significant role in salary determination with CBSE students having higher mean salary.","345d784c":"#checking impact of Specialization on salary\n\n#H0: There is no impact of Specialization on Salary\n#H1: There is a significant impact of Specialization on Salary\n\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\n\nmod = ols('Salary~Specialization',data=new_df).fit()\nsm.stats.anova_lm(mod)\n\n#Since p-value > 0.5, we fail to reject the null hypothesis and therefore it appears\n#that specialization is insignificant vaiable for salary prediction as per f-test.","805b553c":"#checking for normality of residuals\n\n#Ho: Residuals are normally distributed\n#H1: Residuals are not normally distributed.\n\nstats.shapiro(mod.resid)\n\n\n#Since p-value < 0.05, Residuals are not normally distributed and therefore we cannot apply anova in real life.\n# Although, I have used Anova to get a bird's eye view of the situation. However, the same will not be used for\n# determining significant features in case assumptions of ANOVA have not been satisfied. One should always perform non-parametric tests if the assumptions\n# of parametric tests are not satisfied.","6b2e0b48":"new_df[cats].nunique()","9d59bc6a":"for i in ['Degree']:\n    s1 = new_df[new_df[i]==new_df[i].unique()[0]]['Salary']\n    s2 = new_df[new_df[i]==new_df[i].unique()[1]]['Salary']\n    s3 = new_df[new_df[i]==new_df[i].unique()[2]]['Salary']\n    s4 = new_df[new_df[i]==new_df[i].unique()[3]]['Salary']\n    print(i)\n    print(stats.f_oneway(s1,s2,s3,s4))\n    print()\n    \n#It appears Degree plays a significant role in determining the salary of students. (pvalue < 0.05)\n# Therefore MTech. \/ M.E and BTech \/ B.E graduates have a relatively higher average salary than MCA. and Msc.(tech)","6ee98ae4":"# Let's get the dummy variables for categorical features-\n\ndf2 = pd.get_dummies(new_df,drop_first=True)","30984097":"x = df2.drop('Salary',1)\ny = df2['Salary']","220053ee":"# let's check for assumptions of linear regression\n# Before checking assumptions, let's deal with the features having high multicollinearity\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nto_drop = []\nx_copy = x.copy()\n\n\nwhile True:\n    xc = sm.add_constant(x_copy)\n    vif = [variance_inflation_factor(xc.values,i) for i in range(xc.shape[1])]\n    d = pd.DataFrame()\n    d['Features'] = xc.columns\n    d['vif'] = vif\n    d = d[1:]\n    d = d.sort_values(by='vif',ascending= False)\n    if d.iloc[0,1] > 15: #taking as per industry standards (although the ideal value is 5)\n        to_drop.append(d.iloc[0,0])\n        x_copy = x_copy.drop(d.iloc[0,0],axis=1)\n        \n    else:\n        break\n        \nprint('Features to be dropped:',to_drop)","bc7ed20b":"#In the above case, we can intuitively observe that 'Specialization in Computers'  maybe having a very high correlation \n#with independent features","1db841f7":"#dropping column with high multicollinearity\nx = df2.drop(['Salary','Specialization_Computers'],1)\ny = df2['Salary']","03eabc00":"#let's build an ols model\nimport statsmodels.api as sm\n\nxc = sm.add_constant(x)\nmod = sm.OLS(y,xc).fit()\nmod.summary()","9fe5d9d1":"# Assumption of no serial correlation\n# Durbin Watson statistic is approximately 2 which indicates no autocorrelation in the data.","9d24c4f4":"#let's check the normality of residuals\n\nsns.kdeplot(mod.resid,shade=True)\nplt.title('Residual Distribution')\nplt.show()","022ffc9d":"stats.probplot(mod.resid,plot=plt)\nplt.show()","3cf3b1e6":"# From the above plot, we can say that the residuals appear to be approximately normally distributed.","16c0723e":"# Let's analyse the regplot plot and check for the assumption of linearity and homoscedasticity\nfig, ax = plt.subplots(1,2,figsize=(18,10))\nsns.regplot(mod.predict(),mod.resid,lowess=True,color='hotpink',ax=ax[0],line_kws={'color': 'red'})\nax[0].set_xlabel('Predicted Values')\nax[0].set_ylabel('Residuals')\nax[0].set_title('Residual plot',fontweight = 'bold')\n\nsns.regplot(mod.predict(),y,lowess=True,color='orange',ax=ax[1],line_kws={'color': 'red'})\nax[1].set_xlabel('Predicted Values')\nax[1].set_ylabel('Observed Values')\nax[1].set_title('Observed vs, Actual',fontweight = 'bold')\n\nplt.show()","205e75be":"# To detect nonlinearity one can inspect plots of observed vs. predicted values or residuals vs. predicted values.\n# The desired outcome is that points are symmetrically distributed around a diagonal line in the latter plot or\n# around horizontal line in the former one. In both cases linearity of residuals can be seen\n# Further, the residual plot does not show any specific funnel-like pattern and therefore, it appears that there is \n# no heteroscedasticity in the data","46197955":"# Let's confirm these assumptions statistically\n\n#let's perform the linear rainbow test for checking linearity \n\nfrom statsmodels.stats.diagnostic import linear_rainbow,het_goldfeldquandt\n\n#H0: The portion of data is linear\n#H1: The portion of data is non linear\n\ntest_stat,pvalue = linear_rainbow(mod)\n\nprint('p-value',pvalue)\n\n#p-value > 0.05, therefore we fail to reject the null hypothesis and conclude that data is linear","b659ff78":"# Let's perform Gold Feld Quandt test for checking the assumption of homoscedasticity\n\n#H0: The residuals have equal variance (Homoscedasticity)\n#H1: The residuals have unequal variance (Heteroscedasticity)\n\nstat, pvalue,alternative = het_goldfeldquandt(mod.resid,mod.model.exog)\nprint('p-value',pvalue)\n\n#p-value > 0.05, therefore we fail to reject the null hypothesis and conclude that rediduals have equal variance","43f573a9":"#now all the assumptions of linear regression have been satisfied.\n#Let's build this baseline model in sklearn and then try to improve the model performance","06f95276":"from sklearn.linear_model import LinearRegression,RidgeCV,LassoCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import cross_val_score,GridSearchCV,KFold,train_test_split","5e1a2d5e":"#Scaling the data\n\n#dropping certain columns since they are already scaled \nto_be_scaled = x.select_dtypes(exclude='uint8').drop(['conscientiousness','agreeableness','extraversion','nueroticism','openess_to_experience'],axis=1)\nalready_scaled = x[['conscientiousness','agreeableness','extraversion','nueroticism','openess_to_experience']]\ndummies = x.select_dtypes(include='uint8')","d736de0c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nscaled = sc.fit_transform(to_be_scaled)\nscaled = pd.DataFrame(scaled,columns=to_be_scaled.columns,index=to_be_scaled.index)\nx_ = pd.concat([scaled,already_scaled],1)\nx_sc = pd.concat([x_,dummies],1)\nx_sc.head()","a0bd8263":"# let's do recursive backward feature eliminaton for selecting significant features\n\ncolumns = list(x_sc)\npmax = 1\nto_drop = []\nx_sc_reg = x_sc.copy()\nwhile len(columns)>0:\n    p = []\n    x_sc_reg = x_sc_reg[columns]\n    xc = sm.add_constant(x_sc_reg)\n    mod = sm.OLS(y,xc).fit()\n    p = pd.Series(mod.pvalues.values[1:],index=columns)\n    pmax = max(p)\n    feature_with_maxp = p.idxmax()\n    if pmax > 0.05:\n        to_drop.append(feature_with_maxp)\n        columns.remove(feature_with_maxp)\n        \n    else:\n        break\n        \nprint('Features to drop',to_drop)","19f61689":"print('Features remaining after feature selection',columns)","83f63184":"x_sc.shape","0f3f34ca":"x_sc_reg = x_sc[columns]\nx_sc_reg.shape","20a2f380":"#let's build our linear regression model and check for model performace using RMSE score\n\nlr = LinearRegression()\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(lr,x_sc_reg,y,cv=kf,scoring='neg_mean_squared_error')\nrmse1 = np.sqrt(np.abs(mse))\nbe_lr = np.mean(rmse1)\nve_lr = np.std(rmse1,ddof=1)\nbe_lr,ve_lr","6df39d93":"from sklearn.linear_model import Lasso,Ridge\nlas = Lasso(max_iter=1000)\nparams= {'alpha':np.arange(1,1000)}\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\ngsv = GridSearchCV(las,params,cv =kf,scoring='neg_mean_squared_error')\ngsv.fit(x_sc,y)\ngsv.best_params_","dd9f4294":"las = Lasso(alpha=92)\nmse = cross_val_score(las,x_sc,y,scoring='neg_mean_squared_error')\nrmse_las = np.sqrt(np.abs(mse))\nbe_las = np.mean(rmse_las)\nve_las = np.std(rmse_las,ddof=1)\nbe_las,ve_las","b1e93e9e":"# [1e-15,1e-10,1e-5,0.001,0.01,0.05,0.005,0.5,0.7,0.1,0.9,1,2,1.5,2.5,3,5,7,10,20,25,30,40,50,100,120,125,130,150]\nrid = Ridge(max_iter=1000)\nparams= {'alpha':np.arange(1,500)}\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\ngsv = GridSearchCV(rid,params,cv =kf,scoring='neg_mean_squared_error')\ngsv.fit(x_sc,y)\ngsv.best_params_","c05bd51c":"rid = Ridge(alpha=10)\nmse = cross_val_score(rid,x_sc,y,scoring='neg_mean_squared_error')\nrmse_rid = np.sqrt(np.abs(mse))\nbe_rid = np.mean(rmse_rid)\nve_rid = np.std(rmse_rid,ddof=1)\nbe_rid,ve_rid","c46cb3e8":"#Let's apply PCA on the all features for taking care of multicollinearity as well as the curse of dimensionality\n\ncov_matrix = np.cov(x_sc.T)\neigen_vals,eigen_vecs = np.linalg.eig(cov_matrix)\ntotal = np.sum(eigen_vals)\nvar = [(i\/total)*100 for i in sorted(eigen_vals,reverse=True)]\ncum_var = np.cumsum(var)\ncum_var","5f9f8c94":"plt.figure(figsize=(12,8))\nplt.bar(np.arange(1,67),var,label='Individual Explained Variance',color='g')\nplt.step(np.arange(1,67),cum_var,label = 'Cumulative Explained Variance',color='orange')\nplt.title('PCA Explained variance')\nplt.xlabel('N components')\nplt.ylabel('Explained Variance ratio')\nplt.show()","2010d6bb":"#observation- \n# The last 23 columns explain less than 1% variation in the data. There appears to be high scope\n# of dimensionality reduction","8430062e":"#let's build our linear regression model using PCA and check for model performace using RMSE score\n\nfor j in range(66,0,-1):\n    eigen_pairs = [(np.abs(eigen_vals[i]),eigen_vecs[:,i])for i in range(len(eigen_vals))]\n    eigen_pairs_sorted = sorted(eigen_pairs,reverse=True)\n    eigen_values_sorted = [i[0] for i in eigen_pairs_sorted]\n    eigen_vecs_sorted = [i[1] for i in eigen_pairs_sorted]\n    p_reduce = np.array(eigen_vecs_sorted[0:j]).transpose()\n    x_pca = np.dot(x_sc,p_reduce)\n    lr = LinearRegression()\n    kf = KFold(n_splits=5,shuffle=True, random_state=1)\n    mse = cross_val_score(lr,x_pca,y,cv=kf,scoring='neg_mean_squared_error')\n    rmse2 = np.sqrt(np.abs(mse))\n    be_lr_pca = np.mean(rmse2)\n    ve_lr_pca = np.std(rmse2,ddof=1)\n    print('{} components'.format(j))\n    print('Bias and Variance Error - {:.3f} and {:.3f}'.format(be_lr_pca,ve_lr_pca))\n    print()","ccbbf74f":"#components for PCA which explains 99% variation in the data\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.99)\npca.fit(x_sc)\nx_pca = pca.transform(x_sc)\nlr = LinearRegression()\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(lr,x_pca,y,cv=kf,scoring='neg_mean_squared_error')\nrmse2 = np.sqrt(np.abs(mse))\nbe_lr_pca = np.mean(rmse2)\nve_lr_pca = np.std(rmse2,ddof=1)\nbe_lr_pca,ve_lr_pca","62c66525":"# The same has been commented due to time and runtime constraint\n\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n# params = {'max_depth':np.arange(1,20),'min_samples_split':np.arange(2,10)}\n# kf = KFold(n_splits=5,shuffle=True, random_state=1)\n# gsv = GridSearchCV(dt,params,cv =kf,scoring='neg_mean_squared_error')\n# gsv.fit(x_sc,y)\n# gsv.best_params_","d4f9333e":"dt = DecisionTreeRegressor(max_depth=3,min_samples_split=2)\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(dt,x_sc,y,cv=kf,scoring='neg_mean_squared_error')\nrmse3 = np.sqrt(np.abs(mse))\nbe_dt = np.mean(rmse3)\nve_dt = np.std(rmse3,ddof=1)\nbe_dt,ve_dt\n\n#observations\n\n#Decision trees increase the bias error drastically. Not performing well on this data","455efce0":"#updating scores\n# let's create a score card before moving ahead\nbias_errors = [be_lr,be_lr_pca,be_las,be_rid]\nvariance_errors = [ve_lr,ve_lr_pca,ve_las,ve_rid]\ncross_val_scores = [rmse1,rmse2,rmse_las,rmse_rid]\nmodels = ['Linear Regression','Linear Regression (PCA)','Lasso Regression','Ridge Regression']\nbias_errors.append(be_dt)\nvariance_errors.append(ve_dt)\ncross_val_scores.append(rmse3)\nmodels.append('Pruned Decision Tree')","fbb366f5":"from sklearn.ensemble import AdaBoostRegressor\n\ndt_boost = AdaBoostRegressor(base_estimator=dt,random_state=1)\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(dt_boost,x_sc,y,cv=kf,scoring='neg_mean_squared_error')\nrmse4 = np.sqrt(np.abs(mse))\nbe_dt_boost = np.mean(rmse4)\nve_dt_boost = np.std(rmse4,ddof=1)\n\nbe_dt_boost,ve_dt_boost\n\n#Although the bias error has improved slighly as compared to Pruned Decision trees. However, the models\n# are still not performing well relative to linear regression","84e0927d":"#updating scores\nbias_errors.append(be_dt_boost)\nvariance_errors.append(ve_dt_boost)\ncross_val_scores.append(rmse4)\nmodels.append('Adaboost Regressor')","35e9a619":"#Let's build a simple Random forest Regressor - \n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(rf,x_sc,y,cv=kf,scoring='neg_mean_squared_error')\nrmse5 = np.sqrt(np.abs(mse))\nbe_rf = np.mean(rmse5)\nve_rf = np.std(rmse5,ddof=1)\n\nbe_rf,ve_rf","bf7d1207":"bias_errors.append(be_rf)\nvariance_errors.append(ve_rf)\ncross_val_scores.append(rmse5)\nmodels.append('Random Forest Regressor')","7411272a":"# Let's try and apply a KNN model. \n# The hyperparameter tuning has been commented due to time and runtime constraint\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# knn = KNeighborsRegressor()\n# params = {'n_neighbors':np.arange(1,101),'weights':['uniform','distance']}\n# kf = KFold(n_splits=5,shuffle=True, random_state=1)\n# gsv = GridSearchCV(knn,param_grid=params,scoring= 'neg_mean_squared_error',cv=kf)\n# gsv.fit(x_sc,y)\n# gsv.best_params_","730a3f87":"knn = KNeighborsRegressor(n_neighbors=47,weights='distance')\nkf = KFold(n_splits=5,shuffle=True, random_state=1)\nmse = cross_val_score(knn,x_sc,y,cv=kf,scoring='neg_mean_squared_error')\nrmse6 = np.sqrt(np.abs(mse))\nbe_knn = np.mean(rmse6)\nve_knn = np.std(rmse6,ddof=1)\n\nbe_knn,ve_knn","1964ee60":"bias_errors.append(be_knn)\nvariance_errors.append(ve_knn)\ncross_val_scores.append(rmse6)\nmodels.append('Tuned K Neighbors Regressor')","8e1688ba":"# 'max_depth':np.arange(1,20),'min_samples_split':np.arange(2,10),\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\nmse = cross_val_score(gb,x_sc,y,cv=kf,scoring='neg_mean_squared_error')\nrmse7 = np.sqrt(np.abs(mse))\nbe_gb = np.mean(rmse7)\nve_gb = np.std(rmse7,ddof=1)\n\nbe_gb,ve_gb","240f78ee":"bias_errors.append(be_gb)\nvariance_errors.append(ve_gb)\ncross_val_scores.append(rmse7)\nmodels.append('Gradient Boosting Regressor')","0ffb4c87":"final_score  = pd.DataFrame()\nfinal_score['Models'] = models\nfinal_score['Cross validated RMSE'] = bias_errors\nfinal_score['Variance Error'] = variance_errors\nfinal_score","23db6be7":"sc = pd.DataFrame(cross_val_scores).T\nsc.columns = models\nplt.figure(figsize=(15,10))\nsc.boxplot(rot=90)\nplt.ylabel('KFold Cross validated RMSE')\nplt.title('Final Score Card')\nplt.show()","6b1281b0":"# Linear regression out performs all the other models","cacc6f33":"<a id=\"f\"> <\/a>\n## Modelling","0d815e22":"<a id=\"f2\"> <\/a>\n## PCA ","1eb177b8":"<a id=\"h\"> <\/a>\n## Inferences \/ Conclusion","09108e20":"<a id=\"f5\"> <\/a>\n## Random Forest Regressor","6a6c7ecf":"AMEO dataset provides anonymised bio data information along with their respective skill scores and employment outcome information. As part of the challenge, the aim is to predict annual salaries of engineering graduates as well as interpreting the factors determining salaries and visualizing the dataset to infer insights.\n\n\n**DEPENDENT VARIABLES** (Here, I have chosen Salary as a target variable. Further, I have excluded, DOJ, DOL and Designation from analysis.)\n\n* Salary - Annual CTC offered to the candidate (in INR)\n* DOJ\t- Date of joining the company\n* DOL-  Date of leaving the company\n* Designation\t- Designation offered in the job!\n\n\n**INDEPENDENT VARIABLES**\n\n* JobCity\t- City in which the candidate is offered the job\n* Gender\t- Candidate's gender\n* DOB\t- Date of birth of candidate\n* 10percentage - Overall marks obtained in grade 10 examinations\n* 10board - The school board whose curriculum the candidate followed in grade 10\n* 12graduation\t- Year of graduation - senior year high school\n* 12percentage - Overall marks obtained in grade 12 examinations\n* 12board\t-The school board whose curriculum the candidate followed\n* CollegeID\t-Unique ID identifying the university\/college which the candidate attended for her\/his undergraduate\n* CollegeTier\t-Each college has been annotated as 1 or 2. The annotations have been computed from the average AMCAT scores obtained by the students in the college\/university. Colleges with an average score above a threshold as tagged as 1 and others as 2.\n* Degree\t-Degree obtained\/pursued by the candidate\n* Specialization\t-Specialization pursued by the candidate\n* CollegeGPA\t-Aggregate GPA at graduation\n* CollegeCityID\t-A unique ID to identify the city in which the college is located in.\n* CollegeCityTier\t-The tier of the city in which the college is located in. This is annotated based on the population of the cities.\n* CollegeState\t-Name of the state in which the college is located\n* GraduationYear\t-Year of graduation (Bachelor's degree)\n* English\t-Scores in AMCAT English section\n* Logical\t-Score in AMCAT Logical ability section\n* Quant\t- Score in AMCAT's Quantitative ability section\n* Domain\t-Scores in AMCAT's domain module\n* ComputerProgramming\t-Score in AMCAT's Computer programming section\n* ElectronicsAndSemicon\t-Score in AMCAT's Electronics & Semiconductor Engineering section\n* ComputerScience\t-Score in AMCAT's Computer Science section\n* MechanicalEngg\t-Score in AMCAT's Mechanical Engineering section\n* ElectricalEngg\t-Score in AMCAT's Electrical Engineering section\n* TelecomEngg\t-Score in AMCAT's Telecommunication Engineering section\n* CivilEngg\t-Score in AMCAT's Civil Engineering section\n* conscientiousness\t-Scores in one of the sections of AMCAT's personality test\n* agreeableness\t-Scores in one of the sections of AMCAT's personality test\n* extraversion\t-Scores in one of the sections of AMCAT's personality test\n* nueroticism\t-Scores in one of the sections of AMCAT's personality test\n* openess_to_experience\t-Scores in one of the sections of AMCAT's personality test!","f0028b13":"<a id=\"f1\"> <\/a>\n## Linear Regression, Lasso and Ridge Regularization","0b20ac2c":"# Table of Content\n\n1. **[Data Description](#a)**\n2. **[Data Cleaning, Outlier Treatment and Feature Extraction](#b)**\n3. **[Visualizations and Insights](#c)**\n4. **[Statistical Analysis](#d)**\n5. **[Assumptions of Linear Regression](#e)**\n6. **[Modelling](#f)**\n    - **[Linear Regression, Lasso and Ridge Regularization](#f1)**\n    - **[PCA](#f2)**\n    - **[Decision Tree with Hyperparameter tuning](#f3)**\n    - **[AdaBoost Regressor](#f4)**\n    - **[Random Forest Regressor](#f5)**\n    - **[KNN Regressor with hyperparameter tuning](#f6)**\n    - **[Gradient Boosting Regressor](#f7)**\n7. **[Final Score Comparison](#g)**\n8. **[Conclusion](#h)**","f7937c89":"A rise in entry level average salaries of students till 2010.\nsince 2010 there has been a sharp decline in the average salary every year uptil 2014.\nThe salaries increases again from 2014 to 2015.\nA sharp decline again between 2016 and 2017.","2646bde7":"<a id=\"f7\"> <\/a>\n## Gradient Boosting Regressor","6c8e5038":"### Brief about data-\n1.\tMost of the engineering students in the data are males.\n2.\tMajority of the students in 10th and 12th standard are from state board followed by CBSE board.\n3.\tTier 2 colleges dominate the data just like they dominate in the real world meaning that only a few colleges are tier 1 colleges\n4.\tB Tech\/ B.E clearly dominates other degrees in terms of frequency of occurrence\n5.\tThe specialization has been manually categorized into 5 buckets with majority of students specializing in computer science, followed by Electronics\/Electrical, IT, Mechanical and others.\n6.\tMajority of the data belongs to students who went to a college in Uttar Pradesh\n7.\tData was checked for all the assumptions of linear regression and all of them were duly satisfied! \n\n### Findings - \n\n- A rise in entry level average salaries of students from 2007 till 2010. Since 2010 there was a sharp decline in the average entry level salaries of engineering students every year up till 2014. The salaries increased again from 2014 to 2015. However, there was a sharp decline again between 2016 and 2017. This suggests that the average salaries of students have shown a sharp decline since 2010. Such a negative rate of growth is a cause for concern.\n\n- Out of all the given variables, it appears that quant skills are the most important in terms of pearson's correlation coefficient. This means higher the score of candiate in quant, higher are the chances of having better salary prospects!\n\n- 10th percentage is positively correlated with 12th percentage\n\n- Scores in AMCAT's Logical aptitude test is having a positive correlation (moderate) with English and Quant tests\n\n- In the AMCAT's personality test, it appears that openness to experience is having a 44% positive correlation with agreeableness in the candidates.\n\n- Mumbai, Bangalore, Pune and Hyderabad have the highest entry level salary while it is surprising to note that cities like Delhi and Kolkata have the lowest entry level salaries for engineering graduates. This was a shocker since these are the metropolitans!\n\n- Mean salary of Mtech \/ M.E and Btech \/ B.E students appear to be slighly higher than other degrees.\n- Another interesting insight is that lower salary quartile (first quartile) of civil engineering students is higher than any other specialization. So do advice your juniors to choose civil engineering ! \ud83d\ude03\n\n### Statistical tests -\nCertain statistical tests were performed and the results for the same have been summarized below:\n\n-\tAs per t-test independent there appears to be a significant difference between Salary of Males and Females (A Null hypothesis of equal average salary was rejected at a  2 tailed p-value of 0.01024). That is so sexist! \ud83d\ude33\n-\tThere appears to be a significant difference between Salary of Students from tier 1 and tier 2. This follows the generally believed fact that an elite college will fetch you a higher package! \n-\tSpecialization did not emerge as a differentiator of average salary among students but do remember that lower quartile of civil engineering salary is better than others!\ud83d\udc4d\n-\tDegree plays a significant role in determining the salary of students. (pvalue < 0.05). Mean salary of B-tech and M-tech degrees were higher relative to BCA and M.Sc (Tech)\n-\tIt appears both 10th and 12th boards plays a significant role in determining the salary of students. (pvalue < 0.05). It appears that CBSE board students have a better average yearly package than students from a state board.\n\n### Prediction Conclusion\nVarious linear and non linear models along with ensemble models were applied to the given dataset to predict the salary. PCA was also used to analyse if the model performance can be improved.\nSurprisingly, linear regression (without using PCA or any data transformation) turned out to be the winner. The Salary for recently graduated engineering students is predicted using K_Fold cross-validation with an error of INR 1,07,679 +\/- 3865. ","56ce26be":"<a id=\"c\"> <\/a>\n# Visualization and insights","9078af9d":"<a id=\"e\"> <\/a>\n## Assumptions of Linear Regression","ecada56d":"- It appears that PCA has not been able to improve the model performace drastically.\n\n- This calls for going ahead and checking various non linear models on the same","d322cfe3":"Another important insight is that lower salary quartile (first quartile) of civil engineering students is higher than any other specialization. ","9066836a":"<a id=\"f6\"> <\/a>\n## KNN Regressor with hyperparameter tuning","63fc0189":"<a id=\"f3\"> <\/a>\n## Decision Tree with Hyper-Parameter tuning","c124ca62":"<a id=\"f4\"> <\/a>\n## AdaBoost Regressor","79038209":"<a id=\"g\"> <\/a>\n## Final Score Comparison","e345957c":"<a id=\"b\"> <\/a>\n## Data Cleaning, Outlier Treatment and Feature Extraction","74b3e990":"<a id=\"a\"> <\/a>\n## Data Description","74e5535d":"<a id=\"d\"> <\/a>\n## Statistical Analysis"}}