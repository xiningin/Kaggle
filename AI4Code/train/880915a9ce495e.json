{"cell_type":{"1245698b":"code","cfb52384":"code","7f6a886c":"code","afa02036":"code","a85eb2f1":"code","c11616ec":"code","7a8c9aab":"code","003b45db":"code","5cc11b6a":"code","3be2c759":"code","c56686bb":"code","ecb05f92":"code","3b7b084b":"code","c3a868a7":"code","e76b9f2d":"code","c3724185":"code","60e1814d":"code","126e927b":"code","9d85ba3d":"code","7f1082e7":"code","1f6049d6":"code","f25c6a02":"code","bc5dd453":"code","ad2e2735":"code","1c9f06c6":"code","81b24ca2":"code","11405b4d":"code","8397bc11":"code","d905dd03":"code","130d365a":"code","ae103922":"code","9e926d0d":"code","f07491d5":"code","b2670530":"code","317a901d":"code","6539c349":"code","b9d835fd":"code","c16d5e37":"code","39b571bb":"code","c534d6e1":"code","9eb5d2a3":"code","537821a8":"code","2edeecec":"code","ab6e078e":"code","3e7a91ad":"code","85953d9f":"code","a72f508c":"code","adf576c1":"code","6082b571":"code","3b1cfbe0":"code","861f8e82":"code","79343a69":"code","27d631d5":"code","c7b84ca3":"code","e6c6891c":"code","0e0cbed4":"markdown","f3be68d6":"markdown","08e253fe":"markdown","1cc0f2ca":"markdown","c1fbc29a":"markdown","5c9074c5":"markdown","310211a1":"markdown","95e21b23":"markdown","5811aeaf":"markdown","f1c8d103":"markdown","cfaf2301":"markdown"},"source":{"1245698b":"#Necessary imports. All open source libraries using python 3.6\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nimport string\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import sent_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom flashtext import KeywordProcessor\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","cfb52384":"#Load the data of job bulletins into a List of text, also load the filenames (to be used later), derive\n# the job position from the filename for some preliminary analysis \n#(actual position for data dictionary will be derived from job bulletin)\ndef load_jobopening_dataset():\n\n    data_path = '..\/input\/cityofla\/CityofLA\/Job Bulletins'\n\n    texts = []\n    positions = []\n    file_names=[]\n    for fname in sorted(os.listdir(data_path)):\n        if fname.endswith('.txt'):\n            file_names.append(fname)\n            with open(os.path.join(data_path, fname),\"rb\") as f:\n                texts.append(str(f.read()))\n                positions.append((re.split(' (?=class)', fname))[0])\n    \n    #print the length of the List of text, length of file_names and positions and make sure they are all equal\n    print(len(texts))\n    print(len(positions))\n    print(len(file_names))\n\n    return (texts,positions,file_names)","7f6a886c":"job_data, positions, file_names = load_jobopening_dataset()","afa02036":"#Let us examine the first job ad\njob_data[0].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")","a85eb2f1":"exclude = set(string.punctuation) \nwpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\n#Remove unnecessary words by including them in stopword list, for ex: we already know city, los angeles \n#are going to be there and are not going to add any extra meaning, we are adding 'may' as this is showing\n#up as most frequent word and this does not carry much information\nnewStopWords = ['city','los','angele','angeles','may']\nstop_words.extend(newStopWords)\ntable = str.maketrans('', '', string.punctuation)\n\nlemma = WordNetLemmatizer()\nporter = PorterStemmer()\n\ndef normalize_document(doc):\n    #replace newline and tab chars\n    doc = doc.replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\").replace(\"\\t\",\" \") #.split(\"b'\")[1]\n    # tokenize document\n    tokens = doc.split()\n    # remove punctuation from each word\n    tokens = [w.translate(table) for w in tokens]\n    # convert to lower case\n    lower_tokens = [w.lower() for w in tokens]\n    #remove spaces\n    stripped = [w.strip() for w in lower_tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter stopwords out of document\n    filtered_tokens = [token for token in words if token not in stop_words]\n    #normalized = \" \".join(lemma.lemmatize(word) for word in filtered_tokens)\n    #join the tokens back to get the original doc\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)","c11616ec":"#apply the text normalization to list of job positions\nnorm_positions=[]\nfor text_sample in positions:\n    norm_positions.append(normalize_document(text_sample))","7a8c9aab":"#apply the text normalization to list of job ads\nnorm_corpus=[]\nfor text_sample in job_data:\n    norm_corpus.append(normalize_document(text_sample))","003b45db":"#check the first position (this is for n-gram analysis after removal of numerics, \n#for actual data dictionary numerics will be considered)\nnorm_positions[0]","5cc11b6a":"#check the first normalized job ad\nnorm_corpus[0]","3be2c759":"#get median number of words per sample from the normalized text\ndef get_num_words_per_sample(sample_texts):\n    num_words = [len(s.split()) for s in sample_texts]\n    return np.median(num_words)","c56686bb":"print(\"Median value of number of words per sample is\")\nprint(get_num_words_per_sample(norm_corpus))","ecb05f92":"#Plot length distribution of job ads in terms of number of words per sample\ndef plot_sample_length_distribution(sample_texts):\n    plt.hist([len(s.split()) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample (No. of words)')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()","3b7b084b":"plot_sample_length_distribution(norm_corpus)","c3a868a7":"#Plot freq distribution of n-grams with single words, bi-grams, tri-grams and four-grams. Plot frequency of\n#n-grams with highest frequencies occuring first, input parameters : ngram_range, \n#maximum n_grams to be considered\ndef plot_frequency_distribution_of_ngrams(sample_texts,\n                                          ngram_range=(1, 2),\n                                          num_ngrams=30):\n    \"\"\"Plots the frequency distribution of n-grams.\n\n    # Arguments\n        samples_texts: list, sample texts.\n        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n            Min and mplt are the lower and upper bound values for the range.\n        num_ngrams: int, number of n-grams to plot.\n            Top `num_ngrams` frequent n-grams will be plotted.\n    \"\"\"\n    # Create args required for vectorizing.\n    kwargs = {\n            'ngram_range': ngram_range,\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': 'word',  # Split text into word tokens.\n    }\n    vectorizer = CountVectorizer(**kwargs)\n\n    vectorized_texts = vectorizer.fit_transform(sample_texts)\n\n    # This is the list of all n-grams in the index order from the vocabulary.\n    all_ngrams = list(vectorizer.get_feature_names())\n    num_ngrams = min(num_ngrams, len(all_ngrams))\n    ngrams = all_ngrams[:num_ngrams]\n\n    # Add up the counts per n-gram ie. column-wise\n    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n\n    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n        zip(all_counts, all_ngrams), reverse=True)])\n    ngrams = list(all_ngrams)[:num_ngrams]\n    counts = list(all_counts)[:num_ngrams]\n\n    idx = np.arange(num_ngrams)\n    plt.figure(figsize=(30,20)) \n    plt.bar(idx, counts, width=0.6, color='b')\n    plt.xlabel('N-grams',fontsize=\"18\")\n    plt.ylabel('Frequencies',fontsize=\"18\")\n    plt.title('Frequency distribution of n-grams',fontsize=\"36\")\n    plt.xticks(idx, ngrams, rotation=45, fontsize=18)\n    plt.yticks(fontsize=18)\n    plt.show()","e76b9f2d":"plot_frequency_distribution_of_ngrams(norm_corpus,ngram_range=(1, 2))","c3724185":"plot_frequency_distribution_of_ngrams(norm_corpus,ngram_range=(2, 2))","60e1814d":"plot_frequency_distribution_of_ngrams(norm_corpus,ngram_range=(3, 3))","126e927b":"plot_frequency_distribution_of_ngrams(norm_corpus,ngram_range=(4, 4),num_ngrams=15)","9d85ba3d":"plot_frequency_distribution_of_ngrams(norm_positions)","7f1082e7":"plot_frequency_distribution_of_ngrams(norm_positions,ngram_range=(2, 2))","1f6049d6":"plot_frequency_distribution_of_ngrams(norm_positions,ngram_range=(3,3))","f25c6a02":"full_norm_corpus=' '.join(norm_corpus)\nstopwords = set(STOPWORDS)\nstopwords.update([\"class\", \"code\"])\n\nwordcloud = WordCloud(stopwords=stopwords,max_words=50).generate(full_norm_corpus)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","bc5dd453":"full_norm_corpus=' '.join(norm_positions)\nstopwords = set(STOPWORDS)\nstopwords.update([\"rev\"])\nwordcloud = WordCloud(stopwords=stopwords,max_words=50).generate(full_norm_corpus)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","ad2e2735":"#Let us check where the keywords like examination are occuring, let us take the first job bulletin as example\n\ntext = job_data[0].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")\nsentences = sent_tokenize(text)\nmy_sentence=[sent for sent in sentences if 'examination' in word_tokenize(sent)]\nprint(my_sentence)","1c9f06c6":"#Let us choose the sentences where the word 'examination' is occuring in the full corpus \n#and check some random sentences\nsentences_ngram=[]\nfor i in range(len(job_data)):\n    text = job_data[i].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")\n    sentences = sent_tokenize(text)\n    selected_sentence=[sent for sent in sentences if 'examination' in word_tokenize(sent)]\n    sentences_ngram.append(selected_sentence)","81b24ca2":"len(sentences_ngram)","11405b4d":"sentence = random.choice(sentences_ngram)\nsentence","8397bc11":"print(os.listdir(\"..\/input\/cityofla\/CityofLA\/Additional data\"))","d905dd03":"print(os.listdir(\"..\/input\/cityofla\/CityofLA\/Additional data\/PDFs\/2018\/December\/Dec 14\"))","130d365a":"!pip install docx2txt\nimport docx2txt\nmy_text = docx2txt.process(\"..\/input\/cityofla\/CityofLA\/Additional data\/Description of promotions in job bulletins.docx\")\nprint(my_text)","ae103922":"titles = pd.read_csv(\"..\/input\/cityofla\/CityofLA\/Additional data\/job_titles.csv\", header=None)","9e926d0d":"titles.head()\ntitle_text = titles[0]\nfor title in title_text:\n    title=title.strip()\nlen(title_text)","f07491d5":"#Let us check the distribution of length (number of words) in the Job Title\nplot_sample_length_distribution(title_text)","b2670530":"#Print job titles greater than 4 words\nfor i in range(len(title_text)):\n    title = title_text[i].split()\n    if (len(title) >=4):\n        title = ' '.join(title)\n        print(title)","317a901d":"sample_job_class = pd.read_csv(\"..\/input\/cityofla\/CityofLA\/Additional data\/sample job class export template.csv\", header=None)","6539c349":"sample_job_class","b9d835fd":"data_dict=pd.read_csv(\"..\/input\/cityofla\/CityofLA\/Additional data\/kaggle_data_dictionary.csv\")","c16d5e37":"data_dict","39b571bb":"#Let us print a job bulletin for reference, in its original form (after replacing \\\\r\\\\n)\ns = job_data[8].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\t\",\"\")\ns","c534d6e1":"def get_position(s):\n    title_match=False\n    pos = re.findall(r'(.*?)Class Code',s)\n    pos1 = re.findall(r'(.*?)Class  Code',s)\n    if (len(pos1) > 0):\n        pos = pos1\n    if (len(pos) > 0):\n        job_title= pos[0].replace(\"b'\",\"\").replace(\"b\\\"\",\"\").replace(\"'\",\"\").replace(\"\\\\\",\"\").strip()\n        #print(job_title)\n        for title in title_text:\n            if (title.replace(\"'\",\"\")==job_title):\n                title_match=True\n                break\n    \n    if(title_match==True):\n        return job_title\n    else:\n        return \"Invalid job title\"\nget_position(s)","9eb5d2a3":"def get_JobCode(s):\n    job_code = 0\n    code = re.findall(r'Class Code:(.*?)Open',s)\n    if (len(code)>0):\n        job_code= int(code[0].strip())\n    return job_code\nget_JobCode(s)","537821a8":"def get_OpenDate(s):\n    openDate = re.findall(r'Open Date:(.*?)ANNUAL',s)\n    if (len(openDate)>0):\n        openDate = openDate[0].strip()\n    return openDate\nget_OpenDate(s)","2edeecec":"def get_SalaryRange(s):\n    salaryRange = re.findall(r'ANNUAL SALARY(.*?)NOTE',s)\n    salaryRange_1 = re.findall(r'ANNUAL SALARY(.*?)DUTIES',s)\n    salaryRange_2 = re.findall(r'ANNUAL SALARY(.*?)\\(flat',s)\n    len1=0\n    len2=0\n    len3=0\n    if (len(salaryRange) > 0):\n        len1 = len(salaryRange[0])\n    if (len(salaryRange_1) > 0):\n        len2 = len(salaryRange_1[0])\n    if (len(salaryRange_2) > 0):\n        len3 = len(salaryRange_2[0])\n    if ((len1 > 0) & (len2 > 0)):\n        if (len1 < len2):\n            salaryRange = salaryRange\n        else:\n            salaryRange = salaryRange_1\n        \n    if (len(salaryRange)>0):\n        salaryRange = salaryRange[0].strip()\n        \n    \n    return salaryRange\nget_SalaryRange(s)","ab6e078e":"def get_qualification(s):\n    qual = re.findall(r'REQUIREMENTS\/MINIMUM QUALIFICATIONS(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENT\/MINIMUM QUALIFICATION(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENTS(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENT(.*?)WHERE TO APPLY',s)\n    if (len(qual)>0):\n        qual = qual[0].replace(\"\\\\'s\",\"'s\").strip()\n    else:\n        qual=\"\"\n    return qual\nqual=get_qualification(s)\nqual","3e7a91ad":"def get_educationMajor(s):\n    educationMajor=\"\"\n    sentences = sent_tokenize(qual)\n    #print(sentences)\n    selected_sentences=[sent for sent in sentences if \"major\" in word_tokenize(sent)]\n    #print(selected_sentences)\n    for i in range(len(selected_sentences)):\n        major = re.findall(r'major in(.*?),',selected_sentences[i])\n        if (len(major)>0):\n            educationMajor=major[0].strip()\n\n    return educationMajor\nmajor=get_educationMajor(qual)\nmajor","85953d9f":"def get_educationDur(s):\n    educationDur=\"\"\n    sentences = sent_tokenize(qual)\n    #print(sentences)\n    selected_sentences=[sent for sent in sentences if \"semester\" in word_tokenize(sent)]\n    #print(selected_sentences)\n    for i in range(len(selected_sentences)):\n        dur = re.findall(r'(.*?)semester',selected_sentences[i])\n        #print(dur)\n        if (len(dur)>0):\n            educationDur=dur[0]+'sememster'\n\n    return educationDur\neduDur=get_educationDur(qual)\neduDur","a72f508c":"def get_Duties(s):\n    duties = re.findall(r'DUTIES(.*?)REQUIREMENT',s)\n    duties=\"\"\n    if (len(duties)>0):\n        duties= duties[0].strip()\n    return duties\nget_Duties(s)","adf576c1":"def get_eduYrs(s):\n    keyword_processor = KeywordProcessor()\n    education_yrs=0.0\n    # keyword_processor.add_keyword(<unclean name>, <standardised name>)\n    keyword_processor.add_keyword('four-year')\n    keyword_processor.add_keyword('four years')\n    sentences = sent_tokenize(qual)\n    #print(sentences)\n    selected_sentences=[sent for sent in sentences if \"degree\" in word_tokenize(sent)]\n    #print(selected_sentences)\n    selected_sentences1=[sent for sent in sentences if \"Graduation\" in word_tokenize(sent)]\n\n    for i in range(len(selected_sentences)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences[i])\n        #print(keywords_found)\n        if (len(keywords_found) > 0):\n            education_yrs=4.0\n    for i in range(len(selected_sentences1)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences1[i])\n        #print(keywords_found)\n        if (len(keywords_found) > 0):\n            education_yrs=4.0\n             \n    \n    return education_yrs\nget_eduYrs(qual)","6082b571":"def get_expYrs(s):\n    keyword_processor = KeywordProcessor()\n    exp_yrs=0.0\n    # keyword_processor.add_keyword(<unclean name>, <standardised name>)\n    keyword_processor.add_keyword('four-year')\n    keyword_processor.add_keyword('four years')\n    keyword_processor.add_keyword('three years')\n    keyword_processor.add_keyword('one year')\n    keyword_processor.add_keyword('two years')\n    keyword_processor.add_keyword('six years')\n    sentences = sent_tokenize(qual)\n    selected_sentences=[sent for sent in sentences if \"experience\" in word_tokenize(sent)]\n    #print(selected_sentences)\n\n    for i in range(len(selected_sentences)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences[i])\n        #print(keywords_found)\n        for i in range(len(keywords_found)):\n            if keywords_found[i]=='two years':\n                exp_yrs=2.0\n            elif keywords_found[i]=='one year':\n                exp_yrs=1.0\n            elif keywords_found[i]=='three years':\n                exp_yrs=3.0\n            elif keywords_found[i]=='six years':\n                exp_yrs=6.0\n            elif keywords_found[i]=='four years':\n                exp_yrs=4.0\n            elif keywords_found[i]=='four-year':\n                exp_yrs=4.0\n                \n    return exp_yrs\nget_expYrs(s)","3b1cfbe0":"def get_DL(s):\n    dl = False\n    dl_valid = False\n    dl_State = \"\"\n    arr = ['driver', 'license']\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keyword('california')\n    if any(re.findall('|'.join(arr), qual)):\n        dl = True\n    if (dl==True):\n        sentences = sent_tokenize(qual)\n        selected_sentence=[sent for sent in sentences if \"driver\" in word_tokenize(sent)]\n        if (len(selected_sentence)>0):\n            words = selected_sentence[0].split()\n            selected_word = [word for word in words if \"valid\" in words]\n            if len(selected_word)>0:\n                dl_valid=True\n        for i in range(len(selected_sentence)):   \n            keywords_found = keyword_processor.extract_keywords(selected_sentence[i])\n            for i in range(len(keywords_found)):\n                if keywords_found[i]=='california':\n                    dl_State=\"CA\"\n                \n    if (dl_valid)==True:\n        dl_valid=\"R\"\n    else:\n        dl_valid=\"P\"\n    return dl_valid,dl_State\nget_DL(qual)","861f8e82":"from __future__ import unicode_literals, print_function\n\nimport plac\nimport spacy\n\ndef filter_spans(spans):\n    # Filter a sequence of spans so they don't contain overlaps\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n    result = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n            result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n    return result\n\n\ndef extract_entity_relations(doc,entity):\n    # Merge entities and noun chunks into one token\n    seen_tokens = set()\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n\n    relations = []\n    for money in filter(lambda w: w.ent_type_ == entity, doc):\n        if money.dep_ in (\"attr\", \"dobj\"):\n            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n            if subject:\n                subject = subject[0]\n                relations.append((subject, money))\n        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n            relations.append((money.head.head, money))\n    return relations","79343a69":"def get_Relations(TEXTS, nlp):\n    entities=[]\n    for text in TEXTS:\n        doc = nlp(text)\n        relations = extract_entity_relations(doc,\"ORG\")\n        for r1, r2 in relations:\n            #print(\"{:<10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))\n            entities.append(r1.text)\n            entities.append(r2.text)\n    imp_entities=' '.join(entities)   \n    return imp_entities\n            \nTEXTS = [qual]\nnlp = spacy.load(\"en_core_web_sm\")\nimp_entities=get_Relations([qual],nlp)\nimp_entities\n#print(\"{:<10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))","27d631d5":"job_data_export=pd.DataFrame(columns=[\"FILE_NAME\",\"JOB_CLASS_TITLE\",\"JOB_CLASS_NO\",\"REQUIREMENT_SET_ID\",\n                                      \"REQUIREMENT_SUBSET_ID\",\"JOB_DUTIES\",\n                                      \"EDUCATION_YEARS\",\"SCHOOL_TYPE\",\"EDUCATION_MAJOR\",\"EXPERIENCE_LENGTH\",\"IMP_ENTITIES_QUAL\",\n                                     \"FULL_TIME_PART_TIME\",\"EXP_JOB_CLASS_TITLE\",\"EXP_JOB_CLASS_ALT_RESP\"\n                                     ,\"EXP_JOB_CLASS_FUNCTION\",\"COURSE_COUNT\",\"COURSE_LENGTH\",\"COURSE_SUBJECT\"\n                                     ,\"MISC_COURSE_DETAILS\",\"DRIVERS_LICENSE_REQ\",\"DRIV_LIC_TYPE\",\n                                     \"ADDTL_LIC\",\"EXAM_TYPE\",\"ENTRY_SALARY_GEN\",\"ENTRY_SALARY_DWP\",\"OPEN_DATE\"])\n","c7b84ca3":"nlp = spacy.load(\"en_core_web_sm\")\n\nfor i in range(len(job_data)):\n    #print(i)\n    s = job_data[i].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\t\",\"\")\n    position = get_position(s)\n    qual = get_qualification(s)\n    DL_valid,DL_state = get_DL(qual)\n    education_yrs = get_eduYrs(qual)\n    education_major = get_educationMajor(qual)\n    job_code = get_JobCode(s)\n    openDate = get_OpenDate(s)\n    salaryRange = get_SalaryRange(s)\n    expYrs = get_expYrs(s)\n    duties = get_Duties(s)\n    imp_qual_entities=get_Relations([qual],nlp)\n    job_data_export.loc[i,\"JOB_CLASS_TITLE\"]=position\n    job_data_export.loc[i,\"FILE_NAME\"]=file_names[i]\n    job_data_export.loc[i,\"DRIVERS_LICENSE_REQ\"]=DL_valid\n    job_data_export.loc[i,\"EDUCATION_YEARS\"]=education_yrs\n    job_data_export.loc[i,\"JOB_CLASS_NO\"]=job_code\n    job_data_export.loc[i,\"OPEN_DATE\"]=openDate\n    job_data_export.loc[i,\"ENTRY_SALARY_GEN\"]=salaryRange\n    job_data_export.loc[i,\"JOB_DUTIES\"]=duties\n    job_data_export.loc[i,\"EXPERIENCE_LENGTH\"]=expYrs\n    job_data_export.loc[i,\"DRIV_LIC_TYPE\"]=DL_state\n    job_data_export.loc[i,\"EDUCATION_MAJOR\"]=education_major\n    job_data_export.loc[i,\"IMP_ENTITIES_QUAL\"]=imp_qual_entities\n    \njob_data_export.head()","e6c6891c":"job_data_export.to_csv(\"job_dictionary.csv\",index=False)\n","0e0cbed4":"Some good bi-grams can be seen here, which are **inclusive, ex: 'disability accomodation' , 'equal employment'**\nThere is a heavy **emphasis on 'minimum requirements'** as can be seen from the bi-grams **'minimum requirements','meet minimum', 'minimum qualifications'**. While this is customary for a Govt job, there is scope of improvement here to make the job ad more inclusive in nature, by removing such formal words. Suggested format is : This job requires 4 yr full time degree, however with sufficient experience this can be waived off. \n**It is interesting to note : the word 'waive' does not occur in the n-gram list.**","f3be68d6":"There are excellent **inclusive tri-grams** occuring in the most frequent list, ex: **'equal employment opportunity', 'disability accommodation form','american disabilities act'**\nThe bi-grams like **'meet minimum requirements' , 'minimum qualifications met' are a bit formal**, customary of Govt job ads. Can be made more informal to make the job ad more appealing","08e253fe":"Some words that might scare off applicants are : **written test, eligible list, minimum qualification**\n\nGood points : **disability accommodation**","1cc0f2ca":" This is just for the first job ad for the post of 311 Director, and it shows that the job ad is quite scary to someone looking at it. Instead of a **negative tone**, it could have been written like **'Candidates need to complete a quick Qualifications Questionnaire' instead of 'Applicants who fail to complete the Qualifications Questionnaire** will not be considered further in this examination, and their application will not be processed.'. The other rules could have stated in simple terms, again instead of a negative tone, it should be : 'Applicants who are already in a City position or on a reserve list can apply for this examination' **","c1fbc29a":"One obs here is that : **50% of the job ads are above 733 words (median) in length**. Its **too verbose**, most applicants might not read this in full. **There needs to be some way to reduce the length, and make the job ad as compact as possible(maybe max 200-300 words)**","5c9074c5":"****Shows Supervisor jobs, Chief jobs, Officer, Engineering, Inspector are most prevalent**, followed by electrician, analyst,associate**\n\n****Some issues are use of terms like worker, operator** ex: 'wastewater treatment operator', 'electric trouble dispatcher' maybe these can be changed like 'waste water treatment technical specialist' for better appeal**","310211a1":"**This analysis again reveals some good and not so good aspects**\nGood aspect is the presence of n-grams like ** 'equal employment opportunity employer',**\nThere is a heavy emphasis on **'ensure minimum qualifications met', 'minimum qualifications stated bulletin'**\n**However, there are presence of n-grams like 'marital status sexual orientation' ** and it is not clear why this information is needed, what is the policy of LA Govt on these, does it have a inclusive policy on all sexual orientations. There is scope of improvement here.","95e21b23":"**Some examples:** 'Only applicants that are currently or have previously worked in the Department of Water and Power in DDR Numbers 93-39121, 93-39137, 93-39120, 93-39135, 93-39119, 93-39138, 93-39010, 93-39126, 93-39002, 93-39023, 93-39026, 42-39301, or 93-39130, and meet the above noted requirement qualify to take this examination.' -** this kind of ad is too verbose and might set off applicants. **\n'This is an extremely competitive examination and a sufficient number of candidates with the highest scores will continue in the selection process.' - **Again a very scary way of posting a job ad. All exams are by nature competitive.**\n'Candidates in the examination process may file protests as provided in Sec.' - **Not a good way of portraying an exam. Why talk about protests ? Does this happen often?**","5811aeaf":"The observation here is that some of the titles are too long (>4 words). It is clear the Dept name is part of the job Title, which should not be the case**Ex of too long title: AIRPORT CHIEF INFORMATION SECURITY OFFICER. Can this be re-organized like Dept : AIRPORT, Title : CHIEF INFORMATION SECURITY OFFICER **","f1c8d103":"Observation here is : the words  **'candidates', 'examination' , 'applicants' are occurring too many times.** This probably will make the job ad very very formal, and scare away applicants.\n\nThe good point is that the word **'disability' is making in the top 20 most frequent words**, which means the job ads are inclusive.","cfaf2301":"**This notebook does analysis of the job bulletins for the city of Los Angeles, and creates \na structured csv file based on the format given, from the content of the job bulletins.\nThe analysis of the job bulletins results in number of recommendations to make the bulletins more \ninclusive and appealing to prospective applicants**"}}