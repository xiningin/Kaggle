{"cell_type":{"862a8686":"code","738d79e0":"code","0d7cf003":"code","83290a27":"code","da204a5c":"code","71269fd0":"code","c6acfaec":"code","18a310c1":"code","f47a42c8":"code","a426cb25":"code","9f015d34":"code","b5f0e3db":"code","9352a203":"code","10997fe2":"code","92f6da68":"code","a2e2bb71":"code","d076fc2e":"code","8f514f4f":"code","fb3ad39e":"code","15e8949f":"code","44082baf":"code","48d80b86":"code","8b082b20":"code","0e3fa5d8":"code","acd41910":"code","8f1f146b":"code","2d8b0a33":"code","9f306153":"code","06dc2675":"code","96abaeab":"code","374e3435":"code","27ec7306":"code","7191ee2d":"code","4441f83e":"code","9a0c2111":"code","afde21f2":"code","fd0b3c49":"code","941a1c09":"code","38123a66":"code","dd43cfdd":"code","3dfe7d90":"code","df9a7a43":"code","1c1efdc1":"code","3bdf13ff":"code","8b8d2460":"code","19f0a59c":"code","db93ae6a":"code","28ef6894":"code","c04e54dc":"code","06f6816d":"code","6dba0c3c":"code","edcb4cf7":"code","5217f6d1":"code","5bca68ff":"code","2ecf30d4":"code","9aef6cab":"code","6166dafd":"code","4e1062da":"code","9a6ed8e8":"code","f389f0a8":"code","3e1a8b0e":"code","c5540612":"code","1104d641":"code","31ef1f9a":"code","f12d4285":"code","229be40c":"code","d0ba7093":"code","0ef08622":"code","f8bfdca9":"code","2b35790a":"code","1341eb2b":"code","e41e2802":"code","ff0f4f12":"code","691384ba":"code","a1c9904f":"markdown","cf2788e2":"markdown","f1602e3a":"markdown","a6fc7ecb":"markdown","dbd15f72":"markdown","4ab34e92":"markdown","126e106b":"markdown","af4b9dee":"markdown","5ada68ef":"markdown","259a548c":"markdown","94ef5a49":"markdown","2d268a1a":"markdown","bf486009":"markdown","992c2983":"markdown","ea5051cf":"markdown","de7ea458":"markdown","11d22cbb":"markdown","0796d82a":"markdown","14ace103":"markdown","3c003045":"markdown","9f782e3f":"markdown","6f8dd010":"markdown","1e6da6e3":"markdown","9974aead":"markdown"},"source":{"862a8686":"# libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nimport seaborn as sns\nimport sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.model_selection import train_test_split","738d79e0":"# read training data\ntrain=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv')\ntest=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv')","0d7cf003":"# display first rows (transposed)\ndf=train\ndf.head(6).T","83290a27":"train.shape , test.shape","da204a5c":"# basis statistics: nominal features \ndf.describe(include=['object']).T","71269fd0":"df = train\ndf=df.rename(columns={\"Unnamed: 0\":\"ClaimNumber\",\"Unnamed: 1\":\"DateTimeOfAccident\",\"Unnamed: 3\":\"Age\",\"Unnamed: 4\":\"Gender\",\"Unnamed: 5\":\"MaritalStatus\",\"Unnamed: 6\":\"DependentChildren\",\"Unnamed: 8\":\"WeeklyWages\",\"Unnamed: 9\":\"PartTimeFullTime\",\"Unnamed: 10\":\"HoursWorkedPerWeek\",\"Unnamed: 12\":\"ClaimDescription\",\"Unnamed: 13\":\"InitialIncurredCalimsCost\",\"Unnamed: 14\":'UltimateIncurredClaimCost'},inplace=False)\ndf=df.drop([0,1])\ndf.head()","c6acfaec":"df.info()","18a310c1":"df['HoursWorkedPerWeek'].fillna(df['HoursWorkedPerWeek'].median(),inplace=True)","f47a42c8":"df['InitialIncurredCalimsCost'] = pd.to_numeric(df['InitialIncurredCalimsCost'],errors = 'coerce')","a426cb25":"df['UltimateIncurredClaimCost'] = pd.to_numeric(df['UltimateIncurredClaimCost'],errors = 'coerce')","9f015d34":"df['HoursWorkedPerWeek'] = pd.to_numeric(df['HoursWorkedPerWeek'],errors = 'coerce')","b5f0e3db":"df['Age'] = pd.to_numeric(df['Age'],errors = 'coerce')","9352a203":"df['WeeklyWages'] = pd.to_numeric(df['WeeklyWages'],errors = 'coerce')","10997fe2":"df['DaysWorkedPerWeek'] = pd.to_numeric(df['DaysWorkedPerWeek'],errors = 'coerce',downcast='integer')","92f6da68":"df['DependentsOther'] = pd.to_numeric(df['DependentsOther'],errors = 'coerce',downcast='integer')\n","a2e2bb71":"df['DependentChildren'] = pd.to_numeric(df['DependentChildren'],errors = 'coerce')","d076fc2e":"df.info()","8f514f4f":"df['WeeklyWages'].fillna(df['WeeklyWages'].mean(),inplace=True)","fb3ad39e":"df['MaritalStatus']=df['MaritalStatus'].fillna(\"s\")","15e8949f":"df['HoursWorkedPerWeek'].fillna(df['HoursWorkedPerWeek'].median(),inplace=True)","44082baf":"\n# Some date features\ndf['YearOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).year\ndf['MonthOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).month\ndf['DayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day\ndf['WeekdayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day_name()\ndf['HourOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).hour\ndf['YearReported']  = pd.DatetimeIndex(df['DateReported']).year\n\n# Reporting delay in weeks \ndf['DaysReportDelay'] = pd.DatetimeIndex(df['DateReported']).date - pd.DatetimeIndex(df['DateTimeOfAccident']).date\ndf['DaysReportDelay'] = (df['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ndf['WeeksReportDelay'] = np.floor(df['DaysReportDelay'] \/ 7.).astype(int)\ndf['WeeksReportDelay'] = np.clip(df['WeeksReportDelay'], a_max=55, a_min=None)\n\n# drop unneccessary columns\ndf.drop(['ClaimNumber','DateTimeOfAccident','DaysReportDelay','DateReported','WeekdayOfAccident'],axis=1,inplace=True)\ndf.shape","48d80b86":"# Skewness of the numerical feature distributions\nprint(df.skew())","8b082b20":"# The log1p function applies log(1+x) to all elements of the column\ndf[\"LogUltimateIncurredClaimCost\"] = np.log1p(df[\"UltimateIncurredClaimCost\"])\ndf[\"LogInitialIncurredCalimsCost\"] = np.log1p(df[\"InitialIncurredCalimsCost\"])\n\n# plot distribution: claim costs (log)\nplt.subplots(figsize=(10, 6))\nsns.distplot(df.LogUltimateIncurredClaimCost, kde=False, label='Ultimate',bins=100)\nsns.distplot(df.LogInitialIncurredCalimsCost, kde=False, label='Initial', bins=100)\nplt.xlabel('claim costs (log)')\nplt.legend()\nplt.show()","0e3fa5d8":"# search for frequent initial costs and calculate some statistics for ultimate costs (mean)\ndf['UltimateIncurredClaimCost'].groupby(df['InitialIncurredCalimsCost']).agg(['mean','median','min','count']).query('count >= 2000')","acd41910":"# Average cost factor Ultimate \/ Initial\ndf.UltimateIncurredClaimCost.sum() \/ df.InitialIncurredCalimsCost.sum()","8f1f146b":"# Scatter plot: claim costs (log)\nplt.subplots(figsize=(10, 7))\nsns.scatterplot(data=df, x=\"LogInitialIncurredCalimsCost\",y=\"LogUltimateIncurredClaimCost\")\nplt.show()","2d8b0a33":"# Scatter plot: zoom into claim costs (log)\nplt.subplots(figsize=(10, 7))\nsns.scatterplot(data=df.query('LogInitialIncurredCalimsCost > 6 and LogInitialIncurredCalimsCost < 12 and LogUltimateIncurredClaimCost < 12'), x=\"LogInitialIncurredCalimsCost\",y=\"LogUltimateIncurredClaimCost\")\nplt.show()","9f306153":"# Generate a list of numerical variables, remove claim cost variables\nnum_list = [c for c in df.columns if((df[c].dtype != np.object) and not \"Cost\" in c)] \n# plot histograms\nfor name in num_list:\n    f, ax = plt.subplots(figsize=(10, 5))\n    nbins = min(df[name].value_counts().count(),70)\n    plt.hist(data=df, x=name, bins=nbins)\n    plt.xlabel(name)\n    plt.show()","06dc2675":"# List of features with to many different values\nnum_list_bins =['Age','InitialIncurredCalimsCost','DaysWorkedPerWeek']\n\n# plot binned plot boxplots for 'LogUltimateIncurredClaimCost'\nfor name in num_list_bins:\n    f, ax = plt.subplots(figsize=(14, 5))\n    df['bin_'] = pd.cut(df[name], 8)\n    sns.boxplot(x='bin_', y='LogUltimateIncurredClaimCost', data=df)\n    plt.xlabel(name)\n    plt.show()\n\ndf.drop(['bin_'],axis=1,inplace=True)","96abaeab":"# calcuate correlation matrix\ncorrmat = df.corr()\n\n# Draw the heatmap \nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, annot=True, square=True, cmap='RdYlGn')\nplt.show()","374e3435":"# Generate a list of categorical variables and remove those with too many different values (e.g. 'ClaimDescription')\n# plot distributi\nsns.countplot(x=\"Gender\",data=df)\nplt.show()","27ec7306":"#  plot boxplots for 'LogUltimateIncurredClaimCost'\nsns.boxplot(x=\"Gender\", y=\"LogUltimateIncurredClaimCost\", data=df)\nplt.show()","7191ee2d":"sns.countplot(x=\"PartTimeFullTime\",data=df)\nplt.show()","4441f83e":"sns.boxplot(x=\"PartTimeFullTime\", y=\"LogUltimateIncurredClaimCost\", data=df)\nplt.show()","9a0c2111":"le=pre.LabelEncoder()\nlist_df=['Gender','MaritalStatus','PartTimeFullTime']\nfor x in list_df:\n  df[x]=le.fit_transform(df[x].astype(str))\n","afde21f2":"df.info()","fd0b3c49":"# dispay claim description of a) the most severe claims ...\nvars = ['UltimateIncurredClaimCost','ClaimDescription']\ndf[vars].sort_values(by='UltimateIncurredClaimCost', ascending=False).head(8)","941a1c09":"# ... and b) the least severe claims\ndf[vars].sort_values(by='UltimateIncurredClaimCost', ascending=True).head(8)","38123a66":"# search for some words and create new features\ntext = ['NECK','BACK','KNEE','FINGER','EYE','STRUCK','HAMMER','LADDER','STAIR','FELT','TRAUMA']\nfor name in text:\n    df['CD_' + name] = np.where( (df['ClaimDescription'].str.find(name) < 0), 0, 1)\n\n# some two or tree word features\ndf['CD_FOREIGN_BODY'] = np.where( (df['ClaimDescription'].str.find('FOREIGN BODY') < 0), 0, 1)\ndf['CD_BACK_STRAIN']  = np.where( (df['ClaimDescription'].str.find('BACK STRAIN') < 0), 0, 1)\ndf['CD_SOFT_TISSUE_'] = np.where( (df['ClaimDescription'].str.find('SOFT TISSUE INJURY') < 0), 0, 1)\ndf['CD_WORKPLACE_STRESS'] = np.where( (df['ClaimDescription'].str.find('WORKPLACE STRESS') < 0), 0, 1)\ndf['CD_LOWER_BACK_STRAIN'] = np.where( (df['ClaimDescription'].str.find('LOWER BACK STRAIN') < 0), 0, 1)\n\n# body side, lacerated\/laceration:\ndf['CD_LEFT_RIGHT'] = np.where( ((df['ClaimDescription'].str.find('LEFT') < 0) & (df['ClaimDescription'].str.find('RIGHT') < 0)), 0, 1)\ndf['CD_LACERAT_'] = np.where( (df['ClaimDescription'].str.find('LACERAT') < 0), 0, 1)\ndf['UltimateIncurredClaimCost'].groupby(df['CD_LACERAT_']).agg(['count','median','mean'])","dd43cfdd":"# Print the most expensive claim amounts\nvars = ['UltimateIncurredClaimCost']\nprint(df[vars].sort_values(by='UltimateIncurredClaimCost', ascending=False).head(8))","3dfe7d90":"\ndf['UltimateIncurredClaimCost'] = np.where(df['UltimateIncurredClaimCost'] > 1000000, 1000000., df['UltimateIncurredClaimCost']) * 1.000\ndf['UltimateIncurredClaimCost'].mean() ","df9a7a43":"# To avoid confusion (e.g. log or untransformed), we give the target a new name: loss\ndf['loss'] = df[\"UltimateIncurredClaimCost\"]\n# drop unneccessary columns\ndf['UltimateIncurredClaimCost'].mean() \n","1c1efdc1":"df.info()","3bdf13ff":"le=pre.LabelEncoder()\nlist_df=['Gender','MaritalStatus','PartTimeFullTime']\nfor x in list_df:\n  df[x]=le.fit_transform(df[x].astype(str))","8b8d2460":"df.info()","19f0a59c":"df.shape","db93ae6a":"# read test data\ntest=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv')\nprint('Number of rows and columns:', test.shape)","28ef6894":"test.head()","c04e54dc":"test.isnull().sum()","06f6816d":"test['InitialIncurredCalimsCost'] = pd.to_numeric(test['InitialIncurredCalimsCost'],errors = 'coerce')\ntest['HoursWorkedPerWeek'] = pd.to_numeric(test['HoursWorkedPerWeek'],errors = 'coerce',downcast='integer')\ntest['WeeklyWages'] = pd.to_numeric(test['WeeklyWages'],errors = 'coerce',downcast='integer')\ntest['Age'] = pd.to_numeric(test['Age'],errors = 'coerce')\ntest['DaysWorkedPerWeek'] = pd.to_numeric(test['DaysWorkedPerWeek'],errors = 'coerce',downcast='integer')\ntest['DependentChildren'] = pd.to_numeric(test['DependentChildren'],errors = 'coerce')\ntest['DependentsOther'] = pd.to_numeric(test['DependentsOther'],errors = 'coerce',downcast='integer')","6dba0c3c":"le=pre.LabelEncoder()\nlist_df=['Gender','MaritalStatus','PartTimeFullTime']\nfor x in list_df:\n  df[x]=le.fit_transform(df[x].astype(str))\n","edcb4cf7":"test['MaritalStatus']=test['MaritalStatus'].fillna(\"s\")","5217f6d1":"test.info()","5bca68ff":"le=pre.LabelEncoder()\nlist_df=['Gender','MaritalStatus','PartTimeFullTime']\nfor x in list_df:\n  test[x]=le.fit_transform(test[x].astype(str))\n","2ecf30d4":"# Some date features\ntest['YearOfAccident']  = pd.DatetimeIndex(test['DateTimeOfAccident']).year\ntest['MonthOfAccident']  = pd.DatetimeIndex(test['DateTimeOfAccident']).month\ntest['DayOfAccident']  = pd.DatetimeIndex(test['DateTimeOfAccident']).day\ntest['WeekdayOfAccident']  = pd.DatetimeIndex(test['DateTimeOfAccident']).day_name()\ntest['HourOfAccident']  = pd.DatetimeIndex(test['DateTimeOfAccident']).hour\ntest['YearReported']  = pd.DatetimeIndex(test['DateReported']).year\n\n# Reporting delay in weeks \ntest['DaysReportDelay'] = pd.DatetimeIndex(test['DateReported']).date - pd.DatetimeIndex(test['DateTimeOfAccident']).date\ntest['DaysReportDelay'] = (test['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ntest['WeeksReportDelay'] = np.floor(test['DaysReportDelay'] \/ 7.).astype(int)\ntest['WeeksReportDelay'] = np.clip(test['WeeksReportDelay'], a_max=55, a_min=None)\n\n# drop unneccessary columns\ntest.drop(['ClaimNumber','DateTimeOfAccident','DaysReportDelay','DateReported','WeekdayOfAccident'],axis=1,inplace=True)\ntest.shape","9aef6cab":"# Very basis text processing: claim description features\n\n# create new features for some \"cheap\" or \"expensive\" words \nfor name in text:\n    test['CD_' + name] = np.where( (test['ClaimDescription'].str.find(name) < 0), 0, 1)\n\n# some two or tree word features\ntest['CD_FOREIGN_BODY'] = np.where( (test['ClaimDescription'].str.find('FOREIGN BODY') < 0), 0, 1)\ntest['CD_BACK_STRAIN']  = np.where( (test['ClaimDescription'].str.find('BACK STRAIN') < 0), 0, 1)\ntest['CD_SOFT_TISSUE_'] = np.where( (test['ClaimDescription'].str.find('SOFT TISSUE INJURY') < 0), 0, 1)\ntest['CD_WORKPLACE_STRESS'] = np.where( (test['ClaimDescription'].str.find('WORKPLACE STRESS') < 0), 0, 1)\ntest['CD_LOWER_BACK_STRAIN'] = np.where( (test['ClaimDescription'].str.find('LOWER BACK STRAIN') < 0), 0, 1)\n\n# body side, lacerated\/laceration:\ntest['CD_LEFT_RIGHT'] = np.where( ((test['ClaimDescription'].str.find('LEFT') < 0) & (test['ClaimDescription'].str.find('RIGHT') < 0)), 0, 1)\ntest['CD_LACERAT_'] = np.where( (test['ClaimDescription'].str.find('LACERAT') < 0), 0, 1)","6166dafd":"test.info()","4e1062da":"le=pre.LabelEncoder()\nlist_df=['Gender','MaritalStatus','PartTimeFullTime']\nfor x in list_df:\n  test[x]=le.fit_transform(test[x].astype(str))\n","9a6ed8e8":"# combine, drop claim description and get dummies\ndf_all = pd.concat([df.assign(role=\"train\"), test.assign(role=\"test\")])\ndf_all.drop(['ClaimDescription'],axis=1,inplace=True)\ndf_all.drop(['UltimateIncurredClaimCost'],axis=1,inplace=True)\ndf_all.drop(['LogUltimateIncurredClaimCost'],axis=1,inplace=True)\n\ndf_all = pd.get_dummies(df_all)\n\n# seperate\ntest_dummies, df_dummies = df_all[df_all[\"role_test\"].eq(1)], df_all[df_all[\"role_train\"].eq(1)]\ndf_dummies.drop(['role_test','role_train'],axis=1,inplace=True)\ntest_dummies.drop(['role_test','role_train','loss'],axis=1,inplace=True)\ndf_dummies.shape","f389f0a8":"seed = 1234\n\n# split data in feature matrix X and label y for training and validation\nX = df_dummies.drop(['loss'], axis=1)\ny = df_dummies['loss']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)\nX_test = test_dummies\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","3e1a8b0e":"pip  install xgboost","c5540612":"!pip install lightgbm","1104d641":"!pip install catboost","31ef1f9a":"# libraries\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport time\n#import shap ","f12d4285":"# CatBoostRegressor (Default values)\ntic = time.time()\nCGB = CatBoostRegressor(logging_level='Silent')    \nCGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = np.sqrt(mean_squared_error(y_val, CGB.predict(X_val)))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(CGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","229be40c":"# CatBoost: Plot feature importance\n(pd.Series(CGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","d0ba7093":"# LGBMRegressor: Hyperparameter tuning with RandomizedSearchCV\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'n_estimators': [500], 'num_leaves': [30,40,50],'feature_fraction': [0.7]} \nLGB_random_search = RandomizedSearchCV(LGBMRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nLGB_random_search.fit(X_train,y_train)\nprint(\"Best parameters:\",LGB_random_search.best_params_)\nLGB = LGBMRegressor(**LGB_random_search.best_params_)    \nLGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = np.sqrt(mean_squared_error(y_val, LGB.predict(X_val)))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(LGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","0ef08622":"# lightGBM: Plot feature importance\n(pd.Series(LGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","f8bfdca9":"# XGBRegressor: Hyperparameter tuning with RandomizedSearchCV\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'max_depth': [3,4,5],'n_estimators': [100],'colsample_bytree': [0.5], 'subsample': [0.4], 'tree_method': [\"hist\"] } \nXGB_random_search = RandomizedSearchCV(XGBRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nXGB_random_search.fit(X_train,y_train)\nprint(\"Best parameters:\",XGB_random_search.best_params_)\nXGB = XGBRegressor(**XGB_random_search.best_params_)    \nXGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = np.sqrt(mean_squared_error(y_val, XGB.predict(X_val)))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(XGB.predict(X_val)))\nprint(\"RMSE:\" + \"%6.2f\" % result)","2b35790a":"# XGBoost: Plot feature importance\n(pd.Series(XGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","1341eb2b":"from sklearn.linear_model import Ridge\nridge = Ridge()\nridge.fit(X_train,y_train)\nprint(\"Train data :\",ridge.score(X_train,y_train))\nprint(\"Test data :\",ridge.score(X_val,y_val))\npred = ridge.predict(X_val)\nprint(np.sqrt(mean_squared_error(y_val, pred)))","e41e2802":"# Validation MSE (Blend of LightGBM and XGBoost)\npredictions_val = 0.5*(LGB.predict(X_val)+XGB.predict(X_val))\nresult = np.sqrt(mean_squared_error(y_val, predictions_val))\nprint(\"MSE:\" + \"%6.2f\" % result)","ff0f4f12":"# Scoring: Make predictions on test data and write submission file: LGB + XGB\npredictions = XGB.predict(X_test)\ndf_test_pred = pd.DataFrame({'UltimateIncurredClaimCost':predictions})\ndf_test_pred.to_csv('submission.csv',index=False)\ndf_test_pred.head()","691384ba":"sub=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')\nsub['UltimateIncurredClaimCost'] = 0.5*(LGB.predict(X_test)+XGB.predict(X_test))\nsub.to_csv('submission_cgb.csv', index = False)\nsub.head(5)\nprint(np.mean(sub['UltimateIncurredClaimCost']))","a1c9904f":"### 1.5 Visualize numerical features","cf2788e2":"The claim descriptions seem to be predictive and should be continued using modern natural language processing techniques (NLP)and PCA.","f1602e3a":"### 1.3 Basic feature engineering","a6fc7ecb":"<a id=\"ch2\"><\/a>\n## 2.  Data preperation for modeling","dbd15f72":"### 2.2 One-hot-Encoding (nominal features to dummies) ","4ab34e92":"### 2.3  Create modeling data sets","126e106b":"### 1.9 Treatment of extreme outliers","af4b9dee":"# missing value treatment","5ada68ef":"### 3.5 Model blend and scoring","259a548c":"<a id=\"ch3\"><\/a>\n## 3. Boosting Models, Validation, Scoring\n\nWe restrict ourselves here to gradient boosted regression tree models. They do not require scaling and are known to be well suited for contests with tabular data. When treating 'ClaimDescription' with NLP methods, this may change towards artificial neural networks.","94ef5a49":"### 1.2 Count and replace missing values","2d268a1a":"### 1.4 Skewness and claim costs (transform, analyze)","bf486009":"### 3.3 XGBoost: Hyperparametertuning","992c2983":"# partimefulltime","ea5051cf":"### 1.10 Define target: UltimateIncurredClaimCost","de7ea458":"### 3.2 lightGBM: Hyperparametertuning","11d22cbb":"### 1.6 Correlations","0796d82a":"### 1.7 Visualize categorical features","14ace103":"The average ultimate costs of frequent low initial costs (500,1000,1500,3500) are by a factor of 2 to 3 higher and thus remarkably higher than average (1.40). \n\nLet's see if there is a pattern in the scatter plot inital vs. ultimate costs:","3c003045":"### 1.1 Read data","9f782e3f":"# gender","6f8dd010":"### Feature engineering test data\n\n","1e6da6e3":"### 3.1 CatBoost (with default settings)","9974aead":"### 1.8 Very basic text processing\n\nIs there predictive power in claim descriptions? Let's get a first impression."}}