{"cell_type":{"3c32dc4a":"code","096cc22f":"code","a01a0164":"code","3d043a6b":"code","ed300ca3":"code","3311fd74":"code","49f944e9":"code","f12d158f":"code","701d873f":"code","4a2df6b5":"code","d34b8d4a":"code","698528f4":"code","f81f22a5":"code","721f16bc":"code","b909d116":"code","21f5c8b6":"code","580a1c28":"code","58301d28":"code","126c83a4":"code","4f9a3e8d":"code","ac24527f":"code","3922fd72":"code","b66663c4":"code","ea770476":"code","b7bab3bc":"code","04b4be5d":"code","2ba6034c":"code","610f4e6e":"code","fbb6f296":"code","ce9aa169":"code","f0bb69a1":"markdown","87551e95":"markdown","4040c9d0":"markdown","c03f6b98":"markdown","6b3f3e21":"markdown","a5d15509":"markdown","a5467b3f":"markdown","f9fd7c20":"markdown","e20ac07a":"markdown","6764211e":"markdown","e4710bf0":"markdown"},"source":{"3c32dc4a":"#Import Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings('ignore')","096cc22f":"#Setting paths\ntest_path = '..\/input\/files1\/Malaria Cells\/testing_set'\ntrain_path = '..\/input\/files1\/Malaria Cells\/training_set'\n\n#Grabbing single images to view\nuninfected_cell = '..\/input\/files1\/Malaria Cells\/training_set\/Uninfected\/C100P61ThinF_IMG_20150918_144104_cell_128.png'\npara_cell = '..\/input\/files1\/Malaria Cells\/training_set\/Parasitized\/C100P61ThinF_IMG_20150918_144104_cell_162.png'\n\nplt.figure(1, figsize = (5 , 3))\nplt.subplot(1 , 2 , 1)\nplt.imshow(imread(uninfected_cell))\nplt.title('Uninfected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(imread(para_cell))\nplt.title('Parasitized Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","a01a0164":"#check dimensions\ndim1 = []\ndim2 = []\n\nfor image_filename in os.listdir(\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\"):\n\n    img = Image.open(\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\n    d1, d2 = img.size\n    dim1.append(d1)\n    dim2.append(d2)","3d043a6b":"#we have some small images, some large ones\nsns.jointplot(x=dim1,y=dim2, kind='hist')","ed300ca3":"#Resize to\nimage_shape = (130,130,3)","3311fd74":"#To increase data points augment existing data\n#Create more blood cell images with resize, rotation etc\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimage_gen = ImageDataGenerator(rotation_range=20,#randomyl rotate 0-20 degrees\n                               width_shift_range = 0.10,#0.10: randomly change size using 0-10%\n                               height_shift_range=0.10,#0.10: randomly change size using 0-10%\n                               shear_range=0.10,\n                               zoom_range=0.10,\n                               horizontal_flip=True,\n                               rescale=1\/255,\n                               fill_mode='nearest')","49f944e9":"para_img = imread(para_cell)\n\nplt.figure(1, figsize = (5 , 3))\nplt.subplot(1 , 2 , 1)\nplt.imshow(para_img)\nplt.title('Original Image')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(image_gen.random_transform(para_img))\nplt.title('Augmented Image')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","f12d158f":"#apply transformation to data set to artificially expand dataset\nimage_gen.flow_from_directory(train_path)","701d873f":"#Creating the model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\n\n# randomly turn off 20% of neurons to reduce overfitting by randomly turning neurons off during training.\nmodel.add(Dropout(0.2))\n\n# binary: use sigmoid\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(learning_rate=2e-5),\n              metrics=['accuracy'])","4a2df6b5":"#Early stopping monitors the fit each loop and terminates training if the loss on the test set is no longer decreasing\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\n\n#training batch size\nbatch_size = 32","d34b8d4a":"model.summary()","698528f4":"#generate additional images for training\n#image_shape[:2] because we only want dim1,dim2, not c from shape\ntrain_image_gen = image_gen.flow_from_directory(train_path,target_size=image_shape[:2],\n                                                color_mode='rgb',batch_size=batch_size,\n                                                class_mode='binary')","f81f22a5":"#generate additional test images\ntest_image_gen = image_gen.flow_from_directory(test_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False)","721f16bc":"results = model.fit(train_image_gen,epochs=20,\n                              validation_data=test_image_gen,\n                              callbacks=[early_stop])","b909d116":"losses = pd.DataFrame(model.history.history)","21f5c8b6":"losses[['loss','val_loss']].plot()","580a1c28":"losses[['accuracy','val_accuracy']].plot()","58301d28":"model.evaluate_generator(test_image_gen)","126c83a4":"pred_probabilities = model.predict(test_image_gen)","4f9a3e8d":"#Since the cost of misidentifying infected vs not infected has not been quantified the model will equally weigh either of the possibilities of misidentification\npredictions = pred_probabilities > 0.50","ac24527f":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(test_image_gen.classes,predictions))","3922fd72":"confusion_matrix(test_image_gen.classes,predictions)","b66663c4":"#Parasitized cells incorrectly classified examples\nfrom tensorflow.keras.preprocessing import image\n\nfile_list = os.listdir(\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\")\nfile_index = 0\nfile_list_len = len(file_list)\nmismatch_not_found = 1\nn_found = 0\nn_to_find = 6\nidxs = np.zeros(n_to_find)\nidxs = idxs.astype('uint32')\nmisclassified=[]\nwhile n_found < n_to_find:\n\n    while file_index < file_list_len and mismatch_not_found:\n\n        image_filename = file_list[file_index]\n        img = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\n\n        my_image = image.load_img(img,target_size=image_shape)\n        my_image_para = image.img_to_array(my_image)\n        my_image_para = np.expand_dims(my_image_para, axis=0)\n        predictions = model.predict(my_image_para)\n        model_prediction = predictions.item(0)\n\n\n        if (model_prediction != 0):\n            mismatch_not_found = 0\n            misclassified.append(file_index)\n        else:\n            file_index += 1    \n\n    idxs[n_found] = file_index\n    #my_image\n    file_index += 1\n    n_found +=1\n    mismatch_not_found = 1\n    \n    image_filename = file_list[idxs[0]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image1 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs[1]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image2 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs[2]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image3 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs[3]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image4 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs[4]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image5 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs[5]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Parasitized\/\"+image_filename)\nmy_image6 = image.load_img(img,target_size=image_shape)\n\nplt.figure(1, figsize = (15 , 7))\nplt.subplot(2 , 3 , 1)\nplt.imshow(my_image1)\nplt.title('a')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 2)\nplt.imshow(my_image2)\nplt.title('b')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2, 3 , 3)\nplt.imshow(my_image3)\nplt.title('c')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 4)\nplt.imshow(my_image4)\nplt.title('d')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 5)\nplt.imshow(my_image5)\nplt.title('e')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2, 3 , 6)\nplt.imshow(my_image6)\nplt.title('f')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","ea770476":"#Uninfected cells incorrectly classified examples\nfrom tensorflow.keras.preprocessing import image\n\nfile_list = os.listdir(\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\")\nfile_index = 0\nfile_list_len = len(file_list)\nmismatch_not_found = 1\nn_found = 0\nn_to_find = 6\nidxs_u = np.zeros(n_to_find)\nidxs_u = idxs.astype('uint32')\nmisclassified=[]\nwhile n_found < n_to_find:\n\n    while file_index < file_list_len and mismatch_not_found:\n\n        image_filename = file_list[file_index]\n        img = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\n\n        my_image = image.load_img(img,target_size=image_shape)\n        my_image_para = image.img_to_array(my_image)\n        my_image_para = np.expand_dims(my_image_para, axis=0)\n        predictions = model.predict(my_image_para)\n        model_prediction = predictions.item(0)\n\n\n        if (model_prediction != 1):\n            mismatch_not_found = 0\n            misclassified.append(file_index)\n        else:\n            file_index += 1    \n\n    idxs_u[n_found] = file_index\n    #my_image\n    file_index += 1\n    n_found +=1\n    mismatch_not_found = 1\n    \nimage_filename = file_list[idxs_u[0]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image7 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs_u[1]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image8 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs_u[2]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image9 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs_u[3]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image10 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs_u[4]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image11 = image.load_img(img,target_size=image_shape)\n\nimage_filename = file_list[idxs_u[5]]\nimg = (\"\/kaggle\/input\/files1\/Malaria Cells\/testing_set\/Uninfected\/\"+image_filename)\nmy_image12 = image.load_img(img,target_size=image_shape)\n\nplt.figure(1, figsize = (15 , 7))\nplt.subplot(2 , 3 , 1)\nplt.imshow(my_image7)\nplt.title('g')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 2)\nplt.imshow(my_image8)\nplt.title('h')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2, 3 , 3)\nplt.imshow(my_image9)\nplt.title('i')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 4)\nplt.imshow(my_image10)\nplt.title('j')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2 , 3 , 5)\nplt.imshow(my_image11)\nplt.title('k')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(2, 3 , 6)\nplt.imshow(my_image12)\nplt.title('l')\nplt.xticks([]) , plt.yticks([])\n \nplt.show()","b7bab3bc":"test_image_gen.class_indices","04b4be5d":"new_image1 = '..\/input\/files1\/Malaria Cells\/single_prediction\/Parasitised.png'\nfrom tensorflow.keras.preprocessing import image\nmy_image_para = image.load_img(new_image1,target_size=image_shape)\nmy_image_para","2ba6034c":"my_image_para = image.img_to_array(my_image_para)\nmy_image_para = np.expand_dims(my_image_para, axis=0)\nmodel.predict(my_image_para)","610f4e6e":"new_image2 = '..\/input\/files1\/Malaria Cells\/single_prediction\/Uninfected.png'\nfrom tensorflow.keras.preprocessing import image\nmy_image_uninfected = image.load_img(new_image2,target_size=image_shape)\nmy_image_uninfected","fbb6f296":"my_image_uninfected = image.img_to_array(my_image_uninfected)\nmy_image_uninfected = np.expand_dims(my_image_uninfected, axis=0)\nmodel.predict(my_image_uninfected)","ce9aa169":"from tensorflow.keras.models import load_model\nmodel.save('.\/malaria_cnn.h5')\nmodel.save('\/kaggle\/working\/malaria_cnn1.h5')\nmodel.save('.\/malaria_cnn2.h5')\nmodel.save_weights('\/kaggle\/working\/malaria_weights.h5')","f0bb69a1":"# Holdout Set","87551e95":"# Introduction\nThroughout history one of the deadliest diseases humanity has faced is malaria. It was dubbed the \"single biggest killer of humans in history\" by the YouTube channel, 'Kurzgesagt - In a nutshell', who published a great video on its transmission and the ways scientists today are trying to fight it. However, before any treatment plan can be recommended the infection needs to identified and the condition diagnosed. This fact, combined with the disease\u2019s severity and prevalence, means that an accurate, fast, and inexpensive detection method should be available to everyone and not just people from first world countries. \n\nThe CDC recognizes this, and recommends malaria be promptly treated to prevent further spread in the community as well as to prevent severe disease and potentially death for the patient. However, at the same time they highly discourage \"presumptive treatment\" (treatment without a confirmed diagnosis,) as overuse of antimalarial drugs may lead to drug-resistant strains as well as unwanted side effects for the patient.\n\nDiagnostic tests can be performed through either blood smear microscopy or PCR testing. PCR testing is more sensitive and specific than microscopy, but the testing is more expensive, and the results are not available as fast as microscopy. \n\nBecause of this, it is proposed that as part of a standard medical screening for populations in high-risk areas of malaria that a portion of blood drawn be devoted to an automated batch microscopy scanning by computer. Automating the task means testing could be implemented for large population sizes at little cost and only requiring minimal personnel with no more than technician level training. Outbreaks could be identified early, and human incurred disability could be minimized.\n\nThis could be implemented using machine learning and artificial neural networks. To show the feasibility, a model CNN will be trained using a small data set and only minimal computer resources. The dataset used in training the model contains images of blood cell slides with and without parasitic infection. Before real-world implementation, a larger dataset and additional training processor-hours would be required. However, even on this small scale the results prove promising. A well-trained model hosted in the cloud could potentially enable any cellphone with an internet connection to function as a malaria testing device.\n\n\nStained thin blood smear slides from 150 infected and 50 healthy patients were collected and photographed by the  Chittagong Medical College Hospital in Bangladesh. These\nThe data with images of infected blood cells, as well as uninfected blood cells. Our goal is to create a model that can differentiate between the two. Let's visualize the two side-by-side:\n\n\nSources:\n\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude RJ, Jaeger S, Thoma GR. 2018. Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images. PeerJ 6:e4568 https:\/\/doi.org\/10.7717\/peerj.4568 \n\nhttps:\/\/www.youtube.com\/watch?v=TnzcwTyr6cE\n\nhttps:\/\/www.cdc.gov\/malaria\/diagnosis_treatment\/clinicians1.html#eval\n\n\n","4040c9d0":"# Data Preprocessing\nIn order to fit a CNN model we need to make sure the dimensions (width & height) of each image are the same. Below, we will check the dimensions of all images, and then choose a final dimension to apply to all images prior to training the model. Having the same input picture size is important, as this determines the architecture of our CNN model. From the joint distribution plot below, we see that most pictures have height and width of around 130 pixels, so this is the reshaping size we select.","c03f6b98":"This simple CNN model achieved over 93% accuracy on the test images, and was still seeing improvements based on the declining loss function. The confusion matrix shows that the model achieved a recall score of 92% for detecting a parasitized cell (0).","6b3f3e21":"Convolutional Neural Net (CNN) models are great at image recognition and widely used for this task. CNN's scan over an image using a filter, usually 3x3 pixels, and determine how similar the current image scan area is to the filter. This helps them perform better at image recognition since they are looking at a region of NxN pixels, vs one pixel, so they can pick up on correlations in images. In contrast, if a regular neural net model were fit to an image, it would skip the feature extraction process, and just save the image inputs as a flattened layer using their pixel values as is. This would lead to loss of nearby pixel information, as well as make the input layer much larger (NxN).\n\n\n![CNN Architecture](https:\/\/raw.githubusercontent.com\/MnNqB\/Malaria_CNN\/main\/CNN%20Architecture_MNB.PNG)\n\nThis model has three convolutional layers, each using rectified linear activation units (ReLU). The convolutional layer applies different filters to try and detect patterns, and outputs a feature map (dot product). The ReLU activation returns only positive values while setting any negative dot products to zero. ReLU activations are widely used because they are more computationally efficient and show better convergence performance than other activation functions, such as the sigmoid function. Each convolutional layer's output is fed into a pooling layer. Next, a Max Pooling layer is applied, which only keeps the maximum output from each quarter of the feature map. This reduced output is fed into the next layer, and eventually to the dense layer.\n\nTo demonstrate a simple image and filter convolution, see CNN hand calculation below. A random filter is selected and applied using a dot product to the picture. A bias term is added, and the final value is added to the feature map. The process is started over after making the appropriate stride. In this model the stride is set to the default of 1.\n\n![CNN hand calc](https:\/\/raw.githubusercontent.com\/MnNqB\/Malaria_CNN\/main\/CNN%20Example1.gif)\n\nThis makes it easy to see how information from a large picture can be compressed down using CNNs. A regular neural net would have fed all 36 pixels to the dense layer, whereas this approach only feeds just 4. Real images are sized a lot more than 6x6, so CNN\u2019s scale much better for real images.\n\nOnce the image and filter dot product is calculated, a bias term is added. Once this is done across the whole image the feature map is done. Next, the ReLU activation is applied which sets any negative numbers to zero. Finally, max pooling is applied followed by the flattening layer, which outputs all values into one column (vector). This is what gets fed into the Dense (neural net) layer. \n\n\n![CNN Hand Calc2](https:\/\/raw.githubusercontent.com\/MnNqB\/Malaria_CNN\/main\/CNN%20Example2.gif)\n\n\nFor more information please check out 'StatQuests' video on CNNs: \nhttps:\/\/www.youtube.com\/watch?v=HGwBXDKFk9I&t=599s","a5d15509":"<h3>Adding Augmented Images<\/h3>\nCNN models need a lot of training images to be able to create robust models. Instead of collecting more blood cells from people, we can create augmented data points using existing images by applying transformations on them, such as rotation or resizing them. Below you can see an original picture next to a slightly modified one.","a5467b3f":"# Misclassified Images\nNext, some of the images the model misclassified will be discussed. From the first panel (images a-f), there are six pictures from the Parasitized folder which the model classified as uninfected. However, most of these pictures do look uninfected, so in this case it is likely the images were mislabeled by the lab, and the model correctly classified them as not infected.\n\nThe next 6 image panel shows uninfected cells, which the model labelled as infected. Several of these images have a different color scheme than what the model expected. \n\nPotentially some images have been misclassified by the lab, and some images are not in 'rgb' mode (which is what the model is expecting.) This suggests cleaning up the data may lead to a higher accuracy rate.\n","f9fd7c20":"# Modeling","e20ac07a":"# Training","6764211e":"# Model Evaluation","e4710bf0":"Finally, the model was tested on the two held-out images, and the model correctly classified both images. First is a parasitized cell and was correctly identified as 0 (parasitized). Second is an uninfected cell, which was also correctly classified as 1 (uninfected)."}}