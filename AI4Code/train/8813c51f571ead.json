{"cell_type":{"dee1cca8":"code","ccdfad58":"code","4b3bf077":"code","1b157502":"code","0da9acce":"code","97174cc2":"code","90494148":"code","95c4e1a4":"code","a3fc5475":"code","241f9024":"code","75bfc4b7":"code","91d9be9c":"code","4bb7e353":"code","1014f93e":"code","f75f23ad":"code","9e9d42b2":"code","7a9a6d37":"code","b64a3d44":"code","f129d07a":"code","efd17355":"code","50ecec9d":"code","2196956c":"code","4fdc08a3":"code","a9d1c7db":"code","d0e40260":"code","02442ad1":"code","79ade49b":"code","f2786402":"code","82c538d7":"code","126670cb":"code","e8ca05ac":"code","d8668431":"code","0ebbaae1":"code","2ad1adb5":"code","85b17785":"code","a07c8261":"code","0f4838a9":"code","71318f53":"code","f4dc0ba9":"code","804af43b":"code","51ce4b76":"code","6e333878":"code","11de7f15":"code","fda93032":"code","bf514abe":"code","7dd9085b":"code","3e83dfd7":"code","267913a5":"code","22b2876f":"code","757a36d6":"code","cba06cb2":"code","5855ffcc":"code","49a67225":"code","0cef06d7":"code","d4538f16":"code","2e502c1a":"code","a2759ade":"code","66b85d68":"code","58897fee":"code","5fe8f07b":"code","071cef65":"code","c149f35e":"code","f5d2116f":"code","4dd03be0":"code","1cee36a2":"code","bad2dc77":"code","0a55b451":"code","f0318afa":"code","5b7d390a":"code","828b4a79":"code","314cb00b":"code","a31cd3cc":"code","b3ab445d":"code","429330a3":"code","271f6056":"code","c65fda8c":"code","a21d165c":"code","c6e8c27d":"code","aca1db51":"code","567ac031":"code","cf7472aa":"code","31c76b45":"code","a2126e5b":"code","ee370aeb":"code","ff0e7f70":"markdown","cec2184a":"markdown","161beb16":"markdown","36d0b74d":"markdown","076b6af4":"markdown","1d5509f3":"markdown","7e345e60":"markdown","072bce31":"markdown","f4f8990b":"markdown"},"source":{"dee1cca8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","ccdfad58":"data_orig = pd.read_csv('..\/input\/train.csv' , sep=',')\ndata = data_orig\n\ndata_test_orig = pd.read_csv('..\/input\/test.csv' , sep=',')\ndata_test = data_test_orig","4b3bf077":"data.head()","1b157502":"data_test.head()","0da9acce":"data.shape","97174cc2":"data_test.shape","90494148":"data.info()","95c4e1a4":"data_test.info()","a3fc5475":"data.shape","241f9024":"#data=data.drop(data.index[10000:99999])\ndata.shape","75bfc4b7":"import seaborn as sns\nf, ax = plt.subplots(figsize=(15, 12))\ncorr = data.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax, annot = True);","91d9be9c":"data = data.drop(['ID','Worker Class','Enrolled','MIC','MOC','MLU','Reason','Area','State','MSA','REG','MOVE','Live','PREV','Teen','COB FATHER','COB MOTHER','COB SELF','Fill','Hispanic','Detailed'], 1)\ndata_test = data_test.drop(['ID','Worker Class','Enrolled','MIC','MOC','MLU','Reason','Area','State','MSA','REG','MOVE','Live','PREV','Teen','COB FATHER','COB MOTHER','COB SELF','Fill','Hispanic','Detailed'], 1)","4bb7e353":"data = data.drop(['OC','Timely Income','Weight','Own\/Self','WorkingPeriod'], 1)\ndata_test = data_test.drop(['OC','Timely Income','Weight','Own\/Self','WorkingPeriod'], 1)","1014f93e":"data['Class'].value_counts()","f75f23ad":"# from imblearn.over_sampling import RandomOverSampler\n\n# ros = RandomOverSampler(random_state=42)\n# data = ros.fit_resample(data)","9e9d42b2":"data.shape","7a9a6d37":"data_test.shape","b64a3d44":"y=data['Class']","f129d07a":"X=data.drop(['Class'],axis=1)\nX = pd.get_dummies(X, columns=['Schooling','Married_Life','Cast','Sex','Full\/Part','Tax Status','Summary','Citizen'])\nX_test = pd.get_dummies(data_test, columns=['Schooling','Married_Life','Cast','Sex','Full\/Part','Tax Status','Summary','Citizen'])\nX.head()","efd17355":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\nX,y = ros.fit_resample(X,y)\n","50ecec9d":"from collections import Counter\n\nCounter(y)","2196956c":"X_test.head()","4fdc08a3":"X_test.shape","a9d1c7db":"X.shape","d0e40260":"for column in X_test:\n    print(column)","02442ad1":"# for column in X:\n#     print(column)","79ade49b":"#X.columns","f2786402":"#X_test.columns","82c538d7":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)","126670cb":"from sklearn import preprocessing\n#Performing Min_Max Normalization\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X_train)\nX_train = pd.DataFrame(np_scaled)\nnp_scaled_val = min_max_scaler.transform(X_val)\nX_val = pd.DataFrame(np_scaled_val)\nX_train.head()","e8ca05ac":"from sklearn import preprocessing\n#Performing Min_Max Normalization\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X_test)\nX_test = pd.DataFrame(np_scaled)\n#np_scaled_val = min_max_scaler.transform(X_val)\n#X_val = pd.DataFrame(np_scaled_val)\n#X_train.head()\nX_test.head()","d8668431":"np.random.seed(42)","0ebbaae1":"from sklearn.naive_bayes import GaussianNB as NB","2ad1adb5":"nb = NB()\nnb.fit(X_train,y_train)\nnb.score(X_val,y_val)","85b17785":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\ny_pred_NB = nb.predict(X_val)\nprint(confusion_matrix(y_val, y_pred_NB))","a07c8261":"print(classification_report(y_val, y_pred_NB))","0f4838a9":"y_pred_test_NB = nb.predict(X_test)\ny_pred_test_NB\nz = y_pred_test_NB.tolist()","71318f53":"res1 = pd.DataFrame(z)\ndata_z = pd.read_csv('..\/input\/test.csv' , sep=',')\nfinal = pd.concat([data_z['ID'], res1], axis=1).reindex()\nfinal = final.rename(columns={0: \"Class\"})\nfinal['Class'] = final.Class.astype(int)\nfinal.head(100)\n#res1.head(100)","f4dc0ba9":"#final=final.drop(final.index[0:175])\nfinal.to_csv('submission.csv', index = False,  float_format='%.f')","804af43b":"data_z.head()\n","51ce4b76":"# from sklearn.neighbors import KNeighborsClassifier","6e333878":"# train_acc = []\n# test_acc = []\n# for i in range(1,15):\n    \n#     knn = KNeighborsClassifier(n_neighbors=i)\n#     knn.fit(X_train,y_train)\n#     acc_train = knn.score(X_train,y_train)\n#     train_acc.append(acc_train)\n#     acc_test = knn.score(X_val,y_val)\n#     test_acc.append(acc_test)","11de7f15":"# plt.figure(figsize=(10,6))\n# train_score,=plt.plot(range(1,15),train_acc,color='blue', linestyle='dashed', marker='o',\n#          markerfacecolor='green', markersize=5)\n# test_score,=plt.plot(range(1,15),test_acc,color='red',linestyle='dashed',  marker='o',\n#          markerfacecolor='blue', markersize=5)\n# plt.legend( [train_score, test_score],[\"Train Accuracy\", \"Test Accuracy\"])\n# plt.title('Accuracy vs K neighbors')\n# plt.xlabel('K neighbors')\n# plt.ylabel('Accuracy')","fda93032":"# knn = KNeighborsClassifier(n_neighbors=5)\n# knn.fit(X_train,y_train)\n# knn.score(X_val,y_val)","bf514abe":"# y_pred_KNN = knn.predict(X_val)\n# cfm = confusion_matrix(y_val, y_pred_KNN, labels = [0,1,2])\n# print(cfm)\n# #entry (i,j) in a confusion matrix is the number of observations actually in group i, but predicted to be in group j.\n\n# print\"True Positives of Class 0: \", cfm[0][0]\n# print\"False Positives of Class 0 wrt Class 1: \", cfm[1][0] # Predicted as 0 but actually in 1 \n# print\"False Positives of Class 0 wrt Class 2: \", cfm[2][0]\n# print\"False Negatives of Class 0 wrt Class 1: \", cfm[0][1] # Precited as 1 but actually in 0\n# print\"False Negatives of Class 0 wrt Class 2: \", cfm[0][2]","7dd9085b":"# print(classification_report(y_val, y_pred_KNN))\n# # Precision of class 0: Out of all those that you predicted as 0, how many were actually 0\n# # Recall of Class 0: Out of all those that were actually 0, how many you predicted to be 0\n# # micro avg = (Total TP)\/(Total TP+FP)\n# # macro avg = unweighted mean of scores of class 0,1,2","3e83dfd7":"# from sklearn import preprocessing\n# #Performing Min_Max Normalization\n# min_max_scaler2 = preprocessing.MinMaxScaler()\n# np_scaled_full = min_max_scaler2.fit_transform(X)\n# X_N = pd.DataFrame(np_scaled_full)","267913a5":"# from sklearn.model_selection import cross_validate\n# from sklearn.metrics import make_scorer\n# from sklearn.metrics import f1_score\n\n# scorer_f1 = make_scorer(f1_score, average = 'micro')\n\n# cv_results = cross_validate(knn, X_N, y, cv=10, scoring=(scorer_f1), return_train_score=True)\n# print cv_results.keys()\n# print\"Train Accuracy for 3 folds= \",np.mean(cv_results['train_score'])\n# print\"Validation Accuracy for 3 folds = \",np.mean(cv_results['test_score'])","22b2876f":"# y_pred_test_KNN = knn.predict(X_test)\n# y_pred_test_KNN\n# z_KNN = y_pred_test_KNN.tolist()","757a36d6":"# res1_KNN = pd.DataFrame(z_KNN)\n# final_KNN = pd.concat([data_test_orig['ID'], res1], axis=1).reindex()\n# final_KNN = final_KNN.rename(columns={0: \"Class\"})\n# final_KNN['Class'] = final_KNN.Class.astype(int)\n# final_KNN.head(100)\n# #res1.head(100)","cba06cb2":"# final_KNN.to_csv('submission_KNN.csv', index = False,  float_format='%.f')","5855ffcc":"# from sklearn.linear_model import LogisticRegression","49a67225":"# lg = LogisticRegression(solver = 'liblinear', C = 1, multi_class = 'ovr', random_state = 42)\n# lg.fit(X_train,y_train)\n# lg.score(X_val,y_val)","0cef06d7":"# lg = LogisticRegression(solver = 'lbfgs', C = 8, multi_class = 'multinomial', random_state = 42)\n# lg.fit(X_train,y_train)\n# lg.score(X_val,y_val)","d4538f16":"# y_pred_LR = lg.predict(X_val)\n# print(confusion_matrix(y_val, y_pred_LR))","2e502c1a":"# print(classification_report(y_val, y_pred_LR))","a2759ade":"# y_pred_test_LR = lg.predict(X_test)\n# y_pred_test_LR\n# z_LR = y_pred_test_LR.tolist()","66b85d68":"# res1_LR = pd.DataFrame(z_LR)\n# final_LR = pd.concat([data_test_orig['ID'], res1], axis=1).reindex()\n# final_LR = final_LR.rename(columns={0: \"Class\"})\n# final_LR['Class'] = final_LR.Class.astype(int)\n# final_LR.head(100)\n# #res1.head(100)","58897fee":"# final_LR.to_csv('submission_LR.csv', index = False,  float_format='%.f')","5fe8f07b":"# from sklearn.tree import DecisionTreeClassifier","071cef65":"# from sklearn.tree import DecisionTreeClassifier\n\n# train_acc = []\n# test_acc = []\n# for i in range(1,15):\n#     dTree = DecisionTreeClassifier(max_depth=i)\n#     dTree.fit(X_train,y_train)\n#     acc_train = dTree.score(X_train,y_train)\n#     train_acc.append(acc_train)\n#     acc_test = dTree.score(X_val,y_val)\n#     test_acc.append(acc_test)","c149f35e":"# plt.figure(figsize=(10,6))\n# train_score,=plt.plot(range(1,15),train_acc,color='blue', linestyle='dashed', marker='o',\n#          markerfacecolor='green', markersize=5)\n# test_score,=plt.plot(range(1,15),test_acc,color='red',linestyle='dashed',  marker='o',\n#          markerfacecolor='blue', markersize=5)\n# plt.legend( [train_score, test_score],[\"Train Accuracy\", \"Validation Accuracy\"])\n# plt.title('Accuracy vs Max Depth')\n# plt.xlabel('Max Depth')\n# plt.ylabel('Accuracy')","f5d2116f":"# from sklearn.tree import DecisionTreeClassifier\n\n# train_acc = []\n# test_acc = []\n# for i in range(2,30):\n#     dTree = DecisionTreeClassifier(max_depth = 9, min_samples_split=i, random_state = 42)\n#     dTree.fit(X_train,y_train)\n#     acc_train = dTree.score(X_train,y_train)\n#     train_acc.append(acc_train)\n#     acc_test = dTree.score(X_val,y_val)\n#     test_acc.append(acc_test)","4dd03be0":"# plt.figure(figsize=(10,6))\n# train_score,=plt.plot(range(2,30),train_acc,color='blue', linestyle='dashed', marker='o',\n#          markerfacecolor='green', markersize=5)\n# test_score,=plt.plot(range(2,30),test_acc,color='red',linestyle='dashed',  marker='o',\n#          markerfacecolor='blue', markersize=5)\n# plt.legend( [train_score, test_score],[\"Train Accuracy\", \"Validation Accuracy\"])\n# plt.title('Accuracy vs min_samples_split')\n# plt.xlabel('Max Depth')\n# plt.ylabel('Accuracy')","1cee36a2":"# dTree = DecisionTreeClassifier(max_depth=9, random_state = 42)\n# dTree.fit(X_train,y_train)\n# dTree.score(X_val,y_val)","bad2dc77":"# y_pred_DT = dTree.predict(X_val)\n# print(confusion_matrix(y_val, y_pred_DT))","0a55b451":"# print(classification_report(y_val, y_pred_DT))","f0318afa":"# y_pred_test_DT = dTree.predict(X_test)\n# y_pred_test_DT\n# z_DT = y_pred_test_DT.tolist()","5b7d390a":"# res1_DT = pd.DataFrame(z_DT)\n# final_DT = pd.concat([data_test_orig['ID'], res1], axis=1).reindex()\n# final_DT = final_DT.rename(columns={0: \"Class\"})\n# final_DT['Class'] = final_DT.Class.astype(int)\n# final_DT.head(100)","828b4a79":"# final_DT.to_csv('submission_DT.csv', index = False,  float_format='%.f')","314cb00b":"from sklearn.ensemble import RandomForestClassifier","a31cd3cc":"score_train_RF = []\nscore_test_RF = []\nfor i in range(1,18,1):\n    rf = RandomForestClassifier(n_estimators=i, random_state = 42)\n    rf.fit(X_train, y_train)\n    sc_train = rf.score(X_train,y_train)\n    score_train_RF.append(sc_train)\n    sc_test = rf.score(X_val,y_val)\n    score_test_RF.append(sc_test)","b3ab445d":"plt.figure(figsize=(10,6))\ntrain_score,=plt.plot(range(1,18,1),score_train_RF,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='green', markersize=5)\ntest_score,=plt.plot(range(1,18,1),score_test_RF,color='red',linestyle='dashed',  marker='o',\n         markerfacecolor='blue', markersize=5)\nplt.legend( [train_score,test_score],[\"Train Score\",\"Test Score\"])\nplt.title('Fig4. Score vs. No. of Trees')\nplt.xlabel('No. of Trees')\nplt.ylabel('Score')","429330a3":"rf = RandomForestClassifier(n_estimators=11, random_state = 42)\nrf.fit(X_train, y_train)\nrf.score(X_val,y_val)","271f6056":"y_pred_RF = rf.predict(X_val)\nconfusion_matrix(y_val, y_pred_RF)","c65fda8c":"print(classification_report(y_val, y_pred_RF))","a21d165c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score\n\nrf_temp = RandomForestClassifier(n_estimators = 11)        #Initialize the classifier object\n\nparameters = {'max_depth':[3, 5, 8, 10],'min_samples_split':[2, 3, 4, 5]}    #Dictionary of parameters\n\nscorer = make_scorer(f1_score, average = 'micro')         #Initialize the scorer using make_scorer\n\ngrid_obj = GridSearchCV(rf_temp, parameters, scoring=scorer)         #Initialize a GridSearchCV object with above parameters,scorer and classifier\n\ngrid_fit = grid_obj.fit(X_train, y_train)        #Fit the gridsearch object with X_train,y_train\n\nbest_rf = grid_fit.best_estimator_         #Get the best estimator. For this, check documentation of GridSearchCV object\n\nprint(grid_fit.best_params_)","c6e8c27d":"rf_best = RandomForestClassifier(n_estimators = 11, max_depth = 10, min_samples_split = 5)\nrf_best.fit(X_train, y_train)\nrf_best.score(X_val,y_val)","aca1db51":"y_pred_RF_best = rf_best.predict(X_val)\nconfusion_matrix(y_val, y_pred_RF_best)","567ac031":"print(classification_report(y_val, y_pred_RF_best))","cf7472aa":"y_pred_test_RF = rf_best.predict(X_test)\ny_pred_test_RF\nz_RF = y_pred_test_RF.tolist()","31c76b45":"res1 = pd.DataFrame(z_RF)\nfinal_RF = pd.concat([data_test_orig['ID'], res1], axis=1).reindex()\nfinal_RF = final_RF.rename(columns={0: \"Class\"})\nfinal_RF['Class'] = final_RF.Class.astype(int)\nfinal_RF.head(100)\n#res1.head(100)","a2126e5b":"final_RF.to_csv('submission_RF.csv', index = False,  float_format='%.f')","ee370aeb":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):\n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html='<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(final_RF)","ff0e7f70":"# Tune\n#### 1. C - Inverse of Regularization Strength\n#### 2.Solver (Only in multi-class problems)\n#### 3.multi_class","cec2184a":"# Tune\n#### 1. n_estimators\n#### 2. max_depth\n#### 3.min_samples_split\n#### 4.min_samples_leaf","161beb16":"# Naive Bayes","36d0b74d":"# Tune \n##### 1.n_neighbors\n##### 2.weights","076b6af4":"# Random Forest","1d5509f3":"# Logistic Regression","7e345e60":"# Decision Tree","072bce31":"# KNN Algorithm","f4f8990b":"# Tune\n#### 1. max_depth\n#### 2. min_samples_split\n#### 3. min_samples_leaf"}}