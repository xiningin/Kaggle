{"cell_type":{"1a1166b4":"code","4cab8d94":"code","4cb019c7":"code","678b091b":"code","205d155d":"code","2deab238":"code","ee3a3ee8":"code","b1ee194c":"code","bb6fc2b9":"code","faada8c0":"code","d8f0c3b4":"code","268562aa":"code","aba70f97":"code","72ea338c":"code","fac511cd":"code","2c94edea":"code","596d3dd3":"code","64011af1":"code","eff965ea":"code","668d6b36":"code","6563b4a8":"code","704396ce":"code","9a39bf13":"code","5f2c4d49":"code","b7ab1348":"code","db5c3b24":"code","c5a89aab":"code","3881fd72":"code","24a08580":"code","8ff4e5d0":"code","6ff1c891":"code","cbcb4943":"markdown","367ec98f":"markdown","8f403553":"markdown","e9d4bb18":"markdown","3c8600c9":"markdown","70903c65":"markdown","4ec5db31":"markdown","54ae4c84":"markdown","a9bd948f":"markdown","255d51fa":"markdown","0a910f12":"markdown","1f76b4ff":"markdown","179884f6":"markdown","069ebdab":"markdown","cd8f4e57":"markdown","66689a02":"markdown","92d8ebcb":"markdown","ca06ce43":"markdown","98e551ae":"markdown","4c9bc81b":"markdown","5bf9a305":"markdown","56082c6e":"markdown","e07f9a4a":"markdown","743c529d":"markdown","e048f2d9":"markdown","3b424c0a":"markdown","5bd3b9db":"markdown","111d4773":"markdown"},"source":{"1a1166b4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler, binarize\nfrom sklearn.metrics import recall_score, roc_auc_score, confusion_matrix\nfrom math import *\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 1000)","4cab8d94":"df = pd.read_csv('..\/input\/heart-disease\/heart.csv')\ndf.head()","4cb019c7":"df.describe()","678b091b":"plt.plot()\nsns.countplot(x='target', data=df)\nplt.title('Target class destribution');","205d155d":"plt.plot()\nsns.heatmap(df.corr())\nplt.title('Heat Map');","2deab238":"sns.pairplot(df, hue='target', diag_kws={'bw':0.5});","ee3a3ee8":"X = df.drop(['target'], axis=1).to_numpy()\ny = df['target'].to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, shuffle=True)\n","b1ee194c":"scaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","bb6fc2b9":"log_reg = LogisticRegression(solver='liblinear')\nparams = {\n    'penalty': ['l1', 'l2'],\n    'C': np.linspace(0, 0.6, 20)\n}","faada8c0":"scoring_list = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\nfig, axs = plt.subplots(1, len(scoring_list), figsize=(30,5))\nfor i in range(len(scoring_list)):\n    scoring = scoring_list[i]\n    grd = GridSearchCV(log_reg, params, scoring=scoring, cv=5)\n    grd.fit(X_train, y_train);\n    C = [param['C'] for param in grd.cv_results_['params']]\n    penalty = [param['penalty'] for param in grd.cv_results_['params']]\n    mean_test_score = grd.cv_results_['mean_test_score']\n    res = pd.DataFrame({'mean_test_score': mean_test_score, 'C': C, 'penalty': penalty})\n    sns.lineplot(x=\"C\", y=\"mean_test_score\", hue=\"penalty\", data=res, ax=axs[i])\n    axs[i].set_title('Scoring: ' + scoring)","d8f0c3b4":"log_reg = LogisticRegression(C=0.2, penalty='l2', solver='liblinear')\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\nprint('Recall: {:.4f}'.format(recall_score(y_test, y_pred)))\nprint('ROC-AUC: {:.4f}'.format(roc_auc_score(y_test, y_pred)))","268562aa":"def get_features_importance(df, target, lin_predictor):\n    '''\n    Input:\n        `df` - Pandas DataFrame that was used for model training. It stores features names.\n        `target` - target cloumn name.\n        `lin_predictor` - linear model that was thained on this data.\n    Output:\n        `features_importance` - Pandas DataFrame that stores the absolute values of coefficients \n            in linear model which shows features importance.\n        barplot of features importance\n    '''\n    features = list(df.columns.values)\n    features.remove(target)\n    importance = [abs(coef) for coef in lin_predictor.coef_[0]]\n    features_importance = pd.DataFrame({'Feature': features, 'Importance': importance}).sort_values(['Importance'], ascending=False)\n    features_importance = features_importance.reset_index().drop('index', axis=1)\n    fig, ax = plt.subplots(figsize=(25,5))\n    bar = sns.barplot(x='Feature', y='Importance', data=features_importance, ax=ax);\n    ax.set_xticklabels(features, rotation=90);\n    ax.set_title('Features Importance')\n    for index, row in features_importance.iterrows():\n        bar.text(index, row['Importance'], round(row['Importance'], 4), color='black', ha=\"center\", fontsize=8)\n\n    return features_importance","aba70f97":"features_importance = get_features_importance(df, 'target', log_reg)","72ea338c":"def get_polynomial_features(df, feature, features_interaction):\n    df_res = pd.DataFrame()\n    for f in features_interaction:\n        df_res[feature + '^2'] = df[feature]**2\n        df_res[feature + '-' + f] = sqrt(2) * df[feature] * df[f]\n        df_res[f + '^2'] = df[f]**2\n    return df_res    ","fac511cd":"all_features = list(df.columns.values)\nall_features.remove('target')\ndf_oldpeak = get_polynomial_features(df, 'oldpeak', all_features)","2c94edea":"df_2 = pd.concat([df, df_oldpeak], axis=1)\ndf_2.head()","596d3dd3":"X_2 = df_2.drop(['target'], axis=1).to_numpy()\ny_2 = df_2['target'].to_numpy()\n\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, random_state=0, shuffle=True)","64011af1":"log_reg_2 = LogisticRegression(C=0.2, penalty='l2', solver='liblinear')\nlog_reg_2.fit(X_train_2, y_train_2)\ny_pred_2 = log_reg_2.predict(X_test_2)","eff965ea":"print('Recall: {:.4f}'.format(recall_score(y_test_2, y_pred_2)))\nprint('ROC-AUC: {:.4f}'.format(roc_auc_score(y_test_2, y_pred_2)))","668d6b36":"features_importance_2 = get_features_importance(df_2, 'target', log_reg_2)","6563b4a8":"features_to_remove = features_importance_2[features_importance_2['Importance'] < 0.01]['Feature'].values","704396ce":"df_3 = df_2.drop(features_to_remove, axis=1)\ndf_3.head()","9a39bf13":"X_3 = df_3.drop(['target'], axis=1).to_numpy()\ny_3 = df_3['target'].to_numpy()\n\nX_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.3, random_state=0, shuffle=True)","5f2c4d49":"log_reg_3 = LogisticRegression(C=0.2, penalty='l2', solver='liblinear')\nlog_reg_3.fit(X_train_3, y_train_3)\ny_pred_3 = log_reg_3.predict(X_test_3)","b7ab1348":"print('Recall: {:.4f}'.format(recall_score(y_test_3, y_pred_3)))\nprint('ROC-AUC: {:.4f}'.format(roc_auc_score(y_test_3, y_pred_3)))","db5c3b24":"features_importance_3 = get_features_importance(df_3, 'target', log_reg_3)","c5a89aab":"def get_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred,labels = [1,0])\n    df = pd.DataFrame({'Actual Positive': cm[0,:], 'Actual Negative': cm[1,:]}, \n                      index=['Predicted\\nPositive', 'Predicted\\nNegative'])\n    sns.heatmap(df, annot=True, cbar=False)\n    plt.show()","3881fd72":"get_confusion_matrix(y_test_3, y_pred_3)","24a08580":"y_pred_proba = log_reg_3.predict_proba(X_test_3)","8ff4e5d0":"thresholds = np.linspace(0, 1, 100)\nrecall_list = []\nroc_auc_list = []\nfor threshold in thresholds:\n    y_pred_shifted = binarize(y_pred_proba, threshold=threshold)[:,1]\n    recall_list.append(recall_score(y_test_3, y_pred_shifted))\n    roc_auc_list.append(roc_auc_score(y_test_3, y_pred_shifted))","6ff1c891":"plt.figure(figsize=(10, 5))\nplt.plot(thresholds, recall_list, label='Recall')\nplt.plot(thresholds, roc_auc_list, label='ROC-AUC')\nplt.xticks(np.linspace(0, 1, 25), rotation=90)\nplt.legend(title='Metric:', loc=3)\nplt.grid()\nplt.title('Threshold vs Metrics');","cbcb4943":"Now we get some polynomial features. According to the **pair plot** it can help to separate classes. We add interactions of the feature ***oldpeak*** fith the others.","367ec98f":"We can see from above that a default threshold of 0.5 provides the best result. Better Recall can be obtained by threshold reduction, but it causes a suffitient ROC-AUC decrease.","8f403553":"## Results Evaluation","e9d4bb18":"The result has clearly improved.","3c8600c9":"Have a look at features importance:","70903c65":"# Heart Disease Prediction: get the most out of Logistic Regression\n## Heart Disease Data set exploration\n### Attribute Information:\n* **Age:** Age <br>\n* **Sex:** Sex (1 = male; 0 = female) <br>\n* **ChestPain:** Chest pain (typical, asymptotic, nonanginal, nontypical) <br>\n* **RestBP:** Resting blood pressure <br>\n* **Chol:** Serum cholestoral in mg\/dl <br>\n* **Fbs:** Fasting blood sugar > 120 mg\/dl (1 = true; 0 = false) <br>\n* **RestECG:** Resting electrocardiographic results <br>\n* **MaxHR:** Maximum heart rate achieved <br>\n* **ExAng:** Exercise induced angina (1 = yes; 0 = no) <br>\n* **Oldpeak:** ST depression induced by exercise relative to rest <br>\n* **Slope:** Slope of the peak exercise ST segment <br>\n* **Ca:** Number of major vessels colored by flourosopy (0 - 3) <br>\n* **Thal:** (3 = normal; 6 = fixed defect; 7 = reversable defect) <br>\n* **target:** AHD - Diagnosis of heart disease (1 = yes; 0 = no) <br>\n\n**Source:** https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\n\n**Creators:**\n\nHungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\nUniversity Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\nUniversity Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\nV.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\nDonor: David W. Aha (aha '@' ics.uci.edu) (714) 856-8779\n\n**Data Set Information:**\n\nThis database contains attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. T\n\nThe \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence from absence (value 0).\n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.","4ec5db31":"## 3nd try for Logistic Regression: remove unimportant features","54ae4c84":"The amount of data is small, thus we can afford an extensive grid search for the best model parameters.","a9bd948f":"There are 3 false negative results which is undesirable for medical test. Can we change the threshold to get rid of them?","255d51fa":"## 2nd try for Logistic Regression: adding polynomial features","0a910f12":"However, with so many features we are about to overfit. Let's drop a number of unimportant features and see the model's performance. ","1f76b4ff":"We need to check if we can tune the model by adjusting a classfication threshold. First of all, have a look at the confusion matrix.","179884f6":"Several features have a good linear correlation with the target: ***slope***, ***cp***, ***exang***, ***oldpeak***, ***ca***. Now we need to draw a pairplot for our features. It could take a while, but give us a better understanding of the data and can provide valuable insights. ","069ebdab":"We see that there are no NaN values, no negative values for nonnegative features. The data was collected correctly and no additional cleaning is needed. Let's check if the classes are balanced:","cd8f4e57":"### Logistic Regression training and results","66689a02":"Some of the conclusions from the plot above:\n* Consider the destributions of the features. Some of them can differentiate the target rather well: ***thal***, ***slope***, ***exang***, ***restecg***, ***thalach***\n* Consider data point. For some pairs the classes can be linearly separated with a good quality.\n* The feature ***oldpeak*** plotted against other features gives the best linear separation of the classes.\n\nWe will use **Logistic Regression** model. It can provide a sufficient prediction quality for this data, in addition it is easy to train and interprete.","92d8ebcb":"## Packages Import","ca06ce43":"## First try for Logistic Regression: base case","98e551ae":"Let's import our dataset and have a closer look at it.","4c9bc81b":"## Data Import and EDA","5bf9a305":"### Grid Search","56082c6e":"As long as we are going to use Logistics Regression, we need to scale our data:","e07f9a4a":"Classes are perfectly balanced. Then investigate a heatmap:","743c529d":"### Pre-processing","e048f2d9":"Firstly, divide the data into train and test sets:","3b424c0a":"Let's train the model and check the performance on the test data:","5bd3b9db":"Conclusions:\n* ***l2*** outperform ***l1*** for ***C*** < 0.3\n* ***l2*** gives better ***ROC-AUC*** regardless of ***C***\n* For this task we must focus on higher ***recall*** score, because in medical diagnostic its more important to reduce False Negative results\n\nConsidering these facts, we choose ***l2*** penalty and ***C*** = 0.2. It gives a reasonably good ***ROC-AUC*** and also high ***recall***.","111d4773":"Features selection has slightly improved the results again."}}