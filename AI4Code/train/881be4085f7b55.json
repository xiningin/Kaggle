{"cell_type":{"41424265":"code","f31d604c":"code","a62bc79d":"code","b0ee3623":"code","fc816db3":"code","93484025":"code","c2977696":"code","be7f415d":"code","d29b8e10":"code","0c11cb26":"code","c55d3deb":"code","93b71cf6":"code","a070d58c":"code","8255ec4b":"code","a276e4d0":"code","00fb311d":"code","323f7605":"code","32467209":"code","00275d83":"code","3a49d543":"code","3bfa9204":"code","9d0740dc":"code","3028de3c":"code","e5d3fcfd":"code","cc4cf8c0":"code","f6531ae6":"code","843dff94":"code","c29c14c5":"code","66e7c2fe":"code","59edb3a4":"code","f0aeb1cf":"code","5b6e288c":"code","f63171f9":"code","173c45b0":"code","315b87cb":"code","d5f5cc9f":"code","d987e47e":"code","abc6b432":"code","193fe3cf":"code","e1b63a7e":"code","b66cb457":"code","2211fe96":"markdown","31e72cb0":"markdown","26c36b0f":"markdown","d52ba9c0":"markdown","28902633":"markdown","a1974df9":"markdown","82c649a5":"markdown","4665c997":"markdown","f1b243f4":"markdown","ecaf5b07":"markdown","a70c76ba":"markdown","d1cf18b6":"markdown","8041bbb3":"markdown","b8805504":"markdown","21c25de3":"markdown","55eb2716":"markdown","b4fe4a3f":"markdown","d80f7591":"markdown","4b2bc06c":"markdown","37c02f79":"markdown","e9fec915":"markdown","7db54487":"markdown"},"source":{"41424265":"import numpy as np \nimport pandas as pd \n\n\n# DRAGONS\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nimport ast\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n\n\nrandom_seed = 2019","f31d604c":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","a62bc79d":"%%time\ntrain = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\ntrain.index = train['id']\ntest.index = test['id']","b0ee3623":"print(\"Dimension of train : \" + str(train.shape) + \" || Dimension of test : \" + str(test.shape))","fc816db3":"train.head()","93484025":"print(\"Types columns : \\n\" + str(train.dtypes))","c2977696":"print(\"Count NA Train for the variable budget : \\n\" + str(train[train.budget == 0].shape[0]) + \"\\n\")\nprint(\"Count NA Test for the variable budget : \\n\" + str(test[test.budget == 0].shape[0]) + \"\\n\")\nprint(\"Count NA Train per column : \\n\" + str(train.isna().sum()))","be7f415d":"train.loc[train['id'] == 16,'revenue'] = 192864         \ntrain.loc[train['id'] == 90,'budget'] = 30000000                  \ntrain.loc[train['id'] == 118,'budget'] = 60000000       \ntrain.loc[train['id'] == 149,'budget'] = 18000000       \ntrain.loc[train['id'] == 313,'revenue'] = 12000000       \ntrain.loc[train['id'] == 451,'revenue'] = 12000000      \ntrain.loc[train['id'] == 464,'budget'] = 20000000       \ntrain.loc[train['id'] == 470,'budget'] = 13000000       \ntrain.loc[train['id'] == 513,'budget'] = 930000         \ntrain.loc[train['id'] == 797,'budget'] = 8000000        \ntrain.loc[train['id'] == 819,'budget'] = 90000000       \ntrain.loc[train['id'] == 850,'budget'] = 90000000       \ntrain.loc[train['id'] == 1007,'budget'] = 2              \ntrain.loc[train['id'] == 1112,'budget'] = 7500000       \ntrain.loc[train['id'] == 1131,'budget'] = 4300000        \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       \ntrain.loc[train['id'] == 1542,'budget'] = 1             \ntrain.loc[train['id'] == 1570,'budget'] = 15800000       \ntrain.loc[train['id'] == 1571,'budget'] = 4000000        \ntrain.loc[train['id'] == 1714,'budget'] = 46000000       \ntrain.loc[train['id'] == 1721,'budget'] = 17500000       \ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      \ntrain.loc[train['id'] == 1885,'budget'] = 12             \ntrain.loc[train['id'] == 2091,'budget'] = 10             \ntrain.loc[train['id'] == 2268,'budget'] = 17500000       \ntrain.loc[train['id'] == 2491,'budget'] = 6              \ntrain.loc[train['id'] == 2602,'budget'] = 31000000       \ntrain.loc[train['id'] == 2612,'budget'] = 15000000       \ntrain.loc[train['id'] == 2696,'budget'] = 10000000      \ntrain.loc[train['id'] == 2801,'budget'] = 10000000       \ntrain.loc[train['id'] == 335,'budget'] = 2 \ntrain.loc[train['id'] == 348,'budget'] = 12\ntrain.loc[train['id'] == 470,'budget'] = 13000000 \ntrain.loc[train['id'] == 513,'budget'] = 1100000\ntrain.loc[train['id'] == 640,'budget'] = 6 \ntrain.loc[train['id'] == 696,'budget'] = 1\ntrain.loc[train['id'] == 797,'budget'] = 8000000 \ntrain.loc[train['id'] == 850,'budget'] = 1500000\ntrain.loc[train['id'] == 1199,'budget'] = 5 \ntrain.loc[train['id'] == 1282,'budget'] = 9              \ntrain.loc[train['id'] == 1347,'budget'] = 1\ntrain.loc[train['id'] == 1755,'budget'] = 2\ntrain.loc[train['id'] == 1801,'budget'] = 5\ntrain.loc[train['id'] == 1918,'budget'] = 592 \ntrain.loc[train['id'] == 2033,'budget'] = 4\ntrain.loc[train['id'] == 2118,'budget'] = 344 \ntrain.loc[train['id'] == 2252,'budget'] = 130\ntrain.loc[train['id'] == 2256,'budget'] = 1 \ntrain.loc[train['id'] == 2696,'budget'] = 10000000","d29b8e10":"test.loc[test['id'] == 3033,'budget'] = 250 \ntest.loc[test['id'] == 3051,'budget'] = 50\ntest.loc[test['id'] == 3084,'budget'] = 337\ntest.loc[test['id'] == 3224,'budget'] = 4  \ntest.loc[test['id'] == 3594,'budget'] = 25  \ntest.loc[test['id'] == 3619,'budget'] = 500  \ntest.loc[test['id'] == 3831,'budget'] = 3  \ntest.loc[test['id'] == 3935,'budget'] = 500  \ntest.loc[test['id'] == 4049,'budget'] = 995946 \ntest.loc[test['id'] == 4424,'budget'] = 3  \ntest.loc[test['id'] == 4460,'budget'] = 8  \ntest.loc[test['id'] == 4555,'budget'] = 1200000 \ntest.loc[test['id'] == 4624,'budget'] = 30 \ntest.loc[test['id'] == 4645,'budget'] = 500 \ntest.loc[test['id'] == 4709,'budget'] = 450 \ntest.loc[test['id'] == 4839,'budget'] = 7\ntest.loc[test['id'] == 3125,'budget'] = 25 \ntest.loc[test['id'] == 3142,'budget'] = 1\ntest.loc[test['id'] == 3201,'budget'] = 450\ntest.loc[test['id'] == 3222,'budget'] = 6\ntest.loc[test['id'] == 3545,'budget'] = 38\ntest.loc[test['id'] == 3670,'budget'] = 18\ntest.loc[test['id'] == 3792,'budget'] = 19\ntest.loc[test['id'] == 3881,'budget'] = 7\ntest.loc[test['id'] == 3969,'budget'] = 400\ntest.loc[test['id'] == 4196,'budget'] = 6\ntest.loc[test['id'] == 4221,'budget'] = 11\ntest.loc[test['id'] == 4222,'budget'] = 500\ntest.loc[test['id'] == 4285,'budget'] = 11\ntest.loc[test['id'] == 4319,'budget'] = 1\ntest.loc[test['id'] == 4639,'budget'] = 10\ntest.loc[test['id'] == 4719,'budget'] = 45\ntest.loc[test['id'] == 4822,'budget'] = 22\ntest.loc[test['id'] == 4829,'budget'] = 20\ntest.loc[test['id'] == 4969,'budget'] = 20\ntest.loc[test['id'] == 5021,'budget'] = 40 \ntest.loc[test['id'] == 5035,'budget'] = 1 \ntest.loc[test['id'] == 5063,'budget'] = 14 \ntest.loc[test['id'] == 5119,'budget'] = 2 \ntest.loc[test['id'] == 5214,'budget'] = 30 \ntest.loc[test['id'] == 5221,'budget'] = 50 \ntest.loc[test['id'] == 4903,'budget'] = 15\ntest.loc[test['id'] == 4983,'budget'] = 3\ntest.loc[test['id'] == 5102,'budget'] = 28\ntest.loc[test['id'] == 5217,'budget'] = 75\ntest.loc[test['id'] == 5224,'budget'] = 3 \ntest.loc[test['id'] == 5469,'budget'] = 20 \ntest.loc[test['id'] == 5840,'budget'] = 1 \ntest.loc[test['id'] == 5960,'budget'] = 30\ntest.loc[test['id'] == 6506,'budget'] = 11 \ntest.loc[test['id'] == 6553,'budget'] = 280\ntest.loc[test['id'] == 6561,'budget'] = 7\ntest.loc[test['id'] == 6582,'budget'] = 218\ntest.loc[test['id'] == 6638,'budget'] = 5\ntest.loc[test['id'] == 6749,'budget'] = 8 \ntest.loc[test['id'] == 6759,'budget'] = 50 \ntest.loc[test['id'] == 6856,'budget'] = 10\ntest.loc[test['id'] == 6858,'budget'] =  100\ntest.loc[test['id'] == 6876,'budget'] =  250\ntest.loc[test['id'] == 6972,'budget'] = 1\ntest.loc[test['id'] == 7079,'budget'] = 8000000\ntest.loc[test['id'] == 7150,'budget'] = 118\ntest.loc[test['id'] == 6506,'budget'] = 118\ntest.loc[test['id'] == 7225,'budget'] = 6\ntest.loc[test['id'] == 7231,'budget'] = 85\ntest.loc[test['id'] == 5222,'budget'] = 5\ntest.loc[test['id'] == 5322,'budget'] = 90\ntest.loc[test['id'] == 5350,'budget'] = 70\ntest.loc[test['id'] == 5378,'budget'] = 10\ntest.loc[test['id'] == 5545,'budget'] = 80\ntest.loc[test['id'] == 5810,'budget'] = 8\ntest.loc[test['id'] == 5926,'budget'] = 300\ntest.loc[test['id'] == 5927,'budget'] = 4\ntest.loc[test['id'] == 5986,'budget'] = 1\ntest.loc[test['id'] == 6053,'budget'] = 20\ntest.loc[test['id'] == 6104,'budget'] = 1\ntest.loc[test['id'] == 6130,'budget'] = 30\ntest.loc[test['id'] == 6301,'budget'] = 150\ntest.loc[test['id'] == 6276,'budget'] = 100\ntest.loc[test['id'] == 6473,'budget'] = 100\ntest.loc[test['id'] == 6842,'budget'] = 30","0c11cb26":"release_dates = pd.read_csv('..\/input\/tmdb-release-dates-per-country\/release_dates_per_country.csv')\nrelease_dates['id'] = range(1,7399)\nrelease_dates.drop(['original_title','title'],axis = 1,inplace = True)\nrelease_dates.index = release_dates['id']\ntrain = pd.merge(train, release_dates, how='left', on=['id'])\ntest = pd.merge(test, release_dates, how='left', on=['id'])","c55d3deb":"vote = pd.read_csv('..\/input\/just-for-fun\/just_for_fun.csv')[[\"vote_count\",\"vote_average\"]]\nvote['id'] = range(1,7399)\nvote.index = vote['id']\ntrain = pd.merge(train, vote, how='left', on=['id'])\ntest = pd.merge(test, vote, how='left', on=['id'])","93b71cf6":"trainAdditionalFeatures = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TrainAdditionalFeatures.csv')[['imdb_id','popularity2','rating']]\ntestAdditionalFeatures = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TestAdditionalFeatures.csv')[['imdb_id','popularity2','rating']]\n\ntrain = pd.merge(train, trainAdditionalFeatures, how='left', on=['imdb_id'])\ntest = pd.merge(test, testAdditionalFeatures, how='left', on=['imdb_id'])","a070d58c":"train.head()","8255ec4b":"x1 = np.array(train[\"budget\"])\ny1 = np.array(train[\"revenue\"])\n\nfig = plt.figure(1, figsize=(9, 5))\n\n\nplt.plot([0,400000000],[0,400000000],c=\"green\")\nplt.scatter(x1, y1, c=['blue'],marker='o')\nplt.grid()\nplt.xlabel(\"budget\", fontsize=10)  \nplt.ylabel(\"revenue\", fontsize=10)\nplt.title(\"Link between revenue and budget\", fontsize=10)","a276e4d0":"sns.set_style('whitegrid')\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nplt.subplot(1,2,1)\nplt.plot([0,300],[0,400000000],c=\"green\")\nplt.scatter(train[\"popularity\"], train[\"revenue\"], marker='o')\nplt.xlabel(\"popularity\", fontsize=9)\nplt.ylabel(\"revenue\", fontsize=9)\nplt.subplot(1,2,2)\nplt.plot([0,120],[0,400000000],c=\"green\")\nplt.scatter(train[\"theatrical\"], train[\"revenue\"], marker='o',color = 'red')\nplt.xlabel(\"theatrical\", fontsize=9)\nplt.ylabel(\"revenue\", fontsize=9)\nplt.show()","00fb311d":"fig, ax = plt.subplots(1,2,figsize=(12,10))\nplt.subplot(1,2,1)\nax = plt.subplot(projection='3d')\ncolors = np.random.rand(len(train))\n\nxs = np.array(train['vote_count'])\nys = np.array(train['budget'])\nzs = np.array(train['revenue'])\nax.scatter(xs,ys, zs, c= colors,marker='o', alpha=1)\nplt.plot([0,17500],[0,400000000],[0,400000000],c=\"green\")\n\nax.set_xlabel('vote_count')\nax.set_ylabel('budget')\nax.set_zlabel('revenue')\n\nplt.show()","323f7605":"plt.figure(figsize=(15,11)) #figure size\n\n#It's another way to plot our data. using a variable that contains the plot parameters\ng1 = sns.boxenplot(x='original_language', y='revenue', \n                   data=train[(train['original_language'].isin((train['original_language'].value_counts()[:10].index.values)))])\ng1.set_title(\"Revenue by original language's movies\", fontsize=20) # title and fontsize\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\ng1.set_xlabel('Original language', fontsize=18) # Xlabel\ng1.set_ylabel('Revenue', fontsize=18) #Ylabel\n\nplt.show()","32467209":"(sns.FacetGrid(train[(train['release_year']\\\n                        .isin(train['release_year']\\\n                              .value_counts()[:5].index.values))],\n               hue='release_year', height=5, aspect=2)\n  .map(sns.kdeplot, 'budget', shade=True)\n .add_legend()\n)\nplt.title(\"Budget's revenue by years\")\nplt.show()\n","00275d83":"plt.figure(figsize=(12,5))\n\n# Subplot allow us to plot more than one \n# in this case, will be create a subplot grid of 2 x 1\n\n# seting the distribuition of our data and normalizing using np.log on values highest than 0 and + \n# also, we will set the number of bins and if we want or not kde on our histogram\nax = sns.distplot(np.log1p(train['revenue']), bins=40, kde=True)\nax.set_xlabel('Revenue', fontsize=15) #seting the xlabel and size of font\nax.set_ylabel('Distribuition', fontsize=15) #seting the ylabel and size of font\nax.set_title(\"Distribuition of Revenue\", fontsize=20) #seting the title and size of font","3a49d543":"col = ['revenue','budget','popularity','theatrical','runtime']\n\nplt.subplots(figsize=(10, 8))\n\ncorr = train[col].corr()\n\nsns.heatmap(corr, xticklabels=col,yticklabels=col, linewidths=.5, cmap=\"Reds\")","3bfa9204":"def prepare(df):\n    global json_cols\n    global train_dict\n    #df['totalVotes'] = df['totalVotes'].fillna(6)\n\n    df[['release_month','release_day','release_year']]=df['release_date'].str.split('\/',expand=True).replace(np.nan, 0).astype(int)\n    df['release_year'] = df['release_year']\n    df.loc[ (df['release_year'] <= 18) & (df['release_year'] < 100), \"release_year\"] += 2000\n    df.loc[ (df['release_year'] > 18)  & (df['release_year'] < 100), \"release_year\"] += 1900\n    \n    rating_na = df.groupby([\"release_year\",\"original_language\"])['rating'].mean().reset_index()\n    df[df.rating.isna()]['rating'] = df.merge(rating_na, how = 'left' ,on = [\"release_year\",\"original_language\"])\n    \n    vote_count_na = df.groupby([\"release_year\",\"original_language\"])['vote_count'].mean().reset_index()\n    df[df.vote_count.isna()]['vote_count'] = df.merge(vote_count_na, how = 'left' ,on = [\"release_year\",\"original_language\"])\n    \n    #budget_na = df.groupby([\"release_year\",\"original_language\"])['budget'].mean().reset_index()\n    #df.loc[df.budget == 0]['budget'] = df.merge(budget_na, how = 'left' ,on = [\"release_year\",\"original_language\"])\n\n    df['budget'] = np.log1p(df['budget'])\n    \n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\n    \n    df['_collection_name'] = df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n    le = LabelEncoder()\n    le.fit(list(df['_collection_name'].fillna('')))\n    df['_collection_name'] = le.transform(df['_collection_name'].fillna('').astype(str))\n    df['_num_Keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n    df['_num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n    \n    releaseDate = pd.to_datetime(df['release_date']) \n    df['release_dayofweek'] = releaseDate.dt.dayofweek \n    df['release_quarter'] = releaseDate.dt.quarter     \n\n    df['_budget_runtime_ratio'] = df['budget']\/df['runtime'] \n    df['_budget_popularity_ratio'] = df['budget']\/df['popularity']\n    df['_budget_year_ratio'] = df['budget']\/(df['release_year']*df['release_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_year']\/df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity']\/df['release_year']\n\n\n    df['meanruntimeByYear'] = df.groupby(\"release_year\")[\"runtime\"].aggregate('mean')\n    df['meanPopularityByYear'] = df.groupby(\"release_year\")[\"popularity\"].aggregate('mean')\n    df['meanBudgetByYear'] = df.groupby(\"release_year\")[\"budget\"].aggregate('mean')\n    df['_popularity_theatrical_ratio'] = df['theatrical']\/df['popularity']\n    df['_budget_theatrical_ratio'] = df['budget']\/df['theatrical']\n    #df['mean_theatrical_ByYear'] = df.groupby(\"release_year\")[\"theatrical\"].aggregate('mean')\n    df['_popularity_totalVotes_ratio'] = df['vote_count']\/df['popularity']\n    df['_totalVotes_releaseYear_ratio'] = df['vote_count']\/df['release_year']\n    df['_budget_totalVotes_ratio'] = df['budget']\/df['vote_count']\n    \n    \n    df['_rating_popularity_ratio'] = df['rating']\/df['popularity']\n    df['_rating_totalVotes_ratio'] = df['vote_count']\/df['rating']\n    df['_budget_rating_ratio'] = df['budget']\/df['rating']\n    df['_runtime_rating_ratio'] = df['runtime']\/df['rating']\n    \n    \n    df['has_homepage'] = 0\n    df.loc[pd.isnull(df['homepage']) ,\"has_homepage\"] = 1\n    \n    df['isbelongs_to_collectionNA'] = 0\n    df.loc[pd.isnull(df['belongs_to_collection']) ,\"isbelongs_to_collectionNA\"] = 1\n    \n    df['isTaglineNA'] = 0\n    df.loc[df['tagline'] == 0 ,\"isTaglineNA\"] = 1 \n\n    df['isOriginalLanguageEng'] = 0 \n    df.loc[ df['original_language'] == \"en\" ,\"isOriginalLanguageEng\"] = 1\n    \n    df['isTitleDifferent'] = 1\n    df.loc[ df['original_title'] == df['title'] ,\"isTitleDifferent\"] = 0 \n\n    df['isMovieReleased'] = 1\n    df.loc[ df['status'] != \"Released\" ,\"isMovieReleased\"] = 0 \n\n    # get collection id\n    df['collection_id'] = df['belongs_to_collection'].apply(lambda x : np.nan if len(x)==0 else x[0]['id'])\n    \n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n\n\n    df['title_word_count'] = df['title'].str.split().str.len()\n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    \n    df['production_countries_count'] = df['production_countries'].apply(lambda x : len(x))\n    df['production_companies_count'] = df['production_companies'].apply(lambda x : len(x))\n    df['cast_count'] = df['cast'].apply(lambda x : len(x))\n    df['crew_count'] = df['crew'].apply(lambda x : len(x))\n\n    \n    \n\n    for col in ['genres', 'production_countries', 'spoken_languages', 'production_companies'] :\n        df[col] = df[col].map(lambda x: sorted(list(set([n if n in train_dict[col] else col+'_etc' for n in [d['name'] for d in x]])))).map(lambda x: ','.join(map(str, x)))\n        temp = df[col].str.get_dummies(sep=',')\n        df = pd.concat([df, temp], axis=1, sort=False)\n    df.drop(['genres_etc'], axis = 1, inplace = True)\n    \n    df = df.drop(['belongs_to_collection','genres','homepage','imdb_id','overview','runtime'\n    ,'poster_path','production_companies','production_countries','release_date','spoken_languages'\n    ,'status','title','Keywords','cast','crew','original_language','original_title','tagline', 'collection_id','movie_id'\n    ],axis=1)\n    \n    df.fillna(value=0.0, inplace = True) \n\n    return df","9d0740dc":"test['revenue'] = np.nan\n\njson_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef get_dictionary(s):\n    try:\n        d = eval(s)\n    except:\n        d = {}\n    return d\n\nfor col in tqdm(json_cols + ['belongs_to_collection']) :\n    train[col] = train[col].apply(lambda x : get_dictionary(x))\n    test[col] = test[col].apply(lambda x : get_dictionary(x))\n\nprint(train.shape)\ntrain.head()","3028de3c":"# parse json data and build category dictionary\ndef get_json_dict(df) :\n    global json_cols\n    result = dict()\n    for e_col in json_cols :\n        d = dict()\n        rows = df[e_col].values\n        for row in rows :\n            if row is None : continue\n            for i in row :\n                if i['name'] not in d :\n                    d[i['name']] = 0\n                d[i['name']] += 1\n        result[e_col] = d\n    return result\n\ntrain_dict = get_json_dict(train)\ntest_dict = get_json_dict(test)\n\n# remove cateogry with bias and low frequency\nfor col in json_cols :\n    \n    remove = []\n    train_id = set(list(train_dict[col].keys()))\n    test_id = set(list(test_dict[col].keys()))   \n    \n    remove += list(train_id - test_id) + list(test_id - train_id)\n    for i in train_id.union(test_id) - set(remove) :\n        if train_dict[col][i] < 10 or i == '' :\n            remove += [i]\n            \n    for i in remove :\n        if i in train_dict[col] :\n            del train_dict[col][i]\n        if i in test_dict[col] :\n            del test_dict[col][i]\n            \n    print(col, 'size :', len(train_id.union(test_id)), '->', len(train_dict[col]))","e5d3fcfd":"# prepare data\nall_data = prepare(pd.concat([train, test]).reset_index(drop = True))\ntrain = all_data.loc[:train.shape[0] - 1,:]\ntest = all_data.loc[train.shape[0]:,:]                           \nprint(train.shape)\ntrain.head()","cc4cf8c0":"features = list(train.columns)\nfeatures =  [i for i in features if i != 'id' and i != 'revenue']","f6531ae6":"from sklearn.metrics import mean_squared_error\ndef score(data, y):\n    validation_res = pd.DataFrame(\n    {\"id\": data[\"id\"].values,\n     \"transactionrevenue\": data[\"revenue\"].values,\n     \"predictedrevenue\": np.expm1(y)})\n\n    validation_res = validation_res.groupby(\"id\")[\"transactionrevenue\", \"predictedrevenue\"].sum().reset_index()\n    return np.sqrt(mean_squared_error(np.log1p(validation_res[\"transactionrevenue\"].values), \n                                     np.log1p(validation_res[\"predictedrevenue\"].values)))","843dff94":"from sklearn.model_selection import GroupKFold\n\nclass KFoldValidation():\n    def __init__(self, data, n_splits=5):\n        unique_vis = np.array(sorted(data['id'].astype(str).unique()))\n        folds = GroupKFold(n_splits)\n        ids = np.arange(data.shape[0])\n        \n        self.fold_ids = []\n        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n            self.fold_ids.append([\n                    ids[data['id'].astype(str).isin(unique_vis[trn_vis])],\n                    ids[data['id'].astype(str).isin(unique_vis[val_vis])]\n                ])\n            \n    def validate(self, train, test, features, model, name=\"\", prepare_stacking=False, \n                 fit_params={\"early_stopping_rounds\": 500, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n        model.FI = pd.DataFrame(index=features)\n        full_score = 0\n        \n        if prepare_stacking:\n            test[name] = 0\n            train[name] = np.NaN\n        \n        for fold_id, (trn, val) in enumerate(self.fold_ids):\n            devel = train[features].iloc[trn]\n            y_devel = np.log1p(train[\"revenue\"].iloc[trn])\n            valid = train[features].iloc[val]\n            y_valid = np.log1p(train[\"revenue\"].iloc[val])\n                       \n            print(\"Fold \", fold_id, \":\")\n            model.fit(devel, y_devel, eval_set=[(valid, y_valid)], **fit_params)\n            \n            if len(model.feature_importances_) == len(features):  \n                model.FI['fold' + str(fold_id)] = model.feature_importances_ \/ model.feature_importances_.sum()\n\n            predictions = model.predict(valid)\n            predictions[predictions < 0] = 0\n            print(\"Fold \", fold_id, \" error: \", mean_squared_error(y_valid, predictions)**0.5)\n            \n            fold_score = score(train.iloc[val], predictions)\n            full_score += fold_score \/ len(self.fold_ids)\n            print(\"Fold \", fold_id, \" score: \", fold_score)\n            if prepare_stacking:\n                train[name].iloc[val] = predictions\n                \n                test_predictions = model.predict(test[features])\n                test_predictions[test_predictions < 0] = 0\n                test[name] += test_predictions \/ len(self.fold_ids)\n                \n        print(\"Final score: \", full_score)\n        return full_score","c29c14c5":"Kfolder = KFoldValidation(train)","66e7c2fe":"lgbmodel = lgb.LGBMRegressor(n_estimators=10000, \n                             objective='regression', \n                             metric='rmse',\n                             max_depth = 5,\n                             num_leaves=30, \n                             min_child_samples=100,\n                             learning_rate=0.01,\n                             boosting = 'gbdt',\n                             min_data_in_leaf= 10,\n                             feature_fraction = 0.9,\n                             bagging_freq = 1,\n                             bagging_fraction = 0.9,\n                             importance_type='gain',\n                             lambda_l1 = 0.2,\n                             bagging_seed=random_seed, \n                             subsample=.8, \n                             colsample_bytree=.9,\n                             use_best_model=True)","59edb3a4":"Kfolder.validate(train, test, features , lgbmodel, name=\"lgbfinal\", prepare_stacking=True) ","f0aeb1cf":"lgbmodel.FI.mean(axis=1).sort_values()[180:250].plot(kind=\"barh\",title = \"Features Importance\", figsize = (10,10))","5b6e288c":"xgbmodel = xgb.XGBRegressor(max_depth=5, \n                            learning_rate=0.01, \n                            n_estimators=10000, \n                            objective='reg:linear', \n                            gamma=1.45, \n                            seed=random_seed, \n                            silent=True,\n                            subsample=0.8, \n                            colsample_bytree=0.7, \n                            colsample_bylevel=0.5)","f63171f9":"Kfolder.validate(train, test, features, xgbmodel, name=\"xgbfinal\", prepare_stacking=True)","173c45b0":"catmodel = cat.CatBoostRegressor(iterations=10000, \n                                 learning_rate=0.01, \n                                 depth=5, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.8,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200,\n                                 random_seed=random_seed)","315b87cb":"Kfolder.validate(train, test, features , catmodel, name=\"catfinal\", prepare_stacking=True,\n               fit_params={\"use_best_model\": True, \"verbose\": 100})","d5f5cc9f":"train['Revenue_lgb'] = train[\"lgbfinal\"]\n\nprint(\"RMSE model lgb :\" ,score(train, train.Revenue_lgb),)\n\ntrain['Revenue_xgb'] = train[\"xgbfinal\"]\n\nprint(\"RMSE model xgb :\" ,score(train, train.Revenue_xgb))\n\ntrain['Revenue_cat'] = train[\"catfinal\"]\n\nprint(\"RMSE model cat :\" ,score(train, train.Revenue_cat))\n\ntrain['Revenue_Dragon1'] = 0.4 * train[\"lgbfinal\"] + \\\n                               0.2 * train[\"xgbfinal\"] + \\\n                               0.4 * train[\"catfinal\"]\n\nprint(\"RMSE model Dragon1 :\" ,score(train, train.Revenue_Dragon1))\n\ntrain['Revenue_Dragon2'] = 0.35 * train[\"lgbfinal\"] + \\\n                               0.3 * train[\"xgbfinal\"] + \\\n                               0.35 * train[\"catfinal\"]\n\nprint(\"RMSE model Dragon2 :\" ,score(train, train.Revenue_Dragon2))","d987e47e":"test['revenue'] =  np.expm1(test[\"lgbfinal\"])\ntest[['id','revenue']].to_csv('submission_lgb.csv', index=False)\ntest[['id','revenue']].head()","abc6b432":"test['revenue'] =  np.expm1(test[\"xgbfinal\"])\ntest[['id','revenue']].to_csv('submission_xgb.csv', index=False)\ntest[['id','revenue']].head()","193fe3cf":"test['revenue'] =  np.expm1(test[\"catfinal\"])\ntest[['id','revenue']].to_csv('submission_cat.csv', index=False)\ntest[['id','revenue']].head()","e1b63a7e":"test['revenue'] =  np.expm1(0.4 * test[\"lgbfinal\"]+ 0.4 * test[\"catfinal\"] + 0.2 * test[\"xgbfinal\"])\ntest[['id','revenue']].to_csv('submission_Dragon1.csv', index=False)\ntest[['id','revenue']].head()","b66cb457":"test['revenue'] =  np.expm1((test[\"lgbfinal\"] + test[\"catfinal\"] + test[\"xgbfinal\"])\/3)\ntest[['id','revenue']].to_csv('submission_Dragon2.csv', index=False)\ntest[['id','revenue']].head()","2211fe96":"**The data below comes from the kernel EDA, Feature Engineering, LGB+XGB+CAT of Kamal Chhirang**","31e72cb0":"# <a href='#6'>Feature engineering<\/a>","26c36b0f":"Contains dates per country","d52ba9c0":"# <a id='5'>EDA<\/a>  ","28902633":"# <a id='4'>Data exploration<\/a>  \n\n## <a id='41'>Check the data<\/a>  \n\nLet's check the train and test set.","a1974df9":"# <a href='#7'>Model<\/a>","82c649a5":"## Load data  ","4665c997":"**Dragon1** :","f1b243f4":"It is clear from the graph below that revenue depends a lot on budget and total_vote","ecaf5b07":"**Dragon2** :","a70c76ba":"Contains the variables totalVotes and vote_average","d1cf18b6":"## <a id='42'>Clean Data<\/a>    ","8041bbb3":"# <a id='1'>Introduction<\/a>  \n\nIn a world... where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's \"You had me at 'Hello.'\" For others, the trailer falls short of expectations and you think \"What we have here is a failure to communicate.\"\n\nIn this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.","b8805504":"**More to come. Stay tuned **\n\n# Thank you for upvoted if it was useful for you\n","21c25de3":"**CAT** :","55eb2716":"# <a id='2'>Prepare for data analysis<\/a>  \n\n\n## Load packages","b4fe4a3f":"# <a href='#8'>Submission<\/a>  ","d80f7591":"**LGB** :","4b2bc06c":"<h1><center><font size=\"6\">TMDB Box Office Prediction<\/font><\/center><\/h1>\n\n\n\n\n<img src=\"https:\/\/www.themoviedb.org\/assets\/2\/v4\/logos\/powered-by-square-blue-30582ce92620a08ce2ab8082fb19897a3c69093aa20e786cfe6a041e54e21b2b.svg\" width=\"500\"><\/img>\n\n\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>\n- <a href='#4'>Data exploration<\/a>   \n - <a href='#41'>Check the data<\/a>   \n  - <a href='#42'>Clean Data<\/a>   \n- <a href='#3'>External Data<\/a>    \n- <a href='#5'>EDA<\/a>     \n- <a href='#6'>Feature engineering<\/a>\n- <a href='#7'>Model<\/a>\n- <a href='#8'>Submission<\/a>  ","37c02f79":"# <a id='3'>External Data<\/a> ","e9fec915":"**XGB** :","7db54487":"### Correlation Matrix"}}