{"cell_type":{"70a5d582":"code","81416517":"code","b290739e":"code","cd3c9263":"code","c1078342":"code","3d314b1f":"code","b6b4b7bc":"code","fa2ee210":"code","c37139e9":"code","ce3940ec":"code","49456ace":"code","5e5c6553":"code","19579967":"code","2ac33af0":"code","81821b54":"code","473b47f5":"code","d565f7f3":"code","879eede0":"markdown","937927fc":"markdown","6bf50745":"markdown","e1a23a73":"markdown","07b96aff":"markdown","d17a4ee9":"markdown","5570e65a":"markdown","5248c94f":"markdown"},"source":{"70a5d582":"%cd \/kaggle\/input\/plantdisease\n!ls\n%cd \/kaggle\/working\n!ls","81416517":"# !git clone https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\n# %cd .\/EfficientNet-PyTorch\n# !pip install -e .","b290739e":"from PIL import Image\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\nimport numpy as np\nimport random\nimport time\nimport copy","cd3c9263":"import torch\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\nprint(torch.cuda.is_available())\n\n!nvidia-smi # gpu \ud655\uc778","c1078342":"batch_size = 64\n\ndata_path = '\/kaggle\/input\/plantdisease\/PlantVillage'\nleaf_datasets = datasets.ImageFolder(\n    data_path,\n    transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n)","3d314b1f":"random_seed = 33\nrandom.seed(random_seed)\ntorch.manual_seed(random_seed)\n\ndataset = {}\n\ntrain_idx, valtest_idx = train_test_split(list(range(len(leaf_datasets))), test_size=0.4, random_state=random_seed)\ndataset['train'] = Subset(leaf_datasets, train_idx)\nvaltest          = Subset(leaf_datasets, valtest_idx)\n\n\nval_idx, test_idx = train_test_split(list(range(len(valtest))), test_size=0.5, random_state=random_seed)\ndataset['valid'] = Subset(valtest, val_idx)\ndataset['test']  = Subset(valtest, test_idx)","b6b4b7bc":"print(len(dataset['train']))\nprint(len(dataset['valid']))","fa2ee210":"dataloaders, batch_num = {}, {}\ndataloaders['train'] = DataLoader(dataset['train'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['valid'] = DataLoader(dataset['valid'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['test'] = DataLoader(dataset['test'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\n\n\nbatch_num['train'], batch_num['valid'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['valid']), len(dataloaders['test'])\nprint('batch_size : %d,  test\/valid : %d \/ %d \/ %d' % (batch_size, batch_num['train'], batch_num['valid'], batch_num['test']))","c37139e9":"count = np.zeros(15)\n\nfor images, labels in dataloaders['train']:  \n    for a in labels:\n      count[a.numpy()] += 1\n\nprint(count)\n\n\nv_count = np.zeros(15)\n\nfor images, labels in dataloaders['valid']:  \n    for a in labels:\n      v_count[a.numpy()] += 1\n\nprint(v_count)","ce3940ec":"imgtest = None\nfor images, labels in dataloaders['train']:  \n    imgtest = images[3]\n    print(imgtest.shape)\n    break\n\nimgtest = imgtest.numpy()\nimgtest = np.moveaxis(imgtest, 0, -1)\nimshow(imgtest)","49456ace":"%cd \/kaggle\/working\/EfficientNet-PyTorch\n\nfrom efficientnet_pytorch import EfficientNet\n# model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=15)\nmodel = EfficientNet.from_name('efficientnet-b0', num_classes=15)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\ndev = torch.device(\"cuda\") ","5e5c6553":"model = model.to(dev)\n\noptimizer = optim.SGD(model.parameters(), \n                         lr = 0.0005,\n                         momentum=0.9,\n                         weight_decay=1e-4)\n\ncriterion = nn.CrossEntropyLoss()","19579967":"num_epochs = 70\ntrn_loss_list, val_loss_list, trn_acc_list, val_acc_list = [], [], [], []\nbest_acc = 0.0\n\nsince = time.time()\nbest_model = copy.deepcopy(model.state_dict())\n\n\nfor epoch in range(num_epochs):    \n    trn_loss, trn_corrects, val_loss, val_corrects = 0.0, 0, 0.0, 0\n    \n    model.train()\n    for i, (data, target) in enumerate(dataloaders['train']):\n        data, target = data.to(dev), target.to(dev)\n\n        optimizer.zero_grad()\n        output = model(data)                \n    \n        loss = criterion(output, target)\n        loss.backward()\n        \n        optimizer.step()  \n        \n        _, preds = torch.max(output, 1)\n        trn_corrects += torch.sum(preds == target.data)\n        trn_loss += loss.item()\n\n    model.eval()\n    with torch.no_grad():\n        for j, (val_data, val_target) in enumerate(dataloaders['valid']):\n            val_data, val_target = val_data.to(dev), val_target.to(dev)\n            \n            val_output = model(val_data)\n            v_loss = criterion(val_output, val_target)\n            \n            _, preds = torch.max(val_output, 1)\n            val_corrects += torch.sum(preds == val_target.data)\n            val_loss += v_loss.item()\n\n\n    trn_acc_list.append(trn_corrects * 0.008)\n    val_acc_list.append(val_corrects * 0.0242)\n    trn_loss_list.append(trn_loss\/batch_num['train'])\n    val_loss_list.append(val_loss\/batch_num['valid'])\n\n    time_elapsed = time.time() - since\n    print(\"epoch: {}\/{} | trn loss: {:.4f} | val loss: {:.4f} | {:.0f}m {:.0f}s elapsed\".format(\n                epoch+1, num_epochs, trn_loss \/ batch_num['train'], val_loss \/ batch_num['valid'], \n                time_elapsed \/\/ 60, time_elapsed % 60))\n\n    if val_corrects * 0.0242 > best_acc:\n      best_acc = val_corrects * 0.0242\n      best_model = copy.deepcopy(model.state_dict())\n      print(\"best model updated-epoch: {} | val_accuracy: {:.4f}\".format(epoch+1, best_acc))\n\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best valid Acc: %.1f' %(best_acc))\n\nmodel.load_state_dict(best_model)\ntorch.save(model.state_dict(), '\/kaggle\/working\/best_model.pt')\nprint(\"model saved\")","2ac33af0":"plt.plot(trn_loss_list, label='train_loss')\nplt.plot(val_loss_list, label='val_loss')\nplt.legend()","81821b54":"plt.plot(trn_acc_list, label='train_acc')\nplt.plot(val_acc_list, label='val_acc')\nplt.legend()","473b47f5":"best_model = EfficientNet.from_name('efficientnet-b0', num_classes=15)\nbest_model.load_state_dict(torch.load('\/kaggle\/working\/best_model.pt'))\n\nbest_model.to(dev)","d565f7f3":"corrects = 0\ntotal = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for j, (data, target) in enumerate(dataloaders['test']):\n        data, target = data.to(dev), target.to(dev)\n\n        output = best_model(data)\n        \n        _, preds = torch.max(output, 1)\n        corrects += torch.sum(preds == target.data)\n        total += len(preds)\n        \nprint(\"{}\/{} | test accuracy: {:.4f}\".format(corrects, total, corrects\/total))","879eede0":"- total number of train data = 12382\n- train acc = corrects \/ 12382 * 100 = corrects * **0.008**\n- total number of validation data = 4128\n- val acc = corrects \/ 4128 * 100 = corrects * **0.0242**","937927fc":"### Import Packages","6bf50745":"### EfficientNet\n\n- [efficientnet-torch](https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch)\n- About A PyTorch implementation of EfficientNet","e1a23a73":"### ImageFolder\n\n- https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\n- Mean and std used in code are calculated based on millions of images.","07b96aff":"## Training\n\n- Train and validation is done in 1 epoch\n- Trace train, val accuracy scores and losses\n- Save the model which has the highest validation accuracy","d17a4ee9":"### class distribution check","5570e65a":"### Data split\n\n- train(60) \/ valid(20) \/ test(20)","5248c94f":"### Directory Checking"}}