{"cell_type":{"f6c3641f":"code","0aea60e4":"code","93dcbe46":"code","364f368b":"code","5be7eca9":"code","f8a4e344":"code","fb17897b":"code","2ee1dc19":"code","2acfb453":"code","8157d0c5":"code","4d0933e3":"code","c2c90a55":"code","952f4bfc":"code","4c8f9d13":"code","08cad30d":"code","91e15966":"code","aa397924":"code","c0d07c83":"code","befedcaf":"code","0632ffc9":"code","dbc79f7e":"code","4d0a053f":"code","12f6f751":"code","04c97dc1":"code","b5d8a967":"code","8b11e7b4":"code","0afb042d":"code","a6cf4e16":"code","6da56c59":"code","7fcec03d":"code","57ce6467":"code","9fc7120c":"code","e7422212":"code","bf934684":"code","91fdffc8":"code","fa14c67a":"code","2616e49d":"code","24062925":"code","a1b2d811":"code","b58e0006":"code","6ec271cc":"code","f65d38c1":"code","b0153ef3":"code","4d8475ea":"code","90ddcdb4":"code","f80b5977":"code","a3fdeffe":"markdown","38e370d2":"markdown","c290ff87":"markdown","4b435c5d":"markdown","63560f4d":"markdown","9b0777a1":"markdown","07e4b6e4":"markdown","1da553f1":"markdown","6b4e078e":"markdown","dfe78f49":"markdown","13000e89":"markdown","b9845a3d":"markdown","38429132":"markdown","7109cc89":"markdown","f8744569":"markdown","71d32521":"markdown","c6cfb1e4":"markdown","7d99b8c3":"markdown","7c696b50":"markdown","6f693c29":"markdown","9e2bfbcb":"markdown","97ae5b3d":"markdown","6c5883d0":"markdown","a754b399":"markdown","22361680":"markdown","446a3c43":"markdown","c14afd9e":"markdown"},"source":{"f6c3641f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0aea60e4":"train = pd.read_csv('..\/input\/imbalanced-data-practice\/aug_train.csv')\ntest =  pd.read_csv('..\/input\/imbalanced-data-practice\/aug_test.csv')","93dcbe46":"print(train.shape)\ntrain.head()","364f368b":"print(test.shape)\ntest.head()","5be7eca9":"train.info()","f8a4e344":"# Check target variable (here binary)\ntrain['Response'].unique()","fb17897b":"# Check Response percent breakdown\nprint(f'Response Negative: {round(len(train[train[\"Response\"] == 0]) \/ train.shape[0], 4) * 100}%')\nprint(f'Response Positive: {round(len(train[train[\"Response\"] == 1]) \/ train.shape[0], 4) * 100}%')","2ee1dc19":"# Combine test and training sets\ncomb_df= pd.concat([train, test]).reset_index(drop=True)\ncomb_df.drop(columns='Response', inplace=True)\ncomb_df.info()","2acfb453":"# Check that all id values are unique\nlen(comb_df['id'].unique())","8157d0c5":"comb_df['Gender'].value_counts()","4d0933e3":"# Convert Male:0, Female:1\ncomb_df['Gender'].replace({'Male' : 0,\n                           'Female' : 1}, inplace=True)","c2c90a55":"comb_df['Age'].describe()","952f4bfc":"comb_df['Age'].hist(grid=False)","4c8f9d13":"# Convert Age values\nbins = [20, 30, 40, 50, 60, 85]\n\n# Create Bins\ncomb_df['age_bins'] = pd.cut(x=comb_df['Age'], bins=bins, labels=['20-30', '30-40', '40-50', '50-60', '60+'])\ncomb_df.drop(columns='Age', inplace=True)","08cad30d":"comb_df['age_bins'].value_counts()","91e15966":"# Create 'dummy' values for each bin\nage_dummies = pd.get_dummies(comb_df['age_bins'], prefix='age_range', drop_first=True)\ncomb_df = pd.concat([comb_df, age_dummies], axis=1)\ncomb_df.drop(columns='age_bins', inplace=True)\n\nprint(comb_df.shape)\ncomb_df.head()","aa397924":"comb_df['Driving_License'].value_counts()","c0d07c83":"len(train[(train['Driving_License']  == 0) &\n          (train['Response']  == 1)])","befedcaf":"comb_df.drop(columns='Driving_License', inplace=True)","0632ffc9":"comb_df['Region_Code'].unique()","dbc79f7e":"# Create 'dummy' values and combine them with data\nrc_dummies = pd.get_dummies(comb_df['Region_Code'], prefix='RC', drop_first=True)\ncomb_df = pd.concat([comb_df, rc_dummies], axis=1)\ncomb_df.drop(columns='Region_Code', inplace=True)\ncomb_df.shape","4d0a053f":"comb_df['Previously_Insured'].value_counts()","12f6f751":"comb_df['Vehicle_Age'].value_counts()","04c97dc1":"# Create Ordinal Values\n# Note: this is done because XGBoost doesn't allow < and > in\n#       in feature names\ncomb_df['Vehicle_Age'].replace({'1-2 Year' : 1,\n                                '< 1 Year' : 0,\n                                '> 2 Years' : 2}, inplace=True)\n\n# One-Hot-Encoding\nvehicle_age_dummies = pd.get_dummies(comb_df['Vehicle_Age'], prefix='v_age', drop_first=True)\ncomb_df = pd.concat([comb_df, vehicle_age_dummies], axis=1)\ncomb_df.drop(columns='Vehicle_Age', inplace=True)\nprint(comb_df.shape)","b5d8a967":"comb_df['Vehicle_Damage'].value_counts()","8b11e7b4":"# Convert Vehicle_Damage values\ncomb_df['Vehicle_Damage'].replace({'No' : 0,\n                                   'Yes' : 1}, inplace=True)","0afb042d":"comb_df['Annual_Premium'].describe()","a6cf4e16":"comb_df[comb_df['Annual_Premium'] > 100000]","6da56c59":"sns.boxplot(y=comb_df['Annual_Premium'], data=comb_df)","7fcec03d":"# Creating Bins for annual premium value ranges\n\n# Create Ordinal Values\ncomb_df.loc[comb_df['Annual_Premium'] < 30_000, 'Annual_Premium'] = 0\ncomb_df.loc[(comb_df['Annual_Premium'] >= 30_000) & (comb_df['Annual_Premium'] < 100_000) , 'Annual_Premium'] = 1\ncomb_df.loc[comb_df['Annual_Premium'] > 100_000, 'Annual_Premium'] = 2\n\n# One-Hot-Encoding\nyr_prem_dummies = pd.get_dummies(comb_df['Annual_Premium'], prefix='yr_prem', drop_first=True)\ncomb_df = pd.concat([comb_df, yr_prem_dummies], axis=1)\ncomb_df.drop(columns='Annual_Premium', inplace=True)\nprint(comb_df.shape)","57ce6467":"comb_df['Policy_Sales_Channel'].unique()","9fc7120c":"# One-Hot-Encoding\nrc_dummies = pd.get_dummies(comb_df['Policy_Sales_Channel'], prefix='PSC', drop_first=True)\ncomb_df = pd.concat([comb_df, rc_dummies], axis=1)\ncomb_df.drop(columns='Policy_Sales_Channel', inplace=True)\ncomb_df.shape","e7422212":"comb_df['Vintage'].hist()","bf934684":"# Create Bins\ncomb_df.loc[comb_df['Vintage'] < 50, 'Vintage'] = 1\ncomb_df.loc[(comb_df['Vintage'] >= 50) & (comb_df['Vintage'] < 100)  , 'Vintage'] = 2\ncomb_df.loc[(comb_df['Vintage'] >= 100) & (comb_df['Vintage'] < 150) , 'Vintage'] = 3\ncomb_df.loc[(comb_df['Vintage'] >= 150) & (comb_df['Vintage'] < 200) , 'Vintage'] = 4\ncomb_df.loc[(comb_df['Vintage'] >= 200) & (comb_df['Vintage'] < 250) , 'Vintage'] = 5\ncomb_df.loc[(comb_df['Vintage'] >= 250) & (comb_df['Vintage'] < 300) , 'Vintage'] = 6","91fdffc8":"comb_df['Vintage'].value_counts()","fa14c67a":"# One-Hot-Encoding\nvintage_dummies = pd.get_dummies(comb_df['Vintage'], prefix='vintage', drop_first=True)\ncomb_df = pd.concat([comb_df, vintage_dummies], axis=1)\ncomb_df.drop(columns='Vintage', inplace=True)\nprint(comb_df.shape)\ncomb_df.head()\n\nprint(comb_df.shape)","2616e49d":"# Creating Training, Testing, and Target variables \nX_train = comb_df[: train.shape[0]].drop(columns='id')\ny = train['Response']\n\nX_test = comb_df[train.shape[0] :].drop(columns='id')\nX_test_ids = test['id']","24062925":"def run_model(x, y, name, t):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n    print(X_train.shape)\n    print(X_test.shape)\n    \n    if name == 'logistic_bal':\n        model = LogisticRegression(max_iter=1000,\n                                   solver='liblinear',\n                                   class_weight='balanced')\n    elif name == 'logistic':\n        model = LogisticRegression(max_iter=1000,\n                                   solver='liblinear')\n    elif name == 'xgb':\n        model = XGBClassifier(scale_pos_weight=19.59)\n    else:\n        print('Error, Incorrect Model')\n\n    # Cross-Validation method 1:  cross_val_predict()\n    cv_pred = cross_val_predict(model, X_train, y_train, cv=5)\n    print(f'Training Data CV Score Method 1: {np.round(metrics.accuracy_score(y_train, cv_pred),4) * 100}%') \n        \n    # Cross-Validation method 2:  cross_val_score()\n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n    cv_result = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n    print(f'Training Data CV Score Method 2: {np.round(cv_result.mean(),4) * 100}%')\n\n    # Fit the Model\n    model.fit(X_train, y_train)\n    \n    # Printed Results\n    if t == 'train':\n        # Classification Report\n        y_pred = model.predict(X_test)\n        print(f'Testing Data Accuracy Score: {np.round(metrics.accuracy_score(y_test, y_pred), 4) * 100}%')\n        print(f'\\n{name} Test Prediction Classification Report:')\n        print('------------------------------------------------------------')\n        print(metrics.classification_report(y_test, y_pred))\n\n        # Confustion Matrix Heat Map\n        sns.heatmap(metrics.confusion_matrix(y_test,y_pred), annot=True, fmt=\".0f\")\n        plt.title(f'{name} confustion matrix')\n        plt.xlabel('Predicted Values')\n        plt.ylabel('Actual Values')\n        plt.show()\n    else:\n        return model","a1b2d811":"run_model(X_train, y, 'logistic', 'train')","b58e0006":"run_model(X_train, y, 'logistic_bal', 'train')","6ec271cc":"run_model(X_train, y, 'xgb', 'train')","f65d38c1":"# Selecting and traning ideal model (Balanced Logistic Regression)\nbest_model = run_model(X_train, y, 'logistic_bal', 'test')","b0153ef3":"# Get predictions for test data\ntest_predictions = best_model.predict(X_test)\ntest_predictions","4d8475ea":"data = {'id'  : X_test_ids.to_numpy(),\n        'response' : test_predictions}\n\n\nfinal_df = pd.DataFrame(data, columns = ['id','response'])\n\nfinal_df.head()","90ddcdb4":"final_df['response'].value_counts()","f80b5977":"final_df.to_csv('submission.csv', index=False)","a3fdeffe":"---\n# Testing Different Models\nNote: in my actual analysis I tested many different models. However, for this notebook I only kept logistic regression and XGBoost as they had the best results. \n\n\n## Unbalanced Logistic Regression","38e370d2":"### 10. Vintage\nThis metric represents the number of days a customer has been insured up until now and is therefore ordinal in nature.","c290ff87":"### There are three metrics for Vehicle_Age and I will change the values like so:\n* Between 1-2 Years : 1\n* Less than 1 Year : 0\n* Greater than 2 Years : 2","4b435c5d":"### Similar to 'Region_Code', the 'Policy_Sales_Channel' feature is numerical but the values are NOT ordered. Therefore I will need to convert this by using one hot encoding. This will create a large number of new features.","63560f4d":"### There is a wide disburtion of annual premium amounts with the high end (>100k) being outliers. I could handle this in a few ways:\n1. Remove the outliers (not ideal)\n2. Create bins for value ranges (better)\n3. Do nothing and scale the values\n\nI am going to start by created the different ranges for the data like so:\n* less than 30k : 0\n* between 30k and 100k : 1\n* greater than or equal to 100k: 2","9b0777a1":"### 6. Vehicle_Age","07e4b6e4":"### 7. Vehicle_Damage ","1da553f1":"### The target variable is highly imbalanced with approximately 16% of its values being positive.\n\n---\n\n### Next I combine the test and training data sets so that any changes performed will only have to be done once.","6b4e078e":"### XGBoost further improved the postive responses to a 98% success rate, however the negative missclassifications increased even more than the balanced logistic regression\n\n---\n## Conclusions\n\n### The goal of this task was to predict whether a customer would be interested in Vehicle Insurance. In this analysis I used some basic models with very little tweaking. The best results came from using Balanced Logistic Regression where I was able to:\n* Achieve an approximate 92% success rate on predicting customers who  WILL purchase insurance\n\n\n* Acheive an approximate 74% success rate on predicting customers who WILL NOT purchase insurance\n\n\n### From a business and advertising costs perspective, if the revenue from new customers is greater than the costs from the increased number of missclassied customers then the Balanced Logistic Regression model would work.\n\n---\n\n## Running the model on the testing data","dfe78f49":"### 5. Previously_Insured","13000e89":"---\n## Feature Examination & Adjustement\n\n### 1. Gender","b9845a3d":"### 4. Region_Code","38429132":"### 3. Driving_License","7109cc89":"### The vast majority of ages are between 20 and 50, I opt to create bins here to reduce the number of potential ages","f8744569":"### Because only 37 of the people without licenses are actually wanting insurance, I don't see much use for this feature without something else to go along with it. Such as a feature that asks if a person will be getting a new vehicle soon (or a drivers license). Therefore, I opt to remove it.","71d32521":"\n### Gender has a pretty even split between male and female customers, next I will replace the values like so:\n* Male : 0\n* Female : 1","c6cfb1e4":"## 9. Policy_Sales_Channel \t","7d99b8c3":"### 'Previously_Insured' has a pretty even split between negative and positive responses and  already has values of 0 and 1, so no further action is required on my end.","7c696b50":"### There are no null values in the data\n---\n### Next I examine the target variable 'Response'","6f693c29":"### Basic unbalanced Logistic Regression performs ok on paper with a score of 84%, however it is misclassifying the positive responses and only gets roughly 36% correct which is unacceptable.\n\n---\n## Balanced Logistic Regression","9e2bfbcb":"### Less than 1% of the samples are individuals without a license. People without a drivers license would not usually be interested in vehicle insurance, therefore I'm going to check to see if any of these individuals actually have a positive response:","97ae5b3d":"### 8. Annual_Premium","6c5883d0":"### Balanced Logistic Regression drasticaly improves the prediction success of positive responses with a 92% success rate. However, the negative responses are now getting classified incorrectly more often. Still the success rate of the negative classifications is 74%.","a754b399":"### There are many different regions being represented in this data, the values are NOT ordinal, therefore I am going to use one hot encoding (i.e. dummy values) for each region. This will considerably add to the number of features, but should prevent any erroneaous calculations by sklearn.","22361680":"### 2. Age","446a3c43":"### Because these values are ordinal, I could just leave this featuer as-is, however because the value ranges are so evenly distributed I'm going to create bins for this as well using 50 day increments.","c14afd9e":"---\n## Modeling \nAt this point the data should be good to go for traning and testing purposes however I must first re-separate the training and testing datasets now that I am done with all the editing."}}