{"cell_type":{"24c4f673":"code","783caddf":"code","02203f14":"code","84cdd1b0":"code","2dbe00b7":"code","58c511ed":"code","2722719f":"code","9adc7419":"code","8753ceba":"code","6501ac69":"code","7ce7c5e8":"code","c0c013d0":"code","5e538606":"code","4c9cba07":"code","44b00e4f":"code","c63cbcc9":"code","af285321":"code","393f7126":"code","3389cfef":"code","bb62b754":"code","6c7ad844":"code","586e19d9":"code","fab9c7e2":"code","9d472830":"code","85f2da83":"code","9f73d31b":"code","074d7d54":"code","cae912bd":"code","a9605485":"code","c4a89901":"code","7dcdea3c":"code","4eb922e9":"code","135d2ec1":"code","89a62013":"code","6c30e441":"code","014f2c9b":"code","b7df98e8":"code","74a68e1e":"code","de252daf":"code","defc00d2":"code","7cf5e035":"code","53c93916":"code","94ade35b":"code","71822d4c":"code","8fe9fdb5":"code","d565200c":"code","03e9d376":"code","481ef4e2":"code","59af42e7":"code","d2df3640":"code","241f8d82":"code","56621d32":"code","297264f4":"code","906ca49e":"code","5ea57e0e":"code","176f3119":"code","e1e07738":"code","617c7ff8":"code","739bef2d":"code","3697a5c7":"code","48d5c52d":"code","0a4e97a8":"code","2a1c4eb5":"code","a0b5c4f0":"code","e8673b50":"code","e8afeee8":"code","8e53bd88":"code","54d7dfc9":"code","beead072":"code","2791f77c":"code","8687d16f":"code","1b56952d":"code","233dcbbc":"code","7595041e":"code","44bc5726":"code","72752adc":"code","1e52782f":"code","6819ef6f":"code","08b9fd46":"code","8dec82bf":"code","ac4e0b69":"code","4fd33ef6":"code","9c8263d3":"code","bf6f84a2":"code","3f1f9161":"code","6382fcba":"code","3ef62ff6":"code","879be92f":"code","d41d89db":"code","0479ab44":"code","fd84e1c9":"code","996627fd":"code","761cd439":"code","0a750785":"code","c3ab139b":"code","7276a83c":"code","c32519b4":"code","84161dc0":"code","ec108a8b":"code","37031acd":"code","9c5c28bf":"code","45fc7d17":"code","dbe4aca3":"code","256934ed":"code","20e629c6":"code","e978a2ce":"code","fdf967e7":"code","d86948c6":"code","cc16bdb1":"code","37246284":"code","24eb2693":"code","987e1a78":"code","b8b3a6bf":"code","8a6f4564":"code","3b5a8ee8":"code","2474c929":"code","ef051913":"code","3fa3e6b9":"code","f33837c5":"code","175b5198":"code","ba6d1892":"code","bccedf27":"code","03ca6d93":"code","f41bb038":"code","ef187c70":"code","691aa747":"code","e29f9a1b":"code","6fd1b440":"code","de3ad26c":"code","23461a5e":"code","dd39ce2f":"code","9c0d14a7":"code","f74fc448":"code","b824195f":"code","c44a54b0":"code","aa17f3d9":"code","043f4da4":"code","e5af45b8":"code","4ed02e8d":"code","d6a97f0a":"code","1e3924ac":"code","64cc8c3c":"code","34dcade2":"code","e4885a4d":"markdown","17779306":"markdown","c44768d2":"markdown","b1e7b40f":"markdown","c3ade7ed":"markdown","77523cd2":"markdown","c7bc5cce":"markdown","e5fb8677":"markdown","81fa721d":"markdown","357feb78":"markdown","bf435d64":"markdown","ee82b24a":"markdown","71316d4c":"markdown","b5d6d19e":"markdown","4b19140f":"markdown","4b7174cf":"markdown","a019f63b":"markdown","c520894f":"markdown","94d70f98":"markdown","8543a506":"markdown","ea772310":"markdown","0dfbf20d":"markdown","300f9482":"markdown","3ad0f093":"markdown","e125b1db":"markdown","c16d6611":"markdown","2239c8f9":"markdown","ea0edcba":"markdown","19619827":"markdown","f530efdc":"markdown","caba25c7":"markdown","9bf0a190":"markdown","af1c369f":"markdown","79f74f20":"markdown","d2de7fc1":"markdown","36caa0a7":"markdown","5d074281":"markdown","ce4a997b":"markdown","b52d4c27":"markdown","98208ddf":"markdown","5ce037c2":"markdown","497765b2":"markdown","e3547ad2":"markdown","5ffb9049":"markdown","f1dacf90":"markdown","cb892d2b":"markdown","2a130c86":"markdown","fd81c9f9":"markdown","85f785ba":"markdown","485c50b4":"markdown","2ef3a822":"markdown","c9441081":"markdown","1302b5d4":"markdown","8db9f580":"markdown","db2b0531":"markdown","ec5552b7":"markdown"},"source":{"24c4f673":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","783caddf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","02203f14":"df=pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv', index_col='job_id')\ndf.head()","84cdd1b0":"df.shape","2dbe00b7":"df.dtypes","58c511ed":"df.duplicated().value_counts()","2722719f":"df[df.fraudulent==0].duplicated().value_counts(normalize=True)","9adc7419":"df[df.fraudulent==1].duplicated().value_counts(normalize=True)","8753ceba":"df.drop_duplicates(inplace=True)","6501ac69":"df.fraudulent.value_counts()","7ce7c5e8":"df.fraudulent.value_counts(normalize=True)","c0c013d0":"df.describe(include='all')","5e538606":"df.isna().sum()","4c9cba07":"df_na=df.fillna('na') ","44b00e4f":"real=df_na[df_na.fraudulent==0]\nreal.describe(include='all')","c63cbcc9":"fake=df_na[df_na.fraudulent==1]\nfake.describe(include='all')","af285321":"fake.sample(10)","393f7126":"fake.loc[17512,:]","3389cfef":"fake.loc[3609, :]","bb62b754":"fake.loc[7655, :]","6c7ad844":"fake.loc[9843, :]","586e19d9":"fake.loc[181,:]","fab9c7e2":"fake.loc[fake.description=='na']","9d472830":"fake.loc[11543, 'description']","85f2da83":"na_rates=pd.DataFrame([col, len(real.loc[real[col]=='na'])\/len(real[col]), len(fake.loc[fake[col]=='na'])\/len(fake[col]) ] for col in df.columns)","9f73d31b":"na_rates.columns=['column','real_na_rates','fake_na_rates']","074d7d54":"na_rates","cae912bd":"from statsmodels.stats.proportion import proportions_ztest\nfor col in df.columns:\n    counts=np.array([len(real.loc[real[col]=='na']), len(fake.loc[fake[col]=='na'])])\n    nobs=np.array([len(real[col]), len(fake[col])])\n    if (counts.sum() !=0) and (nobs.sum() !=0):\n        na_rates.loc[na_rates.column==col,'zstat'], na_rates.loc[na_rates.column==col,'p_value']=proportions_ztest(count=counts, nobs=nobs,  alternative='two-sided')","a9605485":"na_rates['significant_diff']=na_rates['p_value']<0.005","c4a89901":"na_rates","7dcdea3c":"df_na['company_profile_length']=df_na.apply(lambda row: len(row.company_profile), axis=1)\ndf_na['description_length']=df_na.apply(lambda row: len(row.description), axis=1)\ndf_na['requirements_length']=df_na.apply(lambda row: len(row.requirements), axis=1)\ndf_na['benefits_length']=df_na.apply(lambda row: len(row.benefits), axis=1)\ndf_na['total_text_length']=df_na['company_profile_length']+df_na['description_length']+df_na['requirements_length']+df_na['benefits_length']","4eb922e9":"df_na.head()","135d2ec1":"sns.displot(df_na, x='company_profile_length', hue='fraudulent', stat='density', bins=20, common_norm=False, multiple='dodge')","89a62013":"ax=sns.displot(df_na, x='company_profile_length', hue='fraudulent', kind='kde', common_norm=False, cut=0)\nax.set(xscale=\"log\")","6c30e441":"sns.displot(df_na, x='description_length', hue='fraudulent', stat='density', bins=20, common_norm=False, multiple='dodge')","014f2c9b":"ax=sns.displot(df_na, x='description_length', hue='fraudulent', kind='kde', common_norm=False, cut=0)\nax.set(xscale=\"log\")","b7df98e8":"ax=sns.displot(df_na, x='requirements_length', hue='fraudulent', kind='kde', common_norm=False, cut=0)\nax.set(xscale=\"log\")","74a68e1e":"ax=sns.displot(df_na, x='benefits_length', hue='fraudulent', kind='kde', common_norm=False, cut=0)\nax.set(xscale=\"log\")","de252daf":"ax=sns.displot(df_na, x='total_text_length', hue='fraudulent', kind='kde', common_norm=False, cut=0)\nax.set(xscale=\"log\")","defc00d2":"sns.catplot(data=df_na, y='fraudulent', x='telecommuting', kind='bar')","7cf5e035":"sns.catplot(data=df_na, y='fraudulent', x='has_company_logo', kind='bar')","53c93916":"sns.catplot(data=df_na, y='fraudulent', x='has_questions', kind='bar')","94ade35b":"sns.catplot(data=df_na, x='fraudulent', y='employment_type', kind='bar')","71822d4c":"sns.catplot(data=df_na, x='fraudulent', y='required_experience', kind='bar')","8fe9fdb5":"sns.catplot(data=df_na, x='fraudulent', y='required_education', kind='bar')","d565200c":"sns.catplot(data=df_na, x='fraudulent', y='function', kind='bar')","03e9d376":"df_na.industry.value_counts()[:20]","481ef4e2":"fake.industry.value_counts()[:20]","59af42e7":"real.industry.value_counts()[:20]","d2df3640":"industry=pd.crosstab(df_na.industry, df_na.fraudulent, normalize='index', margins=True)\nindustry.iloc[:,1].sort_values(ascending=False)[:20].to_frame(name='fraud rates')","241f8d82":"df_na.title.value_counts()[:20]","56621d32":"real.title.value_counts()[:20]","297264f4":"fake.title.value_counts()[:20]","906ca49e":"fake.title.str.contains('$', regex=False).value_counts(normalize=True)","5ea57e0e":"real.title.str.contains('$', regex=False).value_counts(normalize=True)","176f3119":"df_na.location.value_counts()[:20]","e1e07738":"real.location.value_counts()[:20]","617c7ff8":"fake.location.value_counts()[:20]","739bef2d":"df.location=df.location.str[:2]","3697a5c7":"df.location.value_counts()","48d5c52d":"real.location=real.location.str[:2]\nreal.location.value_counts(normalize=True).head(10)","0a4e97a8":"fake.location=fake.location.str[:2]\nfake.location.value_counts(normalize=True).head(10)","2a1c4eb5":"df.salary_range.value_counts()","a0b5c4f0":"real.salary_range.value_counts(normalize=True)","e8673b50":"fake.salary_range.value_counts(normalize=True)","e8afeee8":"real_text=real.title+' '+real.company_profile+' '+real.description+' '+real.requirements+' '+real.benefits","8e53bd88":"real_text_frame=real_text.to_frame(name='text')","54d7dfc9":"fake_text=fake.title+' '+fake.company_profile+' '+fake.description+' '+fake.requirements+' '+fake.benefits","beead072":"fake_text_frame=fake_text.to_frame(name='text')","2791f77c":"fake_text.str.contains('#URL', regex=False).value_counts(normalize=True)","8687d16f":"real_text.str.contains('#URL', regex=False).value_counts(normalize=True)","1b56952d":"fake_text.str.contains('#EMAIL', regex=False).value_counts(normalize=True)","233dcbbc":"real_text.str.contains('#EMAIL', regex=False).value_counts(normalize=True)","7595041e":"fake_text.str.contains('#PHONE', regex=False).value_counts(normalize=True)","44bc5726":"real_text.str.contains('#PHONE', regex=False).value_counts(normalize=True)","72752adc":"fake_text[9835]","1e52782f":"!pip install word2number","6819ef6f":"!pip install contractions","08b9fd46":"from bs4 import BeautifulSoup\nimport spacy\nimport unidecode\nfrom word2number import w2n\nimport contractions\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# exclude words from spacy stopwords list\ndeselect_stop_words = ['no', 'not']\nfor w in deselect_stop_words:\n    nlp.vocab[w].is_stop = False\n\n\ndef strip_html_tags(text):\n    \"\"\"remove html tags from text\"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text(separator=\" \")\n    return stripped_text\n\n\ndef remove_whitespace(text):\n    \"\"\"remove extra whitespaces from text\"\"\"\n    text = text.strip()\n    return \" \".join(text.split())\n\n\ndef remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. caf\u00e9\"\"\"\n    text = unidecode.unidecode(text)\n    return text\n\n\ndef expand_contractions(text):\n    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n    text = contractions.fix(text)\n    return text\n\n\ndef text_preprocessing(text, accented_chars=True, contractions=True, \n                       convert_num=True, extra_whitespace=True, \n                       lemmatization=True, lowercase=True, punctuations=True,\n                       remove_html=True, remove_num=True, special_chars=True, \n                       stop_words=True):\n    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n    if remove_html == True: #remove html tags\n        text = strip_html_tags(text)\n    if extra_whitespace == True: #remove extra whitespaces\n        text = remove_whitespace(text)\n    if accented_chars == True: #remove accented characters\n        text = remove_accented_chars(text)\n    if contractions == True: #expand contractions\n        text = expand_contractions(text)\n    if lowercase == True: #convert all characters to lowercase\n        text = text.lower()\n\n    doc = nlp(text) #tokenise text\n\n    clean_text = []\n    \n    for token in doc:\n        flag = True\n        edit = token.text\n        # remove stop words\n        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n            flag = False\n        # remove punctuations\n        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n            flag = False\n        # remove special characters\n        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n            flag = False\n        # remove numbers\n        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n        and flag == True:\n            flag = False\n        # convert number words to numeric numbers\n        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n            edit = w2n.word_to_num(token.text)\n        # convert tokens to base form\n        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n            edit = token.lemma_\n        # append tokens edited and not removed to list \n        if edit != \"\" and flag == True:\n            clean_text.append(edit)        \n    return clean_text","8dec82bf":"real_text_frame['clean']=real_text_frame.apply(lambda x: text_preprocessing(x['text']), axis=1)\nreal_text_frame['clean']=real_text_frame.apply(lambda x: \" \".join(x['clean']), axis=1)","ac4e0b69":"from collections import Counter\n\nreal_total_text = [text for text in real_text_frame['clean']]\nreal_total_text = ' '.join(real_total_text).split()\n\nreal_counts = Counter(real_total_text)\n\nreal_common_words = [word[0] for word in real_counts.most_common(20)]\nreal_common_counts = [word[1] for word in real_counts.most_common(20)]\n\nfig = plt.figure(figsize=(18,6))\nsns.barplot(x=real_common_words, y=real_common_counts)\nplt.title('Most Common Words used in Real Job Ads')\nplt.show()","4fd33ef6":"fake_text_frame['clean']=fake_text_frame.apply(lambda x: text_preprocessing(x['text']), axis=1)\nfake_text_frame['clean']=fake_text_frame.apply(lambda x: \" \".join(x['clean']), axis=1)","9c8263d3":"from collections import Counter\n\nfake_total_text = [text for text in fake_text_frame['clean']]\nfake_total_text = ' '.join(fake_total_text).split()\n\nfake_counts = Counter(fake_total_text)\n\nfake_common_words = [word[0] for word in fake_counts.most_common(20)]\nfake_common_counts = [word[1] for word in fake_counts.most_common(20)]\n\nfig = plt.figure(figsize=(18,6))\nsns.barplot(x=fake_common_words, y=fake_common_counts)\nplt.title('Most Common Words used in Fake Job Ads')\nplt.show()","bf6f84a2":"finaldf=df.copy()","3f1f9161":"finaldf['missing_company_profile']=finaldf.company_profile.isnull().astype(int)","6382fcba":"finaldf['missing_salary_range']=finaldf.company_profile.isnull().astype(int)","3ef62ff6":"finaldf.fillna('na', inplace=True)","879be92f":"text_columns=['title','company_profile','description','requirements','benefits']\nfor column in text_columns:\n    finaldf.loc[finaldf[column]=='na', column]=' '","d41d89db":"finaldf['title_mention_$']=finaldf.title.str.contains('$', regex=False).astype(int)","0479ab44":"def location_transform(location):\n    country_list=['US','GB','CA','DE','NZ','AU','IN','MY','na']\n    country=location[:2]\n    if country in country_list:\n        return country\n    else:\n        return 'other'\n    \nfinaldf.location=finaldf.apply(lambda x: location_transform(x['location']), axis=1)","fd84e1c9":"finaldf.location.value_counts()","996627fd":"finaldf['text']=finaldf.title+' '+finaldf.company_profile+' '+finaldf.description+' '+finaldf.requirements+' '+finaldf.benefits\nfinaldf['email_link']=finaldf.text.str.contains('#EMAIL', regex=False).astype(int)\nfinaldf['phone_link']=finaldf.text.str.contains('#PHONE', regex=False).astype(int)","761cd439":"import math\nfinaldf['text_length']=finaldf.apply(lambda x: math.log(len(x['text'])+1), axis=1) \n#putting the length in logarithm to transform the skew distribution, add one to avoid the error of log(0)","0a750785":"# creating the \"cleaned text\" column\nfinaldf['cleaned_text']=finaldf.apply(lambda x: text_preprocessing(x['text']), axis=1)\nfinaldf['cleaned_text']=finaldf.apply(lambda x: \" \".join(x['cleaned_text']), axis=1)","c3ab139b":"finaldf","7276a83c":"from sklearn.feature_selection import mutual_info_classif\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","c32519b4":"feature_set=['location','missing_salary_range','missing_company_profile','telecommuting','has_company_logo','has_questions',\n             'employment_type','required_experience','required_education','industry','function','title_mention_$',\n             'email_link','phone_link','text_length']\nscores=make_mi_scores(finaldf[feature_set], finaldf['fraudulent'])\nplot_mi_scores(scores)","84161dc0":"finaldf_getdummy=pd.get_dummies(data=finaldf, columns=['location','employment_type','required_experience',\n                                                       'required_education','industry','function'])","ec108a8b":"finaldf_getdummy","37031acd":"X=finaldf_getdummy.drop(['fraudulent','title','department','salary_range','company_profile','description','requirements','benefits'], axis=1)\ny=finaldf_getdummy['fraudulent']","9c5c28bf":"X","45fc7d17":"scores=make_mi_scores(X,y)[2:32]\nplot_mi_scores(scores)","dbe4aca3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state=50, stratify=y)","256934ed":"X_train1=X_train[y_train==0].iloc[0::3].append(X_train[y_train==1])\nX_train2=X_train[y_train==0].iloc[1::3].append(X_train[y_train==1])\nX_train3=X_train[y_train==0].iloc[2::3].append(X_train[y_train==1])","20e629c6":"y_train1=y_train[y_train==0].iloc[0::3].append(y_train[y_train==1])\ny_train2=y_train[y_train==0].iloc[1::3].append(y_train[y_train==1])\ny_train3=y_train[y_train==0].iloc[2::3].append(y_train[y_train==1])","e978a2ce":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(sampling_strategy=0.6, random_state= 42)\nX_train1_resampled, y_train1_resampled = ros.fit_resample(X_train1, y_train1)\nX_train2_resampled, y_train2_resampled = ros.fit_resample(X_train2, y_train2)\nX_train3_resampled, y_train3_resampled = ros.fit_resample(X_train3, y_train3)","fdf967e7":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\n\nvectorizer = CountVectorizer(ngram_range=(1,2)) # looking for both unigrams and bigrams\nclf = LinearSVC(C=0.01, class_weight='balanced', random_state=42)\nbowpipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\nbowpipe.fit(X_train1_resampled['cleaned_text'], y_train1_resampled)\nbow_predict = bowpipe.predict(X_test['cleaned_text'])","d86948c6":"from sklearn.metrics import accuracy_score, classification_report\nprint(\"accuracy:\", accuracy_score(y_test, bow_predict))\nprint(classification_report(y_test, bow_predict))\n","cc16bdb1":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm_bow=confusion_matrix(y_test, bow_predict)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm_bow)\ndisp.plot()\nplt.show()","37246284":"def printNMostInformative(vectorizer, clf, N):\n    feature_names = vectorizer.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n    topClass0 = coefs_with_fns[:N]\n    topClass1 = coefs_with_fns[:-(N + 1):-1]\n    print(\"Class 0 best: \")\n    for feat in topClass0:\n        print(feat)\n    print(\"Class 1 best: \")\n    for feat in topClass1:\n        print(feat)\n\nprint(\"Top 30 features used to predict: \")\nprintNMostInformative(vectorizer, clf, 30)\ntransform = vectorizer.fit_transform(X_train1_resampled['cleaned_text'], y_train1_resampled)\n\nvocab = vectorizer.get_feature_names()\nfor i in range(len(X_train1_resampled['cleaned_text'])):\n    s = \"\"\n    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n    for idx, num in zip(indexIntoVocab, numOccurences):\n        s += str((vocab[idx], num))","24eb2693":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\nt_vectorizer = TfidfVectorizer(ngram_range=(1,2))\nt_clf = RandomForestClassifier(class_weight='balanced', random_state=42)\nt_bowpipe = Pipeline([('vectorizer', t_vectorizer), ('clf', t_clf)])\nt_bowpipe.fit(X_train2_resampled['cleaned_text'], y_train2_resampled)\nt_bow_predict = t_bowpipe.predict(X_test['cleaned_text'])","987e1a78":"print(\"accuracy:\", accuracy_score(y_test, t_bow_predict))\nprint(classification_report(y_test, t_bow_predict))","b8b3a6bf":"cm_t_bow=confusion_matrix(y_test, t_bow_predict)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm_t_bow)\ndisp.plot()\nplt.show()","8a6f4564":"import eli5\neli5.explain_weights(t_bowpipe, vec=t_vectorizer, top=30, targets=y_train2_resampled)","3b5a8ee8":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = ['text_length']\nct = ColumnTransformer([('num', StandardScaler(), num_cols)], remainder='passthrough') \n# Standardize the value of numerical features to avoid overweighted in training","2474c929":"Xtr3=X_train3_resampled.drop(['text','cleaned_text'], axis=1)\nXte=X_test.drop(['text','cleaned_text'], axis=1)","ef051913":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nxgbc=XGBClassifier(use_label_encoder=False, objective='binary:logistic', eval_metric='error',\n                   eta=0.7, gamma=0,\n                  max_depth=7, min_child_weight=1, random_state=42)\nxgbcpipe = Pipeline([('preprocessor', ct), ('clf', xgbc)])\nxgbcpipe.fit(Xtr3, y_train3_resampled)\nxgbc_predict = xgbcpipe.predict(Xte)","3fa3e6b9":"print(\"accuracy:\", accuracy_score(y_test, xgbc_predict))\nprint(classification_report(y_test, xgbc_predict))","f33837c5":"cm_xgbc=confusion_matrix(y_test, xgbc_predict)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm_xgbc)\ndisp.plot()\nplt.show()","175b5198":"#showing importance by features using xgboost\npd.DataFrame({'Variable':Xtr3.columns,\n              'Importance':[round(v,2) for v in xgbc.feature_importances_]}) \\\n  .sort_values('Importance', ascending=False) \\\n  .style.bar(color=['grey', 'lightblue'], align='zero')","ba6d1892":"ensemble_test_score=[bow_predict[i]+t_bow_predict[i]+xgbc_predict[i] for i in range(len(bow_predict))]\nensemble_test_predict=[int(i > 1) for i in ensemble_test_score]","bccedf27":"print(\"accuracy:\", accuracy_score(y_test, ensemble_test_predict))\nprint(classification_report(y_test, ensemble_test_predict))","03ca6d93":"cm_ensemble_test=confusion_matrix(y_test, ensemble_test_predict)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm_ensemble_test)\ndisp.plot()\nplt.show()","f41bb038":"#apply the ensemble models for the whole dataset\nbow_svm_predict_full=bowpipe.predict(X['cleaned_text'])\nbow_rf_predict_full=t_bowpipe.predict(X['cleaned_text'])\nxgbc_predict_full = xgbcpipe.predict(X.drop(['text','cleaned_text'], axis=1))","ef187c70":"finaldf['bow_svm_predict_full']=bow_svm_predict_full\nfinaldf['bow_rf_predict_full']=bow_rf_predict_full\nfinaldf['xgbc_predict_full']=xgbc_predict_full","691aa747":"finaldf['ensemble_total']=finaldf['bow_svm_predict_full']+finaldf['bow_rf_predict_full']+finaldf['xgbc_predict_full']\nfinaldf['ensemble_predict']=finaldf.apply(lambda x: int(x['ensemble_total'] >1), axis=1)","e29f9a1b":"print(\"accuracy:\", accuracy_score(y, finaldf['ensemble_predict']))\nprint(classification_report(y, finaldf['ensemble_predict']))","6fd1b440":"cm_ensemble=confusion_matrix(y, finaldf['ensemble_predict'])\ndisp=ConfusionMatrixDisplay(confusion_matrix = cm_ensemble)\ndisp.plot()\nplt.show()","de3ad26c":"tp_2pt=finaldf[(finaldf.ensemble_predict==1) & (finaldf.fraudulent==1) & (finaldf.ensemble_total==2)]","23461a5e":"len(tp_2pt[tp_2pt.bow_svm_predict_full==0])","dd39ce2f":"len(tp_2pt[tp_2pt.bow_rf_predict_full==0])","9c0d14a7":"len(tp_2pt[tp_2pt.xgbc_predict_full==0])","f74fc448":"fn=finaldf[(finaldf.ensemble_predict==0) & (finaldf.fraudulent==1)]","b824195f":"fn","c44a54b0":"fn.loc[1203]","aa17f3d9":"eli5.show_prediction(clf, X['cleaned_text'].loc[1203], show_feature_values=True,\n                        vec=vectorizer, targets=[0,1], target_names=['real','fake'], top=20)","043f4da4":"eli5.show_prediction(t_clf, X['cleaned_text'].loc[1203], show_feature_values=True,\n                        vec=t_vectorizer, targets=[0,1], target_names=['real','fake'], top=20)","e5af45b8":"xgbc.get_booster().feature_names=list(X.drop(['text','cleaned_text'], axis=1).columns)\neli5.show_prediction(xgbc, X.drop(['text','cleaned_text'], axis=1).loc[1203], \n                     feature_names=list(X.drop(['text','cleaned_text'], axis=1).columns),\n                     show_feature_values=True,\n                     targets=[0,1], target_names=['real','fake'])","4ed02e8d":"fp=finaldf[(finaldf.ensemble_predict==1) & (finaldf.fraudulent==0)]","d6a97f0a":"fp.sample(15, random_state=56)","1e3924ac":"eli5.show_prediction(clf, X['cleaned_text'].loc[4521], vec=vectorizer,\n                     show_feature_values=True, targets=[0,1], target_names=['real','fake'], top=20)","64cc8c3c":"eli5.show_prediction(t_clf, X['cleaned_text'].loc[4521], \n                     show_feature_values=True, vec=t_vectorizer, targets=[0,1], target_names=['real','fake'], top=20)","34dcade2":"eli5.show_prediction(xgbc, X.drop(['text','cleaned_text'], axis=1).loc[4521], \n                     feature_names=list(X.drop(['text','cleaned_text'], axis=1).columns),\n                     show_feature_values=True,\n                     targets=[0,1], target_names=['real','fake'])","e4885a4d":"The **locations** in the dataset include states and cities. To simplify the analysis, I only consider the countries, where 84% of fake ads list the US as location, while 58% of real ads come from the US.","17779306":"After one hot encoding, \"industry: oil and energy\" and \"location: US\" emerge among features with highest MI scores.","c44768d2":"The following graphs of three binary features show that ads of jobs involving **telecommuting**, do not have **company logo** and **screening questions** are more likely to be fake. Among the three features, the absence of company logo is the most indicative, with 16% of ads without company logo to be fake, while only 2% of ads with company logo are fraudulent.","b1e7b40f":"# SVM BoW Model","c3ade7ed":"# Evaluation of Incorrect Classifications","77523cd2":"In terms of **employment type**, ads which specify \"full-time\", \"contract\" and \"temporary\" are less likely to be fake. For **required experience**, ads which specify \"associate\",\"mid-senior level\" and \"internship\" are less likely to be fake. And for **required education**, each category has more or less the same low fraudulent rates, with the exception of \"some high school coursework\", where the fraudulent rate is 75%. For **function** listed, \"administrative\" with a fraudulent rate about 20% stands out.","c7bc5cce":"The examination of **title** again reveals that some fake jobs ads emphasise **money earned in dollar sign**. 6.8% of fake ads have this characteristic, but only 0.4% of real ads do so.","e5fb8677":"# Sampling and Model Construction Strategy","81fa721d":"Next we examine the length of text fields in real and fake ads.","357feb78":"A casual review of the fake ads reveals some observations, such as some fake ads are using typical spam phrases, such as **\"Use Your Spare Time to Start Earning More**\" in title, or emphasize the **money earned with dollar signs in titles**, and talk about **earning money remotely**. These ads seem to **target people with high school educational attainment**. Some fake ads include **external links of URL, EMAIL and PHONE**. Is it due to scammers wanting to bypass the ad system to make direct contacts with their targets?","bf435d64":"# XGBoost Features Model","ee82b24a":"A bag-of-word model simply keeps counts of occurance of each word in the text. By feeding it into a machine learning algorithm, we hope the algorithm can identify keywords that can distinguish fake job ads from the real ones. Linear SVM is known to be working quickly on high dimensional data and is highly interpretable. I use [Susan Li's](https:\/\/www.kdnuggets.com\/2018\/09\/machine-learning-text-classification-using-spacy-python.html) function for listing the top textual features used for predictions, and it reveals that some of the words raised in earlier discussion like \"**earn**\", \"**immediate**\", \"**cash**\", \"**money**\", \"**apply link**\", \"**phone**\", \"**work home**\" emerge as keywords for predictions, while certain top words related to \"**northwestern hospital build website**\" apparently learned from a series of fake ads in the sample.","71316d4c":"Before we go into data exploration, perhaps we should decide on whether we should **drop duplicated** records in dataset first. This is a contentious issue among data scientists. One view is that duplicated records could result in bias in trained models, the other view is they reflect the distribution of the population of observations, and should be left as they are. In the EMSCAD dataset there are a small portion of identical records, supposed to be job ads reposted. But among genuine and fraudulent job ads, the rates of duplication are more or the same, so apparently they can be safely dropped.","b5d6d19e":"The **salary range** of the dataset has quite a large number of unique values. But since more than 80% of ads do not list it, apparently the only thing that matters is whether the ad omits this field, as noted earlier.","4b19140f":"In dealing with imbalanced classification, two of the most common strategies are undersampling and oversampling. One drawback of undersampling is the undersampled majority class sample may lose some information and be biased, and one drawback of upsampling is it may be prone to overfitting, especially when the upsampling rate is high. \n \nOn the other hand, this dataset has rich textual contents for constructing bag-of-words(BoW) models, and also has a large number of features for training a separate model. So to keep the upsampling rate low while using all of the majority class cases, I devise a strategy of dividing the majority class cases into three portions, each combine with the randomly upsampled full set of minority class cases for training three models, and make a ensemble of the three models by simple majority votes.\n \nIn the dataset, the real ads to fake ads ratio is about 19:1. When we divide the real cases into three portions while keeping the whole fake cases, the ratio becomes about 6.3:1. With an upsampling rate of about 4 times, the ratio can further be reduced to 1:0.6.\n \nAnd the three models are:\n1. **A BoW model using simple Countvectorizer and linear support vector machine(SVM)**\n1. **A BoW model using TF-IDF vectorizer and random forest model**\n1. **A XGBoost classifier model on the non-textual features**\n \nThe choice of algorithms for modeling also take imbalanced classification as consideration. Both SVM and random forest have inbuilt feature of balancing class weight, and XGBoost is known to be effective on an imbalanced dataset.\n","4b7174cf":"# Conclusion","a019f63b":"In most cases, the problem of missing values is an inconvenient trouble, as it reflects system malfunction or human error. But in this job ads dataset, we can assume that it is resulted from willful omission by the clients. Sometimes they choose to, say, put information of benefits in the field of description or even in title, sometimes they just leave some fields blank. So missing value itself may become a clue for identifying fake ads. To examine this possibility, we first look into the na rates of each features of real and fake ads:","c520894f":"# Preliminary Textual Analysis","94d70f98":"But there are also fake ads with long descriptions that appear like genuine ones, like the following one pretending from Netgear.","8543a506":"# The Dataset","ea772310":"To make good decisions about data analysis and modeling, we have to understand the data first. The dataset, named The Employment Scam Aegean Dataset (EMSCAD) by researchers of University of the Aegean, consists of 17,800 job ads posted between 2012 to 2014 through Workable, a recruiting software, whose 866 fraudulent job ads were manually annotated by employees of Workable. The criteria of inclusion is said to be including \"client\u2019s suspicious activity on the system, false contact or company information, candidate complaints and periodic meticulous analysis of the clientele\". So on one hand there may be a small number of mislabeled job ads, on the other hand the annotation may include factors not contained in the dataset.\n\nAnd the dataset includes structured and unstructured data, open text fields include \"title\", \"company profile\", \"description\", \"requirements\", \"benefits\", and to some extent 'location\", \"department\" and \"salary range\". Structured fields include \"employment type\", \"required experience\", \"required education\", \"industry\" and \"function\", and there are binary fields indicating whether the ad has \"company logo\" and screening \"questions\", and whether the job involves \"telecommuting\". The following table lists the details of the feature fields of the dataset:\n\n\n\n\n**String Data**                                              \n1. **Title**: The job advertisement header\n1. **Location**: The location of the job adviser\n1. **Department**: Job relevant department like sales\n1. **Salary range**: Suggested Salary Range such as $50,000 - 60,000\n\n**HTML Fragment**\n1. **Company Profile**: A brief description of the company\n1. **Description**: Advertised Job details\n1. **Requirement**: Required list for job\n1. **Benefits**: Benefits list offered by employer\n\n**Binary**\n1. **Telecommuting**: True for Telecommuting positions\n1. **Company Logo**: True if company logo exists\n1. **Questions**: True if screening question exists\n1. **Fraudulent**: Classification attribute\n\n**Nominal**\n1. **Employment Type**: Full-type, Part-time, Contract, etc.\n1. **Required Experience**: Executive, Entry level, Intern, etc.\n1. **Required Education**: Doctorate, Master\u2019s Degree, Bachelor\u2019s, etc.\n1. **Industry**: Automotive, IT, Health care, Real estate, etc.\n1. **Function**: Consulting, Engineering, Research, Sales etc.\n","0dfbf20d":"From the table we can see in all but one feature which has missing values, the fake ads are more likely to have missing values, with the exception of \"salary range\", in which fake ads are more likely to provide information. But are these differences of missing value rates statistically significant? I perform two proportion z-test on each of them, with the two-tailed p-value significance level setting at 0.005.","300f9482":"And it is evidently clear that fake ads generally have shorter textual contents.","3ad0f093":"Then we do one-hot-encoding with the categorical features, make the final features set as X, and separate the column \"fraudulent\" as y.","e125b1db":"Then let's have a look at the fake ads.","c16d6611":"Among the 36 false negative cases, this one which makes all 3 component models get it wrong. The content looks genuine and without all the telltale words, so it escapes the detection of both BoW models. And on the features side, though it has no company profile and company logo, it seems that it does not share many common features with other fake ads to make the XGBoost features model raise the flag.","2239c8f9":"TD-IDF(term frequency\u2013inverse document frequency) changes the word counts of BoW model by suppressing the importance of words occurring across texts, thus concentrating on unique words that appear frequently in a particular text. The eli5 library can reveal the textual features carry the most weight i the random forest model, but don't distinguish whether they are used to predict real or fake ads, but we can see certain words like \"**earn**\", \"**work home**\", \"**no experience**\" appear again. And compared with the SVM BoW model, which has more false positives than false negatives, the random forest TF-IDF BoW Model has more false negatives than false positives.","ea0edcba":"This exercise of ensemble model illustrates that, even by just using simple majority vote, ensemble method can let different algorithms complement each other's limitations and improve performance, which should be welcomed in the tricky problem of imbalanced classification. Apparently by changing the threshold of classification, say raise it from two votes to three votes, or lower it from two votes to one vote the other way round, we could change the numbers of false positives and false negatives, depending on which way suits the need.\n \nThough the modeling method can be improved, the examination of false negative and false positive cases shows that, in identifying fraudulent job ads, or spam emails or fake news, we need to use two kinds of information. The first is more \"**generic**\", such as the emphasis of \"easy money\" and the lack of information provided, somehow anyone who sees them should raise suspicion. But the fraudsters can always make a trap by making the ads seem genuine, and there are always real ads sharing much common characteristics of the fake ones. To make proper distinctions, we need the more '**empirical**' information, such as the location and industries the fraudsters habitually targeted, or the genuine companies they used to impose. This kind of information is not apparent as it seems, and needs to be inferred from known cases. That means an automated algorithm in itself is never sufficient. We need other ways to know and keep on collecting fraudulent ads, and continue to analyse and feed them to machine learning models.","19619827":"Still we should examine cases of incorrect classifications to see what can be improved.","f530efdc":"# Classification of Real and Fake Jobpostings Using Ensemble Model","caba25c7":"# The Information of Missing Values","9bf0a190":"# Length of Text Fields","af1c369f":"As for the 131 categories of **industry** listed, the most common in fake ads is \"oil and energy\". Among ads in that industry, the fraud rate 37.8% is also among the highest, though some isolated cases make the fraud rates of some industries like ranching, military and animation even higher.","79f74f20":"The Z-tests indicate that differences of na-rates among real and fake job-postings are mostly statistically significant, except in 'location', 'department', 'benefits' and 'function'. The difference is largest in **company profile**, where 68% of fake ads do not have a company profile, but only 16.1% of real ads omit company profiles.","d2de7fc1":"The following analysis shows that among the correctly predicted fake ads which get 2 votes and just pass the majority threshold, the 2 votes are coming from all three combinations, indicating the three component models complement each other.","36caa0a7":"Furthermore, some fake ads are **very brief**, barely provide any details, with one extreme case that just has title and location, and leaves all other fields blank.","5d074281":"When we apply the ensemble model for the whole dataset, the numbers look even more impressive. But as each component model has seen 1\/4 of class 0 data and 3\/4 of class 1 data, the numbers are somehow inflated.","ce4a997b":"# Features Construction","b52d4c27":"Then I join the open textual fields for analysis. The first thing to consider is checking whether external links are characteristics of fake job ads. Comparisons reveal while real and fake ads have **#URL** links in more or less same rates, 20% of fake ads have **#EMAIL** links but less than 7% of real ads do so, and 9.2% of fake ads have **#PHONE** links while only 2.6% of real ads have them.","98208ddf":"![\u7121\u6a19\u984c\u6587\u4ef6 - \u5713\u5f62\u5716 1.png](attachment:a690b861-1889-4c0d-b039-e8bde91c9221.png)","5ce037c2":"For **text preprocessing**, I use the function by [Jiahao Weng](https:\/\/gist.github.com\/jiahao87\/d57a2535c2ed7315390920ea9296d79f). It includes remove HTML tags, remove extra whitespaces, convert accented characters to ASCII characters, expand contractions, remove special characters, lowercase all texts, convert number words to numeric form, remove numbers, remove stopwords and lemmatization. Then I look into the most common words in real and fake job ads.","497765b2":"To recap the above observations, the following characteristics have potentials to identify fake job ads and to be constructed as features for machine learning:\n* title: mention '$'\n* location: from US or AU\n* salary range: less likely to omit\n* company profile: more likely to omit\n* textual descriptions tend to be shorter\n* textual descriptions more likely to include links for email and phone contacts\n\nFor the missing values, in categorical fields \"na\" would be used as a category, but in the combined text it is replaced by empty space to avoid overlapping with the newly created features denoting absence of values. Furthermore, the less frequently appeared countries in the location column are simplified as \"other\".","e3547ad2":"The non-textual features model, even using the highly powerful XGBoost algorithm, still has lower accuracy, particularly it has a large number of false positives, perhaps reflecting that without textual information it is not enough to distinguish truly fake ads from the genuine ones. And the feature importance figures show that it is dominated by variables of industry, required education and location, while missing salary range becomes the top one.","5ffb9049":"Classification of dataset with imbalanced classes and the treatment of missing values are two of the tricky issues of machine learning. In this notebook on a dataset of real and fake job postings, I am trying to tackle these two problems with an ensemble model, by training three different machine learning models with different segments of samples, and taking a simple majority vote of three models as the final predictions. Apparently the three models complement each other and achieve decent results.","f1dacf90":"# Random Forest TF-IDF BoW Model","cb892d2b":"In this preliminary analysis it is not apparent that any keywords can separate real ads and fake ads, as most common words such as \"work\", \"experience\", \"service\" and \"skill\" are featured in both classes of text. A bag-of-words analysis should be better in doing this job. But it is noticeable that the dummy word \"**na**\" for missing values is featured high in the most common words list in fake ads.","2a130c86":"Though the duplicated records are dropped, the dataset still has a lot of very similar records, such as among the real ads, there are 309 pieces titled \"English Teacher Abroad\", and there are 20 fake ads titled \"Cruise Staff Wanted URGENT\". But these ads are not identical in all fields, so they are reserved.","fd81c9f9":"# Making the Ensemble Model","85f785ba":"For data exploration, I provisionally fill the null values with 'na', then separate the real and fake job ads to look into differences between them.","485c50b4":"At the first look of the dataset with duplicates dropped, two things stand out. The first is **highly imbalance of data**, with the fraudulent class of job ads totaling less than 5% of all the records. So we may set the baseline of accuracy of the trained model at 95%, as this score can be achieved by simply labelling all ads as genuine. \n\nThe second thing is the **wide spread of missing values**, with some features the missing value rates exceeding 80%.","2ef3a822":"The accuracy (98.6%) and f1 scores (0.85) of the ensemble model is higher than the SVM BoW model (97.7% and 0.78), the random forest TF-IDF BoW model (98.3% and 0.81) and the XGBoost features model (96.0% and 0.66).","c9441081":"Mutual information scores show that among the non-textual features, \"text length\", \"industry\", \"missing company profile\", \"missing salary range\" and \"has company logo\" are the top 5 features that have a relationship with fake job ads.","1302b5d4":"And in this false positive case which gets all three component models to wrongly raise the flags, it seems that the term \"datum entry\" leads both BoW models to classify it as faked. And on the features side, it seems that the omission in several fields and relative shortness of the text contents leads to the classification of fake by XGBoost. One point worth noticing is in this prediction, the XGBoost model takes missing salary range and nontelecommuting as top contributing factors of classification as fraudulent, which are contrary to the observations from data exploration, which raise the possibility that the model operates in a more complex way beyond easy comprehension.","8db9f580":"# Examination of Other Features","db2b0531":"The ensemble model is made in a simple way. We have the predictions of the three models, where 1 denotes a prediction of a fake ad, adding the scores of each record, and when the total score is 2 or 3, it has the majority vote of classified as fraudulent, otherwise it is classified as real.","ec5552b7":"# Data Exploration: First look"}}