{"cell_type":{"5920e29d":"code","9c8e17b9":"code","02e816eb":"code","5492680a":"code","c70bbb9e":"code","92d31f36":"code","467db7b8":"code","adf79931":"code","8b3e5741":"code","69733c2d":"code","46768db1":"code","56be3eb2":"code","9bf5ae50":"code","cdc52020":"code","9f61373b":"code","37cf1eee":"code","d935b249":"code","7dc6662b":"code","643537bb":"code","2dbeec46":"code","c8bf431b":"code","216202bf":"code","6a5a0953":"code","c4aac33d":"code","469ade81":"code","2e2bb9d7":"code","542597e4":"code","51225645":"code","b133306a":"code","d37cc589":"code","58274586":"code","4e8a6641":"code","e3361668":"code","e0a39b9c":"code","aa2d2cd4":"code","6a37b9c4":"code","0fe368d7":"code","3ed7012c":"code","9fdce4e4":"code","a7d12db5":"code","bdca19c1":"code","493ae784":"code","a2e0256c":"code","920d4596":"code","00eb856f":"code","326e93ff":"code","8abdafcf":"markdown","22632f93":"markdown","796260d2":"markdown","05471936":"markdown","3ddf21f1":"markdown","81c6ad6b":"markdown","f615eb5a":"markdown","3c0adbdb":"markdown","c417d39a":"markdown","3f0a7adf":"markdown","a978d7e9":"markdown","5297c70f":"markdown","c3a08fc9":"markdown","f54eb6d4":"markdown","922b3e44":"markdown","0ecca3e7":"markdown","1d0a1110":"markdown","21aaa616":"markdown","df8d786a":"markdown","6aa4264b":"markdown","7f57b537":"markdown","3912413e":"markdown","e961037c":"markdown","9722792c":"markdown","75037ecb":"markdown","cddbfa2a":"markdown","5db816ea":"markdown","43fb9244":"markdown"},"source":{"5920e29d":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression\nimport datetime\nimport matplotlib.pyplot as plt \nimport warnings\nimport tensorflow as tf \nfrom scipy.stats import exponweib\n%matplotlib inline","9c8e17b9":"data = pd.read_csv('https:\/\/covidtracking.com\/api\/v1\/states\/daily.csv')\ndata.head()","02e816eb":"data.info()","5492680a":"data.isnull().sum()","c70bbb9e":"data.sort_values('date', inplace=True)","92d31f36":"# drop columns with 50% null values \ndef drop_na_50(df):\n    columns = df.columns\n    for col in columns:\n       if data[col].isnull().sum() > 0.5 * len(df):\n         df.drop(col, 1, inplace=True)\n    return df \n         \nnew_data = drop_na_50(data)","467db7b8":"new_data.isnull().sum()","adf79931":"new_data.fillna(0, inplace=True)","8b3e5741":"new_data[new_data.state=='CT'].hist(bins=10, figsize=(20, 15))\nplt.show()","69733c2d":"# drop variables with few\/no variations\nnew_data.drop(['commercialScore', 'fips', 'hospitalizedIncrease', 'negativeRegularScore', 'negativeScore', 'positiveScore', 'score'], axis=1, inplace=True)","46768db1":"new_data[new_data.state=='CT'].hist(bins=10, figsize=(20, 15))\nplt.show()","56be3eb2":"new_data.isnull().sum()","9bf5ae50":"# dictonary for mapping state to abbreviation \nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n","cdc52020":"us_state_abbrev['Connecticut']","9f61373b":"# this data frame is going to provide static variables\nadditional_data = pd.read_csv('..\/input\/covid19-state-data\/COVID19_state.csv')\nadditional_data.head()","37cf1eee":"additional_data.isnull().sum()","d935b249":"# additional_data.hist(bins=10, figsize=(20, 15))\n# plt.show()","7dc6662b":"# make the data consistent with the other data frame \nfor i in additional_data.State.unique():\n    additional_data.replace(i, us_state_abbrev[i], inplace=True)","643537bb":"additional_data[additional_data.State=='CA']['Pop Density']","2dbeec46":"# incoporate population density into prediction \npop_dense = [] \nfor i in new_data.state:\n    pop_dense.append(additional_data[additional_data.State==i]['Pop Density'].sum())","c8bf431b":"new_data['pop_dense'] = pop_dense","216202bf":"new_data['pop_dense'].head()","6a5a0953":"# scales the population density based on all the states \nmin_max_scaler = MinMaxScaler()\nnew_data['pop_dense'] = min_max_scaler.fit_transform(np.array(new_data['pop_dense']).reshape(-1, 1))","c4aac33d":"new_data['pop_dense'].head()","469ade81":"#this dataset is going to provide mobility information within the US\n\nmobility_data = pd.read_csv('https:\/\/covid19-static.cdn-apple.com\/covid19-mobility-data\/2014HotfixDev17\/v3\/en-us\/applemobilitytrends-2020-08-16.csv')\nmobility_data.head()","2e2bb9d7":"mobility_data.isnull().sum()","542597e4":"# make the data consistent with the other data frame \nfor i in mobility_data['sub-region'].unique():\n    if i in us_state_abbrev.keys():\n        mobility_data.replace(i, us_state_abbrev[i], inplace=True)","51225645":"mobility_data['transportation_type'].unique()","b133306a":"# get individual states and dates\nunique_states = new_data.state.unique()\nunique_states.sort()\nunique_dates = new_data.date.unique()\n\n# making sure that the dates match between mobility and testing\/cases\nmobility_latest_date = datetime.datetime.strptime(mobility_data.columns[-1], '%Y-%m-%d').strftime('%Y%m%d')\nmobility_latest_index = np.where(unique_dates == int(mobility_latest_date))[0][0]\n\n# start from a later date 3\/1\/2020\nunique_dates = unique_dates[39:mobility_latest_index+1]","d37cc589":"# gets the mobility information of a particular day\ndef get_mobility_by_state(transport_type, state, day):\n    return mobility_data[mobility_data['sub-region']==state][mobility_data['transportation_type']==transport_type].sum()[day]","58274586":"get_mobility_by_state('walking', 'FL', '2020-03-01')","4e8a6641":"# change the date format to match the mobility data \nrevised_unique_dates = [] \nfor i in range(len(unique_dates)):\n    revised_unique_dates.append(datetime.datetime.strptime(str(unique_dates[i]), '%Y%m%d').strftime('%Y-%m-%d'))\nrevised_unique_dates","e3361668":"print(get_mobility_by_state('transit', 'FL', revised_unique_dates[9]))","e0a39b9c":"def convert_date_to_int(d):\n    return [i for i in range(len(revised_unique_dates))]\n\ndays_since_3_1 = convert_date_to_int(revised_unique_dates)","aa2d2cd4":"days_ahead = 10\nfuture_dates = [i for i in range(len(revised_unique_dates)+days_ahead)]","6a37b9c4":"def svm_reg(X_train, X_test, y_train, y_test, future_forecast, state):\n        \n    svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01, epsilon=1, degree=4, C=0.1)\n    svm_confirmed.fit(X_train, y_train)\n    test_svm_pred = svm_confirmed.predict(X_test)\n    svm_pred = svm_confirmed.predict(future_forecast)\n    \n    plt.plot(y_test)\n    plt.plot(test_svm_pred)\n    plt.title('Testing Set Evaluation for {}'.format(state))\n    plt.xlabel('Days since 3\/1\/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.legend(['Actual', 'Predicted'])\n    plt.show()\n    \n    print('MAE:', mean_absolute_error(test_svm_pred, y_test))\n    print('MSE:',mean_squared_error(test_svm_pred, y_test))\n\n    # plot the graph to see compare predictions and actual coronavirus cases\n    plt.plot(positive)\n    plt.plot(svm_pred)\n    plt.title('Coronavirus Cases in {}'.format(state))\n    plt.legend(['Actual cases', 'Predicted cases using support vector regression'])\n    plt.xlabel('Days since 3\/1\/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.show()\n    print('Completed:', state)\n","0fe368d7":"def bayesian_ridge(X_train, X_test, y_train, y_test, future_forecast, state):\n        \n    # convert data to be compatible with polynomial regression\n    bayesian_poly = PolynomialFeatures(degree=3)\n    bayesian_poly_X_train = bayesian_poly.fit_transform(X_train)\n    bayesian_poly_X_test = bayesian_poly.fit_transform(X_test)\n    bayesian_poly_future_forecast = bayesian_poly.fit_transform(future_forecast)\n    \n    # polynomial regression model\n    # bayesian ridge polynomial regression\n    tol = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n    alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n    normalize = [True, False]\n    fit_intercept = [True,  False]\n    lambda_init = [1e-2, 1e-1, 1, 1e1]\n\n    bayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2, \n                    'normalize' : normalize, 'fit_intercept': fit_intercept, 'lambda_init' : lambda_init}\n\n    bayesian = BayesianRidge()\n    bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_root_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=200, verbose=1)\n    bayesian_search.fit(bayesian_poly_X_train, y_train)\n    \n    # get the best estimator \n    best_params = bayesian_search.best_params_\n    bayesian_confirmed = BayesianRidge(**best_params)\n    bayesian_confirmed.fit(bayesian_poly_X_train, y_train)\n    \n    test_bayesian_pred = bayesian_confirmed.predict(bayesian_poly_X_test)\n    bayesian_pred = bayesian_confirmed.predict(bayesian_poly_future_forecast)\n    \n    plt.plot(y_test)\n    plt.plot(test_bayesian_pred)\n    plt.title('Testing Set Evaluation for {}'.format(state))\n    plt.xlabel('Days since 3\/1\/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.legend(['Actual', 'Predicted'])\n    plt.show()\n    \n    print('MAE:', mean_absolute_error(test_bayesian_pred, y_test))\n    print('MSE:',mean_squared_error(test_bayesian_pred, y_test))\n    print('Weight:', bayesian_confirmed.coef_)\n\n    # plot the graph to see compare predictions and actual coronavirus cases\n    plt.plot(positive)\n    plt.plot(bayesian_pred)\n    plt.title('Coronavirus Cases in {}'.format(state))\n    plt.legend(['Actual cases', 'Predicted cases using bayesian ridge'])\n    plt.xlabel('Days since 3\/1\/2020')\n    plt.ylabel('# of positive coronavirus cases')\n    plt.show()\n    print('Completed:', state)\n","3ed7012c":"# helper function for finding daily change \ndef daily_change(y2, y1):\n    return (y2-y1)","9fdce4e4":"# get moving average for positive case \n\ndef moving_positive_cases(data, window_size):\n    moving_positive = []\n    for i in range(len(data)):\n        if i + window_size < len(data):\n            moving_positive.append(np.mean(data[i:i+window_size]))\n        else:\n            moving_positive.append(np.mean(data[i:len(data)]))\n    return moving_positive","a7d12db5":"def future_testing_extrapolation(X, y, future_forecast, state):\n    poly = PolynomialFeatures(degree=3)\n    poly_X = poly.fit_transform(X)\n    poly_future_forecast = poly.fit_transform(future_forecast)\n    \n    poly_confirmed = LinearRegression(fit_intercept=True, normalize=True)\n    poly_confirmed.fit(poly_X, y)\n    \n    poly_pred = poly_confirmed.predict(poly_future_forecast)\n    \n    plt.plot(y)\n    plt.plot(poly_pred)\n    plt.title('Coronavirus testing in {}'.format(state))\n    plt.legend(['Actual testing', 'Predicted testing using polynomial regression'])\n    plt.xlabel('Days since 3\/1\/2020')\n    plt.ylabel('# of testing')\n    plt.show()\n    \n    future_increases = [] \n    \n    # calulate future rates of change \n    for i in range(days_ahead):\n        c = len(X) - 1\n        future_increases.append(daily_change(poly_pred[c+i+1], poly_pred[c+i]))\n    return future_increases","bdca19c1":"days_since_3_1 = np.array(days_since_3_1).reshape(-1, 1)\nfuture_dates = np.array(future_dates).reshape(-1, 1)","493ae784":"def window_average(window_size, data, method):\n    avg_data = [] \n    date_length = len(data)\n    \n    for i in range(len(data)):\n        remainder = i % window_size \n        if method == 'median':\n            if i - remainder + window_size - 1 < date_length:\n                avg_data.append(np.median(data[i-remainder:i-remainder+window_size-1]))\n            else:\n                delta = date_length % window_size \n                avg_data.append(np.median(data[date_length-delta-1:date_length-1]))\n        elif method == 'mean':\n             if i - remainder + window_size - 1 < date_length:\n                avg_data.append(np.mean(data[i-remainder:i-remainder+window_size-1]))\n             else:\n                delta = date_length % window_size \n                avg_data.append(np.mean(data[date_length-delta-1:date_length-1]))\n        else:\n            warnings.warn('Methods can only be mean or median')\n            \n    return avg_data","a2e0256c":"# returns true if it is a weekend, and false if it is a weekday \ndef weekday_or_weekend(date):\n    date_obj = datetime.datetime.strptime(str(date), '%Y%m%d')\n    day_of_the_week =  date_obj.weekday()\n    if (day_of_the_week+1) % 6 == 0 or (day_of_the_week+1) % 7 == 0:\n        return True \n    else:\n        return False ","920d4596":"len(future_dates[-10:])","00eb856f":"# implementing this in the prediction in the future \ndef mobility_scenario(mobility, mode):\n    local_min = np.min(mobility)\n    local_max = np.max(mobility)\n    \n    local_min_index = np.where(mobility==local_min)[0]\n    local_max_index = np.where(mobility==local_max)[0]\n    \n    slope = (local_max - local_min) \/ (local_max_index - local_min_index)\n    \n#     plt.axhline(y=local_min, color='red')\n#     plt.axhline(y=local_max, color='purple')\n    \n    # if mobility increases \n    \n    if mode == 'decrease':\n        # extrapolating mobility \n        m = mobility[-1] + slope \n        if m < local_min:\n            future_mobility = local_min\n        else:\n            future_mobility = m\n    \n    # if mobility decreases \n    if mode == 'increase':\n         # extrapolating mobility \n        m = mobility[-1] + np.abs(slope) \n        if m > local_max:\n            future_mobility = local_max\n        else:\n            future_mobility = m\n            \n    return future_mobility","326e93ff":"states = ['FL', 'CA', 'GA', 'TX']\n\nfor state in states:\n    positive = []\n    pop_density = [] \n    testing = [] \n    \n    # mobility data\n    walking_weekday = [] \n    walking_weekend = [] \n    walking = []\n    walking_weekday_window = 7\n    walking_weekend_window = 7\n    \n    # adjust window size for mobility\n    \n    date_length = len(revised_unique_dates)\n    \n    # get cases in sequential order for each state\n    for i in range(date_length):\n        positive.append(new_data[new_data.date==unique_dates[i]][new_data.state==state].positive.sum())\n        pop_density.append(new_data[new_data.state==state]['pop_dense'].max())\n        testing.append(new_data[new_data.date==unique_dates[i]][new_data.state==state].totalTestResults.sum())\n        \n        # determines if it is a weekend or weekday \n        if weekday_or_weekend(unique_dates[i]): \n            walking_weekend.append(get_mobility_by_state('walking', state, revised_unique_dates[i]))\n        else:\n            walking_weekday.append(get_mobility_by_state('walking', state, revised_unique_dates[i]))\n        \n#         remainder = i % window_size \n#         if i - remainder + window_size < date_length:\n#             walking.append(get_mobility_by_state('walking', state, revised_unique_dates[i-remainder], revised_unique_dates[i-remainder+window_size-1], 'median'))\n#         else:\n#             # if extrapolating use the mobility average from the last few days based on the window size\n#             delta = date_length % window_size \n#             walking.append(get_mobility_by_state('walking', state, revised_unique_dates[date_length-delta-1], revised_unique_dates[date_length-1], 'median'))\n\n\n    # remove any decreases in cum testing and positive cases\n    for i in range(len(testing)):\n        if i != 0:\n            if testing[i] < testing[i-1]:\n                testing[i] = testing[i-1]\n            if positive[i] < positive[i-1]:\n                positive[i] = positive[i-1]\n    \n    # remove 0 in mobility from both weekday and weekend data (there are few null values from Apple's mobility data)\n    for i in range(len(walking_weekend)):       \n        if walking_weekend[i] == 0 and i != 0:\n            walking_weekend[i] = walking_weekend[i-1]\n            \n    for i in range(len(walking_weekday)):\n        if walking_weekday[i] == 0 and i != 0:\n            walking_weekday[i] = walking_weekday[i-1]\n            \n    \n    # taking window average for mobility \n    walking_weekday_avg = window_average(7, walking_weekday, 'mean')\n    walking_weekend_avg = window_average(7, walking_weekend, 'mean')\n\n    \n    # making sure the shape of the mobility arrays match \n    r_walking_weekday_avg = [] \n    r_walking_weekend_avg = [] \n    \n    k = 0 \n    j = 0 \n    for i in range(date_length):\n        if i % walking_weekday_window == 0 and i != 0:\n            if k + walking_weekday_window < len(walking_weekday_avg):\n                k += walking_weekday_window\n            else:\n                k = len(walking_weekday_avg) - 1 \n                \n            if j + walking_weekend_window < len(walking_weekend_avg):\n                j += walking_weekend_window\n            else:\n                j = len(walking_weekend_avg) - 1\n        \n        r_walking_weekday_avg.append(walking_weekday_avg[k])\n        r_walking_weekend_avg.append(walking_weekend_avg[j])\n        \n\n    # take moving average for positive cases\n    positive = moving_positive_cases(positive, 3)\n\n    # future testing extrapolations from poylnomial prediction \n    future_testing = future_testing_extrapolation(days_since_3_1, testing, future_dates, state)\n    for i in future_testing:\n        testing.append(testing[-1] + i)\n    \n    testing = np.array(testing).reshape(-1, 1)\n    positive = np.array(positive).reshape(-1, 1)\n    r_walking_weekday_avg = np.array(r_walking_weekday_avg).reshape(-1, 1)\n    r_walking_weekend_avg = np.array(r_walking_weekend_avg).reshape(-1, 1)\n    \n    min_max_scaler = MinMaxScaler()\n    testing = min_max_scaler.fit_transform(testing)\n    r_walking_weekday_avg = min_max_scaler.fit_transform(r_walking_weekday_avg)\n    r_walking_weekend_avg = min_max_scaler.fit_transform(r_walking_weekend_avg)\n    \n    # combining the two features\n    X = [] \n    future_forecast = []\n    \n    for i in range(len(days_since_3_1)):\n        X.append([days_since_3_1[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[i][0], r_walking_weekend_avg[i][0]])\n    \n    X = np.array(X, object).reshape(-1, 5)\n    \n    for i in range(len(future_dates)):\n        if i < date_length:\n            future_forecast.append([future_dates[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[i][0], r_walking_weekend_avg[i][0]])\n        else:\n            future_forecast.append([future_dates[i][0], pop_density[0], testing[i][0], r_walking_weekday_avg[-1][0], r_walking_weekend_avg[-1][0]])\n            \n    future_forecast = np.array(future_forecast, object).reshape(-1, 5)\n    \n    # splitting into training and testing sets \n    X_train, X_test, y_train, y_test = train_test_split(X, positive, shuffle=False, test_size=0.05)\n    bayesian_ridge(X_train, X_test, y_train, y_test, future_forecast, state)","8abdafcf":"# Let's look at Connecticut as an example. ","22632f93":"# We want to make sure the dates from the mobility dataset matches with others. ","796260d2":"# We are taking winodws averages for our mobility data. It will give us a better view of the situation.","05471936":"# Determine if a day is a weekend or weekday. We want to separate weekend mobility from weekday mobility due to statistical differences.","3ddf21f1":"# Scale the popluation density between 0 and 1. It is always good to normalize your data.","81c6ad6b":"# Load state information, we want the population density from each state from this. ","f615eb5a":"# We are taking moving averages for coronavirus cases (our label). It gives us a better picture than a single day value.","3c0adbdb":"# Drop the features we don't need ","c417d39a":"Take a look at Apple's mobility data. ","3f0a7adf":"# Bayesian ridge implementation of the prediction model.","a978d7e9":"# Handling null values ","5297c70f":"\n<html>\n\t<link rel=\"stylesheet\" href=\"https:\/\/stackpath.bootstrapcdn.com\/bootstrap\/4.5.1\/css\/bootstrap.min.css\" integrity=\"sha384-VCmXjywReHh4PwowAiWNagnWcLhlEJLA5buUprzK8rxFgeH0kww\/aWY76TfkUoSX\" crossorigin=\"anonymous\">\n    <body>\n\t\t<div class=\"container-fluid\">\n\t\t\t<h1> Multi-variable Coronavirus Projections for US States <img src='https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Ftoppng.com%2Fuploads%2Fthumbnail%2Fcoronavirus-covid-19-11582576817hpyucwma6o.png&f=1&nofb=1' width='50', height='50'> <\/h1>\n\t\t\t<p class=\"text-monospace\"> This notebook aims to predict future coronvirus cases in US States. It will use \n\t\t\tmultiple variables as input. They include population density, testing, as well as mobility data.<\/p>\n\t\t\t<p class=\"text-monospace\"> Sources:  <\/p>\n\t\t\t<ul class=\"list-group list-group-horizontal-lg\">\n\t\t\t  <li class=\"list-group-item\">Mobility data: <a href='https:\/\/www.apple.com\/covid19\/mobility'>Apple<\/a> <\/li>\n\t\t\t  <li class=\"list-group-item\">COVID-19 tracking API: <a href='https:\/\/covidtracking.com\/'> The Covid Tracking<\/a><\/li>\n\t\t\t  <li class=\"list-group-item\">COVID-19 State Data: <a href='https:\/\/www.kaggle.com\/nightranger77\/covid19-state-data'> Nightranger77's Dataset<\/a><\/li>\n                 <li class=\"list-group-item\">COVID-19 Image: <a href='https:\/\/toppng.com\/uploads\/thumbnail\/coronavirus-covid-19-11582576817hpyucwma6o.png'> Toppng.com\n                     <\/a><\/li>\n\t\t\t<\/ul>\n\t\t\t<br>\n\t\t\t<b><p class=\"text-monospace\">Feel free to provide me with feedback. I really want to hear about your ideas.<\/p><\/b>\n            <p class=\"text-monospace\">Longer outputs are being hidden.<\/p>\n\t\t\t<p class=\"text-monospace\">Check out my visualization notebook <a href='https:\/\/www.kaggle.com\/therealcyberlord\/coronavirus-covid-19-visualization-prediction'>here<\/a>.<\/p>\n\n    \n<html>","c3a08fc9":"# Let's take a look at California's population density. ","f54eb6d4":"# We are predicting 10 days. Extrapolation way into the future may not yield the most accurate results. ","922b3e44":"# We want to convert our dates into numbers. Our machine learning algorithm cannot take text.","0ecca3e7":"Checking out the data for null values ","1d0a1110":"Take a look at Florida's mobility at a random day.","21aaa616":"# Building the model. ","df8d786a":"# We also want the date format to match with others as well ","6aa4264b":"# We want to extrapolate testing information using a polynomial regression. This will provide us with values for future prediction. ","7f57b537":"We need a way to map states to their abbreviations. This will help us later in the next dataset.","3912413e":"# This is a SVM implementation I wrote. Feel free to use this instead of bayesian ridge. ","e961037c":"# Reshape our arays for sklearn. ","9722792c":"# Use the dictionary we created before to ensure that data are consistent ","75037ecb":"# Again, let's use Connecticut as an example. It is an amazing state. ","cddbfa2a":"Simple function to get mobility of a state at a particular day.","5db816ea":"# Sort values by date since we are approaching it in a chronological order","43fb9244":"# Read the coronavirus dataset from the covid tracking API"}}