{"cell_type":{"cbc7d9eb":"code","f5da424d":"code","68ab2b0f":"code","4d971733":"code","f8d47041":"code","080c8343":"code","69357042":"markdown","1b6e4b7e":"markdown","0878e939":"markdown","f2213009":"markdown","9510c34a":"markdown","70a9dbb3":"markdown","2287f74b":"markdown"},"source":{"cbc7d9eb":"import numpy as np\n\n#Tout les imports - certains ne sont pas utilis\u00e9s\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.neural_network import MLPClassifier\n\nimport os\nds = open(\"..\/input\/heart.csv\")\n\n# Initialisation des donn\u00e9es\n\nNB_EXEMPLES = 303\nNB_ATTR = 13\n\n# On fait deux tableaux de la taille de nos donn\u00e9es initialis\u00e9s \u00e0 0\nX = np.zeros((NB_EXEMPLES, NB_ATTR))\nY = np.zeros(NB_EXEMPLES)\n\n# on remplit nos tableaux avec nos donn\u00e9es\nfor i,l in enumerate(ds):\n    if i == 0: continue # on enl\u00e8ve la ligne descriptive\n    t_l = l.rstrip('\\r\\n').split(',')\n    for j,c in enumerate(t_l[0:13]):\n        X[i-1,j] = float(c)\n    Y[i-1] = float (t_l[-1])\n\n# On d\u00e9finit les ensembles d'entrainement et de test\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,shuffle=True)\n\n# On normalise les attributs pour leur donner le meme poids\nscale = MinMaxScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.transform(x_test)","f5da424d":"# Random Forest\n# On d\u00e9finit le classifieur : ici un random forest\nrf = RandomForestClassifier(random_state=0)\n# On donne les valeurs test\u00e9es pour chacun des param\u00e8tres\nparam_rf={'n_estimators':[10,100,200,300,400], 'max_depth':[1,2,3,4,5]}\n# On cr\u00e9\u00e9 un classifieur qui teste toutes les combinaisons d\u00e9finies ci-dessus\n# En plus de cela, le param\u00e8tre cv permet la cross-validation, et n_jobs parralellise les calculs.\nclf = GridSearchCV(estimator=rf, param_grid=param_rf, cv=10, n_jobs=4, verbose=0, scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train)\npredictions = clf.predict(x_test)\nprint (\"best n is:\", clf.best_estimator_.n_estimators)\nprint (\"best depth is:\", clf.best_estimator_.max_depth)\nprint(classification_report(y_test, predictions,digits=4))","68ab2b0f":"# Bayesian\n# On d\u00e9finit un classifieur Bayesien\nbay=GaussianNB()\n# On donne les valeurs test\u00e9es pour le lissage : ici 20 valeurs comprises entre 10e-20 et 10e-1\nparam_b = {'var_smoothing':np.logspace(-20, -1, num=20)}\nclf = GridSearchCV(estimator=bay, param_grid=param_b, cv=10, n_jobs=4, verbose=0, scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best smoothing is:\", clf.best_estimator_.var_smoothing)\nprint(classification_report(y_test, predictions,digits=4))\n","4d971733":"#SVM\n# On d\u00e9finit un classifieur de machine \u00e0 vecteurs de support\nk = 'rbf' # noyau gaussien\nsvm = SVC(kernel=k,verbose=False) \n\n# On donne les valeurs test\u00e9es pour chacun des param\u00e8tres\nparam_svm = {'gamma':np.logspace(-10, 10, num=10),'C':np.logspace(-10, 10, num=10)}\nclf = GridSearchCV(estimator=svm, param_grid=param_svm,cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best gamma is:\", clf.best_estimator_.gamma)\nprint (\"best C     is:\", clf.best_estimator_.C)\nprint(classification_report(y_test, predictions,digits=4))\n","f8d47041":"#knn\n# On d\u00e9finit un classifieur aux k plus proches voisins\nknn = KNeighborsClassifier()\n\n# On mets dans un tableau les valeurs test\u00e9es pour les plus proches voisins : entre 1 et 60\ntab = []\nfor i in range (1,60):\n    tab.append(i)\n\n# On donne les valeurs test\u00e9es pour chacun des param\u00e8tres\nparam_knn = {'n_neighbors':tab, 'algorithm' : ['ball_tree', 'kd_tree', 'brute']}\nclf = GridSearchCV(estimator=knn, param_grid=param_knn, cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train)\nprint (\"best k is:\", clf.best_estimator_.n_neighbors)\nprint (\"best algorithm is:\", clf.best_estimator_.algorithm)\npredictions = clf.predict(x_test)\nprint(classification_report(y_test, predictions,digits=4))","080c8343":"# neural network\n# On cr\u00e9\u00e9 un r\u00e9seau de neurones\nnn = MLPClassifier(hidden_layer_sizes=(15, 20)) \n\n# On d\u00e9finit plusieurs fonctions d'activation, et plusieurs optimisateurs\nparam_nn = {'activation':['identity', 'logistic', 'tanh', 'relu'], 'solver' : ['lbfgs', 'sgd', 'adam']}\nclf = GridSearchCV(estimator=nn, param_grid=param_nn, cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best activation function is:\", clf.best_estimator_.activation)\nprint (\"best solver is:\", clf.best_estimator_.solver)\nprint(classification_report(y_test, predictions,digits=4))","69357042":"**Machine \u00e0 vecteurs de support**\nIci, on projete nos exemples dans un espace \u00e0 14 dimensions ou plus, avec un noyau de transformation gaussien (le RBF). Le but est de trouver un hyperplan s\u00e9parateur optimal pour toutes nos donn\u00e9es. On a test\u00e9 diff\u00e9rentes combinaisons de param\u00e8tres pour le RBF. Les r\u00e9sultats ici sont similaires ou meilleurs au Random Forest. Par contre le temps de calcul est assez long pour trouver le s\u00e9parateur optimal.\n","1b6e4b7e":"**En conclusion**\nOn peut dire que les m\u00e9thodes utilis\u00e9es sont  plus fiables que l'al\u00e9atoire pur. Les m\u00e9thodes propos\u00e9es avaient de tr\u00e8s bons r\u00e9sultats sur notre jeu de donn\u00e9es. Mention sp\u00e9ciale pour le r\u00e9seau de neurones, qui aligne de meilleurs r\u00e9sultats compar\u00e9s aux autres classifieurs. On peut noter tout de m\u00eame que le Random Forest donne de bons r\u00e9sultats compar\u00e9 au temps de calcul.","0878e939":"**Classifieur Bay\u00e9sien Na\u00eff**\nOn a tent\u00e9 un apprentissage bay\u00e9sien na\u00eff : c-a-d avec l'hypoth\u00e8se que chacun de nos attributs soient ind\u00e9pendants les uns envers les autres. On fait aussi l'hypoth\u00e8se que chaque attribut num\u00e9rique suit une r\u00e9partition gaussienne.\nMalgr\u00e9 une hypoth\u00e8se na\u00efve discutable, le classifieur montre de bons r\u00e9sultats autour de 80% de f-mesure.","f2213009":"**Le r\u00e9seau de neurones**\nPour classifier les donn\u00e9es, nous avons aussi utilis\u00e9 un r\u00e9seau de neurones. Nous avons mis une vingtaine de couches cach\u00e9es, pour que les fonctions non lin\u00e9aires puissent \u00eatre exploit\u00e9es au maximum, ainsi qu'une quinzaine de neurones par couche, car de toute fa\u00e7on nous n'avons que 14 attributs, et plus de neurones par couche ne seront pas utiles. \nLes r\u00e9sultats sont g\u00e9n\u00e9ralement sup\u00e9rieurs au Random Forest (entre 80 et 85% de f-mesure). Les fonctions d'activations qui semblent les meilleures pour ce probl\u00e8me sont le tanh ainsi que le RELU (deux fonctions non lin\u00e9aires), et adam semble \u00eatre le solveur le plus optimal.\n","9510c34a":"# Quelques mots sur notre jeu de donn\u00e9es :\n\nNous avons choisi le set de donn\u00e9es  Heart Disease UCI. Il est compos\u00e9 de 303 exemples et de 14 attributs (pression art\u00e9rielle, localisation de la douleur...) num\u00e9riques (sauf le dernier, qui est binaire), et le but et de deviner \u00e0 partir des 13 premiers attributs si le patient a une maladie cardiaque ou non.","70a9dbb3":"**K plus proches voisins**\nOn teste ici un apprentissage fain\u00e9ant : nos exemples sont projet\u00e9s dans un espace \u00e0 13 dimensions, et les exemples tests sont projet\u00e9s \u00e0 leur tour dans cet espace, et ils sont \u00e9tiquet\u00e9s en fonction des k plus proches voisins au sens de la distance de minkowski. Ce classifieur est tr\u00e8s rapide, mais pr\u00e9sente des r\u00e9sultats g\u00e9n\u00e9ralement en de\u00e7\u00e0 des classifieurs pr\u00e9c\u00e9dents (entre 75 et 80% de f-mesure)","2287f74b":"**Pour commencer, nous avons fait un classifieur Random Forest**\nOn a test\u00e9 diff\u00e9rentes valeurs pour chacuns des parametres, \u00e0 savoir le nombre d'arbres dans la for\u00eat, et la profondeur de chaque arbre.\nLes r\u00e9sultats du Random Forest sont tr\u00e8s bons g\u00e9n\u00e9ralement, avec des f-mesures sup\u00e9rieures \u00e0 80%. Habituellement, le nombre d'arbres est assez \u00e9lev\u00e9, mais la profondeur est g\u00e9n\u00e9ralement de 2."}}