{"cell_type":{"3b9e1b04":"code","4cc1fd21":"code","c3351fcc":"code","749a8fee":"code","214abb57":"code","b455e782":"code","a9938544":"code","5ffcc23e":"code","e06def5b":"code","230e6520":"code","bf6b5e59":"code","a2701666":"code","6a380590":"code","6c542fe1":"code","f4983888":"code","81ec5697":"code","66f47fbd":"code","47d75427":"code","733381f1":"code","8d4b4857":"code","00dbbd31":"code","781679a2":"code","ea6af5ee":"code","01a5003c":"code","2bf7769f":"code","75fe11f2":"code","c2d67301":"code","30befa0e":"code","5c65991c":"code","948a59a8":"code","c6e10304":"code","be6621a9":"code","f0156845":"code","a99875eb":"code","8e68eaf1":"code","e4e93317":"code","8af440b5":"code","4492e506":"code","b6c4f804":"code","ed57253d":"markdown","178bea12":"markdown","709f8831":"markdown","6dd137d2":"markdown","10447c27":"markdown","04509720":"markdown","02c85fc6":"markdown","20c8ded1":"markdown","9da6426d":"markdown","2f9f5c60":"markdown","3a03456a":"markdown","ab229034":"markdown","ed56fa62":"markdown","2d9dfe13":"markdown","d747d48b":"markdown","643d3157":"markdown","6a89a387":"markdown","5b9b5a7b":"markdown","e62da259":"markdown","bdb1ed00":"markdown","142bfb5f":"markdown","48ddc25e":"markdown","2207e41e":"markdown","e7284a9d":"markdown","e1a24ae3":"markdown","a7a6b9bf":"markdown","17d0d246":"markdown"},"source":{"3b9e1b04":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn')\n%matplotlib inline","4cc1fd21":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nimport lightgbm as lgb\nfrom lightgbm import plot_importance\nimport xgboost as xgb","c3351fcc":"train_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\ntrain_df.head()","749a8fee":"train_df.info()","214abb57":"train_df.describe()","b455e782":"sns.color_palette(\"RdPu\", as_cmap=True)","a9938544":"flare_palette = sns.color_palette(\"flare\", 10)\nRdPu_palette = sns.color_palette(\"RdPu\", 10)\n\nsns.palplot(RdPu_palette)\nsns.palplot(flare_palette)","5ffcc23e":"train_df_null_count = pd.DataFrame(train_df.isnull().sum().sort_values(ascending=False), columns=[\"Train Null count\"])\ntest_df_null_count = pd.DataFrame(test_df.isnull().sum().sort_values(ascending=False), columns=[\"Test Null count\"])\n\nnull_df = pd.concat([train_df_null_count,test_df_null_count],axis=1)\nnull_df.head().style.background_gradient(cmap='RdPu')","e06def5b":"msno.matrix(df=train_df.iloc[:,:],figsize=(15,5),color=RdPu_palette[3])\nplt.show()","230e6520":"fig, ax = plt.subplots(1,2,figsize=(10,6))\nsns.countplot('target', data=train_df, ax=ax[0], palette=[RdPu_palette[4], RdPu_palette[3]])\nax[0].patch.set_alpha(0)\nax[0].set_title('Count plot - target',fontweight=\"bold\")\ntrain_df['target'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[1], shadow=True, colors=[RdPu_palette[2], RdPu_palette[0]])\nax[1].set_title('Pie plot - target',fontweight=\"bold\")\n\n\nfig.text(0.25,0.96,\"Plot the target data percent\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()\n#target == 0 : negative(fake)\n#tarege == 1 : postive(real)","bf6b5e59":"features = train_df.drop(['ID_code','target'],axis=1)","a2701666":"fig, axes = plt.subplots(2,1, figsize=(15,8), constrained_layout=True)\n\n#axes[0]\nsns.distplot(train_df[features.columns].mean(axis=1),color=flare_palette[0], kde=True,bins=120, ax=axes[0], label='train')\nsns.distplot(test_df[features.columns].mean(axis=1),color=flare_palette[4], kde=True,bins=120, ax=axes[0], label='test')\naxes[0].set_title(\"Distribution of mean values per row for the train set and the test set\", fontweight=\"bold\", fontfamily='serif', fontsize=15)\naxes[0].patch.set_alpha(0) \naxes[0].legend()\n\n#axes[1]\nsns.distplot(train_df[features.columns].skew(axis=1),color=RdPu_palette[4], kde=True,bins=120, ax=axes[1], label='train')\nsns.distplot(test_df[features.columns].skew(axis=1),color=RdPu_palette[9], kde=True,bins=120, ax=axes[1], label='test')\naxes[1].set_title(\"Distribution of mean values per row for the train set and the test set\", fontweight=\"bold\", fontfamily='serif', fontsize=15)\naxes[1].patch.set_alpha(0)\naxes[1].legend()\n\nfig.text(0.32,1.05,\"Distribution of triansets and testsets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()","6a380590":"corr = train_df.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, cmap='RdPu')\nplt.title(\"Customer Transaction train data Heatmap\", fontweight=\"bold\", fontsize=17)\nplt.show()","6c542fe1":"train_df.drop('ID_code', axis=1, inplace=True)\ntest_df.drop('ID_code', axis=1, inplace=True)","f4983888":"train_df.head()","81ec5697":"x = train_df.drop(['target'], axis=1)\ny = train_df['target']","66f47fbd":"scaler = StandardScaler()\nx_scaler = scaler.fit_transform(x)\nx_scaler_df = pd.DataFrame(x_scaler, columns=x.columns)\n\npca = PCA(n_components=2)\nx_scaler_pca = pca.fit_transform(x_scaler)\nx_scaler_pca_df = pd.DataFrame(x_scaler_pca)","47d75427":"x_scaler_pca_df.head().style.background_gradient(cmap='RdPu')","733381f1":"print(\"Results of variance in 2 columns used : {}\".format(pca.explained_variance_ratio_))","8d4b4857":"print(\"Results of variance : {}\".format(sum(pca.explained_variance_ratio_)))","00dbbd31":"x_scaler_pca_df['target'] = y","781679a2":"fig = plt.figure(figsize=(10,7))\nplt.scatter(x_scaler_pca_df.loc[:, 1], x_scaler_pca_df.loc[:, 0], c=y,  cmap=\"RdPu\")\nplt.axis('off')\nfig.patch.set_facecolor(RdPu_palette[3])\nplt.title(\"Results of dimensionality reduction with 2 columns\", fontweight=\"bold\")\nplt.colorbar()\nplt.show()","ea6af5ee":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","01a5003c":"print(\"Train Dataset shape {} \/ {}\".format(x_train.shape, y_train.shape))\nprint(\"Test Dataset shape {} \/ {}\".format(x_test.shape, y_test.shape))","2bf7769f":"# Set the function to output evaluation indicators\ndef get_clf_eval(y_test, y_pred):\n    confusion = confusion_matrix(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    F1 = f1_score(y_test, y_pred)\n    AUC = roc_auc_score(y_test, y_pred)\n    \n    print('Confusion_matrix:\\n', confusion)\n    print('Accuracy: {:.4f}'.format(accuracy))\n    print('Precision: {:.4f}'.format(precision))\n    print('Recall: {:.4f}'.format(recall))\n    print('F1: {:.4f}'.format(F1))\n    print('AUC: {:.4f}'.format(AUC))","75fe11f2":"log_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)\npred = log_reg.predict(x_test)","c2d67301":"get_clf_eval(y_test, pred)","30befa0e":"lgbm_x_test, lgbm_x_val, lgbm_y_test, lgbm_y_val = train_test_split(x_test, y_test, test_size=0.5)","5c65991c":"train_data = lgb.Dataset(x_train, label=y_train)\nval_data = lgb.Dataset(lgbm_x_val, label=lgbm_y_val)\nparams = {\n    'device' : 'gpu',\n    'n_estimators': 7000,\n    'num_leaves': 20,\n    'max_depth': -1,\n    'min_data_in_leaf': 80,\n    'learning_rate': 0.008,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'n_jobs': -1\n}","948a59a8":"lgbm_model = lgb.train(params,\n                  train_data,\n                  valid_sets=val_data, \n                  valid_names=['train','valid'],\n                  early_stopping_rounds=100)","c6e10304":"pred = lgbm_model.predict(lgbm_x_test)\npred = (pred >= 0.5).astype(int)","be6621a9":"get_clf_eval(lgbm_y_test, pred)","f0156845":"RdPu_palette_20 = sns.color_palette(\"RdPu\", 20)\nsns.palplot(RdPu_palette_20)","a99875eb":"fig, ax = plt.subplots(1,1, figsize=(10,8))\nplot_importance(lgbm_model, max_num_features=20,color=RdPu_palette_20, ax=ax)\nax.set_title(\"Distribution of Column Specific Importance \", fontweight=\"bold\", fontsize=15)\nax.patch.set_alpha(0) \nplt.show()","8e68eaf1":"lgbm_submission  = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nlogistic_submission = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\n\nlgbm_submission.head()","e4e93317":"lgbm_target = lgbm_model.predict(test_df)\nlgbm_submission['target'] = lgbm_target\nlgbm_submission.head()","8af440b5":"lgbm_submission.to_csv('lgbm_submission1.csv', index=False)","4492e506":"logistic_target = log_reg.predict(test_df)\nlogistic_submission['target'] = logistic_target\nlogistic_submission.head()","b6c4f804":"logistic_submission.to_csv('logistic_submission.csv', index=False)","ed57253d":"### 6-1) LogisticRegression Modeling","178bea12":"# 2. Check out my data\n* Check Shape \/ Info\n* Set color palette","709f8831":"### 5-2) Split Train data \/ Test data","6dd137d2":"#### => Graph does not need to be normalized to bell shape","10447c27":"# 6.Modeling\n* LogisticRegression Modeling\n* LGBMClassifier Modeling","04509720":"### 5-1) Check the need for PCA technology","02c85fc6":"# 7. Submission\n* LGBMClassifier predict\n* LogisiticRegressor predict","20c8ded1":"# 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the null values\n* Plot the \"target\" columns count\n* Plot the Distribution of triandata and testdata\n* Customer Transaction train data Heatmap","9da6426d":"# 4. Prepocessing Data\n* Drop useless columns","2f9f5c60":"### 6-2) LGBMClassifier Modeling","3a03456a":"![shopping_mall_illustration.jpg](attachment:5e0cfc44-30d0-4256-96c2-09e5d2b3939f.jpg)","ab229034":"#### => This confirms that there are no missing values.","ed56fa62":"#### \u2714\ufe0f This notebook will use this palettes.","2d9dfe13":"***\n\n## My workflow\n#### 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray\n\n#### 2. Check out my data\n* Check Shape \/ Info\n* Set color palette\n\n#### 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the null values\n* Plot the \"target\" columns count\n* Plot the Distribution of triandata and testdata\n* Customer Transaction train data Heatmap\n\n#### 4. Prepocessing Data\n* Drop useless columns\n\n#### 5. Feature Enginnering \n* Check the need for PCA technology\n* Split Train data \/ Test data\n\n#### 6.Modeling\n* LogisticRegression Modeling\n* LGBMClassifier Modeling\n\n#### 7. Submission\n* LGBMClassifier predict\n* LogisiticRegressor predict","d747d48b":"### 3-3) Plot the Distribution of triandata and testdata","643d3157":"##### reference \n* https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n* https:\/\/www.kaggle.com\/roydatascience\/eda-pca-simple-lgbm-on-kfold-technique\n\n###  If this notebook is useful for your kaggling, \"UPVOTE\" for it \ud83d\udc40\n#### THX to Reading My Notebook\ud83c\udf08","6a89a387":"# 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray","5b9b5a7b":"\n# Santander Customer Transaction Prediction\n\n## Overview\n\nThis Notebook will be completed in two main ways.<br\/>\nFirst, find and visualize useful data or meaningful relationships within the data.<br\/>\nSecond, select a model based on the visualization of the previous process. Transform or refine the data into the appropriate form for the model to be used.<br\/><br\/>\n\nThe number of columns in the competition data is 202. So we have to determine how to use and discard this heat, and predict the results through the model. In this notebook, I will deal with data in various ways based on my experience.\n##### \"Dealing with a lot of data is the core of this competition.\"<br\/>\n\n\n#### My opinion :\n* 1) At first, I think dimensionality reduction is necessary due to the vast amount of data, but I think of other ways with poor results.<br\/>\n* 2) Moreover, it occurred to me that detecting outliers well and dealing with them is an important part.","e62da259":"### 3-1) Plot the null values","bdb1ed00":"### 3-4) Customer Transaction train data Heatmap","142bfb5f":"* To go through the process of PCA, you have to go through the process of StandardSclaer.","48ddc25e":"### 7-2) LogisiticRegressor predict","2207e41e":"* => So we cant use PCA","e7284a9d":"# 5. Feature Enginnering \n* Check the need for PCA technology\n* Split Train data \/ Test data","e1a24ae3":"* Two columns of dimension reduction represent only 0.01059.","a7a6b9bf":"### 7-1) LGBMClassifier predict","17d0d246":"#### => We can see that the value of target data is disproportionate."}}