{"cell_type":{"65eebd10":"code","db4fb376":"code","1f1eb568":"code","5edc0379":"code","a25a410b":"code","baf34500":"code","6ff025e7":"code","a8428ee7":"code","68936f28":"code","f6bc2e80":"code","0e354f2b":"code","22673b6a":"code","10809284":"code","e139e66b":"code","9809ef47":"code","e4869d4f":"code","6c84c73e":"code","c4d15882":"code","58d39031":"code","fe8176cf":"code","9c7c3bfb":"code","6b50e3b6":"code","24b68fec":"code","80260e20":"code","58628458":"code","33ac6a2e":"code","4fb53457":"code","fffbc24a":"code","4fa31f9e":"code","4818c594":"code","4149e6da":"code","1e61a743":"code","87831d88":"code","1bba4f4e":"code","e4f3f970":"code","d3972170":"code","69bba083":"code","f1a4d367":"code","f6bea5c9":"code","1d913ecc":"code","df46fcd3":"code","deebe5e2":"code","875286c1":"code","dc1c7fb3":"code","8d4ebafc":"code","01fc75e6":"code","5123984a":"code","478f27d3":"code","f8184add":"code","f5c2c7e0":"code","96d9aabe":"code","d9c16b62":"code","d9c8ae55":"code","23edc066":"code","a37d3d8c":"code","161c41e2":"markdown","76a22aed":"markdown","9aa31bf7":"markdown","ef87179c":"markdown","57b15422":"markdown","2ec71387":"markdown","a6849d0d":"markdown","b00878ae":"markdown","0d209caf":"markdown","ed79db00":"markdown","4177451e":"markdown","20bb339c":"markdown","d4fd8cf3":"markdown","40dc3363":"markdown","15a61f57":"markdown","3b02405e":"markdown","c026913d":"markdown","3df93fc4":"markdown","9ad56bfc":"markdown","6b2e2395":"markdown","bb73385e":"markdown","7e3bb211":"markdown","669e047c":"markdown","c679c3bb":"markdown","d00c6f6b":"markdown","4de92028":"markdown","eaaff919":"markdown","68fb77e5":"markdown","10696db5":"markdown"},"source":{"65eebd10":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","db4fb376":"# inputting the files\ntest_path = '..\/input\/nlp-getting-started\/test.csv'\ntrain_path = '..\/input\/nlp-getting-started\/train.csv'\n\ntest = pd.read_csv(test_path, index_col='id')\ntrain = pd.read_csv(train_path, index_col='id')","1f1eb568":"# test shape - 3263 x 3\n# train shape - 7613 x 4\nprint(\" test shape: \" ,test.shape)\nprint(\" train shape: \" ,train.shape)","5edc0379":"train.head()","a25a410b":"print(\"---% missing in train---\")\nprint(train.isnull().sum()\/7613*100)\n\nprint(\"---% missing in test---\")\nprint(test.isnull().sum()\/3263*100)","baf34500":"# this would set the target_mean as the mean of the target belonging to that keyword\ntrain['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n# check if it is set properly\ntrain[train['keyword']=='ablaze']","6ff025e7":"## The code of this graph is from the notebook \"NLP with Disaster Tweets - EDA, Cleaning and BERT\"\n\nfig1 = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'], hue=train.sort_values(by='target_mean', ascending=False)['target'])\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc='upper right')\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\n## space is shown as %20 - what to do with multiword keywords?","a8428ee7":"train.keyword.unique()","68936f28":"print(\"# of unique values in train - # of unique values in test:\" , train.keyword.nunique() - test.keyword.nunique())\nprint(set(train['keyword'].unique()) == set(test['keyword'].unique())) # so there are no new keyworkds in test that wasn't in train\nprint(\"# of unique locations in train - # of unique locations in test:\" , train.location.nunique() - test.location.nunique())","f6bc2e80":"# how many word in the tweets\ntrain['word_counts'] = train['text'].apply(lambda x: len(str(x).split()))\ntest['word_counts'] = test['text'].apply(lambda x: len(str(x).split()))\n# average length of chars in the word\ntrain['avg_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest['avg_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# how many characters in the tweets\ntrain['char_counts'] = train['text'].apply(lambda x: len(str(x)))\ntest['char_counts'] = test['text'].apply(lambda x: len(str(x)))","0e354f2b":"train.head()","22673b6a":"fig2, axes = plt.subplots(ncols=3, nrows=1, figsize=(20, 10), dpi=100)\n\nbins = 100\nplt.subplot(1, 3, 1)\nplt.hist(train[train.target == 0]['word_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\nplt.hist(train[train.target == 1]['word_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\nplt.xlabel('word counts')\nplt.ylabel('count')\nplt.legend(loc='upper right')\nplt.subplot(1, 3, 2)\nplt.hist(train[train.target == 0]['char_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\nplt.hist(train[train.target == 1]['char_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\nplt.xlabel('characters counts')\nplt.ylabel('count')\nplt.legend(loc='upper right')\nplt.subplot(1, 3, 3)\nplt.hist(train[train.target == 0]['avg_word_length'], alpha = 0.6, bins=bins, label='Fake', color='green')\nplt.hist(train[train.target == 1]['avg_word_length'], alpha = 0.6, bins=bins, label='Real', color='red')\nplt.xlabel('average word length')\nplt.ylabel('count')\nplt.legend(loc='upper right')\nplt.show()","10809284":"## definte the stopwords\nSTOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","e139e66b":"from collections import defaultdict\nimport string\ndef make_pun_dict(val):\n    pun_dict = defaultdict(int)\n    for tweet in train[train.target == int(val)]['text']:\n        pun = [pun for pun in str(tweet).lower().split() if pun in string.punctuation]\n        for sym in pun:\n            pun_dict[sym] +=1\n    return pun_dict","9809ef47":"real_pun_dict = make_pun_dict(1)\nfake_pun_dict = make_pun_dict(0)","e4869d4f":"fig3 = plt.figure(figsize=(10, 5), dpi=100)\nplt.bar(real_pun_dict.keys(), real_pun_dict.values(), color='r', label='Real', alpha=0.6)\nplt.bar(fake_pun_dict.keys(), fake_pun_dict.values(), color='g', label='Fake', alpha=0.6)\nplt.xlabel('symbol')\nplt.ylabel('count')\nplt.legend(loc='upper right')","6c84c73e":"fig4 = plt.figure(figsize=(10, 5), dpi=100)\ntrain['pun_counts'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest['pun_counts'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\nbins = 100\nplt.hist(train[train.target == 0]['pun_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\nplt.hist(train[train.target == 1]['pun_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\nplt.xlabel('punctuations counts')\nplt.ylabel('count')\nplt.xlim(0,40)\nplt.legend(loc='upper right')","c4d15882":"import regex as re\ndef remove_tag(text):\n    tag =re.compile(r'<(.*?)>')\n    return tag.sub(r'',text)\n\ntrain['cleaned']=train['text'].apply(lambda x : remove_tag(x))\ntest['cleaned']=test['text'].apply(lambda x : remove_tag(x))","58d39031":"def remove_link(text):\n    # all links start with 'http:\/\/t.co\/' or 'https:\/\/t.co\/'\n    # the ? means 0 or 1, so s is optional in https\n    link=re.compile(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\")\n    return link.sub(r'',text)\n\ntrain['cleaned']=train['cleaned'].apply(lambda x : remove_link(x))\ntest['cleaned']=test['cleaned'].apply(lambda x : remove_link(x))","fe8176cf":"def remove_mention(text):\n    mention=re.compile(r\"@[A-Za-z0-9_]+[ :]\")\n    return mention.sub(r'',text)\n\ntrain['cleaned']=train['cleaned'].apply(lambda x : remove_mention(x))\ntest['cleaned']=test['cleaned'].apply(lambda x : remove_mention(x))","9c7c3bfb":"# testing the functions to remove link and tags\nword = remove_link(\"word a b c <a>http:\/\/t.co\/abdcd<\/a> hello @aria_ahrary @TheTawniest:hello\")\nword = remove_mention(word)\nword = remove_tag(word)\n\nprint(word)","6b50e3b6":"# code taken from https:\/\/stackoverflow.com\/questions\/33404752\/removing-emojis-from-a-string-in-python\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain['cleaned']=train['cleaned'].apply(lambda x : remove_emoji(x))\ntest['cleaned']=test['cleaned'].apply(lambda x : remove_emoji(x))","24b68fec":"# testing the functions to remove emoji\nword = remove_emoji(\"Haha \ud83d\ude02\")\nprint(word)","80260e20":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ntrain['cleaned']=train['cleaned'].apply(lambda x : remove_punct(x))\ntest['cleaned']=test['cleaned'].apply(lambda x : remove_punct(x))","58628458":"# testing the functions to remove punctuation\nword = remove_punct(\"Hello. I am having a great time.\")\nprint(word)","33ac6a2e":"def remove_stopword(text):\n    for word in STOPWORDS:\n            token = \" \" + word + \" \"\n            text = text.replace(token, \" \")\n            text = text.replace(\"  \", \" \")\n    return text\n\ntrain['cleaned']=train['cleaned'].apply(lambda x : remove_stopword(x))\ntest['cleaned']=test['cleaned'].apply(lambda x : remove_stopword(x))","4fb53457":"# testing the functions to remove punctuation\nword = remove_stopword(\"Hello. I am having a great time.\")\nprint(word)","fffbc24a":"train.tail()\n# 7613 x 10","4fa31f9e":"train_labels = np.array(train['target'])\nprint(train_labels)\n\n#tweets = train['cleaned']\n#tweets_test = test['cleaned']\n\ntweets = train['text']\ntweets_test = test['text']","4818c594":"from sklearn.model_selection import train_test_split\ntweets_train, tweets_valid, labels_train, labels_valid = train_test_split(tweets, train_labels, test_size = 0.2, shuffle=True, random_state=0)\n","4149e6da":"print(\"tweets_train shape:\", tweets_train.shape)\nprint(\"labels_train shape:\", labels_train.shape)\n\nprint(\"tweets_valid shape:\", tweets_valid.shape)\nprint(\"labels_valid shape:\", labels_valid.shape)","1e61a743":"tweets.head()","87831d88":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 1000\nembedding_dim=8\nmax_length = 27\nnum_epochs = 10\ntoken = \"<Unknown>\"\n\n\n# num_words=vocab_size\n# oov_token=\"<Unknown>\"\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\ntokenizer.fit_on_texts(tweets_train)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(tweets_train)\ntraining_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\nvalid_sequences = tokenizer.texts_to_sequences(tweets_valid)\nvalid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","1bba4f4e":"print(\"training_padded shape:\",training_padded.shape)\nprint(\"valid_padded shape:\",valid_padded.shape)","e4f3f970":"print(len(word_index))","d3972170":"import tensorflow as tf\nmodel = tf.keras.Sequential([\n    # make word embedding\n    tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n    #flatten the network or use pooling\n    tf.keras.layers.Flatten(),\n    # dense neural network\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","69bba083":"history = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))","f1a4d367":"def plot_loss_and_accuracy(history, val,nrows,ncols,index):\n    plt.subplot(nrows,ncols,index)\n    plt.plot(history.history[val])\n    plt.plot(history.history['val_'+val])\n    plt.xlabel(\"# of Epochs\")\n    plt.ylabel(val)\n    plt.legend([val, \"val_\"+val])\n\nfig5, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\nplot_loss_and_accuracy(history,\"accuracy\",1,2,1)\nplot_loss_and_accuracy(history,\"loss\",1,2,2)\nplt.show()","f6bea5c9":"vocab_size = 500\nembedding_dim=16\nmax_length = 27\nnum_epochs = 10\ntoken = \"<Unknown>\"\n\n\n# num_words=vocab_size\n# oov_token=\"<Unknown>\"\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\ntokenizer.fit_on_texts(tweets_train)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(tweets_train)\ntraining_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\nvalid_sequences = tokenizer.texts_to_sequences(tweets_valid)\nvalid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","1d913ecc":"model_LSTM = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel_LSTM.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory_LSTM = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))","df46fcd3":"fig6, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\nplot_loss_and_accuracy(history_LSTM,\"accuracy\",1,2,1)\nplot_loss_and_accuracy(history_LSTM,\"loss\",1,2,2)\nplt.show()","deebe5e2":"model_GRU = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel_GRU.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory_GRU = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))","875286c1":"fig7, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\nplot_loss_and_accuracy(history_GRU,\"accuracy\",1,2,1)\nplot_loss_and_accuracy(history_GRU,\"loss\",1,2,2)\nplt.show()","dc1c7fb3":"model_Conv1D = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n    tf.keras.layers.Conv1D(128,5,activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel_Conv1D.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory_Conv1D = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))","8d4ebafc":"fig8, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\nplot_loss_and_accuracy(history_Conv1D,\"accuracy\",1,2,1)\nplot_loss_and_accuracy(history_Conv1D,\"loss\",1,2,2)\nplt.show()","01fc75e6":"#import tensorflow_datasets as tfds\n\n#tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for text in tweets_train), target_vocab_size = 2**10)","5123984a":"vocab_sizes = [250,500,1000,2000,5000]\nembedding_dim=16\nmax_length = 27\nnum_epochs = 10\ntoken = \"<Unknown>\"\n\nmodels=[]\n\nfor vocab_size in vocab_sizes:\n    tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n    tokenizer.fit_on_texts(tweets_train)\n    word_index = tokenizer.word_index\n\n    training_sequences = tokenizer.texts_to_sequences(tweets_train)\n    training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\n    valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n    valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\n    model = tf.keras.Sequential([\n        # make word embedding\n        tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n        #flatten the network or use pooling\n        tf.keras.layers.Flatten(),\n        # dense neural network\n        tf.keras.layers.Dense(6,activation='relu'),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n    ])\n\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n    models.append(model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid),verbose=0))\n    print(\"Done with a round\")\n\nprint(\"All Done\")","478f27d3":"\nfig6, axes = plt.subplots(5,2, figsize=(10, 15), dpi=100)\nfor i in range(len(models)):\n    plot_loss_and_accuracy(models[i],\"accuracy\",5,2,(i+(i+1)))\n    plot_loss_and_accuracy(models[i],\"loss\",5,2,(i+(i+2)))\nplt.show()\n","f8184add":"vocab_sizes = 500\nembedding_dims= [16,32]\nmax_length = 27\nnum_epochs = 10\ntoken = \"<Unknown>\"\n\nmodels=[]\n\nfor embedding_dim in embedding_dims:\n    tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n    tokenizer.fit_on_texts(tweets_train)\n    word_index = tokenizer.word_index\n\n    training_sequences = tokenizer.texts_to_sequences(tweets_train)\n    training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\n    valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n    valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\n    model = tf.keras.Sequential([\n        # make word embedding\n        tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n        #flatten the network or use pooling\n        tf.keras.layers.Flatten(),\n        # dense neural network\n        tf.keras.layers.Dense(6,activation='relu'),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n    ])\n\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n    models.append(model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid),verbose=0))\n    print(\"Done with a round\")\n\nprint(\"All Done\")","f5c2c7e0":"\nfig7, axes = plt.subplots(2,2, figsize=(10, 10), dpi=100)\nfor i in range(len(models)):\n    plot_loss_and_accuracy(models[i],\"accuracy\",2,2,(i+(i+1)))\n    plot_loss_and_accuracy(models[i],\"loss\",2,2,(i+(i+2)))\nplt.show()\n","96d9aabe":"vocab_size = 1000\nembedding_dim=8\nmax_length = 27\nnum_epochs = 4\ntoken = \"<Unknown>\"\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\ntokenizer.fit_on_texts(tweets)\nword_index = tokenizer.word_index\n\nall_sequences = tokenizer.texts_to_sequences(tweets)\nall_padded = pad_sequences(all_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\ntest_sequences = tokenizer.texts_to_sequences(tweets_test)\ntest_padded = pad_sequences(test_sequences, truncating = 'post', padding='post', maxlen=max_length)\n\nmodel = tf.keras.Sequential([\n        # make word embedding\n    tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n        #flatten the network or use pooling\n    tf.keras.layers.Flatten(),\n        # dense neural network\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nfinal = (model.fit(all_padded, train_labels, epochs=num_epochs, verbose=2))","d9c16b62":"prediction = model.predict(test_padded)","d9c8ae55":"prediction_flatten = np.ravel(prediction) # flatten it in order to make into a series in a dataframe","23edc066":"submission = pd.DataFrame({\n        \"id\": test.index,\n        \"target\": prediction_flatten\n    })\n\ndef threshold(val):\n    if val >= 0.5:\n        value = 1\n    else:\n        value = 0\n    return value\nsubmission['target'] = submission['target'].apply(threshold)\n\nsubmission.head()","a37d3d8c":"submission.to_csv('submission.csv', index=False)","161c41e2":"# Step 3 : clean up the tweets\n\nGet rid of :\n* Links \/ html tags\n\n* Punctuation\n\n* Emojis - perhaps in other model, might be helpful, but in this one, ignore\n\n* Mentions\n\n* Stopwords","76a22aed":"# Tweets - real or fake disaster\n* Disaster detection from twitter feed\n* wild fire, accident, you name it! We will find out whether the tweets is indicative of a real disaster or not\n\n## Apporach\n1. get the data\n2. inspect the data\n3. clean up the tweets, stripping, pre-processing\n    * need to shuffle the train data before splitting it into train and cross validation\n4. decide what deep learning model to use\n    * use tensorflow to do tokenization,embedding and dense layers\n5. improve model using cross validation data\n6. output the test data\n7. submit","9aa31bf7":"## Checking for useless information:\n\n### Stop words\n* words that carry no meanings and do not need to be used in the analysis\n\n### Punctuation\n* Unless real disaster use less of a certain puncutation, not useful --- let's check before removing\n\n### Links \/ html tags\n* Unless real disaster has less links, not useful --- let's check before removing","ef87179c":"* Let's try different vocab_size","57b15422":"with sub_words","2ec71387":"Let's check if there is any specific punctuation that is most common","a6849d0d":"# Step 6 : get the output file","b00878ae":"Defining the stopwords --- stopwords comng from Yoast SEO","0d209caf":"The code of the graph below is from the notebook \"NLP with Disaster Tweets - EDA, Cleaning and BERT\"\n","ed79db00":"# Step 4: Selecting the model to use and getting \n## TensorFlow Keras Tokenizer","4177451e":"## Word counts","20bb339c":"With 1D Convolutional network","d4fd8cf3":"Looking at the distribution of these new features","40dc3363":"Looks like the punctuation counts are very common, let's check the number of punctuation just in case","15a61f57":"## Missing Data:\n* Keyword is mostly present, with <1 % missing, can manifacture keyword from tweets, or set it as 'unknown'\n* location missing for most, might be better to not use as a feature","3b02405e":"# Step 1. get the data","c026913d":"* Let's settle on a final model","3df93fc4":"# Step 2. inspect the data","9ad56bfc":"The number of punctuations used in real disaster tweet is slighter higher than in fake ones, but not big difference","6b2e2395":"* Looks like by removing punctuations, we are removing the dash in between words too, not ideal","bb73385e":"Let's try GRU","7e3bb211":"Let's split the training data into training and validation","669e047c":"Let's try some LSTM","c679c3bb":"## Feature engineering\n* If we want to use the keyword as a feature, let's check whether certain keywords are indicative of a real disaster or a fake","d00c6f6b":"# Step 5 : tuning the hyperparamters\n* changing the following:\n    * vocab_size = 1000\n    * embedding_dim=16\n    * max_length = 27\n    * num_epochs = 30","4de92028":"Looking at the cleaned text","eaaff919":"* We would want all the words to be lower case before tokenizing them, but let's just try TensorFlow tokenizer first\n    * automatically remove punctuation\n    * automatically lower case words","68fb77e5":"## Columns:\n* keyword - ponential useful in identifying whether it is a disaster\n* location - might not be relevant except to try and match the tweets, but would require alot of work\n* text - the tweets, main input\n* target - 1 as real disaster, 0 as not a disaster","10696db5":"Looks like real disaster tweets have :\n* higher word counts\n* larger characters counts\n* higher average word length\nThese 3 are closely related to each other, quite redundent, probably only would use one\n* average word lengths looks like good candidate sine it is roughly uniform and clearer separation"}}