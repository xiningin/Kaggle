{"cell_type":{"d0e0837d":"code","f71aba55":"code","9f7047a2":"code","1b6866bd":"code","ac962181":"code","59462c0c":"code","fc8f7d74":"code","7e3b4197":"code","a2f898fb":"code","83dd634e":"code","8a174cd8":"code","685a85af":"code","1e34cee0":"code","07ad0077":"code","511be678":"code","aceaae87":"code","027793e5":"code","ed918a44":"code","1e9823c2":"markdown","adf88b84":"markdown","b5527f75":"markdown","ccb8c285":"markdown","56224614":"markdown","4b191ca7":"markdown","fc35aafd":"markdown","56a52be9":"markdown","3c8d217f":"markdown","ca7f1a2c":"markdown","2fafcbbf":"markdown","22423548":"markdown","d3f2a25d":"markdown","ba42c236":"markdown","72603c3a":"markdown","4bc03477":"markdown","ee880806":"markdown"},"source":{"d0e0837d":"import numpy as np\nimport pandas as pd\nimport csv\nfrom collections import defaultdict","f71aba55":"SALES = \"..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\"\nPRICES = \"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\"\nCALENDAR = \"..\/input\/m5-forecasting-accuracy\/calendar.csv\"\n\n# SALES = \"..\/data\/raw\/sales_train_validation.csv\"\n# PRICES = \"..\/data\/raw\/sell_prices.csv\"\n# CALENDAR = \"..\/data\/raw\/calendar.csv\"\n\nNUM_SERIES = 30490\nNUM_TRAINING = 1941\nNUM_TEST = NUM_TRAINING + 1 * 28","9f7047a2":"series_ids = np.empty(NUM_SERIES, dtype=object)\nitem_ids = np.empty(NUM_SERIES, dtype=object)\ndept_ids = np.empty(NUM_SERIES, dtype=object)\ncat_ids = np.empty(NUM_SERIES, dtype=object)\nstore_ids = np.empty(NUM_SERIES, dtype=object)\nstate_ids = np.empty(NUM_SERIES, dtype=object)","1b6866bd":"qties = np.zeros((NUM_TRAINING, NUM_SERIES), dtype=float)\nsell_prices = np.zeros((NUM_TEST, NUM_SERIES), dtype=float)","ac962181":"%%time\nid_idx = {}\nwith open(SALES, \"r\", newline='') as f:\n    is_header = True\n    i = 0\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        series_id, item_id, dept_id, cat_id, store_id, state_id = row[0:6]\n        # Remove '_validation\/_evaluation' at end by regenerating series_id\n        series_id = f\"{item_id}_{store_id}\"\n\n        qty = np.array(row[6:], dtype=float)\n\n        series_ids[i] = series_id\n\n        item_ids[i] = item_id\n        dept_ids[i] = dept_id\n        cat_ids[i] = cat_id\n        store_ids[i] = store_id\n        state_ids[i] = state_id\n\n        qties[:, i] = qty\n\n        id_idx[series_id] = i\n\n        i += 1","59462c0c":"%%time\nwm_yr_wk_idx = defaultdict(list)  # map wmyrwk to d:s\nwith open(CALENDAR, \"r\", newline='') as f:\n    for row in csv.DictReader(f):\n        d = int(row['d'][2:])\n        wm_yr_wk_idx[row['wm_yr_wk']].append(d)\n        # TODO: Import the rest of the data","fc8f7d74":"%%time\nwith open(PRICES, \"r\", newline='') as f:\n    is_header = True\n    for row in csv.reader(f):\n        if is_header:\n            is_header = False\n            continue\n        store_id, item_id, wm_yr_wk, sell_price = row\n        series_id = f\"{item_id}_{store_id}\"\n        series_idx = id_idx[series_id]\n        for d in wm_yr_wk_idx[wm_yr_wk]:\n            sell_prices[d - 1, series_idx] = float(sell_price)","7e3b4197":"qty_ts = pd.DataFrame(qties,\n                      index=range(1, NUM_TRAINING + 1),\n                      columns=[state_ids, store_ids,\n                               cat_ids, dept_ids, item_ids])\n\nqty_ts.index.names = ['d']\nqty_ts.columns.names = ['state_id', 'store_id',\n                        'cat_id', 'dept_id', 'item_id']\n\nprice_ts = pd.DataFrame(sell_prices,\n                        index=range(1, NUM_TEST + 1),\n                        columns=[state_ids, store_ids,\n                                 cat_ids, dept_ids, item_ids])\nprice_ts.index.names = ['d']\nprice_ts.columns.names = ['state_id', 'store_id',\n                          'cat_id', 'dept_id', 'item_id']","a2f898fb":"qty_ts","83dd634e":"price_ts","8a174cd8":"LEVELS = {\n    1: [],\n    2: ['state_id'],\n    3: ['store_id'],\n    4: ['cat_id'],\n    5: ['dept_id'],\n    6: ['state_id', 'cat_id'],\n    7: ['state_id', 'dept_id'],\n    8: ['store_id', 'cat_id'],\n    9: ['store_id', 'dept_id'],\n    10: ['item_id'],\n    11: ['state_id', 'item_id'],\n    12: ['item_id', 'store_id']\n}","685a85af":"COARSER = {\n    'state_id': [],\n    'store_id': ['state_id'],\n    'cat_id': [],\n    'dept_id': ['cat_id'],\n    'item_id': ['cat_id', 'dept_id']\n}","1e34cee0":"def aggregate_all_levels(df):\n    levels = []\n    for i in range(1, max(LEVELS.keys()) + 1):\n        level = aggregate_groupings(df, i, *LEVELS[i])\n        levels.append(level)\n    return pd.concat(levels, axis=1)\n\ndef aggregate_groupings(df, level_id, grouping_a=None, grouping_b=None):\n    \"\"\"Aggregate time series by summing over optional levels\n\n    New columns are named according to the m5 competition.\n\n    :param df: Time series as columns\n    :param level_id: Numeric ID of level\n    :param grouping_a: Grouping to aggregate over, if any\n    :param grouping_b: Additional grouping to aggregate over, if any\n    :return: Aggregated DataFrame with columns as series id:s\n    \"\"\"\n    if grouping_a is None and grouping_b is None:\n        new_df = df.sum(axis=1).to_frame()\n    elif grouping_b is None:\n        new_df = df.groupby(COARSER[grouping_a] + [grouping_a], axis=1).sum()\n    else:\n        assert grouping_a is not None\n        new_df = df.groupby(COARSER[grouping_a] + COARSER[grouping_b] +\n                            [grouping_a, grouping_b], axis=1).sum()\n\n    new_df.columns = _restore_columns(df.columns, new_df.columns, level_id,\n                                      grouping_a, grouping_b)\n    return new_df","07ad0077":"def _restore_columns(original_index, new_index, level_id, grouping_a, grouping_b):\n    original_df = original_index.to_frame()\n    new_df = new_index.to_frame()\n    for column in original_df.columns:\n        if column not in new_df.columns:\n            new_df[column] = None\n\n    # Set up `level` column\n    new_df['level'] = level_id\n\n    # Set up `id` column\n    if grouping_a is None and grouping_b is None:\n        new_df['id'] = 'Total_X'\n    elif grouping_b is None:\n        new_df['id'] = new_df[grouping_a] + '_X'\n    else:\n        assert grouping_a is not None\n        new_df['id'] = new_df[grouping_a] + '_' + new_df[grouping_b]\n\n    new_index = pd.MultiIndex.from_frame(new_df)\n    # Remove \"unnamed\" level if no grouping\n    if grouping_a is None and grouping_b is None:\n        new_index = new_index.droplevel(0)\n    new_levels = ['level'] + original_index.names + ['id']\n    return new_index.reorder_levels(new_levels)","511be678":"aggregate_all_levels(qty_ts)","aceaae87":"def calculate_weights(totals):\n    \"\"\"Calculate weights from total sales.\n\n    Uses all data in the dataframe so remember to calculate total sales\n    (quantity times sell price) and .\n\n    :param totals: Total sales\n    :return: Series of weights with (level, *_id, id:) as multi-index\n    \"\"\"\n    summed = aggregate_all_levels(totals).sum()\n    \n    return summed \/ summed.groupby(level='level').sum()","027793e5":"final_month_totals = (qty_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1] *\n                      price_ts.loc[NUM_TRAINING - 28 + 1:NUM_TRAINING + 1])\n\nweights = calculate_weights(final_month_totals)","ed918a44":"weights_export = weights.transpose()\\\n        .reset_index(level=['level', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id'],\n                    drop=True)\nweights_export.to_csv(\"weights.csv\")","1e9823c2":"We need to save the weights to a CSV (since this is the purpose of this notebook.)","adf88b84":"## Aggregation","b5527f75":"## Create dataset\n\nUsing Pandas directly to read the data and reshape it appears to be a bit slow and uses a significant amount of memory.  Instead we'll read the data line by line and store it in NumPy arrays (but we'll try and keep the rest of the code in the notebook nicely vectorized and high-level =).","ccb8c285":"> **NB.** I'm writing this notebook when the public leaderboard is based on the actual final month (strictly speakin, the final 28 day period) of the training data, therefore the weights are actually calculated using the month before that.  A bit confusing, I know.","56224614":"### Weights\n\nThe scoring takes into account the final month's total sales and weights the series on every level accordingly.","4b191ca7":"A small complication is that Pandas doesn't align during column-wise concatenation, ie., if two dataframes have some different column levels, `pd.concat` does not match levels that are the same between the frames.\n\nThe easiest solution is to add back the levels we lost after grouping for now.","fc35aafd":"And if we look at the data, we see how the series are organized into columns:","56a52be9":"### Importing calendar data\n\nThe calendar data has information about which day of the week a given day is, if there are any special events, and most importantly for this notebook, which week (`wm_yr_wk`) the day is in.  We'll need this to get the prices of items, which in turn is necessary in order to calculate the weights we need for estimating our scores.","3c8d217f":"### Importing price data\n\nThe price data describes the weekly prices for each item in every store.","ca7f1a2c":"In this competition, our models are evaluated on 12 different levels defined by combinations of the groupings of the series.  \n\nIt is important that we can aggregate our time series, eg., calculate the total sales in each state, so that\nwe can evaluate a model's per-store item sales data forecasts on every level.\n\nThe levels used in the competition are:","2fafcbbf":"The data for the competition consists primarily of 30490 time series of sales data for 3049 items sold in 10 different stores in 3 states.  The items are classified as being in one of 3 categories that are further subdivided into a total of 7 departments.\n\nThe representation we'll look at in this notebook is representing each individual time series as a column in a data frame indexed by the day (`d`).\n\nFor the individual (level 12 series), we'll index the series in the columns by `(state_id, store_id, cat_id, dept_id, item_id)`.","22423548":"A quick peek at the aggregated sales data:","d3f2a25d":"Pandas views all column levels as independent, but here they are not; all series with the same `dept_id` belong to the same `cat_id`, for example.  When grouping our columns, we'll also keep any coarser groupings.","ba42c236":"# Calculation of Evaluation Period Weights\n\nUses the same approach as I used in [this kernel for HTS in Pandas with fast loading](https:\/\/www.kaggle.com\/christoffer\/pandas-multi-indices-for-hts-fast-loading-etc).  \n\nSee that notebook for more info and some sample models (seasonal na\u00efve, PyTorch neural net, simple ensemble). (This is pretty much just a fork with most of the irrelevant bits cut out.)","72603c3a":"## Evaluation","4bc03477":"### Building DataFrame\n\nWe'll store the dataset in two dataframes:\n\n- **`qty_ts`:** sales data.\n- **`price_ts`:** prices.","ee880806":"### Importing and reshaping sales data\n\nEach row in the sales data consists of six columns for an id of the series together with the five levels item, department, category, store, and, state."}}