{"cell_type":{"69e864ca":"code","ea9635b0":"code","2d336fd5":"code","3c211c9e":"code","b6009c42":"code","0b67dabc":"code","243a1dfc":"code","e72591ea":"code","2e053b52":"code","73dc6459":"code","4c469ab7":"code","a0c554bc":"code","f8fde0f4":"code","ee8bdcd1":"code","4f354117":"code","015d3d92":"code","9da02f27":"code","ad4ab381":"code","b47d0c61":"code","c2b5c6e7":"code","3adb0c6b":"code","fe743c58":"code","712cbae8":"code","309b9774":"code","f0dc90e1":"code","00ebed2b":"code","675c1d8e":"code","e0757316":"code","fe093f0d":"code","09cf2444":"code","87e69dd2":"code","ade3104f":"code","03a8f2f3":"code","06313e28":"code","4aff3f4c":"code","194cdf63":"code","559c8582":"code","dd46d0e0":"code","78156ab1":"code","2e3de27c":"code","fef8b10a":"code","7e843215":"code","cf61dc3d":"code","9d59fd55":"code","cbc09dfe":"code","0c33b4a8":"code","dbd7b9ce":"code","ddb1bb58":"code","d8bffbf8":"code","e0b0092e":"code","2d92a386":"code","b3b48256":"code","6229b511":"code","ac554da3":"code","ea31440f":"code","1a324e16":"code","3c016560":"code","c4e5706a":"code","a4c1f2db":"code","2a46b5d6":"code","4937a110":"code","eecc0089":"code","ec3ed040":"markdown","c26f6645":"markdown","baebb1ce":"markdown","f704f815":"markdown","5cbe8fd0":"markdown","2e4f22ac":"markdown","5e2342be":"markdown","34b4661f":"markdown","3508d0fe":"markdown","aeff563e":"markdown","b18e3ddc":"markdown","78d4da79":"markdown","74b91016":"markdown","bf01e6ec":"markdown","8963e1ba":"markdown","e0e85430":"markdown","e6ec9714":"markdown","05652400":"markdown","a581981c":"markdown","f393e3be":"markdown","63e2e75d":"markdown","a0b5305f":"markdown","13e322ed":"markdown","835493a9":"markdown","6d3c4b3d":"markdown","f72681a4":"markdown"},"source":{"69e864ca":"#Importing the Required Python Packages\nimport shutil\nimport string\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nimport ast\nfrom sklearn import utils\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport spacy\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestRegressor\npd.set_option('display.max_colwidth', None)","ea9635b0":"# Lets load the default english model of spacy\nnlp = spacy.load('en_core_web_sm')","2d336fd5":"datapath = \"..\/input\/medal-emnlp\/pretrain_subset\"","3c211c9e":"# Lets load the train dataset.\n\ntrain = pd.read_csv(datapath + '\/train.csv')\n","b6009c42":"# Lets load validation and test datasets as well\nvalid = pd.read_csv(f'{datapath}\/valid.csv')\ntest = pd.read_csv(f'{datapath}\/test.csv')\nvalid = valid[:10000]\ntest = test[:10000]","0b67dabc":"# Lets create a function to create a new feature 'ABV' from dataset\ndef createFeature(df):    \n    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]","243a1dfc":"train['ABV'] = createFeature(train)\nvalid['ABV'] = createFeature(valid)\ntest['ABV'] = createFeature(test)","e72591ea":"grouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\ngrouped = grouped.sort_values(by='TEXT', ascending = False)","2e053b52":"topAbv = grouped['ABV'][:20]","73dc6459":"train = train[train['ABV'].isin(topAbv)]\nvalid = valid[valid['ABV'].isin(topAbv)]\ntest = test[test['ABV'].isin(topAbv)]","4c469ab7":"# Lets create a function to remove all the Punctuations from Text\ndef removePunctuation(df):\n    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]","a0c554bc":"# Lets create a function to Tokenize the Text column of dataset\ndef createTokens(df):\n    return df['TEXT'].apply(lambda x: x.split(' '))","f8fde0f4":"#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\ndef dropCols(df):\n    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])","ee8bdcd1":"# Lets create a function to remove stop words from the Text column\ndef removeStop(df):\n    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n    # Remove any stopwords which appear to be an Abbreviation\n    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])","4f354117":"def tolower(df):\n    return [t.lower() for t in df['TEXT']]","015d3d92":"def preProcessData(df):   \n    df['TEXT'] = tolower(df)\n    df['TEXT'] = removePunctuation(df)\n    df['TOKEN'] = createTokens(df)\n    df = dropCols(df)\n    df['TOKEN'] = removeStop(df)\n    return df","9da02f27":"# Lets load the train dataset.\ntrain = preProcessData(train)\nvalid = preProcessData(valid)\ntest = preProcessData(test)","ad4ab381":"train.head()\n","b47d0c61":"valid.head(3)","c2b5c6e7":"test.head(3)","3adb0c6b":"    abbrev = list(train['ABV'].unique())\n    valid = valid[valid['ABV'].isin(abbrev)]\n    test = test[test['ABV'].isin(abbrev)]\n    labels = list(train['LABEL'].unique())\n    valid = valid[valid['LABEL'].isin(labels)]\n    test = test[test['LABEL'].isin(labels)]\n","fe743c58":"# # Convert TOKEN column from string to list\n# train['TOKEN'] = train['TOKEN'].apply(lambda x: ast.literal_eval(x))\n# valid['TOKEN'] = valid['TOKEN'].apply(lambda x: ast.literal_eval(x))\n# test['TOKEN'] = test['TOKEN'].apply(lambda x: ast.literal_eval(x))","712cbae8":"train_tagged = train.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\nvalid_tagged = valid.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\ntest_tagged = test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)","309b9774":"train_tagged.values[:5]","f0dc90e1":"vectorize = Doc2Vec(dm=0, vector_size=100, min_count=2, window = 2)\nvectorize.build_vocab(train_tagged.values)","00ebed2b":"vectorize.train(train_tagged.values, total_examples=len(train_tagged.values), epochs=30)","675c1d8e":"def vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=30)) for doc in sents])\n    return targets, regressors","e0757316":"y_train, X_train = vec_for_learning(vectorize, train_tagged)","fe093f0d":"# param_grid = {'n_estimators':[10, 100, 1000]}\n# grid_model = GridSearchCV(RandomForestRegressor(random_state=42), param_grid)\n# grid_model.fit(X_train, y_train)","09cf2444":"# y_pred_valid = grid_model.decision_function(X_valid)\n# valid[\"Prediction\"] = y_pred_valid\n# valid[\"Target\"] = y_valid\n# valid.to_csv(\"Random_Forest_results.csv\")","87e69dd2":"def abbrev_predict(model, x_vec, x_abv, abv_dict):\n    pred_probs = model.predict_proba(x_vec)\n    \n    returned_preds = []\n    #     sorted_probs = dict(sorted(probs.items(), key=lambda item:item[1]))\n    #loop through the arrays of predicted probabilities\n    for pred_prob, x_abv in zip(pred_probs, x_abv):\n        #allign probabilities to its corresponding class\n        probs = dict(zip(model.classes_, pred_prob))\n        probs_sorted = {k: v for k, v in sorted(probs.items(), key=lambda item: item[1], reverse=True)}\n        #loop through the pr\n        for class_, prob in probs_sorted.items():\n            if abv_dict[class_] == x_abv:\n                returned_preds.append(class_)\n                break\n    \n    return returned_preds","ade3104f":"# param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100]}\n# grid_model = GridSearchCV(LogisticRegression(n_jobs=-1), param_grid)","03a8f2f3":"# grid_model.fit(X_train, y_train)","06313e28":"### Best parameters for the Grid Search\n# grid_model.best_params_","4aff3f4c":"### Accuracy Score\n# grid_model.best_score_","194cdf63":"logreg = LogisticRegression(n_jobs=-1, C=1)\nlogreg.fit(X_train, y_train)\npickle.dump(logreg, open(\"MeDal_logistic.model\", 'wb'))\n\n\n","559c8582":"### Apply the above Model on Validation Set\ny_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\n\nabvs = train[[\"LABEL\", \"ABV\"]].drop_duplicates()\nabvs_dict = dict(zip(abvs.LABEL, abvs.ABV))\n\n\ny_pred_valid = abbrev_predict(logreg, X_valid, valid.ABV, abvs_dict)","dd46d0e0":"valid[\"Prediction\"] = y_pred_valid\nvalid[\"Target\"] = y_valid\nvalid.to_csv(\"Logistic_Regression_Results.csv\")","78156ab1":"print('Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\nprint('Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","2e3de27c":"### Apply the above Model on Test Set\ny_test, X_test = vec_for_learning(vectorize, test_tagged)\ny_pred_test = logreg.predict(X_test)","fef8b10a":"accuracy = accuracy_score(y_test, y_pred_test)\nf1_scr = f1_score(y_test, y_pred_test, average='weighted')\nprint('Test Accuracy:', accuracy)\nprint('Test F1-Score:', f1_scr)","7e843215":"# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}\n# grid_svm = GridSearchCV(SVC(), param_grid)","cf61dc3d":"# grid_svm.fit(X_train, y_train)","9d59fd55":"### Best parameters for the Grid Search\n# grid_svm.best_params_","cbc09dfe":"### Accuracy Score\n# grid_svm.best_score_","0c33b4a8":"#potentially useful\n#https:\/\/www.discoverbits.in\/371\/sklearn-attributeerror-predict_proba-available-probability\n\nsvcModel = SVC(C=10, gamma=0.01, kernel='rbf', probability=True)\nsvcModel.fit(X_train, y_train)\npickle.dump(svcModel, open(\"MeDal_svc.model\", 'wb'))","dbd7b9ce":"### Apply the above Model on Validation Set\ny_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\ny_pred_valid = abbrev_predict(svcModel, X_valid, valid.ABV, abvs_dict)","ddb1bb58":"print('SVM Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\nprint('SVM Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","d8bffbf8":"### Apply the above Model on Test Set\ny_test, X_test = vec_for_learning(vectorize, test_tagged)\ny_pred_test = svcModel.predict(X_test)","e0b0092e":"accuracy = accuracy_score(y_test, y_pred_test)\nf1_scr = f1_score(y_test, y_pred_test, average='weighted')\nprint('SVM Test Accuracy:', accuracy)\nprint('SVM Test F1-Score:', f1_scr)","2d92a386":"valid[\"Prediction\"] = y_pred_valid\nvalid[\"Target\"] = y_valid\nvalid.to_csv(\"SVM_results.csv\")","b3b48256":"# param_grid = {'n_estimators':[100, 500, 1000], 'max_depth':[5, 6, 7], 'min_child_weight': [3, 5, 8]}","6229b511":"# unique = list(set(y_train))\n# X_train = pd.DataFrame(X_train)\n# y_train = np.asarray(y_train)","ac554da3":"# XGBgrid = GridSearchCV(XGBClassifier(learning_rate= 0.1, gamma= 0, objective= 'multi:softmax', num_classes= len(unique), seed= 27), param_grid)","ea31440f":"# XGBgrid.fit(X_train, y_train)","1a324e16":"### Best parameters for the Grid Search\n# XGBgrid.best_params_","3c016560":"### Accuracy Score\n# grid_svm.best_score_","c4e5706a":"# # XGBModel = XGBClassifier(\n#  learning_rate =0.1,\n#  n_estimators=1000,\n#  max_depth=7,\n#  min_child_weight=4,\n#  gamma=0,\n#  objective= 'multi:softmax',\n#  seed=27)   \n# XGBModel.fit(X_train, y_train)\n# pickle.dump(logreg, open(\"MeDal_logistic.model\", 'wb'))","a4c1f2db":"# ### Apply the above Model on Validation Set\n# y_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\n# X_valid = pd.DataFrame(X_valid)\n# y_valid = np.asarray(y_valid)\n# y_pred_valid = XGBModel.predict(X_valid)","2a46b5d6":"# print('XGBoost Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\n# print('XGBoost Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","4937a110":"# ### Apply the above Model on Test Set\n# y_test, X_test = vec_for_learning(vectorize, test_tagged)\n# X_test = pd.DataFrame(X_test)\n# y_test = np.asarray(y_test)\n# y_pred_test = XGBModel.predict(X_test)","eecc0089":"# accuracy = accuracy_score(y_test, y_pred_test)\n# f1_scr = f1_score(y_test, y_pred_test, average='weighted')\n# print('XGBoost Test Accuracy:', accuracy)\n# print('XGBoost Test F1-Score:', f1_scr)","ec3ed040":"### Building the Final Vector Feature Classifier","c26f6645":"### Lets calculate some Performance Metrics on the Test predictions.","baebb1ce":"## Model# 2: SVM","f704f815":"### Lets perform a Grid Search to get the best possible combination of Hyperparameters for SVM's","5cbe8fd0":"## Model# 1: Logistic Classifier","2e4f22ac":"### Lets keep only relevant records in Valid and test set.","5e2342be":"### As per the above analysis of validation set, it can be seen that the Logistic Classification model gives\n1. F1- Score of: 0.68\n2. Accuracy of: 69%\n\nHence, lets apply this model to our Test set and check its performance metrics.","34b4661f":"# Next Steps","3508d0fe":"## Step# 1: Loading Dataset","aeff563e":"## Step# 2: Apply Doc2vec vectorizer on the Dataset","b18e3ddc":"### Lets perform a Grid Search to get the best possible combination of Hyperparameters for Logistic Regression Model","78d4da79":"## We can see that a basic Logistic Classification implementation gives 70% Accurate results hence, for next steps we can:\n1. Try tuning the Doc2Vec vectorizer's Hyperparameters.\n2. Try some other Classification Algorithms like SVN, Random Forrest and compare results.\n3. Present model has been trained to disambiguate 20 'Medical Abbreviations' but this same model can be generalized to be used in other fields as well. Some including Scientific Researches and Internet Slags.","74b91016":"### Lets tag every Token List with its Label","bf01e6ec":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.71\n2. Average Recall: 0.71","8963e1ba":"### Train a XGBoost Classifier","e0e85430":"### As per the above analysis of validation set, it can be seen that the SVC Classification model gives\n1. F1- Score of: 0.71\n2. Accuracy of: 70%\n\nHence, lets apply this model to our Test set and check its performance metrics.","e6ec9714":"### Lets calculate some Performance Metrics on the Test predictions.","05652400":"### Apply the best parameters to SVC and train the model","a581981c":"### As per the above analysis of validation set, it can be seen that the XGBoost Classification model gives\n1. F1- Score of: 0.58\n2. Accuracy of: 59.1%\n\nHence, lets apply this model to our Test set and check its performance metrics.","f393e3be":"## Model# 3: XGBoost","63e2e75d":"### Lets calculate some Performance Metrics on the Test predictions.","a0b5305f":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.69\n2. Average Recall: 0.70","13e322ed":"### Lets create a parameter grid for XGBoost Model","835493a9":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.59\n2. Average Recall: 0.59","6d3c4b3d":"### Apply the best parameters to Logistic Regression and train the model.","f72681a4":"# Abbreviation Disambiguation in Medical Texts - Data Modeling\n\nThis Notebook is in continuation of the notebook- 'Step 2- Data Preprocessing' and lists down:\n\n1. Modeling Preprocessed data using: GridSearchCV on Logistic Regression, SVM and XG Boost.\n2. Testing the models using Test set.\n3. Comparing the models and identifying the Next Steps"}}