{"cell_type":{"dc041a9a":"code","81a1da82":"code","9b84c4d0":"code","d3510da6":"code","f30fc277":"code","57a34ab1":"code","2b3f2d34":"code","01de5475":"code","f7ba6e96":"code","8cc00e34":"code","838f1dd2":"code","457f37a7":"code","d3595b78":"code","a6689cc3":"code","06369c3e":"code","28f39cf0":"code","fe1e6b75":"code","8bc3765f":"code","0e88a61f":"code","277600c9":"code","54994fa2":"code","63e38501":"code","0cb72312":"code","07e0889e":"code","206d4720":"code","c6cdf1bb":"code","f5cadd4c":"code","3b088f6f":"code","22e64e25":"code","4cdd64d5":"code","d1cac967":"code","c1e8f6c9":"code","55224561":"code","4bf11210":"code","0431d2d6":"code","61240dfb":"code","c8d80512":"code","0bb9a5e0":"code","1da9811f":"code","5a9b3b0e":"code","549ce9df":"code","9823895d":"code","2ad0d8d2":"code","82fdf9c9":"code","b18feed8":"code","87d836fa":"code","32c5e571":"code","4a3260eb":"code","f4f5152d":"code","34f7e0ad":"code","bf645def":"code","6bb4082d":"code","bae1c962":"code","1a97fdd5":"code","04a06ae6":"code","46889c4a":"code","1776fe9c":"code","db24d8c9":"code","f680749f":"code","e986476c":"code","9e5fffd5":"code","03d674a5":"code","c48e72ac":"code","740ada3a":"code","315cedb7":"code","3a4519b6":"code","6073d91c":"code","0dc3eb38":"code","cef3127a":"code","7e6ca70d":"code","fa334ce9":"code","4a9821a0":"code","74d55158":"code","f4a26f11":"code","db16fe71":"code","10541171":"code","3ab46d8b":"code","e3555c29":"code","ec770b5f":"code","53736a28":"code","dceab55a":"code","22228b3f":"code","e385306a":"code","bbc01d75":"code","9199ea99":"code","08315b8d":"code","4563ea77":"code","a205bc28":"markdown","27d47a1c":"markdown","c1e84b74":"markdown","2f6a66fd":"markdown","07399954":"markdown","172dc0ae":"markdown","fddfb147":"markdown","776e7ead":"markdown","129b8d90":"markdown","021c48fd":"markdown","32fc142e":"markdown","d6650378":"markdown","765008ce":"markdown","ae8d0fdf":"markdown","940e00fe":"markdown","115933a1":"markdown","71046630":"markdown","94926cda":"markdown","fb280485":"markdown","964258c8":"markdown","1269e7cd":"markdown","979e6e96":"markdown","35a96b65":"markdown","e53ff76b":"markdown","2813125e":"markdown","3a1f54e0":"markdown","05a1f919":"markdown","a42b0b5e":"markdown","5b03021a":"markdown","0e3e6142":"markdown","faa97023":"markdown","0c9857d9":"markdown","becd3f60":"markdown","4c01711d":"markdown","a32f0129":"markdown","96f00b60":"markdown","cfca196a":"markdown","56357bab":"markdown","cd7e60e0":"markdown","67e560be":"markdown","8f298666":"markdown","c218aaa9":"markdown","b6e49ff4":"markdown","df473826":"markdown"},"source":{"dc041a9a":"%%HTML\n<h1>Majdoubi's Guide to a beginner's first Kaggle Project (top 20%)<\/h1>\n<h3>Made by : Ahmed Amine MAJDOUBI<\/h3>","81a1da82":"%%HTML\n<h1>Importing Libraries and Datasets<\/h1>","9b84c4d0":"#importing librairies\n\nimport pandas as pd # data processing\nimport numpy as np # numeric computation\nimport matplotlib.pyplot as plt # plot visualization\n%matplotlib inline\nimport seaborn as sns # data visualisation\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport scipy.stats as st # statistics\npd.options.display.max_columns = None # show all columns\nimport missingno as msno # missing data visualizations and utilities\nimport warnings # ignore file warnings\nwarnings.filterwarnings('ignore')\n\n# Importing the train and test datasets in  pandas dataframe\n\ntrainData = pd.read_csv('..\/input\/train.csv')\ntestData = pd.read_csv('..\/input\/test.csv')\ntrainData.drop(columns = 'Id', inplace =True)\ny_train = trainData['SalePrice']\n","d3510da6":"%%HTML\n<h1>Data Description<\/h1>","f30fc277":"# shape of data\ntrainData.shape , testData.shape, y_train.shape","57a34ab1":"# A basic description of the dataset\ntrainData.describe()","2b3f2d34":"# display the first five rows of the train dataset.\ntrainData.head()","01de5475":"# display the last five rows of the train dataset.\ntrainData.tail()","f7ba6e96":"# Numeric and categorical features in the dataset\ntrainData.select_dtypes(include=[np.number]).columns, trainData.select_dtypes(include=[np.object]).columns","8cc00e34":"%%HTML\n<h1>Correlation Features<\/h1>","838f1dd2":"# Showing the numerical varibales with the highest correlation with 'SalePrice', sorted from highest to lowest\ncorrelation = trainData.select_dtypes(include=[np.number]).corr()\nprint(correlation['SalePrice'].sort_values(ascending = False))","457f37a7":"# Heatmap of correlation of numeric features\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(correlation,square = True,  vmax=0.8)","d3595b78":"# Zoomed HeatMap of the most Correlayed variables\nzoomedCorrelation = correlation.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea'], ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(zoomedCorrelation, square = True, linewidths=0.01, vmax=0.8, annot=True,cmap='viridis',\n            linecolor=\"black\", annot_kws = {'size':12})","a6689cc3":"# Pair plot\nsns.set()\ncols = ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']\nsns.pairplot(trainData[cols],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","06369c3e":"%%HTML\n<h1>Removing Outliers<\/h1>","28f39cf0":"plt.figure(figsize=(7,5))\nplt.scatter(x = trainData.TotalBsmtSF,y = trainData.SalePrice)\nplt.title('TotalBsmtSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = trainData['1stFlrSF'],y = trainData.SalePrice)\nplt.title('1stFlrSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = trainData.GrLivArea,y = trainData.SalePrice)\nplt.title('GrLivArea', size = 15)","fe1e6b75":"# Removing the outliers\ntrainData.drop(trainData[trainData['TotalBsmtSF'] > 5000].index,inplace = True)\ntrainData.drop(trainData[trainData['1stFlrSF'] > 4000].index,inplace = True)\ntrainData.drop(trainData[(trainData['GrLivArea'] > 4000) & (trainData['SalePrice']<300000)].index,inplace = True)\ntrainData.shape","8bc3765f":"%%HTML\n<h1>Imputation of missing values<\/h1>","0e88a61f":"# Visualising missing values of numeric features for sample of 200\nmsno.matrix(trainData.select_dtypes(include=[np.number]).sample(200))","277600c9":"# Visualising percentage of missing values of the top 10 numeric variables\ntotal = trainData.select_dtypes(include=[np.number]).isnull().sum().sort_values(ascending=False)\npercent = (trainData.select_dtypes(include=[np.number]).isnull().sum()\/trainData.select_dtypes(include=[np.number]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)","54994fa2":"# Visualising missing values of categorical features for sample of 200\nmsno.matrix(trainData.select_dtypes(include=[np.object]).sample(200))","63e38501":"# Visualising percentage of missing values of the top 10 categorical variables\ntotal = trainData.select_dtypes(include=[np.object]).isnull().sum().sort_values(ascending=False)\npercent = (trainData.select_dtypes(include=[np.object]).isnull().sum()\/trainData.select_dtypes(include=[np.object]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)","0cb72312":"# Visualization of nullity by column\nmsno.bar(trainData.sample(1000))","07e0889e":"# Nullity correlation heatmap : how strongly the presence or absence of one variable affects the presence of another\nmsno.heatmap(trainData)\n\n# -1 : if one variable appears the other definitely does not\n# 0 : variables appearing or not appearing have no effect on one another \n# 1 : if one variable appears the other definitely also does","206d4720":"# Dendogram for variable completion, reveals trends deeper than the pairwise ones visible in the correlation heatmap\nmsno.dendrogram(trainData)","c6cdf1bb":"# Concatenate the training and test datasets into a single datafram\ndataFull = pd.concat([trainData,testData],ignore_index=True)\ndataFull.drop('Id',axis = 1,inplace = True)\ndataFull.shape","f5cadd4c":"# Sum of missing values by feature\nsumMissingValues = dataFull.isnull().sum()\nsumMissingValues[sumMissingValues>0].sort_values(ascending = False)","3b088f6f":"# Numeric features : replace with 0\nfor col in ['BsmtFullBath','BsmtHalfBath','BsmtUnfSF','TotalBsmtSF','GarageCars','BsmtFinSF2','BsmtFinSF1','GarageArea']:\n    dataFull[col].fillna(0,inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","22e64e25":"# Categorical features : replace with the mode (most frequently occured value)\nfor col in ['MSZoning','Functional','Utilities','KitchenQual','SaleType','Exterior2nd','Exterior1st','Electrical']:\n    dataFull[col].fillna(dataFull[col].mode()[0],inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","4cdd64d5":"# Impute features with more than five missing values\n\n# Categorical features : replace all with 'None'\nfor col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n    dataFull[col].fillna('None',inplace = True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","d1cac967":"dataFull['MasVnrArea'].fillna(dataFull['MasVnrArea'].mean(), inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","c1e8f6c9":"# Cut 'YearBuilt' into 10 parts\ndataFull['YearBuiltCut'] = pd.qcut(dataFull.YearBuilt,10)\n# Impute the missing values of 'GarageYrBlt' based on the median of 'YearBuilt' \ndataFull['GarageYrBlt']= dataFull.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\n# convert the values to integers\ndataFull['GarageYrBlt'] = dataFull['GarageYrBlt'].astype(int)\n# Drop 'YearBuiltCut' column\ndataFull.drop('YearBuiltCut',axis=1,inplace=True)\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","55224561":"# Cut 'LotArea' into 10 parts\ndataFull['LotAreaCut'] = pd.qcut(dataFull.LotArea,10)\n# Impute the missing values of 'LotFrontage' based on the median of 'LotArea' and 'Neighborhood'\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n# Drop 'LotAreaCut' column\ndataFull.drop('LotAreaCut',axis=1,inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","4bf11210":"%%HTML\n<h1>Correcting Features<\/h1>","0431d2d6":"dataFull.select_dtypes(include=[np.number]).columns","61240dfb":"# Converting numeric features to categorical features\nstrCols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\nfor i in strCols:\n    dataFull[i]=dataFull[i].astype(str)","c8d80512":"%%HTML\n<h1>Adding Features<\/h1>","0bb9a5e0":"dataFull.select_dtypes(include=[np.object]).columns","1da9811f":"dataFull[\"oExterQual\"] = dataFull.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oBsmtQual\"] = dataFull.BsmtQual.map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oBsmtExposure\"] = dataFull.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\ndataFull[\"oHeatingQC\"] = dataFull.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oKitchenQual\"] = dataFull.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oFireplaceQu\"] = dataFull.FireplaceQu.map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\ndataFull[\"oGarageFinish\"] = dataFull.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\ndataFull[\"oPavedDrive\"] = dataFull.PavedDrive.map({'N':1, 'P':2, 'Y':3})","5a9b3b0e":"dataFull.select_dtypes(include=[np.number]).columns","549ce9df":"dataFull['HouseSF'] = dataFull['1stFlrSF'] + dataFull['2ndFlrSF'] + dataFull['TotalBsmtSF']\ndataFull['PorchSF'] = dataFull['3SsnPorch'] + dataFull['EnclosedPorch'] + dataFull['OpenPorchSF'] + dataFull['ScreenPorch']\ndataFull['TotalSF'] = dataFull['HouseSF'] + dataFull['PorchSF'] + dataFull['GarageArea']","9823895d":"%%HTML\n<h1>Skewness and Kurtosis<\/h1>","2ad0d8d2":"# Estimate Skewness and Kurtosis of the data\ntrainData.skew(), trainData.kurt()","82fdf9c9":"# Plot the Skewness of the data\nsns.distplot(trainData.skew(),axlabel ='Skewness')","b18feed8":"# Plot the Kurtosis of the data\nsns.distplot(trainData.kurt(),axlabel ='Kurtosis')","87d836fa":"%%HTML\n<h1>Label Encoding<\/h1>","32c5e571":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder, Imputer\nfrom scipy.stats import skew\n\n# Label encoding class\nclass labenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        label = LabelEncoder()\n        X['YrSold']=label.fit_transform(X['YrSold'])\n        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n        X['MoSold']=label.fit_transform(X['MoSold'])\n        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n        return X\n    \n# Skewness transform class\nclass skewness(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        skewness = X.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= 1].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        return X\n\n# One hot encoding class\nclass onehotenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X = pd.get_dummies(X)\n        return X","4a3260eb":"# Creating a copy of the full dataset\ndataFullCopy = dataFull.copy()\n\n# Creating a new fata with aplied transformations using sklearn Pipeline\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('labenc',labenc()),('skewness',skewness()),('onehotenc',onehotenc())])\ndataPipeline = pipeline.fit_transform(dataFullCopy)\ndataFull.shape, dataPipeline.shape","f4f5152d":"dataPipeline.head()","34f7e0ad":"\nX_train = dataPipeline[:trainData.shape[0]]\ny_train = X_train['SalePrice']\nX_train.drop(columns = 'SalePrice', inplace=True)\nX_test = dataPipeline[trainData.shape[0]:]\nX_test.drop(columns = 'SalePrice', inplace=True)\nX_train.shape, y_train.shape, X_test.shape","bf645def":"%%HTML\n<h1>Transformation and Scaling<\/h1>","6bb4082d":"# SalesPrices plot with three different fitted distributions\nplt.figure(2); plt.title('Normal')\nsns.distplot(y_train, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y_train, kde=False, fit=st.lognorm)\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y_train, kde=False, fit=st.johnsonsu)","bae1c962":"# transforming 'SalePrice' into normal distribution\ny_train_transformed = np.log(y_train)\ny_train_transformed.skew(), y_train_transformed.kurt()","1a97fdd5":"# plotting 'SalePrice' before and after the transformation\nplt.figure(1); plt.title('Before transformation')\nsns.distplot(y_train)\nplt.figure(2); plt.title('After transformation')\nsns.distplot(y_train_transformed)","04a06ae6":"# Using Robust Scaler to transform X_train\nfrom sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\nX_train_scaled = robust_scaler.fit(X_train).transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)","46889c4a":"# Shape of final data we will be working on\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","1776fe9c":"%%HTML\n<h1>Feature Selection<\/h1>","db24d8c9":"# Display features by their importance (lasso regression coefficient)\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\nlasso.fit(X_train_scaled,y_train_transformed)\ny_pred_lasso = lasso.predict(X_test_scaled)\nlassoCoeff = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=dataPipeline.drop(columns = 'SalePrice').columns)\nlassoCoeff.sort_values(\"Feature Importance\",ascending=False)","f680749f":"# Plot features by importance (feature coefficient in the model)\nlassoCoeff[lassoCoeff[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(20,35))","e986476c":"%%HTML\n<h1>Principal Component Analysis<\/h1>","9e5fffd5":"from sklearn.decomposition import PCA\n# Concatenate the training and test datasets into a single datafram\ndataFull2 = np.concatenate([X_train_scaled,X_test_scaled])\n# Choose the number of principle components such that 95% of the variance is retained\npca = PCA(0.95)\ndataFull2 = pca.fit_transform(dataFull2)\nvarPCA = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n# Principal Component Analysis of data\nprint(varPCA)","03d674a5":"# Principal Component Analysis plot of the data\nplt.figure(figsize=(16,12))\nplt.bar(x=range(1,len(varPCA)+1), height = varPCA)\nplt.ylabel(\"Explained Variance (%)\", size = 15)\nplt.xlabel(\"Principle Components\", size = 15)\nplt.title(\"Principle Component Analysis Plot : Training Data\", size = 15)\nplt.show()","c48e72ac":"# Shape of final data we will be working on\nX_train_scaled = dataFull2[:trainData.shape[0]]\nX_test_scaled = dataFull2[trainData.shape[0]:]\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","740ada3a":"%%HTML\n<h1>Testing Different Models<\/h1>","315cedb7":"# importing the models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.svm import LinearSVR,SVR\n# creating the models\nmodels = [\n             LinearRegression(),\n             SVR(),\n             SGDRegressor(),\n             SGDRegressor(max_iter=1000, tol = 1e-3),\n             GradientBoostingRegressor(),\n             RandomForestRegressor(),\n             Lasso(),\n             Lasso(alpha=0.01,max_iter=10000),\n             Ridge(),\n             BayesianRidge(),\n             KernelRidge(),\n             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n             ElasticNet(),\n             ElasticNet(alpha = 0.001,max_iter=10000),    \n             ExtraTreesRegressor(),\n             ]\n\nnames = ['Linear regression','Support vector regression','Stochastic gradient descent','Stochastic gradient descent 2','Gradient boosting tree','Random forest','Lasso regression','Lasso regression 2','Ridge regression','Bayesian ridge regression','Kernel ridge regression','Kernel ridge regression 2','Elastic net regularization','Elastic net regularization 2','Extra trees regression']\n\n\n","3a4519b6":"# Define a root mean square error function\ndef rmse(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n    return rmse","6073d91c":"from sklearn.model_selection import KFold,cross_val_score\nwarnings.filterwarnings('ignore')\n\n# Perform 5-folds cross-calidation to evaluate the models \nfor model, name in zip(models, names):\n    # Root mean square error\n    score = rmse(model,X_train_scaled,y_train_transformed)\n    print(\"- {} : mean : {:.6f}, std : {:4f}\".format(name, score.mean(),score.std()))","0dc3eb38":"%%HTML\n<h1>Hyper-parameter Tuning<\/h1>","cef3127a":"from sklearn.model_selection import GridSearchCV\n\nclass gridSearch():\n    def __init__(self,model):\n        self.model = model\n    def grid_get(self,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5,scoring='neg_mean_squared_error')\n        grid_search.fit(X_train_scaled,y_train_transformed)\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n        print('\\nBest parameters : {}, best score : {}'.format(grid_search.best_params_,np.sqrt(-grid_search.best_score_)))","7e6ca70d":"gridSearch(KernelRidge()).grid_get(\n        {'alpha':[3.5,4,4.5,5,5.5,6,6.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[1,1.5,2,2.5,3,3.5]})","fa334ce9":"gridSearch(ElasticNet()).grid_get(\n        {'alpha':[0.006,0.0065,0.007,0.0075,0.008],'l1_ratio':[0.070,0.075,0.080,0.085,0.09,0.095],'max_iter':[10000]})","4a9821a0":"gridSearch(Ridge()).grid_get(\n        {'alpha':[10,20,25,30,35,40,45,50,55,57,60,65,70,75,80,100],'max_iter':[10000]})","74d55158":"gridSearch(SVR()).grid_get(\n        {'C':[13,15,17,19,21],'kernel':[\"rbf\"],\"gamma\":[0.0005,0.001,0.002,0.01],\"epsilon\":[0.01,0.02,0.03,0.1]})","f4a26f11":"gridSearch(Lasso()).grid_get(\n       {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'max_iter':[10000]})","db16fe71":"lasso = Lasso(alpha= 0.0006, max_iter= 10000)\nridge = Ridge(alpha=35, max_iter= 10000)\nsvr = SVR(C = 13, epsilon= 0.03, gamma = 0.001, kernel = 'rbf')\nker = KernelRidge(alpha=6.5 ,kernel='polynomial', degree=3 , coef0=2.5)\nela = ElasticNet(alpha=0.007,l1_ratio=0.07,max_iter=10000)\nbay = BayesianRidge()","10541171":"%%HTML\n<h1>Combining Models<\/h1>","3ab46d8b":"from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor","e3555c29":"# Creating the stacking function\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        kff = KFold(n_splits=5, random_state=42, shuffle=True)\n        self.kf = kff\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","ec770b5f":"# Impute the training dataset\nX_scaled_imputed = Imputer().fit_transform(X_train_scaled)\ny_log_imputed = Imputer().fit_transform(y_train_transformed.values.reshape(-1,1)).ravel()","53736a28":"X_scaled_imputed.shape,y_log_imputed.shape,X_test_scaled.shape","dceab55a":"# Calculating the score\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nscore = rmse(stack_model,X_scaled_imputed,y_log_imputed)\nprint(score.mean())","22228b3f":"# Combining the extracted features generated from stacking whith original features\nX_train_stack,X_test_stack = stack_model.get_oof(X_scaled_imputed,y_log_imputed,X_test_scaled)\nX_train_add = np.hstack((X_scaled_imputed,X_train_stack))\nX_test_add = np.hstack((X_test_scaled,X_test_stack))\nX_train_add.shape,X_test_add.shape","e385306a":"# Calculate the final score of the model\nscore = rmse(stack_model,X_train_add,y_log_imputed)\nprint(score.mean())","bbc01d75":"%%HTML\n<h1>Making Predictions<\/h1>","9199ea99":"# Fit the model to the dataset generated with stacking\nstack_model.fit(X_train_add,y_log_imputed)","08315b8d":"# Making prediction on the test set generated by stacking\npredicted_prices = np.exp(stack_model.predict(X_test_add))\n# Prepare the csv file\nmy_submission = pd.DataFrame({'Id': testData.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","4563ea77":"%%HTML\n<h2>Thanks for reading my notebook. If you liked my kernel please kindly UPVOTE for other people to see. If you have any remarks, please leave a comment bellow.<h2>","a205bc28":"Based on the previous correlation heatmap, 'GarageYrBlt' is highly correlated with 'YearBuilt', so let's replace the missing values by medians of 'YearBuilt'. To do that, we need to cut 'YearBuilt' into sections since it is a numeric variable","27d47a1c":"Since 'MasVnrArea' only have 23 missing values, we can replace them with the mean of the column","c1e84b74":"We observe that 'PoolQC', 'MiscFeature', 'Alley', 'Fence' and 'FireplaceQu' have a significant amount of missing values (at least more than half of observation)","2f6a66fd":"Now it's time to make predictions and store them in a csv file with corresponding Ids. after we make prediction we need to transform them to their original shape with exponential function","07399954":"Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well","172dc0ae":"Now that we have finished preparing our data, it's time to test different models to see which one performs the best.\nThe models we will be testing are : \n- Linear regression\n- Support vector regression\n- Stochastic gradient descent\n- Gradient boosting tree\n- Random forest\n- Lasso regression\n- Ridge regression\n- Elastic net regularization\n- Extra trees regression","fddfb147":"- We observe that 'SalePrice' increases almost quadratically with 'TotalBsmtSF', 'GrLivArea', '1stFlrSF'. So we conclude that the price of the houses increases quadratically with its surface area. We also observe that 'SalePrice' increases exponentially with 'OverallQual'.\n- We also observe from 'GrLivArea'-'1stFlSF' and '1stFlSF'-'TotalBsmSF' that all the points are above the identity fucntion line, which means that the ground living area has the biggest surface of all floors, and that the first floor area is generally bigger than the basement area.\n- We observe the same phenomenon for 'GarageYrBlt'-'YearBuilt'. which makes sense since we start building the garage after building the house, altough there are some exceptions in the data.","776e7ead":"Now we split the data to training and testing datasets","129b8d90":"First of all, let's start by replacing the missing values in both the training and the test set. So we will be combining both datasets into one dataset","021c48fd":"For this section we will use Pipelines which are a way to streamline a lot of the routine processes. It provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\nWe will create three classes : first for label encoding, second for skewness, and third for one hot label encoding.","32fc142e":"Surprisingly, the Random forest and Extra trees regression models are the ones who performed the worst, and the linear regression model performed actually pretty good relative to the other models.\nBy compiling the above code several times and observing the different scores each time, we can classify the models by accuracy :\n\n- 1st : Kernel ridge regression\n- 2nd : Elastic net regularization and Bayesian ridge regression\n- 3rd : Ridge regression and Linear regression\n- 4rth : Support vector regression\n- 5th : Gradient boosting tree\n- 6th : Stochastic gradient  and Lasso regression\n- 7th : Random forest and Extra trees regression\n\nI think we got a good score in Elastic net regularization, Lasso regression and Stochastic gradient descent because we chose some good parameters. We can see that their score above is very bad when not specifing parameter values. So if we really want to know to best model, we need to choose optimal parameters for all the models, and tha's what we will do in the next section.","d6650378":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance","765008ce":"3. Ridge regression","ae8d0fdf":"We conclude that :\n- 'TotalBsmtSF' and '1stFlrSF' are strongly correlated\n- 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n- 'GarageCars' and 'GarageArea' are strongly correlated\n- 'GarageYrBlt' and 'YearBuilt' are strongly correlated\n- 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n- 'OverallQual', 'GrLivArea' and 'TotRmsAbvGrd' are strongly correlated with 'SalePrice'","940e00fe":"We see that now we have 87 features instead of the 327 features that we had before using PCA.","115933a1":"Let's look at some correlation features between features.","71046630":"The last time I submitted, I was the 859th (top 20%) with a score of 0.11934","94926cda":"2. Elastic net regularization","fb280485":"4. Support vector regression","964258c8":"1. Kernel ridge regression","1269e7cd":"Based on the previous correlation heatmap, 'LotFrontage' is highly correlated with 'LotArea' and 'LotFrontage'. So let's use the same method to fill the missing values","979e6e96":"- We observe two white squares (2,2 and 3,3) in the heatmap indicating high correlation. The first group of highly correlated variables is 'TotalBsmtSF' and '1stFlrSF'. The second group is 'GarageYrBlt', 'GarageCars' and 'GarageArea'. This indicates the presence of multicollinearity.\n- The other four white squares (1,1) just indicate an obvious correlation between 'GarageYrBlt' and 'YearBuilt' and between 'TotRmsAbvGrd' and 'GrLivArea'\n- We also observe  from the heatmap and the previous correlation list that'GrLivArea', 'TotalBsmtSF', 'OverallQual', 'FullBath', 'TotRmsAbvGrd' and 'YearBuilt' are highly correlated with 'SalePrice'","35a96b65":"If we take a look at the numeric variables, we see that some of them obviously don't make a sense being numerical like year related features. Let's take a closer look at each one of them in the data description file and see which ones need to be converted to categorical type.","e53ff76b":"5. Lasso regression","2813125e":"Let's start by imputing features with less than five missing values","3a1f54e0":"The last score we obtain is 0.1074, which is quite good","05a1f919":"What's intersting here is that two of the variables that we have created 'HouseSF' and 'PorchSF' perform actually bad compared to their components. But when we sum all the surfaces as in 'TotalSF', which is just a combination of features that are significantly unimportant in this model, we suddently obtain the most important feature in the dataset.","a42b0b5e":"We observe that 'LotFrontage', 'GarageYrBlt' and 'MasVnrArea' are the only one who have missing values","5b03021a":"We can see now that the number of features increases from 88 to 328","0e3e6142":"First let's start by looking at the data and getting a general idea about it","faa97023":"In this notebook, we will go through the basic steps of making predictions based on a given dataset. The steps that I followed in this notebook are as follows :\n- Importing Libraries and Datasets\n- Data Description\n- Finding Correlation Features\n- Removing Outliers\n- Imputation of missing values\n- Correcting Features\n- Adding Features\n- Skewness and Kurtosis\n- Label Encoding\n- Transformation and Scaling\n- Feature Selection\n- Principal Component Analysis\n- Testing Different Models\n- Hyper-parameter Tuning\n- Combining Models\n- Making Predictions","0c9857d9":"For choosing the most optimal hyper-parameters, we will perform gird search. the class GridSearchCV exhaustively considers all parameter combinations and generates candidates from a grid of parameter values specified with the param_grid parameter.\nSince we will use the same procedure for all models, we will start by creating a function which takes specified parameter values as entry.","becd3f60":"Since only two outliers were dropped, it means that the three features shared the same outlier ","4c01711d":"First, we will map some categorical variable that represent some sort of rating to an integer score","a32f0129":"We obtain a score of 0.113, which is slightly better than the average score of the other models","96f00b60":"Next, we will add up some numeric features with each other to create new features that make sense","cfca196a":"There isn't much kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical","56357bab":"In order to further improve our model accuracy. We will use an ensemble method. I chose to use stacking. Stacked generalization is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm.","cd7e60e0":"We see that the models perform almost the same way with a score of 0.116. Let's define these models with the their respective best hyper-parameters.","67e560be":"Cluster leaves which linked together at a distance of zero fully predict one another's presence : one variable might always be empty when another is filled, or they might always both be filled or both empty.","8f298666":"We will use lasso regression (l1 regularization method). Lasso shrinks the less important feature\u2019s coefficient to zero thus, removing some feature altogether. We can also use it to find the most important features in our dataset.","c218aaa9":"From the previous pair plots, we can see that there are outliers for 'TotalBsmtSF', '1stFlrSF' and 'GrLivArea'. Let's use the scatterplot to observe these outliers more precisely","b6e49ff4":"Let's look at the missing valeus in our data. We will be using msno library. Msno provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset","df473826":"The only missing values that are left are within SalePrice, which is exactly the number of lignes in the test data (the values that we need to predict)"}}