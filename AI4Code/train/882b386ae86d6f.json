{"cell_type":{"37932609":"code","35f87c60":"code","d9dd8eb6":"code","48d5050c":"code","a561aff3":"code","b10a325e":"code","8c3d8fdc":"code","eabf5bc4":"code","0795e4e0":"code","856e682f":"code","1b257350":"code","955a69f7":"code","0486bbbd":"code","2c792d2f":"code","7a4be978":"code","120b4dc4":"code","0211a9f0":"code","179dcff0":"code","3892d140":"code","47f95822":"code","950a606e":"code","665bb5a1":"code","cb31cdf5":"code","c094d8b9":"code","3e08fc12":"code","fb33190c":"code","c0e7eb98":"code","fdcb133f":"code","ef2bec53":"code","527d5550":"code","cc510621":"code","65aa331d":"code","dcf84f24":"markdown","fa50721a":"markdown","ab32660f":"markdown","c35f3d66":"markdown"},"source":{"37932609":"# load libraries\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport xgboost as xgb\n\nsns.set_palette('deep')\n\n%matplotlib inline","35f87c60":"# read the data\npath = '\/kaggle\/input\/tabular-playground-series-apr-2021'\ntrain_data = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_data = pd.read_csv(os.path.join(path, 'test.csv'))\ntrain_data.head()","d9dd8eb6":"train_data.info()","48d5050c":"test_data.head()","a561aff3":"# distribution of target variable\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Survived', data=train_data, ec='k')\nplt.show()","b10a325e":"# checking the survival rates\nwomen = train_data.loc[train_data['Sex'] == 'female']['Survived']\nsurvival_rate_women = sum(women) \/ len(women)\nprint('Survival rate of women: {:.2f}'.format(survival_rate_women*100))","8c3d8fdc":"men = train_data.loc[train_data['Sex'] == 'male']['Survived']\nsurvival_rate_men = sum(men) \/ len(men)\nprint('Survival rate of men: {:.2f}'.format(survival_rate_men*100))","eabf5bc4":"# checking the distribution of the features\nplt.figure(figsize=(8, 6))\nsns.histplot(data=np.log(train_data['Fare']), bins=30)\nplt.show()","0795e4e0":"plt.figure(figsize=(8, 6))\nsns.histplot(data=train_data['Age'], bins=30)\nplt.show()","856e682f":"plt.figure(figsize=(8, 6))\nsns.countplot(x='Pclass', data=train_data, ec='k')\nplt.show()","1b257350":"plt.figure(figsize=(8, 6))\nsns.countplot(x='Sex', data=train_data, hue='Survived', ec='k')\nplt.show()","955a69f7":"# checking null values\nfor data in [train_data, test_data]:\n    print(data.isnull().sum())\n    print('-'*50)","0486bbbd":"# fill null values in the age \ndef fillnan_age(df):\n    age_avg = df['Age'].mean()\n    age_std = df['Age'].std()\n    age_null_count = df['Age'].isnull().sum()\n    random_age = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    df['Age'][df['Age'].isnull()] = random_age\n    return df","2c792d2f":"data = [train_data, test_data]\nfor df in data:\n    # fill the missing values\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n    df = fillnan_age(df)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    \n    # count all family members\n    df['FamilyCount'] = df['SibSp'] + df['Parch'] + 1\n    \n    # ticket prefix\n    df['TicketPrefix'] = df.Ticket.map(\n        lambda x: str(x).split()[0] if len(str(x).split()) > 1 else 'N\/A'\n    )\n    \n    # last name\n    df['LastName'] = df['Name'].map(lambda x: str(x).split(',')[0])\n    duplicated_lastname = df['LastName'].value_counts().to_dict()\n    df['NumLastName'] = df['LastName'].map(duplicated_lastname)\n\n    # check if the passenger had a cabin\n    df['HasCabin'] = df['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n\n    # new column if the passenger is alone\n    df['IsAlone'] = (df['FamilyCount'] == 1).astype(int)\n    \n    # new column to calculate the logarithm of Fare distributed in 4 bins\n    df['LogFareGroup'] = pd.cut(np.log(df['Fare']+0.0001), 4, labels=range(4))\n    \n    # Distribute the age in 5 bins\n    df['AgeGroup'] = pd.cut(df['Age'], 5, labels=range(5))\n    \n#     df.drop(['Cabin', 'Ticket'] , axis=1, inplace=True)\n    print(df.isnull().sum())\n    print('-'*50)","7a4be978":"plt.figure(figsize=(8, 6))\nsns.countplot(x='IsAlone', data=train_data, ec='k')\nplt.show()","120b4dc4":"plt.figure(figsize=(8, 6))\nsns.countplot(x='Embarked', data=train_data, ec='k')\nplt.show()","0211a9f0":"plt.figure(figsize=(8, 6))\nsns.countplot(x='FamilyCount', data=train_data, ec='k')\nplt.show()","179dcff0":"plt.figure(figsize=(8, 6))\nsns.countplot(x='HasCabin', data=train_data, ec='k')\nplt.show()","3892d140":"plt.figure(figsize=(8, 6))\nsns.countplot(x='LogFareGroup', data=train_data, hue='Survived', ec='k')\nplt.show()","47f95822":"plt.figure(figsize=(8, 6))\nsns.countplot(x='AgeGroup', data=train_data, hue='Survived', ec='k')\nplt.show()","950a606e":"# One hot encode the categorical features\ndf = pd.concat([train_data, test_data], axis=0)\ndf = pd.get_dummies(df, columns=['Pclass', 'Embarked', 'LogFareGroup', 'AgeGroup'])\nlabel_encode_cols = ['Sex', 'TicketPrefix']\nfor col in label_encode_cols:\n    le = preprocessing.LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\ntrain_data = df.iloc[:len(train_data), :]\ntest_data = df.iloc[len(train_data):, :]\ntest_data.drop('Survived', axis=1, inplace=True)\ndel df\ngc.collect()\ntrain_data.head()","665bb5a1":"features = [col for col in train_data.columns if col not in \n            ['PassengerId', 'Name', 'Survived', 'SibSp', 'Parch', 'Cabin', 'Ticket', 'LastName']]\nfeatures","cb31cdf5":"# feature correlations with target\ncorrelations = pd.DataFrame(train_data[features + ['Survived']].corr()['Survived'])\ncorrelations","c094d8b9":"pruned_features = correlations[(abs(correlations.Survived) > 0.08)].index.tolist()\npruned_features","3e08fc12":"# class to help training different models\nclass TrainHelper:\n    def __init__(self, clf, seed=23, params=None, scale_features=None):\n        self.normalized_features = False\n        if params is not None:\n            params['random_state'] = seed\n            self.clf = clf(**params)\n        else:\n            self.clf = clf(random_state=seed)\n        self.scale_features = scale_features\n        if type(self.clf).__name__ in ['RidgeClassifier', 'LogisticRegression']:\n            if scale_features is not None:\n                self.normalized_features = True\n            else:\n                raise Exception('Cannot do feature scaling')\n\n    def fit(self, X_train, y_train):\n        if self.normalized_features:\n            preprocessor = self.normalize()\n            X_train = preprocessor.fit_transform(X_train)\n\n        return self.clf.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        if self.normalized_features:\n            preprocessor = self.normalize()\n            X_test = preprocessor.fit_transform(X_test)\n        return self.clf.predict(X_test)\n\n    def normalize(self):\n        ct = ColumnTransformer(\n                [('scale', preprocessing.StandardScaler(), self.scale_features)],\n                remainder='passthrough',\n                n_jobs=-1\n            )\n        return ct\n\n    def predict_proba(self, X_test):\n        if hasattr(self.clf, 'predict_proba'):\n            if self.normalized_features:\n                preprocessor = self.normalize()\n                X_test = preprocessor.fit_transform(X_test)\n            return self.clf.predict_proba(X_test)[:, 1]\n        else:\n            return 'The classifier has no method predict_proba'","fb33190c":"# applying k-fold cross-validation\ndef cross_validate(X, y, X_test, clf, n_folds, seed=23, threshold=0.7):\n    print(f'Cross-validating for {type(clf.clf).__name__}')\n    kf = model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    train_oof = np.zeros((len(X), ))\n    test_preds = 0\n    for f_, (t_, v_) in enumerate(kf.split(X=X, y=y)):\n        X_train, y_train = X.loc[t_], y[t_]\n        X_valid, y_valid = X.loc[v_], y[v_]\n\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_valid)\n        y_pred_proba = clf.predict_proba(X_valid)\n        accuracy = metrics.accuracy_score(y_valid, y_pred)\n        if not isinstance(y_pred_proba, str):\n            roc_score = metrics.roc_auc_score(y_valid, y_pred_proba)\n        else:\n            roc_score = 0.0\n        train_oof[v_] = y_pred\n        test_preds += clf.predict(X_test) \/ n_folds\n        print(f'Fold: {f_+1}, accuracy: {accuracy:.6f}, roc-auc score: {roc_score:.6f}')\n    test_preds = (test_preds > threshold).astype(int)\n    print(f'Overall training accuracy: {metrics.accuracy_score(train_oof, y):.6f}')\n    print('-'*60)\n    return train_oof, test_preds","c0e7eb98":"train_data[features]","fdcb133f":"train_data = train_data.sample(frac=1).reset_index(drop=True)\npruned_features = [feature for feature in pruned_features if feature != 'Survived']\nX = train_data[features]\nX_test = test_data[features]\ny = train_data['Survived'].values\n# print(y)\nNUM_FOLDS = 10\nSEED = 2020\nscale_features = ['Age', 'Fare', 'FamilyCount', 'TicketPrefix', 'NumLastName']\n\n# create models\nlgbm_params = dict(\n    n_jobs=-1,\n    n_estimators=1000,\n    learning_rate=0.02,\n    num_leaves=39,\n    colsample_bytree=0.6993443635848076,\n    subsample=0.7146065596315723,\n    max_depth=28,\n    reg_alpha=13.0124692806962,\n    reg_lambda=17.429087848443793,\n    cat_smooth=8.61671087256764,\n    min_split_gain=0.0222415,\n    min_child_weight=39.3259775,\n    silent=-1,\n    verbose=-1,\n)\nlgbm = TrainHelper(clf=lgb.LGBMClassifier, params=lgbm_params)\n\ncb_params = {'colsample_bylevel': 0.06780062117211266, 'depth': 13, \n             'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', \n             'subsample': 0.10286610214134947, 'custom_loss':['Accuracy'],\n             'logging_level':'Silent'}\ncb = TrainHelper(clf=CatBoostClassifier, params=cb_params)\n\nridge_params = {'alpha': 0.01, 'fit_intercept': True}\nridge = TrainHelper(clf=linear_model.RidgeClassifier, params=ridge_params,\n                    scale_features=scale_features)\n\n# cross-validate for each model\nlgbm_train_oof, lgbm_test_preds = cross_validate(X, y, X_test, lgbm, n_folds=NUM_FOLDS, \n                                                 seed=SEED, threshold=0.75)\ncb_train_oof, cb_test_preds = cross_validate(X, y, X_test, cb, n_folds=NUM_FOLDS, \n                                             seed=SEED, threshold=0.75)\nridge_train_oof, ridge_test_preds = cross_validate(X, y, X_test, ridge, n_folds=NUM_FOLDS, \n                                                   seed=SEED, threshold=0.75)","ef2bec53":"# create dataframe for base level predictions\nbase_predictions_df = pd.DataFrame(\n    {\n        'LightGBM': lgbm_train_oof,\n        'CatBoost': cb_train_oof,\n        'RidgeClf': ridge_train_oof\n    }\n)\nbase_predictions_df = base_predictions_df.astype(int)\nbase_predictions_df","527d5550":"plt.figure(figsize=(8, 6))\nsns.heatmap(base_predictions_df.corr(), square=True, \n            cmap=plt.cm.RdBu, annot=True, alpha=0.6)\nplt.show()","cc510621":"X_train_2 = base_predictions_df.values\nX_test_2 = np.concatenate((lgbm_test_preds.reshape(-1, 1),\n                           cb_test_preds.reshape(-1, 1),\n                           ridge_test_preds.reshape(-1, 1)), axis=1)\nxg = xgb.XGBClassifier(learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\nxg.fit(X_train_2, y)\npredictions = xg.predict(X_test_2)\npredictions = predictions.astype(int)\npredictions[:5]","65aa331d":"output = pd.DataFrame({'PassengerId': test_data['PassengerId'], \n                       'Survived': predictions})\n\noutput.to_csv('stacked_submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","dcf84f24":"## Visualizing new features","fa50721a":"> NOTE: Some ideas for feature engineering have been taken from the brilliant notebooks by [Anisotropic](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) and [Sina](https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier).","ab32660f":"## Data Preprocessing and Feature Engineering","c35f3d66":"## Visualizing Data"}}