{"cell_type":{"701a435f":"code","ac9fca38":"code","30d4c78a":"code","88c2c908":"code","d992315d":"code","51cc6437":"code","1c02d6ff":"code","e882c0e1":"code","26f45d40":"code","b69284fe":"code","886c3440":"code","ca2272c3":"code","52f0b452":"code","aaad4bef":"code","eb2f67d6":"code","e405e3dc":"code","72923303":"code","cf141c70":"code","3844a424":"code","8c75d831":"code","22f07b06":"code","f8f23ed6":"code","605aa379":"code","cf9c495d":"code","d976109c":"code","0517284f":"code","5125042f":"code","59c757ee":"code","30f90763":"code","8f5f8699":"code","0557a5c6":"code","47f4da66":"code","71dcb271":"code","769f4f01":"code","46bc901e":"code","545b12fc":"code","5d29a0ee":"code","7a471708":"code","a675d48a":"code","19f62095":"code","53f3237e":"code","6f5763bc":"code","cd3d460c":"code","4a348e2e":"code","508939aa":"code","bb716932":"code","7335c3fa":"code","9b606c0d":"code","fa6a49dd":"code","90107d62":"code","740bc616":"code","af5c6f0a":"code","73d85581":"code","8d3fb6bd":"code","d28db539":"code","6307562d":"code","c81921b4":"code","558bf0e2":"code","5777d254":"code","35b190f7":"code","baa39458":"code","34f6dfb6":"code","be16c8c8":"code","d57eb63e":"code","81aec1df":"code","87f2763c":"code","6cd12eb7":"code","8dd524d7":"code","3867dd31":"code","b2493db8":"code","8d2e4f4a":"code","931df19e":"code","f77b8355":"code","2578a214":"code","dab14543":"code","ca714f0b":"code","1ef6e129":"code","00da4825":"code","4d0af767":"code","f34fff18":"code","d4d707bc":"code","dd4b7c9c":"code","bb7eda04":"code","b4bf5ce5":"code","f3af5493":"code","5b7528bb":"code","86b65c2a":"code","a948ba34":"code","c1cdfd9b":"code","2476dabd":"code","8b2c0ce7":"code","e2124aa7":"code","a335ea9e":"code","96fb3aa0":"code","b4a5ef64":"code","26a8d928":"code","ee3d7c27":"code","f02b854f":"code","7af76d15":"code","21f0a95d":"code","2ac8974e":"code","e892b878":"code","2ef8e055":"code","4c65ac03":"code","c98eb836":"code","74de5c49":"code","abb97515":"markdown","5e433fe4":"markdown","6d15d468":"markdown","a7cc691c":"markdown","886898e6":"markdown","a1fe0987":"markdown","5cc8871c":"markdown","ba4116d3":"markdown","21f40fcf":"markdown","e5bfaefd":"markdown","0895309a":"markdown","bdacbfca":"markdown","68b3988e":"markdown","e37f8ca4":"markdown","4c0fa990":"markdown","aec2f77a":"markdown","502738b8":"markdown","c6c50227":"markdown","3af8bbb6":"markdown","0fb2e98a":"markdown","70c6e65a":"markdown","3b34e5b9":"markdown","27e1d586":"markdown","687d642a":"markdown","935ddcfc":"markdown","eac8b78f":"markdown","00465e3a":"markdown","9662a8bf":"markdown","47c6b9b1":"markdown","727030ad":"markdown","d02f3619":"markdown","1215e858":"markdown","2453f1e6":"markdown","6415be37":"markdown","9b6d1063":"markdown","bc687ebc":"markdown","7ee0ad40":"markdown","d647ffef":"markdown","8fd9fd48":"markdown","637b6a83":"markdown","d7466583":"markdown"},"source":{"701a435f":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline ","ac9fca38":"def read_data():\n    print(f'Read data')\n    train_df = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\n    test_df = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\n    train_labels_df = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\n    specs_df = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\n    sample_submission_df = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')\n    print(f\"train shape: {train_df.shape}\")\n    print(f\"test shape: {test_df.shape}\")\n    print(f\"train labels shape: {train_labels_df.shape}\")\n    print(f\"specs shape: {specs_df.shape}\")\n    print(f\"sample submission shape: {sample_submission_df.shape}\")\n    return train_df, test_df, train_labels_df, specs_df, sample_submission_df","30d4c78a":"train_df, test_df, train_labels_df, specs_df, sample_submission_df = read_data()","88c2c908":"train_df.head()","d992315d":"test_df.head()","51cc6437":"train_labels_df.head()","1c02d6ff":"pd.set_option('max_colwidth', 150)\nspecs_df.head()","e882c0e1":"sample_submission_df.head()","26f45d40":"print(f\"train installation id: {train_df.installation_id.nunique()}\")\nprint(f\"test installation id: {test_df.installation_id.nunique()}\")\nprint(f\"test & submission installation ids identical: {set(test_df.installation_id.unique()) == set(sample_submission_df.installation_id.unique())}\")","b69284fe":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","886c3440":"missing_data(train_df)","ca2272c3":"missing_data(test_df)","52f0b452":"missing_data(train_labels_df)","aaad4bef":"missing_data(specs_df)","eb2f67d6":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","e405e3dc":"unique_values(train_df)","72923303":"unique_values(test_df)","cf141c70":"unique_values(train_labels_df)","3844a424":"unique_values(specs_df)","8c75d831":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","22f07b06":"most_frequent_values(train_df)","f8f23ed6":"most_frequent_values(test_df)","605aa379":"most_frequent_values(train_labels_df)","cf9c495d":"most_frequent_values(specs_df)","d976109c":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    ","0517284f":"plot_count('title', 'title (first most frequent 20 values - train)', train_df, size=4)","5125042f":"plot_count('title', 'title (first most frequent 20 values - test)', test_df, size=4)","59c757ee":"print(f\"Title values (train): {train_df.title.nunique()}\")\nprint(f\"Title values (test): {test_df.title.nunique()}\")","30f90763":"plot_count('type', 'type - train', train_df, size=2)","8f5f8699":"plot_count('type', 'type - test', test_df, size=2)","0557a5c6":"plot_count('world', 'world - train', train_df, size=2)","47f4da66":"plot_count('world', 'world - test', test_df, size=2)","71dcb271":"plot_count('event_code', 'event_code - test', train_df, size=4)","769f4f01":"plot_count('event_code', 'event_code - test', test_df, size=4)","46bc901e":"for column in train_labels_df.columns.values:\n    print(f\"[train_labels] Unique values of {column} : {train_labels_df[column].nunique()}\")","545b12fc":"plot_count('title', 'title - train_labels', train_labels_df, size=3)","5d29a0ee":"plot_count('accuracy', 'accuracy - train_labels', train_labels_df, size=4)","7a471708":"plot_count('accuracy_group', 'accuracy_group - train_labels', train_labels_df, size=2)","a675d48a":"plot_count('num_correct', 'num_correct - train_labels', train_labels_df, size=2)","19f62095":"plot_count('num_incorrect', 'num_incorrect - train_labels', train_labels_df, size=4)","53f3237e":"for column in specs_df.columns.values:\n    print(f\"[specs] Unique values of `{column}`: {specs_df[column].nunique()}\")","6f5763bc":"sample_train_df = train_df.sample(100000)","cd3d460c":"sample_train_df.head()","4a348e2e":"sample_train_df.iloc[0].event_data","508939aa":"sample_train_df.iloc[1].event_data","bb716932":"%%time\nextracted_event_data = pd.io.json.json_normalize(sample_train_df.event_data.apply(json.loads))","7335c3fa":"print(f\"Extracted data shape: {extracted_event_data.shape}\")","9b606c0d":"extracted_event_data.head(10)","fa6a49dd":"missing_data(extracted_event_data)","90107d62":"def existing_data(data):\n    total = data.isnull().count() - data.isnull().sum()\n    percent = 100 - (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    tt = pd.DataFrame(tt.reset_index())\n    return(tt.sort_values(['Total'], ascending=False))","740bc616":"stat_event_data = existing_data(extracted_event_data)","af5c6f0a":"plt.figure(figsize=(10, 10))\nsns.set(style='whitegrid')\nax = sns.barplot(x='Percent', y='index', data=stat_event_data.head(40), color='blue')\nplt.title('Most frequent features in event data')\nplt.ylabel('Features')","73d85581":"stat_event_data[['index', 'Percent']].head(20)","8d3fb6bd":"specs_df.args[0]","d28db539":"specs_args_extracted = pd.DataFrame()\nfor i in range(0, specs_df.shape[0]): \n    for arg_item in json.loads(specs_df.args[i]) :\n        new_df = pd.DataFrame({'event_id': specs_df['event_id'][i],\\\n                               'info':specs_df['info'][i],\\\n                               'args_name': arg_item['name'],\\\n                               'args_type': arg_item['type'],\\\n                               'args_info': arg_item['info']}, index=[i])\n        specs_args_extracted = specs_args_extracted.append(new_df)","6307562d":"print(f\"Extracted args from specs: {specs_args_extracted.shape}\")","c81921b4":"specs_args_extracted.head(5)","558bf0e2":"tmp = specs_args_extracted.groupby(['event_id'])['info'].count()\ndf = pd.DataFrame({'event_id':tmp.index, 'count': tmp.values})\nplt.figure(figsize=(6,4))\nsns.set(style='whitegrid')\nax = sns.distplot(df['count'],kde=True,hist=False, bins=40)\nplt.title('Distribution of number of arguments per event_id')\nplt.xlabel('Number of arguments'); plt.ylabel('Density'); plt.show()","5777d254":"plot_count('args_name', 'args_name (first 20 most frequent values) - specs', specs_args_extracted, size=4)","35b190f7":"plot_count('args_type', 'args_type - specs', specs_args_extracted, size=3)","baa39458":"plot_count('args_info', 'args_info (first 20 most frequent values) - specs', specs_args_extracted, size=4)","34f6dfb6":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    df['dayofyear'] = df['timestamp'].dt.dayofyear\n    df['quarter'] = df['timestamp'].dt.quarter\n    df['is_month_start'] = df['timestamp'].dt.is_month_start\n    return df","be16c8c8":"train_df = extract_time_features(train_df)","d57eb63e":"test_df = extract_time_features(test_df)","81aec1df":"train_df.head()","87f2763c":"test_df.head()","6cd12eb7":"plot_count('year', 'year - train', train_df, size=1)","8dd524d7":"plot_count('month', 'month - train', train_df, size=1)","3867dd31":"plot_count('hour', 'hour -  train', train_df, size=4)","b2493db8":"plot_count('dayofweek', 'dayofweek - train', train_df, size=2)","8d2e4f4a":"plot_count('weekofyear', 'weekofyear - train', train_df, size=2)","931df19e":"plot_count('is_month_start', 'is_month_start - train', train_df, size=1)","f77b8355":"plot_count('year', 'year - test', test_df, size=1)","2578a214":"plot_count('month', 'month - test', test_df, size=1)","dab14543":"plot_count('hour', 'hour -  test', test_df, size=4)","ca714f0b":"plot_count('dayofweek', 'dayofweek - test', test_df, size=2)","1ef6e129":"plot_count('weekofyear', 'weekofyear - test', test_df, size=2)","00da4825":"plot_count('is_month_start', 'is_month_start - test', test_df, size=1)","4d0af767":"numerical_columns = ['game_time', 'month', 'dayofweek', 'hour']\ncategorical_columns = ['type', 'world']\n\ncomp_train_df = pd.DataFrame({'installation_id': train_df['installation_id'].unique()})\ncomp_train_df.set_index('installation_id', inplace = True)","f34fff18":"def get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'min', 'max', 'std', 'skew']})\n    df[column].fillna(df[column].mean(), inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_min', f'{column}_max', f'{column}_std', f'{column}_skew']\n    return df","d4d707bc":"for i in numerical_columns:\n    comp_train_df = comp_train_df.merge(get_numeric_columns(train_df, i), left_index = True, right_index = True)","dd4b7c9c":"print(f\"comp_train shape: {comp_train_df.shape}\")","bb7eda04":"comp_train_df.head()","b4bf5ce5":"# get the mode of the title\nlabels_map = dict(train_labels_df.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n# merge target\nlabels = train_labels_df[['installation_id', 'title', 'accuracy_group']]\n# replace title with the mode\nlabels['title'] = labels['title'].map(labels_map)\n# join train with labels\ncomp_train_df = labels.merge(comp_train_df, on = 'installation_id', how = 'left')\nprint('We have {} training rows'.format(comp_train_df.shape[0]))","f3af5493":"comp_train_df.head()","5b7528bb":"print(f\"comp_train_df shape: {comp_train_df.shape}\")\nfor feature in comp_train_df.columns.values[3:20]:\n    print(f\"{feature} unique values: {comp_train_df[feature].nunique()}\")","86b65c2a":"plot_count('title', 'title - compound train', comp_train_df)","a948ba34":"plot_count('accuracy_group', 'accuracy_group - compound train', comp_train_df, size=2)","c1cdfd9b":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of log(`game time mean`) values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(np.log(red_comp_train_df['game_time_mean']), kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","2476dabd":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of log(`game time std`) values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(np.log(red_comp_train_df['game_time_std']), kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","8b2c0ce7":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `game time skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['game_time_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","e2124aa7":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour mean` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_mean'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","a335ea9e":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour std` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_std'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","96fb3aa0":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","b4a5ef64":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month mean` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_mean'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","26a8d928":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month std` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_std'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","ee3d7c27":"plt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()","f02b854f":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of log(`game time mean`) values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(np.log(red_comp_train_df['game_time_mean']), kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","7af76d15":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of log(`game time std`) values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(np.log(red_comp_train_df['game_time_std']), kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","21f0a95d":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `game time skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['game_time_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","2ac8974e":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour mean` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_mean'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","e892b878":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour std` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_std'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","2ef8e055":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","4c65ac03":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month mean` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_mean'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","c98eb836":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month std` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_std'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","74de5c49":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","abb97515":"### Specs","5e433fe4":"Let's look to the first 40 values, ordered by percent of existing data (descending).","6d15d468":"### Train_labels","a7cc691c":"There are no missing data in the datasets.","886898e6":"### Specs","a1fe0987":"We inspect now the date\/time type data.","5cc8871c":"### Train labels","ba4116d3":"We apply the function to extract time features.","21f40fcf":"# <a id=\"2\">Prepare the data analyisis<\/a>  \n\nWe load the packages needed for data processing and visualization and we read the data.  ","e5bfaefd":"### Specs","0895309a":"## <a id=\"34\">Values distribution<\/a>  ","bdacbfca":"Let's see the distribution of the number of arguments for each `event_id`.","68b3988e":"### Train labels","e37f8ca4":"Let's look to some of the `event_data` in this sample.","4c0fa990":"We use **json** package to normalize the json; we will create one column for each key; the value in the column will be the value associated to the key in the json. The extracted data columns will be quite sparse.","aec2f77a":"## <a id=\"36\">Extract features from specs\/args<\/a>  \n\nLet's try to extract data from `args` column in `specs_df` similarly we did for `event_data`.","502738b8":"### Test","c6c50227":"### Train","3af8bbb6":"# <a id=\"3\">Data exploration<\/a>  ","0fb2e98a":"We define a function to show the number and percent of each category in the current selected feature.","70c6e65a":"<h1>2019 Data Science Bowl EDA<\/h1>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n    -<a href='#21'>Load the packages<\/a>  \n    -<a href='#22'>Load the data<\/a>  \n- <a href='#3'>Data exploration<\/a>  \n    -<a href='#30'>Glimpse the data<\/a>  \n    -<a href='#31'>Missing data<\/a>  \n    -<a href='#32'>Unique values<\/a>  \n    -<a href='#33'>Most frequent values<\/a>      \n    -<a href='#34'>Values distribution<\/a>   \n    -<a href='#35'>Extract features from train\/event_data<\/a>  \n    -<a href='#36'>Extract features from specs\/args<\/a>      \n    -<a href='#37'>Merged data distribution<\/a>  \n- <a href='#4'>Next step<\/a>  \n    ","3b34e5b9":"We have 17K different installation_id in train and 1K in test sets (these are similar with the ones in sample_submission).","27e1d586":"There is a variable number of arguments for each `event_id`.","687d642a":"## <a id=\"37\">Merged data distribution<\/a>  \n\nLet's merge train and train_labels.","935ddcfc":"Here we define the numerical columns and the categorical columns. We will use these to calculate the aggregated functions for the merge.","eac8b78f":"## <a id=\"21\">Load the packages<\/a>  ","00465e3a":"## <a id=\"31\">Missing values<\/a>  \n\nWe define a function to calculate the missing values and also show the type of each column.","9662a8bf":"## <a id=\"32\">Most frequent values<\/a>  \n\nWe define a function for most frequent values.","47c6b9b1":"### Extract time features\n\nWe define a function to extract time features. We will apply this function for both train and test datasets.","727030ad":"We modify the `missing_data` function to order the most frequent encountered event data features (newly created function `existing_data`).","d02f3619":"## <a id=\"35\">Extract features from train\/event_data<\/a>\n\nWe will parse a subset of train_df to extract features from event_data. We only extract data from 100K random sampled rows. This should be enough to get a good sample of the content.","1215e858":"## <a id=\"32\">Unique values<\/a>  \n\nWe define a function to show unique values.","2453f1e6":"# <a id=\"1\">Introduction<\/a>  \n\nThis Kernel objective is to explore the dataset for 2019 Data Science Bowl EDA.   ","6415be37":"## <a id=\"22\">Load the data<\/a>  \n\nWe define a function to read all the data and report the shape of datasets.  \n","9b6d1063":"Let's check the statistics of the missing values in these columns.","bc687ebc":"Then, we calculate the compacted form of train, by merging the aggregated numerical features from train with the dataset with unique `installation_id`.","7ee0ad40":"Each row contains a list of key-values pairs (a dictionary), with the keys: `name`, `type` & `info`.\nWe will parse this structure and generate new rows for each spec.","d647ffef":"### Train","8fd9fd48":"## <a id=\"30\">Glimpse the data<\/a> \n\nWe will inspect the dataframes to check the data distribution.  \n\nWe will focus on the following data frames:  \n- train_df;  \n- test_df;  \n- train_labels_df;  \n","637b6a83":"### Test","d7466583":"# <a id=\"4\">Next step<\/a>  \n\nThe next step will be to use the ideas from data exploration to start extracting, selecting, engineering features and prepare models.  \n\n"}}