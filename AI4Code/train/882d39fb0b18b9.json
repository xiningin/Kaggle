{"cell_type":{"0da380ce":"code","7af7e3eb":"code","92b1542c":"code","a8dbc696":"code","e9a8c455":"code","b18704bd":"code","141fc8ef":"code","11b268b5":"code","1be64cf0":"code","2d1a6f06":"code","debaf47d":"code","1db2edce":"code","754f01d8":"code","b02de685":"code","4b1a81c2":"code","8ed82ded":"code","ec8a5078":"code","d581f992":"code","4cbaf3d5":"code","aed5aecb":"code","3ee286f6":"code","69220957":"code","da79d9bb":"code","330c9fde":"code","3c3e2ae2":"code","e96b50e3":"code","56ad690e":"code","5f918f59":"code","3334af69":"code","8fea10d5":"code","353a2295":"code","3600f399":"code","a36d37b7":"code","92b5017c":"code","42d6b84c":"code","fa2f1942":"code","49e8f39f":"code","1390e53a":"code","0e24939f":"code","d74c5eda":"code","cdd18a78":"code","508de053":"code","a867979f":"code","f01c85bc":"code","979c27be":"code","bb5b907c":"code","c812e055":"code","f726e9be":"code","ef398826":"code","285352f1":"code","fef275df":"code","d67f4554":"code","26aa911f":"code","3a57c43e":"code","cf7bcfff":"code","87b82f6d":"code","0d280706":"code","3d0d1c63":"code","282ec4d2":"code","6e386485":"code","83db08b6":"code","054cb94f":"code","3a87df3a":"code","812e0380":"code","70fd5759":"code","b58e1e60":"code","aff171e9":"code","47dd0778":"code","1c3d84fe":"code","099b274c":"code","e6ebe7c5":"code","a95dd06f":"code","590a06b3":"code","2ebb124d":"code","ffab7784":"code","9fdd7202":"code","a6ea4b5f":"code","5220c9f7":"code","2ac3b237":"code","e5421aa8":"code","71ff9220":"markdown","6139861c":"markdown","50e6a7aa":"markdown","c01fa11f":"markdown","ad1ba3e3":"markdown","ac80895f":"markdown","fd12d75f":"markdown","90bd5b23":"markdown","a554a184":"markdown","75e52106":"markdown","059ba554":"markdown","f91452fe":"markdown","0e0d41ce":"markdown","12bcbe28":"markdown","cb527759":"markdown","0c275c3b":"markdown","d642f47c":"markdown","209a87e8":"markdown","524d1883":"markdown","d6791402":"markdown","e1222444":"markdown","6222bdef":"markdown","387084f6":"markdown","b3641c08":"markdown","33798f39":"markdown","417a9c33":"markdown","d947df74":"markdown","def3bd9a":"markdown","7f457fcd":"markdown","0ef82687":"markdown"},"source":{"0da380ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.pipeline import Pipeline \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nimport time\nimport datetime\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport numpy as np, pandas as pd, os, gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7af7e3eb":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')","92b1542c":"def detect_num_cols_to_shrink(list_of_num_cols, dataframe):\n \n    convert_to_int8 = []\n    convert_to_int16 = []\n    convert_to_int32 = []\n    \n    #  sadly the datatype float8 does not exist\n    convert_to_float16 = []\n    convert_to_float32 = []\n    \n    for col in list_of_num_cols:\n        \n        if dataframe[col].dtype in ['int', 'int8', 'int32', 'int64']:\n            \n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 255:\n                convert_to_int8.append(col)\n                \n            elif diff < 65535:\n                convert_to_int16.append(col)\n                \n            elif diff < 4294967295:\n                convert_to_int32.append(col)   \n                \n        elif dataframe[col].dtype in ['float', 'float16', 'float32', 'float64']:\n            \n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 65535:\n                convert_to_float16.append(col)\n                \n            elif diff < 4294967295:\n                convert_to_float32.append(col) \n        \n    list_of_lists = []\n    list_of_lists.append(convert_to_int8)\n    list_of_lists.append(convert_to_int16)\n    list_of_lists.append(convert_to_int32)\n    list_of_lists.append(convert_to_float16)\n    list_of_lists.append(convert_to_float32)\n    \n    return list_of_lists","a8dbc696":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e9a8c455":"train_transaction = reduce_mem_usage(train_transaction, verbose = True)\ntrain_identity = reduce_mem_usage(train_identity,verbose = True)\ntest_transaction = reduce_mem_usage(test_transaction, verbose=True)\ntest_identity = reduce_mem_usage(test_identity,verbose = True)","b18704bd":"pd.set_option('display.max_columns', None)","141fc8ef":"train_transaction.head()","11b268b5":"train_identity.head()","1be64cf0":"test_transaction.head()","2d1a6f06":"test_identity.head()","debaf47d":"#gom 2 dataset l\u1ea1i th\u00e0nh 1 theo transactionID\nfinal_train = pd.merge(train_transaction, train_identity, how = 'left', on = 'TransactionID')\n\nfinal_test = pd.merge(test_transaction, test_identity, how = 'left', on = 'TransactionID')\n#final_test = test_transaction.merge(test_identity, on='TransactionID', how = 'inner')\n","1db2edce":"del train_transaction\ndel train_identity\ndel test_transaction\ndel test_identity","754f01d8":"print(final_train.shape)\nprint(final_test.shape)","b02de685":"def different_columns(traincols, testcols):\n    \n    for i in traincols:\n        \n        if i not in testcols:\n            \n            print(i)\n            \ndifferent_columns(final_train.columns, final_test.columns)","4b1a81c2":"final_test = final_test.rename(columns = {\"id-01\": \"id_01\", \"id-02\": \"id_02\", \"id-03\": \"id_03\", \n                            \"id-06\": \"id_06\", \"id-05\": \"id_05\", \"id-04\": \"id_04\", \n                            \"id-07\": \"id_07\", \"id-08\": \"id_08\", \"id-09\": \"id_09\", \n                            \"id-10\": \"id_10\", \"id-11\": \"id_11\", \"id-12\": \"id_12\", \n                            \"id-15\": \"id_15\", \"id-14\": \"id_14\", \"id-13\": \"id_13\", \n                            \"id-16\": \"id_16\", \"id-17\": \"id_17\", \"id-18\": \"id_18\", \n                            \"id-21\": \"id_21\", \"id-20\": \"id_20\", \"id-19\": \"id_19\", \n                            \"id-22\": \"id_22\", \"id-23\": \"id_23\", \"id-24\": \"id_24\", \n                            \"id-27\": \"id_27\", \"id-26\": \"id_26\", \"id-25\": \"id_25\", \n                            \"id-28\": \"id_28\", \"id-29\": \"id_29\", \"id-30\": \"id_30\", \n                            \"id-31\": \"id_31\", \"id-32\": \"id_32\", \"id-33\": \"id_33\", \n                            \"id-34\": \"id_34\", \"id-35\": \"id_35\", \"id-36\": \"id_36\", \n                            \"id-37\": \"id_37\", \"id-38\": \"id_38\"})\nfinal_test.head()","8ed82ded":"different_columns(final_train.columns, final_test.columns)","ec8a5078":"y = final_train.isFraud\ny.describe\n#1 l\u00e0 fraud, c\u00f2n 0 l\u00e0 k ph\u1ea3i fraud\nsns.displot(\n    data=y\n)\n#d\u1eef li\u1ec7u kh\u00e1 ph\u00e2n b\u1ed1 sai, x\u1eed l\u00fd ntn","d581f992":"y.dtypes","4cbaf3d5":"def getNulls(data):\n    \n    total = data.isnull().sum()\n    percent = data.isnull().sum() \/ data.isnull().count()\n    missing_data = pd.concat([total, percent], axis = 1, keys = ['total', 'precent'])\n    \n    return missing_data","aed5aecb":"missing_data_train = getNulls(final_train)\nmissing_data_train.head(434).T","3ee286f6":"del missing_data_train","69220957":"numerical = final_train.select_dtypes(include='number')\nnumerical.head()","da79d9bb":"def make_corr(Vs,Vtitle=''):\n    cols = ['isFraud'] + Vs\n    plt.figure(figsize=(15,15))\n    sns.heatmap(numerical[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n    if Vtitle!='': plt.title(Vtitle,fontsize=14)\n    else: plt.title(Vs[0]+' - '+Vs[-1],fontsize=14)\n    plt.show()\nVs = ['V'+str(i) for i in range(5,16)]\nVtitle = 'V5 - V16'\nmake_corr(Vs,Vtitle)","330c9fde":"sns.distplot(a = numerical['card1'])","3c3e2ae2":"numerical['TransactionAmt'].describe()","e96b50e3":"sns.distplot(a = numerical['TransactionAmt'])","56ad690e":"sns.distplot(a= np.log1p(numerical['TransactionAmt']))","5f918f59":"#coi th\u1eed categorical\ncategorical = final_train.select_dtypes(exclude='number')\ncategorical","3334af69":"ntrain = final_train.shape[0]\nntest = final_test.shape[0]\nall_data = pd.concat([final_train, final_test], axis = 0, sort = False)","8fea10d5":"all_data = all_data.drop(columns=['isFraud'])","353a2295":"del numerical\ndel categorical\ndel final_train\ndel final_test","3600f399":"all_data.shape","a36d37b7":"all_data_cols = all_data.columns","92b5017c":"n = (all_data.dtypes != 'object')\n\nnum_all_cols = list(n[n].index) \n\nprint(num_all_cols)","42d6b84c":"num_cols_to_shrink_all = detect_num_cols_to_shrink(num_all_cols, all_data)\n\nconvert_to_int8 = num_cols_to_shrink_all[0]\nconvert_to_int16 = num_cols_to_shrink_all[1]\nconvert_to_int32 = num_cols_to_shrink_all[2]\n\nconvert_to_float16 = num_cols_to_shrink_all[3]\nconvert_to_float32 = num_cols_to_shrink_all[4]","fa2f1942":"print(\"starting with converting process....\")\n\nfor col in convert_to_int16:\n    \n    all_data[col] = all_data[col].astype('int16') \n    \nfor col in convert_to_int32:\n    all_data[col] = all_data[col].astype('int32') \n\nfor col in convert_to_float16:\n    all_data[col] = all_data[col].astype('float16')\n    \nfor col in convert_to_float32:\n    all_data[col] = all_data[col].astype('float32')\n    \nprint(\"successfully converted!\")","49e8f39f":"all_data","1390e53a":"v =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\nv += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\nv += [332, 325, 335, 338] # b4 lots NAN","0e24939f":"cols = ['V'+str(x) for x in v]\nfor i in all_data_cols:\n    if (i.startswith(\"V\")) and i not in cols:\n        all_data = all_data.drop(columns=[i])\n\nall_data     ","d74c5eda":"# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\n    all_data = all_data.drop(columns=[i])\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\n    all_data = all_data.drop(columns=[i])\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)\n    all_data = all_data.drop(columns=[i])\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\n    all_data = all_data.drop(columns=[i])","cdd18a78":"numerical = all_data.select_dtypes(include='number')\ncategorical = all_data.select_dtypes(exclude = 'number')\n\nnumeric_transformer = Pipeline(steps=[('mean',SimpleImputer(strategy='constant',fill_value=-1))])\ncategorical_transformer = Pipeline(steps=[('constant', SimpleImputer(strategy='constant',fill_value=-1))])","508de053":"#final_all_data[numerical.columns]= numeric_transformer.fit_transform(final_all_data[numerical.columns])\n#final_all_data[categorical.columns] = categorical_transformer.fit_transform(final_all_data[categorical.columns])","a867979f":"final_train = all_data[ : ntrain]\nfinal_test = all_data[ntrain : ]","f01c85bc":"del all_data","979c27be":"for i,f in enumerate(final_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES\n    if (np.str(final_train[f].dtype)=='category')|(final_train[f].dtype=='object'): \n        df_comb = pd.concat([final_train[f],final_test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        final_train[f] = df_comb[:len(final_train)].astype('int16')\n        final_test[f] = df_comb[len(final_train):].astype('int16')\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT']:\n        mn = np.min((final_train[f].min(),final_test[f].min()))\n        final_train[f] -= np.float32(mn)\n        final_test[f] -= np.float32(mn)\n        final_train[f].fillna(-999,inplace=True)\n        final_test[f].fillna(-999,inplace=True)","bb5b907c":"y.value_counts()","c812e055":"final_train.head()","f726e9be":"final_test.head()","ef398826":"#s\u1eed d\u1ee5ng CatBoostClassifier regressor\n#model = LogisticRegression(solver='lbfgs',max_iter=10000)\n\n#model = CatBoostClassifier()\nmodel = xgb.XGBClassifier(n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        #nthread=4,\n        tree_method='hist' \n        #tree_method='gpu_hist' \n                           )","285352f1":"X_train = final_train\ny_train = y\nX_test = final_test","fef275df":"del final_train\ndel y\ndel final_test","d67f4554":"final_col = X_train['TransactionID']\nX_train = X_train.drop(columns=['TransactionID'])","26aa911f":"final_col","3a57c43e":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","cf7bcfff":"cols = list(X_train.columns )\ncols.remove('TransactionDT')","87b82f6d":"idxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","0d280706":"X_train","3d0d1c63":"#h = model.fit(X_train.loc[idxT,cols], y_train[idxT], \n        #eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n       # verbose=50, early_stopping_rounds=100)","282ec4d2":"cols","6e386485":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            tree_method='hist'\n            # USE GPU\n            #tree_method='gpu_hist' \n        )        \n    h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n    oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n    preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        \nprint('#'*20)\nprint ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","83db08b6":"def encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","054cb94f":"encode_CB('card1','addr1')\n#encode_CB('card1_addr1','P_emaildomain')\n\nX_train['day'] =X_train.TransactionDT \/ (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT \/ (24*60*60)\nX_test['uid'] =X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","3a87df3a":"%%time\n# FREQUENCY ENCODE UID\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nencode_FE(X_train,X_test,['uid'])\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_test,X_test,fillna=True,usena=True)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n# NEW FEATURE\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","812e0380":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60) ","70fd5759":"cols = list(X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['DT_M','day','uid']:\n    cols.remove(c)\n    \n","b58e1e60":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","aff171e9":"X_train","47dd0778":"#X = X.drop(columns = ['id_07','id_21','id_22','id_24' ,'id_25' ,'id_26'])\n# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","1c3d84fe":"X_train = X_train.drop(columns=['uid'])","099b274c":"cols = X_train.columns","e6ebe7c5":"X_train","a95dd06f":"idxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","590a06b3":"cols","2ebb124d":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clfz = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            tree_method='hist'\n            # USE GPU\n            #tree_method='gpu_hist' \n        )        \n    hz = clfz.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n    oof[idxV] += clfz.predict_proba(X_train[cols].iloc[idxV])[:,1]\n    preds += clfz.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        \nprint('#'*20)\nprint ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","ffab7784":"#start_time = time.time()\n#predicted_X_test = hz.predict_proba(final_test[X.columns])\n#print(\"Time to execute SKLearn: %s\" % (time.time() - start_time))","9fdd7202":"#predicted_X_test.shape","a6ea4b5f":"#predicted_X_test[:, 1]","5220c9f7":"test_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\nfinal_col = test_transaction['TransactionID']","2ac3b237":"final_col","e5421aa8":"my_submission = pd.DataFrame({'TransactionID':final_col, 'isFraud': preds})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('.\/submission.csv', index=False)\nmy_submission.head(5)","71ff9220":"### **Vesta features**: ###\nThe V columns share a \u201clot\u201d of correlation and a large number of Nan\u2019s. The goal of this step is to find \u201csimilar\u201d columns based on number of \u201cNaN\u2019s\u201d, and Correlation>0.75. This process is automated for all the different NaN groups, using a script. The method is explained as below:","6139861c":"### **Gi\u1ea3m k\u00edch th\u01b0\u1edbc d\u1eef li\u1ec7u**","50e6a7aa":"**Ki\u1ec3m tra l\u1ea1i train v\u00e0 test c\u00f3 c\u00f2n c\u1ed9t kh\u00e1c kh\u00f4ng**","c01fa11f":"### **Card1, 2, 3, 5:** ###","ad1ba3e3":"## **2.2 D\u1eef li\u1ec7u Categorical**","ac80895f":" **1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t Metric**\n\n - Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n - N\u00ean s\u1eed d\u1ee5ng F1 score, t\u00ecm Precision, Recall, Confusion Matrix\n - N\u00ean d\u00f9ng model n\u00e0o \u0111\u1ec3 d\u1ec5 t\u1ed1i \u01b0u AUC???\n\n\n","fd12d75f":"T\u1ed5ng qu\u00e1t:\n- **1. GI\u1edaI THI\u1ec6U DATA:**\n    - 1.1 Th\u00f4ng tin t\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u\n    - 1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t v\u1ec1 Target\n    - 1.3 Metric \u0111\u00e1nh gi\u00e1\n- **2. KHAI PH\u00c1 D\u1eee LI\u1ec6U**\n    - 2.1 D\u1eef li\u1ec7u Numerical\n    - 2.2 D\u1eef li\u1ec7u Categorical\n    - 2.3 T\u1ed5ng h\u1ee3p c\u00e1c d\u1eef li\u1ec7u l\u1ea5y l\u00e0m Feature\n- **3. FEATURE ENGINEERING**\n    - 3.1 X\u1eed l\u00fd d\u1eef li\u1ec7u numerical\n    - 3.2 X\u1eed l\u00fd d\u1eef li\u1ec7u Categorical\n- **4. X\u00c2Y D\u1ef0NG M\u00d4 H\u00ccNH**","90bd5b23":"- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n- TransactionAMT: transaction payment amount in USD\n- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc. \n- addr: address\n- dist: distance\n- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. \n- D1-D15: timedelta, such as days between previous transaction, etc.\n- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.","a554a184":"### **Pipeline Preprocessing**","75e52106":"### **Distance**: ###\ndistances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.","059ba554":"X\u1eed d\u1ee5ng Log1P \u0111\u1ec3 \u0111\u01b0a v\u1ec1 normal distribution","f91452fe":"feature_imp = pd.DataFrame(sorted(zip(clfz.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB95 Most Important Features')\nplt.tight_layout()\nplt.show()","0e0d41ce":"- ProductCD: product code, the product for each transaction \n- card4, card6: payment card information, such as card type, card category, issue bank, country, etc. \n- M1-M9: match, such as names on card and address, etc.\n- idxx, DeviceType, DeviceInfo: Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions. \n","12bcbe28":"# **2. KHAI PH\u00c1 D\u1eee LI\u1ec6U**","cb527759":" **Ki\u1ec3m tra gi\u1eefa 2 file c\u00f3 c\u00e1c c\u1ed9t n\u00e0o kh\u00e1c nhau**\n\nWe have observed there exist difference style in column name. Therefore, need to solve this issue.","0c275c3b":"### **Check Nan trong Data**","d642f47c":"### **FACTORIZE CATEGORICAL VARIABLES and SHIFT ALL NUMERICS POSITIVE SET NAN to -1**","209a87e8":"# **Submission**","524d1883":"### **1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t Target**","d6791402":"## **2.1 D\u1eef li\u1ec7u Numerical**:","e1222444":"### **T\u1ed5ng k\u1ebft v\u1ec1 Nan:** ###\n- Nhi\u1ec1u c\u1ed9t c\u00f3 Nan h\u01a1n 25% **=>** N\u1ebfu b\u1ecf th\u00ec s\u1ebd m\u1ea5t kh\u00e1 nhi\u1ec1u data\n- \u00cdt d\u1eef li\u1ec7u n\u00ean **fill Nan b\u1eb1ng s\u1ed1 \u1ea3o** (-999 ho\u1eb7c -1)\n- C\u00f3 nhi\u1ec1u c\u1ed9t **mang th\u00f4ng tin \u0111\u1eb7c tr\u01b0ng** (card1-6 l\u00e0 s\u1ed1 th\u1ebb, addr1-2 l\u00e0 s\u1ed1 nh\u00e0). **Fill b\u1eb1ng mean s\u1ebd kh\u00f4ng c\u00f3 \u00fd ngh\u0129a**","6222bdef":"# **1. GI\u1edaI THI\u1ec6U DATA**","387084f6":"- **1.1 T\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u v\u00f4**\n\n- train_identity:\n - 23 lo\u1ea1i Numerical\n - 14 categorical\n - 4 Boolean\n - T\u1ed5ng quan: Nhi\u1ec1u bi\u1ebfn thi\u1ebfu d\u1eef li\u1ec7u\n\n- train_transaction:\n - 356 lo\u1ea1i Numerical @@ (30\/6\/2021 ch\u01b0a s\u1eeda)\n - 30 categorical\n - 8 lo\u1ea1i boolean\n - T\u1ed5ng quan: File h\u1ebft 1.7gb, qu\u00e1 nhi\u1ec1u bi\u1ebfn, c\u1ea7n gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u\n - Gi\u1ea3m chi\u1ec1u ntn ch\u01b0a bi\u1ebft","b3641c08":"# **4. X\u00e2y d\u1ef1ng Model**","33798f39":"## **UID Magic**","417a9c33":"## **No UID**","d947df74":"### **TransactionAmt**: ###","def3bd9a":"# **3. Feature Engineering**","7f457fcd":"## **2.3 D\u1eef li\u1ec7u Boolean**","0ef82687":"### **Ch\u1ecdn Features**"}}