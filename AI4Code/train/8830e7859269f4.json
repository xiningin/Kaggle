{"cell_type":{"dc5c333d":"code","f446dc9d":"code","3ee05e7f":"code","116bbf78":"code","1d236b6c":"code","42246c66":"code","a250f9a9":"code","b8029500":"code","f02b9741":"code","4a82c2ec":"code","45e25ad5":"code","d8c1762e":"code","bf67e70e":"code","c43c1c84":"code","990bd04a":"code","650fe053":"code","ead7d107":"code","e766c931":"code","4574af01":"code","093aa3a6":"code","a10874ad":"code","8611b780":"code","080c22e4":"code","8326d9cc":"code","a46d91ef":"code","8d598c70":"code","df880a06":"code","50b95d63":"markdown","952b3298":"markdown","0d64174f":"markdown","13a20342":"markdown","1c1e3c2f":"markdown"},"source":{"dc5c333d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f446dc9d":"from sklearn.preprocessing import LabelEncoder","3ee05e7f":"df = pd.read_csv('..\/input\/wine-quality\/winequalityN.csv')","116bbf78":"df.shape","1d236b6c":"df.head()","42246c66":"encoder = LabelEncoder()\nencoder.fit(df['type'])\ndf['type'] = encoder.transform(df['type'])","a250f9a9":"df = df[['fixed acidity', 'volatile acidity', 'citric acid',\n       'residual sugar', 'chlorides', 'free sulfur dioxide',\n       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n       'quality','type']]","b8029500":"df.columns[:-1]","f02b9741":"df[df.columns[:-1]] = df[df.columns[1:]].fillna(0)   \nfor i in df.columns[:-1]:\n    df[i] = df[i].apply(lambda x : float(x))\nX = df[df.columns[:-1]]\ny = df[['type']]","4a82c2ec":"X.columns","45e25ad5":"y.columns","d8c1762e":"df.shape","bf67e70e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\ntrain = X_train.join(y_train)","c43c1c84":"# Make Predictions with Naive Bayes On The Iris Dataset\nfrom csv import reader\nfrom math import sqrt\nfrom math import exp\nfrom math import pi\n\n# Load a CSV file\ndef load_csv(filename):\n\tdataset = list()\n\twith open(filename, 'r') as file:\n\t\tcsv_reader = reader(file)\n\t\tfor row in csv_reader:\n\t\t\tif not row:\n\t\t\t\tcontinue\n\t\t\tdataset.append(row)\n\treturn dataset\n\n# Convert string column to float\ndef str_column_to_float(dataset, column):\n\tfor row in dataset:\n\t\trow[column] = float(row[column].strip())\n\n# Convert string column to integer\ndef str_column_to_int(dataset, column):\n\tclass_values = [row[column] for row in dataset]\n\tunique = set(class_values)\n\tlookup = dict()\n\tfor i, value in enumerate(unique):\n\t\tlookup[value] = i\n\t\tprint('[%s] => %d' % (value, i))\n\tfor row in dataset:\n\t\trow[column] = lookup[row[column]]\n\treturn lookup\n\n# Split the dataset by class values, returns a dictionary\ndef separate_by_class(dataset):\n\tseparated = dict()\n\tfor i in range(len(dataset)):\n\t\tvector = dataset[i]\n\t\tclass_value = vector[-1]\n\t\tif (class_value not in separated):\n\t\t\tseparated[class_value] = list()\n\t\tseparated[class_value].append(vector)\n\treturn separated\n\n# Calculate the mean of a list of numbers\ndef mean(numbers):\n\treturn sum(numbers)\/float(len(numbers))\n\n# Calculate the standard deviation of a list of numbers\ndef stdev(numbers):\n\tavg = mean(numbers)\n\tvariance = sum([(x-avg)**2 for x in numbers]) \/ float(len(numbers)-1)\n\treturn sqrt(variance)\n\n# Calculate the mean, stdev and count for each column in a dataset\ndef summarize_dataset(dataset):\n\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n\tdel(summaries[-1])\n\treturn summaries\n\n# Split dataset by class then calculate statistics for each row\ndef summarize_by_class(dataset):\n\tseparated = separate_by_class(dataset)\n\tsummaries = dict()\n\tfor class_value, rows in separated.items():\n\t\tsummaries[class_value] = summarize_dataset(rows)\n\treturn summaries\n\n# Calculate the Gaussian probability distribution function for x\ndef calculate_probability(x, mean, stdev):\n\texponent = exp(-((x-mean)**2 \/ (2 * stdev**2 )))\n\treturn (1 \/ (sqrt(2 * pi) * stdev)) * exponent\n\n# Calculate the probabilities of predicting each class for a given row\ndef calculate_class_probabilities(summaries, row):\n\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n\tprobabilities = dict()\n\tfor class_value, class_summaries in summaries.items():\n\t\tprobabilities[class_value] = summaries[class_value][0][2]\/float(total_rows)\n\t\tfor i in range(len(class_summaries)-2):\n\t\t\tmean, stdev, _ = class_summaries[i]\n\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n\treturn probabilities\n\n# Predict the class for a given row\ndef predict(summaries, row):\n\tprobabilities = calculate_class_probabilities(summaries, row)\n\tbest_label, best_prob = None, -1\n\tfor class_value, probability in probabilities.items():\n\t\tif best_label is None or probability > best_prob:\n\t\t\tbest_prob = probability\n\t\t\tbest_label = class_value\n\treturn best_label\n\n# Make a prediction with Naive Bayes on Iris Dataset\n\ndataset = train.values\nmodel = summarize_by_class(dataset)\n# define a new record\ntest = X_test.values\nlabel = []\n# predict the label\nfor i in range(len(test)):\n    label.append(predict(model,list(test[i][:])))\n    \n","990bd04a":"model","650fe053":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, label))","ead7d107":"X = df[df.columns[:-1]]\ny = df['type'].ravel()","e766c931":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train)\nprint(gnb.score(X_test, y_test))","4574af01":"from sklearn.ensemble import AdaBoostClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))","093aa3a6":"from sklearn import ensemble\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01}\n\nxg_model = ensemble.GradientBoostingClassifier(**params)\n\n\nxg_model.fit(X_train,y_train)\nxg_model.score(X_test, y_test)","a10874ad":"from sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)\nlin = LogisticRegression(solver = 'liblinear')\nlin.fit(X_train,y_train)\nprint(lin.score(X_test,y_test))","8611b780":"X = df[df.columns[:-1]]\ny = df['type']","080c22e4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","8326d9cc":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)","a46d91ef":"dtree.score(X_test,y_test)","8d598c70":"X = df[df.columns[:-1]]\ny = df['type']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)\ntrain_data = X_train.join(y_train)","df880a06":"# k-nearest neighbors on the Iris Flowers Dataset\nfrom random import seed\nfrom random import randrange\nfrom csv import reader\nfrom math import sqrt\n\n\n# Find the min and max values for each column\ndef dataset_minmax(dataset):\n\tminmax = list()\n\tfor i in range(len(dataset[0])):\n\t\tcol_values = [row[i] for row in dataset]\n\t\tvalue_min = min(col_values)\n\t\tvalue_max = max(col_values)\n\t\tminmax.append([value_min, value_max])\n\treturn minmax\n\n# Rescale dataset columns to the range 0-1\ndef normalize_dataset(dataset, minmax):\n\tfor row in dataset:\n\t\tfor i in range(len(row)):\n\t\t\trow[i] = (row[i] - minmax[i][0]) \/ (minmax[i][1] - minmax[i][0])\n\n\n# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):\n\tcorrect = 0\n\tfor i in range(len(actual)):\n\t\tif actual[i] == predicted[i]:\n\t\t\tcorrect += 1\n\treturn correct \/ float(len(actual)) * 100.0\n\n\n\n# Calculate the Euclidean distance between two vectors\ndef euclidean_distance(row1, row2):\n\tdistance = 0.0\n\tfor i in range(len(row1)-1):\n\t\tdistance += (row1[i] - row2[i])**2\n\treturn sqrt(distance)\n\n# Locate the most similar neighbors\ndef get_neighbors(train, test_row, num_neighbors):\n\tdistances = list()\n\tfor train_row in train:\n\t\tdist = euclidean_distance(test_row, train_row)\n\t\tdistances.append((train_row, dist))\n\tdistances.sort(key=lambda tup: tup[1])\n\tneighbors = list()\n\tfor i in range(num_neighbors):\n\t\tneighbors.append(distances[i][0])\n\treturn neighbors\n\n# Make a prediction with neighbors\ndef predict_classification(train, row, num_neighbors):\n    neighbors = get_neighbors(train, row, num_neighbors)\n    output_values = [row[-1] for row in neighbors]\n    prediction = max(set(output_values), key=output_values.count)\n    return prediction\n\n# kNN Algorithm\ndef k_nearest_neighbors(train, test, num_neighbors):\n\tpredictions = list()\n\tfor row in test:\n\t\toutput = predict_classification(train, row, num_neighbors)\n\t\tpredictions.append(output)\n\n\treturn(predictions)\n\n# Test the kNN on the Iris Flowers dataset\ntrain = train_data.values\ntest = X_test.values\nn_folds = 5\nnum_neighbors = 5\n\nscores = k_nearest_neighbors(train, test, num_neighbors)\n#print('Scores: %s' % scores)\n#print('Mean Accuracy: %.3f%%' % (sum(scores)\/float(len(scores))))\nprint(accuracy_score(y_test, scores))","50b95d63":"# AdaBoost Classifier","952b3298":"# Logistic Regression","0d64174f":"# Gradient boosting method","13a20342":"# Classification and Regression Trees :\n\nThe split with the best cost (lowest cost because we minimize cost) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function.\n\n* Regression: The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n\n\n* Classification: The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.Smart move would be to use the sklearn libraries .","1c1e3c2f":"# Naive Bayes theorom"}}