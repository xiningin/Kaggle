{"cell_type":{"60c23f14":"code","c3ea0860":"code","b5b9d8c0":"code","39d99734":"code","b0f4efb3":"code","4c79300b":"code","533e07c5":"code","7b33500a":"code","089d09e6":"code","a668d922":"code","4f22fe75":"code","e94cc801":"code","1c3b2b31":"code","33d8d64f":"code","d5a8bb00":"code","17ce7327":"code","b1e8ae97":"code","6cb013e1":"code","4b551518":"code","a6afdd5a":"code","1a8b0874":"code","cfe49ba2":"code","a417e08d":"code","add959f2":"code","97b39605":"code","46d74058":"code","a73e3b93":"code","1f662b12":"code","8b6eff87":"code","5b11dcf0":"markdown","84b2e823":"markdown","2d3403e3":"markdown","00693944":"markdown","25228597":"markdown","a16ca128":"markdown","ba787c36":"markdown","ec7e0885":"markdown","592d041b":"markdown","aeb661f7":"markdown","bfb43e06":"markdown","ef5b488f":"markdown","ce1de0e2":"markdown","d0c69c8a":"markdown","b5f8f1a8":"markdown","18fda632":"markdown","2a0e0c74":"markdown","a0f556cd":"markdown","3d321371":"markdown","d8e91377":"markdown","2da93aa3":"markdown"},"source":{"60c23f14":"#%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\nimport seaborn as sns","c3ea0860":"train = pd.read_csv('..\/input\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})","b5b9d8c0":"train.columns","39d99734":"train.head()","b0f4efb3":"train.describe()","4c79300b":"train.info()","533e07c5":"'''\nAppend train and test data so that all the data manipulations are common to both\n'''\nfull_data = train.append(test)\nfull_data.reset_index(inplace=True)\n\nprint (train.info())\npd.set_option('display.expand_frame_repr', False)\nprint (train.describe())\npd.set_option('display.expand_frame_repr', True)\nprint (train.head())\nprint (train.describe(include=['O']))\nprint (train.head())","7b33500a":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","089d09e6":"print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())","a668d922":"print (train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean())\nprint (train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean())\nfull_data['FamilySize'] = full_data['SibSp'] + full_data['Parch'] + 1\nprint (full_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","4f22fe75":"full_data['IsAlone'] = 0\nfull_data.loc[full_data['FamilySize'] == 1, 'IsAlone'] = 1\nprint (full_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","e94cc801":"print (full_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).count())\nfull_data['Embarked'] = full_data['Embarked'].fillna('S')\nprint (full_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","1c3b2b31":"full_data['Fare'] = full_data['Fare'].fillna(full_data['Fare'].median())","33d8d64f":"full_data['CategoricalFare'] = pd.qcut(full_data['Fare'], 8)\nprint (full_data[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","d5a8bb00":"age_avg \t   = full_data['Age'].mean()\nage_std \t   = full_data['Age'].std()\nage_null_count = full_data['Age'].isnull().sum()\n    \nage_null_random_list = np.random.randint(age_avg - 1.5*age_std, age_avg + 1.5*age_std, size=age_null_count)\nfull_data['Age'][np.isnan(full_data['Age'])] = age_null_random_list\nfull_data['Age'] = full_data['Age'].astype(int)\n    \nfull_data['CategoricalAge'] = pd.cut(full_data['Age'], 8)\n\nprint (full_data[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())","17ce7327":"def get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"\n\nfull_data['Title'] = full_data['Name'].apply(get_title)\n\nprint(pd.crosstab(full_data['Title'], full_data['Sex']))","b1e8ae97":"full_data['Title'] = full_data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\nfull_data['Title'] = full_data['Title'].replace('Mlle', 'Miss')\nfull_data['Title'] = full_data['Title'].replace('Ms', 'Miss')\nfull_data['Title'] = full_data['Title'].replace('Mme', 'Mrs')\n\nprint (full_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","6cb013e1":"#Turning cabin number into Deck\ncabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n#full_data['Deck']=full_data['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\na= full_data['Cabin'].astype(str).str[0]\nfull_data['Cabin']=a.str.upper()\nprint (full_data[['Cabin','Survived']].groupby(['Cabin'], as_index=False).mean())\nprint (full_data[['Cabin','Pclass','Survived']].groupby(['Cabin','Pclass'], as_index=False).mean())","4b551518":"import matplotlib.pyplot as plt\nfull_data.columns,train.columns,train.index\n#full_data.loc[train.index,:]\nf, ax = plt.subplots(figsize=[10,10])\nsns.heatmap(full_data.loc[train.index,:].corr(),\n            annot=True, fmt=\".2f\",cbar_kws={'label': 'Percentage %'},cmap=\"plasma\",ax=ax)\nax.set_title(\"Correlation Plot\")\nplt.show()","a6afdd5a":"full_data['Sex'] = full_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n# Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfull_data['Title'] = full_data['Title'].map(title_mapping)\nfull_data['Title'] = full_data['Title'].fillna(0)\n    \n    # Mapping Embarked\nfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n\nfull_data.loc[ full_data['Fare'] <= 7.75, 'Fare'] \t\t\t\t\t\t            = 0\nfull_data.loc[(full_data['Fare'] > 7.75) & (full_data['Fare'] <= 7.896), 'Fare']    = 1\nfull_data.loc[(full_data['Fare'] > 7.896) & (full_data['Fare'] <= 10.008), 'Fare']  = 2\nfull_data.loc[(full_data['Fare'] > 10.008) & (full_data['Fare'] <= 14.454), 'Fare'] = 3\nfull_data.loc[(full_data['Fare'] > 14.454) & (full_data['Fare'] <= 24.15), 'Fare']  = 4\nfull_data.loc[(full_data['Fare'] > 24.15) & (full_data['Fare'] <= 31.275), 'Fare']  = 5    \nfull_data.loc[(full_data['Fare'] > 31.275) & (full_data['Fare'] <= 69.55), 'Fare']  = 6 \nfull_data.loc[(full_data['Fare'] > 69.55) , 'Fare']  = 7    \n\nfull_data['Fare'] = full_data['Fare'].astype(int)\n\n\n    # Mapping Age\n\nfull_data.loc[ full_data['Age'] <= 10, 'Age'] \t\t\t\t\t         = 0\nfull_data.loc[(full_data['Age'] > 10) & (full_data['Age'] <= 20), 'Age'] = 1\nfull_data.loc[(full_data['Age'] > 20) & (full_data['Age'] <= 30), 'Age'] = 2\nfull_data.loc[(full_data['Age'] > 30) & (full_data['Age'] <= 40), 'Age'] = 3\nfull_data.loc[(full_data['Age'] > 40) & (full_data['Age'] <= 50), 'Age'] = 4\nfull_data.loc[(full_data['Age'] > 50) & (full_data['Age'] <= 60), 'Age'] = 5\nfull_data.loc[(full_data['Age'] > 60) & (full_data['Age'] <= 70), 'Age'] = 6\nfull_data.loc[ full_data['Age'] > 70, 'Age']                             = 7\n\n# Mapping Cabin\ncabin_mapping={\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5,\"F\": 6,\"G\": 7 , \"T\":8,\"N\":0}\nfull_data['Cabin'] = full_data['Cabin'].map(cabin_mapping)\nfull_data['Cabin'] = full_data['Cabin'].fillna(0)\n\nfull_data.head()\n\n# Feature Selection\ndrop_elements = ['index','Name', 'Ticket', 'SibSp',\\\n                 'Parch', 'FamilySize']\nfull_data = full_data.drop(drop_elements, axis = 1)\nfull_data = full_data.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)","1a8b0874":"train = full_data.iloc[:891]\ntest = full_data.iloc[891:]\ntargets = full_data['Survived'].iloc[:891]\n\ntest.drop(['Survived','PassengerId'],axis=1,inplace=True)\ntrain.drop(['Survived','PassengerId'],axis=1,inplace=True)","cfe49ba2":"print (full_data[full_data['Survived'].isnull()])","a417e08d":"import matplotlib.gridspec as gridspec\n\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(train, targets)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=train.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()\n\ndf= full_data\ngs = gridspec.GridSpec(28,1)\nplt.figure(figsize=(6,28*4))\nfor i,col in enumerate(df[df.iloc[:,0:28].columns]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[col][df.Survived ==1],bins=50,color='b')\n    sns.distplot(df[col][df.Survived ==0],bins=50,color='r')\n    #ax.set_title('feature '+str(col))\nplt.show()","add959f2":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# encode Sex labels using one-hot encoding scheme\ngen_ohe = OneHotEncoder()\ngen_feature_arr = gen_ohe.fit_transform(full_data[['Sex']]).toarray()\n#gen_feature_labels = list(gen_le.classes_)\ngen_features = pd.DataFrame(gen_feature_arr)\n\ndf = pd.concat([full_data, gen_features], axis=1)\ndf.rename(columns={0:'female',1:'male'},inplace=True)\n#df.drop('Sex',axis=1,inplace=True)\n\n# encode Pclass status labels using one-hot encoding scheme\nleg_ohe = OneHotEncoder()\nleg_feature_arr = leg_ohe.fit_transform(df[['Pclass']]).toarray()\nleg_features = pd.DataFrame(leg_feature_arr)\n\nleg_features.rename(columns={0:'Pclass0',1:'Pclass1',2:'Pclass2'},inplace=True)\n\ndf = pd.concat([df, leg_features], axis=1)\ndf.drop('Pclass',axis=1,inplace=True)\n\n# encode Title status labels using one-hot encoding scheme\nleg_title = OneHotEncoder()\nleg_feature_title = leg_title.fit_transform(df[['Title']]).toarray()\nleg_features = pd.DataFrame(leg_feature_title)\nleg_features.rename(columns={0:'Title0',1:'Title1',2:'Title2',3:'Title3',4:'Title4'},inplace=True)\n\ndf = pd.concat([df, leg_features], axis=1)\n#df.drop('Title',axis=1,inplace=True)\n\n'''\n# encode Fare status labels using one-hot encoding scheme\nleg_title = OneHotEncoder()\nleg_feature_fare = leg_title.fit_transform(df[['Fare']]).toarray()\nleg_features = pd.DataFrame(leg_feature_fare)\nleg_features.rename(columns={0:'Fare0',1:'Fare1',2:'Fare2',3:'Fare3'},inplace=True)\n\ndf = pd.concat([df, leg_features], axis=1)\n#df.drop('Fare',axis=1,inplace=True)\n'''\n\n'''\n# encode Cabin status labels using one-hot encoding scheme\nleg_title = OneHotEncoder()\nleg_feature_fare = leg_title.fit_transform(df[['Cabin']]).toarray()\nleg_features = pd.DataFrame(leg_feature_fare)\nleg_features.rename(columns={1:'CabA', 2:'CabB', 3:'CabC', 4:'CabD', 5:'CabE',6:'CabF', 7:'CabG' , 8:'CabT',0:'CabN'},inplace=True)\n\ndf = pd.concat([df, leg_features], axis=1)\ndf.drop('Cabin',axis=1,inplace=True)\n'''\n\n\n# encode Embarked status labels using one-hot encoding scheme\nleg_ohe = OneHotEncoder()\nleg_feature_arr = leg_ohe.fit_transform(df[['Embarked']]).toarray()\nleg_features = pd.DataFrame(leg_feature_arr)\n\nleg_features.rename(columns={0:'Embarked0',1:'Embarked1',2:'Embarked2'},inplace=True)\n\ndf = pd.concat([df, leg_features], axis=1)\ndf.drop('Embarked',axis=1,inplace=True)","97b39605":"train = df.iloc[:891]\ntrain.drop(['Survived','PassengerId','Title','Sex'],axis=1,inplace=True)\ntargets = df['Survived'].iloc[:891]\n\nmodel = ExtraTreesClassifier()\nmodel.fit(train, targets)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=train.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()","46d74058":"train.columns","a73e3b93":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nprint(\"Start classifer comparison\")\ncol_selected = ['male','female','Fare','Age','Pclass0','Pclass1','Pclass2','Title0','Title1','Title2','Title3','Title4']\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=1788,min_samples_split= 5, min_samples_leaf= 4, max_features= 'auto', max_depth= 32, bootstrap=True),\n\tAdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(),\n    XGBClassifier()]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog \t = pd.DataFrame(columns=log_cols)\n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n\nfull_data = df\n\ntrain = full_data.iloc[:891]\ntest = full_data.iloc[891:]\ntargets = full_data['Survived'].iloc[:891]\n\ntest.drop('Survived',axis=1,inplace=True)\ntrain.drop('Survived',axis=1,inplace=True)\n\ntest = test[col_selected]\ntrain = train[col_selected]\n\n#targets = pd.read_csv('..\/input\/train.csv', usecols=['Survived'])['Survived'].values\n\n\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(train, targets)\n'''\nclf = DecisionTreeClassifier(max_features ='sqrt',splitter='random',max_depth = 50 )\nclf = clf.fit(train, targets)\n'''\ntrain_predictions = clf.predict(test).astype(int)\n\ndf_output = pd.DataFrame()\naux = pd.read_csv('..\/input\/test.csv')\ndf_output['PassengerId'] = aux['PassengerId']\ndf_output['Survived'] = train_predictions\ndf_output[['PassengerId','Survived']].to_csv('titanic_submission_final.csv', index=False)\nprint(\"File saved\")\n\n\nX = train[col_selected]\ny = targets\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n\tX_train, X_test = X.iloc[train_index], X.iloc[test_index]\n\ty_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\t  \n\tfor clf in classifiers:\n\t\tname = clf.__class__.__name__\n\t\tclf.fit(train, targets)\n\t\ttrain_predictions = clf.predict(X_test)\n\t\tacc = accuracy_score(y_test, train_predictions)\n\t\tif name in acc_dict:\n\t\t\tacc_dict[name] += acc\n\t\telse:\n\t\t\tacc_dict[name] = acc\n\nfor clf in acc_dict:\n\tacc_dict[clf] = acc_dict[clf] \/ 10.0\n\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n\tlog = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\n\nclf = RandomForestClassifier(n_estimators=1788,min_samples_split= 5, min_samples_leaf= 4, max_features= 'auto', max_depth= 32, bootstrap=True)\nclf.fit(X, y)\nresult = clf.predict(test).astype(int)\n\ndf_output = pd.DataFrame()\naux = pd.read_csv('..\/input\/test.csv')\ndf_output['PassengerId'] = aux['PassengerId']\ndf_output['Survived'] = result\ndf_output[['PassengerId','Survived']].to_csv('titanic_submission_final_20190926.csv', index=False)\nprint(\"File saved\")\n","1f662b12":"log","8b6eff87":"'''\nclf.get_params()\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\nmax_features = ['auto', 'log2']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\nprint(rf_random.best_params_)\n'''","5b11dcf0":"## 4. Embarked ##\nthe embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).","84b2e823":"good! the impact is considerable.","2d3403e3":"## 7. Name ##\ninside this feature we can find the title of people.","00693944":"# Data Cleaning #\ngreat! now let's clean our data and map our features into numerical values.","25228597":"## 1. Pclass ##\nthere is no missing value on this feature and already a numerical value. so let's check it's impact on our train set.","a16ca128":"# Classifier Comparison #","ba787c36":"it seems has a good effect on our prediction but let's go further and categorize people to check whether they are alone in this ship or not.","ec7e0885":"<a id=\"section-two\"><\/a>\n# Variable Description","592d041b":"## 8. Deck ## \nA cabin number looks like \u2018C123\u2019. The letter refers to the deck, and so we\u2019re going to extract these just like the titles. Let's check the impact of this on the survival rate","aeb661f7":"good! now we have a clean dataset and ready to predict. let's find which classifier works better on this dataset. ","bfb43e06":"<a id=\"section-three\"><\/a>\n# Exploratory Analysis","ef5b488f":" so we have titles. let's categorize it and check the title impact on survival rate.","ce1de0e2":"## 6. Age ##\nThere are plenty of missing values in this feature. # generate random numbers between (mean - 1.5 x std) and (mean + 1.5 x std).\nthen we categorize age into 8 quantiles","d0c69c8a":"<a id=\"section-one\"><\/a>\n# Load Data","b5f8f1a8":"1. PassengerId: unique id number to each passenger - int64(5)\n\n2. Survived: passenger survive(1) or died(0) - int64(5)\n\n3. Pclass: passenger class - int64(5)\n\n4. Name: name - object(5)\n\n5. Sex: gender of passenger - object(5)\n\n6. Age: age of passenger - float64(2)\n\n7. SibSp: number of siblings\/spouses - int64(5)\n\n8. Parch: number of parents\/children - int64(5)\n\n9. Ticket: ticket number - object(5)\n\n10. Fare: amount of money spent on ticket - float64(2)\n\n11. Cabin: cabin category - object(5)\n\n12. Embarked: port where passenger embarked(C = Cherbourg, Q = Queenstown, S = Southampton) - object(5)","18fda632":"# **Index**\n* [Load Data](#section-one)\n* [Variable Description](#section-two)\n* [Exploratory Analysis](#section-three)","2a0e0c74":"## 5. Fare ##\nFare also has some missing value and we will replace it with the median.","a0f556cd":"Categorize fare into 8 quantiles","3d321371":"## Introduction ##\n\nThis is my first work of machine learning. the notebook is written in python and has inspired from [\"Exploring Survival on Titanic\" by Megan Risdal, a Kernel in R on Kaggle][1].\n\n\n  [1]: https:\/\/www.kaggle.com\/mrisdal\/titanic\/exploring-survival-on-the-titanic\n","d8e91377":"## 2. Sex ##","2da93aa3":"## 3. SibSp and Parch ##\nWith the number of siblings\/spouse and the number of children\/parents we can create new feature called Family Size."}}