{"cell_type":{"a7fcd99a":"code","2b7e20c2":"code","d700bcd0":"code","65364f9e":"code","8662a9f8":"code","f6ec2579":"code","4b6b901a":"code","19f3b15d":"code","603e9349":"code","a2cc81a1":"code","36ca65a0":"code","e861be55":"code","5bc436df":"code","1b8bf69b":"code","c807a820":"code","26c285fd":"code","7e120a6d":"code","e8378b9c":"code","38386b2c":"code","125204f3":"code","0d920d3b":"code","1ac5cd67":"code","1c6210e5":"code","2c8f06f1":"code","a35da32b":"code","0c18935a":"code","5079eb40":"code","7a8ada5b":"code","c95605ee":"code","c6272208":"code","265209b3":"code","5e84ac20":"markdown","3fd36d7a":"markdown","bf7178a3":"markdown","25ca4265":"markdown","70085bab":"markdown","6218d4fb":"markdown"},"source":{"a7fcd99a":"# Imporitng libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.impute import SimpleImputer\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n!pip install catboost","2b7e20c2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d700bcd0":"# Importing datasets\ntrain_prepro = pd.read_csv(\"..\/input\/advanced-regression-preprocessed\/train_preprocessed.csv\")\ntest_prepro = pd.read_csv(\"..\/input\/advanced-regression-preprocessed\/test_preprocessed.csv\")\nsample = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","65364f9e":"# Taking the target train values (SalePrice)\ninit_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ny_train = init_train[\"SalePrice\"]","8662a9f8":"train_prepro.head()","f6ec2579":"# Split the train set into categorical and continous\n# DO SAME IN TEST\ncat_index = train_prepro.columns.get_loc(\"MSZoning_C (all)\")\ntrain_cat = train_prepro.iloc[:,cat_index:]\ntrain_con = train_prepro.iloc[:,:cat_index]\ntest_cat = test_prepro.iloc[:,cat_index:]\ntest_con = test_prepro.iloc[:,:cat_index]","4b6b901a":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\npoly_train_con = poly.fit_transform(train_con)\npoly_test_con = poly.transform(test_con)\npoly_train_con = pd.DataFrame(data=poly_train_con)\npoly_test_con = pd.DataFrame(data=poly_test_con)\ntrain_poly = pd.concat([poly_train_con, train_cat],axis=1)\ntest_poly = pd.concat([poly_test_con, test_cat], axis=1)","19f3b15d":"train_poly.tail()","603e9349":"from sklearn.preprocessing import StandardScaler\nstd_poly_scaler = StandardScaler()\nstd_scaler = StandardScaler()\ntrain_poly_sc = std_poly_scaler.fit_transform(train_poly)\ntest_poly_sc = std_poly_scaler.transform(test_poly)\ntrain_sc = std_scaler.fit_transform(train_con)\ntest_sc = std_scaler.transform(test_con)\ntrain_sc = pd.DataFrame(data=train_sc)\ntest_sc = pd.DataFrame(data=test_sc)\ntrain_sc = pd.concat([train_sc, train_cat],axis=1)\ntest_sc = pd.concat([test_sc, test_cat],axis=1)","a2cc81a1":"from sklearn.decomposition import PCA\n\n# Perform PCA with ratio of variance we want to preserve\npca = PCA(n_components=0.85)\ntrain_poly_reduced = pca.fit_transform(train_poly_sc)\ntest_poly_reduced = pca.transform(test_poly_sc)","36ca65a0":"pca.n_components_","e861be55":"train_poly_reduced","5bc436df":"#Ridge Regression GridSearch\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\"alpha\" : [300, 500, 600, 650, 700, 750, 800], \"solver\" : [\"cholesky\"]}\n\nridge_reg = Ridge(random_state=42)\ngrid_search = GridSearchCV(ridge_reg, params, cv=5, scoring=\"neg_mean_squared_log_error\", n_jobs = -1)\n#Performing gridsearch with our original preprocessed dataset cv-5\ngrid_search.fit(train_prepro, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","1b8bf69b":"#Ridge Regression GridSearch with Scaled Polynomial features after performing PCA cv-5\ngrid_search.fit(train_poly_reduced, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","c807a820":"#Ridge Regression GridSearch with Scaled only features cv-5\ngrid_search.fit(train_sc, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","26c285fd":"#ElasticNet Regression GridSearch\n\nfrom sklearn.linear_model import ElasticNet\n\nparams = {\"alpha\" : [150, 200, 300, 500, 700, 1200], \"l1_ratio\" : [0,0.2,0.4,0.6,0.8,1]}\n\nelastic_reg = ElasticNet(random_state=42)\ngrid_search = GridSearchCV(elastic_reg, params, cv=5, scoring=\"neg_mean_squared_log_error\", n_jobs = -1)\ngrid_search.fit(train_prepro, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","7e120a6d":"#Ridge Regression GridSearch with Scaled Polynomial features after performing PCA cv-5\ngrid_search.fit(train_poly_reduced, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","e8378b9c":"# #Ridge Regression GridSearch with Scaled only features\n# grid_search.fit(train_sc, y_train)\n# print(\"Best parameters : \", grid_search.best_params_)\n# print(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","38386b2c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Values set after many tries i've done\nparams = {'learning_rate':[0.09],\n          'subsample':[0.8],\n          'max_features':['auto'],\n}\n\n# original preprocessed data\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=800, random_state=42)\ngrid_search = GridSearchCV(gbrt, params, cv=5, scoring=\"neg_mean_squared_log_error\", n_jobs = -1)\ngrid_search.fit(train_prepro, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","125204f3":"# Polynomial reduced features\ngrid_search = GridSearchCV(gbrt, params, cv=5, scoring=\"neg_mean_squared_log_error\", n_jobs = -1)\ngrid_search.fit(train_poly_reduced, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","0d920d3b":"#Scaled original preprocessed features\ngrid_search.fit(train_sc, y_train)\nprint(\"Best parameters : \", grid_search.best_params_)\nprint(\"Best score : \", np.sqrt(np.absolute(grid_search.best_score_)))","1ac5cd67":"train_prepro.head()","1c6210e5":"# Default parameters\nfrom catboost import CatBoostRegressor, Pool, cv\n\ncat_features = train_prepro.columns\ncat_features = cat_features[cat_index:]\n\ntrain_pool = Pool(train_prepro, \n                  y_train,\n                  cat_features)\n\ncatboost_model = CatBoostRegressor(iterations=1200,\n                                task_type='GPU',\n                                devices='0:1',\n                                loss_function='RMSE',\n                                eval_metric='MSLE',\n                                random_seed=42)\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n#acc_train = catboost_model.score(train_prepro,y_train) # Actually calculates R2 metric so ignore","2c8f06f1":"# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n","a35da32b":"acc_cv_catboost = np.min(cv_data['test-MSLE-mean'])\nprint(\"Best RMSLE metric score CV10: \", np.sqrt(acc_cv_catboost))","0c18935a":"train_prepro.head()","5079eb40":"train_sc.head()","7a8ada5b":"# Default parameters\nfrom catboost import CatBoostRegressor, Pool, cv\n\ncat_features = train_sc.columns\ncat_features = cat_features[36:]\n\ntrain_pool = Pool(train_sc, \n                  y_train,\n                  cat_features)\n\ncatboost_model_sc = CatBoostRegressor(iterations=1000,\n                                      task_type=\"GPU\",\n                                      devices=\"0:1\",\n                                      eval_metric = \"MSLE\",\n                                      loss_function='RMSE')\n\n# Fit CatBoost model\ncatboost_model_sc.fit(train_pool,\n                   plot=True)\n\n#acc_train = catboost_model.score(train_sc,y_train) # Actually calculates R2 metric so ignore","c95605ee":"# Set params for cross-validation\ncv_params = catboost_model_sc.get_params()\n\n# Run the cross-validation for 5-folds \ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=5,\n             plot=True)\n\nacc_cv_catboost = np.min(cv_data['test-RMSE-mean'])","c6272208":"acc_cv_catboost = np.min(cv_data['test-MSLE-mean'])\nprint(\"Best RMSLE metric score CV5: \", np.sqrt(acc_cv_catboost))","265209b3":"# Best submission yet Gradient Boosting Regressor (0.1257----5 fold Cross Validation)\n#gbrt.fit(train_prepro,y_train)\ny_pred = grid_search.predict(test_prepro)\nresults = sample.copy()\nresults[\"SalePrice\"] = y_pred\nresults.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","5e84ac20":"## Gradient Boosting Regressor","3fd36d7a":"## ElasticNet Regression","bf7178a3":"## CatBoostRegressor","25ca4265":"### For simplicity of the notebook and readability i've done the preprocessing and I import the final train and test sets.","70085bab":"### With scaled values","6218d4fb":"## Ridge Regression"}}