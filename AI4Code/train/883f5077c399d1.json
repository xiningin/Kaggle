{"cell_type":{"341e4218":"code","f7591615":"code","3f357007":"code","b81cefde":"code","abb04106":"code","c5b12268":"code","4d8d58eb":"code","eb3c6964":"code","9999c137":"code","c6f2526c":"code","84d4d37e":"code","57bfd058":"code","e9b7d4a6":"code","46bb5e62":"code","3dad287b":"code","7ff79e67":"code","ceebef91":"code","1724fe2a":"code","a11871b8":"code","7f3ba1e0":"code","f57b50ca":"code","6c1dbe98":"code","998a2b4d":"code","767fe663":"code","c993ada5":"code","81d4449e":"code","1cc5fc67":"code","9ea6e182":"code","c1f045b2":"code","7a78f4f6":"code","7bb39e1e":"code","bdc111bb":"code","2e348412":"code","582434ac":"code","53f80c5a":"code","fb34377d":"code","ab9fc8ba":"code","0191274a":"code","1f5dd2c8":"code","983c654f":"code","2ce56136":"code","f9b89892":"code","79f4691e":"code","99a44f00":"code","eb6a95fc":"code","fe6f1b51":"code","b65ca497":"markdown","f20efb3c":"markdown","28505750":"markdown","58a951b7":"markdown","fa4fd379":"markdown","f1a7eb22":"markdown","9daee27f":"markdown","f1679d7b":"markdown","a26209ad":"markdown","76677491":"markdown","04ab0d71":"markdown","a1102b05":"markdown","b4d89abd":"markdown","d6c532f1":"markdown","2a7af494":"markdown","4554699f":"markdown","8d60c0db":"markdown","eb611efa":"markdown","cb6744c7":"markdown","4d340ca9":"markdown","5bbad39e":"markdown","bbb8e1ec":"markdown","8fccda1b":"markdown","61ebbae6":"markdown","60caaaeb":"markdown","8fc4245f":"markdown","44f45d87":"markdown","78ad637c":"markdown","c25a0aa8":"markdown","a91d30d2":"markdown","003c6531":"markdown","957e1875":"markdown","c10aaeab":"markdown","6463b445":"markdown","a4bab3b6":"markdown","eac53b58":"markdown"},"source":{"341e4218":"import math\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport librosa\nimport librosa.display as lds","f7591615":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","3f357007":"fig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\nax[0].set(title = 'Piano')\nlds.waveplot(piano, sr=sample_rate, ax=ax[0])\n\nax[1].set(title = 'String')\nlds.waveplot(string, sr=sample_rate, ax=ax[1])\n\nax[2].set(title = 'Violin')\nlds.waveplot(violin, sr=sample_rate, ax=ax[2])\n\nplt.show()","b81cefde":"file_path = '..\/input\/wavfiles-of-instruments-audio\/12Electric_Guitar.wav'\n\nelectric_guitar1, sr1 = librosa.load(file_path, sr=10000) # 10kHz\nelectric_guitar2, sr2 = librosa.load(file_path, sr=1000)  # 1kHz\nelectric_guitar3, sr3 = librosa.load(file_path, sr=100)   # 100Hz","abb04106":"fig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\nax[0].set(title = 'Sample rate of 10kHz')\nlds.waveplot(electric_guitar1, sr=sr1, ax=ax[0])\n\nax[1].set(title = 'Sample rate of 1kHz')\nlds.waveplot(electric_guitar2, sr=sr2, ax=ax[1])\n\nax[2].set(title = 'Sample rate of 100Hz')\nlds.waveplot(electric_guitar3, sr=sr3, ax=ax[2])\n\nplt.show()","c5b12268":"# Create function for mask\ndef env_mask(wav, threshold):\n    # Absolute value\n    wav = np.abs(wav)\n    \n    # Point wise mask determination.\n    mask = wav > threshold\n    \n    return mask","4d8d58eb":"# Initialize mask\npiano_mask = env_mask(piano, 0.1)\nstring_mask = env_mask(string, 0.1)\nviolin_mask = env_mask(violin, 0.1)","eb3c6964":"# Plotting the new signals\n\nfig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\n# Visualize wave plots with mask applied.\n\nax[0].set(title = 'Piano')\nlds.waveplot(piano[piano_mask], sr=sample_rate, ax=ax[0])\n\nax[1].set(title = 'String')\nlds.waveplot(string[string_mask], sr=sample_rate, ax=ax[1])\n\nax[2].set(title = 'Violin')\nlds.waveplot(violin[violin_mask], sr=sample_rate, ax=ax[2])\n\nplt.show()","9999c137":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","c6f2526c":"def AE(signal, frame_length):\n    '''\n    Create the Amplitude Envelope of a given signal.\n    \n    Parameters:\n    ------------\n    - signal (numpy array):\n        The signal\/sound in question to be analyzed. \n    - frame_size (int):\n        Number of samples within each frame. Formally defined as K.\n    \n    Returns:\n    ---------\n    Array of amplitude envelope\n    '''\n    \n    AE = []\n    \n    # Calculate number of frames\n    num_frames = math.floor(signal.shape[0] \/ frame_length)\n    \n    for t in range(num_frames):\n        # Calculate bounds of each frame\n        # By doing this, our hop length is the same as the frame length\n        # Therefore, these frames are NOT overlapping.\n        lower = t*frame_length\n        upper = (t+1)*(frame_length-1)\n        \n        # Find maximum of each frame and add it to our array\n        AE.append(np.max(signal[lower:upper]))\n        \n    return np.array(AE)","84d4d37e":"fig, ax = plt.subplots(2, 3, figsize=(30, 15))\n\n# Piano\nax[0,0].set(title='Wave Form of Piano')\nlds.waveplot(piano, sr=sample_rate, ax=ax[0, 0])\nax[1,0].set(title = 'Amplitude Envelope of Piano')\nax[1,0].plot(AE(piano, 1024))\n\n# String\nax[0,1].set(title='Wave Form of String')\nlds.waveplot(string, sr=sample_rate, ax=ax[0, 1])\nax[1,1].set(title = 'Amplitude Envelope of String')\nax[1,1].plot(AE(string, 1024))\n\n# Violin\nax[0,2].set(title='Wave Form of Violin')\nlds.waveplot(violin, sr=sample_rate, ax=ax[0, 2])\nax[1,2].set(title = 'Amplitude Envelope of Violin')\nax[1,2].plot(AE(violin, 1024))","57bfd058":"# Define RMS for each instrument\nrms_piano = librosa.feature.rms(y=piano, frame_length=1024, hop_length=1024)\nrms_string = librosa.feature.rms(y=string, frame_length=1024, hop_length=1024)\nrms_violin = librosa.feature.rms(violin, frame_length=1024, hop_length=1024)","e9b7d4a6":"fig, ax = plt.subplots(2, 3, figsize=(30, 15))\n\n# Piano\nax[0,0].set(title='Wave Form of Piano')\nlds.waveplot(piano, sr=sample_rate, ax=ax[0, 0])\nax[1,0].set(title = 'RM Energy of Piano')\nax[1,0].plot(rms_piano.T) # Returned shape is (1,t) so we take the transpose.\n\n# String\nax[0,1].set(title='Wave Form of String')\nlds.waveplot(string, sr=sample_rate, ax=ax[0, 1])\nax[1,1].set(title = 'RM Energy of String')\nax[1,1].plot(rms_string.T)\n\n# Violin\nax[0,2].set(title='Wave Form of Violin')\nlds.waveplot(violin, sr=sample_rate, ax=ax[0, 2])\nax[1,2].set(title = 'RM Energy of Violin')\nax[1,2].plot(rms_violin.T)","46bb5e62":"# Define RMS for each instrument\nzcr_piano = librosa.feature.zero_crossing_rate(y=piano, frame_length=1024, hop_length=1024)\nzcr_string = librosa.feature.zero_crossing_rate(y=string, frame_length=1024, hop_length=1024)\nzcr_violin = librosa.feature.zero_crossing_rate(violin, frame_length=1024, hop_length=1024)","3dad287b":"fig, ax = plt.subplots(2, 3, figsize=(30, 15))\n\n# Piano\nax[0,0].set(title='Wave Form of Piano')\nlds.waveplot(piano, sr=sample_rate, ax=ax[0, 0])\nax[1,0].set(title = 'ZCR of Piano')\nax[1,0].plot(zcr_piano.T) # Returned shape is (1,t) so we take the transpose.\n\n# String\nax[0,1].set(title='Wave Form of String')\nlds.waveplot(string, sr=sample_rate, ax=ax[0, 1])\nax[1,1].set(title = 'ZCR of String')\nax[1,1].plot(zcr_string.T)\n\n# Violin\nax[0,2].set(title='Wave Form of Violin')\nlds.waveplot(violin, sr=sample_rate, ax=ax[0, 2])\nax[1,2].set(title = 'ZCR of Violin')\nax[1,2].plot(zcr_violin.T)","7ff79e67":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","ceebef91":"# Filter out dead noise\ndef trimmed(wav, threshold):\n    # Absolute value\n    wav = np.abs(wav)\n    \n    # Point wise boolean mask determination.\n    mask = wav > threshold\n    \n    return wav[mask]","1724fe2a":"# Filter sounds\npiano = trimmed(piano, 0.005)\nstring = trimmed(string, 0.005)\nviolin = trimmed(violin, 0.005)","a11871b8":"# Visualize wave forms\n\nfig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\nax[0].set(title = 'Piano')\nlds.waveplot(piano, sr=sample_rate, ax=ax[0])\n\nax[1].set(title = 'String')\nlds.waveplot(string, sr=sample_rate, ax=ax[1])\n\nax[2].set(title = 'Violin')\nlds.waveplot(violin, sr=sample_rate, ax=ax[2])\n\nplt.show()","7f3ba1e0":"def fft_components(sound):\n    # Find FFT\n    fft = np.fft.fft(sound)\n    \n    # Find magnitude\n    mag = np.abs(np.real(fft))\n    \n    # Find frequency\n    freq = np.linspace(0, sample_rate, len(mag))\n    \n    return mag, freq","f57b50ca":"# Using function from above to find components\np_mag, p_freq = fft_components(piano)\ns_mag, s_freq = fft_components(string)\nv_mag, v_freq = fft_components(violin)","6c1dbe98":"# Visualize the FFT\n\nfig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\nax[0].plot(p_freq, p_mag)\nax[0].set(title = 'Piano')\n\nax[1].plot(s_freq, s_mag)\nax[1].set(title = 'String')\n\nax[2].plot(v_freq, v_mag)\nax[2].set(title = 'Violin')\n\nplt.show()","998a2b4d":"p_half = int(len(p_mag) \/ 2)\ns_half = int(len(s_mag) \/ 2)\nv_half = int(len(v_mag) \/ 2)","767fe663":"# Visualize the FFT\n\nfig, ax = plt.subplots(1, 3, figsize=(30, 6), sharey=True)\n\nax[0].plot(p_freq[:p_half], p_mag[:p_half])\nax[0].set(title = 'Piano')\n\nax[1].plot(s_freq[:s_half], s_mag[:s_half])\nax[1].set(title = 'String')\n\nax[2].plot(v_freq[:v_half], v_mag[:v_half])\nax[2].set(title = 'Violin')\n\nplt.show()","c993ada5":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","81d4449e":"def to_decibles(signal):\n    # Perform Short Time Fourier Transformation of signal and take absolute value of results\n    stft = np.abs(librosa.stft(signal))\n    \n    # Convert to dB (db - Decibel, to measure the sound, if larger db then lound or the other way round)\n    D = librosa.amplitude_to_db(stft, ref=np.max) # Set reference value to the maximum value of stft.\n    \n    return D # Return converted audio signal","1cc5fc67":"# Function to plot the converted audio signal\ndef plot_spec(D, sr, instrument):\n    fig, ax = plt.subplots(figsize = (30, 5))\n    spec = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear', ax=ax)\n    ax.set(title = 'Spectrogram of ' + instrument)\n    fig.colorbar(spec)","9ea6e182":"plot_spec(to_decibles(piano), sample_rate, 'Piano')","c1f045b2":"plot_spec(to_decibles(violin), sample_rate, 'Violin')","7a78f4f6":"plot_spec(to_decibles(string), sample_rate, 'String')","7bb39e1e":"# Create function to convert Hz to Mels\ndef freqToMel(f):\n    return 1127 * math.log(1 + (f\/700))","bdc111bb":"# Vectorize function to apply to numpy arrays\nfreqToMelv = np.vectorize(freqToMel)","2e348412":"# Observing 0 to 10,000 Hz\nHz = np.linspace(0, 1e4) \n\n# Now we just apply the vectorized function to the Hz variable\nMel = freqToMelv(Hz) ","582434ac":"# Plotting the figure:\nfig, ax = plt.subplots(figsize = (6, 4))\nax.plot(Hz, Mel)\nplt.title('Hertz to Mel')\nplt.xlabel('Hertz Scale')\nplt.ylabel('Mel Scale')\nplt.show()","53f80c5a":"# Using env_mask\ndef env_mask(wav, threshold):\n    # Absolute value\n    wav = np.abs(wav)\n    \n    # Point wise mask determination.\n    mask = wav > threshold\n    \n    return wav[mask]","fb34377d":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","ab9fc8ba":"# Mask audio to trim out dead noise (simple data cleaning)\npiano = env_mask(piano, 0.005)\nstring = env_mask(string, 0.005)\nviolin = env_mask(violin, 0.005)","0191274a":"# Create Mel Spectrograms of sounds\npiano_spec = librosa.feature.melspectrogram(piano)\nstring_spec = librosa.feature.melspectrogram(string)\nviolin_spec = librosa.feature.melspectrogram(violin)","1f5dd2c8":"# Convert amplitudes to dB\np = librosa.amplitude_to_db(piano_spec)\ns = librosa.amplitude_to_db(string_spec)\nv = librosa.amplitude_to_db(violin_spec)","983c654f":"# Plot mel spectrograms\nfig, ax = plt.subplots(1, 3, figsize=(20, 4))\n\nax[0].set(title='Mel Spectrogram of Piano')\ni = librosa.display.specshow(p, ax=ax[0])\n\nax[1].set(title='Mel Spectrogram of String')\nlibrosa.display.specshow(s, ax=ax[1])\n\nax[2].set(title='Mel Spectrogram of Violin')\nlibrosa.display.specshow(v, ax=ax[2])\n\nplt.colorbar(i)","2ce56136":"# Take MFCCs of sounds:\np_mfcc = librosa.feature.mfcc(piano)\ns_mfcc = librosa.feature.mfcc(string)\nv_mfcc = librosa.feature.mfcc(violin)","f9b89892":"# Plot MFCCs\nfig, ax = plt.subplots(1,3, figsize = (20, 4))\n\nax[0].set(title='MFCCs of Piano')\ni = librosa.display.specshow(p_mfcc, x_axis='time', ax=ax[0])\n\nax[1].set(title='MFCCs of String')\nlibrosa.display.specshow(s_mfcc, x_axis='time', ax=ax[1])\n\nax[2].set(title='MFCCs of Violin')\nlibrosa.display.specshow(v_mfcc, x_axis='time', ax=ax[2])\n\nplt.colorbar(i)","79f4691e":"# From https:\/\/librosa.org\/doc\/latest\/generated\/librosa.filters.chroma.html#librosa.filters.chroma\n\nchromafb = librosa.filters.chroma(22050, 4096)\nfig, ax = plt.subplots(figsize=(20, 5))\nimg = librosa.display.specshow(chromafb, x_axis='linear', ax=ax)\nax.set(ylabel='Chroma filter', title='Chroma filter bank')\nfig.colorbar(img, ax=ax)","99a44f00":"piano, sample_rate = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Piano.wav')\nstring, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10String.wav')\nviolin, _ = librosa.load('..\/input\/wavfiles-of-instruments-audio\/10Violins .wav')","eb6a95fc":"# Create chromagrams\npiano_chroma = librosa.feature.chroma_stft(piano, sr=sample_rate)\nstring_chroma = librosa.feature.chroma_stft(string, sr=sample_rate)\nviolin_chroma = librosa.feature.chroma_stft(violin, sr=sample_rate)","fe6f1b51":"# Visualize the STFT chromagrams\nfig, ax = plt.subplots(1,3, figsize=(30, 6))\n\nimg = librosa.display.specshow(piano_chroma, y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(title = 'Piano Chromagram')\n\nlibrosa.display.specshow(string_chroma, y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(title = 'String Chromagram')\n\nlibrosa.display.specshow(violin_chroma, y_axis='chroma', x_axis='time', ax=ax[2])\nax[2].set(title = 'Violin Chromagram')\n\nfig.colorbar(img, ax=ax)\nplt.show()","b65ca497":"Within these examples, I will elaborate on what the feature is, how to formally define it, and show how to extract the features in Python.","f20efb3c":"### Amplitude Envelope\n\nThe `Amplitude Envelope (AE)` aims to extract the `maximum amplitude` within each frame and string them all together. It is important to remember that the amplitude represents the volume (or loudness) of the signal. First, we split up the signal into its constituent windows and find the maximum amplitude within each window. From there, we plot the maximum amplitude in each window along time.\n\nWe can use the AE for `onset detection`, or the detection of the beginning of a sound. In various speech processing applications this could be someone speaking or external noise, whereas in `Music Information Retrieval (MIR)` this could be the beginning of a note or instrument.\n\nThe main downfall of the AE is that is `not as robust to outliers` as `Root-Mean-Square Energy` which will we study soon.\n\nHere is how we can formalize this concept:\n\n$$\\text{AE}_t = \\max_{k = t \\cdot K}^{(t+1) \\cdot (K-1)} s(k) $$","28505750":"Loading the same audio with different `sample rates`.","58a951b7":"The second value returned by the `load` method is the `sample rate` and since we haven't specified sample rate while loading, the default sample rate will be 22050 i.e. 22.05kHz.","fa4fd379":"This function creates a mask to develop an envelope that trims out unnecessary dead noise. This allows us to focus on the significant portions of the audio.","f1a7eb22":"![](https:\/\/miro.medium.com\/max\/525\/1*uxQnCXu0qR1KeOOEZBq4-g.gif)\n\n`By Lucas V. Barbosa \u2014 Own work, Public Domain`\n\nIt is important to note that by using this transformation, we will be translating the audio from the time-domain to the frequency-domain. Here are some key points about the two:\n\n- The time domain looks at the variation of the signal\u2019s amplitude over time. This is useful for understanding its physical shape. In order to plot this, we need time on the x-axis and amplitude on the y-axis. The shape gives us a good idea of how loud or quiet the sound will be.\n- The frequency domain observes the constituent signals our recording is comprised of. By doing this, we can find a sort of \u201cfingerprint\u201d of the sound. In order to plot this, we need frequency on the x-axis and magnitude on the y-axis. The larger the magnitude, the more important that frequency is. The magnitude is simply the absolute value of our results from the FFT.\n\n### Discrete Fourier Transformation (DFT)\n\nThe DFT, which is what `numpy` builds off of, is as follows:\n\n$$ \\hat x_k = \\frac{1}{\\sqrt{N}} \\displaystyle\\sum_{n=0}^{N-1} x_n \\omega_{N}^{nk}  $$\n\nwhere\n\n$$ \\omega_{N} = \\exp \\left(\\frac{-2 \\pi i}{N} \\right) \\\\ k = 0, 1, \\dots, (N - 1)$$\n\n`Equations from Derek L. Smith, University of California, Santa Barbara.`\n\nWhen learning about the `Fourier Transformation`, you may see a continuous version \u2014 which utilizes an integral as opposed to a sum \u2014 suitably called the `Continuous Fourier Transform`. The discrete version of the transformation is generally used for computers\u2019 operations. In fact, even in the discrete form, most computers still lack the proper computational power to solve for the transformation as the raw equation below. The DFT has some nice properties that allow for computational ease, known as the `Fast Fourier Transformation`.\n\n### Fast Fourier Transformation (FFT)\n\nThe FFT is as follows:\n\n$$ \\hat x_k = \\frac{1}{\\sqrt{N}} \\displaystyle\\sum_{n=0}^{m-1} x_{2n} \\omega_{m}^{nk} + \\frac{\\omega_N}{\\sqrt{N}} \\displaystyle\\sum_{n = 0}^{m-1}x_{2n+1} \\omega_{m}^{nk}$$\n\n`Equation from Derek L. Smith, University of California, Santa Barbara.`\n\nDo not let the notation worry you. In the Discrete Fourier Transformation, you are dealing with a sum of products x, and omega. This equation splits sum of products into two \u2014 one along the odd indices and another along the even. This procedure above can be modeled much more efficiently with computer processes as opposed to the DFT.\n\nNow, let\u2019s visualize what the `FFT` looks like. In our application, we will be using the `numpy.fft.fft` function and apply it to our sound waves.","9daee27f":"### Null Data in Audio\n\nThere are many ways to treat null audio data in the `time domain`. However, the following approach often is the simplest. Given the signal and a minimum `threshold` for the `amplitude` of the signal:\n\n- Take the `absolute value` of each point in the signal\n- If the point is greater than the `threshold`, we keep it. Otherwise, we remove it.","f1679d7b":"## Wave Forms\n\nAn introduction to wave forms and dealing with null data.\n\n### Introduction\n\nAudio is an extremely rich data source. Depending on the `sample rate` (the number of points sampled per second to quantify the signal), one second of data could contain thousands of points. Scale this up to hours of recorded audio, and you can see how Machine Learning and Data Science nicely intertwine with signal processing techniques.\n\nThis article aims to break down what exactly wave forms are as well as utilize `librosa` in Python for analysis and visualizations alongside `numpy` and `matplotlib`.\n\n### Understanding Wave Forms\n\n`Waves` are repeated `signals` that oscillate and vary in amplitude, depending on their complexity. In the real world, waves are `continuous` and `mechanical` which is quite different from computers being `discrete and digital`.\n\nSo, how do we translate something continuous and mechanical into something that is discrete and digital?\n\nThis is where the `sample rate` defined earlier comes in. Say, for example, the sample rate of the recorded audio is `100`. This means that for every recorded second of audio, the computer will place 100 points along the `signal` in attempts to best `trace` the continuous curve. Once all the points are in place, a smooth curve joins them all together for humans to be able to visualize the sound. Since the recorded audio is in terms of `amplitude` and `time`, we can intuitively say that the `wave form operates in the time domain`.\n\nTo better understand what something like this sounds like, we will look at three sounds: a kick drum, a guitar, and a snare drum.","a26209ad":"---","76677491":"Second, I am going to define two functions; one that will perform all the necessary steps and output the processed signal, and another that will plot the spectrogram. Make sure to read through the comments and lines to understand the process in which this is done.","04ab0d71":"Now that the data is loaded in, let\u2019s visualize these sounds i.e. `wave forms`.","a1102b05":"As we can see, not only does the resolution of the amplitudes diminish significantly, but also the length in which the entire song dwindles too.\n\nIf you see in visualization that they are not as distinct as we would like it to be like `audio signals` do not just `suddenly disappear`, they in fact `fade out` until it is impossible to perceive. This means that in terms of audio, this constitutes as `null data`.","b4d89abd":"### Root-Mean-Square Energy\n\nAs mentioned previously, the Root-Mean-Square (RMS) Energy is quite similar to the AE. As opposed to onset detection, however, it attempts to perceive loudness, which can be used for event detection. Furthermore, it is much more robust against outliers, meaning if we segment audio, we can detect new events (such a a new instrument, someone speaking, etc.) much more reliably.\n\nThe formal definition of RMS Energy:\n$$\\text{RMS}_t = \\sqrt{\\frac{1}{K} \\cdot \\displaystyle\\sum^{(t+1) \\cdot (K-1)}_{k = t \\cdot K} s(k)^2}$$\n\nIf you are familiar with the concept of the Root Mean Square, this will not be too new to you. However, if you are not, do not worry.\n\nAs we window across our wave form, we square the amplitudes within the window and sum them up. Once that is complete, we will divide by the frame length, take the square root, and that will be the RMS energy of that window.\n\nTo extract the RMS, we can simply use `librosa.feature.rms`. Now, we visualize it:","d6c532f1":"## Fourier Transformations\n\nBreaking down a fundamental equation in signal processing\n\n### Introduction\n\nIn Wave Forms, we looked at what waves are, how to visualize them, and how to deal with null data. In this article, I aim to develop an intuition on what the Fourier Transformation is, why it is useful when studying audio, show mathematical proofs to make it computationally efficient, and visualize the results.","2a7af494":"### Conclusion\n\nAs a wrap-up for this article, you have now learned:\n- What the Mel Scale is and how it plays a role in human-like interpretation of audio\n- How to map the Mel Scale onto spectrograms\n- What MFCCs are, certain use cases of MFCCs, and how to develop them\n\nLeveraging Mel Spectrograms is a fantastic way to process audio such that various Deep Learning and Machine Learning problems can learn from the recorded sounds.","4554699f":"As we can see from the graph above, frequencies that are lower in Hz have a larger distance between them Mels, whereas frequencies that are higher in Hz have a smaller distance between them in Mels, reinforcing its human-like properties.\n\nNow that we have a good understanding of the Mel Scale\u2019s utility, let\u2019s use this intuition to develop Mel Spectrograms.\n\n### Mel Spectrograms\n\nMel Spectrograms are spectrograms that visualize sounds on the Mel scale as opposed to the `frequency domain`, as we saw previously. Now, I know what you are thinking, is it really that simple? Yes, it is.\n\nAs soon as the intuition of spectrograms are established, it makes learning various flavors of them very easy. All that is required is the new framework in which we develop our spectrograms under. I will assume that you know the underlying properties of how this is done. Developing Mel Spectrograms are even easier than their definition.","8d60c0db":"You can think of `thresholds` as a sort of parameter for the `recordings`. Different thresholds work differently for various sounds. Playing around with the threshold is a good way to see how and why this visualization changes.\n\nOnce the null data is beed removed from recordings (audio), it's much easier to see the personality in each sound like uniform shape, drowning out gradually with time, loud at beginning and drowns out with some remnants of sound remaining.\n\n### Conclusion\n\nThis concludes the basics of dealing with audio signals in Python with `librosa`.","eb611efa":"### Conclusion\n\nBy now, you should have an idea of how time feature extraction works, how it can be utilized in various audio based applications, and how to develop the feature extraction methods yourself. By leveraging the amplitude within specific windows, we open up numerous insights into various applications in MIR and ASR.","cb6744c7":"We can see how the pitch changes over time.\n\n### Conclusion\n\nBy now, you should have a good understanding of what pitch is, how we classify each pitch, what the chroma filter aims to do, and how we visualize chromagrams.","4d340ca9":"### Zero-Crossing Rate\n\nThe Zero-Crossing Rate (ZCR) aims to study the the rate in which a signal\u2019s amplitude changes sign within each frame. Compared to the previous two features, this one is quite simple to extract.\n\nThe formal definition of ZCR is the following:\n$$\\text{ZCR}_t = \\frac{1}{2} \\cdot \\displaystyle\\sum^{(t+1) \\cdot (K-1)}_{k = t \\cdot K} \\lvert \\text{sgn}(s(k)) - \\text{sgn}(s(k+1)) \\rvert $$\n\nFor MIR, this feature is relevant for identifying percussion sound as they often have fluctuating signals that can ZCR can detect quite well as well as pitch detection. However, this feature is generally used as a feature in speech recognition for voice activity detection.\n\nUsing `librosa`, we can extract the ZCR using `librosa.feature.zero_crossing_rate`.","5bbad39e":"# Learning from Audio\n\nStarting point for signal processing, spectograms and other topics for dealing with audio data while building deep learning models where the data is audio.\n\nThis notebook is a compilation of Learning from Audio series by [@AdamSabra](https:\/\/theadamsabra.medium.com\/) on Medium.com. GitHub repo for this can be found [here](https:\/\/github.com\/theadamsabra\/LearningfromAudio). \n\nIn this notebook the data used is different from AdamSabra's work and the dataset can be found [here](https:\/\/www.kaggle.com\/mayur1999\/wavfiles-of-instruments-audio). The dataset is by [@MayurShah](https:\/\/www.kaggle.com\/mayur1999) and its a collection of `.wav` audio files for different instruments.\n\n### Table of contents\n\n- Wave Forms\n- Time Domain Features\n- Fourier Transformations\n- Spectrograms\n- The Mel Scale, Mel Spectrograms, and Mel Frequency Cepstral Coefficients\n- Pitch and Chromagrams","bbb8e1ec":"Before beginning I would like to establish some notation:\n    \n- $s(k)$ refers to the Amplitude of the  $k^{th}$ sample.\n- $K$  is the frame size, or the number of samples within each frame.\n- $t$ represents the frame number.","8fccda1b":"### Conclusion\n\nBy now, you should be able to understand how spectrograms are created using the Short Time Fourier Transformation, as well as how to create them in Python. These representations of audio allow for various Deep Learning architectures to extract features much easier than wave form and even Fourier representations.","61ebbae6":"## Pitch and Chromagrams\n\nInterpreting pitch changes for Music Information Retrieval\n\n### Introduction\n\nNow that the idea of a spectrogram is fully understood, we want to delve deeper into various structures beyond the frequency over time. When studying the sound waves over the Mel Scale, we got a taste of this \u2014 especially with the MFCCs. While visualizing MFCCs are technically not spectrograms, the rough idea still holds.\n\nThis article will be a bit more focused towards Music Information Retrieval (MIR) however, in that we are going to study the changes in pitch over time. To do this, we are going to need to understand what pitch is, how it is notated, and how we can use the Fourier Transformation to determine the changes in pitch.\n\n### What is Pitch?\n\nPitch can be understood as a relative highness\/lowness of a sound. The higher the sound, the higher the pitch and the lower the sound, the lower the pitch. Simple enough, right? To fully understand pitch, we need to understand the pitch classes and octaves.\n\nPitch classes are the letters that are associated with each sound. All pitches fall under one of the 7 letters: A, B, C, D, E, F, and G. Let\u2019s assume we were playing the piano and we started at A. We would keep going up one white key until we hit the key G. As we ascend with each white key, we inevitably cycle through the letters over and over. However, when we hit the second A key after the G, it is important to note that it is not the same sound we started at originally. The pitch class is the same; it is the A key. However the sound is higher than before, indicating we are in the next (higher) octave.\n\nBefore diving into octaves, let\u2019s quickly cover the black keys. If we start at C and go up one black key, we hit what\u2019s called C# (pronounced C sharp.) If we look at the figure once more, we also see that it is also called D\u266d(pronounced D flat.) This is because the black key is also before D. Simply put, if we go up one black key, the note is \u201csharped\u201d and if we go down one black key, then the note is \u201cflattened.\u201d In terms of what humans hear, they are equivalent, so we would call these notes enharmonic. As for the case for when there is no black key between two white keys, you do not have to worry about that for this article as that involves more music theory than what is necessary.\n\nAn octave is the number specifying which group of pitches we are in. If we repeated our example knowing we were in A3, the next A key we hit is A4. Again, this should not be too hard to wrap your head around. The middle key of the keyboard is C4 and it is generally used as a reference for which octave you are in.\nIt is important to note that the number does not increase after every A value, but rather after every C value. This means that if we were to start at the middle C, or C4, the B key behind it is B3 and the B key after it is B4.\n\n### About Chromagrams\n\nNow that we understand pitch in Music, we can dive into chroma filters which acts as the basis of our chromagrams.\n\nThe Chroma filters can be derived from the Chroma filter bank. The filter bank aims to project all the energy of the recorded sound into 12 bins i.e. all the notes we saw, plus the minor\/major keys (which is also the black keys of the piano) irrespective of the octave it is in. By disregarding octave, we can then create a heat-map of how the pitch changes over time, which is an important aspect of MIR. For example, should you decide to teach an LSTM how to play music, these features would be very useful as they would tell the neural network how the pitch would likely change over time.\n\nLet\u2019s visualize the Chroma filter bank across various frequencies.","60caaaeb":"Now, with our functions defined, we can simply use `plot_spec` to graph the results!","8fc4245f":"### Conclusion\n\nIn this article, you now should be able to extract the Fourier Transformation of any complex signal of any type and find its unique fingerprint.","44f45d87":"Upon searching for a defined method in Python that could accomplish this task, I could not find it. Therefore, we will define it from scratch as it will be quite easy to do. The other feature extraction methods we will be looking at have already been defined in `librosa`, so we will be using those functions after formally defining them.\n\nIt is important to note that by the setup in this `for` loop, that we do not designate `hop length`. This means that when we create our upper and lower bounds, the windows do NOT overlap, making the `hop length` and the `frame length` the same.","78ad637c":"Again, there are some slight problems with the visualizations. However, this is a quite easy fix as the DFT has a symmetric property. This makes sense as our visualization on the left half mimics the visualizations on the right half. We will slice the arrays in half and visualize from there.","c25a0aa8":"### Fourier Transformation\n\n#### What is the Fourier Transformation? Why is it Important?\n\nWhen looking at the figure above, we can see various spikes of amplitudes creating a unique pattern to the unique sound. This is because all complex sounds \u2014 like the sounds of our voices \u2014 are really just a sum of many sine and cosine signals. The `gif` below presents a nice visualization of the objective:","a91d30d2":"## The Mel Scale, Mel Spectrograms, and Mel Frequency Cepstral Coefficients\n\nBreaking down the intuition for human-like audio representations\n\nBy now, we have developed a stronger intuition as to what spectrograms are, and how to create them. Simply put, spectrograms allow us to visualize audio and the pressure these sound waves create, thus allowing us to see the shape and form of the recorded sound.\n\nThe main aim of this article is to introduce a new flavor of spectrograms \u2014 one that is widely used in the Machine Learning space as it represents `human-like perception` very well.\n\n### Mel Scale\n\nBefore discussing `Mel Spectrograms`, we first need to understand what the Mel Scale is and why it is useful. The Mel Scale is a `logarithmic transformation` of a signal\u2019s frequency. The core idea of this transformation is that sounds of equal distance on the Mel Scale are perceived to be of `equal distance` to humans. What does this mean?\n\nFor example, most human beings can easily tell the difference between a 100Hz and 200Hz sound. However, by that same token, we should assume that we can tell the difference between 1000Hz and 1100Hz, right? Wrong.\n\nIt is actually much harder for humans to be able to differentiate between higher frequencies, and easier for lower frequencies. So, even though the distance between the two sets of sounds are the same, our perception of the distance is not. This is what makes the Mel Scale fundamental in Machine Learning applications to audio, as it mimics our own perception of sound.\n\nThe transformation from the Hertz scale to the Mel Scale is the following:\n\n$$m = 1127 \\cdot \\log \\left(1 + \\frac{f}{700} \\right) $$\n\nNote that `log` in this case refers to the natural logarithm (also denoted as ln.) If the logarithm were of base 10, the equation\u2019s coefficient (1127) would alter slightly. However, in this article, we will simply refer to the equation stated above.\n\nLet\u2019s visualize the relationship between `Hertz` and `Mels`:","003c6531":"By taking the dot product of our Fourier based spectrogram (by taking the STFT) and this filter bank, we can then map the songs in question onto the set of pitches we discussed prior. Luckily, thanks to librosa the function to create the chromagram has been created for us already. Note that we do not have to use the STFT on the audio as our basis, but that is outside the scope of this article.","957e1875":"Similar to our results in spectrograms, we can see how each sound takes a unique shape based off of the sound it actually produces.\n\n### Mel Frequency Cepstral Coefficients\n\n`Mel Frequency Cepstral Coefficients (MFCCs)` were originally used in various speech processing techniques, however, as the field of `Music Information Retrieval (MIR)` began to develop further adjunct to Machine Learning, it was found that MFCCs could represent timbre quite well.\n\nThe basic procedure to develop MFCCs is the following:\n- Convert from Hertz to Mel Scale\n- Take logarithm of Mel representation of audio\n- Take logarithmic magnitude and use Discrete Cosine Transformation\n- This result creates a spectrum over Mel frequencies as opposed to time, thus creating MFCCs\n\nIf the ML problem warrants MFCCs to be used, such as automatic speech recognition or denoising audio, the number of coefficients used is a hyperparameter of the model. Because of this, the number of MFCCs will vary based on the problem. However, for this example, we will use `librosa\u2019s` default 20 MFCCs. In `librosa`, we can do all of this and visualize the output in a just few lines of code:","c10aaeab":"## Time Domain Features\n\nGoing deeper in time.\n\n## Introduction\n\nWhile Deep Learning often utilizes processes in the frequency domain, there are still many relevant features to be leveraged within the time domain that are relevant to many Machine Learning techniques. Simply put, these features can be extracted and analyzed to understand the wave form\u2019s properties. When extracting features within the time-domain, we will generally study the amplitude of each sample. How we manipulate the amplitude gives us certain details about the signal in question.","6463b445":"Now, to visualize and compare the AE among different instruments:","a4bab3b6":"## Spectrograms\n\nVisualizing the structure of audio down to the millisecond.\n\n### Introduction\n\nWhen it comes to Machine Learning, or even Deep Learning, how the data is processed is fundamental to the model\u2019s training and testing performance. When working in the domain of audio, there are a few steps to understand before getting to this stage, but once you get there, learning from audio becomes a quite easy task.\n\nIn this article I aim to break down what exactly a spectrogram is, how it is used in the field Machine Learning, and how you can use them for whatever problem you are attempting to solve.\n\n### What is a Spectrogram?\n\nYou can think of spectrograms as pictures of audio. Kind of weird, I know, but you should strengthen this intuition as much as you can. The spec portion in spectrogram comes from `spectrum` and the color-bar you see on the right of the figure is just that. What is the spectrum of? The frequencies in which the audio has.\n\nWith all of this information in mind, let me formalize the definition.\n\n> A spectrogram is a figure which represents the spectrum of frequencies of a recorded audio over time.\n\nThis means that as we get brighter in color in the figure, the sound is heavily concentrated around those specific frequencies, and as we get darker in color, the sound is close to empty\/dead sound. This allows us to get a good understanding of the shape and structure of the audio without even listening to it! This is where the power of spectrograms come into play for various ML\/DL models.\n\n### How to Create Spectrograms\n\nNow a question arises, how do we calculate the spectrograms? The answer to this question is much simpler than expected.\n\n- Split the audio into overlapping chunks, or windows.\n- Perform the `Short Time Fourier Transformation` on each window. Remember to take its absolute value!\n- Each resulting window has a vertical line representing the magnitude vs frequency.\n- Take the resulting window and convert to decibels. This gives us a rich image of the sound\u2019s structure.\n- Finally, we lay out these windows back into the length of the original song and display the output.\n\nNow that we have a decent understanding of what spectrograms are exactly, let\u2019s learn how to retrieve them from sounds in Python! Using functions in `librosa`, we can have this be done for us with little to no effort.","eac53b58":"Loading the `.wav` files using librosa. It will load the audio file in as a numpy array."}}