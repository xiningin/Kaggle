{"cell_type":{"be49cea8":"code","7169c614":"code","5392fe89":"code","9f4101a0":"code","354f97f0":"code","ccb45eca":"code","55cb546c":"code","61175347":"code","9677512a":"code","7dabbed6":"code","abd8fc89":"code","ec2966c2":"code","40db9701":"code","7348d8e9":"code","bf0f2152":"code","f4d87911":"code","89aae9e1":"code","16301500":"code","1c082b40":"code","2e085127":"code","06da5b86":"code","0c999b32":"code","06bc40df":"code","69b5076d":"code","0cf87105":"code","e8d1abde":"code","5464c624":"code","52cdbfb1":"code","3934786b":"code","1cbd2d52":"code","3a7c79ce":"code","e4ff2265":"markdown","56fdb9ea":"markdown","6e053bad":"markdown","40031014":"markdown","a8dbfc45":"markdown","1c4ae421":"markdown","3480f48f":"markdown","a1f57c95":"markdown","252091b4":"markdown","d2e1e965":"markdown","d9732f5b":"markdown","971292df":"markdown","b2070dc1":"markdown","2af19fa7":"markdown","99ab4687":"markdown","390878a7":"markdown","52cff1d8":"markdown"},"source":{"be49cea8":"import numpy as np\nimport pandas as pd\nimport string\nimport re\nfrom matplotlib import pyplot as plt\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords","7169c614":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5392fe89":"# Let's use the Natural Language ToolKit: https:\/\/www.nltk.org\/\nnltk.download('stopwords')","9f4101a0":"train_filepath = '\/kaggle\/input\/nlp-getting-started\/train.csv'\ndf_train = pd.read_csv(train_filepath, index_col='id')\ndf_train.head()","354f97f0":"# Percentage of missing values\ndf_train.isnull().sum() \/ len(df_train) * 100","ccb45eca":"disaster_ratio = df_train.groupby('target').count()\ndisaster_ratio.plot(kind='pie', y='text', labels=['Not Disaster', 'Disaster'])\n\nnot_disater = disaster_ratio.iloc[0]['text'] \/ len(df_train) * 100\nprint(\"Disaster :\", 100-not_disater, \"%\")\nprint(\"Not Disaster :\", not_disater, \"%\")","55cb546c":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\nstopwords_english = stopwords.words('english') \nstemmer = PorterStemmer()","61175347":"def process_tweet(tweet, tokenizer=tokenizer, stopwords_english=stopwords_english, stemmer=stemmer):\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)  # remove old style retweet text \"RT\"\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)  # remove hyperlinks\n    tweet = re.sub(r'#', '', tweet)  # remove hashtags\n\n    tweet_tokens = tokenizer.tokenize(tweet)\n    \n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and word not in string.punctuation):\n            tweets_clean.append(word)\n            \n    tweets_stem = [] \n    for word in tweets_clean:\n        stem_word = stemmer.stem(word)\n        tweets_stem.append(stem_word)\n        \n    return tweets_stem","9677512a":"def count_tweets(result, tweets, ys):\n    '''\n    Input:\n        result: a dictionary that will be used to map each pair to its frequency\n        tweets: a list of tweets\n        ys: a list indicating if each tweet is a disaster or not (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    '''\n\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            result[pair] = result.get(pair, 0) + 1\n\n    return result","7dabbed6":"def train_naive_bayes(freqs, train_x, train_y):\n    '''\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of tweets\n        train_y: a list of labels correponding to the tweets (0,1)\n    Output:\n        logprior: the log prior\n        loglikelihood: the log likelihood of you Naive bayes equation\n    '''\n    loglikelihood = {}\n    logprior = 0\n\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    N_ndis = N_dis = 0\n    for pair in freqs.keys():\n        if pair[1] > 0:\n            N_dis += freqs.get(pair, 0)\n        else:\n            N_ndis += freqs.get(pair, 0)\n\n    D = len(train_x)\n    D_dis = np.sum(train_y)\n    D_ndis = D - D_dis\n\n    logprior = np.log(D_dis) - np.log(D_ndis)\n\n    for word in vocab:\n        freq_ndis = freqs.get((word, 0), 0)\n        freq_dis = freqs.get((word, 1), 0)\n\n        p_w_ndis = (freq_ndis + 1) \/ (N_ndis + V)\n        p_w_dis = (freq_dis + 1) \/ (N_dis + V)\n\n        loglikelihood[word] = np.log(p_w_dis \/ p_w_ndis)\n\n    return logprior, loglikelihood","abd8fc89":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n    '''\n    Input:\n        tweet: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n\n    '''\n    word_l = process_tweet(tweet)\n\n    p = 0\n    p += logprior\n\n    for word in word_l:\n        if word in loglikelihood:\n            p += loglikelihood.get(word, 0)\n\n    return p","ec2966c2":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        test_x: A list of tweets\n        test_y: the corresponding labels for the list of tweets\n        logprior: the logprior\n        loglikelihood: a dictionary with the loglikelihoods for each word\n    Output:\n        accuracy: (# of tweets classified correctly)\/(total # of tweets)\n    \"\"\"\n    accuracy = 0\n\n    y_hats = []\n    for tweet in test_x:\n        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n            y_hat_i = 1\n        else:\n            y_hat_i = 0\n\n        y_hats.append(y_hat_i)\n\n    error = np.mean(np.abs(y_hats - test_y))\n    accuracy = 1 - error\n\n    return accuracy","40db9701":"freqs = count_tweets({}, df_train['text'], df_train['target'])\nlogprior, loglikelihood = train_naive_bayes(freqs, df_train['text'], df_train['target'])\n\nprint(logprior)\nprint(len(loglikelihood))","7348d8e9":"print(\"Naive Bayes accuracy:\", test_naive_bayes(df_train['text'], df_train['target'], logprior, loglikelihood) * 100, \"%\")","bf0f2152":"print('Truth Predicted Tweet')\nfor x, y in zip(df_train['text'], df_train['target']):\n    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n    if y != (np.sign(y_hat) > 0):\n        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n            process_tweet(x)).encode('ascii', 'ignore')))","f4d87911":"df_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","89aae9e1":"df_test['target'] = df_test['text'].apply(lambda tweet : int(np.sign(naive_bayes_predict(tweet, logprior, loglikelihood)) > 0))","16301500":"submission = df_test[['id', 'target']]\nsubmission.to_csv('nb_submission.csv', index=False)  # Public score 0.78976","1c082b40":"# Official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","2e085127":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","06da5b86":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","0c999b32":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","06bc40df":"# Remove target column from df_test\nif 'target' in df_test.columns.tolist():\n    del df_test['target']","69b5076d":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","0cf87105":"train_input = bert_encode(df_train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(df_test.text.values, tokenizer, max_len=160)\ntrain_labels = df_train.target.values","e8d1abde":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","5464c624":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","52cdbfb1":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\nhistory = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=2,\n    callbacks=[checkpoint],\n    batch_size=16\n)","3934786b":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","1cbd2d52":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","3a7c79ce":"df_test['target'] = test_pred.round().astype(int)\n\nsubmission = df_test[['id', 'target']]\nsubmission.to_csv('bert_submission.csv', index=False)","e4ff2265":"### Prediction function","56fdb9ea":"### Basic exploration","6e053bad":"## Data\n\n### Import","40031014":"For an advanced EDA check [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) from Gunes Evitan","a8dbfc45":"### Test","1c4ae421":"# BERT\n\nBidirectional Encoder Representations from Transformers (BERT) is a technique for NLP pre-training developed by Google in 2018. The research paper, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, can be found on [arXiv](https:\/\/arxiv.org\/abs\/1810.04805v2)\n\nThis part is based on the notebook [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) from xhlulu","3480f48f":"### Submission","a1f57c95":"## Model","252091b4":"### Prediction","d2e1e965":"## Data transformation\n\nLet's:\n- Remove noise i.e removing common \/ stop words\n- Remove stock market tickers, retweet symbols, hyperlinks, and hashtags\n- Remove all the punctuation\n- Use stemming to only keep track of one variation of each word","d9732f5b":"# Naive Bayes vs BERT","971292df":"## Prediction & Submission","b2070dc1":"## Load and Preprocess\n\n* Load BERT from the Tensorflow Hub\n* Load tokenizer from the bert layer\n* Encode the text into tokens, masks, and segment flags","2af19fa7":"## Model\n\nNaive Bayes is a probalistic technique for constructing classifiers. One of its major advantages is to take into account prior knowledge i.e. the prior probability distribution  which might be based on our knowledge of frequencies in the larger population, or on frequency in the training set. More information on this technique can be found [here](http:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)","99ab4687":"# Naive Bayes","390878a7":"### Train","52cff1d8":"## Error analysis\n\nLet's see misclassified tweets"}}