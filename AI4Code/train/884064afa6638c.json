{"cell_type":{"9297867b":"code","a9f6b0b5":"code","630253e3":"code","be912573":"code","1d1c720c":"code","8f6c075b":"code","f1004723":"code","13d648cc":"code","47bbe8ec":"code","a838e7fc":"code","019f7f7f":"code","8f4a3b44":"code","6a0cdb25":"code","2f2933f9":"code","343c0670":"code","1b27c5b1":"code","f583da1b":"code","99526c9d":"code","8014b490":"code","dad48fd2":"code","fe44a72a":"code","7f722cf0":"code","eb305f4d":"code","75358e8a":"code","c43ff5b6":"code","ea0a90e5":"code","1786abd9":"code","fac598b3":"code","2209fba0":"code","65ec4e60":"code","503a6ae2":"code","dee0e14e":"code","617d7ac1":"code","1be5afb8":"code","e2fca251":"code","291fd934":"code","b99bbe5f":"markdown","23807906":"markdown","68b90d9d":"markdown","ed28e00d":"markdown","5702303e":"markdown","46c526ea":"markdown","b2414672":"markdown","50592263":"markdown","59768c9c":"markdown","fa63cb09":"markdown","f882b722":"markdown","2073f5c5":"markdown","3f97e887":"markdown","74b1a194":"markdown","07faca23":"markdown","f1c2ac39":"markdown"},"source":{"9297867b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","a9f6b0b5":"import re\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS","630253e3":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom sklearn.model_selection import train_test_split","be912573":"df_twitter =  pd.read_csv(\"..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv\")\ndf_reddit = pd.read_csv(\"..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Reddit_Data.csv\")","1d1c720c":"df_twitter.head()","8f6c075b":"df_reddit.head()","f1004723":"df_twitter.rename(columns={\"clean_text\": \"comment\",\"category\":\"label\"}, inplace = True)\ndf_reddit.rename(columns={\"clean_comment\": \"comment\",\"category\":\"label\"}, inplace = True)","13d648cc":"df_twitter = df_twitter.dropna()\ndf_reddit = df_reddit.dropna()","47bbe8ec":"punct = string.punctuation\npunct","a838e7fc":"stopWords = stopwords.words('english')","019f7f7f":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()","8f4a3b44":"def cleanData(text):\n    \n    # To convert the all uppercase to lowercase\n    text = text.lower()\n    \n    # This is a reguglar expression to replace anything char that is not alphabet or numeric.\n    text = re.sub(r\"[^A-Za-z0-9]\",' ', text)\n    \n    # The above regular expression itself will take care of punctuation, below is an alternative to remove only punctuation.\n    text = ''.join([char for char in text if char not in punct])\n    \n    # This will remove the stopwords and lemmatize the remaining word to its root word.\n    text = [wn.lemmatize(word) for word in text.split(' ') if ((word not in stopWords) & len(word)!=0)]\n    \n    return ' '.join(text)","6a0cdb25":"df_twitter['comment'] = df_twitter['comment'].apply(cleanData)\ndf_reddit['comment'] = df_reddit['comment'].apply(cleanData) ","2f2933f9":"def find_len(txt):\n    return len(txt.split())","343c0670":"df_twitter['Txt_len'] = [find_len(txt) for txt in df_twitter['comment']]\ndf_reddit['Txt_len'] = [find_len(txt) for txt in df_reddit['comment']]","1b27c5b1":"df_twitter.Txt_len.mean()","f583da1b":"df_reddit.Txt_len.mean()","99526c9d":"new_data = [df_twitter, df_reddit]\ndf_combine_data = pd.concat(new_data)","8014b490":"df_combine_data","dad48fd2":"data_leght_1_15 = df_combine_data[df_combine_data.Txt_len <= 6]\ndata_leght_15_30 = df_combine_data[df_combine_data.Txt_len > 6]","fe44a72a":"def split_data_by_leght(data_leght):\n    X_leght = data_leght['comment']\n    y_leght = data_leght['label'] + 1\n    return X_leght, y_leght","7f722cf0":"X_leght_15, y_leght_15 = split_data_by_leght(data_leght_1_15)\nX_leght_30, y_leght_30 = split_data_by_leght(data_leght_15_30)","eb305f4d":"print(\"size lenght <= 15: {}\".format(len(X_leght_15)))\nprint(\"size lenght > 15: {}\".format(len(X_leght_30)))","75358e8a":"X_leght_15, y_leght_15 = X_leght_15[:76000],y_leght_15[:76000]\nX_leght_30, y_leght_30 = X_leght_30[:76000],y_leght_30[:76000]","c43ff5b6":"def split_train_test(X, y):\n    comment=list(X.astype(str))\n    sentiment=list(y)\n    X_train, X_test, y_train, y_test = train_test_split(comment, sentiment, test_size=0.33, random_state=42)\n    sentiment_train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train ))\n    sentiment_test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test ))\n    return sentiment_train_ds, sentiment_test_ds","ea0a90e5":"sentiment_train_ds_15, sentiment_test_ds_15 = split_train_test(X_leght_15, y_leght_15 )\nsentiment_train_ds_30, sentiment_test_ds_30 = split_train_test(X_leght_30, y_leght_30)","1786abd9":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64","fac598b3":"train_dataset_15 = sentiment_train_ds_15.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset_15 = sentiment_test_ds_15.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset_30 = sentiment_train_ds_30.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset_30 = sentiment_test_ds_30.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","2209fba0":"def encoder(data):\n    VOCAB_SIZE = 1000\n    encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n    encoder.adapt(data.map(lambda text, label: text))\n    return encoder","65ec4e60":"def model_init(encoder, model):\n    model = tf.keras.Sequential([\n        encoder,\n        tf.keras.layers.Embedding(\n            input_dim=len(encoder.get_vocabulary()),\n            output_dim=64,\n            # Use masking to handle the variable sequence lengths\n            mask_zero=True),\n        model,\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(3, activation = 'softmax')\n    ])\n    \n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer='adam',\n              metrics=['accuracy'])\n    return model","503a6ae2":"encoder = encoder(train_dataset_15)\ncell = tf.keras.layers.LSTM(64)\nLSTM_model = model_init(encoder, cell)\nhistory = LSTM_model.fit(train_dataset_15, epochs=10,validation_data=train_dataset_15,validation_steps=30)","dee0e14e":"train_dataset_15","617d7ac1":"cell = tf.keras.layers.GRU(64)\nGRU_model = model_init(encoder, cell)\nhistory = GRU_model.fit(train_dataset_15, epochs=10,validation_data=train_dataset_15,validation_steps=30)","1be5afb8":"encoder = encoder(train_dataset_30)\n","e2fca251":"cell = tf.keras.layers.LSTM(64)\nLSTM_model = model_init(encoder, cell)\nhistory = LSTM_model.fit(train_dataset_30,  epochs=30,validation_data=test_dataset_30,validation_steps=30)","291fd934":"cell = tf.keras.layers.GRU(64)\nLSTM_model = model_init(encoder, cell)\nhistory = LSTM_model.fit(train_dataset_30, epochs=30,validation_data=test_dataset_30,validation_steps=30)","b99bbe5f":"#### LSTM cell with lenght smaller 15","23807906":"### Split data","68b90d9d":"### Drop Nan","ed28e00d":"#### Data size","5702303e":"### Remove punctuation","46c526ea":"### Stop Word","b2414672":"## Clean data","50592263":"## Mean Legnht","59768c9c":"## Compare lenght effect to LSTM, RNN, GRU","fa63cb09":"## RNN MODEL","f882b722":"#### GRU cell with lenght smaller 15","2073f5c5":"### Combine Data","3f97e887":"### Compare Method","74b1a194":"#### lstm cell with lenght larger 15","07faca23":"# Set Up","f1c2ac39":"#### RNN cell basic with lenght smaller 15"}}