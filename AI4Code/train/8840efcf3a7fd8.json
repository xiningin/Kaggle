{"cell_type":{"c1294bcc":"code","f42dc15d":"code","7018b8c6":"code","ac52d3cd":"code","6bd77d21":"code","9f0cb212":"code","4ed98721":"code","96a7501e":"code","97822139":"code","681a84c8":"code","e92b502c":"code","8e75c9d6":"code","a0c82954":"code","df2ba624":"code","909b30b8":"code","10ae601e":"code","9aa2e80b":"code","7ed7a091":"code","95740028":"code","2781fded":"code","225fc2de":"code","6931e176":"code","05735f3b":"code","8c845945":"code","ad26c476":"code","3b863753":"code","541e12d5":"code","f155eb36":"code","347efc1e":"code","f607a8c7":"code","e62f1f6d":"code","8406bae5":"code","55ce37a6":"code","eb5996a7":"code","adef40a6":"code","500d9df2":"code","6597befb":"code","67a73b63":"code","80bccc65":"code","473ebb5d":"code","278f6c8f":"code","fc8ec128":"code","ab248730":"code","879b0931":"code","bc01d5c3":"code","5b3ca5b5":"code","ef6b0bd0":"code","29025ef4":"code","3f2db530":"code","3f6cd7ce":"code","4d2cb6d0":"code","e135c2b6":"code","bc1c685f":"code","de1aea5d":"code","12179968":"code","3f62bf8c":"code","a86d6741":"code","aba3848a":"code","8cff421d":"code","29443293":"code","a56709a0":"code","093d6ec6":"code","15210890":"code","723136d0":"code","2357730a":"code","39086e7c":"code","5a68fe1c":"code","96356812":"code","0f8e0b7e":"code","f50ca9c4":"code","9f800571":"code","6bf4cdfb":"code","15d06a1c":"code","41ac102c":"code","4799f0fb":"code","de64e710":"code","2bab2811":"code","97307660":"code","68ebf69c":"code","2ad59692":"code","88721d59":"code","75f55076":"code","836cd3b1":"code","0e5e4ce9":"code","d3ad3c79":"code","46f41261":"code","902420e0":"code","f4988e00":"code","06a3dd01":"code","89177380":"code","db7cacc7":"code","6da6c529":"code","d8d9ed5d":"code","010ccbb6":"code","f087aabe":"code","0b82270d":"markdown","a020d9ae":"markdown","a389c5d9":"markdown","4b84820c":"markdown","cf6bb705":"markdown","79d15148":"markdown","42021e1b":"markdown","37556393":"markdown","7e3f62e6":"markdown","14f72f98":"markdown","c9cf1153":"markdown","960ed231":"markdown","28527116":"markdown","5104a870":"markdown","648652cf":"markdown","c08bbbe3":"markdown","858c537c":"markdown","c944824e":"markdown","de1c5ea5":"markdown","8ba23d06":"markdown","9ff815de":"markdown","e798f107":"markdown","b4e73a2a":"markdown","4c12fa62":"markdown","7e38937e":"markdown","4058deef":"markdown","f746ce00":"markdown","d797e49c":"markdown","a87d9461":"markdown","da881e44":"markdown","44f54a8b":"markdown","db49b6ab":"markdown","8f17dfe8":"markdown","968703db":"markdown","45c3b9c6":"markdown","2719e4f0":"markdown","2bb50840":"markdown","b0eff9ed":"markdown","5b543653":"markdown","ca842951":"markdown","8b27b7be":"markdown","f14df8b2":"markdown","77897969":"markdown","aff2e288":"markdown","59eea3d0":"markdown","da81973d":"markdown","ebd3cb81":"markdown","ae8abe86":"markdown","96306192":"markdown","a0b5d7a0":"markdown","78cb1111":"markdown","0d89d8d5":"markdown","16220060":"markdown","c3067221":"markdown","7d704e1b":"markdown","e64a8a01":"markdown","3bf0ebf0":"markdown","b0fa163d":"markdown","1d6c60d9":"markdown","ba3b1887":"markdown","3b7f78d8":"markdown","3993d143":"markdown","1c078e35":"markdown","6aba2333":"markdown","406eb481":"markdown","750db2b7":"markdown","eb179e35":"markdown","4c71feee":"markdown","69c861b5":"markdown","50aa4587":"markdown","af9b03d5":"markdown","c3de5b26":"markdown","7d6fe1a7":"markdown","ebf67571":"markdown","d51d1eac":"markdown","51aa6efb":"markdown","44531b3c":"markdown","e7c8af01":"markdown","69fb9678":"markdown","29b42de7":"markdown"},"source":{"c1294bcc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport xgboost as xgb # (eventually not used)\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nnp.random.seed(seed = 2021)\nfrom itertools import product\nimport shap\nfrom tqdm import tqdm\nimport copy\nimport gc","f42dc15d":"!pip install pickle5\nimport pickle5 as pickle","7018b8c6":"print(np.__version__)\nprint(pd.__version__)\nprint(sns.__version__)\nprint(lgb.__version__)\nprint(xgb.__version__)","ac52d3cd":"pd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 160)","6bd77d21":"with open(\"..\/input\/pfs-data3\/data3.pickle\", \"rb\") as f:\n    data3 = pickle.load(f)","9f0cb212":"gc.collect()","4ed98721":"data3.info()","96a7501e":"data3.describe()","97822139":"class Model():\n    def train(self, x_tr, y_tr, x_va, y_va):\n        pass\n    \n    def predict(self, x_test):\n        pass","681a84c8":"class RidgeModel(Model):\n    def __init__(self, normalize = False):\n        self.normalize = normalize\n    \n    def train(self, x_tr, y_tr, x_va, y_va):\n        x_tr = x_tr.reset_index()\n        x_va = x_va.reset_index()\n        id_tr = x_tr.index\n        id_va = x_va.index\n        x = pd.concat([x_tr, x_va])\n        y = pd.concat([y_tr, y_va])\n        x = x.set_index('index')\n        \n        regressor = RidgeCV(cv = [(id_tr, id_va)], normalize = self.normalize).fit(x, y)\n        self.regressor = regressor\n        \n    def predict(self, x_test): \n        pred_test = self.regressor.predict(x_test)\n        return pred_test","e92b502c":"class LassoModel(Model):\n    def __init__(self, positive = True):\n        self.positive = positive\n    \n    def train(self, x_tr, y_tr, x_va, y_va):\n        x_tr = x_tr.reset_index()\n        x_va = x_va.reset_index()\n        id_tr = x_tr.index\n        id_va = x_va.index\n        x = pd.concat([x_tr, x_va])\n        y = pd.concat([y_tr, y_va])\n        x = x.set_index('index')\n        \n        regressor = LassoCV(cv = [(id_tr, id_va)], n_jobs = -1, positive = self.positive).fit(x, y)\n        self.regressor = regressor\n        \n    def predict(self, x_test): \n        pred_test = self.regressor.predict(x_test)\n        return pred_test","8e75c9d6":"class LgbModel(Model):\n    def __init__(self, lgb_params, num_boost_round = 10000, early_stopping_rounds = 1000, verbose_eval = 1000):\n        self.lgb_params = lgb_params\n        self.num_boost_round = num_boost_round\n        self.early_stopping_rounds = early_stopping_rounds\n        self.verbose_eval = verbose_eval\n    \n    def train(self, x_tr, y_tr, x_va, y_va):\n        # validation is necessary for early stopping.\n        train_set = lgb.Dataset(data = x_tr, label = y_tr)\n        valid_set = lgb.Dataset(data = x_va, label = y_va)\n        bst = lgb.train(\n            params = self.lgb_params,\n            train_set = train_set,\n            num_boost_round = self.num_boost_round,\n            valid_sets = [valid_set, train_set],\n            valid_names = ['va', 'tr'],\n            early_stopping_rounds = self.early_stopping_rounds,\n            verbose_eval = self.verbose_eval\n        )\n        self.bst = bst\n        \n    def predict(self, x_test): \n        pred_test = self.bst.predict(x_test, num_iteration = self.bst.best_iteration)\n        return pred_test","a0c82954":"class XgbModel(Model):\n    def __init__(self, xgb_params, num_boost_round = 100, early_stopping_rounds = 10, verbose_eval = 10):\n        self.xgb_params = xgb_params\n        self.num_boost_round = num_boost_round\n        self.early_stopping_rounds = early_stopping_rounds\n        self.verbose_eval = verbose_eval\n    \n    def train(self, x_tr, y_tr, x_va, y_va):\n        x_tr_dmatrix = xgb.DMatrix(x_tr, label = y_tr)\n        x_va_dmatrix = xgb.DMatrix(x_va, label = y_va)\n        # validation is necessary for early stopping.\n        bst = xgb.train(\n            params = self.xgb_params,\n            dtrain = x_tr_dmatrix,\n            num_boost_round = self.num_boost_round,\n            evals = [(x_tr_dmatrix, 'tr'), (x_va_dmatrix, 'va')],\n            early_stopping_rounds = self.early_stopping_rounds,\n            verbose_eval = self.verbose_eval\n        )\n        self.bst = bst\n        \n    def predict(self, x_test): \n        x_test_dmatrix = xgb.DMatrix(x_test)\n        pred_test = self.bst.predict(x_test_dmatrix, ntree_limit = self.bst.best_ntree_limit)\n            \n        return pred_test","df2ba624":"def train(data, models = [], features = [], start_block = 14, target_block = 34, block_column = 'date_block_num', target_column = 'target'):  \n    trained_models = {}\n    for (model_name, model) in models.items():\n        trained_models[model_name] = {}\n    \n    for block in tqdm(range(start_block, target_block + 1)):\n        # train the model for the block with 'n == block' using blocks with 'n < block'.\n        # make sure that the model does not use the block with 'n == block'.\n        id_tr = (data[block_column] < block - 1) & (data[block_column] >= start_block - 2)\n        id_va = (data[block_column] == block - 1)\n        x_tr, y_tr = data.loc[id_tr, features], data.loc[id_tr, target_column]\n        x_va, y_va = data.loc[id_va, features], data.loc[id_va, target_column]\n\n        for (model_name, model) in models.items():\n            print(f'# {block}, {model_name}')\n            model.train(x_tr, y_tr, x_va, y_va)\n            trained_models[model_name][block] = copy.copy(model)\n    \n    return trained_models","909b30b8":"def test(data, trained_models = {}, features = [], target_block = 34, block_column = 'date_block_num', target_column = 'target'):  \n    data_layer = data.loc[:, [block_column, target_column]]\n    scores = {}\n    submissions = {}\n    \n    for (model_name, models) in tqdm(trained_models.items()):\n        scores[model_name] = []\n        for (block, model) in models.items():\n            # predict for the block with 'n == block'.\n            id_test = (data[block_column] == block)\n            x_test, y_test = data.loc[id_test, features], data.loc[id_test, target_column]\n            pred_test = model.predict(x_test)\n            data_layer.loc[id_test, f'pred_{model_name}'] = pred_test\n            if block < target_block:\n                score = np.sqrt(metrics.mean_squared_error(y_true = y_test, y_pred = pred_test.clip(0, 20)))\n                scores[model_name].append(score)\n            else:\n                submissions[model_name] = pred_test.clip(0, 20)\n        scores[model_name] = pd.Series(scores[model_name], index = list(models.keys())[:-1])\n        scores[model_name].name = model_name\n            \n    score_table = pd.concat(scores, axis = 1)\n    corr_matrix = data_layer.drop(columns = block_column).corr()\n    submissions = pd.DataFrame(submissions)\n    \n    return (data_layer, score_table, corr_matrix, submissions)","10ae601e":"data_layer_1 = data3.copy()\ndel data3\ngc.collect()","9aa2e80b":"models_1A = {}\n\nfor d in [3, 4, 5]:\n    lgb_params = {\n        'objective': 'regression',\n        'metric': ['rmse'],\n        'learning_rate': 0.1,\n        'num_leaves': 2 ** d,\n        'min_data_in_leaf': 20,\n        'max_depth': -1,\n        'seed': 2021\n    }\n    models_1A[f'lgb_1A_d{d}'] = LgbModel(lgb_params)\n    \nfeatures_1A = ['year', 'month', 'shop_id', 'item_id', 'item_category_id'] \\\n    + [f'item_count_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'item_price_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'item_sales_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'word_{i}' for i in range(50)] \\\n    + ['city_id', 'item_category_id1', 'item_category_id2'] \\\n    + ['prev_days_on_sale__item',\n       'prev_blocks_on_sale__item',\n       'prev_days_on_sale__item_shop',\n       'prev_blocks_on_sale__item_shop',\n       'prev_days_on_sale__item_city',\n       'prev_blocks_on_sale__item_city',\n       'day_quality'\n    ] \\\n    + ['prev_days_on_sale__item__encoded',\n       'prev_blocks_on_sale__item__encoded',\n       'prev_days_on_sale__item_shop__encoded',\n       'prev_blocks_on_sale__item_shop__encoded',\n       'prev_days_on_sale__item_city__encoded',\n       'prev_blocks_on_sale__item_city__encoded',\n       'month__encoded',\n       'item_id__encoded',\n       'item_category_id__encoded',\n       'item_category_id1__encoded',\n       'item_category_id2__encoded',\n       'shop_id__encoded',\n       'city_id__encoded'\n    ]\nlen(features_1A)","7ed7a091":"with open('..\/input\/pfs-trained-models\/PFS_trained_models_1A.pickle', 'rb') as f:\n    trained_models_1A = pickle.load(f)","95740028":"data_layer_2A, score_table_1A, corr_matrix_1A, submissions_1A = test(data_layer_1, trained_models_1A, features_1A, target_block = 34)","2781fded":"score_table_1A.boxplot()","225fc2de":"lgb.plot_importance(trained_models_1A[f'lgb_1A_d4'][34].bst, figsize = (12, 12), importance_type = 'split', max_num_features = 50)","6931e176":"lgb.plot_importance(trained_models_1A[f'lgb_1A_d4'][34].bst, figsize = (12, 12), importance_type = 'gain', max_num_features = 50)","05735f3b":"x_train = data_layer_1.loc[data_layer_1['date_block_num'].isin(range(12, 34)), features_1A]","8c845945":"x = x_train.sample(10000)\nexplainer = shap.TreeExplainer(trained_models_1A[f'lgb_1A_d4'][34].bst, data = x)\nx_shap = explainer.shap_values(x)","ad26c476":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","3b863753":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","541e12d5":"del trained_models_1A\ngc.collect()","f155eb36":"models_1B = {}\n\nfor d in [3, 4, 5]:\n    lgb_params = {\n        'objective': 'regression',\n        'metric': ['rmse'],\n        'learning_rate': 0.1,\n        'num_leaves': 2 ** d,\n        'min_data_in_leaf': 20,\n        'max_depth': -1,\n        'seed': 2021\n    }\n    models_1B[f'lgb_1B_d{d}'] = LgbModel(lgb_params)\n    \nfeatures_1B = ['year', 'month', 'shop_id', 'item_id', 'item_category_id'] \\\n    + [f'item_count_{i}' for i in [1, 2, 3]] \\\n    + [f'item_price_{i}' for i in [1, 2, 3]] \\\n    + [f'item_sales_{i}' for i in [1, 2, 3]] \\\n    + [f'word_{i}' for i in range(50)] \\\n    + ['city_id', 'item_category_id1', 'item_category_id2'] \\\n    + ['prev_days_on_sale__item',\n       'prev_blocks_on_sale__item',\n       'prev_days_on_sale__item_shop',\n       'prev_blocks_on_sale__item_shop',\n       'prev_days_on_sale__item_city',\n       'prev_blocks_on_sale__item_city',\n       'day_quality'\n    ] \\\n    + ['prev_days_on_sale__item__encoded',\n       'prev_blocks_on_sale__item__encoded',\n       'prev_days_on_sale__item_shop__encoded',\n       'prev_blocks_on_sale__item_shop__encoded',\n       'prev_days_on_sale__item_city__encoded',\n       'prev_blocks_on_sale__item_city__encoded',\n       'month__encoded',\n       'item_id__encoded',\n       'item_category_id__encoded',\n       'item_category_id1__encoded',\n       'item_category_id2__encoded',\n       'shop_id__encoded',\n       'city_id__encoded'\n    ]\nlen(features_1B)","347efc1e":"with open('..\/input\/pfs-trained-models\/PFS_trained_models_1B.pickle', 'rb') as f:\n    trained_models_1B = pickle.load(f)","f607a8c7":"data_layer_2B, score_table_1B, corr_matrix_1B, submissions_1B = test(data_layer_1, trained_models_1B, features_1B, target_block = 34)","e62f1f6d":"score_table_1B.boxplot()","8406bae5":"lgb.plot_importance(trained_models_1B[f'lgb_1B_d4'][34].bst, figsize = (12, 12), importance_type = 'split', max_num_features = 50)","55ce37a6":"lgb.plot_importance(trained_models_1B[f'lgb_1B_d4'][34].bst, figsize = (12, 12), importance_type = 'gain', max_num_features = 50)","eb5996a7":"x_train = data_layer_1.loc[data_layer_1['date_block_num'].isin(range(5, 34)), features_1B]","adef40a6":"x = x_train.sample(10000)\nexplainer = shap.TreeExplainer(trained_models_1B[f'lgb_1B_d4'][34].bst, data = x)\nx_shap = explainer.shap_values(x)","500d9df2":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","6597befb":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","67a73b63":"del trained_models_1B\ngc.collect()","80bccc65":"models_1C = {}\n\nfor d in [2, 3, 4, 5]:\n    lgb_params = {\n        'objective': 'regression',\n        'metric': ['rmse'],\n        'learning_rate': 0.1,\n        'num_leaves': 2 ** d,\n        'min_data_in_leaf': 20,\n        'max_depth': -1,\n        'seed': 2021\n    }\n    models_1C[f'lgb_1C_d{d}'] = LgbModel(lgb_params)\n    \nfeatures_1C = ['year', 'month', 'shop_id', 'item_id', 'item_category_id'] \\\n    + [f'item_count_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'item_price_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'item_sales_{i}' for i in [1, 2, 3, 12]] \\\n    + [f'word_{i}' for i in range(50)] \\\n    + ['city_id', 'item_category_id1', 'item_category_id2'] \\\n    + ['prev_days_on_sale__item',\n       'prev_blocks_on_sale__item',\n       'prev_days_on_sale__item_shop',\n       'prev_blocks_on_sale__item_shop',\n       'prev_days_on_sale__item_city',\n       'prev_blocks_on_sale__item_city',\n       'day_quality'\n    ] \\\n    + ['prev_days_on_sale__item__encoded',\n       'prev_blocks_on_sale__item__encoded',\n       'prev_days_on_sale__item_shop__encoded',\n       'prev_blocks_on_sale__item_shop__encoded',\n       'prev_days_on_sale__item_city__encoded',\n       'prev_blocks_on_sale__item_city__encoded',\n       'month__encoded',\n       'item_id__encoded',\n       'item_category_id__encoded',\n       'item_category_id1__encoded',\n       'item_category_id2__encoded',\n       'shop_id__encoded',\n       'city_id__encoded'\n    ] \\\n    + ['month__item_id__encoded',\n       'month__item_category_id__encoded',\n       'month__shop_id__encoded',\n       'month__city_id__encoded',\n       'shop_id__item_id__encoded',\n       'shop_id__item_category_id__encoded',\n       'shop_id__item_category_id1__encoded',\n       'shop_id__item_category_id2__encoded',\n       'city_id__item_id__encoded',\n       'city_id__item_category_id__encoded',\n       'city_id__item_category_id1__encoded',\n       'city_id__item_category_id2__encoded',\n       'prev_blocks_on_sale__item__month__encoded'\n      ]\nlen(features_1C)","473ebb5d":"with open('..\/input\/pfs-trained-models\/PFS_trained_models_1C.pickle', 'rb') as f:\n    trained_models_1C = pickle.load(f)","278f6c8f":"data_layer_2C, score_table_1C, corr_matrix_1C, submissions_1C = test(data_layer_1, trained_models_1C, features_1C, target_block = 34)","fc8ec128":"score_table_1C.boxplot()","ab248730":"lgb.plot_importance(trained_models_1C[f'lgb_1C_d4'][34].bst, figsize = (12, 12), importance_type = 'split', max_num_features = 50)","879b0931":"lgb.plot_importance(trained_models_1C[f'lgb_1C_d4'][34].bst, figsize = (12, 12), importance_type = 'gain', max_num_features = 50)","bc01d5c3":"x_train = data_layer_1.loc[data_layer_1['date_block_num'].isin(range(14, 34)), features_1C]","5b3ca5b5":"x = x_train.sample(10000)\nexplainer = shap.TreeExplainer(trained_models_1C[f'lgb_1C_d4'][34].bst, data = x)\nx_shap = explainer.shap_values(x)","ef6b0bd0":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","29025ef4":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","3f2db530":"del trained_models_1C\ngc.collect()","3f6cd7ce":"models_1D = {}\n\nfor d in [2, 3, 4, 5]:\n    lgb_params = {\n        'objective': 'regression',\n        'metric': ['rmse'],\n        'learning_rate': 0.1,\n        'num_leaves': 2 ** d,\n        'min_data_in_leaf': 20,\n        'max_depth': -1,\n        'seed': 2021\n    }\n    models_1D[f'lgb_1D_d{d}'] = LgbModel(lgb_params)\n    \nfeatures_1D = ['year', 'month', 'shop_id', 'item_id', 'item_category_id'] \\\n    + [f'item_count_{i}' for i in [1, 2, 3]] \\\n    + [f'item_price_{i}' for i in [1, 2, 3]] \\\n    + [f'item_sales_{i}' for i in [1, 2, 3]] \\\n    + [f'word_{i}' for i in range(50)] \\\n    + ['city_id', 'item_category_id1', 'item_category_id2'] \\\n    + ['prev_days_on_sale__item',\n       'prev_blocks_on_sale__item',\n       'prev_days_on_sale__item_shop',\n       'prev_blocks_on_sale__item_shop',\n       'prev_days_on_sale__item_city',\n       'prev_blocks_on_sale__item_city',\n       'day_quality'\n    ] \\\n    + ['prev_days_on_sale__item__encoded',\n       'prev_blocks_on_sale__item__encoded',\n       'prev_days_on_sale__item_shop__encoded',\n       'prev_blocks_on_sale__item_shop__encoded',\n       'prev_days_on_sale__item_city__encoded',\n       'prev_blocks_on_sale__item_city__encoded',\n       'month__encoded',\n       'item_id__encoded',\n       'item_category_id__encoded',\n       'item_category_id1__encoded',\n       'item_category_id2__encoded',\n       'shop_id__encoded',\n       'city_id__encoded'\n    ] \\\n    + ['month__item_id__encoded',\n       'month__item_category_id__encoded',\n       'month__shop_id__encoded',\n       'month__city_id__encoded',\n       'shop_id__item_id__encoded',\n       'shop_id__item_category_id__encoded',\n       'shop_id__item_category_id1__encoded',\n       'shop_id__item_category_id2__encoded',\n       'city_id__item_id__encoded',\n       'city_id__item_category_id__encoded',\n       'city_id__item_category_id1__encoded',\n       'city_id__item_category_id2__encoded',\n       'prev_blocks_on_sale__item__month__encoded'\n      ]\nlen(features_1D)","4d2cb6d0":"with open('..\/input\/pfs-trained-models\/PFS_trained_models_1D.pickle', 'rb') as f:\n    trained_models_1D = pickle.load(f)","e135c2b6":"data_layer_2D, score_table_1D, corr_matrix_1D, submissions_1D = test(data_layer_1, trained_models_1D, features_1D, target_block = 34)","bc1c685f":"score_table_1D.boxplot()","de1aea5d":"lgb.plot_importance(trained_models_1D[f'lgb_1D_d4'][34].bst, figsize = (12, 12), importance_type = 'split', max_num_features = 50)","12179968":"lgb.plot_importance(trained_models_1D[f'lgb_1D_d4'][34].bst, figsize = (12, 12), importance_type = 'gain', max_num_features = 50)","3f62bf8c":"x_train = data_layer_1.loc[data_layer_1['date_block_num'].isin(range(5, 34)), features_1D]","a86d6741":"x = x_train.sample(10000)\nexplainer = shap.TreeExplainer(trained_models_1D[f'lgb_1D_d4'][34].bst, data = x)\nx_shap = explainer.shap_values(x)","aba3848a":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","8cff421d":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","29443293":"del trained_models_1D\ngc.collect()","a56709a0":"models_1E = {}\n\nfor d in [2, 3, 4, 5]:\n    lgb_params = {\n        'objective': 'regression',\n        'metric': ['rmse'],\n        'learning_rate': 0.1,\n        'num_leaves': 2 ** d,\n        'min_data_in_leaf': 20,\n        'max_depth': -1,\n        'seed': 2021\n    }\n    models_1E[f'lgb_1E_d{d}'] = LgbModel(lgb_params)\n    \nfeatures_1E = ['year', 'month', 'shop_id', 'item_id', 'item_category_id'] \\\n    + [f'item_count_{i}' for i in [1]] \\\n    + [f'item_price_{i}' for i in [1]] \\\n    + [f'item_sales_{i}' for i in [1]] \\\n    + [f'word_{i}' for i in range(50)] \\\n    + ['city_id', 'item_category_id1', 'item_category_id2'] \\\n    + ['prev_days_on_sale__item',\n       'prev_blocks_on_sale__item',\n       'prev_days_on_sale__item_shop',\n       'prev_blocks_on_sale__item_shop',\n       'prev_days_on_sale__item_city',\n       'prev_blocks_on_sale__item_city',\n       'day_quality'\n    ] \\\n    + ['prev_days_on_sale__item__encoded',\n       'prev_blocks_on_sale__item__encoded',\n       'prev_days_on_sale__item_shop__encoded',\n       'prev_blocks_on_sale__item_shop__encoded',\n       'prev_days_on_sale__item_city__encoded',\n       'prev_blocks_on_sale__item_city__encoded',\n       'month__encoded',\n       'item_id__encoded',\n       'item_category_id__encoded',\n       'item_category_id1__encoded',\n       'item_category_id2__encoded',\n       'shop_id__encoded',\n       'city_id__encoded'\n    ] \\\n    + ['month__item_id__encoded',\n       'month__item_category_id__encoded',\n       'month__shop_id__encoded',\n       'month__city_id__encoded',\n       'shop_id__item_id__encoded',\n       'shop_id__item_category_id__encoded',\n       'shop_id__item_category_id1__encoded',\n       'shop_id__item_category_id2__encoded',\n       'city_id__item_id__encoded',\n       'city_id__item_category_id__encoded',\n       'city_id__item_category_id1__encoded',\n       'city_id__item_category_id2__encoded',\n       'prev_blocks_on_sale__item__month__encoded'\n      ]\nlen(features_1E)","093d6ec6":"with open('..\/input\/pfs-trained-models\/PFS_trained_models_1E.pickle', 'rb') as f:\n    trained_models_1E = pickle.load(f)","15210890":"data_layer_2E, score_table_1E, corr_matrix_1E, submissions_1E = test(data_layer_1, trained_models_1E, features_1E, target_block = 34)","723136d0":"score_table_1E.boxplot()","2357730a":"lgb.plot_importance(trained_models_1E[f'lgb_1E_d4'][34].bst, figsize = (12, 12), importance_type = 'split', max_num_features = 50)","39086e7c":"lgb.plot_importance(trained_models_1E[f'lgb_1E_d4'][34].bst, figsize = (12, 12), importance_type = 'gain', max_num_features = 50)","5a68fe1c":"x_train = data_layer_1.loc[data_layer_1['date_block_num'].isin(range(3, 34)), features_1E]","96356812":"x = x_train.sample(10000)\nexplainer = shap.TreeExplainer(trained_models_1E[f'lgb_1E_d4'][34].bst, data = x)\nx_shap = explainer.shap_values(x)","0f8e0b7e":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","f50ca9c4":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","9f800571":"del trained_models_1E\ngc.collect()","6bf4cdfb":"data_layer_2 = pd.concat([\n    data_layer_2A[['date_block_num', 'target']],\n    data_layer_2A.drop(columns = ['date_block_num', 'target']),\n    data_layer_2B.drop(columns = ['date_block_num', 'target']),\n    data_layer_2C.drop(columns = ['date_block_num', 'target']),\n    data_layer_2D.drop(columns = ['date_block_num', 'target']),\n    data_layer_2E.drop(columns = ['date_block_num', 'target'])\n], axis = 1)","15d06a1c":"del data_layer_1\ndel data_layer_2A\ndel data_layer_2B\ndel data_layer_2C\ndel data_layer_2D\ndel data_layer_2E\ngc.collect()","41ac102c":"score_table = pd.concat([\n    score_table_1A,\n    score_table_1B,\n    score_table_1C,\n    score_table_1D,\n    score_table_1E\n], axis = 1)","4799f0fb":"score_table","de64e710":"fig = plt.figure(figsize = (16, 10))\nscore_table[14:].boxplot()","2bab2811":"sns.heatmap(data_layer_2[data_layer_2['date_block_num'] == 34][[\n    'pred_lgb_1A_d3', 'pred_lgb_1A_d4', 'pred_lgb_1A_d5',\n    'pred_lgb_1B_d3', 'pred_lgb_1B_d4', 'pred_lgb_1B_d5',\n    'pred_lgb_1C_d2', 'pred_lgb_1C_d3', 'pred_lgb_1C_d4', 'pred_lgb_1C_d5',\n    'pred_lgb_1D_d2', 'pred_lgb_1D_d3', 'pred_lgb_1D_d4', 'pred_lgb_1D_d5',\n    'pred_lgb_1E_d2', 'pred_lgb_1E_d3', 'pred_lgb_1E_d4', 'pred_lgb_1E_d5',\n]].corr(), cmap = sns.diverging_palette(230, 20, as_cmap = True))","97307660":"models_2 = {}\nmodels_2[f'lasso_2'] = LassoModel(positive = 'True')\nmodels_2[f'ridge_2'] = RidgeModel()\n\nfeatures_2 = [\n    'pred_lgb_1A_d3', 'pred_lgb_1A_d4', 'pred_lgb_1A_d5',\n    'pred_lgb_1B_d3', 'pred_lgb_1B_d4', 'pred_lgb_1B_d5',\n    'pred_lgb_1C_d2', 'pred_lgb_1C_d3', 'pred_lgb_1C_d4', 'pred_lgb_1C_d5',\n    'pred_lgb_1D_d2', 'pred_lgb_1D_d3', 'pred_lgb_1D_d4', 'pred_lgb_1D_d5',\n    'pred_lgb_1E_d2', 'pred_lgb_1E_d3', 'pred_lgb_1E_d4', 'pred_lgb_1E_d5',\n]\nlen(features_2)","68ebf69c":"%%time\ntrained_models_2 = train(data_layer_2, models_2, features_2, start_block = 16, target_block = 34)","2ad59692":"data_layer_3, score_table_2, corr_matrix_2, submissions_2 = test(data_layer_2, trained_models_2, features_2, target_block = 34)","88721d59":"score_table_2","75f55076":"score_table_2.boxplot()","836cd3b1":"corr_matrix_2","0e5e4ce9":"x_train = data_layer_2.loc[data_layer_2['date_block_num'].isin(range(14, 34)), features_2]\nx = x_train.sample(1000)","d3ad3c79":"explainer = shap.Explainer(trained_models_2[f'lasso_2'][34].regressor.predict, x)\nx_shap = explainer(x)","46f41261":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns\n)","902420e0":"shap.summary_plot(\n    shap_values = x_shap,\n    features = x,\n    feature_names = x.columns,\n    plot_type='bar',\n    max_display = 25\n)","f4988e00":"submission_lasso = submissions_2[['lasso_2']].reset_index()\nsubmission_lasso.columns = ['ID', 'item_cnt_month']\nsubmission_lasso","06a3dd01":"submission_lasso.to_csv('submission_lasso.csv', index=False)","89177380":"submission_ridge = submissions_2[['ridge_2']].reset_index()\nsubmission_ridge.columns = ['ID', 'item_cnt_month']\nsubmission_ridge","db7cacc7":"submission_ridge.to_csv('submission_ridge.csv', index=False)","6da6c529":"submission = submissions_2.mean(axis = 1).reset_index()\nsubmission.columns = ['ID', 'item_cnt_month']\nsubmission","d8d9ed5d":"score_table = pd.concat([\n    score_table,\n    score_table_2,\n], axis = 1)","010ccbb6":"score_table[16:]","f087aabe":"fig = plt.figure(figsize = (16, 10))\nscore_table[16:].boxplot()","0b82270d":"```python\nsales = sales_train.copy()\nsales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\nsales['weekday'] = sales.date.dt.dayofweek\nsales['day'] = sales.date.dt.dayofyear \nsales['day'] += 365 * (sales.date.dt.year-2013)\n```","a020d9ae":"```python\ndef compress_data(data):\n    for column in data.columns:\n        if column.startswith('word'):\n            data[column] = data[column].astype('int8')\n        if column.startswith('prev_days'):\n            data[column] = data[column].astype('int16')\n        if column.startswith('prev_blocks'):\n            data[column] = data[column].astype('int8')\n        if column in ['date_block_num', 'month']:\n            data[column] = data[column].astype('int8')\n        if column in ['year']:\n            data[column] = data[column].astype('int16')\n        if column.endswith('id'):\n            data[column] = data[column].astype('int16') \n        if data[column].dtype == 'int64':\n            data[column] = data[column].astype('int32')\n        if data[column].dtype == 'float64':\n            data[column] = data[column].astype('float32')\n    return data\n```","a389c5d9":"First, I constructed 18 Lightgbm models of 5 types: A, B, C, D, and E for the first layer of the stacking as follows:","4b84820c":"I utilized this note: https:\/\/www.kaggle.com\/homiarafarhana\/predict-future-sales#Feature-engineering-and-data-cleaning.","cf6bb705":"# 2nd Layer","79d15148":"```python\ngc.collect()\n```","42021e1b":"## Lags","37556393":"```python\nsales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample_submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\n```","7e3f62e6":"```python\nday_quality = pd.merge(\n    sales.groupby(['shop_id','weekday']).agg(\n        shop_day_sales = ('item_cnt_day', np.sum)\n    ).reset_index(),\n    sales.groupby(['shop_id']).agg(\n        shop_sales = ('item_cnt_day', np.sum)\n    ).reset_index(),\n    on='shop_id',\n    how='left'\n)\nday_quality['day_quality'] = day_quality['shop_day_sales'] \/ day_quality['shop_sales']\n```","14f72f98":"```python\ndata2['split'] = data2['item_category_name'].str.split('-')\ndata2['item_category_name1'] = data2['split'].map(lambda x: x[0].strip())\ndata2['item_category_id1'] = LabelEncoder().fit_transform(data2['item_category_name1'])\ndata2['item_category_name2'] = data2['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ndata2['item_category_id2'] = LabelEncoder().fit_transform(data2['item_category_name2'])\ndata2.drop(['split'], axis=1, inplace = True)\n```","c9cf1153":"```python\ntfidf = TfidfVectorizer(max_features=50)\nitems2 = pd.DataFrame(tfidf.fit_transform(items['item_name']).toarray(), columns = [f'word_{i}' for i in range(len(tfidf.get_feature_names()))])\nitems2.index.name = 'item_id'\nitems2 = items2.reset_index()\ndata2 = pd.merge(data2, items2, on = 'item_id', how = 'left')\ndata2 = compress_data(data2)\n```","960ed231":"```python\ndata2 = data.copy()\n```","28527116":"```python\ndata = compress_data(data)\n```","5104a870":"```python\ndata2.loc[data2['shop_name'] == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\ndata2['city'] = data2['shop_name'].str.split(' ').map(lambda x: x[0])\ndata2.loc[data2['city'] == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\ndata2['city_id'] = LabelEncoder().fit_transform(data2['city'])\n```","648652cf":"```python\ndata2['first_sale_day__item_city'] = data2.groupby(['item_id', 'city_id'])['first_sale_day__item'].transform(np.min)\ndata2['first_sale_block__item_city'] = data2.groupby(['item_id', 'city_id'])['first_sale_block__item'].transform(np.min)\ndata2['prev_days_on_sale__item_city'] = (data2['first_day_of_month'] - data2['first_sale_day__item_city']).fillna(0).clip(lower = 0).astype(np.int16)\ndata2['prev_blocks_on_sale__item_city'] = (data2['date_block_num'] - data2['first_sale_block__item_city']).fillna(0).clip(lower = 0).astype(np.int8)\n```","c08bbbe3":"I add some features into the dataset.","858c537c":"```python\ndata_train = sales_train.copy()\ndata_train['item_sales'] = data_train['item_price'] * data_train['item_cnt_day']\ndata_train = data_train.groupby(['date_block_num', 'shop_id', 'item_id']).agg(\n    item_count = ('item_cnt_day', np.sum),\n    item_price = ('item_price', np.mean),\n    item_sales = ('item_sales', np.sum)\n)\ndata_train = data_train.reset_index()\n```","c944824e":"(I decided not to use Xgboost)","de1c5ea5":"Next, I used Lasso and Ridge linear regresions for the second layer of the stacking and chose the one (i.e. Lasso) which gave the better internal score.","8ba23d06":"```python\nwith open('trained_models_1C.pickle', 'wb') as f:\n    pickle.dump(trained_models_1C, f)\n```","9ff815de":"This is a notebook for the final project of the Coursera course [\"How to win a data science competition\"](https:\/\/www.coursera.org\/learn\/competitive-data-science).\nThe course is the 2nd course in the specialization [\"Advanced Machine Learning Specialization\"](https:\/\/www.coursera.org\/specializations\/aml) organized by HSE University.\nThis notebook was written in February 2021.\nI did not intend to write this notebook for public.\nHowever, I decided to publish this notebook.","e798f107":"# Feature Engineering","b4e73a2a":"# Summary","4c12fa62":"# Validation Scheme","7e38937e":"```python\ngrid = []\nfor block_num in np.arange(34):\n    cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[[block_num], cur_shops, cur_items])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = ['date_block_num', 'shop_id', 'item_id'],dtype=np.int32)\n```","4058deef":"```python\ndata2['prev_days_on_sale__item'] = (data2['first_day_of_month'] - data2['first_sale_day__item']).fillna(0).clip(lower = 0).astype(np.int16)\ndata2['prev_blocks_on_sale__item'] = (data2['date_block_num'] - data2['first_sale_block__item']).fillna(0).clip(lower = 0).astype(np.int8)\ndata2['prev_days_on_sale__item_shop'] = (data2['first_day_of_month'] - data2['first_sale_day__item_shop']).fillna(0).clip(lower = 0).astype(np.int16)\ndata2['prev_blocks_on_sale__item_shop'] = (data2['date_block_num'] - data2['first_sale_block__item_shop']).fillna(0).clip(lower = 0).astype(np.int8)\n```","f746ce00":"```python\ndata3 = data2.copy()\n```","d797e49c":"## Label encoding for 'shop_names' and 'item_category_name'","a87d9461":"I add 50 TfIdf features for items names.","da881e44":"```python\ndata2 = compress_data(data2)\n```","44f54a8b":"```python\nwith open('trained_models_1B.pickle', 'wb') as f:\n    pickle.dump(trained_models_1B, f)\n```","db49b6ab":"## First day on sale for each item","8f17dfe8":"```python\nshop_month_quality = pd.merge(\n    dates,\n    day_quality,\n    on = ['weekday'],\n    how = 'left'\n)[['date_block_num', 'shop_id', 'day_quality']].groupby(['date_block_num', 'shop_id']).sum().reset_index()\n```","968703db":"I add monthly statistics of items for previous months.","45c3b9c6":"```python\ndef shift_feature(data, groupby_columns, lags, features, default_value = None):\n    data_tmp = data\n    for lag in lags:\n        column_names = [feature + '_' + str(lag) for feature in features]\n        data_tmp[column_names] = data.groupby(groupby_columns)[features].shift(periods = lag)\n        if default_value is not None:\n            data_tmp[column_names] = data_tmp[column_names].fillna(default_value)\n    return data_tmp\n```","2719e4f0":"For the features, in addition to the lag features, I utilized:\n- TfIdf for 'item_name',\n- Label encodings for 'shop_names' and 'item_category_name',\n- Days elapsed from the first day on sale for each item.\n\nIn particular, the last feature 'prev_days_on_sale' is a very useful feature.","2bb50840":"```python\ndates = pd.DataFrame(data = {'date':pd.date_range(start = '2013-01-01', end = '2015-11-30')})\ndates['weekday'] = dates.date.dt.dayofweek\ndates['month'] = dates.date.dt.month\ndates['year'] = dates.date.dt.year - 2013\ndates['date_block_num'] = dates['year'] * 12 + dates['month'] - 1\n```","b0eff9ed":"I used the module [shap](https:\/\/shap.readthedocs.io) as an explanation for the model output.","5b543653":"## Helper Functions","ca842951":"```python\n%%time\ntrained_models_1B = train(data_layer_1, models_1B, features_1B, start_block = 5, target_block = 34)\n```","8b27b7be":"```python\n%%time\ntrained_models_1C = train(data_layer_1, models_1C, features_1C, start_block = 14, target_block = 34)\n```","f14df8b2":"```python\ndata2 = pd.merge(\n    data2,\n    shop_month_quality,\n    on = ['date_block_num', 'shop_id'],\n    how = 'left'\n)\n```","77897969":"I utilized this note: https:\/\/www.kaggle.com\/sushmaguntupalli\/predict-future-sales-light-gbm-top-1#2.-Preparing-Training-Dataset-&-Feature-Engineering.","aff2e288":"```python\n%%time\ntrained_models_1A = train(data_layer_1, models_1A, features_1A, start_block = 14, target_block = 34)\n```","59eea3d0":"```python\ndata_train = pd.merge(data_train, grid, on = ['date_block_num', 'shop_id', 'item_id'], how = 'outer').sort_values(['date_block_num', 'shop_id', 'item_id'])\ndata_train['item_count'] = data_train['item_count'].fillna(0.0).astype(np.int32)\ndata_train['item_sales'] = data_train['item_sales'].fillna(0.0).astype(np.float32)\ndata_train['target'] = data_train['item_count'].fillna(0.0).clip(0, 20).astype(np.float32)\ndata_train['year'] = data_train['date_block_num'] \/\/ 12 + 2013\ndata_train['month'] = data_train['date_block_num'] % 12 + 1\n```","da81973d":"```python\nwith open('trained_models_1A.pickle', 'wb') as f:\n    pickle.dump(trained_models_1A, f)\n```","ebd3cb81":"```python\ndata_test = pd.merge(test, items, on = 'item_id', how = 'left')\ndata_test = pd.merge(test, items, on = 'item_id', how = 'left')\ndata_test = pd.merge(data_test, item_categories, on = 'item_category_id', how = 'left')\ndata_test = pd.merge(data_test, shops, on = 'shop_id', how = 'left')\ndata_test = data_test.set_index('ID')\ndata_test['date_block_num'] = 34\ndata_test['year'] = 2015\ndata_test['month'] = 11\n```","ae8abe86":"```python\nwith open('trained_models_1E.pickle', 'wb') as f:\n    pickle.dump(trained_models_1E, f)\n```","96306192":"```python\ntmp = data2[['year', 'month']].copy()\ntmp['day'] = 1\ntmp['day_of_year'] = pd.to_datetime(tmp[['year', 'month', 'day']]).dt.dayofyear\ntmp['first_day_of_month'] = 365 * (tmp['year'] - 2013) + tmp['day_of_year']\ndata2['first_day_of_month'] = tmp['first_day_of_month']\n```","a0b5d7a0":"```python\ndata_train = pd.merge(data_train, items, on = 'item_id', how = 'left')\ndata_train = pd.merge(data_train, item_categories, on = 'item_category_id', how = 'left')\ndata_train = pd.merge(data_train, shops, on = 'shop_id', how = 'left')\n```","78cb1111":"# 1st Layer: E","0d89d8d5":"For example, the type 'A' uses lag features of 1,2,3, and 12 months with mean target encodings of categorical variables.\nThere are three different models of type 'A': models with the number of leaves $2^3$, $2^4$, and $2^5$.","16220060":"## Mean target encodings","c3067221":"```python\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['month', 'item_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['month', 'item_category_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['month', 'shop_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['month', 'city_id'], default_value = 0.0)\n\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['shop_id', 'item_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['shop_id', 'item_category_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['shop_id', 'item_category_id1'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['shop_id', 'item_category_id2'], default_value = 0.0)\n\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['city_id', 'item_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['city_id', 'item_category_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['city_id', 'item_category_id1'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['city_id', 'item_category_id2'], default_value = 0.0)\n\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_blocks_on_sale__item', 'month'], default_value = 0.0)\n```","7d704e1b":"```python\ndef mean_target_encoding(data, block_column, target_column, groupby_columns, default_value = None):\n    index_columns = [block_column] + groupby_columns\n    encoded_column = '__'.join(groupby_columns) + '__encoded'\n    \n    data_grouped = data[[target_column] + index_columns].fillna(default_value).groupby(index_columns).agg(\n        target_sum = (target_column, np.sum),\n        target_count = (target_column, np.size)\n    )\n    \n    data_grouped['target_cumsum'] = data_grouped.groupby(groupby_columns)['target_sum'].cumsum() - data_grouped['target_sum']\n    data_grouped['target_cumcount'] = data_grouped.groupby(groupby_columns)['target_count'].cumsum() - data_grouped['target_count']\n    data_grouped[encoded_column] = data_grouped['target_cumsum'] \/ data_grouped['target_cumcount']\n    \n    return pd.merge(data.reset_index(), data_grouped[[encoded_column]], left_on = index_columns, right_index = True, how = 'left').fillna(default_value).set_index('index')\n```","e64a8a01":"# 1st Layer: C","3bf0ebf0":"# 1st Layer: D","b0fa163d":"# 1st Layer: A","1d6c60d9":"```python\ndata2 = shift_feature(data2, groupby_columns = ['item_id', 'shop_id'], lags = [1, 2, 3, 12], features = ['item_count', 'item_sales'], default_value = 0.0)\ndata2 = shift_feature(data2, groupby_columns = ['item_id', 'shop_id'], lags = [1, 2, 3, 12], features = ['item_price'], default_value = 0.0)\n```","ba3b1887":"# Loading Data","3b7f78d8":"# Importing Modules","3993d143":"```python\n%%time\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_days_on_sale__item'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_blocks_on_sale__item'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_days_on_sale__item_shop'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_blocks_on_sale__item_shop'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_days_on_sale__item_city'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['prev_blocks_on_sale__item_city'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['month'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['item_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['item_category_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['item_category_id1'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['item_category_id2'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['shop_id'], default_value = 0.0)\ndata3 = mean_target_encoding(data3, 'date_block_num', 'item_count', ['city_id'], default_value = 0.0)\n```","1c078e35":"# 1st Layer","6aba2333":"```python\ndata3.to_pickle('data3.pickle')\n```","406eb481":"```python\ndata2 = data2.set_index('index')\n```","750db2b7":"First, I load and compress the data; it is about 615.5+ MB with 214,199 rows \u00d7 13 columns.","eb179e35":"Type | # of models | Lag features | blocks used for training | # of leves in lgb models   | Mean Target Encoding |\n---- | ----------- | ---------- | --------------------------| -------------------------- |----------------------|\nA    |           3 | 1, 2, 3, 12 | 12 ~ 33 (22 blocks)       | $2^3$, $2^4$, $2^5$        | YES (without interaction terms)\nB    |           3 | 1, 2, 3     | 3 ~ 33 (31 blocks)        | $2^3$, $2^4$, $2^5$        | YES (without interaction terms)\nC    |           4 | 1, 2, 3, 12 | 12 ~ 33 (22 blocks)       | $2^2$, $2^3$, $2^4$, $2^5$ | YES (with interaction terms)\nD    |           4 | 1, 2, 3     | 3 ~ 33 (31 blocks)        | $2^2$, $2^3$, $2^4$, $2^5$ | YES (with interaction terms)\nE    |           4 | 1           | 1 ~ 33 (33 blocks)        | $2^2$, $2^3$, $2^4$, $2^5$ | YES (with interaction terms)","4c71feee":"```python\n%%time\ntrained_models_1E = train(data_layer_1, models_1E, features_1E, start_block = 3, target_block = 34)\n```","69c861b5":"```python\n%%time\ntrained_models_1D = train(data_layer_1, models_1D, features_1D, start_block = 5, target_block = 34)\n```","50aa4587":"Leaderboard Score.  \n(Public): 0.882002 (1089\/13338; Top 9%)  \n(Private): 0.877179\n\nThe competition is still active; thefore we do not know the final score.\nThe private score should not be disclosed; however, the Coursera grader gives the private score as well.","af9b03d5":"```python\ndata2 = data2.reset_index()\ndata2 = pd.merge(\n    data2,\n    sales.groupby('item_id').agg(\n        first_sale_day__item = ('day', np.min),\n        first_sale_block__item = ('date_block_num', np.min),\n    ).reset_index(),\n    on = ['item_id'],\n    how = 'left'\n)\ndata2 = pd.merge(\n    data2,\n    sales.groupby(['item_id', 'shop_id']).agg(\n        first_sale_day__item_shop = ('day', np.min),\n        first_sale_block__item_shop = ('date_block_num', np.min),\n    ).reset_index(),\n    on = ['item_id', 'shop_id'],\n    how = 'left'\n)\n```","c3de5b26":"## TfIdf for item names","7d6fe1a7":"# Final project for \"How to win a data science competition\" Coursera course","ebf67571":"I have created a validation scheme as follows:\nfor blocks $n = \\{ \\text{start_block}, \\text{start_block} + 1, \\cdots, \\text{target_block} \\}$, we\n- use the blocks $n = \\{ \\text{start_block}, \\text{start_block} + 1 \\cdots, \\text{target_block} - 2 \\}$ for the training data,\n- use the block $n = \\{ \\text{target_block} - 1 \\}$ for the validation (especially for the early stopping),\n- and predict for the block $n = \\{ \\text{target_block} \\}$.\n\nRecall that $n = 34$ is the block for the submission.","d51d1eac":"```python\nwith open('trained_models_1D.pickle', 'wb') as f:\n    pickle.dump(trained_models_1D, f)\n```","51aa6efb":"```python\ntmp = data2[['year', 'month']].copy()\ntmp['day'] = 1\ntmp['day_of_year'] = pd.to_datetime(tmp[['year', 'month', 'day']]).dt.dayofyear\ntmp['first_day_of_month'] = 365 * (tmp['year'] - 2013) + tmp['day_of_year']\ndata2['first_day_of_month'] = tmp['first_day_of_month']\n```","44531b3c":"Let us see the correlation matrix.","e7c8af01":"```python\ndata = pd.concat([data_train, data_test])\n```","69fb9678":"Add interaction terms:","29b42de7":"# 1st Layer: B"}}