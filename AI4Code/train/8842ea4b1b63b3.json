{"cell_type":{"f749d2a3":"code","448b7a32":"code","b50c0c8e":"code","01b5ed7d":"code","e2db3adc":"code","8c3f2909":"code","7d8651b6":"code","dec1ba2f":"code","52977dd0":"code","41ceddb6":"code","49cb3fd7":"code","2d64083c":"code","089244bf":"code","11d5da52":"code","158e98bc":"code","ec878a1a":"code","fdc0677f":"code","b94c5111":"code","2f6d1660":"code","950290ae":"code","1f2305e1":"code","a1163c8f":"code","67464012":"code","365d7015":"code","4f1e0d05":"code","1460aa43":"code","e377b99d":"code","10537999":"code","50236494":"markdown","a00551d8":"markdown","6ba847ef":"markdown"},"source":{"f749d2a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","448b7a32":"from torch.utils.data import DataLoader, TensorDataset\nimport torch.nn.functional as F\nfrom torch import nn\nimport torch\n\nimport pytorch_lightning as pl\n\nprint(\"TORCH:\", torch.__version__)","b50c0c8e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error","01b5ed7d":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import OrderedDict, Counter\nfrom tqdm import tqdm\nimport random","e2db3adc":"# Function for setting the seed\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():  # GPU operation have separate seed\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","8c3f2909":"datapath = '..\/input\/ventilator-pressure-prediction'\n\ntrain = pd.read_csv(datapath + '\/train.csv')\ntest = pd.read_csv(datapath + '\/test.csv')\n\nprint('Train Shape -> ', end='')\nprint(train.shape)\nprint(train.head())\n\nprint('\\nTest Shape -> ', end='')\nprint(test.shape)\nprint(test.head())\n\ncount = list(Counter(train.breath_id).values())\n\nN_TIME_STEPS_PER_EXAMPLE = max(count)\nassert N_TIME_STEPS_PER_EXAMPLE == min(count)","7d8651b6":"def add_diff(array: np.array, index: int = 0):\n    data = array.copy()\n    adding = np.zeros(data[:, :, index].shape)\n    adding[:, 1:] = data[:, :-1, index]\n    data[:, :, index] -= adding\n    return data\n\ndef pressure_log(array, index: int = 0):\n    data = np.zeros_like(array[:, :, index])\n    data[:, 1:] = array[:, :-1, index]\n    return data\n    \n\ndef featurize(dataframe: pd.DataFrame):\n    # Dropping unecessary columns\n    data = dataframe.copy().drop(columns=['id', 'breath_id'])\n    \n    # Nomalizing some features\n    data['norm_R'] = (data.R - data.R.min()) \/ (data.R.max() - data.R.min())\n    data['norm_C'] = (data.C - data.C.min()) \/ (data.C.max() - data.C.min())\n    \n    # Adding the difference between some features\n    data['time_step_diff'] = add_diff(data.time_step.to_numpy().reshape(-1, N_TIME_STEPS_PER_EXAMPLE, 1)).flatten()\n    data['u_in_diff'] = add_diff(data.u_in.to_numpy().reshape(-1, N_TIME_STEPS_PER_EXAMPLE, 1)).flatten()\n\n    # New cross features\n    data['time_cross_var'] = data.time_step * data.time_step_diff\n    data['u_in_cross_var_in_time'] = data.u_in * data.u_in_diff * data.time_cross_var\n    data['norm_R_C_time_cross_var'] = data.norm_R * data.norm_C * data.time_cross_var\n    data['u_in_norm_R_C'] = data.norm_R * data.norm_C * data.u_in\n    data['u_in_norm_R_C_time_cross_var'] = data.u_in_norm_R_C * data.time_cross_var \n    data['norm_R_C_u_in_cross_var'] = data.norm_R * data.norm_C * data.u_in_diff\n\n    # Dropping some features that I don't want to use\n    data = data.drop(columns=['R', 'C', 'time_step', 'norm_C', 'norm_R'])\n    \n    return data","dec1ba2f":"training_features_dataframe = featurize(train.drop(columns='pressure'))\ntraining_targets_dataframe = train.pressure\ntraining_features_dataframe.head(7)","52977dd0":"training_targets_dataframe.head(7)","41ceddb6":"N_FEATURES = len(training_features_dataframe.columns)\n\ntraining_features = training_features_dataframe.to_numpy().reshape(-1, N_TIME_STEPS_PER_EXAMPLE, N_FEATURES)\ntraining_targets = training_targets_dataframe.to_numpy().reshape(-1, N_TIME_STEPS_PER_EXAMPLE)\n\nprint('Features Shape:', training_features.shape, 'Targets Shape:', training_targets.shape)","49cb3fd7":"X_train, X_dev, y_train, y_dev = train_test_split(training_features, training_targets, test_size=0.2)\nX_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, test_size=0.5)\n\nprint('Train Shape:', X_train.shape, '\\nDev Shape:', X_dev.shape, '\\nTest shape:', X_test.shape)","2d64083c":"BATCH_SIZE = 50\n\ntrainset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ndevset = TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev))\ntestset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n\ntrainLoader = DataLoader(trainset, BATCH_SIZE, shuffle=False)\ndevLoader = DataLoader(devset, BATCH_SIZE, shuffle=False)\ntestLoader = DataLoader(testset, BATCH_SIZE, shuffle=False)","089244bf":"def hidden_block(x, y, activation: str = 'relu', drop: float = 0.05):\n    activations = nn.ModuleDict([\n        ['selu', nn.SELU()],\n        ['relu', nn.ReLU()],\n        ['lrelu', nn.LeakyReLU()],\n        ['none', nn.Identity()]\n    ])\n    return nn.Sequential(nn.Linear(x, y), nn.Dropout(drop), activations[activation])\n\n\nclass LongShortTermNetwork(pl.LightningModule):\n\n    def __init__(self, input_size: int, output_size: int, lstm_hidden_size: int, num_layers: int,\n        bidirectional: bool, lstm_drop: float = 0, linear_hidden_sizes: list = [256, 64], hidden_activation: str = 'selu') -> None:\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.in_size = input_size\n        self.out_size = output_size\n        self.lstm_hidden_size = lstm_hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.n_stacks = 1 + int(self.bidirectional)\n        \n        self.lstm_layer = nn.LSTM(\n            input_size=self.in_size,\n            hidden_size=self.lstm_hidden_size,\n            num_layers=self.num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=lstm_drop\n        )\n\n        self.linear_sizes = [2 * self.lstm_hidden_size, *linear_hidden_sizes]\n        self.hidden_layer = nn.Sequential(OrderedDict([\n            (f'block_{i}', hidden_block(x, y, hidden_activation, 0.2))\n                for i, (x, y) in enumerate(zip(self.linear_sizes, self.linear_sizes[1:]), 1)\n        ]))\n\n        self.output = nn.Linear(self.linear_sizes[-1], self.out_size)\n        \n        \n    def init_hidden(self, n_samples: int):\n        total_layers = self.n_stacks * self.num_layers\n        weights = (\n            torch.zeros(total_layers, n_samples, self.lstm_hidden_size).float(),\n            torch.zeros(total_layers, n_samples, self.lstm_hidden_size).float()\n        )\n\n        if torch.cuda.is_available():\n            weights = tuple(each.cuda() for each in weights)\n\n        return weights\n\n    def forward(self, input_t: torch.TensorType, hidden=None, prev_pred=None):\n        if hidden is not None:\n            h_t, c_t = hidden\n        else:\n            h_t, c_t = self.init_hidden(input_t.size(0))\n        \n        out, (h_t, c_t) = self.lstm_layer(input_t, (h_t, c_t))\n        out = self.hidden_layer(out)\n        out = self.output(out)\n        \n        if hidden is not None:\n            return out, (h_t, c_t)\n        return out\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), 3e-3)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n    \n    def training_step(self, train_batch, batch_idx):\n        self.train()\n        x, y = train_batch\n\n        x = x.float()\n        y = y.float()\n\n        out = self(x)\n        loss = F.l1_loss(out.flatten(), y.flatten())\n\n        with torch.no_grad():\n            mse = F.mse_loss(out.flatten(), y.flatten())\n\n        self.log('loss', loss)\n        self.log('mse_loss', mse, prog_bar=True)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        self.eval()\n        with torch.no_grad():\n            x, y = val_batch\n            x = x.float()\n            y = y.float()\n            out = self(x)\n            loss = F.l1_loss(out.flatten(), y.flatten())\n            mse = F.mse_loss(out.flatten(), y.flatten())\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_mse_loss', mse, prog_bar=True)\n    \n    def training_epoch_end(self, outputs):\n        sch = self.lr_schedulers()\n\n        # If the selected scheduler is a ReduceLROnPlateau scheduler.\n        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            sch.step(self.trainer.callback_metrics[\"val_loss\"])\n        else:\n            sch.step()\n    ","11d5da52":"#device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n#print(\"Device is:\", device, \"(CUDA is recommended for faster training!)\\n\")\n\nmodel = LongShortTermNetwork(N_FEATURES, 1, 256, 4, True, 0.2, [256, 128], 'none')\nprint(model)","158e98bc":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    monitor=\"val_loss\",\n    dirpath=\".\/checkpoints\",\n    filename=\"sample-pressure-model-{epoch:02d}-{val_loss:.3f}\",\n    save_top_k=3,\n    mode=\"min\",\n)\nlr_callback = pl.callbacks.LearningRateMonitor(logging_interval='epoch')","ec878a1a":"trainer = pl.Trainer(gpus=1, max_epochs=100, callbacks=[checkpoint_callback, lr_callback])\ntrainer.fit(model, trainLoader, devLoader)","fdc0677f":"print('Best Score:', checkpoint_callback.best_model_score)","b94c5111":"best_model_states = torch.load(checkpoint_callback.best_model_path)\nbest_model_states.keys()","2f6d1660":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","950290ae":"pressure_net = pressure_net.to(DEVICE)\nwith torch.no_grad():\n    pressure_net.eval()\n    losses = []\n    mse_losses = []\n    for batch in testLoader:\n        x, y = batch\n        x, y = x.to(DEVICE).float(), y.to(DEVICE).float()\n        \n        out = pressure_net(x)\n        \n        mae = F.l1_loss(out.flatten(), y.flatten())\n        mse = F.mse_loss(out.flatten(), y.flatten())\n        \n        losses.append(mae.item())\n        mse_losses.append(mse.item())\nmean_loss = np.mean(losses)\nmean_mse_loss = np.mean(mse_losses)","1f2305e1":"print(f\"Mean Testing Loss (MAE): {mean_loss:.4f}...\")\nprint(f\"Mean Testing MSE Loss: {mean_mse_loss:.4f}...\")","a1163c8f":"def predict(net: nn.Module, features, device: str = 'cpu', eval_batch: int = 200) -> np.array:\n    \"\"\"Return the predictions feedforwarding the features to the model.\"\"\"\n    with torch.no_grad():\n        net.eval()\n        net = net.to(device)\n        predictions = []\n        for i in range(0, features.size(0), eval_batch):\n            input_t = features[i:min(i+eval_batch, features.size(0))].to(device)\n            prediction = net(input_t.float())\n            predictions.append(prediction.cpu())\n    return torch.cat(predictions, dim=0).numpy()","67464012":"K = 1122\n\ny = y_test[K]\ny_hat = predictions[K]\n\nplt.plot(y)\nplt.plot(y_hat)\nplt.title(f'Constant showcase example n={K}')\nplt.legend(['Targets', 'Predictions'])\nplt.ylabel('Pressure')\nplt.show();\n\nprint('MSE: %.4f...' % mean_squared_error(y, y_hat))\nprint('MAE: %.4f...' % mean_absolute_error(y, y_hat))","365d7015":"k = np.random.randint(0, predictions.shape[0])\n\ny = y_test[k]\ny_hat = predictions[k]\n\nplt.plot(y)\nplt.plot(y_hat)\nplt.title(f'Random Showcase example n={k}')\nplt.legend(['Targets', 'Predictions'])\nplt.ylabel('Pressure')\nplt.show();\n\nprint('MSE: %.4f...' % mean_squared_error(y, y_hat))\nprint('MAE: %.4f...' % mean_absolute_error(y, y_hat))","4f1e0d05":"testing_set_dataframe = featurize(test)\ntesting_set_dataframe.head()","1460aa43":"testing_set = testing_set_dataframe.to_numpy().reshape(-1, N_TIME_STEPS_PER_EXAMPLE, N_FEATURES)\ntest_predictions = predict(model, torch.from_numpy(testing_set), device='cuda' if torch.cuda.is_available() else 'cpu')","e377b99d":"submission = pd.DataFrame({\n    'id': np.arange(1, test_predictions.size+1),\n    'pressure': test_predictions.flatten()\n})\nsubmission.head()","10537999":"submission.to_csv('submission.csv', index=False)","50236494":"### Testing the model with my test set","a00551d8":"Last but one testing losses:\n\n > Mean Testing Loss (MAE): 0.2141...  \n > Mean Testing MSE Loss: 0.1239...\n \n","6ba847ef":"# Now the Testing data"}}