{"cell_type":{"9a56e789":"code","e22a6f64":"code","f391cf58":"code","aee7429b":"code","11567a37":"code","6be6df06":"code","c08f2fd4":"code","4733b5b4":"code","3b6fc9ab":"code","d594a3dc":"code","cf1649ad":"code","13234444":"code","4e7b8edb":"code","eba081dc":"code","7a176674":"markdown","2af473ae":"markdown"},"source":{"9a56e789":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport cv2\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Dropout,\\\n        Flatten,GlobalAveragePooling2D,BatchNormalization, Activation\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import layers","e22a6f64":"WORK_DIR = '..\/input\/cassava-leaf-disease-classification'\nos.listdir(WORK_DIR)","f391cf58":"#setting\nSEED = 42\nDEBUG = False\nWANDB = True\nTARGET_SIZE = 300\nVALIDATION_SIZE = 0.2\nBATCH_SIZE = 24\nEPOCHS=40\nMODEL_NAME = \"EfficentNetB4\"\n\nif DEBUG:\n    EPOCHS = 3\n    TARGET_SIZE = 300\n    BATCH_SIZE = 24","aee7429b":"df = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/train.csv')\n\ndf['image_path'] = df['image_id'].map(lambda x : os.path.join(\n    '..\/input\/cassava-leaf-disease-classification\/train_images', x\n))\n\ndf.head()\n\n\ndf['label'] = df['label'].astype('str') # Since we are using inbuilt generator it takes label as string\n\n# X_train, X_valid = train_test_split(df, test_size = test_size, random_state=SEED, shuffle=True)\n\nSTEPS_PER_EPOCH = len(df)*(1-VALIDATION_SIZE) \/ BATCH_SIZE\nVALIDATION_STEPS = len(df)*VALIDATION_SIZE \/ BATCH_SIZE","11567a37":"train_datagen = ImageDataGenerator(validation_split = VALIDATION_SIZE,\n                                     preprocessing_function = None,\n                                     rotation_range = 20,\n                                     zoom_range = [0.5,1.0],\n                                     horizontal_flip = True,\n                                     vertical_flip = False,\n                                     fill_mode = 'nearest',\n                                     shear_range = 0.1,\n                                     brightness_range=[0.2,1.0],\n                                     height_shift_range = 0.1,\n                                     width_shift_range = 0.1)\n\ntrain_generator = train_datagen.flow_from_dataframe(df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         subset = \"training\",\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\")\n\n\nvalidation_datagen = ImageDataGenerator(validation_split = VALIDATION_SIZE)\n\nvalidation_generator = validation_datagen.flow_from_dataframe(df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         subset = \"validation\",\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\")","6be6df06":"if DEBUG:\n    t_x, t_y = next(train_generator)\n    fig, m_axs = plt.subplots(4, 6, figsize = (32, 16))\n    for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n        c_ax.imshow(c_x.astype(np.uint8))\n        c_ax.set_title(np.argmax(c_y))\n        c_ax.axis('off')","c08f2fd4":"if DEBUG:\n    t_x, t_y = next(validation_generator)\n    fig, m_axs = plt.subplots(4, 6, figsize = (32, 16))\n    for (c_x,  c_ax) in zip(t_x, m_axs.flatten()):\n        c_ax.imshow(c_x.astype(np.uint8))\n        c_ax.set_title(np.argmax(c_y))\n        c_ax.axis('off')","4733b5b4":"def get_model():\n    i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)\n    x = tf.cast(i, tf.float32)\n    x = tf.keras.applications.efficientnet.preprocess_input(x)\n    core = tf.keras.applications.EfficientNetB4(\n                        include_top=False, input_tensor=None,weights='imagenet',\n                        input_shape=(TARGET_SIZE, TARGET_SIZE,3)\n                            )\n    model = core(x)\n#     model = core.output\n    \n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(5, activation = \"softmax\")(model)\n    \n#     model = tf.keras.Model(core.input, outputs = model)\n    model = tf.keras.Model(inputs=[i], outputs=[model])\n    \n    return model\n\nmodel = get_model()\n\noptimizer = tf.keras.optimizers.Adam(lr = 0.003)\n\nmodel.compile(\n        optimizer=optimizer,\n        loss = \"sparse_categorical_crossentropy\",\n        metrics = [\"acc\"]\n    )\n\nmodel.summary()","3b6fc9ab":"if WANDB:\n    !pip install --upgrade wandb\n    !wandb login enter your wandb access token\n\n    # Init wandb\n\n","d594a3dc":"if WANDB:\n    import wandb\n    from wandb.keras import WandbCallback\n    wandb.init(project=\"cassava-leaf-disease\")\n    wandb.run.name= MODEL_NAME\n","cf1649ad":"weight_path_save = 'best_model.hdf5'\nlast_weight_path = 'last_model.hdf5'\n\ncheckpoint = ModelCheckpoint(weight_path_save, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode= 'min', \n                             save_weights_only = False)\ncheckpoint_last = ModelCheckpoint(last_weight_path, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=False, \n                             mode= 'min', \n                             save_weights_only = False)\n\n\nearly = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 5, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, \n                              patience = 1, min_delta = 0.001, \n                              mode = 'min', verbose = 1)\n\nif not WANDB:\n    callbacks_list = [checkpoint, checkpoint_last, early, reduceLROnPlat]\nelse:\n\n    callbacks_list = [WandbCallback(), checkpoint, checkpoint_last, early, reduceLROnPlat]","13234444":"history = model.fit( train_generator,\n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            epochs = EPOCHS,\n                            validation_data = validation_generator,\n                            validation_steps = VALIDATION_STEPS,\n                            callbacks = callbacks_list,\n                           #class_weight=class_weights_dict\n                          )","4e7b8edb":"def plot_hist(hist):\n    plt.figure(figsize=(15,5))\n    plt.plot(np.arange(EPOCHS), hist.history[\"acc\"], '-o', label='Train Accuracy',color='#ff7f0e')\n    plt.plot(np.arange(EPOCHS), hist.history[\"val_acc\"], '-o',label='Val Accuracy',color='#1f77b4')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Accuracy',size=14)\n    plt.legend(loc=2)\n    \n    plt2 = plt.gca().twinx()\n    plt2.plot(np.arange(EPOCHS) ,hist.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(np.arange(EPOCHS) ,hist.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    plt.legend(loc=3)\n    plt.ylabel('Loss',size=14)\n    plt.title(\"Model Accuracy and loss\")\n    \n    #plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    \n    plt.savefig('loss.png')\n    plt.show()\n    \nplot_hist(history)","eba081dc":"from sklearn.metrics import confusion_matrix, classification_report\n\nmodel.load_weights(weight_path_save) ## load the best model or all your metrics would be on the last run not on the best one\n\n\npred_valid_y = model.predict(validation_generator,  verbose = True)\npred_valid_y_labels = np.argmax(pred_valid_y, axis=-1)\nvalid_labels=validation_generator.labels\n\nprint(classification_report(valid_labels, pred_valid_y_labels ))\n\nprint(\"****************\")\nprint(confusion_matrix(valid_labels, pred_valid_y_labels ))\n","7a176674":"[Inferance link](https:\/\/www.kaggle.com\/durbin164\/efficientnetb4-starting-inferance-score-86)\n\n","2af473ae":"Use WANBD = False if you have no account in Wandb.ai "}}