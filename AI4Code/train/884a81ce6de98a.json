{"cell_type":{"935d7407":"code","c5590223":"code","fe46efbf":"code","91d53162":"code","9cb8ad17":"code","9a02af86":"code","297bba3c":"code","6aad22b8":"code","9593ed91":"code","e80dc5fb":"code","710ec98a":"code","6536af6b":"code","fef22cd8":"code","22bb17cd":"code","c4351920":"code","d6150059":"code","96359711":"code","39db544e":"code","21b4c48d":"code","80dab58b":"markdown","4c87aee3":"markdown"},"source":{"935d7407":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_boston\nimport warnings\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor, RandomForestRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","c5590223":"# Function Approach to do the same\ndef _df(load):\n  # Creating Dataframe\n  df = pd.DataFrame(data = load.data, columns = load.feature_names)\n  # Adding Output to the DF\n  df[\"label\"] = load.target\n  # Return df\n  return(df)","fe46efbf":"df = _df(load_boston()) # Running Function Created Above","91d53162":"df.columns","9cb8ad17":"# Data Dictionary\nprint(load_boston().DESCR)","9a02af86":"# Splitting the Data in Train and Test\nfrom sklearn.model_selection import train_test_split\n\n# Getting X and Ys\nX = df.drop(\"label\", axis = 1)\ny = df.label\n\n# Creating Train and Test\nxtrain,xtest,ytrain,ytest = train_test_split(X, y, test_size = 0.33, random_state = None)","297bba3c":"# Apply Linear Regression Model as Base Model\nlr = LinearRegression()\npred_lr = lr.fit(xtrain, ytrain).predict(xtest)\n\n# Checking Model Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint(\"R2 Score: \", r2_score(ytest, pred_lr))\nprint(\"RMSE: \", np.sqrt(mean_squared_error(ytest, pred_lr)))","6aad22b8":"# Checking Sklearn Version\nimport sklearn\nsklearn.__version__","9593ed91":"# Get a List of Models as Base Models\ndef base_models():\n  models = dict()\n  models['lr'] = LinearRegression()\n  models[\"Ridge\"] = Ridge()\n  models[\"Lasso\"] = Lasso()\n  models[\"Tree\"] = DecisionTreeRegressor()\n  models[\"Random Forest\"] = RandomForestRegressor()\n  models[\"Bagging\"] = BaggingRegressor()\n  models[\"GBM\"] = GradientBoostingRegressor()\n  return models\n","e80dc5fb":"# Now we will apply K Fold Cross Validation. We will now create a evaluate function with Repeated Stratified K Fold\n# And Capture the Cross Val Score\nfrom sklearn.model_selection import RepeatedKFold\nfrom matplotlib import pyplot\n\n# Function to evaluate the list of models\ndef eval_models(model):\n  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n  scores = -cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, \n                            error_score='raise')\n  return scores","710ec98a":"# Lets Apply and Calculate the Scores\n# Getting X and Ys\nX = df.drop(\"label\", axis = 1)\ny = df.label\n\n# get the models to evaluate\nmodels = base_models()\n# evaluate the models and store results\nresults, names = list(), list() \n\nfor name, model in models.items():\n  scores = eval_models(model)\n  results.append(scores)\n  names.append(name)\n  print('>%s %.3f (%.3f)' % (name, scores.mean(), scores.std()))\n","6536af6b":"# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.xticks(rotation = 90)\npyplot.ylabel(\"MSE\")\npyplot.title(\"Model Performance\")\npyplot.show()","fef22cd8":"# get a stacking ensemble of models\ndef get_stacking():\n\t# define the base models\n  level0 = list()\n  level0.append(('Tree', DecisionTreeRegressor()))\n  level0.append(('RF', RandomForestRegressor()))\n  level0.append(('XGB', XGBRegressor()))\n  level0.append(('Bagging', BaggingRegressor()))\n\t# define meta learner model\n  level1 = LGBMRegressor()\n\t# define the stacking ensemble\n  model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n  return model","22bb17cd":"def base_models():\n  models = dict()\n  models[\"Tree\"] = DecisionTreeRegressor()\n  models[\"Random Forest\"] = RandomForestRegressor()\n  models[\"Bagging\"] = BaggingRegressor()\n  models[\"XGB\"] = XGBRegressor()\n  models[\"Stacked Model\"] = get_stacking()\n  return models","c4351920":"# Function to evaluate the list of models\ndef eval_models(model):\n  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n  scores = -cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, \n                            error_score='raise')\n  return scores\n\n\n# get the models to evaluate\nmodels = base_models()\n# evaluate the models and store results\nresults, names = list(), list() \n\nfor name, model in models.items():\n  scores = eval_models(model)\n  results.append(scores)\n  names.append(name)\n  print('>%s %.3f (%.3f)' % (name, scores.mean(), scores.std()))\n\n\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.xticks(rotation = 90)\npyplot.ylabel(\"MSE\")\npyplot.title(\"Model Performance\")\npyplot.show()","d6150059":"# Lets apply on Train and Test Split\nlevel0 = list()\nlevel0.append(('Tree', DecisionTreeRegressor()))\nlevel0.append(('RF', RandomForestRegressor()))\nlevel0.append(('GBM', GradientBoostingRegressor()))\nlevel0.append(('Bagging', BaggingRegressor()))\nlevel0.append((\"XGB\", XGBRegressor()))","96359711":"level1 = LGBMRegressor()\nmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)","39db544e":"# Making Predictions on Train & Test\nstacked_pred = model.fit(xtrain, ytrain).predict(xtest)","21b4c48d":"# Checking Model Metrics\nprint(r2_score(ytest, stacked_pred))\nprint(np.sqrt(mean_squared_error(ytest, stacked_pred)))","80dab58b":"# Base Model: Linear Regression\n\nThe Base Model reports R Sqaured Value: 0.67 and RMSE: 5.6.\nNow, we will use Stacking to Improve the Performance of the Model(s).\n\nStacking generally uses lots of models and finds the best predicted values from each model and creates a meta learner to learn from the training set + predicted values to generate a Super Model that improves the overall performance.","4c87aee3":"## Conclusion\n\nHence, it is clear that LightGBM is a better stacked model as it has increased the R2 Score to 0.82 & RMSE has been reduced to 3.78."}}