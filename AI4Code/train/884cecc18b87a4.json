{"cell_type":{"1ece390f":"code","2321cc18":"code","3119b2fc":"code","3f2176b4":"code","d1082f0a":"code","1c678478":"code","99fb787d":"code","d5cfd380":"code","bdedae43":"code","4a94d5c2":"code","2f28f2e7":"code","f3aa2476":"code","791f6f53":"code","8409dad1":"code","2a9e944e":"code","622f2f06":"code","7f1cc095":"code","48d4a443":"code","f44a7906":"code","a59d2fdd":"code","335bf8a1":"code","ef625be3":"code","7655332c":"code","3c32e8c7":"code","550e56f7":"code","0bc2f8e9":"code","c9cb8846":"code","19757527":"code","ae25a434":"code","cf8292dd":"code","c6883cf2":"code","f3cd197e":"code","3ab5190a":"code","6e337ec4":"code","63130aa7":"code","a1c0a533":"code","453e1275":"code","58379e56":"code","cee15520":"code","cae002d2":"code","c33de2e3":"code","5bc6c6aa":"code","7dc91265":"code","f846f035":"code","afbd4933":"code","f41935f0":"code","f39f695c":"code","72ee934c":"code","1e0ceada":"code","7926206f":"code","3e87d3b3":"code","fdd66914":"code","9ffb11ca":"code","763504bd":"code","38154f80":"code","6222c45e":"code","7ccd5619":"code","8152a9d5":"code","56114361":"code","0496779d":"code","7fa358e2":"code","1a00433c":"code","f1876644":"code","43c575ea":"code","68360ba7":"code","ee7791f8":"code","2296e3f8":"code","5f2fc2b8":"code","2f73d8ed":"code","90514429":"code","89b6c9b5":"code","0236bae7":"code","4cb9e052":"code","b34f6777":"code","a0cfe6db":"code","21572442":"markdown","62d98640":"markdown","9b05c8db":"markdown","a47d5a98":"markdown","25053df3":"markdown","efdc273a":"markdown","98bba348":"markdown","c5fc65c5":"markdown","bcd124ee":"markdown","db75390d":"markdown","4ee77523":"markdown","64f9de32":"markdown","06a8a822":"markdown","d1dd9fa9":"markdown","f5237490":"markdown","5f40f76e":"markdown","426d6257":"markdown","fa8fa837":"markdown","d4042773":"markdown","58bd415b":"markdown","13d5ce8e":"markdown","ac80d25f":"markdown","d878fe6f":"markdown","babcfd98":"markdown","de1c482b":"markdown","2d4f34bc":"markdown","d002e9c6":"markdown","02ffe322":"markdown","1671aa92":"markdown","8d8e0dcc":"markdown","60587d99":"markdown","70c5cc37":"markdown","c76531cc":"markdown","3d9f42a8":"markdown","9f700b07":"markdown","67a308f5":"markdown","a890b4db":"markdown","93f6e48e":"markdown","61bd3319":"markdown","db364231":"markdown","d7240c16":"markdown","34e03359":"markdown","52000750":"markdown","1650a10f":"markdown","f880e8c1":"markdown","74d76c19":"markdown","e1877be1":"markdown","6f03f2ec":"markdown","3acb2a2a":"markdown","fecce0aa":"markdown","14b7c58c":"markdown","6b2c9ee8":"markdown","ec612f8e":"markdown","5ec38af7":"markdown","348d1b92":"markdown","6e6f17c8":"markdown","e1df0bd3":"markdown","6c087853":"markdown","0daf774e":"markdown","48b28ec2":"markdown","17454427":"markdown","d8dacfec":"markdown","ad4a5b2f":"markdown","bc9ef4ac":"markdown","e9172ee8":"markdown","bb3e1241":"markdown","05ca7680":"markdown","1a6e6fee":"markdown","24551804":"markdown","0d50e590":"markdown"},"source":{"1ece390f":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom math import sqrt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nfrom matplotlib import style\nstyle.use('seaborn')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\npd.options.display.float_format = '{:,.2f}'.format\n\nrandom_state = 5","2321cc18":"PATH = '..\/input\/'","3119b2fc":"files = ['train', 'test', 'sample_submission']\ntabs = [pd.read_csv(f'{PATH}{f}.csv', low_memory=False) for f in files]\nfor t in tabs: display(t.head())","3f2176b4":"train = tabs[0]\ntrain.describe()","d1082f0a":"test = tabs[1]\ntest.describe()","1c678478":"data = [train, test]","99fb787d":"for t in data: display(t.shape)","d5cfd380":"bins = train['Target'].unique()\ntrain['Target'].hist(bins=len(bins))\nplt.xticks(bins)\nplt.show()","bdedae43":"for t in data: display(t.isna().sum()[t.isna().sum() != 0] \/ t.shape[0])","4a94d5c2":"for t in data: display(train[['age', 'rez_esc']].groupby('age').mean().head(25))","2f28f2e7":"for t in data: t['rez_esc'].fillna(0, inplace=True)","f3aa2476":"for t in data:\n    mask = t['v2a1'].isna()\n    name = \"Training\" if \"Target\" in t.columns else \"Test\"\n    print(f\"\"\"\n{name} Set\n__________\nTotal missing v2a1 observations: {t[mask].shape[0]}\n\nOf which:\n> own and fully paid house: {t[mask & (t['tipovivi1'] == 1)].shape[0]}\n> own, paying in installments: {t[mask & (t['tipovivi2'] == 1)].shape[0]}\n> rented: {t[mask & (t['tipovivi3'] == 1)].shape[0]}\n> precarious: {t[mask & (t['tipovivi4'] == 1)].shape[0]}\n> other (assigned,  borrowed): {t[mask & (t['tipovivi5'] == 1)].shape[0]}\n    \"\"\")","791f6f53":"for t in data: t['v2a1'].fillna(0, inplace=True)","8409dad1":"train[['v18q1', 'v18q']].fillna('missing').groupby('v18q1').sum()","2a9e944e":"for df in data: df['v18q1'].fillna(0, inplace=True)","622f2f06":"for t in data: display(t[['idhogar', 'age', 'escolari']][t['meaneduc'].isna()])","7f1cc095":"for t in data:\n    di = dict(t[['idhogar', 'escolari']][(t['meaneduc'].isna()) & (t['age'] >= 18)].groupby('idhogar').mean()['escolari'])\n    no18 = set(t['idhogar'][t['meaneduc'].isna()]) - set([x for x in di])\n    di.update([[x, 0] for x in no18])\n    for d in di:\n        t.loc[(t['idhogar'] == d), 'meaneduc'] = di[d]\n        t.loc[(t['idhogar'] == d), 'SQBmeaned'] = di[d]**2","48d4a443":"for t in data: display(t.isna().sum().sum())","f44a7906":"cats = set()\nfor t in data: cats.update({var for var in t.drop(columns=['Id', 'idhogar']).columns if t[var].dtypes == \"O\"})\ncats","a59d2fdd":"for c in cats: display(c, train[c].unique())","335bf8a1":"select = ['idhogar', 'parentesco1', 'male', 'escolari', 'edjefe', 'edjefa', 'SQBedjefe']\nfor t in data: display(t[select][t['parentesco1'] ==1].head(10))","ef625be3":"for t in data: \n    for f in ['edjefe', 'edjefa']: display(t[[f, 'male']][(t[f] == 'no') & (t['parentesco1'] == 1)].groupby('male').count())","7655332c":"for t in data:\n    display(t[['escolari', 'edjefe', 'edjefa']][(t['parentesco1'] == 1) &\n                                        (((t['edjefe'] == 'no') & (t['male'] == 1)) \n                                         | ((t['edjefa'] == 'no') & (t['male'] == 0)))\n                                       ].groupby(['edjefe', 'edjefa']).mean())","3c32e8c7":"for t in data: \n    display(t[select][t['edjefe'] == 'yes'].head(10))\n    display(t[select][t['edjefa'] == 'yes'].head(10))","550e56f7":"for t in data:    \n    for f in ['edjefe', 'edjefa']:\n        display(t[[f, 'escolari']][(t[f] == 'yes') & (t['parentesco1'] == 1)].groupby(f).mean())","0bc2f8e9":"for t in data: display(t[select][(t['edjefa'] == 'no') & (t['edjefe'] == 'no') & (t['parentesco1'] == 0)].head(10))","c9cb8846":"for t in data:\n    di = dict(t[['idhogar', 'escolari']][t['parentesco1'] == 1].values)\n    pe = t[['idhogar', 'parentesco1']].groupby('idhogar').agg('max')\n    di.update(dict(pe[pe['parentesco1'] != 1]['parentesco1']))\n    t['edjef'] = t['idhogar'].map(lambda x: di[x])\n    t.drop(columns=['edjefe', 'edjefa'], inplace=True)","19757527":"for t in data: display(t[['dependency', 'SQBdependency']][t['dependency']\n                                                              .isin(['yes', 'no'])].groupby('dependency').agg(['min', 'mean', 'max']))","ae25a434":"for t in data:\n    t.loc[t['dependency'] == 'yes', 'dependency'] = 1\n    t.loc[t['dependency'] == 'no', 'dependency'] = 0\n    t['dependency'] = t['dependency'].astype('float')","cf8292dd":"for t in data: display([var for var in t.drop(columns=['Id', 'idhogar']).columns if t[var].dtypes == \"O\"])","c6883cf2":"id_ = ['Id', 'idhogar', 'Target']","f3cd197e":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","3ab5190a":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","6e337ec4":"train_orig = train.copy()\ntest_orig = test.copy()","63130aa7":"frame = test.copy()\nframe['Target'] = np.nan\nframe = frame.append(train)","a1c0a533":"frame_agg = frame[frame['parentesco1'] == 1].drop(columns=(sqr_ + ind_bool + ind_ordered))","453e1275":"def agg_features(feats, agg=['min', 'mean', 'max', 'sum']):\n    global frame_agg\n    ind_agg = frame[['idhogar'] + feats].groupby('idhogar').agg(agg)\n    new_cols = []\n    for col in ind_agg.columns.levels[0]:\n        for stat in ind_agg.columns.levels[1]:\n            new_cols.append(f'{col}_{stat}')\n\n    ind_agg.columns = new_cols\n    ind_agg.reset_index(inplace=True)\n    frame_agg = frame_agg.merge(ind_agg, on='idhogar', how='left')","58379e56":"agg_features(ind_ordered, agg=['min', 'mean', 'max', 'sum'])","cee15520":"agg_features(ind_bool, agg=['mean', 'sum'])","cae002d2":"corr_matrix = frame_agg.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\nto_drop","c33de2e3":"frame_agg.drop(columns=to_drop, inplace=True)","5bc6c6aa":"test = frame_agg[frame_agg['Target'].isna()].drop(columns='Target').copy()\ntrain = frame_agg[~frame_agg['Target'].isna()].copy()","7dc91265":"from sklearn.cluster import KMeans","f846f035":"for t in train[\"Target\"].unique():\n    wcss = []\n    for i in range(1, 11):\n        kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state=random_state)\n        kmeans.fit(train.drop(columns=['Id', 'idhogar'])[train[\"Target\"] == t])\n        wcss.append(kmeans.inertia_)\n    fig = plt.figure()\n    plt.plot(range(1,11), wcss)\n    plt.title(f'Target: {t}')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('WCSS')\nplt.show()","afbd4933":"def cluster(kmeans_groups):\n    di = {}\n    for group in kmeans_groups:\n        kmeans = KMeans(n_clusters = group[1], init = 'k-means++', max_iter = 300, n_init = 10, random_state=random_state)\n        di.update(dict(zip(train.index[train['Target'] == group[0]], \n                           [f'{group[0]}_{k}' for k in kmeans.fit_predict(train.drop(columns=['Id', 'idhogar'])[train['Target'] == group[0]])])))\n    train['clusters'] = train.index.map(lambda x: di[x])\n\nkmeans_groups = [[1, 2], [2, 2], [3, 2], [4, 6]]\n\ncluster(kmeans_groups)","f41935f0":"train[['Target', 'clusters', 'v2a1']].groupby(['Target', 'clusters']).agg(['count', 'mean'])","f39f695c":"cl = '4_3'\n\ntrain['idhogar'][train['clusters'] == cl]","72ee934c":"for t in [train[train['clusters'] == cl], train]: display(t.describe())","1e0ceada":"train = train[~(train['idhogar'] == '563cc81b7')].drop(columns='clusters')","7926206f":"cluster(kmeans_groups)","3e87d3b3":"train[['Target', 'clusters', 'v2a1']].groupby(['Target', 'clusters']).agg(['count', 'mean'])","fdd66914":"split = 0.2\nval = []\nfor c in train['clusters'].unique(): val += list(train['Id'][train['clusters'] == c].sample(frac=split, random_state=random_state))\nidxs = train['Id'].isin(val)","9ffb11ca":"t_set = train[~idxs].copy()\nv_set = train[idxs].reset_index(drop=True).copy()\n\nt_set.shape, v_set.shape","763504bd":"def oversample(df):\n    d = df.copy()\n    max_group = d[['clusters', 'v2a1']].groupby(['clusters']).count().max().values[0]\n    for c in d['clusters'].unique():\n        ratio = 1 if c[0] != '4' else 3\n        to_sample = max(int(max_group\/ratio) - d['clusters'][d['clusters'] == c].count(), 0)\n        d = d.append(df[df['clusters'] == c].sample(\n            n=to_sample, replace=True, random_state=random_state), ignore_index=True)\n    return d\n             \nt_set = oversample(t_set)","38154f80":"t_set.shape, v_set.shape","6222c45e":"for t in [t_set, v_set]: \n    display(t['Target'].hist())\n    plt.figure()\nplt.show()","7ccd5619":"to_drop = ['Id', 'idhogar', 'parentesco1_mean', 'parentesco1_sum']\nX_train = t_set.copy().drop(columns=(to_drop + ['Target', 'clusters']))\nX_val = v_set.copy().drop(columns=(to_drop + ['Target', 'clusters']))","8152a9d5":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\ndef to_float(df): \n    for c in df.columns: df[c] = df[c].astype('float64')\n    return df\n\nX_train = sc.fit_transform(to_float(X_train))\nX_val = sc.transform(to_float(X_val))","56114361":"y_train = t_set['Target'].copy().astype('category').cat.as_ordered().astype('float64') - 1\ny_val = v_set['Target'].copy().astype('category').cat.as_ordered().astype('float64') - 1","0496779d":"for t in [X_train, y_train, X_val, y_val]: display(t.shape)","7fa358e2":"from sklearn.metrics import f1_score","1a00433c":"import torch as t\nimport torch.nn as nn\nfrom torch import autograd, optim\nimport torch.nn.functional as F","f1876644":"class Net(nn.Module):\n    \n    def __init__(self, X, y, nodes=[200], p=0, act='lrelu'):\n        super().__init__()\n        self.input_size = X.shape[1]\n        self.output_size = int(y.nunique())\n        self.nodes = nodes\n        self.p = p\n        self.act = act\n        \n        if self.p > 1 or self.p < 0: raise Exception(f\"Expected p to be <0,1> but got {self.p}\")\n        \n        if nodes == []: raise Exception(\"No node sizes input\")\n        \n        self.h1 = nn.Linear(self.input_size, self.nodes[0])       \n        self.hid = nn.ModuleList([nn.Linear(self.nodes[i], self.nodes[i + 1]) for i in range(len(self.nodes) - 1)])\n        self.out = nn.Linear(self.nodes[-1], self.output_size)\n        \n        self.bn = nn.ModuleList([nn.BatchNorm1d(self.nodes[i]) for i in range(1, len(self.nodes))])\n        \n#         for mod in enumerate(self.modules()):\n#             if mod[0] > 0: mod[1].weight = nn.init.kaiming_normal(mod[1].weight)\n        \n        if self.act == 'lrelu': self.act1 = F.leaky_relu\n        elif self.act == 'relu': self.act1 = F.relu\n        elif self.act == 'tanh': self.act1 = F.tanh\n        else: raise Exception(\"Invalid activation function - choose between lrelu, relu or tanh\")\n        \n        self.act2 = F.softmax\n        \n    def forward(self, X):\n        self.inp = autograd.Variable(t.cuda.FloatTensor(X))\n        \n        o = self.act1(self.h1(self.inp))\n        \n        for h, b in zip(self.hid, self.bn):\n            o = F.dropout(b(self.act1(h(o))), p=self.p)\n        \n        o = self.act2(self.out(o), dim=1)\n        return o","43c575ea":"def train_nn(model, n_epochs=10000, lr=1e-6, bs=900, wd=0, update_cycle=200, use_SGD=True, t_cycles=1, updates=True, cycle_double=True):\n    loss_plt = []\n    f1s = []\n    vf1s = []\n    opt = optim.Adam(params=model.parameters(), lr=lr, weight_decay=wd)\n    targ = autograd.Variable(t.cuda.FloatTensor(y_train.values).long())\n\n    for i in range(1, t_cycles + 1):\n        if t_cycles > 1 and updates: print('='*100, f'\\nCycle: {i}, Current Learning Rate: {lr}, Epochs at Cycle: {n_epochs} - updates every {update_cycle} epoch')\n        \n        for epoch in range(1, n_epochs + 1):\n            targ.require_grad = False\n\n            if use_SGD:\n                idxs = np.arange(len(X_train))\n                np.random.shuffle(idxs)\n                for b in range(0, len(X_train), bs):\n                    loss = 0\n                    targb = autograd.Variable(t.cuda.FloatTensor(y_train.values[idxs[b:b+bs]]).long())\n                    targb.require_grad = False\n                    out = model(X_train[idxs[b:b+bs]])      \n                    loss = F.nll_loss(out, targb)\n            else:\n                loss = 0\n                out = model(X_train)\n                loss = F.nll_loss(out, targ)\n            \n            model.zero_grad()\n            loss.backward()\n            opt.step()\n            if epoch % update_cycle == 0:\n                out = model(X_train)\n                loss = F.nll_loss(out, targ)\n                loss_plt.append(loss.cpu().data.numpy())\n                vout = model(X_val)\n                f1 = f1_score(y_train.values, out.max(dim=1)[1].cpu().data.numpy(), average='macro')\n                vf1 = f1_score(y_val.values, vout.max(dim=1)[1].cpu().data.numpy(), average='macro')\n                f1s.append(f1)\n                vf1s.append(vf1)\n                if updates: print(\"_\"*50,'\\n', f'Epoch: {epoch}, Training Loss: {loss.cpu().data.numpy()}, Training F1: {f1}, Validation F1: {vf1}')\n        lr \/= 10\n        if cycle_double: \n            n_epochs *= 2\n            update_cycle *= 2\n    print(f'Final validation F1: {vf1s[-1:][0]}')\n    return model, vf1s[-1:][0]","68360ba7":"\ndef train_ensamble(nodes = [89, 89], p = 0.7, wd = 1e-7, lr = 1e-1, bs = (int(X_train.shape[0] \/ 4 + 1)),\n             t_cycles = 6, epochs_per_cycle = 1, cycle_double = True, cycles = 1, n_models = 31, stop_threshold = 0.46):\n    \n    global models\n    models = []\n    for n in range(n_models): models.append(Net(X_train, y_train, nodes=nodes, p=p).cuda())\n\n\n    # main training loop:\n    final_res = []\n    mod = 1\n    for model in models:\n        print('='*50, f'\\nMODEL {mod}')\n        model.train(mode=True)\n        for i in range(1, cycles + 1): \n            print(f'Cycle {i}')\n            _, res = train_nn(model, n_epochs = epochs_per_cycle, \n                              lr = lr, bs = bs, wd=wd, update_cycle=epochs_per_cycle, t_cycles = t_cycles, \n                              updates=False, cycle_double=cycle_double)\n            if res >= stop_threshold: break\n        final_res.append(res)\n        mod +=1\n    plt.scatter(np.arange(len(final_res)), final_res)\n    plt.plot([np.mean(final_res) for a in range(len(final_res))], color='r', linestyle=':')\n    plt.show()","ee7791f8":"%%time\ntrain_ensamble()","2296e3f8":"def ensamble_predict(models, x):\n    pred = np.zeros((x.shape[0], 4))\n    for model in models: \n        pred += (model.train(mode=False)(x).cpu().data.numpy() \/ len(models))\n    return np.array([pred[r].argmax(axis=0) for r in range(len(pred))])","5f2fc2b8":"f1_score(y_val.values, ensamble_predict(models, X_val), average='macro')","2f73d8ed":"from sklearn.metrics import confusion_matrix\n\ndef conf_mat(targ, preds):\n    cm = confusion_matrix(targ, preds)\n    cmp = pd.DataFrame(cm)\n    cmp.index.name = 'Actuals'\n    cmp.columns.name = 'Preds'\n\n    plt.figure(figsize=(5, 5))\n    plt.title(\"CONFUSION MATRIX\")\n    sns.heatmap(cmp, annot=True, cmap=\"seismic\", fmt='.0f')\n\n    plt.tight_layout()","90514429":"targ = y_val.values\npreds = ensamble_predict(models, X_val)\n\nconf_mat(targ, preds)","89b6c9b5":"scores = []\ndef f_i(df, num_features = 15):\n    global scores\n    scores = []\n    Xb = to_float(t_set.copy().drop(columns=(to_drop+['Target', 'clusters'])))\n    targ = f1_score(y_train.values, ensamble_predict(models, sc.transform(Xb)), average='macro')\n\n    for c in Xb.columns:\n        X = Xb.copy()\n        X[c] = X[[c]].sample(frac=1, random_state=random_state).set_index(X.index)[c]\n        scores.append([c, targ - f1_score(y_train.values, ensamble_predict(models, sc.transform(X)), average='macro')])\n\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    pos = np.arange(len(scores))\n    scores_names = [x[0] for x in scores]\n    scores_f1 = [x[1] for x in scores]\n\n    plt.figure(figsize=(8, 6))\n    plt.title('All Features')\n    plt.plot(scores_f1)\n    plt.tight_layout()\n    \n    plt.figure(figsize=(8, 6))\n    plt.title(f'Top {num_features} Features')\n    plt.barh(pos[:num_features], scores_f1[:num_features], align='center')\n    plt.yticks(pos[:num_features], scores_names[:num_features])\n    plt.gca().invert_yaxis()\n    \n    plt.show()","0236bae7":"f_i(t_set, num_features=25)","4cb9e052":"X_test = sc.transform(test.drop(columns=to_drop))\ndi = dict(zip(test['idhogar'], ensamble_predict(models, X_test) + 1))\nsub = test_orig[['Id', 'idhogar']].copy()\nsub['Target'] = sub['idhogar'].map(di).fillna(4)\nsub.drop(columns='idhogar', inplace=True)","b34f6777":"from datetime import datetime\nnow = datetime.now().strftime('%y-%m-%d-%H_%M')\n\nname = f'submission_{now}'\nsub = sub.set_index('Id')\nsub['Target'] = sub['Target'].astype('int')\nsub.to_csv(name + '.csv')","a0cfe6db":"pd.read_csv(name + '.csv').head()","21572442":"And now to check the validation F1 score of the ensamble:","62d98640":"Based on the above results, our largest group (`Target` = 4) will be split into 6 clusters and the remaining ones into 2 each:","9b05c8db":"As per the competition rules, the results will be evaluated using the macro [F1 score](https:\/\/en.wikipedia.org\/wiki\/F1_score) (surprisingly, not the balanced version).\n\nTo keep track of how we are doing during and after training, we will import this metric from sklearn:","a47d5a98":"**Missing values check**\n\nSome of the features in the training dataset appear to be incomplete:","25053df3":"Clearly the majority of cases relate to the family either living in their own house or in an assigned\/borrowed house. Again, we will simply fill in the missing values with 0:","efdc273a":"The final step here is to split up the frame back to the train and test sets using `Target` to differentiate between the two:","98bba348":"Inspired by colleagues in this Kaggle competition, we will deal with this issue by replacing `edjefe` and `edjefa` with a single feature that we will name `edjef` - this feature will indicate the years of education of the head of the house, irrispective of the sex (0 if there is none):","c5fc65c5":"For the boolean individual features we will not need the minimum and maximum values (by definition 0 and 1 for all of them), we'll retain the average and combined household values though:","bcd124ee":"Looks like we stumbled upon a very wealthy household, not very representative of our dataset. We will exclude this observation from our dataset as an outlier:","db75390d":"The rest of the variables in our dataset are already at household level so we can take them as is from the household heads observations.\n\nIn order to perform the aggregation effectively, we will temporarily merge the training and test sets into a single frame:","4ee77523":"Ok, it looks like there are cases when both `edjefe` and `edjefa` have the value 'no' - it seems to be when the family head spent no time in education.\n\nLet's check the 'yes' cases now:","64f9de32":"**Aggregating the dataset**\n\nAs per the competition rules, the entries will be evaluated at household (identified through the family head) - and not individual - level. This means aggregating our dataset at household level for model training purposes should be a wise choice.\n\nIn order to do that, we must understand how to aggregate the individual features that we have. To do this effectively, we'll split them into buckets. The approach taken here has largely been inspired by [Will Koehrsen's kernel](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-complete-walkthrough).\n\nWe start off with the features that will not be used as independant variables in the training but are essential to infer from the test set:","06a8a822":"## Problem Statement","d1dd9fa9":"## Prediction on the Test Set","f5237490":"**Oversampling**\n\nTime to perform some oversampling! As mentioned earlier, the minority `Target` classes clusters will be oversampled to match the largest cluster in the set. Within the majority class we will also do some balancing:","5f40f76e":"> `dependency`\n\nIn case of this feature we also have a squared value (`SQBdependancy`) to help us with the interpretation:","426d6257":"## Reviewing the Results","fa8fa837":"**Model architecture**\n\nWe start off by defining the model class. To allow some experimentation with the model structure, the number of layers and neurons within layers will be defined by a list where each element represents the number of neurons for a particular layer. We also define a forward method that is executed immediately after class instantiation. \n\nTo limit potential overfitting we will include dropout for all layers of the network. \n\nTo improve training speed and accuracy we will implement batch normalisation at each hidden layer of the network.","d4042773":"We will fill in the missing values with 0:","58bd415b":"Easily enough, we should interpret 'yes' as 1 (i.e. full dependancy) and 'no' as 0:","13d5ce8e":"Let's review our newly identified cluster by number of observations:","ac80d25f":"The next step is to define the predict function for the ensamble, which - as mentioned - will basically be an average of predictions across all models:","d878fe6f":"## Balancing the Training Dataset","babcfd98":"The training set is almost x2.5 the size of the training set... This is going to be a true challenge!","de1c482b":">`edjefe` & `edjefa`\n\nIn order to understand the 'yes' and 'no' observations, we will compare them with some of the other features in our dataset. \n\nLet's look at the family heads first:","2d4f34bc":"The same features are incomplete at a similar rates in both the training and test set, hinting that this might not be a case of data quality issues.\n\nLet's look closer at the features with missing values:\n1. `v2a1` - Monthly rent payment\n1.  `v18q1` - Number of tablets household owns\n1. `rez_esc` - Years behind in school\n1. `meaneduc` - Average years of education for adults (18+)\n1. `SQBmeaned` - Average years of education for adults squared\n\nTackling them in turns:","d002e9c6":"There seem to be some cases where there's a 'no' in `edjefe` when the head is a male and in `edjefa` when the head is a female. Let's check the average years of education is such cases:","02ffe322":"And for a final check to verify that we dealt with all non-numerical features...","1671aa92":"Before applying a neural network model it's preferable to apply feature scaling to the input matrix - and this will be our next step:","8d8e0dcc":"> `rez_esc`\n\nReviewing the values in years behind in school against the age hints at the reason why so many observations are missing for this feature: the people with missing values are most likely not yet - or no longer - in the education system:","60587d99":">`v2a1`\n\nFor the monthly rent payment, over 72% of records appear to be missing. One potential explanation for this could be when the family actually owns a house and pays no rent as such. Let's look at some of the other features provided - specifically `tipovivi1-5` which describe the household ownership type:","70c5cc37":"Let's rerun the clustering analysis: ","c76531cc":"**Handling categorical data**\n\nBefore we proceed to modelling, we need to ensure all of the features we plan to use in modelling are numerical. `Id` and `idhogar` are certainly string values but this doesn't bother us as they will not be used in training of the model. \n\nHowever, there are some other non-numerical features in our dataset as well: ","3d9f42a8":"This kernel focuses on solving a problem as outlined by the [Costa Rican Household Poverty Level Prediction](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction) Kaggle competition.\n\nAs per the competition overview page:\n> The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\nMost certainly - let's do this!","9f700b07":"Apparently, there also seem to be cases of households with no family heads - in this case both `edjefe` and `edjefa` are also a 'no':","67a308f5":"**Confusion matrix**\n\nTime to review the model results. Firstly, let's look at the confusion matrix:","a890b4db":"## Imports","93f6e48e":"This kernel will be focused on modelling using a neural network framework that we will construct using the famous PyTorch library:","61bd3319":"As mentioned earlier, before doing any modelling we will need to balance the training dataset. There are various ways to go about this issue. The method chosen for this kernel is performing oversampling of the minority classes with the aid fo k-means clustering, described at length - along with other possible methodologies - [here](https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-classification-problem\/).\n\nThe basic idea here is that instead of random oversampling of our minority classes (`Target` 1, 2 and 3), we will identify clusters of similar observations within these groups and oversample to a point where the clusters are of equal size. Our primary focus is to have a similar number of observations for all classes, therefore as a point of reference we will enlarge the minority classes to the size of the largest cluster identified in the majority class (`Target` 4). \n\n\n**K-means clustering**\n\nWe will start off by analysing the training set by looking at each `Target` group individually and finding potential clusters within these groups. We will identify the number of such clusters by using the [elbow method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)):","db364231":"> `v18q1`\n\nIn case of the missing values in the number of tablets a household owns, we also have a supporting feature - namely `v18q` - indicating if a person owns a tablet or not:","d7240c16":"Alright, now it's finally time to use the algorithm on the Test Set. Since the results need to be shared by individual, we will first predict the `Target` for households and then map them to the individuals using `idhogar`:","34e03359":"It appears that for family heads, we basically get 'no' for `edjefa` if the head is a male and a 'no' for `edjefe` if the head is a female. Let's verify:","52000750":"## Modelling","1650a10f":"Lastly, let's prepare the dependant variable sets:","f880e8c1":"To further limit collinearity issues, we'll get rid of features that are 99% correlated: ","74d76c19":"**Training the model**\n\nNow time to define the model training function. It's basic features are as follows:\n* by default, it implements Stochastic Gradient Descent (with index reshuffling at each epoch)\n* updates that include the current loss and F1 score on the training and validation sets are printed in epoch intervals specified by `update_cycle`\n* the 'standard' training cycle is where parameter adjustment runs for the stated number of epochs and at the stated learning rate\n* apart from the standard training, it is possible to define `t_cycles` as an integer higher than the default 1. This will mean that after the standard training, there will be an additional sequence of training cycles, with the number of epochs doubled and learning rate halved at each cycle. This way as we close in on the minimum, we reduce the learning rate to not overshoot.","e1877be1":"Then we have the squared features. Since we will be using a neural network to model the data, removing these from training can prove to be not only possible but even beneficial due to getting rid of potential issues related to collinearity:","6f03f2ec":"The 'yes' in both `edjefe` and `edjefa` seems to indicate that this family head has 1 year of education. Let's verify:","3acb2a2a":"By now, we should be rid of all the missing values - let's verify this is the case:","fecce0aa":"The categorical features in our dataset appear to actually be continuous with some observations noted as \"yes\" or \"no\" instead of numbers. Let's look at them a little closer, starting with:","14b7c58c":"We start off by reducing the dataset to family heads only and excluding the individual and squared features:","6b2c9ee8":"Before commencing with the training, we will implement a couple of ideas on top of the current framework.\n\nThe first one is a quasi-cyclical learning rate, inspired by [this paper](https:\/\/arxiv.org\/abs\/1506.01186). In our implementation this will mean the cycle of gradually reducing the learning rate and increasing the number of epochs described earlier will be repeated a number of times, specified by the `cycles` parameter.\n\nIn addition, we will also add in an element of ensamble learning. The training sample is relatively small for neural network training purposes, leading to relatively high variance in the results from a single trained neural network, therefore we will train a number of such networks (specified by `n_models`) and average the predictions over the whole group.\n\nLastly, the threshold parameter (`stop_threshold`) will indicate the level of the validation F1 score sufficient to cease training of a single network in order to move to training the next one.\n\nWe will plot the F1 validation scores of particular models and the average across the group.","ec612f8e":"Finally, time to prepare the submission csv file:","5ec38af7":"Then we group the features that refer to individuals and as such will require a form of aggregation. Within this group we distinct between boolean and ordered continuous features as the approach in aggregating these will differ:","348d1b92":"Time to see what sort of features we're up against here:\n1. `dependency` - dependency rate\n1. `edjefe` - years of education of male head of household\n1. `edjefa` - years of education of female head of household","6e6f17c8":"## Data Review","e1df0bd3":"The training set looks to be a lot more balanced now with respect to the `Target` feature.","6c087853":"**Feature scaling and final modelling sets**\n\nTime to prepare the final datasets for training and validation. First, let's dispose of the features that will not be used in modelling:","0daf774e":"Conclusions:\n1. Not surprisingly, the male and female features are practicaly collinear - it makes a lot of sense to keep just one version.\n1. `hogar_total`, `tamhog` and `r4t3` are duplications of `hhsize`. \n1. `area2` is perfectly collinear with `area1`\n\nIn light of this, we will now dispose of these variables from our set:","48b28ec2":"**Feature importance**\n\nLet's review the most important features according to our trained network. The way we define feature importance here is by measuring how much the trainin F1 score deteriorates if a given feature is randomly shuffled and passed along with the rest of the original features through the model. We will order the features by maximum impact and review the top contributors.","17454427":"## Getting the Data","d8dacfec":"Now, time to create the aggregated individual features. We'll start with the ordered continuous ones, which will be transfomed into household aggregates: minimum, average, maximum and combined values:","ad4a5b2f":"In summary, `v18q1` is only missing when none of the people in the household own a tablet. This means that we can safely fill the missing values with 0 again:","bc9ef4ac":"It seems there is a mix of adults and non-adults in the families in the test set. Therefore, to fill in the missing values we will use the average years of schooling (`escolari`) of the adults or 0 if there are none in the household.","e9172ee8":"Unsurprisingly, we have an overrepresentation of the non-vulnerable households in our dataset. This was to be expected since normally government aid would be targeted at a minority of households that are in need the most. \n\nThis however poses a difficulty when training a machine learning algorithm since our results will most likely be biased towards forecasting the non-vulnerable households if we do not adress this issue. This problem of an imbalanced dataset is something we'll look into before going into modelling.","bb3e1241":"The 4_3 cluster looks unusually small - only 1 observation! Let's take a closer look:","05ca7680":"**Target feature**\n\nWe are asked to predict the feature called `Target` - an ordinal variable indicating groups of income levels as follows:\n1.  = extreme poverty \n2. = moderate poverty \n3. = vulnerable households \n4. = non vulnerable households\n\nLet's look at the `Target` distribution in our training data:","1a6e6fee":"**Creating the validation set**\n\nBefore we move on to oversampling, it will be a good idea to separate the validation set from data we will use in training. To ensure the validation set is representative of the population, we will draw samples applying the split ratio to all clusters individually. The validation set will also remain unbalanced - just like the test set - to make the end results more representative.\n","24551804":"> `meaneduc` & `SQBmeaned`\n\nThis leaves us with the average years of education for adults and it's squared values. As in previous cases, we'll leverage on the other features to gain the insight. In this case we will use `age` (since there is an age census for this variable) and `escolari`, the years of schooling of a particular person:","0d50e590":"The results reveal a somewhat low accuracy of the model, in particular in the minority classes. Judging from F1 scores within this ballpark obtained by other participants in the challenge, this is most likely a result of a relatively small sample used for training."}}