{"cell_type":{"d0b5a93a":"code","6dc3a1b3":"code","f612ec49":"code","98b20aed":"code","050075f5":"code","1c98c6a7":"code","3a25f7e8":"code","bfcd47be":"code","86dfd778":"code","03db29ba":"code","ae9ae22d":"code","3c69fa30":"code","2b865a06":"code","7eeddd36":"code","5650f6e7":"code","b42e49ac":"code","a4b1ef4c":"code","52fbce77":"code","70903fc5":"code","3658c26e":"code","24fb3558":"code","1c2b3b0f":"code","47a9d25f":"code","e5d4f5ad":"code","2efcda4b":"code","db0ed268":"code","48ae0206":"code","b20488eb":"code","f32d23e3":"code","8737a9e7":"code","bea0d73f":"code","46d72f85":"code","cd531126":"code","5ca7f497":"code","ce68e231":"code","b8d29c60":"code","03eca094":"code","14cd1074":"code","5b29acb9":"code","5e513990":"code","bc4465b5":"code","949d181f":"code","11e62562":"code","fbc960d6":"code","84596fe6":"code","fdd96966":"code","eef3d242":"code","dced42e3":"code","a6298bca":"code","a378441e":"code","3484ffb2":"code","9723bd12":"code","c4213131":"code","1a5e402c":"code","1e107939":"code","a6e35d9d":"code","87153f14":"code","9d788371":"code","0d273891":"code","843a32ca":"code","4833342b":"code","d8725afc":"code","ba8eea41":"code","2b7643b9":"code","ade4281b":"code","9ea364a2":"code","ed10bf3b":"code","731a44e0":"code","393d6658":"code","cefee72b":"markdown","5f56f448":"markdown","e0b57ec6":"markdown","0d875773":"markdown","644b7fc3":"markdown","f7fa6930":"markdown","cd52c5cd":"markdown","a75e9cca":"markdown","a116260a":"markdown","b883f1db":"markdown","2b76f0e7":"markdown","4da604dd":"markdown","0ae78ecc":"markdown","eb85d785":"markdown","4703f292":"markdown","2ddcb3bd":"markdown","553199dc":"markdown","99547c7c":"markdown","40e84b92":"markdown","34138a20":"markdown","af20f58e":"markdown","f02a672c":"markdown","71b75086":"markdown","0f7827a7":"markdown","2daaff29":"markdown","0232bb0f":"markdown","3a69855f":"markdown","9e90615d":"markdown","d06fc4a2":"markdown","512c937a":"markdown","7e8a973c":"markdown","0ae7035c":"markdown","baabb15b":"markdown","65fe82d1":"markdown","1db9dc2c":"markdown","8a4cf71e":"markdown","b47a7c68":"markdown","25bd195b":"markdown","ac2c5c8a":"markdown","74ec3506":"markdown","d5972870":"markdown","eb126507":"markdown","1fd65c42":"markdown","f2dc037e":"markdown","01cf1608":"markdown","6ede8921":"markdown","629d26d3":"markdown"},"source":{"d0b5a93a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6dc3a1b3":"DATA_PATH = '\/kaggle\/input\/bi-attrition-predict\/'\ntrain = pd.read_csv(f'{DATA_PATH}\/train.csv')\ntest = pd.read_csv(f'{DATA_PATH}\/test.csv')","f612ec49":"len(train), len(test)","98b20aed":"train.head()","050075f5":"test.head()","1c98c6a7":"train.info()\n#user_id:\u5458\u5de5Id(str) Age:\u5e74\u9f84(int) Attrition:\u662f\u5426\u79bb\u804c(bool) BusinessTravel:\u51fa\u5dee\u60c5\u51b5(categorical) \n#DailyRate\uff1a\uff1f(int) Department: \u90e8\u95e8(categorical) DistanceFromHome:\u5c45\u4f4f\u5730\u4e0e\u5de5\u4f5c\u5355\u4f4d\u8ddd\u79bb(int) Education:\u6559\u80b2\u65f6\u95f4(int)\n#EducationField:\u6559\u80b2\u80cc\u666f(categorical) EmployeeCount:?(int) EmployeeNumber:\u5458\u5de5\u53f7\u7801(int) EnvironmentSatisfaction:\u73af\u5883\u6ee1\u610f\u5ea6(int\/categorical)\n#Gender:\u6027\u522b(bool) HourlyRate:?(int) JobInvolvement:\u5de5\u4f5c\u6295\u5165\u5ea6(int) JobLevel:\u804c\u4f4d\u7b49\u7ea7(categorical) JobRole:\u804c\u4f4d\n#JobSatisfaction:\u5de5\u4f5c\u6ee1\u610f\u5ea6(int\/categorical) MaritalStatus:\u5a5a\u59fb\u72b6\u6001(bool) MonthlyIncome:\u6708\u6536\u5165(int) MonthlyRate:?(int)\n#NumCompaniesWorked:\u4efb\u804c\u8fc7\u7684\u516c\u53f8\u6570(int) Over18:\u662f\u5426\u6210\u5e74(bool) OverTime:\u662f\u5426\u52a0\u73ed(bool) PercentSalaryHike:\u5de5\u8d44\u63d0\u9ad8\u6bd4\u7387(int)\n#PerformanceRating:\u7ee9\u6548\u8bc4\u4f30(int) RelationshipSatisfaction:\u4eba\u9645\u5173\u7cfb\u6ee1\u610f\u5ea6(int) StandardHours:\u6807\u51c6\u5de5\u4f5c\u65f6\u95f4(int)\n#StockOptionLevel:\u80a1\u7968\u5360\u6709\u7b49\u7ea7(int) TotalWorkingYears:\u603b\u8ba1\u5de5\u4f5c\u5e74\u6570(int) TrainingTimeLastYear:\u53bb\u5e74\u57f9\u8bad\u65f6\u957f(int)\n#WorkLifeBalance:\u5de5\u4f5c\u751f\u6d3b\u5e73\u8861\u60c5\u51b5(int) #YearsAtCompany:\u5728\u516c\u53f8\u5de5\u4f5c\u5e74\u6570(int) #YearsInCurrentRole:\u5728\u8fd9\u4e00\u804c\u4f4d\u65f6\u957f(int)\n#YearsSinceLastPromotion:\u8ddd\u79bb\u4e0a\u4e00\u6b21\u5347\u804c\u65f6\u95f4(int) YearsWithCurrManager:\u4e0e\u540c\u4e00\u4e0a\u7ea7\u5de5\u4f5c\u65f6\u957f(int)","3a25f7e8":"#0.80\nid_col = 'user_id'\ntarget_col = 'Attrition'\n\ndigital_cols = ['Age', 'DailyRate', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n                'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ncategory_cols = ['BusinessTravel', 'Department',  'Education', 'EducationField',\n                'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel','DistanceFromHome',\n                'JobRole', 'JobSatisfaction', 'MaritalStatus', 'Over18', 'OverTime',\n                'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'PerformanceRating', 'TrainingTimesLastYear','WorkLifeBalance' ]","bfcd47be":"# Credits to https:\/\/www.kaggle.com\/a763337092\/lr-baseline-for-bi-class#Data-process\n# For categorical data\nfor col in category_cols:\n    nunique_tr = train[col].nunique()\n    nunique_te = test[col].nunique()\n    na_tr = len(train.loc[train[col].isna()]) \/ len(train)\n    na_te = len(test.loc[test[col].isna()]) \/ len(test)\n    print(f'Col name:{col:30}\\tunique cate num in train:{nunique_tr:5}\\tunique cate num in train:{nunique_te:5}\\tnull sample in train:{na_tr:.2f}\\tnull sample in test:{na_te:.2f}')","86dfd778":"#For numerical data\n\nfor col in digital_cols:\n    \n    min_tr = train[col].min()\n    max_tr = train[col].max()\n    mean_tr = train[col].mean()\n    median_tr = train[col].median()\n    std_tr = train[col].std()\n    x = ['min','mean','median','std','max']\n    y = [min_tr,mean_tr,median_tr,std_tr,max_tr]\n\n    \n    \n    min_te = test[col].min()\n    max_te = test[col].max()\n    mean_te = test[col].mean()\n    median_te = test[col].median()\n    std_te = test[col].std()\n    x = ['min','mean','median','std','max']\n    y = [min_tr,mean_tr,median_tr,std_tr,max_tr]\n    \n    na_tr = len(train.loc[train[col].isna()]) \/ len(train)\n    na_te = len(test.loc[test[col].isna()]) \/ len(test)\n    print(f'\\tIn train data:\\tnan sample rate:{na_tr:.2f}\\t')\n    print(f'\\tIn test data\\tnan sample rate:{na_te:.2f}\\t')\nplt.bar(x, y)\nplt.title(col)\nplt.show","03db29ba":"#age and attrition\nplt.figure(figsize=(4,3))\nprint(train['Attrition'])\nsns.barplot(x='Attrition', y='Age', data = train , palette = 'Set2')","ae9ae22d":"figure, ax = plt.subplots(figsize=(10, 10))\ndata = pd.concat([train.drop(['user_id','Attrition','EmployeeNumber','EmployeeCount', 'Over18','StandardHours'],axis = 1), test]).corr() ** 2\n#data = np.tril(data, k=-1)\ndata[data==0] = np.nan\nsns.heatmap(np.sqrt(data), annot=False, cmap='viridis', ax=ax)","3c69fa30":"print(type(data))","2b865a06":"train = pd.read_csv(f'{DATA_PATH}\/train.csv')\ntarget_col_dict = {'Yes': 1, 'No': 0}\ntrain1 = train\ntrain1['Attrition'] = train1['Attrition'].map(target_col_dict).values\ntrain2 = train1\n#train2.drop(['Attrition'])\ndata = train2.corrwith(train1['Attrition']).agg('square')\ndata = data.drop('Attrition')\nfigure, ax = plt.subplots(figsize=(10, 10))\ndata.agg('sqrt').plot.bar(ax=ax)\n# del data","7eeddd36":"from sklearn.preprocessing import MinMaxScaler\n\nsacalar = MinMaxScaler()\ntrain_digital = sacalar.fit_transform(train[digital_cols])\ntest_digital = sacalar.transform(test[digital_cols])\n","5650f6e7":"# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# train_category, test_category = None, None\n# drop_cols = ['EmployeeNumber', 'Over18', 'StandardHours','BusinessTravel']\n# for col in [var for var in category_cols if var not in drop_cols]:\n#     lbe, ohe = LabelEncoder(), OneHotEncoder()\n    \n#     lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n#     train[col] = lbe.transform(train[col])\n#     test[col] = lbe.transform(test[col])\n    \n#     ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n#     oht_train = ohe.transform(train[col].values.reshape(-1, 1)).todense()\n#     oht_test = ohe.transform(test[col].values.reshape(-1, 1)).todense()\n    \n#     if train_category is None:\n#         train_category = oht_train\n#         test_category = oht_test\n#     else:\n#         train_category = np.hstack((train_category, oht_train))\n#         test_category = np.hstack((test_category, oht_test))\n# print(train_category[0,:])\n# lbe.fit(pd.concat([train['BusinessTravel'], test['BusinessTravel']]).values.reshape(-1, 1))\n# BT_train = lbe.transform(train['BusinessTravel'])\n# BT_test = lbe.transform(test['BusinessTravel'])\n# train_category = np.insert(train_category, 0, values=BT_train, axis=1)\n# test_category = np.insert(test_category, 0, values=BT_test, axis=1)\n# print(train_category)\n\n","b42e49ac":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ntrain_category, test_category = None, None\ndrop_cols = ['EmployeeNumber', 'Over18', 'StandardHours']\nfor col in [var for var in category_cols if var not in drop_cols]:\n    lbe, ohe = LabelEncoder(), OneHotEncoder()\n    \n    lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n    train[col] = lbe.transform(train[col])\n    test[col] = lbe.transform(test[col])\n    \n    ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n    oht_train = ohe.transform(train[col].values.reshape(-1, 1)).todense()\n    oht_test = ohe.transform(test[col].values.reshape(-1, 1)).todense()\n    \n    if train_category is None:\n        train_category = oht_train\n        test_category = oht_test\n    else:\n        train_category = np.hstack((train_category, oht_train))\n        test_category = np.hstack((test_category, oht_test))","a4b1ef4c":"train_digital.shape, test_digital.shape, train_category.shape, test_category.shape","52fbce77":"feature_names = ['Age', 'DailyRate', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n                'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n                'BusinessTravel', 'Department', 'DistanceFromHome', 'Education', 'EducationField',\n                'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel',\n                'JobRole', 'JobSatisfaction', 'MaritalStatus', 'Over18', 'OverTime',\n                'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TrainingTimesLastYear',\n                'WorkLifeBalance']","70903fc5":"train_features = np.hstack((train_digital, train_category))\ntest_features = np.hstack((test_digital, test_category))\ntrain_features.shape, test_features.shape","3658c26e":"# target_col_dict = {'Yes': 1, 'No': 0}\n# train_labels = train[target_col].map(target_col_dict).values\ntrain_labels = train[target_col]\ntrain_labels.shape","24fb3558":"# from imblearn.over_sampling import KMeansSMOTE\n# sm = KMeansSMOTE(random_state=42 ,cluster_balance_threshold = 0.3, k_neighbors=2)\n# X_res, y_res = sm.fit_resample(X_train, y_train)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# from imblearn.over_sampling import ADASYN\n# ad = ADASYN(random_state  = 42)\n# X_res, y_res = ad.fit_resample(X_train, y_train)\n\n# from imblearn.combine import SMOTEENN\n# sme = SMOTEENN(random_state = 42)\n# X_res, y_res = sme.fit_resample(X_train, y_train)","1c2b3b0f":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)","47a9d25f":"X_train.shape,X_val.shape,y_train.shape,y_val.shape","e5d4f5ad":"# from sklearn.decomposition import PCA\n# #pca = PCA(n_components = 'mle')\n# pca = PCA(n_components = 110)\n# X_val_new = pca.fit_transform(X_val)\n# train_features_new = pca.fit_transform(train_features)\n# test_features_new = pca.fit_transform(test_features)\n# X_train_new = pca.fit_transform(X_train)\n# print('\u4fdd\u7559\u7684\u7279\u5f81\u6570\uff1a')\n# print(pca.n_components_)\n# print('\u7279\u5f81\u6240\u5360\u6bd4\u91cd\u5206\u522b\u4e3a\uff1a')\n# print(pca.explained_variance_ratio_)\n# print('\u7279\u5f81\u6240\u5360\u6bd4\u91cd\u4e4b\u548c\uff1a')\n# print(sum(pca.explained_variance_ratio_))\n\n","2efcda4b":"### fit model for train data\nimport xgboost as xgb\n#dataset\ndtrain = xgb.DMatrix(X_train, label = y_train)\n\n#parameters\nnum_round = 500\nparam = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\nparam['nthread'] = 2\nparam['eval_metric'] = 'auc'\nplst = param.items()","db0ed268":"np.any(np.isnan(y_train))","48ae0206":"bst = xgb.train(plst, dtrain, num_round)\n#save the model\nbst.save_model('0001.model')","b20488eb":"dtest = xgb.DMatrix(X_val)\nypred = bst.predict(dtest)","f32d23e3":"test_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint(test_auc)","8737a9e7":"from xgboost import XGBClassifier\n\nbst2 = XGBClassifier(#learning_rate=0.08,\n                      n_estimators=300,         # \u6811\u7684\u4e2a\u6570--300\u68f5\u6811\u5efa\u7acbxgboost\n                      #max_depth=3,               # \u6811\u7684\u6df1\u5ea6\n                      #min_child_weight = 1,      # \u53f6\u5b50\u8282\u70b9\u6700\u5c0f\u6743\u91cd\n                      #gamma=0.3,                  # \u60e9\u7f5a\u9879\u4e2d\u53f6\u5b50\u7ed3\u70b9\u4e2a\u6570\u524d\u7684\u53c2\u6570\n                      #subsample=0.8,             # \u968f\u673a\u9009\u62e980%\u6837\u672c\u5efa\u7acb\u51b3\u7b56\u6811\n                      #colsample_btree=0.8,       # \u968f\u673a\u9009\u62e980%\u7279\u5f81\u5efa\u7acb\u51b3\u7b56\u6811\n#                       objective='binary:logistic', # \u6307\u5b9a\u635f\u5931\u51fd\u6570\n#                       scale_pos_weight=1,        # \u89e3\u51b3\u6837\u672c\u4e2a\u6570\u4e0d\u5e73\u8861\u7684\u95ee\u9898\n#                       random_state=27            # \u968f\u673a\u6570\n                      )\nbst2.fit(X_train,\n          y_train,\n           eval_set = [(X_val,y_val)],\n           eval_metric = \"auc\",\n          #early_stopping_rounds = 10,\n          verbose = False)\n#save the model\nbst2.save_model('0002.model')\n\nypred2 = bst2.predict(X_val)\ntest_auc = metrics.roc_auc_score(y_val,ypred2)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint(test_auc)","bea0d73f":"from sklearn.preprocessing import minmax_scale\nfeature_importance = bst.get_score(importance_type = 'total_cover')\nfeature_values = list(feature_importance.values())\nfeature_keys = list(feature_importance.keys())\noriginal_importance_sort = np.sort(feature_values)[::-1]\nnew_dict = {v : k for k, v in feature_importance.items()} #key,value \u4e92\u6362\u4f4d\u7f6e\nfeature_label = []\nfor i in range(len(new_dict)): feature_label.append(new_dict[original_importance_sort[i]])\nprint(feature_label)\nprint(feature_keys)\nfeature_importance = np.sort(minmax_scale(feature_values,feature_range = (0,1)))[::-1]\nprint(feature_importance)\n\n\n\nfrom xgboost import plot_importance\nfig,ax = plt.subplots(figsize=(15,15))\nplot_importance(bst,\n                height=0.5,\n                ax=ax,\n                max_num_features=64)","46d72f85":"feature_importance_per = feature_importance \/ sum(feature_importance)\nprint(feature_importance_per)\n\nthreshold = 0.95\n#\u6839\u636e\u91cd\u8981\u6027\u9608\u503c\u9009\u62e9\u524dN\u4e2a\u7279\u5f81\ndef TopiElements(feature_importance_per,threshold):\n    score = 0\n    for i in range(len(feature_importance_per)):\n        if score < threshold:\n            score += feature_importance_per[i]\n        else:\n            return i+1\ndef TopiFeaturesName(feature_label,feature_importance_per,threshold):\n    return feature_label[0:TopiElements(feature_importance_per,threshold)-1]\nprint('\u5171\u6709',TopiElements(feature_importance_per,threshold),'\u4e2a\u7279\u5f81')\nTopiFeatures = TopiFeaturesName(feature_label,feature_importance_per,threshold)\nprint(TopiFeatures)","cd531126":"#\u8f93\u5165features\u4e3a\u77e9\u9635,\u8fd4\u56de\u503c\u4e3a\u77e9\u9635\ndef GenerateNewFeatures(features,TopiFeatures):\n    feature_columns = []\n    for i in range(110): feature_columns.append('f{}'.format(i))\n    train_Features = pd.DataFrame(features,columns = [feature_columns])\n    train_Features.head()\n    new_train_Features = train_Features[TopiFeatures]\n    return new_train_Features.values","5ca7f497":"new_train_features = GenerateNewFeatures(train_features,TopiFeatures)\nnew_test_features = GenerateNewFeatures(test_features,TopiFeatures)","ce68e231":"# from sklearn.feature_selection import SelectFromModel\n\n# thresholds = sorted(bst2.feature_importances_)\n# for thresh in thresholds:\n#     # select features using threshold\n#     selection = SelectFromModel(bst2, threshold=thresh, prefit=True)\n#     select_X_train = selection.transform(X_train)\n#     # train model\n#     selection_model = XGBClassifier()\n#     selection_model.fit(select_X_train, y_train)\n#     # eval model\n#     select_X_val = selection.transform(X_val)\n#     ypred2 = selection_model.predict(select_X_val)\n#     predictions = [round(value) for value in ypred2]\n#     auc = metrics.roc_auc_score(y_val,ypred2)\n#     print(\"Thresh=%.3f, n=%d, auc: %.2f%%\" % (thresh, select_X_train.shape[1], auc))\n","b8d29c60":"threshold_res = 0.5\ndef EncodeResult(threshold, results):\n    encodeResult = []\n    for i in range(len(results)):\n        encodeResult.append(1) if results[i] > threshold else encodeResult.append(0)\n    return encodeResult","03eca094":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(new_train_features, train_labels, test_size=0.20, random_state=42)\n#dataset\ndtrain = xgb.DMatrix(X_train, label = y_train)\n\n#parameters\nnum_round = 300\nparam = {'bst:max_depth':3, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\nparam['nthread'] = 2\nplst = param.items()\n\nbst = xgb.train(plst, dtrain, num_round)\n#save the model\nbst.save_model('0001.model')\n\ndtest = xgb.DMatrix(X_val)\nypred = bst.predict(dtest)\n\ntest_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint(test_auc)\nprint(ypred)\nencode_result = EncodeResult(threshold_res,ypred)\nprint(encode_result)\ntest_acc = metrics.accuracy_score(y_val,encode_result)\nprint(test_acc)","14cd1074":"from sklearn.linear_model import LinearRegression\n\nclf_after = LinearRegression()\nclf_after.fit(X_train, y_train)","5b29acb9":"ypred = clf_after.predict(X_val)\nypred.shape\ntest_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint(test_auc)","5e513990":"X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)\nclf = LinearRegression()\nclf.fit(X_train, y_train)\nypred = clf.predict(X_val)\nypred.shape\ntest_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint(test_auc)","bc4465b5":"#X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=0,max_features = 0.40)\nrf.fit(X_train,y_train)\nypred = rf.predict(X_val)\ntest_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint('Random forest auc:',test_auc)\n#print('acc:',metrics.accuracy_score(y_val,ypred))\n#print(ypred)","949d181f":"X_train, X_val, y_train, y_val = train_test_split(new_train_features, train_labels, test_size=0.20, random_state=42)\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=0,max_features = 0.30)\nrf.fit(X_train,y_train)\nypred = rf.predict(X_val)\ntest_auc = metrics.roc_auc_score(y_val,ypred)#\u9a8c\u8bc1\u96c6\u4e0a\u7684auc\u503c\nprint('auc:',test_auc)\nprint('acc:',metrics.accuracy_score(y_val,ypred))","11e62562":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb","fbc960d6":"X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)\nn_folds = 5\ndef acc_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    acc= cross_val_score(model, X_train, y_train, scoring=\"accuracy\", cv = kf)\n    return(acc)","84596fe6":"Adaboost = make_pipeline(RobustScaler(),\n                         AdaBoostClassifier(base_estimator=None,\n                                            n_estimators = 56,\n                                            learning_rate= 0.18,\n                                            algorithm='SAMME.R',\n                                            random_state = 1)\n                        )","fdd96966":"GBoosting = make_pipeline(RobustScaler(), \n                          GradientBoostingClassifier(loss='deviance',\n                                                     learning_rate = 0.05,\n                                                     n_estimators = 56,\n                                                     min_samples_split = 9,\n                                                     min_samples_leaf = 2,\n                                                     max_depth = 4,\n                                                     random_state = 1,\n                                                     max_features = 9)\n                         )","eef3d242":"SVC =  make_pipeline(RobustScaler(), \n                     SVC(decision_function_shape = 'ovr',\n                         random_state = 1,\n                         max_iter = 14888,\n                         kernel = 'poly',\n                         degree = 2,\n                         coef0 = 0.49, \n                         C =  9.6)\n                     )","dced42e3":"RF = make_pipeline(RobustScaler(), \n                   RandomForestClassifier(criterion='gini', \n                                          n_estimators=364,\n                                          max_depth = 11,                    \n                                          min_samples_split=6,\n                                          min_samples_leaf=1,\n                                          max_features='auto',\n                                          oob_score=True,\n                                          random_state=1,\n                                          )\n                  )","a6298bca":"xgbc = make_pipeline(RobustScaler(), \n                     xgb.XGBClassifier(n_estimators=121,\n                                       reg_lambda = 0.9,\n                                       reg_alpha = 0.5,\n                                       max_depth = 9,\n                                       learning_rate = 0.55,\n                                       gamma = 0.5,\n                                       colsample_bytree = 0.4,\n                                       coldsample_bynode = 0.15,\n                                       colsample_bylevel = 0.5)\n                    )","a378441e":"score = acc_cv(Adaboost)\nprint(\"Adaboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(GBoosting)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(SVC)\nprint(\"SVC  score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(RF)\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = acc_cv(xgbc)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3484ffb2":"class AveragingModels(BaseEstimator):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","9723bd12":"%%capture\naveraged_models = AveragingModels(models = (Adaboost,SVC, GBoosting, RF,xgbc))\naveraged_models.fit(X_train, y_train)\n#predict\ntrain_pred = averaged_models.predict(X_train)\ntest_pred = averaged_models.predict(X_val)","c4213131":"test_pred.shape\nprint('ensemble_auc:', metrics.roc_auc_score(y_val,test_pred))","1a5e402c":"# train_pred = np.round(train_pred)\n# test_pred = np.round(test_pred)\n\nacc_averaged = np.round((train_pred==y_train).sum()\/train_pred.shape[0],5)\nprint(f\"Averaged models accuracy: {acc_averaged}\")","1e107939":"#new features\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(new_train_features, train_labels, test_size=0.20, random_state=42)","a6e35d9d":"#old features\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)","87153f14":"from pandas import DataFrame\nX_train = DataFrame(X_train, index = None)\ny_train = DataFrame(y_train, index = None)\nX_val = DataFrame(X_val,index = None)\ny_val = DataFrame(y_val,index = None)","9d788371":"import lightgbm as lgb\ntrain_data = lgb.Dataset(data = X_train, label = y_train)\ntest_data = lgb.Dataset(data = X_val, label = y_val)\n# parameters\nparam = {'num_leaves':20, 'num_trees':300, 'objective':'binary'}\nparam['metric'] = ['binary_logloss']\n# use test_data as validation dataset\nnum_round = 100\nbst = lgb.train(param, train_data, num_round, valid_sets = test_data, early_stopping_rounds =10)\ny_pred = bst.predict(X_val)\nprint('lightGBM_auc:', metrics.roc_auc_score(y_val,y_pred))","0d273891":"from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=3, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_val, y_val))\ntpot.export('tpot_titanic_pipeline.py')\n\ny_pred = tpot.predict(X_val)\nprint('topt AUC:',metrics.roc_auc_score(y_val,y_pred))","843a32ca":"# bernoli distribution\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB(alpha = 2, fit_prior=False)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_val)\nprint('BernoulliNB AUC:',metrics.roc_auc_score(y_val,y_pred))","4833342b":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n\nexported_pipeline = make_pipeline(\n    StandardScaler(),\n    GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=0.45, min_samples_leaf=6, min_samples_split=5, n_estimators=100, subsample=0.25)\n)\nexported_pipeline.fit(X_train, y_train)\ny_pred = exported_pipeline.predict(X_val)\nprint('GBDT AUC:',metrics.roc_auc_score(y_val,y_pred))","d8725afc":"from sklearn.svm import SVR\nfrom sklearn import metrics\nX_train, X_val, y_train, y_val = train_test_split(new_train_features, train_labels, test_size=0.20, random_state=42)\nsvc = SVR(kernel = 'linear')\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_val)\nprint('SVM_auc:',metrics.roc_auc_score(y_val,y_pred))\n","ba8eea41":"import numpy as np\nfrom catboost import Pool, CatBoostRegressor\n\nid_col = 'user_id'\ntarget_col = 'Attrition'\n\ndigital_cols = ['Age', 'DailyRate', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n                'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ncategory_cols = ['BusinessTravel', 'Department',  'Education', 'EducationField',\n                 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel','DistanceFromHome',\n                'JobRole', 'JobSatisfaction', 'MaritalStatus', 'OverTime',\n                'RelationshipSatisfaction',  'StockOptionLevel', 'PerformanceRating', 'TrainingTimesLastYear','WorkLifeBalance' ]\nfeature_cols = digital_cols + category_cols\n\nX_train, X_val, y_train, y_val = train_test_split(train[feature_cols], train[target_col], test_size=0.20, random_state=42)\n\ncat_feature_indice = [i for i in range(10,28)]\ntrain_pool = Pool(X_train, \n                  y_train, \n                  cat_features=cat_feature_indice)\ntest_pool = Pool(X_val, \n                 cat_features=cat_feature_indice) \n\n# specify the training parameters \n# ctb = CatBoostRegressor(iterations=1200, \n#                           depth=2, \n#                           learning_rate=0.1, \n#                           loss_function='RMSE')\nctb = CatBoostRegressor(iterations=1200, \n                          depth=2, \n                          learning_rate=0.1, \n                          loss_function='RMSE')\n\n#train the model\nctb.fit(train_pool)\n# make the prediction using the resulting model\ny_pred = ctb.predict(test_pool)\nprint('catboost_auc:',metrics.roc_auc_score(y_val,y_pred))","2b7643b9":"#catboost\nimport numpy as np\nfrom catboost import Pool, CatBoostRegressor\n\nid_col = 'user_id'\ntarget_col = 'Attrition'\n\ndigital_cols = ['Age', 'DailyRate', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n                'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ncategory_cols = ['BusinessTravel', 'Department',  'Education', 'EducationField',\n                'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel','DistanceFromHome',\n                'JobRole', 'JobSatisfaction', 'MaritalStatus', 'Over18', 'OverTime',\n                'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'PerformanceRating', 'TrainingTimesLastYear','WorkLifeBalance' ]\nfeature_cols = digital_cols + category_cols\n\n#X_train, X_val, y_train, y_val = train_test_split(train[feature_cols], train[target_col], test_size=0.20, random_state=42)\n\ncat_feature_indice = [i for i in range(10,31)]\ntrain_pool = Pool(train[feature_cols], \n                  train[target_col], \n                  cat_features=cat_feature_indice)\ntest_pool = Pool(test[feature_cols], \n                 cat_features=cat_feature_indice) \n\n# specify the training parameters \n# ctb = CatBoostRegressor(iterations=1200, \n#                           depth=2, \n#                           learning_rate=0.1, \n#                           loss_function='RMSE')\nctb = CatBoostRegressor(iterations=1100, \n                          depth=2, \n                          learning_rate=0.1, \n                          loss_function='RMSE')\n\n#train the model\nctb.fit(train_pool)\n# make the prediction using the resulting model\nypred = ctb.predict(test_pool)","ade4281b":"#linear\n# from sklearn.linear_model import LinearRegression\n\n# clf_after = LinearRegression()\n# clf_after.fit(new_train_features, train_labels)\n# ypred = clf_after.predict(new_test_features)\n\nfrom sklearn.linear_model import LinearRegression\n\nclf_after = LinearRegression()\nclf_after.fit(train_features, train_labels)\nypred = clf_after.predict(test_features)\n","9ea364a2":"#ensemble__classification\naveraged_models = AveragingModels(models = (Adaboost,SVC, GBoosting, RF,xgbc))\naveraged_models.fit(train_features, train_labels)\n#predict\nypred = averaged_models.predict(test_features)","ed10bf3b":"#xgboost\n### fit model for train data\nimport xgboost as xgb\n#dataset\ndtrain = xgb.DMatrix(train_features, label = train_labels)\n\n#parameters\nnum_round = 500\nparam = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\nparam['nthread'] = 2\nparam['eval_metric'] = 'auc'\nplst = param.items()\nbst = xgb.train(plst, dtrain, num_round)\ndtest = xgb.DMatrix(test_features)\nypred = bst.predict(dtest)","731a44e0":"#SMOTE SVM\n# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=42)\n# X_res, y_res = sm.fit_resample(train_features, train_labels)\n\n\nfrom sklearn.svm import SVR\nfrom sklearn import metrics\n#X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)\nsvc = SVR(kernel = 'linear')\nsvc.fit(new_train_features, train_labels)\nypred = svc.predict(new_test_features)\n\n# #X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.20, random_state=42)\n# svc = SVR(kernel = 'linear')\n# svc.fit(X_res, y_res)\n# ypred_up = svc.predict(test_features)\n","393d6658":"sub = test[['user_id']].copy()\nsub['Attrition'] = ypred\nsub['Attrition'] = sub['Attrition'].apply(lambda x: x if x >=0 else 0.0005)\nsub.to_csv('submission.csv', index=False)\n\n# sub['Attrition'] = ypred_up\n# sub['Attrition'] = sub['Attrition'].apply(lambda x: x if x >=0 else 0.0005)\n# sub.to_csv('submission_up.csv', index=False)","cefee72b":"#### XGBOOST","5f56f448":"### For categorical cols","e0b57ec6":"## 1.Load data","0d875773":"## Submission","644b7fc3":"## 6. RandomForest","f7fa6930":"### SMOTE Upsampling","cd52c5cd":"### Feature Selection","a75e9cca":"## 2.Data EDA","a116260a":"#### Feature importance Using Xgboost","b883f1db":"### XGBClassifier","2b76f0e7":"#### Adaboost","4da604dd":"#### Random Forest","0ae78ecc":"### 4.1 Settings","eb85d785":"### PCA","4703f292":"#### Before Feature Selection","2ddcb3bd":"## 5.LR on validation set","553199dc":"### 2.3 Visualization","99547c7c":"#### Use SelectFromModel to select","40e84b92":"#### After Feature Selection","34138a20":"### 4.6 Try xgb model with New Features","af20f58e":"### 4.3 Prediction","f02a672c":"#### Gradient Boosting","71b75086":"#### Correlation between vars and target","0f7827a7":"### Calculate model ACC","2daaff29":"## End: On test set","0232bb0f":"### Average ensembling","3a69855f":"### For digital cols","9e90615d":"## 9.TPOT","d06fc4a2":"## 7. Ensemble-----classification","512c937a":"### Split validation set","7e8a973c":"### 4.2 Training","0ae7035c":"## 10.SVM","baabb15b":"#### SVC","65fe82d1":"### 2.2 Nan and unique values","1db9dc2c":"#### After Feature Selection","8a4cf71e":"#### Scores","b47a7c68":"### 4.5 Explore the features","25bd195b":"## 3.Data preprocessing","ac2c5c8a":"## 8.LightGBM","74ec3506":"#### HeatMap of Correlations","d5972870":"### original","eb126507":"## 11. catboost","1fd65c42":"#### Before Feature Selection","f2dc037e":"### 2.1 Check all the columns","01cf1608":"## 4.Using Xgboost on validation set","6ede8921":"### xgb","629d26d3":"### 4.4 Evaluation"}}