{"cell_type":{"6c1ff5d4":"code","b56830f3":"code","66d8e77e":"code","891f46e2":"code","e3b610c9":"code","b9fb43de":"code","4510a99b":"code","4d00a4f9":"code","f6d552fd":"code","da781fff":"code","f90b8150":"code","1b811825":"code","44f5b9ff":"code","dedd6f92":"code","42ff304f":"code","81a176fa":"code","750e9def":"code","fd7289ca":"code","9eab8fe1":"code","57592394":"code","c72b91b1":"code","e8a850bd":"code","0dd7b407":"code","1559fe0d":"code","9cbcc32b":"code","476b3c2c":"code","4af9bfea":"code","fb9ab6dd":"code","439e2aa6":"markdown","13364aa4":"markdown","74e863cb":"markdown","ee43cca6":"markdown","0c615db3":"markdown","50f24f5b":"markdown","050ac052":"markdown","221746d2":"markdown","0516120f":"markdown","768e0b09":"markdown","8dc1f9d3":"markdown","c42712f8":"markdown","f1ab1b86":"markdown","44184eac":"markdown","b31a8430":"markdown","38a6cf8f":"markdown","a9d1e5a3":"markdown","154e887d":"markdown","ed2d55ef":"markdown","2b2338f2":"markdown","660e48c7":"markdown","c9136908":"markdown","10d00d84":"markdown","9cae00a6":"markdown","b8ed6874":"markdown","231c893a":"markdown","0f31735d":"markdown","f863a38b":"markdown","e634723b":"markdown","7aef6c22":"markdown","53ed894d":"markdown"},"source":{"6c1ff5d4":"\nimport numpy as np \nimport pandas as pd \nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n#Data Processing\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Model creation and hyperparameter search\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\n\n#Validation and visualitzation of scores\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, precision_score, recall_score, auc\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score","b56830f3":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndataset = df.values\ndf.head()","66d8e77e":"df.shape","891f46e2":"df.isnull().sum()","e3b610c9":"names = list(df.columns)\nx = df[names[1:]] ## Dataset with independent charecteristics\ny = df['class'] ## Dataset with the target characteristic\n","b9fb43de":"colors = ('#EF8787','#9CF29C')\npalette = sns.set_palette(sns.color_palette(colors))\n\nf, ax = plt.subplots(figsize=(15, 10))\nlabels = (('Poisonous','Edible'))\ndf['class'].value_counts().plot.pie(labels= labels, shadow= True, ax= ax, autopct='%1.1f%%', colors= colors,textprops={'fontsize': 12} )\n\nax.set_title('Mushroom Class Distribution', fontsize = 15);","4510a99b":"features = df.columns\nf, axes = plt.subplots(11,2, figsize=(30,150))\naxes = axes.flatten()\nk = 1\n\nfor i in range(0,22):\n    s = sns.countplot(x = features[k], data = df, hue = 'class', ax=axes[i], palette = palette)\n    axes[i].set_xlabel(features[k], fontsize=30)\n    axes[i].set_ylabel(\"Count\", fontsize=30)\n    axes[i].tick_params(labelsize=20)\n    axes[i].legend(loc=2, prop={'size': 20})\n    k = k+1\n    for p in s.patches:\n        s.annotate(format(p.get_height()), (p.get_x() + p.get_width() \/ 2, p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 9), \n        fontsize = 20,\n        textcoords = 'offset points')","4d00a4f9":"df=df.drop([\"veil-type\"],axis=1)\nnames = list(df.columns)\nx = x.drop([\"veil-type\"],axis=1)\ny = df['class'] ## Dataset with the target characteristic\n","f6d552fd":"labelencoder=LabelEncoder()\ndf_enc = df.copy()\nfor column in df.columns:\n    df_enc[column] = labelencoder.fit_transform(df[column])","da781fff":"plt.figure(figsize=(14,12))\nsns.heatmap(df_enc.corr(),linewidths=.1,annot=True, cmap=\"magma\")","f90b8150":"ohe_x = OneHotEncoder(drop='first').fit(x)\nohe_x = ohe_x.transform(x).toarray()\n\naux = y.values.reshape(-1, 1)\nohe_y = OneHotEncoder(drop='first').fit(aux)\nohe_y = ohe_y.transform(aux).toarray()\nohe_y = ohe_y.flatten()","1b811825":"x_train, x_test, y_train, y_test = train_test_split(ohe_x,ohe_y,test_size=0.2, random_state=1)","44f5b9ff":"models = ['LogisticRegression','NaiveBayes','KernelSVM'\n          ,'RandomForest','KNearestNeighbors']\n\nscores = [None] * len(models)\n\ndef show_results(best_model, prediction,model):\n    acc = accuracy_score(y_test, prediction)\n    \n    scores[models.index(model)] = acc\n        \n    prec = precision_score(y_test, prediction)\n    rec = recall_score(y_test, prediction)\n    error = plot_confusion_matrix(best_model, x_test, y_test, normalize='true', cmap=\"magma\")\n    error = 1-(sum(np.diag(error.confusion_matrix)) \/ sum(error.confusion_matrix.ravel()))\n    error = (\"%.4f\" % (error*100))\n    print(f'Accuracy:{acc}')\n    print(f'Precision:{prec}')\n    print(f'Recall:{rec}')\n    print(f\"Error rate: {error}%\")","dedd6f92":"from sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\ny_pred = lr.predict(x_test)\naccuracy = lr.score(x_test, y_test)\n\nshow_results(lr, y_pred,\"LogisticRegression\")\n","42ff304f":"score_list = cross_val_score(lr,ohe_x,ohe_y, cv=10)\nscore = np.mean(score_list)\nprint (score)\n\n# we swap the score obtained before with the cross_val_score\nscores[0] = score\n","81a176fa":"nb = GaussianNB()\nnb.fit(x_train, y_train)\npreds= nb.predict(x_test)\nshow_results(nb,preds,\"NaiveBayes\")","750e9def":"score_list = cross_val_score(nb,ohe_x,ohe_y, cv=5)\nprint(score)\nscore = np.mean(score_list)\n# we swap the score obtained before with the cross_val_score\nscores[1] = score","fd7289ca":"print(score_list)","9eab8fe1":"cv_split = TimeSeriesSplit(n_splits=5)","57592394":"svc = svm.SVC(random_state=1, probability=True)\nsvc_params = {\n    'model__C': [0.1, 1, 10, 100],  \n    'model__kernel': ['poly', 'rbf', 'sigmoid']\n} \nsvc_pipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('model', svc)\n])\ngridsearch_svc = GridSearchCV(estimator=svc_pipe,\n                          param_grid = svc_params)\ngridsearch_svc.fit(x_train, y_train)","c72b91b1":"svc_best_model = gridsearch_svc.best_estimator_\npredictions = svc_best_model.predict(x_test)\nshow_results(svc_best_model,predictions,\"KernelSVM\")","e8a850bd":"rf = RandomForestClassifier(random_state=1)\nrf_params = {\n    'model__n_estimators': list(range(25,251,25)),\n    'model__max_features': list(np.arange(0.1,0.36,0.05))\n}\nrf_pipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('model', rf)\n])\ngridsearch_rf = GridSearchCV(estimator=rf_pipe,\n                          param_grid = rf_params,\n                          cv = cv_split,\n                         )\ngridsearch_rf.fit(x_train, y_train)","0dd7b407":"rf_best_model = gridsearch_rf.best_estimator_\npredictions = rf_best_model.predict(x_test)\nshow_results(rf_best_model,predictions,'RandomForest')","1559fe0d":"knn = KNeighborsClassifier()\nknn_params = {\n    'n_neighbors': list(range(4,10)),\n    'weights': ['uniform','distance']\n}\n\ngridsearch_knn = GridSearchCV(knn,\n                          param_grid = knn_params,\n                          cv = cv_split,\n                         )\ngridsearch_knn.fit(x_train, y_train)","9cbcc32b":"knn_best_model = gridsearch_knn.best_estimator_\npredictions = knn_best_model.predict(x_test)\n\nshow_results(knn_best_model,predictions,'KNearestNeighbors')","476b3c2c":"distances= [1,2,5]\nweights = ['distance','uniform']\n\n\n\nfor dist in distances:\n    i = 0\n    fig, axs = plt.subplots(1,2,figsize=(20,5))\n\n    for w in weights:\n        list1 = []\n        for neighbors in range(3,10):\n            classifier = KNeighborsClassifier(n_neighbors=neighbors, p=dist, weights = w)\n            classifier.fit(x_train, y_train)\n            y_pred = classifier.predict(x_test)\n            list1.append(accuracy_score(y_test,y_pred))\n\n        axs[i].plot(list(range(3,10)), list1, linewidth=3)\n        axs[i].set_title(\"p = \"+str(w), fontsize=15)\n        axs[i].set_xlabel(\"K neighbors\", fontsize=15)\n        axs[i].set_ylabel(\"Accuracy\", fontsize=15)\n        i +=1\n    fig.suptitle(dist, fontsize=20, y = 1.02)   \n    plt.show()\n","4af9bfea":"plt.rcParams['figure.figsize']=15,8 \nax = sns.barplot(x=models, y=scores, palette = \"magma\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 13, horizontalalignment = 'center', rotation = 0)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.01), ha='center', fontsize = 13)\nplt.show()","fb9ab6dd":"palette = sns.set_palette(sns.color_palette('Set1')) #just to define de plot palette\n\nfor i in range(0,5):\n    random.seed(i)\n    randlist = list(names[x] for x in random.sample(range(0,21),k=5))\n    rand_df = df[randlist]\n    rand_df = pd.get_dummies(rand_df)\n\n    x2_train, x2_test, y_train, y_test = train_test_split(rand_df, ohe_y, test_size=0.2)\n\n    lr = LogisticRegression(solver=\"lbfgs\").fit(x2_train, y_train)\n    predicted = lr.predict(x2_test)\n\n    y_probs = lr.predict_proba(x2_test)\n    y_probs = pd.DataFrame(y_probs)[1]\n\n    roc_auc=\"%.2f\" % roc_auc_score(y_test, predicted)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, y_probs)\n\n    plt.plot(lr_fpr, lr_tpr, marker='.', label=(str(randlist) + \" \" + str(roc_auc)))\n    # axis labels\n    plt.xlabel('False Positive Rate',fontsize = 15)\n    plt.ylabel('True Positive Rate',fontsize = 15)\n    # show the legend\n    plt.legend(fontsize = 11)\n    \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.gcf().set_size_inches(15,10)","439e2aa6":"## Step 3: Data Preprocessing ##","13364aa4":"## Step 5: 100% Accuracy, too good to be true? ","74e863cb":"# Mushrooms, safe to eat or poisonous? #\n**Introduction**\n\nIn this Kaggle I'll work with diferent types of models to classificate mushrooms as edible (e) or poisonous (p). Understanding the state of art on the dataset used is important since the accuracies of other projects on this dataset are around 100%. Having this in mind I'll focus this Kaggle on a correct and explained use of different models rather than obtaining the best results.","ee43cca6":"We can see how some features gives us way more information than others.\nWe can also see that there's only one veil-type, so we are going to delete that feature.","0c615db3":"## Conclusions ##\n\nEither KNN or RandomForests will give us almost a perfect accuracy.\nEven if the dataset features allow to easily have high accuracies it's always important to properly process the data and tune the models correctly, understanding what the program is doing rather than just focusing on having better scores. Doing this we'll know if our scores are correct or we are doing something wrong.","50f24f5b":"## Step 2: EDA ##\nEDA or Exploratory Data Analysis helps us to better understand the data we are working  with so we know how to adress the problems later.","050ac052":"As previously explained this data set's features can easily explain the data  to classify the mushrooms as edible or poisonous.\nWe have seen how all the tested models can get high accuracies and how little the accuracies vary between different parameters (illustrated with the KNN example).\nTo finally understand how easy we can achieve high accuracies we'll plot various ROC curves for various subgroups of random features.","221746d2":"**Separate Target from the data**\n\nOur target variable to predict is going to be *class* so we are going to separate the depentent variable from the independent ones","0516120f":"**Naive Bayes**","768e0b09":"**Target variable distribution**","8dc1f9d3":"## Step 1: Import the libraries ##","c42712f8":"**Random Forest**","f1ab1b86":"**Feature distribution between poisonous and edible**","44184eac":"**Show Data size**","b31a8430":"**Logistic Regression**","38a6cf8f":"We have obtained a 0.997% of accuracy, we could be having an overfitting issue. To better evaluate the models from now on we are going to use crossvalidation. Doing so, we will probably obtain worse results, but these are going to be more reliable.","a9d1e5a3":"**KNN**","154e887d":"**Data Correlation**\n\nSince our data is categorical we'll use a heatmap to observe correlation between features. To do that, first we'll have to do some basic label encoding.","ed2d55ef":"**Data Encoding**\n\nSince all our data is categorical, in order to work with it we must encode it. We need to keep in mind that  for some models, directly encoding the values with numbers(like we have done to plot the heatmap) can create bias towards higher valued features. To avoid that we'll use One-Hot Encoding.\n","2b2338f2":"**Data normalization**\n\nSince all our data is categorical there's no need to normalize the data.","660e48c7":"It's now clear that the 95% accuracy wasn't really reliable.","c9136908":"Again we can see how after using cross-Validation our accuracie has decreased this time from 95% to 82%. To understand what happened we must see the scores obtained from the cross_validation.","10d00d84":"**SVM**","9cae00a6":"First we define a function that will help us printing the results of each model.","b8ed6874":"**Data separation in Folds**","231c893a":"**We load the dataset and visualize its first elements**","0f31735d":"After previous results we can already think that regardless the hyperparameter search and the tunning of the variables we are prone to obtain very high accuracies. To ilustrate that we can look at the next code which executes KNN with various parameters.","f863a38b":"**Libraries used** \n\n* [Numpy](https:\/\/numpy.org\/): To treat and work with the data (linear algebra)\n* [Pandas](https:\/\/pandas.pydata.org\/): To work with the dataset\n* [Sklearn](https:\/\/scikit-learn.org\/stable\/): To create and work with the models\n* [Seaborn](https:\/\/seaborn.pydata.org\/): To visualize the data with graphs\n* [Matplotlib](https:\/\/matplotlib.org\/): To visualize the data with graphs\n","e634723b":"**We check for null values**","7aef6c22":"## Step 4: Model Selection ##\n\nThere are many different models and many different variations of each model, to chose the ones that will have a better performance we need to consider which one will better fit our dataset.\n\nOur dataset is balanced, the input and the output is categorical and we have around 8.000 data examples. Considering this we'll use the following models:\n\n* Logisitc Regression\n* Naive Bayes\n* SVM\n* Random Forest\n* KNN\n\n","53ed894d":"**Data Split**\n\nIn order to train and test our models with different data we need to split it."}}