{"cell_type":{"41b33b4e":"code","b4f91fd8":"code","86815d16":"code","dc5a8542":"code","9336c84c":"code","4b9a4d94":"code","4791ff60":"code","b92aada3":"code","95e967ab":"code","0f4b2842":"code","cf0f2ca3":"code","8424942b":"code","1085fe3e":"code","35b45d15":"code","94c37f7a":"markdown","82f911c5":"markdown","c60f86b5":"markdown","3bcd5379":"markdown"},"source":{"41b33b4e":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, f1_score, precision_score \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","b4f91fd8":"df = pd.read_csv('..\/input\/indian-liver-patients-feature-selection-2-5\/liver_reduced_features.csv')","86815d16":"df.head()","dc5a8542":"scaler = StandardScaler()\nX = df.iloc[:,:-1].values #features\ny = df.iloc[:,-1].values.reshape(583,1) #targer\n\nX_scaled = StandardScaler().fit_transform(X) #scale features","9336c84c":"#Cross Validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)","4b9a4d94":"lr = LogisticRegression(random_state=1)\nnb = GaussianNB()\nknn = KNeighborsClassifier(n_neighbors=5)\nrf = RandomForestClassifier(max_depth=10, min_samples_split = 5)\n#mlp = MLPClassifier(hidden_layer_sizes=(1,9), activation='logistic', solver='sgd', alpha=1e-5, learning_rate='adaptive', random_state=100, verbose=False)\nclassifiers = [lr, nb, knn, rf]","4791ff60":"d = {}\nl = ['Logistic Regression', 'Naive Bayes', 'kNN', 'Random Forest']\ni = 0\nmetrics = pd.DataFrame(columns = ['Accuracy','F1 Score','Sensitivity', 'Specificity', 'AUC']) \nfor clf in classifiers:\n    clf_accuracy = []\n    clf_f1_score = []\n    clf_specificity = []\n    clf_sensitivity = []\n    clf_auc = []\n    tprs = []\n    mean_fpr = []\n    \n    for train_index, test_index in cv.split(X_scaled, y.ravel()):\n        #train test split\n        x_train_fold, x_test_fold = X_scaled[train_index], X_scaled[test_index] \n        y_train_fold, y_test_fold = y.ravel()[train_index], y.ravel()[test_index] \n        \n        #fit the model\n        clf.fit(x_train_fold, y_train_fold)\n        \n        #prediction\n        y_pred = clf.predict(x_test_fold)\n        \n        #confusion matrix\n        cm = confusion_matrix(y_test_fold,y_pred)\n        \n        tp = cm[1][1]\n        tn = cm[0][0]\n        fp = cm[0][1]\n        fn = cm[1][0]\n        \n        accuracy = (tp + tn) \/(tp + tn + fp + fn)\n        precision = (tp) \/ (tp + fp)\n        recall = (tp) \/ (tp + fn)\n        f1score = (2 * precision * recall) \/ (precision + recall)\n        sensitivity = tp \/ (tp + fn)\n        specificity = tn \/ (tn + fp)\n\n        #auc score\n        prediction = clf.fit(x_train_fold,y_train_fold).predict_proba(x_test_fold)\n        \n        fpr, tpr, t = roc_curve(y_test_fold.ravel(), prediction[:, 1])\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        \n        \n        #append the scores\n        clf_accuracy.append(accuracy)\n        clf_f1_score.append(f1score)\n        clf_sensitivity.append(sensitivity)\n        clf_specificity.append(specificity)\n        clf_auc.append(roc_auc)\n    \n    \n    d[l[i]]={'Accuracy': sum(clf_accuracy)\/10,'F1 Score': sum(clf_f1_score)\/10, 'Sensitivity': sum(clf_sensitivity)\/10,'Specificity': sum(clf_specificity)\/10, 'AUC': sum(clf_auc)\/10}\n    i+=1\n    #metrics.loc[-1] = d  # adding a row\n    #metrics.index = metrics.index + 1  # shifting index\n    #metrics = metrics.sort_index()  # sorting by index","b92aada3":"d","95e967ab":"l = ['Logistic Regression', 'Naive Bayes', 'kNN', 'Random Forest']","0f4b2842":"for i in l:\n    metrics = metrics.append(d[i], ignore_index = True)","cf0f2ca3":"metrics['Classifier'] = l","8424942b":"metrics","1085fe3e":"sns.set_theme()\nmetrics.plot.bar(figsize=(8,6), colormap = 'viridis')\nplt.xticks(ticks=[0, 1,2,3], labels=metrics['Classifier'])\nplt.show()","35b45d15":"# --------> code from: https:\/\/www.kaggle.com\/kanncaa1\/roc-curve-with-k-fold-cv\ni = 0\nfor clf in classifiers:\n    print(clf)\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    for train_index, test_index in cv.split(X_scaled, y.ravel()):\n        prediction = clf.fit(X_scaled[train_index],y.ravel()[train_index]).predict_proba(X_scaled[test_index])\n        fpr, tpr, t = roc_curve(y.ravel()[test_index], prediction[:, 1])\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n        i= i+1\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC')\n    plt.legend(loc=\"lower right\")\n    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n    plt.show()","94c37f7a":"## Classification Results\nBecause the dataset is really unbalanced, the classifiers cannot manage to find a way to classify correctly cases of the minority class. Due to this fact, I have calculated measuses like f1 score and specificity in order to evaluate the performance of the classifiers. Note that spesificity is really low in most cases. ","82f911c5":"## Scale the data","c60f86b5":"## Classifiers\nI implement Logistic Regression, Naive Bayes, kNN and Random Forest classifiers. Due to the imbalance cases on the two classes, it is a challege to find a classifier that manages to correctly classify both negative and positive cases. I use stratified 10 fold cross validation to train the models. Also, I do not use Accuracy as the representative metric to evaluate a classifier, but measures like f1 score, AUC, sensitivity and specificity will be calculated.\n1. Sensitivity = $ \\frac{TP}{TP+FN} $\n2. Specificity = $ \\frac{TN}{TN+FP} $\n3. Accuracy = $ \\frac{TN+TP}{TN+TP+FP+FN}$\n4. F1 Score = $ \\frac{2TP}{2TP+FP+FN}$","3bcd5379":"## Indian Liver Dataset Patients: Classification (4\/5)"}}