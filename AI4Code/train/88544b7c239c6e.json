{"cell_type":{"99ee53aa":"code","9e2a06a4":"code","4083d99e":"code","53ba9561":"code","ab2dc177":"code","0f03b0e0":"code","4322f59f":"code","b0dd7d57":"code","0046b32b":"code","2ef7594c":"code","392bfcce":"code","9d2585d5":"code","0779ca79":"code","7d8a829f":"code","55f43dc7":"code","32927b96":"code","9a2d8643":"code","beb8b3ca":"code","a63dfc52":"code","5c0aef12":"code","27b1d7f0":"code","d0103c5f":"code","041215e8":"code","8c1bdc96":"code","ecb1622c":"code","35f79e02":"code","fb55f67d":"code","0104cf6a":"code","3b3c8156":"code","51174127":"code","e8b0d56e":"code","8ff1c758":"code","3009f3a4":"code","6b1e3eae":"code","dccb9b6e":"code","b23b865d":"code","ace875e1":"code","9b25b703":"code","0442c305":"code","41c8eebf":"code","d67eca01":"code","1877ef13":"code","3a841aa7":"code","5177cdeb":"code","46c0ce0d":"code","e4f851a2":"code","ce91b37c":"code","436705dc":"code","684f6735":"markdown","0f80b8a2":"markdown","89fe01ac":"markdown","6b05bab4":"markdown","66aedbd2":"markdown","890ba475":"markdown","b21ca949":"markdown","adc02e3d":"markdown","7465caab":"markdown","bc56dc2c":"markdown","301c1117":"markdown","29a01931":"markdown","9c89e4d8":"markdown","8ab94bdb":"markdown","21953e6e":"markdown","f44550e4":"markdown","0e86f2d1":"markdown","e9aa3b70":"markdown","4be341c6":"markdown","90f68f83":"markdown","fd0039ee":"markdown","94f6b45d":"markdown","2d62a92d":"markdown","40d7c9b4":"markdown","a7772383":"markdown","a210782a":"markdown","ac4ffe4b":"markdown","480d125e":"markdown","04e90314":"markdown","a393f60b":"markdown","48ecfed8":"markdown","8d3f1473":"markdown","cb6711fa":"markdown","4dc6215d":"markdown","d534ebd2":"markdown","ee8c9722":"markdown","3cb2b33d":"markdown","8a334f2f":"markdown","b1f7aae3":"markdown","836d92d6":"markdown","864c7a05":"markdown","bf250d37":"markdown","3ffa1ed1":"markdown","ad73b551":"markdown","f78103cf":"markdown","d946355b":"markdown","6ea1ce3b":"markdown","36244336":"markdown","99c19f6f":"markdown","5fa22208":"markdown","45d06819":"markdown","52668291":"markdown"},"source":{"99ee53aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O \nimport os # operating system interaction\n\nfrom collections import Counter # counting total feature outliers\n\n# data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.preprocessing import LabelEncoder # encoding categoricals\nfrom sklearn.model_selection import train_test_split # train-test split\nfrom sklearn.preprocessing import StandardScaler # standardising\n\n# classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score # cross validation\n\nfrom sklearn.model_selection import GridSearchCV # hyperparameter tuning\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report # model evaluation\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e2a06a4":"CSV_PATH = \"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"\n\ndf = pd.read_csv(CSV_PATH)\n\ndf.shape ","4083d99e":"df.head()","53ba9561":"df.info()","ab2dc177":"df['quality'].unique()","0f03b0e0":"df['quality'].value_counts()","4322f59f":"df.isnull().sum()","b0dd7d57":"features = df.columns.values.tolist()\nfeatures.remove(\"quality\")\n\noutlier_index = list()\n\nfor feature in features:\n    \n    q1 = np.percentile(df[feature], 25)\n    q3 = np.percentile(df[feature], 75)\n    \n    iqr = q3 - q1\n    \n    step = iqr * 1.5\n    \n    lower_bound = q1 - step\n    upper_bound = q3 + step\n    \n    # List of indices where the attribute value lies outside the bounds\n    feature_outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)].index\n    \n    outlier_index.extend(feature_outliers)\n\noutlier_index = Counter(outlier_index) # returns a Counter object (similar to a dictionary)","0046b32b":"outlier_list = [index for index, count in outlier_index.items() if count > 3]\n\nprint(f\"Found {len(outlier_list)} outliers in the entire dataset.\")","2ef7594c":"df.loc[outlier_list]","392bfcce":"df = df.drop(outlier_list, axis=0).reset_index(drop=True)\ndf.shape","9d2585d5":"ax = plt.subplots(figsize=(10, 8))\nax = sns.heatmap(df.corr(),annot=True, fmt=\".2f\", cmap=\"coolwarm\")","0779ca79":"sns.lineplot(x=\"quality\", y=\"fixed acidity\", data=df)\nplt.title(\"Fixed Acidity against Quality\")\nplt.grid()","7d8a829f":"sns.lineplot(x=\"quality\", y=\"volatile acidity\", data=df)\nplt.title(\"Volatile Acidity against Quality\")\nplt.grid()","55f43dc7":"sns.lineplot(x=\"quality\", y=\"citric acid\", data=df)\nplt.title(\"Citric Acid against Quality\")\nplt.grid()","32927b96":"sns.lineplot(x=\"quality\", y=\"residual sugar\", data=df)\nplt.title(\"Residual Sugar against Quality\")\nplt.grid()","9a2d8643":"sns.lineplot(x=\"quality\", y=\"chlorides\", data=df)\nplt.title(\"Chlorides against Quality\")\nplt.grid()","beb8b3ca":"sns.lineplot(x=\"quality\", y=\"free sulfur dioxide\", data=df)\nplt.title(\"Free SO2 against Quality\")\nplt.grid()","a63dfc52":"sns.lineplot(x=\"quality\", y=\"total sulfur dioxide\", data=df)\nplt.title(\"Total SO2 against Quality\")\nplt.grid()","5c0aef12":"sns.lineplot(x=\"quality\", y=\"density\", data=df)\nplt.title(\"Density against Quality\")\nplt.grid()","27b1d7f0":"sns.lineplot(x=\"quality\", y=\"pH\", data=df)\nplt.title(\"pH against Quality\")\nplt.grid()","d0103c5f":"sns.lineplot(x=\"quality\", y=\"sulphates\", data=df)\nplt.title(\"Sulphates against Quality\")\nplt.grid()","041215e8":"sns.lineplot(x=\"quality\", y=\"alcohol\", data=df)\nplt.title(\"Alcohol against Quality\")\nplt.grid()","8c1bdc96":"bins = (2, 6.5, 8)\nlabels = ['bad', 'good']\ndf['quality'] = pd.cut(x=df['quality'], bins=bins, labels=labels)","ecb1622c":"encoder = LabelEncoder()\ndf['quality'] = encoder.fit_transform(df['quality'])","35f79e02":"df.head()","fb55f67d":"df['quality'].value_counts()","0104cf6a":"sns.countplot(x=df['quality'])","3b3c8156":"X = df.drop('quality', axis=1)\ny = df['quality'].copy()","51174127":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e8b0d56e":"print(f\"X Train : {X_train.shape}\")\nprint(f\"Y Train : {y_train.shape}\")\nprint(f\"X Test : {X_test.shape}\")\nprint(f\"Y Test : {y_test.shape}\")","8ff1c758":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","3009f3a4":"kfolds = StratifiedKFold(n_splits=10)","6b1e3eae":"# Setting a random_state value to reproduce the same results each time\nrnd = 42\n\nclassifiers = [\n    LogisticRegression(random_state=rnd),\n    SVC(random_state=rnd),\n    KNeighborsClassifier(),\n    SGDClassifier(random_state=rnd),\n    RandomForestClassifier(random_state=rnd),\n    GaussianNB(),\n]","dccb9b6e":"cv_scores = []\n\nfor classifier in classifiers :\n    cv_scores.append(cross_val_score(classifier, X_train, y_train, scoring = \"f1_macro\", cv=kfolds, n_jobs=-1))","b23b865d":"cv_means = []\ncv_std = []\n\nfor cv_score in cv_scores:\n    cv_means.append(cv_score.mean())\n    cv_std.append(cv_score.std())","ace875e1":"# Creating a Pandas DataFrame to store the classifier name, F1 mean and F1 std dev.\ncv_results = pd.DataFrame({\n    \"Classifier\": [\n        'Logistic Regression',\n        'Support Vector',\n        'K-Neighbors',\n        'SGD',\n        'Random Forest',\n        'Naive Bayes'\n    ],\n    \"CVMean\": cv_means,\n    \"CVSD\": cv_std\n})","9b25b703":"cv_results = cv_results.sort_values(['CVMean'], ascending=False).reset_index(drop=True)","0442c305":"ax = sns.barplot(x=\"CVMean\", y=\"Classifier\", data=cv_results, palette=\"plasma\", orient=\"h\", **{'xerr':cv_std})\nax.set_xlabel(\"Average F1 score\")\nax.set_title(\"Average F1 against Classifier\")","41c8eebf":"cv_results.head(1)","d67eca01":"param_grid = {\n    'n_estimators': [n for n in range(50, 100, 10)],\n    'max_features': ['auto', 'sqrt', 'log2', None],\n    'max_depth': [d for d in range(5, 9)],\n    'min_samples_leaf': [l for l in range(2, 5)],\n    'bootstrap': [True, False],\n}\n\nparam_grid","1877ef13":"RFmodel = RandomForestClassifier(random_state=rnd)\n\nRFtuner = GridSearchCV(\n    RFmodel,\n    param_grid=param_grid,\n    cv=kfolds,\n    scoring=\"f1_macro\",\n    verbose=2,\n    n_jobs=-1,\n)\n\nRFtuner.fit(X_train, y_train)","3a841aa7":"RFtuner.best_estimator_","5177cdeb":"best_RF = RFtuner.best_estimator_","46c0ce0d":"RFtuner.best_score_","e4f851a2":"rf_pred = best_RF.predict(X_test)\nprint(classification_report(rf_pred, y_test))","ce91b37c":"conf_matrix = confusion_matrix(y_test, rf_pred)\nconf_matrix","436705dc":"rf_pred_train = best_RF.predict(X_train)\n\nprint(f\"Train Set : {accuracy_score(rf_pred_train, y_train) * 100:.2f}%\")\nprint(f\"Test Set : {accuracy_score(rf_pred, y_test) * 100:.2f}%\")","684f6735":"We will create a list of all the classifiers and iterate through the list, train each model with cross-validation and finally compare the F1 scores (we are using macro F1 scores since we have imbalanced class but both classes are important)","0f80b8a2":"From the Pearson's correlation plot we can make out that quality seems to be highly correlated to the alcohol content, sulphates, citric acids, and volatile acidity.\n\nWe will explore all the predictors in detail now.","89fe01ac":"##### **Sulphates**","6b05bab4":"If a certain training example has 3 or more feature outliers, then we will delete the training example.","66aedbd2":"### **Data Visualisation**\nNow that we have successfully handled outliers, we can plot graphs for each of the features to understand its correlation with our ground truth (quality).","890ba475":"### **Tackling outliers**\nSome Machine Learning algorithms may work poorly when outliers are also passed to the algorithm. Mainly, models which make use of Euclidean distance for prediction are sensitive to outliers. \n\nWe will simple remove these outliers using Tukey's fences by defining our range as follows:\n\n${\\displaystyle {\\big [}Q_{1}-1.5(Q_{3}-Q_{1}),Q_{3}+1.5(Q_{3}-Q_{1}){\\big ]}}$\n\nwhere $Q_{1}$ is the 25th percentile and $Q_{3}$ is the 75th percentile\n\nIf the value lies outside these bounds, then it is considered to be an outlier.\n\nWe can repeat this process for each numerical attribute and if the wine sample has 3 or more attribute outliers, we delete the training example entirely.","b21ca949":"##### **Volatile Acidity**","adc02e3d":"##### **Total Sulphur Dioxide**","7465caab":"We can clearly see that the Random Forest Classifier model performs the best compared to the rest. We will choose this model and carry out hyperparameter optimisation in order to further improve the model's performance.","bc56dc2c":"##### **Chlorides**","301c1117":"##### **Standard Scaling**\nWe will standardise the numerical attributes so that they have a mean of 0 and a standard deviation of 1, feature-wise. This will improve the stability and covergence speed of our data. We will standardise after the train-test split to prevent any data leaks since we do not want our standardising to be based on the test set since that will significantly increase our performance on the test set.","29a01931":"We can clearly observe that as the volatile acidity decreases, the quality score increases.","9c89e4d8":"As the chloride concentration decreases, the quality score increases.","8ab94bdb":"There is a strong positive correlation between the sulphate levels and the quality score. As the sulphate level increases, the quality score also increases.","21953e6e":"##### **Residual Sugar**","f44550e4":"Again, there is a strong positive correlation between the alcohol level and the quality score. As the alcohol content increases, the quality score given also increases.","0e86f2d1":"##### **Alcohol**","e9aa3b70":"A perfect confusion matrix must have all the values along the diagonal from top-left to bottom-right and zeros elsewhere. Our confusion matrix gets close enough to that.\n\nOur model is performing good on unseen data. We can also calculate the accuracy, though it is not a reliable metrics for primary evaluation. ","4be341c6":"There is a weak negative correlation between pH and quality score. As the wine becomes more acidic (pH decreases), the quality score increases.","90f68f83":"### **Importing dependencies**","fd0039ee":"### **Loading the dataset**\n\nWe will first load the CSV dataset into a Pandas DataFrame. The dataset has 1599 examples with 11 features and one ground truth (class).","94f6b45d":"Now that we have classified the wine into one of the two categories, we can encode the quality feature to either 0 or 1 rather than passing a categorical ground truth to our machine learning algorithm.","2d62a92d":"This is great. All our predictors are entirely numeric which which means that we do not have to handle any categorical features. We also notice that the ground truth is an integer. We will look at this field in more detail now.","40d7c9b4":"We can now delete these outliers from our DataFrame. Remaining examples : $1599 - 15 = 1584$","a7772383":"The dataset has no NULL values. This means that we do not have to impute any values.","a210782a":"As the sulphates increase, the quality score increases till 5 then it starts decreasing. Again, there doesn't seem to be a strong positive or negative correlation.","ac4ffe4b":"##### **Splitting predictors and classes**","480d125e":"### **Evaluating on the Test Set**\nNow that we have tuned the Random Forest model, we will evaluate its final performance on the Test set which we had earlier set aside.","04e90314":"##### **Bins for Quality**\nWe will seperate the quality scores into two bins corresponding to 'good' and 'bad' wine. This will allow us to model our problem as a simple binary classification tast.","a393f60b":"##### **pH (acidity or basicity)**","48ecfed8":"No clear correlation between free $SO_2$ and quality score.","8d3f1473":"### **Hyperparameter Optimisation**\n\nWe will optimise the Random Forest Classifier using ```GridSearchCV()```","cb6711fa":"### **Data Preprocessing**\nNow that we have looked at the key features that affect the quality score provided, we can modify the data in order to improve the performance of our machine learning models.","4dc6215d":"##### **Fixed Acidity**","d534ebd2":"There is no clear trend between fixed acidity and quality.","ee8c9722":"##### **Pearson's Correlation**","3cb2b33d":"Turns out, our ground truth has only 6 unique values. Rather than identifying the exact class of the wine based on the predictors, we will transform this feature by allowing it to only take two values corresponding to 'good' or 'bad'. This will allow us to train a binary classifier.","8a334f2f":"We must note that there is a class imbalance in the dataset. Each quality class is not represented equally.","b1f7aae3":"We must note that there is a significant class imbalance in the dataset. This tells us that accuracy is not at all a good metrics to determine and compare the performance of our models. We can instead use the F1 score to ensure that the class imbalance does not give us a false idea that our model is performing well.\n\nThis is because if we simple train a model that predicts all wines have the ground truth as 0 we will still have an accuracy of 83.6% however the F1 score for this model will be very low since it takes into account both the precision and the recall of our predictions.\n\nHence, the F1 score will be more reliable and robust compared to the accuracy.","836d92d6":"##### **Free Sulphur Dioxide**","864c7a05":"#### **Thank you for reading through my kernel. If you liked it please do upvote and if you have any suggestions or feedback you could comment down below :))**","bf250d37":"##### **Train-Test Split**\nWe will split the data into a training set to train our models and a testing set to finally evaluate our models.","3ffa1ed1":"### **Training and Evaluating our models**\nNow that we have created bins and scaled the numerical features, we can now train different classifier models and evaluate their performance against each other.\n\nWe will be training 6 classifiers on our training set on 10 cross-validation folds. The models are:\n\n* Logistic Regression\n* Support Vector Machine Classifier\n* K-Nearest Neighbors Classifier\n* Stochastic Gradient Decent Classifier\n* Random Forest Classifier\n* Naive Bayes Classifier","ad73b551":"# Red Wine Quality Prediction\nBy: [Rishikesh Kanabar](https:\/\/www.kaggle.com\/rishikeshkanabar)\n\nA simple classification problem to identify whether the red wine is 'good' or 'bad' based on several features.","f78103cf":"Since the training set accuracy is greater than the testing set accuracy, our model might be slighly overfitting the data. Nevertheless, it has performed well on the test set.","d946355b":"##### **Citric Acid**","6ea1ce3b":"There is no distinct correlation between the residual sugar and the quality score.","36244336":"The is a negative correlation between density and quality. As the density decreases, the quality increases.","99c19f6f":"##### **Density**","5fa22208":"It can be seen from the plot that as the citric acid increases, the quality score also increases.","45d06819":"### **Data Exploration**\nNow that the dataset is successfully loaded, we will identify each feature, its datatype and check whether the dataset has any missing values.\n\nThis step will help us during data preprocessing and even when choosing our classification models.","52668291":"We will compute the mean and standard deviation of each fold of cross-validation to get a more reliable output of the F1 score."}}