{"cell_type":{"dcd5c40f":"code","6d3c53bf":"code","ece447fb":"code","8ce32c15":"code","6e6c5e62":"code","69f61a46":"code","f22dcae7":"code","44b94a8f":"code","28b10f90":"code","6bd1cb5d":"code","2cb8a6a6":"code","d61d3c9e":"markdown","8a5029ee":"markdown","e93c1963":"markdown","75d7e863":"markdown","044096a6":"markdown","5870ac05":"markdown","4dc78aaf":"markdown","a93cfe78":"markdown","cdb8b1f8":"markdown","fc77c5fb":"markdown","242a25b5":"markdown","a3d2f998":"markdown","ce6e2574":"markdown","233cbd8a":"markdown","9ac1d25b":"markdown"},"source":{"dcd5c40f":"# basic \nimport numpy as np\nimport pandas as pd\nimport sys\n\n# ui\nimport albumentations\nimport cv2\nfrom tqdm import tqdm\n\n# model training \nimport torch\nimport torch.nn as nn\n\n# model evaluation\nfrom sklearn import metrics","6d3c53bf":"sys.path.append(\"..\/input\/tez-lib\/\")\nimport tez\nfrom tez.callbacks import EarlyStopping","ece447fb":"sys.path.append(\"..\/input\/timmmaster\/\")\nimport timm","8ce32c15":"VALIDATION_FOLD = 0\nIMAGE_SIZE = 256\nBATCH_SIZE = 64\nEPOCHS = 20 ","6e6c5e62":"df = pd.read_csv(\"..\/input\/same-old-creating-folds\/train_10folds.csv\")\ntrain = df[df.kfold != VALIDATION_FOLD].reset_index(drop=True)\nvalid = df[df.kfold == VALIDATION_FOLD].reset_index(drop=True)\n\nprint(df.columns)\n\ndense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]","69f61a46":"train_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in train[\"Id\"].values]\nvalid_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in valid[\"Id\"].values]","f22dcae7":"train_aug = albumentations.Compose(\n    [\n        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE, p=1),\n        albumentations.HueSaturationValue(\n            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n        ),\n        albumentations.RandomBrightnessContrast(\n            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n        ),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)\n\nvalid_aug = albumentations.Compose(\n    [\n        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE, p=1),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)","44b94a8f":"class PawpularDataset:\n    def __init__(self, image_paths, dense_features, targets, augmentations):\n        self.image_paths = image_paths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):\n        image = cv2.imread(self.image_paths[item])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        features = self.dense_features[item, :]\n        targets = self.targets[item]\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"features\": torch.tensor(features, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }","28b10f90":"train_dataset = PawpularDataset(\n    image_paths=train_img_paths,\n    dense_features=train[dense_features].values,\n    targets=train.Pawpularity.values,\n    augmentations=train_aug,\n)\n\nvalid_dataset = PawpularDataset(\n    image_paths=valid_img_paths,\n    dense_features=valid[dense_features].values,\n    targets=valid.Pawpularity.values,\n    augmentations=valid_aug,\n)\n","6bd1cb5d":"class PawpularModel(tez.Model):\n    def __init__(self):\n        super().__init__()\n\n        self.model = timm.create_model(\"tf_efficientnet_b0_ns\", pretrained=True, in_chans=3)\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(128 + 12, 1)\n        \n        self.step_scheduler_after = \"epoch\"\n\n    def monitor_metrics(self, outputs, targets):\n        outputs = outputs.cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        rmse = metrics.mean_squared_error(targets, outputs, squared=False)\n        return {\"rmse\": rmse}\n\n    def fetch_scheduler(self):\n        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n        )\n        return sch\n\n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n        return opt\n\n    def forward(self, image, features, targets=None):\n\n        x = self.model(image)\n        x = self.dropout(x)\n        x = torch.cat([x, features], dim=1)\n        x = self.out(x)\n\n        if targets is not None:\n            loss = nn.MSELoss()(x, targets.view(-1, 1))\n            metrics = self.monitor_metrics(x, targets)\n            return x, loss, metrics\n        return x, 0, {}","2cb8a6a6":"model = PawpularModel()\n\nes = EarlyStopping(\n    monitor=\"valid_rmse\",\n    model_path=f\"model_f{VALIDATION_FOLD}.bin\",\n    patience=3,\n    mode=\"min\",\n    save_weights_only=True,\n)\n\nmodel.fit(\n    train_dataset,\n    valid_dataset=valid_dataset,\n    train_bs=BATCH_SIZE,\n    valid_bs=2*BATCH_SIZE,\n    device=\"cuda\",\n    epochs=EPOCHS,\n    callbacks=[es],\n    fp16=True,\n)","d61d3c9e":"# 0.3 variables","8a5029ee":"# 1.1. load data (feature + target)","e93c1963":"https:\/\/pypi.org\/project\/timm\/","75d7e863":"# 1.make dataset ","044096a6":"# 1.4. make dataset (1.1 + 1.2 + 1.3)","5870ac05":"# 3.build model","4dc78aaf":"https:\/\/pypi.org\/project\/tez\/","a93cfe78":"# 0.2.sys.path.append + import ","cdb8b1f8":"# 1.2. load image path ","fc77c5fb":"### tez: a simple pytorch trainer","242a25b5":"### timm: PyTorch Image Models","a3d2f998":"# 0.1.import libraries","ce6e2574":"# 2.define model","233cbd8a":"Idea around tez is simple:\n\n1.keep things as simple as possible\n\n2.make it as customizable as possible\n\n3.clean code\n\n4.faster prototyping\n\n5.production ready","9ac1d25b":"# 1.3. make augmentation"}}