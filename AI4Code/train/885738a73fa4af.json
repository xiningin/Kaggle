{"cell_type":{"35d3ba34":"code","2e906412":"code","43560ba5":"code","9e7bc13d":"code","2a1f9a19":"code","458b8f2d":"code","8bae69a0":"code","91d03587":"code","7623e1a5":"code","da068473":"code","eddf750f":"code","ddaff187":"code","ff09d06c":"code","34fd3ab5":"code","39220e6d":"code","4c24ca6d":"code","1e698777":"code","1ecc0e69":"code","1c856b2a":"code","a2a8a6de":"code","f4413b1c":"code","33f12ba8":"code","86c30893":"markdown","e03ae8f6":"markdown","c0e7be7b":"markdown","61f7b08a":"markdown","03cbd77a":"markdown","3010f2fc":"markdown"},"source":{"35d3ba34":"#################################\n########## LIBRARIES ############\n#################################\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport pandas as pd\nfrom tabulate import tabulate\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom sklearn.preprocessing import StandardScaler as scale\nfrom sklearn import preprocessing\n\n\n################################\n######### LOADING DATA #########\n################################\n# Read CSVs\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nall_data = pd.concat([train,test], axis=0)\n\n# Df display settings\npd.pandas.set_option('display.max_rows', None)\npd.pandas.set_option('display.max_columns', None)\n\n\n################################\n###### INITIAL PROFILING #######\n################################\ntrain_profile = pp.ProfileReport(train, minimal=False)\ntest_profile = pp.ProfileReport(test, minimal=False)","2e906412":"train.head()","43560ba5":"test.head()","9e7bc13d":"################################\n########## TRAIN DF ############\n################################\n# Recording our data types as lists to index later\ntrain_categorical = [col for col in train.columns if train[col].dtypes=='O']\ntrain_numerical = [col for col in train.columns if train[col].dtypes!='O']\ntrain_numerical_null = [col for col in train.columns if train[col].isnull().any() and train[col].dtypes!='O']\ntrain_categorical_null = [col for col in train.columns if train[col].isnull().any() and train[col].dtypes=='O']\ntrain_discrete = [col for col in train_numerical if len(train[col].unique())<25 and col not in ['Id']]\ntrain_continuous = [col for col in train_numerical if col not in train_discrete+['Id']]\n\n\n################################\n########### TEST DF ############\n################################\ntest_categorical = [col for col in test.columns if test[col].dtypes=='O']\ntest_numerical = [col for col in test.columns if test[col].dtypes!='O']\ntest_numerical_null = [col for col in test.columns if test[col].isnull().any() and test[col].dtypes!='O']\ntest_categorical_null = [col for col in test.columns if test[col].isnull().any() and test[col].dtypes=='O']\n\n#  Tabulated summary of datatypes and shape\nsummary_data = [\n    ['Train',train.shape,'Categorial', len(train_categorical), len(train_categorical_null), train_categorical_null],\n    ['','','Numerical', len(train_numerical), len(train_numerical_null), train_numerical_null],\n    ['Test',test.shape,'Categorial', len(test_categorical), len(test_categorical_null), test_categorical_null],\n    ['','','Numerical', len(test_numerical), len(test_numerical_null), test_numerical_null]]\n\nsummary_table = pd.DataFrame(summary_data, columns = ['Dataframe','Shape','Data Type', 'Total Columns','No. Columns w\/ Missing Values', 'Columns w\/ Missing Values'])\nsummary_table","2a1f9a19":"#train_profile","458b8f2d":"#test_profile","8bae69a0":"# SciPy packages\nfrom scipy.stats import norm, kurtosis, kurtosistest, skewtest\n\n# Set response variable\nresponseVariable = train['SalePrice']","91d03587":"################################\n###### NORMALITY TESTING #######\n################################\n# The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\nshapiro_t,shapiro_p = stats.shapiro(responseVariable)\n\n################################\n########### KURTOSIS ###########\n################################\n# The kurtosis of the normal distribution is zero. Distributions with a higher kurtosis have a heavier tail. \n# This function tests the null hypothesis that the kurtosis of the population from which the sample was drawn is that of the normal distribution.\nkurtosis_t,kurtosis_p = kurtosistest(responseVariable)\nkurtosis_value = abs(kurtosis(responseVariable))\n\n################################\n########### SKEWNESS ###########\n################################\nskewness_t,skewness_p = skewtest(responseVariable)\nskewness_value = abs(responseVariable).skew()\n\n\n################################\n###### TABULATED OUTPUT ########\n################################\n# Hypothesis tested results\nshaprio_result = \"Reject\" if shapiro_p < shapiro_t else \"Accept\"     \nkurtosis_result = \"Reject\" if kurtosis_p < kurtosis_t else \"Accept\" \nskewness_result = \"Reject\" if skewness_p < skewness_t else \"Accept\" \n\n# Table creation\nnormality_data = [\n    ['Shapiro','-', shapiro_t, shapiro_p, shaprio_result],\n    ['Kurtosis',kurtosis_value, kurtosis_t, kurtosis_p, kurtosis_result],\n    ['Skewness',skewness_value, skewness_t, skewness_p, skewness_result]\n]\nnormality_table = pd.DataFrame(normality_data, columns = ['Test','Value','T-value','P-value','Outcome'])\nnormality_table","7623e1a5":"################################\n###### DISTRUBUTION VIZ ########\n################################\n# Getting the main parameters of the Normal Ditribution ()\n# Reference: https:\/\/www.kaggle.com\/marto24\/ for graph formatting\n(mu, sigma) = norm.fit(responseVariable)\n\nplt.figure(figsize = (12,6))\nsns.distplot(responseVariable, kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","da068473":"#def cramers_corrected_stat(confusion_matrix):\n    #\"\"\" \n    #Calculated Cramers V statistics for categorical->categorical correlation\n    #\"\"\"\n#    chi2 = chi2_","eddf750f":"################################\n###### CORRELATION MATRIX ######\n################################\n\n# Correlation Matrix\nf, ax = plt.subplots(figsize=(30, 25))\nmat = train.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5)\nplt.show()\n\n# Correlation with output variable\ncor = train.corr()\ncor_target = abs(cor[\"SalePrice\"])\n\n# Correlation values\ncorrelation_values = cor_target\ncorrelation_values = correlation_values.drop('SalePrice').sort_values(ascending=False).to_frame('correlation')\n\n# Highly correlated features\nhighly_correlated_features = cor_target[cor_target>0.7]\nhighly_correlated_features = highly_correlated_features.drop('SalePrice').sort_values(ascending=False).to_frame('correlation')\n\n# Moderately correlated features\nmoderately_correlated_features = cor_target[cor_target>0.5]\nmoderately_correlated_features = moderately_correlated_features.drop('SalePrice').sort_values(ascending=False).to_frame('correlation')\nmoderately_correlated_features","ddaff187":"################################\n##     ##\n################################\n# Making a list of column indexes from moderately_correlated_features, then removing the features that do not appear in train_continuous\n# This is done so we can group not only the categorical\/numerical features with high correlation, but it makes displaying them optimally easier \nindex = moderately_correlated_features.index.tolist()\nremove_from_index = list(set(index) - set(train_continuous))\nindex = [e for e in index if e not in remove_from_index]\n\nfor i in index:\n    plt.figure(figsize = (16,7))\n    sns.regplot(data=train, x=i, y='SalePrice', scatter_kws={'alpha':0.2})\n    plt.title(i + 'vs SalePrice', fontsize = 12)\n    plt.legend(['$Pearson=$ {:.2f}'.format(cor_target[i])], loc = 'best')\n    plt.show()","ff09d06c":"################################\n######## NaN OCCURANCE #########\n################################\n#  First let's plt how the NaN's are looking across the 2 data sets\n\nnan_test = pd.DataFrame(test.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan_test['Perc(%)'] = (nan_test['NaN_sum']\/len(test))*100\n\nnan_train = pd.DataFrame(train.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan_train['Perc(%)'] = (nan_train['NaN_sum']\/len(train))*100\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\nsns.barplot(x = nan_train.index, y = nan_train['Perc(%)'], ax=ax[1], order=nan_train.iloc[:7].index).set_title('Feature NaN% in train set')\nsns.barplot(x = nan_test.index, y = nan_test['Perc(%)'], ax=ax[0], order=nan_test.iloc[:7].index).set_title('Feature NaN% in test set')\nplt.show()","34fd3ab5":"################################\n######## NEW VARIABLES #########\n################################\n# Declaring new dataframes and variables for targets\/features\n# DF match\ntest_Id = test['Id']\ntrain_Id = train['Id']\n\n# Response variable\nsaleprice_target = train['SalePrice']\n\n# Correlation target\ncor = train.corr()\ncor_target = abs(cor[\"SalePrice\"])\n\n# Dataframe copies \ntest_df = test\ntrain_df = train","39220e6d":"################################\n######## NaN OCCURANCE #########\n################################\n#  Dataframe for all data \nall_data = pd.concat([train_df,test_df], axis=0, sort=False)\n\n#  NaN counts and % for each column  \nnan = pd.DataFrame(all_data.isna().sum(), columns = ['Count'])\nnan['%NaN'] = (nan['Count']\/len(all_data))*100\nnan['%Cor'] = correlation_values\nnan = nan[nan['Count'] > 0]\nnan = nan.sort_values(by = ['Count'], ascending=False)\nnan = nan.T\nnan","4c24ca6d":"################################\n############ MNAR  #############\n################################\n# Let's start by filling the obvious MNAR values given from the aggle data_description.txt \n# Then we can deal with the MNAR sets of data with common key words (Bsmt, Garage ... etc) \n# It would be best to first determine the numerical MNAR values of Garage, Bsmt and Mas, if they are zero then we can fill the other rows with the coreresponding value of None\/NA\nall_data = pd.concat([train_df,test_df], axis=0, sort=False)\n\n# MNAR values given from the aggle data_description.txt \nall_data['Functional'] = all_data['Functional'].fillna('Typ')\nall_data['Fence'] = all_data['Fence'].fillna('None')\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')\nall_data['Alley'] = all_data['Alley'].fillna('None')\n\n# Since masonry veneer walls consist of a single non-structural external layer of masonry, typically made of brick, stone or manufactured stone - we can assume that a NaN in both MasVnrArea and MasVnrType means there is not a veneer wall\n# all_data[all_data['MasVnrArea'].isnull()]\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data['MasVnrType'] = np.where(all_data['MasVnrArea'] == 0, 'NA', all_data['MasVnrType'])\n\n# Check if the PoolArea ==0, if True, then replace it with None.  If False, replace it with Other\n# Since there were 3 values MCAR we'll ammend those using a replace by comparing them in a boxplot to PoolArea \nall_data['PoolQC'] = np.where(all_data['PoolArea'] == 0, 'NA', all_data['PoolQC'])\n\n# Check if Fireplaces==0, if True, then replace it with None.  If False, replace it with Other\nall_data['FireplaceQu'] = np.where(all_data['Fireplaces'] == 0, 'NA', all_data['FireplaceQu'])\n\n# Bsmt column nan values\n#['BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1',]\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0) # Before filling TotalBsmtSF with 0 - we need to chec if any of the categorical exist\nall_data['BsmtExposure'] = np.where(all_data['TotalBsmtSF'] == 0, 'NA', all_data['BsmtExposure'])\nall_data['BsmtCond'] = np.where(all_data['TotalBsmtSF'] == 0, 'NA', all_data['BsmtCond'])\nall_data['BsmtQual'] = np.where(all_data['TotalBsmtSF'] == 0, 'NA', all_data['BsmtQual'])\nall_data['BsmtFinType2'] = np.where(all_data['TotalBsmtSF'] == 0, 'NA', all_data['BsmtFinType2'])\nall_data['BsmtFinType1'] = np.where(all_data['TotalBsmtSF'] == 0, 'NA', all_data['BsmtFinType1'])\nall_data['BsmtFullBath'] = np.where(all_data['TotalBsmtSF'] == 0, 0, all_data['BsmtFullBath'])\nall_data['BsmtHalfBath'] = np.where(all_data['TotalBsmtSF'] == 0, 0, all_data['BsmtHalfBath'])\nall_data['BsmtFinSF2'] = np.where(all_data['TotalBsmtSF'] == 0, 0, all_data['BsmtFinSF2'])\nall_data['BsmtFinSF1'] = np.where(all_data['TotalBsmtSF'] == 0, 0, all_data['BsmtFinSF1'])\nall_data['BsmtUnfSF'] = np.where(all_data['TotalBsmtSF'] == 0, 0, all_data['BsmtUnfSF'])\n\n# Garage column nan values\n# ['GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'GarageCars', 'GarageArea']\nall_data['GarageArea'] = all_data['GarageArea'].fillna(0) # Before filling TotalBsmtSF with 0 - we need to chec if any of the categorical exist\nall_data['GarageFinish'] = np.where(all_data['GarageArea'] == 0, 'NA', all_data['GarageFinish'])\nall_data['GarageQual'] = np.where(all_data['GarageArea'] == 0, 'NA', all_data['GarageQual'])\nall_data['GarageCond'] = np.where(all_data['GarageArea'] == 0, 'NA', all_data['GarageCond'])\nall_data['GarageType'] = np.where(all_data['GarageArea'] == 0, 'NA', all_data['GarageType'])\nall_data['GarageCars'] = np.where(all_data['GarageArea'] == 0, 0, all_data['GarageCars'])\n\n\n#bsmt_columns = [col for col in ammended_data.columns if 'Bsmt' in col]\n# bsmt_columns_null = nan[bsmt_columns_null][all_data.isnull().any(axis=1)]\n# DF with nulls of bsmt columns\n#all_data[bsmt_columns_null][all_data.isnull().any(axis=1)]\n#print(all_data['BsmtFullBath'].value_counts())\n\n#all_data[bsmt_columns_null][all_data[bsmt_columns_null].isnull().any(axis=1)]\n#all_data[garage_columns_null][all_data[garage_columns_null].isnull().any(axis=1)]","1e698777":"################################\n###### VISUALISING NaN'S #######\n################################\nremaining = all_data.drop('SalePrice', axis = 1) # We can just drop this since there isn't a major correlation with the response variable and it constains a value we can't really impute\n\n#  Plotting the remaining NaN features occurance\nnan = pd.DataFrame(remaining.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan['Perc(%)'] = (nan['NaN_sum']\/2919)*100\nnan['%Cor'] = correlation_values\nplt.figure(figsize = (15,5))\nsns.barplot(x = nan['Perc(%)'], y = nan.index, order=nan.iloc[:10].index)\nplt.xticks(rotation=45)\nplt.title('Features containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()\nnan = nan.T\nnan\n\n","1ecc0e69":"################################\n##### MODE FILLING NaN'S #######\n################################\n# Filling with mode\n# Creating index of column names\nindex = [i for i in remaining.columns if remaining[i].isnull().any()]\n\nfor i in all_data[index]:\n    all_data[i].fillna(all_data[i].mode()[0], inplace=True)","1c856b2a":"train_test_dummy = pd.get_dummies(all_data)\ntrain_test_dummy.shape \n\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i])\n    \ntarget_log = np.log1p(responseVariable)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","a2a8a6de":"import shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n\ntrain = train_test_dummy[0:1460]\ntest = train_test_dummy[1460:]\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)\n\n# 10 Fold Cross validation\nkf = KFold(n_splits=5, random_state=1, shuffle=True)\ncv_scores = []\ncv_std = []\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']\n\n\n########\n\n\n# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n\n\nfinal_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","f4413b1c":"final_cv_score","33f12ba8":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=45)\nplt.show()","86c30893":"## MODELLING ##\n\n.....","e03ae8f6":"### EXPLORATORY ANALYSIS ###\nExploring our target variable and the correlation each feature has with it.","c0e7be7b":"### COMBING DATA SETS ###\n\nSince there is a significant amount of missing values across similar features in either data set lets check their correlation and see if we can work them together to start filling in our training set.","61f7b08a":"##############################\n# IGNORE FOR NOW\n### Potential independance testing etc.. for feature engineering in later revisions.....\n\nThe **Chi-Square** test of independence is used to determine if there is an association between two categorical variables. A contingency table or a crosstab is used by the chi-sq. test to check for the frequency in those categories.\n* A typical rule is that all of the observed and expected frequencies should be at least 5.\n* the sum of the observed and expected frequencies must be the same for the test to be valid\n\nThe **Cramer\u2019s coefficient** is a measure of association between two nominal variables, giving a value between 0 and +1.\n\n* The columns \u2014 \u201cIndependent\u201d show the result of the chi-sq test significance. If the p-value for chi-sq was <0.05, the association is going to be dependent.\n\n##############################","03cbd77a":"## IMPUTATION ##\n\nLotFrontage still contains a large number of NaN's but since there is also a medium correlation between this value and our response variable, I'm a little apprehensive to just use a fill na.  Instead, I think I'll use 2\/3 features that most closely assosiate with LotFrontage, and run a KNN imputer with a set of 5 or so neighbours.  This way we can use more than 1 feature to inform our decision on filling such a large number of missing values.\n\n\n\n\n\n#### KNN IMPUTER ####\nKNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n\n\n#### ITERATIVE IMPUTER ####\nOn the contrary, we still have a number of MCAR\/MAR in our set.  We were able to remove the majority of NaN values with decent accuracy, so now we can utilise an IterativeImputer to build a linear regression model of these features to determine the remaining NaNs.\n\n","3010f2fc":"### DATA PREPROCESSING ###\nMost of this section pertains to MAR analysis, determinining maximum likelihood and multivariate imputation. "}}