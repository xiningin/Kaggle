{"cell_type":{"cee69b8f":"code","37a9b3f0":"code","c91a3e0b":"code","59ad09ce":"code","070a746e":"code","d893a7c6":"code","f33b9b24":"code","a574d95a":"code","ff196ddc":"code","69e3720d":"code","689007c6":"code","138712f6":"code","3745881f":"code","01451b8b":"code","342bc047":"code","751c8a3d":"code","e289b7a8":"code","12a50d63":"code","331fec01":"code","f97a2d2f":"code","2da87cfe":"code","eef59c84":"code","cd3f523e":"code","154371ee":"code","38321c93":"code","6cb7dfe0":"code","e0426664":"code","d9cb6063":"code","a97b40f9":"code","3605299d":"code","d3157b90":"code","bb6db796":"code","510704b5":"code","dd2c1c2d":"code","1b44b0d7":"code","0fb63fad":"code","59dc621e":"code","ad7ff32b":"code","746a8d46":"code","c09a36fd":"code","2df321cf":"code","07945cff":"code","ae9e3b8c":"code","728d0e9c":"code","176b4fcb":"code","29af92e6":"code","623006ba":"code","2ce8c245":"code","50310d5e":"code","ccf758c0":"code","bb43d3fd":"code","55d65abc":"code","65ec784f":"code","c958f11d":"code","801b5aaf":"code","29835655":"code","736885fe":"code","6d30f9d3":"code","fdf9ed63":"code","5e448f2e":"markdown","c9bf5d01":"markdown","d6cba382":"markdown","12003fc8":"markdown","1a0f2323":"markdown","83c5f79a":"markdown","32119493":"markdown","a920d321":"markdown","5337b01c":"markdown","fb41d5aa":"markdown","51ae7324":"markdown","ccb8d09f":"markdown","c14ee871":"markdown","cc2c6126":"markdown","ec4913a8":"markdown","9f5b93d7":"markdown","f10b85ee":"markdown","5a49e354":"markdown"},"source":{"cee69b8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37a9b3f0":"# importing required libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score,roc_auc_score,confusion_matrix,roc_curve","c91a3e0b":"# reading the data and displaying the head of the data\ndf=pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","59ad09ce":"# checking the shape\ndf.shape","070a746e":"# checking info \ndf.info()","d893a7c6":"# checking five point summary of data\ndf.describe(include = 'all')\n","f33b9b24":"# checking for duplicates\nsum(df['id'].duplicated())==0","a574d95a":"# checking the null values in DataFrame\nround(df.isnull().sum()*100\/len(df),2)","ff196ddc":"df['bmi'].describe()","69e3720d":"# distribution plot for BMI\nplt.figure(figsize=[12,8])\nsns.distplot(df['bmi'])\nplt.axvline(df['bmi'].mean(),label='mean',color='r')\nplt.axvline(df['bmi'].median(),label='median',color='g')\nplt.legend()\nplt.show()","689007c6":"# replacing null values with median\ndf.bmi.fillna(28.1,axis=0, inplace=True)","138712f6":"# checking for null values after null value treatment\ndf.isnull().sum()","3745881f":"df.dtypes","01451b8b":"# mapping 0 to no and 1 to yes for hyper tension and heart_disease variables\ncols=['hypertension','heart_disease']\nfor col in cols:\n    df[col]=df[col].map({1:'Yes',0:'No'})","342bc047":"df.head()","751c8a3d":"# creating numerical and categorical columns in each list\nnum_cols=list(df.select_dtypes(include=np.number).columns)\nprint(num_cols)\ncat_cols=list(df.select_dtypes(include='object').columns)\nprint(cat_cols)","e289b7a8":"# checking distribution of numerical columns\nplt.figure(figsize=[15,10])\nfor col in enumerate(num_cols[1:]):\n    plt.subplot(2,2,col[0]+1)\n    sns.distplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","12a50d63":"# checking countplot for categorical columns \nplt.figure(figsize=[20,15])\nfor col in enumerate(cat_cols):\n    plt.subplot(4,2,col[0]+1)\n    sns.countplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","331fec01":"# scatter plot for age vs average_glucose_level\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['avg_glucose_level'],color='cyan')\nplt.show()","f97a2d2f":"# scatter plot for age vs bmi\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['bmi'],color='orange')\nplt.show()","2da87cfe":"# scatter plot for avg_glucose_level vs bmi\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['avg_glucose_level'],df['bmi'],color='g')\nplt.show()","eef59c84":"# checking countplot with stroke for categorical columns \nplt.figure(figsize=[20,18])\nfor col in enumerate(cat_cols):\n    plt.subplot(4,2,col[0]+1)\n    sns.countplot(df[col[1]],hue=df['stroke'])\n    plt.tight_layout()\nplt.show()\n","cd3f523e":"# pair plot \nplt.figure(figsize=[20,12])\nsns.pairplot(data=df,hue='stroke')\nplt.show()","154371ee":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['avg_glucose_level'],hue=df['stroke'],color='cyan')\nplt.show()","38321c93":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['bmi'],hue=df['stroke'])\nplt.show()\n","6cb7dfe0":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['avg_glucose_level'],df['bmi'],hue=df['stroke'])\nplt.show()","e0426664":"# correlation matrix\ndf.corr()","d9cb6063":"# heatmap\nplt.figure(figsize=[10,6])\nsns.heatmap(df.corr(),cmap='RdYlGn',annot=True)\nplt.show()","a97b40f9":"# checking for outliers in numerical columns\nplt.figure(figsize=[15,10])\nfor col in enumerate(num_cols[1:-1]):\n    plt.subplot(2,2,col[0]+1)\n    sns.boxplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","3605299d":"# IQR capping method\n# x = df.describe()\n# for i in num_cols[2:-1]:\n#     q1=x.loc['25%',i]\n#     q3=x.loc['75%',i]\n#     iqr=q3-q1\n#     uppl=q3+(1.5*iqr)\n#     lowl=q1-(1.5*iqr)\n#     df[i]=df[i].apply(lambda x:uppl if x>uppl else x )\n#     df[i]=df[i].apply(lambda x: lowl if x<lowl else x)","d3157b90":"\n# plt.figure(figsize=[15,10])\n# for col in enumerate(num_cols[1:-1]):\n#     plt.subplot(2,2,col[0]+1)\n#     sns.boxplot(df[col[1]])\n#     plt.tight_layout()\n# plt.show()","bb6db796":"df.shape","510704b5":"x=df.drop(['stroke','id'],axis=1)\ny=df['stroke']","dd2c1c2d":"# creating dummies\nxd=pd.get_dummies(x,drop_first=True)\nxd.head()","1b44b0d7":"# checking the correlation after creating the dummies\nplt.figure(figsize=[20,10])\nsns.heatmap(xd.corr(),annot=True,cmap='RdYlGn')\nplt.show()","0fb63fad":"x_train,x_test,y_train,y_test=train_test_split(xd,y,test_size=0.3,random_state=100)","59dc621e":"# checking the shape of x_train x_test y_train y_test\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","ad7ff32b":"from sklearn.preprocessing import MinMaxScaler","746a8d46":"cols_to_scale=['age','avg_glucose_level','bmi']","c09a36fd":"# creating scaler instance\nscaler=MinMaxScaler()\n\n# fit transform for x_train\nx_train[cols_to_scale]=scaler.fit_transform(x_train[cols_to_scale])\n\n# transforming for x_test\nx_test[cols_to_scale]=scaler.transform(x_test[cols_to_scale])","2df321cf":"x_train.head()","07945cff":"# creating an instance for logistic regression\nlogreg=LogisticRegression(solver='liblinear')","ae9e3b8c":"logreg.fit(x_train,y_train)","728d0e9c":"y_train_pred=logreg.predict(x_train)\ny_train_pred","176b4fcb":"accuracy_score(y_train,y_train_pred)","29af92e6":"y_train_prob=logreg.predict_proba(x_train)[:,1]\ny_train_prob","623006ba":"roc_auc_score(y_train,y_train_prob)","2ce8c245":"print(confusion_matrix(y_train,y_train_pred))","50310d5e":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(12, 8))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","ccf758c0":"from sklearn import metrics","bb43d3fd":"fpr,tpr,thresholds=metrics.roc_curve(y_train_pred,y_train_prob,drop_intermediate=False)","55d65abc":"draw_roc(y_train,y_train_prob)","65ec784f":"y_test_pred=logreg.predict(x_test)\naccuracy_score(y_test,y_test_pred)","c958f11d":"print(confusion_matrix(y_test,y_test_pred))","801b5aaf":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV,StratifiedKFold\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n","29835655":"gbc=GradientBoostingClassifier()\n\nparams={'n_estimators':sp_randint(50,250),'max_depth':sp_randint(1,15),\n        'learning_rate':sp_uniform(0,0.5),'learning_rate':range(0,2)}\n\nr_search=RandomizedSearchCV(estimator=gbc,param_distributions=params,cv=3,n_iter=10,scoring='roc_auc',\n                           random_state=4,n_jobs=-1)\n\nprint(r_search.fit(xd,y))\nprint(r_search.best_params_)","736885fe":"gbc=GradientBoostingClassifier(**r_search.best_params_,random_state=4)\ngbc.fit(x_train,y_train)\ny_train_pred=gbc.predict(x_train)\ny_train_prob=gbc.predict_proba(x_train)[:,1]\nprint('train - confusion matrix : ','\\n',confusion_matrix(y_train,y_train_pred))\nprint('train - accuracy score : ','\\n', accuracy_score(y_train,y_train_pred))\nprint('train - AUC : ', roc_auc_score(y_train,y_train_prob))\n\ny_pred=gbc.predict(x_test)\ny_prob=gbc.predict_proba(x_test)[:,1]\nprint('test - confusion matrix : ','\\n',confusion_matrix(y_test,y_pred))\nprint('test - accuracy score : ','\\n', accuracy_score(y_test,y_pred))\nprint('test - AUC : ', roc_auc_score(y_test,y_prob))","6d30f9d3":"draw_roc(y_train,y_train_prob)","fdf9ed63":"draw_roc(y_test,y_prob)","5e448f2e":"# Step 7 : Model building","c9bf5d01":"__we can see from the above plot that both mean and median are close to each other. so we can replace null values with  median because median is less affected by outliers than mean__","d6cba382":"## b) Bivariate Analysis","12003fc8":"__Here we are checking each id value to check ther duplicates in the data and we are taking sum of all the boolean values and equating it to zero to check if it is True__","1a0f2323":"# scaling","83c5f79a":"## Base model","32119493":"# countplots for categorical columns","a920d321":"__we can see that avg_glucose_level and bmi are having so many outliers so treating them with various preprocessing techniques and iqr iqr capping method__","5337b01c":"# step 3 : Exploratory Data Analysis","fb41d5aa":"__As we can see BMI is the only variable containing null values . lets check this columns for more details__","51ae7324":"# Final model : gradient boosting","ccb8d09f":"# Step 4 : Outlier Treatment","c14ee871":"# step 2 - checking for duplicates and missing values ","cc2c6126":"## step 5: Dummies Creation","ec4913a8":"# step 1 : Reading and understanding the data","9f5b93d7":"## Step 6 : Train Test Split ","f10b85ee":"## c)Multivariate analysis","5a49e354":"__A) univariate analysis__"}}