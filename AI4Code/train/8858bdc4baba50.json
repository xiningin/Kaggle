{"cell_type":{"24a35b87":"code","e9359537":"code","8abb7d66":"code","3fd73cda":"code","08c2064d":"code","d6702620":"code","e4db6300":"code","2823d80a":"code","5f0d2624":"code","79b6fd17":"code","a7d3a353":"code","0d79c6c3":"code","d9d87c72":"code","d69e4018":"code","3a8a0813":"code","7a121740":"code","22cdd010":"code","61e7b77b":"code","4c5a3957":"code","3499f009":"code","7fddf9a9":"code","8c21d244":"code","18c1b75b":"code","6593ada0":"code","e5efff4a":"code","07db99ac":"markdown","841861dd":"markdown","a6b274a9":"markdown","268b07dd":"markdown","97ac8e51":"markdown","e4727222":"markdown"},"source":{"24a35b87":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp","e9359537":"day_df=pd.read_csv('..\/input\/injury-prediction-for-competitive-runners\/day_approach_maskedID_timeseries.csv')","8abb7d66":"day_df.head(10)","3fd73cda":"# set seed for reproducibility\nnp.random.seed(0) ","08c2064d":"#get the number of missing data points per column\nmissing_values_count = day_df.isnull().sum()\nmissing_values_count","d6702620":"columns_with_na_dropped = day_df.dropna(axis=1)\ncolumns_with_na_dropped.head()","e4db6300":"day_df.info()","2823d80a":"day_df.describe()","5f0d2624":"print(\"Columns in original dataset: %d \\n\" % day_df.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])\n","79b6fd17":"day_df.columns","a7d3a353":"day_df.count()","0d79c6c3":"day_df.sum()","d9d87c72":"# get all the unique values in the 'Country' column\ncountries = day_df['total km'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries","d69e4018":"day_df['Date'].plot.hist()","3a8a0813":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = day_df.loc[:,day_df.columns != 'injury'], day_df.loc[:,'injury']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","7a121740":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = day_df.loc[:,day_df.columns != 'injury'], day_df.loc[:,'injury']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","22cdd010":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","61e7b77b":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = day_df[day_df['injury'] =='Abnormal']\nx = np.array(day_df.loc[:,'Date']).reshape(-1,1)\ny = np.array(day_df.loc[:,'Athlete ID']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('Date')\nplt.ylabel('Athlete ID')\nplt.show()","4c5a3957":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('injury')\nplt.ylabel('Date')\nplt.show()","3499f009":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","7fddf9a9":"# Ridge\nfrom sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint('Ridge score: ',ridge.score(x_test,y_test))\n","8c21d244":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = day_df.loc[:,day_df.columns != 'injury'], day_df.loc[:,'injury']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","18c1b75b":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","6593ada0":"sns.countplot(x=\"injury\", data=day_df)\nday_df.loc[:,'injury'].value_counts()","e5efff4a":"sns.countplot(x=\"Date\", data=day_df)\nday_df.loc[:,'Date'].value_counts()","07db99ac":"![123](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRHYA1RK9WHQcr3Wzlk3s9hYidWYFvDoshYsQ&usqp=CAU)","841861dd":"**Do some analyzes on the data**","a6b274a9":"Accuracy is 98% so is it good ? I do not know actually, we will see at the end of tutorial.\nNow the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity","268b07dd":"# Introduction\n**The popularity of running continues to increase, which means that the incidence of running-related injuries will probably also continue to increase. Little is known about risk factors for running injuries and whether they are sex-specific.\nLongitudinal cohort studies with a minimal follow-up of 1 month that investigated the association between risk factors (personal factors, running\/training factors and\/or health and lifestyle factors) and the occurrence of lower limb injuries in runners were included.\nOf 400 articles retrieved, 15 longitudinal studies were included, of which 11 were considered high-quality studies and 4 moderate-quality studies. Overall, women were at lower risk than men for sustaining running-related injuries. Strong and moderate evidence was found that a history of previous injury and of having used orthotics\/inserts was associated with an increased risk of running injuries. Age, previous sports activity, running on a concrete surface, participating in a marathon, weekly running distance (30\u201339 miles) and wearing running shoes for 4 to 6 months were associated with a greater risk of injury in women than in men. A history of previous injuries, having a running experience of 0\u20132 years, restarting running, weekly running distance (20\u201329 miles) and having a running distance of more than 40 miles per week were associated with a greater risk of running-related injury in men than in women.**\n","97ac8e51":"# Call some libraries","e4727222":"# Read Dataset"}}