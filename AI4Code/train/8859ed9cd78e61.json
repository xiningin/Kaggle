{"cell_type":{"5bb96824":"code","94d85bae":"code","4cbc51a2":"code","96dca315":"code","c008aafc":"code","c87fb5af":"code","af842f43":"code","1b545418":"code","6bde08f4":"code","81d8f48e":"code","a40cbd61":"code","da0a00d9":"code","9c4b954b":"code","0d2c2d5a":"code","85fd91f9":"code","5f6bc167":"code","81e380ac":"code","4b102efb":"code","84ff22ea":"code","54550791":"code","b1226d8f":"code","d96320bf":"code","0cff9285":"code","5da46773":"code","980c7833":"code","412e5c9c":"code","b83e1152":"code","c6f39f24":"code","4e1a5fe6":"code","218ce690":"code","d6f448a1":"markdown","2640a2ed":"markdown","4f56e7ac":"markdown","3a4e9249":"markdown","ea8ed315":"markdown","8378d095":"markdown","17ff57bb":"markdown"},"source":{"5bb96824":"from tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","94d85bae":"df = pd.read_csv('..\/input\/beijing-pm25-data-data-set\/PRSA_data_2010.1.1-2014.12.31.csv')\ndf","4cbc51a2":"df.info()","96dca315":"df['pm2.5'].isna().sum()","c008aafc":"# drop the rows directly -> mess up the order\n# first 24 rows have pm2.5 value that is NaN -> discard\n# else: forward filling\ndf = df[24:].fillna(method='ffill')\ndf['pm2.5'].isna().sum()","c87fb5af":"import datetime\n\ndf['time'] = df.apply(lambda x : datetime.datetime(year=x['year'], month=x['month'], day=x['day'], hour=x['hour']), axis=1)\ndf.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True)\ndf = df.set_index('time')\ndf.head()","af842f43":"df['cbwd'].unique()","1b545418":"df = df.join(pd.get_dummies(df['cbwd'])) # one-hot encoding\ndel df['cbwd']\ndf.head()","6bde08f4":"df['pm2.5'][-1000:].plot()","81d8f48e":"df['TEMP'][-1000:].plot()","a40cbd61":"seq_len = 5*24 # observe the data for the past 5 days\ndelay = 1*24 # predict the PM2.5 value one day after\n\ndf_ = np.array([df.iloc[i : i + seq_len + delay].values for i in range(len(df) - seq_len - delay)])\ndf_.shape","da0a00d9":"np.random.shuffle(df_)\nx = df_[:, :5*24, :]\ny = df_[:, -1, 0]\nx.shape, y.shape","9c4b954b":"split = int(y.shape[0]*0.8)\ntrain_x = x[:split]\ntrain_y = y[:split]\ntest_x = x[split:]\ntest_y = y[split:]\n\nmean = train_x.mean(axis=0)\nstd = train_x.std(axis=0)\ntrain_x = (train_x - mean) \/ std\ntest_x = (test_x - mean) \/ std # Use the mean & std of train. Since there's no way for us to know the future.","0d2c2d5a":"train_x.shape, test_x.shape","85fd91f9":"model = keras.Sequential()\nmodel.add(layers.Flatten(input_shape=(120, 11)))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1)) # Regression -> No Need for Activation\n\nmodel.summary()","5f6bc167":"model.compile(optimizer='adam', loss='mse', metrics=['mae'])\nhistory = model.fit(train_x, train_y, batch_size=128, epochs=50, validation_data=(test_x, test_y))","81e380ac":"plt.plot(history.epoch, history.history['mae'], c='m')\nplt.plot(history.epoch, history.history['val_mae'], c='c')","4b102efb":"model = keras.Sequential()\nmodel.add(layers.LSTM(32, input_shape=(120, 11)))\nmodel.add(layers.Dense(1))\nmodel.summary()","84ff22ea":"model.compile(optimizer='adam', loss='mae')\nhistory = model.fit(train_x, train_y, batch_size=128, epochs=150, validation_data=(test_x, test_y))","54550791":"plt.plot(history.epoch, history.history['loss'], c='m')\nplt.plot(history.epoch, history.history['val_loss'], c='c')","b1226d8f":"model = keras.Sequential()\nmodel.add(layers.LSTM(32, input_shape=(120, 11), return_sequences=True)) \nmodel.add(layers.LSTM(32, return_sequences=True)) \nmodel.add(layers.LSTM(32)) \nmodel.add(layers.Dense(1))\nmodel.summary()","d96320bf":"lr_reduced = keras.callbacks.ReduceLROnPlateau('val_loss', patience=3, factor=0.5, min_lr=0.00001)","0cff9285":"model.compile(optimizer='adam', loss='mae')\nhistory = model.fit(train_x, train_y, batch_size=128, epochs=150, validation_data=(test_x, test_y), callbacks=[lr_reduced])","5da46773":"plt.plot(history.epoch, history.history['loss'], c='m')\nplt.plot(history.epoch, history.history['val_loss'], c='c')","980c7833":"model.evaluate(test_x, test_y, verbose=0)","412e5c9c":"test_predict = model.predict(test_x)\ntest_x.shape, test_predict.shape","b83e1152":"test_predict[:5]","c6f39f24":"test_data = df[-120:]\ntest_data = (test_data - mean)\/std\ntest_data","4e1a5fe6":"test_data = np.expand_dims(test_data, axis=0)\ntest_data.shape","218ce690":"model.predict(test_data) # 2015.1.1 11pm pM2.5","d6f448a1":"## Evaluation & Prediction","2640a2ed":"## Start by Trying a Simple Model","4f56e7ac":"## Split & Normalize the Data","3a4e9249":"## Build LSTM Model (Multi-Layer)\nReturn the state info to feed back to the second LSTM layer\n\nUse callbacks to decrease learning rate","ea8ed315":"## Build LSTM Model (Single-Layer)\n\n(34924, 120, 11) -> (batch, time for observation, features per observation)","8378d095":"## Preprocessing","17ff57bb":"## Determine Parameters"}}