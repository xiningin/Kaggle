{"cell_type":{"8a6f40e9":"code","47a09063":"code","9dab79ab":"code","fd956151":"code","88163cb2":"code","5b20ec9a":"code","d0f42adb":"code","3030c34b":"code","a1023674":"code","161fa1ef":"code","d04cf879":"code","1ceee998":"code","d661b9be":"code","0014764f":"code","026efa78":"code","64669db0":"code","f0e21019":"code","1a4faadd":"code","c2f782e8":"code","16e70a35":"code","8f1eb6a6":"code","d3eb562f":"code","c1b28246":"code","49624d7f":"code","155fedc1":"code","4537198d":"code","7df577b4":"code","6557715b":"code","12808d75":"code","59766217":"code","a92d7e54":"code","00a4b2e3":"code","c3de7dfa":"code","6f605c92":"code","551718c2":"code","07dd9695":"code","360bb5b3":"code","4f530c14":"code","419e7cde":"code","973069bc":"code","7ae6bb8d":"code","9e108946":"code","87f9ace3":"code","0451c140":"code","75f245b1":"code","fbda5e18":"code","4b61f891":"markdown","2fb73c31":"markdown","9d0754df":"markdown","d4b37871":"markdown","88587163":"markdown","12e259e6":"markdown","d31026d0":"markdown","2246eada":"markdown","aa3c99a7":"markdown","8b66b60b":"markdown","e00fda19":"markdown","3b53eb5c":"markdown","364893cb":"markdown","7673e0f5":"markdown","07aef4e8":"markdown","05c68177":"markdown","34b6904d":"markdown","20a910c3":"markdown","a8c44b01":"markdown","ef907806":"markdown","8f311ec2":"markdown","fc6a6ab6":"markdown"},"source":{"8a6f40e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# for plotting \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","47a09063":"building_metadata = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\")\nweather_train = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\")\ntrain = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\")","9dab79ab":"print('Size of train data', train.shape)\nprint('Size of weather_train data', weather_train.shape)\nprint('Size of building_meta data', building_metadata.shape)","fd956151":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","88163cb2":"## REducing memory\nfor df in [train, weather_train, building_metadata]:\n    df = reduce_mem_usage(df)","5b20ec9a":"train.head(3)\n# print(train.dtypes)","d0f42adb":"train['meter_reading'].hist(figsize=(6, 5))","3030c34b":"np.log1p(train['meter_reading']).hist(figsize=(6, 5))","a1023674":"weather_train.head(3)\n# print(weather_train.dtypes)","161fa1ef":"weather_train[\"air_temperature\"].hist(figsize=(6, 4))","d04cf879":"weather_train[\"cloud_coverage\"].hist(figsize=(6, 4))","1ceee998":"weather_train[\"dew_temperature\"].hist(figsize=(6, 4))","d661b9be":"weather_train[\"sea_level_pressure\"].hist(figsize=(6, 4))","0014764f":"weather_train[\"wind_speed\"].hist(figsize=(6, 4))","026efa78":"building_metadata.head(3)\nprint(building_metadata.dtypes)","64669db0":"building_metadata[\"square_feet\"].hist(figsize=(6, 4))","f0e21019":"building_metadata[\"square_feet\"] = building_metadata[\"square_feet\"].apply(np.log1p)\nbuilding_metadata[\"square_feet\"].hist(figsize=(6, 4))","1a4faadd":"building_metadata[\"primary_use\"].value_counts()","c2f782e8":"def check_missing(df, ascending=False):\n    total = df.isnull().sum().sort_values(ascending = ascending)\n    percent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = ascending)\n    missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    # Only want to check columns with null values\n    missing_data = missing_data[missing_data['Total']!=0]\n    return missing_data","16e70a35":"check_missing(train).head(len(train))","8f1eb6a6":"check_missing(weather_train).head(len(weather_train))","d3eb562f":"weather_train.drop(['cloud_coverage', 'precip_depth_1_hr'], axis=1, inplace=True)\nweather_train.replace('NaN', np.nan, inplace=True)\nfor col in [\"air_temperature\", \"dew_temperature\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]:\n    weather_train[col].fillna(weather_train[col].mean(), inplace=True)","c1b28246":"check_missing(building_metadata).head(len(building_metadata))","49624d7f":"building_metadata.drop(['floor_count', 'year_built'], axis=1, inplace=True)","155fedc1":"def merging(df, weather):\n    df = df.merge(building_metadata, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n    df = df.merge(weather, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\n    return df","4537198d":"train = merging(train, weather_train)\ndel weather_train\ngc.collect()","7df577b4":"check_missing(train).head(len(train))","6557715b":"categorical_features = []\nprint(train.columns)\nfor col in train.columns:\n    if train[col].dtype == \"object\":\n        categorical_features.append(col)\nprint(categorical_features)","12808d75":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\nprint(f'columns with unique value in train{one_value_cols}')","59766217":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"day\"] = train[\"timestamp\"].dt.day\ntrain[\"weekend\"] = train[\"timestamp\"].dt.weekday\ntrain[\"month\"] = train[\"timestamp\"].dt.month","a92d7e54":"# # train = pd.get_dummies(train, columns=categorical_features)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"primary_use\"])\ntrain[\"primary_use\"] = le.transform(train[\"primary_use\"])","00a4b2e3":"drop_columns = [\"timestamp\", \"meter_reading\"]\ntarget = np.log1p(train[\"meter_reading\"])\ntrain.drop(drop_columns, axis=1, inplace=True)","c3de7dfa":"from sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","6f605c92":"x_train, x_test , y_train, y_test = train_test_split(train, target , test_size= 0.2, random_state=1)","551718c2":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'rmse'},\n    'learning_rate': 0.3,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8\n}\n","07dd9695":"lgb_train = lgb.Dataset(x_train, y_train)\nlgb_test = lgb.Dataset(x_test, y_test)\ndel x_train, x_test , y_train, y_test","360bb5b3":"gbm = lgb.train(params, lgb_train, num_boost_round=2000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=20, verbose_eval = 20)","4f530c14":"del lgb_train, lgb_test, train, target\ngc.collect()","419e7cde":"weather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\")\ntest = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\")\nprint('Size of weather_test_df data', weather_test.shape)\nprint('Size of test_df data', test.shape)","973069bc":"row_id = test['row_id']\ntest.drop(['row_id'], axis=1, inplace=True)","7ae6bb8d":"for df in [test, weather_test]:\n    df = reduce_mem_usage(df)\n\nweather_test.drop(['cloud_coverage', 'precip_depth_1_hr'], axis=1, inplace=True)\nweather_test.replace('NaN', np.nan, inplace=True)\nfor col in [\"air_temperature\", \"dew_temperature\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]:\n    weather_test[col].fillna(weather_test[col].mean(), inplace=True)\n\ntest = merging(test, weather_test)   \ndel weather_test, building_metadata\ngc.collect()\n","9e108946":"test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"hour\"] = test[\"timestamp\"].dt.hour\ntest[\"day\"] = test[\"timestamp\"].dt.day\ntest[\"weekend\"] = test[\"timestamp\"].dt.weekday\ntest[\"month\"] = test[\"timestamp\"].dt.month\ntest.drop([\"timestamp\"], axis=1, inplace=True)\n\ntest[\"primary_use\"] = le.transform(test[\"primary_use\"])\n\ndel le\ngc.collect()","87f9ace3":"pred = []\nstep = 50000\nfor i in range(0, len(test), step):\n    pred.extend(np.expm1(gbm.predict(test.iloc[i: min(i+step, len(test)), :], num_iteration=gbm.best_iteration)))","0451c140":"submission = pd.DataFrame({'row_id':row_id, 'meter_reading': pred})\nsubmission['meter_reading'].describe()","75f245b1":"submission['meter_reading'] = submission['meter_reading'].apply(lambda x: 0 if x<0 else x)\nsubmission.to_csv(\"submission.csv\", index = False)","fbda5e18":"submission['meter_reading'].describe()","4b61f891":"# Test data conversion","2fb73c31":"Both of them have high missing values . So we'll drop them both for now.","9d0754df":"# Train with LightGBM","d4b37871":"# Data description\n## Files\n### train.csv\n* building_id - Foreign key for the building metadata.\n* meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.\n* timestamp - When the measurement was taken\n* meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.","88587163":"Check that this train data which initially didn't have any null value, now has some . It is because, weather_train doesn't have value for all (site_id, timestamp) pair in train data.","12e259e6":"# Input data","d31026d0":"# Feature engineering\nTimestamp related featuring. All timestamp are in year 2016.","2246eada":"So no column with unique value","aa3c99a7":"# Fixing missing values","8b66b60b":"So there is no negative value in train and energy consumption can't be negative , but the model seems to predict them. Let's fix them and then write to csv. ","e00fda19":"# Reducing memory size\nThes section is borrowed from the greate kernel https:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction. Reducing size is important as because of the very last train (20 million +) and test ( 40 million + ) the kernet RAM is being overflowed and restarting frequently. \n","3b53eb5c":"# Input test data","364893cb":"# Making prediction\nAs the RAM overflowing issue persists with predicting whole testset at once , predicting step by step resolved that problem.","7673e0f5":"### building_metadata.csv\n* site_id - Foreign key for the weather files.\n* building_id - Foreign key for training.csv\n* primary_use - Indicator of the primary category of activities for the building based on EnergyStar property type definitions\n* square_feet - Gross floor area of the building\n* year_built - Year building was opened\n* floor_count - Number of floors of the building","07aef4e8":"So the number of columns isn't big for each dataset , but the number of rows are huge. For train its 20 million+ and for test it is 41 million+. If we don't reduce the memory consumption, given kaggle's RAM allocation limit , the kernel will run out of memory limit and reboot frequently.","05c68177":"So except cloud_coverage and precip_depth_1_hr others have less than 30% missing values. We'll fill them by their mean values. ","34b6904d":"# Merging files","20a910c3":"Very skewed and also have some outliers. It is less skewed after taking log. So it is better to train with log value. ","a8c44b01":"# Introduction to competition \nQ: How much does it cost to cool a skyscraper in the summer?\nA: A lot! And not just in dollars, but in environmental impact.\n\nThankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That\u2019s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don\u2019t work with different building types.\n\nIn this competition, you\u2019ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.","ef907806":"## weather_[train\/test].csv\nWeather data from a meteorological station as close as possible to the site.\n* site_id\n* air_temperature - Degrees Celsius\n* cloud_coverage - Portion of the sky covered in clouds, in oktas\n* dew_temperature - Degrees Celsius\n* precip_depth_1_hr - Millimeters\n* sea_level_pressure - Millibar\/hectopascals\n* wind_direction - Compass direction (0-360)\n* wind_speed - Meters per second","8f311ec2":"Though this only finds 2 categorical values , actually there are more. For example from data description, we know \n* The meter id code is int and has 4 unique values from 0 to 3. \n* year_built has 116 unique values except NAN. meter has only . \n*  site_id has values from 0 to 15. \n* building_id is an int value, but it has 1449 unique items.\nHowever to keep the data small, not trying one hot encoding right now.","fc6a6ab6":"## any column with unique value ?"}}