{"cell_type":{"ba9aa3a2":"code","8b748bb5":"code","5ef0db9d":"code","8409b65a":"code","d6728fa4":"code","3879f7b7":"code","406952be":"code","8137978f":"code","4fa45949":"code","1faf8ed2":"code","0ac5d7ca":"code","cb24fb0b":"code","b7eac9c5":"code","4c99818e":"code","9726ca75":"code","a2635080":"code","4c2ede48":"code","5efc0ea1":"code","bf6c64a8":"code","d3a7c59e":"code","7eb8ad83":"code","85d912e9":"code","6fa1371b":"code","16a018f1":"code","95e0cdd4":"code","e5700fe2":"code","e93fe745":"code","26f8a0bc":"code","adac9cc2":"code","dddc1213":"code","9f8f3b15":"code","e26ce2ec":"code","4ea26131":"code","4895f76c":"code","4f3cc05f":"code","6fbd0136":"code","60648a52":"code","a1edc79c":"code","317a6d39":"code","61157804":"code","6c5c236b":"code","06dae43c":"code","22d30878":"code","d37af14d":"code","eaa0d029":"code","992a96d1":"code","2f8aa43e":"code","1e7734e4":"code","42ec0351":"code","ad30dd8d":"code","76e70f56":"code","fbf57364":"code","c5490347":"code","ecc07dc7":"code","3be81065":"code","9c157eb8":"code","7bfe6ef1":"code","af5817b2":"code","c131a415":"code","f9b5e3b6":"code","7ea4eddb":"code","c9b2029d":"code","83518c75":"code","6c380552":"code","bf43fb3f":"code","76cd98b7":"code","78d50930":"code","0b7ea0a5":"code","ea0a0817":"code","16d939b1":"code","048db0e7":"code","5afb4614":"code","70e5ef8c":"code","b4523086":"code","5c353222":"code","1791c515":"code","c1aa1b67":"code","60f51c50":"code","6da33da4":"markdown","fd03e426":"markdown","6aa29289":"markdown","2ebe040f":"markdown","7707d096":"markdown","bb034833":"markdown","7aae7146":"markdown","7f495e8b":"markdown","f3d59133":"markdown","a2e77ecd":"markdown","8a50aba1":"markdown","89702f4c":"markdown","3e8a253b":"markdown","2f2a3da9":"markdown","4110c1ca":"markdown","a7d5aeac":"markdown","584cc0c5":"markdown"},"source":{"ba9aa3a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b748bb5":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5ef0db9d":"df=pd.read_csv(r\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","8409b65a":"df.head()","d6728fa4":"df.shape","3879f7b7":"df['Amount'].mean()","406952be":"df.isnull().sum().max()","8137978f":"sns.heatmap(df.isnull(), cbar=False)","4fa45949":"sns.countplot('Class',data=df)","1faf8ed2":"\nprint('No Fraud transaction in ', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Fraud transaction in ', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","0ac5d7ca":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nplt.show()","cb24fb0b":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=20)\nax[0].set_xlim([ min(amount_val),max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","b7eac9c5":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","4c99818e":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","9726ca75":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n#original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    print(len(test_index),\"---\")\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]","a2635080":"train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)","4c2ede48":"test_counts_label","5efc0ea1":"# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","bf6c64a8":"print(len(original_Xtrain))","d3a7c59e":"\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)","7eb8ad83":"print('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","85d912e9":"\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","6fa1371b":"print(new_df['Class'].value_counts()\/len(new_df))","16a018f1":"sns.countplot('Class', data=new_df, palette='rainbow')\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","95e0cdd4":"# Make sure we use the subsample in our correlation\n\nf, ax = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax[0])\nax[0].set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\n# Entire DataFrame\ncorr = new_df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax[1])\nax[1].set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\nplt.show()\n","e5700fe2":"from scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","e93fe745":"new_df.shape","26f8a0bc":"#plotting the outliers\n\n\nf,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n\nplt.show()\n","adac9cc2":"\n\n#  -----> V14 Removing Outliers (Highest Negative Correlated)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\n\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 outliers:{}'.format(outliers))  \n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 22)","dddc1213":"# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 22)","9f8f3b15":"# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\n\nprint('----' * 22)","e26ce2ec":"new_df.shape","4ea26131":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),arrowprops=dict(facecolor='black'),fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),arrowprops=dict(facecolor='black'),fontsize=14)\n\nplt.show()","4895f76c":"# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","4f3cc05f":"\n\n\nimport time\n\n# T-SNE Implementation\n\nfrom sklearn.manifold import TSNE\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nfrom sklearn.decomposition import PCA, TruncatedSVD\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","6fbd0136":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nimport matplotlib.patches as mpatches\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label=' No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y ==1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\n\n\nplt.show()","60648a52":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","a1edc79c":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","317a6d39":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","61157804":"len(X_train)","6c5c236b":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections","06dae43c":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","22d30878":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","d37af14d":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","eaa0d029":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","992a96d1":"from sklearn.model_selection import KFold, StratifiedKFold\n\n# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\nsss=StratifiedKFold(n_splits=5)\nsss.get_n_splits(undersample_X, undersample_y)\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    \n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n","2f8aa43e":"len(undersample_Xtrain)","1e7734e4":"undersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []","42ec0351":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ad30dd8d":"# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss(sampling_strategy='majority').fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))","76e70f56":"len(X_nearmiss)","fbf57364":"log_reg","c5490347":"\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg)\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))\n    \n    ","ecc07dc7":"undersample_accuracy","3be81065":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","9c157eb8":"len(y_train)","7bfe6ef1":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","af5817b2":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)","c131a415":"def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","f9b5e3b6":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","7ea4eddb":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","c9b2029d":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 22)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 22)\n\n# How it should look like\nprint('---' * 22)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 22)","83518c75":"undersample_y_score = log_reg.decision_function(original_Xtest)#Predict confidence scores for samples.","6c380552":"undersample_y_score","bf43fb3f":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(undersample_average_precision))","76cd98b7":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)\n","78d50930":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))","0b7ea0a5":"# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []","ea0a0817":"log_reg_sm = LogisticRegression()\n\n\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}","16d939b1":"# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 22)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 22)","048db0e7":"y_score = best_est.decision_function(original_Xtest)","5afb4614":"average_precision = average_precision_score(original_ytest, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format( average_precision))","70e5ef8c":"fig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=16)","b4523086":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)","5c353222":"undersample_score","1791c515":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(original_Xtest)\nundersample_score = accuracy_score(original_ytest, y_pred)\n\n\n\n# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n\nd = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n# Note how high is accuracy score it can be misleading! \nfinal_df\n","c1aa1b67":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(original_ytest, y_pred)\n","60f51c50":"confusion_matrix(original_ytest, y_pred_sm)","6da33da4":"# We will undersample during cross validating","fd03e426":"# Removing outliers from fraud transactions","6aa29289":"The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.","2ebe040f":"# SMOTE(Over Sampling)\n<img src=\"https:\/\/raw.githubusercontent.com\/rikunert\/SMOTE_visualisation\/master\/SMOTE_R_visualisation_3.png\", width=800>\n\n\nSMOTE stands for Synthetic Minority Over-sampling Technique. Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\".\n\n\n### Understanding SMOTE:\n\n* **Solving the Class Imbalance:** SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class.\n* **Location of the synthetic points:** SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points.\n* **Final Effect:** More information is retained since we didn't have to delete any rows unlike in random undersampling.\n* **Accuracy || Time Tradeoff:** Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.","7707d096":"> NearMiss(sampling_strategy='majority') means that it resamples the most count 0 or 1 which we say majority.In our case 0 is majority because most of the records output is 0.we resamples it and reduces it equal to minority(1)","bb034833":"* Negative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these    values are, the more likely the end result will be a fraud transaction.\n* Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.","7aae7146":"## Cross Validation Overfitting Mistake:\u00b6\n\n\n## Overfitting during Cross Validation:\nIn our undersample analysis I want to show you a common mistake I made that I want to share with all of you. It is simple, if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the validation set before implementing cross-validation causing a \"data leakage\" problem. In the following section you will see amazing precision and recall scores but in reality our data is overfitting!\n\nThe wrong","7f495e8b":"# Anomaly Detection:\n\n\nOur main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.\n\n\n### Interquartile Range Method:\n* ***Interquartile Range (IQR):***\nWe calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.\n\n\n* ***Boxplots:*** \nBesides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme).\n\n\n* ***Outlier Removal Tradeoff:***\nWe have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.\n\n\n* ***The Tradeoff:*** \nThe lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models","f3d59133":"# NearMiss Algorithm \u2013 Undersampling\nNearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes.","a2e77ecd":">  Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.","8a50aba1":"## Learning Curves:\n* The wider the gap between the training score and the cross validation score, the more likely your model is overfitting (high variance).\n* If the score is low in both training and cross-validation sets this is an indication that our model is underfitting (high bias)\n","89702f4c":"# Dimensionality Reduction and Clustering:\n\n## Understanding t-SNE:\n\n* <a href=\"https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM&t=96s\"> Watch the video about t-SNE by StackQuest<\/a>\n    \n*  t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset","3e8a253b":"# Test Data with Logistic Regression:\n\n## Confusion Matrix:\n* **Positive\/Negative:** Type of Class (label) [\"No\", \"Yes\"] True\/False: Correctly or Incorrectly classified by the model.\n\n\n* **True Negatives (Top-Left Square):** This is the number of correctly classifications of the \"No\" (No Fraud Detected) class.\n\n\n* **False Negatives (Top-Right Square):** This is the number of incorrectly classifications of the \"No\"(No Fraud Detected) class.\n\n\n* **False Positives (Bottom-Left Square):** This is the number of incorrectly classifications of the \"Yes\" (Fraud Detected) class\n\n\n* **True Positives (Bottom-Right Square):** This is the number of correctly classifications of the \"Yes\" (Fraud Detected) class.","2f2a3da9":"## Correlation Matrices","4110c1ca":" **Note**: Notice how ***IMBALANCED*** is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!","a7d5aeac":"**Conclusion**:\n We finally tested our model with the original trainig data,we can see that Over sampling method has gave us very good accuracy","584cc0c5":"### Random Under-Sampling:\n"}}