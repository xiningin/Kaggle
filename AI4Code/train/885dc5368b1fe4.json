{"cell_type":{"a350189b":"code","6c5ee029":"code","af7c1650":"code","93d32f0f":"code","d6cc36fd":"code","5731c547":"code","df199aef":"code","93e6e729":"code","174668e0":"code","2fa1f70a":"code","2c0cd08f":"code","ed44cd5a":"code","3163a7dc":"code","b66a7f0a":"code","f72322bd":"code","ba9ff413":"code","4afebb00":"code","2ce570d1":"code","fb5922f1":"code","2dd9f786":"code","154ab4bd":"code","a028d3db":"code","74cf32c3":"code","5c5bbc54":"code","12abac7b":"code","7b68bdcf":"code","059eab65":"code","ea28baee":"code","ca8cc503":"code","1cde5cfc":"code","bb937459":"code","6981c63c":"code","623e104d":"code","2b4723e5":"code","e200a204":"code","cc0ee034":"code","5025ae7a":"code","9b61f421":"code","e5bbd8fc":"code","8c0437c3":"code","0e2072fa":"code","ff6148a4":"code","0a6f5356":"code","f4422f4b":"code","e143ccaa":"code","79846dd0":"code","a141b031":"code","67813b79":"code","5ab1c2f2":"code","3caa7b2a":"code","5211ff87":"code","af0d3672":"code","fb027338":"code","b6ec5fa9":"code","1db80279":"code","55ce211c":"code","d661117c":"code","bfc6cd71":"code","9498e038":"code","7b3c18b5":"code","7cd6cdbe":"code","5fcdcffe":"markdown","75ea893e":"markdown","0e419c58":"markdown","e436ec4e":"markdown","edb3ba40":"markdown","f986de03":"markdown","e1cb2d5e":"markdown","02dabd3d":"markdown","50f27f5e":"markdown","ff1af4fa":"markdown","4af167c2":"markdown","a2c54233":"markdown","6d104aee":"markdown","91a50d5d":"markdown","ef3ab0b3":"markdown","5d6a7a3c":"markdown","26774638":"markdown","478d788e":"markdown","2c52d63e":"markdown","36bedeeb":"markdown","666e0d22":"markdown","280e9c51":"markdown","deed82c6":"markdown","cd58cb39":"markdown","afa31e2e":"markdown","62ace687":"markdown","01d27807":"markdown","09a2da84":"markdown","901eae3b":"markdown","6903f7be":"markdown","ad583fd4":"markdown","2223343b":"markdown","e0f80ffb":"markdown","25c9065c":"markdown","d6d42dc1":"markdown","563fb74f":"markdown","f20abca5":"markdown","c3336b4c":"markdown","a12811fc":"markdown","db9f58ed":"markdown","266075e6":"markdown","4f39ec91":"markdown","49539f4e":"markdown","50497dfe":"markdown","f6012ffb":"markdown","4dde6d2e":"markdown","5c258e90":"markdown","afeacc91":"markdown","48e7ef37":"markdown","47b201e8":"markdown","14442606":"markdown","4ecf782e":"markdown","0f6cd2d1":"markdown"},"source":{"a350189b":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/competiongoal\/archive\/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","6c5ee029":"from pathlib import Path\nimport pandas as pd\nimport glob\nimport numpy as np\nimport pyarrow.parquet\nimport pyarrow as pa\nimport seaborn as sns\n#import tqdm\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')","af7c1650":"sample_submission = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\nsample_submission","93d32f0f":"train_labels =pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\ntrain_labels","d6cc36fd":"!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","5731c547":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","df199aef":"dicom = pydicom.read_file('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/FLAIR\/Image-1.dcm')","93e6e729":"data = dicom.pixel_array\nplt.imshow(data)","174668e0":"data.shape","2fa1f70a":"list = glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/FLAIR\/*')\nlist = sorted(list)","2c0cd08f":"fig, ax = plt.subplots(4, 6, figsize=(20, 20))\n\nfor i in range(24):\n    dicom = pydicom.read_file(list[i])\n    data = dicom.pixel_array\n    plt.subplot(4,6,i+1)\n    plt.imshow(data)","ed44cd5a":"!ls ..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train","3163a7dc":"len(glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/FLAIR\/*'))","b66a7f0a":"len(glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/T1w\/*'))","f72322bd":"len(glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/T1wCE\/*'))","ba9ff413":"len(glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/T2w\/*'))","4afebb00":"train = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\ntrain","2ce570d1":"train['FLAIR'] = 0\ntrain['T1w']=0\ntrain['T1wCE']=0\ntrain['T2w']=0","fb5922f1":"train","2dd9f786":"train.info()","154ab4bd":"#train['BraTS21ID'] = train['BraTS21ID'].astype(str)\n#train['BraTS21ID'] =train['BraTS21ID'].str.zfill(5)\n#train","a028d3db":"train[\"BraTS21ID\"] = train[\"BraTS21ID\"].apply(lambda x: str(x).zfill(5))\ntrain","74cf32c3":"file ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/FLAIR\/*'.format(i)\nlen(glob.glob(file))","5c5bbc54":"a=0\nfor i in train['BraTS21ID']:\n    file ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/FLAIR\/*'.format(i)\n    train['FLAIR'][a] = len(glob.glob(file))\n    file ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1w\/*'.format(i)\n    train['T1w'][a] = len(glob.glob(file))\n    file ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1wCE\/*'.format(i)\n    train['T1wCE'][a] = len(glob.glob(file))\n    file ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T2w\/*'.format(i)\n    train['T2w'][a] = len(glob.glob(file))\n    a+=1\n","12abac7b":"train","7b68bdcf":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/FLAIR\/*'.format('01005')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","059eab65":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/FLAIR\/*'.format('01009')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","ea28baee":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1w\/*'.format('01005')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","ca8cc503":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1w\/*'.format('01009')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","1cde5cfc":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1wCE\/*'.format('01005')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","bb937459":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T1wCE\/*'.format('01009')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","6981c63c":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T2w\/*'.format('01005')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","623e104d":"fig, ax = plt.subplots(5,5, figsize=(20, 20))\n\nfile ='..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{}\/T2w\/*'.format('01009')\nfile_path = sorted(glob.glob(file))\n\nfor i in range(23):\n\n    dicom = pydicom.read_file(file_path[i])\n    data = dicom.pixel_array\n    plt.subplot(5,5,i+1)\n    plt.title(i)\n    plt.axis(\"off\")\n    plt.imshow(data)","2b4723e5":"#!pip install ..\/input\/timminstall\/Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n#!pip install ..\/input\/timminstall\/numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n!pip install ..\/input\/timminstall\/timm\/timm-0.4.12-py3-none-any.whl\n#!pip install ..\/input\/timminstall\/torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl\n#!pip install ..\/input\/timminstall\/torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl\n#!pip install ..\/input\/timminstall\/typing_extensions-3.10.0.0-py3-none-any.whl","e200a204":"import glob\nimport os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\nimport cv2\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","cc0ee034":"#timm.list_models()","5025ae7a":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            ToTensorV2(),\n        ])","9b61f421":"train_data =pd.DataFrame()\nlist = sorted(glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/*\/FLAIR\/*'))\ntrain_data['path'] =list\ntrain_data['BraTS21ID'] = train_data['path'].str.split('\/').str[4]\ntrain_data","e5bbd8fc":"train_data = pd.merge(train_data,train,on='BraTS21ID')\ntrain_data","8c0437c3":"# ====================================================\n# Dataset\n# ====================================================\nclass TrainDataset(Dataset):\n    def __init__(self,df=train_data): #,transform=None):\n        self.df = df\n        self.file_paths = df['path']\n        self.labels = df['MGMT_value']\n        #self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n        \n    def __getitem__(self, idx):\n\n        dicom = pydicom.read_file(self.file_paths.iloc[idx])\n        image = dicom.pixel_array\n        image = cv2.resize(image,(512,512))\n        image = image[np.newaxis,:,:]\n        image = torch.from_numpy(image).int()\n        \n        #print(image_all,' ',type(image_all))\n        label = torch.tensor(self.labels.iloc[idx]).int()\n        #print(label)\n        \n        return image, label","0e2072fa":"        dicom = pydicom.read_file(train_data.path[0])\n        image = dicom.pixel_array\n        image = cv2.resize(image,(512,512))\n        image = image[np.newaxis,:,:]\n        image = torch.from_numpy(image).int()\n        \n        #print(image_all,' ',type(image_all))\n        label = torch.tensor(train_data.loc[0,'MGMT_value']).int()\n        #print(label)","ff6148a4":"label","0a6f5356":"image","f4422f4b":"class CFG:\n    apex=False\n    debug=False\n    print_freq=100\n    num_workers=4\n    model_name='tf_efficientnet_b7_ns'\n    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    epochs=3\n    T_max=3 # CosineAnnealingLR\n    lr=1e-4\n    min_lr=1e-6\n    batch_size=1\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=42\n    target_size=1\n    target_col='MGMT_value'\n    n_fold=5\n    trn_fold=[0]","e143ccaa":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = roc_auc_score(y_true, y_pred)\n    return score\n\n\ndef init_logger(log_file='train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()","79846dd0":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","a141b031":"#TrainDataset.__getitem__(train,0)","67813b79":"# ====================================================\n# MODEL\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.model = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=1) #\u5165\u529b\u30c1\u30e3\u30cd\u30eb1\u306b\u3057\u3066\u3044\u308b\u3002 \n        self.n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(self.n_features, self.cfg.target_size)\n\n    def forward(self, x):#torch.Size([64, 1, 69, 193])\n        output = self.model(x)\n        return output\n        ","5ab1c2f2":"#timm.create_model('tf_efficientnet_b7_ns', pretrained=False ,in_chans=1) ","3caa7b2a":"def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    if CFG.apex:\n        scaler = GradScaler()\n    batch_time = AverageMeter() #class AverageMeter(object):\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    # switch to train mode\n    model.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        batch_size = labels.size(0)\n        if CFG.apex:\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds.view(-1), labels)\n        else:\n            print(images.size())\n            y_preds = model(images) \n            loss = criterion(y_preds.view(-1), labels)\n        # record loss\n        losses.update(loss.item(), batch_size)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        if CFG.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            if CFG.apex:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  #'LR: {lr:.6f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(train_loader)),\n                   grad_norm=grad_norm,\n                   #lr=scheduler.get_lr()[0],\n                   ))\n            \n    return losses.avg","5211ff87":"def valid_fn(valid_loader, model, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to evaluation mode\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds.view(-1), labels)\n        losses.update(loss.item(), batch_size)\n        # record accuracy\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}\/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                   step, len(valid_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                   ))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","af0d3672":"# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    folds = train_data\n    train['no'] =train['BraTS21ID'].astype(int)\t\n    trn_idx = folds[folds.no < 500].index\n    val_idx = folds[folds.no <= 500].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds[CFG.target_col].values\n\n    train_dataset = TrainDataset(train_folds) #, transform=get_transforms(data='train'))\n    valid_dataset = TrainDataset(valid_folds) #, transform=get_transforms(data='train'))\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size, \n                              shuffle=True, \n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size * 2, \n                              shuffle=False, \n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, pretrained=True)\n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss()\n\n    best_score = 0.\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n\n        # scoring\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n    \n    valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth', \n                                      map_location=torch.device('cpu'))['preds']\n\n    return valid_folds","fb027338":"#!mkdir \/root\/.cache\n!mkdir \/root\/.cache\/torch\n!mkdir \/root\/.cache\/torch\/hub\n!mkdir \/root\/.cache\/torch\/hub\/checkpoints\n!cp ..\/input\/timminstall\/tf_efficientnet_b7_ns-1dbc32de.pth\/tf_efficientnet_b7_ns-1dbc32de.pth \/root\/.cache\/torch\/hub\/checkpoints\/","b6ec5fa9":"train_folds","1db80279":"    train_data['no'] =train['BraTS21ID'].astype(int)\t\n    folds = train_data.copy()\n    trn_idx = folds[folds.no < 500].index\n    val_idx = folds[folds.no >= 500].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds[CFG.target_col].values\n\n    train_dataset = TrainDataset(train_folds) #, transform=get_transforms(data='train'))\n    valid_dataset = TrainDataset(valid_folds) #, transform=get_transforms(data='train'))\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=2, #CFG.batch_size, \n                              shuffle=True, \n                              num_workers=1) #, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size * 2, \n                              shuffle=False, \n                              num_workers=CFG.num_workers) #, pin_memory=True, drop_last=False)\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, pretrained=True)\n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss()\n\n    best_score = 0.\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n\n        # scoring\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n    \n    valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth', \n                                      map_location=torch.device('cpu'))['preds']\n","55ce211c":"train_folds","d661117c":"valid_folds ","bfc6cd71":"    for step, (images, labels) in enumerate(train_loader):\n        images","9498e038":"train_dataset.__getitem__(0)","7b3c18b5":"# ====================================================\n# main\n# ====================================================\ndef main():\n\n    \"\"\"\n    Prepare: 1.train \n    \"\"\"\n\n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df[CFG.target_col].values\n        score = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}')\n    \n    if 1==1:\n        # train \n        oof_df = pd.DataFrame()\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(train, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        # CV result\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)","7cd6cdbe":"if __name__ == '__main__':\n    main()","5fcdcffe":"<pre>\ud83c\udf3c\u306a\u305c\u304b\u306f\u308f\u304b\u308a\u307e\u305b\u3093\u304c\u3001\u6570\u306b\u3070\u3089\u3064\u304d\u304c\u3042\u308a\u307e\u3059\u3002580\u884c\u76ee,583\u884c\u76ee\u306f4\u9805\u76ee\u3068\u3082\u540c\u3058\u6570\u3067\u3059\u3002\n\n\u3053\u308c\u3092\u307f\u308b\u3068\u306a\u306b\u304b\u308f\u304b\u308b\u304b\u3082\u3002580\u884c\u76ee\u306e\u60a3\u8005\u3055\u3093\u306fMGMT_value=1\u3060\u304b\u3089\u764c\u306e\u53ef\u80fd\u6027\u304c\u3042\u308b\u65b9\u3067\u3059\u306d\u3002583\u884c\u76ee\u306f\u764c\u306e\u53ef\u80fd\u6027\u306f\u306a\u3044\u65b9\u3067\u3059\u306d\u3002\n\n\u60a3\u8005\u3055\u309301005\uff08\u764c\u53ef\u80fd\u6027\u3042\u308a\uff09\u306801009\uff08\u764c\u53ef\u80fd\u6027\u306a\u3057\uff09\u3092\u8abf\u3079\u3066\u307f\u307e\u3059\u3002\uff12\uff13\u679a\u3002","75ea893e":"### \u307e\u305a\u306f\u3001FLAIR\u3092\u8abf\u3079\u3066\u307f\u308b","0e419c58":"\u3069\u3046\u3082\u5358\u7d14\u306b\u3088\u3093\u3067\u3082\u30c0\u30e1\u307f\u305f\u3044\u3067\u3059\u3002\uff11\uff12\u679a\u30bb\u30c3\u30c8\u306b\u306a\u3063\u3066\u3044\u308b\u6c17\u3082\u3057\u306a\u3044\u3067\u3082\u306a\u3044\u3002\u9014\u4e2d\u306b\u30d6\u30e9\u30f3\u30af\u306e\u753b\u50cf\u304c\u3042\u308b\u306e\u3067\u3002","e436ec4e":"MRI\u306e\u77e5\u8b58\u304c\u306a\u3044\u306e\u3067\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002","edb3ba40":"### \u533b\u7642\u7cfb\u3067\u3059\u3002DCM\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u306e\u3067\u753b\u50cf\u8b58\u5225\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\n### \u307e\u305a\u306f\u3001EfficinetNe\u3092\u4f7f\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n### GPU\u3059\u3054\u3044\u5fc5\u8981\u306b\u306a\u308b\u3057\u3001\u307e\u3060\u3088\u304f\u308f\u304b\u3089\u3044\u3068\u3053\u308d\u304c\u591a\u3044\u306e\u3067\u8f9b\u3044\u3093\u3067\u3059\u304c\u3002\n### \u611a\u75f4\u3089\u306a\u3044\u3067\u3001\u30c7\u30fc\u30bf\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002","f986de03":"\u60a3\u8005\u3055\u3093\u6bce\u306e\u753b\u50cf\u679a\u6570\u3092train\u3078\u5165\u308c\u3066\u3044\u304d\u307e\u3059\u3002\u3068\u308a\u3042\u3048\u305a\u3001\u67a0\u3092\u4f5c\u308a\u307e\u3059\u3002","e1cb2d5e":"### MGMT=0","02dabd3d":"\u6570\u3092\u6570\u3048\u3066\u3044\u308c\u3066\u3001\u9805\u76ee\u306b\u5165\u308c\u3066\u3044\u304d\u307e\u3059\u3002\u9177\u3044\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u3059\u304c\u3001\u3084\u308a\u65b9\u3053\u308c\u3057\u304b\u73fe\u5728\u3057\u3089\u306a\u3044\u306e\u3067\u3002\u3002","50f27f5e":"### \u3053\u3046\u3044\u3046\u3084\u308a\u65b9\u304c\u6b63\u3057\u3044\u304b\u308f\u304b\u308a\u307e\u305b\u3093\u3002\u305f\u3060\u3001\u79c1\u306e\u3088\u3046\u306a\u521d\u5fc3\u8005\u306f\u3053\u3093\u306a\u611f\u3058\u3067\u30b3\u30f3\u30da\u3092\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n### \u4ed6\u306e\u4eba\u306eEDA\u3092\u898b\u3066\u3082\u3044\u3044\u306e\u3067\u3059\u304c\u3001\u898b\u308b\u306e\u3068\u5b9f\u969b\u306b\u3084\u308b\u306e\u3068\u3067\u306f\u3001\u305d\u306e\u3042\u3068\u306e\u81ea\u5206\u306e\u6210\u9577\u304c\u5927\u304d\u304f\u7570\u306a\u3063\u3066\u304f\u308b\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n### \u306a\u306e\u3067\u3001\u52b9\u7387\u304c\u60aa\u304f\u3066\u3082\u81ea\u5df1\u6d41\u3067\u3042\u308b\u7a0b\u5ea6\u306f\u3084\u308b\u3093\u3067\u3059\u3002giveup\u3057\u3066\u304b\u3089\u3001\u4ed6\u306e\u4eba\u306e\u30b3\u30fc\u30c9\u3092\u307f\u308b\u3068\u611f\u52d5\u3057\u307e\u3059\u3002","ff1af4fa":"\u5148\u307b\u3069\u3088\u3093\u3060train_labels.csv\u3092train\u3078\u5165\u308c\u307e\u3059\u3002","4af167c2":"\u3053\u308c\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3068\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u30a2\u30af\u30bb\u30b9\u306a\u3057\u3067\u3067\u304d\u307e\u3059\u3002","a2c54233":"\ud83d\udc90\u9055\u3044\u304c\u308f\u304b\u308a\u307e\u305b\u3093\u3002\u3068\u3044\u3046\u304b\u3069\u3053\u3092\u898b\u3066\u5224\u65ad\u3059\u308c\u3070\u3044\u3044\u306e\u304b\u3002\n\n\u307e\u305f\u3001\u524d\u56de\u3068\u4e00\u7dd2\u3067\u533b\u8005\u3058\u3083\u306a\u304d\u3083\u898b\u3066\u3082\u308f\u304b\u3089\u306a\u3044\u3088\u3002\u30e2\u30c7\u30eb\u3092\u7d44\u3093\u3067\u30c7\u30fc\u30bf\u3092\u5165\u308c\u3066\u3054\u3089\u3093\u3001\u3068\u30b0\u30e9\u30f3\u30c9\u30de\u30b9\u30bf\u30fc\u306b\u8a00\u308f\u308c\u305d\u3046\u3002<br>\nSIIM\u306e\u3068\u304d\u306f\u3001bbox\u304clabel\u3068\u3057\u3066\u3042\u308a\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306f\u306a\u306b\u3082\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u7121\u99c4\u3060\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u4ed6\u306e\u7a2e\u985e\u306e\u30d5\u30a1\u30a4\u30eb\u898b\u3066\u307f\u3088\u3046\u3002\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306f\u8f9b\u62b1\u5f37\u3055\u304c\u5fc5\u8981\u3089\u3057\u3044\u3057\u3002","6d104aee":"### MGMT=1","91a50d5d":"<pre> ### \u30d5\u30a1\u30a4\u30eb\u306e\u69cb\u6210\nTraining\/Validation\/Testing\n\u2502\n\u2514\u2500\u2500\u2500 00000\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 FLAIR\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T1w\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T1wCE\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T2w\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 .....\n\u2502   \n\u2514\u2500\u2500\u2500 00001\n\u2502   \u2502 ...\n\u2502   \n\u2502 ...   \n\u2502   \n\u2514\u2500\u2500\u2500 00002\n\u2502   \u2502 ...","ef3ab0b3":"# \u3053\u3053\u304b\u3089\u6311\u6226\u3057\u3066\u3044\u304d\u307e\u3059\u3002","5d6a7a3c":"### MGMT=0","26774638":"### \u7c21\u5358\u306b\u3088\u3081\u305d\u3046\u306a\u3082\u306e\u304b\u3089\u8aad\u3093\u3067\u3044\u304d\u307e\u3059","478d788e":"> \ud83e\uddd1\u200d\ud83d\udcbbtrain\u30c7\u30fc\u30bf\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3001\u691c\u8a3c\uff08\u516c\u958b\uff09\u3001\u304a\u3088\u3073\u30c6\u30b9\u30c8\uff08\u975e\u516c\u958b\uff09\u306e3\u3064\u306e\u30b3\u30db\u30fc\u30c8\u306b\u3088\u3063\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002 \u300c\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u300d\u30b3\u30db\u30fc\u30c8\u3068\u300c\u691c\u8a3c\u300d\u30b3\u30db\u30fc\u30c8\u306f\u53c2\u52a0\u8005\u306b\u63d0\u4f9b\u3055\u308c\u307e\u3059\u304c\u3001\u300c\u30c6\u30b9\u30c8\u300d\u30b3\u30db\u30fc\u30c8\u306f\u7af6\u6280\u4e2d\u304a\u3088\u3073\u7af6\u6280\u5f8c\u306f\u5e38\u306b\u975e\u8868\u793a\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n> \n> \u3053\u308c\u3089\u306e3\u3064\u306e\u30b3\u30db\u30fc\u30c8\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u308c\u305e\u308c\u306e\u72ec\u7acb\u3057\u305f\u30b1\u30fc\u30b9\u306b\u306f\u30015\u6841\u306e\u756a\u53f7\u3067\u8b58\u5225\u3055\u308c\u308b\u5c02\u7528\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u308c\u3089\u306e\u300c\u30b1\u30fc\u30b9\u300d\u30d5\u30a9\u30eb\u30c0\u306e\u305d\u308c\u305e\u308c\u306e\u4e2d\u306b\u30014\u3064\u306e\u30b5\u30d6\u30d5\u30a9\u30eb\u30c0\u304c\u3042\u308a\u3001\u305d\u308c\u305e\u308c\u304cDICOM\u5f62\u5f0f\u306e\u69cb\u9020\u7684\u30de\u30eb\u30c1\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30afMRI\uff08mpMRI\uff09\u30b9\u30ad\u30e3\u30f3\u306e\u305d\u308c\u305e\u308c\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002 \u542b\u307e\u308c\u3066\u3044\u308b\u6b63\u78ba\u306ampMRI\u30b9\u30ad\u30e3\u30f3\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n> \n> Fluid Attenuated Inversion Recovery (FLAIR)<br>\n> T1-weighted pre-contrast (T1w)<br>\n> T1-weighted post-contrast (T1Gd)<br>\n> T2-weighted (T2)","2c52d63e":"> \ud83d\udc68\u200d\ud83d\udd2c\u8133\u306e\u60aa\u6027\u816b\u760d\u306f\u751f\u547d\u3092\u8105\u304b\u3059\u72b6\u614b\u3067\u3059\u3002\u81a0\u82bd\u816b\u3068\u3057\u3066\u77e5\u3089\u308c\u308b\u3053\u306e\u8133\u816b\u760d\u306f\u3001\u6210\u4eba\u3067\u6700\u3082\u4e00\u822c\u7684\u306a\u8133\u816b\u760d\u3067\u3042\u308a\u3001\u4e88\u5f8c\u304c\u6700\u3082\u60aa\u304f\u3001\u751f\u5b58\u671f\u9593\u306e\u4e2d\u592e\u5024\u306f1\u5e74\u672a\u6e80\u3067\u3059\u3002 MGMT\u30d7\u30ed\u30e2\u30fc\u30bf\u30fc\u306e\u30e1\u30c1\u30eb\u5316\u3068\u3057\u3066\u77e5\u3089\u308c\u308b\u816b\u760d\u5185\u306e\u7279\u5b9a\u306e\u907a\u4f1d\u5b50\u914d\u5217\u306e\u5b58\u5728\u306f\u3001\u5316\u5b66\u7642\u6cd5\u306b\u5bfe\u3059\u308b\u53cd\u5fdc\u6027\u306e\u597d\u307e\u3057\u3044\u4e88\u5f8c\u56e0\u5b50\u304a\u3088\u3073\u5f37\u529b\u306a\u4e88\u6e2c\u56e0\u5b50\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n> \n> \n> \n> \u73fe\u5728\u3001\u764c\u306e\u907a\u4f1d\u5b50\u89e3\u6790\u306b\u306f\u3001\u7d44\u7e54\u30b5\u30f3\u30d7\u30eb\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306e\u624b\u8853\u304c\u5fc5\u8981\u3067\u3059\u3002\u305d\u306e\u5f8c\u3001\u816b\u760d\u306e\u907a\u4f1d\u7684\u7279\u5fb4\u3092\u6c7a\u5b9a\u3059\u308b\u306e\u306b\u6570\u9031\u9593\u304b\u304b\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u7d50\u679c\u3068\u9078\u629e\u3057\u305f\u521d\u671f\u6cbb\u7642\u306e\u7a2e\u985e\u306b\u3088\u3063\u3066\u306f\u3001\u305d\u306e\u5f8c\u306e\u624b\u8853\u304c\u5fc5\u8981\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u753b\u50cf\u8a3a\u65ad\uff08\u3059\u306a\u308f\u3061\u3001\u30e9\u30b8\u30aa\u30b2\u30ce\u30df\u30af\u30b9\uff09\u306e\u307f\u3067\u764c\u306e\u907a\u4f1d\u5b66\u3092\u4e88\u6e2c\u3059\u308b\u6b63\u78ba\u306a\u65b9\u6cd5\u3092\u958b\u767a\u3067\u304d\u308c\u3070\u3001\u624b\u8853\u306e\u6570\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u3001\u5fc5\u8981\u306a\u6cbb\u7642\u306e\u7a2e\u985e\u3092\u6539\u5584\u3067\u304d\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n> \n> \u5317\u7c73\u653e\u5c04\u7dda\u5b66\u4f1a\uff08RSNA\uff09\u306f\u3001\u533b\u7528\u753b\u50cf\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u304a\u3088\u3073\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u652f\u63f4\u4ecb\u5165\u5b66\u4f1a\uff08MICCAI\u5354\u4f1a\uff09\u3068\u5354\u529b\u3057\u3066\u3001\u81a0\u82bd\u816b\u60a3\u8005\u306e\u8a3a\u65ad\u304a\u3088\u3073\u6cbb\u7642\u8a08\u753b\u3092\u6539\u5584\u3057\u307e\u3057\u305f\u3002<font color=\"orange\">\u3053\u306e\u30b3\u30f3\u30c6\u30b9\u30c8\u3067\u306f\u3001MRI\uff08\u78c1\u6c17\u5171\u9cf4\u753b\u50cf\u6cd5\uff09\u30b9\u30ad\u30e3\u30f3\u3092\u4f7f\u7528\u3057\u3066\u795e\u7d4c\u81a0\u82bd\u816b\u306e\u907a\u4f1d\u7684\u30b5\u30d6\u30bf\u30a4\u30d7\u3092\u4e88\u6e2c\u3057\u3001\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304a\u3088\u3073\u30c6\u30b9\u30c8\u3057\u3066\u3001MGMT\u30d7\u30ed\u30e2\u30fc\u30bf\u30fc\u306e\u30e1\u30c1\u30eb\u5316\u306e\u5b58\u5728\u3092\u691c\u51fa\u3057\u307e\u3059\u3002<\/font>\n> \n> \u6210\u529f\u3059\u308c\u3070\u3001\u8133\u816b\u760d\u60a3\u8005\u304c\u3088\u308a\u4fb5\u8972\u6027\u306e\u4f4e\u3044\u8a3a\u65ad\u3068\u6cbb\u7642\u3092\u53d7\u3051\u308b\u306e\u3092\u52a9\u3051\u308b\u3067\u3057\u3087\u3046\u3002\u624b\u8853\u524d\u306e\u65b0\u3057\u304f\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3055\u308c\u305f\u6cbb\u7642\u6226\u7565\u306e\u5c0e\u5165\u306f\u3001\u8133\u816b\u760d\u60a3\u8005\u306e\u7ba1\u7406\u3001\u751f\u5b58\u3001\u304a\u3088\u3073\u898b\u901a\u3057\u3092\u6539\u5584\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002","36bedeeb":"<div class=\"alert simple-alert\"><font color=\"black\">\n\ud83e\uddd1\u200d\u2695\ufe0f <b>Competition Goal<\/b>: \u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304a\u3088\u3073\u30c6\u30b9\u30c8\u3057\u3066\u3001MGMT\u30d7\u30ed\u30e2\u30fc\u30bf\u30fc\u306e\u30e1\u30c1\u30eb\u5316\u306e\u5b58\u5728\u3092\u691c\u51fa\u3057\u307e\u3059\u3002\n<\/div>","666e0d22":"### \u60a3\u8005\u3055\u309300000\u306eFLAIR\/T1w\/T1wCE\/T2w\u306e\u753b\u50cf\u6570\u3092\u6570\u3048\u3066\u307f\u308b","280e9c51":"<pre>\u3046\u30fc\u3093\u3001\u753b\u50cf\u6570\u306f\u7d50\u69cb\u9055\u3044\u307e\u3059\u306d\u3002\u898f\u5247\u6027\u3082\u307f\u3048\u306a\u3044\u3002\u305f\u307e\u305f\u307e\u3053\u3046\u306a\u3063\u3066\u3044\u308b\u306e\u304b\u3002\n\u9762\u5012\u3067\u3059\u304c\u3001\u5168\u54e1\u5206\u307f\u306a\u3044\u3068\u308f\u304b\u3089\u306a\u3044\u3067\u3059\u306d\u3002","deed82c6":"### tensor int\u78ba\u8a8d","cd58cb39":"\u30d5\u30a9\u30eb\u30c0\u540d\u3068\u540c\u3058\u6570\u5b57\uff15\u6841\u5316\u304c\u3067\u304d\u307e\u3057\u305f\u3002","afa31e2e":"### MGMT=1","62ace687":"## timm\u3067\u4f7f\u3048\u308b\u30e2\u30c7\u30eb\u4e00\u89a7","01d27807":"# \ud83c\udf37T1wCE","09a2da84":"### \ud83d\udd36sample_submission.csv\n\u3053\u308c\u3092\u63d0\u51fa\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002","901eae3b":"# \ud83c\udf37T2w","6903f7be":"![image.png](attachment:161c3f3e-8fcc-46a4-82d9-700749d08fab.png)","ad583fd4":"### dcm\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u3081\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002","2223343b":"![image.png](attachment:cdeb0593-6837-4d90-8070-71bcdf527289.png)","e0f80ffb":"\u306a\u306b\u3082\u306a\u3044\u3002\u3002\u3002\u3002","25c9065c":"# \ud83c\udf37FLAIR","d6d42dc1":"### mgmt=0","563fb74f":"### MGMT=1","f20abca5":"![image.png](attachment:e6f5cff2-15e7-4c2b-afc0-f9fb37a6d248.png)","c3336b4c":"### MGMT=0","a12811fc":"### \u3058\u3083\u3042\u3001FLAIR\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u5c11\u3057\u8aad\u3093\u3067\u307f\u308b","db9f58ed":"\u67a0\u304c\u3067\u304d\u307e\u3057\u305f\u3002","266075e6":"# \ud83c\udf37T1w","4f39ec91":"### MGMT=1","49539f4e":"### \ud83d\udd36train_labels.csv\nsubmission\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u3058\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3059\u306d\u3002","50497dfe":"\u30d5\u30a9\u30eb\u30c0\u540d\u304c\uff15\u6841\u306e\u6570\u5b57\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001BraTS21ID\u306e\u5024\u3092\uff15\u6841\u306b\u3057\u3066\u3042\u3052\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","f6012ffb":"# \u3044\u307e\u306e\u611f\u60f3\u3002\u5927\u5909\u3060\u3051\u3069\u3001efficientnet\u3067\u307e\u305a\u306f\u3044\u3063\u3066\u307f\u3088\u3046\u3002","4dde6d2e":"\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","5c258e90":"\u3053\u3061\u3089\u306e\u30b3\u30fc\u30c9\u306e\u65b9\u304c\u304b\u3063\u3053\u3044\u3044\u306e\u3067\u3001\u3053\u3061\u3089\u306b\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\u3084\u3063\u3066\u3044\u308b\u3053\u3068\u306f\u540c\u3058\u3067\u3059\u3002","afeacc91":"### \u308f\u304b\u3089\u306a\u3044\u306e\u3067\u8a66\u3057\u306b\uff11\u679a\u306e\u753b\u50cf\u3092\u307f\u3066\u307f\u307e\u3059","48e7ef37":"### MGMT\u306fDNA\u4fee\u5fa9\u9175\u7d20\u3067\u3042\u308a\u3001DNA\u4e2d\u306eguanine\u304b\u3089\u30a2\u30eb\u30b1\u30eb\u57fa\u3092\u9664\u53bb\u3059\u308b\u50cd\u304d\u304c\u3042\u308b\u3002\u305d\u308c\u306b\u3088\u308a\u3001\u30a2\u30eb\u30ad\u30ea\u5316\u5264\u304b\u3089\u7d30\u80de\u3092\u4fdd\u8b77\u3059\u308b\u50cd\u304d\u304c\u3042\u308b\u3002<br>\n### MGMT\u306e\u6709\u7121\u304c\u60aa\u6027\u795e\u7d4c\u816b\u760d\u306e\u5b50\u5f8c\u56e0\u5b50\u3067\u3042\u308b\u3053\u3068\u304c\u5831\u544a\u3055\u308c\u305f\u3002 \u3088\u3093\u3067\u3082\u3055\u3063\u3071\u308a\u308f\u304b\u308a\u307e\u305b\u3093\u3002\n\n### MGMT\u306e\u6709\u7121\u304c\u91cd\u8981\u307f\u305f\u3044\u3067\u3059\u3002","47b201e8":"![image.png](attachment:034d7a31-735d-4f61-89cc-eaf93995d67d.png)","14442606":"<pre>train\u30d5\u30a9\u30eb\u30c0\u306f\u4ee5\u4e0b\u306b\u306f00000\uff5e01010\u307e\u3067\u3042\u308a\u307e\u3059\u3002\n\u3068\u3044\u3046\u3053\u3068\u306f\u3001train\u30d5\u30a1\u30a4\u30eb\u306eBraTS21IDn\u306e\u9805\u76ee\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u306f\u60a3\u8005\u3055\u3093\u3092\u7279\u5b9a\u3059\u308b\u756a\u53f7\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002\u8aac\u660e\u306b\u66f8\u3044\u3066\u304a\u3044\u3066\u307b\u3057\u3044\u3067\u3059\u306d\u3002\n\n\u3058\u3083\u3042\u3001\u5148\u307b\u3069\u8aad\u3093\u3060\u753b\u50cf\u306f\u307f\u3093\u306a\u540c\u3058\u60a3\u8005\u3055\u3093\u306e\u5199\u771f\u3060\u3002\u3069\u3046\u307f\u308c\u3070\u3044\u3044\u306e\u304b\u306a\u3002","4ecf782e":"### train\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u898b\u3066\u307f\u308b\u3002","0f6cd2d1":"<pre><font color=\"orange\">Python\u3067\u6587\u5b57\u5217\u30fb\u6570\u5024\u3092\u30bc\u30ed\u57cb\u3081\uff08\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0<\/font>\uff09\n\n  \u53f3\u5bc4\u305b\u30bc\u30ed\u57cb\u3081: zfill()\n  \u53f3\u5bc4\u305b\u3001\u5de6\u5bc4\u305b\u3001\u4e2d\u592e\u5bc4\u305b: rjust(), ljust(), center()"}}