{"cell_type":{"0c771675":"code","785a6cf5":"code","d1185bb5":"code","503628a3":"code","82d519ea":"code","bd8e728a":"code","dca6aed0":"code","e6c418fb":"code","b23360ee":"code","f9e771d2":"code","e47fe57f":"code","f739f98e":"code","ae5afeaa":"code","9e6c826d":"code","c5d7f1f9":"code","eca86666":"code","156b2081":"code","a3a389cb":"code","5fe4323e":"code","3ef3e4e4":"code","2c206758":"code","9aa2fc8a":"code","b9f9eec4":"markdown","5cce5e16":"markdown","f5d32f8c":"markdown","5efc99eb":"markdown","36dd8d70":"markdown","4cf7ad8d":"markdown","bf7e887b":"markdown","584a9311":"markdown","11341ca5":"markdown","4a5be887":"markdown","0337ca77":"markdown","443232b6":"markdown","b035b488":"markdown","77d2c5b7":"markdown","d8ed04bb":"markdown","2afc8f18":"markdown","e2894fad":"markdown","102bc9f6":"markdown","260b055c":"markdown","301584ac":"markdown","2f24783a":"markdown","72affb51":"markdown","7bc37899":"markdown","ac787071":"markdown","bf7fb20a":"markdown"},"source":{"0c771675":"!pip install -q nlpretext loguru","785a6cf5":"import os\nimport gc\nimport copy\nimport time\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom loguru import logger\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom nlpretext import Preprocessor\nfrom nlpretext.basic.preprocess import (normalize_whitespace, remove_punct, \n                                        remove_eol_characters, remove_stopwords, \n                                        lower_text, unpack_english_contractions)\nfrom nlpretext.social.preprocess import remove_html_tags\n\nfrom colorama import Fore\nb_ = Fore.BLUE\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d1185bb5":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.head()","503628a3":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.head()","82d519ea":"preprocessor = Preprocessor()\npreprocessor.pipe(unpack_english_contractions)\npreprocessor.pipe(remove_eol_characters)\npreprocessor.pipe(lower_text)\npreprocessor.pipe(normalize_whitespace)","bd8e728a":"train_df['excerpt'] = train_df['excerpt'].apply(preprocessor.run)","dca6aed0":"excerpt_lenghts = train_df['excerpt'].apply(lambda x: len(x.split()))\nmax(excerpt_lenghts)","e6c418fb":"class CONFIG:\n    seed = 42\n    max_len = 205\n    train_batch_size = 32\n    valid_batch_size = 32\n    epochs = 10\n    learning_rate = 1e-5\n    n_accumulate = 1\n    folds = 5\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    tokenizer.save_pretrained('.\/tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","b23360ee":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","f9e771d2":"def create_folds(df, n_s=5, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG.seed)\n        target = df.target\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG.seed)\n        df['grp'] = pd.cut(df.target, n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n    return df","e47fe57f":"df = create_folds(train_df, n_s=CONFIG.folds, n_grp=12)\ndf.head()","f739f98e":"class BERTDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","ae5afeaa":"def criterion(outputs, targets):\n    return nn.MSELoss()(outputs.view(-1), targets.view(-1))","9e6c826d":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, 1)\n        self.dropout = nn.Dropout(p=0.3)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask = mask, \n                              token_type_ids = token_type_ids, \n                              return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n\nmodel = BERTClass()\nmodel.to(CONFIG.device);","c5d7f1f9":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(ids, mask, token_type_ids)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG.n_accumulate\n            \n        scaler.scale(loss).backward()\n        \n        if (step + 1) % CONFIG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # zero the parameter gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","eca86666":"@torch.no_grad()\ndef valid_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        outputs = model(ids, mask, token_type_ids)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        PREDS.extend(outputs.cpu().detach().numpy().tolist())\n        TARGETS.extend(targets.cpu().detach().numpy().tolist())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    \n    return epoch_loss, val_rmse","156b2081":"@logger.catch\ndef run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG.device, epoch=epoch)\n        \n        valid_epoch_loss, valid_epoch_rmse = valid_one_epoch(model, optimizer, scheduler,\n                                                             dataloader=valid_loader, \n                                                             device=CONFIG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        history['Valid RMSE'].append(valid_epoch_rmse)\n        \n        print(f'Valid RMSE: {valid_epoch_rmse}')\n        \n        # deep copy the model\n        if valid_epoch_rmse <= best_epoch_rmse:\n            print(f\"{b_}Validation RMSE Improved ({best_epoch_rmse} ---> {valid_epoch_rmse})\")\n            best_epoch_rmse = valid_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"Loss{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","a3a389cb":"def prepare_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train, CONFIG.tokenizer, CONFIG.max_len)\n    valid_dataset = BERTDataset(df_valid, CONFIG.tokenizer, CONFIG.max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch_size, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch_size, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","5fe4323e":"train_loader, valid_loader = prepare_data(fold=0)","3ef3e4e4":"# Defining Optimizer with weight decay to params other than bias and layer norms\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n\n# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_loader)*CONFIG.epochs\n)","2c206758":"model, history = run(model, optimizer, scheduler=scheduler, device=CONFIG.device, num_epochs=CONFIG.epochs)","9aa2fc8a":"epochs = list(range(1, CONFIG.epochs + 1))\nfig = go.Figure()\ntrace1 = go.Scatter(x=epochs, y=history['Train Loss'],\n                    mode='lines+markers',\n                    name='Train Loss')\ntrace2 = go.Scatter(x=epochs, y=history['Valid Loss'],\n                    mode='lines+markers',\n                    name='Valid Loss')\nlayout = go.Layout(template=\"plotly_dark\", title='Loss Curve', \n                   xaxis=dict(title='Epochs'), yaxis=dict(title='Loss'))\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.show()","b9f9eec4":"<h3>\ud83d\udccc Explore T5: <a href='https:\/\/www.kaggle.com\/debarshichanda\/explore-t5'>https:\/\/www.kaggle.com\/debarshichanda\/explore-t5<\/a><\/h3>","5cce5e16":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Train Fold: 0<\/h1>","f5d32f8c":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Define Optimizer and Scheduler<\/span>","5efc99eb":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">CommonLit Readability<br> BERT Baseline<\/h1>\n<br>","36dd8d70":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility<\/h1>","4cf7ad8d":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data \ud83d\udcd6<\/h1>","bf7e887b":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Create Dataloaders<\/span>","584a9311":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loss Function<\/h1>","11341ca5":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">We will use <i>NLPretext<\/i> library for preprocessing our text<\/span>","4a5be887":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries \ud83d\udcda<\/h1>","0337ca77":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class<\/h1>","443232b6":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model<\/h1>","b035b488":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualizations \ud83d\udcc9<\/h1>","77d2c5b7":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Maximum Length of Text present in the Dataset<\/span>","d8ed04bb":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Validation Function<\/h1>","2afc8f18":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Function<\/h1>","e2894fad":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","102bc9f6":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds<\/h1>","260b055c":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries<\/h1>","301584ac":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration \u2699\ufe0f<\/h1>","2f24783a":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Preprocessing<\/h1>","72affb51":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run<\/h1>","7bc37899":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6\">https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6<\/a><\/span>","ac787071":"![](https:\/\/github.com\/artefactory\/NLPretext\/raw\/master\/references\/logo_nlpretext.png)","bf7fb20a":"![](https:\/\/res-3.cloudinary.com\/crunchbase-production\/image\/upload\/c_lpad,f_auto,q_auto:eco\/v1475197388\/qcmvdzlsrxyqyftnahs1.png)"}}