{"cell_type":{"820727fd":"code","5b6848a5":"code","a9afbca0":"code","b566961a":"code","cc7e79df":"code","c4cbceac":"code","2ccb519a":"code","3d5082dd":"code","0b01e463":"code","5ef23c56":"code","fc688ce6":"code","91fec496":"code","7f097419":"code","1ebd8594":"code","4f6b48ee":"code","3cad4622":"code","ca4eafd8":"code","d53dc9fa":"code","f991592d":"code","acfb9b2a":"code","4cf502bd":"code","e2d159f0":"code","69372a0b":"code","d3ea59f5":"code","2b9661bb":"code","2f288d25":"code","844ca2b6":"code","96acab34":"code","7abd3822":"code","a3ce8822":"code","9287dfb8":"code","d1532019":"code","a99d3114":"code","c5b90e7c":"code","32615974":"code","c4225e17":"code","85b7793a":"code","0a28d86d":"code","904f5bb9":"code","42c46743":"code","4b0a2c51":"code","1c4c5d06":"markdown","14c22fbd":"markdown","d14ac4d4":"markdown","07c70332":"markdown","13b2cb5a":"markdown","49748b65":"markdown","5e8fa32e":"markdown","046db37c":"markdown","9c348544":"markdown","b461db1d":"markdown","847466f2":"markdown","e04ff8f3":"markdown","27977cdb":"markdown","f1d91746":"markdown","db11c519":"markdown","5a6a0887":"markdown","058c08fb":"markdown","82c7dade":"markdown","f50d68b5":"markdown","06c835eb":"markdown","e5ebc78e":"markdown","4b514da3":"markdown","9c81b997":"markdown","d0224d59":"markdown","50e1fd6b":"markdown","7032b762":"markdown","ce660b61":"markdown","d1767169":"markdown","c3f44c3e":"markdown","1bf2891c":"markdown","ad6d5fa4":"markdown","cbeb707f":"markdown","fff740d2":"markdown","16581a46":"markdown"},"source":{"820727fd":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport warnings\n\n# Scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\n\n# Boosters\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Hyperparameter \nimport optuna\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\nDEMO=True\n\n# pd.set_option(\"max_rows\" , None)\n# pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n# warnings.filterwarnings(\"ignore\")","5b6848a5":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","a9afbca0":"# (PRO) The id column disrupts the training so delete it\nif not DEMO:\n    train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv').drop(columns=['id'])\n\n    train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv', index_col=0)","b566961a":"%%time\ntrain = dt.fread(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\").to_pandas().drop(columns=['id'])\ntest = dt.fread(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\").to_pandas()\nsubmission = dt.fread('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv').to_pandas()","cc7e79df":"# (PRO)\nif not DEMO:\n    train =dt.fread(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", columns=lambda cols:[col.name not in (\"id\") for col in cols]).to_pandas()","c4cbceac":"train.head()","2ccb519a":"# (PRO)\ntrain.head().T","3d5082dd":"train.info()","0b01e463":"# (PRO)\ntrain.info(verbose=True)","5ef23c56":"# (PRO)\ntrain.describe().T","fc688ce6":"# (PRO)\ntrain.sample(4).T","91fec496":"train = train.fillna(0) # replace with 0\ntrain = train.fillna(train.mean()) # replace missing values using the mean along each column\ntrain = train.fillna(train.min()) # replace with the minimum value of each column \ntrain = train.fillna(train.max()) # replace with the maximum value of each column \ntrain = train.dropna()","7f097419":"if not DEMO:\n    impute = SimpleImputer()\n    # Separate\n    impute.fit(train)\n    train = impute.transform(train)\n\n    # Together\n    train = impute.fit_transform(train)","1ebd8594":"if not DEMO:\n    impute = KNNImputer()\n    # Separate\n    impute.fit(train)\n    train = impute.transform(train)\n\n    # Together\n    train = impute.fit_transform(train)","4f6b48ee":"# from sklearn.preprocessing import StandardScaler\nz = StandardScaler()\n\n# Separate\nz.fit(train)\ntrain_z = z.transform(train)\n\n# Together\ntrain_z = z.fit_transform(train)","3cad4622":"# from sklearn.preprocessing import QuantileTransformer\nif not DEMO:\n    qt = QuantileTransformer(output_distribution = 'normal')\n    \n    # Separate\n    qt.fit(train.iloc[:,:-1])\n    train_qt = qt.transform(train.iloc[:,:-1])\n    test_qt = qt.transform(test)\n\n    # Together\n    train_qt = qt.fit_transform(train.iloc[:,:-1])\n    test_qt = qt.transform(test)","ca4eafd8":"n_splits = 4\ny = train['claim']\nX = train.drop(columns=['claim'])","d53dc9fa":"X_test, X_train = np.split(X, [train.shape[0] \/\/ n_splits])\ny_test, y_train = np.split(y, [train.shape[0] \/\/ n_splits])","f991592d":"print(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","acfb9b2a":"train = train.sample(frac=1).reset_index(drop=True)\ny = train['claim']\nX = train.drop(columns=['claim'])","4cf502bd":"%%time\n# from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)\nprint(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","e2d159f0":"%%time\n# from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","69372a0b":"%%time\n# from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=59) \n\nfor train_index, test_index in skf.split(X=X, y=y):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","d3ea59f5":"%%time\n# from sklearn.model_selection import ShuffleSplit\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n    \n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","2b9661bb":"# from lightgbm import LGBMClassifier\nparameters = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'device' : 'gpu'\n}\nmodel = LGBMClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","2f288d25":"# from catboost import CatBoostClassifier\nparameters = {\n    'objective' : 'Logloss',\n    'eval_metric' : 'AUC',\n    'task_type' : 'GPU'\n}\n\nmodel = CatBoostClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","844ca2b6":"# from xgboost import XGBClassifier\nparameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\n\nmodel = XGBClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","96acab34":"parameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\nmodel = XGBClassifier(**parameters)\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test  = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test  = y.iloc[test_index]\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False\n    )\n    y_predicted = model.predict(X_test)\n    \n    # print(f'fold: {fold} |  Score: {get_score_1(y_test, y_predicted)} \\n')","7abd3822":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","a3ce8822":"def lgbm_objective(trial):\n    parameters = {\n        \"device\" : \"gpu\",\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1, # < 0: Fatal, = 0: Error (Warning), = 1: Info, > 1: Debug\n        \"boosting_type\": \"gbdt\", # gbdt, rf, dart, goss\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    model = LGBMClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)]\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","9287dfb8":"# import optuna\nlgbm_study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nlgbm_study.optimize(lgbm_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(lgbm_study.trials)))\n\nprint(\"Best trial for LGBM:\")\ntrial = lgbm_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","d1532019":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","a99d3114":"def catboost_objective(trial):\n    parameters = {\n            \"eval_metric\" : \"AUC\",\n            \"task_type\" : \"GPU\",\n            \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n            \"bootstrap_type\": trial.suggest_categorical(\n                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n            )\n        }\n\n    if parameters[\"bootstrap_type\"] == \"Bayesian\":\n        parameters[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif parameters[\"bootstrap_type\"] == \"Bernoulli\":\n        parameters[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    model = CatBoostClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        early_stopping_rounds=100\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","c5b90e7c":"# import optuna\ncatboost_study = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost Classifier\")\ncatboost_study.optimize(catboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(catboost_study.trials)))\n\nprint(\"Best trial for CatBoost:\")\ntrial = catboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","32615974":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","c4225e17":"def xgboost_objective(trial):\n    parameters = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"predictor\": \"gpu_predictor\",\n        \"eval_metric\" : \"auc\",\n        # use exact for small dataset.\n        \"tree_method\": \"gpu_hist\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n\n    # maximum depth of the tree, signifies complexity of the tree.\n    parameters[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n    parameters[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n    parameters[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n    # defines how selective algorithm is.\n    parameters[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n    parameters[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    model = XGBClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","85b7793a":"# import optuna\nxgboost_study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost Classifier\")\nxgboost_study.optimize(xgboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(xgboost_study.trials)))\n\nprint(\"Best trial for XGBoost:\")\ntrial = xgboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","0a28d86d":"# from sklearn.metrics import *\ndef get_score_1(y_test, y_predicted):\n    fpr, tpr, _ = roc_curve(y_test, y_predicted[:, 1])\n    return auc(fpr, tpr)","904f5bb9":"# from sklearn.metrics import *\ndef get_score_2(y_test, y_predicted):\n    return roc_auc_score(y_test, y_predicted[:, 1])","42c46743":"# from sklearn.metrics import *\ndef get_accuracy_1(y_test, y_predicted):\n    labels_predicted = np.rint(y_predicted)\n    return accuracy_score(y_test, labels_predicted[:, 1])","4b0a2c51":"submission['claim'] = model.predict_proba(test)[:, 1]\nsubmission.to_csv('tps-0921-submit.csv', index = False)","1c4c5d06":"## 5.2 Scikit Learn<a id=\"5.2\"><\/a>\n\nThe above two operations are performed in different ways\n\n### 5.2.1 train_test_split<a id=\"5.2.1\"><\/a>","14c22fbd":"### 5.2.4 ShuffleSplit<a id=\"5.2.4\"><\/a>","d14ac4d4":"## 6.3 XGBoost<a id=\"6.3\"><\/a>\nOfficial documentation for parameters:\n- [https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)","07c70332":"## 7.2 Optuna for CatBoost<a id=\"7.2\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py)","13b2cb5a":"# 4. Preprocessing","49748b65":"<div class=\"alert alert-danger\">\n<svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" viewBox=\"0 0 32 32\">\n  <path d=\"M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z\"\/>\n<\/svg>\n<b style=\"font-size: x-large;\">ATTENTION<\/b><br>\n    A good and accurate way to get lost data is <code>KNNImputer<\/code> but this operation is takes very time because Scikit learn does not support GPUs. No library has been created for KNNImputer class with GPU yet.\n<\/div>","5e8fa32e":"For better results, we can `shuffle` the data","046db37c":"### 5.2.2 K-Fold<a id=\"5.2.2\"><\/a>\n\nIn the following three methods, the splitting operation is repeated `n_splits` times.","9c348544":"It semble [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) is good idea to show any other methods of DataFrame.","b461db1d":"### 5.2.3 StratifiedKFold<a id=\"5.2.3\"><\/a>","847466f2":"We can't view all the columns and datatypes when we want print a concise summary of a DataFrame, we use `verbose` as argument to solve this problem","e04ff8f3":"## 3.3 Statistics<a id=\"3.3\"><\/a>\n\nDisplay head of DataFrame is not normally legible. so we [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) it.","27977cdb":"## 3.1 Pandas<a id=\"3.1\"><\/a>","f1d91746":"## 4.1 Missing values\n\nThese are the methods that can be used to fill in the null values or delete them","db11c519":"# 6 Modeling<a id=\"6\"><\/a>\n\nLightGBM, CatBoost and XGBoost are known as new ways to build models. Parameter settings are very important in these three methods. Only the basic parameters are listed here. \n\n## 6.1 LightGBM<a id='6.1'><\/a>\nOfficial documentations for parameters:\n- [https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)\n- [https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst)","5a6a0887":"The difference between the three methods: [(see here for more info)](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html)\n\n![kfold-StratifiedKFold](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/model-selection-3-models-kfold.jpg)","058c08fb":"## 3.2 Datatable<a id=\"3.2\"><\/a>\n\nTo learn more about Datatable [see this kaggle code](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-python-datatable). Datatable `fread` documentation [is here](https:\/\/datatable.readthedocs.io\/en\/latest\/api\/dt\/fread.html)","82c7dade":"## 5.1 Numpy<a id=\"5.1\"><\/a>","f50d68b5":"# 5. Cross Validation (K-Fold)<a id=\"5\"><\/a>\n\nThere are several ways to split data. In the following, we consider the number of parts equal to 4. We put 25% of the data for testing and 75% for training.","06c835eb":"## 6.4 Boosters with cross validation<a id=\"6.4\"><\/a>","e5ebc78e":"## 4.2 Standardization<a id=\"4.2\"><\/a>\n\nThe `StandardScaler` method uses the following formula\n\n![StandardScaler](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/StandardScaler-formula.jpg)","4b514da3":"# 1. Abstract<a id=\"1\"><\/a>\n\nThese are my personal notes that I need each time when I run a machine learning model. I would like to share them with you and I hope it can be useful for you, even if it is a small help. I will try to update these notes over time.","9c81b997":"# 9. Submission<a id=\"9\"><\/a>","d0224d59":"# 3. Load Data<a id=\"3\"><\/a>\n\nIt seems that Pandas load data slowly. One alternative is to use `datatable` and convert theme to `Pandas`. [Link](https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro#Read-in-the-massive-dataset)","50e1fd6b":"# Contents<a id=\"0\"><\/a>\n\n- [1. Abstract](#1)\n- [2. Modules](#2)\n- [3. Load Data](#3)\n  - [3.1 Pandas](#3.1)\n  - [3.2 Datatable](#3.2)\n  - [3.3 Statistics](#3.3)\n- [4. Preprocessing](#4)\n  - [4.1 Missing values](#4.1)\n  - [4.2 Standardization](#4.2)\n- [5. Cross Validation (K-Fold)](#5)\n  - [5.1 Numpy](#5.1)\n  - [5.2 Scikit Learn](#5.2)\n    - [5.2.1 train_test_split](#5.2.1)\n    - [5.2.2 K-Fold](#5.2.2)\n    - [5.2.3 StratifiedKFold](#5.2.3)\n    - [5.2.4 ShuffleSplit](#5.2.4)\n- [6. Modeling](#6)\n  - [6.1 LightGBM](#6.1)\n  - [6.2 CatBoost](#6.1)\n  - [6.3 XGBoost](#6.3)\n  - [6.4 Boosters with cross validation](#6.4)\n- [7. Hyperparameter](#7)\n  - [7.1 Optuna for LightGBM](#7.1)\n  - [7.2 Optuna for CatBoost](#7.2)\n  - [7.3 Optuna for XGBoost](#7.3)\n- [8. Quality of predictions](#8)\n- [9. Submission](#9)","7032b762":"# 2. Modules<a id=\"2\"><\/a>","ce660b61":"The `QuantileTransformer` method seems to be better. [link](https:\/\/www.kaggle.com\/melanie7744\/tps9-how-to-transform-your-data)","d1767169":"### 4.1.1 Pandas<a id=\"4.1.1\"><\/a>","c3f44c3e":"## 7.3 Optuna for XGBoost<a id=\"7.3\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py)","1bf2891c":"# 8. Quality of predictions<a id=\"8\"><\/a>","ad6d5fa4":"### 4.1.2 Scikit Learn<a id=\"4.1.2\"><\/a>","cbeb707f":"# 7. Hyperparameter <a id=\"7\"><\/a>\nAs I said, parameters are very important in LightGBM, CatBoost and XGBoost methods. Optuna is one of the automatic hyperparameter optimization software framework.\n- [https:\/\/optuna.org](https:\/\/optuna.org)\n- [https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html)\n\n## 7.1 Optuna for LightGBM<a id=\"7.1\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py)","fff740d2":"## 6.2 CatBoost<a id=\"6.2\"><\/a>\nOfficial documentation for parameters:\n- [https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list)","16581a46":"### 4.1.3 capability to handle missing\nSome models that have capability to handle missing value by default are:\n\n- XGBoost: [https:\/\/xgboost.readthedocs.io\/en\/latest\/faq.html](https:\/\/xgboost.readthedocs.io\/en\/latest\/faq.html)\n- LightGBM: [https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html)\n- Catboost: [https:\/\/catboost.ai\/docs\/concepts\/algorithm-missing-values-processing.html](https:\/\/catboost.ai\/docs\/concepts\/algorithm-missing-values-processing.html)"}}