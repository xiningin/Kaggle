{"cell_type":{"312a0f9f":"code","9a712694":"code","342bcb57":"code","33a6ec04":"code","5f71576a":"code","c173a6fe":"code","d36d173b":"code","6f8c6ee5":"code","b69c8630":"code","bac99463":"code","e6b69c2b":"code","021f4a2f":"code","c2fa24ca":"code","fab90abc":"code","2841423c":"code","fbffc0fd":"code","687c4f62":"code","0f3e52e3":"markdown","e12cbc06":"markdown","17d5719a":"markdown","89272d07":"markdown","ddc57480":"markdown","9373b333":"markdown","b6d3e5e1":"markdown","01b975bf":"markdown","a4d4706b":"markdown"},"source":{"312a0f9f":"%%time\n# INSTALL RAPIDS OFFLINE (FROM KAGGLE DATASET). TAKES 1 MINUTE :-)\nimport sys\n!cp ..\/input\/rapids\/rapids.0.11.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","9a712694":"import cudf, cupy, math, warnings, time \nfrom numba import cuda, float32, int8\nimport numpy as np, pandas as pd, os, gc\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\n\nLOCAL_VALIDATION = False\n\n# LIST COLUMNS TO LOAD\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n# USEFUL V COLUMNS\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\nv += [107, 108, 111, 115, 117, 120, 121, 123] \nv += [124, 127, 129, 130, 136] \nv += [138, 139, 142, 147, 156, 162] \nv += [165, 160, 166] \nv += [178, 176, 173, 182] \nv += [187, 203, 205, 207, 215] \nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] \nv += [218, 223, 224, 226, 228, 229, 235] \nv += [240, 258, 257, 253, 252, 260, 261] \nv += [264, 266, 267, 274, 277] \nv += [220, 221, 234, 238, 250, 271] \nv += [294, 284, 285, 286, 291, 297] \nv += [303, 305, 307, 309, 310, 320] \nv += [281, 283, 289, 296, 301, 314] \ncols += ['V'+str(x) for x in v]\n\n# DECLARE COLUMN DTYPES\ndtypes = {'isFraud':'int8'}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]: dtypes[c] = 'float32'\nfor c in ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]: dtypes[c] = 'float32'\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\nstr_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\nfor c in str_type: dtypes[c] = 'category'\n#for c in str_type: dtypes[c] = 'str'\n\nstart = time.time()\nprint('RAPIDS =',cudf.__version__)","342bcb57":"%%time\n# LOAD TRAIN\nX_train = cudf.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',index_col='TransactionID', usecols=cols+['isFraud'],dtype=dtypes)\ntrain_id = cudf.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv',index_col='TransactionID',dtype=dtypes)\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n# LOAD TEST\nX_test = cudf.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv',index_col='TransactionID', usecols=cols,dtype=dtypes)\ntest_id = cudf.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv',index_col='TransactionID', dtype=dtypes)\nfix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\ntest_id.rename(columns=fix, inplace=True)\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n# PRINT SHAPE\ndel train_id, test_id; x = gc.collect()\nprint('Train shape on GPU',X_train.shape,'Test shape on GPU',X_test.shape)","33a6ec04":"%%time\n# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60) \n       \n# LABEL ENCODE\ndef encode_LE(df1,df2,col,verbose=True):\n    df_comb = cudf.concat([df1[col],df2[col]],axis=0)\n    df_comb,_ = df_comb.factorize()\n    df1[col] = df_comb[:len(df1)].astype('int32')\n    df2[col] = df_comb[len(df1):].astype('int32')\n    if verbose: print(col,', ',end='')\n        \n# SET NAN to -1\nfor i,f in enumerate(X_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES. SET NAN to -1\n    if (X_train[f].dtype=='object'): \n        encode_LE(X_train,X_test,f,False)\n    elif f in str_type:\n        X_train[f].fillna(-1,inplace=True)\n        X_test[f].fillna(-1,inplace=True)\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT','isFraud']:\n        mn = np.min((X_train[f].min(),X_test[f].min()))\n        X_train[f] -= np.float32(mn)\n        X_test[f] -= np.float32(mn)\n        X_train[f].fillna(-1,inplace=True)\n        X_test[f].fillna(-1,inplace=True)","5f71576a":"def mean2(x,y_out):\n    # ALLOCATE SHARED MEMORY\n    sum = cuda.shared.array((2),dtype=float32)\n    sum[0] = 0; sum[1] = 0\n    cuda.syncthreads()\n    # COMPUTE SUM AND SKIP NULL\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        if (x[i]!=-1): cuda.atomic.add(sum,0,x[i])\n        else: cuda.atomic.add(sum,1,1)\n    cuda.syncthreads()\n    # OUTPUT MEAN WITHOUT NULL\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        if (len(x)-sum[1])<=0: y_out[i] = -1\n        else: y_out[i] = sum[0]\/(len(x)-sum[1])\n        \ndef std2(x,y_out):\n    # ALLOCATE SHARED MEMORY\n    sum = cuda.shared.array((3),dtype=float32)\n    for i in range(3): sum[i] = 0\n    cuda.syncthreads()\n    # COMPUTE MEAN AND SKIP NULL\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        if (x[i]!=-1): cuda.atomic.add(sum,0,x[i])\n        else: cuda.atomic.add(sum,2,1)\n    cuda.syncthreads()\n    if cuda.threadIdx.x==0: sum[0] = sum[0]\/(len(x)-sum[2])\n    cuda.syncthreads()\n    # COMPUTE SUM OF SQUARES AND SKIP NULL\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        if (x[i]!=-1): cuda.atomic.add(sum,1,(x[i]-sum[0])**2) \n    cuda.syncthreads()\n    # OUTPUT STANDARD DEVIATION WITHOUT NULL\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        if (len(x)-sum[2])<=1: y_out[i] = -1\n        else: y_out[i] = math.sqrt( sum[1]\/(len(x)-sum[2]-1) )\n            \ndef count2(x,y_out):\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):  \n        y_out[i] = len(x)\n        \ndef nunique2(x,y_out):\n    # ALLOCATE SHARED MEMORY\n    record = cuda.shared.array((2048),dtype=int8)\n    for i in range(2048): record[i] = 0\n    cuda.syncthreads()\n    # RECORD UNIQUES\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        record[ int(x[i]*1e6)%2048 ] = 1\n    cuda.syncthreads()\n    # OUTPUT NUNIQUE\n    sum = 0\n    for j in range(2048): sum = sum + record[j]\n    for i in range(cuda.threadIdx.x,len(x),cuda.blockDim.x):\n        y_out[i] = sum","c173a6fe":"# GROUP AGGREGATIONS\ndef add_feature(df1,df2,uid,col,agg,verbose=True):\n    if agg=='count': func = count2\n    elif agg=='mean': func = mean2\n    elif agg=='std' : func = std2\n    elif agg=='nunique': func = nunique2\n    else: return\n    df1['idx'] = np.arange(len(df1))\n    df2['idx'] = np.arange(len(df2))+len(df1)\n    temp_df = cudf.concat([df1[[uid,col,'idx']], df2[[uid,col,'idx']]])\n    tmp = temp_df.groupby(uid,method='cudf').apply_grouped(\n        func,\n        incols={col:'x'},\n        outcols=dict(y_out=np.float32),\n        tpb=32\n    ).rename({'y_out':'new'})  \n    tmp = tmp.sort_values('idx')\n    df1[uid+'_'+col+'_'+agg] = tmp.iloc[:len(df1)].new\n    df2[uid+'_'+col+'_'+agg] = tmp.iloc[len(df1):].new\n    if verbose: print(uid+'_'+col+'_'+agg,', ',end='')\n    df1.drop_column('idx')\n    df2.drop_column('idx')\n    \ndef add_features(df1,df2,uids,cols,aggs,verbose=True):\n    for col in cols:\n        for uid in uids:\n            for agg in aggs:\n                add_feature(df1,df2,uid,col,agg,verbose)\n                \n# COMBINE FEATURES\ndef encode_CB(df1,df2,col1,col2,verbose=True):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(df1,df2,nm,verbose=False)\n    if verbose: print(nm,', ',end='')","d36d173b":"%%time\n# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - X_train['TransactionAmt'].floor()).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - X_test['TransactionAmt'].floor()).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE\nadd_features(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'],['TransactionDT'],['count'])\n# COMBINE COLUMNS \nencode_CB(X_train,X_test,'card1','addr1')\nencode_CB(X_train,X_test,'card1_addr1','P_emaildomain')\n# FREQUENCY ENCODE\nadd_features(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'],['TransactionDT'],['count'])\n# GROUP AGGREGATE\nadd_features(X_train,X_test,['card1','card1_addr1','card1_addr1_P_emaildomain'],\\\n    ['TransactionAmt','D9','D11'],['mean','std'])","6f8c6ee5":"import datetime\n# ADD MONTH FEATURE\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].to_pandas().map(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].to_pandas().map(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","b69c8630":"# ADD UID FEATURE\nX_train['day'] = X_train.TransactionDT \/ (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+(X_train.day-X_train.D1).floor().astype(str)\n\nX_test['day'] = X_test.TransactionDT \/ (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+(X_test.day-X_test.D1).floor().astype(str)\n# LABEL ENCODE\nencode_LE(X_train,X_test,'uid',verbose=False)","bac99463":"%%time\n# FREQUENCY ENCODE \nadd_features(X_train,X_test,['uid'],['TransactionDT'],['count'])\n# AGGREGATE \nadd_features(X_train,X_test,['uid'],['TransactionAmt','D4','D9','D10','D15'],['mean','std'])\n# AGGREGATE\nadd_features(X_train,X_test,['uid'],['C'+str(x) for x in range(1,15) if x!=3],['mean'])\n# AGGREGATE\nadd_features(X_train,X_test,['uid'],['M'+str(x) for x in range(1,10)],['mean'])\n\n# AGGREGATE \nadd_features(X_train,X_test,['uid'],['P_emaildomain','dist1','DT_M','id_02','cents'],['nunique'])\n# AGGREGATE\nadd_features(X_train,X_test,['uid'],['C14'],['std'])\n# AGGREGATE \nadd_features(X_train,X_test,['uid'],['C13','V314'],['nunique'])\n# AGGREATE \nadd_features(X_train,X_test,['uid'],['V127','V136','V309','V307','V320'],['nunique'])","e6b69c2b":"cols = list( X_train.columns )\n# REMOVE FEATURES\ncols.remove('TransactionDT'); cols.remove('isFraud')\nfor c in ['D6','D7','D8','D9','D12','D13','D14'] + ['DT_M','day','uid']:\n    cols.remove(c)  \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","021f4a2f":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nprint(\"We read data from disk and created all these features in %.2f seconds!\" % (time.time()-start))\nnp.array(cols)","c2fa24ca":"X_train = X_train.sort_index()\nsplit = 3*len(X_train)\/\/4","fab90abc":"if LOCAL_VALIDATION:\n    import xgboost as xgb\n    print(\"XGBoost version:\", xgb.__version__)\n\n    xgb_parms = { \n        'n_estimators':2000,\n        'max_depth':12, \n        'learning_rate':0.02, \n        'subsample':0.8,\n        'colsample_bytree':0.4, \n        'missing':-1, \n        'eval_metric':'auc',\n        'objective':'binary:logistic',\n        'tree_method':'gpu_hist' \n    }\n    train = xgb.DMatrix(data=X_train.iloc[:split][cols],label=X_train.iloc[:split]['isFraud'])\n    valid = xgb.DMatrix(data=X_train.iloc[split:][cols],label=X_train.iloc[split:]['isFraud'])\n    clf = xgb.train(xgb_parms, dtrain=train,\n        num_boost_round=2000,evals=[(train,'train'),(valid,'valid')],\n        early_stopping_rounds=100,maximize=True,\n        verbose_eval=50)","2841423c":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\n\nX_test = X_test.sort_index()\noof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\n\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, X_train.isFraud, groups=X_train['DT_M'].to_pandas()) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        \n    xgb_parms = { \n        'max_depth':12, \n        'learning_rate':0.02, \n        'subsample':0.8,\n        'colsample_bytree':0.4, \n        'missing':-1, \n        'eval_metric':'auc',\n        'objective':'binary:logistic',\n        'tree_method':'gpu_hist' \n    }\n    train = xgb.DMatrix(data=X_train.iloc[idxT][cols],label=X_train.iloc[idxT]['isFraud'])\n    valid = xgb.DMatrix(data=X_train.iloc[idxV][cols],label=X_train.iloc[idxV]['isFraud'])\n    clf = xgb.train(xgb_parms, dtrain=train,\n        num_boost_round=2000,evals=[(train,'train'),(valid,'valid')],\n        early_stopping_rounds=200,maximize=True,\n        verbose_eval=100)   \n    \n    oof[idxV] += clf.predict(valid)\n    test = xgb.DMatrix(data=X_test[cols])\n    preds += clf.predict(test)\/skf.n_splits\n    del clf; x=gc.collect()\nprint('#'*20)\nprint ('XGB96 OOF CV=',roc_auc_score(X_train.isFraud.to_array(),oof))","fbffc0fd":"plt.hist(oof,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB OOF')\nplt.show()\n\nX_train['oof'] = oof\nX_train = X_train.reset_index()\nX_train[['TransactionID','oof']].to_pandas().to_csv('oof_xgb_96.csv')\nX_train = X_train.set_index('TransactionID',drop=True)","687c4f62":"sample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_xgb_96.csv',index=False)\n\nplt.hist(sample_submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB96 Submission')\nplt.show()","0f3e52e3":"# GPU Load Data\nHere we read the data from the disk with cuDF directly into the GPU. With CPU Pandas this takes 46 seconds. With GPU RAPIDS this takes 4.7 seconds! ","e12cbc06":"# Submit to Kaggle\nThis submission achieves LB 0.960. If we post process these predictions we achieve LB 0.962. If we ensemble these with CatBoost and LGBM models, we achieve LB 0.968.","17d5719a":"# GPU Feature Engineering\nBelow is where we create all our new engineered features. The work below takes 10 seconds. The work above took 10 seconds. Using RAPIDS GPU is 15x faster than Pandas CPU.","89272d07":"# Install RAPIDS\nHere we install RAPIDS from a Kaggle dataset taking 1 minute. (Install from Conda shown [here][1], if this doesn't work).\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/rapids-data-augmentation-mnist-0-985","ddc57480":"# GPU Encoding Functions\nThe following four functions are Numba CUDA JIT kernels. These functions are optimized to use Nvidia GPU. We will use these together with RAPIDS cuDF's `groupyby(col,method='cudf').apply_grouped(func)` to create blazingly fast custom feature engineering functions! Tutorials about this are [here][1], [here][2], and [here][3].\n\n[1]: https:\/\/rapidsai.github.io\/projects\/cudf\/en\/0.11.0\/guide-to-udfs.html\n[2]: https:\/\/github.com\/daxiongshu\/notebooks-extended\/blob\/kdd_plasticc\/advanced_notebooks\/tutorials\/rapids_customized_kernels.ipynb\n[3]: https:\/\/numba.pydata.org\/numba-doc\/latest\/cuda\/index.html","9373b333":"# Local Holdout Validation","b6d3e5e1":"# GPU Preprocess\nFirst we normalize D Columns, label encode all categorical columns, shift numerics postive, and fill NaN with -1. Note that RAPIDS cuDF has already label encoded all the categorical variables when they were read from disk if `dtype='category'` was used instead of `dtype='str'`.","01b975bf":"# RAPIDS - Feature Engineering - 1st Place Fraud Comp - [0.96]\nThe secret to creating a high scoring model in Kaggle's IEEE CIS Fraud Competition is feature engineering. A list of feature engineering techniques is posted [here][1]. The most important features in Fraud Comp are new columns created from group aggregations of other columns. Why this works is explained [here][2]. Computing group aggregations can naturally be done in parallel and benefit from using GPU instead of CPU.\n\nThis notebook contains the XGBoost model of the 1st place Fraud Comp solution converted to use RAPIDS cuDF. (The entire 1st place solution is an ensemble of XGBoost, CatBoost, and LightGBM with additional post processing described [here][4]). To read one million rows from disk and create 262 features on CPU using Pandas takes 5 minutes. To read and create those features on GPU with RAPIDS cuDF takes 20 seconds as shown below. RAPIDS is 15x faster!\n\n![speedup2.JPG](attachment:speedup2.JPG)\n\nIndividual GPU times are listed beneath code blocks below. Pandas CPU times are displayed in the notebook [here][3] beneath code blocks.\n\n[1]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/108575\n[2]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111453\n[3]: https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\n[4]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111284","a4d4706b":"# Cross Validation and Inference"}}