{"cell_type":{"28f3f08f":"code","a649042a":"code","a2039215":"code","8d8fc7dd":"code","29146e11":"code","3a29a1eb":"code","ce7c30f8":"code","a0507b0d":"code","bd0112f1":"code","47dd4cc5":"code","4c2aeaec":"code","d5ef4d90":"code","f774ee3e":"code","3f7b0c6f":"code","fb9dc3b0":"code","777d67d2":"code","7faf97ec":"code","4e1a521d":"code","535aa06d":"code","be962437":"code","c5ed760c":"code","497a411f":"code","c0250b0f":"code","3bbbab74":"code","d51c6adb":"code","36886138":"code","32e36a2a":"code","f40d64c2":"code","21f4d005":"code","d6182066":"code","d89581b6":"code","fc0b498c":"code","5078a292":"code","3ec714b0":"code","1335f143":"code","097ad06b":"code","fe8596de":"code","89f89f80":"code","462b6e57":"code","af54da7f":"code","1efebffc":"code","edca896f":"code","1eb9963e":"code","c150fc77":"code","3cd46e43":"code","058b01a8":"code","b54f0966":"code","237efc07":"code","0fe2dfb6":"code","92007331":"code","a48a23c7":"code","c719a17a":"code","7e21cb5f":"code","0ef5328d":"markdown","6fd1c560":"markdown","6d76703d":"markdown","7dba2f32":"markdown","9517c209":"markdown","bb616d80":"markdown","1a8ad288":"markdown","a0039ae5":"markdown","add0efed":"markdown","2c4cd4e8":"markdown","f2210f95":"markdown","4f840523":"markdown","7c9d7e65":"markdown","155a7287":"markdown","09cc0c05":"markdown","4b9fbca8":"markdown","0c424752":"markdown","5181df43":"markdown","fc3fa56d":"markdown","f2f3a606":"markdown","1e0fa9c5":"markdown","f20f69a7":"markdown","39ffd399":"markdown","7066f16d":"markdown","b5fe0408":"markdown"},"source":{"28f3f08f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a649042a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder","a2039215":"def clean(df):\n    # No specific cleaning is required for this dataset\n    return df","8d8fc7dd":"# The numeric features : PassengerId, Age, SibSp, Parch, Fare\n# The object dtype features : Name, Ticket  \n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"Sex\", \"Embarked\", \"Cabin\"]\n\n# The ordinal (ordered) categorical features \nordered_levels = {\n    \"Pclass\": [1, 2, 3]\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in ordered_levels.items()}\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            if df[name].isnull().sum()!=0:\n                df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,ordered=True))\n    return df\n","29146e11":"def replace_null_values_mean_std(train_data,test_data,feature):\n    train_data_mean = train_data[feature].mean()\n    test_data_std = test_data[feature].std()\n    train_data[feature] = train_data[feature].replace(np.NaN, np.random.randint(train_data_mean - test_data_std, train_data_mean + test_data_std))\n    test_data[feature] = test_data[feature].replace(np.NaN, train_data_mean)\n    df = pd.concat([train_data, test_data])\n    return df","3a29a1eb":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        if df[name].isnull().sum()!=0:\n            df[name] = df[name].fillna(\"None\")\n    for name in df.select_dtypes(\"object\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","ce7c30f8":"# Load data set\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data = train_data.set_index([pd.Index(range(891))])\ntest_data = test_data.set_index([pd.Index(range(891,1309))])\n\n# Merge the splits so we can process them together\ndf = pd.concat([train_data, test_data])\n\n# Preprocessing\ndf = clean(df) # cleaning dataset\ndf = encode(df) # encode dataset dtypes\ndf = replace_null_values_mean_std(train_data, test_data, \"Fare\") #replace missing values\ndf = replace_null_values_mean_std(train_data, test_data, \"Age\") #replace missing values\ndf = impute(df) # impute missing values in the dataset\n\n# Reform splits\ntrain_data = df.loc[train_data.index, :]\ntest_data = df.loc[test_data.index, :]\n\ntrain_data[\"Survived\"] = train_data.Survived.astype(\"category\")","a0507b0d":"# Display data\nprint(\"\\nTraining Data:\\n\")\ndisplay(train_data.head())\nprint(\"\\nTest Data:\\n\")\ndisplay(test_data.head())\n\n# Display information about dtypes and missing values\nprint(\"\\nTraining Data Information:\\n\")\ndisplay(train_data.info())\nprint(\"\\nTest Data Information:\\n\")\ndisplay(test_data.info())","bd0112f1":"random_forest_model = RandomForestClassifier(\n#     n_estimators=100, max_depth=5, random_state=1,\n    criterion='gini', \n    n_estimators=700,\n    min_samples_split=10,\n    min_samples_leaf=1,\n    max_features='auto',\n    oob_score=True,\n    random_state=1,\n    n_jobs=-1\n)","47dd4cc5":"# score dataset with one hot encoding without droping first column\ndef score_dataset(X, y, model=random_forest_model):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        # X[colname], _ = X[colname].factorize() # Label encoding\n        dummies = pd.get_dummies(X[colname], prefix=colname, drop_first=False) # One hot encoding\n        X = X.join(dummies)\n        X = X.drop(colname, 1)\n        \n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","4c2aeaec":"# Function to get predictions\ndef get_predictions(X, train_data, test_data, y, ids, model=random_forest_model):\n    \n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        # X[colname], _ = X[colname].factorize()\n        dummies = pd.get_dummies(X[colname], prefix=colname, drop_first=False) # One hot encoding\n        X = X.join(dummies)\n        X = X.drop(colname, 1)\n        \n    train_data_1 = X.loc[train_data.index, :]\n    test_data_1 = X.loc[test_data.index, :]\n    \n    model = model\n    model.fit(train_data_1, y)\n    predictions = model.predict(test_data_1)\n    predictions = predictions.astype(int)\n\n    output = pd.DataFrame({'PassengerId': ids, 'Survived': predictions})\n    output.to_csv('my_submission_11.csv', index=False)\n    print(output.head())\n    return","d5ef4d90":"X_train = train_data.copy()\ny_train = X_train.pop(\"Survived\")\n\nbaseline_score = score_dataset(X_train.copy(), y_train)\nprint(f\"Baseline score before applying feature engineering: {baseline_score:.5f} RMSLE\")","f774ee3e":"# Remove PassengerId, Name, Ticket\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]\n\nX_train_new = X_train[features].copy()\n\nbaseline_score = score_dataset(X_train_new.copy(), y_train)\nprint(f\"New score after removing high cardinality columns: {baseline_score:.5f} RMSLE\")","3f7b0c6f":"# Utility functions from Tutorial\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n        # Dummies = pd.get_dummies(X[colname], drop_first=False,prefix = colname)\n        # X = X.join(Dummies)\n        # X = X.drop(colname,1)\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","fb9dc3b0":"X_all = train_data.copy()\n\nmi_scores = make_mi_scores(X_train, y_train)\nprint (\"Mi Scores:\\n\", mi_scores)\nplot_mi_scores(mi_scores.head(20))","777d67d2":"sns.relplot(\n    x=\"value\", y=\"Survived\", col=\"variable\", data=X_all.melt(id_vars=\"Survived\", value_vars=features), facet_kws=dict(sharex=False),\n);","7faf97ec":"sns.countplot(X_all['Pclass'], hue=X_all['Survived'], palette='Paired')","4e1a521d":"sns.countplot(X_all['Sex'], hue=X_all['Survived'], palette='rocket')","535aa06d":"sns.countplot(X_all['Age'], hue=X_all['Survived'], palette='Paired')","be962437":"sns.countplot(X_all['SibSp'], hue=X_all['Survived'], palette='flare')","c5ed760c":"def breakDownCabinToFirstLetter(X):\n    X_new = X.copy()\n    X_new[\"CabinLetter\"] = np.where(X_new[\"Cabin\"] != \"None\", X_new[\"Cabin\"].astype(str).str[0], \"None\")\n    X_new[\"CabinLetter\"] = X_new[\"CabinLetter\"].astype(\"category\")\n    return X_new","497a411f":"def breakDownName(X, NameCol):\n    X1 = X.copy()\n    X_new = pd.DataFrame()\n    X_new[\"Title\"] = NameCol.str.split(\", \", n=1, expand=True)[1]\n    X1[\"Title\"] = X_new[\"Title\"].copy().str.split(\".\", n=1, expand=True)[0]\n    X1[\"Title\"] = X1[\"Title\"].astype(\"category\")\n    return X1","c0250b0f":"def getTicketLetter(X, TicketCol):\n    X1 = X.copy()\n#     X1[\"TicketLength\"] = TicketCol.apply(lambda x: len(x))\n    X1[\"TicketLetter\"] = TicketCol.apply(lambda x: str(x)[0])\n    X1[\"TicketLetter\"] = X1[\"TicketLetter\"].astype(\"category\")\n    return X1","3bbbab74":"def getNameLength(X, NameCol):\n    X_new = X.copy()\n    X_new[\"NameLength\"] = NameCol.str.len()\n    return X_new","d51c6adb":"def binningFare(X):\n    X_new = X.copy()\n    cut_list = list(range(0,100,10))\n    cut_list.extend(list(range(100,700,100)))\n    X_new['FareBin']=pd.cut(X_new['Fare'],cut_list,labels=False,right=False)\n    X_new[\"FareBin\"] = X_new[\"FareBin\"].astype(\"category\")\n    return X_new\n\ndef binningAge(X):\n    X_new = X.copy()\n    cut_list = [0,4,8,12,18,30,40,55,65,80]\n    X_new['AgeBin'] = pd.cut(X_new['Age'], cut_list, labels=False)\n    X_new[\"AgeBin\"] = X_new[\"AgeBin\"].astype(\"category\")\n    return X_new","36886138":"def getMedianFareWithEmbarked(X):\n    X_new = X.copy()\n    X_new[\"MedianFareWithEmbarked\"] = X_new.groupby(\"Embarked\")[\"Fare\"].transform(\"median\")\n    return X_new","32e36a2a":"def cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    \n    # Standardize\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    \n    #Fit the KMeans model to X_scaled and create the cluster labels\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n    \n    return X, kmeans\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","f40d64c2":"def addClusterDistanceFeatures (X):\n    X_new = X.join(cluster_distance(X, cluster_features, 10))\n    return X_new","21f4d005":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","d6182066":"pca_features = [\n        \"Age\",\n        \"Fare\",\n        \"SibSp\",\n        \"Parch\"\n    ]\ndef getCorrelationsForFeatures():\n    print(\"Correlation with Survived:\\n\")\n    print(train_data[pca_features].corrwith(train_data.Survived))\n    return","d89581b6":"def getPcaFeatures (X):\n    X_new = X.loc[:, pca_features]\n\n    # `apply_pca`, defined above, reproduces the code from the tutorial\n    pca, X_pca, loadings = apply_pca(X_new)\n    print(loadings)\n    plot_variance(pca)\n    return X_pca","fc0b498c":"def addPcaFeatures (X, X_pca):\n    X_with_pca_features = X.copy()\n    X_with_pca_features = X_with_pca_features.join(X_pca)\n    return X_with_pca_features","5078a292":"def createNewFeatureFromPCA (X):\n    X_with_new_features = X.copy()\n    X_with_new_features[\"TotSibSpParch\"] = X[\"SibSp\"] + X[\"Parch\"]\n    # X_with_new_features[\"FareToAgeRatio\"] = X[\"Fare\"]\/X[\"Age\"]\n    return X_with_new_features","3ec714b0":"def groupFeature (X):\n    X_new = X.copy()\n    Family_category_map = {0: 'Single', 1: 'Small', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Medium', 7: 'Large', 10: 'Large'}\n    X_new['FamilyCategory'] = X_new['TotSibSpParch'].map(Family_category_map)\n    X_new['FamilyCategory'] = X_new['FamilyCategory'].astype('category')\n    return X_new","1335f143":"X_train_test = pd.concat([train_data, test_data])\nX = X_train_test[features]\n\nX1 = breakDownName(X, X_train_test[\"Name\"])\nprint(f\"New score after breaking-down Names: {score_dataset(X1.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\n\nX2 = getNameLength(X1, X_train_test[\"Name\"])\nprint(f\"New score after getting Name Length feature: {score_dataset(X2.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\n\nX3 = getTicketLetter(X2, X_train_test[\"Ticket\"])\nprint(f\"New score after getting Ticket Letter feature: {score_dataset(X3.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")","097ad06b":"X4 = breakDownCabinToFirstLetter(X3)\nprint(f\"New score after breaking down Cabin to Cabin's first Letter: {score_dataset(X4.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\n\nX5 = binningFare(X4)\nprint(f\"New score after binning fare: {score_dataset(X5.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\n\nX6 = binningAge(X5)\nprint(f\"New score after binning age: {score_dataset(X6.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\n\nX7 = getMedianFareWithEmbarked(X6)\nprint(f\"New score after getting median of fare with embarked: {score_dataset(X7.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")","fe8596de":"cluster_features = [\n    \"Age\",\n    \"Fare\"\n]\nX8, kmeans_model = cluster_labels(X7.copy(), cluster_features, 10)\nprint(f\"New score after creating cluster feature: {score_dataset(X8.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")\nX8.head()","89f89f80":"X_clustered = X8.copy()\n\nsns.relplot(\n    x=\"Age\", y=\"Fare\", hue=\"Cluster\", data=X_clustered, height=6,\n);\n\nX_clustered[\"Cluster\"] = X_clustered.Cluster.astype(\"category\")\nX_clustered[\"Survived\"] = y_train\nsns.relplot(\n    x=\"value\", y=\"Survived\", hue=\"Cluster\", col=\"variable\",\n    height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n    data=X_clustered.melt(\n        value_vars=cluster_features, id_vars=[\"Survived\", \"Cluster\"],\n    ),\n);","462b6e57":"# X9 = addClusterDistanceFeatures (X8)\n# print(f\"New score after adding cluster distance features: {score_dataset(X9.loc[train_data.index, :].copy(), y):.5f} RMSLE\")","af54da7f":"X_pca = getPcaFeatures(X8)","1efebffc":"# X10 = addPcaFeatures(X8, X_pca)\n# print(f\"New score after adding pca features: {score_dataset(X10.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")","edca896f":"X11 = createNewFeatureFromPCA (X8)\nprint(f\"New score after creating a new feature from comparing pca loadings: {score_dataset(X11.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")","1eb9963e":"X_plot = X11.copy()\nX_plot[\"Survived\"] = y_train\nsns.countplot(x='TotSibSpParch', hue='Survived', data=X_plot)\nplt.title('Count of Survival in Family Feature',size=15)","c150fc77":"X12 = groupFeature(X11)\nprint(f\"New score after grouping the last feature: {score_dataset(X12.loc[train_data.index, :].copy(), y_train):.5f} RMSLE\")","3cd46e43":"sns.countplot(X12.loc[train_data.index, :]['FamilyCategory'], hue=y_train, palette='flare')","058b01a8":"print(\"Get how many categories, each categorical feature in the dataset has:\\n\")\ndisplay(X12.select_dtypes([\"category\"]).nunique())\nprint(\"Get how many times a category occurs in the dataset:\\n\")\ndisplay(X12[\"Title\"].value_counts())","b54f0966":"# # Encoding split\n# X_copy = X_with_new_features.copy()\n# X_copy[\"Ticket\"] = train_data[\"Ticket\"]\n# X_copy[\"Survived\"] = y.copy()\n\n# X_encode = X_copy.sample(frac=0.20, random_state=0)\n# y_encode = X_encode.pop(\"Survived\")\n\n# # Training split\n# X_pretrain = X_copy.drop(X_encode.index)\n# y_train = X_pretrain.pop(\"Survived\")\n\n# # print(X_copy.head(900))\n# # print(y_encode.head(900))\n# # print(X_pretrain.head(900))\n# # print(y_train.head(900))\n\n# # Choose a set of features to encode and a value for m\n# encoder = MEstimateEncoder(cols=[\"Ticket\"], m=0.5)\n\n# # Fit the encoder on the encoding split\n# encoder.fit(X_encode, y_encode)\n\n# # Encode the training split\n# X_train = encoder.transform(X_pretrain, y_train)\n\n# encoder_feature = [\"Ticket\"]\n\n# plt.figure(dpi=90)\n# ax = sns.distplot(y_train, kde=True, hist=False)\n# ax = sns.distplot(X_train[encoder_feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\n# ax.set_xlabel(\"Survived\");","237efc07":"print (\"Final dataset Information: \\n\")\ndisplay(X12.info())","0fe2dfb6":"mi_scores = make_mi_scores(X12.loc[train_data.index, :], y_train)\nprint (\"Mi Scores:\\n\", mi_scores)\nplot_mi_scores(mi_scores.head(25))","92007331":"sns.countplot(X12.loc[train_data.index, :]['AgeBin'], hue=y_train, palette='winter')","a48a23c7":"sns.countplot(X12.loc[train_data.index, :]['FareBin'], hue=y_train, palette='Set1')","c719a17a":"XFinal = X12.copy()\nXFinal = XFinal[['Pclass', 'Sex', 'Age', 'Fare', 'Cabin', 'Embarked', 'TicketLetter', 'Title',  \n                 'CabinLetter', 'FareBin', 'Cluster', 'FamilyCategory']]\nscore_dataset(XFinal.loc[train_data.index, :].copy(), y_train)","7e21cb5f":"get_predictions(XFinal, train_data, test_data, y_train, test_data[\"PassengerId\"])","0ef5328d":"Get the corresponding corelations","6fd1c560":"## Binning","6d76703d":"# Data Preprocessing","7dba2f32":"## Define the model, functions to score dataset and get predictions","9517c209":"## Imports and Configurations","bb616d80":"### Get Predictions","1a8ad288":"**Breaking Down Name to Title**","a0039ae5":"## Clustering With K-Means","add0efed":"### Encode Data","2c4cd4e8":"## Finalize the features","f2210f95":"### Group Transformation","4f840523":"## Load and apply preprocessing to data","7c9d7e65":"MedianFareWithEmbarked, Parch, AgeBin, SibSp features has removed as they have less mutual information with respect to the target Survived and the information related to them have been given to the model in terms of other features. NameLength and TotSibSpParch are also removed to get a better score.","155a7287":"### Breaking-Down Features","09cc0c05":"## Create Features","4b9fbca8":"### Remove high cardinality columns\nHigh Cardinality columns (Columns with many unique values) are remove to get better score. ","0c424752":"**Create New Features using PCA**","5181df43":"Signs and magnitudes of the principal component's loadings tell us what kind of variation it has captured. The PC1 shows that SibSp and Parch has the same sign. So we cam add these two features and create a new feature called TotSibSpParch.","fc3fa56d":"**Cluster-Distance Features**","f2f3a606":"### Clean Data","1e0fa9c5":"## Principal Component Analysis","f20f69a7":"### Handle Missing Values","39ffd399":"### Mutual Information","7066f16d":"# Feature Engineering","b5fe0408":"## Target Encoding"}}