{"cell_type":{"aa350f8a":"code","c503f8ee":"code","27856119":"code","c8c9be65":"code","0b49ecfc":"code","1c3e854f":"code","6ec9ea33":"code","3b1028bb":"code","806de8fe":"code","f61c835e":"markdown","78dceaff":"markdown","b6c1503e":"markdown","38245d92":"markdown","e0c18da3":"markdown","68e22a9e":"markdown"},"source":{"aa350f8a":"import os\nimport time\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom matplotlib import animation, rc\nimport cv2","c503f8ee":"class PokemonDataset(Dataset):\n    def __init__(self, data_dir, num='all', transforms=None):\n        self.paths = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n        if num != 'all' and num < len(self.paths):\n            self.paths = self.paths[:num]\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx])\n        if self.transforms is not None:\n            img = self.transforms(img)\n        return img\n    ","27856119":"class WSConv2d(nn.Module):\n    \"\"\"\n    Weight scaled Conv2d (Equalized Learning Rate)\n    Note that input is multiplied rather than changing weights\n    this will have the same result.\n    Inspired and looked at:\n    https:\/\/github.com\/nvnbny\/progressive_growing_of_gans\/blob\/master\/modelUtils.py\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, gain=2):\n        super(WSConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.scale = (gain \/ (in_channels * (kernel_size ** 2))) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n\n        # initialize conv layer\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n    \nclass PixelNorm(nn.Module):\n    def __init__(self, eps=1e-8):\n        super(PixelNorm, self).__init__()\n        self.eps = eps\n        \n    def forward(self, x):\n        return x \/ torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n    \n# (Conv3*3 + LReLU + PixelNorm) * 2\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.block = nn.Sequential(\n            WSConv2d(in_channels, out_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            PixelNorm(),\n            WSConv2d(out_channels, out_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            PixelNorm(),\n        )\n    \n    def forward(self, x):\n        return self.block(x)\n\n# ToRGB\nclass ToRGB(nn.Module):\n    def __init__(self, in_channels):\n        super(ToRGB, self).__init__()\n        self.rgb = WSConv2d(in_channels, 3, 1)\n        \n    def forward(self, x):\n        return self.rgb(x)\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=512, in_channels=512, max_size=128):\n        super(Generator, self).__init__()\n        self.input = nn.Sequential(\n            PixelNorm(),\n            nn.ConvTranspose2d(z_dim, in_channels, 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            PixelNorm(),\n            WSConv2d(in_channels, in_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            PixelNorm(),\n        ) # out: in_channels * 4 * 4\n        \n        steps = int(np.log2(max_size) - 2)\n        self.blocks = nn.ModuleList([])\n        self.rgbs = nn.ModuleList([ToRGB(in_channels)])\n        channels = in_channels\n        for i in range(steps):\n            if i < 3:\n                self.blocks.append(ConvBlock(channels, channels))\n                self.rgbs.append(ToRGB(channels))\n            else:\n                self.blocks.append(ConvBlock(channels, channels\/\/2))\n                self.rgbs.append(ToRGB(channels\/\/2))\n                channels = channels \/\/ 2\n                \n    def fade_in(self, block, up, alpha):\n        return torch.tanh(alpha * block + (1 - alpha) * up)\n    \n    def forward(self, z, step, alpha):\n        x = self.input(z)\n        if step == 0:\n            return self.rgbs[step](x)\n        \n        for i in range(step-1):\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            x = self.blocks[i](x)\n        \n        x1 = F.interpolate(x, scale_factor=2, mode='nearest')\n        block = self.rgbs[step](self.blocks[step-1](x1))\n        \n        up = self.rgbs[step-1](x)\n        up = F.interpolate(up, scale_factor=2, mode='nearest')\n        \n        return self.fade_in(block, up, alpha)\n\ndef minibatch_standard_deviation(x):\n    mean_std = torch.std(x, dim=0).mean()\n    mean_std = mean_std.repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n    return torch.cat([x, mean_std], dim=1)\n    \n    \nclass FromRGB(nn.Module):\n    def __init__(self, out_channels):\n        super(FromRGB, self).__init__()\n        self.rgb = WSConv2d(3, out_channels, 1)\n        \n    def forward(self, x):\n        return self.rgb(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, max_size=128, out_channels=512):\n        super(Discriminator, self).__init__()\n        \n        self.out = nn.Sequential(\n            WSConv2d(out_channels+1, out_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            WSConv2d(out_channels, out_channels, 4)\n        )\n        self.linear = nn.Linear(out_channels, 1)\n        \n        num_blocks = int(np.log2(max_size) - 2)\n        self.blocks = nn.ModuleList([])\n        self.from_rgbs = nn.ModuleList([FromRGB(out_channels)])\n        channels = out_channels\n        for i in range(num_blocks):\n            if i < 3:\n                self.from_rgbs.append(FromRGB(channels))\n                self.blocks.append(ConvBlock(channels, channels))\n            else:\n                self.from_rgbs.append(FromRGB(channels\/\/2))\n                self.blocks.append(ConvBlock(channels\/\/2, channels))\n                channels = channels \/\/ 2\n        \n        self.down = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n    def fade_in(self, block, down, alpha):\n        return torch.tanh(alpha * block + (1 - alpha) * down)\n    \n    def forward(self, x, step, alpha):\n        if step == 0:\n            x = self.from_rgbs[0](x)\n            x = minibatch_standard_deviation(x)\n            x = self.out(x).view(x.shape[0], -1)\n            return self.linear(x)\n        \n        block = self.down(self.blocks[step-1](self.from_rgbs[step](x)))\n        down = self.from_rgbs[step-1](self.down(x))\n        x = self.fade_in(block, down, alpha)\n        \n        for i in range(step-2, -1, -1):\n            x = self.down(self.blocks[i](x))\n            \n        x = minibatch_standard_deviation(x)\n        x = self.out(x).view(x.shape[0], -1)\n        return self.linear(x)\n\ndef load_dict(net, model_dict, device=None):\n    net_dict = net.state_dict()\n    state_dict = {k:v for k,v in model_dict.items() if k in net_dict.keys()}\n    net_dict.update(state_dict)\n    net.load_state_dict(net_dict)\n    if device is not None:\n        net = net.to(device)\n    return net\n","c8c9be65":"class ProWGANDLoss(nn.Module):\n    def __init__(self):\n        super(ProWGANDLoss, self).__init__()\n    \n    def gradient_penalty(self, D, real_img, fake_img, step, alpha, LAMDA=10):\n        batch_size = real_img.size(0)\n        device = real_img.device\n        gp_alpha = torch.rand(batch_size, 1)\n        gp_alpha = gp_alpha.expand(batch_size, real_img.nelement()\/\/batch_size).reshape(real_img.shape).to(device)\n        x = (gp_alpha * real_img + (1 - gp_alpha) * fake_img).requires_grad_(True).to(device)\n        out = D(x, step=step, alpha=alpha)\n\n        grad_outputs = torch.ones(out.shape).to(device)\n\n        gradients = torch.autograd.grad(outputs=out, inputs=x, grad_outputs=grad_outputs, create_graph=True, only_inputs=True)[0]\n        gradients = gradients.reshape(batch_size, -1)\n\n        return LAMDA * ((gradients.norm(2, dim=1)-1)**2).mean()\n    \n    def forward(self, G, D, z, img, step, alpha):\n        # D loss\n        loss_real = -D(img, step=step, alpha=alpha).mean()\n        fake_img = G(z, step=step, alpha=alpha)\n        loss_fake = D(fake_img.detach(), step=step, alpha=alpha).mean()\n        gp = self.gradient_penalty(D, img.detach(), fake_img.detach(), step=step, alpha=alpha, LAMDA=10)\n        \n        return loss_real + loss_fake + gp","0b49ecfc":"class Trainer:\n    def __init__(self, cfg, dataset):\n        print(f'Load config')\n        self.cfg = cfg\n        \n        print(f'Load dataset')\n        self.dataset = dataset\n        \n        print('Init G and D models')\n        self.G = Generator(z_dim=self.cfg.net.z_dim, in_channels=self.cfg.net.max_channels, max_size=self.cfg.net.max_size).to(self.cfg.train.device)\n        self.D = Discriminator(max_size=self.cfg.net.max_size, out_channels=self.cfg.net.max_channels).to(self.cfg.train.device)\n        \n        # pretrained\n        if self.cfg.net.preG_path is not None:\n            print('Load pretrained G model')\n            self.G = load_dict(self.G, torch.load(self.cfg.net.preG_path), device=self.cfg.train.device)\n        if self.cfg.net.preD_path is not None:\n            print('Load pretrained D model')\n            self.D = load_dict(self.D, torch.load(self.cfg.net.preD_path), device=self.cfg.train.device)\n        \n        print('Set optimizers and loss function')\n        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.cfg.train.lr, betas=(0.0, 0.99))\n        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.cfg.train.lr, betas=(0.0, 0.99))\n        self.lossfn_D = ProWGANDLoss().to(self.cfg.train.device)\n        \n        self.fixed_z = torch.normal(self.cfg.dataset.noise_mean, self.cfg.dataset.noise_std, size=(64, self.cfg.net.z_dim, 1, 1)).to(self.cfg.train.device)\n        \n        # output\n        self.imgs = []\n        print('Initial done')\n        \n    def get_loader(self, step):\n        img_size = 2 ** (step + 2)\n        transforms_ = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((img_size, img_size))\n        ])\n        img_dataset = self.dataset(data_dir=self.cfg.dataset.data_dir, transforms=transforms_, num=self.cfg.dataset.num_dataset)\n        return len(img_dataset), DataLoader(\n            img_dataset,\n            batch_size=self.cfg.train.batch_size_list[step],\n            shuffle=False,\n            num_workers=2\n        )\n        \n    def train(self):\n        total_steps = int(np.log2(self.cfg.net.max_size) - 1)\n        assert self.cfg.train.current_step < total_steps, f'current_step of config must less than {total_steps}'\n        print(f'Start training at steps: {self.cfg.train.current_step+1}\/{total_steps}')\n        for step in range(self.cfg.train.current_step, total_steps):\n            num_dataset, loader = self.get_loader(step)\n            start_epoch = self.cfg.train.current_epoch if self.cfg.train.current_step == step else 0\n            alpha = start_epoch * 2 \/ self.cfg.train.epoches if self.cfg.train.current_step == step else 1e-8\n            for epoch in range(start_epoch, self.cfg.train.epoches):\n                begin_time = time.time()\n                for i, img in enumerate(tqdm(loader, desc=f'epoch {epoch+1}\/{self.cfg.train.epoches}')):\n                    z = torch.normal(self.cfg.dataset.noise_mean, self.cfg.dataset.noise_std, size=(img.size(0), self.cfg.net.z_dim, 1, 1))\n                    img, z = img.to(self.cfg.train.device), z.to(self.cfg.train.device)\n\n                    # train D\n                    self.D.zero_grad()\n                    loss_D = self.lossfn_D(self.G, self.D, z, img, step, alpha)\n                    loss_D.backward()\n                    self.optimizer_D.step()\n\n                    # train G\n                    self.G.zero_grad()\n                    loss_G = -self.D(self.G(z, step=step, alpha=alpha), step=step, alpha=alpha).mean()\n                    loss_G.backward()\n                    self.optimizer_G.step()\n\n                    # smooth increase alpha\n                    # it reaches 1 after half of epoches\n                    alpha += 2 * img.size(0) \/ (self.cfg.train.epoches * num_dataset)\n                    alpha = min(alpha, 1)\n                end_time = time.time()\n                \n                self.log(begin_time, end_time, step, total_steps, epoch, alpha, loss_G, loss_D)\n                self.generate_img(step, alpha)\n        print('Train done')\n                    \n    def generate_img(self, step, alpha):\n        with torch.no_grad():\n            out_img = make_grid(self.G(self.fixed_z, step=step, alpha=alpha), padding=2, normalize=True).cpu().permute(1,2,0).numpy()\n        out_img = (out_img - np.min(out_img)) \/ (np.max(out_img) - np.min(out_img)).astype(np.float)\n        self.imgs.append(out_img)\n    \n    def log(self, begin_time, end_time, step, total_steps, epoch, alpha, loss_G, loss_D):\n        out_str = '[total time: {:.5f}s] '.format(end_time-begin_time) + f'[Step: {step+1}\/{total_steps}] [Epoch: {epoch+1}\/{self.cfg.train.epoches}] [alpha: {format(alpha, \".2e\")}] [G loss: {loss_G.item()}] [D loss: {loss_D.item()}]'\n        print(out_str)\n        ","1c3e854f":"class Config:\n    def __init__(self, z_dim, max_channels, max_size, preG_path, preD_path, data_dir, num_dataset, \\\n                 noise_mean, noise_std, device, lr, batch_size_list, epoches, current_step, current_epoch):\n        self.net = type('', (), {})()\n        self.net.z_dim = z_dim\n        self.net.max_channels = max_channels\n        self.net.max_size = max_size\n        self.net.preG_path = preG_path\n        self.net.preD_path = preD_path\n        \n        self.dataset = type('', (), {})()\n        self.dataset.data_dir = data_dir\n        self.dataset.num_dataset = num_dataset\n        self.dataset.noise_mean = noise_mean\n        self.dataset.noise_std = noise_std\n        \n        self.train = type('', (), {})()\n        self.train.device = device\n        self.train.lr = lr\n        self.train.batch_size_list = batch_size_list\n        self.train.epoches = epoches\n        self.train.current_step = current_step\n        self.train.current_epoch = current_epoch\n        ","6ec9ea33":"cfg = Config(\n    z_dim = 512, max_channels = 512, max_size = 64, \\\n    preG_path = '..\/input\/progan-pretrained-models\/pokemon_G_step4_dict.pth', preD_path = '..\/input\/progan-pretrained-models\/pokemon_D_step4_dict.pth', \\\n    data_dir = '..\/input\/pokemon-mugshots-from-super-mystery-dungeon\/smd\/smd', num_dataset = 2000, \\\n    noise_mean = 0, noise_std = 1, device = 'cuda:0', lr = 0.001, batch_size_list = 3*[1024, 256, 64, 16, 4], \\\n    epoches = 100, current_step = 4, current_epoch = 95\n)\n\ntrainer = Trainer(cfg=cfg, dataset=PokemonDataset)\ntrainer.train()","3b1028bb":"rc('animation', html='jshtml')\nrc('animation', embed_limit=100.0)\ndef create_animation(ims):\n    fig = plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    im = plt.imshow(ims[0])\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 500)","806de8fe":"create_animation(np.stack(trainer.imgs))","f61c835e":"# Results","78dceaff":"# Train with pretrained model","b6c1503e":"# WGAN Loss","38245d92":"**Because of the time constraints of kaggle, I pre-trained with all the images and uploaded the training results to the [progan](https:\/\/www.kaggle.com\/lmyybh\/progan-pretrained-models) dataset**\n\n\n**Here, on the basis of the pre-trained model, I only use 2000 images in 5 epoches to demonstrate the training effect**","e0c18da3":"# Model","68e22a9e":"# Dataset"}}