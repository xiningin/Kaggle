{"cell_type":{"ed67e53c":"code","a274c955":"code","38b73020":"code","c7911761":"code","ddf2ee2a":"code","3f5e68ef":"code","844d72b7":"markdown"},"source":{"ed67e53c":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n\n# Path of the file to read\nmelb_housing = pd.read_csv('..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')","a274c955":"melb_housing.head()","38b73020":"melb_housing.describe()","c7911761":"features = ['Rooms', 'Bathroom', 'YearBuilt', 'Landsize']\nX = melb_housing[features]\n\nnull_columns = X.columns[X.isnull().any()]\nX[null_columns].isnull().sum()","ddf2ee2a":"# melb_housing.Price.dropna()\n# melb_housing.Bathroom.dropna()\n# melb_housing.YearBuilt.dropna()\n# melb_housing.Landsize.dropna()\nX = X.dropna()\ny = y.dropna()","3f5e68ef":"# Create predicted variable and features\nfeatures = ['Rooms', 'Bathroom', 'YearBuilt', 'Landsize', 'Price']\ndrop = melb_housing[features]\ndrop = drop.dropna()\nfeatures = ['Rooms', 'Bathroom', 'YearBuilt', 'Landsize']\ny = drop.Price\nX = drop[features]\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Group numeric and non-numeric into lists\nnumeric = make_column_selector(dtype_include = \"number\")\nnon_numeric = make_column_selector(dtype_exclude = 'number')\n\n# Create preprocessor for OH encoder and Imputer for missing numeric values\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric), # select all numeric columns, impute mean and scale the data\n    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), non_numeric) # select all non numeric columns, impute most frequent value and OneHotEncode\n) \n# Make pipeline for with model\npipe = make_pipeline(preprocessor, DecisionTreeRegressor(random_state=0))\n\npipe.fit(train_X, train_y)\ny_pred = pipe.predict(val_X)\nval_mae = mean_absolute_error(y_pred, val_y)\nval_mae","844d72b7":"## Decision Trees\nDecison Tress can give boolean predictions like Logistic Regression but also do the same with numeric data. Each branch down the tree narrows the search.\n\nLingo:\n- Top box is the Root Node. Have arrows pointing from them\n- Middle boxes are Nodes or Internal Nodes. Have arrows pointing from and to them.\n- Bottom boxes are the Leaf Nodes or Leaves. Have arrows pointing to them.\n\n![decison-tree](https:\/\/www.displayr.com\/wp-content\/uploads\/2018\/07\/what-is-a-decision-tree.png)\n\n### Example\nGoing from columns and rows to a decision tree. \n\nWe're trying to predict whether a patient has heart disease based on a number of parameters. \n\nParameters:\n1. Chest Pain\n2. Good Circulation\n3. Blocked Arteries\n\nThe decision tree does this by figuring out which parameter should be the root node of the decision tree.\n\nSteps:\n- The tree goes through each row for the first parameter (chest pain)\n- Checks if whether chest pain and heart disease are true\n- Stores the result in a node\n- Repeats for each row\n- Moves onto remaining parameters and repeats earlier steps\n\nAfter completing the steps above, there are no leaf nodes which 100% indicate heart disease so they are all considered **impure**. To find the root node now, we need to find a way of measuring impurity.\n\n### Gini Method\nThis is a way of measuring the impurity of leaf nodes.\n\n$$ Gini(t) = 1 - \\sum_{i=1}^{j} P(i | t)^2 $$\n\n- j is the length of the set\n- i is yes\n- t is no\n\nFor this example, one of the leaves has a **105 with heart disease and chest pain** while having **39 with only chest pain**.\n\nThe probability of yes:\n$$ = \\frac{105}{105 + 39} ^2 = 0.5329 $$\n\nThe probability of no:\n$$ = \\frac{39}{105 + 39} ^2 = 0.0729 $$\n\nGini Impurity is these taken away:\n$$ 1 - 0.0729 - 0.5329 = 0.395 $$\n\nThe same is repeated for another leaf node which has a Gini Impurity of **0.336**. The issue here is that the first leaf node has 144 patients and the second one has 159. The total Gini impurity for using Chest Pain is the **weighted mean of the leaf node impurities**.\n\n$$ \\frac{144}{144 + 159} 0.395 + \\frac{159}{144 + 159} 0.336 = 0.364 $$\n\nWhen it comes to Good Blood Circulation, 0.36 was the Gini impurity. The lower the impurity the better suited is for the root node. \n\nGood Blood Circulation will now be used for the root node. Now the same method is applied but with Good Blood Circulation at the top of the tree.\n\n1. Calculate all of the Gini impurities\n2. The lowest impurity becomes a leaf node\n3. If separating the data results in an improvement, pick the separation with the lowest value"}}