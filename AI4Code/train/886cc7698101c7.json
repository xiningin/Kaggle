{"cell_type":{"a17c822a":"code","b1b7b9e2":"code","a7154340":"code","c999942b":"code","50fff5dc":"code","84f71908":"code","5de97982":"code","957be4cd":"code","e8403a72":"code","fd3bbdaa":"code","b304952f":"code","8d0eb357":"code","15d35f14":"code","1362082e":"code","1e2b44da":"code","baeb6427":"markdown","826665c4":"markdown","957a995c":"markdown"},"source":{"a17c822a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","b1b7b9e2":"import sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nnp.random.seed(697)","a7154340":"from keras.callbacks import Callback\nfrom keras import backend as K\n\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    The amplitude of the cycle can be scaled on a per-iteration or\n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    For more detail, please see paper.\n    # Example for CIFAR-10 w\/ batch size 100:\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    # References\n      - [Cyclical Learning Rates for Training Neural Networks](\n      https:\/\/arxiv.org\/abs\/1506.01186)\n    \"\"\"\n\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 \/ (2.**(x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations \/ (2 * self.step_size))\n        x = np.abs(self.clr_iterations \/ self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","c999942b":"train_df = pd.read_csv('..\/input\/train.csv', index_col=0)\ntest_df = pd.read_csv('..\/input\/test.csv', index_col=0)","50fff5dc":"train_df.shape","84f71908":"X = train_df.drop(['target'],axis=1)\ny = train_df['target']","5de97982":"X_test = test_df","957be4cd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_s=scaler.fit_transform(X)\nX_test_s=scaler.transform(X_test)","e8403a72":"X=pd.DataFrame(X_s)\nX_test=pd.DataFrame(X_test_s)","fd3bbdaa":"X_test.index=test_df.index","b304952f":"seed = 7\nnp.random.seed(seed)","8d0eb357":"# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)","15d35f14":"adam = optimizers.adam\nmodel = Sequential()\nmodel.add(Dense(64, input_shape=(200,1),\n                kernel_initializer='normal',\n                activation=\"relu\"))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=['accuracy'])\n\nclr = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=400., mode='triangular')\n","1362082e":"preds = []\nc = 0\noof_preds = np.zeros((len(X), 1))\n\nfor train, valid in cv.split(X, y):\n    print(\"VAL %s\" % c)\n    X_train = np.reshape(X.iloc[train].values, (-1, 200, 1))\n    y_train_ = y.iloc[train].values\n    X_valid = np.reshape(X.iloc[valid].values, (-1, 200, 1))\n    y_valid = y.iloc[valid].values\n    early = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=20)\n    model.fit(X_train, y_train_, validation_data=(X_valid, y_valid), epochs=200, verbose=2, batch_size=1024,\n              callbacks=[early,clr])\n    X_test1 = np.reshape(X_test.values, (200000, 200, 1))\n    curr_preds = model.predict(X_test1, batch_size=1024)\n    oof_preds[valid] = model.predict(X_valid)\n    preds.append(curr_preds)\n    c += 1\nauc = roc_auc_score(y, oof_preds)\nprint(\"CV_AUC: {}\".format(auc))\n","1e2b44da":"preds = np.asarray(preds)\npreds = preds.reshape((5, 200000))\npreds_final = np.mean(preds.T, axis=1)\nsubmission = pd.read_csv('.\/..\/input\/sample_submission.csv')\nsubmission['target'] = preds_final\nsubmission.to_csv('submission.csv', index=False)","baeb6427":"# Please upvote, if you find this kernel interesting","826665c4":"# Few lessons learnt while working on this competetion. Might be helpful for beginners.\n\n\n1) Initially predict hard values 0 or 1 and roc_auc scores ended up in 65 - 70. Came to know that we need to use probabilities for AUC, which increased the score to 86.\n\n2) Changing the input data dimension from 1D --> 2D improved score a lot. Got this idea from this [kernal](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82863)\n\n3) Dimensionality reduction: If we want to visualize 2 variables(2 dimensional) we can do it using any type of plot, 1 variable at x-axis and another in y-axis. But, what if want to visualize 200 variables (200 dimension)? we can use dimensionality reduction techniques like PCA,t-SNE, UMAP for the same. These techniques intelligently summarizes\/group information related to multi dimension to the required low dimension. Unfortunately this techniques didn't  help much in this competetion. [PCA](https:\/\/www.kaggle.com\/sandeep8530\/pca-for-santander)\n\n4) Cyclelr: Learning rate is one of the important hyperparameters. Varying learning rate helps in fast and effective model. [Reference](https:\/\/github.com\/bckenstler\/CLR) \n\n5) K-fold: Came to know that apart from using it for cross_val_score to know the robustness of the model, it can used to predict values at each fold which improves the score(kind of ensemble).","957a995c":"# Please upvote, if you find this kernel interesting"}}