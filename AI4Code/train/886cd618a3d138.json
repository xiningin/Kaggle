{"cell_type":{"85b2ed2e":"code","34043f42":"code","87ff113f":"code","bd886aaf":"code","7fe7da50":"code","b5dbf240":"code","30bfa0a8":"code","096f3a84":"code","814d0deb":"code","9cd3fa86":"code","3260ac1f":"code","5a7774ca":"code","5f4bfb88":"code","5009d6e5":"code","ae39646f":"code","41f008a6":"code","51200f19":"code","9b3c3399":"code","303a2716":"code","2f7cee69":"code","e5d9e695":"code","b22517f1":"code","dbc6d072":"code","ba2da531":"code","aee3d38e":"code","ead0a573":"code","4250a27e":"code","73e21790":"code","aacb2d5b":"code","cf08194e":"code","b8d44b39":"code","25fa2974":"code","16e89c7e":"code","7dacc551":"code","09ddb23c":"code","721b6ec3":"code","e90cccf7":"code","97b1165d":"code","bf060cc5":"code","299ce428":"code","22fcf1cb":"code","9aceca20":"markdown","e201ec41":"markdown","3f89497f":"markdown","848e5aaa":"markdown","96cf4f4a":"markdown","9dd3255e":"markdown","7523b935":"markdown","834923a6":"markdown","4760f355":"markdown","1c88d19b":"markdown","d0118851":"markdown","90005473":"markdown","55d9f36e":"markdown","4377f367":"markdown","e1a63bf3":"markdown","7b5a805c":"markdown","4c61b9fe":"markdown","22c806e6":"markdown","c21c85fa":"markdown","627dbf33":"markdown","7451d584":"markdown","74e91ab0":"markdown","f8c3f717":"markdown","5fe3e24b":"markdown","79f584ab":"markdown","990973ca":"markdown","ece2be64":"markdown","2ee1b76f":"markdown","fc088386":"markdown","7c10f248":"markdown","86b81a1d":"markdown","3a488cd1":"markdown","cb7871ba":"markdown","bc08eecc":"markdown","55c9a4f9":"markdown","96502244":"markdown","3e812d0b":"markdown","4da9fb8c":"markdown","4d5d5725":"markdown","f96fecf0":"markdown","cfe93fbd":"markdown","a6539195":"markdown","887dfc5f":"markdown","77e2803f":"markdown"},"source":{"85b2ed2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","34043f42":"# Total Samples Available\nprint('Train Images = ',len(os.listdir('..\/input\/train')))\nprint('Test Images = ',len(os.listdir('..\/input\/test')))","87ff113f":"df = pd.read_csv('..\/input\/train_labels.csv')\nprint('Shape of DataFrame',df.shape)\ndf.head()","bd886aaf":"TRAIN_DIR = '..\/input\/train\/'","7fe7da50":"fig = plt.figure(figsize = (20,8))\nindex = 1\nfor i in np.random.randint(low = 0, high = df.shape[0], size = 10):\n    file = TRAIN_DIR + df.iloc[i]['id'] + '.tif'\n    img = cv2.imread(file)\n    ax = fig.add_subplot(2, 5, index)\n    ax.imshow(img, cmap = 'gray')\n    index = index + 1\n    color = ['green' if df.iloc[i].label == 1 else 'red'][0]\n    ax.set_title(df.iloc[i].label, fontsize = 18, color = color)\nplt.tight_layout()\nplt.show()","b5dbf240":"# removing this image because it caused a training error previously\ndf[df['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing this image because it's black\ndf[df['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\ndf.head()","30bfa0a8":"fig = plt.figure(figsize = (6,6)) \nax = sns.countplot(df.label).set_title('Label Counts', fontsize = 18)\nplt.annotate(df.label.value_counts()[0],\n            xy = (0,df.label.value_counts()[0] + 2000),\n            va = 'bottom',\n            ha = 'center',\n            fontsize = 12)\nplt.annotate(df.label.value_counts()[1],\n            xy = (1,df.label.value_counts()[1] + 2000),\n            va = 'bottom',\n            ha = 'center',\n            fontsize = 12)\nplt.ylim(0,150000)\nplt.ylabel('Count', fontsize = 16)\nplt.xlabel('Labels', fontsize = 16)\nplt.show()","096f3a84":"SAMPLE_SIZE = 80000\n# take a random sample of class 0 with size equal to num samples in class 1\ndf_0 = df[df['label'] == 0].sample(SAMPLE_SIZE, random_state = 0)\n# filter out class 1\ndf_1 = df[df['label'] == 1].sample(SAMPLE_SIZE, random_state = 0)\n\n# concat the dataframes\ndf_train = pd.concat([df_0, df_1], axis = 0).reset_index(drop = True)\n# shuffle\ndf_train = shuffle(df_train)\n\ndf_train['label'].value_counts()","814d0deb":"# train_test_split\n# stratify=y creates a balanced validation set.\ny = df_train['label']\n\ndf_train, df_val = train_test_split(df_train, test_size = 0.1, random_state = 0, stratify = y)","9cd3fa86":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#Folder Structure\n\n'''\n    * base_dir\n        |-- train_dir\n            |-- 0   #No Tumor\n            |-- 1   #Has Tumor\n        |-- val_dir\n            |-- 0\n            |-- 1\n'''\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n# create new folders inside train_dir\nno_tumor = os.path.join(train_dir, '0')\nos.mkdir(no_tumor)\nhas_tumor = os.path.join(train_dir, '1')\nos.mkdir(has_tumor)\n\n\n# create new folders inside val_dir\nno_tumor = os.path.join(val_dir, '0')\nos.mkdir(no_tumor)\nhas_tumor = os.path.join(val_dir, '1')\nos.mkdir(has_tumor)\n\n\nprint(os.listdir('base_dir\/train_dir'))\nprint(os.listdir('base_dir\/val_dir'))","3260ac1f":"# Set the id as the index in df_data\ndf.set_index('id', inplace=True)\n\n# Get a list of train and val images\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    file_name = image + '.tif'\n    # get the label for a certain image\n    target = df.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = '0'\n    elif target == 1:\n        label = '1'\n    \n    # source path to image\n    src = os.path.join('..\/input\/train', file_name)\n    # destination path to image\n    dest = os.path.join(train_dir, label, file_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dest)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    file_name = image + '.tif'\n    # get the label for a certain image\n    target = df.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = '0'\n    elif target == 1:\n        label = '1'\n    \n\n    # source path to image\n    src = os.path.join('..\/input\/train', file_name)\n    # destination path to image\n    dest = os.path.join(val_dir, label, file_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dest)","5a7774ca":"print(len(os.listdir('base_dir\/train_dir\/0')))\nprint(len(os.listdir('base_dir\/train_dir\/1')))","5f4bfb88":"from keras.preprocessing.image import ImageDataGenerator\nIMAGE_SIZE = 96\ntrain_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\ntest_path = '..\/input\/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 32 #10\nval_batch_size = 32 #10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\n\n\ndatagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","5009d6e5":"#Import Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import SeparableConv2D\nfrom keras.layers.core import Activation","ae39646f":"class Net:\n    @staticmethod\n    def build(width, height, depth, classes):\n            \n            #initializa model\n            model = Sequential()\n            \n            inputShape = (height, width, depth)\n            \n            #Add First Layer CONV => ReLU => Pooling\n            model.add(Conv2D(filters = 32, kernel_size = (5,5), padding=\"same\", activation='relu', input_shape= inputShape))\n            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(Conv2D(filters = 32, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n            model.add(Dropout(0.2))\n                      \n            #Add Second Layer CONV => ReLU => Pooling\n            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n            model.add(Dropout(0.2))\n            \n            #Add Third Layer CONV => ReLU => Pooling\n            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(Conv2D(filters = 128, kernel_size = (3,3), padding=\"same\", activation='relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n            model.add(Dropout(0.25))\n            \n            \n            #FC => ReLU\n            model.add(Flatten())\n            model.add(Dense(units = 500, activation = 'relu'))\n            model.add(Dropout(0.2))\n            #FC => Output\n            model.add(Dense(classes, activation='softmax'))\n            \n            model.summary()\n            \n            return model","41f008a6":"class CancerNet:\n    @staticmethod\n    def build(width, height, depth, classes):\n        \n        # initialize the model along with the input shape to be\n        # \"channels last\" and the channels dimension itself\n        model = Sequential()\n        inputShape = (height, width, depth)\n        chanDim = -1\n        \n        # CONV => RELU => POOL\n        model.add(SeparableConv2D(32, (3, 3), padding=\"same\",input_shape = inputShape))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        # (CONV => RELU => POOL) * 2\n        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        # (CONV => RELU => POOL) * 3\n        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # first (and only) set of FC => RELU layers\n        model.add(Flatten())\n        model.add(Dense(256))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.2))\n\n        # softmax classifier\n        model.add(Dense(classes))\n        model.add(Activation(\"softmax\"))\n        \n        model.summary()\n\n        # return the constructed network architecture\n        return model","51200f19":"model = Net.build(width = 96, height = 96, depth = 3, classes = 2)\n#model = CancerNet.build(width = 96, height = 96, depth = 3, classes = 2)\nfrom keras.optimizers import SGD, Adam, Adagrad\n#Edit:: Adagrad(lr=1e-2, decay= 1e-2\/10) was used previous;y\nmodel.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])","9b3c3399":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","303a2716":"# !wget 'https:\/\/www.kaggleusercontent.com\/kf\/10003609\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lla3iIArVKorEUKxxzzMxg.Ju_WeWrdCHBebCvN-AdSwFCZRJIm1Ru5gJkP-v0jz212zkjh0ojBQ1uHu7Cv7eBXHx8xrBXQHAJpdEy8TQ59Z26Onub-OkbUbWmto-FcjuRGJfFHlxnehCU0fLVB3ZTye4beLcsar4TV_VlKHOic4QP0MW7ajdUimXs09qZhpwI.oZo9D1Huxk091PMK1QJslQ\/checkpoint.h5'\n# model.load_weights('checkpoint.h5')","2f7cee69":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfilepath = \"checkpoint.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose = 1, \n                             save_best_only = True, mode = 'max') #Save Best Epoch\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor = 0.5, patience = 2, verbose = 1, mode = 'max', min_lr = 0.00001)                              \ncallbacks_list = [checkpoint, reduce_lr] # LR Scheduler Used here\n\nhistory = model.fit_generator(train_gen, steps_per_epoch = train_steps, \n                    validation_data = val_gen,\n                    validation_steps = val_steps,\n                    epochs = 11,\n                    verbose = 1,\n                    callbacks = callbacks_list)","e5d9e695":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='best')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='best')\nplt.show()","b22517f1":"# Here the best epoch will be used.\nmodel.load_weights('checkpoint.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, steps=len(df_val))\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","dbc6d072":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","ba2da531":"# Put the predictions into a dataframe.\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\ndf_preds.head()","aee3d38e":"# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels as probabilities\ny_pred = df_preds['has_tumor']","ead0a573":"from sklearn.metrics import roc_auc_score, roc_curve, auc\nprint('ROC AUC Score = ',roc_auc_score(y_true, y_pred))","4250a27e":"fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\nauc_keras = auc(fpr_keras, tpr_keras)","73e21790":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='area = {:.2f}'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","aacb2d5b":"from sklearn.metrics import confusion_matrix\n# For this to work we need y_pred as binary labels not as probabilities\ny_pred_binary = predictions.argmax(axis=1)\ncm = confusion_matrix(y_true, y_pred_binary)\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True,\n                               cmap = 'Dark2')\nplt.show()","cf08194e":"from sklearn.metrics import classification_report\n# Generate a classification report\n\nreport = classification_report(y_true, y_pred_binary, target_names = ['no_tumor', 'has_tumor'])\nprint(report)","b8d44b39":"shutil.rmtree('base_dir')","25fa2974":"#Folder Structure\n\n'''\n    * test_dir\n        |-- test_images\n'''\n\n# We will be feeding test images from a folder into predict_generator().\n\n# create test_dir\ntest_dir = 'test_dir'\nos.mkdir(test_dir)\n    \n# create test_images inside test_dir\ntest_images = os.path.join(test_dir, 'test_images')\nos.mkdir(test_images)\n\n# check that the directory we created exists\nos.listdir('test_dir')","16e89c7e":"# Transfer the test images into image_dir\ntest_list = os.listdir('..\/input\/test')\n\nfor image in test_list:    \n    fname = image\n    # source path to image\n    src = os.path.join('..\/input\/test', fname)\n    # destination path to image\n    dst = os.path.join(test_images, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\nprint('Total Test Images = ',len(os.listdir('test_dir\/test_images')))","7dacc551":"test_path ='test_dir'\ntest_gen = datagen.flow_from_directory(test_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","09ddb23c":"num_test_images = 57458 #len(os.listdir('test_dir\/test_images')\n\npredictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)","721b6ec3":"if predictions.shape[0] == num_test_images:\n    print('All Predictions Done!')\nelse:\n    print('Error!')","e90cccf7":"# Put the predictions into a dataframe\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor', 'has_tumor'])\ndf_preds.head()","97b1165d":"# This outputs the file names in the sequence in which the generator processed the test images.\ntest_filenames = test_gen.filenames\n\n# add the filenames to the dataframe\ndf_preds['file_names'] = test_filenames\n\n# Create an id column\n# A file name now has this format: \n# images\/00006537328c33e284c973d7b39d340809f7271b.tif\n\n# This function will extract the id:\n# 00006537328c33e284c973d7b39d340809f7271b\ndef extract_id(x):\n    \n    # split into a list\n    a = x.split('\/')\n    # split into a list\n    b = a[1].split('.')\n    extracted_id = b[0]\n    \n    return extracted_id\n\ndf_preds['id'] = df_preds['file_names'].apply(extract_id)\n\ndf_preds.head()","bf060cc5":"# Get the predicted labels.\n# We were asked to predict a probability that the image has tumor tissue\ny_pred = df_preds['has_tumor']\n\n# get the id column\nimage_id = df_preds['id']","299ce428":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('submission.csv', columns=['label'])","22fcf1cb":"# Delete the test_dir directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('test_dir')","9aceca20":"## **Classification Report**","e201ec41":"**Let's plot our ROC Curve**","3f89497f":"### **Validate the model (Measure Model Performance)**","848e5aaa":"**Move the Test images into a directory 'test_dir'**","96cf4f4a":"**The images are labeled as 0 or 1, where 0 = No Tumor Tissue and 1 = Has Tumor Tissue(s)**","9dd3255e":"## **Model Architecture**(will be shown here when notebook will run, otherwise see Output Visualization Section)  \n<img src='.\/model.png' alt = 'Run_the_notebook_to see_model'>","7523b935":"### Total Samples Available","834923a6":"**Transfer the respective images into their respective folders**","4760f355":"# **Histopathologic Cancer Detection**\n***Identification of Metastatic Tissue in Histopathologic Scans of Lymph Node Sections***    \n\n### **By [Soumya Ranjan Behera](https:\/\/www.linkedin.com\/in\/soumya044)**","1c88d19b":"Here the **Label-1** is **60%** and **Label-0** is **40%** of the whole train images. There is a little imbalance here which we can rectify to get better performance.","d0118851":"## **Major Outcomes:**  \nThe presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic (ROC) curve analysis.","90005473":"## **Confusion Matrix**","55d9f36e":"### **Make Submission File**","4377f367":"**Extract ID field from Test Image file names**","e1a63bf3":"### **Author: [Soumya Ranjan Behera](https:\/\/www.linkedin.com\/in\/soumya044)**   \nFeel free to connect [LinkedIn](https:\/\/www.linkedin.com\/in\/soumya044)","7b5a805c":"### **Split into Train and Validation Sets**","4c61b9fe":"***Remove the test_dir to free up memory and commit our kernel successfully!***","22c806e6":"**Creating Directory Structure**","c21c85fa":"**Submission File :** [Download Link](.\/submission.csv)","627dbf33":"### **Define LR Scheduler and Save Model Checkpoint on Maximum Validation Accuracy**","7451d584":"# **References:**  \n1. B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. \"Rotation Equivariant CNNs for Digital Pathology\". [arXiv:1806.03962](http:\/\/arxiv.org\/abs\/1806.03962)\n\n2.  Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199\u20132210. [doi:jama.2017.14585](https:\/\/doi.org\/10.1001\/jama.2017.14585)\n\n3. [Marsh's Kernel \" how-to-use-160-000-images-without-crashing \" ](https:\/\/www.kaggle.com\/vbookshelf\/cnn-how-to-use-160-000-images-without-crashing)\n\n4. [Breast cancer classification with Keras and Deep Learning (CancerNet Architecture) by Adrian Rosebrock](https:\/\/www.pyimagesearch.com\/2019\/02\/18\/breast-cancer-classification-with-keras-and-deep-learning\/)\n\n5. Original Data Set [PatchCamelyon (PCam) ](https:\/\/www.kaggle.com\/c\/histopathologic-cancer-detection)","74e91ab0":"# **Make Test Predictions for Kaggle**","f8c3f717":"### See the distribution of Train Labels","5fe3e24b":"### **Load the saved weights**","79f584ab":"## **Abstract**\n**Importance:**  \nApplication of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.\n\n**Objective:**  \nAssess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin\u2013stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists\u2019 diagnoses in a diagnostic setting.","990973ca":"### **Visualize our model architecture**","ece2be64":"# **Model Training**","2ee1b76f":"# **Conclusion**  \n**The proposed deep learning model works good under Kaggle environments. But we can use other deeper or pre-trained models with higher availability of resources.**  \n**These deep learning models may have achieved better diagnostic performance than real pathologists, but this will require evaluation in a clinical setting to measure its utility in diagnosis of lymph node metastases in tissue sections**","fc088386":"# **Exploratory Data Analysis**","7c10f248":"***Remove our base_dir to free up some memory***","86b81a1d":"# **Model Evaluation**","3a488cd1":"We are getting around **0.9** ROC AUC value, which is a quite good performance.","cb7871ba":"### **Take 80K images from both categories**","bc08eecc":"# **Create our Model (CancerNet)** ","55c9a4f9":"### **Put the two types of images into two folder to help Keras ImageGenerator**","96502244":"We can determine our epochs based on the convergence of below graphs.","3e812d0b":"**As per this [kernel](https:\/\/www.kaggle.com\/vbookshelf\/cnn-how-to-use-160-000-images-without-crashing) we can balance the labels using Random Sampling and reduce the memory usage or potential crash.**","4da9fb8c":"### **Compare Training and Validation Metrics**","4d5d5725":"## **About the Data Set**  \nThe data for this kernel is a slightly modified version of the [PatchCamelyon (PCam)](https:\/\/github.com\/basveeling\/pcam) benchmark dataset. The original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates.  \n\nThe PatchCamelyon benchmark is a new and challenging image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than imagenet, trainable on a single GPU.  \n\nPCam packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability.","f96fecf0":"# **Feature Engineering**","cfe93fbd":"**Recall** = The classifier's ability to detect a given class. It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).  \n**Precision** = Given a class prediction from a classifier, how likely is it to be correct? It is the number of correct positive results divided by the number of positive results predicted by the classifier.  \n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.  \n\nMore about Evaluation Metrics [here](https:\/\/towardsdatascience.com\/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234)\n\nFrom the confusion matrix and classification report we see that our model is equally good at detecting both classes.","a6539195":"### **Visualize some Train Images**","887dfc5f":"**Specify optimizer and loss function**","77e2803f":"### Create a DataFrame of all Train Image Labels"}}