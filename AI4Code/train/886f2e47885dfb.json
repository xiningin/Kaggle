{"cell_type":{"d0ac6afd":"code","99be1c96":"code","dcbdf343":"code","8fe62916":"code","53c042ac":"code","39f19198":"code","24f96b6c":"code","b63d1b72":"code","d98776a1":"code","b4e0b520":"code","49356989":"code","60e18528":"code","68fdf1db":"code","a092492d":"code","99f3dc2d":"code","78e98350":"code","b3074ac9":"code","0cfef2f3":"code","e472458c":"code","5bd8b1b9":"code","2ced4649":"code","4f32cc62":"code","19d62dff":"code","4c5b79d5":"code","74c419ad":"code","7461b709":"code","ed2bbb31":"code","06c19268":"code","77fc189f":"code","73735a7d":"code","a8c9be73":"code","0aba3b0c":"code","4aaac9de":"code","b1187fbb":"code","544b3b24":"code","598de12c":"code","30d209c2":"code","4e13cd92":"code","41259e87":"code","6baef8c7":"code","d783b9f0":"code","2836a68b":"code","5ef5c96e":"code","04d1cf7c":"code","0cb37e32":"code","a0fa26ad":"code","cf28fce0":"code","64def876":"code","b230bf56":"code","4d5f26fd":"code","1847fae7":"code","6bee61b6":"code","99bff70e":"markdown"},"source":{"d0ac6afd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","99be1c96":"import pandas as pd\nimport nltk\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nstop=set(stopwords.words('english'))\nfrom collections import  Counter\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Bidirectional, LeakyReLU, Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\nsub = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","dcbdf343":"plt.rcParams['patch.force_edgecolor'] = True\nplt.rcParams['figure.figsize'] = (12,7)","8fe62916":"nltk.download('stopwords')","53c042ac":"train.head()","39f19198":"print('Number of rows in train dataset are: {} \\nNumber of columns in train dataset are: {} '\n      .format(train.shape[0],train.shape[1]))","24f96b6c":"train['target'].value_counts().plot(kind = 'bar')\nplt.xlabel('Targets')\nplt.ylabel('Count of Targets')\nplt.xticks(rotation=0)\nplt.plot()","b63d1b72":"# determining character in tweets\nfig,(ax1,ax2) = plt.subplots(1,2)\ntweets = train[train['target'] ==1]['text'].str.len()\nax1.hist(tweets)\nax1.set_title('Disaster Tweets')\ntweets = train[train['target']==0]['text'].str.len()\nax2.hist(tweets)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle('Character in Tweets')\nplt.show()","d98776a1":"# Disaster Tweets tend to be a litter shorter than non-disaster tweets","b4e0b520":"# Checking number of words in a tweet\n\nfig,(ax1,ax2) = plt.subplots(1,2)\ntweet_len = train[train['target']==1]['text'].str.split().map(lambda x:len(x))\nax1.set_title('Disaster Tweets')\nax1.hist(tweet_len)\ntweet_len = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle('Words in Tweets')\nplt.show()","49356989":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","60e18528":"# First we will analyze tweets with class 0.\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","68fdf1db":"x,y=zip(*top)\nplt.bar(x,y)","a092492d":"# Now,we will analyze tweets with class 1\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","99f3dc2d":"\nx,y=zip(*top)\nplt.bar(x,y)","78e98350":"# analysing punctuation for class 0\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","b3074ac9":"# analysing punctuation for class 1\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","0cfef2f3":"# N-gram analysis\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","e472458c":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'])[:10]\nx,y=zip(*top_tweet_bigrams)\nplt.bar(x,y)","5bd8b1b9":"#Cleaning dataset","2ced4649":"df=pd.concat([train,test])\ndf.shape","4f32cc62":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","19d62dff":"df['text']=df['text'].apply(lambda x : remove_URL(x))","4c5b79d5":"#removing html tags","74c419ad":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","7461b709":"df['text']=df['text'].apply(lambda x : remove_html(x))","ed2bbb31":"#Removing emojis","06c19268":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","77fc189f":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","73735a7d":"#Removing punctuations","a8c9be73":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","0aba3b0c":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","4aaac9de":"#Correcting spellings","b1187fbb":"# spell = SpellChecker()\n# def correct_spellings(text):\n#     corrected_text = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_text.append(spell.correction(word))\n#         else:\n#             corrected_text.append(word)\n#     return \" \".join(corrected_text)","544b3b24":"# df['text']=df['text'].apply(lambda x : correct_spellings(x))","598de12c":"# 100d Gove Vecorisation","30d209c2":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","4e13cd92":"corpus=create_corpus(df)","41259e87":"embedding_dict={}\nwith open('\/kaggle\/input\/glove6b200d\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","6baef8c7":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","d783b9f0":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","2836a68b":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","5ef5c96e":"model=Sequential()\n\nembedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(8))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(rate = 0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","04d1cf7c":"model.summary()","0cb37e32":"train1=tweet_pad[:train.shape[0]]\ntest1=tweet_pad[train.shape[0]:]","a0fa26ad":"X_train,X_test,y_train,y_test=train_test_split(train1,train['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","cf28fce0":"early = EarlyStopping(mode = 'min',monitor='X_test',patience=3)","64def876":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=1,\n                 callbacks=[early])","b230bf56":"pre = model.predict(test1)","4d5f26fd":"pre = np.round(pre).astype(int).reshape(3263)","1847fae7":"sub=pd.DataFrame({'id':sub['id'].values.tolist(),'target':pre})\nsub.to_csv('submission.csv',index=False)","6bee61b6":"from IPython.display import FileLink\nFileLink(r'submission.csv')","99bff70e":"Removing URLs"}}