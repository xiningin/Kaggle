{"cell_type":{"eb70c66c":"code","b5428d2c":"code","64842ffc":"code","59e5c9cc":"code","1c629671":"code","95e711f0":"code","9fc77044":"code","c80ce88d":"code","4a184c62":"code","7f54762c":"code","6b925b06":"code","b71dfaee":"code","3220dd02":"code","e70db754":"code","81b8acbc":"code","72e12390":"code","1071548e":"code","47ae1899":"code","3e7322f9":"code","b1226d84":"markdown","923ba5b5":"markdown","f38c9f28":"markdown"},"source":{"eb70c66c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nprint(\"xgb version: {}\". format(xgb.__version__))\nfrom xgboost import plot_importance\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV \n\nimport imblearn # for using oversampling\nprint(\"imblearn version: {}\". format(imblearn.__version__))\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5428d2c":"df_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv') \n\n# label encode the target column\nle = LabelEncoder()\ndf_train.target = le.fit_transform(df_train.target)\nfeatures = df_train.columns[1:51]\n#features = [col for col in df_train.columns if col.startswith(\"feature\")]\ndf_train[features].head()","64842ffc":"# drop duplicates\ndisplay(df_train[df_train[features].duplicated(keep=\"first\")])\ndf_train= df_train.drop_duplicates(subset = features)\ndisplay(df_train.shape)","59e5c9cc":"# let's try to one hot encode all columns that might be \"true categoric features\". This is just a guess based on my EDA (seperate notebook)\ncat_features = [\"feature_2\", \"feature_13\", \"feature_22\", \"feature_36\"]\n#df_train = pd.get_dummies(df_train, columns= cat_features)\n#df_test = pd.get_dummies(df_test, columns= cat_features)","1c629671":"# prepare test data\ndf_test.head()\nX_test=df_test.drop(columns=\"id\")\nX_test.head()","95e711f0":"# define X and y\nX = df_train.drop(columns=[\"id\",\"target\"])\ny = df_train.target\ndisplay(X.shape)\ndf_train.target.value_counts()","9fc77044":"# dividing X, y into train and test data\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 0, stratify=y)\ndisplay(X_train.shape)\ny_train.value_counts()","c80ce88d":"# in this cell SMOTE is implemented, can be turned on or off, balanced oversampling is tested and oversampling with a sampling strategy\n# I turned if OFF as the results were worse\n\n# scale the data when using SMOTE as it runs a KNN algorithm internally\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#X_train = scaler.fit_transform(X_train)\n#X_val = scaler.transform(X_val)\n#X_test = scaler.transform(X_test)\n\n#oversample = SMOTE() # for balanced oversampling\n#strategy = {0:10000, 1:43122, 2:16065, 3:14000} # setting sampling goals manually: only slight increase in underrepresented classes\n#oversample = SMOTE(sampling_strategy=strategy) # for strategic oversampling\n#X_train, y_train = oversample.fit_resample(X_train, y_train) # oversampling must only be used on training data\n\n#display(X_train.shape)\n#y_train.value_counts()","4a184c62":"# train model\nclf_xgb = xgb.XGBClassifier(objective='multi:softprob', \n                            seed=42, \n                            use_label_encoder=False, \n                            num_class=4, \n                            colsample_bytree=0.5, \n                            subsample=0.9,\n                            eta=0.1,\n                            gamma = 0.25,\n                            max_depth = 3,\n                            reg_lambda = 10,\n                            n_estimators = 500\n                            )\nclf_xgb.fit(X_train, \n            y_train,\n            verbose=True,\n            ## the next three arguments set up early stopping.\n            early_stopping_rounds=50,\n            eval_metric=['mlogloss'],\n            eval_set=[(X_train, y_train),(X_val, y_val)]\n           )\n# from Grid Search Round 2: {'gamma': 0.25, 'learning_rate': 0.3, 'max_depth': 3, 'reg_lambda': 10}","7f54762c":"# 1.[58]\tvalidation_0-mlogloss:1.01002\tvalidation_1-mlogloss:1.09621 - with eta= 0.3\n# 2.[81]\tvalidation_0-mlogloss:1.01296\tvalidation_1-mlogloss:1.09486 - with eta= 0.2\n# 3.[82]\tvalidation_0-mlogloss:1.01188\tvalidation_1-mlogloss:1.09464 - with the 4 duplicates in training data removed\n# 4.[76]\tvalidation_0-mlogloss:1.01515\tvalidation_1-mlogloss:1.09554 - with one hot encoding of 4 columns\n# 5.[138]\tvalidation_0-mlogloss:1.06422\tvalidation_1-mlogloss:1.09208 - with optimized parameters and increased n_estimators as early stopping was not triggered with 100 estimators\n# 6.[159]\tvalidation_0-mlogloss:1.06088\tvalidation_1-mlogloss:1.09217 - one hot encoding removed\n# 7.[170]\tvalidation_0-mlogloss:1.05924\tvalidation_1-mlogloss:1.09238 - changed n_estimators from 200 to 500 and early_stopping from 20 to 50\n# 8.[499]\tvalidation_0-mlogloss:1.18783\tvalidation_1-mlogloss:1.22255 - with balanced SMOTE, early stopping did not kick in \n# 9.[173]\tvalidation_0-mlogloss:1.08663\tvalidation_1-mlogloss:1.12238 - with strategic SMOTE\n# 10.[280]\tvalidation_0-mlogloss:1.05667\tvalidation_1-mlogloss:1.09219 - no SMOTE, eta = 0.2\n# 11.[241]\tvalidation_0-mlogloss:1.02529\tvalidation_1-mlogloss:1.09228 - with strategic SMOTE used correctly (scaling and only X_train)\n#12.[280]\tvalidation_0-mlogloss:1.05667\tvalidation_1-mlogloss:1.09218 - no SMOTE, just scaling (as expected same as #10.)\n#13.[348]\tvalidation_0-mlogloss:1.00963\tvalidation_1-mlogloss:1.09786 - balanced SMOTE used correctly (scaling and only X_train)\n#14.[326]\tvalidation_0-mlogloss:1.06012\tvalidation_1-mlogloss:1.09166 - not SMOTE, eta =1.5\n#15.[499]\tvalidation_0-mlogloss:1.05940\tvalidation_1-mlogloss:1.09133 - eta =1, used the maximum numbers of estimators","6b925b06":"# 1.Public Score: not submitted\n# 2.Public Score: 1.09190\n# 3.Public Score: 1.09128\n# 4.Public Score: 1.09082\n# 5.Public Score: 1.08813\n# 6.Public Score: 1.08804\n# 7.Public Score: 1.08804\n# 8.Public Score: 1.27648\n# 9.Public Score: 1.09327\n#10.Public Score: 1.08792\n#11.Public Score: 1.08950\n#12.Public Score: 1.08894\n#13.Public Score: 1.09411\n#14.Public Score: 1.08779","b71dfaee":"# visualize training\/validation results\nresults = clf_xgb.evals_result()\n\nplt.figure(figsize=(10,7))\nplt.plot(results[\"validation_0\"][\"mlogloss\"], label=\"Training loss\")\nplt.plot(results[\"validation_1\"][\"mlogloss\"], label=\"Validation loss\")\nplt.axvline(clf_xgb.best_ntree_limit, color=\"gray\", label=\"Optimal tree number\")\nplt.annotate(str(clf_xgb.best_ntree_limit), xy=(clf_xgb.best_ntree_limit, 1.2),color=\"gray\")\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Multiclass LogLoss\")\nplt.legend()\nplt.show()","3220dd02":"predicted_classes = clf_xgb.predict(X_val) # used only for confusion matrix later, therefore with X_val\npredictions = clf_xgb.predict_proba(X_test) # used for submission\npredictions","e70db754":"class_labels = [\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\"]\nplot_confusion_matrix(clf_xgb, X_val, y_val, display_labels=class_labels)\nprint(\"Accuracy: {}\\n\".format(accuracy_score(y_val,predicted_classes)))\nprint(\"Confusion Matrix:\")","81b8acbc":"pd.Series(predicted_classes).value_counts(normalize=True)\n# Class_2 is predicted 98% of the time :(\n# when the 4 duplicates in the training set are dropped, Class_1 is not predicted at all :(\n# with one hot encoding of 4 columns Class_3 is not predicted at all\n# interesting optimization: now it only predicts 2 classes, but logloss is better\n# with one hot encoding removed again only Class_1 is not predicted. Accuracy is slightly better\n# with balanced SMOTE all classes are predicted, but much more evenly than they should be. !! SMOTE was used wrongly !!\n# with strategic SMOTE the confusion matrix looks more similar to no SMOTE. As expected. Now it's Class_1 that is not predicted at all.!! SMOTE was used wrongly !!\n# strategic SMOTE used correctly only Class_2 and Class_3 are predicted\n# balanced SMOTE used correctly: Class_1 is not predicted","72e12390":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(clf_xgb,(10,12) )\nplt.show()","1071548e":"# create submission file\nsample_submission.drop(columns=class_labels, inplace=True)\nsubmission = (sample_submission.join(pd.DataFrame(data=predictions, columns=class_labels)))\nsubmission.to_csv(\"my_submission.csv\", index=False)\nsubmission","47ae1899":"# Grid Search for better hyperparameters\n# Round 1\n#param_grid = {\n#     'max_depth': [3, 4, 5], #4\n#     'learning_rate': [0.2, 0.1, 0.05],\n#     'gamma': [0, 0.25, 0.5],\n#     'reg_lambda': [5.0, 10.0, 15.0] #10\n# }\n# Round 2\nparam_grid = {\n     'max_depth': [2 ,3, 4],\n     'learning_rate': [0.3, 0.2, 0.1],\n     'gamma': [0.25],\n     'reg_lambda' : [10] #10\n }\noptimal_params = GridSearchCV(\n     estimator=xgb.XGBClassifier(objective='multi:softprob', eval_metric=['mlogloss'], seed=42, use_label_encoder=False, colsample_bytree=0.5, subsample=0.9),\n     param_grid=param_grid,\n     scoring = 'neg_log_loss',\n     verbose=0, # NOTE: If you want to see what Grid Search is doing, set verbose=2\n     cv = 3\n )\n# Round 1: {'gamma': 0.25, 'learning_rate': 0.2, 'max_depth': 3, 'reg_lambda': 10.0}\n# Round 2: {'gamma': 0.25, 'learning_rate': 0.3, 'max_depth': 3, 'reg_lambda': 10}","3e7322f9":"#optimal_params.fit(X_train,y_train,early_stopping_rounds=30,eval_set=[(X_val, y_val)],verbose=50)\n#print(optimal_params.best_params_)","b1226d84":"This plot shows the number of estimators used for the \"optimal\" result. No need to increase the number of estimators beyond that. \nDepending on whether SMOTE is used or not the picture changes.","923ba5b5":"# About\n\nThis is my notebook for the Tabular Playground Series - May 2021 using XGBoost. I have a [seperate notebook for my EDA](https:\/\/www.kaggle.com\/melanie7744\/tps5-eda-raising-more-questions-than-answers).\n\nI frist ran plain XGBoost with default parameters to see how it fares. Then I \n- removed the duplicates from the training set\n- one hot encoded four parameters that I guessed to be categorical\n- ran Grid Search CV to search for better hyperparameters\n- increased the number of estimators and early stopping rounds\n- implemented oversampling (SMOTE) to deal with the imbalance in the target variable (corrected later on, initial implementation was incorrect)\n- reduced the learning rate\n\nThe results are listed as comments below. It was an iterative process with the current version of the notebook only showing the last step.\n\n\nI ran out of things to try with XGBoost. My biggest issue is, that I did not find a way to deal with the imbalance of the target variable. Stratification during generation of the validation set was not enough, neither was oversampling. As a last tuning step reducing the learning rate helped to get a better log loss score. But only slightly. \n\nAny suggesetions are very welcome!","f38c9f28":"Looking at the feature importance graph, it looks like one hot encoding is quite unimportant. However it led to a small improvement in the Public Score. I will run one last try and remove one hot encoding again. Result: those features are still at the end of the importance list, so one-hot encoding was not the culprit. "}}