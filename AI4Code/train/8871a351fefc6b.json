{"cell_type":{"b72c98c6":"code","261c5378":"code","647ae128":"code","5d449e12":"code","afc33d17":"code","1e54ac64":"code","a8889006":"code","4d241993":"code","e905a409":"code","b55708cf":"code","70e22bba":"code","12c1c575":"code","823f11b5":"code","b7e33502":"code","b583eb23":"code","f6c3486b":"code","e51b4da3":"code","b0c3f438":"code","a526c9a6":"code","b7591867":"code","f078f54a":"code","8fd1d75b":"code","214d4337":"code","e10c0fdf":"code","d989b38a":"code","86ced806":"code","14da8fee":"code","5123b961":"code","21bb5375":"code","3b84b183":"code","72edc870":"code","b43a1692":"code","8236df43":"code","d0ba4796":"code","2cd2b40c":"code","fc9cb56b":"code","26d77073":"code","a91858aa":"code","bcbc5f18":"code","2f346329":"code","a46aae0c":"code","996a71cf":"code","c5ad27fb":"code","dbec1588":"code","3177b104":"code","1282baf8":"code","7ac6332e":"code","d03b48d9":"code","49a58b5c":"code","286f9893":"code","b27ee754":"markdown","e84d0fd5":"markdown","2d5bf828":"markdown","a9428690":"markdown","167749c6":"markdown","79d99e3f":"markdown","aded3dd9":"markdown","42e7449c":"markdown","6514125e":"markdown","d7f35592":"markdown","eac3891a":"markdown","092e7cd8":"markdown","a989aa5e":"markdown","7e78138c":"markdown","80bfd3bc":"markdown","10deddb6":"markdown","4ebea630":"markdown","f2b33fc6":"markdown","e030eb7f":"markdown","e555bcd9":"markdown","b699d765":"markdown","af65f4ed":"markdown","8fc6beb4":"markdown","eaace466":"markdown","d41ddf3d":"markdown","3b5165bc":"markdown","f22ea915":"markdown","790f456c":"markdown","e903a58f":"markdown","20e6404b":"markdown","d278a1c9":"markdown","5d484eac":"markdown","499c3d43":"markdown","9fbff49f":"markdown","bbcff505":"markdown","b980f189":"markdown","8b855425":"markdown","848fab2d":"markdown","28bbacd3":"markdown","c66f2074":"markdown","22e82c85":"markdown","4954f51e":"markdown","6f98747a":"markdown","9b294e52":"markdown","4dc9c877":"markdown","01520be6":"markdown","f3512f5f":"markdown","94088d99":"markdown","d2768a2d":"markdown"},"source":{"b72c98c6":"# Importing the libraries.\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport nltk\nfrom wordcloud import WordCloud\nimport textstat as ts\nfrom numpy.random import seed\nseed(100)\n\n\nnltk.download('stopwords')\n\n\n%matplotlib inline","261c5378":"# Importing the dataset.\n\ndata_path = \"..\/input\/fake-and-real-news-dataset\/\"\n\ntrue_news_data = pd.read_csv(data_path + \"True.csv\")\nfake_news_data = pd.read_csv(data_path + \"Fake.csv\")","647ae128":"\"\"\"\nA utility function to calculate the avg. length of the text (in number of words.)\n\n\"\"\"\n\ndef avg_text_length(dataframe):\n    rowCounts = [len(x.split()) for x in dataframe[\"text\"].tolist()]\n    avgCount = sum(rowCounts)\/\/len(rowCounts)\n    \n    return avgCount","5d449e12":"print(\"Avg. text length for true news: {}\".format(avg_text_length(true_news_data)))\nprint(\"Avg. text length for fake news: {}\".format(avg_text_length(fake_news_data)))","afc33d17":"# Plotting the histogram for meaningful insights.\n\ntrue_lengths = pd.Series([len(x.split()) for x in true_news_data[\"text\"].tolist()])\nfake_lengths = pd.Series([len(x.split()) for x in fake_news_data[\"text\"].tolist()])\n\nplt.figure(figsize = (15,10))\nplt.hist(true_lengths, bins = 100, range = [0, 2000], color = \"brown\", label = \"true-news length\")\nplt.hist(fake_lengths, bins = 100, range = [0, 2000], color = \"blue\", alpha = 0.5, label = \"fake-news length\")\nplt.xlabel(\"Length of a news article in no. of words.\", fontsize = 15)\nplt.ylabel(\"Number of articles with x word length.\", fontsize = 15)\nplt.title(\"Class wise distribution of length of news article.\", fontsize = 15)\nplt.legend()\nplt.show()","1e54ac64":"\"\"\"\nA utility function to calculate the five number summary of the length of the text of news articles.\n\n\"\"\"\n\ndef five_num_summary(dataframe):\n    quartiles = np.percentile([len(x.split()) for x in dataframe[\"text\"].tolist()], [0, 25, 50, 75, 100])\n    \n    return quartiles","a8889006":"true_summ = five_num_summary(true_news_data)\n\nprint(\"Text length - five number summary for true news data:\")\nfor i in range(0, 101,25):\n    print(\"{}'th %ile: {}\".format(i, true_summ[i\/\/25]))\n    \n    \nprint(\"\\n\")\n\n\nfake_summ = five_num_summary(fake_news_data)\n\nprint(\"Text length - five number summary for fake news data:\")\nfor i in range(0, 101,25):\n    print(\"{}'th %ile: {}\".format(i, fake_summ[i\/\/25]))","4d241993":"stopwords = nltk.corpus.stopwords.words(\"english\")\n\n\"\"\"\nA utility function to calculate the average % of stopwords content in true and fake news.\n\n\"\"\"\n\ndef stopwords_frequency(dataframe):\n    words_list = [x.split() for x in dataframe[\"text\"].tolist()]\n    frequencies = []\n    for row in words_list:\n        if(len(row)) > 0:\n            row_frequency = len([w for w in row if w in stopwords])\n            row_frequency = (row_frequency\/len(row))*100\n\n            frequencies.append(row_frequency)\n    \n    avg = sum(frequencies)\/len(frequencies)\n    \n    return avg","e905a409":"true_frequencies = stopwords_frequency(true_news_data)\nfake_frequencies = stopwords_frequency(fake_news_data)\n\nprint(\"Average stopwords frequency in true-news: {}\".format(true_frequencies))\nprint(\"Average stopwords frequency in fake-news: {}\".format(fake_frequencies))","b55708cf":"cloud = WordCloud(width=1440, height=1080).generate(\" \".join(true_news_data[\"text\"].astype(str)))\nplt.figure(figsize=(15, 10))\nplt.imshow(cloud)\nplt.axis('off')\n\nprint(\"True news articles wordcloud.\")","70e22bba":"cloud = WordCloud(width=1440, height=1080).generate(\" \".join(fake_news_data[\"text\"].astype(str)))\nplt.figure(figsize=(15, 10))\nplt.imshow(cloud)\nplt.axis('off')\n\nprint(\"Fake news articles wordcloud.\")","12c1c575":"true_news_readability = []\nfake_news_readability = []\n\nfor sentence in true_news_data[\"text\"].tolist():\n    temp = ts.flesch_reading_ease(sentence)\n    true_news_readability.append(temp)\n    \nfor sentence in fake_news_data[\"text\"].tolist():\n    temp = ts.flesch_reading_ease(sentence)\n    fake_news_readability.append(temp)","823f11b5":"true_readability_df = pd.Series(true_news_readability)\nfake_readability_df = pd.Series(fake_news_readability)\n\nplt.figure(figsize = (15,10))\nplt.hist(true_readability_df, bins = 10, range = [0, 100], color = \"brown\", label = \"true-news readability\")\nplt.hist(fake_readability_df, bins = 10, range = [0, 100], color = \"blue\", alpha = 0.5, label = \"fake-news readability\")\nplt.xlabel(\"Flesch readability easiness for a news article.\", fontsize = 15)\nplt.ylabel(\"Number of articles with x word length.\", fontsize = 15)\nplt.title(\"Class wise Flesch readability ease score distribution.\", fontsize = 15)\nplt.legend()\nplt.show()","b7e33502":"fake_datewise_counts = fake_news_data.groupby('date').date.agg([('count', 'count')]).reset_index().sort_values(by = \"count\", ascending = False)\n\nfake_datewise_counts = fake_datewise_counts.head(50)","b583eb23":"plt.figure(figsize = (15,10))\nplt.xticks(rotation = 90)\nplt.bar(fake_datewise_counts[\"date\"], fake_datewise_counts[\"count\"], align = \"center\", color = \"orange\")\nplt.xlabel(\"Date.\", fontsize = 15)\nplt.ylabel(\"Count of fake news articles released.\", fontsize = 15)\nplt.title(\"Date-wise distribution of number of fake news articles released.\", fontsize = 15)\nplt.show()","f6c3486b":"fake_news_data.loc[fake_news_data[\"date\"] == \"May 10, 2017\"][\"title\"].tolist()[:10]","e51b4da3":"# Introducing the label column stating if the news article is True or False.\n\ntrue_news_data[\"label\"] = \"True\"\nfake_news_data[\"label\"] = \"Fake\"\n\nall_news_data = true_news_data.append(fake_news_data, ignore_index = True)","b0c3f438":"del all_news_data['title']\ndel all_news_data['subject']\ndel all_news_data['date']\n\nall_news_data.head()","a526c9a6":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\n\n# Preprocessing function definitions\n\n\n\"\"\"\nA utility function to remove punctuations from the text.\n\n\"\"\"\n\ndef remove_punctuations(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\n\n\"\"\"\nA utility function to remove numerical characters from the text.\n\n\"\"\"\n\ndef remove_nums(text):\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\n\n\"\"\"\nA utility function to remove URL links from the text.\n\n\"\"\"\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\n\"\"\"\nA utility function to remove HTML tags from the text.\n\n\"\"\"\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\n\"\"\"\nA utility function to remove emojis from the text.\n\n\"\"\"\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\nstop_words = set(stopwords.words('english'))\n\n\"\"\"\nA utility function to remove stopwords from the text.\n\n\"\"\"\n\ndef clean_stopwords(text):\n    res = [w for w in text.split() if not w in stop_words]\n    res_string = \" \".join(str(x) for x in res)\n    return res_string\n","b7591867":"all_news_data_processed = all_news_data.copy()\n\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: remove_punctuations(x))\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: remove_nums(x))\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: remove_URL(x))\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: remove_html(x))\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: remove_emoji(x))\nall_news_data_processed[\"text\"] = all_news_data_processed[\"text\"].apply(lambda x: clean_stopwords(x))\n\n# Shuffling the rows.\n\nall_news_data_processed = all_news_data_processed.sample(frac = 1).reset_index(drop=True).reset_index(drop = True)\n\nall_news_data_processed.head(10)","f078f54a":"all_news_data_processed = all_news_data_processed[all_news_data_processed[\"text\"].str.split().str.len().gt(0)]","8fd1d75b":"len(all_news_data_processed)","214d4337":"train_X = all_news_data_processed.loc[:38000, \"text\"].values\ntrain_Y = all_news_data_processed.loc[:38000, \"label\"].values\nvalidation_X = all_news_data_processed.loc[38000:, \"text\"].values\nvalidation_Y = all_news_data_processed.loc[38000:, \"label\"].values","e10c0fdf":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(train_X)\nvalidation_vectors = vectorizer.transform(validation_X)","d989b38a":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nimport time\n\nstart = time.time()\n\nmnb_classifier = MultinomialNB().fit(train_vectors, train_Y)\nlinear_svc_classifier = SVC(kernel = \"linear\").fit(train_vectors, train_Y)\n\nend = time.time()\n\nprint(\"Trained 2 models in {} seconds.\".format(end - start))","86ced806":"from  sklearn.metrics  import accuracy_score\n\nmnb_predicted = mnb_classifier.predict(validation_vectors)\nlinear_svc_predicted = linear_svc_classifier.predict(validation_vectors)\n\n\nprint(\"Validation accuracy - Multinomial Naive Bayes: {}\".format(accuracy_score(validation_Y, mnb_predicted)))\nprint(\"Validation accuracy - Linear Support Vector Classifier: {}\".format(accuracy_score(validation_Y, linear_svc_predicted)))","14da8fee":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve","5123b961":"print(\"A detailed report on the model performance:\")\n\nprint(\"Model type: Multinomial Naive Bayes\")\nprint(classification_report(validation_Y, mnb_predicted))\n\nprint(\"\\n\")\n\nprint(\"Model type: Support Vector Machines\")\nprint(classification_report(validation_Y, linear_svc_predicted))","21bb5375":"all_news_data_processed['label'] = all_news_data_processed['label'].map( {'Fake':1, 'True':0} )\n\nall_news_data_processed.head(10)","3b84b183":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ntf.random.set_seed(100)\n\n\nvocab_size = 1000000\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 38000\nmax_length = 100\n\n# Splitting the train and the test sentences list.\n\ntemp = [x for x in all_news_data_processed[\"text\"].tolist()]\ntrain_sentences = temp[:training_size]\ntest_sentences = temp[training_size:]\n\n# Splitting the train and the test labels list.\n\ntemp2 = [x for x in all_news_data_processed[\"label\"].tolist()]\ntrain_labels = temp2[:training_size]\ntest_labels = temp2[training_size:]\n\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\n\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","72edc870":"# Conversion to numpy array equivalents\n\ntrain_padded = np.array(train_padded)\ntrain_labels = np.array(train_labels)\n\ntest_padded = np.array(test_padded)\ntest_labels = np.array(test_labels)","b43a1692":"\"\"\"\nA utility function plot learning curves for the trained model.\n\n\"\"\"\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","8236df43":"# Regular dense neural network with word-embeddings dimension = 5.\n\nembedding_dim = 5\n\ndnn_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\ndnn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","d0ba4796":"num_epochs = 10\n\nearly_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\nearly_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n\ndnn_history = dnn_model.fit(train_padded, train_labels, epochs=num_epochs, validation_split = 0.2, \\\n                        callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss], verbose=1)\n\nplot_graphs(dnn_history, \"accuracy\")\nplot_graphs(dnn_history, \"loss\")","2cd2b40c":"dnn_results = dnn_model.evaluate(test_padded, test_labels, batch_size=128)\n\nprint('Regular dense network - test loss: {}'.format(dnn_results[0]))\nprint('Regular dense network - test accuracy: {}'.format(dnn_results[1]))","fc9cb56b":"dnn_pred = (dnn_model.predict(test_padded) >= 0.5).astype(\"int\")\n\nprint(\"A detailed report on the model performance:\")\n\nprint(\"Model type: Regular dense neural network.\")\nprint(classification_report(test_labels, dnn_pred))","26d77073":"print(confusion_matrix(test_labels, dnn_pred))","a91858aa":"# Recurrent LSTM network with word-embeddings dimension = 10\n\nembedding_dim = 10\n\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.LSTM(12),\n    tf.keras.layers.Dense(12, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nlstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","bcbc5f18":"num_epochs = 10\n\nearly_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\nearly_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n\nlstm_history = lstm_model.fit(train_padded, train_labels, epochs=num_epochs, validation_split = 0.2, \\\n                         callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss], verbose=1)\n\nplot_graphs(lstm_history, \"accuracy\")\nplot_graphs(lstm_history, \"loss\")","2f346329":"# Recurrent LSTM network with word-embeddings dimension = 10\n\nembedding_dim = 10\n\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.LSTM(12),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(18, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nlstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","a46aae0c":"num_epochs = 10\n\nearly_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\nearly_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n\nlstm_history = lstm_model.fit(train_padded, train_labels, epochs=num_epochs, validation_split = 0.2, \\\n                         callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss], verbose=1)\n\nplot_graphs(lstm_history, \"accuracy\")\nplot_graphs(lstm_history, \"loss\")","996a71cf":"lstm_results = lstm_model.evaluate(test_padded, test_labels, batch_size=128)\n\nprint('LSTM Model - test loss: {}'.format(lstm_results[0]))\nprint('LSTM Model - test accuracy: {}'.format(lstm_results[1]))","c5ad27fb":"lstm_pred = (lstm_model.predict(test_padded) >= 0.5).astype(\"int\")\n\nprint(\"A detailed report on the model performance:\")\n\nprint(\"Model type: LSTM recurrent network.\")\nprint(classification_report(test_labels, lstm_pred))","dbec1588":"print(confusion_matrix(test_labels, lstm_pred))","3177b104":"all_news_data_title = true_news_data.append(fake_news_data, ignore_index = True)\n\nall_news_data_processed_title = all_news_data_title.copy()\n\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: remove_punctuations(x))\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: remove_nums(x))\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: remove_URL(x))\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: remove_html(x))\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: remove_emoji(x))\nall_news_data_processed_title[\"title\"] = all_news_data_processed_title[\"title\"].apply(lambda x: clean_stopwords(x))\n\nall_news_data_processed_title['label'] = all_news_data_processed_title['label'].map( {'Fake':1, 'True':0} )\n\ndel all_news_data_processed_title[\"text\"]\ndel all_news_data_processed_title[\"subject\"]\ndel all_news_data_processed_title[\"date\"]\n\nall_news_data_processed_title = all_news_data_processed_title[all_news_data_processed_title[\"title\"].str.split().str.len().gt(0)]\n\nall_news_data_processed_title = all_news_data_processed_title.sample(frac = 1).reset_index(drop=True).reset_index(drop = True)\n\nall_news_data_processed_title.head(10)","1282baf8":"tf.random.set_seed(100)\n\ntemp = [x for x in all_news_data_processed_title[\"title\"].tolist()]\ntrain_sentences_title = temp[:training_size]\ntest_sentences_title = temp[training_size:]\n\ntemp2 = [x for x in all_news_data_processed_title[\"label\"].tolist()]\ntrain_labels_title = temp2[:training_size]\ntest_labels_title = temp2[training_size:]\n\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\n\nword_index = tokenizer.word_index\n\ntrain_sequences_title = tokenizer.texts_to_sequences(train_sentences_title)\ntrain_padded_title = pad_sequences(train_sequences_title, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences_title = tokenizer.texts_to_sequences(test_sentences_title)\ntest_padded_title = pad_sequences(test_sequences_title, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n\ntrain_padded_title = np.array(train_padded_title)\ntrain_labels_title = np.array(train_labels_title)\n\ntest_padded_title = np.array(test_padded_title)\ntest_labels_title = np.array(test_labels_title)","7ac6332e":"# Regular dense neural network with word-embeddings dimension = 5.\n\nembedding_dim = 5\n\ndnn_model_title = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\ndnn_model_title.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nnum_epochs_title = 10\n\nearly_stopping_callback_loss = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1)\nearly_stopping_callback_val_loss = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)\n\ndnn_history_title = dnn_model_title.fit(train_padded_title, train_labels_title, epochs=num_epochs_title, validation_split=0.2,\n                        callbacks = [early_stopping_callback_loss, early_stopping_callback_val_loss], verbose=1)\n\nplot_graphs(dnn_history_title, \"accuracy\")\nplot_graphs(dnn_history_title, \"loss\")","d03b48d9":"dnn_results_title = dnn_model_title.evaluate(test_padded_title, test_labels_title, batch_size=128)\n\nprint('Regular dense network - test loss: {}'.format(dnn_results_title[0]))\nprint('Regular dense network - test accuracy: {}'.format(dnn_results_title[1]))","49a58b5c":"dnn_pred_title = (dnn_model_title.predict(test_padded_title) >= 0.5).astype(\"int\")\n\nprint(\"A detailed report on the model performance:\")\n\nprint(\"Model type: Regular dense neural network.\")\nprint(classification_report(test_labels_title, dnn_pred_title))","286f9893":"print(confusion_matrix(test_labels_title, dnn_pred_title))","b27ee754":"For our initial modeling purposes, we intend to classify whether a news article is True or Fake solely from its text, hence we exclude the other features such as date, subject as well as title.","e84d0fd5":"Note: The X-axis of the learning curve plots below start with epoch 0 to <i>num_epochs - 1<\/i> while in my explanation, I have referred it as starting with epoch 1 to <i>num_epochs<\/i>.","2d5bf828":"# Data Preprocessing","a9428690":"The above histogram bins the news articles together with same word length and plots the frequency distribution for each word length. The bars in brown shows the frequency distribution of word length of true news articles while the bars in lighter blue shade shows the same for fake news articles. Note that the part of bars in darker blue (more like purple) shade are the areas common to both true and fake news articles.\n\nSome of the insights from this histogram:\n\n1. Rarely, a <b>true<\/b> news article contains more than 1400 words. On the contrary, there are comparitively much greater number of fake articles with word length more than 1400 words.\n\n2. The mode of the word length of <b>true<\/b> news articles is somewhere between 200 to 250 words.\n\n3. There are more than 1100 <b>fake<\/b> news articles with text length equal to 0.\n\nNote: The range of the histogram is set to 2000 because there are extremely less number of news articles with word length greater than 2000.","167749c6":"We begin the analysis by calculating the average length of the text in news articles measured in number of words. The average length can be a distinguishing feature among the two categories.","79d99e3f":"The above frequency distribution barchart shows that the highest number of fake news articles are released on May 10th, 2017 with more than 40 fake news articles released.\n\nBelow are some of the fake news articles that were released on May 10 2017:","aded3dd9":"# Conclusion","42e7449c":"The above learning curves shows us that the training process in our updated LSTM model is much better as compared to the prior one. The early stopping criteria is met since the validation loss increases at the end of epoch: 3 of the training process.","6514125e":"All the above analysis can show that the average fake news article is going to be somewhat longer than the average true news article.","d7f35592":"# Statistical and Visual analysis","eac3891a":"Having evaluated the performance of a dense neural network, let us now experiment by fitting a more complex neural network on the data. LSTM is a recurrent neural network which is capable of learning the dependence of the output on the order of the input sequences. It is used where the training data has a sequential order which might play a crucial role in deciding the class of the sequence in case of classification or generating the next element in the sequence in case of generation. It used in different applications such as machine translation, speech recognition, time-series analysis etc. More on LSTMs: https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory","092e7cd8":"The above two wordclouds makes some good points about the dataset:\n\n1. The word combination <b>Hillary Clinton<\/b> occur more frequently in the fake news articles than the true news articles.\n\n2. The word combination <b>the FBI<\/b> also occur more frequently in the fake news articles as compared to the same in true news articles.\n\n3. The word <b>Reuters<\/b> has occured many times in the true news articles.\n\n4. The word combination containing the word <b>twitter<\/b> has occured many times in the fake news dataset. It may be true that a good chunk of fake news articles were associated to the twitter accounts of the subjects in the articles.","a989aa5e":"#### Regular dense neural network","7e78138c":"So far, we have analyzed the dataset based on some key metrics of textual data, experimented with machine learning modeling by training the data on multinomial naive bayes, support vector classifiers and then experimented the training of deep learning models such as dense regular networks and LSTM models.\n<br \/>\n\nI hope you have enjoyed the work in this notebook. Thank you for reading :)","80bfd3bc":"Recall that in our machine learning modeling, we used tf-idf for vectorization. TF-IDF is based on the bag of words model which means that it focuses on frequencies of the words for vectorization and the semantic context in the sequence is lost. This is a major disadvantage of tf-idf based vectorization.\n\nIn modeling of neural networks, we make use of word-embeddings which is a learned representation for text where words that have the same meaning have a similar representation. Thus, the word-embeddings do a better job in preserving the context of the text data than the tf-idf based vectors. \n\nOur deep learning data preprocessing goes as follows:\n\n1. First we distribute the data among train and test sentences.\n2. Then we use the Tokenizer class to tokenize the words in our training data and create an indexed corpus.\n3. Then we use the tokenizer to encode the sentences in the training and test data into sequences.\n4. Next, we perform padding of the sequences which ensures that all the sequences in the dataset are of equal length. It achieves this by padding 0's either at the beginning of the sequence or at the end. \n5. Also, since tokenization will only happen on our training data, what if we encounter a word in the test data which is not present in our vocabulary? To solve this problem, we make use of something called as \"OOV token\" ( Out Of Vocabulary token) which is basically used by the library as a replacement word to replace the unknown words in the test data.","10deddb6":"Further, we can check for the dates on which the maximum number of fake news articles were released. ","4ebea630":"For our modeling, we perform a train - validation split with 38000 examples in the training set and others in the validation set.","f2b33fc6":"The above confusion matrix translates to:\n\nActual True news & Predicted True news: 3089 <br \/>\nActual True news but Predicted Fake news: 10 <br \/>\nActual Fake news but Predicted True news: 49 <br \/>\nActual Fake news & Predicted Fake news: 3119 <br \/>\n\nThe above confusion matrix shows that there is a fair balance among the true positives, false positives, true negatives and false negatives. \n\nWe thus note that our regular dense neural network performed in a better way than our LSTM model.","e030eb7f":"This notebook attempts to analyze the <a href = \"https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset\">Fake and real news dataset<\/a>  and train a machine learning algorithm that can classify whether a news article is fake or real.","e555bcd9":"For preprocessing, we will use preprocessing functions provided in this amazing kernel: https:\/\/www.kaggle.com\/vanshjatana\/a-simple-guide-to-text-cleaning","b699d765":"The above confusion matrix translates to:\n\nActual True news & Predicted True news: 3273 <br \/>\nActual True news but Predicted Fake news: 39 <br \/>\nActual Fake news but Predicted True news: 32 <br \/>\nActual Fake news & Predicted Fake news: 3554 <br \/>\n\nThe above confusion matrix shows that there is a fair balance among the true positives, false positives, true negatives and false negatives.","af65f4ed":"Another key metric that we can calculate is the frequency of stopwords in the true and fake news articles. Stopwords are the most commonly occuring words in a language data. More on stopwords: https:\/\/en.wikipedia.org\/wiki\/Stop_words","8fc6beb4":"# Fake news classification - Analysis and Modeling","eaace466":"We wish to check and visualize how the training progresses at every epochs. Thus for plotting the learning curves ( accuracy and loss ) we write the following utility function:","d41ddf3d":"Note that we also need to remove the rows with text length = 0.","3b5165bc":"#### Recurrent neural network - LSTM","f22ea915":"Next, we have computed the five-number summary of the word length of true and fake news articles. The five number summary shows the minimum, maximum, the upper quartile, the lower quartile and the median of a data distribution. More on five number summary: https:\/\/en.wikipedia.org\/wiki\/Five-number_summary\n\nWe notice that the minimum value for true and fake articles is equal to 0 while the maximum value for fake article is much higher than that of a true news article. The median word length for both fake and true news articles is close to each other.","790f456c":"# Index\n\n\n<a href=\"#Statistical-and-Visual-analysis\">Statistical and Visual data analysis<\/a>\n\n<a href=\"#Data-Preprocessing\">Data Preprocessing<\/a>\n\n<a href=\"#Machine-Learning-modeling\">Machine Learning modeling - Multinomial naive bayes and Support Vector Machines<\/a>\n\n<a href=\"#Deep-learning-modeling\">Deep learning modeling<\/a>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Regular-dense-neural-network\">Regular dense neural network<\/a>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#Recurrent-neural-network---LSTM\">Recurrent neural network - LSTM<\/a>\n\n<a href=\"#Modeling-over-title\">Modeling over title<\/a>\n\n<a href=\"#Conclusion\">Conclusion<\/a>","e903a58f":"As you can see in the above learning curve, the validation loss as well as the training loss is inconsistent throughout the training process. The validation loss increases at the first epoch itself. The early stopping criteria was triggered because the training loss increased instead of decreasing.\n<br \/>\n<br \/>\n\nWe know that the regular dense network did well on the dataset. Thus to ensure better training and loss minimization, we add more number of hidden nodes in our first dense layer. Also, in order to regularize the output of the LSTM layer, we add a dropout layer with keep_prob = 0.75. This dropout layer will discard 25% of the output from the LSTM layer and thus introducing some noise in the input feeded to the dense layer.\n\nNote: Here, the dropout layer is passed a parameter = 0.25 which denotes the rate at which the nodes are to be eliminated.\n\n","20e6404b":"The below dense neural network contains an embedding layer which creates the word-embeddings of the vectors in a <i>embedding_dim<\/i> dimensional space. The next layer is a pooling layer which basically pools the high dimensional structure of embedding into lower dimensional vector to reduce computational complexity. It is followed by a fully connected dense layer of 24 hidden nodes with ReLU activation. Then there is a final layer with one node with sigmoid activation.\n\n\nThe model uses Adam optimizer for weights optimization to minimize the binary crossentropy loss. More on Adam optimization: https:\/\/arxiv.org\/pdf\/1412.6980.pdf\n\n\nThe model is trained for 10 epochs with an early stopping criteria monitoring the performance of training loss.","d278a1c9":"The above confusion matrix translates to:\n\nActual True news & Predicted True news: 3086 <br \/>\nActual True news but Predicted Fake news: 13 <br \/>\nActual Fake news but Predicted True news: 27 <br \/>\nActual Fake news & Predicted Fake news: 3141 <br \/>\n\nThe above confusion matrix shows that there is a decent balance among the true positives, false positives, true negatives and false negatives.","5d484eac":"We notice that both the true and fake news articles have similar level of stopwords frequency (36% and 39.5% respectively). This means that the frequency of stopwords is not a good distinguisher among the two categories.","499c3d43":"We have created a model predicting if a news is fake or not from its text content. Now let us train a deep neural network which will predict if a news is fake or not from its title. This is important because when we see the usecases of this model for a real world fake news classifier application, it is tedious to copy an entire 1000-2000 words length news article and easier to simply copy the title. Therefore we will experiment with a model on the same.","9fbff49f":"We get a good F1 score for the above trained model which is equal to 0.99 for both the categories. ","bbcff505":"The above figure shows the frequency distribution of flesch readability score for true and fake news articles. We can notice that:\n\n1. The flesch readability of true news articles is much lesser than the fake news articles.\n\n2. The maximum flesch readability of true news articles goes upto 80 while the maximum flesch readability for fake news articles go upto 100.","b980f189":"The above learning curves show that the training process was consistent throughout all the epochs and the validation loss increased marginally after training for epoch 10. Thus we have created a decent model on classifying a news article as fake or true based on its title. Moving forward to check the test accuracy and confusion matrix:","8b855425":"Vectorization of textual data is important in text mining. It converts the text into mathematical vectors so that machine learning can be applied on it. In order to vectorize our text data, we use TF-IDF vectorization. More on TF-IDF: https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf","848fab2d":"# Modeling over title","28bbacd3":"The above learning curves shows that the loss curve is consistently decreasing and then becomes almost stable. Hence we could have stopped our training process after the epoch 4. The extremely low difference between the training data and validation data also shows that there is no overfitting discovered yet. Let us test the model on our test data vectors.","c66f2074":"We notice that on average, the fake news articles contain about 40 words more than the average true news articles. The average gives us a good measure of the central tendency of the data but let us dive further into the analysis for length of the news articles.","22e82c85":"For the machine learning modeling, we will train our data on two algorithms: Multinomial naive bayes and support vector classifier.\n\n1. MultinomialNB is a simple model which is fast and has been demonstrated to be decently accurate in many applications of NLP. More on MultinomialNB: https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier#Multinomial_na%C3%AFve_Bayes\n\n2. Support vector classifier is also another well performing algorithm for classification purposes. More on SVC: https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine","4954f51e":"In this modeling, we will first explore how a regular deep neural network and a recurrent neural network - LSTM performs on training over this data.","6f98747a":"# Deep learning modeling","9b294e52":"The below neural network contains an embedding layer which creates the word-embeddings of the vectors in a <i>embedding_dim<\/i> dimensional space. The next layer is a LSTM layer with 12 hidden states. It is followed by a fully connected dense layer of 12 hidden nodes with ReLU activation. Then there is a final layer with one node with sigmoid activation.\n\n\nThis model also uses adam optimization to minimize the binary cross entropy loss with an early stopping criteria monitoring the training loss.","4dc9c877":"The above classification report shows that the F1 score for both Fake and True categories from support vector machines is equal to 1 while the same for Multinomial naive bayes is 0.94. Also, the support for both fake and true categories shows that the dataset is nearly balanced ( ~51% fake vs ~49% true).","01520be6":"We can also experiment with the readability index for each true and fake news articles. \n\nFor readability index, we use Flesch reading ease score formula. The flesch reading ease score determines the level of difficulty of a text. The values of the flesch reading ease score categorizes as:\n\n<b>\n90-100 : Very Easy\n<br \/>\n80-89 : Easy\n<br \/>\n70-79 : Fairly Easy\n<br \/>\n60-69 : Standard\n<br \/>\n50-59 : Fairly Difficult\n<br \/>\n30-49 : Difficult\n<br \/>\n0-29 : Very Confusing\n<\/b>\n\nThe formula for calculating the flesch reading easiness score is given below:\n\n<i>RE = 206.835 \u2013 (1.015 x ASL) \u2013 (84.6 x ASW)<\/i>\n\n<i>RE = Readability Ease<\/i>\n\n<i>ASL = Average Sentence Length (i.e., the number of words divided by the number of sentences)<\/i>\n\n<i>ASW = Average number of syllables per word (i.e., the number of syllables divided by the number of words)<\/i>","f3512f5f":"The below dense neural network model is of the same architecture as our previous dense network. The only difference is that this model is fit on the training data containing the title of the news articles and the previous model was trained on the training data containing the text content of the news articles.\n\n\nThe model is trained for 10 epochs with an early stopping criteria monitoring the performance of training loss.\nThis model also uses adam optimization to minimize the binary cross entropy loss with an early stopping criteria monitoring the training loss.","94088d99":"In order to check for most frequent words in both the datasets, we plot wordcloud of text contents of both true and fake news data.","d2768a2d":"# Machine Learning modeling"}}