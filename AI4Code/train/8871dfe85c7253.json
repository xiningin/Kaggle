{"cell_type":{"822b0a95":"code","c8045f6b":"code","f0ce02aa":"code","9dd2062f":"code","f3c3a5bf":"code","f29ae324":"code","faeae2e7":"code","6899aab2":"code","7f77dbfc":"code","ad28f70c":"code","a03949d7":"code","476e0132":"code","bcb5d4ae":"code","06ed604f":"code","332e760d":"code","5731d002":"code","43ee08f9":"code","8bd2b45c":"code","68df207f":"code","96b8e200":"code","6d1a242b":"code","c467d5e9":"code","84e358b8":"code","42f75389":"code","fa3bcb0f":"code","47b4cc0e":"code","f8663626":"code","c0ac9e0b":"code","867caf0c":"code","311ca8fe":"code","78ea8dfc":"code","971282b8":"code","f1f5298d":"code","7433e8e3":"code","d1910259":"code","504c099c":"code","635f8417":"code","b1249bdc":"code","034482e1":"code","bf59b0ab":"code","53165415":"code","afa987ad":"code","a9fb8bdc":"code","54c9d032":"code","c0045c5f":"code","6a4a7888":"code","97ca2d58":"code","324e0bc2":"code","667af72d":"code","e1084109":"code","a6dfee41":"code","0653b98b":"code","60464f08":"code","72b51e3f":"code","09041e37":"markdown","722c944d":"markdown","a4a635c9":"markdown","8b547de1":"markdown","b03f1f27":"markdown","fe9eac00":"markdown","fa1e97aa":"markdown","6b98c482":"markdown","60f88f06":"markdown","d679e9f6":"markdown","d5e752a3":"markdown","104ced8e":"markdown","6b8ce105":"markdown","a3b30ee4":"markdown","3998f65b":"markdown","0c599339":"markdown","01075818":"markdown","936887d0":"markdown","a737a50f":"markdown","3d20bfaa":"markdown","c6a97912":"markdown","9d53e138":"markdown","bcf9475c":"markdown","2318a6f9":"markdown","a69dd4c5":"markdown","8a081ae2":"markdown","3b1ff2b2":"markdown","d4efd663":"markdown","00cc4dea":"markdown","a4c88ea9":"markdown"},"source":{"822b0a95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c8045f6b":"# Importing necessary liraries:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f0ce02aa":"# Importing data:\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9dd2062f":"# Viewing the training data:\n\ntrain.head(3)","f3c3a5bf":"# Viewing the test data: \n\ntest.head(3)     #Only difference in terms of column with `train data` is absence of 'Survived' column.","f29ae324":"# Info() method will help us mainly to identify the amount null values.\n\ntrain.info()     #We have null values for Age, Cabin & Embarked.","faeae2e7":"# Test data:\n\ntest.info()       #Null values for Age, Fare & Cabin Column","6899aab2":"# Getting information about the rows and column of train and test data set:\n\ntrain.shape, test.shape","7f77dbfc":"# For getting an overall statistics of the train data:\n\ntrain.describe()","ad28f70c":"# 'O' is for getting a statistics of \"Object Data Type\":\n\ntrain.describe(include=['O'])  ","a03949d7":"# Using seaborn's heatmap to explore the missing values:\n\nsns.heatmap(train.isnull(), yticklabels = False, cbar = False)","476e0132":"train.isnull().sum()       #More easy way to count the number null values. ","bcb5d4ae":"# Visualizing Number of passengers Survived or Not:\n\nsns.set(style = 'whitegrid')\nsns.countplot(x = 'Survived', data = train)","06ed604f":"# Visualizing Number of passengers Survived or Not based on Sex:\n\nsns.countplot(x = 'Survived', hue = 'Sex', data = train)","332e760d":"# Visualizing Number of passengers Survived or Not based on Passenger Class:\n\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train, palette = 'rainbow')","5731d002":"# Getting an idea about the age of passengers:\n\nsns.distplot(train['Age'].dropna(), kde = False, color = 'darkgreen', bins = 30)   #setting kde false to view histogram.","43ee08f9":"# Visualizing various price range of ticket: \n\ntrain['Fare'].hist(bins = 30, figsize = (8, 4))","8bd2b45c":"# Finding the mean age of each passenger class:\n\nplt.figure(figsize = (12, 8))\nsns.boxplot(x = 'Pclass', y = 'Age', data = train)","68df207f":"# Setting the Missing Age values with Mean Age on the basis of Passenger Class:\n\ndef mean_age_finder(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","96b8e200":"# Updating the Age column with apply() method:\n\ntrain['Age'] = train[['Age', 'Pclass']].apply(mean_age_finder, axis = 1)","6d1a242b":"# Verifying if there is any null value in Age column:\n\ntrain.isnull().sum()","c467d5e9":"# Now we will replace the two null values of 'Embarked' column.\n# We will replace NaN values with most frequent value of this column: (Port from where the particular passenger was embarked\/boarded): \n\nmost_frequent_embarked = train['Embarked'].value_counts().index[0]      #Output: 'S'\ntrain['Embarked'].fillna(most_frequent_embarked, inplace = True)","84e358b8":"# Verifying if there is null values in Embarked column. Also we can see that Cabin column has a lot of null values. So we will drop it.\n\ntrain.isnull().sum()","42f75389":"# Dropping `Cabin` column from train data:\n\ntrain.drop('Cabin', axis = 1, inplace = True)","fa3bcb0f":"# Verifying if there is any null values remains in our train data:\n\n# sns.heatmap(train.isnull(), yticklabels = False, cbar = False)     #Way 1\ntrain.isnull().sum()                                                 #Way 2 (I prefer this method)","47b4cc0e":"# Finding the mean age of each passenger class:\n\nplt.figure(figsize = (12, 8))\nsns.boxplot(x = 'Pclass', y = 'Age', data = test)","f8663626":"# Setting the Missing Age values with Mean Age on the basis of Passenger Class:\n\ndef mean_age_finder_test(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 42\n        elif Pclass == 2:\n            return 27\n        else:\n            return 24\n    else:\n        return Age","c0ac9e0b":"# Updating the Age column using apply() method:\n\ntest['Age'] = test[['Age', 'Pclass']].apply(mean_age_finder_test, axis = 1)","867caf0c":"# Verifying if there is any null value in Age column:\n\ntest.isnull().sum()","311ca8fe":"#Replacing the missing value of 'Fare' column with mean value:\n\ntest['Fare'].fillna(test['Fare'].mean(), inplace = True)","78ea8dfc":"# Dropping `Cabin` column from test data:\n\ntest.drop('Cabin', axis = 1, inplace = True)","971282b8":"# Verifying if there is any null value in test data:\n\ntest.isnull().sum()","f1f5298d":"# Verifying is there is any null values remains in our test data (Another Way):\n\nsns.heatmap(test.isnull(), yticklabels = False, cbar = False)","7433e8e3":"# Setting drop_first = True is for preventing multicollinearity, as one column is opposite of another. \n\nsex = pd.get_dummies(train['Sex'], drop_first = True)\nembarked = pd.get_dummies(train['Embarked'], drop_first = True)","d1910259":"# Removing Categorical Columns and added newly converted dummy columns:\n\ntrain.drop(['Name', 'Sex', 'Ticket', 'Embarked'], axis = 1, inplace = True)\ntrain = pd.concat([train, sex, embarked], axis = 1)","504c099c":"# Viewing train data after dropping and adding columns:\n\ntrain.head()","635f8417":"# Creating dummy variables:\n\nsex = pd.get_dummies(test['Sex'], drop_first = True)\nembarked = pd.get_dummies(test['Embarked'], drop_first = True)","b1249bdc":"# Removing Categorical Columns and added newly converted dummy columns (test data):\n\ntest.drop(['Name', 'Sex', 'Ticket', 'Embarked'], axis = 1, inplace = True)\ntest = pd.concat([test, sex, embarked], axis = 1)","034482e1":"# Viewing test data after dropping and adding columns:\n\ntest.head()","bf59b0ab":"X_train = train.drop(['PassengerId', 'Survived'], axis = 1)\ny_train = train['Survived']\nX_test = test.drop(\"PassengerId\", axis=1).copy()","53165415":"X_train.shape, y_train.shape, X_test.shape","afa987ad":"from sklearn.linear_model import LogisticRegression\n\nlogmodel = LogisticRegression(solver = 'liblinear')\nlogmodel.fit(X_train, y_train)\n\nlog_predictions = logmodel.predict(X_test)","a9fb8bdc":"accuracy = round(logmodel.score(X_train, y_train) * 100, 2)\nprint('Logistic Regression Accuracy: ', accuracy)","54c9d032":"Submission = pd.DataFrame({\n    \"PassengerId\": test['PassengerId'],\n    \"Survived\": log_predictions\n})\n\nSubmission.to_csv('Submission.csv', index = False)","c0045c5f":"# Viewing the submission file: \n\nSubmission.head()","6a4a7888":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)      # Model's Accuracy depends on number of neighbors.\nknn.fit(X_train, y_train)\nknn_predctions = knn.predict(X_test)\n\nknn_accuracy = round(knn.score(X_train, y_train) * 100, 2)\nprint('KNN Model Accuracy: ', knn_accuracy)","97ca2d58":"knnOut = pd.DataFrame({\n    \"PassengerId\": test['PassengerId'],\n    \"Survived\": knn_predctions\n})\n\nknnOut.to_csv('KNN.csv', index = False)","324e0bc2":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\ndtree_predictions = dtree.predict(X_test)\n\ndtree_accuracy = round(dtree.score(X_train, y_train) * 100, 2)\nprint('Decision Tree Model Accuracy: ', dtree_accuracy)","667af72d":"dtreeOut = pd.DataFrame({\n    \"PassengerId\": test['PassengerId'],\n    \"Survived\": dtree_predictions\n})\n\ndtreeOut.to_csv('dtree.csv', index = False)","e1084109":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100)\nrfc.fit(X_train, y_train)\nrfc_predictions = rfc.predict(X_test)\n\nrfc_accuracy = round(rfc.score(X_train, y_train) * 100, 2)\nprint('Random Forest Model Accuracy: ', rfc_accuracy)","a6dfee41":"rfcOut = pd.DataFrame({\n    \"PassengerId\": test['PassengerId'],\n    \"Survived\": rfc_predictions\n})\n\nrfcOut.to_csv('rfc.csv', index = False)","0653b98b":"from sklearn.svm import SVC\nsvc = SVC(gamma = 'auto')\nsvc.fit(X_train, y_train)\nsvc_predictions = svc.predict(X_test)\n\nsvc_accuracy = round(svc.score(X_train, y_train) * 100, 2)\nprint('SVC Accuracy: ', svc_accuracy)","60464f08":"svcOut = pd.DataFrame({\n    \"PassengerId\": test['PassengerId'],\n    \"Survived\": svc_predictions\n})\n\nsvcOut.to_csv('rfc.csv', index = False)","72b51e3f":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'K-Nearest Neighbors','Decision Tree', 'Random Forest', 'Support Vector Machines'],\n    \n    'Score': [accuracy, knn_accuracy, dtree_accuracy, \n              rfc_accuracy,  svc_accuracy]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","09041e37":"We will now drop the `Cabin` Column as it contains too many null values. ","722c944d":"__So our train data is now free from null values.__","a4a635c9":"### KNN:","8b547de1":"# Data Cleaning","b03f1f27":"# Model Comparison:","fe9eac00":"## Train Data:","fa1e97aa":"**Comments regarding null values found in the heatmap avove:**   \nWe can see that roughly 25% of the `Age` column contains null values. Also `Cabin` column contains more than 80% null values.  ","6b98c482":">Getting more information about our data. ","60f88f06":"#### Decision Tree Output File:","d679e9f6":"# Converting Categorical Features","d5e752a3":"### Random Forest","104ced8e":"#### We will fill the missing `Age` data with the mean age of passenger of each class. ","6b8ce105":"#### We will convert the categorical features to dummy variables using pandas. Otherwise our Machine Learning algorithm will not be able to take them as inputs. ","a3b30ee4":"### Decision Tree:","3998f65b":"# Feature Selection","0c599339":"__Our `test` data is also free from Null values now.__","01075818":"#### KNN Output File:","936887d0":"__I am thankful to__:\n- [Titanic Solution: A Beginner's Guide](https:\/\/www.kaggle.com\/chapagain\/titanic-solution-a-beginner-s-guide?scriptVersionId=1473689)\n- [\u099c\u09c1\u09aa\u09bf\u099f\u09be\u09b0\u09c7 \u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f \u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995](https:\/\/rakibul-hassan.gitbook.io\/mlbook-titanic\/j_notebook\/titanic-project-test#undefined-6)\n- [Python for Data Science and Machine Learning Bootcamp**](https:\/\/www.udemy.com\/python-for-data-science-and-machine-learning-bootcamp\/)","a737a50f":"#### Test Data:","3d20bfaa":"## Test Data:","c6a97912":"# Applying Logistic Regression Model","9d53e138":"# Further Improvement","bcf9475c":"#### Training Data:","2318a6f9":"### Support Vector Machine (SVM):","a69dd4c5":"# __Exploratory data analysis by Visualization (Using training data)__","8a081ae2":"__The Below Cell is required for working in Kaggle Enviornment. So we need to run the below cell and the output shows if we are good to go or not.__","3b1ff2b2":"# First Submission File","d4efd663":"*My first submission was using Logistic Regression. The accuracy was good. But we can improve the accuracy using more some advanced algorithm. Below I tested some algorithm to identify which algorithm gives the best accuracy.*   \n\n  *All the accuracy scores are based on our training dataset.* ","00cc4dea":"#### Random forest output file:","a4c88ea9":"#### SVC Output file:"}}