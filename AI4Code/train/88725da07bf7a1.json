{"cell_type":{"45076039":"code","a3dd2413":"code","e04a1ff3":"code","812ffb97":"code","e7cb1eac":"code","d3a3bd62":"code","58402320":"code","5d0b831b":"code","380e82c2":"code","ae87ca13":"code","9f096542":"markdown","4f0cd5bc":"markdown","8f8bcb6f":"markdown","74001a9d":"markdown","fb1d0b04":"markdown","6247252d":"markdown","804a4138":"markdown","728f0729":"markdown","5bda3579":"markdown","02c279c4":"markdown","91514a81":"markdown","37a23f0a":"markdown","fe96ff7a":"markdown"},"source":{"45076039":"#chrome\n!wget https:\/\/dl.google.com\/linux\/direct\/google-chrome-stable_current_amd64.deb\n!apt install .\/google-chrome-stable_current_amd64.deb -y\n#chromedriver\n!wget https:\/\/chromedriver.storage.googleapis.com\/88.0.4324.96\/chromedriver_linux64.zip\n!unzip chromedriver_linux64.zip\n!mv chromedriver \/usr\/local\/bin\n!chown root:root \/usr\/local\/bin\/chromedriver\n!chmod +x \/usr\/local\/bin\/chromedriver","a3dd2413":"import pandas as pd\nimport re\nimport numpy as np\n!pip install bs4\n!pip install selenium\nfrom bs4 import BeautifulSoup\nimport requests\nimport ftplib \nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait as wait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom datetime import datetime, date, timezone\nfrom selenium.common.exceptions import TimeoutException\n\n#download listed stocks to get symbols\nFTP_host = 'ftp.nasdaqtrader.com'\nFTP = ftplib.FTP()\nFTP.connect(FTP_host)\nFTP.login()\nFTP.cwd('\/symboldirectory')\nfilename = 'nasdaqlisted.txt'\nlocalfile = open(filename, 'wb')\nFTP.retrbinary(\"RETR nasdaqlisted.txt\", localfile.write)\nFTP.close()\nlocalfile.close()\n","e04a1ff3":"#read the downloaded symbols file and format as dataframe\nsymbols = pd.read_csv('nasdaqlisted.txt', sep='delimiter', header=None, engine='python')\nheadings = symbols.iloc[0][0].split('|')\nsymbols[headings] = symbols[0].str.split('|', expand=True)\ndel symbols[0]\nsymbols = symbols.iloc[1:].reset_index(drop=True)\n\nprint(symbols.head())\n#create a sample to reduce run time\nsample = symbols.head()","812ffb97":"db = pd.DataFrame()\n#to capture the stocks for which financial data does not exist\nerrors = []\ni = 0\nwhile i < len(sample):\n    # Enter a stock symbol\n    index = sample.loc[i, 'Symbol']\n    \n    # URL links for the various sections we want to extract information from\n    url_is = 'https:\/\/finance.yahoo.com\/quote\/' + index + '\/financials?p=' + index\n    url_bs = 'https:\/\/finance.yahoo.com\/quote\/' + index + '\/balance-sheet?p=' + index\n    url_cf = 'https:\/\/finance.yahoo.com\/quote\/' + index + '\/cash-flow?p=' + index\n    leave = 0\n    \n    for info in [url_is, url_bs, url_cf]:\n        df = pd.DataFrame()\n        res = requests.get(info)\n        soup = BeautifulSoup(res.content,'lxml')\n            \n        headings = []\n        table = soup.find('section', attrs={'data-test':'qsp-financial'})\n        if table == None:\n            errors.append(i)\n            leave = 1\n            i += 1\n            break\n        \n        \n        #if we don't get a successful response, there may be a short term restriction on our IP address. Simple fix is to put the scraper to rest for a short period.\n        check = table.find_all('div')\n        if check[-1].text == 'Please try reloading the page.':\n            print('Timeout. Rest for 5 minutes')\n            time.sleep(300)\n        \n        #Sometimes the data does not completely load on first attempt so this gives a few opportunities to reload the page and extract the data\n        retries = 0\n        while retries < 3:\n            try:\n                for l in table.find_all('span', attrs={'class':'Va(m)'}): \n                    headings.append(l.string) # add each element one by one to the list\n                data = table.find_all('div', attrs={'data-test':'fin-row'})\n                dateData = table.find('div', attrs={'class':'D(tbhg)'}).find_all('div')\n                break\n            except AttributeError:\n                res = requests.get(info)\n                soup = BeautifulSoup(res.content,'lxml')\n                table = soup.find('section', attrs={'data-test':'qsp-financial'})\n                retries += 1\n        \n        #Now if we do not find any data, we simply add the stock to the error list and move on to the next stock in the list\n        if retries == 3:\n            errors.append(i)\n            leave = 1\n            i += 1\n            break\n        \n        #The below code will place the desired data in a pandas dataframe and arrange it\n        ls = [] # Create empty list\n        for j in range(len(data)):\n            a = data[j].find_all('div')\n            b = [ele.text for ele in a if len(ele.find_all('div')) == 0][1:]\n            ls.append([x for x in b if x])\n            \n        df = pd.DataFrame(ls).transpose()\n        df.columns = headings\n        \n        dates = []    \n        b = [ele.text for ele in dateData if len(ele.find_all('div')) == 0]\n        dates.append([x for x in b if x])\n            \n        df['date'] = dates[0]\n        cols = list(df)\n        cols.insert(0, cols.pop(cols.index('date')))\n        df = df.loc[:, cols]\n        df['stock'] = index\n        cols = list(df)\n        cols.insert(1, cols.pop(cols.index('stock')))\n        df = df.loc[:, cols]\n        \n        if info == url_is:\n            comb = df\n        else:\n            comb = comb.merge(df, on=['date', 'stock'], how='left')\n            \n    if leave == 1:\n        continue\n    else:\n        i += 1\n        db = pd.concat([db, comb], ignore_index=True)   \n\ndb.to_csv('stockScrape.csv')\n    ","e7cb1eac":"db = pd.DataFrame()\ni = 0\nchrome_options = Options()\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_options.binary_location=\"\/usr\/bin\/google-chrome\"\n\ndriver = webdriver.Chrome('\/usr\/local\/bin\/chromedriver', options=chrome_options)\nwhile i < len(sample):\n    \n    index = sample.loc[i, 'Symbol']\n    \n    #there are a variety of stocks in our original list that are not the actual company stock price, rather other issues such as warrants or rights.\n    match = re.search(r'\\b(right|unit|warrant|subordinated)\\b',sample.loc[i, 'Security Name'].lower())\n    if match:    \n        i+=1\n        continue\n    \n    url = 'https:\/\/www.morningstar.com\/stocks\/xnas\/' + index + '\/price-fair-value'\n    driver.get(url)\n    \n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')    \n    \n    #Some stocks won't have any data here so we will skip them\n    try:\n        if soup.find('div', attrs={'class':'error__heading'}).text == 'Like guarantees of future returns, this page doesn\u2019t exist.':\n            i+=1\n            continue\n    except AttributeError:\n        pass\n    \n    element = wait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'total-table')))\n    \n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')\n    table = soup.find('table', attrs={'class':'total-table'})\n    headings = table.find('tr', attrs={'class':'thead'}).find_all('td')\n    headings = [e.text.strip() for e in headings]\n    returns = table.find_all('tr', attrs={'class':'annual-data-row'})[1].find_all('td')\n    returns = [e.text.strip() for e in returns]\n    returns = pd.DataFrame(returns).transpose()\n    returns.columns = headings\n    returns['stock'] = index\n    \n    db = pd.concat([db, returns], ignore_index=True)   \n    i+=1\n\ndriver.close()\ndb.to_csv('stockPriceHistory.csv')\n\n","d3a3bd62":"print(db)","58402320":"#scrape valuation data history from morningstar\ndb = pd.DataFrame()\ni = 0\ndriver = webdriver.Chrome('\/usr\/local\/bin\/chromedriver', options=chrome_options)\nwhile i < len(sample):\n    \n    index = sample.loc[i, 'Symbol']\n    #there are a variety of stocks in our original list that are not the actual company stock price, rather other issues such as warrants or rights.\n    match = re.search(r'\\b(right|unit|warrant|subordinated)\\b',sample.loc[i, 'Security Name'].lower())\n    if match:    \n        i+=1\n        continue\n    \n    url = 'https:\/\/www.morningstar.com\/stocks\/xnas\/' + index + '\/valuation'\n    driver.get(url)\n    \n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')    \n    try:\n        #if data doesn't exist for the stock, we skip\n        if soup.find('div', attrs={'class':'error__heading'}).text in ['Like guarantees of future returns, this page doesn\u2019t exist.']:\n            i+=1\n            continue\n        #sometimes there is just a loading error. This exception will handle that and reload the page.\n        if soup.find('div', attrs={'class':'error__heading'}).text in ['This isn\u2019t working, but it\u2019s not your fault.']:\n            continue\n        \n    except AttributeError:\n        pass\n    \n    try:\n        element = wait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'sal-component-ctn.sal-component-bar-chart')))\n    except TimeoutException:\n        try:\n            html = driver.execute_script('return document.body.innerHTML;')\n            soup = BeautifulSoup(html,'lxml')    \n            #if data doesn't exist for the stock, we skip\n            if soup.find('div', attrs={'class':'no-available-data ng-scope'}).text.strip() in ['There is no Valuation data available.']:\n                i+=1\n                continue\n        except AttributeError:\n            continue\n    \n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')\n    try:\n        if soup.find('div', attrs={'class':'no-available-data ng-scope'}).text.strip() in ['There is no Valuation data available.']:\n            i+=1\n            continue\n    except AttributeError:\n        pass\n    \n    #extract the table headings\n    table = soup.find('table', attrs={'class':'report-table ng-isolate-scope'})\n    headings = table.find('tr', attrs={'class':'thead ng-scope'}).find_all('td')\n    headings = [e.text.strip() for e in headings]\n    #extract the data\n    returns = table.find_all('tr', attrs={'class':'report-table-row ng-scope'})\n    info  = []\n    for j in range(len(returns)):\n        current = returns[j].find_all('td')\n        current  = [e.text.strip() for e in current]\n        info.append(current)\n    \n    returns = pd.DataFrame(info)\n    returns.columns = headings\n    for col in headings:\n        if col in ['5-Yr', 'Index']:\n            del returns[col]\n    \n    returns = pd.DataFrame(returns).transpose().reset_index()\n    returns = returns.rename(columns=returns.iloc[0])\n    returns = returns.drop(returns.index[0]).reset_index(drop=True)\n    returns['stock'] = index\n    \n    db = pd.concat([db, returns], ignore_index=True)   \n    i+=1\n\ndb.to_csv('valuationScrape.csv')\ndriver.close()","5d0b831b":"print(db.head())","380e82c2":"financials = pd.read_csv('stockScrape.csv', index_col=0)\n\n#we are assuming that for all stocks, ttm numbers are equal to end of 2020 numbers. So remove ttm numbers (ttm = trailing twelve months)\nfinancials = financials[financials['date'] != 'ttm'].reset_index(drop=True)\n\n#get an idea of which columns appear in the majority of companies financials\ncols = []\nfor col in financials.columns:\n    cols.append(len(financials[financials[col].isna()]) \/ len(financials))\n\ncols = pd.DataFrame(cols).transpose()\ncols.columns = financials.columns\ntest = cols.transpose().reset_index()\n#retain only the information items that are in more than 75% of the companies in the dataset\nheadings = np.array(test[test[0] < 0.25]['index'])\nfinancialsNew = financials[headings]\n\n#we have over 2000 instances of companies that have their reporting at calendar year end. Enough for our analysis, as this will only give examples that \/\n#line up with the annual returns sourced from Morningstar\nstocks = financialsNew.loc[financialsNew['date'] == '12\/31\/2019', 'stock'].reset_index(drop=True)\ndf = financialsNew[financialsNew['stock'].isin(np.array(stocks))].reset_index(drop=True)\n\ndf['year'] = pd.to_datetime(df['date'], infer_datetime_format=True)\ndf['year'] = [x.year for x in df['year']]\ndf['stock'] = df['stock'].astype('|S').str.decode('utf-8') \n\n#rearrange columns\ncols = list(df)\ncols.insert(2, cols.pop(cols.index('year')))\ndf = df.loc[:, cols]\n\n#now bring in other valuation data for these companies and dates.\nvaluation = pd.read_csv('valuationScrape.csv', index_col=0)\n\ncols = []\nfor col in valuation.columns:\n    cols.append(len(valuation[valuation[col].isna()]) \/ len(valuation))\n\ncols = pd.DataFrame(cols).transpose()\ncols.columns = valuation.columns\ntest = cols.transpose().reset_index()\n#retain only the information items that are in more than 95% of the companies in the dataset\nheadings = np.array(test[test[0] < 0.05]['index'])\nvalNew = valuation[headings]\nvalNew = valNew.rename(columns={'Calendar':'year'})\nvalNew['stock'] = valNew['stock'].astype('|S').str.decode('utf-8') \nvalNew = valNew[valNew['year'] != 'Current'].reset_index(drop=True)\nvalNew['year'] = valNew['year'].astype(str).astype(int)\n\n#merge datasets to bring in valuation data\ndf = df.merge(valNew, on=['year', 'stock'], how='left').fillna(0)\ntry:\n    df['Enterprise Value (Mil)'] = df['Enterprise Value (Mil)'].replace(['#VALUE!'], 0)\nexcept KeyError:\n    pass\n   \n#now bring in stock price annual returns\nreturns = pd.read_csv('stockPriceHistory.csv', index_col=0)\n\n#unstack data to get it in a form where we can do an easy merge\na = returns.iloc[:, :-2].unstack().reset_index()\nfor i in range(len(a)):\n    a.loc[i, 'stock'] = returns.loc[a.loc[i, 'level_1'], 'stock']\n\ndel a['level_1']\na.columns = ['year', 'return', 'stock']\na['stock'] = a['stock'].astype('|S').str.decode('utf-8') \na['year'] = a['year'].astype(str).astype(int)\na = a[a['year'] > 2015].reset_index(drop=True)\na['return'] = a['return'].fillna(0)\n\n#final merge to bring in annual return data\ndf = df.merge(a, on=['year', 'stock'], how='left').fillna(0)\n","ae87ca13":"print(df)","9f096542":"# Conclusion\n\nNow we can go ahead and do some analysis on the data to try and determine an algorithm that could produce a profitable portfolio of stocks based on the historical data we have collected. I will leave that part up to you! Comment below your ideas that could achieve such an algorithm or any other thoughts!","4f0cd5bc":"# Extracting financial data from Yahoo Finance\n\nThe code below will now loop through our list of stock symbols above and add the information available from Yahoo Finance for the specific stock. In this section we are specifically extracting the financials, balance sheet and cash flow information. ","8f8bcb6f":"# Collate data in the one dataset for analysis\n\nTo bring all the above together we can arrange the various datasets and merge them to collate all the information we have extracted with the below code.","74001a9d":"# Extract valuation data for stocks from Morningstar\n\nThe final piece of data we wish to extract is the valuation data. (Information on price\/sales, price\/book value ratios and so on.) This we will extract from Morningstar as well.","fb1d0b04":"![](https:\/\/miro.medium.com\/max\/3504\/1*NpT5pyemQQsGEHXbfS51Zw.png)","6247252d":"Our output shows annual returns for each stock as desired.","804a4138":"# Introduction\n\nThis post will show you how to do the following:\n\n1. Extract a list of stock symbols that are current active on the NASDAQ index.\n2. Extract financial information by year for these stocks from Yahoo Finance.\n3. Extract historical stock price information and valuation data for these stocks from Morningstar.\n4. Collate data for analysis.\n\nAll code is provided to replicate in Python.","728f0729":"Now we will clean up the data to be able to create a scraping process to loop through each stock symbol in our list and extract the financial information and historical stock price information for each listed stock from Yahoo Finance.","5bda3579":"# Import listed of NASDAQ stock symbols\n\nFirst we import modules required and download list of current NASDAQ listed stocks from the source below.","02c279c4":"Now we have valuation data by year for each stock as desired as well.","91514a81":"# Extracting historical returns for stocks from Morningstar\n\nNow we have a pandas dataframe with financial information for all the stocks in the list for all data points available on Yahoo Finance (currently 2017 - 2020). Next we want to add some information on historical stock price. For this we will use Morningstar. What we are extracting below is the annual return for each stock from 2011. (These are calendar year end measures typically). \n\nHere is the code for the scrape.","37a23f0a":"# Prerequisites\n\nDownload relevant chrome and chromedriver files to be used later in our data scraping bot.","fe96ff7a":"# Done!\n\nNow we have our final result of a dataset that contains all desired data for each stock, including:\n1. Financial Data (Balance Sheet, Income Statement & Cash Flow)\n2. Valuation Data (Price\/Sales, Price\/Book etc ratios)\n3. Annual stock returns\n\nSample printed below."}}