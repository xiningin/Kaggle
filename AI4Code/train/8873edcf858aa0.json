{"cell_type":{"c449d0de":"code","2cb73908":"code","0b82c8ff":"code","1da2deb9":"code","74533753":"code","919cefeb":"code","ef50b4ca":"code","b9c4efc0":"code","87d487d4":"code","12872f9c":"code","f5725030":"code","a18d8832":"code","a3bff8be":"code","23d78b74":"code","66e79035":"code","a45987b4":"code","d8553b48":"code","e3edb62b":"code","64acc2c2":"code","4a1632fb":"code","9c893160":"code","f11b61ec":"code","4da3f517":"code","5260a141":"code","3ed9c4fe":"code","3206ae1f":"code","412e5143":"code","4f9c92d0":"code","aaa7ec87":"markdown","00dc7aff":"markdown","358d3bbd":"markdown","20ee0746":"markdown","03d0c7af":"markdown","1e35942b":"markdown","ce30ce28":"markdown","10161406":"markdown","c396e1e6":"markdown","618173b9":"markdown","96c38b3f":"markdown","1c971703":"markdown","85081d36":"markdown"},"source":{"c449d0de":"### Import libraries\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n## Packages for basic text processing\nimport re\nimport string \n\n## Visualization tools\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n## Import Logistic Regression function for model training from linear model module of sklearn package\nfrom sklearn.linear_model import LogisticRegression\n\n## Import functions for evaluation from  model selection module of sklearn package\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n## Import functions for model selection from model selection module of sklearn package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n## Import functions for text vectorization from feature extraction module of sklearn package\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n## Import function from NLTK package for Text tokenization and Normalization\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n## Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2cb73908":"tweets_raw = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntweets_raw_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","0b82c8ff":"tweets_raw","1da2deb9":"# Tweets about real disaster\ntweets_raw[tweets_raw[\"target\"] == 1][\"text\"].values[:5]\n","74533753":"# Tweets not about real disaster\ntweets_raw[tweets_raw[\"target\"] == 0][\"text\"].values[:5]\n","919cefeb":"# Check for missing data\ntweets_raw.isna().sum()","ef50b4ca":"# Discarding columns except text and target\ntweets=tweets_raw[['text','target']]","b9c4efc0":"# Number of occurences of real disasters\nsns.countplot(x=tweets[\"target\"])","87d487d4":"print(f'{tweets.target[tweets.target==1].count()\/tweets.target.count()*100:.2f} % of tweets are labeled as disaster tweets in data')","12872f9c":"# Check for missing data\ntweets.isna().sum()","f5725030":"# Splitting data set into 80:20 ratio\ntrain, test = train_test_split(tweets,test_size=0.25,random_state=8)","a18d8832":"stop_words = stopwords.words('english')\nstemmer = PorterStemmer()\ndef lower_text(text):\n    \"\"\"\n        function to convert text into lowercase\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = text.lower() # lowering\n    return text\n    \ndef remove_newline(text):\n    \"\"\"\n        function to remove new line characters in text\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = re.sub(r'\\n',' ', text)\n    return text\n\ndef remove_punctuations(text):\n    \"\"\"\n        function to remove punctuations from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    return text\n\n\ndef remove_links(text):\n    \"\"\"\n        function to links and urls from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    \n    return text\n\ndef remove_tags(text):\n    \"\"\"\n        function to remove references and hashtags from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\"\",text)\n    return text\n    \ndef remove_multiplespaces(text):\n    \"\"\"\n        function to remove multiple spaces from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    return text\n\ndef remove_specialchars(text):\n    \"\"\"\n        function to remove special characters from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\W', ' ', text)\n    return text\n\ndef remove_stopwords(text):\n    \"\"\"\n        function to tokenize the words using nltk word tokenizer and remove the stop words using nltk package's english stop words\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n    return text\n\ndef word_stemming(text):\n    \"\"\"\n        function to perform stemming using porter stemmer from nltk package\n        input: text\n        output: cleaned text\n    \"\"\"        \n    text=' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n    return text","a3bff8be":"# Covert text to lowercase\ntrain.text=train.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntrain.text=train.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntrain.text=train.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntrain.text=train.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntrain.text=train.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntrain.text=train.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntrain.text=train.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntrain.text=train.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntrain.text=train.text.apply(lambda text: word_stemming(text))","23d78b74":"# Covert text to lowercase\ntest.text=test.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntest.text=test.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntest.text=test.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntest.text=test.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntest.text=test.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntest.text=test.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntest.text=test.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntest.text=test.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntest.text=test.text.apply(lambda text: word_stemming(text))\n\n","66e79035":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 12))\n\nwc_disaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==1]['text']))\n\nwc_nondisaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==0]['text']))\n\nax1.imshow(wc_disaster)\nax1.set_title(\"Word cloud of disaster tweets\", fontsize=20)\nax1.axis(\"off\")\n\nax2.imshow(wc_nondisaster)\nax2.set_title(\"Word cloud of non disaster tweets\", fontsize=20)\nax2.axis(\"off\")\n\nfig.show()","a45987b4":"# Bag-of-words\ncount_vectorizer = CountVectorizer()\ntrain_vectors_bow = count_vectorizer.fit_transform(train[\"text\"])\ntest_vectors_bow = count_vectorizer.transform(test[\"text\"])\n\n# TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\ntrain_vectors_tf = tfidf_vectorizer.fit_transform(train[\"text\"])\ntest_vectors_tf = tfidf_vectorizer.transform(test[\"text\"])\n\n","d8553b48":"# Bag-of-words\nclf = LogisticRegression()\n\nprint(\"Bag-of-words:\\n\")\nscores = cross_val_score(clf, train_vectors_bow, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors_bow, train[\"target\"])\n\n# TF-IDF\nclf = LogisticRegression()\n\nprint(\"\\nTF-IDF:\\n\")\nscores = cross_val_score(clf, train_vectors_tf, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors_tf, train[\"target\"])","e3edb62b":"# Bag-of-words\ntest[\"pred\"] = clf.predict(test_vectors_bow)\n\nprint(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))\n\n\n","64acc2c2":"# Bag-of-words\ntn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn\/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp\/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn\/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp\/tot)*100))\n\n","4a1632fb":"# TF-IDF\ntest[\"pred\"] = clf.predict(test_vectors_tf)\n\nprint(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))\n","9c893160":"# TF_IDF\ntn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn\/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp\/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn\/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp\/tot)*100))\n\n","f11b61ec":"# Import data\ncomp_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\n# Check for missing data\ncomp_test.isna().sum()","4da3f517":"comp_test","5260a141":"# Isolating text column\ntest=comp_test[['text','id']]","3ed9c4fe":"## Cleaning and normalizing data\n\n# Covert text to lowercase\ntest.text=test.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntest.text=test.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntest.text=test.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntest.text=test.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntest.text=test.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntest.text=test.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntest.text=test.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntest.text=test.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntest.text=test.text.apply(lambda text: word_stemming(text))\n","3206ae1f":"## Predictions\n\n\ntest_vectors_tf = tfidf_vectorizer.transform(test[\"text\"])\n\ntest[\"pred\"] = clf.predict(test_vectors_tf)\n","412e5143":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsample_submission[\"target\"] = test[\"pred\"]\n\nsample_submission.to_csv(\"submission.csv\", index=False)\n\nsample_submission[:25]\n\n\n\n","4f9c92d0":"sample_submission","aaa7ec87":"## Loading data","00dc7aff":"## Training model\n\nWe will use logistic regression to train this model\n.","358d3bbd":"### Making predictions from competition test data","20ee0746":"### Cleaning and normalizing validation dataset","03d0c7af":"## Text Cleaning and Normalization\n\nWe will:\n\n *  Lowercase all words in text\n *  Remove newline characters if any in text\n *  Remove punctuations\n *  Remove url's and links from text\n *   Remove tags from text\n *  Remove multiple spaces from text\n *   Remove special characters\n *  Remove stop words from text\n *   Apply stemming to normalize the text\n\n\n","1e35942b":"## Splitting dataset into training and validation sets","ce30ce28":"## Exploratory Data Analysis (EDA)\n\n","10161406":"### Making predictions using model\n","c396e1e6":"### Cleaning and normalizing test dataset","618173b9":"## Text visualization","96c38b3f":"**Name:** Disaster Tweet Classifier\n\n**Author:** Sharome Burton\n\n**Date:** 07\/19\/2021\n\n**Description:** Machine learning model used to predict whether a tweet is about a real disaster or not.\n\n## 1. Problem definition\n> How well can we predict whether a tweet is about a disaster or not?\n\n## 2. Data\n    * `train.csv` - the training set\n    * `test.csv` - the test set\n    \n source: https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data\n\n   \n## 3. Evaluation \n\n> **Goal:** Predict the whether a tweet is about a disaster or not with >80% accuracy.\n\n## 4. Features\n\n   * id - a unique identifier for each tweet\n   * text - the text of the tweet\n   * location - the location the tweet was sent from (may be blank)\n   * keyword - a particular keyword from the tweet (may be blank)\n   * target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","1c971703":"## Exporting predictions","85081d36":"## Text vectorization\n\nTo process the text data first we need to convert text data into numerical representation for systems to learn further from data. Vectorization is the process of converting a word ito a vector of numbers that contains the information in the word.\n\nWe will try:\n* Bag-of-words vectorization\n* TF-IDF vectorization"}}