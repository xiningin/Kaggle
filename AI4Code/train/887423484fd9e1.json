{"cell_type":{"cdd3f10c":"code","d31e71b3":"code","2c9a1672":"code","cd29eedf":"code","81637333":"code","44254dae":"code","6fe077bb":"code","3344f86b":"code","2b8d5146":"code","7ff07196":"code","d7d5edaf":"code","d99a8775":"code","9b375a90":"code","369b9203":"code","9f230ed4":"code","4da6d302":"code","4993479a":"code","7c4aa383":"markdown","06e5258f":"markdown","e990a05b":"markdown","404c6a70":"markdown","ca2b13eb":"markdown","5cff1b61":"markdown","8e8fe036":"markdown","1fd7d7b5":"markdown","ea5952f8":"markdown"},"source":{"cdd3f10c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob \nfrom skimage.io import imread #read images from files\nimport os\nimport keras.backend as K\nimport tensorflow as tf","d31e71b3":"base_tile_dir = '..\/input\/train\/'\ndf = pd.DataFrame({'path': glob(os.path.join(base_tile_dir,'*.tif'))})\ndf['id'] = df.path.map(lambda x: x.split('\/')[3].split(\".\")[0])\nlabels = pd.read_csv(\"..\/input\/train_labels.csv\")\ndf = df.merge(labels, on = \"id\")\ndf.head(10)","2c9a1672":"df0 = df[df.label == 0].sample(5000, random_state = 42)\ndf1 = df[df.label == 1].sample(5000, random_state = 42)\ndf = pd.concat([df0, df1], ignore_index=True).reset_index()\ndf = df[[\"path\", \"id\", \"label\"]]\ndf.sample(10)","cd29eedf":"df['image'] = df['path'].map(imread)\ndf.sample(3)","81637333":"import matplotlib.pyplot as plt\n\nimages = [(df['image'][0], df['label'][0]), \n          (df['image'][1], df['label'][1]),\n          (df['image'][2], df['label'][2]),\n          (df['image'][5000], df['label'][5000]),\n          (df['image'][5001], df['label'][5001]),\n          (df['image'][5002], df['label'][5002])]\n\nfig, m_axs = plt.subplots(1, len(images), figsize = (20, 2))\n#show the images and label them\nfor ii, c_ax in enumerate(m_axs):\n    c_ax.imshow(images[ii][0])\n    c_ax.set_title(images[ii][1])","44254dae":"input_images = np.stack(list(df.image), axis = 0)\ninput_images.shape","6fe077bb":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\n\n\ntrain_fraction = 0.8\n\nencoder = LabelBinarizer()\ny = encoder.fit_transform(df.label)\nx = input_images\n\ntrain_tensors, test_tensors, train_targets, test_targets =\\\n    train_test_split(x, y, train_size = train_fraction, random_state = 42)\n\nval_size = int(0.5*len(test_tensors))\n\nval_tensors = test_tensors[:val_size]\nval_targets = test_targets[:val_size]\ntest_tensors = test_tensors[val_size:]\ntest_targets = test_targets[val_size:]","3344f86b":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import Sequential\nfrom tensorflow import set_random_seed\n\nset_random_seed(42)\n\nearly_stopping = EarlyStopping(monitor = 'val_loss', patience = 5)\ncheckpointer = ModelCheckpoint(filepath='weights.hdf5', \n                               verbose=1, save_best_only=True)\nmodel = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3)) \n\nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3)) \n\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3))\n\nmodel.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\nmodel.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\nmodel.add(Conv2D(filters = 256, kernel_size = 3, padding = 'same', activation = 'elu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation = 'sigmoid'))","2b8d5146":"#working AUC metric for keras from here: \n#https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/41015\ndef auc(y_true, y_pred):   \n    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return K.sum(s, axis=0)\n\n#---------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)\n    return FP\/N\n\n#----------------\n# PTA prob true alerts for binary classifier\ndef binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)\n    return TP\/P\n\nfrom keras.optimizers import SGD\n# sgd = SGD(lr = 0.0001, momentum = 0.9)\nmodel.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nepochs = 15\nmodel.fit(train_tensors, train_targets, \n          validation_data=(val_tensors, val_targets),\n          epochs=epochs, batch_size=80, verbose=1, callbacks = [early_stopping, checkpointer])","7ff07196":"model.load_weights('weights.hdf5')\n\ncancer_predictions =  [model.predict(np.expand_dims(tensor, axis=0))[0][0] for tensor in test_tensors]\n\ntest_accuracy = 100*np.sum(np.round(cancer_predictions).astype('int32')==test_targets.flatten())\/len(cancer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","d7d5edaf":"#AUC score\nfrom sklearn.metrics import roc_auc_score\nscore = roc_auc_score(np.round(cancer_predictions).astype('int32'), test_targets)\nscore","d99a8775":"base_tile_dir = '..\/input\/test\/'\ntest_df = pd.DataFrame({'path': glob(os.path.join(base_tile_dir,'*.tif'))})\ntest_df['id'] = test_df.path.map(lambda x: x.split('\/')[3].split(\".\")[0])","9b375a90":"test_df['image'] = test_df['path'].map(imread)","369b9203":"test_images = np.stack(test_df.image, axis = 0)\ntest_images.shape","9f230ed4":"predicted_labels =  [model.predict(np.expand_dims(tensor, axis=0))[0][0] for tensor in test_images]","4da6d302":"predictions = np.array(predicted_labels)\ntest_df['label'] = predictions\nsubmission = test_df[[\"id\", \"label\"]]\nsubmission.head()","4993479a":"#submission\nsubmission.to_csv(\"submission.csv\", index = False, header = True)","7c4aa383":"# Introduction\n____\nIn this kernel I'll show how to get the data in the proper format to use it on a CNN and make the classification. First let's import the necessary libraries.","06e5258f":"Now that we know that both classes are balanced and how many of them there are, we can start looking at the images. To do so, I'll use imread function imported on the first code chunk.","e990a05b":"Great! Now we can see that our images are represented by an array of arrays. To read them, we can make use of either matplotlib or the more computer vision-focused opencv. Here I'll use the first option for simplicity. Let's see two sets of images: one with label 0 and another with label 1.","404c6a70":"Now, before we look at the images, it is important to note that there are LOTS of images and this can easily blow up the kernel's memory if we use all of them in training (considering that we are using a Kaggle's kernel). To avoid this problem and also keep balance in our training data, I'll use 5k examples for each label. ","ca2b13eb":"Next we create a dataframe using the image's path as the first column and the id as the second column (if you don't understand why that split is being used that way, I suggest that you get one of the paths available and try using `.split('\/')[3]...` to see what is going on here. Next we read the labels and merge with our dataframe through their ids, so we know which image corresponds to each label. ","5cff1b61":"Well, the nest accuracy with this kernel is around 80% which is pretty good for such a straightfoward model. And here is the magic of Deep Learning. Without any feature preprocessing nor previous knowledge, we were able to correctly classify around 800 out of 1000 images. Now it's time to make our predictions and submit the results. \n\nNote: The accuracies may be lower than 80% due to some randomness. I'm using previously set random seeds where I can, but I've read that there can be some randomness when running in GPU (which I am for faster kernels) and this makes things harder to follow. However I think this is a good starting point for anyone that wants to get into CNN using keras. You can later check other kernels in this competition that use transfer learning from previously trained neural nets like NasNet, Resnet, Inception, Xception etc.\n\nThe preprocessing steps are very similar to what we did before in the training set.","8e8fe036":"Now our work is pretty straightforward. We split our input_images in training, validation and testing sets and run them through our model.","1fd7d7b5":"Only by looking at the images above I can hardly tell why the three to the right were detected with cancer. Maybe our model can find patterns that we - that don't have any specialized training - can't? Let's create our set of inputs. By using `np.stack` we create a 4-rank tensor: 10000 observations where each one is a 96x96x3 image.","ea5952f8":"Using the trained model to make the predictions"}}