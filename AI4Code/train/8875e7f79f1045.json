{"cell_type":{"aab0d344":"code","1b96fbf9":"code","649bf2af":"code","90ae9a45":"code","595aa121":"code","c7013e29":"code","77342796":"code","acc2d3ea":"code","babd307b":"code","97bdc3b8":"code","67a2e710":"code","eb8f547e":"code","bd1113ea":"code","8d367035":"code","cb72ea18":"code","763d1103":"code","1e3cdabc":"code","c563e005":"code","3340024d":"code","81460631":"code","a7ecc1a7":"code","9090a922":"code","8d550c72":"code","9b39b856":"code","fc2d0f80":"code","ceea9a5f":"code","ae0c0729":"code","30fc2472":"code","cf81fa7a":"code","ee2671cb":"code","ada001c5":"code","bb04248e":"code","dab34a10":"code","769b1d23":"code","c25f2c7d":"code","9adc3214":"code","6d6d7758":"code","c1333860":"code","b76a6048":"code","c2e3d2bd":"code","782873d2":"code","dde5fc7b":"code","24d6ef5f":"code","62450646":"code","557b3e63":"code","530bf1d1":"code","d223b211":"code","bcc0a591":"code","6341b2de":"code","1669eb59":"code","66dec406":"code","8f836e38":"code","3243718b":"code","63b89301":"code","59b3c695":"code","f9f157d7":"markdown","8bc3b8e2":"markdown","5eaddfe3":"markdown","d1b1ac3b":"markdown","12848ea0":"markdown","8189bda0":"markdown","736516c5":"markdown","707f49dc":"markdown","a988393c":"markdown","4f824755":"markdown","9254906c":"markdown","b9916665":"markdown","e776b5c3":"markdown","95c453d8":"markdown","35574daa":"markdown","5848c331":"markdown","1507112f":"markdown","1261951d":"markdown","0c9d6ee4":"markdown","d5651dfe":"markdown","d5904a21":"markdown","7f44e0ee":"markdown","24c05d12":"markdown","cd51f162":"markdown","9a50d93e":"markdown","9f3defa3":"markdown","00686af0":"markdown","9e9fd80d":"markdown","a6f4a5eb":"markdown","09886ebb":"markdown","0f92210d":"markdown","a5f50353":"markdown","8879fe92":"markdown","0debd9fa":"markdown","12c1c74f":"markdown","e2774099":"markdown","2be86c20":"markdown","79a5881b":"markdown","04da5995":"markdown","f9580061":"markdown","3271979f":"markdown","cc346694":"markdown","717cd18d":"markdown","4c9c78eb":"markdown","ba4d2187":"markdown","dfa135fc":"markdown","66d4ab4e":"markdown","3190c453":"markdown","fb706319":"markdown","127ce2e7":"markdown","4a5b79b4":"markdown","598f7eca":"markdown","59e4ad65":"markdown","357a11f0":"markdown"},"source":{"aab0d344":"import pandas as pd\npd.options.display.max_columns=None\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nstyle.use('bmh')\nimport seaborn as sns\nsns.set_style('dark')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport csv","1b96fbf9":"with open('..\/input\/titanic\/train.csv', 'r', encoding='utf-8') as f:\n    train = pd.read_csv(f)\n    train.name = 'Train'\n    \nwith open('..\/input\/titanic\/test.csv', 'r', encoding='utf-8') as f:\n    test = pd.read_csv(f)\n    test.name = 'Test'\n    \ntrain.head(10) # train set","649bf2af":"test.head(10) # test set","90ae9a45":"train.info()\nprint('\\n', '=' * 40, '\\n')\ntest.info()","595aa121":"print('For Train: ')\ntrain.describe(include='all') # looking at all columns","c7013e29":"print('For Test; ')\ntest.describe(include='all')","77342796":"train[['Pclass', 'Survived']].groupby('Pclass').mean().sort_values(by='Survived', ascending=False) # with Pclass","acc2d3ea":"train[['Sex', 'Survived']].groupby('Sex').mean().sort_values(by='Survived', ascending=False) # with gender","babd307b":"train[['Parch', 'Survived']].groupby('Parch').mean().sort_values(by='Survived', ascending=False) # with parents or children","97bdc3b8":"train[['SibSp', 'Survived']].groupby('SibSp').mean().sort_values(by='Survived', ascending=False) # with siblings or spouses","67a2e710":"train[['Embarked', 'Survived']].groupby('Embarked').mean().sort_values(by='Survived', ascending=False) # with embarked location","eb8f547e":"age = train.groupby('Survived')['Age']\n\nfig = plt.figure(figsize=(12, 7))\nsns.histplot(age.get_group(0), color='red', alpha=0.7, label='Dead')\nsns.histplot(age.get_group(1), color='blue', alpha=0.7, label='Survived')\nplt.title('Age of the Survived')\nplt.legend(loc='best', fontsize=14)\nplt.tight_layout()","bd1113ea":"pclass_age = train.groupby(['Pclass', 'Survived', 'Sex'])['Age'] # age of passengers based on survivability, sex and pclass\n\n\nfig = plt.figure(figsize=(13, 8.5))\ngrid = gridspec.GridSpec(nrows=6, ncols= 4, figure=fig)\n\nfor x in np.arange(2, 7, 2):\n    ax1 = fig.add_subplot(grid[x-2:x, :2])\n    sns.histplot(pclass_age.get_group((x\/2, 1, 'male')), color='blue', alpha=0.6, bins=20, ax=ax1)\n    sns.histplot(pclass_age.get_group((x\/2, 0, 'male')), color='red', alpha=0.7, bins=20, ax=ax1)\n    plt.title(f'Pclass = {int(x\/2)} | Survived = 1\/0 for Males')\n    plt.tight_layout()\n    if x != 6:\n        ax1.xaxis.set_visible(False)\n\n    ax2 = fig.add_subplot(grid[x-2:x, 2:], sharey=ax1)\n    sns.histplot(pclass_age.get_group((x\/2, 1, 'female')), color='blue', alpha=0.6, bins=20, ax=ax2, label='Survived')\n    sns.histplot(pclass_age.get_group((x\/2, 0, 'female')), color='red', alpha=0.6, bins=20, ax=ax2, label='Dead')\n    plt.title(f'Pclass = {int(x\/2)} | Survived = 1\/0 for Females')\n    ax2.yaxis.set_visible(False)\n    plt.tight_layout()\n    if x != 6:\n        ax2.xaxis.set_visible(False)\n    if x == 2:\n        plt.legend(loc='best', fontsize=15)\n\n\nplt.show()","8d367035":"embarked_age = train.groupby(['Embarked', 'Survived', 'Sex'])['Age'] # age of passengers based on sex, survivablity and departure location\nembarked_loc = list(train.Embarked.unique())[:3]\nembarked_loc.insert(0, 'dummy value')\n\nfig = plt.figure(figsize=(13, 8.5))\ngrid = gridspec.GridSpec(nrows=6, ncols= 4, figure=fig)\n\n\nfor x in np.arange(2, 7, 2):\n    ax1 = fig.add_subplot(grid[x-2:x, :2])\n    sns.histplot(embarked_age.get_group((embarked_loc[int(x\/2)], 1, 'male')), color='blue', alpha=0.6, bins=20, ax=ax1)\n    sns.histplot(embarked_age.get_group((embarked_loc[int(x\/2)], 0, 'male')), color='red', alpha=0.6, bins=20, ax=ax1)\n    plt.title(f'Embarked = {embarked_loc[int(x\/2)]} Survived = 1\/0 for Males')\n    plt.tight_layout()\n    if x != 6:\n        ax1.xaxis.set_visible(False)\n        \n    ax2 = fig.add_subplot(grid[x-2:x, 2:], sharey=ax1)\n    sns.histplot(embarked_age.get_group((embarked_loc[int(x\/2)], 1, 'female')), color='blue', alpha=0.6, bins=20, ax=ax2, label='Survived')\n    sns.histplot(embarked_age.get_group((embarked_loc[int(x\/2)], 0, 'female')), color='red', alpha=0.6, bins=20, ax=ax2, label='Dead')\n    plt.title(f'Embarked = {embarked_loc[int(x\/2)]} Survived = 1\/0 for Females')\n    plt.tight_layout()\n    ax2.yaxis.set_visible(False)\n    if x != 6:\n        ax2.xaxis.set_visible(False)\n    if x == 2:\n        plt.legend(loc='best', fontsize=15)\n        \nplt.show()","cb72ea18":"fare_survived = train.groupby(['Embarked', 'Survived'])[['Fare', 'Sex']]\nembarked_loc = list(train.Embarked.unique())[:3]\nembarked_loc.insert(0, 'dummy value')\n\nfig = plt.figure(figsize=(13, 8.5))\ngrid = gridspec.GridSpec(nrows=6, ncols= 4, figure=fig)\n\n\nfor x in np.arange(2, 7, 2):\n    ax1 = fig.add_subplot(grid[x-2:x, :2])\n    sns.barplot(x=fare_survived.get_group((embarked_loc[int(x\/2)], 0)).Sex, \n                        y=fare_survived.get_group((embarked_loc[int(x\/2)], 0)).Fare, ci=None, palette=['red', 'blue'], alpha=0.7, ax=ax1)\n    plt.title(f'Embarked = {embarked_loc[int(x\/2)]} | Dead')\n    plt.tight_layout()\n    plt.ylim([0, 100])\n    if x != 6:\n        ax1.xaxis.set_visible(False)\n        \n    ax2 = fig.add_subplot(grid[x-2:x, 2:])\n    sns.barplot(x=fare_survived.get_group((embarked_loc[int(x\/2)], 1)).Sex, \n                        y=fare_survived.get_group((embarked_loc[int(x\/2)], 1)).Fare, ci=None, palette=['red', 'blue'], alpha=0.7, ax=ax2)\n    plt.title(f'Embarked = {embarked_loc[int(x\/2)]} | Survived')\n    plt.tight_layout()\n    plt.ylim([0, 100])\n    ax2.yaxis.set_visible(False)\n    if x != 6:\n        ax2.xaxis.set_visible(False)\n    \n    \nplt.show()","763d1103":"plt.figure(figsize=(12, 8))\nsns.barplot(y='Fare', x='Pclass', data=train, hue='Survived', ci=None, palette=['red', 'blue'], alpha=0.6)\nplt.title('Fare price between classes and embarkment location')\nplt.tight_layout()\nplt.legend(fontsize=30)\nplt.show()","1e3cdabc":"print('Before:', train.shape, test.shape)\ntrain, test = train.drop(['Cabin', 'Ticket'], axis=1), test.drop(['Cabin', 'Ticket'], axis=1) # dropped Cabin and Ticket features\nprint('After:', train.shape, test.shape)","c563e005":"datasets = [train, test] # combining so changes apply to both tables\nfor dataset in datasets:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False) # extracting titles from names\n    \npd.crosstab(train.Title, train.Sex)","3340024d":"# replacing specifc mistyped titles with correct ones\nfor dataset in datasets:\n    dataset.Title = dataset.Title.replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'})\n\n# grouping names with a very low count\nrare_names = ['Capt', 'Col', 'Countess', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir', 'Dona']\nfor dataset in datasets:\n    dataset.Title = dataset.Title.replace(rare_names, 'Rare')\npd.crosstab(train.Title, train.Sex)","81460631":"train[['Title', 'Survived']].groupby('Title').mean() # we can observe the survivability rate for each title","a7ecc1a7":"title_conv = LabelEncoder().fit(train.Title)# preparing to attach numbers to each unique title\nsex_conv = LabelEncoder().fit(train.Sex) # converting to numerical gender values\n\nfor dataset in datasets:\n    dataset.Title = title_conv.transform(dataset.Title) + 1\n    dataset.Sex = sex_conv.transform(dataset.Sex) + 1\n    \ntrain.head(10)","9090a922":"train = train.drop(['PassengerId', 'Name'], axis=1)\ntest = test.drop(['PassengerId', 'Name'], axis=1)\n    \nprint(train.shape, test.shape)","8d550c72":"grid = sns.FacetGrid(train, col='Pclass', row='Sex', aspect=1.5, height=4.5)\ngrid.map(sns.histplot, 'Age', color='black', bins=20, alpha=0.7)\nplt.tight_layout()\nplt.show()","9b39b856":"# taking the median of every sub group of Pclass and Sex and getting their age to the nearest 0.5 and converting the column to int datatype\ntrain.Age = train.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(int(x.median() \/ 0.5 + 0.5) * 0.5)).astype(int)\ntest.Age = test.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(int(x.median() \/ 0.5 + 0.5) * 0.5)).astype(int)\ntrain.head()","fc2d0f80":"train['AgeBand'] = pd.qcut(train.Age, 5)\ntest['AgeBand'] = pd.qcut(test.Age, 5)\n    \ntrain[['AgeBand', 'Survived']].groupby('AgeBand').mean() # survival rate per age band","ceea9a5f":"train.Age = LabelEncoder().fit_transform(train.AgeBand) + 1\ntest.Age = LabelEncoder().fit_transform(test.AgeBand) + 1\n\n# once we are done creating categorical age features, we can drop the AgeBand feature for both datasets\ntrain.drop('AgeBand', axis=1, inplace=True)\ntest.drop('AgeBand', axis=1, inplace=True)\ntrain.head()","ae0c0729":"train['FamilySize'] = train.SibSp + train.Parch\ntest['FamilySize'] = test.SibSp + test.Parch\n\ntrain[['FamilySize', 'Survived']].groupby('FamilySize').mean().sort_values(by='Survived', ascending=False)","30fc2472":"train.head(10)","cf81fa7a":"train['IsAlone'] = train.FamilySize.transform(lambda x: 1 if x == 0 else 0)\ntest['IsAlone'] = test.FamilySize.transform(lambda x: 1 if x == 0 else 0)\n\ntrain.sample(10)","ee2671cb":"train[['IsAlone', 'Survived']].groupby('IsAlone').mean() # survival rate for passengers who were and weren't alone","ada001c5":"print('Train before:', train.shape, '\\nTest before', test.shape, '\\n')\ntrain.drop(['SibSp', 'Parch', 'FamilySize'], axis=1, inplace=True)\ntest.drop(['SibSp', 'Parch', 'FamilySize'], axis=1, inplace=True)\nprint('Train after:', train.shape, '\\nTest after', test.shape)","bb04248e":"train.head()","dab34a10":"train[['Survived', 'Embarked', 'Pclass']].groupby(['Pclass', 'Embarked']).mean()","769b1d23":"fig = plt.figure(figsize=(9, 7))\nsns.barplot(x='Pclass', y='Survived', data=train, hue='Embarked', ci=None)\nplt.legend(loc='best', title='Embarked', fontsize=15)\nplt.title('Survival rate between Embarked location and Pclass')\nplt.tight_layout()\nplt.show()","c25f2c7d":"train.Embarked = train.groupby('Pclass')['Embarked'].transform(lambda x: x.fillna(x.mode()[0]))\ntrain.Embarked.isnull().sum() #  embarked feature having no more empty values","9adc3214":"train[['Embarked', 'Survived']].groupby('Embarked').mean()","6d6d7758":"embarked_conv = LabelEncoder().fit(train.Embarked)\ntest.Embarked = embarked_conv.transform(test.Embarked) + 1\ntrain.Embarked = embarked_conv.transform(train.Embarked) + 1","c1333860":"train.head(10)","b76a6048":"test.Fare = test[['Pclass', 'Fare']].groupby('Pclass').transform(lambda x: x.fillna(x.median()))\ntest.Fare.isnull().sum()","c2e3d2bd":"train['FareBand'] = pd.qcut(train.Fare, 5)\ntest['FareBand'] = pd.qcut(test.Fare, 5)\ntrain[['FareBand', 'Survived']].groupby('FareBand').mean()","782873d2":"#### LabelEncoding any FareBand features we make for both datasets\ntrain['Fare'] =  LabelEncoder().fit_transform(train.FareBand) + 1\ntest['Fare'] = LabelEncoder().fit_transform(test.FareBand) + 1\n\ntrain.drop('FareBand', axis=1, inplace=True) # dropping the FareBand feature as we have no use for it anymore\ntest.drop('FareBand', axis=1, inplace=True)                                \n\ntrain.head()","dde5fc7b":"train.info()\nprint('\\n', '=' * 40, '\\n')\ntest.info()","24d6ef5f":"train['AgePclass'] = train.Age * train.Pclass # Age and Pclass\ntest['AgePclass'] = test.Age * test.Pclass\n\ntrain['FareEmbarked'] = train.Fare * train.Embarked # Fare and Embarked\ntest['FareEmbarked'] = test.Fare * test.Embarked\n\ntest.head()","62450646":"X_train = train.drop('Survived', axis=1).copy()\ny_train = train['Survived']\nX_test = test.copy()\n\nX_train[:5]","557b3e63":"stan_scaler = StandardScaler().fit_transform(X_train) # standard scaler\nrob_scaler = RobustScaler().fit_transform(X_train) # robust scaler\nminmax_scaler = MinMaxScaler().fit_transform(X_train) # min max scaler","530bf1d1":"stan_log = np.round(LogisticRegression().fit(stan_scaler, y_train).score(stan_scaler, y_train) * 100, 2)\nrob_log = np.round(LogisticRegression().fit(rob_scaler, y_train).score(rob_scaler, y_train) * 100, 2)\nminmax_log = np.round(LogisticRegression().fit(minmax_scaler, y_train).score(minmax_scaler, y_train) * 100, 2)\n\nprint(f'With StandardScaler the accruacy is = {stan_log}%\\nWith RobustScaler the accuracy is ={rob_log}%\\nWith MinMaxScaler the accuracy is = {minmax_log}%')","d223b211":"log_score = 0\nbest_c = 0\n\nfor x in np.arange(0.01, 5, 0.01):\n    clf = LogisticRegression(C=x).fit(minmax_scaler, y_train)\n    clf_score = np.round(clf.score(minmax_scaler, y_train) * 100, 3)\n    if clf_score > log_score:\n        best_c = x\n        log_score = clf_score\n        \nprint('Highest score for LogisticRegression is {0}% with C of {1}'.format(log_score, best_c))","bcc0a591":"k_score = 0\nbest_k = 0\n\nfor x in np.arange(1, 51, 1):\n    clf = KNeighborsClassifier(n_neighbors=x).fit(minmax_scaler, y_train)\n    clf_score = np.round(clf.score(minmax_scaler, y_train) * 100, 3)\n    if clf_score > k_score:\n        k_score = clf_score\n        best_k = x\n        \nprint('Highest score for KNeighborsClassifier is {0}% with K of {1}'.format(k_score, best_k))","6341b2de":"svc_score = 0\nbest_c = 0\n\nfor x in np.arange(0.01, 5, 0.01):\n    clf = SVC(C=x).fit(minmax_scaler, y_train)\n    clf_score = np.round(clf.score(minmax_scaler, y_train) * 100, 3)\n    if clf_score > svc_score:\n        svc_score = clf_score\n        best_c = x\n        \nprint('Highest score for SVM is {0}% with C of {1}'.format(svc_score, best_c))","1669eb59":"dec_score = np.round(DecisionTreeClassifier().fit(minmax_scaler, y_train).score(minmax_scaler, y_train) * 100, 3)\nprint('Highst score for DecisionTreeClassifier is {}%'.format(dec_score))","66dec406":"forest_score = 0\nforest_trees = 0\n\nfor x in np.arange(100, 500, 100):\n    clf = RandomForestClassifier(n_estimators=x).fit(minmax_scaler, y_train)\n    clf_score = np.round(clf.score(minmax_scaler, y_train) * 100, 3)\n    if clf_score > forest_score:\n        forest_score = clf_score\n        forest_trees = x\n        \nprint('Highest score for RandomForestClassifier is {0}% with {1} trees'.format(forest_score, forest_trees))","8f836e38":"models = pd.DataFrame({'Models':['LogisticRegression', 'KNeighborsClassifier', 'Support Vector Machine', \n                                                  'DecisionTreeClassifier', 'RandomForestClassifier'], \n                       'Score':[log_score, k_score, svc_score, dec_score, forest_score]}).sort_values(by='Score', ascending=False)\n\nmodels","3243718b":"test_scaled = MinMaxScaler().fit_transform(X_test)\n\npredicts = RandomForestClassifier(n_estimators=100).fit(minmax_scaler, y_train).predict(test_scaled)\npredicts[:5]","63b89301":"i = list(range(892, 942, 1))\nrows = list(zip(i, predicts))\nrows[:5]","59b3c695":"with open('submission.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerows(rows)","f9f157d7":"#### Checking trend of survivability with specific features","8bc3b8e2":"There are a multitiude of algorithms we can use for our model. We will try to choose the best one with the highest score.\n**After** we label our data correctly","5eaddfe3":"#### Alone or not\n\nFor this feature that we want to create, we would be taking into account the **SibSp and the Parch** values to determine whether a passenger was **alone or not** when boarding the Titanic. If the passenger has no siblings, spouses, parents or children, they will be categorized as alone.","d1b1ac3b":"There are a few observations we can make from this:\n* Age groups can be used to better define observations\n* The elderly did not have a good survival rate\n* Infant passengers mostly survived\n\nAge **will** be used as a feature when making the model.","12848ea0":"Now to create a feature where we will use the values in **FamilySize** to determine is the passenger was alone or not","8189bda0":"* There is a trend going on where **majority of the passengers in Pclass 1 embarked from C**\n* Majority of the **Pclass 2 passengers embarked from Q**\n* Most of the passengers have embarked from **C**\n* The least amount of passengers to have embarked from a location is at **S**\n* **Pclass 3** has the least amount of passengers from all embarked locations\n\nI believe we can use Pclass and it's modal value for Embarked location to predict the missing values for the Embarked feature","736516c5":"#### Fare","707f49dc":"### Deciding on choosing features ","a988393c":"### Quick look at the features \n\nSince we have made some assumptions about our given data and have formed some decisions on which data to keep and which to remove, we can also get started on creating some more features based on the already present data.","4f824755":"We can either choose to fill the blanks of the **Age** value by taking the **median or mode age based on the Pclass and the Sex value**, I will choose to take the median age.","9254906c":"#### Support Vector Machine","b9916665":"#### Best Classifier","e776b5c3":"### Making the submission file","95c453d8":"#### KNeighborsClassifier","35574daa":"Now we have a **continuous value** for the Age feature, we can convert them to a **band feature** so we can better determination the correlation between it and Survived","5848c331":"#### RandomForestClassifier","1507112f":"As we can see, the **DecisionTreeClassifer** performed the best with the higest score and will be chosen for our model.","1261951d":"**LabelEncoding** the AgeBand values so that we can convert Age to a **categorical feature**","0c9d6ee4":"#### Decicing which scaler is best","d5651dfe":"We should convert the **continuous** Fare feature to a **categorical** Fare band Feature","d5904a21":"### Modeling the data","7f44e0ee":"#### Pclass","24c05d12":"#### Dropping features","cd51f162":"### Filling in the missing values\n\nWe will start of with age, there are multiple methods for which we can use to guess the age of the passenger whose age value is not filled in. The method we are going to use is to get the median of the age distribution of people per Pclass and their gender","9a50d93e":"We have to choose which numerical can be used when predicting if the passenger has survived, from the previous graphs we have already chosen a couple of features: sex, Pclass, SibSp and Parch. We have to keep visualizing the data so that we can come to a more decisive conclusion on which data to use, and which data to drop.","9f3defa3":"Now that we have decided to use the MinMaxScaler for our training data, we have to choose model for our classifier:\n* LogisticRegression\n* KNeighborsClassifier\n* Support Vector Machines\n* DecisionTreeClassifier\n* RandomForestClassifier","00686af0":"#### DecisionTreeClassifier","9e9fd80d":"#### Another last look at the databases","a6f4a5eb":"We can now drop the **SibSp, Parch and FamilySize** columns as we have used them to get the **IsAlone** column and have no further use for them","09886ebb":"#### Fare","0f92210d":"#### Embarked Location","a5f50353":"#### Age","8879fe92":"Im open to any criticism on this, it'll help me learn and well.. I want to learn :)","0debd9fa":"Converting string features to **numerical**","12c1c74f":"From the graphs we can see that:\n* Females had a **much better survival rate** than males \n* The trend of females having a better survival rate applies to all Pclass values\n* **Younger males had a better surivval rate** in Pclass 1 and 2 than in Pclass 3\n* Pclass 3 had the **lowest survival rates for younger-middle aged men**\n* Pclass 2 had the highest amount of kids surviving\n\n**Gender will be added to the model for training while Pclass will be considered**\n\nThere are more observations that we can derive from the graphs but I degress onto the next feature.","e2774099":"There is only one missing value, which is in the test dataset, for the Fare feature. We will fill it up by the **median Fare price** depending on the passengeres **Pclass**","2be86c20":"What we can learn from the graphs:\n* The notion holds that generally more females survived than males for all classes\n* Almost all male passengers who embarked from Q died\n* Younger male passengers who embarked from S had a lower survivability rate than younger male passengers from other locations\n* Q had the highest rate of younger male passengers dying\n\n**Embarked will be added as a feature to train the model**","79a5881b":"#### Embarkment","04da5995":"We can try to see if there is a certain trend between **Pclass** and **Embarked** location","f9580061":"#### Overall look","3271979f":"#### Observations\n\nWe can see that based on the tables, most of the passengers who survived tend to have had **less family members**, there is a disparity between the **genders** of the passengers who survived, and a steady difference in the survival rate of the passengers as compared to the **classes there were in** when on the boat.","cc346694":"We can see that **Fares** will be a good addition to the model when trying to predict survivability as both **Pclass** and Fare have a good relationship to help us with our prediciton","717cd18d":"#### LogisticRegression","4c9c78eb":"#### Getting basic analysis of test and train","ba4d2187":"#### Deciding which model to use","dfa135fc":"#### New Title feature ","66d4ab4e":"From these graphs we learn that:\n* **Higher paying passengers** generally had a better survival probability\n* Generally **men paid more** for their fares than women\n* Passengers that embarked from C paid **considerably more** for their fare than other passengers from other locations\n\n**Banding Fare and consider using for the model**","3190c453":"### Predicting the data","fb706319":"### Train and test","127ce2e7":"Now that there are titles which we can use to group the passengers with, we can safely drop the Name and PassengerId columns.","4a5b79b4":"### Thank you for going through this!! :)","598f7eca":"#### Age","59e4ad65":"### Creating new features\n\nTo give our data more depth for it to predict the right outcome, we can decide to make some new features by simply doing some basic mathemtical opersations between the values of 2 features","357a11f0":"Converting the Embarked feature to a numerical categorical feature"}}