{"cell_type":{"79ce7561":"code","855c27be":"code","a3cb5ca0":"code","846f0bed":"code","07ef0f6f":"code","2e02e885":"code","14b1337f":"code","2d85f142":"code","0af79d4b":"code","9b0756a6":"code","2d860b39":"code","c50d523a":"code","f085d420":"code","407d464c":"code","a8d34027":"code","b846cec2":"code","3611c608":"code","48a6baf4":"code","59752dd6":"code","234413c2":"code","0d427d3c":"code","8010729c":"code","bc344fdf":"code","35c59dc9":"code","b6855532":"code","ce87e7c3":"code","433a0317":"code","b2097c7f":"markdown","ff874ef0":"markdown","220a6217":"markdown","824ea350":"markdown","5f0479fa":"markdown","595f9f5d":"markdown","b8849a62":"markdown","8e17b5cf":"markdown","8d6185b6":"markdown","f8d60a2c":"markdown","f96181ab":"markdown","cf69b5c4":"markdown","246e7864":"markdown","d23f88e7":"markdown","7040f388":"markdown","3db9dcbc":"markdown","876b93cc":"markdown","eca5b10a":"markdown","f92baaf6":"markdown","e0375511":"markdown","58ba0297":"markdown","41f170e5":"markdown","fbd7cdbb":"markdown","bece7e10":"markdown","3e29733a":"markdown","47db1ed5":"markdown","7c85b1ab":"markdown","58b95e36":"markdown","ce2f6e5b":"markdown","8eee4b64":"markdown","537807de":"markdown","41f6acf1":"markdown","66e95a18":"markdown","3cafd2ad":"markdown","2440fa63":"markdown","54754e57":"markdown"},"source":{"79ce7561":"import pandas as pd\nimport numpy as np\n\n!pip install 'networkx == 2.3'\n!pip install causality\n\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom causality.inference.search import IC\nfrom causality.inference.independence_tests import RobustRegressionTest\nfrom sklearn.preprocessing import (StandardScaler, RobustScaler)\nimport seaborn as sns\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import (LinearRegression, HuberRegressor)\nfrom sklearn.metrics import mean_squared_error, make_scorer, mean_squared_log_error\nfrom sklearn.neighbors import KNeighborsRegressor\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.manifold import MDS\n\nfrom category_encoders import MEstimateEncoder\n","855c27be":"#read training set from csv\nX_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col = 'Id')\n\n#read test set from csv\nX_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col = 'Id')\n\n#separate target variable from the training set\nyt= X_train['SalePrice']\n\n#drop target variable from the training set\nX_train.drop('SalePrice', axis = 1, inplace = True)\n\n#comine X_train and X_test into a single set for preprocessing\ncombined = pd.concat([X_train,X_test])\n\n#create a datetime series\ndatetimes = combined.YrSold.astype(str)+'-'+combined.MoSold.astype(str)\ndatetimes = pd.to_datetime(datetimes, format=\"%Y-%m\")\ndatetimes = pd.DataFrame(datetimes, columns = ['Date'])\n","a3cb5ca0":"\n# Alley : data description says NA means \"no alley access\"\ncombined.loc[:, \"Alley\"] = combined.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ncombined.loc[:, \"BedroomAbvGr\"] = combined.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ncombined.loc[:, \"BsmtQual\"] = combined.loc[:, \"BsmtQual\"].fillna(\"No\")\ncombined.loc[:, \"BsmtCond\"] = combined.loc[:, \"BsmtCond\"].fillna(\"No\")\ncombined.loc[:, \"BsmtExposure\"] = combined.loc[:, \"BsmtExposure\"].fillna(\"No\")\n\n\ncombined.loc[:, \"BsmtFinType1\"] = combined.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ncombined.loc[:, \"BsmtFinSF1\"] = combined.loc[:, \"BsmtFinSF1\"].fillna(0)\ncombined.loc[:, \"BsmtFinSF2\"] = combined.loc[:, \"BsmtFinSF2\"].fillna(0)\ncombined.loc[:, \"TotalBsmtSF\"] = combined.loc[:, \"TotalBsmtSF\"].fillna(0)\n\ncombined.loc[:, \"BsmtFinType2\"] = combined.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ncombined.loc[:, \"BsmtFullBath\"] = combined.loc[:, \"BsmtFullBath\"].fillna(0)\ncombined.loc[:, \"BsmtHalfBath\"] = combined.loc[:, \"BsmtHalfBath\"].fillna(0)\ncombined.loc[:, \"BsmtUnfSF\"] = combined.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ncombined.loc[:, \"CentralAir\"] = combined.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ncombined.loc[:, \"Condition1\"] = combined.loc[:, \"Condition1\"].fillna(\"Norm\")\ncombined.loc[:, \"Condition2\"] = combined.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ncombined.loc[:, \"EnclosedPorch\"] = combined.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ncombined.loc[:, \"ExterCond\"] = combined.loc[:, \"ExterCond\"].fillna(\"TA\")\ncombined.loc[:, \"ExterQual\"] = combined.loc[:, \"ExterQual\"].fillna(\"TA\")\n# Fence : data description says NA means \"no fence\"\ncombined.loc[:, \"Fence\"] = combined.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ncombined.loc[:, \"FireplaceQu\"] = combined.loc[:, \"FireplaceQu\"].fillna(\"No\")\ncombined.loc[:, \"Fireplaces\"] = combined.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ncombined.loc[:, \"Functional\"] = combined.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ncombined.loc[:, \"GarageType\"] = combined.loc[:, \"GarageType\"].fillna(\"No\")\ncombined.loc[:, \"GarageFinish\"] = combined.loc[:, \"GarageFinish\"].fillna(\"No\")\ncombined.loc[:, \"GarageQual\"] = combined.loc[:, \"GarageQual\"].fillna(\"No\")\ncombined.loc[:, \"GarageCond\"] = combined.loc[:, \"GarageCond\"].fillna(\"No\")\ncombined.loc[:, \"GarageArea\"] = combined.loc[:, \"GarageArea\"].fillna(0)\ncombined.loc[:, \"GarageCars\"] = combined.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ncombined.loc[:, \"HalfBath\"] = combined.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ncombined.loc[:, \"HeatingQC\"] = combined.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ncombined.loc[:, \"KitchenAbvGr\"] = combined.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ncombined.loc[:, \"KitchenQual\"] = combined.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ncombined.loc[:, \"LotFrontage\"] = combined.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ncombined.loc[:, \"LotShape\"] = combined.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ncombined.loc[:, \"MasVnrType\"] = combined.loc[:, \"MasVnrType\"].fillna(\"None\")\ncombined.loc[:, \"MasVnrArea\"] = combined.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ncombined.loc[:, \"MiscFeature\"] = combined.loc[:, \"MiscFeature\"].fillna(\"No\")\ncombined.loc[:, \"MiscVal\"] = combined.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ncombined.loc[:, \"OpenPorchSF\"] = combined.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ncombined.loc[:, \"PavedDrive\"] = combined.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ncombined.loc[:, \"PoolQC\"] = combined.loc[:, \"PoolQC\"].fillna(\"No\")\ncombined.loc[:, \"PoolArea\"] = combined.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ncombined.loc[:, \"SaleCondition\"] = combined.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ncombined.loc[:, \"ScreenPorch\"] = combined.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ncombined.loc[:, \"TotRmsAbvGrd\"] = combined.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ncombined.loc[:, \"Utilities\"] = combined.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ncombined.loc[:, \"WoodDeckSF\"] = combined.loc[:, \"WoodDeckSF\"].fillna(0)\n\ncombined.dropna(axis = 1, thresh = len(combined)*.8,inplace = True)\n\ncolumns_with_na = [col for col in combined.columns if len(combined[col].dropna()) < 2919]\nnum_na = [(len(combined[col])-len(combined[col].dropna())) for col in columns_with_na]\ndtype_na = [(combined[col]).dtype for col in columns_with_na]\ndf = pd.DataFrame([num_na,dtype_na], index = ['na_vals', 'dtype'], columns = columns_with_na)\n\n\nfor column_name, column in df.iteritems():\n    \n    \n    if column['dtype'] == 'object':\n        combined[column_name].fillna(value = combined[column_name].value_counts(dropna =True).index[0], inplace = True)\n    else:\n        combined[column_name].fillna(value = combined[column_name].median(), inplace = True)","846f0bed":"# Some numerical features are actually really categories\ncombined = combined.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })\n\n# Encode some categorical features as ordered numbers when there is information in the order\ncombined = combined.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )\n","07ef0f6f":"combined['YrSold']=combined['YrSold'].astype(str)\ncombined['MoYrSold']=combined['MoSold']+combined['YrSold']","2e02e885":"\ncategorical_columns = list(combined.select_dtypes(include = 'object').columns)\n\n\nencoder = MEstimateEncoder(cols = categorical_columns, drop_invariant = True, m = 15)\n\nX_train = combined.loc[X_train.index]\nX_test = combined.loc[X_test.index]\n\nX_train = encoder.fit_transform(X_train, yt)\nX_test = encoder.transform(X_test)\n\ncombined  = pd.concat([X_train,X_test])\n\n","14b1337f":"scaler = StandardScaler()\ncombined = pd.DataFrame(scaler.fit_transform(combined), columns = combined.columns, index = combined.index)\n\n","2d85f142":"\nvariable_types_all = {}\nfor column_name, column in combined.iteritems():\n    if len(column.unique())<20:\n        variable_types_all[column_name] = 'd'\n        \n    else:\n        variable_types_all[column_name] = 'c'\n\n\nfor column in variable_types_all:\n    \n    if (variable_types_all[column_name] == 'c') & (abs(combined[column].skew())>0.5):\n        pt = PowerTransformer(method = 'yeo-johnson')\n        combined[column] = pt.fit_transform(combined[column].to_numpy().reshape(-1,1))\n       \n","0af79d4b":"X_train = combined[:len(X_train)]\nX_test = combined[len(X_train):]\n","9b0756a6":"\n\ndf = X_train.copy()\n\ncorr_df = df.corr().abs()\nmask = np.triu((np.ones_like(corr_df, dtype = bool)))\ntri_df = corr_df.mask(mask)\n\nto_drop = []\n\nfor index, row in tri_df.iterrows():\n    for col in row.index:\n        if tri_df.loc[index, col]>.9:\n            to_drop.append((index, col))\n\n\nto_drop = [val[0] for val in to_drop]\n\ndf = df.drop(to_drop, axis = 1)","2d860b39":"variable_types = {}\nfor column_name, column in df.iteritems():\n    if len(column.unique())<20:\n        variable_types[column_name] = 'd'\n        \n    else:\n        variable_types[column_name] = 'c'\n\n\nvariable_types['SalePrice'] = 'c'\n\n\ndf = pd.concat([df, yt], axis = 1)\n\nic_algorithm = IC(RobustRegressionTest, alpha = .1, k = 1)\ngraph = ic_algorithm.search(df, variable_types)","c50d523a":"G = nx.DiGraph()\nG.add_edges_from(graph.edges(data=True))\n\nplt.figure(figsize=(15, 15))\ng = nx.drawing.kamada_kawai_layout(graph)\n\nnx.draw_networkx(G, g)\n\nnx.draw_networkx_nodes(graph, g, nodelist= ['SalePrice'], edgecolors='k', node_color = 'r')\n\n\nplt.show()\n","f085d420":"# find neighbors of SalePrice in graph\nfeatures = list(nx.classes.function.neighbors(graph, 'SalePrice'))\n\nfeatures","407d464c":"#output of algorithm with just numerical features\n\nfeatures = ['MSSubClass',\n 'LotArea',\n 'Neighborhood',\n 'OverallQual',\n 'YearRemodAdd',\n 'BsmtQual',\n 'BsmtExposure',\n 'TotalBsmtSF',\n '1stFlrSF',\n 'GrLivArea',\n 'KitchenQual',\n 'Fireplaces',\n 'GarageFinish',\n 'GarageCars',\n 'PavedDrive',\n 'WoodDeckSF']\n\n\n\n#create reduced train and test sets with causal features\nX_train_reduced = X_train.loc[:,features]\nX_test_reduced = X_test.loc[:,features]\n\n#log transform y\ny= pd.DataFrame(np.log1p(yt), columns = ['SalePrice'], index = yt.index)\n\n#perform automatic outlier detection to get rid of outliers \nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_train_reduced)\nmask = yhat != -1\nmask[523] = False\nmask[1298] = False\nmask[297] = False\n\n\noutlier_mask = yhat == -1\n\n\noutlier_mask[523] = True\noutlier_mask[1298] = True\noutlier_mask[297] = True\ny_outliers = y.loc[outlier_mask]\n\n\n#remove outliers from train set\nX_train_reduced_outliers =  X_train_reduced.loc[outlier_mask,:]\nX_train_reduced = X_train_reduced.loc[mask,:]\ny = y.loc[mask]\n\n","a8d34027":"# define root mean error and log root mean error scoring functions\ndef ms_score(y, ypred):\n    y = np.expm1(y)\n    ypred = np.expm1(ypred)\n    return mean_squared_error(y, ypred, squared = False)\n    #return np.sqrt(mean_squared_log_error(y, ypred))\n\ndef lms_score(y, ypred):\n    return mean_squared_error(y, ypred, squared = False)\n   \nms_scorer = make_scorer(ms_score)\nlms_scorer = make_scorer(lms_score)\n\n#calculate learning curve results for ms_scorer and plot\ntrain_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X_train_reduced, y.to_numpy().ravel(), train_sizes=[50, 200, 400, 600,800, 1000, 1075], cv=5, scoring = ms_scorer)\n\nplt.style.use('ggplot')\n\nfig, (ax, ax1) = plt.subplots(1,2, figsize = (10, 5))\n\nax.plot(train_sizes, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax.fill_between(train_sizes, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.plot(train_sizes, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax.fill_between(train_sizes, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.set_ylabel('Root Mean Error ($)')\nax.set_xlabel('Train Sizes')\nax.set_title('Root Mean Error')\nax.legend(loc = 'upper right')\n\n#calculate learning curve results for lms and plot\n\ntrain_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X_train_reduced, y.to_numpy().ravel(), train_sizes=[50, 200, 400, 600,800, 1000, 1075], cv=5, scoring = lms_scorer)\n\nax1.plot(train_sizes, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax1.fill_between(train_sizes, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.plot(train_sizes, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax1.fill_between(train_sizes, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.set_ylabel('Log Root Mean Error')\nax1.set_xlabel('Train Sizes')\nax1.set_title('Log Root Mean Error')\nax1.legend(loc = 'upper right')\n\nplt.show()\n\n","b846cec2":"\n\nypred = []\n\n#fit linear model to X_train_reduced to predict non-outlier values of X_test \nlr = LinearRegression()\nlr.fit(X_train_reduced, y.to_numpy().ravel())\n\nX_test_reduced = X_test.loc[:,features]\n\n\n\n\n\n#perform outlier detection on test set\nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_test_reduced)\nmask = yhat != -1\n\nmask = pd.DataFrame(mask, columns = ['mask'], index = X_test_reduced.index)\n\n#read in csv for SalePrice of test set for evaluation purposes\ny_val = pd.read_csv('\/kaggle\/input\/perfect-score-for-evaluation-purposes\/full-score.csv')\ny_val.set_index('Id', inplace = True)\n\n\n\n#define knn regressor to predict X_test outliers\nknn = KNeighborsRegressor(n_neighbors = 30)\nknn.fit(X_train_reduced, y)\n\n#predict outliers with knn regressor, predict non-outliers with linear regressor \nfor i, row in mask.iterrows():\n    if row['mask'] == True:\n        pred = lr.predict(X_test_reduced.loc[i].to_numpy().reshape(1,-1))\n    else:\n        pred = knn.predict(X_test_reduced.loc[i].to_numpy().reshape(1,-1))\n        \n        \n    ypred.append(pred)\n\n\nypred = np.array(ypred).ravel()\n\nypred = np.expm1(ypred.astype(float))\n\n\n#calcualte and print root mean error and log root mean error scores\nprint(mean_squared_error(y_val.SalePrice.to_numpy(),ypred, squared = False))\nprint(np.sqrt(mean_squared_log_error(y_val.SalePrice.to_numpy(),ypred)))","3611c608":"#Use permutation importance measures to evaluate which features generalized well to the test set\n\nms_scorer = make_scorer(lms_score, greater_is_better = False)\nperm = PermutationImportance(lr, random_state=1, scoring = ms_scorer).fit(X_test_reduced,  np.log1p(y_val))\ndf = pd.DataFrame(perm.feature_importances_, index = X_train_reduced.columns, columns = ['weights']).sort_values(by='weights', ascending = False)\neli5.show_weights(perm, top = 60, feature_names = X_train_reduced.columns.tolist())","48a6baf4":"#fit linear model to X_train_reduced\nlr = LinearRegression()\nlr.fit(X_train_reduced, y)\n\n#predict X_train_reduced\ny_pred = lr.predict(X_train_reduced)\n\n#calcualte errors\nerrors =  np.expm1(y.to_numpy().ravel()) - np.expm1(y_pred).ravel()\nerrors = pd.DataFrame(errors, columns = ['Error'], index = X_train_reduced.index)\n\n#plot histogram\nerrors.hist(bins = 90)\nplt.show()\n","59752dd6":"\n#run IC on total set of variables\ndf = X_train.loc[errors.index]\n\n\n#determine variable types of df\nvariable_types = {}\nfor column_name, column in df.iteritems():\n    if len(column.unique())<20:\n        variable_types[column_name] = 'd'\n        \n    else:\n        variable_types[column_name] = 'c'\n\n\nvariable_types['Error'] = 'c'\n\n#concat df with errors\ndf = pd.concat([df, errors], axis = 1)\n\n#run IC algorithm\nic_algorithm = IC(RobustRegressionTest, alpha = .1, k = 1)\ngraph2 = ic_algorithm.search(df, variable_types)\n\n#find direct neighbors of errors\nnew_features = list(nx.classes.function.neighbors(graph2, 'Error'))\nnew_features\n","234413c2":"new_features =['OverallCond',\n 'BsmtFinSF1',\n 'BsmtUnfSF',\n 'Functional',\n 'ScreenPorch',\n 'SaleCondition']\n\nnew_features = new_features + ['CentralAir',\n'YearBuilt',\n'BldgType',\n'MSZoning',\n'YrSold',\n'Condition1'\n]\n","0d427d3c":"\n#add new features to features\nnew_features = [feature for feature in new_features if feature not in features]\nfeatures = new_features + features\n\n#create X_train_reduced and X_test_reduced from causal features\nX_train_reduced = X_train.loc[:,features]\n\n\nX_test_reduced = X_test.loc[:,features]\n\n#log transform y\ny= pd.DataFrame(np.log1p(yt), columns = ['SalePrice'], index = yt.index)\n\n#automatic outlier detection for X_train reduced\nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_train_reduced)\nmask = yhat != -1\nmask[523] = False\nmask[1298] = False\nmask[297] = False\n\n\noutlier_mask = yhat == -1\n\n\noutlier_mask[523] = True\noutlier_mask[1298] = True\noutlier_mask[297] = True\ny_outliers = y.loc[outlier_mask]\n\n#remove outliers from train set\ny_outliers = y.loc[outlier_mask]\nX_train_reduced_outliers =  X_train_reduced.loc[outlier_mask,:]\nX_train_reduced = X_train_reduced.loc[mask,:]\ny = y.loc[mask]\n","8010729c":"\n# define root mean error and log root mean error functions \ndef ms_score(y, ypred):\n    y = np.expm1(y)\n    ypred = np.expm1(ypred).astype(int)\n    return mean_squared_error(y, ypred, squared = False)\n    #return np.sqrt(mean_squared_log_error(y, ypred))\n\ndef lms_score(y, ypred):\n    return mean_squared_error(y, ypred, squared = False)\n   \nms_scorer = make_scorer(ms_score)\nlms_scorer = make_scorer(lms_score)\n\n#calculate and plot learning curves for ms_scorer\ntrain_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X_train_reduced, y.to_numpy().ravel(), train_sizes=[400, 600,800, 1000, 1100], cv=5, scoring = ms_scorer)\n\nplt.style.use('ggplot')\n\nfig, (ax, ax1) = plt.subplots(1,2, figsize = (10, 5))\n\nax.plot(train_sizes, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax.fill_between(train_sizes, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.plot(train_sizes, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax.fill_between(train_sizes, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.set_ylabel('Root Mean Error ($)')\nax.set_xlabel('Train Sizes')\nax.set_title('Root Mean Error')\nax.legend(loc = 'upper right')\n\n#calculate and plot learning curves for lms_scorer\ntrain_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X_train_reduced, y.to_numpy().ravel(), train_sizes=[ 400, 600,800, 1000, 1035], cv=5, scoring = lms_scorer)\n\nax1.plot(train_sizes, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax1.fill_between(train_sizes, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.plot(train_sizes, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax1.fill_between(train_sizes, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.set_ylabel('Log Root Mean Error')\nax1.set_xlabel('Train Sizes')\nax1.set_title('Log Root Mean Error')\nax1.legend(loc = 'upper right')\n\nplt.show()\n\n","bc344fdf":"ypred = []\n\n\n\n#add new features to features\nnew_features = [feature for feature in new_features if feature not in features]\nfeatures = new_features + features\n\n#create X_train_reduced and X_test_reduced from causal features\nX_train_reduced = X_train.loc[:,features]\nX_test_reduced = X_test.loc[:,features]\n\nX_train_reduced = pd.get_dummies(X_train_reduced, columns = ['OverallQual',  'Functional'], drop_first = True)\nX_test_reduced = pd.get_dummies(X_test_reduced, columns = ['OverallQual', 'Functional'], drop_first = True)\n\n\n#log transform y\ny= pd.DataFrame(np.log1p(yt), columns = ['SalePrice'], index = yt.index)\n\n#automatic outlier detection for X_train reduced\nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_train_reduced)\nmask = yhat != -1\nmask[523] = False\nmask[1298] = False\nmask[297] = False\n\n\noutlier_mask = yhat == -1\n\n\noutlier_mask[523] = True\noutlier_mask[1298] = True\noutlier_mask[297] = True\ny_outliers = y.loc[outlier_mask]\n\n#remove outliers from train set\ny_outliers = y.loc[outlier_mask]\nX_train_reduced_outliers =  X_train_reduced.loc[outlier_mask,:]\nX_train_reduced = X_train_reduced.loc[mask,:]\ny = y.loc[mask]\n\n\nlr = LinearRegression()\nlr.fit(X_train_reduced, y.to_numpy().ravel())\n\n\ny_val = pd.read_csv('\/kaggle\/input\/perfect-score-for-evaluation-purposes\/full-score.csv')\n\n\n\nlof = LocalOutlierFactor(contamination = .0001)\nyhat = lof.fit_predict(X_test_reduced)\nmask = yhat != -1\n\nmask = pd.DataFrame(mask, columns = ['mask'], index = X_test_reduced.index)\n\n\ny_val = pd.read_csv('\/kaggle\/input\/perfect-score-for-evaluation-purposes\/full-score.csv')\ny_val.set_index('Id', inplace = True)\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor(n_neighbors = 1)\nknn.fit(X_train_reduced_outliers, y_outliers)\n\nn_out = 0\n\nfor i, row in mask.iterrows():\n    if row['mask'] == True:\n        pred = lr.predict(X_test_reduced.loc[i].to_numpy().reshape(1,-1))\n        \n    else:\n        pred = knn.predict(X_test_reduced.loc[i].to_numpy().reshape(1,-1))\n        n_out = n_out + 1\n        \n    ypred.append(pred)\n\n\nypred = np.array(ypred).ravel()\n\nypred = np.expm1(ypred.astype(float))\n\n\n\nprint(mean_squared_error(y_val.SalePrice.to_numpy(),ypred, squared = False))\nprint(np.sqrt(mean_squared_log_error(y_val.SalePrice.to_numpy(),ypred)))\n\n#calcualte and plot histogram of errors\nerrors =  y_val.SalePrice - ypred\nerrors.hist(bins= 100)\nplt.show()","35c59dc9":"#create submission file\nsubmission = pd.DataFrame(ypred, index = X_test_reduced.index, columns = ['SalePrice'])\nsubmission.to_csv('submission.csv', index = True)","b6855532":"X_train_reduced = X_train.loc[:,features]\nX_test_reduced = X_test.loc[:,features]\n\n\n#log transform y\ny= pd.DataFrame(np.log1p(yt), columns = ['SalePrice'], index = yt.index)\n\n#automatic outlier detection for X_train reduced\nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_train_reduced)\nmask = yhat != -1\nmask[523] = False\nmask[1298] = False\nmask[297] = False\n\n\noutlier_mask = yhat == -1\n\n\noutlier_mask[523] = True\noutlier_mask[1298] = True\noutlier_mask[297] = True\ny_outliers = y.loc[outlier_mask]\n\n#remove outliers from train set\ny_outliers = y.loc[outlier_mask]\nX_train_reduced_outliers =  X_train_reduced.loc[outlier_mask,:]\nX_train_reduced = X_train_reduced.loc[mask,:]\ny = y.loc[mask]\n\n\nlr = LinearRegression()\nlr.fit(X_train_reduced, y.to_numpy().ravel())\n\n\nms_scorer = make_scorer(lms_score, greater_is_better = False)\nperm = PermutationImportance(lr, random_state=1, scoring = ms_scorer).fit(X_test_reduced.loc[y_val.index],  np.log1p(y_val))\neli5.show_weights(perm, top = 107, feature_names = X_train_reduced.columns.tolist())\n","ce87e7c3":"#calcualte the effect of non having central air on a house having a median base price\nc = np.unique(X_train_reduced.CentralAir)*(-.021609)\nprint(np.expm1(c[0] + np.median(y))- np.expm1(c[1] + np.median(y)))","433a0317":"X_train_reduced = X_train.loc[:,features]\nX_train_reduced = pd.get_dummies(X_train_reduced, columns = ['OverallQual',  'Functional'], drop_first = True)\n\ny= pd.DataFrame(np.log1p(yt), columns = ['SalePrice'], index = yt.index)\n\n\nlof = LocalOutlierFactor(contamination = .005)\nyhat = lof.fit_predict(X_train_reduced)\nmask = yhat != -1\nmask[523] = False\nmask[1298] = False\nmask[297] = False\n\n\nX_train_reduced = X_train_reduced.loc[mask,:]\ny = y.loc[mask]\n\nX_embedded  = MDS(n_components=2).fit_transform(X_train_reduced)\n\n\nresolution = 100\n\n\nlr = LinearRegression()\nlr.fit(X_train_reduced, y.to_numpy().ravel())\nlr_predicted = lr.predict(X_train_reduced)\n\nX2d_xmin, X2d_xmax = np.min(X_embedded[:,0]), np.max(X_embedded[:,0])\n\nX2d_ymin, X2d_ymax = np.min(X_embedded[:,1]), np.max(X_embedded[:,1])\n\nxx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))\n\nbackground_model= LinearRegression().fit(X_embedded, lr_predicted) \n\nBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n\nBackground = Background.reshape((resolution, resolution))\n\n\n\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import BoundaryNorm\n\nfig, (ax) = plt.subplots(1,1, figsize = (10,10))\n\nlevels = MaxNLocator(nbins=15).tick_values(Background.min(), Background.max())\ncmap = plt.get_cmap('PiYG')\nnorm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n\n\ncs = ax.contourf(xx, yy, Background, alpha = .5, levels=levels,\n                  cmap=cmap)\n\n\nax.scatter(X_embedded[:,0], X_embedded[:,1], c=y.to_numpy(), cmap = cmap, norm = norm)\nplt.colorbar(cs, ax= ax)\nplt.show()\n\n","b2097c7f":"### Remove Highly Corelated Features\n\nIf two variables have more than 90% correlation one is removed since the two variables likely contain the same information.","ff874ef0":"In order to build a model which accurately predicts the true effect of a set of variables x on a target variable y, given confounding paths z, it is necessary to satisfy the backdoor criterion. The backdoor criterion states that you need to condition on variables such that i)  you do not condition on any descendents of x and 2) that given confounding paths z, that you condition on a node along each confounding path such that you do not violate condition (i). An easy way to satisfy the backdoor criterion is to condition on all direct neighbors of the sale price.  Therefore, we will build a linear model using only direct neighbors of the sale price. The features which the algorithm found to have a direct effect on the sale price are shown below. ","220a6217":"### Encode Categoricals Variables <a id = 'dummies'><\/a>\n\nCategorical features are encoded using an m-estimator encoder. This is simply a variant of target encoding in which each category of a categorical feature is encoded using the estimated mean value of the target variable for that particular category. The tradeoff when using this type of encoding scheme versus dummy variable encoding is some loss in information for a reduction in dimensionality. In this case I\u2019ve used both dummy and target encoding methods, and if anything have found a slight performance increase using target encoding. Further, the reduction in dimensionality significantly reduces the run time of the IC algorithm below. \n\nMEstimateEncoder takes one hyperparameter m, which is a regularization term. The optimal value for m was determined by trial and error.\n","824ea350":"['OverallCond',\n 'BsmtFinSF1',\n 'BsmtUnfSF',\n 'KitchenAbvGr',\n 'Functional',\n 'ScreenPorch']","5f0479fa":"### Run Inductive Causation (IC*) Algorithm\n\nThe IC algorithm is run on all variables including sale price. The IC object takes  3 parameters: the conditional independence test to use (independence_test), the confidence interval to use (alpha), and the maximum number of variables used to attempt to block two variables from each other (k). The IC algorithm is run using the search method of the IC object, which takes two parameters data and variable_types. The data parameter is just your data. The variable_type is a dictionary with keys corresponding to column names and values corresponding to whether the variable is continuous 'c' or discrete 'd'. The search method returns a networkx graph where nodes correspond to variables and edges between variables representing possible causal links.\n\nThe way the algorithm works is it starts with a networkx graph with nodes representing variables where all nodes are initially connected. Then for each variable pair, the algorithm tries to find a set of other variables that can block the dependence between the pair of variables. The dependence between two variables is considered blocked if the test is less than (1- alpha)X100 percent sure that a dependency exists. By setting alpha to .1 we are saying that we want to be at least 90% sure of dependency to continue with the algorithm. If the algorithm finds a set of variables that can block the dependency of the variables pair it cuts the edge between the pair. If it can't find a blocking set it continues with another pair of variables. Getting to the k parameter, k is the maximum number of variables used in the blocking set. By setting k to 1, we cap the blocking set to 1. As you can see, there is a trade-off between the stringency of the test and the time it takes to run the test.\n\nSetting k too high potentially makes the criteria too stringent and blocks useful variables from being returned. Since, I would rather cast a wider net here, I am setting k to 1. Another benefit of setting k to 1, is that it reduces the run time of the algorithm significantly, which also increases exponentially with the number of variables in the dataset. \n\nThe chosen independence test is RobustRegressionTest, uses the confidence interval of the coefficient of a Huber regression as a measure of the confidence in the determined dependency. The algorithm decides whether to keep an edge between the two variables by comparing the determined confidence interval with the chosen alpha. By selecting this independence test we are effectively assuming a linear relationship. \n\n\n\n","595f9f5d":"### Split into Train and Test Sets <a id = 'split'><\/a>\n\nPreprocessed data is split back into train and test sets.","b8849a62":"### Cross-Validation of Updated Model\n\nThe learning curves below indicate that the updated model performs better than the orignal model in cross valdation on the traning set. For instance, the RME is now below $23,000 and the LRME is about .116, as shown below. ","8e17b5cf":"## Updated Model Validation <a id = 'validation2'><\/a>","8d6185b6":"## Visualization of Model Performance <a id = 'visualize'><\/a>\n\nThis is just a visualization of model performance. I use dimensional reduction techniques to plot the data color coded by sale price. The background is colored to conform with the model's expected price in the dimensionally reduced space. The more the color coded data points conform with the expected background color the better the model. ","f8d60a2c":"We can also investigate the relative importance of the model variables in generalization to the test set through permutation importance measurements. The way it works is it considers how model performance reacts to scrambling or permuting the values in any given variable. The more important a variable is, the more the scrambling of the variable values should affect performance. As seen below, the most important features as measured by permutation importance are overall quality, ground living area, neighborhood and total basement area.","f96181ab":"### Scaling <a id = 'scaling'><\/a>\n\nFeatures are scaled using a standard scaler. I'm scaling prior to the power transforms because of a bug in the in sklearn's yeo-johnson method which returned columns of zeros for some of the columns. Scaling prior to transformation seemed to fix this issue.","cf69b5c4":"## The Data <a id = 'data'><\/a>\n\nThe Ames Housing dataset was derived from a data dump obtained directly from the Ame's City Assessor's Office. The original data contained 113 variables describing 3970 property sales that occured in Ames Iowa between 2006-2010, however the data was edited to remove any variables that required specialized knowledge or were based on specialized calculations used by the assessor's office. After editing the data now contains 80 variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) and 2930 observations. The data was collected and edited by Dean De Cock. \n","246e7864":"## Feature Selection using IC Algorithm <a id = 'IC'><\/a>\n\n","d23f88e7":"## Model Performance on Unseen Test Data\n\nWe can also evaluate the model\u2019s performance on the unseen test data by reading in the sale-prices of the test data split. We can do this since this is a publicly available dataset. This should give us a good idea of how we would perform in the competition if we submitted the results of the given model. The only nuance here is that the true performance of the model is somewhat obscured by a handful of outliers in the test-data. Therefore, I decided to use the LocalOutlierFactor method to predict outliers in the test-set. Instead of using the linear model to predict these outliers, I instead train a knn regressor to predict these observations. As seen below, using this method of dealing with outliers the overall performance of the linear model is less obscured and more or less comports with our expectations from the learning curves. It performs slightly better on the RME score ($24000) but worse on the LRME score (.145).\n","7040f388":"## Automatic Outlier Detection <a id = 'outliers'><\/a>\n\nThe last step, before we validate our model is to create reduced test\/train sets with only the determined causal features and to remove outliers. Here, I use an automatic outlier detection method called LocalOutlierFactor from sklearn. According to the documentation it measures the local deviation of density of a given sample with respect to its neighbors to calculate an anomaly score. By comparing the local density of a sample to the local densities of its neighbors. It identifies samples that have a substantially lower density than their neighbors, which it considers outliers. I am setting its contamination parameter which is the proportion of outliers in the dataset to be quite low (.005) to avoid removing too many observations. At this level of contamination the method only removes 8 observations. This step is important since while this dataset seems to only have a couple of outliers, these outliers can have a very large effect on evaluation scores.","3db9dcbc":"## Find Additional Features by Running IC Algorithm on Model Errors <a id = 'IC2'><\/a>\n\n\nThe initial model performs decently, but maybe we could improve performance with the inclusion of additional features. Here I propose that we use the IC algorithm to find new variables by seeing which variables have a causal relationship with the errors of the initial model. If the algorithm finds variables that cause the error that weren\u2019t included in the initial model it might indicate that they should be included to boost performance. Alternatively, if  the IC algorithm returns variables that were already included in the model it might indicate that they should be removed since they might be the direct cause of the errors. Therefore, first we train a linear model using the training set. Then we make predictions on the same training set. The error is calculated by the difference between the predictions and the actual values of the sale price in the training set. A histogram of the errors is shown below. Finally, we run the IC algorithm on all variables including the calculated errors. As before the features of interest are the features which are direct neighbors of  the errors in the resultant graph.\n","876b93cc":"### Proper Encoding of Numerical\/Categorical Features <a id = 'encoding'><\/a>\n\nNumerical features which are really categorical features are changed to strings to prepare for them for dummy encoding, while values of categorical features which can be represented as ordered numbers are changed to integers.\n","eca5b10a":"The results of the IC algorithm are stored in the new_features variable below. If we were to re-validate the model by adding these variables to the previously found features it would boost model performance to about .122. For the sake of brevity I am also going to include some additional features found to boost model performance. These additional features were found by running the IC algorithm with a less stringent alpha, or by running the IC algorithm with a removed feature. Some are guesses based on what was deemed important by IC based on dummy variable encoding of the categoricals. \n","f92baaf6":"## Cross-Validation of Simple Linear Model\n\n\nPlotted below are learning curves for a simple linear model trained on the reduced training set with five-fold cross validation. I'm Using both the root mean error and log root mean error for evaluation. The former is a little more intuitive since it is measured in dollars, while the latter is the evaluation metric of the competition. As you can see the learning curve indicates that the model generalizes well to unseen data as indicated by the strong agreement between the training accuracy and the validation accuracy. The learning curve indicates that we can expect an RME of about $25000 and LRME of .14 on unseen data. ","e0375511":"## Context\/Objective <a id = 'context'><\/a>\n\n\nCausal inference is not a topic that I've seen discussed in many data-science notebooks. By enlarge, data science seems preoccupied with boosting prediction scores by any means necessary, not with determining true causal effects between variables. One problem with this approach is that models become like black boxes, providing accurate predictions without providing true insight or understanding of the system of interest. Another problem is that models built on correlation alone cannot be used to accurately predict the result of an intervention into a system. For instance, a model trained to predict sales prices of houses might not be able to determine the effect of an intervention on sales price, e.g. remodeling the kitchen. Only by modeling causal relationships between variables and conditioning on variables according to the backdoor criterion can the effect of an intervention into a system accurately be  modeled. \n\nTo this end, the basic objective of this notebook is to use the Inferred Causality (IC) algorithm as a feature selection technique in the data-science pipeline. Using IC we will find a set of variables determined to directly affect sales prices. Then we will fit and validate a simple linear model built using these causal features. Hopefully, this technique will yield a model that both provides accurate predictions and gives insight into the factors which truly drive sale prices within the dataset. \n\nIn a nutshell, IC uses conditional probability tests between variables to partially reconstruct the causal graph of a system. There\u2019s a lot to unpack here which is outside the scope of this notebook, however, if you are interested I would highly recommend reading this ongoing series of [blog posts](https:\/\/medium.com\/causal-data-science\/causal-data-science-721ed63a4027) on the Pearlian Causality Framework. \n\n","58ba0297":"## Create Submission File <a id = 'submission'><\/a>\n\nThe submission file is created using the predicted sales prices. ","41f170e5":"## Read and Preprocess Data <a id = 'preprocess'><\/a>","fbd7cdbb":"### Feature Engineering\n\nI'm only going to create one additional feature entitled MoYrSold, which is simply a combination of the MoSold and YrSold features. The rationale behind this feature is to capture changes in price which are caused by market effects over time. It turns out that market effects play a pretty small role in this dataset due time span of the dataset. For instance, take a look at this house price index for Ames, Iowa shown on the Federal Reserve economic data [website](https:\/\/fred.stlouisfed.org\/series\/ATNHPIUS11180Q) which shows a relatively flat curve for the period between 2006 and 2010, over which the dataset was compiled. In general it is clear that market effects play a major role in housing prices over time, however, the 2007-2009 recession effectively halted price inflation in the Ames housing market for the particular period in question, which is why market effects over time is not a particularly important feature in the given dataset. \n\n","bece7e10":"### Validation of Updated Model on Unseen Test Data\n\nNow we evaluate the updated model on the unseen test data. Similar to before we use the local outlier factor method  to identify outliers in the test data and use knn to predict the sale price of the outliers. In this case we set the contamination factor to a very low value. This effectively only uses knn to predict the sale price of a single observation. \n","3e29733a":"### Example: Median Discount for Lack of Central Air\n\nAnother potential benefit of trying to build a model based on actual causal effects is that assuming the model is complete and satisfies the backdoor criterion, as explained, above the model would better predict intervention into the system. For instance, assuming that we owned a house whose current price is about the median price in the dataset, how much could we reasonably expect the price of the house to increase if we installed central air? Below I simulate this scenario by taking the difference between a median valued house with and without central air. As can be seen below, our current model predicts a difference of about $14,605. To tell you the truth I have no idea if that is actually accurate, but assuming we've taken into account all confounding variables we should be able to at least estimate such an intervention. Therefore, if we were hypothetically selling a house without central air we could better decide based on the cost of installation whether it would be worth it to install central air, or perform other remodelling work. \n","47db1ed5":"### Read Data from CSV <a id = 'read'><\/a>\n\nData is read from csv and train and test sets are combined for preprocessing. ","7c85b1ab":"### Plot Network of Variables\n\nThe output of the IC algorithm is plotted below. Sale price is plotted in red. \nFor the most part you should ignore the exact direction of the arrows as meaningful since technically, the arrowed edges can represent a causal relationship in either direction. In fact, the only edges of whose direction the algorithm is certain are those whose \u2018marked\u2019 attribute is equal to True. However, in this case even if the algorithm is not certain whether the sale price is a cause or an effect, using a little intuition we can assume that the sale price is caused by the physical attributes of the house rather than the other way around. \n\n","58b95e36":"### Handle Nans <a id = 'nans'><\/a>\n\nNans are filled with there most likely meanings for some columns. For columns in which Nans don't have a likely meanng, Nans are filled with either the median value or the most frequent value depending on whether the variable are numerical or categorical. ","ce2f6e5b":"### Power Transforms <a id = 'power_transform'><\/a>\n\nContinuous variables are transformed to make the variables more normal-like using the yeo-johnson method. ","8eee4b64":"### Prepare Data for Updated Model Validation\n\nTrain and test sets are prepared for validation, by subsetting the datasets to include only the identified causal features and performing automatic outlier detection to remove outliers from the training set. ","537807de":"## Model Explanation\/Feature Importances <a id = 'explanation'><\/a>\n\nUsing permutation importances we can get a sense of how each variable contributes to model performance. Based on the table below the five most important features are ground living area, overall quality, overall condition, total basement surface area and neighborhood. Other important features include the year the house was built, the area of the lot, the condition of the sale and the number of cars that fit in the garage. One positive aspect of using a causal approach to feature selection is that it yields a simple intuitive model which also compares favorably in terms of performance relative to more complex and less interpretable models. For instance, we only use simple linear regression without any regularization and with a log root mean squared error of .1187 we can expect to be in the top 5% of kaggle submissions. Definitely not the best model out there, but also by far not the worst. \n","41f6acf1":"# Predicting Home Prices with Inferred Causality","66e95a18":"## Table of Contents\n* [Context\/Objective](#context)\n* [The Data](#data)\n* [Read and Preprocess Data](#preprocess)\n    * [Imports](#imports)\n    * [Read Data from CSV](#read)\n    * [Handle Nans](#nans)\n    * [Proper Encoding of Numerical\/Categorical Features](#encoding)\n    * [Encode Categoricals](#dummies)\n    * [Power Transform](#power_transform)\n    * [Scaling](#scaling)\n    * [Create Test and Train Sets](#split)\n* [Feature Selection using IC Algorithm](#IC)\n* [Automatic Outier Detection](#outliers)\n* [Validate Model](#validation)\n* [Find Additional Features by Running IC Algorithm on Model Errors](#IC2)\n* [Validate New Features on Errors](#new_feature_val)\n* [Validate Updated Model](#validation2)\n* [Create Submission File](#submission)\n* [Model Explanation](#explanation)\n* [Visualize Model Performace](#visualize)\n* [Conclusion](#conclusion)\n\n","3cafd2ad":"## Conclusion <a id = 'conclusion'><\/a>\n\nIn conclusion, I think this notebook demonstrates that the inferred causality algorithm can be a powerful tool for feature selection which can yield a model which performs well in prediction tasks but is also explainable according to actual cause and effect relationships between the features and the target variable. ","2440fa63":"## Model Validation <a id = 'validation'><\/a>","54754e57":"### Imports <a id = 'imports'><\/a>\n\nn particular take note of the <code>causality<\/code> package which is used to perform the IC algorithm. According to its github page it's still in its alpha phase of development so it's a little rough around the edges. I had to install an old version of networkx for the package to work. Check out the documentation on [Github](https:\/\/github.com\/akelleh\/causality)."}}