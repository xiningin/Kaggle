{"cell_type":{"d9358efc":"code","261209a0":"code","a86569e3":"code","30745a19":"code","05df5850":"code","95b9634f":"code","3986b23f":"code","b608afe9":"code","aeb5f594":"code","24184cd3":"code","35d59848":"code","cb3a2962":"code","bbddcbcd":"code","3a8cfaaf":"code","5c8aa618":"code","be66d805":"code","ebb8747c":"code","923a7085":"code","1247f3d1":"code","9c7a7ec6":"code","34edd0ed":"code","a62a191b":"code","6fd9da5d":"code","c1fd25cc":"code","a8c66d08":"code","501fbbec":"code","99431511":"code","7366cf0b":"code","43c34953":"markdown","f1b7f5e7":"markdown","6564229f":"markdown","1ff069a8":"markdown","ae364557":"markdown","f90246f0":"markdown"},"source":{"d9358efc":"#Load the csv file into a dataframe\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('..\/input\/100k-courseras-course-reviews-dataset\/reviews.csv')\ndf.head() # What's in there?","261209a0":"# We don't need the Id column. Let's drop it !\ndf.drop(columns=['Id'],inplace=True)\ndf.head()","a86569e3":"# How many data points?\ndf.shape #(rows, columns)","30745a19":"df.isna().any() # Any null values?","05df5850":"df.dropna(how='any',inplace=True) # drop any null values!","95b9634f":"from keras.preprocessing.text import Tokenizer\nMAX_WORDS = 8000 \nt = Tokenizer(num_words=MAX_WORDS)\nt.fit_on_texts(df['Review']) # assigns unique int to all the words\nword_index=t.word_index\nword_index # Can you see that?! It is a dictionary of words to numbers","3986b23f":"df['Review'][0] # The first review","b608afe9":"df['Review']=t.texts_to_sequences(df['Review']) #Let's apply the transformation on our reviews !","aeb5f594":"df['Review'][0] # Did you notice something? [good and interesting] is now [16, 2, 39]","24184cd3":"# Split our data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(df['Review'],df['Label'], test_size = 0.2)","35d59848":"# What is the max and min length of reviews?\nreview_length = [len(x) for x in X_train]\nprint(max(review_length))\nprint(min(review_length))","cb3a2962":"# We need to provide a fixed length input so the length of all the reviews should be the same. \n# We are going to take the first 500 words of each review to predict the number of stars.\nfrom keras.preprocessing import sequence\ninput_limit = 500 # first 500 words only of each review to be considered\nX_train = sequence.pad_sequences(X_train,maxlen=input_limit) # pad with 0 if length is less than 500\nX_test = sequence.pad_sequences(X_test,maxlen=input_limit) # pad with 0 if length is less than 500","bbddcbcd":"y_train.unique() # how many stars?","3a8cfaaf":"# Ofcourse we need to do one hot encoding here! \nprint(y_train[0]) #before one hot encoding\nfrom keras.utils import np_utils\ny_train = np_utils.to_categorical(y_train-1,num_classes=5)\ny_test = np_utils.to_categorical(y_test-1,num_classes=5)\ny_train[0] # How label looks after one encoding","5c8aa618":"# Split the test set into test and validation set.\nX_test,X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5)","be66d805":"from keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Flatten\n\nmodel = Sequential()\n# We choose to represnt each word as a 16 element vector=output_dim\n# Note that the output of Embedding layer will be MAX_WORDS*16 i.e. each of the MAX_WORDS will be\n# represented as a 16 element vector.  \nmodel.add(Embedding(input_dim=MAX_WORDS, output_dim=16,input_length=input_limit))\nmodel.add(Flatten())\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(optimizer='adam',metrics=['accuracy'],loss='categorical_crossentropy')\nmodel.summary()","ebb8747c":"#Let's fit the model\nfrom keras.callbacks import ModelCheckpoint\nmodel_checkpoint = ModelCheckpoint('best.hdf5',save_best_only=True)\nmodel.fit(X_train,y_train, validation_data=(X_valid,y_valid),batch_size=64, epochs=10, callbacks=[model_checkpoint])","923a7085":"# Time to check for accuracy\nmodel.load_weights('best.hdf5')\nscore= model.evaluate(X_test,y_test)\nscore[1]","1247f3d1":"from keras.layers import Conv1D,MaxPooling1D\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=MAX_WORDS,output_dim=256,input_length=500))\nmodel.add(Conv1D(filters=256,kernel_size=2,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=128,kernel_size=2,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=64,kernel_size=2,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=32,kernel_size=2,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","9c7a7ec6":"from keras.callbacks import ModelCheckpoint\nmodel_checkpoint = ModelCheckpoint('best.hdf5',save_best_only=True)\nmodel.fit(X_train,y_train, validation_data=(X_valid,y_valid),batch_size=64, epochs=10, callbacks=[model_checkpoint])","34edd0ed":"model.load_weights('best.hdf5')\nscore= model.evaluate(X_test,y_test)\nscore[1]","a62a191b":"import os\nembedding_index={}\nf = open(os.path.join('..\/input\/glove-100-dimension','glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float')\n    embedding_index[word] = coefs\nprint('Found %s word vectors' % len(embedding_index))","6fd9da5d":"EMBED_DIM=100\nembedding_matrix = np.zeros((MAX_WORDS, EMBED_DIM))\nprint(embedding_matrix.shape)\nfor word, index in word_index.items():\n    if index > MAX_WORDS-1:\n        continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","c1fd25cc":"embedding_matrix.shape","a8c66d08":"from keras.layers import Embedding\nembedding_layer = Embedding(MAX_WORDS,\n                            EMBED_DIM,\n                            weights=[embedding_matrix],\n                            input_length=input_limit,\n                            trainable=False)","501fbbec":"from keras.layers import Conv1D, MaxPooling1D,Flatten,Dense,GlobalMaxPooling1D, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=128,kernel_size=5,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=128,kernel_size=5,padding='same',activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=128,kernel_size=5,padding='same',activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.01, momentum=0.9),metrics=['acc'])\nmodel.summary()","99431511":"from keras.callbacks import ModelCheckpoint\nmodel_checkpoint = ModelCheckpoint('best.hdf5',save_best_only=True)\nmodel.fit(X_train,y_train, validation_data=(X_valid,y_valid),batch_size=128, epochs=10, callbacks=[model_checkpoint])","7366cf0b":"model.load_weights('best.hdf5')\nscore= model.evaluate(X_test,y_test)\nscore[1]","43c34953":"In this kernel, we will build different models which can predict the number of stars given by each review. We will then compare the accuracies of all models to find out the best one. Let's get started ! :) ","f1b7f5e7":"To conclude, basic MLP and CNN perform almost the same ~80% as using Glove embedding :) \n\nPlease upvote if you found this useful!\n","6564229f":"**Text pre-processing**\n\nWe are going to use Tokenizer API provided by keras for text pre-processing. Our models cannot accept the texts as input. So, we need to convert them into integers first. There could be a large number of words but we are going to deal with only the first 8000 words ;) ","1ff069a8":"**MLP**\n\nLet's begin with using a MLP to build our model and see how it performs! Btw, keras provides an **embedding layer** which we are going to use here.\nEmbedding layers are used to represent each word as a vector in a pre-defined vector space(we have define the number of elements to be in this vector). **Word embeddings** are used to learn the semantic relationship among words. For example, in a n-dimensional vector space, the words *apple* and *orange* will be closer to each other than the word *car*. It tries to find the contextual meaning and helps to learn better about the given text.","ae364557":"**CNN**\n\nHere, we are going to keep things nearly the same  as above except for adding some convolutional layers.","f90246f0":"**Pre-trained embedding - Word2Vec **\n\nIn both the previous models, along with learning the params for other layers, word embedding values are also learnt i.e. the word vector values get updated with each epoch. Now, we are going to use some embeddings which have already learned for some thousand words like Glove. Note that we are not going to update the values of the already learned word embeddings so we used the parameter `trainable=False` while adding the embedding layer. Major steps to follow are :\n\n1. Load the word embedding.\n2. Create an embedding matrix such that element i represents the word vector for the word whose word_index is i.\n3. Load this embedding matrix into the embedding layer with weights frozen.\n4. Build a CNN on top of it."}}