{"cell_type":{"2e2969fe":"code","f44b8938":"code","a826a0f9":"code","829d9675":"code","fb4a5a72":"code","1cccec4d":"code","5a9b84db":"code","d3ceff25":"code","71d77493":"code","600155c1":"code","53f1973c":"code","3042eaa0":"code","dc7fac9c":"code","00e328cf":"code","12acff6b":"code","e7efd22b":"code","078ef00f":"code","e3a98b47":"code","5b5393c2":"code","40479f79":"markdown","63597f69":"markdown","83e7645a":"markdown","eeac680f":"markdown","33f6adfe":"markdown","e4fe334a":"markdown","0efc2c69":"markdown","21bbca94":"markdown","27148e96":"markdown","2fdcf82a":"markdown","c42b5c93":"markdown","ed9f1f09":"markdown","110c69f0":"markdown","bbf94fe7":"markdown","d4349425":"markdown","c1bc8cbe":"markdown","d5e43cd3":"markdown","3020f5f7":"markdown","f5851f20":"markdown"},"source":{"2e2969fe":"import numpy as np\nimport pandas as pd \nimport re #for immplementing regular expressions\nfrom sklearn.feature_extraction.text import TfidfVectorizer #for implementing Tf-Idf method of Text Processing\nfrom sklearn.metrics.pairwise import linear_kernel #for getting a similarity matrix by comparing the similarity between the data-pooints\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f44b8938":"podcasts=pd.read_csv(\"..\/input\/all-podcast-episodes-published-in-december-2017\/podcasts.csv\")\nprint(podcasts.shape)\npodcasts.head()","a826a0f9":"podcasts.info()","829d9675":"podcasts.language.value_counts()","fb4a5a72":"podcasts=podcasts[podcasts.language == \"English\"]\npodcasts.shape","1cccec4d":"podcasts.info()","5a9b84db":"podcasts=podcasts.dropna(subset=[\"description\"])\npodcasts.info()","d3ceff25":"podcasts=podcasts.drop_duplicates(\"itunes_id\")\npodcasts.shape","71d77493":"podcasts[\"description_#words\"]=[len(x.description.split()) for _,x in podcasts.iterrows()]\npodcasts[\"description_#words\"].describe()","600155c1":"podcasts = podcasts[podcasts[\"description_#words\"] > 19]\npodcasts.shape","53f1973c":"fav_podcasts = [\"Earth 919: A Comic Book Podcast\", \"Digital India\" ,\"Top 5 Comics Podcast\"]\nfav_df=podcasts[podcasts.title.isin(fav_podcasts)]\nfav_df","3042eaa0":"podcasts=podcasts[~podcasts.isin(fav_df)].sample(25000)\npodcasts_df=pd.concat([fav_df,podcasts],sort=True).reset_index(drop=True)\npodcasts_df=podcasts_df.dropna(subset=[\"description\"])\nprint(podcasts_df.shape)\npodcasts_df.head()","dc7fac9c":"podcasts_df.info()","00e328cf":"tfidf_model = TfidfVectorizer(analyzer='word',ngram_range=(1,3),stop_words='english')\ntf_idf = tfidf_model.fit_transform(podcasts_df['description'])\ntf_idf","12acff6b":"similarity_matrix = linear_kernel(tf_idf,tf_idf)\nsimilarity_matrix","e7efd22b":"fav1=podcasts_df[podcasts_df.title == \"Top 5 Comics Podcast\"].index[0]\nindex_recom = similarity_matrix[fav1].argsort(axis=0)[-5:-1]\n\nfor i in index_recom:\n    print(\"Score:\",similarity_matrix[fav1][i],\"\\t Title:\",podcasts_df.title[i])\n    print(podcasts_df.description[i],\"\\n\")\nprint(\"Original Description : \",podcasts_df.description[fav1])","078ef00f":"fav2=podcasts_df[podcasts_df.title == \"Digital India\"].index[0]\nindex_recom = similarity_matrix[fav2].argsort(axis=0)[-5:-1]\n\nfor i in index_recom:\n    print(\"Score:\",similarity_matrix[fav2][i],\"\\t Title:\",podcasts_df.title[i])\n    print(podcasts_df.description[i],\"\\n\")\nprint(\"Original Description : \",podcasts_df.description[fav2])","e3a98b47":"fav3=podcasts_df[podcasts_df.title == \"Earth 919: A Comic Book Podcast\"].index[0]\nindex_recom = similarity_matrix[fav3].argsort(axis=0)[-5:-1]\n\nfor i in index_recom:\n    print(\"Score:\",similarity_matrix[fav3][i],\"\\t Title:\",podcasts_df.title[i])\n    print(podcasts_df.description[i],\"\\n\")\nprint(\"Original Description : \",podcasts_df.description[fav3])","5b5393c2":"fav4=podcasts_df[podcasts_df.title == \"Eat Sleep Code Podcast\"].index[0]\nindex_recom = similarity_matrix[fav4].argsort(axis=0)[-5:-1]\n\nfor i in index_recom:\n    print(\"Score:\",similarity_matrix[fav4][i],\"\\t Title:\",podcasts_df.title[i])\n    print(podcasts_df.description[i],\"\\n\")\nprint(\"Original Description : \",podcasts_df.description[fav4])","40479f79":"Now we can calculate the similarity between the podcasts on the basis of their tf-idf scores or values( received as a sparse matrix) using an appropriate kernel method, such that the words with closer scores tend to have a similar type of value and this value changes(either increases or decreases) as the difference between the tf-idf scores increases or decreases. These values will be stored in a separate dataframe, this would be our Recommender DataFrame.<br><br>\nHere, we use the Linear Kernel, which is based on the *cosine-similarity* of the elements. For the linear_kernel method, closer the value is to 0, for given 2 data-points, more similar are the data-points.","63597f69":"Now let's move further towards the text processing. According to me, using the technique TF-IDF (term frequency \u2013 inverse document frequency) to find out \u2018How important is a word in it\u2019s corresponding document\u2019 is much more efficient. The product of term frequency and inverse document frequency for each word acts up as a score of it\u2019s importance. \n<br>\n<br>\n**Term Frequency** tells us the probability of a word occurring in a document (i.e. number of times the word occur\/total number of words in a document).\n<br>**Inverse Document Frequency** of a word in a given document corpus(dataset) is the logarithmic ratio of the total number of documents to the number of documents int which the word occurs. In our case, the description of each podcast is a document and the collection of all the descriptions is the document corpus.\n<br><\/br><br>It is better to consider the values of each unigram, bigram as well as trigram in the podcasts\u2019 descriptions for our recommendation engine. \n","83e7645a":"These are some of my favourite podcasts that I am going to use for testing the recommendations.","eeac680f":"# Podcast Recommendation Engine using Content-Based Filtering","33f6adfe":"Ever since the concept of recommendation engines have been introduced to the world, organisations having been trying harder and harder in order to perfect the recommender algorithms. Companies like YouTube, Netflix, Spotify and many more are dependent on the effectiveness of their recommendation engines in order to attract new users and manipulating the users to spend more time with their products. Music, movies, series, podcasts etc. each one of them has its own recommendation system.\n\nDue to the advent of better recommendation algorithms, there have been an exponential increase in the demand and usage of podcasts. Podcasts are basically digital audio files of a series of spoken words, audio episodes, that a user can listen to.\n\nHere is a link to my article over '*Building a Podcast Recommendation Engine*'- https:\/\/medium.com\/analytics-vidhya\/building-a-podcast-recommendation-engine-357cda661a12","e4fe334a":"Let's sample out only 25000 data points, rather than running it over the whole dataset for now.","0efc2c69":"Thanks for reading through my kernel! I am working on implementing ***Collaborative Filtering*** for the podcast recommendations and will publish soon, so stay tuned for more!!!","21bbca94":"As we can see in all the 3 examples above, the recommendations are purely based on the **description** of the given favourite podcasts. The system compares the important terms and words, i.e. the keywords like comics,digital, media ,etc. and also some meaningful phrases due to the consideraton of bigrams and trigrams. Although, it's ignoring the semantic meaning due to the implementation of Tf-idf, but it is doing it's job to some extent. ","27148e96":"Thus, we got our Similarity matrix, now let's proceed to test out the system by providing my favourite podcasts.","2fdcf82a":"We can see the number of null objects present in each column, and since we know that the **content** is the most important part for *Content-Based Filtering* so removing the data points having null description values is of utmost importance.","c42b5c93":"There might be some duplicate values for some podcasts, this can be resolved by comparing the itunes_id as each podcast ha it's unique ID and if there is a repeatition, then it's definetly a duplicate value. So, it is better to remoe it.","ed9f1f09":"About 25% of our descriptions have less than 11 words. Obviously, they won't be proper inputs to the recommender system, as we can't depict much from such a small description of any podcast. Here, I'm considering only those podcasts which have more than 19 words in their description.","110c69f0":"Let's count the number of words in each description.","bbf94fe7":"Here I will try to implement *Content-Based Filtering*, even though it has chances of providing lesser accuracy compared to *Collaborative Filtering* but as a begineer *Content-Based Filtering* is easier to apply.\n\nDataset used: *All Podcast Episodes Published In December 2017*, it has a metadata of 121k podcasts and 881k episodes with audio urls.","d4349425":"Well, I just got lucky here, as I didn't expect it to sample this data-point too.\n<br>\nBut as we can see, the system is giving recommendations based on the description of the provided podcast.","c1bc8cbe":"I'll just be using the podcasts.csv for this project.","d5e43cd3":"Now, let us check for the different langauages that the podcasts are present in.","3020f5f7":"I'll be considering only the English podcasts for now, as we know that for each laguage has different semantics and lemmatization methods (for e.g. consider Japanese or Mandarin where each character is a word in itself.) ","f5851f20":"Now let's try the system for another one of my favourite podcast ***\"Eat Sleep Code Podcast\"***."}}