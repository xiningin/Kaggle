{"cell_type":{"472a781c":"code","0e1c35ed":"code","814c41bf":"code","0e743f44":"code","01cc3cf6":"code","e8185dbe":"code","b12ff588":"code","fbe742db":"code","d8ca7e34":"code","11f67230":"code","83ea4b71":"code","0ca046b6":"code","b2268b78":"code","0742b558":"code","04d2125b":"code","d86d327b":"code","456a74b9":"code","d1b82413":"code","dd3d2a94":"code","575badef":"code","ecb98865":"markdown","57231b30":"markdown","b59000d5":"markdown","5e9e8352":"markdown","3825c331":"markdown","24856a34":"markdown","42a4a12e":"markdown","79af6ae6":"markdown","75edcf1f":"markdown","79cc2785":"markdown","fb1f7d9a":"markdown","b475fd8d":"markdown","6da5b958":"markdown","47f361fa":"markdown","d4d00658":"markdown","8d7b3cc7":"markdown","34db8139":"markdown","7c1fff50":"markdown","5833d186":"markdown","ab98d0e8":"markdown","51ac57e7":"markdown","1360d83a":"markdown","a0ffdc09":"markdown","78b5de2d":"markdown","7ac8134d":"markdown","b8411184":"markdown","02ed9c46":"markdown","c039947f":"markdown","83aceff6":"markdown","324f8122":"markdown","8936e85e":"markdown","8e2197a8":"markdown","ac629a61":"markdown","4dbbe9ac":"markdown","9f0b69ae":"markdown","d0d7a0ba":"markdown","f7180783":"markdown","463757de":"markdown","29784855":"markdown","59e9734b":"markdown","f5e28344":"markdown","cfeccabf":"markdown","4e7f4575":"markdown","69990323":"markdown","529bef6a":"markdown","c361482c":"markdown","5f2536ac":"markdown","1ef2b407":"markdown"},"source":{"472a781c":"import numpy as np\nimport pandas as pd\n\nfrom lightgbm.sklearn import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline\n\nimport warnings                                  # `do not disturbe` mode\nwarnings.filterwarnings('ignore')","0e1c35ed":"from sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\nn = diabetes.data.shape[0]\n\ndata = diabetes.data\ntargets = diabetes.target","814c41bf":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nrandom_state=42\nn_iter=50\n\ntrain_data, test_data, train_targets, test_targets = train_test_split(data, targets, \n                                                                      test_size=0.20, shuffle=True,\n                                                                      random_state=random_state)\n\nnum_folds=2\nkf = KFold(n_splits=num_folds, random_state=random_state)","0e743f44":"model = LGBMRegressor(random_state=random_state)","01cc3cf6":"%%time\nscore = -cross_val_score(model, train_data, train_targets, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\nprint(score)","e8185dbe":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid={'learning_rate': np.logspace(-3, -1, 3),\n            'max_depth':  np.linspace(5,12,8,dtype = int),\n            'n_estimators': np.linspace(800,1200,5, dtype = int),\n            'random_state': [random_state]}\n\ngs=GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', fit_params=None, \n                n_jobs=-1, cv=kf, verbose=False)\n\ngs.fit(train_data, train_targets)\ngs_test_score=mean_squared_error(test_targets, gs.predict(test_data))\n\n\nprint(\"Best MSE {:.3f} params {}\".format(-gs.best_score_, gs.best_params_))","b12ff588":"gs_results_df=pd.DataFrame(np.transpose([-gs.cv_results_['mean_test_score'],\n                                         gs.cv_results_['param_learning_rate'].data,\n                                         gs.cv_results_['param_max_depth'].data,\n                                         gs.cv_results_['param_n_estimators'].data]),\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ngs_results_df.plot(subplots=True,figsize=(10, 10))","fbe742db":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_grid_rand={'learning_rate': np.logspace(-5, 0, 100),\n                 'max_depth':  randint(2,20),\n                 'n_estimators': randint(100,2000),\n                 'random_state': [random_state]}\n\nrs=RandomizedSearchCV(model, param_grid_rand, n_iter = n_iter, scoring='neg_mean_squared_error', fit_params=None, \n                n_jobs=-1, cv=kf, verbose=False, random_state=random_state)\n\nrs.fit(train_data, train_targets)\n\nrs_test_score=mean_squared_error(test_targets, rs.predict(test_data))\n\nprint(\"Best MSE {:.3f} params {}\".format(-rs.best_score_, rs.best_params_))","d8ca7e34":"rs_results_df=pd.DataFrame(np.transpose([-rs.cv_results_['mean_test_score'],\n                                         rs.cv_results_['param_learning_rate'].data,\n                                         rs.cv_results_['param_max_depth'].data,\n                                         rs.cv_results_['param_n_estimators'].data]),\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\nrs_results_df.plot(subplots=True,figsize=(10, 10))","11f67230":"!pip install hyperopt\n#!conda install -c conda-forge hyperopt","83ea4b71":"from hyperopt import fmin, tpe, hp, anneal, Trials","0ca046b6":"def gb_mse_cv(params, random_state=random_state, cv=kf, X=train_data, y=train_targets):\n    # the function gets a set of variable parameters in \"param\"\n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth']), \n             'learning_rate': params['learning_rate']}\n    \n    # we use this params to create a new LGBM Regressor\n    model = LGBMRegressor(random_state=random_state, **params)\n    \n    # and then conduct the cross validation with the same folds as before\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n\n    return score","b2268b78":"%%time\n\n# possible values of parameters\nspace={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=gb_mse_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=n_iter, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n         )\n\n# computing the score on the test set\nmodel = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),\n                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\nmodel.fit(train_data,train_targets)\ntpe_test_score=mean_squared_error(test_targets, model.predict(test_data))\n\nprint(\"Best MSE {:.3f} params {}\".format( gb_mse_cv(best), best))","0742b558":"tpe_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\n\ntpe_results_df=pd.DataFrame(tpe_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ntpe_results_df.plot(subplots=True,figsize=(10, 10))","04d2125b":"%%time\n\n# possible values of parameters\nspace={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=gb_mse_cv, # function to optimize\n          space=space, \n          algo=anneal.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=n_iter, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n         )\n\n# computing the score on the test set\nmodel = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),\n                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\nmodel.fit(train_data,train_targets)\nsa_test_score=mean_squared_error(test_targets, model.predict(test_data))\n\nprint(\"Best MSE {:.3f} params {}\".format( gb_mse_cv(best), best))","d86d327b":"sa_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\n\nsa_results_df=pd.DataFrame(sa_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\nsa_results_df.plot(subplots=True,figsize=(10, 10))","456a74b9":"scores_df=pd.DataFrame(index=range(n_iter))\nscores_df['Grid Search']=gs_results_df['score'].cummin()\nscores_df['Random Search']=rs_results_df['score'].cummin()\nscores_df['TPE']=tpe_results_df['score'].cummin()\nscores_df['Annealing']=sa_results_df['score'].cummin()\n\nax = scores_df.plot()\n\nax.set_xlabel(\"number_of_iterations\")\nax.set_ylabel(\"best_cumulative_score\")","d1b82413":"print('Test MSE scored:')\nprint(\"Grid Search {:.3f}\".format(gs_test_score))\nprint(\"Random Search {:.3f}\".format(rs_test_score))\nprint(\"TPE {:.3f}\".format(tpe_test_score))\nprint(\"Annealing {:.3f}\".format(sa_test_score))","dd3d2a94":"# installing hpsklearn\n!pip install hpsklearn","575badef":"%%time\n\nfrom hpsklearn import HyperoptEstimator, xgboost_regression\n\nestim = HyperoptEstimator(regressor=xgboost_regression('my_gb'), max_evals=n_iter, trial_timeout=60, seed=random_state)\n\nestim.fit(train_data, train_targets)\n\nprint(mean_squared_error(test_targets, estim.predict(test_data)))","ecb98865":"## Intro","57231b30":"We will demonstrate and compare different algorithms on diabetes dataset from sklearn.datasets. Let's load it.","b59000d5":"We can see that the movement of the parameters are quite random but the results become better with time: there are no extremely bad scores after 25 iterations but the number of good solutions increases. Algorithm started to predict quite good solutions, using information from the previous steps.","5e9e8352":"Let's train a baseline model with the default parameters:","3825c331":"## Random Search","24856a34":"First of all let's import some useful functions from the hyperopt:\n- fmin - the main function for us, it will minimize our functional\n- tpe and anneal - optimization approaches\n- hp - include different distributions of variables\n- Trials - is used for logging","42a4a12e":"## Resume","79af6ae6":"Let's try to use RandomizedSearchCV from sklearn.model_selection.\n\nWe will start with very broad parameters space and make only 50 random steps:","75edcf1f":"The dataset is very small. I have selected it because it will be easy to demonstrate basic concept using it. You will not need to wait hours when everything is calculating. We will divide the dataset into train and test parts. The train part will be split into 2 folds, we will use Cross Validation MSE as a final metrics according to which we are optimizing parameters.\n\nDisclaimer: this toy example is far from the real life and it is used only for the fast illustration. Because of the small dataset and only 2 folds it can be unstable - results change drastically with different random_state.","79cc2785":"Now you know more about different hyperparameters optimization approaches and can apply hyperopt library on practice. In real case you never know in advance which approach will be the best (even in this toy example with some random states RandomizedSearch can win), and sometimes fast GridSearch or simple RandomizedSearch can be a good choice, but it is always useful to know about alternatives.\n\nHope this tutorial will save you a lot of time in the future ML projects.","fb1f7d9a":"Actually hypeopt has wrappers for the most popular sklearn functions. Using them you don't even need to specify the space of parameters you need just tell hyperopt, what function you want to use. Just look at example below (we will use  XGBoost as an example because there is no LGBMRegressor in hpsklearn).","b475fd8d":"While ${T_i}$ is high, the algorithm performs a lot of exploration steps (similar to random search) as the probability to update ${x^*}$ is high even if ${F(x_i)}>{F(x^*)}$\n\nBut when T became lower the algorithm focuses on exploitation - all ${x_i}$ are near one of the best solutions found so far.\n\nIn the end with the right algorithms parameters it can reach a good balance between exploitation \/ exploration and can lead to the better results comparing to the Random Search. Let's check it on our toy example.","6da5b958":"You can try to implement your own realization of the simulated annealing algorithm (it is much easier than TPE), but it is already implemented in hyperopt and we can just set \"algo\" param of fmin to \"anneal.suggest\" (hyperopt will automatically chose parameters of anneal for you).","47f361fa":"Using of fmin is very simple. We just need to define the possible space of our parameters and call the function.","d4d00658":"This visualization shows the main idea of the Simulated annealing algorithm very well. At the beginning when the temperature is high it works similarly to Random Search - it is just exploring all possible states. But along with cooling it moves to exploitation stage and focus on the most promising areas. And finally converges to the very good solution.","8d7b3cc7":"You can find the description of the dataset  here: [https:\/\/www4.stat.ncsu.edu\/~boos\/var.select\/diabetes.html ]\n    \nLong story short: this is the dataset with information about some patients and target metric \"quantitative measure of disease progression one year after baseline\". For the purpose of this tutorial you don't even need to understand the data, just keep in mind that we are solving some regression problem and want to tune our hyperparameters.","34db8139":"We will use hyperopt [https:\/\/github.com\/hyperopt\/hyperopt ] library to deal with this algorithms. It is one of the most popular libraries for the hyperparameter optimization.","7c1fff50":"### Simulated Anneal","5833d186":"We can modify random search adding more attention to the areas where we have already find quite a good solutions. There are different ways to do it. We will consider two of them Tree-structured Parzen Estimator and Simulated Anneal.","ab98d0e8":"### Tree-structured Parzen Estimator","51ac57e7":"TPE is a default algorithm for the Hyperopt. It uses Bayesian approach for optimization. At every step it is trying to build probabilistic model of the function and choose the most promising parameters for the next step. Generally this types of algorithms works like this:\n\n1. Generate random initial point ${x^*}$\n2. Calculate ${F(x^*)}$\n3. Using the history of trials try to build the conditional probability model $P(F | x)$\n4. Choose ${x_i}$ that according to the $P(F | x)$ will most probably result in better ${F(x_i)}$\n5. Compute the real value of the ${F(x_i)}$\n6. Repeat steps 3-5 until one of the stop criteria is satisfied, for example i > max_eval\n\nMore information about particular TPE algorithm you can find, for example, here [https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f ] or in other articles. But it is beyond the scope of this tutorial.\n\nLet's go to practice.","1360d83a":"The first and the simplest method to try is GridSearchCV which is included in sklearn.model_selection\nThis approach just trying all available parameters' combinations 1 by 1 and choose the one with the best cross validation results.\n\nThis approach has several drawbacks:\n1. It is very slow - you just try ALL combinations of ALL parameters and it takes a lot of time. Any additional parameter to variate multiply the number of iterations you need to complete. Imagine that you add to the parameter grid a new parameter with 10 possible values, this parameter can turn out to be meaningless but the computational time will be increased 10 times.\n2. It can work only with discrete values. If the global optimum is on n_estimators=550, but you are doing GridSearchCV from 100 to 1000 with step 100, you will never reach the optimal point.\n3. You need to know \/ guess the approximate localization of the optimum to complete the search in a reasonable time.\n\nYou can overcome some of this drawbacks: you can do grid search parameter by parameter, or use it several times starting from the broad grid with large steps and narrowing the boundaries and decreasing step sizes on any iterations. But is still will be very computationally intensive and long.","a0ffdc09":"It is turned out that the results from the Annealing algorithm actually has the lowest test score. The results shake and on average better test score comparing to the CV are the consequences of the very small dataset.","78b5de2d":"We will try to solve the problem using LGBMRegressor. Gradient Boosting has a lot of hyperparameters to optimize and that's why it is a good choice for our demonstration.","7ac8134d":"Simulated anneal minimizes function ${F(x)}$ (in case of hyperparameters optimization x - parameters, F() - cross validation score function) as follows:\n\n1. Generate random initial point ${x^*}$\n2. Calculate ${F(x^*)}$\n\n3. Generate ${x_i}$ randomly in some neighbourhood of ${x*}$\n4. Calculate ${F(x_i)}$\n5. Update ${x*}$ according to the rule:\n\n    if ${F(x_i)}<={F(x^*)}: {x^*} = {x_i}$ \n\n    else: ${x^*} = {x_i}$ with probability $p=\\exp\\left(\\dfrac{F({x^*})-F({x_i})}{T_i}\\right)$\n\nwhere ${T_i}$, called temperature is constantly decreasing sequence\n\nRepeat steps 3-5 until one of the stop criteria is satisfied:\n- i > max_eval\n- ${T_i} < {T_{min}}$","b8411184":"Let's estimate the time to do the Grid Search in our case. Let's suppose we want our grid to consist of 20 possible values of 'n_estimators' (100 to 2000), 19 values of 'max_depth' (2 to 20), and 5 values of 'learning_rate' (10e-4 to 0.1).\n\nThis means we need to compute cross_val_score 20\\*19\\*5 = 1 900 times. If 1 computation takes ~0.5-1.0 second, our grid search will last for ~15-30 minutes. It is too much for the dataset with ~400 data points.\n\nWe don't want to wait so long. We need to narrow down intervals we want to analyze using this method. I have left only 5\\*8\\*3=120 combinations. On my laptom it was computed in 1,5 minutes on kaggle kernel ~20 seconds.\n\nLet's do the computations:","02ed9c46":"The result of out of the box model is 3532. Let's try to improve it using different optimization approaches.\nFor the demonstration purposes we will optimize model tunning only 3 parameters:\n- n_estimators: from 100 to 2000\n- max_depth: from 2 to 20\n- learning_rate: from 10e-5 to 1","c039947f":"We have managed to improve the results. But spent a lot of time on it. Let's look how our parameters have been changing from iteration to iteration:","83aceff6":"We have done almost nothing but got quite a good result. Moreover, if you are lazy enough you don't even need to choose the model, hyperopt will select it for you. Just ask hyperopt to find the best estimator for the given data (don't pass \"regressor\" parameter to the HyperoptEstimator) and wait - you can try to do it by yourself. Welcome to the AutoML world :)","324f8122":"We can see that for example max_depth is the least important parameter it does not influence score significantly. But we are searching over 8 different values of max_depth, and with any fixed value search over other parameters. It is obvious waste of time and resources.\n\nLet's try a RandomizedSearch approach now.","8936e85e":"## Hyperopt","8e2197a8":"As we can see every step is completely random. It helps not to spent time on useless parameters, but it still does not use the information gathered on the first steps to improve outcomes of the latter ones.","ac629a61":"## GridSearch","4dbbe9ac":"Random Search is on average more effective than Grid Search.\n\nMain advantages:\n1. Don't spend time on meaningless parameters. On every step random search variate all parameters.\n2. On average finds ~optimal parameters much faster than Grid search.\n3. It is not limited by grid when we optimize continuous parameters.\n\nDisadvantages:\n1. It may not find the global optimal parameter on a grid.\n2. All steps are independent. On every particular step it does not use any information about the results gathered so far. But they can be useful. For example, if we found a good solution it can be useful to search around it to find even better point comparing to looking at other completely random variants.","9f0b69ae":"We are finally prepared - we have a function gb_mse_cv(), that we will minimize varying parameters: 'learning_rate', 'max_depth', 'n_estimators'. Let's start with the TPE algorithm.","d0d7a0ba":"## Results","f7180783":"And just to be sure that everything was correct let's compare test data results, and ensure they are inline with cross validation ones","463757de":"We can see than TPE and Annealing algorithms actually keeps improving search results over time even on later steps while Random search randomly found quite a good solution in the beginning and then only slightly improved the results. The current difference between TPE and RandomizedSearch results is quite small but in some real life applications with more diversified range of hyperparameters hyperopt can give you significant time\/score improvement.\n\nNote: in real life it is more correct to use time and not a number of iterations for comparison, but in our toy example the proportion of time spent on the additional calculations in tpe and annealing is to high comparing to cross_val_score calculation time so I have decided not to mislead you about computational speed of the hyperopt and plot scores in relation to the iteration number.","29784855":"As we can see, the results are already better than GridSearchCV. We have spent less time and made more complete search. Let's look at our visualization:","59e9734b":"Let's plot best_cumulative_score vs. number_of_iterations for all approaches:","f5e28344":"## Bonus","cfeccabf":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n### <center> Author: Ilya Larchenko, ODS Slack: ilya_l\n    \n## <center> Tutorial\n## <center> Forget about GridSearch - how to tune hyperparameters using Hyperopt","4e7f4575":"Hyperparameters tunning is an essential part of any Machine Learning project and one of the most time consuming.\nEven for the simplest models it can take hours to find the optimal parameters not mentioning neural nets that can be optimized day, weeks or even longer.\n\nThere are standard approaches to solve this task - Grid Search and Random Search. Every data scientist is familiar with them. But are there alternatives? Are there ways to find better parameters and do it faster?\n\nThe answer is yes - hyperparameters tunning is no more than function optimization task. And obviously Grid or Random search do not seem to be the only and the best algorithms.\n\nIn this tutorial I will consider a couple of alternative approaches - TPE and Simulated Annealing. These approaches are not the only alternatives but usually they work better than standard search approaches and are quite simple to implement. I will describe how they work from the theoretical standpoint and will show you how to use them in practice using Hyperopt library.\n\nAfter this tutorial you will know how to easily speed up your modeling process.\n\n\nDisclaimer: I will use toy example here and will not utilize the optimal order of parameters optimization for LGBMRegressor, the sole goal of the tutorial is to demonstrate and explain how to use Hyperopt on the extremely simplified example.","69990323":"The interface of hyperop.fmin differs from Grid or Randomized search. First of all we need to create a function to minimize.","529bef6a":"To install the library you can use either pip or conda (depending on your environment)","c361482c":"## Preparation step","5f2536ac":"Let's import some standard libraries","1ef2b407":"We have managed to find even better solution comparing to the RandomizedSearch.\nLet's look at the visualization of the process"}}