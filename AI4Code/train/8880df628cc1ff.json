{"cell_type":{"6f46383b":"code","425d63a4":"code","90e50623":"code","67488291":"code","88eba2e1":"code","86f16b05":"code","92ca4671":"code","14f84f8e":"code","61bf71b7":"code","dc15f906":"code","51945383":"code","bd2cf519":"code","cc21e6ec":"code","9facf360":"code","48353e7a":"code","3827a9e0":"code","74dbbc2f":"code","c1b5ef21":"code","199063f2":"code","ee31ffd6":"code","baf0892c":"code","52264928":"code","9ff76198":"code","b4dc0097":"code","8966349f":"code","2a5db0dd":"code","fb519987":"code","cbbb9ad1":"code","c26ed5a8":"code","b33888e6":"markdown","dbc7d58e":"markdown","3d05d65f":"markdown","6e5d9110":"markdown","78fcf32f":"markdown","b223371b":"markdown","94b578d7":"markdown","39d4396f":"markdown","65343880":"markdown","2a11220e":"markdown","4ca44277":"markdown","e9dc0c2a":"markdown","09389cb8":"markdown","73ac2bcb":"markdown","88d6ebc9":"markdown","21d83988":"markdown","61b6fe2d":"markdown","fac6b74e":"markdown"},"source":{"6f46383b":"# import basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","425d63a4":"data = pd.read_csv('..\/input\/palmer-archipelago-antarctica-penguin-data\/penguins_size.csv')\n# Check the data\ndata.info()","90e50623":"data.head()","67488291":"sns.countplot(data['species'],palette='spring');","88eba2e1":"sns.pairplot(data,hue='species');","86f16b05":"print(\"Let's explore distribution of our data.\")\nfig,axes=plt.subplots(4,1,figsize=(5,20))\nsns.boxplot(x=data.species,y=data.flipper_length_mm,ax=axes[0],palette='summer')\naxes[0].set_title(\"Flipper length distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.culmen_length_mm,ax=axes[1],palette='rocket')\naxes[1].set_title(\"Culmen length distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.culmen_depth_mm,ax=axes[2],palette='twilight')\naxes[2].set_title(\"Culmen depth distribution\",fontsize=20,color='Red')\nsns.boxplot(x=data.species,y=data.body_mass_g,ax=axes[3],palette='Set2')\naxes[3].set_title(\"Body mass distribution\",fontsize=20,color='Red')\nplt.tight_layout();","92ca4671":"print(\"Mean body mass index distribution\")\ndata.groupby(['species','sex']).mean()['body_mass_g'].round(2)","14f84f8e":"100*data.isnull().sum()\/len(data)","61bf71b7":"data['sex'].fillna(data['sex'].mode()[0],inplace=True)\ncol_to_be_imputed = ['culmen_length_mm', 'culmen_depth_mm','flipper_length_mm', 'body_mass_g']\nfor item in col_to_be_imputed:\n    data[item].fillna(data[item].mean(),inplace=True)","dc15f906":"data.species.value_counts()","51945383":"data.island.value_counts()","bd2cf519":"data.sex.value_counts()","cc21e6ec":"data[data['sex']=='.']","9facf360":"data.loc[336,'sex'] = 'FEMALE'","48353e7a":"# Target variable can also be encoded using sklearn.preprocessing.LabelEncoder\ndata['species']=data['species'].map({'Adelie':0,'Gentoo':1,'Chinstrap':2})\n\n# creating dummy variables for categorical features\ndummies = pd.get_dummies(data[['island','sex']],drop_first=True)","3827a9e0":"# we do not standardize dummy variables \ndf_to_be_scaled = data.drop(['island','sex'],axis=1)\ntarget = df_to_be_scaled.species\ndf_feat= df_to_be_scaled.drop('species',axis=1)","74dbbc2f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_feat)\ndf_scaled = scaler.transform(df_feat)\ndf_scaled = pd.DataFrame(df_scaled,columns=df_feat.columns[:4])\ndf_preprocessed = pd.concat([df_scaled,dummies,target],axis=1)\ndf_preprocessed.head()","c1b5ef21":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\nkmeans = KMeans(3,init='k-means++')\nkmeans.fit(df_preprocessed.drop('species',axis=1))\nprint(confusion_matrix(df_preprocessed.species,kmeans.labels_))","199063f2":"print(classification_report(df_preprocessed.species,kmeans.labels_))","ee31ffd6":"f\"Accuracy is {np.round(100*accuracy_score(df_preprocessed.species,kmeans.labels_),2)}\"","baf0892c":"wcss=[]\nfor i in range(1,10):\n    kmeans = KMeans(i)\n    kmeans.fit(df_preprocessed.drop('species',axis=1))\n    pred_i = kmeans.labels_\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,10),wcss)\nplt.ylim([0,1800])\nplt.title('The Elbow Method',{'fontsize':20})\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-cluster Sum of Squares');","52264928":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# We need to split data for supervised learning models.\nX_train, X_test, y_train, y_test = train_test_split(df_preprocessed.drop('species',axis=1),target,test_size=0.50)\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\npreds_knn = knn.predict(X_test)\nprint(confusion_matrix(y_test,preds_knn))","9ff76198":"plt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(confusion_matrix(y_test,preds_knn),annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24});","b4dc0097":"print(classification_report(y_test,preds_knn))","8966349f":"print(accuracy_score(y_test,preds_knn))","2a5db0dd":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(knn.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(knn.score(X_test, y_test)))","fb519987":"error_rate=[]\nfor i in range(1,10):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i!=y_test))","cbbb9ad1":"plt.figure(figsize=(10,6))\nplt.plot(range(1,10),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","c26ed5a8":"knn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train,y_train)\npreds_knn = knn.predict(X_test)\nprint(confusion_matrix(y_test,preds_knn))\nprint(classification_report(y_test,preds_knn))","b33888e6":"## <a id ='kneighbor'>K-Nearest Neighbours Classification<\/a>\n\nIt is a **supervised learning algorithm** which can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry.\n\nWith the given data, KNN can classify new, unlabelled data by analysis of the ***k number of the nearest data points***.","dbc7d58e":"We can observe from graph that as k goes from 1 to 3 there is a steep decline in wcss. As k increases further decrease in k becomes linear. So, k=3 is a good choice.","3d05d65f":"Oops! Where did this '.' entry came from?","6e5d9110":"Percentage of missing data is very less. Let's impute it with median in numerical features and mode in categorical feature.\nHere, I have used .fillna method from pandas library.\n\nMissing values can also be filled using pre-defined functions like SimpleImputer from sklearn.","78fcf32f":"**Figuring out best value for k**","b223371b":"## About the dataset\n\n<img src=\"https:\/\/previews.123rf.com\/images\/aomeditor\/aomeditor1903\/aomeditor190300021\/122254680-illustrator-of-body-parts-of-penguin.jpg\" height='400px' width='400px'>","94b578d7":"## <a id='kmeans'>K-Means Clustering<\/a>\n\nIt is very important to note, we actually have the labels for this data set, but we will NOT use them for the K-Means clustering algorithm, since that is an **unsupervised learning algorithm**. K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid. The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n\nK Means Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted, and the algorithm just tries to find patterns in the data. In k means clustering, we have to specify the ***number of clusters k*** we want the data to be grouped into.","39d4396f":"### Dealing with categorical features","65343880":"So, we achieved 99% accuracy.","2a11220e":"### Standardizing feature variables.","4ca44277":"From the graph, we can notice that best vaalue for k is 6.\n\n**Best Model:**","e9dc0c2a":"Our data is not balanced. As data is not big enough so I will not balance it.\n\nIn case of unbalanced data we can either up-sample minority class or down-sample majority class. To see an example see my following notebooks:\n* [up-sampling-minority-class](https:\/\/www.kaggle.com\/ayushikaushik\/up-sampling-to-tackle-unbalanced-dataset)\n* [down-sampling-majority-class](https:\/\/www.kaggle.com\/ayushikaushik\/down-sampling-majority-class-6-classification-algo)\n\nTo know cons of imbalanced data and more ways to handle it, read [this](https:\/\/elitedatascience.com\/imbalanced-classes) article.","09389cb8":"# K-Means Clustering and K Nearest Neighbors Project\n\nJump to:\n* [K Means clustering model](#kmeans)\n* [K Nearest Neighbors model](#kneighbor)","73ac2bcb":"### Dealing with missing values\n\nLet's check out the percentage of missing values.","88d6ebc9":"We can see clusters are easily separable in the cases:\n1. culmen_length_mm  vs  culmen_depth_mm ;\n2. culmen_length_mm  vs  flipper_length_mm ;\n3. culmen_length_mm  vs  body_mass_g.","21d83988":"# EDA","61b6fe2d":"### <b>Columns in the dataset<\/b>\n<ul>\n    <li><b>Species: <\/b>penguin species (Chinstrap, Ad\u00e9lie, or Gentoo)<\/li>\n    <li><b>Island: <\/b>island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica)<\/li>\n    <li><b>culmen_length_mm: <\/b>culmen length (mm)<\/li>\n    <li><b>culmen_depth_mm: <\/b>culmen depth (mm)<\/li>\n    <li><b>flipper_length_mm: <\/b>flipper length (mm)<\/li>\n    <li><b>body_mass_g: <\/b>body mass (g)<\/li>\n    <li><b>Sex: <\/b>penguin sex<\/li>\n<\/ul>","fac6b74e":"> Running above cell again and again gives different results each time. While making this notebook, best accuracy wass 95%. After commiting, it can show anything. I don't know how to handle this."}}