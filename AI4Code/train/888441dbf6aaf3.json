{"cell_type":{"79f8bc5d":"code","d4b071b6":"code","98c99788":"code","2dc8479e":"code","9c2856a0":"code","d7cc6f28":"code","fb8ee929":"code","b5708734":"code","0a0dc0eb":"code","1bb91f4f":"code","3573b96e":"code","bcb4951c":"code","52b318c4":"code","962352ce":"code","65e7528b":"code","0c9fecd4":"code","c541ff01":"code","bef14954":"code","e67cc466":"code","66775c2e":"code","5dd13ab2":"code","44a05fc1":"code","8e7c6e8b":"code","cb510b8c":"code","82c2a9dd":"code","2452abeb":"code","571c2fdf":"code","11c004af":"code","275da6da":"code","57f781a8":"code","e70f08e9":"code","441f2279":"code","4123d6bf":"code","ec617d87":"code","999db888":"code","f943c479":"code","af23e843":"code","cd7f689f":"code","69f83114":"code","3acda034":"code","121dc87a":"code","f8566625":"code","43abe145":"code","c6e093b9":"code","a1a87b83":"code","516bfb75":"code","ae87d728":"code","02ba25c1":"markdown","250d649a":"markdown","86013a47":"markdown","939ac766":"markdown","861f0031":"markdown","701f4cc6":"markdown","f6646fc9":"markdown","5a5e03c2":"markdown","c86f09bd":"markdown","32d0d9fc":"markdown","25dc270a":"markdown","e3c6e30b":"markdown","5f847ffb":"markdown","206f9ec5":"markdown","3887f54e":"markdown","48cf823e":"markdown","90c6ead6":"markdown","655f37f0":"markdown","bcebb932":"markdown","9d7bfdc8":"markdown","c939bd3a":"markdown","afbf2a9f":"markdown","2fd6a26b":"markdown","ee3ab5a2":"markdown","ffef9ecb":"markdown","003f153b":"markdown","8465e1db":"markdown","cb5507a9":"markdown","e2daea7f":"markdown","079e9ce4":"markdown","27c8e2a6":"markdown","c2bdeed6":"markdown","6e1cbc27":"markdown","fc063e71":"markdown","d15c23c9":"markdown","7867d874":"markdown","c14bf5f6":"markdown","27fe8b4b":"markdown","283b8622":"markdown","b790ec36":"markdown","5b43ef7d":"markdown","1ef10b58":"markdown","560af10a":"markdown","0e11d3d9":"markdown","3ddd3aa4":"markdown","f4388fbe":"markdown","be86ee2d":"markdown","c9fc1abf":"markdown","207571e6":"markdown","7b08da78":"markdown"},"source":{"79f8bc5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4b071b6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px","98c99788":"dataset = pd.read_csv (\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","2dc8479e":"dataset.shape","9c2856a0":"dataset.columns","d7cc6f28":"dataset.info ()","fb8ee929":"dataset.head ()","b5708734":"dataset.isnull ()","0a0dc0eb":"dataset.isna (). sum ()","1bb91f4f":"dict = {}\nfor i in list(dataset.columns):\n    dict[i] = dataset[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","3573b96e":"dataset.describe ()","bcb4951c":"dataset.duplicated (). sum ()","52b318c4":"dataset.drop_duplicates (inplace = True)\ndataset.duplicated (). sum ()","962352ce":"df = pd.DataFrame (dataset, columns = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'])\ndf.corr ()","65e7528b":"corr_Matrix = df.corr ()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap (corr_Matrix, linewidths = 0.5, annot = True, fmt= '.1f',ax=ax)\nplt.show ()","0c9fecd4":"fig = px.histogram (dataset, x = \"age\", nbins = 6, facet_row = \"output\", title = \"Heart Attacks Per Age Group\", template = 'plotly_dark')\nfig.show ()","c541ff01":"fig =  px.pie (dataset, names = \"cp\", hole = 0.4, template = \"gridon\", title = \" Types Of Chest Pain\")\nfig.show ()\nsns.countplot (x = 'cp', data = dataset)","bef14954":"fig = px.sunburst(dataset, names = \"cp\", path = [\"output\",\"cp\"], template = \"gridon\", title = \"Heart Attack Chances Based On Chest Pain \")\nfig.show()","e67cc466":"fig = px.scatter (dataset, x = \"age\", y = \"trtbps\", color = \"output\", template = \"plotly\", size = \"trtbps\", title = \"Age vs Resting Blood Sugar Level and Impact On Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"trtbps\", nbins = 12, facet_row = \"output\", title = \"Heart Attacks per Resting Blood Sugar Levels\", template = 'plotly_dark')\nfig.show ()","66775c2e":"fig = px.scatter (dataset, x = \"age\", y = \"chol\", color = \"output\", template = \"plotly\", size = \"chol\", title = \"Age vs Cholestrol Level and Impact On Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"chol\", nbins = 10, facet_row = \"output\", title = \"Heart Attacks per Cholestrol Levels\", template = 'plotly_dark')\nfig.show ()","5dd13ab2":"fig =  px.pie (dataset, names = \"fbs\", hole = 0.4, template = \"gridon\", title = \"Fasting Blood Sugar Levels\")\nfig.show ()\nsns.countplot (x = 'fbs', data = dataset)","44a05fc1":"fig = px.sunburst(dataset, names = \"fbs\", path = [\"output\",\"fbs\"], template = \"gridon\", title = \"Heart Attack Chances Based On Fasting Blood Sugar Levels\")\nfig.show ()","8e7c6e8b":"fig =  px.pie (dataset, names = \"restecg\", hole = 0.4, template = \"gridon\", title = \"Resting Electrocardiographic Results\")\nfig.show ()\nsns.countplot (x = 'restecg', data = dataset)","cb510b8c":"fig = px.sunburst(dataset, names = \"restecg\", path = [\"output\",\"restecg\"], template = \"gridon\", title = \"Heart Attack Chances Based On Resting Electrocardiographic Results\")\nfig.show ()","82c2a9dd":"fig = px.scatter (dataset, x = \"age\", y = \"thalachh\", color = \"output\", size = \"thalachh\", template = \"plotly\", title = \"Age vs Maxmum Heart Rate and Impact on Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"thalachh\", nbins = 7, facet_row = \"output\", title = \"Heart Attacks per Maximum Heart Rate\", template = 'plotly_dark')\nfig.show ()","2452abeb":"fig = px.scatter_3d (dataset, x = \"age\", y = \"trtbps\", z = \"chol\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Resting Blood Sugar vs Cholestrol\")\nfig.show ()","571c2fdf":"fig = px.scatter_3d (dataset, x = \"age\", y = \"trtbps\", z = \"fbs\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Resting Blood Sugar vs Fasting Blood Sugar\")\nfig.show ()","11c004af":"fig = px.scatter_3d (dataset, x = \"age\", y = \"thalachh\", z = \"cp\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Maximum Heart Rate vs Chest Pain\")\nfig.show ()","275da6da":"sns.pairplot (dataset, hue = \"output\")\nplt.show ()","57f781a8":"X = dataset.iloc [ : , : -1].values\nY = dataset.iloc [ :, -1].values\nX.shape","e70f08e9":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.25, random_state = 1)","441f2279":"X_train.shape ","4123d6bf":"Y_train.shape ","ec617d87":"X_test.shape","999db888":"Y_test.shape","f943c479":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler ()\nX_train = sc.fit_transform (X_train)\nX_test = sc.transform (X_test)","af23e843":"print (X_train [ : 5, : ])","cd7f689f":"print (X_test [ : 5, : ])","69f83114":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV","3acda034":"from sklearn.linear_model import LogisticRegression \nclassifier_log = LogisticRegression ()\nclassifier_log.fit (X_train, Y_train)\nY_pred_log = classifier_log.predict (X_test)\nacc_log = accuracy_score (Y_test, Y_pred_log)\nparameters = [{'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\ngrid_search = GridSearchCV(estimator = classifier_log,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_log = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","121dc87a":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier ()\nclassifier_knn.fit (X_train, Y_train)\nY_pred_knn = classifier_knn.predict (X_test)\nacc_knn = accuracy_score (Y_test, Y_pred_knn)\nparameters = [{'n_neighbors': [3,5,7,10,13,15], 'weights': ['uniform', 'distance'],\n                'p': [1,2]}] \ngrid_search = GridSearchCV(estimator = classifier_knn,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_knn = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","f8566625":"from sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB ()\nclassifier_nb.fit (X_train, Y_train)\nY_pred_nb = classifier_nb.predict (X_test)\nacc_nb = accuracy_score (Y_test, Y_pred_nb)","43abe145":"from sklearn.svm import SVC\nclassifier_svm = SVC (kernel = 'rbf', random_state = 0)\nclassifier_svm.fit (X_train, Y_train)\nY_pred_svm = classifier_svm.predict (X_test)\nacc_svm = accuracy_score (Y_test, Y_pred_svm)\nparameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf'],\n                'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier_svm,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_svm = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","c6e093b9":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dtc = DecisionTreeClassifier (criterion = 'entropy', random_state = 0)\nclassifier_dtc.fit (X_train, Y_train)\nY_pred_dtc = classifier_dtc.predict (X_test)\nacc_dtc = accuracy_score (Y_test, Y_pred_dtc)\nparameters = [{'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150], \n                'max_leaf_nodes': [2,4,6,10,15,30,40,50,100], 'min_samples_split': [2, 3, 4]}]\ngrid_search = GridSearchCV(estimator = classifier_dtc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_dtc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","a1a87b83":"from sklearn.ensemble import RandomForestClassifier\nclassifier_rfc = RandomForestClassifier (n_estimators = 100, criterion = 'entropy', random_state = 1)\nclassifier_rfc.fit (X_train, Y_train)\nY_pred_rfc = classifier_rfc.predict (X_test)\nacc_rfc = accuracy_score (Y_test, Y_pred_rfc)\nparameters = [{'n_estimators': [100,200,300],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [10,25,50,'none'],\n               'min_samples_leaf': [1, 2], \n               'min_samples_split': [2, 5]}]\ngrid_search = GridSearchCV(estimator = classifier_rfc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_rfc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","516bfb75":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier (use_label_encoder = False)\nclassifier_xgb.fit (X_train, Y_train, eval_metric = \"logloss\")\nY_pred_xgb = classifier_xgb.predict (X_test)\nacc_xgb = accuracy_score (Y_test, Y_pred_xgb)\nparameters = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2],\n        'max_depth': [3, 4, 5]}\ngrid_search = GridSearchCV(estimator = classifier_xgb,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_xgb = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","ae87d728":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\", \"XGBOOST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc, acc_xgb],\n           \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\" : [best_accuracy_log, best_accuracy_knn, \"-\", best_accuracy_svm, best_accuracy_dtc, best_accuracy_rfc, best_accuracy_xgb]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","02ba25c1":"# **CONCLUSION**\n\nAfter applying EDA, VDA and numerous algorithms, finally it came down to the **K-NN Model** with the highest accuracy score of **0.850395 (85%)** after hyper-parameter tuning.","250d649a":"## **RANDOM FOREST MODEL**","86013a47":"### **CHECKING FOR DUPLICATED DATA**","939ac766":"### **SVM MODEL**","861f0031":"# **OBJECTIVES OF THE NOTEBOOK**\n\n1. Perform the EDA (Exploratory Data Analysis) followed by the VDA (Visual Data Analysis) to better understand the given dataset. \n\n2. Correctly predict (max accuracy) the probability of a person succumbing to a heart attack with the help of the various features present in the dataset.\n\n3. Apply and compare the various ML classification algorithms we have at our disposal and select the one with the highest accuracy.\n\n4. Boost the ML classification models using processing techniques to examine any substantial increase in the acuracy scores.  ","701f4cc6":"### **HEART ATTACK vs FASTING BLOOD SUGAR**","f6646fc9":"# **GENERAL DESCRIPTION OF THE DATASET**\n\nA heart attack is quite a common ailment for humans with average age of 65 (for men) and 72 (for women). Although new researches in the recent past do put the spotlight on an alarming trend - \"**a rising incidence of heart attacks in younger adults**.\"\n\nThis dataset contains information about people and their chances of succumbing to a **HEART ATTACK**.\n\nThe contents of the dataset range from personal records like age, sex, etc. to more cardiovascular related information like blood pressure levels, types of chest pain, cholestrol levels, etc. \n\n\n#### Given below is a brief explanation of the terminology used in the given dataset :-\n\n1. age : Age of the patient\n2. sex : Sex of the patient\n3. exng : exercise induced angina (1 = yes; 0 = no)\n4. ca : number of major vessels (0-3)\n5. cp : Chest Pain type <br>\n   \u2022 Value 1: typical angina <br>\n   \u2022 Value 2: atypical angina <br>\n   \u2022 Value 3: non-anginal pain <br>\n   \u2022 Value 4: asymptomatic <br>\n6. trtbps : resting blood pressure (in mm Hg)\n7. chol : cholestoral in mg\/dl fetched via BMI sensor\n8. fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n9. rest_ecg : resting electrocardiographic results <br>\n   \u2022 Value 0: normal <br>\n   \u2022 Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV) <br>\n   \u2022 Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n10. thalach : maximum heart rate achieved\n11. target : <br>\n    \u2022 0 = less chance of heart attack <br>\n    \u2022 1 = more chance of heart attack","5a5e03c2":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to fasting blood sugar level** <br>\ntype 0 - fbs<120mg\/dL <br>\ntype 1 - fbs>120mg\/dL <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n\u2022 85.9% patients prone to a heart attack have a fasting blood sugar level greater than 120 mg\/dL. <br>\n\u2022 84% patients not prone to a heart attack have a fasting blood sugar level less than 120 mg\/dL.","c86f09bd":"### **DATASET**","32d0d9fc":"### **NAIVE BAYES MODEL**","25dc270a":"### **LIBRARIES**","e3c6e30b":"### **HEART ATTACK vs AGE** ","5f847ffb":"### **PRINCIPLE COMPONENT ANALYSIS (PCA)**\n\nPCA algorithm helps in optimizing the data to its best analytical form. It removes any unneeded features which have zero or play next to negligible part in determining the accuracy of our models and reducing the dimentions of our data. It is usually helpful in datasets which have a strong correlation between features (>30%). \n\nAfter analysing the correlation heatmap of our dataset, most of the features have a correlation of less than 30% with some even showing negative correlation. Due to this, it would be redundant to apply PCA in this dataset.","206f9ec5":"### **FEATURE SCALING**","3887f54e":"Maximum number of people suffer from type 0\/typical angina (47.4%) chest pain followed by type 2\/non-anginal pain        (28.5%), type 1\/atypical angina (16,6%) chest pain and lastly type 3\/asymptomatic (7.62%). <br>","48cf823e":"### **SOME 3D PLOTS AND PAIRPLOTS**","90c6ead6":"### **CHECKING FOR MISSING DATA**","655f37f0":"### **SPLITTING DATASET INTO TRAINING AND TEST SETS**","bcebb932":"# **IMPORTING THE LIBRRIES AND DATASTET**","9d7bfdc8":"In the output of the **isna ()** function only the first 5 and the last 5 data entries are visible and we dont get any actual idea whether any data point is mising or not in the middle of the table. We will rectify that in the next code cell.","c939bd3a":"### **DECISION TREE MODEL**","afbf2a9f":"# **MODEL IMPLEMENTATIONS WITH HYPER PARAMETER TUNING**","2fd6a26b":"# **VISUAL DATA ANALYSIS (VDA)**\n\nAfter doing a theoretical analysis in the previous section, we will be moving on to visual analysis of the dataset. This will include quite a number of scatter plots, bar plots, etc. between the different features and how they affect one other and how they will affect the working algorithm. We will also be getting a general idea about which features will play a more active role while determining the accuracy of the model.\n\nFor this section we will be using a new library **plotly.express** as well.","ee3ab5a2":"### **K-NN MODEL**","ffef9ecb":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n\u2022 patients prone to a heart attack have a resting blood sugar level in the range of 120-140 mm Hg. <br>\n\u2022 patients not prone to a heart attack have a resting blood sugar distributed more or less evenly in the range of 110-150   mm Hg. ","003f153b":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to chest pain types** <br>\ntype 0 - typical angina <br>\ntype 1 - atypical angina <br>\ntype 2 - non-anginal pain <br>\ntype 3 - asymptomatic <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n\u2022 patients with higher chances of heart attack or have suffered a heart attack tend to experience non-anginal\/type 2 pain   (41.4%) the most and asymptomatic\/type 3 pain (9.7%) the least. <br>\n\u2022 75.3% of the patients with a lesser chance of suffering from a heart attack experience typical angina (type 0) pain.","8465e1db":"### **ACCURACY COMPARISON**","cb5507a9":"### **SPLITTING DATASET INTO MATRIX OF FEATURES AND RESPONSE VARIABLES**","e2daea7f":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to resting electrocardiographic results** <br>\ntype 0: normal <br>\ntype 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV) <br>\ntype 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n\u2022 heart attack prone patients predominantly have ST-T wave abnormality\/type 1 restecg result (57.9%) followed by             normal\/type 0 restecg result (41.4%). <br>\n\u2022 patients not prone to a heart attack have an opposite scenario with normal\/type 0 restecg result being the major case     (57.2%) and ST-T wave abnormality\/type 1 restecg result being secondary (40.5%). <br>\n\u2022 both the categories have a marginal number of patients who have a type 2 restecg result (0.006% and 0.02% respectively).","079e9ce4":"### **LOGISTIC REGRESSION**","27c8e2a6":"To the naked eye, all of the features that we have present in our dataset appear to contribute to the final predictions of the heart attck analysis in some form or the other. Hence, in our intial stages of EDA we wont be dropping any feature\/column. ","c2bdeed6":"### **HEART ATTACK vs RESTING ELECTROCARDIOGRAPHIC RESULTS**","6e1cbc27":"The **isna (). sum ()** function gives the sum of all the missing or uncorrectly entered data points for each and every column in the dataset. Since, none of the features in our dataset is capable of having negative values it is safe to state that the dataset doestn't contain any missing values. ","fc063e71":"# **DATA PREPROCESSING**\n\n1. Splitting dataset into matrix of features and response variables. <br>\n2. Splitting dataset into training and test sets. <br>\n3. Feature scaling.","d15c23c9":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n\u2022 maximum number of patients prone to a heart attack have cholestrol level in the range of 200-250 mg\/dL. <br>\n\u2022 maximum cholestrol level reached for a heart attack prone victim was 564 mg\/dL. <br>\n\u2022 patients not prone to a heart attack have a cholestrol level distributed evenly in the range of 200-300 mg\/dL. ","7867d874":"It can be seen from the heatmap that we don't have much strong correlation neither between any two features nor between a feature and the response variable (output column). Therefore, it won't be advantageous for us to make choice of features on the basis of such low correlation scores.\n\nFor this dataset we will make use of the medical knowledge available pertaining to heart attacks and what are the main factors that influence it.\n\nFor example, from our features and consulting the internet it becomes fairly obvious that features like age, cp, trtbps, chol, etc will be accounted for.\n\nThe list of relevant features is as follows :- <br>\n\u2022 age <br>\n\u2022 cp <br>\n\u2022 trtbps <br>\n\u2022 chol <br>\n\u2022 fbs <br>\n\u2022 rest_ecg <br>\n\u2022 thalachh","c14bf5f6":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n\u206b\u2022 people in the age bracket of 50-60 (65 cases) have the maximum tendency to have a heart attack closely followed by the     age group of 40-50 (50 cases). <br>\n\u206b\u2022 10 cases in the age range of 30-40 is an alarming figure because people in this age group weren't known to suffer from     heart attacks. <br>\n\u206b\u2022 the age group of 50-60 (60 cases) also leads the chart in the maximum number of people not succumbing to heart attacks     followed by people in the range of 60-70 (48 cases).","27fe8b4b":"### **HEART ATTACK vs CHOLESTROL**","283b8622":"**1 - fbs > 120mg\/dL** <br>\n**0 - fbs < 120mg\/dL** <br>\n\nMaximum people have a fasting blood sugar lower than 120mg\/dL","b790ec36":"### **HEART ATTACK vs CHEST PAIN**","5b43ef7d":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n\u2022 patients with the maximum heart rate in the range of 160-180 are more prone to a heart attack. <br>\n\u2022 at the same time patients with the maximum heart rate in the range of 140-160 are less prone to suffer a heart attack.","1ef10b58":"The percentage of patients with a restecg of type 2 (1.32%) is more or less negligible in comparison to types 0 and 1 (48.7% and 50% respectively). ","560af10a":"### **HEART ATTACK vs RESTING BLOOD SUGAR**","0e11d3d9":"### **HEART ATTACK vs MAXIMUM HEART RATE**","3ddd3aa4":"### **XGBOOST MODEL**","f4388fbe":"### **CHECKING THE NUMBER OF UNIQUE VALUES**\n","be86ee2d":"# **EXPLORATORY DATA ANALYSIS (EDA)**\n\nIn this section, we will be performing some basic operations on the dataset in order to break down our data to the rudimentary level. For example, checking for missing values, what are the input types of the features, dropping unvalued features etc. to name a few.","c9fc1abf":"**DataFrame.describe()** method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\n\u2022 count tells us the number of NoN-empty rows in a feature. <br>\n\u2022 mean tells us the mean value of that feature. <br>\n\u2022 std tells us the Standard Deviation Value of that feature. <br>\n\u2022 min tells us the minimum value of that feature. <br>\n\u2022 25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers. <br>\n\u2022 max tells us the maximum value of that feature.","207571e6":"A duplicate row has been found and will have to be removed to establish chances of best results.","7b08da78":"### **CORRELATION MATRIX AND HEATMAP**"}}