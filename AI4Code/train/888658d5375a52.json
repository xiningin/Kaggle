{"cell_type":{"4e2b43e1":"code","b06c1378":"code","0f30e27d":"code","e7387574":"code","f803375a":"code","dc588ff7":"code","5fb8c6d1":"code","59a23b50":"code","5abac9b6":"code","54ddec7d":"code","151e6c10":"code","f610a16f":"code","8f691652":"code","57a6c633":"code","61d8cab4":"code","aeceb496":"code","5c2a63a6":"code","58be00c6":"code","7ac2d3c3":"code","2410b8bf":"code","9798574b":"markdown"},"source":{"4e2b43e1":"import pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b06c1378":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\n\ndf.head(2)","0f30e27d":"df.isnull().sum()","e7387574":"df = df.drop(['Unnamed: 32', 'id'], axis=1)\n\ndf.head()","f803375a":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\n\nfeatures_to_convert = [\"diagnosis\"]\n\nfor i in features_to_convert:\n    df[i] = enc.fit_transform(df[i].astype('str'))\n\ndf.head(5)\n\n#1 Mal\n#0 Ben","dc588ff7":"df['target'] = df['diagnosis']\n\ndf = df.drop(['diagnosis'], axis=1)\n\ndf.head(2)","5fb8c6d1":"df.shape","59a23b50":"X = df.iloc[:,0:30]  #independent columns\ny = df.iloc[:,30]    #target column ","5abac9b6":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top k best features\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(5,'Score'))  #print k best features","54ddec7d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n#apply SelectKBest class to extract top k best features\nbestfeatures = SelectKBest(score_func=f_classif, k=5)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(5,'Score'))  #print k best features","151e6c10":"dfnew = pd.DataFrame(df[['area_worst', 'area_mean', 'area_se', 'perimeter_worst', 'perimeter_mean',\n                        'concave points_worst', 'concave points_mean', 'radius_worst']])\ndfnew.head()","f610a16f":"from sklearn import preprocessing\n\n# Get column names first\nnames = dfnew.columns\n\n# Create the Scaler object\nscaler = preprocessing.MinMaxScaler()\n\n# Fit your data on the scaler object\nscaled_df = scaler.fit_transform(dfnew)\nscaled_df = pd.DataFrame(scaled_df, columns=names)\n\nscaled_df","8f691652":"scaled_df['target'] = df['target']\n\nscaled_df.head(2)","57a6c633":"X = scaled_df.drop(\"target\", axis=1)\nY = scaled_df[\"target\"]","61d8cab4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=101)\n\nprint(X_train.shape, X_test.shape)","aeceb496":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(max_depth=2)\nmodel.fit(X_train, Y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\na = accuracy_score(Y_test, y_predict)\nprint(\"Accuracy: \", a.round(4))\n\nfrom sklearn.model_selection import cross_val_score\nimport statistics\n\nkfoldscore = cross_val_score(model, X_train, Y_train, cv=5)\n\nprint(\"kFold Scores: {}\".format(kfoldscore.round(4)))\nprint(\"kFold Score Mean: \", statistics.mean(kfoldscore).round(4))","5c2a63a6":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=10)\nmodel.fit(X_train, Y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\na = accuracy_score(Y_test, y_predict)\nprint(\"Accuracy: \", a.round(4))\n\nfrom sklearn.model_selection import cross_val_score\nimport statistics\n\nkfoldscore = cross_val_score(model, X_train, Y_train, cv=5)\n\nprint(\"kFold Scores: {}\".format(kfoldscore.round(4)))\nprint(\"kFold Score Mean: \", statistics.mean(kfoldscore).round(4))","58be00c6":"from sklearn.svm import SVC\n\nmodel = SVC(gamma='auto')\nmodel.fit(X_train, Y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\na = accuracy_score(Y_test, y_predict)\nprint(\"Accuracy: \", a.round(4))\n\nfrom sklearn.model_selection import cross_val_score\nimport statistics\n\nkfoldscore = cross_val_score(model, X_train, Y_train, cv=5)\n\nprint(\"kFold Scores: {}\".format(kfoldscore.round(4)))\nprint(\"kFold Score Mean: \", statistics.mean(kfoldscore).round(4))","7ac2d3c3":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=6)\nmodel.fit(X_train, Y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\na = accuracy_score(Y_test, y_predict)\nprint(\"Accuracy: \", a.round(4))\n\nfrom sklearn.model_selection import cross_val_score\nimport statistics\n\nkfoldscore = cross_val_score(model, X_train, Y_train, cv=5)\n\nprint(\"kFold Scores: {}\".format(kfoldscore.round(4)))\nprint(\"kFold Score Mean: \", statistics.mean(kfoldscore).round(4))","2410b8bf":"from sklearn.neural_network import MLPClassifier\n\nmodel = MLPClassifier(max_iter=100, solver='lbfgs', learning_rate='adaptive')\nmodel.fit(X_train, Y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\na = accuracy_score(Y_test, y_predict)\nprint(\"Accuracy: \", a.round(4))\n\nfrom sklearn.model_selection import cross_val_score\nimport statistics\n\nkfoldscore = cross_val_score(model, X_train, Y_train, cv=5)\n\nprint(\"kFold Scores: {}\".format(kfoldscore.round(4)))\nprint(\"kFold Score Mean: \", statistics.mean(kfoldscore).round(4))","9798574b":"This tutorial covers:\n* Feature Selection (*Reducing unrelevant labels and size of your dataset*)\n* Scaling Data\n* Basic Pandas dataframe manipulation\n* kFold cross validation techniques\n* Implementing DecisionTreeClassifier, RandomForestClassifier, SVC, KNeighborsClassifier, and MLPClassifier. \n* Evaluating scores, determining best algorithm\n"}}