{"cell_type":{"56bd3523":"code","6a01b655":"code","95bd3287":"code","889ee2be":"code","77c719ff":"code","cc1d202c":"code","cfe6a772":"code","fc3ff4c7":"code","3c8881bc":"code","190183cb":"code","ff362cc4":"code","0068fc3a":"code","855342cc":"code","5a0e3b63":"code","854bd2ab":"code","12ff0796":"code","d7040577":"code","e0db70fb":"markdown","13ffcfe1":"markdown","78d29535":"markdown","a579e0cd":"markdown","3f36c4ad":"markdown","4699151d":"markdown","f7c6109a":"markdown"},"source":{"56bd3523":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Load data\ndf = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head()","6a01b655":"# Are there any null values?\ndf.isna().any()","95bd3287":"# Convert gender to 1 and 0\ndf['Gender'].replace({'Male':0,'Female':1},inplace=True)\ndf.drop(['CustomerID'],axis=1,inplace=True) # We dont need Customer ID\ndf.head()","889ee2be":"df.shape # How many data points?","77c719ff":"# Any features strongly correlated?\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nfor col in df.columns:\n    X = df.drop([col], axis=1)\n    y = df[col]\n    X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n    reg = DecisionTreeRegressor()\n    reg.fit(X_train,y_train)\n    print('Score for {} as dependent variable is {}'.format(col,reg.score(X_test,y_test)))","cc1d202c":"# Visualize the feature correlation using scatter plot\n# Are there any outliers?\npd.scatter_matrix(df, figsize=(16,9), diagonal ='kde')","cfe6a772":"# Visualize the feature correlation using heatmap\nimport seaborn as sns\nsns.heatmap(df.corr(),xticklabels=df.columns,yticklabels=df.columns)","fc3ff4c7":"# Find the principal components!\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(df)\npca.explained_variance_ratio_","3c8881bc":"pca.components_","190183cb":"dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\ncomponents = pd.DataFrame(pca.components_,columns=df.columns)\ncomponents.index = dimensions\n\nvariance = pd.DataFrame(pca.explained_variance_ratio_, columns=['Explained Variance'])\nvariance.index = dimensions\n\npd.concat([variance,components], axis=1)","ff362cc4":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(16,9))\ncomponents.plot(kind='bar', ax=ax)\nax.set_xticklabels(dimensions)\nfor i,variance in enumerate(pca.explained_variance_ratio_):\n    ax.text(i,ax.get_ylim()[1]+0.05,'Explained variance {}'.format(np.round(variance,4)))\nplt.show()","0068fc3a":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(df)\npca.explained_variance_ratio_","855342cc":"transformed_data = pca.transform(df)\ntransformed_data = pd.DataFrame(transformed_data,columns=['Dimension 1','Dimension 2'])\ntransformed_data.head()","5a0e3b63":"# Use silhouette score to find the ideal number of clusters.\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nno_of_clusters= range(2,11)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [silhouette_score(transformed_data,kmeans[i].fit(transformed_data).predict(transformed_data),metric='euclidean') for i in range(len(kmeans))]\nplt.plot(no_of_clusters,score)\nplt.xlabel('No of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()","854bd2ab":"kmeans = KMeans(n_clusters=5)\nkmeans.fit(transformed_data)\nkmeans.predict(transformed_data)\n\nplt.scatter(transformed_data.iloc[:,0],transformed_data.iloc[:,1],c=kmeans.labels_,cmap='rainbow')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color='black')\nplt.show()","12ff0796":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=5)\ngmm.fit(transformed_data)\nlabels = gmm.predict(transformed_data)\n\nplt.scatter(transformed_data.iloc[:,0],transformed_data.iloc[:,1],c=labels,cmap='rainbow')\nplt.show()","d7040577":"cluster_proba_df = pd.DataFrame(gmm.predict_proba(transformed_data), columns = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'])\ncluster_proba_df['Belongs to'] = cluster_proba_df.idxmax(axis=1)\ncluster_proba_df.head()","e0db70fb":"In this kernel, we are going to first find the principle components and then use them for clustering. Let's get started ! :) ","13ffcfe1":"**K - Means Clustering**","78d29535":"As the variance explained by the first two principal components is ~ 0.9(0.45+0.44) we can say its good to use only the first two principal components. ","a579e0cd":"This is a soft clustering method. So, each data point is associated with a probability value for each of the clusters. We can say the cluster with the highest probability owns the data point ! :)  ","3f36c4ad":"**Gaussian Mixture**","4699151d":"The score starts decreasing after 5. So, we are going to use 5 clusters.","f7c6109a":"**What is the ideal number of clusters?**"}}