{"cell_type":{"9fad0b63":"code","5a370634":"code","fc469c0d":"code","b503760a":"code","8c1662ea":"code","24f6a9c6":"code","415fe0c4":"code","cb011817":"code","c2015a49":"code","ac28d081":"code","8a4633ad":"code","47ef3db8":"code","72785f4c":"code","529f7220":"code","eb9bad7b":"code","5c06c907":"code","0a9094e3":"code","4ed48c7e":"code","faf404c3":"code","e32e66ac":"code","fd42e8e2":"code","02960570":"code","2a3f280e":"code","edc94609":"code","76d97aee":"markdown","8eaa43e0":"markdown","aed5f73d":"markdown","075221a4":"markdown","fdd5af54":"markdown","8c39ee3f":"markdown","8e9eff24":"markdown","ae876961":"markdown","147306e2":"markdown","00a3debc":"markdown","9351add6":"markdown","46408a48":"markdown","82ea76d5":"markdown","d7889d2d":"markdown","893beb07":"markdown","477314ea":"markdown"},"source":{"9fad0b63":"import os\nimport zipfile\nimport datetime\nfrom collections import Counter\n\nimport sklearn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow import keras\nfrom tensorflow.keras import models,layers\nfrom tensorflow.keras.utils import plot_model\n\n\nimport matplotlib.pyplot as plt\nimport PIL.Image\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","5a370634":"img_path = '\/kaggle\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/'\ncovid_imgs = os.listdir(img_path+'\/COVID')\nnormal_imgs = os.listdir(img_path+'\/Normal')\npneumonia_imgs = os.listdir(img_path + '\/Viral Pneumonia')\n\nprint(\"Normal chest x-ray\")\nim = PIL.Image.open(os.path.join(img_path,'Normal',normal_imgs[0]))\nplt.imshow(im,cmap=\"gray\")\nplt.show()\n\nprint(\"Covid chest x-ray\")\nim = PIL.Image.open(os.path.join(img_path,'COVID',covid_imgs[0]))\nplt.imshow(im,cmap=\"gray\")\nplt.show()\n\nprint(\"Viral Pneumonia chest x-ray\")\nim = PIL.Image.open(os.path.join(img_path,'Viral Pneumonia',pneumonia_imgs[0]))\nplt.imshow(im,cmap=\"gray\")\nplt.show()\n","fc469c0d":"BATCH_SIZE = 20\nIMAGE_SIZE = 224\ninput_shape = (IMAGE_SIZE,IMAGE_SIZE,3)","b503760a":"list_files = []\nfor dir in os.listdir(img_path):\n  aux_dir = os.path.join(img_path,dir)\n  \n  if os.path.isdir(aux_dir):\n    for img in os.listdir(aux_dir):\n      list_files.append([os.path.join(dir,img),dir])\ndf = pd.DataFrame(list_files, columns=['id','label'])\ndf.describe()","8c1662ea":"def frequency_plot(df):\n  freq_abs = Counter(df.label);\n  freq_a = pd.DataFrame.from_dict(freq_abs, orient='index').reset_index()\n  total = len(df.index)\n  freq_r = freq_a[0]\/total\n  freq_a[1] = freq_r\n  freq_a.columns = ['Label','Frequ\u00eancia absoluta','Frequ\u00eancia relativa']\n  return freq_a\nfrequency_plot(df)","24f6a9c6":"def balance_dataset(focus_class, df):\n  classes = df.label.unique()\n  classes_df_list = []\n  focus_class_df = df[df.label == focus_class]\n  classes_df_list.append(focus_class_df)\n  count_focus_class = len(focus_class_df)\n  ##Divide by class\n  for classe in classes:\n    class_df = df[df.label == classe]\n    count_class = len(class_df)\n    if classe == focus_class:\n      continue\n    if count_class > count_focus_class+1000:\n      class_df = class_df.sample(count_focus_class+1000)\n    classes_df_list.append(class_df)\n  df_under = pd.concat(classes_df_list, axis=0) \n  return df_under\n\n\ndf = balance_dataset('COVID',df)\n\nfrequency_plot(df)","415fe0c4":"def split_dataset(df,test_size,random_state = 101):\n  sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n  df = df.reset_index()\n  for train_index, test_index in sss.split(df['id'], df['label']):\n    X_train, X_test = df['id'][train_index], df['id'][test_index]\n    y_train, y_test = df['label'][train_index], df['label'][test_index]\n  traindf = pd.concat([X_train, y_train], axis=1)\n  testdf = pd.concat([X_test, y_test], axis=1)\n  return traindf,testdf\n\n#divide train\/val\/test in 80\/10\/10\ntraindf,testdf = split_dataset(df,test_size = 0.25)\nvaldf,testdf= split_dataset(testdf,test_size = 0.50)\n# traindf = balance_dataset('COVID',df)\n\n\ntraindf.id = img_path + traindf.id\nvaldf.id = img_path + valdf.id\ntestdf.id = img_path + testdf.id","cb011817":"frequency_plot(traindf)","c2015a49":"frequency_plot(valdf)","ac28d081":"frequency_plot(testdf)","8a4633ad":"BATCH_SIZE = 20\nIMAGE_SIZE = 224\ninput_shape = (IMAGE_SIZE,IMAGE_SIZE,3)","47ef3db8":"BATCH_SIZE=50\n\n\"\"\"\ntrain_datagen = ImageDataGenerator(          \n    rotation_range = 40,\n    width_shift_range = 0.2,\n  height_shift_range=0.2,\n  shear_range = 0.2,\n  zoom_range = 0.1,\n  fill_mode = 'nearest',\n)\n\"\"\"\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\nprint(\"Creating train generator...\")\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=traindf,\n    directory=\"COVID-19_Radiography_Dataset\",\n    x_col=\"id\",\n    y_col=\"label\",\n    batch_size=BATCH_SIZE,\n    color_mode=\"rgb\",\n    seed=5,\n    shuffle=True,\n    class_mode=\"categorical\",\n    target_size=(IMAGE_SIZE,IMAGE_SIZE)\n)\n\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=testdf,\n    directory=\"COVID-19_Radiography_Dataset\",\n    x_col=\"id\",\n    y_col=\"label\",\n    color_mode=\"rgb\",\n    seed=5,\n    shuffle = False,\n    batch_size=30,\n    class_mode=\"categorical\",\n    target_size=(IMAGE_SIZE,IMAGE_SIZE)\n)\n\nprint(\"\\nCreating val generator...\")\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=valdf,\n    directory=\"COVID-19_Radiography_Dataset\",\n    x_col=\"id\",\n    y_col=\"label\",\n    batch_size=5,\n    color_mode=\"rgb\",\n    seed=5,\n    shuffle=True,\n    class_mode=\"categorical\",\n    target_size=(IMAGE_SIZE,IMAGE_SIZE)\n)\n\n\n","72785f4c":"from keras.applications.vgg16 import VGG16\nbase_convnet = VGG16(include_top=False, input_shape=input_shape,\n                     weights='imagenet')\n\n\n#Freeze base convolutional layers\nbase_convnet.trainable = False\nbase_convnet.summary()","529f7220":"model = models.Sequential()\nmodel.add(base_convnet)\n\n\"\"\"\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(256, 3,padding='same',activation='relu'))\nmodel.add(layers.Dropout(0.30))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(256, 3,padding='same',activation='relu'))\nmodel.add(layers.Dropout(0.30))\n\"\"\"\n\n# model.add(layers.Conv2D(64, 3, activation='relu'))\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(1024, activation='relu'))\nmodel.add(layers.Dropout(0.30))\nmodel.add(layers.Dense(1024, activation='relu'))\nmodel.add(layers.Dropout(0.30))\nmodel.add(layers.Dense(4, activation='softmax',name=\"fc_out\"))\n\nmodel.summary()\n\n\n","eb9bad7b":"plot_model(model, to_file='model.png', show_shapes=True)\nImage(filename='model.png')","5c06c907":"checkpoints_path = '\/kaggle\/output\/models_checkpoints\/covid_project'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoints_path,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n#to avoid overfitting\nearly = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=7)\nmcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')","0a9094e3":"EPOCHS = 50\nLEARNING_RATE = 1e-5\nDECAY= LEARNING_RATE\/EPOCHS\nmodel.compile(\n    loss='categorical_crossentropy',\n    # optimizer = tf.keras.optimizers.RMSprop(lr=2e-5),\n     optimizer = tf.keras.optimizers.Nadam(lr=LEARNING_RATE),\n    metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),\n             'acc']\n)\n\ncallbacks = [model_checkpoint_callback,early,mcp_save]\nhistory1 = model.fit_generator(\n    train_generator,\n    steps_per_epoch = BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    validation_data = val_generator,\n    validation_steps=BATCH_SIZE,\n    verbose=1\n)","4ed48c7e":"callbacks = [model_checkpoint_callback,early,mcp_save]\n","faf404c3":"base_convnet.trainable= True\nEPOCHS = 50\n\nhistory2 = model.fit_generator(\n    train_generator,\n    steps_per_epoch = BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    validation_data = val_generator,\n    validation_steps=BATCH_SIZE,\n    verbose=1\n)","e32e66ac":"vgg16_model = model","fd42e8e2":"vgg16_model.evaluate(test_generator)","02960570":"from sklearn.metrics import classification_report\n\ntest_pred_raw = vgg16_model.predict(test_generator)\ntest_labels = np.asarray(test_generator.classes)\ntest_pred = np.argmax(test_pred_raw, axis=1)\n\nclass_names = list(test_generator.class_indices.keys())\nprint(classification_report(test_labels, test_pred))\nprint(test_generator.class_indices)\n","2a3f280e":"import matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \n\n\n    accuracy = np.trace(cm) \/ np.sum(cm).astype('float')\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n      cm = cm \/ np.expand_dims(cm.sum(axis=1),axis=1)\n        # cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n      pass\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","edc94609":"cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\nplot_confusion_matrix(cm,class_names)","76d97aee":"## Fine-tuning\n\n<p> Ser\u00e1 realizado um fine tuning no modelo, descongelando as camadas do vgg16 e treinando <\\p>\n    \n**English**\n    \n<p> The model will be fine tuned, unfreezing the vgg16 layers and training <\\p>","8eaa43e0":"<p>Project name: COVID-19 classifier by means of radiographic images<p\/>\n<p>Description: identifying people infected with COVID-19 by means of radiographic images using the VGG16 deep learning model.<p\/>\n<p>Author: Matheus Henrique<p\/>\n","aed5f73d":"## Importing VGG16 model","075221a4":"## Image Data Generator","fdd5af54":"# Evaluate Model","8c39ee3f":"# Classifier Model","8e9eff24":"## Data visualization","ae876961":"### confusion matrix code","147306e2":"## Image pre-processing and train-test split","00a3debc":"## Model visualization","9351add6":"# Dataset","46408a48":"## Training the classification model","82ea76d5":"## Addition of fully connected layers","d7889d2d":"### Confusion Matrix","893beb07":"## Undersampling classes\n\nO conjunto de dados est\u00e1 desequilibrado. Como nosso foco \u00e9 a classe COVID, iremos subamostrar as classes com mais exemplos do que a classe COVID e manter as classes com menos exemplos.\n\n**English**\n\nThe dataset is unbalanced. As our focus is the COVID class, we will undersample classes with more examples than the COVID class and keep classes with less examples.","477314ea":"# Conclus\u00e3o\n<p>O que podemos concluir ao an\u00e1lisar os resultados obtidos \u00e9 que com o simples uso de transfer learning de um modelo treinado em um conjunto de dados(imagenet) totalmente diferente desse problema pode alcan\u00e7ar resultados satisfat\u00f3rios ao treinar apenas a camada fully connected, alcan\u00e7ando uma acur\u00e1cia m\u00e9dia de 0.9242. Ap\u00f3s descongelarmos, e realizarmos um fine-tuning, conseguimos aumentar em apr\u00f3ximadamente 2% a acur\u00e1cia. <p\\>\n    \n<p>Ao analizarmos a precis\u00e3o e o recall da classe COVID-19 no conjunto de testes, podemos observar que na maior parte dos casos que \u00e9 dito que \u00e9 COVID-19, realmente \u00e9, pois h\u00e1 0.97 de recall<\\p>\n    \n**English**\n    \n<p>What we can conclude when analyzing the results obtained is that with the simple use of transfer learning of a model trained in a dataset (imagenet) totally different from this problem, it can achieve satisfactory results when training only the fully connected layer, reaching an average accuracy of 0.9242. After thawing and performing a fine-tuning, we were able to increase the accuracy by approximately 2%. <p\\>\n    \n<p>By analyzing the accuracy and recall of the COVID-19 class in the test set, we can see that in most cases it is said to be COVID-19, it really is, as there is 0.97 recall<\\p>\n\n"}}