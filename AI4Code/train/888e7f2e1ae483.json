{"cell_type":{"1166b4e8":"code","35d053e7":"code","0b13f898":"code","19c2d4ed":"code","380e503b":"code","73b2281e":"code","0377ede7":"code","9c7186bb":"code","923a6572":"code","8f99fc77":"code","ae08c21d":"code","0376e57d":"code","51c6be4b":"code","dedaa4bf":"code","deaddc4e":"code","06c42f6b":"code","88872179":"code","390bcd38":"code","6c510f3f":"code","6f53c5ed":"code","1e20c710":"code","188de810":"code","2a9022a5":"code","1ae0cfbb":"code","2f5158a2":"code","7a124048":"code","d8f9b657":"code","13818d5c":"code","f366ad7e":"code","28b0073e":"code","2b9219c0":"code","4bc45a8a":"code","141acd81":"code","1ebb33e4":"code","f2b7293a":"code","8420e4a9":"code","c88c0322":"code","230f4913":"code","02a6e452":"code","22a71b78":"code","9ae6ab31":"code","b2d588cd":"code","dc65be3a":"code","6a388904":"code","14a2acd8":"code","8beb4de2":"code","b5ec4090":"code","b8dbed84":"code","f4f8ff8d":"code","a340f802":"code","31ad97ba":"code","2cbb65dc":"code","3530871e":"code","f938d59f":"code","bd3d414e":"code","e8bc5943":"code","3c3d2510":"code","b5db4e99":"code","d3fcec7b":"code","9d440228":"code","b7e80a9c":"code","335cb459":"code","b9abcd01":"code","95830af5":"code","e26000fc":"code","5b1515f8":"code","e70ec8d6":"code","8bb0fcff":"code","40dc96df":"code","09f126bd":"code","baf0d4e6":"code","c804c914":"code","e63357eb":"code","587dbb3c":"code","b4148e4e":"code","fc7c3360":"code","99707cb1":"code","8442660a":"code","c1d6fcc5":"code","10dc07f0":"code","95474c2a":"code","833e3f2f":"code","042435d1":"code","13e5b51d":"code","8dd6d011":"code","60a38578":"code","0ad2e492":"code","1ffbf726":"markdown","d80bad5c":"markdown","f6da691e":"markdown","a38c2744":"markdown","b951c395":"markdown","8523bfa1":"markdown","5fc6fc8f":"markdown","4ac77807":"markdown","af9aab2e":"markdown","27c36353":"markdown","ffa726b6":"markdown","69f9a169":"markdown","bf275461":"markdown","86a49561":"markdown","9239d942":"markdown","479f140a":"markdown","d70eefaf":"markdown","460157fd":"markdown","6c1439c2":"markdown","04cb286d":"markdown","b5bcc8bf":"markdown","550ec1c4":"markdown","908437f7":"markdown","08ab9788":"markdown","d5062281":"markdown","f50eefce":"markdown","d42c8f7c":"markdown","64550849":"markdown","6de4f4f4":"markdown","cf4d22aa":"markdown","417de121":"markdown","5c0049ae":"markdown","ea1b8329":"markdown","144c90d2":"markdown","20a3f2cf":"markdown","cd94ed17":"markdown","94114ee3":"markdown","eb905e54":"markdown","58b995e9":"markdown","4f54e1a3":"markdown"},"source":{"1166b4e8":"import os\nprint(os.listdir(\"..\/input\"))","35d053e7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import rcParams\nimport xgboost as xgb\n%matplotlib inline \nsns.set_style('whitegrid')","0b13f898":"import scipy.stats as stats\nfrom scipy import stats\nfrom scipy.stats import pointbiserialr, spearmanr, skew, pearsonr","19c2d4ed":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.linear_model import Ridge, RidgeCV, LassoCV\nfrom sklearn import linear_model","380e503b":"house_train = pd.read_csv(\"..\/input\/train.csv\")\nhouse_test = pd.read_csv(\"..\/input\/test.csv\")","73b2281e":"house_train.shape, house_test.shape","0377ede7":"house_train.head()","9c7186bb":"house_test.head()","923a6572":"# \"Descriptive Statistics\": Summary of Target Variable\nhouse_train['SalePrice'].describe()","8f99fc77":"# Let's plot histogram to check data is normally distributed or not?\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.distplot(house_train['SalePrice'])","ae08c21d":"#skewness and kurtosis\nprint(\"Skewness: %f\" % house_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % house_train['SalePrice'].kurt())","0376e57d":"house_train.info()","51c6be4b":"house_train.describe(include='all')","dedaa4bf":"#correlation matrix\nc_mat = house_train.corr()\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(c_mat, square=True)","deaddc4e":"# Highly Correlated Features or Variables\nc_mat = house_train.corr()\ntop_corr_features = c_mat.index[abs(c_mat[\"SalePrice\"])>0.4]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(house_train[top_corr_features].corr(),annot=True)","06c42f6b":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(house_train[cols], size = 2.5)\nplt.show()","88872179":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([house_train['SalePrice'], house_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","390bcd38":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([house_train['SalePrice'], house_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","6c510f3f":"#Box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([house_train['SalePrice'], house_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","6f53c5ed":"var = 'YearBuilt'\ndata = pd.concat([house_train['SalePrice'], house_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90)","1e20c710":"house_train[['OverallQual','SalePrice']].groupby(['OverallQual'],\nas_index=False).mean().sort_values(by='OverallQual', ascending=False)","188de810":"house_train[['GarageCars','SalePrice']].groupby(['GarageCars'],\nas_index=False).mean().sort_values(by='GarageCars', ascending=False)","2a9022a5":"house_train[['Fireplaces','SalePrice']].groupby(['Fireplaces'],\nas_index=False).mean().sort_values(by='Fireplaces', ascending=False)","1ae0cfbb":"house_train.isnull().sum().sort_values(ascending=False).head(20)","2f5158a2":"#plot of missing value features\nplt.figure(figsize=(12, 8))\nsns.heatmap(house_train.isnull())\nplt.show()","7a124048":"house_test.isnull().sum().sort_values(ascending=False).head(20)","d8f9b657":"#plot of missing value features\nplt.figure(figsize=(12, 8))\nsns.heatmap(house_test.isnull())\nplt.show()","13818d5c":"total = house_train.isnull().sum().sort_values(ascending=False)\npercent = (house_train.isnull().sum()\/house_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","f366ad7e":"total = house_test.isnull().sum().sort_values(ascending=False)\npercent = (house_test.isnull().sum()\/house_test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","28b0073e":"#Create a list of column to fill NA with \"None\" or 0.\nto_null = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n           'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt', 'BsmtFullBath', 'BsmtHalfBath',\n           'PoolQC', 'Fence', 'MiscFeature']\nfor col in to_null:\n    if house_train[col].dtype == 'object':\n\n        house_train[col].fillna('None',inplace=True)\n        house_test[col].fillna('None',inplace=True)\n    else:\n\n        house_train[col].fillna(0,inplace=True)\n        house_test[col].fillna(0,inplace=True)","2b9219c0":"#Fill NA with common values.\nhouse_test.loc[house_test.KitchenQual.isnull(), 'KitchenQual'] = 'TA'\nhouse_test.loc[house_test.MSZoning.isnull(), 'MSZoning'] = 'RL'\nhouse_test.loc[house_test.Utilities.isnull(), 'Utilities'] = 'AllPub'\nhouse_test.loc[house_test.Exterior1st.isnull(), 'Exterior1st'] = 'VinylSd'\nhouse_test.loc[house_test.Exterior2nd.isnull(), 'Exterior2nd'] = 'VinylSd'\nhouse_test.loc[house_test.Functional.isnull(), 'Functional'] = 'Typ'\nhouse_test.loc[house_test.SaleType.isnull(), 'SaleType'] = 'WD'\nhouse_train.loc[house_train['Electrical'].isnull(), 'Electrical'] = 'SBrkr'\nhouse_train.loc[house_train['LotFrontage'].isnull(), 'LotFrontage'] = house_train['LotFrontage'].mean()\nhouse_test.loc[house_test['LotFrontage'].isnull(), 'LotFrontage'] = house_test['LotFrontage'].mean()","4bc45a8a":"house_train.loc[house_train.MasVnrType == 'None', 'MasVnrArea'] = 0\nhouse_test.loc[house_test.MasVnrType == 'None', 'MasVnrArea'] = 0\nhouse_test.loc[house_test.BsmtFinType1=='None', 'BsmtFinSF1'] = 0\nhouse_test.loc[house_test.BsmtFinType2=='None', 'BsmtFinSF2'] = 0\nhouse_test.loc[house_test.BsmtQual=='None', 'BsmtUnfSF'] = 0\nhouse_test.loc[house_test.BsmtQual=='None', 'TotalBsmtSF'] = 0","141acd81":"#Let's check again is there any missing values present in data or not\nhouse_train.columns[house_train.isnull().any()]\nplt.figure(figsize=(10, 5))\nsns.heatmap(house_train.isnull())","1ebb33e4":"house_test.loc[house_test.GarageCars.isnull(), 'GarageCars'] = 0\nhouse_test.loc[house_test.GarageArea.isnull(), 'GarageArea'] = 0","f2b7293a":"#Let's check again is there any missing values present in data or not\nhouse_test.columns[house_test.isnull().any()]\nplt.figure(figsize=(10, 5))\nsns.heatmap(house_test.isnull())","8420e4a9":"total = house_test.isnull().sum().sort_values(ascending=False)\npercent = (house_test.isnull().sum()\/house_test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)","c88c0322":"corr = house_train.corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, vmax=1)","230f4913":"threshold = 0.8 # Threshold value.\ndef correlation():\n    for i in house_train.columns:\n        for j in house_train.columns[list(house_train.columns).index(i) + 1:]: #Ugly, but works. This way there won't be repetitions.\n            if house_train[i].dtype != 'object' and house_train[j].dtype != 'object':\n                #pearson is used by default for numerical.\n                if abs(pearsonr(house_train[i], house_train[j])[0]) >= threshold:\n                    yield (pearsonr(house_train[i], house_train[j])[0], i, j)\n            else:\n                #spearman works for categorical.\n                if abs(spearmanr(house_train[i], house_train[j])[0]) >= threshold:\n                    yield (spearmanr(house_train[i], house_train[j])[0], i, j)","02a6e452":"corr_list = list(correlation())\ncorr_list","22a71b78":"#It seems that SalePrice is skewered, so it needs to be transformed.\nsns.distplot(house_train['SalePrice'], kde=False, color='c', hist_kws={'alpha': 0.9})","9ae6ab31":"#As expected price rises with the quality.\nsns.regplot(x='OverallQual', y='SalePrice', data=house_train, color='Orange')","b2d588cd":"#Price also varies depending on neighborhood.\nplt.figure(figsize = (12, 6))\nsns.boxplot(x='Neighborhood', y='SalePrice',  data=house_train)\nxt = plt.xticks(rotation=30)","dc65be3a":"#There are many little houses.\nplt.figure(figsize = (12, 6))\nsns.countplot(x='HouseStyle', data=house_train)\nxt = plt.xticks(rotation=30)","6a388904":"#And most of the houses are single-family, so it isn't surprising that most of the them aren't large.\nsns.countplot(x='BldgType', data=house_train)\nxt = plt.xticks(rotation=30)","14a2acd8":"#Most of fireplaces are of good or average quality. And nearly half of houses don't have fireplaces at all.\npd.crosstab(house_train.Fireplaces, house_train.FireplaceQu)","8beb4de2":"sns.factorplot('HeatingQC', 'SalePrice', hue='CentralAir', data=house_train)\nsns.factorplot('Heating', 'SalePrice', hue='CentralAir', data=house_train)","b5ec4090":"#One more interesting point is that while pavement road access is valued more, for alley they quality isn't that important.\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nsns.boxplot(x='Street', y='SalePrice', data=house_train, ax=ax[0])\nsns.boxplot(x='Alley', y='SalePrice', data=house_train, ax=ax[1])","b8dbed84":"#We can say that while quality is normally distributed, overall condition of houses is mainly average.\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nsns.countplot(x='OverallCond', data=house_train, ax=ax[0])\nsns.countplot(x='OverallQual', data=house_train, ax=ax[1])","f4f8ff8d":"fig, ax = plt.subplots(2, 3, figsize = (16, 12))\nax[0,0].set_title('Gable')\nax[0,1].set_title('Hip')\nax[0,2].set_title('Gambrel')\nax[1,0].set_title('Mansard')\nax[1,1].set_title('Flat')\nax[1,2].set_title('Shed')\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Gable'], jitter=True, ax=ax[0,0])\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Hip'], jitter=True, ax=ax[0,1])\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Gambrel'], jitter=True, ax=ax[0,2])\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Mansard'], jitter=True, ax=ax[1,0])\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Flat'], jitter=True, ax=ax[1,1])\nsns.stripplot(x=\"RoofMatl\", y=\"SalePrice\", data=house_train[house_train.RoofStyle == 'Shed'], jitter=True, ax=ax[1,2])","a340f802":"sns.stripplot(x=\"GarageQual\", y=\"SalePrice\", data=house_train, hue='GarageFinish', jitter=True)","31ad97ba":"sns.pointplot(x=\"PoolArea\", y=\"SalePrice\", hue=\"PoolQC\", data=house_train)","2cbb65dc":"#There is only one such pool and sale condition for it is 'Abnorml'.\nhouse_train.loc[house_train.PoolArea == 555]","3530871e":"fig, ax = plt.subplots(1, 2, figsize = (12, 5))\nsns.stripplot(x=\"SaleType\", y=\"SalePrice\", data=house_train, jitter=True, ax=ax[0])\nsns.stripplot(x=\"SaleCondition\", y=\"SalePrice\", data=house_train, jitter=True, ax=ax[1])","f938d59f":"#MSSubClass shows codes for the type of dwelling, it is clearly a categorical variable.\nhouse_train['MSSubClass'].unique()","bd3d414e":"house_train['MSSubClass'] = house_train['MSSubClass'].astype(str)\nhouse_test['MSSubClass'] = house_test['MSSubClass'].astype(str)","e8bc5943":"for col in house_train.columns:\n    if house_train[col].dtype != 'object':\n        if skew(house_train[col]) > 0.75:\n            house_train[col] = np.log1p(house_train[col])\n        pass\n    else:\n        dummies = pd.get_dummies(house_train[col], drop_first=False)\n        dummies = dummies.add_prefix(\"{}_\".format(col))\n        house_train.drop(col, axis=1, inplace=True)\n        house_train = house_train.join(dummies)\n        \nfor col in house_test.columns:\n    if house_test[col].dtype != 'object':\n        if skew(house_test[col]) > 0.75:\n            house_test[col] = np.log1p(house_test[col])\n        pass\n    else:\n        dummies = pd.get_dummies(house_test[col], drop_first=False)\n        dummies = dummies.add_prefix(\"{}_\".format(col))\n        house_test.drop(col, axis=1, inplace=True)\n        house_test = house_test.join(dummies)","3c3d2510":"#This is how the data looks like now.\nhouse_train.head()","b5db4e99":"# Spilit training and testing dataset\nX_train = house_train.drop('SalePrice',axis=1)\nY_train = house_train['SalePrice']\nX_test  = house_test","d3fcec7b":"#Function to measure accuracy.\ndef rmlse(val, target):\n    return np.sqrt(np.sum(((np.log1p(val) - np.log1p(np.expm1(target)))**2) \/ len(target)))","9d440228":"Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, test_size=0.30)","b7e80a9c":"ridge = Ridge(alpha=10, solver='auto').fit(Xtrain, ytrain)\nval_ridge = np.expm1(ridge.predict(Xtest))\nrmlse(val_ridge, ytest)","335cb459":"ridge_cv = RidgeCV(alphas=(0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nridge_cv.fit(Xtrain, ytrain)\nval_ridge_cv = np.expm1(ridge_cv.predict(Xtest))\nrmlse(val_ridge_cv, ytest)","b9abcd01":"las = linear_model.Lasso(alpha=0.0005).fit(Xtrain, ytrain)\nlas_ridge = np.expm1(las.predict(Xtest))\nrmlse(las_ridge, ytest)","95830af5":"las_cv = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nlas_cv.fit(Xtrain, ytrain)\nval_las_cv = np.expm1(las_cv.predict(Xtest))\nrmlse(val_las_cv, ytest)","e26000fc":"model_xgb = xgb.XGBRegressor(n_estimators=340, max_depth=2, learning_rate=0.2) #the params were tuned using xgb.cv\nmodel_xgb.fit(Xtrain, ytrain)\nxgb_preds = np.expm1(model_xgb.predict(Xtest))\nrmlse(xgb_preds, ytest)","5b1515f8":"forest = RandomForestRegressor(min_samples_split =5,\n                                min_weight_fraction_leaf = 0.0,\n                                max_leaf_nodes = None,\n                                max_depth = None,\n                                n_estimators = 300,\n                                max_features = 'auto')\n\nforest.fit(Xtrain, ytrain)\nY_pred_RF = np.expm1(forest.predict(Xtest))\nrmlse(Y_pred_RF, ytest)","e70ec8d6":"coef = pd.Series(las_cv.coef_, index = X_train.columns)\nv = coef.loc[las_cv.coef_ != 0].count() \nprint('So we have ' + str(v) + ' variables')","8bb0fcff":"#Basically I sort features by weights and take variables with max weights.\nindices = np.argsort(abs(las_cv.coef_))[::-1][0:v]","40dc96df":"#Features to be used. I do this because I want to see how good will other models perform with these features.\nfeatures = X_train.columns[indices]\nfor i in features:\n    if i not in X_test.columns:\n        print(i)","09f126bd":"X_test['RoofMatl_ClyTile'] = 0","baf0d4e6":"X = X_train[features]\nXt = X_test[features]","c804c914":"Xtrain1, Xtest1, ytrain1, ytest1 = train_test_split(X, Y_train, test_size=0.33)","e63357eb":"ridge = Ridge(alpha=5, solver='svd').fit(Xtrain1, ytrain1)\nval_ridge = np.expm1(ridge.predict(Xtest1))\nrmlse(val_ridge, ytest1)","587dbb3c":"las_cv = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10)).fit(Xtrain1, ytrain1)\nval_las = np.expm1(las_cv.predict(Xtest1))\nrmlse(val_las, ytest1)","b4148e4e":"model_xgb = xgb.XGBRegressor(n_estimators=340, max_depth=2, learning_rate=0.2) #the params were tuned using xgb.cv\nmodel_xgb.fit(Xtrain1, ytrain1)\nxgb_preds = np.expm1(model_xgb.predict(Xtest1))\nrmlse(xgb_preds, ytest1)","fc7c3360":"forest = RandomForestRegressor(min_samples_split =5,\n                                min_weight_fraction_leaf = 0.0,\n                                max_leaf_nodes = None,\n                                max_depth = 100,\n                                n_estimators = 300,\n                                max_features = None)\n\nforest.fit(Xtrain1, ytrain1)\nY_pred_RF = np.expm1(forest.predict(Xtest1))\nrmlse(Y_pred_RF, ytest1)","99707cb1":"las_cv1 = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nlas_cv1.fit(X, Y_train)\nlasso_preds = np.expm1(las_cv1.predict(Xt))","8442660a":"#I added XGBoost as it usually improves the predictions.\nmodel_xgb = xgb.XGBRegressor(n_estimators=340, max_depth=2, learning_rate=0.1)\nmodel_xgb.fit(X, Y_train)\nxgb_preds = np.expm1(model_xgb.predict(Xt))","c1d6fcc5":"preds = 0.7 * lasso_preds + 0.3 * xgb_preds","10dc07f0":"submission = pd.DataFrame({\n        'Id': house_test['Id'].astype(int),\n        'SalePrice': preds\n    })\nsubmission.to_csv('home.csv', index=False)","95474c2a":"model_lasso = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 100))\nmodel_lasso.fit(X_train, Y_train)\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nv1 = coef.loc[model_lasso.coef_ != 0].count()\nprint('So we have ' + str(v1) + ' variables')","833e3f2f":"indices = np.argsort(abs(model_lasso.coef_))[::-1][0:v1]\nfeatures_f=X_train.columns[indices]","042435d1":"print('Features in full, but not in val:')\nfor i in features_f:\n    if i not in features:\n        print(i)\nprint('\\n' + 'Features in val, but not in full:')\nfor i in features:\n    if i not in features_f:\n        print(i)","13e5b51d":"for i in features_f:\n    if i not in X_test.columns:\n        X_test[i] = 0\n        print(i)\nX = X_train[features_f]\nXt = X_test[features_f]","8dd6d011":"model_lasso = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nmodel_lasso.fit(X, Y_train)\nlasso_preds = np.expm1(model_lasso.predict(Xt))","60a38578":"model_xgb = xgb.XGBRegressor(n_estimators=340, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(X, Y_train)\nxgb_preds = np.expm1(model_xgb.predict(Xt))","0ad2e492":"solution = pd.DataFrame({\"id\":house_test.Id, \"SalePrice\":0.7*lasso_preds + 0.3*xgb_preds})\nsolution.to_csv(\"House_price.csv\", index = False)","1ffbf726":"#### Pivotal Features","d80bad5c":"Quite a lot of variables. Many categorical variables, which makes analysis more complex. And a lot of missing values. Or are they merely missing values? There are many features for which NaN value simply means an absense of the feature (for example, no Garage).","f6da691e":"### Submission","a38c2744":"Maybe a good idea would be to create some new features, but I decided to do without it. It is time-consuming and model is good enough without it. Besides, the number of features if quite high already.","b951c395":"## Training and Testing the Model","8523bfa1":"### Data Analysis","5fc6fc8f":"# Project Work Flow\n* Problem Defintion : \nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n* Loading Packages or Import Libraries\n* Gathering Data or Data Collection\n* Exploratory Data Analysis(EDA)\n    - Data Analysis\n    - Data Visualization\n    - Data Pre-processing\n    - Data Wraggling\n* Training and Testing the model\n* Evaluation\n* Submission","4ac77807":"Most of the sold houses are either new or sold under Warranty Deed. And only a little number of houses are sales between family, adjoining land purchases or allocation.","af9aab2e":"Houses with central air conditioning cost more. And it is interesting that houses with poor and good heating quality cost nearly the same if they have central air conditioning. Also only houses with gas heating have central air conditioning.","27c36353":"## Data Visualization","ffa726b6":"There are several additional cases: when a categorical variable is None, relevant numerical variable should be 0. For example if there is no veneer (MasVnrType is None), MasVnrArea should be 0.","69f9a169":"A lot of difference between the selected features. I suppose that the reason for this is that there was too little data relatively to the number of features in the first case. So I'll use the features obtain with the analysis of the whole train dataset.","bf275461":"I hope this kernal is useful to you to learn exploratory data analysis and regression problem.\n\nIf find this notebook help you to learn, Please Upvote.\n\nThank You!!","86a49561":"#### Relationship with numerical variables","9239d942":"#### Lets' check corelation of target varible with other variables","479f140a":"### Let's Check Missing or Null Values in data\nMissing values in the training data set can affect prediction or classification of a model negatively.\n\nBut filling missing values with mean\/median\/mode or using another predictive model to predict missing values is also a prediction which may not be 100% accurate, instead you can use models like Decision Trees and Random Forest which handle missing values very well.","d70eefaf":"#### Imputting missing values\n","460157fd":"![](http:\/\/investmarbellaproperty.com\/wp-content\/uploads\/2019\/04\/Torremolinos-1024x640.jpg)","6c1439c2":"#### Relationship with categorical features","04cb286d":"### Data Fields (or Variables)\n\nHere's a brief version of what you'll find in the data description file.\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","b5bcc8bf":"It is worth noting that there are only 7 different pool areas. And while for most of them mean price is ~200000-300000$, pools with area 555 cost very much. Let's see.","550ec1c4":"# House_Price_Prediction : Regression_Model\n\n![](http:\/\/investmarbellaproperty.com\/wp-content\/uploads\/2019\/04\/4c96032809c10d54e3e216015aecf32a_XL-1-1080x675.jpg)","908437f7":"Transforming skewered data and dummifying categorical.","08ab9788":"These graphs show information about roof materials and style. Most houses have Gable and Hip style. And material for most roofs is standard.","d5062281":"It seems that only several pairs of variables have high correlation. But this chart shows data only for pairs of numerical values. I'll calculate correlation for all variables.","f50eefce":"### Outliers\nTreat outliers from the dataset. There are some values that are quite different from the rest. It makes sense to delete these variables as the Linear regression methods are very sensitive to outliers.","d42c8f7c":"RoofMatl_ClyTile\n\nThere is only one selected feature which isn't in test data. I'll simply add this column with zero values.[](http:\/\/)","64550849":"## Data preparation","6de4f4f4":"Let's see whether something changed.","cf4d22aa":"## Exploratory Data Analysis","417de121":"# Loading Packages or Import Libraries\nLoading and Inspecting Data","5c0049ae":"But the result wasn't very good. I thought for some time and then decided that the problem could lie in feature selection - maybe I selected bad features or Maybe random seed gave bad results. I decided to try selecting features based on full training dataset (not just on part of the data).","ea1b8329":"So linear models perform better than the others. And lasso is the best.\n\nLasso model has one nice feature - it provides feature selection, as it assignes zero weights to the least important variables.","144c90d2":"Now all necessary features are present in both train and test.","20a3f2cf":"This is a list of highly correlated features. They aren't surprising and none of them should be removed.","cd94ed17":"Most finished garages gave average quality.","94114ee3":"The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.","eb905e54":"At first I'll look into data correlation, then I'll visualize some data in order to see the impact of certain features.","58b995e9":"#### Analysis : Sales Price (Target or Dependent Variable)","4f54e1a3":"##### The distribution does not look normal, it is positively skewed, some outliers can also be seen. Simple log transformation might change the distribution to normal. "}}