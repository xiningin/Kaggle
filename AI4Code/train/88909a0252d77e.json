{"cell_type":{"889e66f8":"code","dcfa08cb":"code","35223cba":"code","1e49fd66":"code","2d3fa959":"code","602ce8c6":"code","d89216ff":"code","6eb7c9f3":"code","3f9c0ace":"code","e4b2c1b5":"code","5b47af3a":"code","3be312ab":"code","71f7952c":"code","0fdc49ea":"code","909be24c":"code","e7ba7c1d":"code","1a49bd4c":"code","f6600d00":"code","1c2a20bd":"code","782a9b23":"code","4cc7f8e7":"code","54ed91f2":"markdown","75d508cb":"markdown","5330c9e3":"markdown","b9ccdefa":"markdown","d7fee0a1":"markdown"},"source":{"889e66f8":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))\nimport operator\n\n#export\nfrom pathlib import Path\nfrom IPython.core.debugger import set_trace\nfrom fastai import datasets\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor\nimport pandas as pd\n\nMNIST_URL='http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl'\n\ndef get_data():\n    path = datasets.download_data(MNIST_URL, ext='.gz')\n    with gzip.open(path, 'rb') as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n    return map(tensor, (x_train, y_train, x_valid, y_valid))\n\ndef normalize(x,m,s): return (x-m)\/s","dcfa08cb":"x_train, y_train, x_valid, y_valid = get_data()\nprint(x_train.mean(), x_train.std())\ntrain_mean, train_valid = x_train.mean(), x_train.std()\n\nx_train = normalize(x_train, train_mean, train_valid)\nx_valid = normalize(x_valid, train_mean, train_valid) # Use train, not valid\nprint(x_train.mean(), x_train.std())","35223cba":"def test_near_zero(a, tol=1e-3): assert a.abs()<tol, f\"Near zero:{a}\"\n\ntest_near_zero(x_train.mean())\ntest_near_zero(1- x_train.std())","1e49fd66":"n,m = x_train.shape\nc   = y_train.max()+1\nnh  = 50\nn,m,c,nh","2d3fa959":"# kaiming init \/ he init\nw1 = torch.randn(m,nh)*math.sqrt(1.\/m)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)*math.sqrt(1.\/nh)\nb2 = torch.zeros(1)\n\ntest_near_zero(w1.mean())\ntest_near_zero(w1.std()-1\/math.sqrt(m))","602ce8c6":"x_valid.mean(), x_valid.std() # Normalized","d89216ff":"def lin(x,w,b): return x@w+b\ndef relu(x): return x.clamp_min(0.)\n\nt = lin(x_valid,w1,b1); print(t.mean(), t.std())\nt = relu(t)           ; print(t.mean(), t.std())\n# relu's mean would be a half, not zero, which is explained later","6eb7c9f3":"# kaiming init \/ he init for relu\nw1 = torch.randn(m,nh)*math.sqrt(2.\/m); print(w1.mean(),w1.std())\nt  = relu(lin(x_valid,w1,b1))        ; print( t.mean(), t.std())","3f9c0ace":"# replacing with pytorch init\nfrom torch.nn import init\nw1 = torch.zeros(m,nh)\ninit.kaiming_normal_(w1, mode='fan_out'); print(w1.mean(), w1.std())\nt  = relu(lin(x_valid,w1,b1))           ; print( t.mean(),   t.std())\n# In this case, {'fan_in':'fan_out' = m:nh}","e4b2c1b5":"from torch import nn\nw1.shape, nn.Linear(m,nh).weight.shape # transposed","5b47af3a":"def relu(x): return x.clamp_min(0.)-0.5\nw1 = torch.randn(m,nh)*math.sqrt(2.\/m)\nt1 = relu(lin(x_valid,w1,b1)); t1.mean(), t1.std()\n# relu's mean is now near zero","3be312ab":"def model(xb): return lin(relu(lin(xb,w1,b1)), w2, b2)\nmodel(x_valid).shape","71f7952c":"def mse(outp, targ): return (outp.squeeze()-targ).pow(2).mean()\ny_train, y_valid = y_train.float(), y_valid.float()\nmse(model(x_train), y_train)","0fdc49ea":"def mse_grad(inp, targ): # inp is previous output == pred\n    inp.g =  2.*(inp.squeeze()-targ).unsqueeze(-1) \/ y_train.shape[0]\n\ndef relu_grad(inp, out):\n    inp.g = (inp>0).float() * out.g\n\ndef lin_grad(inp, out, w, b):\n    inp.g = out.g @ w.t()\n    w.g   = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g   = out.g.sum(0)\n\ndef fwd_bk(inp, targ):\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    loss = mse(out, targ)\n    \n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)","909be24c":"fwd_bk(x_train, y_train)\n\nw1g = w1.g.clone(); w2g = w2.g.clone()\nb1g = b1.g.clone(); b2g = b2.g.clone()\nig  = x_train.g.clone()\n\n# PyTorch can manage above backprop process\nxt2 = x_train.clone().requires_grad_(True)\nw12 = w1.clone().requires_grad_(True)\nw22 = w2.clone().requires_grad_(True)\nb12 = b1.clone().requires_grad_(True)\nb22 = b2.clone().requires_grad_(True)","e7ba7c1d":"def forward(inp, targ): return mse(relu(inp @ w12 + b12) @ w22 + b22, targ)\n\nloss = forward(xt2, y_train)\nloss.backward()","1a49bd4c":"def test(a,b,cmp,cname=None):\n    if cname is None: cname=cmp.__name__\n    assert cmp(a,b), f\"{cname}:\\n{a}\\n{b}\"\ndef test_eq(a,b): test(a,b,operator.eq, '==')\ndef near(a,b): return torch.allclose(a,b,1e-3,1e-5)\ndef test_near(a,b): test(a,b,near)\n\ntest_near(w22.grad, w2g)\ntest_near(b22.grad, b2g)\ntest_near(w12.grad, w1g)\ntest_near(b12.grad, b1g)\ntest_near(xt2.grad, ig)","f6600d00":"# Refactoring\n# I'm skipping","1c2a20bd":"class Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n        self.loss = mse\n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x.squeeze(), targ)","782a9b23":"%time loss = Model(m, nh, 1)(x_train, y_train)","4cc7f8e7":"%time loss.backward()","54ed91f2":"# Lesson video & note\n\nhttps:\/\/course.fast.ai\/videos\/?lesson=8                 \nhttps:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl2\/02_fully_connected.ipynb","75d508cb":"## Grad and backward pass","5330c9e3":"## nn.Linear and nn.Module","b9ccdefa":"## Basic Architecture","d7fee0a1":"## Loss func: MSE"}}