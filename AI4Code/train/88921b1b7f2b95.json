{"cell_type":{"2225df76":"code","add8aa32":"code","7460b9ef":"code","30e69fc1":"code","0403f34e":"code","c23b96ab":"code","8ccf68fa":"code","81458b69":"code","d8d383e5":"code","3f522946":"code","3166f0d6":"code","8e3a0679":"code","728bdd36":"code","f9e270b9":"code","ebfb75a8":"code","ecb6b8c7":"code","1a9d39c9":"code","81a8aa5e":"code","31e93552":"markdown","19739d7d":"markdown","e824fa7d":"markdown","2ffc0ed0":"markdown","6e88f9fa":"markdown","eba6aee4":"markdown","4a412df5":"markdown","3a3671b2":"markdown","a02d39c2":"markdown","0e89f2fc":"markdown","dccab7a4":"markdown","9dd7c29d":"markdown","e5e3366e":"markdown","f41b67b6":"markdown","407ba801":"markdown","a20eab69":"markdown","fed5dc21":"markdown","db40054f":"markdown","bde3b441":"markdown"},"source":{"2225df76":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport pandas as pd\n\n# mapping the 3 target classes names to numeric \n# values to be able to use them later in our model\n#since our model only accepts numeric numbers\nX = pd.read_csv(\"..\/input\/Iris.csv\")\n\n# To map the strings you need to use map(dict) function \n# from the dataframe, This function accepts a dictionary \n# with the values to be replaced as Keys and the values \n# that would replace them as the values for those keys \nz = {'Iris-setosa' : 1, 'Iris-versicolor' : 2, 'Iris-virginica' : 3 }\n\nX['Species'] = X['Species'].map(z)\nprint (\"Number of data points ::\", X.shape[0])\nprint(\"Number of features ::\", X.shape[1])","add8aa32":"X.head()","7460b9ef":"import matplotlib.pyplot as plt\nimport numpy as np\n# X.species holds the classes that we have after mapping which are {1, 2, 3}\nclasses = np.array(list(X.Species.values))\n\n# Now we will use matplotlib to plot the classes with the axis x and y representing two of the features that we have\n# We do that to see if some features contribute to the seperablity of the dataset more than the others\ndef plotRelation(first_feature, sec_feature):\n    \n    plt.scatter(first_feature, sec_feature, c = classes, s=10)\n    plt.xlabel(first_feature.name)\n    plt.ylabel(sec_feature.name)\n    \nf = plt.figure(figsize=(25,20))\nf.add_subplot(331)\nplotRelation(X.SepalLengthCm, X.SepalWidthCm)\nf.add_subplot(332)\nplotRelation(X.PetalLengthCm, X.PetalWidthCm)\nf.add_subplot(333)\nplotRelation(X.SepalLengthCm, X.PetalLengthCm)\nf.add_subplot(334)\nplotRelation(X.SepalLengthCm, X.PetalWidthCm)\nf.add_subplot(335)\nplotRelation(X.SepalWidthCm, X.PetalLengthCm)\nf.add_subplot(336)\nplotRelation(X.SepalWidthCm, X.PetalWidthCm)","30e69fc1":"import matplotlib.pyplot as plt\nimport numpy as np\n# X.species holds the classes that we have after mapping which are {1, 2, 3}\nclasses = np.array(list(X.Species.values))\n\n# Now we will use matplotlib to plot the classes with the axis x and y representing two of the features that we have\n# We do that to see if some features contribute to the seperablity of the dataset more than the others\nimport seaborn as sns\n\nExploration_columns = X.drop('Id' ,   axis = 1)\nsns.pairplot(Exploration_columns, hue = \"Species\")","0403f34e":"import seaborn as sns\n\n# Here we use the seaborn library to visualize the correlation matrix\n# The correlation matrix shows how much are the features and the target correlated\n# This gives us some hints about the feature importance\nimport matplotlib.pyplot as plt\n\n\ncorr = X.corr()\nf, ax = plt.subplots(figsize=(15, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","c23b96ab":"f = plt.figure(figsize=(25,10))\nf.add_subplot(221)\nX.SepalWidthCm.hist()\nf.add_subplot(222)\nX.SepalLengthCm.hist()\nf.add_subplot(223)\nX.PetalLengthCm.hist()\nf.add_subplot(224)\nX.PetalWidthCm.hist()","8ccf68fa":"f = plt.figure(figsize=(25,10))\nf.add_subplot(221)\nsns.boxplot(x=X['PetalWidthCm'])\nf.add_subplot(222)\nsns.boxplot(x=X['PetalLengthCm'])\nf.add_subplot(223)\nsns.boxplot(x=X['SepalLengthCm'])\nf.add_subplot(224)\nsns.boxplot(x=X['SepalWidthCm'])\n\n\nsns.boxplot(x=X['PetalWidthCm'])","81458b69":"from scipy import stats\nimport numpy as np\n\nz = np.abs(stats.zscore(X))\n\nzee = (np.where(z > 2.5))[1]\n\nprint(\"number of data examples greater than 3 standard deviations = %i \" % len(zee))","d8d383e5":"data_delete = X[(z >= 2.5)]\ndata_delete.drop_duplicates(keep='first', inplace=True)\nunique, count= np.unique(data_delete[\"Species\"], return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","3f522946":"X = X[(z <= 2.5)]","3166f0d6":"# Removing the label from the training data\ny = X['Species']\nX = X.drop([\"Species\"], axis = 1)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","8e3a0679":"fig = plt.figure(1, figsize=(16, 9))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(X_scaled)\n\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()\nprint(\"The number of features in the new subspace is \" ,X_reduced.shape[1])","728bdd36":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Scatter3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2], mode='markers', marker=dict( size=4, color=y, colorscale= \"Portland\", opacity=0.))])\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","f9e270b9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n                        X_reduced, y, test_size=0.2, random_state=42)","ebfb75a8":"from sklearn.svm import LinearSVC\n\nclf = LinearSVC(penalty='l2', loss='squared_hinge',\n                dual=True, tol=0.0001, C=100, multi_class='ovr',\n                fit_intercept=True, intercept_scaling=1, class_weight=None,verbose=0\n                , random_state=0, max_iter=1000)\nclf.fit(X_train,y_train)\n\nprint('Accuracy of linear SVC on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n\nprint('Accuracy of linear SVC on test set: {:.2f}'.format(clf.score(X_test, y_test)))","ecb6b8c7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nimport numpy as np\n    \nc = np.logspace(start = -15, stop = 1000, base = 1.02)\nparam_grid = {'C': c}\n\n\ngrid = GridSearchCV(clf, param_grid =param_grid, cv=3, n_jobs=-1, scoring='accuracy')\ngrid.fit(X_train, y_train)\n  \nprint(\"The best parameters are %s with a score of %0.0f\" % (grid.best_params_, grid.best_score_ * 100 ))\nprint( \"Best estimator accuracy on test set {:.2f} \".format(grid.best_estimator_.score(X_test, y_test) * 100 ) )","1a9d39c9":"from sklearn.svm import SVC\n\nclf_SVC = SVC(C=100.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n          probability=False, tol=0.001, cache_size=200, class_weight=None, \n          verbose=0, max_iter=-1, decision_function_shape=\"ovr\", random_state = 0)\nclf_SVC.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf_SVC.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf_SVC.score(X_test, y_test) * 100))","81a8aa5e":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nimport numpy as np\n    \nc_SVC = np.logspace(start = 0, stop = 10, num = 100, base = 2 , dtype = 'float64')\nprint( 'the generated array of c values')\nprint ( c_SVC )\nparam_grid_S = {'C': c_SVC}\n\n\n\nprint(\"\\n Array of means \\n\")\nclf = GridSearchCV(clf_SVC, param_grid =param_grid_S, cv=20 , scoring='accuracy')\nclf.fit(X_train, y_train)\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nprint(means)\n\ny_true, y_pred = y_test, clf.predict(X_test)\nprint( '\\nClassification report\\n' )\nprint(classification_report(y_true, y_pred))\n","31e93552":"Many machine learning algorithms require the data to be in normal distribution that is the Mean equals 0 and with unit vairance. This will help the features to represent the data better since a feature with high variance will not dominate the others, to know more about that refere to the Sklearn doumentation [here](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) .\nHere we will use the **Standard Scaler** to transform the data.","19739d7d":"Let's not try non linear SVC and see the results","e824fa7d":"## Algorithm choice\nSince we have only 150 training examples then I will try **SVMs**. If you don't know which estimator or algorithm to use you can check the Scikit Learn Cheat sheet below.\n![](http:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)","2ffc0ed0":"It seems the training score is a little higher than linear SVC but let's try to tune the parameters using grid search to optmize more","6e88f9fa":"Let's now check the histograms of the data features to know more about the data stastically","eba6aee4":"The training results are not improved after tuning the c parameter with grid search and the test score is still 100 which is a good indication that the model is not overfitting nor underfitting. but we can still try improve the training accuracy to ensure that it will catch more test data if new data were intrerduced as the dataset is really small and that may be a bit tricky.","4a412df5":"Now let's discover the seperability of data and the relation between features using plots, histograms and heatmaps","3a3671b2":"<h2>Dimentionality Reduction<\/h2>\nDimentionality reduction is a really important concept in Machine Learning since it reduces the number of features in a dataset and hence reduces the computations needed to fit the model. PCA is one of the well known efficient dimentaniolaty reduction techniques. in this tutorial we will use  **PCA** which compreses the data by projecting it to a new subspace that can help in reducing the effect of the **curse of dimentionality**.\nOur dataset consists of 4 dimentions(4 features) so we will project it to a 3 dimentions space and Plot in in a 3d graph.","a02d39c2":"Let's now try fitting a Linear SVM that is imported from sklearn.","0e89f2fc":"Below is an interactive 3d visualization after performing PCA with three components","dccab7a4":"It's time to tune the C parameter and see if we can reach better results. we will perform the tuning automatically using grid search algorithm that is imported from scikit learn. we will use exponential range for c parameters with base = 1.02.","9dd7c29d":"<h2>Data exploration and cleaning<\/h2>\nData exploration is very important befoe trying to classify or fit the data as it will help you choose the best algorithm to use. Data exploration will also till you about the initial connfiguration of some hyper parameters like the degree of polynomial or the type of the kernel.\nNow let's import the libraries we are going to use in this stage and then load the data from the dataset. The X represents the **features matrix** and the y represents the **target vector**","e5e3366e":"It seems the model will not improve further using SVC after tuning the c parameter.","f41b67b6":"It is time to see the correlation between features and the label vector as well.","407ba801":"It seems we have some outliers in the petalWidthCm so we will make further outlier analysis using the zscore method","a20eab69":"From the previous Heatmap of correlation it seems that all features have either strong positive or negative correlation with the Target(label) vector species that we want to classify.","fed5dc21":"Now we will explore outliers in the data using boxplot and zscores methods. We will consider any point with score greater than 2.5 std as an outlier. you can know more about this [here](https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba).","db40054f":"Now les't perform the train test split on the transformed data. We will use the convention that splits the data in 80% training\/validation and 20% test set. We set the random state to a constant value in order to get consistent results when we rerun the code.","bde3b441":"<h1>Classifying the Iris dataset using **Support Vector Machines**  (SVMs)<\/h1>\n\nIn this tutorial I are going to show how to explore and classify the [Iris dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris) and also how to analyse the results of classification using SVMs. In this tutorial i will be using Support vector machines with dimentianility reduction techniques like PCA and Scalers to classify the dataset efficiently.\n1. Data Exploration and Cleaning.\n2. Dimentality Reduction\n3. Algorithm Choice"}}