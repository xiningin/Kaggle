{"cell_type":{"d147b053":"code","4b3e879b":"code","68463c21":"code","c3556738":"code","4c5e2fbc":"code","0a1ef9d3":"code","a7c1ee07":"code","addca9db":"code","7c4c9faf":"code","536952f5":"code","bbd82e80":"code","4fd6f333":"code","43480c2d":"code","7cecbf61":"code","f5c1ca43":"code","28e03c54":"code","360f190b":"code","c021140e":"code","dd4eb3b2":"code","f0e90f32":"code","77454a5b":"code","006eeb06":"code","0e1a984a":"code","d211072b":"code","b966902a":"code","530a5c4b":"code","d14b78b7":"markdown","5c0a9b20":"markdown","7a25fc94":"markdown","3600fc2d":"markdown","c449fc21":"markdown","9c64c0a9":"markdown","57386390":"markdown","6e478eec":"markdown","8e7444a1":"markdown","13ff58ac":"markdown","a22b9a56":"markdown","f56f7103":"markdown","053179f8":"markdown","67abc130":"markdown","6ae5b28e":"markdown","27ddbe2b":"markdown","bcfffe0c":"markdown","40a28bc1":"markdown","7ed23a0e":"markdown","45da6b72":"markdown","7d14df7f":"markdown","a5354871":"markdown"},"source":{"d147b053":"# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cv2       # open CV\nimport os\n\n# Data manipulation & visulation\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom keras.layers import Dense\nfrom keras.layers import Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.utils import to_categorical    # one hot encoding\nfrom keras.models import Sequential\nfrom keras.applications import VGG16\nfrom keras.callbacks import ReduceLROnPlateau\n\n\nimport random as rn\nfrom tqdm import tqdm\n","4b3e879b":"print(os.listdir('..\/input\/flowers-recognition\/flowers\/flowers'))","68463c21":"Daisy_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/daisy'\nSunflower_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/sunflower'\nTulip_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/tulip'\nDandelion_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/dandelion'\nRose_flower_dir = '..\/input\/flowers-recognition\/flowers\/flowers\/rose'","c3556738":"images = []\nlabels = []\nimg_size = 150\n\ndef image_data(flower_name, DIR):\n    for i in tqdm(os.listdir(DIR)):\n        try:\n            \n            path = os.path.join(DIR,i)\n            img = cv2.imread(path)\n            img = cv2.resize(img, (img_size, img_size))\n            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        \n            images.append(np.array(img))\n            labels.append(str(flower_name))\n            \n        except:\n            print(path)\n            \n","4c5e2fbc":"image_data('Daisy', Daisy_flower_dir)\nlen(images)","0a1ef9d3":"image_data('Sunflower', Sunflower_flower_dir)\nlen(images)","a7c1ee07":"image_data('Tulip', Tulip_flower_dir)\nlen(images)","addca9db":"image_data('Dandelion', Dandelion_flower_dir)\nlen(images)","7c4c9faf":"image_data('Rose', Rose_flower_dir)\nlen(images)","536952f5":"data = np.array(images)\nlabels = np.array(labels)\nprint('Input(Feature) Data shape :', data.shape)\nprint('Output(Labels) Data shape :', labels.shape)","bbd82e80":"fig, ax = plt.subplots(4, 2, figsize = (15, 20))\nfor i in range(4):\n    for j in range(2):\n        l = rn.randint(0, data.shape[0])\n        ax[i,j].imshow(data[l])\n        ax[i,j].set_title(labels[l])\n        ax[i,j].axis('off')\n        ","4fd6f333":"sns.countplot(labels)\nplt.title(\"Class Distribution\")\nplt.xlabel(\"Class Number\")","43480c2d":"le = LabelEncoder()\ny = le.fit_transform(labels)\ny = to_categorical(y, 5)\ny[0]","7cecbf61":"# Normalize the input data in range [0 1]\nX = data\/255","f5c1ca43":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 5)\n\nprint(\"X_train shape :\",X_train.shape)\nprint(\"y_train shape :\",y_train.shape)\nprint(\"X_test shape :\" ,X_test.shape)\nprint(\"y_test shape :\",y_test.shape)","28e03c54":"np.random.seed(40)\nrn.seed(40)","360f190b":"Base_model = VGG16(include_top= False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nBase_model.summary()","c021140e":"model = Sequential()\n\nmodel.add(Base_model)\nmodel.add(Dense(256,activation='relu'))\n# adding prediction(softmax) layer\nmodel.add(Dense(5,activation=\"softmax\"))","dd4eb3b2":"# freeze layers(Base Model)\nBase_model.trainable = False","f0e90f32":"# Set a learning rate annealer\n\nred_lr=ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=2, verbose=1)\n","77454a5b":"datagen = ImageDataGenerator(featurewise_center= False,\n                              samplewise_center= False,\n                              featurewise_std_normalization= False,\n                              samplewise_std_normalization=False,\n                              rotation_range= 10,        # 0- 180\n                              zca_whitening=False,\n                              zoom_range=0.1,            # Randomly zoom image\n                              width_shift_range=0.2,     # randomly shift images horizontally (fraction of total width)\n                              height_shift_range=0.2,    # randomly shift images vertically (fraction of total height)\n                              horizontal_flip=True,      # randomly flip images\n                              vertical_flip=False)       # randomly flip images\n                             \ndatagen.fit(X_train)","006eeb06":"model.summary()","0e1a984a":"model.compile(optimizer=Adam(lr = 1e-4), loss= 'categorical_crossentropy', metrics=['accuracy'])","d211072b":"batch_size=64\nHistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = 50, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size)","b966902a":"plt.plot(History.epoch, History.history['accuracy'])\nplt.plot(History.epoch, History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.legend(['train', 'test'])\nplt.xlabel('No. of Epochs')\nplt.ylabel('Accuracy')","530a5c4b":"plt.plot(History.epoch, History.history['loss'])\nplt.plot(History.epoch, History.history['val_loss'])\nplt.title('Model Loss')\nplt.legend(['train', 'test'])\nplt.xlabel('No. of Epochs')\nplt.ylabel('Loss')","d14b78b7":"This is complete summary of model","5c0a9b20":"* Data is Balanced","7a25fc94":"* We are not using fully connected layers of VVG16 model(include_top= False). We will add it from our side because in VGG16 softmax layer has 1000 classes and We have only 5 classes.\n* We are using pretrained weights of Imagenet\n* This is a sumary of my base model not a actual model","3600fc2d":"# **Transfer Learning : VGG16**","c449fc21":"For the data augmentation, i choosed to :\n\n* Randomly rotate some training images by 10 degrees\n* Randomly Zoom by 10% some training images\n* Randomly shift images horizontally by 10% of the width\n* Randomly shift images vertically by 10% of the height","9c64c0a9":"### Annealer\n\nIn order to make the optimizer converge faster and closest to the global minimum of the loss function, i used an annealing method of the learning rate (LR).\n\nThe LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n\nTo keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved)","57386390":"### Load DataSet","6e478eec":"### Add own fully connected Layers","8e7444a1":"\n### Data augmentation to prevent Overfitting","13ff58ac":"There are five types flowers images","a22b9a56":"### Visualizing random Images","f56f7103":"### Preparing Base model\n\nTransfer learning refers to using a pretrained model on some other task for your own task. Hence we need to specify the particular model which we are deploying in our task and thus needs to specify the base model.\n\nIn our case we are using the VGG16 model from the 'keras.applications' library as the base model.","053179f8":"### Model Loss","67abc130":"### One Hot Encoding of labels","6ae5b28e":"### Compile and train the model","27ddbe2b":"### Count plot for each class","bcfffe0c":"### Spliting data in to training and test set","40a28bc1":"### Import Libraries","7ed23a0e":"![](https:\/\/previews.123rf.com\/images\/verooka\/verooka1608\/verooka160800020\/63263448-multi-colored-bouquet-of-different-autumn-flowers-background.jpg)","45da6b72":"### Set Random Seed","7d14df7f":"In this kernel I have demonstrated the general techniques that can be used with Transfer Learning.\n\nFor this kernel I have used the Flower Recognition dataset .\n\nBasically , you need to watch two things\n\n1) The simalarity of your dataset with that of the pre-trained model and\n\n2) The amount of the data that you have.","a5354871":"### Model Accuracy"}}