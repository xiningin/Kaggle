{"cell_type":{"aa6e5259":"code","c6f2d6bd":"markdown","fd71d497":"markdown"},"source":{"aa6e5259":"########################################## KAGGLE SPECIFIC CODE ####################################################\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n###################################################################################################################\n\n\n##### Let's follow a typical data science workflow #####\n\n##### 1. problem description and objective\n# * problem description: Titanic sank on maiden voyage after colliding with an iceberg, killing 2\/3 of the passengers\n# * objective: predict if a passenger survived or not (binary classification problem)\n\n##### 2. data acquisition\n# 2a. early negotiating data access with stakeholders too reduce risk\n# 2b. storing the data securely and according to privacy regulations (GDPR)\n# 2c. spend your time wisely. More data samples? More features? Artificially augment data?\n\n# in this case Kaggle has peformed data acquisition for us\nfrom sklearn.utils import shuffle\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")  # it has features and labels\nkaggle_competition_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")  # for the Kaggle competition; it features but has no labels!\n\n\n##### 3. data exploration using descriptive statistics and visualization\n# Some options:\n# 3a. find out for each feature: data type\n# 3b. find out for each feature: numerical, nominal categorical, ordinal categorical\n# 3c. heat map to find correlations between features and between features and target variable\n\n# just a pretty random tryout\nmen = df.loc[df.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)\n\n# show some data samples to get a feeling about the data\nprint('\\n************************* df.head():')\nprint(df.head())\n# to note:\n# * what is the difference between passenger id and ticket number?\n\n# some descriptive statistics\nprint('\\n************************* df.describe():')\nprint(df.describe())\n\n# find out the types of the features and find the features that contain missing data (NaN)\nprint('\\n************************* df.info():')\nprint(df.info())\n# to note:\n# * feature Cabin has very many missing values\n# * feature Age has quite some missing values\n# * feature Embarked has missing values\n# we also need to investigate the kaggle_competition_data\nprint('\\n************************* kaggle_competition_data.info():')\nprint(kaggle_competition_data.info())\n# to note:\n# * feature Fare has missing values\n\n# use google to find information about the feature\n# http:\/\/campus.lakeforest.edu\/frank\/FILES\/MLFfiles\/Bio150\/Titanic\/TitanicMETA.pdf\n# (int)(numerical)           PassengerId\n# (int)(target variable)     Survived  Survival (0 = No; 1 = Yes)\n# (int)(ordinal categorical) Pclass    Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n# (str)(nominal categorical) Name      \n# (str)(nominal categorical) Sex       \n# (flt)(numerical)           Age       \n# (int)(numerical)           SibSp     Number of Siblings\/Spouses Aboard\n# (int)(numerical)           Parch     Number of Parents\/Children Aboard\n# (str)(nominal categorical) Ticket    Ticket Number\n# (flt)(numerical)           Fare      Passenger Fare (British pound)\n# (str)(nominal categorical) Cabin     Cabin (letter of the cabin number indicates the deck)\n# (str)(nominal categorical) Embarked  Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n# heatmap: visualization of correlation between features\n# heatmap ignores categorical features that are not numbers\nimport seaborn as sns\nsns.heatmap(df.corr(), annot = True, vmin=-1, vmax=1, center= 0)\n\n\n##### 4. data preprocessing \n# 4a. dealing with missing data\n# 4b. encode categorical features as dummy variables\n\n# below we're going to split df into a training and test set to allow model evaluation. To avoid differences between train \n# and test dataset let's shuffle df so that the split will be random. \ndf = shuffle(df)\n\n# it is handy to perform data processing on the complete data set at once, so on df and kaggle_competition_data together\n# however df contains labels and kaggle_competition_data does not, so let's split df into features and labels\ndf_y = df['Survived']\ndf_X = df.drop('Survived', axis=1)\n# to remember what part is df and what part is kaggle_competition_data\ndf_len = len(df)\n\n# * split the data into training, validation and test data sets to allow hyperparameter turning and model evaluation\n# * be very careful that no knowledge of the test_set spills into training of the model\n# * note that df_X, train_X, validation_X and test_X are REFERENCES pointing to the same dataset. So a change to df will be visible\n#   in train_data and test_data and vice versa! This is handy, as it allows to perform some actions on the complete data set\n#   at once, but also something to continuously be aware of!!\ntrain_X = df_X[:int(df_len * 0.6)]\nvalidation_X = df_X[int(df_len * 0.6):int(df_len * 0.8)]\ntest_X = df_X[int(df_len * 0.8):]\ntrain_y = df_y[:int(df_len * 0.6)]\nvalidation_y = df_y[int(df_len * 0.6):int(df_len * 0.8)]\ntest_y = df_y[int(df_len * 0.8):]\nprint('length train_X:', len(train_X))\nprint('length validation_X:', len(validation_X))\nprint('length test_X:', len(test_X))\nprint('length test_y:', len(test_y))\n\n# add kaggle_competition_data to df_X\ndf_X = pd.concat([df_X, kaggle_competition_data], axis=0)\n\n# impute train_data and test_data as random forest alg can't handle missing values\n# remember from above, feature Age (numerical), Cabin (categorical) and Embarked (categorical) and Fare (numerical) contain missing values\nprint('\\n************************* df_X.info() before imputation:')\nprint(df_X.info())\ntrain_X_age_mean = train_X['Age'].mean()\ndf_X['Age'].fillna(train_X_age_mean, inplace=True)  # use the mean of train data on validation_data and test_data to avoid leakage!\ndf_X['Cabin'].fillna('VAL', inplace=True)\ndf_X['Embarked'].fillna('EM', inplace=True)\ntrain_X_fare_mean = train_X['Fare'].mean()\ndf_X['Fare'].fillna(train_X_fare_mean, inplace=True)  # use the mean of train data on validation_data and test_data to avoid leakage!\nprint('\\n************************* df_X.info() after imputation:')\nprint(df_X.info())\n\n# hot encode categorical features as random forest cannot handle strings\n# * also ordinal categorical features, although of type numerical, need to be one hot encoded as well, \n#   as the distance between the numbers provide wrong information to the training algorithm, affecting the model we train\n# * note that get_dummies encodes a missing value into 0,0,0 instead of nan,nan,nan; that's why we perform imputation \n#   *before* one hot encoding\n# * what about the features 'Ticket' and 'Cabin'?? \ncategorical_features = ['Pclass', 'Sex', 'Embarked']\ndf_X = pd.get_dummies(df_X, columns=categorical_features)\nprint('\\n************************* df_X.info() after one hot encoding:')\nprint(df_X.info())\nprint('\\n************************* train_X.info() after one hot encoding:')\nprint(train_X.info())\n# we see that the changes to df_X are not visible from train_X, so do split again\ntrain_X = df_X[:int(df_len * 0.6)]\nvalidation_X = df_X[int(df_len * 0.6):int(df_len * 0.8)]\ntest_X = df_X[int(df_len * 0.8):df_len]\nkaggle_competition_data = df_X[df_len:]\nprint('\\n************************* train_X.info() after one hot encoding and after resplit:')\nprint(train_X.info())\n\n\n##### 5. modeling: feature selection\n\n# Based on the data exploration, let's skip the features 'PassengerId', 'Name', 'Ticket', 'Cabin'\nfeatures = train_X.columns\nfeatures = [f for f in features if f not in ['PassengerId', 'Name', 'Ticket', 'Cabin']]\ntrain_X = train_X[features]\nvalidation_X = validation_X[features]\ntest_X = test_X[features]\ntrain_X = train_X[features]\nkaggle_competition_X = kaggle_competition_data[features]  # renamed to kaggle_competition_X as we need kaggle_competition_data later\n\n##### 6. modeling: model training\n# 6a. algorithm selection\n# 6b. hyperparameter tuning\n\n# algorithm selection - random forest is flexible, easy to use with great result most of the time\n# hyperparameter tuning - for random forest some hyperparameters are n_estimators=100, max_depth=5. A validation data set is used for this.\n# Question: why don't we use the test data to perform hyperparameter tuning? Answer: because otherwise the model might become \n# optimized for the test data; this is not good as the model must remain fully independent of the test data to see how well\n# the model *generalizes* (== works well on unseen data).\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfor n_estimators in [10, 100, 500]:\n    for max_depth in [1, 5, 50]:\n        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=1)\n        model.fit(train_X, train_y)\n        predictions = model.predict(validation_X)\n        print('accuracy_score of prediction on validation data, using n_estimators =', n_estimators, 'and max_depth =', max_depth, ':', accuracy_score(validation_y, predictions))\n# using n_estimators = 500 and max_depth = 50 gives highest accuracy on the validation data (several attempts), so we'll use these values\nmodel = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=1)\nmodel.fit(train_X, train_y)\n\n##### 7. modeling: model evaluation\n# 7a. choosing evaluation metric: in this case, Kaggle chooses the evaluation metric \"accuracy\" for us\n\n# evaluating against the *TRAIN_DATA*\n# gives an impression of the BIAS\/VARIANCE of the model: to what extent the model is able to fit the train_data\npredictions = model.predict(train_X)\nprint('accuracy_score of prediction on train_data', accuracy_score(train_y, predictions))\n\n# evaluating against the *TEST_DATA*\n# shows how well the model GENERALIZES, ability to predict using unseen data\npredictions = model.predict(test_X)\nprint('accuracy_score of prediction on test_data', accuracy_score(test_y, predictions))\n\n\n##### 8. deployment of the model\n# will be done in a separate hands-on workshop\n# very important, but easily forgotten, ALL data transformations that are part of data preparation (e.g. feature normalization,\n# imputing, one hot encoding) must be done as well when using the model for predictions, otherwise the predictions will be incorrect.\n\n\n##### 9. Kaggle competition\n# This step is not part of the data science workflow, but is specific to a Kaggle competition\n# 9a. create a submission file. \n\nkaggle_competition_predictions = model.predict(kaggle_competition_X)\n\noutput = pd.DataFrame({'PassengerId': kaggle_competition_data.PassengerId, 'Survived': kaggle_competition_predictions})  # kaggle_competition_X has no feature PassengerId any more\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n# 9b. you upload the submission file to the Kaggle competition using the Kaggle GUI. You'll find the submission file in the right sidebar and then\n#     under 'data'. After submission, your result will show up in the leader board.\n","c6f2d6bd":"# The Kaggle eco-system\n\nKaggle is owned by Google. Kaggle is so worldwide known, that a good ranking on Kaggle might be as useful as your bachelor degree.\n1. Create an account on Kaggle.\n2. Search a competition and join.\n3. You can start with an empty Jupyter notebook (called a kernel in Kaggle lingo) or start with a copy of somebody else's notebook. With every\n   competition there are many, many notebooks from other that you can use for inspiration.\n4. With the \"Share\" button you decide whether others can see your code.\n5. You can now add or modify the source code and run the notebook to see the result. Note that \"run\" does not clear variables from the previous run.\n6. Your notebook is a draft and edits are saved automatically (equivalent in git to locally saving your files on your laptop without committing).\n7. The notebook runs in the Google cloud and uses fast GPU's (probably faster than your laptop). So it is also a way to get free fast CPU time.\n   Also the data files remain in the Google cloud and do not end up on your laptop.\n8. If you're happy with your result, you can make a submission, which will show up on the leaderboard. To do this:\n  * Save this version of the source code, so that you can look back to it later, by pressing \"Save Version\" (Save & Run All)\n    to commit this version. This version becomes part of you version history (like a git commit). If your notebook as set to\n    \"public\", this committed version will be the version that is visible by others.\n  * Click on the number of the version. A new window appears. Click on the ellipsis of the last version, choose \"Submit to\n    Competition\" and submit the output. You'll show up on the leaderboard.\n10. Improve your score by improving your code (new iteration). This could be a lead to start: https:\/\/www.kaggle.com\/eraaz1\/a-comprehensive-guide-to-titanic-machine-learning\n","fd71d497":"![Data Science Lifecycle](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/team-data-science-process\/media\/overview\/tdsp-lifecycle2.png)\n\nPlease note that this is not a linear process. There will be iterations of the whole process but also of parts of the process.\n\nSteps of the data science lifecycle:\n1. problem description and objective\n\n2. data acquisition\n  * early negotiating data access with stakeholders too reduce risk\n  * storing the data securely and according to privacy regulations (GDPR)\n  * spend your time wisely. More data samples? More features? Artificially augment data?\n\n3. data exploration using descriptive statistics and visualization\n\n  some options:\n  * find out for each feature: data type\n  * find out for each feature: numerical, nominal categorical, ordinal categorical\n  * heat map to find correlations between features and between features and target variable\n\n4. data preprocessing \n  * dealing with mising data\n  * encode categorical features as dummy variables\n\n5. modeling: feature selection\n\n6. modeling: model training\n  * algorithm selection\n  * hyperparameter tuning\n\n7. modeling: model evaluation\n  * choosing evaluation metric\n  \n8. deployment of the model\n  * appify the model\n  * continuous monitoring of model performance\n"}}