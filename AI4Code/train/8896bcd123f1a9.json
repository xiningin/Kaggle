{"cell_type":{"6458373c":"code","0410ddfc":"code","bf0c424a":"code","eac1aa67":"code","2f79d769":"code","f89840b5":"code","e65fb48d":"code","a7a84332":"code","47a5d305":"code","e2b128aa":"code","db9d0aee":"code","78293318":"code","c43411e4":"code","e68c6151":"code","0a2951bf":"code","17ce0684":"code","1841611a":"code","200242e3":"code","d88b82ac":"code","c4d11c18":"code","8975e64d":"code","dd2a63fd":"code","0bb3584a":"code","088217fd":"code","54e18da7":"code","14aa673b":"code","3fbe7c7f":"code","e35b8f32":"code","78cccee1":"code","e66d4fe3":"code","b884eb75":"code","bf54fba0":"code","938e1d53":"code","c6fd1505":"code","c64fd7aa":"code","7a9920e3":"code","46055a15":"code","6d8c4045":"code","67f74d5b":"code","59d4bc90":"markdown","b4f9ee60":"markdown","355e6317":"markdown","7f30086c":"markdown","5eaeca8e":"markdown","bc6a1f2a":"markdown","200b4bd6":"markdown","ca5fc5dd":"markdown","f4195ec7":"markdown","82667f38":"markdown","19f5df59":"markdown","65f8c134":"markdown","86bccaa8":"markdown","15125e6d":"markdown"},"source":{"6458373c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0410ddfc":"import matplotlib.pyplot as plt\nimport seaborn as sns","bf0c424a":"from keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences","eac1aa67":"from keras import Sequential\n\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D\nfrom keras.layers import SpatialDropout1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers import Dense\n\nfrom keras.callbacks import EarlyStopping","2f79d769":"from sklearn.metrics import confusion_matrix","f89840b5":"PATH = '\/kaggle\/input\/sms-spam-collection-dataset\/'\n!ls $PATH","e65fb48d":"sms_data = pd.read_csv(PATH+'spam.csv', encoding='latin_1')\nsms_data","a7a84332":"# checking for missing values\n\nsms_data.isna().sum()","47a5d305":"# dropping (almost) empty columns as not important\n\ncols_to_drop = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\nsms_data.drop(columns=cols_to_drop, inplace=True)\nsms_data","e2b128aa":"# renaming feature and target columns\n\nfeat_name = 'sms_text'\ntarget_name = 'spam'\nsms_data.columns = [target_name, feat_name]\nsms_data","db9d0aee":"sms_data.isna().sum()","78293318":"# checking (binary) target distribution\n\nsms_data['spam'].value_counts()","c43411e4":"# checking if all sms texts are unique\n\nlen(sms_data['sms_text'].unique()) == len(sms_data['sms_text'])","e68c6151":"# fitting Tokenizer on the \"sms_text\" corpus\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sms_data['sms_text'])","0a2951bf":"# showing learned vocabulary with indices\n\n# tokenizer.word_index","17ce0684":"num_tokens = len(tokenizer.word_index)\nprint('Encoded \"sms_text\" corpus with {} token indices'\n      .format(num_tokens))","1841611a":"# showing a few sample encodings\n\ntokenizer.texts_to_sequences(['we are your friends', \n                              'nothing last forever',                               \n                              'how do you feel today'])","200242e3":"# showing example reverse encoding\n\ntokenizer.sequences_to_texts([[1, 86, 3], [49, 22, 3]])","d88b82ac":"# encoding whole \"sms_text\" data\nsequences = tokenizer.texts_to_sequences(sms_data['sms_text'])\n\n# padding sequences for equal length\nsequences = pad_sequences(sequences)\n\nnum_seq = sequences.shape[0]\nlen_seq = sequences.shape[1]\n\nprint('Encoded {} sequences and padded for equal length of {} tokens'\n      .format(num_seq, len_seq))","c4d11c18":"# encoding (binary) target variable\n\nsms_data['target'] = [1 if is_spam == 'spam' else 0 for is_spam in sms_data['spam']]\nsms_data.head(20)","8975e64d":"# checking if target and feature lengths match\n\nsms_data['target'].shape[0] == sequences.shape[0]","dd2a63fd":"# embedding layer parameters\n\ninput_dim = num_tokens + 1\noutput_dim = 64\ninput_length = len_seq\n\n# convolutional (1D) layer parameters\nfilters = 256\nkernel_size = 3","0bb3584a":"optimizer = 'adam'\nloss = 'binary_crossentropy'\nmetrics = ['accuracy']","088217fd":"batch_size = 128\nepochs = 10\n\n# setting up the \"EarlyStopping\" callback\nearly_stop = EarlyStopping(monitor='val_loss', \n                           min_delta=0, \n                           patience=3, \n                           verbose=True, \n                           mode='auto', \n                           baseline=None, \n                           restore_best_weights=False)\n\ncallbacks = [early_stop]\n\nvalidation_split = 0.20\n\n# setting class weights for the loss function to adjust for class imbalance\n# 'spam' is set to weight 8x more\nclass_weight = {0: 1, 1: 8}","54e18da7":"model = Sequential()\n\nmodel.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\nmodel.add(Conv1D(filters=filters, kernel_size=kernel_size))\nmodel.add(SpatialDropout1D(rate=0.25))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","14aa673b":"model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","3fbe7c7f":"# extracting target to a single array for simplicity\n\ntarget = sms_data['target'].values\ntarget","e35b8f32":"# training model with validation and early stopping\n\nmodel.fit(x=sequences, y=target, \n          batch_size=batch_size, epochs=epochs, \n          verbose=True, callbacks=callbacks, \n          validation_split=validation_split, \n          class_weight=class_weight)","78cccee1":"# showing history of 'accuracy'\n\nplt.figure()\nplt.plot(model.history.history['accuracy'], label='TRAIN ACC')\nplt.plot(model.history.history['val_accuracy'], label='VAL ACC')\nplt.legend()\nplt.show()","e66d4fe3":"# showing history of 'loss'\n\nplt.figure()\nplt.plot(model.history.history['loss'], label='TRAIN LOSS')\nplt.plot(model.history.history['val_loss'], label='VAL LOSS')\nplt.legend()\nplt.show()","b884eb75":"# making predictions for training sequences (in-sample check)\n\npred = model.predict_classes(sequences)\npred.shape","bf54fba0":"# showing confusion matrix\n\ncm = confusion_matrix(y_true=target, y_pred=pred)\ncm = pd.DataFrame(cm)\ncm","938e1d53":"# plotting the confusion matrix heatmap\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True)","c6fd1505":"# setting the optimal number of epochs\n\nepochs = early_stop.stopped_epoch + 1","c64fd7aa":"model.fit(x=sequences, y=target, \n          batch_size=batch_size, epochs=epochs, \n          verbose=True, \n          class_weight=class_weight)","7a9920e3":"\"\"\"\nhelper function: check if a SMS text provided would be classified as spam or not\nargument: <string> with SMS text to be checked\nif no argument provided, read the user's input\n\"\"\"\n\ndef check_if_spam(sms=None):\n\n    # read user's input if no argument provided\n    if sms is None:\n        sms = input('Enter SMS text: ')\n    \n    # tokenize the SMS text and pad sequence to match training sequences length\n    sms = [sms,]\n    sequence = tokenizer.texts_to_sequences(sms)\n    sequence = pad_sequences(sequence, maxlen=len_seq)\n    \n    # predict class and give feedback\n    pred_class = model.predict_classes(sequence)\n    is_spam = 'This is SPAM !!!' if pred_class == 1 else 'This is not spam.'\n        \n    return is_spam","46055a15":"my_sample = ['Final chance to win free tickets. Call now!', \n             'Suspicious activity detected. Follow this link to change password immediately.',\n             'Get over here and call me tonite. Only 2 USD for minute.',\n             'What are you waiting for! These are final days of our xmass promo deals.',\n             'We have new offers for you. Visit our webpage and see.',\n             'Binary FX options trading and 100 USD on your account. Hurry up.',\n             'Huge discounts this weekend. Check this site to learn more.',\n             'You can also earn easy money. Call us now.',\n             'Congratulations! to claim your reward you must reply immediately',\n             'For our database update we need a contact from you. Call us at.'\n            ]\n\nfor text in my_sample:\n    print('\\nChecking:     ', text)\n    print(check_if_spam(text))","6d8c4045":"# call the 'check_if_spam' function with no arguments to provide custom text\n# uncomment to see in action\n\n# check_if_spam()","67f74d5b":"# helper script to show random \"spam message\"\n\nspams = sms_data[sms_data['spam'] == 'spam']\nidx = np.random.randint(len(spams))\nspam = spams.iloc[idx]['sms_text']\nprint(spam)","59d4bc90":"### Setup & Imports","b4f9ee60":"### Data loading & inspection","355e6317":"##### setting layers parameters","7f30086c":"#### Re-training the model on full train data","5eaeca8e":"### Data preparation","bc6a1f2a":"#### Model evaluation","200b4bd6":"#### Learning history","ca5fc5dd":"##### defining model architecture","f4195ec7":"##### setting training parameters","82667f38":"### Predicting","19f5df59":"##### setting up the learning process","65f8c134":"This notebook contains a simple spam classifier trained on the SMS Spam Collection Dataset (data source: https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset).\n\nA very simple neural network architecture is used: just one 1D-convolutional layer, preceded by initial embedding layer. \nEmbedding weights are learned from the SMS texts corpus as a part of model training, i.e. no pre-trained embeddings are employed.\n\nImbalance of classes (only 747 instances of \"spam\") is compensated by setting custom class weights for the training loss function.\n\nOnce trained, the model can be used for inference, i.e. predicting whether a particular SMS would be classified as \"spam\" or not. For test purposes, I handcrafted a bunch of messages which I would definitely not want to appear on my phone. A helper function 'check_if_spam' can be used to check for any other message (try it yourself...). Notice that due to very small size of the training sample, model predictions frequently run counterintuitive. More precisely, plenty of 'suspicious' texts are classified as non-spam. \n\n\nI took an inspiration for this project from the book: \nGULLI, KAPOOR, PAL [2019]: Deep Learning with TensorFlow 2 and Keras - Second Edition, Packt Publishing.","86bccaa8":"### Modelling","15125e6d":"#### model architecture: \n\none-layer 1-dimensional Convolutional Neural Network with initial embedding layer\n\nembedding weights are learned 'from scratch' as a part of model training"}}