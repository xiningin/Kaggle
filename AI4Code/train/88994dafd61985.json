{"cell_type":{"0e90a7bf":"code","ee041712":"code","a1898973":"code","0ef63690":"code","6c3a5ba8":"code","c1443ae4":"code","c9a14053":"code","c9edc0a4":"code","fdebeef4":"code","24215e29":"code","e85ce63e":"code","67e3aa36":"code","863b2afd":"code","a25d3189":"code","403f4c3f":"markdown","c49a02d5":"markdown","efb6469a":"markdown","086f3f65":"markdown","9cc6b680":"markdown","d68c2340":"markdown","a84f81a3":"markdown","84f825f6":"markdown"},"source":{"0e90a7bf":"import os\nimport io\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport cudf\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","ee041712":"%%time\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv').set_index(\"id\")\ntest = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv').set_index(\"id\")\nsample_submission = cudf.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\").to_pandas()","a1898973":"feature_cols = test.columns.tolist()\n\ncnt_features = []\ncat_features = []\n\nfor col in feature_cols:\n    if train[col].dtype=='float64':\n        cnt_features.append(col)\n    else:\n        cat_features.append(col)\n        \n        \nstart_mem = (train.memory_usage(deep=True) \/ 1024 ** 2).sum()\n\ntrain[cnt_features] = train[cnt_features].astype('float32')\ntrain[cat_features] = train[cat_features].astype('uint8')\ntest[cnt_features] = test[cnt_features].astype('float32')\ntest[cat_features] = test[cat_features].astype('uint8')\n\nend_mem = (train.memory_usage(deep=True) \/ 1024 ** 2).sum()\n\n\ntrain = train.to_pandas()\ntest = test.to_pandas()","0ef63690":"print(\"Mem. usage decreased from {:.2f} MB to {:.2f} MB ({:.2f}% reduction)\".format(start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))","6c3a5ba8":"%%time\nbins = 128\nn = 0\nbins_list = []\n\nbins_list.append(-np.inf)\nfor i in range(1,bins):\n    n += 1.\/bins\n    bins_list.append(n)\nbins_list.append(np.inf)\n\nlabels = [i for i in range(bins)]\nfor col in cnt_features:\n    train[col] = pd.cut(train[col], bins=bins_list, labels=labels)\n    test[col] = pd.cut(test[col], bins=bins_list, labels=labels)\n    \ntrain.head()","c1443ae4":"train[cnt_features] = train[cnt_features].astype('uint8')\ntest[cnt_features] = test[cnt_features].astype('uint8')","c9a14053":"memory_usage = train.memory_usage(deep=True) \/ 1024 ** 2\nprint(\"Mem. usage decreased from {:.2f} MB to {:.2f} MB ({:.2f}% reduction)\".format(end_mem, memory_usage.sum(), 100 * (end_mem - memory_usage.sum()) \/ end_mem))","c9edc0a4":"x1 = train[cnt_features].values\nx2 = train[cat_features].values\ny  = train['target'].values","fdebeef4":"def get_model():\n    AF = \"relu\"\n    input_1 = layers.Input(shape=(x1.shape[-1]), name=\"continuous\")\n    x_1 = layers.Embedding(input_dim=bins, output_dim=4)(input_1)\n    x_1 = layers.TimeDistributed(layers.Dense(64, activation=AF))(x_1)\n    x_1 = layers.TimeDistributed(layers.Dense(64, activation=AF))(x_1)\n    x_1 = layers.Flatten()(x_1)\n    x_1 = layers.Dense(128, activation=AF)(x_1)\n    x_1 = layers.Dense(128, activation=AF)(x_1)\n    \n    input_2 = layers.Input(shape=x2.shape[-1], name=\"categories\")\n    x_2 = layers.Dense(128, activation=AF)(input_2)\n    x_2 = layers.Dense(128, activation=AF)(x_2)\n\n    x = layers.Concatenate()([x_1,x_2])\n    x = layers.Dense(64, activation=AF)(x)\n    x = layers.Dense(128, activation=AF)(x)\n    output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n\n    model = tf.keras.Model([input_1,input_2], output)\n    return model\n\n\nmodel = get_model()\nmodel.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"AUC\"])\n    \ntf.keras.utils.plot_model(model, show_shapes=True)","24215e29":"cb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1)\ncb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, mode=\"max\", min_lr=0.0001, verbose=1)\n\nhistory = model.fit((x1,x2), \n                    y, \n                    epochs=20, \n                    validation_split=0.2, \n                    batch_size=512, \n                    validation_batch_size=512,\n                    callbacks=[cb_es, cb_lr])","e85ce63e":"e = model.layers[1]\nweights = e.get_weights()[0]\nprint(weights.shape)\n\nwords = [f\"{i} ({np.round(bins_list[i],3)}-{np.round(bins_list[i+1],3)})\" for i in labels]\n\nvecs = io.open('vecs.tsv', 'w', encoding='utf-8')\nmeta = io.open('meta.tsv', 'w', encoding='utf-8')\nfor i in range(bins):\n    vecs.write(words[i] + \"\\n\")\n    meta.write('\\t'.join([str(x) for x in weights[i]]) + \"\\n\")\nvecs.close()\nmeta.close()","67e3aa36":"preds = model.predict((test[cnt_features].values, test[cat_features].values))","863b2afd":"plt.figure(figsize=(15,8))\nsns.histplot(x=preds.reshape(-1), kde=True, color=\"blue\")\nplt.title(\"Predictions Distribution\")\nplt.xlabel(\"Prediction\")\nplt.show()","a25d3189":"sample_submission['target'] = np.squeeze(preds)\nsample_submission.to_csv(\"submission.csv\", index=False)","403f4c3f":"# Predict","c49a02d5":"# Plot Predictions","efb6469a":"# Load Data","086f3f65":"# Embeddings Projection","9cc6b680":" You can uppload these two files (`vecs.tsv` and `meta.tsv`) on http:\/\/projector.tensorflow.org\/ to visualize embedding layer","d68c2340":"# Submission","a84f81a3":"# Neural Network Model","84f825f6":"# Categorize Data"}}