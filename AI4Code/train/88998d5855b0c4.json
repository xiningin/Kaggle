{"cell_type":{"71ceaed7":"code","2d57e53d":"code","8f20b70f":"code","b88124bd":"code","bff0f217":"code","44749774":"code","cfb90f25":"code","38fef2c2":"code","040bbb5c":"code","f1188a4d":"code","7c658cb8":"code","283708b6":"code","51895568":"code","b37c1371":"code","8f5132fa":"code","6ffb8ad7":"code","2e8c61e7":"code","a9773b36":"code","376f77b9":"code","fb250ae0":"code","53d56881":"code","d9248535":"code","7a1afba7":"code","f4812a27":"code","e9d9530c":"code","4b35beda":"code","7b969dc4":"code","0a2d4ff3":"code","fa9c3d45":"code","e9b240f6":"code","2affad77":"code","b2286b63":"code","17228021":"code","bd30a482":"code","d62b4d98":"code","50b815bd":"code","fc7a7758":"code","1f8aeaa3":"code","b9e7c176":"code","db3e0265":"code","df3295df":"code","d28f926b":"code","7ca225db":"code","5281050a":"code","25ce465e":"code","97f96e84":"code","1094f7ea":"code","c2a60dca":"code","565a5336":"code","1f87df17":"code","4b8887a8":"code","29cce514":"code","236828cb":"code","8c4b0d2b":"code","7fb25332":"code","be29c3d8":"code","433da158":"code","d73ccdff":"code","69ae0cac":"code","14202fac":"code","54a1e5fa":"code","af97abdf":"code","8e713b9b":"code","48bd69c0":"code","74d4cfdb":"code","7564f5c6":"code","ac340e01":"code","652834b8":"code","a1779d3a":"code","2a7c6f19":"code","1adfff34":"code","7b24bb87":"code","ab214b25":"code","a8502d0a":"code","22206ada":"code","cf8c0c96":"code","68f0f100":"code","1af6a924":"code","33165208":"code","6b7da65a":"code","2dfb8891":"code","9c796e8a":"code","9933b817":"code","92cedbf8":"code","22b3b150":"code","dda8a461":"code","6f13e3ee":"code","5d2b6a8d":"code","cd42655d":"code","f9b8f5a3":"code","a9bffbfe":"code","c6398162":"code","c8d1a6a3":"markdown","a698dfc9":"markdown","7fc18d52":"markdown","78cfbef6":"markdown","d0d425fe":"markdown","feff6231":"markdown","a678ad8d":"markdown","fb9eb573":"markdown","50cd19a2":"markdown","56e41303":"markdown","3dc9c702":"markdown","5885f10b":"markdown","d5b9e997":"markdown","30c89a86":"markdown","ddf3b4d0":"markdown","50eeed79":"markdown","f0e0c95c":"markdown","8384dcb0":"markdown","6342e8eb":"markdown","d7d9da7a":"markdown","8db13a6f":"markdown","621fa2d0":"markdown","0dc40c33":"markdown","6ed6e624":"markdown","e80fe513":"markdown","d66aa7c8":"markdown","f3c4c09f":"markdown","26b61684":"markdown","208ce0f9":"markdown","06cce94f":"markdown","e49a6d58":"markdown","26d27727":"markdown","c70fdeb8":"markdown","565bf7c8":"markdown","8655ac49":"markdown","79afc4aa":"markdown","c8f622f1":"markdown","c96c4ad4":"markdown","dbcf66ce":"markdown","fffc1bae":"markdown","a328f8ca":"markdown","3ea8af5c":"markdown","48f7ea73":"markdown","8e43dd91":"markdown","8b098edc":"markdown","1aacab5f":"markdown","98458db4":"markdown","0b33b840":"markdown","7ac6930d":"markdown","8cee3701":"markdown","8286a6c3":"markdown","f942cf31":"markdown","9617cef5":"markdown","ac0161e1":"markdown","491c0946":"markdown","ae5c5be6":"markdown","44eec08e":"markdown","d49a76d4":"markdown","90c64bc1":"markdown","15e302a2":"markdown","1c41f464":"markdown","59065c5a":"markdown","440626be":"markdown","e72aaf67":"markdown","0f61666d":"markdown","7c8e2787":"markdown","b462fbe4":"markdown","b517b460":"markdown","05494aaf":"markdown","8d3aa04c":"markdown","346e7d8f":"markdown","80141111":"markdown","ffd1ee99":"markdown","a8e65200":"markdown","a2dcaddf":"markdown","9f32db26":"markdown","fb26813e":"markdown","9513ceee":"markdown"},"source":{"71ceaed7":"#Libraries required\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","2d57e53d":"data = pd.read_csv('..\/input\/weatherAUS.csv')","8f20b70f":"# Let's see how our data looks like\nprint('Weather dataframe dimension: ',data.shape)\ndata.describe()","b88124bd":"# We can see that there are lot of NaN values in the dataframe.  \n# Let's check which column has maximum Nan values\nprint(data.count().sort_values())\n\n#Graph to find missing values in the dataframe\nimport missingno\nmissingno.matrix(data, figsize = (30,10))","bff0f217":"data = data.drop(columns = ['Sunshine','Evaporation','Cloud3pm','Cloud9am','Location','Date','RISK_MM'],axis=1)","44749774":"print(data.shape)\ndata.head()","cfb90f25":"def find_missing_values(df,columns):\n    missing_vals = {}\n    df_length = len(df)\n    for column in columns:\n        total_column_values = df[column].value_counts().sum()\n        missing_vals[column] = df_length - total_column_values\n    return missing_vals\n\nmissing_values = find_missing_values(data,data.columns)\nmissing_values","38fef2c2":"data = data.dropna(axis = 'index',how='any')\nprint(data.shape)\n\nmissing_values = find_missing_values(data,data.columns)\nmissing_values","040bbb5c":"final = pd.DataFrame()","f1188a4d":"#Data transformation\n#For the categorical columns, we will change the value 'Yes' and 'No' to '1' and '0' respectively\ndata['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\ndata['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n\n#See unique values and convert them to int using pd.getDummies()\ncategorical_columns = ['WindGustDir', 'WindDir3pm', 'WindDir9am']\nfor col in categorical_columns:\n    print(np.unique(data[col]))\n# transform the categorical columns\nfinal = pd.get_dummies(data, columns=categorical_columns)\n","7c658cb8":"final.head()","283708b6":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(final)\nfinal = pd.DataFrame(scaler.transform(final), index=final.index, columns=final.columns)\nfinal.head()","51895568":"#Now we will just see how many times it rained the next day?\nfig = plt.figure(figsize = (20,3))\nsns.countplot(y='RainTomorrow', data=final);\nprint(final.RainTomorrow.value_counts())","b37c1371":"missing_values['MinTemp']","8f5132fa":"data.MinTemp.value_counts()","6ffb8ad7":"final_bin = pd.DataFrame()\nfinal_bin['RainTomorrow'] = final['RainTomorrow']","2e8c61e7":"final_bin['MinTemp'] = pd.cut(data['MinTemp'],bins = 5) #discretising the float numbers into categorical","a9773b36":"final_bin.MinTemp.value_counts()","376f77b9":"final.head()","fb250ae0":"def plot_count_dist(df,label_column,target_column,figsize=(20,5)):\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1,2,1)\n        sns.countplot(y=target_column, data = df);\n        plt.subplot(1,2,2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column],\n                    kde_kws={\"label\" : \"Yes\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column],\n                    kde_kws={\"label\" : \"No\"});\n    ","53d56881":"#Calling the function above we will visualise the MinTemp bin counts as well as the MinTemp distribution versus RainTomorrow\nplot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'MinTemp', figsize = (20,10))","d9248535":"# Let's cross check the missing values\nmissing_values['MaxTemp']","7a1afba7":"data['MaxTemp'].value_counts()","f4812a27":"final_bin['MaxTemp'] = pd.cut(data['MaxTemp'],bins = 5) #discretising the float numbers into categorical","e9d9530c":"final_bin['MaxTemp'].value_counts()","4b35beda":"final.head()","7b969dc4":"#Calling the function above we will visualise the MaxTemp bin counts as well as the MaxTemp distribution versus RainTomorrow\nplot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'MaxTemp', figsize = (20,10))","0a2d4ff3":"# Let's cross check the missing values\nmissing_values['Rainfall']","fa9c3d45":"data['Rainfall'].value_counts()","e9b240f6":"print(\"There are {} unique minimum temperature values.\".format(len(data.Rainfall.unique())))","2affad77":"final_bin['Rainfall'] = pd.cut(data['Rainfall'],bins = 5) #discretising the float numbers into categorical","b2286b63":"final_bin['Rainfall'].value_counts()","17228021":"final.head()","bd30a482":"#Calling the function above we will visualise the MaxTemp bin counts as well as the MaxTemp distribution versus RainTomorrow\nplot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'Rainfall', figsize = (20,10))","d62b4d98":"missing_values['WindGustDir']","50b815bd":"WindGustDir_table = pd.crosstab(index=data[\"WindGustDir\"], columns=data[\"RainTomorrow\"])\nWindGustDir_table","fc7a7758":"WindGustDir_table.plot(kind=\"bar\", figsize=(15,8),stacked=False)","1f8aeaa3":"missing_values['WindGustSpeed']","b9e7c176":"plot_count_dist(df= final, label_column = 'RainTomorrow', target_column = 'WindGustSpeed', figsize = (20,10))","db3e0265":"missing_values['WindDir9am']","df3295df":"WindDir9am_table = pd.crosstab(index=data[\"WindDir9am\"], columns=data[\"RainTomorrow\"])\nWindDir9am_table","d28f926b":"WindDir9am_table.plot(kind=\"bar\", figsize=(15,8),stacked=False)","7ca225db":"missing_values['WindDir3pm']","5281050a":"WindDir3pm_table = pd.crosstab(index=data[\"WindDir3pm\"], columns=data[\"RainTomorrow\"])\nWindDir3pm_table","25ce465e":"WindDir3pm_table.plot(kind=\"bar\", figsize=(15,8),stacked=False)","97f96e84":"missing_values['WindSpeed9am']","1094f7ea":"plot_count_dist(df= final, label_column = 'RainTomorrow', target_column = 'WindSpeed9am', figsize = (20,10))","c2a60dca":"missing_values['WindSpeed3pm']","565a5336":"plot_count_dist(df= final, label_column = 'RainTomorrow', target_column = 'WindSpeed3pm', figsize = (20,10))","1f87df17":"missing_values['Humidity9am']","4b8887a8":"plot_count_dist(df= final, label_column = 'RainTomorrow', target_column = 'Humidity9am', figsize = (20,10))","29cce514":"missing_values['Humidity3pm']","236828cb":"plot_count_dist(df= final, label_column = 'RainTomorrow', target_column = 'Humidity3pm', figsize = (20,10))","8c4b0d2b":"missing_values['Pressure9am']","7fb25332":"final['Pressure9am'].value_counts()","be29c3d8":"final_bin['Pressure9am'] = pd.cut(data['Pressure9am'],bins = 5) #discretising the float numbers into categorical","433da158":"final_bin['Pressure9am'].value_counts()","d73ccdff":"final.head()","69ae0cac":"plot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'Pressure9am', figsize = (20,10))","14202fac":"missing_values['Pressure3pm']","54a1e5fa":"final['Pressure3pm'].value_counts()","af97abdf":"final_bin['Pressure3pm'] = pd.cut(data['Pressure3pm'],bins = 5) #discretising the float numbers into categorical\n","8e713b9b":"final_bin['Pressure3pm'].value_counts()","48bd69c0":"final.head()","74d4cfdb":"plot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'Pressure3pm', figsize = (20,10))","7564f5c6":"missing_values['Temp9am']","ac340e01":"final['Temp9am'].value_counts()","652834b8":"final_bin['Temp9am'] = pd.cut(data['Temp9am'],bins = 5) #discretising the float numbers into categorical","a1779d3a":"final_bin['Temp9am'].value_counts()","2a7c6f19":"final.head()","1adfff34":"plot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'Temp9am', figsize = (20,10))","7b24bb87":"missing_values['Temp3pm']","ab214b25":"final['Temp3pm'].value_counts()","a8502d0a":"final_bin['Temp3pm'] = pd.cut(data['Temp3pm'],bins = 5) #discretising the float numbers into categorical\n","22206ada":"final_bin['Temp3pm'].value_counts()","cf8c0c96":"final.head()","68f0f100":"plot_count_dist(df= final_bin, label_column = 'RainTomorrow', target_column = 'Temp3pm', figsize = (20,10))","1af6a924":"#Now we will just see how many times it rained the current day?\nfig = plt.figure(figsize = (20,3))\nsns.countplot(y='RainToday', data=final);\nprint(final.RainToday.value_counts())","33165208":"final.head()","6b7da65a":"f, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.xticks(rotation=90)","2dfb8891":"final_bin.head()","9c796e8a":"final.shape","9933b817":"#Let's get hold of the independent variables and assign them as X\n\nX = final.loc[:, final.columns != 'RainTomorrow']\ny = final['RainTomorrow']\nX.shape","92cedbf8":"# PCA to find the best number of features based on explained variance for each attribute\n#Fitting the PCA algorithm with our Data\nfrom sklearn.decomposition import PCA\npca = PCA().fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('WeatherAUS Dataset Explained Variance')\nplt.show()","22b3b150":"#Using SelectKBest to get the top features!\nfrom sklearn.feature_selection import SelectKBest, chi2\nselector = SelectKBest(chi2, k=40)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)]) #top 40 columns","dda8a461":"X = final[['MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n       'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp3pm',\n       'RainToday', 'WindGustDir_E', 'WindGustDir_ENE', 'WindGustDir_ESE',\n       'WindGustDir_N', 'WindGustDir_NNW', 'WindGustDir_NW', 'WindGustDir_W',\n       'WindGustDir_WNW', 'WindDir3pm_E', 'WindDir3pm_ENE', 'WindDir3pm_ESE',\n       'WindDir3pm_N', 'WindDir3pm_NNW', 'WindDir3pm_NW', 'WindDir3pm_SE',\n       'WindDir3pm_SW', 'WindDir3pm_W', 'WindDir3pm_WNW', 'WindDir9am_E',\n       'WindDir9am_ENE', 'WindDir9am_ESE', 'WindDir9am_N', 'WindDir9am_NNE',\n       'WindDir9am_NNW', 'WindDir9am_NW', 'WindDir9am_SE', 'WindDir9am_SSE',\n       'WindDir9am_W', 'WindDir9am_WNW']] # let's use all 40 features\ny = final[['RainTomorrow']]","6f13e3ee":"#Split the data into train and test data\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)","5d2b6a8d":"from sklearn.metrics import accuracy_score\nimport time\nt0=time.time()\nlogreg = LogisticRegression(random_state=0, class_weight={0:0.3,1:0.7})\nlogreg = logreg.fit(X_train,y_train)\ny_predLR = logreg.predict(X_test)\nscore = accuracy_score(y_test,y_predLR)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)\n\n\n\n","cd42655d":"t0=time.time()\n#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\ndt = DecisionTreeClassifier(random_state=0,class_weight={0:0.3,1:0.7})\ndt.fit(X_train,y_train)\ny_predDT = dt.predict(X_test)\nscore = accuracy_score(y_test,y_predDT)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","f9b8f5a3":"from sklearn.ensemble import RandomForestClassifier\nt0=time.time()\nrf = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0,class_weight={0:0.3,1:0.7})\nrf.fit(X_train,y_train)\ny_predRF = rf.predict(X_test)\nscore = accuracy_score(y_test,y_predRF)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","a9bffbfe":"from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Creating an object of the classifier.\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n\n#Training the classifier.\nbbc.fit(X_train, y_train)\ny_predBBC = bbc.predict(X_test)\nscore = accuracy_score(y_test,y_predBBC)\nprint('Accuracy :',score)\nprint('Time taken :' , time.time()-t0)","c6398162":"from sklearn.metrics import confusion_matrix\npred_models = []\npred_models.append(('LogisticRegression', y_predLR))\npred_models.append(('DecisionTree', y_predDT))\npred_models.append(('RandomForest', y_predRF))\npred_models.append(('BalancedBaggingClassifier', y_predBBC))\n\n\nfor name, pred_model in pred_models:\n    cm = confusion_matrix(y_test, pred_model)\n    #print(cm)\n    plt.figure(figsize = (3,3))\n    sns.heatmap(cm,fmt=\"d\",annot=True,xticklabels=[\"No\",\"Yes\"],yticklabels=[\"No\",\"Yes\"],cbar=False)\n    plt.title(name+\" \"+\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actuals\")\n    plt.show()","c8d1a6a3":"Let us start building our predictive models","a698dfc9":"**CORRELATION MATRIX**","7fc18d52":"**Feature 2: MinTemp**\n\nDescription : Minimum temperature in degree celsius","78cfbef6":"since it is a continuous value, let's do binning and put this to our final_bin dataframe","d0d425fe":"The graph on the left shows the binned histogram for Temp9am and we can see the most frequent temperature being in [14.22,22.88]. The right graph conveys that temperature between 7 and 15 degree Celsius mostly results in RainTomorrow being 'Yes', otherwise results in RainTomorrow being \u2018No\u2019.","feff6231":"From the graph we can know the ratio of cases where the next day is predicted as rainy day or not. From here, it gives a clue that - in future if our machine learning model is predicting more RainTomorrow = 'No' than number of RainTomorrow = 'Yes' with our test data, it means that we should check on the model. It basically gives us information about the bias or balance of targets in our data.\nThe table above the graph presents frequency counts for the binary variable.","a678ad8d":"**FEATURE EXPLORATION**","fb9eb573":"**Feature 11 : Humidity3pm**","50cd19a2":"The left plot shows how many different values does MinTemp has.The right plot means that when MaxTemp is between 0.0 - 0.4, the target RainTomorrow being 'Yes' is more and when MinTemp is greater than 0.4 the target RainTomorrow being 'No' is more.","56e41303":"Now lets see a function to create count and distribution for any variable we want","3dc9c702":"**Feature 15 : Temp3pm**","5885f10b":"Description : The direction of the strongest wind gust in the 24 hours to midnight","d5b9e997":"Description : Atmospheric pressure reduced to mean sea level at 3pm, measured in hpm","30c89a86":"Description : The speed (km\/h) of the strongest wind gust in the 24 hours to midnight","ddf3b4d0":"**Download the data**  \nThe data has been downloaded from kaggle datasets : https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package","50eeed79":"**Feature 5 : WindGustSpeed**","f0e0c95c":"since it is a floating value, let's do binning and put this to our final_bin dataframe","8384dcb0":"since it is a continuous value, let's do binning and put this to our final_bin dataframe","6342e8eb":"**Feature 7 : WindDir3pm**","d7d9da7a":"Description : Temperature at 3pm, measured in degree Celsius","8db13a6f":"**Feature 9 : WindSpeed3pm**","621fa2d0":"Now lets standardise the data","0dc40c33":"The graph on the left provides binned histogram for Humidity9am and the plot on left illustrates that pressure of 0-1012 hpa has more chances of RainTomorrow being 'Yes' and pressure greater the above range means target RainTomorrow being 'No'.","6ed6e624":"From the graph we can know the ratio of cases where the next day is predicted as rainy day or not. The table above the graph presents frequency counts for the binary values 'Yes' and 'No' for RainToday.","e80fe513":"The left plot shows that most of the values for rainfall in mm lies in between  -0.001 to 0.2 and the right plot shows it is obvious that current day's rainfall of nearly '0'mm indicates that mostly there will be no rainfall next day.","d66aa7c8":"**Feature 6 : WindDir9am**","f3c4c09f":"Description : Atmospheric pressure reduced to mean sea level at 9am, measured in hpa","26b61684":"**Feature 3 : MaxTemp**","208ce0f9":"**Feature Selection**\n1. From the above result we can see that the columns **Sunshine,Evaporation,Cloud3pm,Cloud9am** have more Nan or null values, they have less than 60% data, hence we are not including these columns.  \n2. Also, we donot need **Location** column because we are trying to predict whether it will rain or not tommorrow and this analysis is not based on location.  \n3. **Date** column can also be removed since the feature is not required for our prediction model.\n4. We must remove **RISK_MM** feature since here we are trying to predict 'RainTommorrow'. RISK_MM is amount of rainfall in millimeters for the next day. It includes all forms of precipitation that reach the ground, such as rain, drizzle, hail and snow. Since it contains information about the future, and information directly about the target variable, including it would leak the future information to the model. Instead the variable itself can be actually used to determine whether or not it rained to create the binary target. For example, if RISKMM was greater than 0, then the RainTomorrow target variable is equal to Yes. Hence, using it as a predictor to build a model and then testing on this dataset would give the false appearance of a high accuracy.","06cce94f":"The left plot shows the unique values of WindSpeed9am and their. The frequencyright plot means that for WindSpeed9am of 0-20 km\/hr the target RainTomorrow being 'No' is more and having greater than 20 km\/hr, the target RainTomorrow being 'Yes' is more.","e49a6d58":"**Model 1 : Logistic Regression**","26d27727":"since it is a floating value, let's do binning and put this to our final_bin dataframe","c70fdeb8":"**Feature 4 : Rainfall**","565bf7c8":"**Feature 14 : Temp9am**","8655ac49":"**Load the data**  \nLoad the data into the notebook (file : weatherAUS.csv)","79afc4aa":"We can also write a function to track the missing values in each of the columns as below:","c8f622f1":"**Model 2 : Decision Tree**","c96c4ad4":"Below are the insights drawn from correlation matrix:\n*\tMinTemp and MaxTemp are highly correlated with r=0.7  \n*\tHumidity3pm and Humidity 9am are highly correlated with r=0.6  \n*\tTemp9am and MinTemp are highly correlated with r=0.9  \n*\tTemp9am and MaxTemp are highly correlated with r=0.9  \n*\tTemp3pm and MinTemp are highly correlated with r=0.6  \n*\tTemp3pm and MaxTemp are highly correlated with r=0.9  \n*\tAlso, Pressure & temperature and Humidity and temperature were negatively correlated.  ","dbcf66ce":"Now we are done with pre-processing and have a basic ides of what each features are and how they are related to the target variable RainTomorrow.\nLet's just see which are the important features to predict RainTomorrow","fffc1bae":"**Feature 1 : 'RainTommorrow' (Target variable)**  \n\nDescription : Did it rain the next day?  \nValues : 'Yes' , 'No'  \nThis is the dependent variable we want our machine learning model to predict, based on other independent variables  ","a328f8ca":"Now let's see how to deal with missing values or Nan values","3ea8af5c":"The left graph gives the frequencies of unique values for Humidity3pm and right graph clearly draws a line which seperates the Humidity3pm as two ranges where 1. 0-60% - RainTomorrow being 'No' is evident and 2. greater than 60% - RainTomorrow being 'Yes' is evident.","48f7ea73":"**Feature 13 : Pressure3pm**","8e43dd91":"> **CONFUSION MATRICES**","8b098edc":"Description : Humidity at 9am in %","1aacab5f":"The graph on the left shows the binned histogram for Temp9am and we can see the most frequent temperature being in [19.7,28.7]. The right graph conveys that temperature between 0 and 19 degree Celsius mostly results in RainTomorrow being 'Yes'. Temperature grater than 20 degree celsius results in mostly target RainTomorrow being 'No'.","98458db4":"**Model 4 : BalancedBagging Classifier**","0b33b840":"**Feature 5 : WindGustDir**","7ac6930d":"Description : Wind speed (km\/hr) averaged over 10 minutes prior to 3pm","8cee3701":"**Feature 12 : Pressure9am**","8286a6c3":"Description : Direction of the wind at 9am","f942cf31":"Description : The maximum temperature in degrees celsius","9617cef5":"Description : The amount of Rainfall in mm recorded for the day","ac0161e1":"We have already transformed this categorical variable using dummies, for the system to understand. So let's go ahead and visualise the data","491c0946":"The left plot shows the frequencies of unique humidity values and right graph means that if Humidity9am is between 0-70 %, then target RainTomorrow will be 'No' and if Humidity9am is more than 70 % the current day then RainTomorrow being 'Yes' is more. And also, we can see that at 100% humidity the case of RainTomorrow = 'Yes' is twice as that of case where RainTomorrow = 'No'.","ae5c5be6":"**Feature 8 : WindSpeed9am**","44eec08e":"since it is a continuous value, let's do binning and put this to our final_bin datafarme","d49a76d4":"Description : Direction of the wind at 3pm","90c64bc1":"The graph on the left provides histogram with bins for Pressure9am and the plot on left illustrates that pressure of 0-1015 hpa has more chances of RainTomorrow being 'Yes' and pressure greater the above range means target RainTomorrow being 'No'.","15e302a2":"For RainTomorrow being \u2018Yes\u2019 the wind at evening mostly blows in North, West and West-Northwest and for RainTomorrow being \u2018No\u2019, the wind mostly blows in direction of South, SouthEast, West-Southwest.","1c41f464":"Description : Wind speed (km\/hr) averaged over 10 minutes prior to 9am","59065c5a":"Description : Humidity at 3pm in %","440626be":"The left plot shows how many different values does MinTemp has. The right plot means that when MinTemp is between 0.0 - 0.3, the target RainTomorrow being 'No' is more and when MinTemp is greater than 0.6 the target RainTomorrow being 'Yes' is more. ","e72aaf67":"**Feature 10 : Humidity9am**","0f61666d":"Description : Temperature at 9am, measured in degrees Celsius","7c8e2787":"since it is a continuous value, let's do binning and put this to our final_bin dataframe.","b462fbe4":"For RainTomorrow being \u2018No\u2019, the wind at morning mostly blows in direction of East,Southeast and South South-east, for RainTomorrow being \u2018Yes\u2019, the wind mostly blows in North North-West,North and North North-East.","b517b460":"We have already transformed this categorical variable using dummies, for the system to understand. So let's go ahead and visualise the data","05494aaf":"The insights convey the chances of being RainTomorrow = 'NO' is when wind blows in direction of East,Southeast and South Southeast mostly.and the chances of being RainTomorrow = 'Yes' is when it is North, West and Northwest.","8d3aa04c":"Description : Precipitation of current day. Boolean: 1 if precipitation (in mm) in the 24 hours to 9am exceeds 1mm, otherwise 0","346e7d8f":"DATA TRANSFORMATION","80141111":"**Feature 16 : RainToday**","ffd1ee99":"The left plot provides unique values and its numbers of WindGustSpeed whereas right plot explains if there is a WindGustSpeed of 0-50 the currentday, then the next day mostly it will not rain. Otherwisw when WindGustSpeed is greater than 50 it says that the next day can see rain.","a8e65200":"RESULTS ACHIEVED:\n\n\u2022\tAs discussed about the gaps in previous work, the kernels aimed to measure the performance of machine learning models based on accuracy and most of the model\u2019s recall was about 50%. This meant that when it actually did rain the next day, the model was only right 50% of the time.\nIn this work, recall of score as high as 68.26 % has been achieved which means that the predictions we make when it actually did rain next day are correct 68% of the time. We can see an increase of recall score by **18.26 %**.\n\n\u2022\tThe average accuracy obtained in the previous kernels are 85%. In this work, even though the recall score was increased, it is managed to gain an accuracy of score 82.66 %.\n\n\u2022\tA better understanding of data has been achieved by exploratory analysis of each and every variable (all 17 features). Also, it is very clear about the relationship and variance of each of the independent variables which helps to predict the dependent variable.\n\n\u2022\tFor Dimensionality reduction, to find out the number of best features to be included in the machine learning model was found using Principal Component Analysis (PCA), in order to get best performance. From the graph obtained, it is evident that out of 62 features (after feature engineering) 40 features had to be used to explain 90% of the variance in the target variable.\n\n\u2022\tLogistic regression outstood as the best predictive model to predict the class labels for this dataset having the best accuracy and recall scores. It suppressed the power of ensemble models by predicting the class labels in as less as 1.34 sec when compared to the Bagging classifier which took 16 sec to predict the target class. \n","a2dcaddf":"We have already transformed this categorical variable using dummies, for the system to understand. So let's go ahead and visualise the data","9f32db26":"The left graph shows the frequencies of unique values in WindSpeed3pm and right graph tells that if the WindSpeed3pm reaches 20 km\/hr then the target RainTomorrow being 'Yes' is more, greater than 25 km\/hr means the target RainTomorrow being \u2018No\u2019 has more chance.","fb26813e":"**Model 3 : RandomForest**","9513ceee":"since it is a continuous value, let's do binning and put this to a sepearate dataframe final_bin for visualization purpose"}}