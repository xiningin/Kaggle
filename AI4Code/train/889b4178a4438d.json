{"cell_type":{"e363493f":"code","546dc41b":"code","bd73ddd0":"code","41abaee3":"code","d51242d8":"code","52aeac92":"code","de84a8fe":"code","8c1fa01d":"code","95e09df4":"code","709a6b12":"code","833dbfec":"code","0e368d27":"code","443c0a65":"code","499985d5":"code","0eb48811":"code","891ab2d5":"code","c2fa02b9":"code","b40cce51":"code","429e3b4f":"code","60f8dbd3":"code","b3bdba29":"code","cc27ec7c":"code","f275712d":"code","35243f1d":"code","5dbf0f4d":"code","dc8e12e3":"code","acd21615":"code","f99b4567":"markdown","5d086c75":"markdown","809413b7":"markdown","e2dbae5d":"markdown","74389de5":"markdown","f901c8f5":"markdown","9519d5b3":"markdown","e79d054e":"markdown","00b7de43":"markdown","051dc2cd":"markdown"},"source":{"e363493f":"# shutil.rmtree(\".\/train\")\n# shutil.rmtree(\".\/val\")","546dc41b":"!pip install -q imutils","bd73ddd0":"# import the necessary packages\nimport os\nimport time\nimport shutil\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom imutils import paths\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.models import resnet50\nfrom torch.utils.data import DataLoader","41abaee3":"if not os.path.exists(\"output\"):  \n    # Create a new directory because it does not exist \n    os.mkdir(\"output\")\n    print(\"The new directory is created!\")","d51242d8":"# define path to the original dataset and base path to the dataset splits\nDATA_PATH = \"..\/input\/flowers-recognition\/flowers\"\nBASE_PATH = \".\/\"\n\n# define validation split and paths to separate train and validation\n# splits\nVAL_SPLIT = 0.1\nTRAIN = os.path.join(BASE_PATH, \"train\")\nVAL = os.path.join(BASE_PATH, \"val\")\n\n# specify ImageNet mean and standard deviation and image size\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\nIMAGE_SIZE = 224\n\n# determine the device to be used for training and evaluation\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# specify training hyperparameters\nFEATURE_EXTRACTION_BATCH_SIZE = 256\nFINETUNE_BATCH_SIZE = 64\nPRED_BATCH_SIZE = 4\nEPOCHS = 20\nLR = 0.001\nLR_FINETUNE = 0.0005\n\n# define paths to store training plots and trained model\nWARMUP_PLOT = os.path.join(\"output\", \"warmup.png\")\nFINETUNE_PLOT = os.path.join(\"output\", \"finetune.png\")\nWARMUP_MODEL = os.path.join(\"output\", \"warmup_model.pth\")\nFINETUNE_MODEL = os.path.join(\"output\", \"finetune_model.pth\")","52aeac92":"def get_dataloader(rootDir, transforms, batchSize, shuffle=True):\n\t# create a dataset and use it to create a data loader\n\tds = datasets.ImageFolder(root=rootDir, transform=transforms)\n\tloader = DataLoader(\n        ds, \n        batch_size=batchSize,\n\t\tshuffle=shuffle,\n\t\tnum_workers=os.cpu_count(),\n\t\tpin_memory=True if DEVICE == \"cuda\" else False\n    )\n    \n\t# return a tuple of  the dataset and the data loader\n\treturn (ds, loader)","de84a8fe":"def copy_images(imagePaths, folder):\n\t# check if the destination folder exists and if not create it\n\tif not os.path.exists(folder):\n\t\tos.makedirs(folder)\n        \n\t# loop over the image paths\n\tfor path in imagePaths:\n\t\t# grab image name and its label from the path and create a placeholder corresponding to the separate label folder\n\t\timageName = path.split(os.path.sep)[-1]\n\t\tlabel = path.split(os.path.sep)[-2]\n\t\tlabelFolder = os.path.join(folder, label)\n        \n\t\t# check to see if the label folder exists and if not create it\n\t\tif not os.path.exists(labelFolder):\n\t\t\tos.makedirs(labelFolder)\n            \n\t\t# construct the destination image path and copy the current image to it\n\t\tdestination = os.path.join(labelFolder, imageName)\n\t\tshutil.copy(path, destination)","8c1fa01d":"# load all the image paths and randomly shuffle them\nprint(\"[INFO] loading image paths...\")\nimagePaths = list(paths.list_images(DATA_PATH))\nnp.random.shuffle(imagePaths)\n\n# generate training and validation paths\nvalPathsLen = int(len(imagePaths) * VAL_SPLIT)\ntrainPathsLen = len(imagePaths) - valPathsLen\ntrainPaths = imagePaths[:trainPathsLen]\nvalPaths = imagePaths[trainPathsLen:]\n\n# copy the training and validation images to their respective directories\nprint(\"[INFO] copying training and validation images...\")\ncopy_images(trainPaths, TRAIN)\ncopy_images(valPaths, VAL)","95e09df4":"# define augmentation pipelines\ntrainTansform = transforms.Compose([\n\ttransforms.RandomResizedCrop(IMAGE_SIZE),\n\ttransforms.RandomHorizontalFlip(),\n\ttransforms.RandomRotation(90),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])\n\nvalTransform = transforms.Compose([\n\ttransforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])","709a6b12":"# create data loaders\n(trainDS, trainLoader) = get_dataloader(TRAIN, transforms=trainTansform, batchSize=FEATURE_EXTRACTION_BATCH_SIZE)\n\n(valDS, valLoader) = get_dataloader(VAL, transforms=valTransform, batchSize=FEATURE_EXTRACTION_BATCH_SIZE, shuffle=False)","833dbfec":"# load up the ResNet50 model\nmodel = resnet50(pretrained=True)\n\n# since we are using the ResNet50 model as a feature extractor we set its parameters to non-trainable (by default they are trainable)\nfor param in model.parameters():\n\tparam.requires_grad = False\n    \n# append a new classification top to our feature extractor and pop it on to the current device\nmodelOutputFeats = model.fc.in_features\nmodel.fc = nn.Linear(modelOutputFeats, len(trainDS.classes))\nmodel = model.to(DEVICE)","0e368d27":"# initialize loss function and optimizer (notice that we are only providing the parameters of the classification top to our optimizer)\nlossFunc = nn.CrossEntropyLoss()\nopt = torch.optim.Adam(model.fc.parameters(), lr=LR)\n\n# calculate steps per epoch for training and validation set\ntrainSteps = len(trainDS) \/\/ FEATURE_EXTRACTION_BATCH_SIZE\nvalSteps = len(valDS) \/\/ FEATURE_EXTRACTION_BATCH_SIZE\n\n# initialize a dictionary to store training history\nH = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}","443c0a65":"# loop over epochs\nprint(\"[INFO] training the network...\")\nstartTime = time.time()\nfor e in tqdm(range(EPOCHS)):\n\t# set the model in training mode\n\tmodel.train()\n    \n\t# initialize the total training and validation loss\n\ttotalTrainLoss = 0\n\ttotalValLoss = 0\n    \n\t# initialize the number of correct predictions in the training and validation step\n\ttrainCorrect = 0\n\tvalCorrect = 0\n    \n\t# loop over the training set\n\tfor (i, (x, y)) in enumerate(trainLoader):\n\t\t# send the input to the device\n\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n        \n\t\t# perform a forward pass and calculate the training loss\n\t\tpred = model(x)\n\t\tloss = lossFunc(pred, y)\n        \n\t\t# calculate the gradients\n\t\tloss.backward()\n        \n        # check if we are updating the model parameters and if so update them, and zero out the previously accumulated gradients\n\t\tif (i + 2) % 2 == 0:\n\t\t\topt.step()\n\t\t\topt.zero_grad()\n            \n\t\t# add the loss to the total training loss so far and calculate the number of correct predictions\n\t\ttotalTrainLoss += loss\n\t\ttrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n        \n    # switch off autograd\n\twith torch.no_grad():\n\t\t# set the model in evaluation mode\n\t\tmodel.eval()\n        \n\t\t# loop over the validation set\n\t\tfor (x, y) in valLoader:\n\t\t\t# send the input to the device\n\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n            \n\t\t\t# make the predictions and calculate the validation loss\n\t\t\tpred = model(x)\n\t\t\ttotalValLoss += lossFunc(pred, y)\n            \n\t\t\t# calculate the number of correct predictions\n\t\t\tvalCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    # calculate the average training and validation loss\n\tavgTrainLoss = totalTrainLoss \/ trainSteps\n\tavgValLoss = totalValLoss \/ valSteps\n    \n\t# calculate the training and validation accuracy\n\ttrainCorrect = trainCorrect \/ len(trainDS)\n\tvalCorrect = valCorrect \/ len(valDS)\n    \n\t# update our training history\n\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n\tH[\"train_acc\"].append(trainCorrect)\n\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n\tH[\"val_acc\"].append(valCorrect)\n    \n\t# print the model training and validation information\n\tprint(\"[INFO] EPOCH: {}\/{}\".format(e + 1, EPOCHS))\n\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(avgValLoss, valCorrect))","499985d5":"# display the total time needed to perform the training\nendTime = time.time()\nprint(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n\n# plot the training loss and accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(H[\"train_loss\"], label=\"train_loss\")\nplt.plot(H[\"val_loss\"], label=\"val_loss\")\nplt.plot(H[\"train_acc\"], label=\"train_acc\")\nplt.plot(H[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(WARMUP_PLOT)\n\n# serialize the model to disk\ntorch.save(model, WARMUP_MODEL)","0eb48811":"# build our data pre-processing pipeline\ntestTransform = transforms.Compose([\n\ttransforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])\n\n# calculate the inverse mean and standard deviation\ninvMean = [-m\/s for (m, s) in zip(MEAN, STD)]\ninvStd = [1\/s for s in STD]\n\n# define our de-normalization transform\ndeNormalize = transforms.Normalize(mean=invMean, std=invStd)","891ab2d5":"# initialize our test dataset and data loader\nprint(\"[INFO] loading the dataset...\")\n(testDS, testLoader) = get_dataloader(VAL,transforms=testTransform, batchSize=PRED_BATCH_SIZE, shuffle=True)","c2fa02b9":"# check if we have a GPU available, if so, define the map location accordingly\nif torch.cuda.is_available():\n\tmap_location = lambda storage, loc: storage.cuda()\n# otherwise, we will be using CPU to run our model\nelse:\n\tmap_location = \"cpu\"\n    \n# load the model\nprint(\"[INFO] loading the model...\")\nmodel = torch.load(\"output\/warmup_model.pth\", map_location=map_location)\n# move the model to the device and set it in evaluation mode\nmodel.to(DEVICE)\nmodel.eval()","b40cce51":"# grab a batch of test data\nbatch = next(iter(testLoader))\n(images, labels) = (batch[0], batch[1])\n\n# initialize a figure\nfig = plt.figure(\"Results\", figsize=(10, 10))","429e3b4f":"# switch off autograd\nwith torch.no_grad():\n\t# send the images to the device\n\timages = images.to(DEVICE)\n    \n\t# make the predictions\n\tprint(\"[INFO] performing inference...\")\n\tpreds = model(images)\n    \n\t# loop over all the batch\n\tfor i in range(0, PRED_BATCH_SIZE):\n\t\t# initalize a subplot\n\t\tax = plt.subplot(PRED_BATCH_SIZE, 1, i + 1)\n        \n\t\t# grab the image, de-normalize it, scale the raw pixel intensities to the range [0, 255], and change the channel\n\t\t# ordering from channels first tp channels last\n\t\timage = images[i]\n\t\timage = deNormalize(image).cpu().numpy()\n\t\timage = (image * 255).astype(\"uint8\")\n\t\timage = image.transpose((1, 2, 0))\n        \n\t\t# grab the ground truth label\n\t\tidx = labels[i].cpu().numpy()\n\t\tgtLabel = testDS.classes[idx]\n        \n\t\t# grab the predicted label\n\t\tpred = preds[i].argmax().cpu().numpy()\n\t\tpredLabel = testDS.classes[pred]\n        \n\t\t# add the results and image to the plot\n\t\tinfo = \"Ground Truth: {}, Predicted: {}\".format(gtLabel, predLabel)\n\t\tplt.imshow(image)\n\t\tplt.title(info)\n\t\tplt.axis(\"off\")\n        \n\t# show the plot\n\tplt.tight_layout()\n\tplt.show()","60f8dbd3":"# define augmentation pipelines\ntrainTansform = transforms.Compose([\n\ttransforms.RandomResizedCrop(IMAGE_SIZE),\n\ttransforms.RandomHorizontalFlip(),\n\ttransforms.RandomRotation(90),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])\n\nvalTransform = transforms.Compose([\n\ttransforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])","b3bdba29":"# create data loaders\n(trainDS, trainLoader) = get_dataloader(TRAIN, transforms=trainTansform, batchSize=FEATURE_EXTRACTION_BATCH_SIZE)\n\n(valDS, valLoader) = get_dataloader(VAL, transforms=valTransform, batchSize=FEATURE_EXTRACTION_BATCH_SIZE, shuffle=False)","cc27ec7c":"# loop over epochs\nprint(\"[INFO] training the network...\")\nstartTime = time.time()\nfor e in tqdm(range(EPOCHS)):\n\t# set the model in training mode\n\tmodel.train()\n    \n\t# initialize the total training and validation loss\n\ttotalTrainLoss = 0\n\ttotalValLoss = 0\n    \n\t# initialize the number of correct predictions in the training and validation step\n\ttrainCorrect = 0\n\tvalCorrect = 0\n    \n\t# loop over the training set\n\tfor (i, (x, y)) in enumerate(trainLoader):\n\t\t# send the input to the device\n\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n        \n\t\t# perform a forward pass and calculate the training loss\n\t\tpred = model(x)\n\t\tloss = lossFunc(pred, y)\n        \n\t\t# calculate the gradients\n\t\tloss.backward()\n        \n\t\t# check if we are updating the model parameters and if so update them, and zero out the previously accumulated gradients\n\t\tif (i + 2) % 2 == 0:\n\t\t\topt.step()\n\t\t\topt.zero_grad()\n            \n\t\t# add the loss to the total training loss so far and calculate the number of correct predictions\n\t\ttotalTrainLoss += loss\n\t\ttrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n        \n    # switch off autograd\n\twith torch.no_grad():\n\t\t# set the model in evaluation mode\n\t\tmodel.eval()\n        \n\t\t# loop over the validation set\n\t\tfor (x, y) in valLoader:\n\t\t\t# send the input to the device\n\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n            \n\t\t\t# make the predictions and calculate the validation loss\n\t\t\tpred = model(x)\n\t\t\ttotalValLoss += lossFunc(pred, y)\n            \n\t\t\t# calculate the number of correct predictions\n\t\t\tvalCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n\t# calculate the average training and validation loss\n\tavgTrainLoss = totalTrainLoss \/ trainSteps\n\tavgValLoss = totalValLoss \/ valSteps\n    \n\t# calculate the training and validation accuracy\n\ttrainCorrect = trainCorrect \/ len(trainDS)\n\tvalCorrect = valCorrect \/ len(valDS)\n    \n\t# update our training history\n\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n\tH[\"train_acc\"].append(trainCorrect)\n\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n\tH[\"val_acc\"].append(valCorrect)\n    \n\t# print the model training and validation information\n\tprint(\"[INFO] EPOCH: {}\/{}\".format(e + 1, EPOCHS))\n\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(avgValLoss, valCorrect))","f275712d":"# display the total time needed to perform the training\nendTime = time.time()\nprint(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n\n# plot the training loss and accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(H[\"train_loss\"], label=\"train_loss\")\nplt.plot(H[\"val_loss\"], label=\"val_loss\")\nplt.plot(H[\"train_acc\"], label=\"train_acc\")\nplt.plot(H[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(FINETUNE_PLOT)\n\n# serialize the model to disk\ntorch.save(model, FINETUNE_MODEL)","35243f1d":"# build our data pre-processing pipeline\ntestTransform = transforms.Compose([\n\ttransforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize(mean=MEAN, std=STD)\n])\n\n# calculate the inverse mean and standard deviation\ninvMean = [-m\/s for (m, s) in zip(MEAN, STD)]\ninvStd = [1\/s for s in STD]\n\n# define our de-normalization transform\ndeNormalize = transforms.Normalize(mean=invMean, std=invStd)","5dbf0f4d":"# initialize our test dataset and data loader\nprint(\"[INFO] loading the dataset...\")\n(testDS, testLoader) = get_dataloader(VAL,transforms=testTransform, batchSize=PRED_BATCH_SIZE, shuffle=True)","dc8e12e3":"# check if we have a GPU available, if so, define the map location accordingly\nif torch.cuda.is_available():\n\tmap_location = lambda storage, loc: storage.cuda()\n# otherwise, we will be using CPU to run our model\nelse:\n\tmap_location = \"cpu\"\n    \n# load the model\nprint(\"[INFO] loading the model...\")\nmodel = torch.load(\"output\/warmup_model.pth\", map_location=map_location)\n# move the model to the device and set it in evaluation mode\nmodel.to(DEVICE)\nmodel.eval()","acd21615":"# switch off autograd\nwith torch.no_grad():\n\t# send the images to the device\n\timages = images.to(DEVICE)\n    \n\t# make the predictions\n\tprint(\"[INFO] performing inference...\")\n\tpreds = model(images)\n    \n\t# loop over all the batch\n\tfor i in range(0, PRED_BATCH_SIZE):\n\t\t# initalize a subplot\n\t\tax = plt.subplot(PRED_BATCH_SIZE, 1, i + 1)\n        \n\t\t# grab the image, de-normalize it, scale the raw pixel intensities to the range [0, 255], and change the channel\n\t\t# ordering from channels first tp channels last\n\t\timage = images[i]\n\t\timage = deNormalize(image).cpu().numpy()\n\t\timage = (image * 255).astype(\"uint8\")\n\t\timage = image.transpose((1, 2, 0))\n        \n\t\t# grab the ground truth label\n\t\tidx = labels[i].cpu().numpy()\n\t\tgtLabel = testDS.classes[idx]\n        \n\t\t# grab the predicted label\n\t\tpred = preds[i].argmax().cpu().numpy()\n\t\tpredLabel = testDS.classes[pred]\n        \n\t\t# add the results and image to the plot\n\t\tinfo = \"Ground Truth: {}, Predicted: {}\".format(gtLabel, predLabel)\n\t\tplt.imshow(image)\n\t\tplt.title(info)\n\t\tplt.axis(\"off\")\n        \n\t# show the plot\n\tplt.tight_layout()\n\tplt.show()","f99b4567":"https:\/\/www.pyimagesearch.com\/2021\/10\/11\/pytorch-transfer-learning-and-image-classification\/","5d086c75":"##  initialize our loss function and optimization method","809413b7":"## Dataset organization","e2dbae5d":"## Import the necessary packages","74389de5":"## Configuration ","f901c8f5":"## train the model","9519d5b3":"# Predict","e79d054e":"## Fine-tuning a CNN with PyTorch","00b7de43":"## DataLoader helper","051dc2cd":"## Feature extraction and transfer learning PyTorch"}}