{"cell_type":{"8e57bb7f":"code","4c76c839":"code","32c4d0b2":"code","a57c8d45":"code","442f128e":"code","380bb8c9":"code","dd6a3f4a":"code","12905ab4":"code","2119663f":"code","28f325c0":"code","6119b49b":"code","3356bc61":"code","1c5deb2e":"code","8034c5ce":"code","6bc35fa1":"code","05511218":"code","cd7cb81a":"code","361c4a64":"code","24fc6504":"code","6da9f5cc":"code","c6b24bb3":"code","c6d3094f":"code","1fbf4958":"code","8c868ed1":"code","b169c6af":"code","0253fed9":"code","974c7b22":"code","8b78e06d":"code","70e0e70d":"code","87c7bf3c":"code","960b510b":"markdown","61f4cfaa":"markdown","8638fba3":"markdown"},"source":{"8e57bb7f":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","4c76c839":"directory = '..\/input\/face-mask-detection\/images'","32c4d0b2":"anno=pd.read_csv('..\/input\/face-mask-get-annotation-info-from-xml\/annotation.csv')\nanno","a57c8d45":"path0='..\/input\/face-mask-detection\/images\/maksssksksss122.png'\nimage=cv2.imread(path0)\nprint(image.shape)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","442f128e":"anno0=anno[anno['file']=='maksssksksss122']\nanno0","380bb8c9":"[xmin,ymin,xmax,ymax]=anno0.iloc[0:1,0:4].values.tolist()[0]\nymin,ymax,xmin,xmax","dd6a3f4a":"image2=image[ymin:ymax,xmin:xmax]\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))","12905ab4":"image3=cv2.resize(image2,(60,60))\nprint(image3.shape)\nplt.imshow(cv2.cvtColor(image3, cv2.COLOR_BGR2RGB))","2119663f":"dataset=[]\nfor file in tqdm(anno['file']):\n    path=os.path.join(directory,file+'.png')\n    image=cv2.imread(path)\n    anno0=anno[anno['file']==file]\n    [xmin,ymin,xmax,ymax]=anno0.iloc[0:1,0:4].values.tolist()[0]\n    image2=image[ymin:ymax,xmin:xmax]\n    if image2.shape[0]>10 and image2.shape[1]>10:\n        image3=cv2.resize(image2,(60,60))\n        dataset+=[image3]","28f325c0":"print(anno['name'].unique())\nnormal_mapping={'with_mask':0, 'mask_weared_incorrect':1, 'without_mask':2}\ndatalabel=anno['name'].map(normal_mapping)\nprint(datalabel[0:5])","6119b49b":"dataset=np.array(dataset)\ndatalabel=np.array(datalabel)","3356bc61":"n=len(dataset)\nprint(n)\nD=list(range(n))\nrandom.seed(2021)\nrandom.shuffle(D)","1c5deb2e":"trainY=np.array(datalabel)[D[0:(n\/\/4)*3]]\ntestY=np.array(datalabel)[D[(n\/\/4)*3:]]\ntrainX=np.array(dataset)[D[0:(n\/\/4)*3]]\ntestX=np.array(dataset)[D[(n\/\/4)*3:]]","8034c5ce":"print(trainX.shape)\nprint(testX.shape)\nprint(trainY.shape)\nprint(testY.shape)","6bc35fa1":"labels1=to_categorical(trainY)\ntrainY2=np.array(labels1)","05511218":"tlabels1=to_categorical(testY)\ntestY2=np.array(tlabels1)","cd7cb81a":"trainx,testx,trainy,testy=train_test_split(trainX,trainY2,test_size=0.2,random_state=44)","361c4a64":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","24fc6504":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","6da9f5cc":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(60,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","c6b24bb3":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(3, activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","c6d3094f":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","1fbf4958":"his=model.fit(datagen.flow(trainx,trainy,batch_size=16),validation_data=(testx,testy),epochs=100)","8c868ed1":"y_pred=model.predict(testX)\npred=np.argmax(y_pred,axis=1)\nground=testY\nprint(classification_report(ground,pred))","b169c6af":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","0253fed9":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","974c7b22":"pred2=model.predict(testX)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","8b78e06d":"ANS=testY","70e0e70d":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","87c7bf3c":"pd.Series(PRED).value_counts()","960b510b":"# Prepare Annotation Information\nhttps:\/\/www.kaggle.com\/stpeteishii\/face-mask-get-annotation-info-from-xml","61f4cfaa":"# Train and Test Setting","8638fba3":"# Annotated Area Extract"}}