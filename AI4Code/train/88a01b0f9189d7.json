{"cell_type":{"fbd083e5":"code","5463d27b":"code","f63298e0":"code","b73e75a5":"code","0d7da9b6":"code","28a70ef2":"code","26c2029b":"code","94d401e3":"code","657d1bdb":"code","15706eac":"code","ee8fa2b9":"code","1c8948d2":"code","5fd03912":"code","2cac649f":"code","e172ffa9":"code","34fbc07c":"code","7f0d398e":"code","ef3426da":"code","7c4773c3":"code","7300a1fe":"code","9689a08a":"code","83ed36cd":"code","a560c79e":"code","ada38f20":"code","d8427ed1":"code","5368c97c":"code","96494e26":"code","9551cd71":"code","ec566e5d":"code","f28e444a":"code","92989159":"markdown","200c0a0a":"markdown","0468d0f7":"markdown","b761bb28":"markdown","b68fa1c6":"markdown","aa6380ee":"markdown","0611f61d":"markdown","01d9dab7":"markdown"},"source":{"fbd083e5":"import numpy as np\nimport math\nimport matplotlib.pyplot as plt \nimport matplotlib.colors\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport time \nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs\n\nimport torch","5463d27b":"torch.manual_seed(0)","f63298e0":"my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",[\"red\",\"yellow\",\"green\"])","b73e75a5":"data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\nprint(data.shape, labels.shape)","0d7da9b6":"plt.scatter(data[:,0],data[:,1], c=labels,cmap=my_cmap)\nplt.show()","28a70ef2":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\nprint(X_train.shape, X_val.shape, labels.shape)","26c2029b":"X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))","94d401e3":"print(X_train.shape, Y_train.shape)","657d1bdb":"def model(x):\n    a1 = torch.matmul(x, weights1) + bias1 #(N, 2) x (2, 2) -> (N, 2)\n    h1 = a1.sigmoid() # (N, 2)\n    a2 = torch.matmul(h1, weights2) + bias2 # (N, 2) x (2, 2) -> (N, 4)\n    h2 = a2.exp()\/a2.exp().sum(-1).unsqueeze(-1) # (N, 4)\n    return h2","15706eac":"# example loss\ny_hat = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.8, 0.1, 0.5, 0.05]])\ny = torch.tensor([2, 0])\n\n(-y_hat[range(y_hat.shape[0]), y].log()).mean().item()\n\n(torch.argmax(y_hat, dim=1) == y).float().mean().item()","ee8fa2b9":"def loss_fn(y_hat, y):\n    return -(y_hat[range(y.shape[0]),y].log()).mean()","1c8948d2":"def accuracy(y_hat, y):\n    pred = torch.argmax(y_hat, dim=1)\n    return (pred == y).float().mean()","5fd03912":"torch.manual_seed(0)\nweights1 = torch.randn(2, 2) \/ math.sqrt(2)\nweights1.requires_grad_()\nbias1 = torch.zeros(2, requires_grad=True)\n\nweights2 = torch.randn(2, 4) \/ math.sqrt(2)\nweights2.requires_grad_()\nbias2 = torch.zeros(4, requires_grad=True)\n\nlearning_rate = 0.2\nepochs = 10000\n\nX_train = X_train.float()\nY_train = Y_train.long()\n\nloss_arr = []\nacc_arr = []\n\nfor epoch in range(epochs):\n    y_hat = model(X_train)\n    loss = loss_fn(y_hat, Y_train)\n    loss.backward()\n    loss_arr.append(loss.item())\n    acc_arr.append(accuracy(y_hat, Y_train))\n    \n    with torch.no_grad():\n        weights1 -= weights1.grad * learning_rate\n        bias1 -= bias1. grad * learning_rate\n        weights2 -= weights2.grad * learning_rate\n        bias2 -= bias2.grad * learning_rate\n        weights1.grad.zero_()\n        bias1.grad.zero_()\n        weights2.grad.zero_()\n        bias2.grad.zero_()\n        \nplt.plot(loss_arr, 'r-')\nplt.plot(acc_arr, 'b-')\nplt.show()\nprint('Loss before training',loss_arr[0])\nprint('Loss after training',loss_arr[-1])\n","2cac649f":"import torch.nn.functional as F","e172ffa9":"torch.manual_seed(0)\nweights1 = torch.randn(2, 2) \/ math.sqrt(2)\nweights1.requires_grad_()\nbias1 = torch.zeros(2, requires_grad=True)\n\nweights2 = torch.randn(2, 4) \/ math.sqrt(2)\nweights2.requires_grad_()\nbias2 = torch.zeros(4, requires_grad=True)\n\nlearning_rate = 0.2\nepochs = 10000\n\nloss_arr = []\nacc_arr = []\n\nfor epoch in range(epochs):\n    y_hat = model(X_train)\n    loss = F.cross_entropy(y_hat, Y_train)\n    loss.backward()\n    loss_arr.append(loss.item())\n    acc_arr.append(accuracy(y_hat, Y_train))\n    \n    with torch.no_grad():\n        weights1 -= weights1.grad * learning_rate\n        bias1 -= bias1.grad * learning_rate\n        weights2 -= weights2.grad * learning_rate\n        bias2 -= bias2.grad * learning_rate\n        weights1.grad.zero_()\n        bias1.grad.zero_()\n        weights2.grad.zero_()\n        bias2.grad.zero_()\n    \nplt.plot(loss_arr, 'r-')\nplt.plot(acc_arr, 'b-')\nplt.show()\nprint('Loss before training', loss_arr[0])\nprint('Loss after training', loss_arr[-1])","34fbc07c":"import torch.nn as nn","7f0d398e":"class FirstNetwork(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(0)\n        self.weights1 = nn.Parameter(torch.randn(2, 2) \/ math.sqrt(2))\n        self.bias1 = nn.Parameter(torch.zeros(2))\n        self.weights2 = nn.Parameter(torch.randn(2, 4) \/ math.sqrt(2))\n        self.bias2 = nn.Parameter(torch.zeros(4))\n        \n    def forward(self, X):\n        a1 = torch.matmul(X, self.weights1) + self.bias1\n        h1 = a1.sigmoid()\n        a2 = torch.matmul(h1, self.weights2) + self.bias2\n        h2 = a2.exp()\/a2.exp().sum(-1).unsqueeze(-1)\n        return h2","ef3426da":"def fit(epochs = 1000, learning_rate = 1):\n    loss_arr = []\n    acc_arr = []\n    for epoch in range(epochs):\n        y_hat = fn(X_train)\n        loss = F.cross_entropy(y_hat, Y_train)\n        loss_arr.append(loss.item())\n        acc_arr.append(accuracy(y_hat, Y_train))\n        \n        loss.backward()\n        with torch.no_grad():\n            for param in fn.parameters():\n                param -= learning_rate * param.grad\n            fn.zero_grad()\n \n    plt.plot(loss_arr,'r-')\n    plt.plot(acc_arr,'b-')\n    plt.show()\n    print('Loss before training', loss_arr[0])\n    print('Loss after training', loss_arr[-1])","7c4773c3":"fn = FirstNetwork()\nfit()","7300a1fe":"class FirstNetwork_v1(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(0)\n        self.lin1 = nn.Linear(2, 2)\n        self.lin2 = nn.Linear(2, 4)\n    \n    def forward(self, X):\n        a1 = self.lin1(X)\n        h1 = a1.sigmoid()\n        a2 = self.lin2(h1)\n        h2 = a2.exp()\/a2.exp().sum(-1).unsqueeze(-1)\n        return h2","9689a08a":"fn = FirstNetwork_v1()\nfit()","83ed36cd":"from torch import optim","a560c79e":"def fit_v1(epochs = 1000, learning_rate = 1):\n    loss_arr = []\n    acc_arr = []\n    opt = optim.SGD(fn.parameters(), lr = learning_rate)\n    \n    for epoch in range(epochs):\n        y_hat = fn(X_train)\n        loss = F.cross_entropy(y_hat, Y_train)\n        loss_arr.append(loss.item())\n        acc_arr.append(accuracy(y_hat, Y_train))\n        \n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    \n    plt.plot(loss_arr,'r-')\n    plt.plot(acc_arr,'b-')\n    plt.show()\n    print('Loss Before Training', loss_arr[0])\n    print('Loss After Training', loss_arr[-1])","ada38f20":"fn = FirstNetwork_v1()\nfit_v1()","d8427ed1":"class FirstNetwork_v2(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(0)\n        self.net = nn.Sequential(\n            nn.Linear(2, 2),\n            nn.Sigmoid(),\n            nn.Linear(2, 4),\n            nn.Softmax()\n        )\n    \n    def forward(self, X):\n        return self.net(X)","5368c97c":"fn = FirstNetwork_v2()\nfit_v1()","96494e26":"def fit_v2(x, y, model, opt, loss_fn, epochs = 1000):\n    \n    for epoch in range(epochs):\n        loss = loss_fn(model(x), y)\n        \n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    \n    return loss.item()","9551cd71":"fn = FirstNetwork_v2()\nloss_fn = F.cross_entropy\nopt = optim.SGD(fn.parameters(), lr=1)\nfit_v2(X_train, Y_train, fn, opt, loss_fn)","ec566e5d":"device = torch.device(\"cuda\")\n\nX_train = X_train.to(device)\nY_train = Y_train.to(device)\nfn = FirstNetwork_v2()\nfn.to(device)\ntic = time.time()\nprint('Final loss', fit_v2(X_train, Y_train, fn, opt, loss_fn))\ntoc = time.time()\nprint('Time taken',toc -tic)","f28e444a":"device = torch.device(\"cpu\")\n\nX_train=X_train.to(device)\nY_train=Y_train.to(device)\nfn = FirstNetwork_v2()\nfn.to(device)\ntic = time.time()\nprint('Final loss', fit_v2(X_train, Y_train, fn, opt, loss_fn))\ntoc = time.time()\nprint('Time taken', toc - tic)","92989159":"### Generate Datasets","200c0a0a":"### Using Torch Tensors and Autograd","0468d0f7":"### Using NN.Sequential","b761bb28":"### Using NN.Parameter","b68fa1c6":"### Using NN.Linear and Optim","aa6380ee":"### Running it on GPUs","0611f61d":"### Using NN.Functional","01d9dab7":"### Outline \n* Feed Forward network with PyTorch tensors and autograd\n* Using Pytorch's NN -> Functional, linear, sequential & PyTorch's Optim\n* Moving things to CUDA\n"}}