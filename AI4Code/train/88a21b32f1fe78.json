{"cell_type":{"ef44d7d6":"code","0bcf3b91":"code","6c72b628":"code","798d9371":"code","0510fa14":"code","46f41647":"code","7b051600":"code","470f79bd":"code","10c232b1":"code","379d4302":"code","03740e74":"code","ef277772":"code","0b307a1a":"code","7b9d92c7":"code","6d7b8185":"code","cd73e42c":"code","1825c72a":"code","f83959ca":"code","aa957042":"code","601aa3f6":"code","b5aeced9":"code","36f0033c":"code","9f4c4edf":"code","fc76687b":"markdown","b51a849d":"markdown","5367453e":"markdown","21a7cf8f":"markdown","a9b8d483":"markdown","901a2aed":"markdown","0286bb4a":"markdown","bef430c3":"markdown","3946aa3e":"markdown","42549a39":"markdown","c9bd19cc":"markdown"},"source":{"ef44d7d6":"# Importing import modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras import optimizers\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom skimage import io\nprint(os.listdir(\"..\/input\/gestures-hand\/data\/data\"))","0bcf3b91":"# Train, test and validation directories\ntrain_dir = \"..\/input\/gestures-hand\/data\/data\/train\"\nval_dir = \"..\/input\/gestures-hand\/data\/data\/validation\"\ntest_dir = \"..\/input\/gestures-hand\/data\/data\/test\"","6c72b628":"img_array1 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/fist\/123.png'))\nplt.imshow(img_array1)","798d9371":"img_array2 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/five\/123.png'))\nplt.imshow(img_array2)","0510fa14":"img_array3 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/none\/123.png'))\nplt.imshow(img_array3)","46f41647":"img_array4 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/okay\/123.png'))\nplt.imshow(img_array4)","7b051600":"img_array5 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/peace\/123.png'))\nplt.imshow(img_array5)","470f79bd":"img_array6 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/rad\/123.png'))\nplt.imshow(img_array6)","10c232b1":"img_array7 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/straight\/123.png'))\nplt.imshow(img_array7)","379d4302":"img_array8 = np.array(Image.open('..\/input\/gestures-hand\/data\/data\/train\/thumbs\/123.png'))\nplt.imshow(img_array8)","03740e74":"# Declaring variables\noutputSize = len(os.listdir(train_dir)) \nepochs = 30 # Number of epochs","ef277772":"# Train Data Generator to do data augmentation on training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)","0b307a1a":"# Test Data Generator\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","7b9d92c7":"# Setting up the train generator to flow from the train directory\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (256,256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    color_mode = 'grayscale'\n)\n\n# Doing the same as above for the validation directory\nval_generator = test_datagen.flow_from_directory(\n    val_dir,\n    target_size = (256,256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    color_mode = 'grayscale'\n)","6d7b8185":"# Function to create keras model for different number of gestures\ndef create_model(outputSize):\n    model = Sequential()\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', input_shape = (256,256,1)))\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu'))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n    model.add(Flatten())\n    model.add(Dropout(rate = 0.5))\n    model.add(Dense(512, activation = 'relu'))\n    model.add(Dense(units = outputSize, activation = 'softmax'))\n    model.compile(optimizer = Adam(lr=1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    return model","cd73e42c":"# Creating the model\nmodel = create_model(outputSize)","1825c72a":"# Summary of model\nmodel.summary()","f83959ca":"import tensorflow as tf\ntf.keras.utils.plot_model(model, show_shapes=True)","aa957042":"# Fitting the model to the data based on a 32 batch size\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=outputSize*1000\/32,\n    epochs=epochs,\n    validation_data=val_generator,\n    validation_steps=outputSize*500\/32\n)","601aa3f6":"# Plotting training acc\/loss and val acc\/loss\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nrange_ep = epochs +1\nepoch_x = range(1, range_ep)\n\nplt.plot(epoch_x,acc,'bo',label=\"Training Acc\")\nplt.plot(epoch_x,val_acc,'b',label='Validation Acc')\nplt.title('Training and Validation Acc')\nplt.legend()\nplt.figure()\n\nplt.plot(epoch_x,loss,'bo',label=\"Training Loss\")\nplt.plot(epoch_x,val_loss,'b',label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.figure()\n\nplt.show()","b5aeced9":"# Setting up the test generator to flow from the test directory\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (256,256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    color_mode = 'grayscale'\n)","36f0033c":"# Test accuracy and test loss calc\ntest_loss, test_acc = model.evaluate_generator(test_generator,steps = outputSize*500\/32)\nprint(\"Test Acc:\",test_acc)\nprint(\"Test Loss:\",test_loss)","9f4c4edf":"# Model weights and model\nmodel.save_weights('gesture_model_weights.h5')\nmodel.save(\"gesture_model.h5\")","fc76687b":"## Overview \n\nThis script explores the dataset `gestures-hand` then the dataset is used to train a CNN model and plot its accuracy and loss.\n\n","b51a849d":"## Future Work\n\nThe model can be improved by \n- reducing overfitting by earlystopping\n- random train test split\n\n","5367453e":"## Exploring the dataset\n\nThe dataset contains three different directories called `train`, `test` and `validation`. Each of these directories contain 7 sub-directories which essentially refer to the six types of hand gestures i.e. `five`, `fist`, `rad`, `okay`, `peace`, `straight` and `thumbs`. Additionally, there is a `none` directory which contains images of the background without any hand in it which can be used as initial background setup. \n\nAll the images in the dataset have been preprocessed already via OpenCV's background subtraction algorithms by the following steps :\n* Applying smoothing filter that keeps edges sharp using `cv2.bilateralFilter`\n* Removing background like eroding background using `cv2.erode` , etc.\n* Create a background mask\n* Created a Region of Interest (ROI)\n* Converting image to gray using `cv2.cvtColor`\n* Blurring the image using `cv2.GaussianBlur`\n* Thresholding the image using `cv2.threshold`\n","21a7cf8f":"## Saving Model and Model Weights\n\nModel progress can be saved during and after training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\n\n  * code to create the model, and\n  * the trained weights, or parameters, for the model\n","a9b8d483":"## Importing dependencies\n\nFirst let us import all the modules and packages that will be required.","901a2aed":"### Visualising each gesture \nShow one image of the each of the 8 classes present.","0286bb4a":"The dataset is already been split into train, test and validation (so we dont need to do it later on) ","bef430c3":"## Defining the CNN\n","3946aa3e":"## Results and Performance Metrics","42549a39":"## Data Augmentation\n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nThe Keras deep learning neural network library provides the capability to fit models using image data augmentation via the `ImageDataGenerator` class.\n\nFirst, the class may be instantiated and the configuration for the types of data augmentation are specified by arguments to the class constructor.\n\nA range of techniques are supported, as well as pixel scaling methods. We will focus on five main types of data augmentation techniques for image data; specifically:\n\n * Image shifts via the `width_shift_range` and `height_shift_range` arguments.\n *   Image flips via the `horizontal_flip` arguments.\n *   Image rotations via the `rotation_range` argument\n *  Shear angle in counter-clockwise direction in degrees via the `shear_range` argument.\n *  Image zoom via the `zoom_range `argument.\n","c9bd19cc":"The variable `outputSize` refers to the number of different hand gestures in the dataset. Since it is a multi-class classification model, this will determine the number of units in the final dense layer preceeding the softmax activation."}}