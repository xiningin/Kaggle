{"cell_type":{"7b644cbc":"code","25ce555d":"code","e26ed3fc":"code","a880fbb3":"code","501c1bac":"code","b35fd398":"code","ee0bb435":"markdown","ce488d53":"markdown","623c4388":"markdown"},"source":{"7b644cbc":"!pip install --upgrade transformers\n!pip install simpletransformers\n# memory footprint support libraries\/code\n!ln -sf \/opt\/bin\/nvidia-smi \/usr\/bin\/nvidia-smi\n!pip install gputil\n!pip install psutil\n!pip install humanize\n!pip install pyspellchecker","25ce555d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e26ed3fc":"import numpy as np\nimport pandas as pd\n# from google.colab import files\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\nimport gc\nfrom scipy.special import softmax\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport sklearn\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom ast import literal_eval\nimport re\nfrom spellchecker import SpellChecker\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\npd.options.display.max_colwidth = 200\n\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\nseed_all(2)","a880fbb3":"train = pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')\nsample_sub = pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/sample_submission.csv')\ntrain.ABSTRACT = train.ABSTRACT.str.replace(\"\\n\",\" \")\ntest.ABSTRACT = test.ABSTRACT.str.replace(\"\\n\",\" \")\n\ntrain[\"text\"] = train[\"TITLE\"] + train[\"ABSTRACT\"]\ntest[\"text\"] = test[\"TITLE\"] + test[\"ABSTRACT\"]\n\ntarget_labels = [\"Computer Science\" ,\"Physics\" , \"Mathematics\", \"Statistics\" , \"Quantitative Biology\" , \"Quantitative Finance\"]\ntrain['label'] = train[target_labels].values.tolist()\n\nle = LabelEncoder()\ntrain['label'] = le.fit_transform(train['label'].astype(str))\ntrain = train[[\"text\",\"label\"]]\ntest = test[[\"text\"]]\ntest[\"label\"] = 0\n\nimport spacy, re\n#Data Cleanup\n\ndef pre_process(df):\n  df['text']=df['text'].str.replace('\\n','')\n  df['text']=df['text'].str.replace('\\r','')\n  df['text']=df['text'].str.replace('\\t','')\n    \n  #This removes unwanted texts\n  df['text'] = df['text'].apply(lambda x: re.sub(r'[0-9]','',x))\n  df['text'] = df['text'].apply(lambda x: re.sub(r'[\/(){}\\[\\]\\|@,;.:-]',' ',x))\n    \n  #Converting all upper case to lower case\n  df['text']= df['text'].apply(lambda s:s.lower() if type(s) == str else s)\n    \n\n  #Remove un necessary white space\n  df['text']=df['text'].str.replace('  ',' ')\n\n  #Remove Stop words\n  nlp=spacy.load(\"en_core_web_sm\")\n  df['text'] =df['text'].apply(lambda x: ' '.join([word for word in x.split() if nlp.vocab[word].is_stop==False ]))\n  return df\n\ntrain = pre_process(train)\ntest = pre_process(test)\n\ndisplay(train)\n\nwordcount = train['text'].str.count(' ') + 1\ndisplay(wordcount.mean())\n\ndisplay(test)\n\nprint(train['label'].nunique())\n\nmodel_2 = ClassificationModel('roberta', 'roberta-base', use_cuda=True,num_labels=24, args={\n                                                                         'train_batch_size':32,\n                                                                         'reprocess_input_data': True,\n                                                                         'overwrite_output_dir': True,\n                                                                         'fp16': False,\n                                                                         'do_lower_case': False,\n                                                                         'num_train_epochs': 3,\n                                                                         'max_seq_length': 256,\n                                                                         'regression': False,\n                                                                         'manual_seed': 2,\n                                                                         \"learning_rate\":5e-5,\n                                                                         'weight_decay':0.0,\n                                                                         \"save_eval_checkpoints\": False,\n                                                                         \"save_model_every_epoch\": False,\n                                                                         \"silent\": False}\n                                                                         )\n                              \n                                                                         \n\nmodel_2.train_model(train)\n\ntst_result_2, tst_model_outputs_2, tst_wrong_predictions_2 = model_2.eval_model(test)\n\npreds_2 = softmax(tst_model_outputs_2,axis=1)\n\nfinalpreds_2 = [np.argmax(x) for x in preds_2]\nsample_sub[\"target\"] = finalpreds_2\nsample_sub[\"target\"] = le.inverse_transform(sample_sub[\"target\"])\nsample_sub.loc[:,'target'] = sample_sub.loc[:,'target'].apply(lambda x: literal_eval(x))\nsample_sub[target_labels] = pd.DataFrame(sample_sub.target.tolist(), index= sample_sub.index)\nsample_sub.drop(\"target\",axis=1,inplace = True)\n\nsub_file_name = \"Submission_BEST_FINAL_2_Bert-Uncased_max_seq_256_epochs_2.csv\"\nsample_sub.to_csv(sub_file_name,index = False)","501c1bac":"model_3 = ClassificationModel('bert', 'bert-base-uncased', use_cuda=True,num_labels=24, args=\n                                                                        {\n                                                                         'train_batch_size':32,\n                                                                         'reprocess_input_data': True,\n                                                                         'overwrite_output_dir': True,\n                                                                         'fp16': False,\n                                                                         'do_lower_case': False,\n                                                                         'num_train_epochs': 2,\n                                                                         'max_seq_length': 256,\n                                                                         'regression': False,\n                                                                         'manual_seed': 2,\n                                                                         \"learning_rate\":5e-5,\n                                                                         'weight_decay':0.0,\n                                                                         \"save_eval_checkpoints\": False,\n                                                                         \"save_model_every_epoch\": False,\n                                                                         \"silent\": False\n                                                                         }\n                              \n                                                                         )\n\nmodel_3.train_model(train)\n\ntst_result_3, tst_model_outputs_3, tst_wrong_predictions_3 = model_3.eval_model(test)\n\npreds_3 = softmax(tst_model_outputs_3,axis=1)\n\nfinalpreds_3 = [np.argmax(x) for x in preds_3]\nsample_sub[\"target\"] = finalpreds_3\nsample_sub[\"target\"] = le.inverse_transform(sample_sub[\"target\"])\nsample_sub.loc[:,'target'] = sample_sub.loc[:,'target'].apply(lambda x: literal_eval(x))\nsample_sub[target_labels] = pd.DataFrame(sample_sub.target.tolist(), index= sample_sub.index)\nsample_sub.drop(\"target\",axis=1,inplace = True)\n\nsub_file_name = \"Submission_BEST_FINAL_2_Bert-Uncased_max_seq_256_epochs_2.csv\"\nsample_sub.to_csv(sub_file_name,index = False)\n","b35fd398":"# Ensemble : \"Blending of Reboerta-base + Bert-base-uncased\" :\n\nsub_file_name = \"FINAL_AV_SUBMISSION_Ensemble_bert-uncased_roberta-base_0.80_0.80.csv\"\npreds_final = preds_2*0.80 + preds_3*0.80\n\nfinalpreds_3 = [np.argmax(x) for x in preds_final]\n\nsample_sub[\"target\"] = finalpreds_3\nsample_sub[\"target\"] = le.inverse_transform(sample_sub[\"target\"])\nsample_sub.loc[:,'target'] = sample_sub.loc[:,'target'].apply(lambda x: literal_eval(x))\nsample_sub[target_labels] = pd.DataFrame(sample_sub.target.tolist(), index= sample_sub.index)\nsample_sub.drop(\"target\",axis=1,inplace = True)\nsample_sub.to_csv(sub_file_name,index = False)\n","ee0bb435":"![Prob.jpg](attachment:Prob.jpg)","ce488d53":"# **Download DATA SET here : https:\/\/www.kaggle.com\/vetrirah\/janatahack-independence-day-2020-ml-hackathon**","623c4388":"## Used simple transformers models - Roberta and Bert to reach Top 25 in Public Leaderboard :"}}