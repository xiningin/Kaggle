{"cell_type":{"25db67f6":"code","f300b219":"code","cdeeb845":"code","ecef77c6":"code","a37d1a5c":"code","6bb9f87b":"code","6726ee49":"code","4e424871":"code","fcd85590":"code","43b55a56":"code","323b51fc":"code","e739050f":"code","b98621de":"code","7a6ca3c6":"code","0ea22703":"code","a25543ae":"code","89ad9156":"code","719c2195":"code","d7bab0eb":"code","f72a91d8":"code","910a37fa":"code","3d893747":"code","66427cdf":"code","dff412d6":"code","4f9d2505":"code","93959b27":"code","c116ea20":"code","2f172d85":"code","5e947cf2":"code","ea140720":"code","1f35194e":"code","22d32387":"code","97bd50ba":"code","b4d62149":"code","a7309a9a":"code","1dc3702f":"markdown","a790f67f":"markdown","d33dea55":"markdown","33f8191a":"markdown","a11186ce":"markdown","8c8b1a93":"markdown","b4960b48":"markdown","7c0d3780":"markdown"},"source":{"25db67f6":"!pip install strsimpy","f300b219":"!pip install -U git+https:\/\/github.com\/albu\/albumentations --no-cache-dir","cdeeb845":"import torch\nimport pandas as pd\nimport albumentations as A\nimport random\n\nfrom PIL import Image\nimport cv2\nimport gc\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Adagrad, SGD\nimport torch.nn.functional as F\n\nimport random\n\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\n\n\nimport torchvision\nimport pickle\nimport json\n\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nfrom itertools import chain\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\n\nfrom string import digits, ascii_uppercase\n\nimport math \nimport utils\n\nfrom strsimpy.levenshtein import Levenshtein","ecef77c6":"torch.cuda.is_available()","a37d1a5c":"#\u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 SEED\nSEED = 1489\n\n\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)","6bb9f87b":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0442\u043e, \u043d\u0430 \u0447\u0451\u043c \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0448\u0430 \u0441\u0435\u0442\u044c\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","6726ee49":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\n\n#\u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c\nTEST_PATH = \"..\/input\/payment-detection\/test\/test\/\" \nTRAIN_PATH = \"..\/input\/payment-detection\/train\/train\/\"\nTRAIN_WITH_AUGS = \".\/\"\nSUBMISSION_PATH = \"..\/input\/payment-detection\/submission.csv\"\nTRAIN_INFO = \"..\/input\/payment-detection\/train.csv\"\n\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435\nTHRESHOLD = 0.35\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nIMAGE_WIDTH = 612\nIMAGE_HEIGHT = 612\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nVAL_SIZE = 0.3\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nN_ITER = 6\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nBATCH_SIZE = 8\nBATCH_SIZE_VAL = 1\n\n\nLR = 1e-4\n","4e424871":"train  = pd.read_csv(TRAIN_INFO)","fcd85590":"from distutils.dir_util import copy_tree\n\ncopy_tree(TRAIN_PATH, TRAIN_WITH_AUGS)","43b55a56":"def augmenting(images, train_set, how_many_times_to_aug):\n  for current_image in images:\n    image = cv2.imread(TRAIN_PATH + current_image) #\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n    x_shape, y_shape = list(image.shape[0:2:])[::-1] #\u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d\u044b \u0438 \u0448\u0438\u0440\u0438\u043d\u044b \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    some_image = train_set[train_set['image'] == current_image] #\u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 bounding box'\u043e\u0432\n    category_ids = list(some_image['label']) #\u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u044e\u0442 \u0432 \u0441\u0435\u0431\u0435 bb\n    another_form = some_image[:].values\n    bboxes = []\n    for i in another_form:\n      list_x1_y1 = []\n      for j in range(1, 5):\n        list_x1_y1.append(i[j])\n      bboxes.append(list_x1_y1)\n\n    for i in range(random.randint(how_many_times_to_aug, how_many_times_to_aug+5)):\n      transform = A.Compose(\n      [A.HorizontalFlip(p=0.7),\n       A.VerticalFlip(p=0.5),\n       A.Blur(blur_limit=7, p=0.1),\n       A.GaussianBlur(p=0.1),\n       A.GaussNoise(p=0.2),\n       A.RandomBrightnessContrast(p=0.4)], #\u041f\u0430\u0439\u043f\u043b\u0430\u0439\u043d \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439\n      bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),)\n      transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids) #\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\n      transformed_colors = cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR)\n      new_image_name = 'new' + current_image.split('.')[0] + str(random.randint(0, 100000)) + '.jpg'\n      transformed1 = cv2.imwrite(TRAIN_WITH_AUGS + new_image_name, transformed_colors) #\u0437\u0430\u043f\u0438\u0441\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0441 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439 \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e \u0441 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u0442\u0440\u0435\u0439\u043d\u043e\u0432\u044b\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438\n      trans = [[round(j) for j in i] for i in transformed['bboxes']]\n\n      for i in range(len(transformed['category_ids'])): #\u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0437\u0430\u043f\u0438\u0441\u0438 \u043e\u0431 \u0438\u0437\u043c\u0435\u043d\u0451\u043d\u043d\u043e\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438 \u0432 train\n        trans[i].insert(0, transformed['category_ids'][i])\n        trans[i].append(x_shape)\n        trans[i].append(y_shape)\n        trans[i].append(new_image_name)\n\n      train_set = train_set.append(pd.DataFrame(trans, columns=train_set.columns))\n  return train_set","323b51fc":"class_UY_images = list(train[train['label'] == \"UY\"]['image'].unique())\nclass_EX_images = list(train[train['label'] == \"EX\"]['image'].unique())\nclass_ST_images = list(train[train['label'] == \"ST\"]['image'].unique())\nclass_PC_images = list(train[train['label'] == \"PC\"]['image'].unique())","e739050f":"train = augmenting(class_UY_images, train, 30)\ntrain.value_counts('label')","b98621de":"train = augmenting(class_EX_images, train, 10)\ntrain.value_counts('label')","7a6ca3c6":"train = augmenting(class_ST_images, train, 5)\ntrain.value_counts('label')","0ea22703":"train = augmenting(class_PC_images, train, 3)\ntrain.value_counts('label')","a25543ae":"test = pd.read_csv(SUBMISSION_PATH)","89ad9156":"valid_images = np.random.choice(train.image.unique(), size=int(VAL_SIZE * train.image.nunique()), replace=False) \n\nvalid_set = train[train.image.isin(valid_images)]\n\ntrain_set = train[~train.image.isin(valid_images)]","719c2195":"#(?)\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0442\u043e, \u043d\u0430 \u0447\u0451\u043c \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0448\u0430 \u0441\u0435\u0442\u044c\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","d7bab0eb":"class ShapeDataset(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(), \n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        def get_boxes(obj):\n            boxes = [[obj[f] for f in ['xmin', 'ymin', 'xmax', 'ymax'] ]]\n            return torch.as_tensor(boxes, dtype=torch.float)\n\n        def get_areas(obj):\n            areas = [(obj['xmax'] - obj['xmin']) * (obj['ymax'] - obj['ymin']) ]\n            return torch.as_tensor(areas, dtype=torch.int64)\n        \n        def get_class(obj):\n            \"\"\"\n            \u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0447\u0438\u0441\u043b\u043e\u043c\n            \"\"\"\n            \n            cards = {\n                'VI':1,\n                'MA':2,\n                'EX':3, \n                'PC':4, \n                'ST':5,\n                'UY':6\n                    }\n            return torch.as_tensor([cards[obj['label']]], dtype=torch.int64)\n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n\n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n        \n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n        \n        row = self.data.iloc[idx]\n        obj = {}        \n        \n        obj['xmin'] = int(row['x1'] * (IMAGE_WIDTH\/ row['width']))\n        obj['xmax'] = int((row['x1'] + row['w'])*(IMAGE_WIDTH\/ row['width']))\n        obj['ymin'] = int(row['y1'] * (IMAGE_HEIGHT\/ row['height']))\n        obj['ymax'] = int((row['y1'] + row['h'])*(IMAGE_HEIGHT\/ row['height']))\n\n        \n        obj['label'] = row['label']\n\n        \n        if self.transform:\n            image = self.transform(img)\n            \n\n        target = {}\n        target['boxes'] = get_boxes(obj)\n\n        target['labels'] = get_class(obj)\n        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n        target['area'] = get_areas(obj)\n        target['iscrowd'] = torch.ones((1,), dtype=torch.int64)\n\n        return image, target\n\n    \nclass ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(),\n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               ) ])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT \n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n        \n        \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n            \n        return image \n\n","f72a91d8":"train_data = ShapeDataset(TRAIN_WITH_AUGS, IMAGE_WIDTH, IMAGE_HEIGHT, train_set)\nvalid_data = ShapeDataset(TRAIN_WITH_AUGS, IMAGE_WIDTH, IMAGE_HEIGHT, valid_set)","910a37fa":"test_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)","3d893747":"dataloader_train = DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\ndataloader_valid = DataLoader(\n    valid_data, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=0, collate_fn=collate_fn)","66427cdf":"device = get_device() #\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u043e, \u043a\u0430\u043a\u043e\u0439 \u0434\u0435\u0432\u0430\u0439\u0441 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #\u041f\u0440\u0435\u0434\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f Faster RCNN\nnum_classes = 7 #\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432\nin_features = model.roi_heads.box_predictor.cls_score.in_features #\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0444\u0438\u0447\u0438\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) #\u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0442\u0440\u0435\u0439\u043d\u043e\u0432\u044b\u0439 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u043e\u0440 \u0441\u0432\u043e\u0438\u043c, \u0441\u043e \u0441\u0432\u043e\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n\nmodel = model.to(device) #\u041f\u043e\u043c\u0435\u0449\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 cpu \u0438\u043b\u0438 gpu","dff412d6":"params = [p for p in model.parameters() if p.requires_grad] ","4f9d2505":"optimizer = optim.Adam(model.parameters(), lr=LR)\nloss_fn = fnn.mse_loss #\u0417\u0430\u0447\u0435\u043c \u044d\u0442\u043e?\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n","93959b27":"def lev_dist(preds, df):\n    \n    \n    def c_sort(sub_li):\n        \"\"\"\n        \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0431\u043e\u043a\u0441\u043e\u0432 \u043f\u043e \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c. \u0421\u043b\u0435\u0432\u0430 \u043d\u0430\u043f\u0440\u0430\u0432\u043e \u0441\u0432\u0435\u0440\u0445\u0443 \u0432\u043d\u0438\u0437.\n        \"\"\"\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n        \n        \"\"\"\n        \u0414\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430.\n        \"\"\"\n        cards = {\n                1:'VI',\n                2:'MA',\n                3:'EX', \n                4:'PC', \n                5:'ST',\n                6:'UY'\n                    }\n        return cards[number]\n    \n    levenshtein = Levenshtein()     \n            \n    imgs = {}\n    \n    \n    \n    for index, row in df.iterrows():\n        \n        #\u0438\u0434\u0451\u043c \u043f\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443, \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        \n        if row['image'] in imgs.keys():    \n            \n            \n            \n            imgs[row['image']].append([row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label'] ])\n            imgs[row['image']] = c_sort(imgs[row['image']])\n\n        else:\n            imgs[row['image']] =  [[row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label']]]\n            \n            \n    labels = {}\n    \n    for i in imgs.keys():\n        \n        #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443\n        \n        labels[i] = \" \".join([j[4] for j in imgs[i]])\n        \n    preds_list = []\n    \n    cnt = 0\n    \n    \n    labels_pred = {}\n    imgs_name = df['image'].tolist()\n\n    for i in preds:\n        for j in i:\n            #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b\n            _temp_boxes = i[j]['boxes'].cpu().detach().numpy().tolist()\n            _temp_label = i[j]['labels'].cpu().detach().numpy().tolist()\n            _temp_confidence = i[j]['scores'].cpu().detach().numpy().tolist()\n               \n            for index, _ in enumerate(_temp_boxes):\n                # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430\n                _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n                _temp_boxes[index].append(_temp_confidence[index])\n                \n                \n            _temp_boxes = c_sort(_temp_boxes)\n\n            # \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u044b\u0448\u0435 THRESHOLD\n            \n            labels_pred[imgs_name[cnt]] = \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD])\n            \n            cnt+=1\n        \n    assert len(imgs) == len(labels_pred)\n    \n    lev_dist = 0\n    \n    for i in imgs.keys():\n        # \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        \n        lev_dist += levenshtein.distance(imgs[i], labels_pred[i])\n    \n    print(\"Levenstein distance {0}\".format(lev_dist\/len(imgs)))\n    \n    return lev_dist\/len(imgs)","c116ea20":"train_losses = []\ntest_losses = []","2f172d85":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            #sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        \n    \n\n    return metric_logger\n\n\ndef _get_iou_types(model): #\u0414\u043b\u044f \u0447\u0435\u0433\u043e \u043d\u0430\u043c \u0437\u0434\u0435\u0441\u044c IoU?\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Val:'\n\n    preds = []\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        \n        preds.append(res)\n        \n\n    # accumulate predictions from all images\n    torch.set_num_threads(n_threads)\n    return preds","5e947cf2":"train_loss_for_epoch = []\nfor epoch in range(N_ITER):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=100)\n    # update the learning rate\n    lr_scheduler.step()\n\nprint(\"Train error\")\npreds_train = evaluate(model, dataloader_train, device=device)\ntrain_error = lev_dist(preds_train, train_set)\ntrain_loss_for_epoch.append(train_error)\n\nprint(\"Validate error\")\npreds_val = evaluate(model, dataloader_valid, device=device)\nlev_dist(preds_val, valid_set)\n\n     \n# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nwith open(f\"model.pth\", \"wb\") as fp:\n    torch.save(model.state_dict(), fp)\n","ea140720":"plt.figure(figsize=(20, 6))\nplt.plot(train_loss_for_epoch, label=\"train_loss_1\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","1f35194e":"def get_prediction(dataset,  model):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    \n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = []\n    \n     #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n        \n        labels_pred = []\n        names_pred = []\n        \n    imgs_name = test.image.unique().tolist()\n\n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for ind, i in enumerate(preds):\n\n        _temp_boxes = i[0]['boxes'].cpu().detach().numpy().tolist()\n        _temp_label = i[0]['labels'].cpu().detach().numpy().tolist()\n        _temp_confidence = i[0]['scores'].cpu().detach().numpy().tolist()\n\n        for index, _ in enumerate(_temp_boxes):\n            _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n            _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(imgs_name[ind])\n        labels_pred.append( \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD]))\n             \n    d = {'image': names_pred, 'payment': labels_pred}\n\n    df = pd.DataFrame(data=d)\n        \n        \n    return df ","22d32387":"metric_logger = utils.MetricLogger(delimiter=\"  \")\n\nheader = \"Test:\"\n\ndataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\npredictions = get_prediction(dataloader_test, model) ","97bd50ba":"predictions.head()","b4d62149":"predictions.to_csv(\"true_submission.csv\", index=None)","a7309a9a":"import os, fnmatch\nlistOfFiles = os.listdir(TRAIN_WITH_AUGS)  \npattern = \"*.jpg\"  \nfor entry in listOfFiles:  \n    if fnmatch.fnmatch(entry, pattern):\n            os.remove(TRAIN_WITH_AUGS+entry)","1dc3702f":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","a790f67f":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430","d33dea55":"# \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u043b\u0438\u0448\u043d\u0438\u0445 \u0444\u0430\u0439\u043b\u043e\u0432","33f8191a":"# \u041a\u043e\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0439\u043d\u043e\u0432\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0432 output\/working \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0445 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439","a11186ce":"\u0427\u0430\u0441\u0442\u0438\u0447\u043d\u043e \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430\nhttps:\/\/pytorch.org\/tutorials\/index.html","8c8b1a93":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043f\u043e\u0441\u043b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0435 train","b4960b48":"## \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","7c0d3780":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432"}}