{"cell_type":{"14d615f7":"code","bf24278c":"code","27d1b0cf":"code","6d5466d0":"code","64ea8926":"code","46287674":"code","6e152514":"code","316d85cc":"code","c1eab22f":"code","9b5d1f55":"code","13a27021":"code","3744d9b7":"code","4d56e163":"code","e94aea7f":"code","4b04a55d":"code","56ced8cc":"code","94ae1301":"code","bdd51547":"code","b31364b2":"code","f25b6cb4":"code","34dbd4b2":"code","6887a390":"code","3e2f3f85":"code","9682d239":"code","82e593c4":"code","f353bc0e":"code","b38c6e8c":"code","97b32373":"code","519af5e3":"code","15c7ee59":"code","f240b101":"code","1e93dbc0":"code","e6642de7":"code","bb25ff74":"markdown","567ac58c":"markdown","02d235b9":"markdown","e8d051ec":"markdown","817b0642":"markdown","40eb67f3":"markdown","813e22fa":"markdown","d8ca5257":"markdown","334bfe5e":"markdown","95270814":"markdown"},"source":{"14d615f7":"\nimport pandas as pd\nimport numpy as np","bf24278c":"\n\nimport nltk\n#nltk.download('')","27d1b0cf":"from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn","6d5466d0":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection,naive_bayes,svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n","64ea8926":"#read the data\ncorpus = pd.read_csv('..\/input\/ham-spam\/SMS-SPAM-BAYES.csv')","46287674":"# statistics for all columns of the dataset\ncorpus.head()","6e152514":"# the number of rows and columns in a dataset\ncorpus.shape","316d85cc":"#to find values of ham and spam\ncorpus['type'].value_counts()","c1eab22f":"# converting into lower case\ncorpus['text'] = [entry.lower() for entry in corpus['text']]","9b5d1f55":"nltk.download('punkt')","13a27021":"corpus['text'] = [word_tokenize(entry) for entry in corpus['text']]","3744d9b7":"corpus['text'].head()","4d56e163":"nltk.download('wordnet')","e94aea7f":"tag_map = defaultdict(lambda : wn.NOUN)\ntag_map['j'] = wn.ADJ\ntag_map['v'] = wn.VERB\ntag_map['v'] = wn.ADV","4b04a55d":"nltk.download('stopwords')","56ced8cc":"stopwords.words('english')","94ae1301":"nltk.download('averaged_perceptron_tagger')","bdd51547":"for index,entry in enumerate(corpus['text']):\n    Final_words = []\n    word_lemmstized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_final = word_lemmstized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_final)\n    corpus.loc[index,'text_final'] = str(Final_words)\n\n","b31364b2":"corpus.head()","f25b6cb4":"get_ipython().system('pip install wordcloud')","34dbd4b2":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt","6887a390":"comment_words = ' '\nstopwords = set(STOPWORDS)","3e2f3f85":"for val in corpus.text_final:\n    val = str(val)\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    for words in tokens:\n        comment_words = comment_words + words + ' '\n","9682d239":"wordcloud = WordCloud(width = 1000, height =1000, background_color = 'white',\n                      stopwords = stopwords,min_font_size = 10).generate(comment_words)\n\n\nplt.figure(figsize = (4,4), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n","82e593c4":"\nTrain_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(corpus['text_final'],corpus['type'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)","f353bc0e":"y = Train_Y.tolist()","b38c6e8c":"y","97b32373":"Tfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(corpus['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\n","519af5e3":"data = Train_X_Tfidf.toarray()","15c7ee59":"data","f240b101":"naive = naive_bayes.MultinomialNB()\nnaive.fit(Train_X_Tfidf,Train_Y)","1e93dbc0":"\npredictions_NB = naive.predict(Test_X_Tfidf)\n","e6642de7":"print(\"Naive Bayes Model Accuracy : \",accuracy_score(predictions_NB,Test_Y)*100)\n","bb25ff74":"# Split data to Train and Test","567ac58c":"# WordNet is the lexical database","02d235b9":"# import libraries","e8d051ec":"# Tokenization:","817b0642":"\n\n dictionary for the English language, specifically designed for natural language processing.\n\nSynset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n\nSynset instances are the groupings of synonymous words that express the same concept","40eb67f3":"# STOPWORDS","813e22fa":"# parts of speech:","d8ca5257":".Stopwords are the most common words in any natural language.\n\n.For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.","334bfe5e":"Tokenization is process of segmentation running text into sentences and words.\n\nIn  essence,it is task of cutting a text into pieces called tokens. \n","95270814":"# Training Naive Bayes Model"}}