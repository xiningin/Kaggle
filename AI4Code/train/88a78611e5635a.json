{"cell_type":{"2d455039":"code","3cdc9bd7":"code","ad9d5b2d":"code","f44ac6de":"code","4c652b4a":"code","df7ca119":"code","72172c93":"code","5f1005db":"code","340eec7b":"code","d7d12ff3":"code","9d81aafa":"code","b718f28c":"code","406e88fc":"code","8e9b4d00":"code","15423b5a":"code","53229cf4":"code","5b095262":"code","6baf91b1":"code","d0b266ae":"code","7cc83a57":"code","a136502f":"code","c9e6b75e":"code","1848be4b":"code","8fa44028":"code","a7960afd":"code","b6bab347":"code","cfe440a6":"code","7e534b11":"code","72a9179c":"code","492c5c7b":"code","5de8890c":"code","cf8a43e9":"code","2057ff35":"code","3a540d9a":"code","78a91636":"code","c3c2c318":"code","73ed22bb":"code","694a5e05":"code","0ab837bf":"code","402497dc":"code","0aaa8974":"code","c78fd8c1":"code","31a0db81":"code","f445cf41":"code","2d66b11a":"code","7e2b6649":"code","3941b26e":"code","196d118f":"markdown","7718df47":"markdown","1ad5117c":"markdown","b5f5efc4":"markdown","8b9911b4":"markdown"},"source":{"2d455039":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cdc9bd7":"#general purpose packages\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#data processing\nimport re, string\nimport emoji\nimport nltk\n\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\n\n\n#Naive Bayes\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\n#transformers\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel\nfrom transformers import RobertaTokenizerFast\nfrom transformers import TFRobertaModel\n\n#keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n#metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#set seed for reproducibility\nseed=42\n\n#set style for plots\nsns.set_style(\"whitegrid\")\nsns.despine()\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)","ad9d5b2d":"def conf_matrix(y, y_pred, title):\n    fig, ax =plt.subplots(figsize=(5,5))\n    labels=['Negative', 'Neutral', 'Positive']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n    plt.title(title, fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=17) \n    ax.yaxis.set_ticklabels(labels, fontsize=17)\n    ax.set_ylabel('Test', fontsize=20)\n    ax.set_xlabel('Predicted', fontsize=20)\n    plt.show()","f44ac6de":"df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding='ISO-8859-1')\ndf_test = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')","4c652b4a":"df['TweetAt'] = pd.to_datetime(df['TweetAt'])\ndf.drop_duplicates(subset='OriginalTweet',inplace=True)\ndf.info()","df7ca119":"tweets_per_day = df['TweetAt'].dt.strftime('%m-%d').value_counts().sort_index().reset_index(name='counts')","72172c93":"plt.figure(figsize=(20,5))\nax = sns.barplot(x='index', y='counts', data=tweets_per_day,edgecolor = 'black',ci=False, palette='Blues_r')\nplt.title('Tweets count by date')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","5f1005db":"tweets_per_country = df['Location'].value_counts().loc[lambda x : x > 100].reset_index(name='counts')\nplt.figure(figsize=(15,6))\nax = sns.barplot(x='index', y='counts', data=tweets_per_country,edgecolor = 'black',ci=False, palette='Spectral')\nplt.title('Tweets count by country')\nplt.xticks(rotation=70)\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","340eec7b":"df = df[['OriginalTweet','Sentiment']]\ndf_test = df_test[['OriginalTweet','Sentiment']]\n##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n\n#Clean emojis from text\ndef strip_emoji(text):\n    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", text) #remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8\/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + '\u00c3'+'\u00b1'+'\u00e3'+'\u00bc'+'\u00e2'+'\u00bb'+'\u00a7'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): # remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)","d7d12ff3":"texts_new = []\nfor t in df.OriginalTweet:\n    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))","9d81aafa":"texts_new_test = []\nfor t in df_test.OriginalTweet:\n    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))","b718f28c":"df['text_clean'] = texts_new\ndf_test['text_clean'] = texts_new_test","406e88fc":"text_len = []\nfor text in df.text_clean:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)","8e9b4d00":"df['text_len'] = text_len\ntext_len_test = []\nfor text in df_test.text_clean:\n    tweet_len = len(text.split())\n    text_len_test.append(tweet_len)\n    ","15423b5a":"df_test['text_len'] = text_len_test","53229cf4":"plt.figure(figsize=(7,5))\nax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\nplt.title('Training tweets with less than 10 words')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()\n","5b095262":"print(f\" DF SHAPE: {df.shape}\")\nprint(f\" DF TEST SHAPE: {df_test.shape}\")","6baf91b1":"df = df[df['text_len'] > 4]\ndf_test = df_test[df_test['text_len'] > 4]","d0b266ae":"print(f\" DF SHAPE: {df.shape}\")\nprint(f\" DF TEST SHAPE: {df_test.shape}\")","7cc83a57":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","a136502f":"token_lens = []\nfor txt in df['text_clean'].values:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    \nmax_len=np.max(token_lens)","c9e6b75e":"token_lens = []\nfor i,txt in enumerate(df['text_clean'].values):\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))\n    if len(tokens)>80:\n        print(f\"INDEX: {i}, TEXT: {txt}\")  ","1848be4b":"df['token_lens'] = token_lens\ndf = df.sort_values(by='token_lens', ascending=False)","8fa44028":"df = df.iloc[12:]\ndf.head()","a7960afd":"df = df.sample(frac=1).reset_index(drop=True)\ntoken_lens_test = []\nfor txt in df_test['text_clean'].values:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens_test.append(len(tokens))\n    \nmax_len=np.max(token_lens_test)","b6bab347":"token_lens_test = []\n\nfor i,txt in enumerate(df_test['text_clean'].values):\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens_test.append(len(tokens))\n    if len(tokens)>80:\n        print(f\"INDEX: {i}, TEXT: {txt}\")","cfe440a6":"df_test['token_lens'] = token_lens_test\ndf_test = df_test.sort_values(by='token_lens', ascending=False)\ndf_test = df_test.iloc[5:]\ndf_test.head(3)","7e534b11":"df_test = df_test.sample(frac=1).reset_index(drop=True)","72a9179c":"df['Sentiment'] = df['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\ndf_test['Sentiment'] = df_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\ndf['Sentiment'].value_counts()","492c5c7b":"ros = RandomOverSampler()\ntrain_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['Sentiment']).reshape(-1, 1));\ntrain_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'Sentiment']);\ntrain_os['Sentiment'].value_counts()","5de8890c":"X = train_os['text_clean'].values\ny = train_os['Sentiment'].values","cf8a43e9":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)\nX_test = df_test['text_clean'].values\ny_test = df_test['Sentiment'].values","2057ff35":"y_train_le = y_train.copy()\ny_valid_le = y_valid.copy()\ny_test_le = y_test.copy()","3a540d9a":"ohe = preprocessing.OneHotEncoder()\ny_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\ny_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\ny_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()","78a91636":"print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )","c3c2c318":"clf = CountVectorizer()\nX_train_cv =  clf.fit_transform(X_train)\nX_test_cv = clf.transform(X_test)","73ed22bb":"tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\nX_train_tf = tf_transformer.transform(X_train_cv)\nX_test_tf = tf_transformer.transform(X_test_cv)","694a5e05":"nb_clf = MultinomialNB()\nnb_clf.fit(X_train_tf, y_train_le)","0ab837bf":"nb_pred = nb_clf.predict(X_test_tf)\nprint('\\tClassification Report for Naive Bayes:\\n\\n',classification_report(y_test_le,nb_pred, target_names=['Negative', 'Neutral', 'Positive']))","402497dc":"MAX_LEN=128\ndef tokenize(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n            data[i],\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","0aaa8974":"train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)","c78fd8c1":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","31a0db81":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    ##params###\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    embeddings = bert_model([input_ids,attention_masks])[1]\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n    \n    model.compile(opt, loss=loss, metrics=accuracy)\n    \n    \n    return model","f445cf41":"model = create_model(bert_model, MAX_LEN)\nmodel.summary()","2d66b11a":"history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=32)","7e2b6649":"result_bert = model.predict([test_input_ids,test_attention_masks])\ny_pred_bert =  np.zeros_like(result_bert)\ny_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1","3941b26e":"conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')","196d118f":"**Sentiment column analysis**","7718df47":"create a new column, for both train and test sets, to host the cleaned version of the tweets' text.","1ad5117c":"**BERT Sentiment Analysis**","b5f5efc4":"**The F1 score is around 70% for the more populated classes (Negative and Positive emotions), and lower for the Neutral class (F1=0.53).\nIn particular, the overall accuracy is 70%.**","8b9911b4":"**Training data deeper cleaning**"}}