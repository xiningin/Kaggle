{"cell_type":{"3329c934":"code","dc8905c3":"code","e687c878":"code","5aaf3483":"code","44048ae2":"code","0d2f016b":"code","e65295f2":"code","c80a2cb8":"code","2a3328d8":"code","3ee9ab2f":"code","efd735cd":"code","a6d435fd":"code","5a6ac499":"code","5771588a":"code","b686f060":"code","48bfce0e":"code","e09eef89":"code","71ece809":"code","7bd5bf0d":"code","f3759250":"code","403fb04c":"code","e0ddb00b":"code","87866e7d":"code","82f25db2":"code","f0e6068c":"code","eaf3dd54":"code","3481292f":"markdown","e21b97e0":"markdown","45b74d75":"markdown","d84408a6":"markdown","bb1f37ee":"markdown","06794c00":"markdown","7a1e7dff":"markdown","00448753":"markdown","c348f23d":"markdown","ac5c559c":"markdown","c2ad4a2d":"markdown","b6585bf1":"markdown","85dc43d2":"markdown","1dcb692b":"markdown"},"source":{"3329c934":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc8905c3":"import numpy as np\nimport warnings\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score, validation_curve\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nfrom pandas.core.common import SettingWithCopyWarning\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)","e687c878":"df=pd.read_csv(\"\/kaggle\/input\/hitters\/Hitters.csv\")\ndf.head()","5aaf3483":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n\ncheck_df(df)","44048ae2":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","0d2f016b":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col)","e65295f2":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=50)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\n    print(\"#####################################\")\n\n\nfor col in num_cols:\n    num_summary(df, col,True)","c80a2cb8":"def target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n    \nfor col in cat_cols:\n    target_summary_with_cat(df, \"Salary\", col)","2a3328d8":"def correlation_matrix(df, cols):\n    fig = plt.gcf()\n    fig.set_size_inches(10, 8)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='RdBu')\n    plt.show(block=True)\n\ncorrelation_matrix(df, num_cols)","3ee9ab2f":"def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):\n    quantile_one = dataframe[variable].quantile(low_quantile)\n    quantile_three = dataframe[variable].quantile(up_quantile)\n    interquantile_range = quantile_three - quantile_one\n    up_limit = quantile_three + 1.5 * interquantile_range\n    low_limit = quantile_one - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\nfor col in num_cols:\n      print(col, check_outlier(df, col))","efd735cd":"sns.boxplot(x=df[\"CHits\"])\nplt.show()","a6d435fd":"# We replace outliers with threshold values.\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\nfor col in num_cols:\n    print(replace_with_thresholds(df,col))\n    \nfor col in num_cols:\n      print(col, check_outlier(df, col))","5a6ac499":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)","5771588a":"## We are deleting the missing values as we do not have job information for now.\n\ndf.dropna(inplace=True)\nmissing_values_table(df)","b686f060":"new_num_cols = [col for col in num_cols if col not in [\"Salary\", \"Years\"]]\ndf[new_num_cols] = df[new_num_cols] + 1\ndf.columns = [col.upper() for col in df.columns]","48bfce0e":"df[\"CATBAT_MEAN\"]=df[\"CATBAT\"]\/df[\"YEARS\"]\ndf[\"CHITS_MEAN\"]=df[\"CHITS\"]\/df[\"YEARS\"]\ndf[\"CHMRUN_MEAN\"]=df[\"CHMRUN\"]\/df[\"YEARS\"]\ndf[\"CRUNS_MEAN\"]=df[\"CRUNS\"]\/df[\"YEARS\"]\ndf[\"CWALKS_MEAN\"]=df[\"CWALKS\"]\/df[\"YEARS\"]\ndf[\"CRBI_MEAN\"]=df[\"CRBI\"]\/df[\"YEARS\"]\n\ndf[\"HITS_PERC\"]=(df[\"HITS\"]\/df[\"ATBAT\"])*100\ndf[\"HMRUN_PERC\"]=(df[\"HMRUN\"]\/df[\"ATBAT\"])*100\n\ndff = df.drop([\"HITS\",\"CRBI\",\"PUTOUTS\",\"CHITS\",\"CATBAT\",\"DIVISION\",\"CRUNS\",\"RUNS\",\"ATBAT\"], axis=1)","e09eef89":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(dff)\n\ndff = one_hot_encoder(dff, cat_cols, drop_first=True)\ndff.head()\ndff.shape","71ece809":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nbinary_cols = [col for col in dff.columns if dff[col].dtypes == \"O\"\n               and len(dff[col].unique()) == 2]\n\nbinary_cols\n\nfor col in binary_cols:\n    label_encoder(dff, col)","7bd5bf0d":"num_cols = [col for col in num_cols if col not in [\"SALARY\"]]\nscaler = StandardScaler()\ndff[num_cols] = scaler.fit_transform(dff[num_cols])\ndff.head()","f3759250":"X = dff.drop(\"SALARY\", axis=1)\ny = dff[[\"SALARY\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)\n\n# sabit (b - bias)\nreg_model.intercept_","403fb04c":"# coefficients (w - weights)\nreg_model.coef_","e0ddb00b":"# Train RMSE\ny_pred = reg_model.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred))","87866e7d":"# TRAIN RKARE\nreg_model.score(X_train, y_train)","82f25db2":"# Test RMSE\ny_pred = reg_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f0e6068c":"# Test RKARE\nreg_model.score(X_test, y_test)","eaf3dd54":"# K-Fold Cross Validation\nnp.mean(np.sqrt(-cross_val_score(reg_model,\n                                 X, y,\n                                 cv=10,\n                                 scoring=\"neg_mean_squared_error\")))","3481292f":"##### Analysis of Numerical Variables","e21b97e0":"### 2. Data Preprocessing & Feature Engineering\n#### Outlier Analysis","45b74d75":"##### Analysis of Categorical Variables","d84408a6":"### 4. Encoding","bb1f37ee":"# SALARY PREDICTION WITH MACHINE LEARNING (HITTERS DATASET)\n\n### This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n### Problem : Estimating baseball players salary applying machine learning algoritms on hitters dataset\n","06794c00":"##### Analysis of Target Variables","7a1e7dff":"#### Numerical and Categorical Variables","00448753":"### 5. Feature Scaling","c348f23d":"### 6. Model\n#### Multiple Linear Regression","ac5c559c":"### 3. Feature Extraction","c2ad4a2d":"##### Analysis of Correlation","b6585bf1":"### 1. Explanatory Data Analysis","85dc43d2":"#### Missing Values Analysis","1dcb692b":"#### Prediction Success"}}