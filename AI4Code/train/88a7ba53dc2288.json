{"cell_type":{"a478ca7f":"code","b1826999":"code","7361d27c":"code","d2ea5fa8":"code","dc8a7f68":"code","5fa497c8":"code","9aab9e48":"code","b8f069f9":"code","04a41c3b":"code","e5e56ae3":"code","a90ac4de":"code","71412c3d":"code","c4622928":"code","8dec496b":"code","8e6bd6c9":"code","adcca128":"code","7387c32f":"code","907ce040":"code","1336ed27":"code","38f60e9e":"code","b84fcc05":"code","26a5394f":"code","c3d7a0da":"code","af7e7e87":"code","ce9a4aec":"code","d78286ab":"code","a1deb658":"code","5e4bc69c":"code","2aa362f8":"code","73520aef":"code","8ed8d3d6":"code","be17104f":"code","daa6f52c":"code","c38e1521":"code","76af54b3":"code","0a63a9b5":"code","4c2cbade":"code","3f6ba1a5":"code","e1473f88":"code","54862f0f":"code","d4cea84e":"code","0ec33698":"code","bd9c5a36":"code","6d5daadf":"code","15f55f04":"code","5a7696dd":"code","2fed5777":"code","cdc3ca43":"code","ea4d35f9":"code","42839e06":"code","2110903c":"code","1e927f58":"code","7338cf3c":"code","c69d01ce":"code","63b69591":"code","f3bdcf57":"code","0bdcd19e":"code","a59f6ddf":"code","420ed722":"code","34a59d2c":"code","d3cc6ca1":"code","fb2dd7a2":"code","fb62fb9a":"code","ff6642f5":"code","6a57437b":"code","b954888d":"code","dd7d81a1":"code","5c572b0d":"code","b4d8d1e5":"code","e9cee77d":"code","3f4bfebf":"code","fa4de696":"code","4123ece8":"code","65bced7c":"code","6a984d6c":"code","c1d7f3db":"code","f3e8dbd4":"code","833b65c3":"code","c00f9c10":"code","dcea9d9e":"code","8ac999c2":"code","0b05c78a":"code","c217b3cc":"code","c23a6708":"code","aa7a05da":"code","591cfad1":"code","cd96e99b":"code","7e83bd0e":"code","ec082a3f":"code","5187c99d":"code","c7a8f763":"code","ff45924a":"code","133320af":"code","90d5c725":"code","aa0f74cb":"code","cff753b5":"code","bb077db5":"code","8f439bbd":"code","c5bcaded":"code","26925664":"code","926ecdee":"code","578bbb6a":"code","3a687a77":"code","44e68b96":"code","80135ae4":"markdown","180a0bd8":"markdown","7af3a013":"markdown","a8422b12":"markdown","9478936b":"markdown","450a3282":"markdown","48446b03":"markdown","296b7e85":"markdown","66ed14c7":"markdown","19827926":"markdown","96d17a49":"markdown","b8b49069":"markdown","c16ca9d4":"markdown","e2418a3c":"markdown","f4d6dbc4":"markdown","9d3e8d83":"markdown","b4372e0f":"markdown","89eb1af7":"markdown","87e9ee72":"markdown","71f95ec0":"markdown","c1fcd7e2":"markdown","1427d397":"markdown","f93d3f42":"markdown","65ecc784":"markdown","59d2d92b":"markdown","92da9fa6":"markdown","895e236d":"markdown","f07d8f5a":"markdown","61ab23bd":"markdown","d778723d":"markdown","c15e4f0d":"markdown","06a219e5":"markdown","13ec0178":"markdown","0c1bb48b":"markdown","9df60fb0":"markdown","7da2a073":"markdown","1241b604":"markdown","27c831cb":"markdown","4843689f":"markdown","26c0c232":"markdown","ee167e74":"markdown","2a568625":"markdown","55287794":"markdown","b989303e":"markdown","52f0bbee":"markdown"},"source":{"a478ca7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1826999":"# carga de librer\u00edas\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from matplotlib import pyplot\nimport seaborn as sns\nsns.__version__\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n#import math\n#import datetime\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.utils import plot_model\nfrom sklearn import preprocessing\n#from sklearn.preprocessing import StandardScaler, Normalizer, Binarizer, MinMaxScaler\n#from sklearn.metrics import mean_squared_error\n# opci\u00f3n para ver todas las columnas\n#pd.options.display.max_columns = 999","7361d27c":"sns.__version__","d2ea5fa8":"datae = pd.read_csv('\/kaggle\/input\/energy-consumption-generation-prices-and-weather\/energy_dataset.csv', parse_dates=['time'])\ndatae.head(2)","dc8a7f68":"datae.info()","5fa497c8":"datae['time']=pd.to_datetime(datae['time'], infer_datetime_format=True, utc=True)\ndatae = datae.set_index('time')","9aab9e48":"datae.columns=[x.replace('\/','_') for x in datae.columns]\ndatae.columns=[x.replace('-','_') for x in datae.columns]\ndatae.columns=[x.replace(' ','_') for x in datae.columns]","b8f069f9":"datae.apply(lambda x: len(x.unique()))","04a41c3b":"datae = datae.drop(['generation_fossil_coal_derived_gas','generation_fossil_oil_shale',\n                    'generation_fossil_peat', 'generation_geothermal',\n                    'generation_hydro_pumped_storage_aggregated', 'generation_marine',\n                    'generation_wind_offshore', 'forecast_wind_offshore_eday_ahead',\n                    'forecast_solar_day_ahead', 'forecast_wind_onshore_day_ahead'], axis=1)","e5e56ae3":"def show_raw_visualization(data, nrows, width, height):\n    #time_data1 = data.index\n    fig, axes = plt.subplots(\n        nrows=nrows, ncols=2, figsize=(width, height), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n    )\n    for i in range(len(feature_keys)):\n        key = feature_keys[i]\n        c = colors[i % (len(colors))]\n        t_data = data[key]\n        t_data.index = data.index\n        t_data.head()\n        ax = t_data.plot(\n            ax=axes[i \/\/ 2, i % 2],\n            color=c,\n            title=\"{} - {}\".format(key, titles[i]),\n            rot=25,\n        )\n        ax.legend([titles[i]])\n    plt.tight_layout()\n    \ncolors = [\"blue\", \"orange\", \"green\", \"red\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\ntitles = [ 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh',\n          'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'MWh', 'Euro', 'Euro']","a90ac4de":"feature_keys = ['generation_biomass', 'generation_fossil_brown_coal_lignite', 'generation_fossil_gas', 'generation_fossil_hard_coal']\nshow_raw_visualization(datae[:24*7*4],nrows=2,width=12,height=4)","71412c3d":"datae.describe().transpose()","c4622928":"def show_hist_boxplot(cols):\n    cols = cols\n    for col in cols:\n        plt.figure(figsize=(12,2))\n        plt.subplot(1,2,1)\n        sns.histplot(data=datae, x=col, bins=30, kde=True)\n        plt.subplot(1,2,2)\n        sns.boxplot(data=datae, x=col)\n        plt.show()","8dec496b":"cols = ['generation_fossil_brown_coal_lignite', 'generation_hydro_pumped_storage_consumption', 'generation_nuclear'] \nshow_hist_boxplot(cols)","8e6bd6c9":"datae.loc[datae.generation_fossil_brown_coal_lignite ==0, 'generation_fossil_brown_coal_lignite'] = np.nan","adcca128":"datae.loc[datae.generation_biomass <110, 'generation_biomass'] = np.nan\ndatae.loc[datae.generation_fossil_gas ==0, 'generation_fossil_gas'] = np.nan\ndatae.loc[datae.generation_fossil_oil <50, 'generation_fossil_oil'] = np.nan\ndatae.loc[datae.generation_nuclear <2000, 'generation_nuclear'] = np.nan\ndatae.loc[datae.generation_other_renewable <20, 'generation_other_renewable'] = np.nan\ndatae.loc[datae.generation_waste <50, 'generation_waste'] = np.nan","7387c32f":"datae.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)","907ce040":"def show_heat_map(df,cols, width=5, height=5):\n    width=width\n    height=height\n    df=df\n    cols=cols\n    plt.figure(figsize=(width,height))\n    sns.heatmap(df[cols].corr(), annot=True)\n    plt.show()","1336ed27":"cols =['generation_hydro_pumped_storage_consumption', 'generation_fossil_gas', 'generation_fossil_oil', \n  'generation_hydro_water_reservoir', 'price_actual', 'generation_fossil_hard_coal', 'generation_fossil_brown_coal_lignite', 'total_load_actual']\nshow_heat_map(datae,cols,10,2)","38f60e9e":"# lectura de datos clim\u00e1ticos con pandas\ndataw = pd.read_csv('\/kaggle\/input\/energy-consumption-generation-prices-and-weather\/weather_features.csv', parse_dates=['dt_iso'])\ndataw.head(2)","b84fcc05":"dataw.info()","26a5394f":"cols = ['pressure', 'humidity', 'wind_speed', 'wind_deg', 'clouds_all', 'weather_id']\nfor col in cols:\n    dataw[col]=dataw[col].values.astype(np.float64)\n\ndataw['time']=pd.to_datetime(dataw['dt_iso'], infer_datetime_format=True, utc=True)\ndataw = dataw.set_index('time')\ndataw = dataw.drop(['dt_iso'], axis=1)","c3d7a0da":"dataw.apply(lambda x: len(x.unique()))","af7e7e87":"dataw = dataw.drop(['rain_1h','weather_main', 'weather_description','weather_icon', 'weather_id'], axis=1)","ce9a4aec":"feature_keys = ['temp', 'pressure', 'humidity','wind_speed']\nshow_raw_visualization(dataw[:24*7*4],nrows=2,width=12,height=4)","d78286ab":"dataw.describe().transpose()","a1deb658":"print(dataw.duplicated().sum())","5e4bc69c":"#dataw = dataw.reset_index().drop_duplicates(subset=['time', 'city_name'], keep='first').set_index('time')\n#print(dataw.duplicated().sum())","2aa362f8":"features = ['pressure','wind_speed', 'wind_deg', 'rain_3h'] \ndef show_hist_boxplot2(features):\n    features = features\n    for feature in features:\n        plt.figure(figsize=(12,3))\n        plt.subplot(1,2,1)\n        sns.histplot(data=dataw, x=feature, bins=30, kde=True, hue='city_name')\n        plt.subplot(1,2,2)\n        sns.boxplot(data=dataw, x=feature, y='city_name')\n        plt.show()","73520aef":"dataw.loc[dataw.pressure >1030, 'pressure'] = np.nan\ndataw.loc[dataw.pressure <800, 'pressure'] = np.nan\ndataw.loc[dataw.wind_speed >100, 'wind_speed'] = np.nan\ndataw.loc[dataw.rain_3h >2, 'rain_3h'] = np.nan","8ed8d3d6":"dataw.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)","be17104f":"wv = dataw['wind_speed']\n# Convert to radians.\nwd_rad = dataw['wind_deg']*np.pi \/ 180\n\n# Calculate the wind x and y components.\ndataw['Wx'] = wv*np.cos(wd_rad)\ndataw['Wy'] = wv*np.sin(wd_rad)","daa6f52c":"dataw.groupby('city_name').size()","c38e1521":"# Split the df_weather into 5 dataframes (one for each city)\ndf_bcn, df_bil, df_mdr, df_sev, df_val = [x for _, x in dataw.groupby('city_name')]\ndfs = [df_bcn, df_bil, df_mdr, df_sev, df_val]","76af54b3":"# Merge all dataframes into the final dataframe\ndata = datae\n\nfor df in dfs:\n    city = df['city_name'].unique()\n    city_str = str(city).replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '')\n    df = df.add_suffix('_{}'.format(city_str))\n    data = data.merge(df, on=['time'], how='outer')\n    data = data.drop('city_name_{}'.format(city_str), axis=1)","0a63a9b5":"cols =['humidity_Valencia','humidity_Barcelona', 'humidity_Seville','humidity_Bilbao',\n       'temp_min_Valencia','temp_Valencia','temp_max_Valencia', 'temp_Seville',\n       'humidity_Madrid','temp_Madrid','wind_speed_Valencia', 'total_load_actual']\nshow_heat_map(data, cols, 10, 2)","4c2cbade":"result = seasonal_decompose(data['total_load_actual'][:24*7*4], model='multiplicative', period = 24)\nt = data.index[:24*7*4]","3f6ba1a5":"def show_decompose_observed(result):\n    result=result\n    s1 = result.observed\n    s2 = result.seasonal\n    s3 = result.trend\n    s4 = result.resid\n    fig, ax = plt.subplots(figsize=(16, 4))\n    ax.set(xlabel=\"Date\",\n           ylabel=\"(MWh)\",\n           title=\"Total actual load\\n jan 2015\"\n          )\n    ax.plot(t,s1,'k')\n    plt.show()","e1473f88":"show_decompose_observed(result)","54862f0f":"pd.DataFrame(np.around(result.seasonal[1:25].values, decimals=2), columns=['ce'],\n             index=['0h','1h','2h','3h','4h','5h','6h','7h','8h','9h','10h','11h',\n                    '12h','13h','14h','15h','16h','17h','18h','19h','20h','21h','22h','23h']).transpose()","d4cea84e":"def show_decompose(result):\n    result=result\n    s1 = result.observed\n    s2 = result.seasonal\n    s3 = result.trend\n    s4 = result.resid\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 8), sharex=True)\n    fig.suptitle('Descomposition of multiplicative time series\\n jan 2015')\n    #fig.subplots_adjust(hspace=0.1)\n\n    ax1.plot(t, s2, 'k')\n    ax1.set_ylabel('Seasonal')\n\n    ax2.plot(t, s3, 'k')\n    ax2.set_ylabel('Trend')\n\n    ax3.plot(t, s4, 'k')\n    ax3.set_xlabel('time (H)')\n    ax3.set_ylabel('Resid')\n\n    plt.show()","0ec33698":"show_decompose(result)","bd9c5a36":"def show_correlogramas():\n    fig, axs = plt.subplots(1, 2, figsize=(16, 3))\n    sm.graphics.tsa.plot_acf(datae['total_load_actual'].values.squeeze(), ax=axs[0], title=(f\"Autocorrelation ({'total_load_actual'})\"), lags=40)\n    sm.graphics.tsa.plot_pacf(datae['total_load_actual'].values.squeeze(), ax=axs[1], title=(f\"Partial Autocorrelation ({'total_load_actual'})\"), lags=40)\n    plt.grid(color='gray', linestyle='-', linewidth=1)\n    plt.show()","6d5daadf":"show_correlogramas()","15f55f04":"scaler = preprocessing.StandardScaler()\n# Create an object to transform the data to fit minmax processor\nx_scaled = scaler.fit_transform(data[['total_load_actual']])\n# Run the normalizer on the dataframe\ndf_normalized = pd.DataFrame(x_scaled, columns=['total_load_actual'], index=data['total_load_actual'].index)","5a7696dd":"load_target = 'total_load_actual' #  \ndf_load = df_normalized[load_target].copy()\n# delete nan rows\ndf_load = df_load.dropna()\ndf_load.head(2)","2fed5777":"# Train-test split\ntest_date = pd.Timestamp(\"2017-12-31 23:00:00+00:00\")\ninit_date = pd.Timestamp(\"2014-12-31 23:00:00+00:00\")\n\ntrain_data = df_load.loc[(df_load.index < test_date) *\n                         (df_load.index > init_date)].values\n\ntest_data = df_load.loc[df_load.index >= test_date].values","cdc3ca43":"mu = train_data.mean()\nsigma = train_data.std()\nprint('mu, sigma: ', mu, sigma)\n\ntrain_data = (train_data - mu) \/ sigma\ntest_data = (test_data - mu) \/ sigma","ea4d35f9":"import random\n\ndef convert2matrix(data_arr, past, future, shuffle=False):\n    X, Y = [], []\n    size = len(data_arr)\n    for i in range(size - future - past + 1):\n        d = i + past\n        y_ind = i + past + future - 1\n        X.append(data_arr[i:d])\n        Y.append(data_arr[y_ind])\n    if shuffle:\n        c = list(zip(X, Y))\n        random.shuffle(c)\n        X, Y = zip(*c)\n    return np.array(X), np.array(Y)","42839e06":"# Create windows\npast, future = (1, 1)\nX_train, y_train = convert2matrix(train_data, past, future, shuffle=True)\nX_test, y_test = convert2matrix(test_data, past, future)","2110903c":"def create_model_dense(past):\n    past = past\n    inputs = keras.layers.Input(shape=[past])\n    outputs = layers.Dense(1) (inputs) \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n    model.summary()\n\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=8)\n    \n    return model,es_callback","1e927f58":"# 1. crear instancia del modelo\nmodel, es_callback = create_model_dense(past=1)","7338cf3c":"# 2. ajuste del modelo con fit\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback], verbose =0)","c69d01ce":"# 3. validaci\u00f3n del modelo \nresults = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Loss: {}'.format(results))","63b69591":"def show_result_model(model):\n    model = model\n    y_pred = model.predict(X_test).flatten() * sigma + mu\n    y_target = y_test * sigma + mu\n    diff = y_pred - y_target\n    print('max deviation: ', np.abs(y_pred - y_target).max())\n    print('RMSE: ', np.mean((y_pred - y_target)**2)**0.5)\n    print('MAE: ', np.abs(y_pred - y_target).mean())\n\n    #plt.figure(figsize=(8, 4))\n    #plt.plot(y_target)\n    #plt.plot(y_pred)\n    #plt.legend(['True {0}'.format(load_target), 'Predictions {0}'.format(load_target)])\n    #plt.grid(True)\n    #plt.show()\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n    # make a little extra space between the subplots\n    #fig.subplots_adjust(hspace=0.5)\n\n    ax1.plot(y_target)\n    ax1.plot(y_pred)\n    ax1.grid(True)\n    ax1.legend(['True {0}'.format(load_target), 'Predictions {0}'.format(load_target)])\n    ax2.plot(y_target[-24*5:])\n    ax2.plot(y_pred[-24*5:])\n    ax2.grid(True)\n    ax2.legend(['Zoom True {0}'.format(load_target), 'Zoom Predictions {0}'.format(load_target)])\n    plt.show()","f3bdcf57":"show_result_model(model)","0bdcd19e":"def model_dnn_uni(past=72, learning_rate=1e-3):\n    learning_rate=learning_rate\n    past=past\n    inputs = keras.layers.Input(shape=[past])\n    l_1 = layers.Dense(10, activation=\"relu\") (inputs) \n    l_2 = layers.Dense(10, activation=\"relu\") (l_1) \n    outputs = layers.Dense(1)(l_2)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.summary()\n    \n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n    path_checkpoint = \"model_checkpoint.h5\"\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        filepath=path_checkpoint,\n        verbose=0,\n        save_weights_only=True,\n        save_best_only=True,\n    )\n    return model, es_callback, modelckpt_callback","a59f6ddf":"def learning_rate_model_dnn_uni(past=72):\n    past=past\n    inputs = keras.layers.Input(shape=[past])\n    l_1 = layers.Dense(10, activation=\"relu\") (inputs) \n    l_2 = layers.Dense(10, activation=\"relu\") (l_1) \n    outputs = layers.Dense(1)(l_2)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n    path_checkpoint = \"model_checkpoint.h5\"\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        filepath=path_checkpoint,\n        verbose=1,\n        save_weights_only=True,\n        save_best_only=True,\n    )\n\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, lr_schedule], verbose=0\n    )\n    return history","420ed722":"history = learning_rate_model_dnn_uni(past=72)","34a59d2c":"# learning rate DNN - univariante\nlrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0.5, 1.3])","d3cc6ca1":"# 1. crear instancia del modelo\nmodel_dnn_uni, es_callback, modelckpt_callback = model_dnn_uni(past=72, learning_rate= 1e-4)","fb2dd7a2":"# 2. ajustar el modelo\nhistory = model_dnn_uni.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, modelckpt_callback], verbose=0\n    )","fb62fb9a":"results = model_dnn_uni.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(results))","ff6642f5":"# 4. forecast dnn\nshow_result_model(model_dnn_uni)","6a57437b":"def learning_rate_model_rnn_uni(past=72):\n    past=72\n    inputs = keras.layers.Input(shape=(past, 1))\n    l_1 = layers.SimpleRNN(10, return_sequences=True, activation=\"relu\") (inputs) \n    l_2 = layers.SimpleRNN(10, return_sequences=False, activation=\"relu\") (l_1) \n    outputs = layers.Dense(1)(l_2)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n        lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\n    es_callback = keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0, patience=8)\n\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, lr_schedule], verbose = 0\n    )\n    return history","b954888d":"def model_rnn_uni(past=1, learning_rate=1e-4):\n    past=72\n    learning_rate=learning_rate\n    inputs = keras.layers.Input(shape=(past, 1))\n    l_1 = layers.SimpleRNN(10, return_sequences=True, activation=\"relu\") (inputs) \n    l_2 = layers.SimpleRNN(10, return_sequences=False, activation=\"relu\") (l_1) \n    outputs = layers.Dense(1)(l_2)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    model.summary()\n    path_checkpoint = \"model_checkpoint.h5\"\n    es_callback = keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0, patience=8)\n\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        filepath=path_checkpoint,\n        verbose=0,\n        save_weights_only=True,\n        save_best_only=True,\n    )\n    \n    \n    \n    return model, es_callback, modelckpt_callback","dd7d81a1":"history = learning_rate_model_rnn_uni(past=72)","5c572b0d":"# learning rate RNN - univariante\nlrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0.5, 1.3])","b4d8d1e5":"# 1. crear instancia del modelo\nmodel_rnn_uni, es_callback, modelckpt_callback = model_rnn_uni(past=72, learning_rate=1e-4)","e9cee77d":"#2. ajustar el modelo\nhistory = model_rnn_uni.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, modelckpt_callback], verbose = 0\n    )","3f4bfebf":"# 3. evaluar el modelo\nresults = model_rnn_uni.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(results))","fa4de696":"# 4. forecast\nshow_result_model(model_rnn_uni)","4123ece8":"def learning_rate_model_gru_uni(past=72):\n    past=72\n    inputs = keras.layers.Input(shape=(past,1))\n    gru_out_1 = keras.layers.GRU(128, return_sequences=True)(inputs)\n    gru_out_2 = keras.layers.GRU(64, return_sequences=True)(gru_out_1)\n    gru_out_3 = keras.layers.GRU(32, return_sequences=False)(gru_out_2)\n    outputs = keras.layers.Dense(1)(gru_out_3)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n        lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\n    es_callback = keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0, patience=8)\n\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 256, callbacks=[es_callback, lr_schedule], verbose = 0\n    )\n    return history","65bced7c":"def model_gru_uni(past=72,learning_rate=1e-4):\n    learning_rate=learning_rate\n    past=past\n    inputs = keras.layers.Input(shape=(past,1))\n    gru_out_1 = keras.layers.GRU(128, return_sequences=True)(inputs)\n    gru_out_2 = keras.layers.GRU(64, return_sequences=True)(gru_out_1)\n    gru_out_3 = keras.layers.GRU(32, return_sequences=False)(gru_out_2)\n    outputs = keras.layers.Dense(1)(gru_out_3)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    model.summary()\n\n    path_checkpoint = \"model_checkpoint.h5\"\n    es_callback = keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0, patience=8)\n\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n            monitor=\"val_loss\",\n            filepath=path_checkpoint,\n            verbose=0,\n            save_weights_only=True,\n            save_best_only=True,\n        )\n    return model, es_callback, modelckpt_callback ","6a984d6c":"history = learning_rate_model_gru_uni(past=72)","c1d7f3db":"# learning rate GRU - univariante\nlrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0.5, 1.3])","f3e8dbd4":"# 1. crear instancia del modelo\nmodel_gru_uni, es_callback, modelckpt_callback = model_gru_uni(past=72)","833b65c3":"# 2. ajuste del modelo\nhistory = model_gru_uni.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_split=0.2, shuffle=True, batch_size = 256, callbacks=[es_callback, modelckpt_callback], verbose = 0\n)","c00f9c10":"# 3. evaluar el modelo\nresults = model_gru_uni.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(results))","dcea9d9e":"# 4. forecast\nshow_result_model(model_gru_uni)","8ac999c2":"def learning_rate_model_lstm_uni(past=72):\n    past=past    \n    inputs = keras.layers.Input(shape=(past,1))\n    lstm_out_1 = keras.layers.LSTM(64, return_sequences=False, activation=\"tanh\")(inputs)\n    #lstm_out_2 = keras.layers.LSTM(64, return_sequences=True, activation=\"relu\")(lstm_out_1)\n    #lstm_out_3 = keras.layers.LSTM(64, return_sequences=False, activation=\"relu\")(lstm_out_2)\n    outputs = keras.layers.Dense(1)(lstm_out_1)\n    model = keras.Model(inputs=inputs, outputs=outputs, name='lstm_univariate_lr')\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n    #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n        lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\n    es_callback = keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0, patience=8)\n\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, lr_schedule], verbose = 0\n    )\n    return history","0b05c78a":"def model_lstm_uni(past=72,learning_rate=1e-3):\n    learning_rate= learning_rate\n    inputs = keras.layers.Input(shape=(past,1))\n    lstm_out_1 = keras.layers.LSTM(32, return_sequences=True, activation=\"relu\")(inputs)\n    lstm_out_2 = keras.layers.LSTM(32, return_sequences=False, activation=\"relu\")(lstm_out_1)\n    #lstm_out_3 = keras.layers.LSTM(32, return_sequences=False, activation=\"relu\")(lstm_out_2)\n    outputs = keras.layers.Dense(1)(lstm_out_2)\n    model = keras.Model(inputs=inputs, outputs=outputs, name='lstm_univariate')\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n        #optimizer=keras.optimizers.Adam()\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    model.summary()\n\n    path_checkpoint = \"model_checkpoint.h5\"\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=8)\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",filepath=path_checkpoint,verbose=0,\n        save_weights_only=True,save_best_only=True,)\n    return model, es_callback, modelckpt_callback \n","c217b3cc":"# defining the learning rate LSTM univariante\nhistory = learning_rate_model_lstm_uni(past=72)","c23a6708":"# learning rate GRU - univariante\nlrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0.6, 1])","aa7a05da":"# 1. crear instancia del modelo\nmodel_lstm_uni, es_callback, modelckpt_callback = model_lstm_uni(past=72, learning_rate=1e-4)","591cfad1":"# 2. ajuste del modelo\nhistory = model_lstm_uni.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback, modelckpt_callback], verbose = 0\n)","cd96e99b":"# 3. evaluar el modelo\nresults = model_lstm_uni.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(results))","7e83bd0e":"# 4. forecast\nshow_result_model(model_lstm_uni)","ec082a3f":"scaler = preprocessing.StandardScaler()\n\n# Create an object to transform the data to fit minmax processor\nx_scaled = scaler.fit_transform(data[['total_load_actual', 'generation_hydro_pumped_storage_consumption',\n                                      'humidity_Valencia', 'temp_Valencia']])\n\n# Run the normalizer on the dataframe\ndf_normalized = pd.DataFrame(x_scaled,\n                             columns=['total_load_actual', 'generation_hydro_pumped_storage_consumption',\n                                      'humidity_Valencia', 'temp_Valencia'],\n                             index=data['total_load_actual'].index)","5187c99d":"load_target = 'total_load_actual' \nloads =  ['total_load_actual', 'generation_hydro_pumped_storage_consumption',\n          'humidity_Valencia', 'temp_Valencia'] \ndf_multi = data[loads].dropna()","c7a8f763":"train_data = df_multi.loc[(df_multi.index < test_date) * (df_multi.index > init_date ), : ].copy()\ntest_data = df_multi.loc[df_multi.index >= test_date, :].copy()","ff45924a":"features = df_multi.columns\nmu_dict = {}\nsigma_dict = {}\nfor c in features:\n    mu = train_data[c].mean()\n    sigma = train_data[c].std()\n    mu_dict[c] = mu\n    sigma_dict[c] = sigma","133320af":"for c in features:\n    mu = mu_dict[c]\n    sigma = sigma_dict[c]\n    train_data.loc[:, c] = (train_data[c].values - mu) \/ sigma\n    test_data.loc[:, c] = (test_data[c].values - mu) \/ sigma","90d5c725":"def convert2matrix_multi(df, past, future, target, shuffle=False):\n    X, Y = [], []\n    size = len(df)\n    for i in range(size - future - past + 1):\n        d = i + past\n        y_ind = i + past + future - 1\n        X.append(df.iloc[i:d, :].values)\n        Y.append(df.iloc[y_ind][target])\n    if shuffle:\n        c = list(zip(X, Y))\n        random.shuffle(c)\n        X, Y = zip(*c)\n    return np.array(X), np.array(Y)","aa0f74cb":"# Create windows\npast, future = (72, 24)\nX_train, y_train = convert2matrix_multi(\n    train_data, past, future, target=load_target, shuffle=True)\nX_test, y_test = convert2matrix_multi(\n    test_data, past, future, target=load_target, shuffle=False)","cff753b5":"X_train.shape, y_train.shape, X_test.shape","bb077db5":"def learning_rate_model_lstm_multi():\n    inputs = keras.layers.Input(shape=(past, len(features)))\n    lstm_out_1 = keras.layers.LSTM(64, return_sequences=True, activation=\"relu\")(inputs)\n    lstm_out_2 = keras.layers.LSTM(64, return_sequences=False, activation=\"relu\")(lstm_out_1)\n    outputs = layers.Dense(1)(lstm_out_2)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    #model.summary()\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n            lambda epoch: 1e-3 * 10**(epoch \/ 20))\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)    \n\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_split=0.2, shuffle=True, batch_size = 256, callbacks=[es_callback, lr_schedule], verbose=1)\n    return history","8f439bbd":"def create_model_lstm_multi(past=72, learning_rate=1e-3):\n    learning_rate=learning_rate\n    past=past\n    inputs = keras.layers.Input(shape=(past, len(features)))\n    lstm_out_1 = keras.layers.LSTM(64, return_sequences=True, activation=\"relu\")(inputs)\n    lstm_out_2 = keras.layers.LSTM(64, return_sequences=False, activation=\"relu\")(lstm_out_1)\n    outputs = layers.Dense(1)(lstm_out_2)\n    model = keras.Model(inputs=inputs, outputs=outputs, name='lstm_multivariate')\n\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    model.summary()\n    path_checkpoint = \"model_checkpoint.h5\"\n    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n\n    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        filepath=path_checkpoint,\n        verbose=0,\n        save_weights_only=True,\n        save_best_only=True)\n    return model, es_callback, modelckpt_callback ","c5bcaded":"#creamos una instancia vacia del modelo\nmodel_lstm_multi, es_callback, modelckpt_callback = create_model_lstm_multi(past=72, learning_rate=1e-3)","26925664":"#ajustamos el modelo\nhistory = model_lstm_multi.fit(X_train, y_train,\n                    epochs=100,validation_split=0.2, shuffle=True,\n                    batch_size = 256, callbacks=[es_callback, modelckpt_callback],\n                    verbose = 1)","926ecdee":"results = model_lstm_multi.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(results))","578bbb6a":"show_result_model(model_lstm_multi)","3a687a77":"lst =[[2.60275911651617,0.31050460453204637,0.23456151420759072],\n      [3.173406952247676, 0.5475616746802036, 0.40711991662272196],\n      [2.6411513789891767, 0.5911976807785342, 0.45908545344480695],\n      [2.57905954244278, 0.6726433873540687, 0.523721400667599],\n      [2.9156839577960874, 0.5965058530867159, 0.4555109290223591],\n      [5.24400301281467,    0.6791002783623662,     0.5188484700825587]]\nbenchmark = pd.DataFrame(lst,columns=['max deviation', 'RMSE', 'MAE' ],\n                         index=['single layer','DNN', 'RNN', 'GRU', 'LSTM', 'LSTM multi' ])\nbenchmark","44e68b96":"plt.figure()    #Figura. Puede incluirse el tama\u00f1o con figsize\nplt.bar(benchmark['max deviation'].index, benchmark['max deviation'].values)          #El gr\u00e1fico\nplt.title('Desviaci\u00f3n m\u00e1xima')      #El t\u00edtulo\nax = plt.subplot()                   #Axis\nax.set_xticks(benchmark.index)             #Eje x\nax.set_xticklabels(benchmark.index)        #Etiquetas del eje x\nax.set_xlabel('Modelos')  #Nombre del eje x\nax.set_ylabel('max deviation')  #Nombre del eje y","80135ae4":"### An\u00e1lisis descriptivo univariable\n\nRealizando un  an\u00e1lisis descriptivo b\u00e1sico. \nGracias a estos resultados, podemos analizar las variables del conjunto de datos y detectar los errores que deben ser tratados posteriormente. En particular,\n\n    -La variable 'pressure',  el valor m\u00e1ximo se encuentra a m\u00e1s de 4 veces de la desviaci\u00f3n est\u00e1ndar del primer cuartil, posible outliner.\n    -La variable 'humidity, el valor m\u00ednimo esta 2 veces la desviaci\u00f3n est\u00e1ndar, posible outliner.\n    -La variable 'wind\\_speed',   el valor m\u00e1ximo se encuentra a m\u00e1s de 33 veces de la desviaci\u00f3n est\u00e1ndar del tercer cuartil, posible outliner.\n","180a0bd8":"### <a name='dnnu'>DNN Deep Neuronal Network - univariate<\/a>","7af3a013":"### <a name='vanila'>Single layer neuronal network - univariate<\/a>\n\nModelo regresi\u00f3n linea de una sola capa","a8422b12":"### Uni\u00f3n de los dos datasets\n\nA continuaci\u00f3n vamos a unir el dataset de datos de energ\u00eda con el dataset de datos clim\u00e1ticos, hemos utilizado el m\u00e9todo empleado por Dimitrios Roussi [[4]](#roussi).","9478936b":"## Serie Temporal Multivariante","450a3282":"### <a name='rnnu'>RNN - univariante<\/a>","48446b03":"# Motivaci\u00f3n\nEn el Acuerdo de Par\u00eds por el cambio clim\u00e1tico de 2015 se acord\u00f3 tomar acciones para salvar el planeta, entre ellas, reducir la emisi\u00f3n de gases de efecto invernadero y mantener el aumento de la temperatura media mundial por debajo de 2\u00b0C [2]. Partiendo de que los sistemas de generaci\u00f3n transporte y distribuci\u00f3n de la energ\u00eda son responsables de aproximadamente una cuarta parte de las emisiones de gases de efecto invernadero (causadas por el hombre)[1]. Mejoras en las predicciones de demanda de electricidad a corto plazo permitir\u00e1 a los operadores el\u00e9ctricos reducir la dependencia de mantener plantas de generaci\u00f3n en funcionamiento a la espera de entrar al sistema el\u00e9ctrico, permitiendo optimizar la gesti\u00f3n del sistema de distribuci\u00f3n y transporte de la energ\u00eda, y por tanto reducir\u00eda significativamente emisiones de CO2 innecesarias. Por ello, la motivaci\u00f3n de este Trabajo de Fin de M\u00e1ster es: encontrar modelos de predicci\u00f3n para determinar la demanda energ\u00e9tica horaria y compararla con las predicciones realizadas por un operador de energ\u00eda, para ello, hacemos uso de t\u00e9cnicas de Machine\/Deep Learning.","296b7e85":"### visualizaci\u00f3n de los datos\n\nPara darnos una idea visual de los datos los representamos gr\u00e1ficamente. Observamos las series temporales de temperatura, presi\u00f3n, porcentaje de humedad relativa y velocidad del viento durante un periodo de un mes. Vemos, tanto en la serie de temperatura como en la de porcentaje de humedad relativa una temporalidad en los datos. ","66ed14c7":"# Visibilidad\n\nEste trabajo hace parte de la evaluaci\u00f3n del M\u00e1ster online en Big Data & Business Analitycs de la Universidad Complutense de Madrid y ntic","19827926":"#### Detecci\u00f3n de las relaciones entre las variables.\n\nA continuaci\u00f3n, ya con el dataset completo con los datos de energ\u00eda y clim\u00e1ticos buscamos otras variables (clim\u00e1ticas) que m\u00e1s se relacionan con 'total_load_actual'. En el diagrama de heat map vemos que la variable que m\u00e1s se relaciona es la humedad en valencia, seguida por la humedad en Barcelona.","96d17a49":"#### Correcci\u00f3n de los errores detectados - datos de energ\u00eda\nMissing no declarados en la variable 'generation_fossil_brown_coal_lignite'. Asignamos NAs a estos valores.","b8b49069":"Normalizaci\u00f3n de los datos","c16ca9d4":"## LSTM -Multivariante","e2418a3c":"Para generar el primer modelo a partir de una serie temporal iniciamos s\u00f3lo con una variable energ\u00e9tica, 'total_load_actual'.","f4d6dbc4":"Verificamos que todas las variables num\u00e9ricas toman m\u00e1s de 10 valores diferentes dropearemos las que lo incumplan.","9d3e8d83":"# <a name='introduccion'>Introducci\u00f3n<\/a>\n\nPara crear el modelo de predicci\u00f3n de demanda energ\u00e9tica horaria, se utilizan dos datasets de  [\"Hourly energy demand generation and weather\"](https:\/\/www.kaggle.com\/nicholasjhana\/energy-consumption-generation-prices-and-weather) [[3]](#ds), disponibles en kaggle.  El primer dataset contiene datos de generaci\u00f3n de energ\u00eda en Espa\u00f1a en Mega Watts (MW) por diferentes tipos de energ\u00eda primaria (Biomasa, Carb\u00f3n, Solar, E\u00f3lica, nuclear, entre otras), datos del precio de la electricidad en EUR\/MWh y datos de las predicciones  realizadas tanto para la generaci\u00f3n total como para el precio de la electricidad a un d\u00eda vista (24 horas en el futuro); y el segundo dataset contiene datos clim\u00e1ticos (temperatura, presi\u00f3n atmosf\u00e9rica, velocidad del viento, entre otros) de cinco ciudades en Espa\u00f1a (Sevilla, Madrid, Valencia, Bilbao y Barcelona). Los datos tienen una frecuencia horaria entre el primero de enero de 2015  y el 31 de diciembre de 2017. Seguiremos una metodolog\u00eda que se inicia con un an\u00e1lisis exploratorio de los datos (EDA), posteriormente un an\u00e1lisis cuantitativo de las variables continuas, seguimos con la creaci\u00f3n de variables nuevas (feature engineering), posteriormente creamos un modelo lo ajustamos y finalmente evaluamos los errores del modelo. ","b4372e0f":"### Representaci\u00f3n gr\u00e1fica y descomposici\u00f3n estacional\n\nObservamos que la serie es estacionaria puesto que sus valores flucutan alrededor  de un termino constante, 30000 MWh. Tambi\u00e9n, podemos observar que la serie tiene un comportamiento estacional, observamos una forma repetible cada d\u00eda en la cual hay una hora con un pico negativo de generaci\u00f3n seguida por un par de picos de generaci\u00f3n positivos. Utilizando la funci\u00f3n seasonal_decompose() con el par\u00e1metro model = 'multiplicative' creamos un objero 'result' con la descomposici\u00f3n estaciona tipo multiplicativo.","89eb1af7":"# <a name='edae'>An\u00e1lisis Exploratorio - Datos de energ\u00eda<\/a>\n\nInicialmente cargamos las librer\u00edas y comenzamos leyendo los datos .csv con la funci\u00f3n read_csv() de pandas. A continuaci\u00f3n, verificamos que los tipos de variables se hayan asignado correctamente.","87e9ee72":"Los valores fuera de rango, aproximadamente, 3 veces alejandos de la desviaci\u00f3n est\u00e1ndar, les asignamos NAs.","71f95ec0":"Observamos que los nombres de las columnas incluyen espacios, '-' y '\/' los cuales los reemplazamos por '_'.","c1fcd7e2":"#### Feature engineering\nCreamos dos nuevas variables que representen vectorialmente a la variable con su m\u00f3dulo y direcci\u00f3n. Para ello, primero transformamos la direcci\u00f3n del viento a radianes y posteriormente calculamos las componentes x e y, utilizamos la metodolog\u00eda descrita en el tutorial de Tensor flow para predicci\u00f3n de series temporales [[6]](#tensorflow).","1427d397":"A continuaci\u00f3n, verificamos los histogramas para verificar posibles outliners. observamos un outliner en la variable de velocidad del viento. Adem\u00e1s, vemos que en la variable de direcci\u00f3n del viento al esta en grados 0 y 360 tendr\u00edan interpretaciones diferentes para el modelo siendo que representan el mismo valor.","f93d3f42":"A continuaci\u00f3n, para verificar si los valores 999 de la variable 'generation_fossil_brown_coal_lignite' son efectivamente datos ausentes y posibles outliners obtenemos sus histogramas y su distribuci\u00f3n por cuartiles.","65ecc784":"Para tener una idea visual de los datos, utiliamos la funci\u00f3n show_raw_visualization() seleccionamos las cuatro primeras variables y observamos los datos del primer mes. Observamos que las series son estacionarias puesto que fluctuan respecto a un valor constante. Adem\u00e1s observamos que tiene estacionalidad, ciclos que repiten diariamente.\n\nHemos utilizado La funci\u00f3n para visualizaci\u00f3n de datos de [Timeseries forecasting for weather prediction](https:\/\/keras.io\/examples\/timeseries\/timeseries_weather_forecasting\/)","59d2d92b":"Tratamiento de datos at\u00edpicos\nLlegados a este punto, los datos est\u00e1n limpios de errores e incongruencias, por lo que podemos centrarnos en la b\u00fasqueda y tratamiento de los datos at\u00edpicos. Para ello, vamos a imputar directamente los datos ausentes por valores v\u00e1lidos usando la opci\u00f3n de interpolaci\u00f3n lineal ya que por la forma en la que est\u00e1n dispuestos estos datos puede ser una buena aproximaci\u00f3n.","92da9fa6":"A continuaci\u00f3n, debemos verificar que todas las variables num\u00e9ricas toman m\u00e1s de 10 valores diferentes. La variable 'generation_fossil_coal_derived_gas' lo incumple, toma valores de 0.0 o NaN. Igual sucede con las otras variables que toman 1 y 2 valores distintos por lo que las vamos a dropear. Adem\u00e1s, dropeamos 'forecast_solar_day_ahead' y 'forecast_wind_onshore_day_ahead' por no estar interesados en el forecast de estas variables.","895e236d":"## LSTM - univariante","f07d8f5a":"#### An\u00e1lisis descriptivo univariable\nComenzamos realizando un primer an\u00e1lisis descriptivo b\u00e1sico con la funci\u00f3n describe(). As\u00ed, podemos analizar las variables del conjunto de datos y detectar los errores que deben ser tratados posteriormente. En particular, \n\n1. En las variables 'generation_biomass', 'generation_fossil_gas', 'generation_fossil_oil', 'generation_nuclear', 'generation_other_renewable', 'generation_waste'  el valor m\u00ednimo se encuentra a m\u00e1s de 2 o 3 veces de la desviaci\u00f3n est\u00e1ndar  del primer cuartil.\n2. La variable  'generation_wind_onshore'  el valor m\u00e1ximo se encuentra a m\u00e1s de 4 veces de la desviaci\u00f3n est\u00e1ndar del tercer cuartil, posible outliner.\n3. La variable 'generation\\_hydro\\_pumped\\_storage\\_consumption', en el primer cuartil sus valores son cero.\n4. La variable 'generation_fossil_brown_coal_lignite' el valor m\u00e1ximo es de 999, lo que suele representar los valores ausentes.","61ab23bd":"# <a name='univariate'>Presentaci\u00f3n de la serie a analizar - Serie temporal univariante<\/a> \nLa serie temporal 'total_load_actual' contiene la carga total generada para el consumo el\u00e9ctrico con 35064 datos horarios entre el 1 de enero de 2015 a las 00:00:00  y el 31 de diciembre de 2018 a las 23:00:00.","d778723d":"A continuaci\u00f3n utilizaremos la funci\u00f3n convert2matrix_multi() para convertir la serie multivariante en un problema de aprendizaje supervisado.","c15e4f0d":"# Referencias\n\n# <a name='ref'>Referencias<\/a>\n\n[1] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio. <a name='tackling'>Tackling Climate Change with Machine Learning<\/a> 2019.\n\n[2] IPCC. <a name='ipcc'>Climate Change 2014<\/a>: Mitigation of Climate Change. Contribution of Working Group III to the Fifth\nAssessment Report of the Intergovernmental Panel on Climate Change O. Edenhofer, R. Pichs-Madruga, Y.\nSokona, E. Farahani, S. Kadner, K. Seyboth, A. Adler, I. Baum, S. Brunner, P. Eickemeier, B. Kriemann, J.\nSavolainen, S. Schlomer, C. von Stechow, T. Zwickel, J.C. Minx, (eds.). 2014. \n\n<a name='ds'>[3]<\/a> Nicholas Jhana, [\"Hourly energy demand generation and weather\nElectrical demand, generation by type, prices and weather in Spain\"](https:\/\/www.kaggle.com\/nicholasjhana\/energy-consumption-generation-prices-and-weather), kaggle. 2019.\n\n<a name='roussi'>[4]<\/a> Dimitrios Roussis [DIMITRIOS ROUSSIS, Electricity price forecasting with DNNs (+ EDA), kaggle.](https:\/\/www.kaggle.com\/dimitriosroussis\/electricity-price-forecasting-with-dnns-eda)\n\n[5] [https:\/\/unfccc.int\/sites\/default\/files\/spanish_paris_agreement.pdf]\n\n\n<a name='tensorflow'>[6]<\/a>Tutorials TensorFlow [Time series forecasting](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series)\n\n<a name='tensorflow'>[7]<\/a> Alberto Ezpondaburu [Intro_deep_learning](https:\/\/github.com\/ezponda\/intro_deep_learning)\n\n<a name='tensorflow'>[8]<\/a> Laurence Moroney\n   [Sequences, Time Series and Prediction](https:\/\/www.coursera.org\/learn\/tensorflow-sequences-time-series-and-prediction)","06a219e5":"Los resultados difieren de los optenidos inicialmente en modo local.\n\nResultados en modo local:","13ec0178":"Observamos que todas las variables son de tipo num\u00e9rico excepto la variable 'time' que ha sido asignada como un objeto, por lo que debemos modificarlo. Para ello, hacemos uso de la funci\u00f3n to_datetime() de pandas para transformarlo en fecha. Le indicamos que infiera el tipo de formato y le indicamos que el tipo de time zone es utc. Adem\u00e1s, lo asignamos como index.","0c1bb48b":"#### Correlogramas\nLos correlogramas los podemos representar con las funciones de autorrelaci\u00f3n simple (ACF) a trav\u00e9s de la funci\u00f3n plot_acf() y plot_pacf() del paquete sm.graphics.tsa.\n\nA partir de la representaci\u00f3n de la funci\u00f3n ACF confirmamos la no estacionaridad ya que los Lag no decrecen lentamente. Adem\u00e1s, confirmamos la estacionalidad ya que aparecen picos en las autocorrelaciones en los retardos multiplos de 24.\n\nPor otro lado, en el autocorrelograma parcial, PACF, vemos que las autocorrelaciones de los retardos 1, 2 y 24 tienen mayor importancia en el primer a\u00f1o que en los siguientes, lo que nos indica una fuerte dependencia por ejemplo entre el instante Xt y Xt-24.","9df60fb0":"# Conclusiones\n\n1. Finalmente se obtuvieron seis distintos modelos de predicci\u00f3n de la demanda el\u00e9ctrica utilizando t\u00e9cnicas de deep learning, de tal manera que los operadores de la red puedan ajustar en tiempo real la generaci\u00f3n total de energ\u00eda a partir de las fuentes de energ\u00eda primaria disponibles, ya sean \u00e9stas energ\u00edas verdes como la e\u00f3lica, solar, h\u00eddrica o a partir de fuentes f\u00f3siles como gas, o carb\u00f3n.\n\n\n2. El primer modelo es una red neuronal de una sola capa y se utiliz\u00f3 como base de referencia (baseline)  para comparar  los otros modelos. Adem\u00e1s, debido a que estamos ante un problema de regresi\u00f3n lineal utilizamos m\u00e9tricas como RMSE y MAE para determinar la precisi\u00f3n de la predicciones y la desviaci\u00f3n respecto del valor actual.\n\n3. Al tratarse de un Para los siguientes cuatro modelos se utiliz\u00f3 una serie temporal univariante con redes neurales DNN, RNN, GRU y LSTM. Para el \u00faltimo modelo se \u00fatilizaron varias series temporales, que adem\u00e1s de los datos de la serie temporal de la demanda total de energ\u00eda tambi\u00e9n contiene otras como variables de energ\u00eda y  variables clim\u00e1ticas, el modelo fue obtenido a partir de redes neuronales LSTM multivariable. \n\n4. Los modelos pueden llegar a productivizarse, es decir, estos reciben un input de 72 horas de la serie temporal y generan un forecast de 24 horas en el futuro.\n\n5. Encontramos que el modelo GRU tiene la menor desviaci\u00f3n, incluso superando al modelo baseline. Por su lado el modelo DNN presenta mejor comportamiento con las parametrizaciones que se han realizado. ","7da2a073":"## GRU - univariante","1241b604":"Al presentar los coeficientes de estacionalidad de cada hora, observamos que a las 3:00 la generaci\u00f3n el\u00e9ctrica disminuye un 32% (1-0.78) y tanto a las 19:00 como a las 20:00 hay un aumento del 15% (1.15). Lo primero se puede explicar debido a que es de madrugada y lo segundo porque es la hora de regreso del trabajo.","27c831cb":"Tratamiento de datos at\u00edpicos\nLlegados a este punto, los datos est\u00e1n limpios de errores e incongruencias, por lo que podemos centrarnos en la b\u00fasqueda y tratamiento de los datos at\u00edpicos. Para ello, vamos a imputar directamente los datos ausentes por valores v\u00e1lidos usando la opci\u00f3n de interpolaci\u00f3n lineal ya que por la forma en la que est\u00e1n dispuestos estos datos puede ser una buena aproximaci\u00f3n.","4843689f":"Detecci\u00f3n de las relaciones entre las variables.\nA continuaci\u00f3n buscamos las variables de energ\u00eda que m\u00e1s se relacionan con 'total_load_actual', como vemos el el heat map la que mayor relaci\u00f3n lineal tiene es generation_hydro_pumped_storage_consumption, seguida de generation_fossil_gas y generation_fossil_oil. Adem\u00e1s, observamos una relaci\u00f3n entre 'generation_fossil_brown_coal_lignite' y 'generation_fossil_hard_coal' y de 'generation_biomass' con 'generation_other'.","26c0c232":"Observamos la variable 'dt_iso' que ha sido asignada como un objeto y las variables num\u00e9ricas tipo int64, por lo que debemos modificarlo. Para ello, hacemos uso de la funci\u00f3n to_datetime() de pandas para transformarlo en fecha. Le indicamos que infiera el tipo de formato y le indicamos que el tipo de time zone es utc. Adem\u00e1s, lo asignamos como index.","ee167e74":"#### Correcci\u00f3n de los errores detectados - datos clim\u00e1ticos\nA los datos de las variable con  outliners les asignamos NAs.","2a568625":"### Escalar los datos","55287794":"#### Transformar la serie temporal a un problema de aprendizaje supervisado\nA continuaci\u00f3n haremos uso de la funci\u00f3n convert2matrix() para crear las ventanas de tipo features y target, siendo past las features y future el target.","b989303e":"### Normalizaci\u00f3n de los datos\u00b6","52f0bbee":"# <a name='edaw'>An\u00e1lisis exploratorio - Datos clim\u00e1ticos<\/a> \n\nPara el segundo dataset comenzamos leyendo los datos tipo csv con la funci\u00f3n read_csv() de pandas. A continuaci\u00f3n, verificamos que los tipos de variables se hayan asignado correctamente."}}