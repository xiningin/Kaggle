{"cell_type":{"09bf9df3":"code","1d5f9e15":"code","feccd123":"code","4d01814a":"code","6826d3bf":"code","308faa49":"code","ec1aef33":"code","f8c0f30c":"code","d92b70b4":"code","e9e3e5a0":"code","74d267f2":"code","cdc1e9a0":"code","c5f1a10d":"code","9f18a336":"code","ccd418a4":"code","24fe8e0d":"code","e6d97d04":"code","89538f41":"code","3c80aaee":"code","1b4ac9a0":"code","0170dae5":"code","c2a06098":"code","cbc7023e":"code","8d60f2cc":"code","4717f1ba":"code","08cb5cf7":"code","f95fb1eb":"code","4d9aa81a":"code","973376a4":"code","30786aa4":"code","b620a675":"code","3f078843":"code","cc4ad699":"code","b765e69f":"code","098ae7a6":"code","7b528c41":"code","e71492f7":"code","8fcdc244":"code","2a82eb56":"code","baeef79f":"code","dcb86685":"code","a97bbf1f":"code","7581a173":"code","3fce8cc5":"code","64a0b346":"code","04ce2690":"code","37a0b985":"code","22a63c01":"code","b8df9614":"markdown","28e3d539":"markdown","8798d2d0":"markdown","a768d0bc":"markdown","ea55e13f":"markdown","b21a7e6a":"markdown","21383968":"markdown","d9aaebd8":"markdown","54013c07":"markdown","b8af80a7":"markdown","dac3828a":"markdown","bfa57886":"markdown","95eb7275":"markdown","d9fb4775":"markdown","9d267919":"markdown","f6dc2b20":"markdown","fddf451d":"markdown","889b3d51":"markdown","6babb2b9":"markdown","94e1e392":"markdown","0d6990b1":"markdown","091c7cee":"markdown","346b5b99":"markdown"},"source":{"09bf9df3":"import io, os, sys, types, time, datetime, math, random, requests, subprocess, tempfile","1d5f9e15":"# Data Manipulation \nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport pylab\nimport scipy.stats as stats\nimport collections\nimport random\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno as ms\nimport seaborn as sns\nimport math\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom collections import Counter\n\n\n# Feature Selection and Encoding\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning \nimport sklearn.ensemble as ske\nfrom sklearn import datasets, model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport tensorflow as tf\n\n# Grid and Random Search\nimport scipy.stats as st\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Metrics\nfrom sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n\n\n\n# Managing Warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Plot the Figures Inline\n%matplotlib inline","feccd123":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(len(train))\nprint(len(test))","4d01814a":"#Set ID column as its own col separate from training set\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain = train.drop('Id', axis = 1)\ntest = test.drop('Id', axis = 1)\n","6826d3bf":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","308faa49":"#Deleting outliers\nprint(len(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index))\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","ec1aef33":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","f8c0f30c":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers\n\n\nnumer = train.select_dtypes([np.int, np.float])\nnums = numer.columns\n\n# detect outliers from numerical columns in our training set\nOutliers_to_drop = detect_outliers(train,4,nums)\n\nprint(len(Outliers_to_drop),'Outliers')\n\nprint(round(len(Outliers_to_drop)\/len(train)*100, 2),'% of the Training set')\n\ntrain.loc[Outliers_to_drop].head() # Show the outliers rows\n","d92b70b4":"# Join Datasets here\n\ndataset = train.append(test)\ndataset.reset_index(inplace=True)\ndataset.drop('index',inplace=True,axis=1)\nprint(len(dataset))\nprint(len(train))\nprint(len(test))","e9e3e5a0":"# Describing all the Numerical Features\ndataset.describe()","74d267f2":"# Describing all the Categorical Features\ndataset.describe(include=['O'])","cdc1e9a0":"ms.matrix(train, figsize = (30,10))","c5f1a10d":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns\n","9f18a336":"missing_values_table(dataset)","ccd418a4":"# To perform our data analysis, let's create new dataframes.\ndataset_bin = pd.DataFrame() # To contain our dataframe with our discretised continuous variables \ndataset_con = pd.DataFrame() # To contain our dataframe with our continuous variables","24fe8e0d":"dataset_bin = dataset\ndataset_con = dataset\n\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \ndataset_bin['MSSubClass'] = dataset['MSSubClass'].apply(str)\ndataset_con['MSSubClass'] = dataset['MSSubClass'].apply(str)\n\ndataset_bin['YrSold'] = dataset['YrSold'].astype(str)\ndataset_con['YrSold'] = dataset['YrSold'].astype(str)\n\ndataset_bin['MoSold'] = dataset['MoSold'].astype(str)\ndataset_con['MoSold'] = dataset['MoSold'].astype(str)\n\n#Lots of missing values,going to fill with most common result\n\ndataset_bin['Functional'] = dataset['Functional'].fillna('Typ')\ndataset_con['Functional'] = dataset['Functional'].fillna('Typ')\n\ndataset_bin['Electrical'] = dataset['Electrical'].fillna(\"SBrkr\")\ndataset_con['Electrical'] = dataset['Electrical'].fillna(\"SBrkr\")\n\ndataset_bin['KitchenQual'] = dataset['KitchenQual'].fillna(\"TA\")\ndataset_con['KitchenQual'] = dataset['KitchenQual'].fillna(\"TA\")\n\ndataset_bin[\"PoolQC\"] = dataset[\"PoolQC\"].fillna(\"None\")\ndataset_con[\"PoolQC\"] = dataset[\"PoolQC\"].fillna(\"None\")\n\ndataset_bin[\"MiscFeature\"] = dataset[\"MiscFeature\"].fillna(\"None\")\ndataset_con[\"MiscFeature\"] = dataset[\"MiscFeature\"].fillna(\"None\")\n\ndataset_bin[\"Alley\"] = dataset[\"Alley\"].fillna(\"None\")\ndataset_con[\"Alley\"] = dataset[\"Alley\"].fillna(\"None\")\n\ndataset_bin[\"Fence\"] = dataset[\"Fence\"].fillna(\"None\")\ndataset_con[\"Fence\"] = dataset[\"Fence\"].fillna(\"None\")\n\ndataset_bin[\"FireplaceQu\"] = dataset[\"FireplaceQu\"].fillna(\"NA\")\ndataset_con[\"FireplaceQu\"] = dataset[\"FireplaceQu\"].fillna(\"NA\")\n\ndataset_bin['Exterior1st'] = dataset['Exterior1st'].fillna(dataset['Exterior1st'].mode()[0])\ndataset_bin['Exterior2nd'] = dataset['Exterior2nd'].fillna(dataset['Exterior2nd'].mode()[0])\ndataset_bin['SaleType'] = dataset['SaleType'].fillna(dataset['SaleType'].mode()[0])\ndataset_con['Exterior1st'] = dataset['Exterior1st'].fillna(dataset['Exterior1st'].mode()[0])\ndataset_con['Exterior2nd'] = dataset['Exterior2nd'].fillna(dataset['Exterior2nd'].mode()[0])\ndataset_con['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)\ndataset_bin['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)\n\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    dataset_con[col] = dataset[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    dataset_con[col] = dataset[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n    dataset_con[col] = dataset[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'Utilities', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF'):\n    dataset_bin[col] = dataset[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    dataset_bin[col] = dataset[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n    dataset_bin[col] = dataset[col].fillna('None')    \n\n    \ndataset_bin['MSZoning'] = dataset.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ndataset_con['MSZoning'] = dataset.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\ndataset_con['LotFrontage'] = dataset.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mode()[0]))\ndataset_bin['LotFrontage'] = dataset.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mode()[0]))\n\n","e6d97d04":"missing_values_table(dataset_con)","89538f41":"# How normally distributed is our output?\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(10,5)) \nsns.distplot(dataset_con['SalePrice']);","3c80aaee":"K = round(stats.kurtosis(dataset_con['SalePrice'].dropna()),2)\nS = round(stats.skew(dataset_con['SalePrice'].dropna()), 2)\nprint('The Kurtosis of the distribution of the target variable is ' + str(K))\nprint('The Skew of the distribution of the target variable is ' + str(S))","1b4ac9a0":"# If we take the log of the Output variable, we can see that it makes the output distribution more normal\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(10,5)) \nsns.distplot(np.log(dataset_con['SalePrice']));","0170dae5":"K = round(stats.kurtosis(np.log(dataset_con['SalePrice']).dropna()),2)\nS = round(stats.skew(np.log(dataset_con['SalePrice']).dropna()), 2)\nprint('The Kurtosis of the distribution of the target variable is now ' + str(K))\nprint('The Skew of the distribution of the target variable is now ' + str(S))","c2a06098":"#Take Log of Saleprice in both datasets (binned and continuous)\n\n#dataset_bin['SalePrice'] = np.log(dataset['SalePrice'])\n#dataset_con['SalePrice'] = np.log(dataset['SalePrice'])","cbc7023e":"categorical = dataset.select_dtypes([np.object])\n\n# Let\u2019s plot the distribution of categorical data feature\ndef plot_distribution(df, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(df.shape[1]) \/ cols)\n    for i, column in enumerate(df.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if df.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=df)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\n    \nplot_distribution(categorical, cols=3, width=20, height=100, hspace=0.45, wspace=0.5)","8d60f2cc":"#Here are all of the columns with numerical values\nnumerical = dataset.select_dtypes([np.int, np.float])\n\ncols1 = numerical.columns\n# cols2 is the subset of these numerical columns that are discrete values\ncols2 = ['MoSold','YrSold', 'GarageCars', 'TotRmsAbvGrd', 'Fireplaces', 'KitchenAbvGr', 'BedroomAbvGr','FullBath','HalfBath','BsmtHalfBath', 'BsmtFullBath', 'OverallQual', 'OverallCond', 'MSSubClass', 'GarageYrBlt']\n\n# cols3 is the list of continuous numerical variables\ncols3 = list(set(cols1).difference(set(cols2)))\n\n#cols3 = [e for e in cols3 if e not in ('PoolArea', 'MiscVal', 'ScreenPorch', \"LowQualFinSF\", 'BsmtFinSF2', 'EnclosedPorch', '3SsnPorch')]\n\ncols3.remove('SalePrice')\ncols3.append('Total_Bathrooms')\ncols3.append('Total_porch_sf')\ncols3.append('TotalSF')\nlen(cols3)","4717f1ba":"A = plt.subplots(figsize=(15,80))\n\nb = 0\n\nfor i in range(10):\n    for j in range(2):\n        ax = plt.subplot2grid((10,2), (i,j))\n        p = sns.scatterplot(x=cols3[b],y = \"SalePrice\", data=numerical, alpha = .35)\n        plt.xticks(rotation = 45)\n        b = b + 1\n        if b > 18:\n            b = 18\n        \nplt.show()\nB = plt.tight_layout()","08cb5cf7":"# simplified features\ndataset_con['HasPool'] = dataset_con['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndataset_con['Has2ndfloor'] = dataset_con['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset_con['HasGarage'] = dataset_con['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndataset_con['HasBsmt'] = dataset_con['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset_con['HasFireplace'] = dataset_con['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#same for the binned dataframe\n\ndataset_bin['HasPool'] = dataset_bin['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndataset_bin['Has2ndfloor'] = dataset_bin['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset_bin['HasGarage'] = dataset_bin['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndataset_bin['HasBsmt'] = dataset_bin['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset_bin['HasFireplace'] = dataset_bin['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","f95fb1eb":"dataset_con.head()","4d9aa81a":"dataset_con['TotalSF'] = dataset_con['TotalBsmtSF'] + dataset_con['1stFlrSF'] + dataset_con['2ndFlrSF']\ndataset_bin['TotalSF'] = dataset_bin['TotalBsmtSF'] + dataset_bin['1stFlrSF'] + dataset_bin['2ndFlrSF']\n\n\ndataset_con['Total_porch_sf'] = (dataset_con['OpenPorchSF'] + dataset_con['3SsnPorch'] +\n                              dataset_con['EnclosedPorch'] + dataset_con['ScreenPorch'] +\n                              dataset_con['WoodDeckSF'])\n\ndataset_bin['Total_porch_sf'] = (dataset_bin['OpenPorchSF'] + dataset_bin['3SsnPorch'] +\n                              dataset_bin['EnclosedPorch'] + dataset_bin['ScreenPorch'] +\n                              dataset_bin['WoodDeckSF'])","973376a4":"dataset_bin['Total_Bathrooms'] = (dataset_bin['FullBath'] + (0.5 * dataset_bin['HalfBath']) +\n                               dataset_bin['BsmtFullBath'] + (0.5 * dataset_bin['BsmtHalfBath']))\ndataset_bin['Total_Bathrooms'] = dataset_bin['Total_Bathrooms'].clip(upper = 4)\n\n\ndataset_con['Total_Bathrooms'] = (dataset_con['FullBath'] + (0.5 * dataset_con['HalfBath']) +\n                               dataset_con['BsmtFullBath'] + (0.5 * dataset_con['BsmtHalfBath']))\n\ndataset_con['Total_Bathrooms'] = dataset_con['Total_Bathrooms'].clip(upper = 4)","30786aa4":"dataset_con.head()","b620a675":"# One Hot Encodes all labels before Machine Learning\n\n#one_hot_cols = dataset_con.columns.tolist()\n#one_hot_cols.remove('SalePrice')\n\ndataset_con_enc = pd.get_dummies(data = dataset_con)\n\ndataset_con_enc.head()","3f078843":"# Label Encode all labels\n#label_enc = dataset_con.columns.tolist()\n#label_enc.remove('SalePrice')\n#dataset_con_enc = dataset_con.apply(LabelEncoder().fit_transform)\n#dataset_con_enc = pd.get_dummies(dataset_bin, columns=one_hot_cols)\n\n#dataset_con_enc.head()","cc4ad699":"missing_values_table(dataset_con_enc)","b765e69f":"print(len(dataset))\nprint(len(test))","098ae7a6":"dataset_con_enc_no_outliers = dataset_con_enc.drop(Outliers_to_drop, axis = 0).reset_index(drop = True)\nprint(len(dataset_con_enc_no_outliers))\ndataset_con_enc_no_outliers.head()\n","7b528c41":"## Split again into train and test\n\ndf_train = dataset_con_enc_no_outliers.iloc[:(1458-len(Outliers_to_drop)),:]\ndf_test = dataset_con_enc_no_outliers.iloc[(1458-len(Outliers_to_drop)):,:]\n\nX = df_train.drop(['SalePrice'], axis=1)\ny = df_train['SalePrice'].astype('float')\ntest_x = df_test.drop(['SalePrice'], axis=1)\n\n\nX.head()","e71492f7":"# Features for feature importances\nfeatures = list(X_train.columns)","8fcdc244":"# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","2a82eb56":"import numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","baeef79f":"from sklearn.model_selection import GridSearchCV\nkfolds=KFold(n_splits=10,shuffle=True,random_state=42)\nscale=RobustScaler().fit(X)\nX1=scale.transform(X)\nfrom sklearn.linear_model import Ridge\n\nmodel=Ridge()\nrid_param_grid = {\"alpha\":[19.8]}\ngrid_search= GridSearchCV(model,param_grid=rid_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nrid_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","dcb86685":"from sklearn.linear_model import Lasso\nmodel=Lasso()\nlas_param_grid = {\"alpha\":[0.0005963623316594642]}\ngrid_search= GridSearchCV(model,param_grid=las_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nlas_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","a97bbf1f":"from sklearn.linear_model import ElasticNet\nmodel=ElasticNet()\nela_param_grid = {\"alpha\":[0.0006951927961775605],\n                 \"l1_ratio\":[0.90]}\ngrid_search= GridSearchCV(model,param_grid=ela_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nela_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","7581a173":"model=GradientBoostingRegressor()\ngbdt_param_grid = {\"n_estimators\":[2200],\n                 \"learning_rate\":[0.05],\n                   \"max_depth\":[3],\n                   \"max_features\":[\"sqrt\"],\n                   \"min_samples_leaf\":[5],\n                   \"min_samples_split\":[12],\n                   \"loss\":[\"huber\"]\n                  }\n                   \n                   \n                   \ngrid_search= GridSearchCV(model,param_grid=gbdt_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\ngbdt_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","3fce8cc5":"from sklearn.model_selection import GridSearchCV\nkfolds=KFold(n_splits=5,shuffle=True,random_state=42)\nscale=RobustScaler().fit(X)\nX1=scale.transform(X)\nmodel=XGBRegressor()\nxgb_param_grid = {\"n_estimators\":[3000],\n                 \"learning_rate\":[0.01],\n                   \"max_depth\":[3],\n                   \"subsample\":[0.8],\n                \"colsample_bytree\":[0.8],\n                 \"gamma\":[0],\n                \"objective\":['reg:linear'],\n                \"min_child_weight\":[2], \n                \"reg_alpha\":[0.1],\n                \"reg_lambda\":[0.5]\n                  }\n                   \n                   \n                   \ngrid_search= GridSearchCV(model,param_grid=xgb_param_grid,cv=kfolds,scoring=\"neg_mean_squared_error\",n_jobs= 10, verbose = 1)\ngrid_search.fit(X1,y)\nxgb_best=grid_search.best_estimator_\nnp.sqrt(-grid_search.best_score_)","64a0b346":"test_x=scale.transform(test_x)","04ce2690":"def blend_models_predict(X):\n    return ((0.15 * ela_best.predict(X)) + \\\n            (#0.3 * las_best.predict(X)) + \\\n            (0.15 * rid_best.predict(X)) + \\\n            (0.7 * xgb_best.predict(X)))))            ","37a0b985":"#predictions = blend_models_predict(test_x)\npredictions = xgb_best.predict(test_x)\n\nsubmission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('TunedXGB.csv')\nsubmission.head()\n\n","22a63c01":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('RandomForest.csv')\nsubmission.shape","b8df9614":"Since we do not expect a normal distribution with all\/or nearly any of our numerical variables, I decided to use the Tukey method (Tukey JW., 1977) to detect ouliers. This is defined as an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is any row that has a feature value outside the IQR +\/- 1 outlier step.\n\nGiven this, I considered any row with 4 or more outliers within one sample as outlier. ","28e3d539":"# Create New Features","8798d2d0":"# Apply Log Transform to SalePrice in Data","a768d0bc":"# Random Forest","ea55e13f":"from sklearn.preprocessing import StandardScaler\n\nTrain_SalePrice = dataset_con_enc_no_outliers['SalePrice']\n\nscaler = StandardScaler()\n\ndataset_con_enc_no_outliers[cols3] = scaler.fit_transform(dataset_con_enc_no_outliers[cols3])\n\n\ndataset_con_enc_no_outliers.head()","b21a7e6a":"I want to split my training set 80-20 to create a validation set.","21383968":"Ensemble!","d9aaebd8":"Standardize and Scale Data","54013c07":"# Feature Engineering","b8af80a7":"# Outlier detection and Removal\n","dac3828a":"XG Boost","bfa57886":"There are 4 different bathroom variables, I am going to combine them to make 1 Total Bathroom feature","95eb7275":"## Split Data, Test, Training Sets","d9fb4775":"# Join Training and Test sets","9d267919":"# Feature Encoding","f6dc2b20":"# Exploratory Data Analysis\n\nAn EDA checklist\n1. What question(s) are we trying to solve (or prove wrong)?\n2. What kind of data do we have and how do we treat different types?\n3. What\u2019s missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data?","fddf451d":"# Feature Reduction\/Selection\n\nRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. The measure based on which the (locally) optimal condition is chosen is called impurity. When training a tree, it can be computed how much each feature decreases the weighted impurity in a tree. For a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. This is the feature importance measure exposed in sklearn\u2019s Random Forest implementations.","889b3d51":"# ML Models\n\n1. Log Transform - Tried this, wasn't helpful for xgBoost model, TBD for other models\n2. Linear, Random Forest, Decision Trees, XGBoost, Neural Network (generate value on continuum)\n3. Split Training set into 80-20 (Train-Validate)\n    * Compare models, compare accuracies\n4. Plot Sale Price vs Predictions\n    * Look for outliers? Remove that point? See what's happening there?\n5. Consider Ensemble Modeling","6babb2b9":"from sklearn.model_selection import train_test_split\n\n# create training and testing vars\n# Split data intro train and validation sets\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X.copy(), y, test_size=0.2, random_state=1)\nprint('Shapes')\nprint('X_train:', X_train.shape)\nprint('X_val:', X_test.shape)\nprint('y_train:', y_train.shape)\nprint('y_val:', y_test.shape)\n\n","94e1e392":"## Dealing with outliers\nOutlinear in the GrLivArea is recommended by the author of the data to remove it. The author says in documentation \u201cI would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students.\u201d","0d6990b1":"Dropping OUtliers here","091c7cee":"# Continuous Features:\n\nSince we have 80 unique variables, I do not think it will be helpful to do deep EDA on each variable, I will choose to do it on the ones more relevant to me, and create more features as well as I see fit. \n\nI will however, separate the 21 continuous variables (not including SalePrice) to see if discretizing them helps the models at all.\n\n## Features: Anything with SF (square footage)\n\nI think that these things can be treated summarily, so I will add all of the housing square footage together, as well as the porch SF. ","346b5b99":"Below we deal with the 35 columns with missing data"}}