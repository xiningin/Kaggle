{"cell_type":{"3a0c6977":"code","08156bde":"code","1cee540d":"code","1003400f":"code","d1adb8fe":"code","4fc071df":"code","339990bf":"code","3c350382":"code","2a58a04b":"code","4e9419a5":"code","3bb13824":"code","4fa8fcba":"code","50ec1203":"code","fbf0dfd9":"markdown","be0fa2bc":"markdown","63417824":"markdown","4b0e81b2":"markdown","3cd05501":"markdown"},"source":{"3a0c6977":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport tensorflow as tf\nimport statsmodels as st\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08156bde":"WEATHER_STA = 14578001\nTEST_WEATHER_STA = 22005003\n\ndef normalize(dataset, target, single_param=False):\n    if single_param:\n        dataNorm = dataset\n        dataNorm[target]=((dataset[target]-dataset[target].min())\/(dataset[target].max()-dataset[target].min()))\n        return dataNorm\n    else:\n        dataNorm=((dataset-dataset.min())\/(dataset.max()-dataset.min()))\n#         dataNorm[target]=dataset[target]\n        return dataNorm\n\ndef segment(dataset, variable, window = 5000, future = 0):\n    data = []\n    labels = []\n    for i in range(len(dataset)):\n        start_index = i\n        end_index = i + window\n        future_index = i + window + future\n        if future_index >= len(dataset):\n            break\n        data.append(dataset[variable][i:end_index])\n        labels.append(dataset[variable][end_index:future_index])\n    return np.array(data), np.array(labels)\n\ndef create_time_steps(length):\n    return list(range(-length, 0))\n\ndef multi_step_plot(history, true_future, prediction):\n    plt.figure(figsize=(12, 6))\n    num_in = create_time_steps(len(history))\n    num_out = len(true_future)\n\n    plt.plot(num_in, np.array(history[:, 0]), label='History')\n    plt.plot(np.arange(num_out), np.array(true_future), label='True Future')\n    if prediction.any():\n        plt.plot(np.arange(num_out), np.array(prediction), 'ro', label='Predicted Future')\n    plt.legend(loc='upper left')\n    plt.show()\n\n","1cee540d":"df2016 = pd.read_csv(r'\/kaggle\/input\/meteonet\/NW_Ground_Stations\/NW_Ground_Stations\/NW_Ground_Stations_2016.csv')\ndf2017 = pd.read_csv(r'\/kaggle\/input\/meteonet\/NW_Ground_Stations\/NW_Ground_Stations\/NW_Ground_Stations_2017.csv')\ndf2018 = pd.read_csv(r'\/kaggle\/input\/meteonet\/NW_Ground_Stations\/NW_Ground_Stations\/NW_Ground_Stations_2018.csv')","1003400f":"weather = df2016[(df2016['number_sta'] == WEATHER_STA)]\nweather = weather.append(df2017[(df2017['number_sta'] == WEATHER_STA)], ignore_index=True)\nweather = weather.append(df2018[(df2018['number_sta'] == WEATHER_STA)], ignore_index=True)\nweather['date'] = pd.to_datetime(weather['date'], format='%Y%m%d %H:%M')\nweather.set_index('date', inplace=True)\nweather['td'] = weather['td'].interpolate('linear')\nweather['precip'] = weather['precip'].interpolate('linear')\nweather['hu'] = weather['hu'].interpolate('linear')\nweather['ff'] = weather['ff'].interpolate('linear')\nweather = weather.drop(['number_sta', 'lat', 'lon', 'height_sta'], axis = 1)\n\nweather_test = df2016[(df2016['number_sta'] == TEST_WEATHER_STA)]\nweather_test = weather_test.append(df2017[(df2017['number_sta'] == TEST_WEATHER_STA)], ignore_index=True)\nweather_test = weather_test.append(df2018[(df2018['number_sta'] == TEST_WEATHER_STA)], ignore_index=True)\nweather_test['date'] = pd.to_datetime(weather_test['date'], format='%Y%m%d %H:%M')\nweather_test.set_index('date', inplace=True)\nweather_test['td'] = weather_test['td'].interpolate('linear')\nweather_test['precip'] = weather_test['precip'].interpolate('linear')\nweather_test['hu'] = weather_test['hu'].interpolate('linear')\nweather_test['ff'] = weather_test['ff'].interpolate('linear')\nweather_test = weather_test.drop(['number_sta', 'lat', 'lon', 'height_sta'], axis = 1)","d1adb8fe":"weather = normalize(weather, 'td', single_param=False)\nweather_test = normalize(weather_test, 'td', single_param=False)","4fc071df":"weather_ds = weather.resample('720T').mean()\nweather_test_ds = weather_test.resample('720T').mean()\n\nweather_ds = weather_ds.fillna(method='bfill')\nweather_test_ds = weather_ds.fillna(method='bfill')","339990bf":"HISTORY_LAG = 100\nFUTURE_TARGET = 50\n\nX_train, y_train = segment(weather_ds, \"td\", window = HISTORY_LAG, future = FUTURE_TARGET)\nX_train = X_train.reshape(X_train.shape[0], HISTORY_LAG, 1)\ny_train = y_train.reshape(y_train.shape[0], FUTURE_TARGET, 1)\nprint(\"Data shape: \", X_train.shape)\nprint(\"Tags shape: \", y_train.shape)","3c350382":"X_test, y_test = segment(weather_test_ds, \"td\", window = HISTORY_LAG, future = FUTURE_TARGET)\nX_test = X_test.reshape(X_test.shape[0], HISTORY_LAG, 1)\ny_test = y_test.reshape(y_test.shape[0], FUTURE_TARGET, 1)\nprint(\"Data shape: \", X_test.shape)\nprint(\"Tags shape: \", y_test.shape)","2a58a04b":"EPOCHS = 200\n\nlstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(HISTORY_LAG, input_shape=X_train.shape[-2:]),\n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Dense(50),\n    tf.keras.layers.Dense(FUTURE_TARGET)\n])\n\nlstm_model.compile(optimizer='adam', metrics=['mae'], loss='mse')\nlstm_model.fit(X_train, y_train, epochs=EPOCHS)\n","4e9419a5":"yPred = lstm_model.predict(X_test, verbose=0)\ny_test = y_test.reshape(y_test.shape[0], FUTURE_TARGET,)","3bb13824":"final_list = []\nval_final_list = []\n\nfor i in yPred:\n    final_list.append(i[40])\n\nnarray = np.array(final_list)\nprint(narray.shape)\n\nfor i in y_test:\n    val_final_list.append(i[40])\n    \nval_narray = np.array(val_final_list)\nprint(val_narray.shape)","4fa8fcba":"plt.figure(figsize=(30,5))\nsns.set(rc={\"lines.linewidth\": 3})\nsns.lineplot(x=np.arange(val_narray.shape[0]), y=val_narray, color=\"green\")\nsns.set(rc={\"lines.linewidth\": 3})\nsns.lineplot(x=np.arange(narray.shape[0]), y=narray, color=\"coral\")\nplt.margins(x=0, y=0.5)\nplt.legend([\"Original\", \"Predicted\"])","50ec1203":"multi_step_plot(X_test[1336], y_test[1336], yPred[1336])","fbf0dfd9":"# Standardized temperature prediction based on history","be0fa2bc":"In this notebook I'll be using a LSTM NN to predict the temperature based on past observations. For this purpose, the steps followed are:\n\n1. Downsampling of the measurements to 1 every 12 hours\n2. Normalization of both feature and target variables\n3. Usage of the last 100 entries (120 hours, 5 days) to predict the next 50 entries (60 hours, 2.5 days)","63417824":"We will use data from the station 14578001 as training data, and data form the station 22005003 as test data","4b0e81b2":"Now that we have trained the network and used it to predict temperatures using data from the 22005003 station, we can check the results. In the first graph, we well see how would it look if we took every 40th prediction from every 100-long-set ","3cd05501":"Here you can check the result of a random prediction (for example, the 1336th set)"}}