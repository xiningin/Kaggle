{"cell_type":{"f500e09f":"code","802174f1":"code","dc0d9776":"code","822b256a":"code","911b97ce":"code","930e1334":"code","01d1f77d":"code","18776d18":"code","02d93a59":"code","e624fbba":"code","59ce60b0":"code","f774e3f4":"code","edd42dd6":"code","aad8b0a4":"code","36837180":"code","09679f8f":"code","b8092f8d":"code","8b4aa422":"code","c6fe6bb7":"code","b73557da":"code","5d8101a0":"code","c973914c":"code","132dcb17":"code","53aadc11":"code","9798e1b7":"code","88cdb7fa":"code","c64a0316":"code","c45f4805":"code","be76667c":"code","9d696242":"code","eac6461a":"code","7ebf164d":"code","da01ce6a":"code","6bffa057":"code","795d763e":"code","e5f95b68":"code","33a53291":"code","74714fe7":"code","a6cfed3c":"code","dbd4e105":"code","1de96bf5":"code","471086b3":"code","7a8b0968":"code","32e7f962":"code","b762133c":"code","52c931a0":"code","b6323175":"code","7bb33fcf":"code","75ae74df":"code","67416c6e":"code","aa7c81e2":"code","323e4aab":"code","ce6d8eb5":"code","f2177f60":"code","736d9df2":"code","7b1c937d":"code","f25a61b2":"code","ccb6def9":"code","52d6e719":"code","3fb41774":"code","d2d7b101":"code","873f9b51":"code","65d3149e":"code","7cb43fb1":"code","ee1e1a22":"markdown","89d4e760":"markdown","c9a43305":"markdown","10fa3601":"markdown","f5c46567":"markdown","591ce47b":"markdown","107a6d74":"markdown","3f13fe75":"markdown","5c3d5ada":"markdown","0df871aa":"markdown","aebbae38":"markdown","01063d06":"markdown","b0250fdc":"markdown","8199c4be":"markdown","0566480a":"markdown","4a30e66d":"markdown","cb60a8ea":"markdown","a81e097b":"markdown","1926e423":"markdown","31c846ea":"markdown","2b2cd045":"markdown","20121b6b":"markdown","09c366e0":"markdown","063f3995":"markdown","1379574d":"markdown","ed45ff1c":"markdown","85e60b98":"markdown","b235c2ae":"markdown","8c5bee02":"markdown","5d073739":"markdown"},"source":{"f500e09f":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()","802174f1":"!pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\n\nfrom torchvision.models import *\nimport pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","dc0d9776":"%%bash\npip install pytorch-pretrained-bert","822b256a":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\",\n)","911b97ce":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","930e1334":"from sklearn.model_selection import train_test_split","01d1f77d":"DATA_ROOT = Path(\"..\") \/ \"input\"\n\ntrain, test = [pd.read_csv(DATA_ROOT \/ fname) for fname in [\"train.csv\", \"test.csv\"]]\ntrain, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)","18776d18":"train.head()","02d93a59":"test.head()","e624fbba":"val.head()","59ce60b0":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","f774e3f4":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])","edd42dd6":"label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\ndatabunch_1 = TextDataBunch.from_df(\".\", train, val, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"comment_text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","aad8b0a4":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","36837180":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","09679f8f":"# this will produce a virtually identical databunch to the code above\ndatabunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=\"comment_text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","b8092f8d":"path=Path('..\/input\/')","8b4aa422":"databunch_2.show_batch()","c6fe6bb7":"databunch_1.show_batch()","b73557da":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\nbert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)","5d8101a0":"loss_func = nn.BCEWithLogitsLoss()","c973914c":"acc_02 = partial(accuracy_thresh, thresh=0.25)","132dcb17":"model = bert_model_class","53aadc11":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch_1, model,\n    loss_func=loss_func, model_dir='\/temp\/model', metrics=acc_02,\n)","9798e1b7":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","88cdb7fa":"x = bert_clas_split(model)","c64a0316":"learner.split([x[0], x[1], x[2], x[3], x[5]])","c45f4805":"learner.lr_find()","be76667c":"learner.recorder.plot()","9d696242":"learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","eac6461a":"learner.save('head')\nlearner.load('head')","7ebf164d":"learner.freeze_to(-2)\nlearner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","da01ce6a":"learner.save('head-2')\nlearner.load('head-2')","6bffa057":"learner.unfreeze()\nlearner.lr_find()\nlearner.recorder.plot(suggestion=True)","795d763e":"learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","e5f95b68":"text = 'you are so sweet'\nlearner.predict(text)","33a53291":"text = 'you are pathetic piece of shit'\nlearner.predict(text)","74714fe7":"src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = \"comment_text\"), \n                   TextList.from_df(val, path=\".\", cols = 'comment_text'))","a6cfed3c":"data_lm = src_lm.label_for_lm().databunch(bs=32)","dbd4e105":"data_lm.show_batch()","1de96bf5":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"\/temp\/model\")","471086b3":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","7a8b0968":"learn.fit_one_cycle(1, max_lr=slice(5e-4, 5e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3))","32e7f962":"learn.save('fit_head')\nlearn.load('fit_head')","b762133c":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","52c931a0":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4,  1e-2))","b6323175":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned')","7bb33fcf":"TEXT = \"He is a piece of\"\nN_WORDS = 10\nN_SENTENCES = 2","75ae74df":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","67416c6e":"src_clas = ItemLists(path, TextList.from_df( train, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab),\n                    TextList.from_df( val, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab))","aa7c81e2":"data_clas = src_clas.label_from_df(cols=label_cols).databunch(bs=32)","323e4aab":"data_clas.show_batch()","ce6d8eb5":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='\/temp\/model', metrics=acc_02, loss_func=loss_func)\nlearn.load_encoder('fine-tuned')","f2177f60":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","736d9df2":"learn.fit_one_cycle(2, max_lr=slice(1e-3, 1e-2), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","7b1c937d":"learn.save('first-head')\nlearn.load('first-head')","f25a61b2":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(5e-2\/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","ccb6def9":"learn.save('second')\nlearn.load('second')","52d6e719":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(5e-2\/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","3fb41774":"learn.save('third')\nlearn.load('third')","d2d7b101":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","873f9b51":"learn.fit_one_cycle(2, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","65d3149e":"learn.predict('she is so sweet')","7cb43fb1":"learn.predict('you are pathetic piece of shit')","ee1e1a22":"# Classification Model","89d4e760":"# Project Description","c9a43305":"# BERT Model","10fa3601":"This is awesome!\n\nWith few number of epochs, we are able to get the accuracy of around 98% on this multi-label classification task.\n\nNow, lets see how does Fastai ULMFiT fare on this task","f5c46567":"In this notebook, I want to use two state of the art Natural Language Processing (NLP) techniques which have sort of revolutionalized the area of NLP in Deep Learning.\n\nThese techniques are as follows:\n\n1. BERT (Deep Bidirectional Transformers for Language Understanding)\n2. Fastai ULMFiT (Universal Language Model Fine-tuning for Text Classification)\n\nBoth these techniques are very advanced and very recent NLP techniques (BERT was introduced by Google in 2018). Both of them incorporate the methods of Transfer Learning which is quite cool and are pre-trained on large corpuses of Wikipedia articles. I wanted to compare the overall performance of these two techniques.\n\nI really like using Fastai for my deep learning projects and can't thank enough for this amazing community and our mentors - Jeremy & Rachael for creating few wonderful courses on the matters pertaining to Deep Learning. Therefore one of my aims to work on this project was to **integrate BERT with Fastai**. This means power of BERT combined with the simplicity of Fastai. It was not an easy task especially implementing Discriminative Learning Rate technique of Fastai in BERT modelling. \n\nIn my project, below article helped me in understanding few of these integration techniques and I would like to extend my gratidue to the writer of this article:\n\n[https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/](http:\/\/)\n\n","591ce47b":"# Fastai - ULMFiT","107a6d74":"Now, we can create our Databunch. Important thing to note here is to use BERT Tokenizer, BERT Vocab. And to and put include_bos and include_eos as False as Fastai puts some default values for these","3f13fe75":"This will have two parts:\n\n1. Training the Language Model\n2. Training the Classifier Model","5c3d5ada":"Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)","0df871aa":"In this section, we will import Fastai libraries and few other important libraries for our task","aebbae38":"## Overall objective","01063d06":"BERT has several flavours when it comes to Tokenization. For our modelling purposes, we will use the most common and standard method named as \"bert-case-uncased\".\n\nWe will name this as bert_tok","b0250fdc":"Loss function to be used is Binary Cross Entropy with Logistic Losses","8199c4be":"Considering this is a multi-label classification problem, we cant use simple accuracy as metrics here. Instead, we will use accuracy_thresh with threshold of 25% as our metric here.","0566480a":"Before we move further, lets have a look at the Data on which we have to work.\n\nWe will split the train data into two parts: Train, Validation. However, for the purpose of this project, we will not be using Test Data","4a30e66d":"## Language Model\n","cb60a8ea":"We will now unfreeze the entire model and train it","a81e097b":"Now, lets create learner function","1926e423":"In following code snippets, we need to wrap BERT vocab and BERT tokenizer with Fastai modules","31c846ea":"We will now see our model's prediction power","2b2cd045":"Important thing to remember in the Language Model is that we train it without label. The basic objective by training language model is to predict the next sentence \/ words in a sequence of text.","20121b6b":"# Importing Libraries & Data Preparation","09c366e0":"Let's split the model now in 6 parts","063f3995":"Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model.","1379574d":"Let's import Huggingface's \"pytorch-pretrained-bert\" model (this is now renamed as pytorch-transformers)\n\n[https:\/\/github.com\/huggingface\/pytorch-transformers](http:\/\/)\n\nThis is a brilliant repository of few of amazing NLP techniques and already pre-trained.","ed45ff1c":"In this project, we will use Jigsaw's Toxic Comments dataset which has categorized each text item into 6 classes -\n\n1. Toxic\n2. Severe Toxic\n3. Obscene\n4. Threat\n5. Insult\n6. Identity Hate\n\nThis is a **multi-label text classification challenge**.","85e60b98":"## Data","b235c2ae":"Both Databunch_1 and Databunch_2 can be used for modelling purposes. In this project, we will be using Databunch_1 which is easier to create and use.","8c5bee02":"Now, we will unfreeze last two last layers and train the model again","5d073739":"As mentioned in the article in first section, we will change the tokenizer of Fastai to incorporate BertTokenizer. One important thing to note here is to change the start and end of each token with [CLS] and [SEP] which is a requirement of BERT."}}