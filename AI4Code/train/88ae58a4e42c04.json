{"cell_type":{"917c8401":"code","28e83eb6":"code","1753d2ff":"code","558e4439":"code","b678f6f3":"code","a911596a":"code","6f6cdbdd":"code","88d5a2b6":"code","4b6e5f29":"code","36d359f3":"code","5811e6bb":"code","4ce403ce":"code","ebde16e8":"code","853d81e8":"code","5439e5d2":"code","2d0dace9":"code","23a4294e":"code","8fb90ba9":"code","cd641fbc":"code","ce2a8ed4":"code","d08db0b4":"code","b477a448":"code","d9b17228":"code","9a2f357a":"code","28b979ab":"code","da394cf7":"code","ed879a85":"code","16bfa213":"code","9afbac8c":"code","eec8241b":"code","8e57e7d8":"code","43c072d0":"code","b9c1cbe5":"code","9950a35d":"code","174871f3":"code","e76ccec3":"code","29d02b09":"code","780297c8":"code","d9f21243":"markdown","c08df3dd":"markdown","3dca8898":"markdown","4e233e5e":"markdown","0b0eabcf":"markdown","b4106f75":"markdown","f78ed9db":"markdown","8d3bc72e":"markdown","cd0fab40":"markdown","9f36460a":"markdown","71ad16a7":"markdown","0779689c":"markdown","0e266270":"markdown","b4d78c7a":"markdown","12a25c1f":"markdown","5aa0bfe4":"markdown","976b7eb5":"markdown","87b7b6f7":"markdown","3df16542":"markdown","5224861f":"markdown","b8712649":"markdown","e61f8875":"markdown","1646c700":"markdown","698489cf":"markdown","f9ee0295":"markdown","89ad6bee":"markdown","9bda929e":"markdown","139e433f":"markdown","9ed20766":"markdown"},"source":{"917c8401":"## Libs utilized\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\n\nimport scikitplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, RocCurveDisplay,roc_curve, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","28e83eb6":"## Data provided\ndf_train = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv', na_values='?')\ndf_test = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv', na_values='?')","1753d2ff":"## Preview of data\ndf_train.head()","558e4439":"## Sample size provided\nn_train_samples = df_train.shape[0]\nn_test_samples = df_test.shape[0]\nproportion = n_test_samples \/ n_train_samples\nprint(f'Number of training samples: {n_train_samples}')\nprint(f'Number of testing samples: {n_test_samples}. This represents {round(proportion,4)*100}% of the traning sample size.')\nprint('')\nprint(f'Number of rows in train dataset with missing values: {n_train_samples - df_train.dropna().shape[0]}.\\\n That represents {round((1-df_train.dropna().shape[0]\/n_train_samples)*100,2)}% of the total dataset.')\nprint(f'Number of rows in test dataset with missing values: {n_test_samples - df_test.dropna().shape[0]}.\\\n That represents {round((1-df_test.dropna().shape[0]\/n_test_samples)*100,2)}% of the total dataset.')","b678f6f3":"missing_data_train = df_train.isna().sum()\nmissing_data_test = df_test.isna().sum()\nmissing_data = pd.concat((missing_data_train,missing_data_test), axis=1)\nmissing_data.columns = ['Training', 'Testing']\nmissing_data = missing_data.loc[missing_data['Training']>0]\nmissing_data['% of train total'] = missing_data['Training'] * 100 \/ n_train_samples\nmissing_data['% of test total'] = missing_data['Testing'] * 100 \/ n_test_samples \nmissing_data[['Training', '% of train total', 'Testing', '% of test total']]","a911596a":"df2plot = df_train.groupby(['income'],as_index=False).agg({'Id':'count'})\ndf2plot.sort_values(by=['Id'], inplace=True, ascending=False)\nf = plt.figure(figsize=(7,7))\nwith sns.axes_style(\"darkgrid\"):\n    ax = sns.barplot(x='income', y='Id', data=df2plot, palette='pastel')\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{round(height\/ df_train.shape[0] * 100,2)}%', (x + width\/2, height*1.02), ha='center')\n\nplt.title('Distribution of income level in the training data')\nplt.xticks(rotation=30)\nplt.ylabel('Count')\nplt.xlabel('Income level')\nplt.show()","6f6cdbdd":"pd.DataFrame(df_train.dtypes, columns=['dtype'])","88d5a2b6":"#renaming for convenience\ndf_train.rename(columns={'marital.status':'marital_status', 'native.country':'native_country'},inplace=True)\ndf_test.rename(columns={'marital.status':'marital_status', 'native.country':'native_country'},inplace=True)\n\ncategorical_features = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']","4b6e5f29":"num_nans = {}\nnum_classes = {}\nfor cat_feat in categorical_features:\n    num_classes[cat_feat] = df_train[cat_feat].nunique()\n    num_nans[cat_feat] = df_train[cat_feat].isna().sum()\npd.DataFrame(num_classes.items(),columns=['feature','num_classes']).set_index(['feature']).sort_values('num_classes',ascending=False)","36d359f3":"## Analysing the frequency for each class for each feature\n\nfor cat_feat in categorical_features:    \n    if cat_feat == 'education':\n        df2plot = df_train.groupby(['education','education.num'],as_index=False).agg({'Id':'count'})\n        df2plot.sort_values(by='education.num', ascending=True, inplace=True)\n    else:\n        df2plot = df_train.groupby([cat_feat],as_index=False).agg({'Id':'count'})\n        df2plot.sort_values(by=['Id'], inplace=True, ascending=False)\n        \n\n    f = plt.figure(figsize=(20,4))\n    with sns.axes_style(\"darkgrid\"):\n        ax = sns.barplot(x=cat_feat, y='Id', data=df2plot, palette='pastel')\n\n    for p in ax.patches:\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{round(height\/ (df_train.shape[0]-num_nans[cat_feat]) * 100,2)}%', (x + width\/2, height*1.02), ha='center')\n\n    plt.title(f'Distribution of {cat_feat} in the training data')\n    plt.xticks(rotation=30)\n    plt.xlabel('Count')\n    plt.ylabel(cat_feat)\nplt.show() ","5811e6bb":"dict_countries = {\n    'United-States':'United-States',\n    # European\n    'England':'Europe',\n    'Greece':'Europe',\n    'Germany':'Europe',\n    'Italy':'Europe',\n    'Poland':'Europe',\n    'Portugal':'Europe',\n    'Ireland':'Europe',\n    'France':'Europe',\n    'Hungary':'Europe',\n    'Yugoslavia':'Europe',\n    'Scotland':'Europe',\n    'Holand-Netherlands':'Europe',\n    # APAC - Asian Pacific\n    'Cambodia':\"APAC\",\n    'India':\"APAC\",\n    'Hong':\"APAC\",\n    'Japan':\"APAC\",\n    'China':\"APAC\",\n    'Philippines':\"APAC\",\n    'Vietnam':\"APAC\",\n    'Laos':\"APAC\",\n    'Taiwan':\"APAC\",\n    'Thailand':'APAC',\n    # LATAM\n    'Puerto-Rico':'LATAM',\n    'Outlying-US(Guam-USVI-etc)':'LATAM',\n    'Cuba':'LATAM',\n    'Honduras':'LATAM',\n    'Jamaica':'LATAM',\n    'Mexico':'LATAM',\n    'Dominican-Republic':'LATAM',\n    'Ecuador':'LATAM',\n    'Haiti':'LATAM',\n    'Columbia':'LATAM',\n    'Guatemala':'LATAM',\n    'Nicaragua':'LATAM',\n    'El-Salvador':'LATAM',\n    'Trinadad&Tobago':'LATAM',\n    'Peru':'LATAM',\n    # Other\n    'Canada':'Other',\n    'South':'Other', # interpreted as South Africa\n    'Iran':'Other'\n}\ndf_train.loc[df_train['native_country'].isna(),'native_country'] = 'United-States'\ndf_train['native_continent'] = [dict_countries[country] for country in df_train['native_country']]\n# Does the same for the test dataset\ndf_test.loc[df_test['native_country'].isna(),'native_country'] = 'United-States'\ndf_test['native_continent'] = [dict_countries[country] for country in df_test['native_country']]\n# Replacing country for continent\ncategorical_features = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_continent']\n\ndf2plot = df_train.groupby(['native_continent'],as_index=False).agg({'Id':'count'})\ndf2plot.sort_values(by=['Id'], inplace=True, ascending=False)\n\n\nf = plt.figure(figsize=(20,4))\nwith sns.axes_style(\"darkgrid\"):\n    ax = sns.barplot(x='native_continent', y='Id', data=df2plot, palette='pastel')\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{round(height\/ (df_train.shape[0]) * 100,2)}%', (x + width\/2, height*1.02), ha='center')\n\nplt.title(f'New distribution of country feature in the training data')\nplt.xticks(rotation=30)\nplt.xlabel('Count')\nplt.ylabel('native_continent')\nplt.show()","4ce403ce":"## Distribution of income level for each class in each feature\n\nfor cat_feat in categorical_features:    \n    total = df_train.groupby([cat_feat],as_index=False).agg({'income':'count'})\n    total.columns=[cat_feat, 'total']\n    low_income = df_train.loc[df_train['income']=='<=50K'].groupby([cat_feat],as_index=False).agg({'income':'count'})\n    low_income.columns=[cat_feat, 'less_than_50K']\n    low_income = low_income.merge(total, on=cat_feat, how='left')\n    low_income['pct_less_than_50K'] = low_income['less_than_50K'] \/ low_income['total'] * 100\n    low_income['dummy'] = 100\n\n    f = plt.figure(figsize=(15,6))\n    with sns.axes_style(\"darkgrid\"):\n        # bar chart 1 -> top bars (group of 'smoker=No')\n        bar1 = sns.barplot(x=cat_feat,  y='dummy', data=low_income, color='darkblue')\n\n        # bar chart 2 -> bottom bars (group of 'smoker=Yes')\n        bar2 = sns.barplot(x=cat_feat, y=\"pct_less_than_50K\", data=low_income, color='lightblue')\n    \n    for p in bar2.patches:\n        width = p.get_width()\n        height = p.get_height()\n        if height == 100:\n            continue\n        x, y = p.get_xy() \n        bar2.annotate(f'{round(height,2)}%', (x + width\/2, height*0.5), ha='center')\n    \n    # add legend\n    top_bar = mpatches.Patch(color='darkblue', label='Income > 50K')\n    bottom_bar = mpatches.Patch(color='lightblue', label='Income <= 50K')\n    plt.legend(handles=[top_bar, bottom_bar], bbox_to_anchor = (1.0, 0.6))\n    plt.xticks(rotation=30)\n    plt.ylabel('%')\n    plt.title(f'Percentual distribution of income within each {cat_feat}')\n    # show the graph\n    plt.show()","ebde16e8":"## Creating new class for Relationship feature\ndf_train.loc[df_train['relationship'].isin(['Husband', 'Wife']), 'relationship'] = 'Married'\ndf_test.loc[df_test['relationship'].isin(['Husband', 'Wife']), 'relationship'] = 'Married'","853d81e8":"## Filling data for Workclass and Occupation.\nworkclass_distribution = df_train['workclass'].value_counts(normalize=True)\ndf_train.loc[df_train['workclass'].isna(),'workclass'] = np.random.choice(workclass_distribution.index, size=df_train['workclass'].isna().sum(),p=workclass_distribution.values)\ndf_test.loc[df_test['workclass'].isna(),'workclass'] = np.random.choice(workclass_distribution.index, size=df_test['workclass'].isna().sum(),p=workclass_distribution.values)\n\noccupation_distribution = df_train['occupation'].value_counts(normalize=True)\ndf_train.loc[df_train['occupation'].isna(),'occupation'] = np.random.choice(occupation_distribution.index, size=df_train['occupation'].isna().sum(),p=occupation_distribution.values)\ndf_test.loc[df_test['occupation'].isna(),'occupation'] = np.random.choice(occupation_distribution.index, size=df_test['occupation'].isna().sum(),p=occupation_distribution.values)","5439e5d2":"# Removing education because it's already encoded\ncategorical_features.remove('education')\ndf_train = pd.get_dummies(df_train, prefix=categorical_features, columns = categorical_features, drop_first=True)\ndf_test = pd.get_dummies(df_test, prefix=categorical_features, columns = categorical_features, drop_first=True)","2d0dace9":"#renaming for convenience\ndf_train.rename(columns={'capital.gain':'capital_gain', 'capital.loss':'capital_loss', 'hours.per.week':'hours_per_week'},inplace=True)\ndf_test.rename(columns={'capital.gain':'capital_gain', 'capital.loss':'capital_loss', 'hours.per.week':'hours_per_week'},inplace=True)\n\nnumerical_features = ['age', 'capital_gain', 'capital_loss', 'hours_per_week']","23a4294e":"df_train.loc[(df_train['capital_gain']!=0) | (df_train['capital_loss']!=0),'capital_moviment'] = 1\ndf_train['capital_moviment'].fillna(0,inplace=True)\ndf_test.loc[(df_test['capital_gain']!=0) | (df_test['capital_loss']!=0),'capital_moviment'] = 1\ndf_test['capital_moviment'].fillna(0,inplace=True)","8fb90ba9":"df_num = df_train[numerical_features+['income']].copy()\n# Creating a numerical column for the target variable\ndf_num.loc[df_num['income']=='>50K','income_level']=1\ndf_num.loc[df_num['income']=='<=50K','income_level']=0","cd641fbc":"df_num.describe()","ce2a8ed4":"for num_feat in numerical_features:\n    f = plt.figure(figsize=(11, 3))\n    with sns.axes_style(\"darkgrid\"):\n        ax = sns.boxplot(x=num_feat, data=df_num, palette=\"pastel\").set_title(f'Overall distribution of {num_feat}.')","d08db0b4":"for num_feat in numerical_features:    \n    f = plt.figure(figsize=(10,6))\n    with sns.axes_style(\"darkgrid\"):\n        ax = sns.boxplot( x=num_feat, y=\"income\", data=df_num, palette=\"pastel\", orient='h').set_title(f'Distribution of {num_feat} based on income level.')","b477a448":"f = plt.figure(figsize=(10,6))\nwith sns.axes_style(\"darkgrid\"):\n    ax = sns.heatmap(df_num.corr(),square = True, annot=True, vmin=0, vmax=1,cmap=\"YlGnBu\")","d9b17228":"X = df_train.drop(columns=['Id','fnlwgt', 'education', 'native_country', 'income'])\nX_test = df_test.drop(columns=['fnlwgt', 'education', 'native_country'])\n\ny = df_train['income'].copy()\n\n#split data into training and test dataset\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.20, random_state = 42)","9a2f357a":"rs = RobustScaler()\ncolumns = list(X_train.columns)\nX_train[columns] = rs.fit_transform(X_train[columns].values)\nX[columns] = rs.transform(X[columns].values)\nX_val[columns] = rs.transform(X_val[columns].values)\nX_test[columns] = rs.transform(X_test[columns].values)","28b979ab":"# model definition\n# 25 defined as hyperparameter in the previous notebook\nknn = KNeighborsClassifier(n_neighbors=25).fit(X_train,y_train)\n\ny_pred = knn.predict(X_val)","da394cf7":"print(f\"F1 score obtained: {round(f1_score(y_val.values,knn.predict(X_val),average='weighted'),4)}\")","ed879a85":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='liblinear',random_state=0).fit(X_train, y_train)\nf1 = round(f1_score(y_val.values,log_reg.predict(X_val),average='weighted'),4)\nprint(f\"F1 score obtained: {f1}\")","16bfa213":"from sklearn import svm\n\nsupport_vector_machine = svm.LinearSVC(random_state=0, tol=1e-5).fit(X_train,y_train)\nprint(f\"F1 score obtained: {round(f1_score(y_val.values,support_vector_machine.predict(X_val),average='weighted'),4)}\")","9afbac8c":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=0).fit(X_train,y_train)\nprint(f\"F1 score obtained: {round(f1_score(y_val.values,tree.predict(X_val),average='weighted'),4)}\")","eec8241b":"from sklearn.ensemble import AdaBoostClassifier\n\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0).fit(X_train,y_train)\nprint(f\"F1 score obtained: {round(f1_score(y_val.values,adaboost.predict(X_val),average='weighted'),4)}\")","8e57e7d8":"from xgboost import XGBClassifier\nxgb = XGBClassifier(max_depth=5,n_estimators=70, nthread=-1,\n                    objective='binary:logistic',seed=0,eval_metric='auc').fit(X_train,y_train)\n\nprint(f\"F1 score obtained: {round(f1_score(y_val.values,xgb.predict(X_val),average='weighted'),4)}\")","43c072d0":"param_test1 = {\n 'max_depth':range(3,11)\n}\ngsearch1 = GridSearchCV(\n                estimator = XGBClassifier(\n                                n_estimators=70, max_depth=5,\n                                objective= 'binary:logistic',eval_metric='auc',nthread=-1,seed=10), \n                param_grid = param_test1, scoring='f1_weighted',n_jobs=-1, cv=5\n            )\ngsearch1.fit(X,y)\nprint(f'Best parameters: {gsearch1.best_params_}, Best F1 score: {gsearch1.best_score_}')","b9c1cbe5":"model  = XGBClassifier(\n            max_depth=5,n_estimators=70, nthread=-1,\n            objective='binary:logistic',seed=0,eval_metric='auc').fit(X,y)","9950a35d":"y_pred = model.predict(X)","174871f3":"print(classification_report(y, y_pred))","e76ccec3":"scikitplot.metrics.plot_confusion_matrix(y, y_pred, normalize = False, figsize = (7,7))\nplt.show()","29d02b09":"# predict\nX_test['income'] = model.predict(X_test.drop(columns = 'Id'))\n\nX_test.head()","780297c8":"export_path = \"submission.csv\"\nX_test[['Id', 'income']].to_csv(export_path, index = False)","d9f21243":"### 3.3 Fitting <a name=\"fitting_knn\"><\/a>","c08df3dd":"## 2. Exploratory Data Analysis <a name=\"eda\"><\/a>\n\nThis sections contains the EDA process.","3dca8898":"### 2.3 Features data type <a name=\"dtype\"><\/a>","4e233e5e":"All features seams to be important, since different classes have very different percentuals of each income level. Excepting **education**, all the other features will be encoded through One Hot Encoding, given that the number of classes is not too high for any of them.\n\nSome specific points for each feature:\n1. **Workclass**: As it seems to be a important feature it's important to fill the 1836 None values present in the original dataset, as they represent a significant amount of the total (approx. 5%). There are different ways to do so and setting all the None values as the most common class is the standard one. But, in this case, the most common class _Private_ has the second highest percentual for low income and setting all the missing values as _Private_ might induce undesirable bias in our model. Besides, there's no apparent reason for which individuals from this class would not have data, which might indicate that this is a case of Missing data At Random (MAR). So, my approach's going to be to **maintain the original distribution between classes**, as shown in the previous plot. Basically, for each sample with None value in workclass, I'll assign a random choice of class based on the original distribution.\n2. **Education**: This feature apparently is important as well and is already encoded with no missing data.\n3. **Marital_status**: The plots indicate that this is a relevant feature as well. As for the enconding, although there is a relationship between these classes, i.e., a _Divorced_ individual was once _Married_, it's not clear how this relationship between classes is quantifiable, so the encode wll be a simple One Hot. \n4. **Occupation**: Again, the distribution is not concentrated in a specific class and each class has very different proportions for each income level. I'll fill in the missing data using the same approach as in **workclass**.\n5. **Relationship**: Apparently important, individuals that are married, i.e., \"Husband\" or \"Wife\" have higher incomes. Therefore, to reduce a sexist bias (given that already exists a feature for **Sex**), I'll treat them both as a new class \"Married\". That will also reduce the cardinality and favor a One Hot Encoding approach.\n7. **Sex**: Although is a unbalanced dataset within this dimesion, the \"Male\" class has a higher income.\n8. **Native_country**: This plot validates the approach used in the previous cell, as the income proportion for LATAM countries is much lower than other continents. In other hand, APAC, Europe and Other countries all have very similar proportions, around 70%.","0b0eabcf":"## 6.2 Metrics <a name=\"metrics\"><\/a>","b4106f75":"### 5.4 Boosting - AdaBoost <a name=\"adaboost\"><\/a>","f78ed9db":"## 6.3 Predictions <a name=\"predictions\"><\/a>","8d3bc72e":"### 3.1 Data prep <a name=\"prep\"><\/a>","cd0fab40":"Analysing the plots, we can infer some facts:\n\n1. **Age**: The sample size is concentrated in people between theirs late-20's and late-40's. There are some outliers older than 80 y.o. but none less than 17, which means that all individuals are legally able to work\n2. **Capital_gain** and **capital_loss**: Over 75% of the individuals don't have neither losses nor gains. The **capital_gains** apparently are higher for people with high income, but the distribution for losses are not that different between income levels.\n3. **Hours_per_week**: It has a high number of outliers as well, with the majority of individuals working around 40 hours per week. That's expected, since that's the standard journey legally in US.","9f36460a":"### 2.5 Numerical features <a name=\"num\"><\/a>\nThe categorical features are: _age, fnlwgt, capital.gain, capital.loss, hours.per.week._\n\nAs written in the document Extra-file-from-UCL.txt : _\"Description of fnlwgt (final weight): The weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US. [...]People with similar demographic characteristics should have similar weights. **There is one important caveat to remember about this statement.  That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.**_\n\nAs said, this feature is only valid when applied for individuals from the same state. But in the dataset provided it's impossible to know which individuals are from which state, basically invalidating this feature. As the source of data itself states this condition, I'll not be using this feature for my classifier. ","71ad16a7":"Analysing the plots:\n1. **Workclass**: is highly concentrated in one class \"Private\". Two of the classes are really rare, \"Without-pay\" and \"Never-worked\", and for the purpose of the classifier they're very similar, given that both of theses classes have no current income. \n2. **Education**: The most frequent classes are of \"HS-grad\" level or higher. Even still, summing all classes that are lower level than \"HS-grad\", we obtain almost 14% of the total population, which is a high percentage. It has a high cardinality.\n3. **Marital_status**: Almost half of the sample is \"Married-civ-spouse\". Also not a high number of classes.\n4. **Occupation**: Also has a high cardinality, i.e., high number of classes. It's not concentraded in any specific class.\n5. **Relationship**: Low cardinality. The majority of the sample is \"Husband\".\n6. **Race**: Low cardinality and highly concentrated in \"White\".\n7. **Sex**: Only two classes. Aprox. 2\/3 of the sample is \"Male\", which might induce the higher concentration in \"Husband\" in the **relantionship** feature.\n8. **Native_country**: The highest cardinality. Also, extremely concentrated in one class, \"United-States\". The other 40 class are all below 1%, except for \"Mexico\". That indicates it might be necessary to reduce this dimensionality. The proposed method here is to create classes based on the continent of each country, resulting in the following classes: \"United-States\", \"LATAM\", \"APAC\", \"Europe\" and \"Other\". Besides, all the None values will be assigned as United-States, since its by far the most common class.","0779689c":"### 3.2 Scalling <a name=\"scale\"><\/a>","0e266270":"Vale notar que o F1 score \u00e9 menor que o mostrado anteriormente, mas os dados utilizados s\u00e3o diferentes. Aqui \u00e9 utilizado tanto o conjunto de teste quanto o de valida\u00e7\u00e3o, anteriormente, apenas o de treinamento.","b4d78c7a":"**Feature engineering**\n1. Create a flag for the individuals that have either gained or lost capital","12a25c1f":"### 2.2 Target variable - Income level <a name=\"target\"><\/a>\n\nCheck the distribution of the target variable.","5aa0bfe4":"### 2.1 Data dimensions <a name=\"dim\"><\/a>","976b7eb5":"## 5 New models <a name=\"new\"><\/a>","87b7b6f7":"## 3. Modelling with KNN <a name=\"model\"><\/a>","3df16542":"Three features have missing data: _workclass_, _occupation_ and _native.country_. It's a significant amount (around 7%), both in the training dataset as well as in the testing. It'll be necessary to find methods to fill in these missing values. That process will be done in a specific manner for each feature, so that the best method is chosen for each feature.","5224861f":"### 5.5 Boosting - XGBoost <a name=\"xgb\"><\/a>","b8712649":"### 5.2 Support Vector Machines <a name=\"svm\"><\/a>","e61f8875":"# 6. Choosing the best model <a name=\"best\"><\/a>\n\nAs shown in the previous section, the F1 score was computed for each of the models tested. This choice is made given the known fact that the dateset is unbalaced. Therefore, the xgboost model is chosen as the best one as it has the highest F1 score.\n\n## 6.1 Hyperparameter tuning <a name=\"hyper\"><\/a>","1646c700":"As seen, of the 14 features, 8 are **categorical** while only 6 are **numerical.** Obviously, the _Id_ field is numerical and the _income_ is categorical (\">50k\"; \"<=50k\").\n\nLet's look into each one of them:\n\n### 2.4 Categorical features <a name=\"cat\"><\/a>\nThe categorical features are: _workclass, education, marital.status, occupation, relationship, race, sex, native.country_.","698489cf":"### 5.1 Logistic regressor <a name=\"log\"><\/a>\n","f9ee0295":"## 4 Validating with KNN model <a name=\"validating_knn\"><\/a>","89ad6bee":"# Adult-PMR3508\n**Author: Lucas Hattori**\n\nThis notebook creates a kNN classifier model for the Adult dataset that predicts whether the income of a certain individual is higher than 50k annualy or not. Furthermore, it also creates 5 other classifiers, compares them and chooses the best one.\n\nTo do so, it was provided both a train dataset and a test dataset that includes an id for each individual, 14 features and the income level (in the train dataset). This notebook is divided in the following sections:\n\n#### Table of contents\n1. [Imports](#imports)\n2. [Exploratory Data Analysis](#eda)\n    1. [Data dimensions](#dim)\n    2. [Target variable](#target)\n    3. [Data types](#dtype)\n    4. [Categorical features](#cat)\n    5. [Numerical features](#num)\n3. [Modelling with KNN](#model)\n    1. [Data prep](#prep)\n    2. [Scalling](#scale)\n    3. [Fitting](#fitting_knn)\n4. [Validation with KNN model](#validating_knn)\n5. [New Models](#new)\n    1. [Logistic Regression](#log)\n    2. [Support Vector Machine](#svm)\n    3. [Decision Tree](#tree)\n    4. [Boosting-Adaboost](#adaboost)\n    5. [Boosting-XGBoost](#xgb)\n6. [Choosing the best model](#best)\n    1. [Hyperparameter tuning](#hyper)\n    2. [Metrics](#metrics)\n    3. [Predictions](#predictions)\n    ","9bda929e":"This plot shows that the dataset is unbalanced, which is expected given the fact that 50k is roughly the median salary in the US and it is known that the income distribution in the US is highly skewed towards lower incomes.","139e433f":"## 1. Imports <a name=\"imports\"><\/a>\n\nThis section contains both importing of libs utilized and reading the data provided.","9ed20766":"### 5.3 Decision Tree <a name=\"tree\"><\/a>"}}