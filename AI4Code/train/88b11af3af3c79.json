{"cell_type":{"030070b0":"code","fc957414":"code","263863fa":"code","251e06b8":"code","0f1f6fde":"code","9f8ec34e":"code","1b09ac43":"code","d591a78e":"code","5c8964a8":"code","8883f4e2":"code","5c95f88c":"code","e74f7c05":"code","68f489e1":"code","a5e44319":"code","72daacf9":"code","70a7e9ea":"code","47ec1ba4":"code","0f69a72b":"code","10d31b60":"code","f2521245":"code","5e3af3f2":"code","425b6262":"code","2eb9e128":"code","6ccda06d":"code","b0f81ff5":"code","1529f89d":"code","84f6da3a":"code","22addfad":"code","823c8a1b":"code","bbbc3cd4":"code","7125572a":"code","7cf25bae":"code","01db8830":"code","8527b53a":"code","8992f02d":"code","0bcdbc27":"code","2377209c":"code","d1f99711":"code","1c0c26b2":"code","99649e17":"markdown","b363e702":"markdown","35b13b33":"markdown","abe3755b":"markdown","5ed7b85e":"markdown","b66587fd":"markdown","12eab333":"markdown","6e066b5f":"markdown","9c0af108":"markdown","bbf89961":"markdown","21908e19":"markdown","04bebf74":"markdown","700bd744":"markdown","1c863f96":"markdown","22989714":"markdown","eb765d61":"markdown","ba4cb61c":"markdown","40926e61":"markdown","55bb45ed":"markdown","ea164952":"markdown","9beda4d2":"markdown","7b58690f":"markdown","dc4a89c9":"markdown","0aaa1125":"markdown","644825e7":"markdown","ea4f9a7a":"markdown","8c1da3a0":"markdown","ef3dce46":"markdown","24172f50":"markdown","5e36947f":"markdown","1cbf5ae2":"markdown","996f6a7c":"markdown"},"source":{"030070b0":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False\n\n# to display all columns\npd.options.display.max_columns = None\npd.options.display.max_rows = None","fc957414":"# read the data\ndata = pd.read_csv('..\/input\/wind-power-forecasting\/Turbine_Data.csv')\ndata.head()","263863fa":"data.info()","251e06b8":"data.describe()","0f1f6fde":"# change the Unnamed: 0 to datetype\ndf_updated = data.copy()\ndf_updated['Unnamed: 0'] = pd.to_datetime(df_updated['Unnamed: 0'])\ndf_updated.rename(columns={'Unnamed: 0': 'date_column'}, inplace=True)\n\nif (df_updated['Blade2PitchAngle'].equals(df_updated['Blade3PitchAngle'])==True):\n  df_updated = df_updated.drop('Blade3PitchAngle', axis=1) \n\n# check if the column is dropped\nassert 'Blade3PitchAngle' not in df_updated.columns","9f8ec34e":"df_updated.head()","1b09ac43":"# Check null values\ndf_updated.isnull().sum()","d591a78e":"df_updated = df_updated.fillna(method='ffill').fillna(method='bfill')\ndf_updated.isnull().sum()","5c8964a8":"df = df_updated[['date_column', 'ActivePower', 'WindSpeed', 'GeneratorRPM', 'ReactivePower', 'RotorRPM', 'AmbientTemperatue', \\\n                 'WindDirection', 'Blade1PitchAngle', 'Blade2PitchAngle', 'HubTemperature', 'MainBoxTemperature', 'GearboxBearingTemperature', \\\n                 'GearboxOilTemperature']].copy()","8883f4e2":"# crete a new column called Weekday\ndf['weekday'] = df['date_column'].dt.dayofweek\n# get one hot encoding\nohe = pd.get_dummies(df['weekday'])\ndf = df.join(ohe)\ndf = df.drop('weekday', axis=1)\ndate = pd.to_datetime(df.pop('date_column'))\ndf.head()","5c95f88c":"# convert datetime column to seconds\nimport datetime\ntimestamp_s = date.map(datetime.datetime.timestamp)","e74f7c05":"# time in seconds may not be an useful input. We could convert it into sin and cos\nday = 24*60*60\nyear = (365.2425)*day\n\ndf['Day sin'] = np.sin(timestamp_s * (2 * np.pi \/ day))\ndf['Day cos'] = np.cos(timestamp_s * (2 * np.pi \/ day))\ndf['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ndf['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))","68f489e1":"df.head()","a5e44319":"plt.plot(np.array(df['Day sin'])[:200])\nplt.plot(np.array(df['Day cos'])[:200])\nplt.xlabel('Time [h]')\nplt.title('Time of day signal');","72daacf9":"column_indices = {name: i for i, name in enumerate(df.columns)}\n\nn = len(df)\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\nnum_features = df.shape[1]","70a7e9ea":"df.head()","47ec1ba4":"# Normalize the data\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) \/ train_std\nval_df = (val_df - train_mean) \/ train_std\ntest_df = (test_df - train_mean) \/ train_std","0f69a72b":"# the code below visualizes the normalized data\ndf_std = (df - train_mean) \/ train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(df.keys(), rotation=90)","10d31b60":"class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","f2521245":"w2 = WindowGenerator(input_width=30*24*6, label_width=15*24*6, shift=15*24*6,\n                     label_columns=['ActivePower'])\n\nw2","5e3af3f2":"def split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window","425b6262":"# Stack three slices, the length of the total window:\nexample_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n                           np.array(train_df[100:100+w2.total_window_size]),\n                           np.array(train_df[200:200+w2.total_window_size])])\n\n\nexample_inputs, example_labels = w2.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'labels shape: {example_labels.shape}')","2eb9e128":"w2.example = example_inputs, example_labels","6ccda06d":"def plot(self, model=None, plot_col='ActivePower', max_subplots=5):\n  inputs, labels = self.example\n  plt.figure(figsize=(70, 20))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=124)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', label='Predictions',\n                  c='#ff0000', s=154)\n\n    if n == 0:\n      plt.legend(prop={'size': 20})\n\n  plt.xlabel('Time [20 min]')\n\nWindowGenerator.plot = plot","b0f81ff5":"w2.plot()","1529f89d":"def make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=120,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset","84f6da3a":"@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","22addfad":"# Each element is an (inputs, label) pair\nw2.train.element_spec","823c8a1b":"for example_inputs, example_labels in w2.train.take(1):\n  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n  print(f'Labels shape (batch, time, features): {example_labels.shape}')","bbbc3cd4":"wide_window = WindowGenerator(\n    input_width=2*6, label_width=2*6, shift=1,\n    label_columns=['ActivePower'])\n\nwide_window","7125572a":"# package the training into a function\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=3):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=3,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanAbsoluteError(),\n                optimizer=tf.optimizers.Adam(lr=0.01),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n","7cf25bae":"OUT_STEPS = 15*24*6\nmulti_window = WindowGenerator(input_width=15*24*6,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window\n","01db8830":"class MultiStepLastBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n                      metrics=[tf.metrics.MeanAbsoluteError()])\n\nmulti_val_performance = {}\nmulti_performance = {}\n\nmulti_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\nmulti_performance['Last'] = last_baseline.evaluate(multi_window.test)\nmulti_window.plot(last_baseline)\n","8527b53a":"class RepeatBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return inputs\n\nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n                        metrics=[tf.metrics.MeanAbsoluteError()])\n\nmulti_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\nmulti_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test)\nmulti_window.plot(repeat_baseline)\n","8992f02d":"OUT_STEPS = 15*24*6\nmulti_window = WindowGenerator(input_width=30*24*6,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window\n","0bcdbc27":"multi_linear_model = tf.keras.Sequential([\n    # Take the last time-step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_linear_model, multi_window)\n\n#IPython.display.clear_output()\nmulti_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\nmulti_performance['Linear'] = multi_linear_model.evaluate(multi_window.test)\nmulti_window.plot(multi_linear_model)\n","2377209c":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\n\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test)\nmulti_window.plot(multi_dense_model)\n","d1f99711":"CONV_WIDTH = 3\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test)\nmulti_window.plot(multi_conv_model)\n","1c0c26b2":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units]\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test)\nmulti_window.plot(multi_lstm_model)\n","99649e17":"**To do**:\nHow to handle the missing data here:\n\n1. Populate the NaN values by `df.fillna(method='ffill')`\n2. Populate the missing values by median value\/ average value? (or moving average value)\n\n","b363e702":"## Plot\n\n* A plot method that allows a simple visualization of the split window","35b13b33":"## Compile and fit","abe3755b":"## Check for null values","5ed7b85e":"#### Baseline model\n\n##### Repeat last input: A simple baseline is to repeat the last input timestep for the required number of output timesteps. ","b66587fd":"**Improvements\/ future development**\n\n* Add more layers to RNN\/CNN\n\n* Try Autoregressive model\n\n* Create custom evaluation metric:\n  * to give more weightage to the closest predictions\n\n* Feature engineering + apply the known datafields to future dates which are fed to the predict function. \n","12eab333":"## Date column\n  * The `date` column in string format is not a useful input. \n  * It could have clear daily and yearly periodicity.\n  * Use `sin` and `cos` to convert the time to clear \"Time of day\" and \"Time of  year signals.","6e066b5f":"* In our case, we want to predict 15 days into the future. \n* input_width = `30*24*6`\n* label_with = `30*24*6`\n* shift = 1","9c0af108":"#### Linear model\n\n* `multi_linear_model`: \n  \n  * Groups a linear stack of layers into a `tf.keras.Model`. ","bbf89961":"### Multi step model\n\n* We want to predict multiple steps into the future (15 days into the future)\n\n* In multi-step prediction, the model needs to learn to predict a range of future values. A sequence of future values are predicted. \n\n* **Two approaches**:\n\n1. Single shot predictions where the entire timeseries is predicted at once\n2. AutoRegressive model: the model makes only single step predictions and its output is fed back as input. ","21908e19":"* This example takes a batch of 3, 4320 timestep windows, with 24 features at each timestep. \n* It splits them into a batch of 2160 timestep, 24 feature inputs, and a 2160 timestep, 1 feature output label. \n* The label only has one feature, because the `WindowGenerator` was initialized with one column. ","04bebf74":"## Import statements","700bd744":"The `WindowGenerator` ibhect holds training, validation, and test data. Add properties for accessing them using the above `make_dataset` method. Also, add a standard example batch for easy access and plotting. ","1c863f96":"This model does better than baseline, but still underpowered. (Also we haven't made use of other feature columns).","22989714":"## Create tf.Datasets \n* The `make_dataset` method below will take a timeseries DataFrame and convert it to a `tf.data.Dataset` of (input_window, label_window) pairs using the `preprocessing.timeseries_dataset_from_array` function.","eb765d61":"## Read the data","ba4cb61c":"## Normalize the data\n\n* It is important to scale features before training a Neural Network\n* Normalization = Subtract the mean and divide by the standard deviation of each feature.\n\n* **thinktank**: normalization could be done using moving averages.","40926e61":"Repeat Baseline: Repeat previous 15 days, assuming the next 15 days will be similar.","55bb45ed":"#### Dense\n* Add layers.dense  between the input and output","ea164952":"We see that the data is populated at intervals of 10 minutes. ","9beda4d2":"#### Indexes and offset","7b58690f":"Iterating over a dataset yields concrete batches. ","dc4a89c9":"Here's a window object that generates these slices from the dataset","0aaa1125":"## Data Windowing\n\n* Models will make predictions based on a window of consecutive samples from the data\n* The main features of the input windows are:\n  * The width (number of timesteps) of the input and label windows\n  * The time offset between them\n  * Which features are used as inputs, labels, or both.","644825e7":"## Split\n\n* Given a list of consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels. ","ea4f9a7a":"**Aim: The aim of this notebook is to predict wind power that could be generated from the windmill for the next 15 days**\n\n## Approach:\n\nThis is a learning project, where I want to use TensorFlow for doing Time Series Project. \n\nAlgorithms tried:\n1. ","8c1da3a0":"## Split the data\n\n* We will use `70%, 20%, 10%` split for the training, validation and test sets.\n\n* Data is not randomly sampled before splitting:\n  * Chopping the data into windows of consecutive samples is still possible.\n  * Ensures that validation\/test sets are more realistic.","ef3dce46":"Visualize the distribution","24172f50":"#### RNN","5e36947f":"#### CNN","1cbf5ae2":"## Preprocessing\n\n1. The column `Unnanmed: 0` looks to be a date column. Let's change the type to datetime. \n\n2. Looking closely at df.describe(), we can see that `Blade2PitchAngle` and `Blade3PitchAngle` are having same values. We will validate this by `df1['col'].equals(df2['col']`, and if both are equal, we will drop one of the columns.","996f6a7c":"### Single shot model\n\n* The model makes the entire sequence prediction in one step\n* Model only needs to reshape the output"}}