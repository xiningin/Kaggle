{"cell_type":{"84353d0d":"code","dceadada":"code","901beafc":"code","e173253c":"code","46b0209b":"code","b0601102":"code","78ced090":"code","0d07c282":"code","c07ae89d":"code","b5c31d21":"code","e9da9f49":"code","550e3207":"code","b63e843b":"code","a725c576":"code","e666b881":"code","d3e99d82":"code","b61955ab":"code","c14b9eb8":"code","bc401f8b":"code","a2aa5956":"code","38450fac":"code","88801178":"code","a082b412":"code","996d6d77":"code","a2793bf6":"code","b76ee961":"code","5727d046":"markdown","459028eb":"markdown","42d98ce9":"markdown","79f75bb6":"markdown","9f60ae82":"markdown","24ccdf09":"markdown","fc2f9a35":"markdown","739c36d8":"markdown","dffe02b0":"markdown","b8268d02":"markdown","e9116bc7":"markdown","655ab63d":"markdown","0c7f0e08":"markdown","40980b1e":"markdown","44edbce3":"markdown","5cfef915":"markdown","a4caa2b0":"markdown","c5c09a8c":"markdown","388fe43f":"markdown","faee6e8f":"markdown"},"source":{"84353d0d":"import numpy as np\nimport pandas as pd\n%matplotlib inline\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.models import ColumnDataSource, HoverTool, Range1d, Span\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import Set1_9\nfrom bokeh.transform import factor_cmap\n\nimport datetime\n\noutput_notebook()","dceadada":"train = pd.read_pickle('..\/input\/gstore-revenue-data-preprocessing\/train.pkl')\ntest = pd.read_pickle('..\/input\/gstore-revenue-data-preprocessing\/test.pkl')","901beafc":"train['visitStartTime'].describe()","e173253c":"test['visitStartTime'].describe()","46b0209b":"test_start_time = np.min(test['visitStartTime'])","b0601102":"test_start_time","78ced090":"train_ts = train.set_index('visitStartTime')\ntest_ts = test.set_index('visitStartTime')","0d07c282":"train_daily_sess_count = pd.DataFrame({'count': train_ts.groupby(pd.Grouper(freq='D')).size(), 'set': 'train'})\ntest_daily_sess_count = pd.DataFrame({'count': test_ts.groupby(pd.Grouper(freq='D')).size(), 'set': 'test'})\ndaily_sess_count = pd.concat([train_daily_sess_count, test_daily_sess_count], axis=0)","c07ae89d":"p = figure(x_axis_type='datetime', width=700, height=400, title='daily session count over time')\np.line(\n    source=daily_sess_count[daily_sess_count['set'] == 'train'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[0])\np.line(\n    source=daily_sess_count[daily_sess_count['set'] == 'test'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(location=test_start_time.timestamp() * 1000, dimension='height', line_dash='dashed')\np.add_layout(test_start)\nshow(p)","b5c31d21":"train_daily_pageviews = pd.DataFrame({\n    'count':\n    train_ts.groupby(pd.Grouper(freq='D'))['totals.pageviews'].sum(),\n    'set':\n    'train'\n})\ntest_daily_pageviews = pd.DataFrame({\n    'count':\n    test_ts.groupby(pd.Grouper(freq='D'))['totals.pageviews'].sum(),\n    'set':\n    'test'\n})\ndaily_pageviews = pd.concat(\n    [train_daily_pageviews, test_daily_pageviews], axis=0)","e9da9f49":"train_daily_hit_count = pd.DataFrame({\n    'count':\n    train_ts.groupby(pd.Grouper(freq='D'))['totals.hits'].sum(),\n    'set':\n    'train'\n})\ntest_daily_hit_count = pd.DataFrame({\n    'count':\n    test_ts.groupby(pd.Grouper(freq='D'))['totals.hits'].sum(),\n    'set':\n    'test'\n})\ndaily_hit_count = pd.concat(\n    [train_daily_hit_count, test_daily_hit_count], axis=0)","550e3207":"p = figure(x_axis_type='datetime', width=700, height=400, title='pageviews over time')\np.line(\n    source=daily_pageviews[daily_pageviews['set'] == 'train'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[0])\np.line(\n    source=daily_pageviews[daily_pageviews['set'] == 'test'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(location=test_start_time.timestamp() * 1000, dimension='height', line_dash='dashed')\np.add_layout(test_start)\nshow(p)\n\np = figure(x_axis_type='datetime', width=700, height=400, title='hits over time')\np.line(\n    source=daily_hit_count[daily_hit_count['set'] == 'train'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[0])\np.line(\n    source=daily_hit_count[daily_hit_count['set'] == 'test'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(\n    location=test_start_time.timestamp() * 1000,\n    dimension='height',\n    line_dash='dashed')\np.add_layout(test_start)\nshow(p)","b63e843b":"train_daily_hitrate = pd.DataFrame({\n    'count':\n    train_ts.groupby(pd.Grouper(freq='D')).apply(\n        lambda x: (x['totals.hits'].sum() + 1) \/ (x['totals.pageviews'].sum() + 1)\n    ),\n    'set':\n    'train'\n})\ntest_daily_hitrate = pd.DataFrame({\n    'count':\n    test_ts.groupby(pd.Grouper(freq='D')).apply(\n        lambda x: (x['totals.hits'].sum() + 1) \/ (x['totals.pageviews'].sum() + 1)\n    ),\n    'set':\n    'test'\n})\ndaily_hitrate = pd.concat([train_daily_hitrate, test_daily_hitrate], axis=0)","a725c576":"p = figure(x_axis_type='datetime', width=700, height=400, title='hits\/pageviews ratio over time')\np.line(\n    source=daily_hitrate[daily_hitrate['set'] == 'train'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[0])\np.line(\n    source=daily_hitrate[daily_hitrate['set'] == 'test'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(location=test_start_time.timestamp() * 1000, dimension='height', line_dash='dashed')\np.add_layout(test_start)\nshow(p)","e666b881":"train_daily_mobile_rate = pd.DataFrame({\n    'count':\n    train_ts.groupby(pd.Grouper(freq='D'))['device.isMobile'].mean(),\n    'set':\n    'train'\n})\ntest_daily_mobile_rate = pd.DataFrame({\n    'count':\n    test_ts.groupby(pd.Grouper(freq='D'))['device.isMobile'].mean(),\n    'set':\n    'test'\n})\ndaily_mobile_rate = pd.concat([train_daily_mobile_rate, test_daily_mobile_rate], axis=0)","d3e99d82":"p = figure(x_axis_type='datetime', width=700, height=400, title='daily mobile percentage over time')\np.line(\n    source=daily_mobile_rate[daily_mobile_rate['set'] == 'train'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[0])\np.line(\n    source=daily_mobile_rate[daily_mobile_rate['set'] == 'test'],\n    x='visitStartTime',\n    y='count',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(location=test_start_time.timestamp() * 1000, dimension='height', line_dash='dashed')\np.add_layout(test_start)\nshow(p)","b61955ab":"train_daily_channel_grouping = train_ts.groupby([pd.Grouper(freq='D'), 'channelGrouping']).size().reset_index()\ntrain_daily_channel_grouping['set'] = 'train'\ntest_daily_channel_grouping = test_ts.groupby([pd.Grouper(freq='D'), 'channelGrouping']).size().reset_index()\ntest_daily_channel_grouping['set'] = 'test'\ndaily_channel_grouping = pd.concat([train_daily_channel_grouping, test_daily_channel_grouping], axis=0)\ndaily_channel_grouping.rename(columns={0: 'count'}, inplace=True)\ndaily_channel_grouping['perc'] = daily_channel_grouping['count']","c14b9eb8":"daily_channel_grouping_merged = pd.merge(\n    daily_channel_grouping,\n    daily_sess_count.reset_index()[['visitStartTime', 'count'\n                                    ]].rename(columns={'count': 'sess_count'}),\n    on='visitStartTime')\ndaily_channel_grouping_merged[\n    'perc'] = daily_channel_grouping_merged['count'] \/ daily_channel_grouping_merged['sess_count']","bc401f8b":"p = figure(\n    x_axis_type='datetime',\n    width=700,\n    height=600,\n    title='channelGrouping percentage shares over time')\nchannels = list(daily_channel_grouping['channelGrouping'].unique())\nfor idx, channel in enumerate(channels):\n    p.line(\n        source=daily_channel_grouping_merged[\n            (daily_channel_grouping_merged['channelGrouping'] == channel)\n            & (daily_channel_grouping_merged['perc'] < 1)],\n        x='visitStartTime',\n        y='perc',\n        color=Set1_9[idx],\n        legend=channel)\np.y_range = Range1d(0, 1)\np.legend.click_policy = 'hide'\np.add_tools(\n    HoverTool(tooltips=[('category',\n                         '@channelGrouping'), ('percentage',\n                                               '@perc'), ('count', '@count')]))\ntest_start = Span(\n    location=test_start_time.timestamp() * 1000,\n    dimension='height',\n    line_dash='dashed')\np.add_layout(test_start)\nshow(p)","a2aa5956":"train_daily_browser = train_ts.groupby([pd.Grouper(freq='D'), 'device.browser']).size().reset_index()\ntrain_daily_browser['set'] = 'train'\ntest_daily_browser = test_ts.groupby([pd.Grouper(freq='D'), 'device.browser']).size().reset_index()\ntest_daily_browser['set'] = 'test'\ndaily_browser = pd.concat([train_daily_browser, test_daily_browser], axis=0)\ndaily_browser.rename(columns={0: 'count'}, inplace=True)\ndaily_browser['perc'] = daily_browser['count']\n\ndaily_browser_merged = pd.merge(\n    daily_browser,\n    daily_sess_count.reset_index()[['visitStartTime', 'count'\n                                    ]].rename(columns={'count': 'sess_count'}),\n    on='visitStartTime')\ndaily_browser_merged[\n    'perc'] = daily_browser_merged['count'] \/ daily_browser_merged['sess_count']","38450fac":"top_browsers = set(train['device.browser'].value_counts().index[:5])\ntop_browsers","88801178":"p = figure(\n    x_axis_type='datetime',\n    width=700,\n    height=600,\n    title='browser percentage shares over time')\nfor idx, browser in enumerate(top_browsers):\n    p.line(\n        source=daily_browser_merged[\n            (daily_browser_merged['device.browser'] == browser)\n            & (daily_browser_merged['perc'] < 1)],\n        x='visitStartTime',\n        y='perc',\n        color=Set1_9[idx],\n        legend=browser)\np.y_range = Range1d(0, 1)\np.legend.click_policy = 'hide'\np.add_tools(\n    HoverTool(tooltips=[('category',\n                         '@{device.browser}'), ('percentage',\n                                                '@perc'), ('count',\n                                                           '@count')]))\ntest_start = Span(\n    location=test_start_time.timestamp() * 1000,\n    dimension='height',\n    line_dash='dashed')\np.add_layout(test_start)\nshow(p)","a082b412":"train_daily_os = train_ts.groupby([pd.Grouper(freq='D'), 'device.operatingSystem']).size().reset_index()\ntrain_daily_os['set'] = 'train'\ntest_daily_os = test_ts.groupby([pd.Grouper(freq='D'), 'device.operatingSystem']).size().reset_index()\ntest_daily_os['set'] = 'test'\ndaily_os = pd.concat([train_daily_os, test_daily_os], axis=0)\ndaily_os.rename(columns={0: 'count'}, inplace=True)\ndaily_os['perc'] = daily_os['count']\n\ndaily_os_merged = pd.merge(\n    daily_os,\n    daily_sess_count.reset_index()[['visitStartTime', 'count'\n                                    ]].rename(columns={'count': 'sess_count'}),\n    on='visitStartTime')\ndaily_os_merged[\n    'perc'] = daily_os_merged['count'] \/ daily_os_merged['sess_count']","996d6d77":"top_os = set(train['device.operatingSystem'].value_counts().index[:6])\ntop_os","a2793bf6":"p = figure(\n    x_axis_type='datetime',\n    width=700,\n    height=600,\n    title='OS percentage shares over time')\nfor idx, os in enumerate(top_os):\n    p.line(\n        source=daily_os_merged[\n            (daily_os_merged['device.operatingSystem'] == os)\n            & (daily_os_merged['perc'] < 1)],\n        x='visitStartTime',\n        y='perc',\n        color=Set1_9[idx],\n        legend=os)\np.y_range = Range1d(0, 0.7)\np.legend.click_policy = 'hide'\np.add_tools(\n    HoverTool(tooltips=[('category',\n                         '@{device.operatingSystem}'), ('percentage',\n                                                        '@perc'), ('count',\n                                                                   '@count')]))\ntest_start = Span(\n    location=test_start_time.timestamp() * 1000,\n    dimension='height',\n    line_dash='dashed')\np.add_layout(test_start)\nshow(p)","b76ee961":"train_daily_revenue = pd.DataFrame({\n    'sum':\n    train_ts.groupby(pd.Grouper(freq='D'))['totals.transactionRevenue'].sum(),\n    'set':\n    'train'\n})\ntest_daily_revenue = pd.DataFrame({'dummy': test_ts.groupby(pd.Grouper(freq='D')).size(), 'set': 'test'})\ntest_daily_revenue['sum'] = train_daily_revenue['sum'].mean()\ntest_daily_revenue.drop('dummy', axis=1, inplace=True)\ndaily_revenue = pd.concat([train_daily_revenue, test_daily_revenue], axis=0, sort=True)\n\np = figure(\n    x_axis_type='datetime',\n    width=700,\n    height=400,\n    title='revenue over time?')\np.line(\n    source=daily_revenue[daily_revenue['set'] == 'train'],\n    x='visitStartTime',\n    y='sum',\n    color=Set1_9[0])\np.line(\n    source=daily_revenue[daily_revenue['set'] == 'test'],\n    x='visitStartTime',\n    y='sum',\n    color=Set1_9[1])\np.legend.location = 'top_left'\ntest_start = Span(\n    location=test_start_time.timestamp() * 1000,\n    dimension='height',\n    line_dash='dashed')\np.add_layout(test_start)\nshow(p)","5727d046":"### Hits\/pageviews over time:","459028eb":"After trying a few generic model-building techniques, I started to reflect on my data exploration and processing procedure to find out why I was not able to build better models that not only score higher but also produce predictions that make sense. I soon realised that I missed a very important step in the analysis: compare different parts of the data as well as compare the training set and the test set to see if the data samples are distributed identically and independently, if the training and test samples are distributed similarly, and if there are any inconsistencies preventing us from generalise from one part of the data to another. For any data that can be treated as a time series, if we are to perform any predictive analysis on it, it is important to look out for the stationarity and trend consistency of the series.\n\nNon-stationary time series is problematic for prediction. If the features next year will look completely different from this year, how can we be sure that we can generalise what we learned this year to the next year? Most machine learning algorithms are better at interpolation than extrapolation, so we always have to be careful when the range of the training features and features used to predict do not overlap well. Sudden changes in the time series may also indicate the occurrence of major events that should be taken into account in the analysis.\n\nFor our problem, we first look at the time ranges of the training and test dataset to see whether they are random samples from the same population or non-overlapping samples from different populations:","42d98ce9":"# Summary of Non-stationarities and Anomalies in Feature Time Series","79f75bb6":"### Sessions over time","9f60ae82":"### Top OS weight over time:","24ccdf09":"Let us record the starting time of the testing samples:","fc2f9a35":"Here we see a large jump of mobile OS share around the same time we saw the overall mobile percentage numbers jump earlier. There are also a few spikes for Android in the test portion of the data, which is quite unlike what it looked like before. There is also a spike followed by continue decline for Mac users.","739c36d8":"Now let us first look at the total sessions over time, including both training and testing data:","dffe02b0":"We might have expected the percentage of mobile users to gradually increase over time, but we also observed a rather peculiar jump around the end of 2016. Does it have more to do with the overall mobile market, or is it just because Google launched or revamped an online store for mobile? Is it related to the dip in daily sessions we see earlier? We have yet to answer these questions.\n\nAnyway, the observation here indicates that the condition for before the end of 2016 and after might be different enough that the usefulness of this feature at predicting revenue might not carry over to afterwards, including the test portion of the data, which is more similar to what happened after 2016. Now I will not be surprised if in some models, removing this feature from data points prior to 2017 actually increases its performance.","b8268d02":"### ChannelGrouping categories weight over time:","e9116bc7":"**(Click on the legend to hide one or more lines in the plot)**\n\nApparently a lot was going on at the end of 2016. Not only did 'Social' channel suffered a massive drop in percentage share, 'Organic Search' recovered from a dip and the 'Direct' channel enjoyed a moderate increase. This is also around the time that the overall session count decreased, so the drop in the absolute number of 'Social' channel is likely to driving force for the series of changes. 'Social' channel further suffered a drop around May of 2017. We also noticed that the 'Display' channel enjoyed a massive share spike in the test period, which had not happened at all throughout the training peroid. Does this mean we have to treat each of the channels differently and assign them different 'usefulness rating' for predicting user revenue? It appears to be the case here. Treating this attribute as a single column looks like a less appealing option after seeing this.","655ab63d":"We do not see a clear difference in the overall trend between pageviews and hits, but we can see that both have a yearly pattern that starts low at the beginning of the year, climbs throughout the year and drops at the end. This trend appears to be consistent in the slice of data we can see, however unfortunately we only have one cycle within the training set, so the cycle-to-cycle pattern and how that relates to overall revenue cannot be easily discovered.","0c7f0e08":"We can see from this plot that the browser percentage shares are not behaving normally around a short period of time, but otherwise appear to be someewhat stationary over time.","40980b1e":"### Daily mobile percentage over time:","44edbce3":"### Revenue over time?","5cfef915":"### Pageviews and hits over time:","a4caa2b0":"As we see below, the training and testing data are split according to time. There is no overlap between them, and apparently they cannot possibly have been drawn from the same sample, and it is likely that something has changed in the behaviour of the features over the transition from the training period to the testing period, and we have to watch out for it.","c5c09a8c":"As we see above, many of the attributes are either non-stationary, have peculiar trends during a certain period of time, or behaves quite differently in the training and test set. Surely, some of these non-stationarities and train\/test distinctions will have to be taken into account for making predictions about future.\n\n(Many questions to answer still)","388fe43f":"Apart from one obvious outlier, we see that daily sessions fluctuates quite a bit over time. Trend-wise, there is a steep decline around the end of 2016 and a slow increasing trend afterwards. Overall, the a period from 2017 to the end of the training data appears to have more in common with the test portion of the data. I would like to find the reason for the decrease in daily sessions around the end of 2016.","faee6e8f":"### Top browser weight over time:"}}