{"cell_type":{"ad271513":"code","f73479a9":"code","8331a6f4":"code","de2f5df7":"code","525e6992":"code","f8622326":"code","49f148a1":"code","b963635e":"code","06699549":"code","f3025ddd":"code","31567869":"code","cb7d5b98":"code","ede554f7":"code","38475b1e":"code","da837ab2":"code","72d2d8a1":"code","755178f9":"code","7e38150f":"code","178671bd":"code","4130556e":"code","bc089d31":"code","36797a7c":"code","ab3fae89":"code","0f4a56e7":"code","1d72d6ad":"code","e0219f13":"code","f837828c":"code","94f9ef0d":"code","a5f56121":"code","6ffc5fc1":"code","6fa29b0e":"code","4926e030":"code","72f101f9":"code","9086417c":"code","683ffc24":"code","36bf64d6":"code","670c36b4":"code","4904fd2c":"code","5be26309":"code","1c677617":"code","ff29207a":"code","a5e85573":"code","a376f769":"code","d78d5541":"code","f94c9408":"code","e35d18c4":"code","a6917489":"code","639065e0":"code","e52db87e":"code","e96d7cb7":"code","a3cfb440":"code","799aceb4":"code","5086629f":"code","13bda691":"code","48ce47cc":"code","d6888b8d":"code","394c581b":"code","4b4f1a0b":"code","75ccebcb":"code","408e60dc":"code","28cdf8d1":"code","68deab19":"markdown","8e783f84":"markdown","ccff671c":"markdown","4744fce5":"markdown","363dd224":"markdown","7cc26f5b":"markdown","f49567f5":"markdown","2cb65cce":"markdown","85fc2bbe":"markdown","e2b0966a":"markdown","d3876703":"markdown","90a1ae6a":"markdown","2305af01":"markdown","fc3643d7":"markdown","8f1918c7":"markdown","bb84a5cd":"markdown","ccb01d44":"markdown","491ff650":"markdown","1ff76d50":"markdown","4bdbf2ff":"markdown","b88c2542":"markdown","73408b88":"markdown","5bfa5704":"markdown","a746908f":"markdown","56b18f12":"markdown","c994cf19":"markdown","fb57d1b4":"markdown","0a591ec0":"markdown","3d9bec3b":"markdown","cb9c7109":"markdown","34e79423":"markdown","a2dd4cf1":"markdown","3f243616":"markdown","970d2902":"markdown","40e71eed":"markdown","c6ee33cc":"markdown"},"source":{"ad271513":"import os\nimport re\nimport gc\nimport math\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom itertools import product\nfrom tqdm import tqdm_notebook as tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nimport stldecompose\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\nskip_processing = False\n\nprint(os.listdir('..\/input'))","f73479a9":"sales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsales['date'] = sales.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\ncategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsales.head()","8331a6f4":"len(sales.date.apply(lambda x: x.strftime('%m-%d')).unique())","de2f5df7":"sales.date_block_num.unique()","525e6992":"def plot_seasonal(res, axes, axes_col):\n    for i, p in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        axes[i, axes_col].plot(getattr(res, p))\n        if not axes_col:\n            axes[i, axes_col].set_ylabel(p.title())","f8622326":"total_ts = sales.groupby('date_block_num').item_cnt_day.sum()\nfig, axes = plt.subplots(4, 3, figsize=(12, 6))\n\nadd_decomposed = sm.tsa.seasonal_decompose(total_ts.values, freq=12, model=\"additive\")\nmul_decomposed = sm.tsa.seasonal_decompose(total_ts.values, freq=12, model=\"multiplicative\")\nstl_decompose = stldecompose.decompose(total_ts.values, period=12)\n\nplot_seasonal(add_decomposed, axes, 0)\nplot_seasonal(mul_decomposed, axes, 1)\nplot_seasonal(stl_decompose, axes, 2)\n\naxes[0, 0].set_title('Additive')\naxes[0, 1].set_title('Multiplicative')\naxes[0, 2].set_title('Lowess')\nfig.tight_layout();","49f148a1":"def plot_monthly_shop_sales(shops_per_row=3, height_scalar=15):\n    shop_sales = sales.groupby(['date', 'shop_id']).agg({'item_cnt_day': 'sum'})\n    shop_sales = shop_sales.unstack(level=0).transpose()\n    \n    num_shops = len(shop_sales.columns)\n    \n    nrows = math.ceil(num_shops \/ shops_per_row)\n    height = height_scalar * shops_per_row\n    \n    fig, axes = plt.subplots(nrows, 1, figsize=(10, height))\n    for i in range(0, num_shops, shops_per_row):\n        ax_row = axes[int(i \/ shops_per_row)]\n        shop_sales.iloc[:,i:i+shops_per_row].plot(ax=ax_row, alpha=0.8)\n        ax_row.set_xlabel('')\n        ax_row.set_xticklabels([])\n    fig.tight_layout()\nplot_monthly_shop_sales()","b963635e":"shop_months = [{'shop_id': ind, 'total_months': len(x), 'min': min(x), \n                'max': max(x), 'missing_months': 1 + max(x) - min(x) != len(x)} \n               for ind, x in \n                   sales.groupby(['shop_id']).date_block_num.unique().items()]\nshop_months = pd.DataFrame(shop_months)\nshop_months[shop_months['max'] < 33]","06699549":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.tail()","f3025ddd":"categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ncategories.head()","31567869":"categories[categories.item_category_name.apply(lambda x: ' - ' not in x and '(' not in x)]","cb7d5b98":"def get_main_cat(name):\n    if ' - ' in name:\n        return name.split(' - ')[0]\n    elif '(' in name:\n        return name.split('(')[0].strip()\n    return name\ncategories['main_category'] = categories.item_category_name.apply(get_main_cat)\ncategories.main_category.unique()","ede554f7":"translate_categories = {\n    'PC': 'PC',\n    '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b': 'Accessories',\n    '\u0426\u0438\u0444\u0440\u0430': 'Figure',\n    '\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430': 'Delivery of goods',\n    '\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438': 'Game consoles',\n    '\u0418\u0433\u0440\u044b': 'Games',\n    '\u0418\u0433\u0440\u044b Android': 'Android games',\n    '\u0418\u0433\u0440\u044b MAC': 'Games MAC',\n    '\u0418\u0433\u0440\u044b PC': 'Games PC',\n    '\u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b': 'Cinema, Music, Games',\n    '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b': 'Payment cards',\n    '\u041a\u0438\u043d\u043e': 'Cinema',\n    '\u0411\u0438\u043b\u0435\u0442\u044b': 'Tickets',\n    '\u041a\u043d\u0438\u0433\u0438': 'Books',\n    '\u041c\u0443\u0437\u044b\u043a\u0430': 'Music',\n    '\u041f\u043e\u0434\u0430\u0440\u043a\u0438': 'Gifts',\n    '\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b': 'Programs',\n    '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435': 'Utilities',\n    '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438': 'Clean Media',\n    '\u042d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043f\u0438\u0442\u0430\u043d\u0438\u044f': 'Batteries'\n}\ncategories['main_category'] = categories.main_category.apply(lambda x: translate_categories[x])\ncategories.main_category.unique()","38475b1e":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv').set_index('shop_id')\nshops.tail()","da837ab2":"_shop_name_replace = {\n    '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f': '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f \"\"',\n    '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c?': '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \"\u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c?\"',\n    '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2': '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \"\u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\"',\n    '\u0412\u043e\u0440\u043e\u043d\u0435\u0436 (\u041f\u043b\u0435\u0445\u0430\u043d\u043e\u0432\u0441\u043a\u0430\u044f, 13)': '\u0412\u043e\u0440\u043e\u043d\u0435\u0436 \"\u041f\u043b\u0435\u0445\u0430\u043d\u043e\u0432\u0441\u043a\u0430\u044f, 13\"',\n    '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d \u0427\u0421': '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d \"\u0427\u0421\"',\n    '\u041c\u043e\u0441\u043a\u0432\u0430 \u041c\u0430\u0433\u0430\u0437\u0438\u043d \u042121': '\u041c\u043e\u0441\u043a\u0432\u0430 \"\u041c\u0430\u0433\u0430\u0437\u0438\u043d \u042121\"',\n    '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d': '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 \"1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d\"',\n    '\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56': '\u042f\u043a\u0443\u0442\u0441\u043a \"\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\"',\n    '!\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56 \u0444\u0440\u0430\u043d': '\u042f\u043a\u0443\u0442\u0441\u043a \"\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56 \u0444\u0440\u0430\u043d\"',\n    '\u0412\u043e\u0440\u043e\u043d\u0435\u0436 \u0422\u0420\u0426 \u0421\u0438\u0442\u0438-\u041f\u0430\u0440\u043a \"\u0413\u0440\u0430\u0434\"': '\u0412\u043e\u0440\u043e\u043d\u0435\u0436 \u0422\u0420\u0426 \"\u0421\u0438\u0442\u0438-\u041f\u0430\u0440\u043a \u0413\u0440\u0430\u0434\"'\n}\nshops['shop_name'] = shops.shop_name.apply(lambda x: _shop_name_replace[x] if x in _shop_name_replace else x)","72d2d8a1":"_shop_type = {\n    '\u0422\u0426': 'Shopping center',\n    '\u0422\u0420\u041a': 'Dispenser',\n    '\u0422\u0420\u0426': 'Shopping mall',\n    '\u0422\u041a': 'TC',\n    '\u041c\u0422\u0420\u0426': 'MTRC',\n    '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439': 'Digital Warehouse',\n    '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d': 'Online'\n}\n#Get the shop type of the store\n_type = shops.shop_name.apply(lambda x: [w for w in x.split() if w in _shop_type])\nshops['shop_type'] = _type.apply(lambda x: _shop_type[x[0]] if len(x) else np.nan)\n#Get the city from the shop name: !Moscow TC \"Store 26\" -> Moscow\n_city = shops.shop_name.apply(lambda x: x.split(' \"')[0])\n_city = _city.apply(lambda x: \" \".join([w for w in x.split() if w not in _shop_type]))\n_city = _city.str.replace('!', '').str.title()\nshops['shop_city'] = _city\nshops.head()","755178f9":"_pop_replace = {\n    '': np.nan, '\u0410\u0434\u044b\u0433\u0435\u044f': 282419, '\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430': 228567, '\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439': 320761, '\u0412\u043e\u043b\u043e\u0433\u0434\u0430': 305397, \n    '\u0412\u043e\u0440\u043e\u043d\u0435\u0436': 997447, '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f': np.nan, '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0423\u043b.': 107994, '\u041a\u0430\u0437\u0430\u043d\u044c': 1169000, \n    '\u041a\u0430\u043b\u0443\u0433\u0430': 328871, '\u041a\u043e\u043b\u043e\u043c\u043d\u0430': 144838, '\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a': 1007000, '\u041a\u0443\u0440\u0441\u043a': 425950, \n    '\u041c\u043e\u0441\u043a\u0432\u0430': 11920000, '\u041c\u044b\u0442\u0438\u0449\u0438': 176825, '\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434': 1257000, '\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a': 1511000, \n    '\u041e\u043c\u0441\u043a': 1159000, '\u0420\u043e\u0441\u0442\u043e\u0432\u043d\u0430\u0434\u043e\u043d\u0443': 1100000, '\u0421\u043f\u0431': 4991000, '\u0421\u0430\u043c\u0430\u0440\u0430': 1170000, \n    '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434': 109076, '\u0421\u0443\u0440\u0433\u0443\u0442': 321062, '\u0422\u043e\u043c\u0441\u043a': 543596, '\u0422\u044e\u043c\u0435\u043d\u044c': 621918, '\u042f\u043a\u0443\u0442\u0441\u043a': 282419, \n    '\u0423\u0444\u0430': 1075000, '\u0425\u0438\u043c\u043a\u0438': 218275, '\u0421\u043a\u043b\u0430\u0434': np.nan, '\u0427\u0435\u0445\u043e\u0432': 71301, '\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c': 597161\n}\nshops['shop_city_pop'] = shops.shop_city.map(lambda x: _pop_replace[x])\nshops.head()","7e38150f":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\ntest.head()","178671bd":"len(set(test.item_id) - set(sales.item_id)), len(set(test.shop_id) - set(sales.shop_id))","4130556e":"test.shop_id.nunique() * test.item_id.nunique() == test.shape[0]","bc089d31":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\n#Create index for every combination of shop and item per month\nindex = []\nfor dbn in sales.date_block_num.unique():\n    shop_ids = sales[sales.date_block_num == dbn].shop_id.unique()\n    item_ids = sales[sales.date_block_num == dbn].item_id.unique()\n    index.append(np.array(list(product(shop_ids, item_ids, [dbn])), dtype='int16'))\nindex = pd.DataFrame(np.vstack(index), columns=['shop_id', 'item_id', 'date_block_num'])\nindex = index.set_index(['shop_id', 'item_id', 'date_block_num']).index\nindex.shape","36797a7c":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\nsales_train = sales.groupby(['shop_id', 'item_id', 'date_block_num'])\nsales_train = sales_train.agg({'item_cnt_day':'sum'})\nsales_train = sales_train.reindex(index=index)\nsales_train = sales_train.reset_index()\nsales_train = sales_train.rename(columns={'item_cnt_day': 'item_cnt_month'})\nsales_train['item_cnt_month'] = sales_train.item_cnt_month.clip(0, 20).fillna(0)\nsales_train = sales_train.set_index(['shop_id', 'item_id', 'date_block_num'])\nsales_train.shape","ab3fae89":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\nsales_train = sales_train.reset_index().merge(items[['item_id', 'item_category_id']], on = 'item_id')\nsales_train.head()","0f4a56e7":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\nsales_train = sales_train.fillna(0)\ntrain_X, train_y = sales_train.drop('item_cnt_month', axis=1), sales_train.item_cnt_month.clip(0, 20)\nmodel = lgb.LGBMModel(objective='regression', max_depth=10, n_estimators=100, min_child_weight=0.5, \n                         random_state=40, n_jobs=-1, silent=False)\nmodel.fit(train_X, train_y, eval_metric='rmse')","1d72d6ad":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest = test.merge(items[['item_id', 'item_category_id']], on='item_id')\ntest = test.set_index('ID')\ntest['date_block_num'] = 34\npred = model.predict(test)\npred = pd.DataFrame(pred, columns=['item_cnt_month']).fillna(0).clip(0, 20)\npred.index.names = ['ID']\npred.to_csv('baseline.csv')","e0219f13":"def set_type(series, to_float=False):\n    ints = [('int8', 255), ('int16', 65535), ('int32', 2147483647), ('int64', np.inf)]\n    floats = [('float16', 32767), ('float32', 2147483647), ('float64', np.inf)]\n    dtype = series.dtype.name\n    if dtype.startswith('int') and not to_float:\n        maxval = series.abs().max()\n        for key, val in ints:\n            if maxval < val:\n                return series.astype(key)\n    if dtype.startswith('float') or (to_float and dtype.startswith('int')):\n        maxval = series.abs().max()\n        for key, val in floats:\n            if maxval < val:\n                return series.astype(key)\n    if dtype in {'object', 'category'}:\n        l = LabelEncoder()\n        return l.fit_transform(series.fillna('Other')).astype('int8')\n    return series\ndef minimize_memory(df, reset_index=True, to_float=False):\n    if reset_index:\n        df = df.reset_index()\n    for col in df.columns:\n        df[col] = set_type(df[col], to_float)\n    return df","f837828c":"del add_decomposed, mul_decomposed, stl_decompose, shop_months, train_X, train_y, model, pred, shop_ids, item_ids\ngc.collect()","94f9ef0d":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\nlags = [1, 2, 3, 6, 12]\n#train\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntrain = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day <= 1000]\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nmedian = train.loc[(train.shop_id==32) & (train.item_id==2973) & \n                   (train.date_block_num==4) & (train.item_price>0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median\ntrain['item_revenue_month'] = train.item_cnt_day * train.item_price\ntrain = train.groupby(['shop_id', 'item_id', 'date_block_num'])\ntrain = train.agg({'item_cnt_day': 'sum', 'item_price': 'mean', 'item_revenue_month': 'sum'})\ntrain.rename(columns={'item_cnt_day': 'item_cnt_month', 'item_price': 'item_mean_price'}, inplace=True)\ntrain = train.reindex(index=index)\n#test\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest_ids = test.ID\ntest['date_block_num'] = 34\ntest = test.set_index(['shop_id', 'item_id', 'date_block_num']).drop('ID', axis=1)\n#join together\ntrain_test = pd.concat([train, test])\ntrain_test = train_test.fillna(0).astype({'item_cnt_month': 'int32', 'item_mean_price': 'int32'})\ntrain_test = minimize_memory(train_test)\ntrain_test.head()","a5f56121":"#Remove outliers\nif skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test.loc[train_test.shop_id == 0, 'shop_id'] = 57\ntrain_test.loc[train_test.shop_id == 1, 'shop_id'] = 58\ntrain_test.loc[train_test.shop_id == 10, 'shop_id'] = 11\ntrain_test['item_cnt_month'] = train_test.item_cnt_month.fillna(0).clip(0, 20)\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)","6ffc5fc1":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ndef add_aggregate_lags(df, gb_cols, target_col, prefix, astype='float64', \n                       fillna=np.nan, lags=lags, agg='mean'):\n    df = df.reset_index()\n    _gb = df.groupby(gb_cols).agg({target_col: agg})\n    for lag in lags:\n        _temp = _gb.copy()\n        name = prefix + str(lag)\n        _temp.reset_index(inplace=True)\n        _temp['date_block_num'] += lag\n        _temp = _temp.rename(columns={target_col: name})\n        df = pd.merge(df, _temp, on=gb_cols, how='left')\n        df[name] = df[name].fillna(fillna).astype(astype)\n    return df.set_index(['shop_id', 'item_id', 'date_block_num'])\ntrain_test = add_aggregate_lags(train_test, ['shop_id', 'item_id', 'date_block_num'], 'item_cnt_month', \n                                'shop_item_sales_lag_', fillna=0, astype='int16', agg='sum')\ntrain_test = add_aggregate_lags(train_test, ['shop_id', 'date_block_num'], 'item_cnt_month', 'shop_sales_lag_', \n                                fillna=0, astype='int32', agg='sum')\ntrain_test = add_aggregate_lags(train_test, ['item_id', 'date_block_num'], 'item_cnt_month', 'item_sales_lag_', \n                                fillna=0, astype='int32', agg='sum')\ntrain_test = add_aggregate_lags(train_test, ['shop_id', 'item_id', 'date_block_num'], 'item_mean_price', \n                                'shop_item_price_lag_', lags=[1, 2, 3])\ntrain_test = add_aggregate_lags(train_test, ['item_id', 'date_block_num'], 'item_mean_price', \n                                'item_price_lag_', lags=[1, 2, 3])\ntrain_test.tail()","6fa29b0e":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ndays_per_block = pd.DataFrame({'days_per_block': sales.date.apply(lambda x: x.strftime('%m-%d'))})\ndays_per_block['date_block_num'] = sales.date_block_num\ndays_per_block = days_per_block.groupby('date_block_num').nunique()[['days_per_block']].reset_index()\ndays_per_block = days_per_block.append({'date_block_num': 34, 'days_per_block': 30}, ignore_index=True)\ntrain_test = train_test.reset_index().merge(days_per_block, on='date_block_num')\ntrain_test['days_per_block'] = (train_test.days_per_block - 30).astype('int8')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test[['days_per_block']].tail()","4926e030":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\n#Inspired by dlarionov\ndef add_since_features(df, key_func, name):\n    known = {}\n    df[name] = -1\n    df[name] = df[name].astype(np.int8)\n    for i, row in df.iterrows():    \n        key = key_func(row)\n        if key not in known:\n            if row.item_cnt_month > 0:\n                known[key] = row.date_block_num\n        else:\n            if known[key] < row.date_block_num:\n                df.at[i, name] = row.date_block_num - known[key]\n                known[key] = row.date_block_num  \n    return df\ntrain_test.reset_index(inplace=True)\ntrain_test = add_since_features(train_test, lambda r: str(r.shop_id) + '_' + str(r.item_id), 'm_since_last_shop_item_sale')\ntrain_test = add_since_features(train_test, lambda r: str(r.shop_id), 'm_since_last_shop_sale')\ntrain_test = add_since_features(train_test, lambda r: str(r.item_id), 'm_since_last_item_sale')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test.iloc[-5:,-3:]","72f101f9":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ndef add_mean_encodings(df, index_cols, target_col, name):\n    _gb = df.groupby(index_cols)[[target_col]].mean()\n    _gb.rename(columns={target_col: name}, inplace=True)\n    return pd.merge(df, _gb.reset_index(), on=index_cols, how='left')\ntrain_test.reset_index(inplace=True)\ntrain_test = add_mean_encodings(train_test, ['shop_id'], 'item_cnt_month', 'shop_mean')\ntrain_test = add_mean_encodings(train_test, ['item_id'], 'item_cnt_month', 'item_mean')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test = minimize_memory(train_test).set_index(['shop_id', 'item_id', 'date_block_num'])\ntrain_test[['shop_mean', 'item_mean']].tail()","9086417c":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\n_gb = sales.groupby(['shop_id', 'item_id', 'date_block_num']).agg({'item_cnt_day': ['sum', 'last']})\n_gb.columns = ['month_sum', 'month_last']\n_gb = pd.merge(_gb.reset_index(), days_per_block, on='date_block_num')\n_gb['date_block_num'] += 1\n_gb['end_of_month_percent'] = (_gb.month_last \/ (_gb.month_sum \/ (_gb.days_per_block + 30)))\n_gb = _gb[['shop_id', 'item_id', 'date_block_num', 'end_of_month_percent']]\n_gb['end_of_month_percent'] = _gb.end_of_month_percent.fillna(0)\ntrain_test = pd.merge(train_test.reset_index(), _gb, \n                      on=['shop_id', 'item_id', 'date_block_num'], how='left')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test[['item_cnt_month', 'end_of_month_percent']].head()","683ffc24":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test.reset_index(inplace=True)\ntrain_test = pd.merge(train_test, shops[['shop_type', 'shop_city', 'shop_city_pop']], \n         on='shop_id', validate='many_to_one', how='left')\ntrain_test = add_mean_encodings(train_test, ['shop_type'], 'item_cnt_month', 'shop_type_mean')\ntrain_test = add_mean_encodings(train_test, ['shop_city'], 'item_cnt_month', 'shop_city_mean')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test[['shop_type', 'shop_city', 'shop_city_pop']].iloc[20000:20010,:]","36bf64d6":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test.reset_index(inplace=True)\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems = items.merge(categories, on='item_category_id', validate='m:1', how='left')\ntrain_test = train_test.merge(items[['item_id', 'item_category_name', 'main_category']], \n                              on='item_id', how='left', validate='m:1')\ntrain_test = train_test.rename(columns={'item_category_name': 'item_category_full', 'main_category': 'item_category_main'})\ntrain_test = add_mean_encodings(train_test, ['item_category_full'], 'item_cnt_month', 'item_category_full_mean')\ntrain_test = add_mean_encodings(train_test, ['item_category_main'], 'item_cnt_month', 'item_category_main_mean')\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test = minimize_memory(train_test).set_index(['shop_id', 'item_id', 'date_block_num'])\ntrain_test.iloc[:5,-4:]","670c36b4":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test = add_aggregate_lags(train_test.astype({'item_revenue_month': 'float64'}), ['date_block_num'], \n                                'item_revenue_month', 'monthly_revenue_lag_', \n                                astype='int32', fillna=0, agg='sum')\ntrain_test = add_aggregate_lags(train_test.astype({'item_revenue_month': 'float64'}), \n                                ['shop_id', 'item_id', 'date_block_num'], 'item_revenue_month', \n                                'shop_item_revenue_lag_', astype='int32', fillna=0, agg='sum')\ntrain_test.iloc[:5,:][['item_revenue_month'] + [c for c in train_test.columns if c.startswith('shop_item_rev')]]","4904fd2c":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test = add_aggregate_lags(train_test, ['date_block_num'], 'item_cnt_month', \n                   'monthly_sales_lag', fillna=0, agg='sum', lags=[1, 2, 3, 6, 12])","5be26309":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\n#Credit to dlarionov\ntrain_test.reset_index(inplace=True)\ntrain_test['m_since_shop_item_first_sale'] = train_test['date_block_num'] - train_test.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\ntrain_test['m_since_item_first_sale'] = train_test['date_block_num'] - train_test.groupby('item_id')['date_block_num'].transform('min')","1c677617":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ntrain_test['month'] = train_test.date_block_num % 12\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)","ff29207a":"if skip_processing:\n    raise Exception(\"Skipping data processing steps\")\ndel _gb, sales, items, categories, test, train, shops\ngc.collect()\ntrain_test.reset_index(inplace=True)\ntrain_test.replace([np.inf, -np.inf], np.nan, inplace=True)\ntrain_test = minimize_memory(train_test, reset_index=False)\ntrain_test.set_index(['shop_id', 'item_id', 'date_block_num'], inplace=True)\ntrain_test.to_pickle('data.pkl')","a5e85573":"train_test = pd.read_pickle('..\/input\/traintestset\/data.pkl')\ntrain_test.info()","a376f769":"train_test.drop(['item_mean_price', 'item_revenue_month'], axis=1, inplace=True) #Only used for lag features\ntrain_test.reset_index(inplace=True)\nX_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 32)].drop('item_cnt_month', axis=1)\ny_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 32)].item_cnt_month\nX_eval = train_test[train_test.date_block_num == 33].drop('item_cnt_month', axis=1)\ny_eval = train_test[train_test.date_block_num == 33].item_cnt_month\ndel train_test\ngc.collect()","d78d5541":"lgb_model = lgb.LGBMRegressor(objective='regression_l2', n_estimators=1000, reg_alpha=0.0, reg_lambda=0.0, \n                              random_state=40, n_jobs=-1, silent=False, max_depth=15, num_leaves=70, \n                              subsample=0.8, learning_rate=0.1)","f94c9408":"categoricals = {'shop_type','shop_city','item_category_full','item_category_main'}\ncategoricals = [i for i, n in enumerate(X_train.columns) if n in categoricals]\nlgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_eval, y_eval)], \n              verbose=True, eval_names=['Train', 'Eval'], early_stopping_rounds=100,\n              categorical_feature=categoricals)","e35d18c4":"lgb_results = pd.DataFrame({'Eval': lgb_model.evals_result_['Eval']['l2'], 'Train': lgb_model.evals_result_['Train']['l2']})\nax = lgb_results.plot(figsize=(12, 6))\nax.set_title('LightGBM Model RMSE')\nax.set_xlabel('Cycle')\nax.set_ylabel('RMSE');","a6917489":"features = pd.DataFrame(list(zip(X_train.columns, lgb_model.feature_importances_)), columns=['Feature', 'Importance'])\nfeatures.sort_values('Importance', ascending=True, inplace=True)\nax = features.plot('Feature', 'Importance', kind='barh', figsize=(15, 12))\nax.set_xlabel('Importance (Higher is better)')\nax.set_ylabel('')\nax.set_title('Feature Importance');","639065e0":"train_test = pd.read_pickle('..\/input\/traintestset\/data.pkl')\npred_index = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","e52db87e":"train_test.drop(['item_mean_price', 'item_revenue_month'], axis=1, inplace=True) #Only used for lag features\ntrain_test.reset_index(inplace=True)\nX_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 33)].drop('item_cnt_month', axis=1)\ny_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 33)].item_cnt_month\nX_eval = train_test[train_test.date_block_num == 34].drop('item_cnt_month', axis=1)\ndel train_test\ngc.collect()","e96d7cb7":"categoricals = {'shop_type','shop_city','item_category_full','item_category_main'}\ncategoricals = [i for i, n in enumerate(X_train.columns) if n in categoricals]\nlgb_model.set_params(n_estimators=332) #Strongest number of trees in last run\nlgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], \n              verbose=True, eval_names=['Train'], early_stopping_rounds=50,\n              categorical_feature=categoricals)","a3cfb440":"pred = lgb_model.predict(X_eval)","799aceb4":"X_eval['item_cnt_month'] = pred\nX_eval = X_eval[['shop_id', 'item_id', 'item_cnt_month']]","5086629f":"pred_index = pred_index.reset_index().merge(X_eval, on=['shop_id', 'item_id'], how='left')\npred_index = pred_index[['ID', 'item_cnt_month']].set_index('ID')\npred_index['item_cnt_month'] = pred_index.item_cnt_month.clip(0, 20).fillna(0)\npred_index.to_csv('final.csv')","13bda691":"train_test = pd.read_pickle('..\/input\/traintestset\/data.pkl')\ntrain_test.drop(['item_mean_price', 'item_revenue_month'], axis=1, inplace=True) #Only used for lag features\ntrain_test.reset_index(inplace=True)\nX_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 32)].drop('item_cnt_month', axis=1)\ny_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 32)].item_cnt_month\nX_eval = train_test[train_test.date_block_num == 33].drop('item_cnt_month', axis=1)\ny_eval = train_test[train_test.date_block_num == 33].item_cnt_month\ndel train_test\ngc.collect()","48ce47cc":"xgb_model = xgb.XGBRegressor(max_depth=9, learning_rate=0.1, n_estimators=200, subsample=1,\n                             colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n                             verbosity=1, n_jobs=-1, gamma=0, random_state=40, \n                             min_child_weight=1, max_delta_step=0, \n                             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5)","d6888b8d":"xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_eval, y_eval)], \n              verbose=True, early_stopping_rounds=10)","394c581b":"#max_depth=3, n_estimators=100 => 0.797904  0.838999 no early stopping, \n#max_depth=9, n_estimators=200 => 0.630982  0.780855 at stop 78\ntrain_test = pd.read_pickle('..\/input\/traintestset\/data.pkl')\npred_index = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\ntrain_test.drop(['item_mean_price', 'item_revenue_month'], axis=1, inplace=True) #Only used for lag features\ntrain_test.reset_index(inplace=True)\nX_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 33)].drop('item_cnt_month', axis=1)\ny_train = train_test[(12 <= train_test.date_block_num) & (train_test.date_block_num <= 33)].item_cnt_month\nX_eval = train_test[train_test.date_block_num == 34].drop('item_cnt_month', axis=1)\ndel train_test\ngc.collect()","4b4f1a0b":"xgb_model = xgb.XGBRegressor(max_depth=10, learning_rate=0.1, n_estimators=80, subsample=1,\n                             colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n                             verbosity=1, n_jobs=-1, gamma=0, random_state=40, \n                             min_child_weight=1, max_delta_step=0, \n                             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5)\nxgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], \n              verbose=True, early_stopping_rounds=50)","75ccebcb":"pred = xgb_model.predict(X_eval)\nX_eval['item_cnt_month'] = pred\nX_eval = X_eval[['shop_id', 'item_id', 'item_cnt_month']]\npred_index = pred_index.reset_index().merge(X_eval, on=['shop_id', 'item_id'], how='left')\npred_index = pred_index[['ID', 'item_cnt_month']].set_index('ID')\npred_index['item_cnt_month'] = pred_index.item_cnt_month.clip(0, 20).fillna(0)\npred_index.to_csv('xgboost_predictions.csv')","408e60dc":"import pickle\nwith open(r\"xgboost_model.pickle\", \"wb\") as output_file:\n    pickle.dump(xgb_model, output_file)","28cdf8d1":"pred_index['item_cnt_month'] = pred_index.item_cnt_month.round()\npred_index.to_csv('xgboost_predictions_rounded.csv')","68deab19":"# Building the Model\n#### Feature ideas\n1. Monthly shop and\/or item sales lag\n2. Monthly shop and\/or item price lag\n3. Number of active days per month\n4. Months\/days since last item sale\n5. Months\/days since last shop sale\n6. Mean encodings of shops and items\n7. End-of-month percent-of-mean encodings\n8. Shop population, category, and type\n9. Item categories\n10. Monthly revenue lag\n11. Total monthly sale lag\n12. Monhs since first sale\n13. Month number","8e783f84":"## Quick Baseline","ccff671c":"## Gather all features together\nTest set contains many new items, but train set only includes data where sales have been made.\n\nNeed to index out every shop\/item combination for each month.","4744fce5":"# 10. Revenue lags","363dd224":"# 8. Shop population, category, and type","7cc26f5b":"# 13. Month Number","f49567f5":"# Model Validation\n#### Strategy is to train on months 12-32 and validate on month 33","2cb65cce":"Test contains all combinations of shops and items, but train contains only combinations which resulted in a sale.","85fc2bbe":"### Many shops are limited and infrequent","e2b0966a":"## How do daily sales by shop look?","d3876703":"## How do monthly total sales look?\nWhat months are there?","90a1ae6a":"### Shops have a mostly consistent nomenclature: City \"Shop name\"\nNeed to fix some exceptions:","2305af01":"# 9. Item categories","fc3643d7":"## 1 & 2. Monthly shop and\/or item sales + price lag","8f1918c7":"363 new items in the test set. No new shops.","bb84a5cd":"## Data Checkpoint","ccb01d44":"# Predicting Future Sales - First Competition Entry\nI broke this notebook down into multiple stages.  For the sake of RAM, I've error'd out many of the cells which clean the data, but keep the code for others to see.  I used Microsoft's LightGBM for my predictions, and much of the processing was done on a Google cloud instance.","491ff650":"### ...with some exceptions","1ff76d50":"# 3. Number of active days per month","4bdbf2ff":"## Finally, what does the test set look like?","b88c2542":"#### Better decipher these...","73408b88":"### What is the frequency of sales data?\n365","5bfa5704":"# 12. Months since first shop\/item sale","a746908f":"# Stacked Ensemble","56b18f12":"* Item names have inconsistent formats\n* One category per item","c994cf19":"## What are shops?","fb57d1b4":"# 11. Total monthly sale lags","0a591ec0":"### Clearly significant missing data\n## What are these items?","3d9bec3b":"### Item names have consistent pattern: Main Category -\/() Subcategory","cb9c7109":"# 6. Mean encodings of shops and items","34e79423":"This model looks very strong, with no signs of overfitting.  Let's train it on months 12-33 and predict month 34.\n# Model Predictions","a2dd4cf1":"# XGBoost Predictions","3f243616":"# 4 & 5. Months since last item and\/or shop sale","970d2902":"Need to set date_block_num to 34.  Are there any new items or shops?","40e71eed":"## What are item categories?","c6ee33cc":"# 7. End-of-month Percent encodings"}}