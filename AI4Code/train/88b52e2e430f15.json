{"cell_type":{"f8e410ef":"code","3b8a6c03":"code","00a17be0":"code","2942262d":"code","f65accb2":"code","6f61397a":"code","e931fe6c":"code","cef00ac6":"code","16fd2bd3":"code","8515bd9d":"code","5252f54f":"code","1166f882":"code","815b9241":"code","15414070":"code","447dad35":"code","21af8533":"code","fad85f5f":"code","fedb62a9":"markdown","25b4ce5e":"markdown","7385b7be":"markdown","29cfb88d":"markdown","be64110c":"markdown","b18fe39b":"markdown"},"source":{"f8e410ef":"!pip install -q efficientnet","3b8a6c03":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn\n\nimport os\nimport matplotlib.pyplot as plt","00a17be0":"\"\"\"\nSettings for training\n\"\"\"\n\n# data splits\nTRAIN_DATA_LIMIT = 50165 #50165  # limit data for faster testing, set to a value >= 50165 to include all train data (in particular advised when submitting)\nSUBMISSION_RUN = True  # set to True when wanting to create submission file `submission.csv` as output. This will result in not creating a holdout set or validation set. \nTEST_SIZE = 0.1  # ratio of all train images (with respect to TRAIN_DATA_LIMIT) used for holdout test set for final scoring. Only considered on non-submission run.\nVALIDATION_SIZE = 0.1  # ratio of all train images (with respect to TRAIN_DATA_LIMIT) used for validation set (after possible holdout set), only used on non-submission run\n\n# training params\nSEED=43\nBATCH_SIZE=32\nNUMBER_TRAIN_EPOCHS = 20  # number epochos in model evaluation run (submission_run = False). Note that we also use early stopping in training, so this number of epochs may not be reached\nNUMBER_SUBMISSION_EPOCHS = 8  # Number of epochs to run on a submission run. Value of 8 set here based on exploration of model in version 14 of notebook","2942262d":"# check GPU and limit GPU memory growth\n#gpu = tf.config.list_physical_devices('GPU')\n#print(\"Num GPUs Available: \", len(gpu))\n#if len(gpu) > 0:\n#    tf.config.experimental.set_memory_growth(gpu[0], True)","f65accb2":"data_dir = Path('..\/input\/seti-breakthrough-listen\/')\ntrain_data_dir = data_dir \/ 'train'\ntest_data_dir = data_dir \/ 'test'\n\ntrain_label_file = data_dir \/ 'train_labels.csv'\nsample_file = data_dir \/ 'sample_submission.csv'","6f61397a":"df_labels = pd.read_csv(train_label_file, index_col='id')\ndf_labels","e931fe6c":"def id_to_path(file_id, train=True):\n    data_dir = train_data_dir if train else test_data_dir\n    return data_dir \/ file_id[0] \/ f'{file_id}.npy'\n\n# simple test\nid_to_path(\"00047dfc96a9\")","cef00ac6":"#check class imbalance \ndf_labels.mean()","16fd2bd3":"# shape of a sample\nnp.load(id_to_path(df_labels.iloc[0].name)).shape","8515bd9d":"#function for visualizing a sample\ndef show_cadence(filename, label):\n    \"\"\"\n    taken from https:\/\/www.kaggle.com\/ihelon\/signal-search-exploratory-data-analysis\n    \"\"\"\n    plt.figure(figsize=(16,10))\n    arr = np.load(filename)\n    for i in range(6):\n        plt.subplot(6, 1, i + 1)\n        if i == 0:\n            plt.title(f\"ID: {os.path.basename(filename)} TARGET: {label}\", fontsize=18)\n        plt.imshow(arr[i].astype(float), interpolation='nearest', aspect='auto')\n        plt.text(5, 100, [\"ON\", \"OFF\"][i % 2], bbox={'facecolor': 'white'})\n        plt.xticks([])\n    plt.show()","5252f54f":"# show some example. Will show a random positive example on every execution\nindex, label = df_labels.query(\"target == 1\").sample(1).reset_index().values[0]\nshow_cadence(id_to_path(index), label)","1166f882":"class SETISequence(Sequence):\n    \"\"\"\n    Taken from this nice starter notebook https:\/\/www.kaggle.com\/kenjirokiyono\/seti-simple-code-for-beginners-tensorflow\n    \"\"\"\n    def __init__(self, x_set, y_set=None, batch_size=BATCH_SIZE):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.is_train = False if y_set is None else True\n    \n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_ids = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        if self.y is not None:\n            batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n        \n        # taking channels \n        list_x = [np.load(id_to_path(x, train=self.is_train)) for x in batch_ids]\n        batch_x = np.moveaxis(list_x,1,-1)\n        batch_x = batch_x.astype(\"float\") \/ 255\n        \n        if self.is_train:\n            return batch_x, batch_y\n        else:\n            return batch_x\n        \n# small output test\nSETISequence([\"00047dfc96a9\"], [1], batch_size=2).__getitem__(0)[0].shape","815b9241":"# list of file_ids for training, limited to TRAIN_DATA_LIMIT parameter which allows fast sandboxing\ntrain_ids = df_labels.index.values[:TRAIN_DATA_LIMIT]\ntrain_y = df_labels['target'].values[:TRAIN_DATA_LIMIT]\n\n# we create a holdout set for scoring only when not creating a submission output\nif not SUBMISSION_RUN:\n    print(\"Not a submission run, creating holdout set for scoring...\")\n    train_ids, test_ids, train_y, test_y = train_test_split(train_ids, train_y, test_size=TEST_SIZE, random_state=SEED)","15414070":"# architecture based on https:\/\/www.kaggle.com\/kenjirokiyono\/seti-simple-code-for-beginners-tensorflow\n# some tuning guidelines: https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n\n# vertical flipping images probably is a meaningful augmentation for this dataset\ndata_augmentation = tf.keras.Sequential([\n  L.experimental.preprocessing.RandomFlip(\"vertical\"),\n])\n\nmodel = tf.keras.Sequential([\n        L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,6)),\n        data_augmentation,\n        efn.EfficientNetB1(input_shape=(273, 256, 3), weights='imagenet', include_top=False, drop_connect_rate=0.4),\n        L.GlobalAveragePooling2D(),\n        L.Dense(1, activation='sigmoid')\n        ])\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n              loss='binary_crossentropy', metrics=[keras.metrics.AUC()])","447dad35":"%%time\n\n# we create a validation set for training only when not submitting, so submission is trained on full data (but taking into account TRAIN_DATA_LIMIT!)\nif not SUBMISSION_RUN:\n    train_ids, val_ids, train_y, val_y = train_test_split(train_ids, train_y, test_size=VALIDATION_SIZE, random_state=SEED)\n    val = SETISequence(val_ids, val_y, batch_size=BATCH_SIZE)\n\ntrain = SETISequence(train_ids, train_y, batch_size=BATCH_SIZE)\n\nif not SUBMISSION_RUN:\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=2, restore_best_weights=True, mode='max')\n    history = model.fit(train, validation_data=val, epochs=NUMBER_TRAIN_EPOCHS, callbacks=[callback])\n    \nelse:\n    history = model.fit(train, epochs=NUMBER_SUBMISSION_EPOCHS)","21af8533":"# check training loss\ndef plot_history(h):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n    for key in h:\n        i = 0 if \"loss\" in key else 1\n        ax[i].plot(h[key], marker='o', label=key)\n    \n    for a in ax:\n        a.legend()\n    \n    plt.show()\n\nplot_history(history.history)","fad85f5f":"%%time\n\n\"\"\"\ncreate submisison on submission run, else evaluate on holdout set\n\"\"\"\n\nif SUBMISSION_RUN: # takes about 11minutes\n    df_submission = pd.read_csv(sample_file, index_col='id')\n    submission_ids = df_submission.index.values\n    submission_pipe = SETISequence(submission_ids, batch_size=BATCH_SIZE)\n    df_submission['target'] = model.predict(submission_pipe).flatten()\n\n    df_submission.to_csv(\"submission.csv\")\n    display(pd.read_csv(\"submission.csv\"))\n    \nelse:\n    test = SETISequence(test_ids, test_y, batch_size=BATCH_SIZE)\n    test_prediction = model.predict(test).flatten()\n    \n    print(f\"\"\"\n    AUC score: {\n    roc_auc_score(\n        y_true=df_labels.loc[test_ids].values.reshape(-1),\n        y_score=test_prediction\n    )}\n    \"\"\")","fedb62a9":"## Libs","25b4ce5e":"## Model\n\n[arXiv : Efficientnet](https:\/\/arxiv.org\/abs\/1905.11946)","7385b7be":"## Prepare data sources","29cfb88d":"## Input Pipeline","be64110c":"## SETI Breakthrough Listen - Single Tensorflow Model Efficient Net\n\nGetting a good score with a single model. Code for basic model evaluation and submission.\n\nVersion for training score: https:\/\/www.kaggle.com\/wspinkaggle\/seti-basic-tensorflow-efficientnet?scriptVersionId=64315935","b18fe39b":"## Basic data overview"}}