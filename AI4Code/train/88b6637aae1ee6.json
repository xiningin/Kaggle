{"cell_type":{"debe76cd":"code","e07ed89b":"code","c73f85a4":"code","3922a598":"code","3ae53a20":"code","9071c773":"code","badce13c":"code","934398a1":"code","c87e4ad3":"code","47ab42e2":"code","e27c0c95":"code","37fe621f":"code","c45a70bf":"code","7814ef68":"code","a0aedfb2":"markdown","d95d018a":"markdown","5bf91564":"markdown","9cf4e295":"markdown","3e46bff6":"markdown","5b024e55":"markdown","d29e738d":"markdown","00e26540":"markdown","28990c4d":"markdown","4a7e96c7":"markdown","086701d8":"markdown","cf3271af":"markdown","e04b0db8":"markdown"},"source":{"debe76cd":"!git clone https:\/\/github.com\/ultralytics\/yolov5\n!mv yolov5\/* .\/","e07ed89b":"!python -m pip install --upgrade pip\n!pip install -r requirements.txt","c73f85a4":"# Download YoloV5 pretrained models\n#!weights\/download_weights.sh","3922a598":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","3ae53a20":"df = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf['x_center'] = df['x'] + df['w']\/2\ndf['y_center'] = df['y'] + df['h']\/2\ndf['classes'] = 0\nfrom tqdm.auto import tqdm\nimport shutil as sh\ndf = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]","9071c773":"df.head()","badce13c":"index = list(set(df.image_id))","934398a1":"source = 'train'\nif True:\n    for fold in [0]:\n        val_index = index[len(index)*fold\/\/5:len(index)*(fold+1)\/\/5]\n        for name,mini in tqdm(df.groupby('image_id')):\n            if name in val_index:\n                path2save = 'val2017\/'\n            else:\n                path2save = 'train2017\/'\n            if not os.path.exists('convertor\/fold{}\/labels\/'.format(fold)+path2save):\n                os.makedirs('convertor\/fold{}\/labels\/'.format(fold)+path2save)\n            with open('convertor\/fold{}\/labels\/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                row = row\/1024\n                row = row.astype(str)\n                for j in range(len(row)):\n                    text = ' '.join(row[j])\n                    f.write(text)\n                    f.write(\"\\n\")\n            if not os.path.exists('convertor\/fold{}\/images\/{}'.format(fold,path2save)):\n                os.makedirs('convertor\/fold{}\/images\/{}'.format(fold,path2save))\n            sh.copy(\"..\/input\/global-wheat-detection\/{}\/{}.jpg\".format(source,name),'convertor\/fold{}\/images\/{}\/{}.jpg'.format(fold,path2save,name))","c87e4ad3":"# Train Model\n!python train.py --img 1024 --batch 2 --epochs 10 --data ..\/input\/configyolo5\/wheat0.yaml --cfg models\/yolov5x.yaml --weight \"\"","47ab42e2":"# copy saved model to weights folder\n!cp runs\/exp0\/weights\/best.pt weights","e27c0c95":"# remove convertor of training data\n!rm -rf convertor","37fe621f":"# Detect Test Images\n!python detect.py --source '..\/input\/global-wheat-detection\/test\/' --weight weights\/best.pt --output 'inference\/output' ","c45a70bf":"!ls -l inference\/output","7814ef68":"from IPython.display import Image, clear_output  # to display images\nImage(filename='inference\/output\/2fd875eaa.jpg', width=600)","a0aedfb2":"## train from scratch don't need the pretrained models","d95d018a":"## Train Model","5bf91564":"train 1 epoch takes ~15 mins using GPU","9cf4e295":"## Prepare Data","3e46bff6":"### convert Train Label","5b024e55":"10 test images in '..\/input\/global-wheat-detection\/test\/'","d29e738d":"output images in 'inference\/output'","00e26540":"## Competition Dataset : [Global Wheat Detection](https:\/\/www.kaggle.com\/c\/global-wheat-detection)\n## Config: [configyolo5](https:\/\/www.kaggle.com\/orkatz2\/configyolo5) ","28990c4d":"# YoloV5 for Global Wheat Detection","4a7e96c7":"## Repro [YoloV5](https:\/\/www.kaggle.com\/ultralytics\/yolov5)","086701d8":"### Display Output Images","cf3271af":"model saved at runs\/exp0\/weights\/best.pt","e04b0db8":"## Test Model"}}