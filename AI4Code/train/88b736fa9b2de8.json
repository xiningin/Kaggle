{"cell_type":{"c2dc75fd":"code","7eb745ad":"code","8473bea4":"code","43824935":"code","290646c7":"code","a4991042":"code","d9c369d6":"code","d35f9345":"code","b8df2dba":"code","a00f4bb8":"code","da16ce95":"code","38b29400":"code","f3e40869":"code","693d552d":"code","8a2a8912":"code","9998b278":"code","f0298c4c":"code","b4582166":"code","b0d7a55b":"code","303577cd":"code","17775015":"code","c4cca118":"code","f5de6f1e":"code","10138e8a":"code","58997361":"code","15534e3f":"code","28395fb3":"code","6a0ad7e4":"code","df86d968":"code","99282b32":"code","092799a9":"code","d12edad8":"code","5cae4592":"code","8a7eac03":"code","a4948c68":"code","9372dc51":"markdown","38474779":"markdown","656f608d":"markdown","f060d26c":"markdown","59275b32":"markdown","3a0866e4":"markdown","e2804bb9":"markdown","105c2554":"markdown","2aad0b41":"markdown","42e26daa":"markdown"},"source":{"c2dc75fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7eb745ad":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import mean_absolute_error\n\n# For future use - to use CNN to extract features instead pf simple min\/max\/etc..\nfrom keras.layers import * \nfrom keras.models import Model, Sequential, load_model\nfrom keras import backend as K \nfrom keras import optimizers \nfrom keras.callbacks import * \nfrom keras.backend import clear_session","8473bea4":"rows = 150_000\nsegment = 0\nfor chunk in tqdm(pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float16}, chunksize=rows * 100)) :\n    if (segment == 0) :\n        X_train = np.int16(chunk.acoustic_data.values)\n        y_raw = np.float16(chunk.time_to_failure.values)\n    else: \n        X_train = np.concatenate((X_train, np.int16(chunk.acoustic_data.values)))\n        y_raw = np.concatenate((y_raw, np.float16(chunk.time_to_failure.values)))\n    segment += 1\nsegments = (segment - 1) * 100","43824935":"# plot the head of the seismic acoustic data\nplt.plot(X_train[0:150000])","290646c7":"X_train = np.reshape(X_train[0:(segments * rows)], (segments, rows,1))\nX_train.shape","a4991042":"# Sanity check - plot the head of the seismic acoustic data\n# plt.plot(X_train[0,:])","d9c369d6":"y_train = np.zeros(segments)\nfor segment in tqdm(range(segments)):\n    seg = y_raw[segment*rows:segment*rows+rows]\n    y_train[segment] = seg[-1]\nmedian = np.median(y_train)\noverall_max = y_train.max()\nprint (\"time to failure median\", median, \"and maximum\", overall_max)","d35f9345":"# add the \"pull alarm\" category\ny_pull_alarm = np.zeros(segments)\nfor segment in tqdm(range(segments)):\n    y = y_train[segment]\n    if (y <= median) :\n        y_pull_alarm[segment] = 1\n    else :\n        y_pull_alarm[segment] = 0\nprint(\"Only pull the alarm if the time to failure is less than or equal to\", median)","b8df2dba":"print(\"The original target data in BLUE is plotted alongside the newly added pull_alarm decision data in ORANGE.\")\nplt.plot(y_train)\nplt.plot(y_pull_alarm)","a00f4bb8":"# Clean up unused RAM \ndel y_raw\ndel chunk","da16ce95":"# Make a CNN Model to learn features\ndef make_model(linear = True) :\n    \n    scale = 1\n    droput = 0.1\n\n    # use a simple sequential convolutional neural network model\n    model2 = Sequential()\n    model2.add(Conv1D(int(12 * scale), 3, strides = 1, activation='relu', input_shape=(rows,1)))\n    model2.add(MaxPooling1D(pool_size = 10))\n    model2.add(BatchNormalization())\n    model2.add(Dropout(droput))\n\n    model2.add(Conv1D(int(100 * scale), 3, strides = 1, activation='relu'))\n    model2.add(Dropout(droput))\n    model2.add(MaxPooling1D(pool_size = 2))\n\n    model2.add(Conv1D(int(200 * scale), 3, strides = 1, activation='relu'))\n    model2.add(Dropout(droput))\n    model2.add(MaxPooling1D(pool_size = 2))\n\n    model2.add(Conv1D(int(300 * scale), 3, strides = 1, activation='relu'))\n    model2.add(Dropout(droput))\n    model2.add(MaxPooling1D(pool_size = 2))\n\n    model2.add(Conv1D(int(200 * scale), 3, strides = 1, activation='relu'))\n    model2.add(Dropout(droput))\n    model2.add(MaxPooling1D(pool_size = 2))\n\n    model2.add(Conv1D(int(100 * scale), 3, strides = 1, activation='relu'))\n    model2.add(Dropout(droput))\n    model2.add(MaxPooling1D(pool_size = 2))\n\n    # Include a couple of dense layers in case the classes are not linearly seperable by this point\n    model2.add(Flatten())\n    model2.add(Dropout(droput))\n\n    model2.add(Dense(int(100 * scale), activation='relu'))\n    model2.add(Dropout(droput))\n\n    model2.add(Dense(int(100 * scale), activation='relu'))\n    model2.add(Dropout(droput))\n\n    if (linear == True) : \n        model2.add(Dense(1, activation = 'linear'))\n    else :\n        model2.add(Dense(1, activation = 'sigmoid'))\n       \n    \n    # binary_crossentropy is used as the error function when we have only two choices, like \"pull_alarm\"\n    if (linear == True) : \n        model2.compile(loss='mse', optimizer='adam')\n    else :\n        # by all rights, should probably use binary_crossentropy below, but seems to get stuck if I do (mse works). Will investigate further at some other time.\n        model2.compile(loss='mse', optimizer='adam')\n        \n    return model2","38b29400":"# we will be slowing down the learning using \"Dropouts\" (see above) so the patience needed to exit local minima can be large\npatience = 15\n# Probably will never reach this many epochs, but want to use a number larger than what we expect\nepochs = 300\n# Divide the data into 15 different versions of training\/validation\n# n_fold = 15\n# Using KFold instead of StratifiedKFold becuase there is a low degree of confidence that the test classification distributions \n# or more importantly, the real world classification probabilities, are equal to those found in the training set\n# folds = KFold(n_splits=n_fold, shuffle=False, random_state=1234)\n\nsam = X_train.shape[0]\ncol = X_train.shape[1]\n\n# sam_test = X_test.shape[0]\n# col_test = X_test.shape[1]\n\n# prediction = np.zeros((sam_test, y_count))\n\nmodel = make_model()\n\ncheckpointer = ModelCheckpoint('LANL_value', verbose=1, save_best_only=True)\n\nearlystopper = EarlyStopping(patience = patience, verbose=1) \n\nresults = model.fit(X_train, y_train, epochs = epochs, batch_size = 32,\n                    callbacks=[earlystopper, checkpointer], validation_split=0.2)\n\nmodel = load_model('LANL_value')","f3e40869":"print(model.summary())","693d552d":"# SCORE FOR SCALED TRAINING SET\ny_pred = model.predict(X_train)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_train, y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","8a2a8912":"score = mean_absolute_error(y_train, y_pred)\nprint(f'Score: {score:0.3f}')","9998b278":"# Create a version of the scaled training set that has 20% additive gaussian white noise slapped on top.\nX_train_random = X_train + (np.random.rand(X_train.shape[0],X_train.shape[1],1) * 0.2)","f0298c4c":"# SCORE FOR RANDOMIZED TRAINING SET\ny_pred = model.predict(X_train_random)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_train, y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","b4582166":"score = mean_absolute_error(y_train, y_pred)\nprint(f'Score: {score:0.3f}')","b0d7a55b":"del X_train_random","303577cd":"# we will be slowing down the learning using \"Dropouts\" (see above) so the patience needed to exit local minima can be large\npatience = 15\n# Probably will never reach this many epochs, but want to use a number larger than what we expect\nepochs = 300\n\nmodel_pull_alarm = make_model(linear = False)\n\ncheckpointer = ModelCheckpoint('LANL_value_pull_alarm', verbose=1, save_best_only=True)\n\nearlystopper = EarlyStopping(patience = patience, verbose=1) \n\nresults = model_pull_alarm.fit(X_train, y_pull_alarm, epochs = epochs, batch_size = 32,\n                    callbacks=[earlystopper, checkpointer], validation_split=0.2)\n\nmodel_pull_alarm = load_model('LANL_value_pull_alarm')","17775015":"print(model_pull_alarm.summary())","c4cca118":"# SCORE \"PULL_ALARM\" VALUE FOR SCALED TRAINING SET\ny_pred = model_pull_alarm.predict(X_train)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_pull_alarm, y_pred)\nplt.xlim(-.1, 1.1)\nplt.ylim(-2, 2)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","f5de6f1e":"score = mean_absolute_error(y_pull_alarm, y_pred)\nprint(f'Score: {score:0.3f}')","10138e8a":"# spread out the results to the desired range of 0 to 1\nymin = y_pred.min()\nymax = y_pred.max()\ny_pred = ((y_pred - ymin) \/ (ymax - ymin))\n\n# Categorize the time remaining as being large (if we are not pulling the alarm) or small (if we are)\ny_pred = overall_max * (1 - y_pred)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(y_train, y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","58997361":"score = mean_absolute_error(y_train, y_pred)\nprint(f'Score: {score:0.3f}')","15534e3f":"y_pred_pull_alarm = y_pred\n\ny_pred = model.predict(X_train)\n# Now combine it with our original time to fault estimates\ny_pred = (y_pred_pull_alarm + y_pred) \/ 2","28395fb3":"plt.figure(figsize=(6, 6))\nplt.scatter(y_train, y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","6a0ad7e4":"score = mean_absolute_error(y_train, y_pred)\nprint(f'Score: {score:0.3f}')","df86d968":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')","99282b32":"del X_train\ndel y_train\ndel y_pred","092799a9":"rows = 150_000\nsegment = 0\nfor seg_id in tqdm(submission.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    if (segment == 0) :\n        X_test = np.int16(seg.acoustic_data.values)\n    else: \n        X_test = np.concatenate((X_test, np.int16(seg.acoustic_data.values)))\n    segment += 1\nsegments = segment","d12edad8":"# plot the head of the seismic acoustic data\nplt.plot(X_test[0:150000])","5cae4592":"X_test = np.reshape(X_test, (segments, rows,1))\nX_test.shape","8a7eac03":"# Sanity check - plot the head of the seismic acoustic data\nplt.plot(X_test[0,:])","a4948c68":"print(\"saving PULL_ALARM estimates, scaled to time\")\ny_pred = model_pull_alarm.predict(X_test)\n# spread out the results to the desired range of 0 to 1\nymin = y_pred.min()\nymax = y_pred.max()\ny_pred = ((y_pred - ymin) \/ (ymax - ymin))\n# Categorize the time remaining as being large (if we are not pulling the alarm) or small (if we are)\ny_pred = overall_max * (1 - y_pred)\nsubmission['time_to_failure'] = y_pred\nsubmission.to_csv('submission_pull_alarm.csv')\n\nprint(\"saving TIME_TO_FAILURE estimates\")\ny_pred_pull_alarm = y_pred\ny_pred = model.predict(X_test)\nsubmission['time_to_failure'] = y_pred\nsubmission.to_csv('submission_time_to_failure.csv')\n\nprint(\"saving average of the above estimates\")\n# Now combine it with our original time to fault estimates\ny_pred = (y_pred_pull_alarm + y_pred) \/ 2\nsubmission['time_to_failure'] = y_pred\nsubmission.to_csv('submission.csv')\n\nprint(\"done!\")","9372dc51":"**Using good machine learning practices learned from medicine.**\n\nIn the aforementioned FDA document, a number of steps and documentation and procedural aspects are discussed. These are definitely worth reading in detail - and if you are familiar with Kaggle competitions, you'll find much of this familiar, but below is a biref summary..\n\n**For the initially created SaMD, the steps include:**\n* Data selection and management\n* Model training and tuning\n* Performance and Clinical model validation\n* Premarket testing\n\n**For modfications (which should be planned for) the steps include:**\n* Data for re-training\n* All of the usual steps abve, with the addition of SaMD Pre-specifications and Algorithm Change Protocol\n\n**Documentation should include:**\n1. Establish clear expectations on quality systems and good ML practices (GMLP);\n2. Conduct premarket review for those SaMD that require premarket submission to demonstrate reasonable assurance of safety and effectiveness and establish clear expectations for manufacturers of AI\/ML-based SaMD to continually manage patient risks throughout the lifecycle;\n3. Expect manufacturers to monitor the AI\/ML device and incorporate a risk management approach and other approaches utlined in \u201cDeciding When to Submit a 510(k) for a Software Change to an Existing Device\u201d Guidance18 in development, validation, and execution of he algorithm changes (SaMD Pre-Specifications and Algorithm Change Protocol); and\n4. Enable increased transparency to users and FDA using postmarket real-world performance reporting for maintaining continued assurance of safety and effectiveness.\n\n**What does this mean for the software?**\n\nAs a Kaggle competition entrant, mostly just the documentation of the above will be sufficient - while adherence to Good Machine Learning Practices (GMLP) should be something you are doing anyway. In the case of this competition, the rules for submissions have already been set - whereas the rules for submission of medical trial documentation will have its own (lengthly) rules which must be adhered to.","38474779":"Forked from Basic Feature Benchmark, courtesy inversion (https:\/\/www.kaggle.com\/inversion\/basic-feature-benchmark)","656f608d":"**Making plans for future modifications.**\n\nAs per the FDA document relating to Medical Devices, modifications are divided into three broad categories as follows:\n* Performance - Example: re-training with new data sets within the intended use population from the same type of input signal. \n* Inputs - Example: modifications to inputs and\/or algorithms with no change in intended use.\n* Intended Use - Example: Change from a laboratory prediction to a real-world use and\/or change from informing to driving or taking action.\n\nIt is clear that if this Kernel is to ever be put into use serving the population living in earthquake areas, there will be a change of intended use. In addition, a change to improve performance as well as inputs will likely also occur as part of this.\n\n**Additional Details**\n\nWhen it comes to all changes, we want to make sure that the device is accurate, reliable, precise, and achieves the intended purpose. Because of this, we need to understand the meaning of these terms and relate them to our application.\n\nAccuracy and Precision: Accuracy refers to the closeness of a measured value to a standard or known value. This competition uses Mean Absolute Error to measure accuracy, and this is a valid measure because it is of the same scale as the Time_to_Failure. Precision, on the other hand, refers to the closeness of two or more measurements to each other. We may want to add training samples that are include additive gaussian white noise (AGWN) to see that small changes to the measurements do not yield large changes to the output (especially with regard ot pulling the alarm).\n\nSometimes precision is equated to \"reliability\", although the latter is a measure of how dependably an observation is exactly the same when repeated. It could be over several earthquakes, or over several sensors measuring the same earthquake.\n\nSometimes accuract is equated to \"achieving the intended purpose\", although the latter assumes that indeed we have chosen the correct training set that allows us to generalize to the real world population of earthquakes.\n\n**What does that mean to the software?***\n\nI'm still trying figure out what \"planning for future modifications\" might mean. Below I am trying out one idea.\n\nIn all of the above situations, it means that we must use \"a priori\" methods, such as the \"scaler parameters\" calculations below, on the original training data, and then AFTERWARDS make additions to our training set (adding noise, additional sensor measurements, etc.) otherwise we will not really be training to improve accuracy and precision.\n\nSo for this reason, we create and scale the training data set, and then afterwards add some AGWN for additional sanity check to see if our precision and accuracy are greatly affected.\n\nOther ideas?","f060d26c":"** Now Back to an alternative Purpose **\n\nWhat if instead of training to find out how much time there is, we instear train to find out if we should pull the alarm or not...","59275b32":"First, some Kernel Basics...","3a0866e4":"**HOW CAN WE TAKE LEARNINGS FROM THE ABOVE AN APPLY THEM TO LANL EARTHQUAKE PREDICTION?**\n\nThere are three main areas where I thought it might be useful to implement \"medical style\" methods to this competition - but I definitely welcome your feedback and further brainstorming.\n\n1. Being clear about how we categorize the device and its intended use.\n1. Making plans for future modifications.\n1. Using good machine learning practices learned from medicine.","e2804bb9":"**What can we learn from Medical warning systems to help improve this Earthquake warning system?**\n\nAs a potential lifesaving warning device, LANL Earthquake Prediction has much in common with other lifesaving warning devices, most notably medical devices. There is additional commonality, in that many data scientists and researchers work on several machine learning areas, so I expect participants analyzing geophysical data in this competition may also be interested in biomedical signal analysis. Because of this, I wanted to share the very latest \"Proposed Regulatory Framework for Modifications to Artificial Intelligence\/Machine Learning (AI\/ML) \u2013 Based Software as a Medical Device (SaMD)\" published earier this month by the United States Food and Drug Administration (FDA).\n\nThe FDA Document helps us categorize the system, describe ongoing changes to the system, and discusses good machine learning practices.\n\nI will break down the components of the proposed regulatory framework, and use this example Kernel to demonstrate how it might impact your future work as a researcher, programmer, and data scientist.","105c2554":"I will first give a **summary of the medical device document**, its pertinent details, and invitation for feedback. I will then **use this Kernel to demonstrate** the similarities between medical warning devices with LANL Earthquake Prediciton, and what learnings from the former might inform our creation of the latter. \n\n**WHAT IS IN THE *MEDICAL DEVICE* DOCUMENT?**\n\nOn April 2, 2019, the US FDA posted the **\"Proposed Regulatory Framework for Modifications to Artificial Intelligence\/Machine Learning (AI\/ML) \u2013 Based Software as a Medical Device (SaMD)\"** document, found here (https:\/\/www.regulations.gov\/document?D=FDA-2019-N-1185-0001)\n\n**EXECUTIVE SUMMARY**\n\nThe framework first categorizes the \u201crisk\u201d of the AI\/ML-based SaMD as being levels, I, II, III, or IV, from lowest to highest, based on a combination of how critical the healthcare situation is, and also how significant the SaMD classification will be to the final healthcare decision. The framework goes on from there to categorize the three (3) \u201ctypes of modifications\u201d which can help developers decide when a formal (510[k]) review is required. The three types are i) Performance, ii) Inputs, and iii) Intended use.\n\n**PERTINENT DETAILS**\n\nThe proposal has not yet been made into regulation, and so may change substantially or be withdrawn, but some aspects of the current proposal are pertinent to those working in the field. These details include:\n\n* SaMD\u2019s are subdivided into three broad classes based on how significant the SaMD classification will be to the final healthcare decision. These three are \u201cInform clinical management,\u201d \u201cDrive clinical management,\u201d and \u201cTreat or diagnose.\u201d These classes drive the clinical risk (from I to IV), further modified by the state of the healthcare situation or condition (Critical, Serious, or Non-serious). \n* Modifications are divided into three broad categories as follows:\n* Performance: Modifications related to performance, with no change to the intended use or new input type. This may include re-training with new data sets within the intended use population from the same type of input signal. As a \u201clitmus test\u201d this type of change will NOT change any of the explicit use claims about the product.\n* Inputs: Modifications to inputs, with no change in intended use. These changes may also involve changes to the algorithm for use with new types of signals, but do not change the product use claims. As examples, the document cites \u201cmodification to support compatibility with CT scanners from additional manufacturers,\u201d or an atrial fibrillation diagnoses system that now will \u201cinclude oximetry data in addition to heart rate data.\u201d\n\n* Intended Use: These types of modifications include those that result in a change in the significance of information provided by the SaMD and\/or result in a change to the healthcare situation or condition explicitly claimed by the manufacturer. Examples include an expanded patient population, such as the inclusion of pediatric population when the original SaMD was initially intended for adults, or an expanded number of diseases or conditions, such as for lesion detection from one type of cancer to another.\n* Finally, the proposal also examines the total product lifecycle, with an understanding that AI\/ML-based technologies have the potential to transform healthcare by deriving new and important insights from the vast amount of data generated during the delivery of healthcare every day.\n\n**INVITATION FOR FEEDBACK BY THE US FDA**\n\nAlthough the entire document is open so as to initiate discussion and for the whole world to provide feedback, there are two major areas that seem to stand out as needing expert advice. The first is the \u201cGMLP\u201d or \u201cGood Machine Learning Practices\u201d which, if followed, can insure that the SaMD is accurate, reliable, precise, and achieves the intended purpose. The second is the establishment of the SPS and ACP filings. The \u201cSPS\u201d, or SaMD Pre-Specification describes the anticipated modifications. The \u201cACP\u201d, or Algorithm Change Protocol, covers the steps used to control risks of violating GMLP.","2aad0b41":"**As noted above...***\n\nI'm still trying figure out what \"planning for future modifications\" might mean. Below I am trying out one idea.\n\nIn all of the above situations, it means that we must use \"a priori\" methods, such as the \"scaler parameters\" calculations below, on the original training data, and then AFTERWARDS make additions to our training set (adding noise, additional sensor measurements, etc.) otherwise we will not really be training to improve accuracy and precision.\n\nSo for this reason, we create and scale the training data set, and then afterwards add some additive gaussian white noise (AGWN) for additional sanity check to see if our precision and accuracy are greatly affected.\n\nOther ideas?","42e26daa":"**Being clear about how we categorize the device and its intended use.**\n\nFrom a medical perspective, the FDA thinks of these deivces ase being used for one of three (3) major purposes.\n\nThe major purposes are \u201cInform clinical management,\u201d \u201cDrive clinical management,\u201d and \u201cTreat or diagnose.\u201d \n\n**How can we relate this to LANL Earthquake prediction?**\n\n1. If this Kernel is to merely \"inform\" then perhaps data will be recorded, or indicators may be used.\n1. If this Kernel must \"drive\" then perhaps an earthquake alarm will sound, and people who have practiced earthquake drills will take action\n1. If this Kernel will \"treat\" or take action, then perhaps the result will be an automatic shutdown of an assembly line, or other costly event that will be worth the cost if there is indeed an earthquake.\n\n**What does that mean to the software?***\n\nThe current contest accounts for major purpose 1 above, with a range of approximately 0 to 16 time to failure. If major purpose 2 or 3 are needed, then perhaps we have to be more certain of not having too many false alarms, and we may want to have a different target \"y\" that is not a numerical \"time till earthquake\" but rather a binary choice. Using the data provided in this contest, perhaps we only \"pull the alarm\" if the earthquake is at some median value."}}