{"cell_type":{"f7dd3a96":"code","3f169abc":"code","d39b434c":"code","5f4c591b":"code","64cbd68b":"code","35b8302c":"code","36c1b95b":"code","550db97b":"code","7f9b033a":"code","d142090c":"code","e192b476":"code","5177a85d":"code","957e3dd8":"code","86d666ec":"code","508a02fb":"code","830720a5":"code","16894311":"code","681815cf":"code","3dfe4b61":"code","36fc860b":"code","aacad561":"code","8f962686":"code","8eecd044":"code","cda947e4":"code","4358f456":"code","75ccd90a":"code","1251813f":"code","50cf28c4":"code","14c1e29f":"code","c90d394f":"code","4dbc6ddf":"code","ba3fcc57":"code","1c69b580":"code","1d407bad":"code","0fbd511b":"code","6f1ccfb1":"code","3940f8e1":"code","642ad3e6":"code","242026a0":"code","a8a379d2":"code","ce2739bd":"code","80bcfb03":"code","58f274cb":"code","f522841c":"code","53bfb5c7":"code","1e002e98":"code","344b148d":"code","36b820a6":"code","a3691856":"code","ff7cce2f":"code","f8776ecf":"code","b43addbd":"code","9dd8c97f":"markdown","9a1d74d2":"markdown","a2faf91a":"markdown","da776b22":"markdown","d871352f":"markdown","e0817777":"markdown","d925e33b":"markdown","4ecbe59a":"markdown","b0e55965":"markdown","0c3c7821":"markdown","e6757781":"markdown","97693375":"markdown","383e795f":"markdown","14f57707":"markdown","7471f980":"markdown","2cbe86ae":"markdown","09148d66":"markdown","a5e27131":"markdown","15201d8e":"markdown","3db5167b":"markdown","3fa5388e":"markdown","a9411d9b":"markdown","1ec0566d":"markdown","2ccc71d3":"markdown"},"source":{"f7dd3a96":"%matplotlib inline\n\n# plotting\nimport matplotlib as mpl\n#mpl.style.use('ggplot')\nimport matplotlib.pyplot as plt\n\n# math and data manipulation\nimport numpy as np\nimport pandas as pd\n\n# to handle paths\nfrom pathlib import Path\n\n# set random seeds \nfrom numpy.random import seed\n#from tensorflow import set_random_seed \nimport tensorflow as tf\n\nRANDOM_SEED = 2018\n#seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n#set_random_seed(RANDOM_SEED) #generate random number","3f169abc":"import os\n\n\"\"\"Check the current working directory and creating a path to the data folder\n\"\"\"\nprint(Path.cwd())\nparent_dir = os.chdir('\/kaggle\/')\nprint(os.listdir(\"..\/kaggle\/input\"))\ndata_dir = Path.cwd() \/'input\/cold-start'\n","d39b434c":"consumption_train = pd.read_csv(data_dir \/ 'train.csv', \n                                sep=';',index_col=0, parse_dates=['timestamp'])\nconsumption_train.head()","5f4c591b":"'''Reset series_ids from col index to normal colums'''\nconsumption_train.reset_index(level=0, inplace=True) \nprint(consumption_train)","64cbd68b":"def describe_training_data(train_df):\n    num_training_series = train_df.series_id.nunique()\n    num_training_days = num_training_series * 28\n    num_training_hours = num_training_days * 24\n    assert num_training_hours == train_df.shape[0]\n    \n    desc = f'There are {num_training_series} training ' \\\n           f'series totaling {num_training_days} days ' \\\n           f'({num_training_hours} hours) of consumption data.'\n    \n    print(desc)\n    \ndescribe_training_data(consumption_train)","35b8302c":"# choose subset of series for training\nfrac_series_to_use = 0.01\n\nrng = np.random.RandomState(seed=RANDOM_SEED)\nseries_ids = consumption_train.series_id.unique()\nseries_mask = rng.binomial(1,\n                           frac_series_to_use,\n                           size=series_ids.shape).astype(bool)\n\ntraining_series = series_ids[series_mask]\n\n# reduce training data to series subset\nconsumption_train = consumption_train[consumption_train.series_id.isin(training_series)]\n\n# describe the reduced set\ndescribe_training_data(consumption_train)","36c1b95b":"cold_start_test = pd.read_csv(data_dir \/ 'cold-start-test.csv', \n                              sep=';',index_col=0, parse_dates=['timestamp'])\ncold_start_test.head()","550db97b":"'''reset series_id from index to normal colum'''\ncold_start_test.reset_index(level=0, inplace=True) \nprint(cold_start_test)","7f9b033a":"'''Load the submission_format file '''\nsubmission_format = pd.read_csv(data_dir \/ 'submission-format.csv',\n                                sep=';',index_col='pred_id',parse_dates=['timestamp'])\nsubmission_format.head()","d142090c":"#submission_format=submission_format.sort_index(ascending=True)","e192b476":"'''reset pre_id from col index to normal colums'''\nsubmission_format.reset_index(level=0, inplace=True)","5177a85d":"# confirm that every series asks for only one type of prediction\nassert all(1 == submission_format.groupby('series_id').prediction_window.nunique())\n\n# use the first() prediction window value from a series_id so as not to overcount\nsubmission_format.groupby('series_id').prediction_window.first().value_counts()","957e3dd8":"ax = (cold_start_test.groupby('series_id').count()\n                                          .timestamp\n                                          .divide(24)\n                                          .value_counts()\n                                          .sort_index()\n                                          .plot.bar(color=['red', 'blue', 'purple', 'grey', 'yellow', 'green', 'pink']))\nax.set_xlabel('Days in Cold Start Data')\nax.set_ylabel('Number of Test Series')\nplt.show()","86d666ec":"'''pred_windows    = submission_format[['series_id', 'prediction_window']].drop_duplicates()\nprint(pred_windows[0:1])\ncold_start_test_1 = cold_start_test.merge(pred_windows, on='series_id')\nprint(cold_start_test_1[0:1])\n'''","508a02fb":"# add prediction_window to the test data\npred_windows    = submission_format[['series_id', 'prediction_window']].drop_duplicates()\ncold_start_test = cold_start_test.merge(pred_windows, on='series_id')\n\nnum_cold_start_days_provided = (cold_start_test.groupby('series_id')\n                                               .prediction_window\n                                               .value_counts()\n                                               .divide(24))\n\nnum_cold_start_days_provided.head()","830720a5":"def _count_cold_start_days(subdf):\n    \"\"\" Get the number of times a certain cold-start period\n        appears in the data.\n    \"\"\"\n    return (subdf.series_id\n                 .value_counts()\n                 .divide(24)  # hours to days \n                 .value_counts())\n\ncold_start_occurrence = (cold_start_test.groupby('prediction_window')\n                                        .apply(_count_cold_start_days))\n\ncold_start_occurrence.head()","16894311":"ax = cold_start_occurrence.unstack(0).plot.bar(figsize=(10, 6),\n                                               rot=0)\n\nax.set_xlabel('Number of Cold-start Days')\nax.set_ylabel('Number of Series in Dataset');","681815cf":"# visualize the cold start and prediction windows\nnum_to_plot = 40\n\nfig, ax = plt.subplots(figsize=(45, 15))\n\ncold_start_map = {'hourly': 1, 'daily': 7, 'weekly': 14}\n\nrng = np.random.RandomState(seed=5425)    # create random number\n\n# generate random unique number from cold start of size 40(num_to_plot = 40)\nseries_to_plot = rng.choice(cold_start_test.series_id.unique(), num_to_plot) \nfor i, series_id in enumerate(series_to_plot):\n    # get relevant information about the test series from coldstart data\n    series_data = cold_start_test[cold_start_test.series_id == series_id]\n    start_cold = series_data.timestamp.min()\n    start_preds = series_data.timestamp.max()\n    \n    # get prediction stop from submission format\n    stop_preds = submission_format[submission_format.series_id == series_id].timestamp.max()\n    \n    # plot the cold start and prediction window relative sizes\n    ax.plot([start_cold, start_preds], [i, i], c='#6496e5', linewidth=num_to_plot \/ 3)\n    ax.plot([start_preds, stop_preds], [i, i], c='#e25522', linewidth=num_to_plot \/ 3)\n\n# the y tick labels don't mean anything so turn them off\nax.set_yticks([])\nplt.tick_params(labelsize=35)\nplt.legend(['Cold Start (1-14 Days)', \n            'Prediction Window'], \n           fontsize=30)\n\nplt.title(f'{num_to_plot} Series Cold Starts And Their Prediction Windows', fontsize=50);\n","3dfe4b61":"# plot a few consumption patterns\nseries_to_plot = rng.choice(consumption_train.series_id.unique(), 3)\n\nfor ser_id in series_to_plot:\n    ser_data = consumption_train[consumption_train.series_id == ser_id]\n    ax = ser_data.plot(x='timestamp',\n                       y='consumption', \n                       title=f\"series_id {int(ser_id)}\",\n                       legend=False)\n    plt.ylabel('consumption (watt-hours)')\n    plt.show()","36fc860b":"(consumption_train.groupby('series_id')\n                  .timestamp\n                  .apply(lambda x: x.max() - x.min())\n                  .value_counts())","aacad561":"def create_lagged_features(df, lag=1):\n    if not type(df) == pd.DataFrame:\n        df = pd.DataFrame(df, columns=['consumption'])\n    \n    def _rename_lag(ser, j):\n        ser.name = ser.name + f'_{j}'\n        return ser\n        \n    # add a column lagged by `i` steps\n    for i in range(1, lag + 1):\n        df = df.join(df.consumption.shift(i).pipe(_rename_lag, i))\n\n    df.dropna(inplace=True)\n    return df\n\n# example series  (change from == 100283 to != to avoid empty DataFrame)\ntest_series = consumption_train[consumption_train.series_id != 100182]\ncreate_lagged_features(test_series.consumption, lag=3).head()  # lag =3 means 3 previous value\n","8f962686":"from sklearn.preprocessing import MinMaxScaler\n\ndef prepare_training_data(consumption_series, lag):\n    \"\"\" Converts a series of consumption data into a\n        lagged, scaled sample.\n    \"\"\"\n    # scale training data\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    consumption_vals = scaler.fit_transform(consumption_series.values.reshape(-1, 1))\n    \n    # convert consumption series to lagged features\n    consumption_lagged = create_lagged_features(consumption_vals, lag=lag)\n\n    # X, y format taking the first column (original time series) to be the y\n    X = consumption_lagged.drop('consumption', axis=1).values\n    y = consumption_lagged.consumption.values\n    \n    # keras expects 3 dimensional X\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    #X = X.reshape(X.shape[0], 32, X.shape[1])\n\n    return X, y, scaler\n\n_X, _y, scaler = prepare_training_data(test_series.consumption, 5)\nprint(_X.shape)\nprint(_y.shape)\nprint(scaler)","8eecd044":"device_name = tf.test.gpu_device_name()\ntf.test.is_gpu_available(\n    cuda_only=False,\n    min_cuda_compute_capability=None)","cda947e4":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import LambdaCallback, ModelCheckpoint\nimport random\nimport sys\nimport io\nfrom tqdm import tqdm\nimport tensorflow as tf","4358f456":"# lag of 23 to simulate smallest cold start window. Our series\n# will be converted to a num_timesteps x lag size matrix\nlag =  23\nopt = tf.keras.optimizers.Adamax(learning_rate=0.01)  #SGD , RMSprop ,Adam , Adadelta ,Adagrad ,Adamax ,Nadam,Ftrl\n\n# model parameters\nnum_neurons = 100\nbatch_size = 1  # this forces the lstm to step through each time-step one at a time\nbatch_input_shape=(batch_size, 1, lag)\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=num_neurons,return_sequences=True,batch_input_shape=batch_input_shape,stateful=True)) #input_shape=(1, look_back), input_shape=(X_train.shape[1],1)\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.add(tf.keras.layers.LSTM(units=num_neurons,return_sequences=False))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer=opt, loss='mse')\n# This builds the model for the first time:\n#model.fit(X_train, y_train, batch_size=32, epochs=5)\n\n","75ccd90a":"#tf.config.list_physical_devices('GPU')","1251813f":"%%time\n\n#reset the LSTM state for training on each series\nfor ser_id, ser_data in consumption_train.groupby('series_id'):\n\n    temp =ser_data.set_index('timestamp')\n    temp =temp.sort_index(ascending=True)\n    #temp =temp.consumption \n    #temp=temp.drop(['temperature', 'series_id'], axis=1)\n            # prepare the data\n    X, y, scaler = prepare_training_data(temp.consumption, lag)\n                #print(scaler)\n                # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n                # and that we reset states only after an entire X has been fit, instead of after\n                # each (size 1) batch, as is the case when stateful=False\n    model.fit(X, y, epochs=50, batch_size=1)\n    #model.reset_states()\n    model.reset_states()","50cf28c4":"'''Run this step again to try different test set\n\n'''\n\nseries_to_plot = rng.choice(cold_start_test.series_id.unique(), 1)\n\nfor ser_id in series_to_plot:\n    ser_data_test = cold_start_test[cold_start_test.series_id == ser_id]\n    ax = ser_data_test.plot(x='timestamp',\n                       y='consumption', \n                       title=f\"series_id {int(ser_id)}\",\n                       legend=False)\n    plt.ylabel('consumption (watt-hours)')\n    plt.show()","14c1e29f":"print(len(ser_data_test))\nprint(ser_data_test[0:1])","c90d394f":"temp_test =ser_data_test.set_index('timestamp')\ntemp_test =temp_test.sort_index(ascending=True)\n#temp =temp.consumption \ntemp_test=temp_test.drop(['temperature', 'series_id','prediction_window'], axis=1)","4dbc6ddf":"cold_X, cold_y, scaler = prepare_training_data(temp_test, lag)","ba3fcc57":"predict = model.predict(cold_X, batch_size=1)","1c69b580":"plt.figure(figsize=(20, 5))\nplt.plot(cold_y[0:-1])\nplt.plot(predict[1:-1])","1d407bad":"cold_y_inv = scaler.inverse_transform(cold_y.reshape(-1, 1)).ravel()\npredict_inv = scaler.inverse_transform(predict.reshape(-1, 1)).ravel()","0fbd511b":"plt.figure(figsize=(20, 5))\nplt.plot(cold_y_inv[0:-1])\nplt.plot(predict_inv[1:-1])","6f1ccfb1":"'''Define all error metric\n'''\nimport numpy as np\n\nEPSILON = 1e-10\n\n\ndef _error(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Simple error \"\"\"\n    return actual - predicted\n\n\ndef _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Percentage error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return _error(actual, predicted) \/ (actual + EPSILON)\n\n\ndef _naive_forecasting(actual: np.ndarray, seasonality: int = 1):\n    \"\"\" Naive forecasting method which just repeats previous samples \"\"\"\n    return actual[:-seasonality]\n\n\ndef _relative_error(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Relative Error \"\"\"\n    if benchmark is None or isinstance(benchmark, int):\n        # If no benchmark prediction provided - use naive forecasting\n        if not isinstance(benchmark, int):\n            seasonality = 1\n        else:\n            seasonality = benchmark\n        return _error(actual[seasonality:], predicted[seasonality:]) \/\\\n               (_error(actual[seasonality:], _naive_forecasting(actual, seasonality)) + EPSILON)\n\n    return _error(actual, predicted) \/ (_error(actual, benchmark) + EPSILON)\n\n\ndef _bounded_relative_error(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Bounded Relative Error \"\"\"\n    if benchmark is None or isinstance(benchmark, int):\n        # If no benchmark prediction provided - use naive forecasting\n        if not isinstance(benchmark, int):\n            seasonality = 1\n        else:\n            seasonality = benchmark\n\n        abs_err = np.abs(_error(actual[seasonality:], predicted[seasonality:]))\n        abs_err_bench = np.abs(_error(actual[seasonality:], _naive_forecasting(actual, seasonality)))\n    else:\n        abs_err = np.abs(_error(actual, predicted))\n        abs_err_bench = np.abs(_error(actual, benchmark))\n\n    return abs_err \/ (abs_err + abs_err_bench + EPSILON)\n\n\ndef _geometric_mean(a, axis=0, dtype=None):\n    \"\"\" Geometric mean \"\"\"\n    if not isinstance(a, np.ndarray):  # if not an ndarray object attempt to convert it\n        log_a = np.log(np.array(a, dtype=dtype))\n    elif dtype:  # Must change the default dtype allowing array type\n        if isinstance(a, np.ma.MaskedArray):\n            log_a = np.log(np.ma.asarray(a, dtype=dtype))\n        else:\n            log_a = np.log(np.asarray(a, dtype=dtype))\n    else:\n        log_a = np.log(a)\n    return np.exp(log_a.mean(axis=axis))\n\n\ndef mse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Squared Error \"\"\"\n    return np.mean(np.square(_error(actual, predicted)))\n\n\ndef rmse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Root Mean Squared Error \"\"\"\n    return np.sqrt(mse(actual, predicted))\n\n\ndef nrmse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Root Mean Squared Error \"\"\"\n    return rmse(actual, predicted) \/ (actual.max() - actual.min())\n\n\ndef me(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Error \"\"\"\n    return np.mean(_error(actual, predicted))\n\n\ndef mae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Absolute Error \"\"\"\n    return np.mean(np.abs(_error(actual, predicted)))\n\n\nmad = mae  # Mean Absolute Deviation (it is the same as MAE)\n\n\ndef gmae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Geometric Mean Absolute Error \"\"\"\n    return _geometric_mean(np.abs(_error(actual, predicted)))\n\n\ndef mdae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Median Absolute Error \"\"\"\n    return np.median(np.abs(_error(actual, predicted)))\n\n\ndef mpe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Percentage Error \"\"\"\n    return np.mean(_percentage_error(actual, predicted))\n\n\ndef mape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Mean Absolute Percentage Error\n\n    Properties:\n        + Easy to interpret\n        + Scale independent\n        - Biased, not symmetric\n        - Undefined when actual[t] == 0\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(np.abs(_percentage_error(actual, predicted)))\n\n\ndef mdape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Median Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.median(np.abs(_percentage_error(actual, predicted)))\n\n\ndef smape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Symmetric Mean Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(2.0 * np.abs(actual - predicted) \/ ((np.abs(actual) + np.abs(predicted)) + EPSILON))\n\n\ndef smdape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Symmetric Median Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.median(2.0 * np.abs(actual - predicted) \/ ((np.abs(actual) + np.abs(predicted)) + EPSILON))\n\n\ndef maape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Mean Arctangent Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(np.arctan(np.abs((actual - predicted) \/ (actual + EPSILON))))\n\n\ndef mase(actual: np.ndarray, predicted: np.ndarray, seasonality: int = 1):\n    \"\"\"\n    Mean Absolute Scaled Error\n\n    Baseline (benchmark) is computed with naive forecasting (shifted by @seasonality)\n    \"\"\"\n    return mae(actual, predicted) \/ mae(actual[seasonality:], _naive_forecasting(actual, seasonality))\n\n\ndef std_ae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Absolute Error \"\"\"\n    __mae = mae(actual, predicted)\n    return np.sqrt(np.sum(np.square(_error(actual, predicted) - __mae))\/(len(actual) - 1))\n\n\ndef std_ape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Absolute Percentage Error \"\"\"\n    __mape = mape(actual, predicted)\n    return np.sqrt(np.sum(np.square(_percentage_error(actual, predicted) - __mape))\/(len(actual) - 1))\n\n\ndef rmspe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Root Mean Squared Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n\n\ndef rmdspe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Root Median Squared Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.sqrt(np.median(np.square(_percentage_error(actual, predicted))))\n\n\ndef rmsse(actual: np.ndarray, predicted: np.ndarray, seasonality: int = 1):\n    \"\"\" Root Mean Squared Scaled Error \"\"\"\n    q = np.abs(_error(actual, predicted)) \/ mae(actual[seasonality:], _naive_forecasting(actual, seasonality))\n    return np.sqrt(np.mean(np.square(q)))\n\n\ndef inrse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Integral Normalized Root Squared Error \"\"\"\n    return np.sqrt(np.sum(np.square(_error(actual, predicted))) \/ np.sum(np.square(actual - np.mean(actual))))\n\n\ndef rrse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Root Relative Squared Error \"\"\"\n    return np.sqrt(np.sum(np.square(actual - predicted)) \/ np.sum(np.square(actual - np.mean(actual))))\n\n\ndef mre(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Relative Error \"\"\"\n    return np.mean(_relative_error(actual, predicted, benchmark))\n\n\ndef rae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Relative Absolute Error (aka Approximation Error) \"\"\"\n    return np.sum(np.abs(actual - predicted)) \/ (np.sum(np.abs(actual - np.mean(actual))) + EPSILON)\n\n\ndef mrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Relative Absolute Error \"\"\"\n    return np.mean(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef mdrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Median Relative Absolute Error \"\"\"\n    return np.median(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef gmrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Geometric Mean Relative Absolute Error \"\"\"\n    return _geometric_mean(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef mbrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Bounded Relative Absolute Error \"\"\"\n    return np.mean(_bounded_relative_error(actual, predicted, benchmark))\n\n\ndef umbrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Unscaled Mean Bounded Relative Absolute Error \"\"\"\n    __mbrae = mbrae(actual, predicted, benchmark)\n    return __mbrae \/ (1 - __mbrae)\n\n\ndef mda(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Directional Accuracy \"\"\"\n    return np.mean((np.sign(actual[1:] - actual[:-1]) == np.sign(predicted[1:] - predicted[:-1])).astype(int))\n\n\nMETRICS = {\n    'mse': mse,\n    'rmse': rmse,\n    'nrmse': nrmse,\n    'me': me,\n    'mae': mae,\n    'mad': mad,\n    'gmae': gmae,\n    'mdae': mdae,\n    'mpe': mpe,\n    'mape': mape,\n    'mdape': mdape,\n    'smape': smape,\n    'smdape': smdape,\n    'maape': maape,\n    'mase': mase,\n    'std_ae': std_ae,\n    'std_ape': std_ape,\n    'rmspe': rmspe,\n    'rmdspe': rmdspe,\n    'rmsse': rmsse,\n    'inrse': inrse,\n    'rrse': rrse,\n    'mre': mre,\n    'rae': rae,\n    'mrae': mrae,\n    'mdrae': mdrae,\n    'gmrae': gmrae,\n    'mbrae': mbrae,\n    'umbrae': umbrae,\n    'mda': mda,\n}\n\n\ndef evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mae', 'mse', 'smape', 'umbrae')):\n    results = {}\n    for name in metrics:\n        try:\n            results[name] = METRICS[name](actual, predicted)\n        except Exception as err:\n            results[name] = np.nan\n            print('Unable to compute metric {0}: {1}'.format(name, err))\n    return results\n\n\ndef evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n    return evaluate(actual, predicted, metrics=set(METRICS.keys()))","3940f8e1":"nrm = nrmse(predict[1:-1],cold_y)\nprint(nrm)","642ad3e6":"# plot a few consumption patterns\nseries_to_plot = rng.choice(consumption_train.series_id.unique(), 1)\n\nfor ser_id in series_to_plot:\n    ser_data = consumption_train[consumption_train.series_id == ser_id]\n    ax = ser_data.plot(x='timestamp',\n                       y='consumption', \n                       title=f\"series_id {int(ser_id)}\",\n                       legend=False)\n    plt.ylabel('consumption (watt-hours)')\n    plt.show()","242026a0":"print(ser_data)","a8a379d2":"temp =ser_data.set_index('timestamp')\ntemp =temp.sort_index(ascending=True)\n#temp =temp.consumption \n#temp=temp.drop(['temperature', 'series_id'], axis=1)\nprint(temp.consumption)","ce2739bd":"X, y, scaler = prepare_training_data(temp.consumption, lag)","80bcfb03":"# lag of 23 to simulate smallest cold start window. Our series\n# will be converted to a num_timesteps x lag size matrix\nlag =  23\nopt = tf.keras.optimizers.Adamax(learning_rate=0.01)  #SGD , RMSprop ,Adam , Adadelta ,Adagrad ,Adamax ,Nadam,Ftrl\n\n# model parameters\nnum_neurons = 100\nbatch_size = 1  # this forces the lstm to step through each time-step one at a time\nbatch_input_shape=(batch_size, 1, lag)\n#---------------------------\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=num_neurons,return_sequences=True,batch_input_shape=batch_input_shape,stateful=True)) #input_shape=(1, look_back), input_shape=(X_train.shape[1],1)\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.add(tf.keras.layers.LSTM(units=num_neurons,return_sequences=False))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer=opt, loss='mse')\n# This builds the model for the first time:\nmodel.fit(X, y, epochs=50, batch_size=batch_size)\n# let's reset the states of the LSTM layer:","58f274cb":"'''Run this step again to load different test set\n\nLoad test series'''\nseries_to_plot = rng.choice(cold_start_test.series_id.unique(), 1)\n\nfor ser_id in series_to_plot:\n    ser_data_test = cold_start_test[cold_start_test.series_id == ser_id]\n    ax = ser_data_test.plot(x='timestamp',\n                       y='consumption', \n                       title=f\"series_id {int(ser_id)}\",\n                       legend=False)\n    plt.ylabel('consumption (watt-hours)')\n    plt.show()","f522841c":"#print(ser_data_test)\ntemp_test =ser_data_test.set_index('timestamp')\ntemp_test =temp_test.sort_index(ascending=True)\n#temp =temp_test.consumption \n#temp_test=temp_test.drop(['temperature', 'series_id','prediction_window'], axis=1)","53bfb5c7":"'''check prediction window'''\nprint(temp_test[0:1])","1e002e98":"cold_X, cold_y, scaler = prepare_training_data(temp_test.consumption, lag)","344b148d":"predict = model.predict(cold_X, batch_size=1)","36b820a6":"print(len(cold_y))\nprint(len(predict))\n","a3691856":"plt.figure(figsize=(45, 15))\nplt.plot(cold_y)\nplt.plot(predict[1:-1])","ff7cce2f":"'''Define all error metric\n'''\nimport numpy as np\n\nEPSILON = 1e-10\n\n\ndef _error(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Simple error \"\"\"\n    return actual - predicted\n\n\ndef _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Percentage error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return _error(actual, predicted) \/ (actual + EPSILON)\n\n\ndef _naive_forecasting(actual: np.ndarray, seasonality: int = 1):\n    \"\"\" Naive forecasting method which just repeats previous samples \"\"\"\n    return actual[:-seasonality]\n\n\ndef _relative_error(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Relative Error \"\"\"\n    if benchmark is None or isinstance(benchmark, int):\n        # If no benchmark prediction provided - use naive forecasting\n        if not isinstance(benchmark, int):\n            seasonality = 1\n        else:\n            seasonality = benchmark\n        return _error(actual[seasonality:], predicted[seasonality:]) \/\\\n               (_error(actual[seasonality:], _naive_forecasting(actual, seasonality)) + EPSILON)\n\n    return _error(actual, predicted) \/ (_error(actual, benchmark) + EPSILON)\n\n\ndef _bounded_relative_error(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Bounded Relative Error \"\"\"\n    if benchmark is None or isinstance(benchmark, int):\n        # If no benchmark prediction provided - use naive forecasting\n        if not isinstance(benchmark, int):\n            seasonality = 1\n        else:\n            seasonality = benchmark\n\n        abs_err = np.abs(_error(actual[seasonality:], predicted[seasonality:]))\n        abs_err_bench = np.abs(_error(actual[seasonality:], _naive_forecasting(actual, seasonality)))\n    else:\n        abs_err = np.abs(_error(actual, predicted))\n        abs_err_bench = np.abs(_error(actual, benchmark))\n\n    return abs_err \/ (abs_err + abs_err_bench + EPSILON)\n\n\ndef _geometric_mean(a, axis=0, dtype=None):\n    \"\"\" Geometric mean \"\"\"\n    if not isinstance(a, np.ndarray):  # if not an ndarray object attempt to convert it\n        log_a = np.log(np.array(a, dtype=dtype))\n    elif dtype:  # Must change the default dtype allowing array type\n        if isinstance(a, np.ma.MaskedArray):\n            log_a = np.log(np.ma.asarray(a, dtype=dtype))\n        else:\n            log_a = np.log(np.asarray(a, dtype=dtype))\n    else:\n        log_a = np.log(a)\n    return np.exp(log_a.mean(axis=axis))\n\n\ndef mse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Squared Error \"\"\"\n    return np.mean(np.square(_error(actual, predicted)))\n\n\ndef rmse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Root Mean Squared Error \"\"\"\n    return np.sqrt(mse(actual, predicted))\n\n\ndef nrmse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Root Mean Squared Error \"\"\"\n    return rmse(actual, predicted) \/ (actual.max() - actual.min())\n\n\ndef me(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Error \"\"\"\n    return np.mean(_error(actual, predicted))\n\n\ndef mae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Absolute Error \"\"\"\n    return np.mean(np.abs(_error(actual, predicted)))\n\n\nmad = mae  # Mean Absolute Deviation (it is the same as MAE)\n\n\ndef gmae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Geometric Mean Absolute Error \"\"\"\n    return _geometric_mean(np.abs(_error(actual, predicted)))\n\n\ndef mdae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Median Absolute Error \"\"\"\n    return np.median(np.abs(_error(actual, predicted)))\n\n\ndef mpe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Percentage Error \"\"\"\n    return np.mean(_percentage_error(actual, predicted))\n\n\ndef mape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Mean Absolute Percentage Error\n\n    Properties:\n        + Easy to interpret\n        + Scale independent\n        - Biased, not symmetric\n        - Undefined when actual[t] == 0\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(np.abs(_percentage_error(actual, predicted)))\n\n\ndef mdape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Median Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.median(np.abs(_percentage_error(actual, predicted)))\n\n\ndef smape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Symmetric Mean Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(2.0 * np.abs(actual - predicted) \/ ((np.abs(actual) + np.abs(predicted)) + EPSILON))\n\n\ndef smdape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Symmetric Median Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.median(2.0 * np.abs(actual - predicted) \/ ((np.abs(actual) + np.abs(predicted)) + EPSILON))\n\n\ndef maape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Mean Arctangent Absolute Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.mean(np.arctan(np.abs((actual - predicted) \/ (actual + EPSILON))))\n\n\ndef mase(actual: np.ndarray, predicted: np.ndarray, seasonality: int = 1):\n    \"\"\"\n    Mean Absolute Scaled Error\n\n    Baseline (benchmark) is computed with naive forecasting (shifted by @seasonality)\n    \"\"\"\n    return mae(actual, predicted) \/ mae(actual[seasonality:], _naive_forecasting(actual, seasonality))\n\n\ndef std_ae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Absolute Error \"\"\"\n    __mae = mae(actual, predicted)\n    return np.sqrt(np.sum(np.square(_error(actual, predicted) - __mae))\/(len(actual) - 1))\n\n\ndef std_ape(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Normalized Absolute Percentage Error \"\"\"\n    __mape = mape(actual, predicted)\n    return np.sqrt(np.sum(np.square(_percentage_error(actual, predicted) - __mape))\/(len(actual) - 1))\n\n\ndef rmspe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Root Mean Squared Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n\n\ndef rmdspe(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\"\n    Root Median Squared Percentage Error\n\n    Note: result is NOT multiplied by 100\n    \"\"\"\n    return np.sqrt(np.median(np.square(_percentage_error(actual, predicted))))\n\n\ndef rmsse(actual: np.ndarray, predicted: np.ndarray, seasonality: int = 1):\n    \"\"\" Root Mean Squared Scaled Error \"\"\"\n    q = np.abs(_error(actual, predicted)) \/ mae(actual[seasonality:], _naive_forecasting(actual, seasonality))\n    return np.sqrt(np.mean(np.square(q)))\n\n\ndef inrse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Integral Normalized Root Squared Error \"\"\"\n    return np.sqrt(np.sum(np.square(_error(actual, predicted))) \/ np.sum(np.square(actual - np.mean(actual))))\n\n\ndef rrse(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Root Relative Squared Error \"\"\"\n    return np.sqrt(np.sum(np.square(actual - predicted)) \/ np.sum(np.square(actual - np.mean(actual))))\n\n\ndef mre(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Relative Error \"\"\"\n    return np.mean(_relative_error(actual, predicted, benchmark))\n\n\ndef rae(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Relative Absolute Error (aka Approximation Error) \"\"\"\n    return np.sum(np.abs(actual - predicted)) \/ (np.sum(np.abs(actual - np.mean(actual))) + EPSILON)\n\n\ndef mrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Relative Absolute Error \"\"\"\n    return np.mean(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef mdrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Median Relative Absolute Error \"\"\"\n    return np.median(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef gmrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Geometric Mean Relative Absolute Error \"\"\"\n    return _geometric_mean(np.abs(_relative_error(actual, predicted, benchmark)))\n\n\ndef mbrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Mean Bounded Relative Absolute Error \"\"\"\n    return np.mean(_bounded_relative_error(actual, predicted, benchmark))\n\n\ndef umbrae(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n    \"\"\" Unscaled Mean Bounded Relative Absolute Error \"\"\"\n    __mbrae = mbrae(actual, predicted, benchmark)\n    return __mbrae \/ (1 - __mbrae)\n\n\ndef mda(actual: np.ndarray, predicted: np.ndarray):\n    \"\"\" Mean Directional Accuracy \"\"\"\n    return np.mean((np.sign(actual[1:] - actual[:-1]) == np.sign(predicted[1:] - predicted[:-1])).astype(int))\n\n\nMETRICS = {\n    'mse': mse,\n    'rmse': rmse,\n    'nrmse': nrmse,\n    'me': me,\n    'mae': mae,\n    'mad': mad,\n    'gmae': gmae,\n    'mdae': mdae,\n    'mpe': mpe,\n    'mape': mape,\n    'mdape': mdape,\n    'smape': smape,\n    'smdape': smdape,\n    'maape': maape,\n    'mase': mase,\n    'std_ae': std_ae,\n    'std_ape': std_ape,\n    'rmspe': rmspe,\n    'rmdspe': rmdspe,\n    'rmsse': rmsse,\n    'inrse': inrse,\n    'rrse': rrse,\n    'mre': mre,\n    'rae': rae,\n    'mrae': mrae,\n    'mdrae': mdrae,\n    'gmrae': gmrae,\n    'mbrae': mbrae,\n    'umbrae': umbrae,\n    'mda': mda,\n}\n\n\ndef evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mae', 'mse', 'smape', 'umbrae')):\n    results = {}\n    for name in metrics:\n        try:\n            results[name] = METRICS[name](actual, predicted)\n        except Exception as err:\n            results[name] = np.nan\n            print('Unable to compute metric {0}: {1}'.format(name, err))\n    return results\n\n\ndef evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n    return evaluate(actual, predicted, metrics=set(METRICS.keys()))","f8776ecf":"nrm = nrmse(predict[1:-1],cold_y)\nprint(nrm)","b43addbd":"!head save_path\/'my_submmission.csv'","9dd8c97f":"All of the series in the training data have the same length.","9a1d74d2":"* Some series may provide little cold start data and ask for a long (weekly) forecast, while others may provide many days of cold start data and ask for a short (hourly) forecast. \n* Let's take a look at how these scenarios are distributed in the test set.\n* For convenience, The prediction_window will be added to the test data.","a2faf91a":"# Build the Model\n\nThis challenge asks for taking the variable length time series as inputs (the cold start data) and map them to variable length time series as outputs (the prediction window). This can be call at a variable length input variable length output sequence to sequence time series regression problem.\n\nhttps:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\n## Long Short Term Memory (LSTM) Network\n\nLong Short Term Memory (LSTM) networks can learn patterns over long sequences and do not neccessarily rely on a pre-specified window of lagged observation as input. Further, their internal states can be built up to include information about cold start data before making any number of forecast predictions we ask for. This means that a single LSTM model can handle all prediction horizons in the data. Our overall approach will be to train the network on 4 week series, then use the cold start data to prime the netowrk's internal state for forecasting, and then forecast.","da776b22":"Plot the distribution of prediction_window for each unique length of cold start data in the test set.","d871352f":"\n    Each series_id has an associated time series of consumption and temperature data. \n    For a given series, the measurements are provided at hourly time-steps. \n    In other words, every row of consumption_train represents an hour of data from series_id. \n    Each unique series_id in consumption_train has four weeks, or 28 days, worth of hourly data.\n\n    Let's figure out how many series we have, and how much training data that corresponds to.\n","e0817777":"## Test the model using test set","d925e33b":"\n## Make Model\n","4ecbe59a":"# References\n\nhttps:\/\/www.drivendata.co\/blog\/benchmark-cold-start-lstm-deep-learning\/\n\n","b0e55965":"### Loading the Data\n\n\n    train.csv             - Historical series that can be used as training data.\n    cold_start_test.csv   - Cold start lead-in periods for the series that appear in the submission format.\n    meta.csv              - Information about the series that are available to competitors.\n    submission_format.csv - This is the format for submssions to the competition. Your submissions must match the columns and    indices exactly.\n","0c3c7821":"## Prepare Training Data for Model\n\nBefore each series is processed, it need to be converted to a supervised learning problem, then scale the data. Let's create a function, prepare_training_data that we can call on each sample in our training loop.\n","e6757781":"As usual with Keras, getting from imports to a simple custom model be done in just a few lines. Next we'll instantiate a Keras Sequential configuration and add to it a single recurrent cell followed by a dense layer to output regressed values.\n\nNote the importance of setting stateful=True below. This allows the LSTM to learn the patterns within a batch (the lagged series     data). Otherwise the state would be reset after each time step, but we want the model to develop states that depend on every         timestep we show it.","97693375":"For each **series_id** in the test set, the amount of hourly cold start data you are provided is never less than one day (24 consecutive consumption measurements) and can be as much as two weeks (336 consecutive consumption measurements). Cold start data only comes in full-day. In other words, some series may have 3, 4, ..., 14 days worth of cold start data, but no series have 3.5, 4.1, ..., 13.9, or any other partial-day's worth of cold start data.\n\nSince each timestep represents and hour, we can visually confirm the above claim with a simple bar plot counting the number of days in each test series.","383e795f":"### Training Data\n","14f57707":"It looks like the various combinations are all represented, with a couple of combinations being more or less represented. However, there does not seem to be any advantage to building this into the algorithm.\n\nThe main training set, consumption_train has many series, each with 4 weeks worth of hourly data. To gain a sense of what the cold start prediction asks , let's look at a few examples of how the cold start data leads up to the prediction window.\n","7471f980":"https:\/\/www.mikulskibartosz.name\/forecasting-time-series-using-lag-features\/","2cbe86ae":"## Preparing the data\nAppropriately preprocessing data is always important. To prepare our data for modeling with an LSTM let's consider a few standard approaches.\n\n## Scaling\n\nDue to their dependence on backpropogation, LSTM networks behave best when the data is within the scale of the activation function used (tanh by default in LSTM, domain (\u22121,1)). Scikit Learn's MinMaxScaler is sufficient for this task, since it allows us to specify a range and invert transformations.\n\n## Stationary\n\nOften in timeseries problems, you want to ensure that your data is stationary, and make it so if it isn't (for example using differencing).\n\n## Converting Timeseries to Supervised\n\nWe'll use a standard lagged variable approach to convert the timeseries problem to a supervised learning problem, where given features X\nand labels y\n\nThe model learns a mapping between them. In this case, the features are our lagged series and the label is the orginal series. This process can be used with any model, but the advantage here is that we can use a stateful LSTM that learns dependencies between each timestep in the data.\n\n## Creating Lagged Features for Timeseries\n\nRead more info on how to creare Lagged Features for Timeseries forecasting\n","09148d66":"\n### Explore the Data\n\nThe task in this challege is to utilize \"cold start\" consumption data to generate consumption forecasts for various prediction windows. Each *series_id* in the test set asks for one (and only one) of three prediction windows:\n\n    hourly  - predict consumption each hour for a day (24 predictions)\n    daily   - predict consumption each day for a week (7 predictions)\n    weekly  - predict consumption each week for two weeks (2 predictions)\n    \nThe prediction window types are pretty evenly distributed across the series_ids","a5e27131":"In order speed up development and facilitate rapid prototyping, let's reduce the training set. Since all of the training series are the same length, we'll uniformly sample a subset of series. The size of our resulting reduced data will be controlled by frac_series_to_use.","15201d8e":"###  Test Data\n\n* The test data is provided in cold_start_test.csv. The data is provided for each new series before the window that we need to predict. \n* There is a different amount of cold-start data provided for different series,the model needs to work well whether the given data 1 day or 14.\n* The predictions windows are obtained from the submission format prediction_window column.\n","3db5167b":"The training data can be use to simulate all of the possible cold start scenarios which enounter in the test set. For example, at one extreme the 4 weeks can be split into 2 weeks of simulated cold start, followed by two weeks worth of hourly consumption that can be use to create a validation set (by summing the consumption for each week, leading to 2 weekly predictions). At the other extreme,the first day of consumption data can be taken from a 4 week long series as cold start and forecast the next 2 weeks in the same way.\n\nUsing the series consumption_train, all cold start windows from 1 to 14 days can be created, with a minimum of 14 more days that can be used to create validation sets.\n\nLet's take a look at the consumption patterns in a few series to see that they're 4 weeks.","3fa5388e":"### Train only one serie and use the model to predict the orther series","a9411d9b":"A lag features is a fancy name for a variable which contains data from prior time steps. Time-series data can convert it into rows. Every row contains data about one observation and includes all previous occurrences of that observation.","1ec0566d":"## Fit Model\n\nNow for the fun part! Above we showed that the series in consumption_train could be used to simulate all sorts of cold start types. Each 4 week series is going to be fitted  using a 24 hour lag, so that ideally the model learns at least patterns over a single day.\n\nTo train, we pass through the consumption data, grouped by series_id. For each series, we first prepare_training_data, then fit using that data, resetting the state only after each row of the series has been seen. In this way, each lagged timeseries is learned by the network before the state is reset and the next series is passed in.\n\nTo keep this example quick, we're only going to make a single pass through the data. An easy way to improve this benchmark is to simply change num_passes_through_data below (to a much larger number).\n","2ccc71d3":"# Introduction\n\n* The data can be donwloaded from the https:\/\/www.drivendata.org\/competitions\/55\/schneider-cold-start\/page\/138\/\n* The goal is to build an algorithm which provides an accurate forecast from the very start of a building's instrumentation."}}