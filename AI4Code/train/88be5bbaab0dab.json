{"cell_type":{"fb7f21a7":"code","ad78dc5f":"code","5b7781d6":"code","79327b24":"code","9a8d3a45":"code","60a802ce":"code","fe958ba1":"code","71587723":"code","c374c82c":"code","ecca3d50":"code","16fce48c":"code","18fd206a":"code","14192758":"code","681085dd":"code","de12c878":"code","4abedda1":"code","1fd288b9":"code","47ef52a2":"code","b50a47ee":"code","04432a0b":"markdown","ff9ffb60":"markdown","36914fad":"markdown","1f722b42":"markdown","8895b0da":"markdown","d781da3e":"markdown","b5305ed5":"markdown","02dfd811":"markdown","4dd7c1ec":"markdown","a11774d2":"markdown","82a9932d":"markdown","4b80f0ed":"markdown","22f74c52":"markdown","54199363":"markdown"},"source":{"fb7f21a7":"import numpy as np\nimport pandas as pd\n\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt","ad78dc5f":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b7781d6":"input_dir = '\/kaggle\/input\/santander-customer-transaction-prediction\/'\ndf_train = pd.read_csv(input_dir + '\/train.csv')\ndf_train","79327b24":"var_colums = [c for c in df_train.columns if c not in ['ID_code','target']]\nX = df_train.loc[:, var_colums]\ny = df_train.loc[:, 'target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","9a8d3a45":"xgboost.XGBClassifier().get_params()","60a802ce":"model_xgboost = xgboost.XGBClassifier(learning_rate=0.1,\n                                      max_depth=5,\n                                      n_estimators=5000,\n                                      subsample=0.5,\n                                      colsample_bytree=0.5,\n                                      eval_metric='auc',\n                                      verbosity=1)\n\neval_set = [(X_valid, y_valid)]\n\nmodel_xgboost.fit(X_train,\n                  y_train,\n                  early_stopping_rounds=10,\n                  eval_set=eval_set,\n                  verbose=True)","fe958ba1":"y_train_pred = model_xgboost.predict_proba(X_train)[:,1]\ny_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n\nprint(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","71587723":"learning_rate_list = [0.02, 0.05, 0.1]\nmax_depth_list = [2, 3, 5]\nn_estimators_list = [1000, 2000, 3000]\n\nparams_dict = {\"learning_rate\": learning_rate_list,\n               \"max_depth\": max_depth_list,\n               \"n_estimators\": n_estimators_list}\n\nnum_combinations = 1\nfor v in params_dict.values(): num_combinations *= len(v) \n\nprint(num_combinations)\nparams_dict","c374c82c":"def my_roc_auc_score(model, X, y): return roc_auc_score(y, model.predict_proba(X)[:,1])\n\nmodel_xgboost_hp = GridSearchCV(estimator=xgboost.XGBClassifier(subsample=0.5,\n                                                                colsample_bytree=0.25,\n                                                                eval_metric='auc',\n                                                                use_label_encoder=False),\n                                param_grid=params_dict,\n                                cv=2,\n                                scoring=my_roc_auc_score,\n                                return_train_score=True,\n                                verbose=4)\n\nmodel_xgboost_hp.fit(X, y)","ecca3d50":"df_cv_results = pd.DataFrame(model_xgboost_hp.cv_results_)\ndf_cv_results = df_cv_results[['rank_test_score','mean_test_score','mean_train_score',\n                               'param_learning_rate', 'param_max_depth', 'param_n_estimators']]\ndf_cv_results.sort_values(by='rank_test_score', inplace=True)\ndf_cv_results","16fce48c":"# First sort by number of estimators as that would be x-axis\ndf_cv_results.sort_values(by='param_n_estimators', inplace=True)\n\n# Find values of AUC for learning rate of 0.05 and different values of depth\nlr_d2 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==2),:]\nlr_d3 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==3),:]\nlr_d5 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==5),:]\nlr_d7 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==7),:]\n\n# Let us plot now\nfig, ax = plt.subplots(figsize=(10,5))\nlr_d2.plot(x='param_n_estimators', y='mean_test_score', label='Depth=2', ax=ax)\nlr_d3.plot(x='param_n_estimators', y='mean_test_score', label='Depth=3', ax=ax)\nlr_d5.plot(x='param_n_estimators', y='mean_test_score', label='Depth=5', ax=ax)\nlr_d7.plot(x='param_n_estimators', y='mean_test_score', label='Depth=7', ax=ax)\nplt.ylabel('Mean Validation AUC')\nplt.title('Performance wrt # of Trees and Depth')","18fd206a":"# First sort by learning rate as that would be x-axis\ndf_cv_results.sort_values(by='param_learning_rate', inplace=True)\n\n# Find values of AUC for learning rate of 0.05 and different values of depth\nlr_t3k_d2 = df_cv_results.loc[(df_cv_results['param_n_estimators']==3000) & (df_cv_results['param_max_depth']==2),:]\n\n# Let us plot now\nfig, ax = plt.subplots(figsize=(10,5))\nlr_t3k_d2.plot(x='param_learning_rate', y='mean_test_score', label='Depth=2, Trees=3000', ax=ax)\nplt.ylabel('Mean Validation AUC')\nplt.title('Performance wrt learning rate')","14192758":"model_xgboost_fin = xgboost.XGBClassifier(learning_rate=0.05,\n                                          max_depth=2,\n                                          n_estimators=5000,\n                                          subsample=0.5,\n                                          colsample_bytree=0.25,\n                                          eval_metric='auc',\n                                          verbosity=1,\n                                          use_label_encoder=False)\n\n# Passing both training and validation dataset as we want to plot AUC for both\neval_set = [(X_train, y_train),(X_valid, y_valid)]\n\nmodel_xgboost_fin.fit(X_train,\n                  y_train,\n                  early_stopping_rounds=20,\n                  eval_set=eval_set,\n                  verbose=True)","681085dd":"y_train_pred = model_xgboost_fin.predict_proba(X_train)[:,1]\ny_valid_pred = model_xgboost_fin.predict_proba(X_valid)[:,1]\n\nprint(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","de12c878":"evaluation_results = model_xgboost_fin.evals_result()\n\n# Index into each key to find AUC values for training and validation data after each tree\ntrain_auc_tree = evaluation_results['validation_0']['auc']\nvalid_auc_tree = evaluation_results['validation_1']['auc']\n\n\n# Plotting Section\nplt.figure(figsize=(15,5))\n\nplt.plot(train_auc_tree, label='Train')\nplt.plot(valid_auc_tree, label='valid')\n\nplt.title(\"Train and validation AUC as number of trees increase\")\nplt.xlabel(\"Trees\")\nplt.ylabel(\"AUC\")\nplt.legend(loc='lower right')\nplt.show()","4abedda1":"df_var_imp = pd.DataFrame({\"Variable\": var_colums,\n                           \"Importance\": model_xgboost_fin.feature_importances_}) \\\n                        .sort_values(by='Importance', ascending=False)\ndf_var_imp[:10]","1fd288b9":"df_test = pd.read_csv(input_dir + 'test.csv')\ndf_sample_submissions = pd.read_csv(input_dir + 'sample_submission.csv')\ndf_test.shape, df_sample_submissions.shape","47ef52a2":"X_test = df_test.loc[:, var_colums]\n\ndf_sample_submissions['target'] = model_xgboost_fin.predict_proba(X_test)[:,1]\ndf_sample_submissions","b50a47ee":"output_dir = '\/kaggle\/working\/'\ndf_sample_submissions.to_csv(output_dir + \"05_xgboost_scores.csv\", index=False)","04432a0b":"### 1. Read Train CSV","ff9ffb60":"### 4. Hyperparameter Tuning\nWe will use `GridSearchCV` for hyperparameter tuning. As the first step, let us define all the possible hyperparameter values which we want to check.","36914fad":"Let us also look at variable importance","1f722b42":"Let us look at the output of grid search step. We will retain only a few relevant columns and sort based on `rank_test_score` i.ee the performance on validation data","8895b0da":"Let us print model performance w.r.t number of trees. The first step is to get AUC values on training and validation dataset for every value of the tree. We can use `eval_result()` function to get that. Then we can go ahead and plot it.","d781da3e":"## 6. Score the test data\nFirst let us import test.csv\n","b5305ed5":"### 3.Evaluate Model Performance","02dfd811":"# Santander Customer Transaction Prediction - XGBoost\n\nIn the Kaggle competition, the objective is to identify which customer will make a transaction in the future.\n\n**Link to the competition**: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/  \n**Type of Problem**: Classification  \n**Metric for evalution**: AOC (Area Under Curve)\n\nThis Python 3 environment comes with many helpful analytics libraries installed\nIt is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python","4dd7c1ec":"### 2. Create XGBoost Model","a11774d2":"Let us try to find how does model perfomance vary with respect to `number of trees` and `depth of tree`","82a9932d":"## 5. Final Model\nWith Best Hyper Parameters from the above step","4b80f0ed":"Now that we have seen the performance is higher for higher values of trees and lower depth, let us find performance w.r.t. learning rate. We fix n_estimators to 3000 and depth to 2.","22f74c52":"Now let us pass this to `GridSearchCV`. We have a custom scoring function based on sklearn's `roc_auc_score()` which calculated area under the curve. CV value is 2, which is not ideal but we want to minimize the execution time.","54199363":"Let us find out the performance of the final model by calculating AUC value on training and validation sets"}}