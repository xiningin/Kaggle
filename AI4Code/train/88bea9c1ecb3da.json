{"cell_type":{"1590554c":"code","fbaa3f51":"code","39fe9d74":"code","448c855e":"markdown"},"source":{"1590554c":"import numpy as np\nimport pandas as pd \n\ndef ensemble():\n    #the submission file\n    stacked_0 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\n    \n    #uses many existing good solutions\n    stacked_1 = pd.read_csv('\/kaggle\/input\/melanoma-submissions2\/submissionImage.csv')\n    stacked_2 = pd.read_csv('\/kaggle\/input\/melanoma-submissions2\/submissionTabular.csv')\n    stacked_4 = pd.read_csv('\/kaggle\/input\/melanoma-submissions2\/submission_multiple_data_source.csv')\n    stacked_3 = pd.read_csv('..\/input\/csv001\/datasets_766461_1344911_submission_rank-then-blend.csv')    \n    stacked_5 = pd.read_csv('..\/input\/csv001\/submission0.9504.csv')\n    stacked_6 = pd.read_csv('..\/input\/csv001\/submission_efnet.csv')\n    stacked_7 = pd.read_csv('..\/input\/csv001\/submission_models_blended.csv')\n    #you may add new solutions you like including your own work\n    \n    sub = pd.DataFrame()\n    sub['image_name'] = stacked_0['image_name']\n    sub['target'] = np.exp(np.mean(\n        [\n            stacked_1['target'].apply(lambda x: np.log2(x)), \\\n            stacked_2['target'].apply(lambda x: np.log2(x)), \\\n            stacked_4['target'].apply(lambda x: np.log2(x)), \\\n            stacked_3['target'].apply(lambda x: np.log2(x)), \n            stacked_5['target'].apply(lambda x: np.log2(x)), \\\n            stacked_6['target'].apply(lambda x: np.log2(x)), \\\n            stacked_7['target'].apply(lambda x: np.log2(x)),           \n            ], axis=0))\n    sub.to_csv('submission.csv', index=False, float_format='%.6f')","fbaa3f51":"ensemble()\n","39fe9d74":"stacked_n = pd.read_csv('submission.csv')\nstacked_n.head()","448c855e":"### Many thanks to the respective authers who wrote them. \n\nThis leverages [QuantScientist](https:\/\/www.kaggle.com\/solomonk\/classification-np-log2-ensemble-0-9504-lb) idea of ensemble and I correct the links and make minor tweaks. This can get you in top 10%\n\nEnsemble learning is an excellent machine learning idea which displays noticeable benefits in many applications, one such notable example is the widespread use of ensembles in Kaggle competitions. In an ensemble several individual models (for instance ResNet18 and VGG16) which were trained on the same corpus, work in tandem and during inference, their predictions are fused by a pre-defined strategy to yield a single prediction. \n\nThe use of np.log2 and np.exp may sometimes produce superior results to plain averaging. This was just an experiment to attest that assertion.\n"}}