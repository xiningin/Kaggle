{"cell_type":{"41d55a6e":"code","43c9931a":"code","a1598215":"code","ac01fa46":"code","128f2ea6":"code","59e4758f":"code","2bd2674f":"code","55ef7076":"code","6113c0a6":"code","fd37d77e":"code","be07cfd9":"code","38e04399":"code","8a05b628":"code","a1b47ec4":"code","5c950927":"code","e48f303f":"code","96099bf0":"code","d4e0e5fb":"code","a19e7778":"code","af3be72d":"code","c1bdd8db":"code","3e6ab26b":"code","2094262c":"code","330a615b":"code","0b964aa9":"code","a092a18b":"code","e4c2b627":"code","86488f34":"code","8e754f83":"code","726fa949":"code","873aafad":"code","51db098e":"code","fbf4d166":"code","5a7748d8":"code","4aed3185":"code","3722da1d":"code","36080ae2":"code","6c0b7e3f":"code","46509c8d":"code","bfdbf509":"code","535ed6a0":"code","1096d18e":"code","195ed552":"code","55ba0203":"code","d2836c8c":"code","4250c584":"code","ca7c798a":"code","90487338":"code","9d7790fb":"markdown","66e64434":"markdown","9cb2dd8e":"markdown","f70ec7fb":"markdown","936cba57":"markdown","5c380354":"markdown","6d6b2762":"markdown","a34c0f81":"markdown","19e5466a":"markdown","c1896cf5":"markdown","547b1eb5":"markdown","c4d6babe":"markdown","5eead61f":"markdown","a9cff176":"markdown","5ac400ed":"markdown","05adc084":"markdown"},"source":{"41d55a6e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.figure_factory as ff\n\nimport cufflinks\ncufflinks.go_offline(connected = True)\ninit_notebook_mode(connected = True)\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold,train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport re","43c9931a":"!ls ..\/input\/product-category-from-invoice\/88b2c062-9-dataset\/Dataset\/","a1598215":"df_train = pd.read_csv(\"..\/input\/product-category-from-invoice\/88b2c062-9-dataset\/Dataset\/Train.csv\")\ndf_test = pd.read_csv(\"..\/input\/product-category-from-invoice\/88b2c062-9-dataset\/Dataset\/Test.csv\")","ac01fa46":"df_train.shape, df_test.shape","128f2ea6":"df_train.info()","59e4758f":"df_test.info()","2bd2674f":"df_train.head()","55ef7076":"df_train['Product_Category'].iplot(kind='hist', yTitle ='count', title = 'Product Category Distribution', color = 'rgb(120,130,140)')","6113c0a6":"df_train['Product_Category'].value_counts()","fd37d77e":"## code for comparing distribution of a column in both train and test dataset\ndef draw_distribution(train_df, test_df, feature_name, top_counts=None):\n    _tmp_material = (train_df[feature_name].value_counts() \/train_df.shape[0] * 100) [:top_counts]\n    tmp_trace = go.Bar(\n                x=_tmp_material.index,\n                y=_tmp_material.values,\n                name='training_dataset',\n            )\n\n    _tmp_material_test = (test_df[feature_name].value_counts() \/ test_df.shape[0] * 100) [:top_counts]\n    tmp_trace_test = go.Bar(\n                x=_tmp_material_test.index,\n                y=_tmp_material_test.values,\n                name='test_dataset'\n            )\n\n    layout = go.Layout(\n            barmode='group',\n            title= \" Train\/Test \" + feature_name + \" distribution\",\n            yaxis=dict(\n                title='Counts',\n            ),\n#             xaxis=dict(\n#                 title=feature_name,\n#             )\n\n        )\n\n    fig = go.Figure(data=[tmp_trace, tmp_trace_test], layout=layout)\n    iplot(fig)","be07cfd9":"draw_distribution(df_train, df_test, 'GL_Code')","38e04399":"print(\"Number of unique GL_Code in training dataset\", df_train['GL_Code'].nunique())\nprint(\"Number of unique GL_Code in test dataset\", df_test['GL_Code'].nunique())","8a05b628":"# Vendor code distribution\ndraw_distribution(df_train, df_test, 'Vendor_Code', 75)","a1b47ec4":"print(\"Number of unique Vendor_Code in training dataset\", df_train['Vendor_Code'].nunique())\nprint(\"Number of unique Vendor_Code in test dataset\", df_test['Vendor_Code'].nunique())","5c950927":"# Viewing Invoice amount distribution\nhist_data = [df_train['Inv_Amt'], df_test['Inv_Amt']]\ngroup_labels = ['training','testing'] # name of the dataset\nfig = ff.create_distplot(hist_data, group_labels, show_hist = False)\nfig.show()\n","e48f303f":"df_train.isnull().sum()","96099bf0":"df_test.isnull().sum()","d4e0e5fb":"for col in ['GL_Code','Vendor_Code']:\n    le = LabelEncoder()\n    le.fit(list(df_train[col])+ list(df_test[col]))\n    df_train[col] = le.transform(df_train[col])\n    df_test[col] = le.transform(df_test[col])","a19e7778":"X = df_train.drop(['Product_Category'], axis = 1)\ny = df_train['Product_Category']\ntarget = LabelEncoder()\ny_encoded = target.fit_transform(y)","af3be72d":"selected_features = ['GL_Code', 'Vendor_Code', 'Inv_Amt']\n","c1bdd8db":"# train test split\nX_train, X_valid, y_train, y_valid = train_test_split(X[selected_features], y_encoded, test_size = 0.3, random_state = 1)","3e6ab26b":"# parameters for xgboost\n\nparam = {}\n# use softmax multi-class classification\nparam['objective']= 'multi:softprob'\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['silent']=1\nparam['nthread'] = 4\nparam['num_class'] = len(target.classes_)\nparam['eval_metric'] = ['mlogloss']\nparam['seed'] = 1","2094262c":"dtrain = xgb.DMatrix(X_train.values, label = y_train)\ndvalid = xgb.DMatrix(X_valid.values, label = y_valid)\nevallist = [(dtrain,'train'), (dvalid, 'valid')]","330a615b":"clf = xgb.train(param, dtrain, 100, evallist, verbose_eval = 50)","0b964aa9":"y_pred_valid = clf.predict(dvalid)\nprint(\"Accuracy:\", accuracy_score(y_valid, np.argmax(y_pred_valid,axis =1)))","a092a18b":"xgb.plot_importance(clf, importance_type='gain')","e4c2b627":"# checking performance in test dataset\ndtest= xgb.DMatrix(df_test[selected_features].values)\ny_test_pred = clf.predict(dtest)","86488f34":"output = df_test[['Inv_Id']].copy()\noutput['Product_Category'] = target.inverse_transform(np.argmax(y_test_pred, axis = 1))","8e754f83":"output.head()","726fa949":"print(\"Total Product Categories : {}, Predicted Product Categories : {}\".format(len(target.classes_), output.Product_Category.nunique()))","873aafad":"output.to_csv(\".\/product_category_submission_selected_features.csv\", index=False)","51db098e":"stop_words = set(stopwords.words('english'))\n\ndef tokenize(text):\n    '''\n        Input: text\n        Returns: clean tokens\n        Desc:\n            Generates a clean token of text (words) by first getting words from the text.\n            Normalize the text by lowering it and removes the extra spaces, punctuation and stopwords.\n    '''    \n    txt =re.sub(\"[^A-Za-z]+\",\" \",text)\n    tokens = txt.split()\n    \n    clean_tokens = []\n    for tok in tokens:\n        if tok not in string.punctuation and tok not in stop_words:\n            clean_tokens.append(tok.lower().strip())\n    \n    return clean_tokens\n        ","fbf4d166":"tfidf = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1,2), use_idf = False, max_features = None)\n","5a7748d8":"tfidf.fit(X['Item_Description'])","4aed3185":"X_bow = tfidf.transform(X['Item_Description'])\nXTest_bow = tfidf.transform(df_test['Item_Description'])","3722da1d":"X_train, X_valid, y_train, y_valid = train_test_split(X_bow, y_encoded, test_size = 0.3, random_state=1)","36080ae2":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_valid, label=y_valid)\n\nevallist = [(dtrain, 'train'), (dvalid, 'eval')]\n\nclf = xgb.train(param, dtrain, 100, evallist, verbose_eval=50)","6c0b7e3f":"y_pred_valid = clf.predict(dvalid)\n\nprint(\"Accuracy : \",accuracy_score(y_valid, np.argmax(y_pred_valid, axis=1)))","46509c8d":"dtest = xgb.DMatrix(XTest_bow)\ny_test_pred = clf.predict(dtest)","bfdbf509":"output['Product_Category'] = target.inverse_transform(np.argmax(y_test_pred, axis=1))","535ed6a0":"print(\"Total Product Categories : {0} | predicted categories: {1} \"\n    .format(len(target.classes_), output['Product_Category'].nunique()))","1096d18e":"output.to_csv(\".\/product_category_submission_bow_features.csv\", index=False)","195ed552":"num_splits = 5\nskf = StratifiedKFold(n_splits = num_splits, random_state =1, shuffle = True)","55ba0203":"y_test_pred = np.zeros((df_test.shape[0], len(target.classes_)))\nprint(y_test_pred.shape)\ny_valid_scores =[]\nX = df_train['Item_Description']\nfold_cnt = 1\ndtest = xgb.DMatrix(XTest_bow)\n\nfor train_index, valid_index in skf.split(X, y_encoded):\n    print(\"\\n Fold...\", fold_cnt)\n    fold_cnt +=1\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y_encoded[train_index], y_encoded[valid_index]\n    \n    X_train_bow = tfidf.transform(X_train)\n    X_valid_bow = tfidf.transform(X_valid)\n    \n    dtrain = xgb.DMatrix(X_train_bow, label = y_train)\n    dvalid = xgb.DMatrix(X_valid_bow, label = y_valid)\n    \n    evallist = [(dtrain,'train'), (dvalid,'valid')]\n    clf = xgb.train(param, dtrain, 100, evallist, verbose_eval = 50)\n    \n    # Predicting on validation data\n    y_pred_valid = clf.predict(dvalid)\n    y_valid_scores.append(accuracy_score(y_valid, np.argmax(y_pred_valid, axis =1)))\n    \n    # predict on test set\n    y_pred= clf.predict(dtest)\n    \n    y_test_pred +=y_pred","d2836c8c":"print(\"Validation Scores :\", y_valid_scores)\nprint(\"Average Score: \",np.round(np.mean(y_valid_scores),3))","4250c584":"\ny_test_pred \/= num_splits","ca7c798a":"output['Product_Category'] = target.inverse_transform(np.argmax(y_test_pred, axis=1))\nprint(\"Total Product Categories : {0} | predicted categories: {1} \"\n    .format(len(target.classes_), output['Product_Category'].nunique()))","90487338":"output.to_csv(\".\/product_category_submission_tfidf_oof.csv\", index=False)","9d7790fb":"# Exploratory data analysis","66e64434":"There are no missing values in train and test dataset","9cb2dd8e":"From above plot, some of the vendor codes from train dataset are missing in test and viceversa","f70ec7fb":"# Missing Values","936cba57":"# Creating feature matrix X and target y:","5c380354":"The 3 features used are GL_Code, Vendor_Code and Inv_Amt. As expected, Inv_Amt has very low importance as it is uniformly distributed and not adding much value","6d6b2762":"# Numerical encoding of categorical features, GL_Code and Vendor_Code","a34c0f81":"# Reference:\nhttps:\/\/www.kaggle.com\/shobhitupadhyaya\/edgeverve-ml-challenge-solution\n","19e5466a":"# Model 2 (BOW Features):\n\n![image.png](attachment:image.png)","c1896cf5":"# Loading dataset","547b1eb5":"AS we can see in above plot, the distribution of Invoice Amount is uniform and the probability of occurance of any value is almost equal which indicates this column doesn't add much value to our analysis","c4d6babe":"Distribution of GL_Code is pretty much same in both train and test dataset","5eead61f":" # Model_1 with selected features:\n![image.png](attachment:image.png)\n","a9cff176":"# Importing Libraries","5ac400ed":"# Model 3 (OOF Prediction):\n\n![image.png](attachment:image.png)","05adc084":"We could see that the class distribution is uneven. "}}