{"cell_type":{"240f89c1":"code","74fd2094":"code","9e92e75b":"code","80159853":"code","4f792220":"code","bc0cd789":"code","a720ab5e":"code","825d98d1":"code","e07dc63a":"code","7d5e262f":"code","eba64f26":"code","413fc5e3":"code","46397c64":"code","6b7aff9c":"code","4a321a89":"code","8e64a44a":"code","124475b0":"code","302db8f8":"code","9013d354":"code","de53fabe":"code","1e6f6d4b":"code","0db3bd76":"code","f61cc6af":"code","35c68152":"code","f51a6d6b":"markdown","d7e84d8f":"markdown","e7c4b1e5":"markdown","e7d07cf6":"markdown","fc4cf96c":"markdown","3f3d92fd":"markdown"},"source":{"240f89c1":"########################################################################\n# This notebook aims to do EDA and develope a model on Instagram Reach #\n########################################################################\n\n#################################\n#  Contents:                    #\n#################################\n# Part 1.) EDA                  #\n# part 2.) Modelling using      #\n#          2 hypothetical Cases #\n#################################","74fd2094":"############################################################################\n# Part 1.) Expolatory Data Analysis                                        #\n############################################################################","9e92e75b":"# making Some essential imports\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sb\nimport tensorflow as tf\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS \nstopwords = set(STOPWORDS) \nstopwords.add('will')\nimport re","80159853":"# loading and checking data\ndataFrame = pd.read_csv('\/kaggle\/input\/instagram-reach\/instagram_reach.csv')\ndataFrame.info()","4f792220":"dataFrame[0:10]","bc0cd789":"# lets know what people in this data set are talking about\ndef WordCloudPlotter(dfColumn):\n    colData = dataFrame[dfColumn]\n    textCloud = ''\n    \n    #text processing\n    # converting colums to a \n    #single line of text\n    for mem in colData:\n        textCloud = textCloud + str(mem)\n    \n    # plotting word cloud\n    wordcloud = WordCloud(width = 800, height = 800,background_color ='grey', \n                          stopwords = stopwords,  min_font_size = 10).generate(textCloud)\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.rcParams.update({'font.size': 25})\n    plt.axis(\"off\") \n    plt.title('Word Cloud: ' + str(dfColumn))\n    plt.tight_layout(pad = 0) \n  \n    plt.show() \n    ","a720ab5e":"WordCloudPlotter('Caption')","825d98d1":"# lets do something similar with hashtags\nWordCloudPlotter('Hashtags')","e07dc63a":"# removing hours and typecasting to int\ndataFrame['Time since posted'] = dataFrame['Time since posted'].map(lambda a: int(re.sub('hours', '', a)))","7d5e262f":"# Plotting Likes relationship with\n# Followers and Hours since posted\ndef PlotDataGiveConclusion2 (featureVector):\n    plt.figure(figsize= (20, 10))\n    \n    pltNum = 1\n    for mem in featureVector:\n        plt.subplot(1, 2 , pltNum)\n        plt.grid(True)\n        plt.title('Regplot Plot for '+ str(mem))\n        sb.regplot(data = dataFrame, x = mem, y = 'Likes' , color = 'green')\n        pltNum += 1\n    \n    plt.show()","eba64f26":"PlotDataGiveConclusion2(['Followers', 'Time since posted'])","413fc5e3":"##############################################################################\n# Part 2.) Predictive Modelling                                              #\n##############################################################################","46397c64":"# lets us now develope a prediction model\n# in the given data set we can have 2 features\n# namely followers and time Posted and our \n# target be number of likes.\n\n# Note since this data set pretains to data science community\n# converting hashtags into features,I belive would not be \n# that great of an idea\n\n# gathering features\nfeatureVector = np.array(dataFrame[['Followers', 'Time since posted']], dtype = 'float32')\ntargets = np.array(dataFrame['Likes'], dtype = 'float32')\nmaxValLikes = max(targets)\nprint('Max value of target is {}'.format(maxValLikes))\n","6b7aff9c":"#diving targets by max values\ntargets = targets\/maxValLikes","4a321a89":"# doing standard stuff now\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nxTrain, xTest, yTrain, yTest = train_test_split(featureVector, targets, test_size = 0.1, random_state = 42)\n\nstdSc = StandardScaler()\nxTrain = stdSc.fit_transform(xTrain)\nxTest = stdSc.transform(xTest)","8e64a44a":"# for sake of conviniece \n# lets choose our model as \n# Gradeint Boosting Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(xTrain, yTrain)","124475b0":"# doing Some Benchmarks\npredictions = gbr.predict(xTest)\nplt.scatter(yTest, predictions)\nplt.xlabel('true values')\nplt.ylabel('predicted values')\nplt.title('GradientRegressor')\nplt.plot(np.arange(0,0.4, 0.01), np.arange(0, 0.4, 0.01), color = 'green')\nplt.grid(True)","302db8f8":"def PredictionsWithConstantFollowers(model, followerCount, scaller, maxVal):\n    followers = followerCount * np.ones(24)\n    hours = np.arange(1, 25)\n    \n    # defining vector \n    featureVector = np.zeros((24, 2))\n    featureVector[:, 0] = followers\n    featureVector [:, 1] = hours\n    \n    # doing scalling\n    featureVector = scaller.transform(featureVector)\n    predictions = model.predict(featureVector)\n    predictions = (maxValLikes * predictions).astype('int')\n    \n    plt.figure(figsize= (10, 10))\n    plt.plot(hours, predictions)\n    plt.scatter(hours, predictions, color = 'g')\n    plt.grid(True)\n    plt.xlabel('hours since posted')\n    plt.ylabel('Likes')\n    plt.title('Likes progression with ' + str(followerCount) +' followers')\n    plt.show()","9013d354":"PredictionsWithConstantFollowers(gbr, 100, stdSc, maxValLikes)","de53fabe":"## let double the Number of Followers\nPredictionsWithConstantFollowers(gbr, 200, stdSc, maxValLikes)","1e6f6d4b":"## let 4X the Number of Followers\nPredictionsWithConstantFollowers(gbr, 400, stdSc, maxValLikes)","0db3bd76":"## let 10X the Number of Followers\nPredictionsWithConstantFollowers(gbr, 1000, stdSc, maxValLikes)","f61cc6af":"def PredictionsWithVariableFollowers(model, followerCount, scaller, maxVal):\n    followers = np.arange(100, 2500, 100)\n    hours = np.arange(1, 25)\n    \n    # defining vector \n    featureVector = np.zeros((24, 2))\n    featureVector[:, 0] = followers\n    featureVector [:, 1] = hours\n    \n    # doing scalling\n    featureVector = scaller.transform(featureVector)\n    predictions = model.predict(featureVector)\n    predictions = (maxValLikes * predictions).astype('int')\n    \n    plt.figure(figsize= (10, 10))\n    plt.plot(hours, predictions)\n    plt.scatter(hours, predictions, color = 'g')\n    plt.grid(True)\n    plt.xlabel('hours since posted')\n    plt.ylabel('Likes')\n    plt.title('Likes progression with variable followers')\n    plt.show()","35c68152":"PredictionsWithVariableFollowers(gbr, 1000, stdSc, maxValLikes)","f51a6d6b":"From Benchmarks this seems the model though not perfect\nis still workable. Lets do some Predictive modelling on the following cases:\n\nCase 1.) You have 100 followers and time passes by one hour for 24 hours\n\ncase 2.) You have 100 followers and you Can 100 follower every hour for 24 hours","d7e84d8f":"According to this model we can infer one thing, that is, If you have higher number of followers your post will\ngain more Likes early in its life time, but maximum likes doesnt increase that much, here 20X the number followers\nyeild only about 20% increase in likes","e7c4b1e5":"This Word Cloud indicates the following:\n\n\n1.) This data sets is collected from data sceince enthusiasts.\n\n2.) It is emphasizing on the need of AI-ML.\n\n3.) Fields of applications are being talked about\n","e7d07cf6":"Looking at the data set lets try to do the following:\n\n\n1.) Plotting word clouds for Captions and hashTags.\n\n2.) developing a relationship between followers and likes.\n\n3.) Time posted and Likes","fc4cf96c":"If our predictive model is right, and case 2 is true\nthen around 15 hours posts like will grow exponentially !! (In my opnion I dont believe so)","3f3d92fd":"Thanks, If you have reached here and read the entire Notebook\nAlso, Todo In, future versions Replacing single GBR model with an\nessembled model\n"}}