{"cell_type":{"0b24068f":"code","d8e26656":"code","e3eb7a9e":"code","32fff65b":"code","4b90c57c":"code","16a1574d":"code","953838cb":"code","cb1c4799":"code","5564e9d1":"code","3b1dafa8":"code","e32cac79":"code","40a1c5a3":"code","3a8cd71b":"code","5d3867e8":"code","59f3a5e1":"code","ebdd5176":"code","6e42124e":"code","c1732940":"code","1ef94d6a":"code","eca92e64":"code","5727d852":"code","33561c3d":"code","897b477a":"code","9eba14f0":"markdown","118226d1":"markdown","42d0d794":"markdown","bfd98c60":"markdown","a1862959":"markdown","db6038bc":"markdown","77fd4623":"markdown","55820c33":"markdown","68c151dc":"markdown","86888f6f":"markdown","d5ced255":"markdown","98f07deb":"markdown","8e95d026":"markdown"},"source":{"0b24068f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# for file downloading\nimport requests\nimport zipfile \nimport os\nimport json\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import TransformerMixin\n        \npd.options.mode.chained_assignment = None\nrandom_seed = 13579","d8e26656":"def download_file(url):\n  \"given a url, download the contents and return the file path\"\n\n  r = requests.get(url, stream=True)\n  save_path = url.split('\/')[-1]\n\n  # download contents\n  with open(save_path, 'wb') as f: \n    for chunk in r.iter_content(chunk_size=128):\n      f.write(chunk)\n\n  # check if it is zip file\n  if 'zip-compressed' in r.headers['content-type']:\n    # unzip contents \n    with zipfile.ZipFile(save_path, 'r') as zip:\n      zip_names = zip.namelist()  # get names \n      zip.extractall('.')\n      \n    # remove zip \n    os.remove(f'.\/{save_path}')\n    return zip_names\n    \n  else: \n    return [save_path]","e3eb7a9e":"# first we download the data\nwith open('\/kaggle\/input\/dl-links\/dl_links.json') as f:\n  dl_links = json.load(f)\n\n# file names \ncsv_names = []\nfor link in dl_links:\n    save_path = download_file(link)\n    try: \n        csv_names += save_path\n    except: \n        continue\n    \nprint(csv_names)","32fff65b":"casualties = pd.read_csv('Road Safety Data - Casualties 2019.csv', dtype={'Accident_Index': str})\naccidents  = pd.read_csv('Road Safety Data - Accidents 2019.csv', index_col='Accident_Index')\n\n# lower the columns names\ncasualties.columns = list(map(lambda x: x.lower(), casualties.columns))\naccidents.columns  = list(map(lambda x: x.lower(), accidents.columns))\n\nprint(\"Accidents DF has \", len(accidents.index), \"Accidents Index\")\nprint(\"Casualty  DF has \", len(casualties['accident_index'].value_counts()), \"Accidents Index\")","4b90c57c":"print(accidents.columns.values)","16a1574d":"## define our variables type \nnum_vars = ['longitude', 'latitude']\ncat_vars = [\n    'day_of_week',\n    '1st_road_class', 'road_type', 'speed_limit', 'junction_detail', \n    'junction_control', '2nd_road_class', 'pedestrian_crossing-human_control',\n    'pedestrian_crossing-physical_facilities', 'light_conditions',\n    'weather_conditions', 'road_surface_conditions',\n    'special_conditions_at_site', 'urban_or_rural_area',\n]\nall_vars = cat_vars + num_vars","953838cb":"# create datatime feature \ndate_feature = (accidents['date']+'-'+accidents['time']).apply(str)\naccidents['timestamp'] = pd.to_datetime(date_feature, format='%d\/%m\/%Y-%H:%M')","cb1c4799":"## a helper function\ndef categorical_summary(df_cat: pd.DataFrame, index_col=None, show=False) -> dict:\n    \"Returns the unique values of the the Dataframe column\"\n    results = {}\n    index_col = index_col if index_col else df_cat.columns[0]\n\n    # create values for each of the categorical columns\n    for column in df_cat:\n      result = df_cat.groupby(by=column, dropna=False).agg(\n          count=pd.NamedAgg(index_col, aggfunc='count'))\n\n      # calculate the percentage\n      result['proportion'] = round(result['count']\/result['count'].sum(), 4)\n\n      # append to the result list\n      results[column] = result\n\n    return results","5564e9d1":"# retrieve the counts for categorical variables\ncat_summaries = categorical_summary(accidents[cat_vars])\n\n# overview of the missing values\nprint(\">>> Missing Values Proportion <<<\")\nfor col in cat_vars:\n    try:\n        print(col, \" : \", cat_summaries[col].loc[-1, 'proportion'])\n    except KeyError:\n        continue","3b1dafa8":"# highlighting the 2nd class == -1 and junction details == 0\nno_junction_ss = accidents['junction_detail'] == 0\nprint(\">>> 2nd Road Class (given there is no Junction near the 1st road) <<<\")\nprint(accidents[no_junction_ss].groupby('2nd_road_class').count()['junction_detail'], end='\\n\\n')\n\n# highlighting the junction control == -1 and junction details == 0\nprint(\">>> Junction Control (given there is no Junction near the 1st road) <<<\")\nprint(accidents[no_junction_ss].groupby('junction_control').count()['junction_detail'])\n\n# changing the co-occurences to 0 instead of making them as nan\nrelabel_2nd_road = (no_junction_ss) & (accidents['2nd_road_class'] == -1)\nrelabel_junction = (no_junction_ss) & (accidents['junction_control'] == -1)\n\naccidents.loc[relabel_2nd_road, '2nd_road_class'] = 0\naccidents.loc[relabel_junction, 'junction_control'] = 0","e32cac79":"accidents.replace(-1, np.nan, inplace=True)\naccidents.dropna(inplace=True)\n\naccidents.info()","40a1c5a3":"accidents['weather_conditions'].hist()","3a8cd71b":"def same_location_consistency_check(arr):\n    \"\"\"\n    An Aggregation Function to check if the same location\n    reported multiple different conditions within an hour.\n    \"\"\"\n    uniques = arr.unique() # retreive unique values of a agg subset\n    if len(uniques) > 1:\n        # set as nan if hourly data are not consistent\n        return np.nan\n    else: \n        return int(uniques[0])\n    \n    \ndef weighted_num_accident(arr, weights={1: 3, 2: 2, 3: 1}):\n    \"\"\" \n    Calculate the weighted number of accidents\n    weights: are the relative resources required to handle an accident\n    \"\"\"\n    for severity_label, weight in weights.items():\n        # find the accident with severity level\n        idx = np.where(arr == severity_label)\n        # adjust the weighted values \n        arr.iloc[idx] = weight\n    # sum up all accidents according their weighted level\n    return arr.sum()\n\n# we would aggregate each unique data as the following: \naccidents_hourly = accidents.groupby([\n    pd.Grouper(key='timestamp', freq='H'),\n    # below are columns that help to narrow down the smallest \n    # unit of unique road section\n    'local_authority_(highway)',\n#     '1st_road_number',\n    'lsoa_of_accident_location']).agg({\n        # target feature aggregation\n        'number_of_casualties': 'count',\n        'accident_severity': weighted_num_accident,\n        # below are categorical features grouped into hourly values\n        **{var: same_location_consistency_check for var in cat_vars}}\n).rename({\n    'accident_severity': 'weighted_num_accidents',\n    'number_of_casualties': 'accidents_raw_count'\n}).reset_index().dropna()","5d3867e8":"accidents_hourly.head()","59f3a5e1":"accidents_hourly['junction_detail'].hist()\n\n# conduct a stratify train test split \nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(\n    accidents_hourly, \n    test_size=0.2, random_state=random_seed, \n    stratify=accidents_hourly['junction_detail'])","ebdd5176":"# train_set['hour'] = train_set['timestamp'].dt.hour\n\n# ## Accidents by the hour of time\n# num_casualties_by_hour = train_set.groupby('hour').mean()['number_of_casualties']\n\n# # graph the data\n# plt.bar(num_casualties_by_hour.index, num_casualties_by_hour.values)\n# plt.xticks(num_casualties_by_hour.index)\n# plt.xlabel('Hours')\n# plt.ylabel('Total number of Casualties')\n# plt.show()\n\ntrain_set = train_set[~train_set['weather_conditions'].isin([8, 9])]","6e42124e":"## Accidents by the hour of time\nnum_casualties_by_dow = test_set.groupby('day_of_week').mean()['number_of_casualties']\n\n# graph the data\nplt.bar(num_casualties_by_dow.index, num_casualties_by_dow.values)\nplt.xticks(num_casualties_by_dow.index)\nplt.xlabel('Days of Weel')\nplt.ylabel('Total number of Casualties')\nplt.show()","c1732940":"(4 - train_set['accident_severity']) * train_set['number_of_casualties']","1ef94d6a":"## Accidents by the hour of time\nnum_casualties_by_road = test_set.groupby('day_of_week').mean()['number_of_casualties']\n\n# graph the data\nplt.bar(num_casualties_by_road.index, num_casualties_by_road.values)\nplt.xticks(num_casualties_by_road.index)\nplt.xlabel('Road Type')\nplt.ylabel('Total number of Casualties')\nplt.show()","eca92e64":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion","5727d852":"# Create preprocessing pipeline \npreprocessing_pipeline = ColumnTransformer([\n    ('onehot', OneHotEncoder(), cat_vars)\n])\ntrain_X = preprocessing_pipeline.fit_transform(train_set)","33561c3d":"# pd.DataFrame({i: val for i, val in enumerate(train_X.T)})\n# pd.DataFrame.sparse.from_spmatrix(train_X)\ntrain_X.shape\n# preprocessing_pipeline.transformers_[0][1].get_feature_names(list(accidents))\npreprocessing_pipeline.transform(test_set)","897b477a":"train_set.head()","9eba14f0":"Here we could see that at the hour of 15, 16, 17 have the highest amount of casualties. ","118226d1":"This section would download the data required ","42d0d794":"## 3. Removing Unhelpful labels","bfd98c60":"## 2. Data Screening\nWe will first do a quick data screening before spliting the training and test sets.  ","a1862959":"Next, we would also conduct some screening on the missing values. Below displays the missing value proportion of each categorical variables. We could see that variables `junction_control` and `2nd_road_class` have unusal proportion of missing values. We might need to check why this occured.","db6038bc":"## Setting up Target Variables\nNow the full dataset is clean with missing values. We will agggregate the dataset on hourly interval and create the target variables (# casualties per hour for road section). Since we are aggregating the accidents details into an hourly interval for each unique road section. To define a unique hourly record, we would use the below variables:  \n1. Using local_authority classes, road numbers, etc to help us define a approximation of an unique road section\n2. group the data by hourly basis for each unique datetime\n3. sum up the number of accidents and weighted by the severity. The severity is assumed as the following and could be adjusted if there are other experts opinions.\n    - Slight Accidents: take 1 unit of resources to handle.\n    - Serious Accidents: take 2 units to handle\n    - Fatal Accidents: take 3 units to handle\n4. set data that are inconsistent as invalid\n\nFor step 4, we need to make some consistency check. For instance, given a unique section of road in an hour of a particular day, the conditions should be consistent within this hour. If that is not the case, we would remove this particular data.","77fd4623":"## Handling coded features","55820c33":"## 3.Train Test Split\nNow we will proceed to train test split. Base on [accident review from ROSPA](https:\/\/www.rospa.com\/rospaweb\/docs\/advice-services\/road-safety\/road-crashes-overview.pdf), junctions' design is one of the key factor of road accident in UK. With regard of this, we would sample our dataset with respect to `junction detail`, such that both train and test set would have a similar distribution of this ","68c151dc":"1. `num_vars` are features that have numerical values\n2. `static_vars` are features would not change when summarizing them into hourly basis. For example, features that related to a particular section of road would not change over time. ","86888f6f":"Now that most of the special cases of missing values are dealt with. We could drop all those records with missing value (-1). We won't be imputing any of the categorical values.","d5ced255":"## 1. Generate Dataframes","98f07deb":"There are some date time elements which are broken down into different columns. We could join them into a timestamp for future use.","8e95d026":"Here we see that lots of values in `2nd_road_class` and `junction_control` are marked as missing values simply because there is no junction nearby. We could relabel those value as 0, which is suggested by the data description. Instead of relabelling all of those, we would only relabel those that match the joint condition since it is possible that some are indeed missing values and we don't have much information to conclude it."}}