{"cell_type":{"21c4e051":"code","039757e5":"code","4bd2c8fc":"code","2e2bedf1":"code","b737620b":"code","f582d63f":"code","ee3d0f30":"code","6ae80c6f":"code","628ddeed":"code","7df1c568":"code","c7c0d110":"code","75b3bbf1":"code","26e6cf42":"code","9af7aa15":"code","fe2f7e95":"code","f66792f2":"code","b089250c":"code","eeaf92a6":"code","7a0a0783":"markdown","89afa864":"markdown"},"source":{"21c4e051":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","039757e5":"!pip install python-vivid","4bd2c8fc":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom vivid.estimators.base import MetaBlock\nfrom vivid.estimators.boosting.mixins import BoostingEarlyStoppingMixin\n\nfrom catboost import CatBoostRegressor","2e2bedf1":"from vivid.features.base import BinningCountBlock\nfrom vivid.features.base import CountEncodingBlock\nfrom vivid.features.base import FilterBlock\n\nfrom vivid.estimators.boosting import XGBRegressorBlock\nfrom vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.estimators.boosting.block import create_boosting_seed_blocks\n\nfrom vivid.estimators.linear import TunedRidgeBlock\nfrom vivid.estimators.svm import SVRBlock\nfrom vivid.estimators.ensumble import RFRegressorBlock\nfrom vivid.estimators.base import EnsembleBlock, BaseBlock","b737620b":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n\nfeature_columns = [\n 'cont1',\n 'cont2',\n 'cont3',\n 'cont4',\n 'cont5',\n 'cont6',\n 'cont7',\n 'cont8',\n 'cont9',\n 'cont10',\n 'cont11',\n 'cont12',\n 'cont13',\n 'cont14',    \n]\n\ny = train_df['target'].values","f582d63f":"from sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\n\nfrom vivid.core import BaseBlock\n\n\nclass PCABlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='pca_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = PCA(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.transform(source_df[self.columns])\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('PCA_')\n    \n\nclass GaussianMixtureBlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='GMM_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = GaussianMixture(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.predict_proba(source_df[self.columns])\n        z = np.clip(z, 1e-6,1 - 1e-6)\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('GMM_')","ee3d0f30":"from vivid.estimators.boosting.mixins import TunedBoostingBlock\nfrom vivid.estimators.boosting.helpers import get_boosting_parameter_suggestions\nfrom vivid.estimators.boosting.lgbm import LGBMRegressorBlock\nimport lightgbm as lgbm\nfrom vivid.runner import create_runner","6ae80c6f":"class TunedLightGBMRegressorBlock(TunedBoostingBlock):\n    model_class = lgbm.LGBMRegressor\n    default_eval_metric = 'rmse'\n    initial_params = LGBMRegressorBlock.initial_params\n    \n    def generate_model_class_try_params(self, trial):\n        param = get_boosting_parameter_suggestions(trial)\n        param['n_jobs'] = -1\n        return param","628ddeed":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nrunner = create_runner(blocks=[\n    # normal lightGBM\n    LGBMRegressorBlock(name='normal_lgbm', parent=feature_blocks),\n    \n    # tuned by optuna. 50 rounds.\n    TunedLightGBMRegressorBlock(name='tuned_lgbm', parent=feature_blocks, n_trials=50)\n])","7df1c568":"oof_results = runner.fit(train_df[feature_columns], y=y)","c7c0d110":"# predict\ntest_results = runner.predict(test_df)","75b3bbf1":"# create out-of-fold overview\noof_df = pd.DataFrame()\n\nfor result in oof_results:\n    oof_df[result.block.name] = result.out_df.values[:, 0]","26e6cf42":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(oof_df.corr(), cmap='Blues', annot=True, fmt='.2f', ax=ax)\nax.set_title('Out of Fold Correlation')","9af7aa15":"fig, ax = plt.subplots(figsize=(10, 6))\n\nfor result in oof_results:\n    sns.distplot(result.out_df.values[:, 0], ax=ax, label=str(result.block.name))\n\nax.legend()\nfig.tight_layout()","fe2f7e95":"from vivid.metrics import regression_metrics\n\nscore_df = pd.DataFrame()\n\nfor name, pred in oof_df.T.iterrows():\n    score_i = regression_metrics(y, pred)\n    score_df = score_df.append(pd.Series(score_i, name=name))","f66792f2":"score_df.sort_values('rmse')","b089250c":"sample_submission_df","eeaf92a6":"for result in test_results:\n    out_df = result.out_df\n    \n    sub_df = sample_submission_df.copy()\n    sub_df['target'] = result.out_df.values[:, 0]\n    to = f'\/kaggle\/working\/{str(result.block.name)}_submission.csv'\n    print('save to {}'.format(to))\n    sub_df.to_csv(to, index=False)","7a0a0783":"### Visualize Models\n\n* Model Output Correation\n* OOf Distribution\n* sort by RMSE","89afa864":"### Run Tuning \n\n* only fit"}}