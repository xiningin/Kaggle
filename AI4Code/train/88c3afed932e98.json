{"cell_type":{"1b2cf3ab":"code","6961840c":"code","9a107cdd":"code","e2d3cb52":"code","dcc1f27a":"code","829e2650":"code","aaf48d89":"code","aeeddc21":"code","039af873":"code","bdc6d7b8":"code","f304a858":"code","faaa9558":"code","0a508d92":"code","6f761ce2":"code","709afc14":"code","b0cf1131":"code","65f7845b":"code","89294afa":"code","6c3e9d42":"code","ba591520":"code","77dff128":"code","26c36f43":"code","f81faef5":"code","c49b8460":"code","8e655335":"code","74b9d9c2":"code","8da148ae":"code","9427f951":"code","9035ab88":"code","cdcfbe1c":"code","0bbfd25d":"code","43e020dd":"code","385822d0":"code","3031e460":"code","ffaf66ff":"markdown","ab6cb226":"markdown","6306a67d":"markdown","9d9a634c":"markdown","bd844a46":"markdown","7e7174a2":"markdown","2ac83866":"markdown","b3c31ffa":"markdown","acc2f9bc":"markdown","40c5cf5d":"markdown","07b27f04":"markdown","bc990460":"markdown","7e517573":"markdown","5d8dbd0e":"markdown","1b0b464a":"markdown","54920e05":"markdown","665a5903":"markdown","c595877d":"markdown","4d13040d":"markdown","228da926":"markdown","bd48a30c":"markdown","efbc0477":"markdown","be203b24":"markdown"},"source":{"1b2cf3ab":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","6961840c":"%matplotlib inline\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\nimport random\npylab.rcParams['figure.figsize'] = (8.0, 10.0)# Import Libraries\n\n# For visualization\nimport os\nimport seaborn as sns\nfrom matplotlib import colors\nfrom tensorboard.backend.event_processing import event_accumulator as ea\nfrom PIL import Image","9a107cdd":"# I am visualizing some images in the 'val\/' directory\n\ndataDir='..\/input\/coco-car-damage-detection-dataset\/val'\ndataType='COCO_val_annos'\nmul_dataType='COCO_mul_val_annos'\nannFile='{}\/{}.json'.format(dataDir,dataType)\nmul_annFile='{}\/{}.json'.format(dataDir,mul_dataType)\nimg_dir = \"..\/input\/coco-car-damage-detection-dataset\/img\"","e2d3cb52":"# initialize coco api for instance annotations\ncoco=COCO(annFile)\nmul_coco=COCO(mul_annFile)","dcc1f27a":"# display categories and supercategories\n\n#Single Class #Damage dataset\ncats = coco.loadCats(coco.getCatIds())\nnms=[cat['name'] for cat in cats]\nprint('COCO categories for damages: \\n{}\\n'.format(', '.join(nms)))\n\nnms = set([cat['supercategory'] for cat in cats])\nprint('COCO supercategories for damages: \\n{}\\n'.format(', '.join(nms)))\n\n#Multi Class #Parts dataset\n\nmul_cats = mul_coco.loadCats(mul_coco.getCatIds())\nmul_nms=[cat['name'] for cat in mul_cats]\nprint('COCO categories for parts: \\n{}\\n'.format(', '.join(mul_nms)))\n\nmul_nms = set([mul_cat['supercategory'] for mul_cat in mul_cats])\nprint('COCO supercategories for parts: \\n{}\\n'.format(', '.join(mul_nms)))","829e2650":"# get all images containing 'damage' category, select one at random\ncatIds = coco.getCatIds(catNms=['damage']);\nimgIds = coco.getImgIds(catIds=catIds );","aaf48d89":"random_img_id = random.choice(imgIds)\nprint(\"{} image id was selected at random from the {} list\".format(random_img_id, imgIds))","aeeddc21":"# Load the image\nimgId = coco.getImgIds(imgIds = [random_img_id])\nimg = coco.loadImgs(imgId)[0]\nprint(\"Image details \\n\",img)","039af873":"I = io.imread(img_dir + '\/' + img['file_name'])\nplt.axis('off')\nplt.imshow(I)\nplt.show()","bdc6d7b8":"#get damage annotations\nannIds = coco.getAnnIds(imgIds=imgId,iscrowd=None)\nanns = coco.loadAnns(annIds)\n","f304a858":"#Plot damages\nplt.imshow(I)\nplt.axis('on')\ncoco.showAnns(anns, draw_bbox=True )","faaa9558":"#get parts annotations\nmul_annIds = mul_coco.getAnnIds(imgIds=imgId,iscrowd=None)\nmul_anns = mul_coco.loadAnns(mul_annIds)","0a508d92":"# Create a dictionary between category_id and category name\ncategory_map = dict()\n\nfor ele in list(mul_coco.cats.values()):\n    category_map.update({ele['id']:ele['name']})","6f761ce2":"category_map","709afc14":"#Create a list of parts in the image\nparts = []\nfor region in mul_anns:\n    parts.append(category_map[region['category_id']])\n\nprint(\"Parts are:\", parts) \n\n#Plot Parts\nI = io.imread(img_dir + '\/' + img['file_name'])\nplt.imshow(I)\nplt.axis('on')\nmul_coco.showAnns(mul_anns, draw_bbox=True )","b0cf1131":"# Install detectron 2\n!python -m pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html","65f7845b":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","89294afa":"assert torch.__version__.startswith(\"1.7\")","6c3e9d42":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\nimport skimage.io as io\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Set base params\nplt.rcParams[\"figure.figsize\"] = [16,9]","ba591520":"# To find out inconsistent CUDA versions, if there is not \"failed\" word in this output then things are fine.\n!python -m detectron2.utils.collect_env","77dff128":"\ndataset_dir = \"..\/input\/coco-car-damage-detection-dataset\"\nimg_dir = \"img\/\"\ntrain_dir = \"train\/\"\nval_dir = \"val\/\"","26c36f43":"from detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"car_dataset_train\", {}, os.path.join(dataset_dir,train_dir,\"COCO_train_annos.json\"), os.path.join(dataset_dir,img_dir))\nregister_coco_instances(\"car_dataset_val\", {}, os.path.join(dataset_dir,val_dir,\"COCO_val_annos.json\"), os.path.join(dataset_dir,img_dir))","f81faef5":"dataset_dicts = DatasetCatalog.get(\"car_dataset_train\")\nmetadata_dicts = MetadataCatalog.get(\"car_dataset_train\")","c49b8460":"#Implementing my own Trainer Module here to use the COCO validation evaluation during training\n# TODO: add data custom augmentation \nclass CocoTrainer(DefaultTrainer):\n\n  @classmethod\n  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n\n    if output_folder is None:\n        os.makedirs(\"coco_eval\", exist_ok=True)\n        output_folder = \"coco_eval\"\n\n    return COCOEvaluator(dataset_name, cfg, False, output_folder)","8e655335":"\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"car_dataset_train\",)\ncfg.DATASETS.TEST = (\"car_dataset_val\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\") # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.001 # pick a good LR\ncfg.SOLVER.WARMUP_ITERS = 800\ncfg.SOLVER.MAX_ITER = 1600 #adjust up if val mAP is still rising, adjust down if overfit\ncfg.SOLVER.STEPS = (600, 1550)\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # faster, and good enough for this dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\ncfg.MODEL.RETINANET.NUM_CLASSES = 1\ncfg.TEST.EVAL_PERIOD = 600\n\n\n\n\n# Clear any logs from previous runs\n#TODO add timestamp to logs\n!rm -rf cfg.OUTPUT_DIR\n\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","74b9d9c2":"# Look at training curves in tensorboard:\n# %reload_ext tensorboard\n# %tensorboard --logdir .\/output","8da148ae":"def smooth(scalars, weight=0.6):\n    \"\"\"\n    Reference: https:\/\/github.com\/plotly\/dash-live-model-training\/blob\/master\/app.py#L163\n    \"\"\"\n    last = scalars[0]\n    smoothed = list()\n    for point in scalars:\n        smoothed_val = last * weight + (1 - weight) * point\n        smoothed.append(smoothed_val)\n        last = smoothed_val\n    return smoothed\n\n\ndef plot(logdir: str, savedir: str, smoothing: float = 0.6, no_title=False, no_legend=False, no_axis_labels=False):\n    \"\"\" re-draw the tf summary events plots  using seaborn\n    :param logdir: Path to the directory having event logs\n    :param savedir: Path to save the seaborn graphs\n    :param smoothing: smoothing window space for the plots\n    \"\"\"\n    assert 0 <= smoothing <= 1, 'Smoothing value should be in [0,1]'\n    \n    plots = []\n    \n    sns.set(style=\"darkgrid\")\n    sns.set_context(\"paper\")\n\n    # Collect data\n    # we recognize all files which have tfevents\n    scalars_info = {}\n    for root, dirs, files in os.walk(logdir):\n        for event_file in [x for x in files if 'tfevents' in x]:\n            event_path = os.path.join(root, event_file)\n\n            acc = ea.EventAccumulator(event_path)\n            acc.Reload()\n\n            # only support scalar now\n            scalar_list = acc.Tags()['scalars']\n            for tag in scalar_list:\n                x = [s.step for s in acc.Scalars(tag)]\n                y = [s.value for s in acc.Scalars(tag)]\n                data = {'x': x, 'y': y, 'legend': root.split(logdir)[1][1:] if root != logdir else None}\n                if tag not in scalars_info:\n                    scalars_info[tag] = [data]\n                else:\n                    scalars_info[tag].append(data)\n\n    # We recognize groups assuming each group name has \/\n    # And, each group is saved in a separate directory\n    for tag, tag_data in scalars_info.items():\n        _split = tag.split('\/')\n        if len(_split) <= 1:\n            _path = os.path.join(savedir, 'seaborn')\n            _name = _split[0]\n        else:\n            _path = os.path.join(savedir, 'seaborn', _split[0])\n            _name = ''.join(_split[1:])\n\n        os.makedirs(_path, exist_ok=True)\n\n        color_list = list(sns.color_palette(palette='dark', n_colors=len(tag_data)))[::-1]\n        for data in tag_data:\n            x, y = data['x'], data['y']\n            y_smooth = smooth(y, weight=smoothing)\n            current_color = color_list.pop()\n            _plt = sns.lineplot(x, y, color=colors.to_rgba(current_color, alpha=0.4))\n            _legend = data['legend'] if not no_legend else None\n            _plt = sns.lineplot(x, y_smooth, label=data['legend'], color=current_color)\n\n        if not no_axis_labels:\n            _plt.set(xlabel='x', ylabel='y')\n        if not no_title:\n            _plt.set_title(_name.capitalize())\n        \n        plots.append(os.path.join(_path, _name + '.png'))\n        plt.savefig(os.path.join(_path, _name + '.png'))\n        plt.clf()\n    return plots","9427f951":"plots = plot(logdir= '.\/output', savedir= '.\/')","9035ab88":"plots","cdcfbe1c":"my_dpi = 1000\nfig, ax = plt.subplots(4,1, figsize = (12,10), dpi=my_dpi)\n\n\nax[0].set_title('Total Loss', fontsize=12)\nax[0].set_xticks([])\nax[0].set_yticks([])\nax[0].imshow(Image.open('.\/seaborn\/total_loss.png'))\n\nax[1].set_title('Bounding Box Average Precision', fontsize=12)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].imshow(Image.open('.\/seaborn\/bbox\/AP.png'))\n\nax[2].set_title('Segmentation Average Precision', fontsize=12)\nax[2].set_xticks([])\nax[2].set_yticks([])\nax[2].imshow(Image.open('.\/seaborn\/segm\/AP.png'))\n\nax[3].set_title('Class accuracy', fontsize=12)\nax[3].set_xticks([])\nax[3].set_yticks([])\nax[3].imshow(Image.open('.\/seaborn\/fast_rcnn\/cls_accuracy.png'))","0bbfd25d":"evaluator = COCOEvaluator(\"car_dataset_val\", cfg, False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, \"car_dataset_val\")\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))","43e020dd":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold for this model\ncfg.DATASETS.TEST = (\"car_dataset_val\", )\npredictor = DefaultPredictor(cfg)","385822d0":"val_dataset_dicts = DatasetCatalog.get(\"car_dataset_val\")\nval_metadata_dicts = MetadataCatalog.get(\"car_dataset_val\")","3031e460":"fig, ax = plt.subplots(2, 2, figsize =(16,12))\nindices=[ax[0][0],ax[1][0],ax[0][1],ax[1][1] ]\ni=-1\nfor d in random.sample(val_dataset_dicts, 4):\n    i=i+1    \n    im = io.imread(d[\"file_name\"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=val_metadata_dicts, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    indices[i].grid(False)\n    indices[i].imshow(out.get_image()[:, :, ::-1])","ffaf66ff":"### Do give this notebook an upvote if you liked my work, thanks!","ab6cb226":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Image with damage annotation<\/center><\/h3>","6306a67d":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Set constant variables<\/center><\/h3>","9d9a634c":"<h2><center> <a href=\"https:\/\/github.com\/facebookresearch\/detectron2\">Detectron2<\/a> is a PyTorch based modular object detection library<\/center><\/h2>\n\n<h4 style=\"text-align: right, line-height: 3.5em;\"> Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. It can be used to train various state-of-the-art models like <a href=\"http:\/\/densepose.org\/\">Densepose <\/a> and <a href=\"https:\/\/ai.facebook.com\/blog\/improving-scene-understanding-through-panoptic-segmentation\/\">panoptic feature pyramid networks<\/a> for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection. With a modular design, Detectron2 is flexible and extensible, and able to provide fast training on single or multiple GPU servers. <\/h4>\n    \n    \n<h4> I hope that releasing Detectron2 will continue to accelerate progress in the area of object detection and segmentation. This Kernel is my attempt of contributing to the progress. <\/h4>   ","bd844a46":"### Installation\n* Most of the libraries required for visualization like [skimage](https:\/\/scikit-image.org\/docs\/dev\/api\/skimage.html) and [matplotlib](https:\/\/matplotlib.org\/) come preinstalled in kaggle environment.\n* One library required to visualize COCO dataset is [Pycocotools]() which can be installed using the following command.\n`pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI`","7e7174a2":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Import Libraries required for training<\/center><\/h3>","2ac83866":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Train <\/center><\/h2>","b3c31ffa":"If you want to use a custom dataset while also reusing detectron2\u2019s data loaders, you will need to\n\n*  Register your dataset (i.e., tell detectron2 how to obtain your dataset).\n\n* Optionally, register metadata for your dataset.","acc2f9bc":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Initialize the COCO API<\/center><\/h3>","40c5cf5d":"* I think the training worked well as the loss has decreased over the runs.\n* The class accuracy and average precision has improved over the runs.","07b27f04":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center>  Register Car Damage Dataset <\/center><\/h3>","bc990460":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Installation <\/center><\/h3>","7e517573":"# <center><img src=\"https:\/\/raw.githubusercontent.com\/facebookresearch\/detectron2\/master\/.github\/Detectron2-Logo-Horz.svg\"><center\/>","5d8dbd0e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center>  Model Metrics and Hyper Parameters Visualization <\/center><\/h3>","1b0b464a":"### Note: Unfortunately, there is some issue with tensorboard in Kaggle so I thought of using seaborn to visualize the plots.\n\nSource: https:\/\/www.kaggle.com\/product-feedback\/89671#764494","54920e05":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Training Object detection model using Detectron 2<\/center><\/h2>","665a5903":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Raw Image<\/center><\/h3>","c595877d":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Image with parts annotation<\/center><\/h3>","4d13040d":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Display damage categories and supercategories<\/center><\/h3>","228da926":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Inference <\/center><\/h2>","bd48a30c":"### <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Import Libraries<\/center><\/h3>","efbc0477":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Evaluation <\/center><\/h2>","be203b24":"### Conclusion\n* I think the results are quite fine even when the training data was around 60 images.\n* Data augmentation can significantly improve the results.\n* I will try doing multiclass object detection next."}}