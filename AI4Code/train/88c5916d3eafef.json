{"cell_type":{"1ed282b9":"code","8ca29e60":"code","d54c2300":"code","df119712":"code","3fd2440c":"code","44fb956c":"code","d80192f7":"code","57d50f53":"code","776e1273":"code","24fe825b":"code","cf8b1773":"code","7588b38c":"code","a039c68d":"code","90fe956f":"markdown","099d215a":"markdown","a9f9857c":"markdown"},"source":{"1ed282b9":"import numpy as np\nimport pandas as pd\nimport collections\nimport gc\nfrom tqdm import tqdm\nimport cv2\nimport cudf, cuml, cupy\nfrom cuml.neighbors import NearestNeighbors\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport transformers\nfrom transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n\n\nimport sys\nsys.path.append('..\/input\/timm-shpee\/pytorch-image-models-master')\nimport timm\nfrom timm.models.layers import SelectAdaptivePool2d\n\nimport warnings\nwarnings.filterwarnings('ignore')","8ca29e60":"COMPUTE_CV = False","d54c2300":"cos_threshs = np.array([0.16])\ntest_batch_size = 128\n\n# image model\nGEM_P = 4\nimage_size = 420\n\n# TTA for image(do not use)\nflip_TTAs = [False, False, False, False, False]\ntesting_scales = [[1.0], [1.0], [1.0], [1.0], [1.0]]\n\n# text model\ntext_max_length = 84\n\n# alpha query expansion\nalpha_query_expansion = True\nqe_mid_knn = True\nqe_ms     = [[1, 1], [1, 1], [1, 1], [2, 1], [1, 1]]\nqe_alphas = [[2, 5], [2, 7], [5, 2], [7, 2], [3, 3]]\n\n# adaptive thresholding\nUSE_ADAPTIVE_THRESHOLDING = False\nCONSERVATIVENESS = 1.0\nBETA = np.mean([0.9, 0.8, 0.9, 0.75, 0.3])\n\n# min num preds\nforce_2preds = True\nforce_2preds_relax = 1.2\n\n# kNN\nKNN = 52\nALPHA_QE_KNN = 8\nknn_metric = 'cosine' # cosine or correlation","df119712":"model_weight_paths = [\n    '..\/input\/joint-f0gem3420768-dbert-highway\/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold0_HighwayConc768x3_ep-002_f1-0.88579_thresh-0.43448_bs-30_emb-2304.pth',\n    '..\/input\/joint-f0gem3420768-dbert-highway\/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold1_HighwayConc768x3_ep-002_f1-0.89122_thresh-0.42069_bs-32_emb-2304.pth',\n    '..\/input\/joint-f0gem3420768-dbert-highway\/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold2_HighwayConc768x3_ep-003_f1-0.88911_thresh-0.44828_bs-30_emb-2304.pth',\n    '..\/input\/joint-f0gem3420768-dbert-highway\/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold3_HighwayConc768x3_ep-002_f1-0.88196_thresh-0.44828_bs-32_emb-2304.pth',\n    '..\/input\/joint-f0gem3420768-dbert-highway\/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold4_HighwayConc768x3_ep-003_f1-0.88931_thresh-0.44828_bs-32_emb-2304.pth'\n                     ]\ntokenizer = DistilBertTokenizer.from_pretrained('..\/input\/distilberttextaugmadgrad5folds\/distil-bert-textaug-madgrad-5folds\/tokenizer')\n\n\n# general\nloader_num_workers = 2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","3fd2440c":"if COMPUTE_CV:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/train.csv').iloc[0:300]\nelse:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\ntest = test.drop(columns='image_phash')\n\nLEN_TEST = len(test)\n\nBASE = '..\/input\/shopee-product-matching\/test_images\/'\nif COMPUTE_CV:\n    BASE = '..\/input\/shopee-product-matching\/train_images\/'\n    \nCHUNK = 1024*4\nCTS = LEN_TEST\/\/CHUNK\nif LEN_TEST%CHUNK!=0:\n    CTS += 1\n    \nif LEN_TEST==3:\n    KNN = 3\n    ALPHA_QE_KNN = 3\n    qe_ms     = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n    qe_alphas = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]","44fb956c":"# tokenize titles\ntexts = list(test['title'].apply(lambda o: str(o)).values)\ntext_encodings = tokenizer(texts, \n                           padding=True, \n                           truncation=True, \n                           max_length=text_max_length)\n\ntest['input_ids'] = text_encodings['input_ids']\ntest['attention_mask'] = text_encodings['attention_mask']\n\ndel texts, text_encodings, tokenizer\n_=gc.collect()","d80192f7":"class Shopee(Dataset):\n    def __init__(self, df, image_dir, augs):\n        self.df = df\n        self.augs = augs \n        self.image_dir = image_dir\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        item = {'input_ids': torch.tensor(self.df['input_ids'].iloc[idx]), 'attention_mask': torch.tensor(self.df['attention_mask'].iloc[idx])}\n        \n        # image\n        image = cv2.imread(self.image_dir + self.df.loc[idx, 'image']).astype(np.uint8)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.augs(image=image)['image']\n\n        return image, item\n    \ndef make_aug(scale=1.0, horizontal_flip=False):\n    im_size = int(round(scale*image_size))\n    if horizontal_flip:\n        valid_aug = A.Compose([A.LongestMaxSize(max_size=im_size, p=1.0),\n                               A.PadIfNeeded(min_height=im_size, min_width=im_size, border_mode=0, p=1.0),\n                               A.HorizontalFlip(p=1.0),\n                               A.Normalize(p=1.0),\n                               ToTensorV2(p=1.0)])\n        \n    else:\n        valid_aug = A.Compose([A.LongestMaxSize(max_size=im_size, p=1.0),\n                               A.PadIfNeeded(min_height=im_size, min_width=im_size, border_mode=0, p=1.0),\n                               A.Normalize(p=1.0),\n                               ToTensorV2(p=1.0)])\n        \n    return valid_aug","57d50f53":"# J3 without joint embeddings(stacked embeddings)\n# joint embeddings give no score(CV and LB) boost, image\/text concat embeddings are sufficient\nclass AdaptiveGeneralizedMeanPool2d(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(AdaptiveGeneralizedMeanPool2d, self).__init__()\n        self.p = p\n        self.eps = eps\n        self.flatten1 = nn.Flatten()\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        x = F.adaptive_avg_pool2d(input=x.clamp(min=eps).pow(p), output_size=(1, 1)).pow(1.\/p)\n        x = self.flatten1(x)\n        return x\n\nclass NFNetF0_GeM_L2(nn.Module):\n    def __init__(self, num_embeddings, pretrained=True):\n        super(NFNetF0_GeM_L2, self).__init__()\n        self.model = timm.create_model('dm_nfnet_f0', pretrained=pretrained)\n        self.model.head.global_pool = AdaptiveGeneralizedMeanPool2d(p=GEM_P)\n        num_features = self.model.head.fc.in_features\n        self.model.head.fc = nn.Linear(num_features, num_embeddings)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = F.normalize(x, p=2, dim=1, eps=1e-12)\n        \n        return x\n\nclass DistilBERT_L2(nn.Module):\n    def __init__(self, bert_model):\n        super(DistilBERT_L2, self).__init__()\n        self.bert_model = bert_model\n    \n    def forward(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state\n        CLS_token_state = last_hidden_state[:, 0, :]\n        CLS_token_state = F.normalize(CLS_token_state, p=2, dim=1, eps=1e-12)\n        \n        return CLS_token_state\n\nclass JointModel(nn.Module):\n    def __init__(self, text_model, image_model):\n        super(JointModel, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # define fc1(embeddings stacking), this won't be used for forward\n        self.fc1 = nn.Linear(768 + 768, 768)\n\n    \n    def forward(self, image_input, batch):\n        # image embeddings\n        image_emb = self.image_model(image_input)\n\n        # CLS_token as text embeddings\n        text_emb = self.text_model(batch)\n        \n        x = torch.cat((image_emb, text_emb), dim=1)\n\n        return x","776e1273":"image_model = NFNetF0_GeM_L2(768, False)\nbert_config = DistilBertConfig(activation='gelu',\n                               attention_dropout=0.1,\n                               dim=768,\n                               dropout=0.1,\n                               hidden_dim=3072,\n                               initializer_range=0.02,\n                               max_position_embeddings=512,\n                               model_type='distilbert',\n                               n_heads=12,\n                               n_layers=6,\n                               output_hidden_states=True,\n                               pad_token_id=0,\n                               qa_dropout=0.1,\n                               seq_classif_dropout=0.2,\n                               sinusoidal_pos_embds=True,\n                               tie_weights_=True,\n                               vocab_size=32000)\nbert_model = DistilBertModel(bert_config)\ntext_model = DistilBERT_L2(bert_model)\nmodel = JointModel(text_model, image_model)\ndel bert_model, text_model, image_model\n\n_=model.to(device)\n_=model.eval()","24fe825b":"def joint_embedder(df, model, scale=1.0, flip=False):\n    embeds = []\n    CHUNK = 1024*4\n    for i,j in enumerate(range(CTS)):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        \n        test_data = Shopee(df.iloc[a:b].reset_index(drop=True),\n                           BASE,\n                           augs=make_aug(scale=scale, horizontal_flip=flip))\n        test_loader = DataLoader(test_data,\n                                 shuffle=False,\n                                 num_workers=loader_num_workers,\n                                 pin_memory=False,# False:faster\n                                 batch_size=test_batch_size)\n        with torch.no_grad():\n            for inputs, batch in tqdm(test_loader):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                inputs = inputs.to(device)\n                embedding = model(inputs, batch).detach().cpu().numpy()\n                embeds.append(embedding)\n        \n    return np.concatenate(embeds)\n\n\ndef distance_to_matching_probability(x):\n    # fit distance to matching probability by poly-lines\n    p1 = [0.2, 0.850*CONSERVATIVENESS]\n    p2 = [0.3, 0.600*CONSERVATIVENESS]\n    p3 = [0.4, 0.225*CONSERVATIVENESS]\n    p4 = [0.5, 0.050*CONSERVATIVENESS]\n    if x < 0.0:\n        y = 1.0\n    elif x < p1[0]:\n        y = x*(p1[1] - 1.0)\/(p1[0] - 0.0) + 1.0\n    elif x < p2[0]:\n        y = (x - p2[0])*(p2[1] - p1[1])\/(p2[0] - p1[0]) + p2[1]\n    elif x < p3[0]:\n        y = (x - p3[0])*(p3[1] - p2[1])\/(p3[0] - p2[0]) + p3[1]\n    elif x < p4[0]:\n        y = (x - p4[0])*(p4[1] - p3[1])\/(p4[0] - p3[0]) + p4[1]\n    elif x < 0.6:\n        y = (x - 0.6)*(0.0 - p4[1])\/(0.6 - p4[0])\n    else:\n        y = 0\n    return y\n\ndef adaptive_thresholding(dists=None, global_thresh=None, beta=None):\n    probs = np.frompyfunc(distance_to_matching_probability, 1, 1)(dists)\n    \n    # expected number of positives\n    ex_num_pos = np.sum(probs > 0.5)\n    \n    # expected F1 change when one more prediction is added\n    # sign of F1 change matters\n    for num_pred in range(0, KNN):\n        #denom = (num_pred + 1 + ex_num_pos)*(num_pred + ex_num_pos)\n        #term1 = 2.0\/denom\n        term2 = (num_pred + ex_num_pos)*probs[num_pred] - np.sum(probs[:num_pred])\n        #dF = term1*term2\n        dF = term2\n        \n        if dF < 0:\n            break\n    \n    best_thresh = dists[num_pred]*1.00001\n    best_thresh = 0.5*(best_thresh-0.3)+0.1875\n    adaptive_thresh = beta*best_thresh + (1.0 - beta)*global_thresh\n    \n    #print(f'{global_thresh}-->{adaptive_thresh}')\n    \n    return adaptive_thresh\n\n\ndef knn_matching(knn_model, embeddings, thresh):\n    preds = []\n    CHUNK = 1024*4\n    \n    CTS = len(embeddings)\/\/CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n        \n    for j in range(CTS):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        distances, indices = knn_model.kneighbors(embeddings[a:b,])\n              \n        for k in range(b-a):\n            dists = distances[k,]\n            if USE_ADAPTIVE_THRESHOLDING:\n                adaptive_thresh = adaptive_thresholding(dists=dists, global_thresh=thresh, beta=BETA)\n            else:\n                adaptive_thresh = thresh\n                \n            IDX = np.where(dists < adaptive_thresh)[0]\n            IDS = indices[k,IDX]\n            \n            # force min_num_preds to be 2\n            if force_2preds:\n                if len(IDS) < 2:\n                    # relax matching threshold\n                    IDX = np.where(dists < thresh*force_2preds_relax)[0]\n                    IDS = indices[k,IDX]\n                        \n            o = test.iloc[IDS].posting_id.values\n            preds.append(o)\n            \n    return preds\n\n\ndef alpha_query_expansion(knn_model, embeddings, qe_alpha, qe_m):\n    expanded_embeddings = []\n    CHUNK = 1024*4\n    \n    CTS = len(embeddings)\/\/CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n        \n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        distances, indices = knn_model.kneighbors(embeddings[a:b,])\n        for i in range(b-a):\n            weights = ((1-distances[i, 0:qe_m+1])**qe_alpha).reshape(qe_m+1, 1)\n            expanded_embedding = np.sum((embeddings[indices[i, 0:qe_m+1]]*weights), axis=0)\/np.sum(weights)\n            expanded_embedding = expanded_embedding \/ np.linalg.norm(expanded_embedding)\n            expanded_embeddings.append(expanded_embedding)\n            \n    return np.array(expanded_embeddings)","cf8b1773":"for i_model, model_weight_path in enumerate(model_weight_paths):\n    model.load_state_dict(torch.load(model_weight_path))\n    \n    # compute embeddings\n    n_test = 0\n    for testing_scale in testing_scales[i_model]:\n        if n_test ==0:\n            joint_embeddings = joint_embedder(test, model, scale=testing_scale, flip=False)\n        else:\n            joint_embeddings += joint_embedder(test, model, scale=testing_scale, flip=False)\n        n_test += 1\n    \n        if flip_TTAs[i_model]:\n            joint_embeddings += joint_embedder(test, model, scale=testing_scale, flip=True)\n            n_test += 1\n            \n    joint_embeddings = joint_embeddings\/n_test\n    \n\n    # alpha query expansion\n    if alpha_query_expansion:\n        knn_model = NearestNeighbors(n_neighbors=ALPHA_QE_KNN, metric=knn_metric)\n        knn_model.fit(joint_embeddings)\n        for qe_alpha, qe_m in zip(qe_alphas[i_model], qe_ms[i_model]):\n            joint_embeddings = alpha_query_expansion(knn_model, joint_embeddings, qe_alpha, qe_m)\n            \n            if qe_mid_knn:\n                knn_model = NearestNeighbors(n_neighbors=ALPHA_QE_KNN, metric=knn_metric)\n                knn_model.fit(joint_embeddings)\n        del knn_model\n        _=gc.collect()\n    \n    # concat fold embeddings\n    if i_model == 0:\n        long_embeddings = joint_embeddings\n    else:\n        long_embeddings = np.hstack([long_embeddings, joint_embeddings])\n        \nprint(long_embeddings.shape)\n\ndel joint_embeddings\n_=gc.collect()\n\n\n# prediction\nknn_model = NearestNeighbors(n_neighbors=KNN, metric=knn_metric)\nknn_model.fit(long_embeddings)\ntest['matches'] = knn_matching(knn_model, long_embeddings, np.mean(cos_threshs))\n\ndel long_embeddings, knn_model, model\n_=gc.collect()","7588b38c":"test['matches'] = test['matches'].map(lambda x : ' '.join(np.array(x)[0:KNN]))","a039c68d":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","90fe956f":"# load data","099d215a":"# model","a9f9857c":"# dataset"}}