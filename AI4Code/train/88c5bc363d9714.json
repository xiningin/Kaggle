{"cell_type":{"b54acebd":"code","5850177d":"code","c7fc394b":"code","d1b41fb6":"code","43b158c1":"code","e0d3aeb9":"code","7b419d20":"code","b5ecb0f2":"code","ce29ff40":"code","a2a177a8":"code","d3eec938":"code","60aeb7de":"code","17fdd1f9":"code","a286efe4":"code","d324c147":"code","3384a48f":"code","aab2e585":"markdown","6e8e8f90":"markdown","ccd2c916":"markdown","4ff1ad3b":"markdown","0e3a1c4a":"markdown","0fb1e328":"markdown","13a850a1":"markdown","ce26281e":"markdown","b511a150":"markdown","f60c9dfe":"markdown"},"source":{"b54acebd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5850177d":"import sys \nimport pandas as pd\nimport matplotlib\nimport numpy as np \nimport scipy as sp\nimport IPython\nfrom IPython import display \nimport sklearn \nimport random\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","c7fc394b":"data_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain= pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\ntest.head()\ntrain.head()\ntrain.info()\n\ndata1  = train.copy(deep = True)\ndata_cleaner = [data1,test]\n\n","d1b41fb6":"for dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n    \n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \ndropcolumns =['PassengerId','Ticket', 'Cabin']\ndata1.drop(dropcolumns,axis=1,inplace=True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(test.isnull().sum())","43b158c1":"for dataset in data_cleaner:\n    dataset['Family Size'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone']= 1\n    dataset['IsAlone'].loc[dataset['Family Size'] > 1] = 0\n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    #Continuous variable bins - https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    # using qcut  - https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n    \nstat_min = 10 \ntitle_names = (data1['Title'].value_counts() < stat_min)\n\n   \ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\n\n#preview data again\ndata1.info()\ntest.info()\ndata1.sample(10)","e0d3aeb9":"#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'Family Size', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'Family Size', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\n\ndata1_dummy.head()","7b419d20":"print('Train columns with null values: \\n', data1.isnull().sum())\nprint (data1.info())\n\n\nprint('Test\/Validation columns with null values: \\n', test.isnull().sum())\nprint (test.info())","b5ecb0f2":"x = data1[data1_x_calc]\ny = data1['Survived']\n\n#remove the hashtag in the line below for train_test_split()\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=1)\n\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )","ce29ff40":"# Create a pipeline that extracts features from the data then creates a model\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\n\nforest = ensemble.RandomForestClassifier()\n# create feature union\nfeatures = []\nfeatures.append(('pcb', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=5)))\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature _union', feature_union))\nestimators.append(('random_forest', forest))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nseed = 7\nkfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(model, x, y, cv=cv_split)\nprint(results.mean())\n\n","a2a177a8":"model.fit(x,y)\n\ny_pred = model.predict(x_test)\n\nprint (\"This is your accuracy!! \", metrics.accuracy_score(y_test,y_pred))","d3eec938":"from sklearn.model_selection import GridSearchCV\nimport joblib\n\n\ndef best_config(model, parameters, train_instances, judgements):\n    clf = GridSearchCV(model, parameters, cv=5,\n                       scoring=\"accuracy\", verbose=5, n_jobs=4)\n    clf.fit(train_instances, judgements)\n    best_estimator = clf.best_estimator_\n\n    return [str(clf.best_params_), clf.best_score_,\n            best_estimator]\n\n# Set grid search params\nparam_range = [9,10]\nparam_range_fl = [1.0, 0.5]\n\ngrid_params_rf = [{'criterion': ['gini', 'entropy'],\n        'max_depth': param_range,\n        'min_samples_split': param_range[1:]}]\n\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n","60aeb7de":"grid_search_model = GridSearchCV(forest, param_grid, cv=2,scoring=\"accuracy\", verbose=5, n_jobs=-1)","17fdd1f9":"grid_search_model.fit(x,y)","a286efe4":"best_grid_model= grid_search_model.best_estimator_\nbest_grid_model.get_params","d324c147":"y_pred1 = best_grid_model.predict(x_test)\nprint (\"This is your accuracy!! \", metrics.accuracy_score(y_test,y_pred1))\n","3384a48f":"#seperate model for submission\n\npredictions = best_grid_model.predict(test[data1_x_calc])\n\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})\n\nsubmission.head() \n\nfilename_csv = \"Titanic dataset 10th predictions.csv\" \nsubmission.to_csv(filename_csv,index=False)\n\nprint('Saved file: ' + filename_csv)","aab2e585":"# Problem Statement\nCreate a model to predict the survival rate of a passenger aboard the Titanic.","6e8e8f90":"* # Creating a model without Grid CV\nHere, you're learning how to create a pipeline. This code is copied from https:\/\/machinelearningmastery.com\/automate-machine-learning-workflows-pipelines-python-scikit-learn\/","ccd2c916":"this function is copied from https:\/\/stackoverflow.com\/questions\/35890397\/scikitlearn-extracting-feature-names-from-featureunion-inside-a-pipeline\n\nAnother link which really helped me -  https:\/\/medium.com\/analytics-vidhya\/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05","4ff1ad3b":"# Submission Code\n\n","0e3a1c4a":"# Model Using GridCV","0fb1e328":"# 2. Importing the data\n\nThe data has been provided. We use the read_csv function of the Panda Library, which is famously used for working with datasets, to import the data from a CSV file. We store the test and train data in seperate variables.","13a850a1":"# 5. Splitting the data\n\nSplitting the data\nWe first saw how to split the data into 2 parts using train_test_split() . By splitting the data this way we we were able to achieve an accuracy of 77% on training data and 71% on test data. The fact that is has a higher accuracy on the training data tells us that there is overfitting taking place.\n\nWe can reduce overfitting by using cross-valdation while splitting the data. In cross-validation we split the data into more that K parts, where K = any natural number, example k =5,10,15... The higher the number the more the computational power.\n\nLet us see how to implement cross validation.\n\nThis link really helped me to learn cross-validation. https:\/\/machinelearningmastery.com\/repeated-k-fold-cross-validation-with-python\/#:~:text=Repeated%20k%2Dfold%20cross%2Dvalidation%20provides%20a%20way%20to%20improve,all%20folds%20from%20all%20runs.","ce26281e":"# 4. Feature Engineering\nNow that all the Null values have been handled we move onto feature engineering. Feature engineering is the part where we create new columns(features) out of the existing features. This part of the framework is really crucial when it comes to improving the performace of a model.\n\n\n","b511a150":"# 3.Handling non-null values\n\nFirstly, we observe that age, cabin and embarked have missing values. Cabin has too many missing values so we can just drop it. Age and embarked can be filled in using either the median,mean or mode. We will be trying all 3 and see which fits our model the best.\n\nA huge part of the data cleaning code was found at : https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\n\n","f60c9dfe":"# 1. Importing libraries"}}