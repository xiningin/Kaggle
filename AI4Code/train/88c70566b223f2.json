{"cell_type":{"9e3662c1":"code","90cb837e":"code","4edf20e3":"code","a0f03e43":"code","bef0bcdc":"code","a86a8f3f":"code","a827a58f":"code","1a3c6e48":"code","73844e8e":"code","a966f3df":"code","f4d46922":"code","851dd2ef":"code","026c1b49":"code","659d6316":"code","7de11729":"code","7a5976bd":"code","625a330c":"code","a05875a8":"code","2a681a54":"code","4b1228ad":"code","2abb6c3a":"code","59c1bbc4":"code","3976acc6":"code","d09a0cb0":"code","4103612f":"code","5f7f1bdd":"code","0d5646d4":"code","b9506b7a":"code","c72ef609":"code","788d32ea":"code","f0505c23":"code","0d919d93":"code","c412942d":"code","f7f73049":"code","1e7a460f":"code","90cdb0bc":"code","12b5cb8a":"code","1d310a80":"code","7a2293fb":"code","03832426":"code","33c03674":"markdown","64464ff1":"markdown","d78bda76":"markdown","a2d78e7d":"markdown","7f70c2e5":"markdown","080ea63d":"markdown","36c6baa0":"markdown","c505b5fc":"markdown","7fc0b499":"markdown","68f22b78":"markdown","534945cc":"markdown","89a19861":"markdown","fd580502":"markdown","074b21d4":"markdown","59f1ddad":"markdown","4f4303aa":"markdown"},"source":{"9e3662c1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette('Set2')\n\nimport datetime as dt\nimport dateutil\n\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk import tokenize, pos_tag\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\n\nimport importlib","90cb837e":"# Type of each field in the input data.\ndf_dtype = pd.read_csv('..\/input\/airline-reviews-eda-and-preprocessing\/PreprocessedDataLightTypes.csv')\ndict_dtype = df_dtype[['index','dtypes']].set_index('index').to_dict()['dtypes']\ndict_dtype['recommended'] = 'bool'","4edf20e3":"# Input data.\ndf = pd.read_csv('..\/input\/airline-reviews-eda-and-preprocessing\/PreprocessedDataLight.csv', dtype=dict_dtype, keep_default_na=False, na_values=['_'])\ndf.drop(columns=['Unnamed: 0'],inplace=True)","a0f03e43":"df.head()","bef0bcdc":"df.shape","a86a8f3f":"cols = df.columns.to_list()\nprint('Columns in the dataset:')\nprint(cols)","a827a58f":"n_reviews = df.shape[0]\nprint('Number of customer reviews in the dataset: {:d}'.format(n_reviews))","1a3c6e48":"# Save the review text to a new list\nreviews_list = df['review_text'].copy()\nreviews_list.shape","73844e8e":"# Copy for the sake of tackling the ambiguity\ndf_nlp = df.copy()","a966f3df":"nltk.download('punkt')\nnltk.download('vader_lexicon')","f4d46922":"sid = SentimentIntensityAnalyzer()","851dd2ef":"# Example of VADER sentiment analyzer on different intensity sentences.\nprint(sid.polarity_scores(\"I liked Forrest Gump.\"))\nprint(sid.polarity_scores(\"I really liked Forrest Gump.\"))\nprint(sid.polarity_scores(\"Forrest Gump is a GREAT movie.\"))\nprint(sid.polarity_scores(\"Forrest Gump is perhaps the GREATEST movie ever!\"))\nprint(sid.polarity_scores(\"Forrest Gump is one of the GREATEST movie of all time! :)\"))","026c1b49":"review = reviews_list[0]\nreview_tok = tokenize.sent_tokenize(review)\nprint(review_tok)","659d6316":"# Example of VADER sentiment analyzer on first review of our dataset\nprint('Review text:')\nprint(review)\n\nreview_polarity_scores = sid.polarity_scores(review)\n\nfor key in sorted(review_polarity_scores.keys()):\n    print('{}: {}, '.format(key,review_polarity_scores[key]), end='')\nprint('\\n')","7de11729":"# Example on a sentence level.\n\n# print('Review text:')\n# print(review_tok)\n\nfor sentence in review_tok:\n    print('Sentence text:')\n    print(sentence)\n    sentence_polarity_scores = sid.polarity_scores(sentence)\n\n    for key in sorted(sentence_polarity_scores.keys()):\n        print('{}: {}, '.format(key,sentence_polarity_scores[key]), end='')\n    print('\\n')","7a5976bd":"# Augment the dataset with the overall polarity score of the review, as obtained using VADER on the review level.\nreviews_polarity = []\n\nfor i_review, review in enumerate(reviews_list):\n    # print('Review text:')\n    # print(review)\n\n    review_polarity_scores = sid.polarity_scores(review)\n    review_polarity_score_compound = review_polarity_scores['compound']\n    \n    '''Uncomment this to print the sentiment score of every review in the dataset.\n    \n    print('Review #{:d}: '.format(i_review), end='')\n    for key in sorted(review_polarity_scores.keys()):\n        print('{}: {:.4f}, '.format(key,review_polarity_scores[key]), end='')\n    print('')  '''\n    \n    reviews_polarity.append(review_polarity_score_compound)\n\n# print(reviews_polarity)","625a330c":"df_nlp['polarity'] = reviews_polarity\ndf_nlp.head()","a05875a8":"corr_values = df_nlp[['polarity','recommended']].dropna(axis=0,how='any').corr()","2a681a54":"def plot_cmap(matrix_values, figsize_w, figsize_h, filename):\n    \"\"\"\n    Plot a heatmap corresponding to the input values.\n    \"\"\"\n    if figsize_w is not None and figsize_h is not None:\n        plt.figure(figsize=(figsize_w,figsize_h))\n    else:\n        plt.figure()\n    cmap = sns.diverging_palette(240, 10, sep=20, as_cmap=True)\n    sns.heatmap(matrix_values, annot=True, fmt=\".2f\", cmap=cmap, vmin=-1, vmax=1)\n    plt.savefig(filename)\n    plt.show()\n    return cmap\n\nplot_cmap(matrix_values=corr_values, \n          figsize_w=4, \n          figsize_h=4,\n          filename='.\/Corr.png')","4b1228ad":"def plot_two_hists_comp_sns(df_1,df_2,label_1,label_2,feat,bins,title,x_label,y_label,filename):\n    \"\"\"\n    Plot two histograms of a given feature in two different datasets.\n    \"\"\"\n    plt.figure(figsize=(6,6))\n    sns.distplot(df_1[feat],hist=True,norm_hist=True,kde=False,label=label_1,bins=bins)\n    sns.distplot(df_2[feat],hist=True,norm_hist=True,kde=False,label=label_2,bins=bins)\n    plt.title(title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.grid(False)\n    plt.legend(loc='best')\n    plt.savefig(filename)\n    plt.show()\n    return\n\nplot_two_hists_comp_sns(df_1=df_nlp[df_nlp['recommended']==True],\n                        df_2=df_nlp[df_nlp['recommended']==False],\n                        label_1='recommended',\n                        label_2='not recommended',\n                        feat='polarity',\n                        bins=30,\n                        title='Distribution of all customer reviews',\n                        x_label='Polarity',\n                        y_label='Entries \/ bin',\n                        filename='.\/HistPolarityByRecommendation.png')","2abb6c3a":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","59c1bbc4":"# Stop words.\n# Airlines appearing in the dataset. This is the official name of the airlines. These words should be removed from the review text.\nairlines_lower = df_nlp['airline'].str.lower().unique().tolist()\n# Words appearing in the official name of the airlines. These words should be removed from the review text.\nairlines_identifier = ['airlines',\n                       'air lines',\n                       'airline',\n                       'air line',\n                       'airways',\n                       'air']\n# In addition to the official name of the airlines, customers can use shortened versions of this name.\nairlines_informal_lower = []\nfor airline in airlines_lower:\n    found = False\n    for airline_identifier in airlines_identifier:\n        if found == False:\n            if str(' '+airline_identifier) in airline:\n                airline_informal = airline.replace(str(' '+airline_identifier),'')\n                airlines_informal_lower.append(airline_informal)\n                found = True\n# Other stop words.\nadditional_stopwords = ['one','get','also','however','even','make']","3976acc6":"nltk_stopwords = stopwords.words('english')\nnltk_stopwords_extended = nltk_stopwords + airlines_lower + airlines_identifier + airlines_informal_lower + additional_stopwords\nprint('Number of stopwords in NLTK: {:d}'.format(len(nltk_stopwords)))\nprint('Number of stopwords after extension: {:d}'.format(len(nltk_stopwords_extended)))","d09a0cb0":"def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef text_cleaner(text):\n    # Transform the text so that all words are lower case.\n    # print(text)\n    text = text.lower()\n    # Remove stop words corresponding to airlines. This is needed here as airline names can consist of multiple words and will not be removed after splitting by words.\n    # print(text)\n    for airline_lower in airlines_lower:\n        text = text.replace(airline_lower, '')\n    # Remove punctuation and tokenize the text into individual words.\n    # print(text)\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # Remove words that contain numbers.\n    # print(text)\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # Remove stop words.\n    # print(text)\n    text = [word for word in text if word not in nltk_stopwords_extended]\n    # Remove empty tokens.\n    # print(text)\n    text = [word for word in text if len(word)>0]\n    # POS tagging of the text.\n    # print(text)\n    pos_tags = pos_tag(text)\n    # Lemmatize the text.\n    # print(text)\n    text = [WordNetLemmatizer().lemmatize(i_pos_tag[0], get_wordnet_pos(i_pos_tag[1])) for i_pos_tag in pos_tags]\n    # Remove words with only one letter.\n    # print(text)\n    text = [word for word in text if len(word)>1]\n    # Join the text with space as a word delimiter.\n    # print(text)\n    text = \" \".join(text)\n    # Remove non-ASCII characters.\n    printable = set(string.printable)\n    text = ''.join(filter(lambda x: x in printable, text))\n    return text","4103612f":"# Example of POS tagging.\npos_tag(tokenize.word_tokenize('A sample to show POS tagging'))","5f7f1bdd":"text_cleaner(reviews_list[0])","0d5646d4":"df_nlp['review_text_clean'] = df_nlp['review_text'].apply(lambda x: text_cleaner(x))","b9506b7a":"df_nlp['review_text_clean'][0]","c72ef609":"# Make a list of the reviews.\ncorpus = df_nlp['review_text_clean'].values","788d32ea":"vectorizer_ngrams = CountVectorizer(binary=False, ngram_range=(1, 1), analyzer='word', min_df=50)\nvectorizer_ngrams.fit(corpus)","f0505c23":"vec_review_text_clean_feats = vectorizer_ngrams.get_feature_names()\nvec_review_text_clean_feats[:10]","0d919d93":"vec_review_text_clean = vectorizer_ngrams.transform(df_nlp['review_text_clean'])\nprint(vec_review_text_clean.shape)\nprint(vec_review_text_clean.dtype)","c412942d":"vec_review_text_clean_feats_new = ['count_'+feat for feat in vec_review_text_clean_feats]\ndf_vec_review_text_clean = pd.DataFrame(vec_review_text_clean.toarray(),columns=vec_review_text_clean_feats_new)","f7f73049":"df_vec_review_text_clean.head()","1e7a460f":"df_nlp['review_text_clean'][0]","90cdb0bc":"df_vec_review_text_clean.iloc[0]['count_lose']","12b5cb8a":"df_nlp_final = pd.concat([df_nlp,df_vec_review_text_clean], axis=1)\ndf_nlp_final.head()","1d310a80":"df_nlp_final_types = df_nlp_final.dtypes.to_frame('dtypes').reset_index()\n\ndf_nlp_types = df_nlp.dtypes.to_frame('dtypes').reset_index()\n\ndf_nlp_final.to_csv('.\/NLPFinalDataLight.csv')\ndf_nlp_final_types.to_csv('.\/NLPFinalDataLightTypes.csv')\n\ndf_nlp.to_csv('.\/NLPDataLight.csv')\ndf_nlp_types.to_csv('.NLPDataLightTypes.csv')","7a2293fb":"with open('.\/VecReviewTextCleanFeats.csv', 'w') as f:\n    f.write(', '.join(vec_review_text_clean_feats_new))","03832426":"with open('.\/NLTKStopWordsExtended.csv', 'w') as f:\n    f.write(', '.join(nltk_stopwords_extended))","33c03674":"# 2. Loading the input data","64464ff1":"## 3.5 Preprocess review text","d78bda76":"### 3.5.1 Handling stopwords","a2d78e7d":"# 1. Necessary imports","7f70c2e5":"### 3.5.2 Handling the cases & punctuation; tokenizing & lemmatizing the text; assign POS tagging\n\nFirst of all, convert all characters in the review text to lower case.\n\nAfter that, remove the punctuation and tokenize each customer review into a list of individual words.\n\nAs a next step, select only those words in the review text that could be relevant to solve the problem at hand. In particular, all stop words should be filtered out as they do not affect the meaning of the sentence.\n\nThen, proceed to POS tagging, which allows to identify the role of each word in the sentence, according to the categories noun, verb, adjective, adverb and others. This is needed for a correct lemmatization of the words in the review text.\n\nThe lemmatization consists in bringing the words to their \"standard\" form, e.g. to convert \"wrote\" to \"write\" or \"writing\" to write.","080ea63d":"### 3.5.3 Vectorizing the text\n\nWe convert the text of each customer review from a textual representation to a numerical representation. The vectors of the numerical representation correspond to the words that appear in the preprocessed text of the customer reviews. The values in the numerical representation correspond to the occurrences of the specified word in the customer review. To avoid ending up with too many features in the numerical representation, we limit the dictionary to the words that appear at least a minimum number of times in the customer reviews. This threshold is specified through the parameter min_df of CountVectorizer.\n\nFor example, if we want to use a 3D numerical representation, we might have features corresponding to the words flight, service, food. For a certain customer review, the value of the feature flight will correspond to how many times flight is mentioned in the text, and similar for the other two features service and food.","36c6baa0":"## 3.3 Plotting the correlation between Polarity and Recommendation","c505b5fc":"**Thanks for reading my work. Any suggestions and comments are more than welcome. Now we have completed the pre-processing of the airline reviews data. It's time to train a model. A separate notebook on model training could be found [here](https:\/\/www.kaggle.com\/divyansh22\/lgbm-classifier-for-airline-recommendation)! If you like the work, please feel free to upvote (motivation :D)!**\n\n**Cheers!!**","7fc0b499":"Get the names of the columns","68f22b78":"# 3. Analysing the sentiment of the review text","534945cc":"Get the total number of reviews in the dataset","89a19861":"# 4. Save the final processed data","fd580502":"## 3.2 Sentiment Analysis using VADER\n\nSentiment analysis is the field of NLP that aims at understanding the sentiment of a certain portion of text. One of the best-known packages for sentiment analysis is the open-source package VADER, which is part of NLTK.\n\nVADER, acronym for Valence Aware Dictionary and sEntiment Reasoner, is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\nVADER is built on social media text but it is in general applicable to other domains, including customer reviews.\nVADER is based on a lexicon (vocabulary) that is validated by multiple human judges according to a well-defined and standard procedure. Each word in the lexicon is associated with a sentiment valence, consisting of two properties, polarity and intensity. The polarity describes if the text is positive\/negative. The intensity describes how much the text is positive\/negative, on a scale from -4 to 4. Words not included in the lexicon are classified as neutral.\n\nTo evaluate the sentiment of a sentence or list of sentences, VADER looks for words in the text that are part of the lexicon, modifies the intensity and polarity of the identified words according to a series of rules, sums up these values and then normalises to the range [-1,1].\nVADER incorporates emojis (for example \":-)\"), acronyms (for example \"LOL\") and slang (for example \"nah\"). The algorithm differs from a Bag of Words approach as it takes words order and degree modifiers into account, e.g. by increasing\/decreasing the intensity of the sentiment.\nFor example, the sentences:\n\n* \"I liked Forrest Gump.\"\n* \"I really liked Forrest Gump.\"\n* \"Forrest Gump is a GREAT movie.\"\n* \"Forrest Gump is perhaps the GREATEST movie ever!\"\n* \"Forrest Gump is one of the GREATEST movie of all time! :)\"\n\n    would have an increasing intensity, triggered by degree modifiers.\n   \nThe output of the sentiment analysis is a series of scores, namely \"compound\", \"pos\", \"neu\" and \"neg\".\nThe compound score is normalized between -1 (extremely negative) and 1 (extremely positive) and is a good metric if we need a single value that summarises the sentiment of a given sentence. The compound score can also be used to classify sentences into positive, neutral and negative by setting an appropriate threshold on the compound score. The official recommended threshold is:\n\n* positive sentiment, compound score >= 0.05\n* neutral sentiment, compound score <= 0.05 and >= -0.05\n* negative sentiment, compound score <= -0.05\n\nThe positive, neutral and negative scores represent the fraction of the sentence that has a positive, neutral and negative sentiment. The sum of these three scores should sum up to 1. The positive, neutral and negative scores are a good metric if we need multiple values that summarise the sentiment of a given sentence.","074b21d4":"# About the data\n\n![](https:\/\/cdn.britannica.com\/41\/123141-050-E6229449\/Air-New-Zealand-Boeing-747-400.jpg)\n\nThis notebook is Part 2 of the airline review data preprocessing which focuses on the Natural Language Processing aspect of our features. Part 1 of the data pre-processing could be found [here](https:\/\/www.kaggle.com\/divyansh22\/airline-reviews-eda-and-preprocessing-pt-1).\n\nThe data we use here is a processed version from Part 1 of this notebook.\n\nThe processed data consists of 22 columns in total, most of which are engineered from the original features provided in the original dataset. The processed data is around 18% of the original data, but it is clean to much extent...\n\nThe original data could be found [here](https:\/\/www.kaggle.com\/efehandanisman\/skytrax-airline-reviews).","59f1ddad":"## 3.1 Review text and a new dataframe for NLP","4f4303aa":"## 3.4 Plotting comparison of histograms for positive and negative customer reviews"}}