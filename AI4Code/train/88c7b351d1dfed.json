{"cell_type":{"019f3df4":"code","6570dacd":"code","de2473e2":"code","2ad1bd2b":"code","ebf2c8f6":"code","5ce6720c":"code","5212d864":"code","c2fa55cc":"code","75ad61c0":"code","661c05f8":"code","544e1a26":"code","5af2146f":"code","98a6b856":"code","05a30e01":"code","1a960c38":"code","dd4838df":"markdown"},"source":{"019f3df4":"# ==================\n# Library\n# ==================\nimport warnings\nwarnings.simplefilter('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport sys\nimport pickle\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom torch.optim import lr_scheduler\nfrom transformers import AdamW, get_linear_schedule_with_warmup","6570dacd":"# ==========================\n# Constant\n# ==========================\nTRAIN_PATH = \"..\/input\/data-science-spring-osaka-2021\/train.csv\"\nTEST_PATH = \"..\/input\/data-science-spring-osaka-2021\/test.csv\"\nACTION_PATH = \"..\/input\/data-science-spring-osaka-2021\/actions.csv\"\nSUB_PATH =\"..\/input\/data-science-spring-osaka-2021\/sample_submission.csv\"\nTRAIN_SEQ_PATH = \"..\/input\/fe001-make-feature\/fe001_train_seq.npy\"\nTEST_SEQ_PATH = \"..\/input\/fe001-make-feature\/fe001_test_seq.npy\"","de2473e2":"# ==========================\n# Settings\n# ==========================\nex = \"001\"\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCH = 20","2ad1bd2b":"# ==========================\n# Function\n# ==========================\n# ====================\n# Function\n# ====================\ndef process_data(data_seq):\n    return {\n        'input_data_seq': data_seq,\n    }\n\nclass DSPO_Dataset(Dataset):\n    \n    def __init__(self, data_seq, train = True, y = None):\n        self.data_seq = data_seq\n        self.train = train\n        self.y = y\n    \n    def __len__(self):\n        return len(self.data_seq)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.data_seq[item],\n            \n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        if self.train : \n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              \"y\":torch.tensor(self.y[item], dtype=torch.float32)\n               }\n        else:\n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n               }\n        \nclass LSTM_model(nn.Module):\n    def __init__(\n        self, dropout=0.2, con_size = 20, linear_emb1 = 100, lstm_emb = 150, linear_emb2 = 100):\n        super(LSTM_model, self).__init__()\n        self.linear1 = nn.Sequential(\n            nn.Linear(con_size , linear_emb1),\n            nn.LayerNorm(linear_emb1 ),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.lstm = nn.LSTM(linear_emb1,lstm_emb,num_layers = 1,batch_first=True)\n        \n        \n        # dense\n        self.linear2 = nn.Sequential(\n            nn.Linear(lstm_emb , linear_emb2),\n            nn.LayerNorm(linear_emb2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.linear_final = nn.Linear(linear_emb2,12)\n\n    def forward(self, data_seq):\n        data_seq = self.linear1(data_seq)\n        output, _ = self.lstm(data_seq)\n        output = self.linear2(output[:,-1,:])\n        output = self.linear_final(output)\n        return output\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef sigmoid(value):\n    return 1 \/ (1 + np.exp(-value))","ebf2c8f6":"# ==========================\n# Main\n# ==========================\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\ntrain_seq = np.load(TRAIN_SEQ_PATH)\ntest_seq = np.load(TEST_SEQ_PATH)\naction = pd.read_csv(ACTION_PATH)\nsub = pd.read_csv(SUB_PATH)","5ce6720c":"action_to_num_dict = {}\nnum_to_action_dict = {}\n\nfor n, i in enumerate(action[\"action_seq\"]):\n    action_to_num_dict[i] = n\n    num_to_action_dict[n] = i","5212d864":"train[\"action_seq_num\"] = train[\"action_seq\"].map(action_to_num_dict)","c2fa55cc":"train[\"action_seq_num\"].value_counts().sort_index()","75ad61c0":"target = train[\"action_seq_num\"].values\ntarget = np.eye(len(action))[target]","661c05f8":"y_oof = np.empty([len(train),12])\ntest_preds = np.empty([len(test),12])\ntest_ = DSPO_Dataset(test_seq, train = False, y = None)\ntest_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\nseed_everything(SEED)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_seq,y = train[\"action_seq_num\"])):\n    x_train_seq = train_seq[train_idx]\n    y_train = target[train_idx]\n    x_val_seq = train_seq[valid_idx]\n    y_val = target[valid_idx]\n    train_ = DSPO_Dataset(x_train_seq, train = True, y = y_train)\n    val_ = DSPO_Dataset(x_val_seq,train = True, y = y_val)\n    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True , num_workers=2)\n    val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n    model = LSTM_model()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,\n                      lr=5e-3,\n                      weight_decay=0.1,\n                      )\n    num_train_optimization_steps = int(len(train_loader) * EPOCH)\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=5,\n                                                num_training_steps=num_train_optimization_steps)\n    \n    criterion = nn.BCEWithLogitsLoss()\n    best_val = None\n    for epoch in tqdm(range(EPOCH)):\n        model.train() \n        train_losses_batch = []\n        val_losses_batch = []\n        epoch_loss = 0\n\n        # ==========================\n        # train\n        # ==========================\n        for d in train_loader:\n\n            # =========================\n            # data loader\n            # =========================\n\n            input_data_seq = d['input_data_seq']\n            y = d[\"y\"]\n            input_data_seq = input_data_seq.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n\n            output = model(input_data_seq)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses_batch.append(loss.item())\n\n        train_loss = np.mean(train_losses_batch)\n        \n        # ==========================\n        # eval\n        # ==========================\n        model.eval()  # switch model to the evaluation mode\n        val_preds = np.ndarray((0,12))\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for d in val_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_data_seq = d['input_data_seq']\n                y = d[\"y\"]\n                input_data_seq = input_data_seq.to(device)\n                y = y.to(device)\n                output = model(input_data_seq)\n\n                loss = criterion(output, y)\n                val_preds = np.concatenate([val_preds, output.detach().cpu().numpy()], axis=0)\n                val_losses_batch.append(loss.item())\n\n\n        val_loss = np.mean(val_losses_batch)\n        acc = accuracy_score(np.argmax(y_val,axis=1), np.argmax(val_preds,axis=1))\n        print(epoch, \"train loss:\", train_loss, \"val loss:\",val_loss, \"val acc:\",acc)\n        \n        if not best_val:\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            continue\n\n        if val_loss <= best_val:\n            best_epoch = epoch\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            \n    print(f\"{fold}_best_poch:{best_epoch},best_val:{best_val}, best_acc:{best_acc}\")\n    \n    # ===================================\n    # test\n    # ===================================\n    model = LSTM_model()\n    model.load_state_dict(torch.load(f\"ex{ex}_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    test_preds_ = np.ndarray((0,12))\n    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n        # Predicting on test set\n        for d in test_loader:\n            # =========================\n            # data loader\n            # =========================\n            input_data_seq = d['input_data_seq']\n            input_data_seq = input_data_seq.to(device)\n\n            output = model(input_data_seq)\n\n            test_preds_ = np.concatenate([test_preds_, output.detach().cpu().numpy()], axis=0)\n\n    #torch.save(best_model.state_dict(), f\"..\/ex\/ex{ex}\/ex{ex}_{b}_{fold}.pth\")  # Saving the model\n    test_preds += test_preds_ \/  N_SPLITS\nacc = accuracy_score(train[\"action_seq_num\"].values,np.argmax(y_oof,axis=1))\nprint(f\"cv:{acc}\")\nnp.save(f\"ex{ex}_test_pred.npy\",test_preds)\nnp.save(f\"ex{ex}_oof.npy\",y_oof)","544e1a26":"test_preds_max = sigmoid(np.sort(test_preds, axis=1)[:, -1:])\ntest_preds_label = np.argmax(test_preds, axis=1)","5af2146f":"test_preds_label_list = []\nfor l,m in zip(test_preds_label,test_preds_max):\n    if m > 0.5:\n        test_preds_label_list.append(num_to_action_dict[l])\n    else:\n        # unseen\n        test_preds_label_list.append(num_to_action_dict[6])","98a6b856":"sub[\"action_seq\"] = test_preds_label_list\nsub.to_csv(f\"ex{ex}.csv\",index=False)","05a30e01":"sub[\"action_seq\"].value_counts()","1a960c38":"num_to_action_dict[6]","dd4838df":"### LSTM\u3092\u3064\u304b\u3063\u305fBaseLine\u3067\u3059\u3002\u53c2\u8003\u307e\u3067\u306b\u5171\u6709\u3057\u307e\u3059\u3002"}}