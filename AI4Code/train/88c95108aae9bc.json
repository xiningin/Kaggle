{"cell_type":{"bea97d64":"code","1d92a316":"code","d02c834e":"code","1b3dbcc2":"code","8e1a7cac":"code","2d84e890":"code","f2899206":"code","1980d481":"code","3c14d4c4":"code","6474dd34":"code","57bedda0":"code","073bb425":"code","11b836a3":"code","4e2b0c48":"code","0db0dfe9":"code","db589e33":"code","937042c7":"code","b7e6ad9c":"code","b3c33713":"code","accef85f":"code","4a70dcef":"code","d84d9e7c":"code","62a89c4c":"code","e1f128ba":"code","25c415e1":"code","a1193c38":"code","bf80744e":"code","07787b46":"code","cd2b390c":"code","b8237fdf":"code","2a7050a4":"code","abd287a1":"code","c763b45d":"code","ebcf03d5":"code","2f69125a":"code","c074e8c6":"code","569a04d2":"code","e07ad51d":"code","af88c344":"code","6838dcca":"code","c1d5a765":"code","479b0907":"code","15f59c27":"markdown","587d6556":"markdown","e7b94fcc":"markdown","27b02f6b":"markdown","de85678e":"markdown","ca5c209f":"markdown","45a36d60":"markdown","d75075af":"markdown"},"source":{"bea97d64":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","1d92a316":"import pathlib\nimport fastai\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\n\nfrom torchvision.models import vgg16_bn\nfrom subprocess import check_output","d02c834e":"input_path = Path('\/kaggle\/input\/denoising-dirty-documents')\nitems = list(input_path.glob(\"*.zip\"))\nprint([x for x in items])","1b3dbcc2":"import zipfile\n\nfor item in items:\n    print(item)\n    with zipfile.ZipFile(str(item), \"r\") as z:\n        z.extractall(\".\")","8e1a7cac":"bs, size = 4, 128\narch = models.resnet34\npath_train = Path(\"train\")\npath_train_cleaned = Path(\"train_cleaned\")\npath_test = Path(\"test\")\npath_submission = Path(\"submission\")","2d84e890":"src = ImageImageList.from_folder(path_train).split_by_rand_pct(0.2, seed=42)","f2899206":"def get_data(src, bs, size):\n    data = (\n        src.label_from_func(lambda x: path_train_cleaned \/ x.name)\n           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)\n           .databunch(bs=bs)       \n           .normalize(imagenet_stats, do_y=True)\n    )\n    data.c = 3\n    return data","1980d481":"data = get_data(src, bs, size)","3c14d4c4":"# Show some validation examples\ndata.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(5, 5), title=\"Some image\")","6474dd34":"t = data.valid_ds[0][1].data\nt = torch.stack([t,t])","57bedda0":"def gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))\/(c*h*w)","073bb425":"base_loss = F.l1_loss","11b836a3":"vgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)","4e2b0c48":"# Show the layers before all pooling layers, which turn out to be ReLU activations.\n# This is right before the grid size changes in the VGG model, which we are using\n# for feature generation.\nblocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\nblocks, [vgg_m[i] for i in blocks]","0db0dfe9":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        \"\"\" m_feat is the pretrained model \"\"\"\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        # hooking grabs intermediate layers\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        # get features for target\n        out_feat = self.make_features(target, clone=True)\n        # features for input\n        in_feat = self.make_features(input)\n        # calc l1 pixel loss\n        self.feat_losses = [base_loss(input,target)]\n        # get l1 loss from all the block activations\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        # so we can show all the layer loss amounts\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","db589e33":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","937042c7":"wd = 1e-3\nlearn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n                     blur=True, norm_type=NormType.Weight)\ngc.collect();","b7e6ad9c":"learn.model_dir = Path('models').absolute()","b3c33713":"learn.lr_find()\nlearn.recorder.plot()","accef85f":"print(f\"Validation set size: {len(data.valid_ds.items)}\")","4a70dcef":"lr = 1e-3","d84d9e7c":"def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n    learn.fit_one_cycle(10, lrs, pct_start=pct_start)\n    learn.save(save_name)\n    learn.show_results(rows=1, imgsize=5)","62a89c4c":"do_fit('1a', slice(lr*10))","e1f128ba":"learn.unfreeze()","25c415e1":"do_fit('1b', slice(1e-5, lr))","a1193c38":"# Increase resolution of the images.\ndata = get_data(src, 12, size*2)","bf80744e":"learn.data = data\nlearn.freeze()\ngc.collect()","07787b46":"learn.load('1b');","cd2b390c":"do_fit('2a')","b8237fdf":"learn.unfreeze()","2a7050a4":"do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)\n\n# save entire configuration\n#learn.export(file = model_path)","abd287a1":"fn = data.valid_ds.x.items[10]; fn","c763b45d":"img = open_image(fn); img.shape","ebcf03d5":"p,img_pred,b = learn.predict(img)\nshow_image(img, figsize=(8,5), interpolation='nearest');","2f69125a":"Image(img_pred).show(figsize=(8,5))","c074e8c6":"# Turn off resizing transformations for inference time.\n# https:\/\/forums.fast.ai\/t\/segmentation-mask-prediction-on-different-input-image-sizes\/44389\nlearn.data.single_ds.tfmargs['size'] = None","569a04d2":"test_images = ImageImageList.from_folder(path_test)\nprint(test_images)","e07ad51d":"img = test_images[0]\nimg.show()\nimg.shape","af88c344":"p, img_pred, b = learn.predict(img)","6838dcca":"def rgb2gray(_img):\n    \"\"\" Convert from 3 channels to 1 channel \"\"\"\n    from skimage.color import rgb2gray as _rgb2gray\n\n    # Rotate channels dimension to the end, per skimage's expectations\n    _img_pred_np = _img.permute(1, 2, 0).numpy()\n    _img_pred_2d = Tensor(_rgb2gray(_img_pred_np))\n    # Add the channel dimension back\n    _img_pred = _img_pred_2d.unsqueeze(0)\n    return _img_pred\n  \nImage(rgb2gray(img_pred)).show(figsize=(8,5))","c1d5a765":"def write_image(fname, _img_tensor):\n    _img_tensor = (_img_tensor * 255).to(dtype=torch.uint8)\n    imwrite(path_submission\/fname, _img_tensor.squeeze().numpy())","479b0907":"import csv\nfrom imageio import imread, imwrite\n\npath_submission.mkdir(exist_ok=True)\n\nwith Path('submission.csv').open('w', encoding='utf-8', newline='') as outf:\n    writer = csv.writer(outf)\n    writer.writerow(('id', 'value'))\n    for i, fname in enumerate(path_test.glob(\"*.png\")):\n        img = open_image(fname)\n        img_id = int(fname.name[:-4])\n        print('Processing: {} '.format(img_id))\n        # Predictions\n        p, img_pred, b = learn.predict(img)\n        # Convert to grayscale and clip out of range values.\n        img_2d = rgb2gray(img_pred).clamp(0, 1)\n        # Write an image file for examination.\n        write_image(fname.name, img_2d)\n        # Write to the submission file, in a very inefficient way.\n        for r in range(img_2d.shape[1]):\n            for c in range(img_2d.shape[2]):\n                id = str(img_id)+'_'+str(r + 1)+'_'+str(c + 1)\n                val = img_2d[0, r, c].item()\n                writer.writerow((id, val))","15f59c27":"# Data Processing","587d6556":"# Setup","e7b94fcc":"## Train","27b02f6b":"# Denoising Document Backgrounds with Fastai Unet\n\n## Purpose\n\nThe purpose of the notebook is to demonstrate the Fastai Library to perform background removal in images using the Unet.  Most of this code comes from his [Super Resolution Notebook](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson7-superres.ipynb) which is exlained in this [fast.ai course video](https:\/\/www.youtube.com\/watch?time_continue=4745&v=9spwoDYwW_I). This was adapted from another [unfinished notebook](https:\/\/www.kaggle.com\/hahmed747\/background-removal-using-fastai-unet-learner).\n\n## Opportunities\n\nThere are a few reasons why this setup as-is won't perform that well:\n* The pretrained vgg model is using photos, not text\n* The pretrained vgg model is using color images, not black and white\n* Insufficient experimentation of hyperparameters\n* The values have to get clipped to 0,1; seems like a bad sign that they are out of that range","de85678e":"## Feature Loss","ca5c209f":"## Processing Test Set\n\nWe could add the test set to the original DataBunch with `add_test_folder`, \n([as discussed](https:\/\/forums.fast.ai\/t\/beginner-question-how-to-predict-on-test-set\/31179)) however this encounters a number of problems. Namely, we want to process\nour entire test images without cropping transformations, but they have difference\nsizes which means a DataBunch (minibatches) can't be used.  \n\nThere is an option to assign a test set when calling `load_learn` but this\nhad problems as well.\n\nWe go with the simple inefficient route of processing them one by one.  This does require a [beautiful hack](https:\/\/forums.fast.ai\/t\/segmentation-mask-prediction-on-different-input-image-sizes\/44389), however which is the first line below.","45a36d60":"# Data Import","d75075af":"## Test on a Validation Example"}}