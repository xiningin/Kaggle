{"cell_type":{"90ef421d":"code","cb1c4f8d":"code","7110dd34":"code","d1dbac4f":"code","71f21f89":"code","a0cc45a7":"code","6b1752aa":"code","707a820b":"code","2e1caaa5":"markdown","59112a35":"markdown","c70582c1":"markdown","123bb904":"markdown","4a453f42":"markdown","8f869bf1":"markdown","558f321c":"markdown"},"source":{"90ef421d":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n","cb1c4f8d":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n#tree classifier\nimport xgboost as xgb\nimport lightgbm as lgb \nfrom sklearn.ensemble import RandomForestClassifier as rf\n#stacking\nfrom sklearn.ensemble import StackingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"XGBoost version:\", xgb.__version__)\nprint(\"XGBoost version:\", lgb.__version__)","7110dd34":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\nexample_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')\nprint (\"Data is loaded!\")","d1dbac4f":"# I have taked this cell from https:\/\/www.kaggle.com\/drcapa\/jane-street-market-prediction-starter-xgb\n# but I am not sure about the choice\ntrain = train[train['weight'] != 0]\ntrain['action'] = (train['resp']>0)*1\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = train.loc[:, 'action']\n\nX_train = X_train.fillna(-999)\ndel train","71f21f89":"lgbclf = lgb.LGBMClassifier(\n        n_estimators=64,\n        max_depth=8,\n        learning_rate=0.01,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        boosting_type= \"gbdt\",\n        nthread=-1,\n        metric=\"AUC\",\n        random_state=2020\n    )\nxgbclf = xgb.XGBClassifier(\n        n_estimators=64,\n        max_depth=8,\n        learning_rate=0.01,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        missing=-999,\n        tree_method='gpu_hist',\n        nthread=-1,\n        random_state=2020\n    )\nrfclf = rf(\n            n_estimators=64,\n            max_depth=8, \n            max_features='sqrt',\n            n_jobs=-1,\n            random_state=2020\n    )","a0cc45a7":"# level-1 ensemble bass models\nmodels = [\n    ('xgb',xgbclf),\n    #('lgb',lgbclf),\n    #('rf',rfclf)\n    \n]\n# level-2 random forest is stacked over the base models\nstack_clf = StackingClassifier(models,final_estimator=rfclf,cv=2)   ","6b1752aa":"%time stack_clf.fit(X_train, y_train)","707a820b":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    X_test.fillna(-999)\n    y_preds = stack_clf.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","2e1caaa5":"# 4.1. **Base models defined**","59112a35":"# 2. **Load training and testing data**","c70582c1":"# 1. **Import necessary libraries**","123bb904":"### **This kernel will serve as a starting point with a potential ensemble model.**\n### **Future work on feature engineering will be incorporated in the future**","4a453f42":"# 4.2. **Models stacking**","8f869bf1":"# 5. **Training and submission generation**","558f321c":"# 3. **Data pre-processing**\nThe naive approach has been taken as a starting point. More will be added in the future."}}