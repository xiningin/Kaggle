{"cell_type":{"1abc4d84":"code","f4eaa491":"code","1fe4e82e":"code","20746424":"code","cb88e99f":"code","3f33431d":"code","d726e86b":"code","4840e995":"code","9e18bdcc":"code","da57c4f7":"code","3653e14d":"code","1684708d":"code","c2607537":"code","28029a1a":"code","a695ca4b":"code","51b45ee1":"code","79cbfabb":"code","75e9fc08":"code","83d592ee":"code","027350b8":"code","0dfbab63":"code","7030d3af":"code","61875f15":"code","066d092c":"code","621c0c5f":"code","8c031e91":"code","2cb37801":"code","5a481860":"code","06b45fa4":"code","3d7628ca":"code","0ebfee22":"code","acd4daa7":"code","4ca3cf1c":"code","cbfdc490":"code","d98f6e64":"code","a025c45e":"code","ac486f9f":"code","ca559f51":"code","0e86c407":"code","c2924b61":"code","301ef410":"code","b80fd0ee":"code","bb303f8c":"code","8d5c905f":"code","14a8f647":"code","6290d9a7":"code","e5043878":"code","ca19e7ce":"code","d70da32a":"code","8b87b45e":"code","de965fbd":"code","0798e6fb":"code","eb1684ab":"code","e3717978":"markdown","a9390741":"markdown","45b92afe":"markdown","6c318e92":"markdown","9360c4e8":"markdown","4d4a5d5a":"markdown","6f40333e":"markdown","b424eb2f":"markdown","a7ad7b1f":"markdown","fa9a2a2c":"markdown","e3c069b9":"markdown","ef655e0d":"markdown","eed9df66":"markdown","afd6260a":"markdown","072d4fdb":"markdown","50940ea8":"markdown"},"source":{"1abc4d84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain_set = pd.read_csv(os.path.join(dirname, 'train.csv'))\ntest_set = pd.read_csv(os.path.join(dirname, 'test.csv'))","f4eaa491":"train_set","1fe4e82e":"train_set.describe()","20746424":"train_set.info()","cb88e99f":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf = train_set\n\ndf.hist(bins=20, figsize =(10,10))","3f33431d":"df = train_set\ncategorical_features = [\"Gender\", \"Vehicle_Age\", \"Vehicle_Damage\"]\nfig, ax = plt.subplots(1, len(categorical_features))\nfor i, categorical_feature in enumerate(df[categorical_features]):\n    df[categorical_feature].value_counts().plot(kind=\"bar\", ax=ax[i], figsize=(25,7)).set_title(categorical_feature)","d726e86b":"def cleaning(train_set):\n    train = train_set.drop('id', axis=1)\n    return train\n\ntrain_copy = cleaning(train_set)\ny = train_copy.Response\nX = train_copy.drop('Response', axis=1)","4840e995":"X","9e18bdcc":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n\nnum_att = ['Age', 'Driving_License', 'Region_Code', 'Previously_Insured', \n           'Annual_Premium', 'Policy_Sales_channel', 'Vintage']\ncat_att = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\ncat_encoder = ColumnTransformer([\n    (\"cat\", OrdinalEncoder(), cat_att)\n], remainder='passthrough')\n\nengineering_pipeline = Pipeline([\n    (\"cat_encoder\", cat_encoder),\n    (\"scaler\", MinMaxScaler())\n])","da57c4f7":"X_prepared = engineering_pipeline.fit_transform(X)","3653e14d":"X_prepared[:5]","1684708d":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nX_train, X_val, y_train, y_val = train_test_split(X_prepared, y, random_state=123)\n","c2607537":"model = LogisticRegression(C=0.01, penalty='l2', n_jobs=-1)\n\ndef train_and_val_score(model, score=roc_auc_score, fit=True):\n    if fit:\n        model.fit(X_train, y_train)\n    y_val_predict = model.predict(X_val)\n    y_train_predict = model.predict(X_train)\n    \n    print(\"Train score:\", score(y_train, y_train_predict)) \n    print(\"Validation score:\", score(y_val, y_val_predict))\n\n    \ndef train_and_val_score_proba(model, score=roc_auc_score, fit=True):\n    if fit:\n        model.fit(X_train, y_train)\n    \n    proba_val_predict = model.predict_proba(X_val)[:,1]\n    proba_train_predict = model.predict_proba(X_train)[:,1]\n    \n    print(\"Train score:\", score(y_train, proba_train_predict)) \n    print(\"Validation score:\", score(y_val, proba_val_predict))\n    \n    \ntrain_and_val_score(model)","28029a1a":"from sklearn.metrics import accuracy_score\ntrain_and_val_score(model, accuracy_score)","a695ca4b":"y_val_predict = model.predict(X_val)\nmax(y_val_predict)","51b45ee1":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_2 = RandomForestClassifier(max_depth=15, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_2)","79cbfabb":"model_2 = RandomForestClassifier(max_depth=25, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_2)","75e9fc08":"model_3 = RandomForestClassifier(max_depth=25, n_estimators=200, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_3)","83d592ee":"model_4 = RandomForestClassifier(max_depth=30, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_4)","027350b8":"model_5 = RandomForestClassifier(max_depth=35, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_5)","0dfbab63":"model_6 = RandomForestClassifier(max_depth=None, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_6)","7030d3af":"model_7 = RandomForestClassifier(max_leaf_nodes=40000, n_jobs=-1, verbose=1)\ntrain_and_val_score(model_7)\ntrain_and_val_score(model_7, f1_score)","61875f15":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel_10 = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=123)\ntrain_and_val_score(model_10)","066d092c":"model_11 = AdaBoostClassifier(n_estimators=500, learning_rate=1.0, random_state=123)\ntrain_and_val_score(model_11)","621c0c5f":"from catboost import CatBoostClassifier\nprint(X_train.shape, X_val.shape)\n\nmodel_13 = CatBoostClassifier(\n    eval_metric='AUC',\n    random_seed=3301,\n)\n\n\nmodel_13.fit(X_train, y_train,\n    eval_set=(X_val, y_val),\n    early_stopping_rounds=30,\n    use_best_model=True,\n    logging_level='Verbose'\n)","8c031e91":"train_and_val_score(model_12, fit=False)","2cb37801":"from xgboost import XGBClassifier, XGBRegressor\nimport xgboost as xgb\n\ndef auc(predt: np.ndarray, dtrain: xgb.DMatrix):\n    y = dtrain.get_label()\n    #predt[predt < -1] = -1 + 1e-6\n    #elements = np.power(np.log1p(y) - np.log1p(predt), 2)\n    return 'AUC', roc_auc_score(y, predt)\n\nmodel_14 = XGBRegressor(n_estimators=200, n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel_14.fit(X_train, y_train,\n            eval_set = [(X_val, y_val)],\n            early_stopping_rounds=20,\n            eval_metric='auc')\n\n","5a481860":"train_and_val_score(model_14, fit=False)\npd.DataFrame(model_14.predict(X_val)).describe()","06b45fa4":"from sklearn.metrics import confusion_matrix\n\ndef full_evaluation(model, fit=True):\n    if fit:\n        model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_train_proba = model.predict_proba(X_train)[:,1]\n    y_val_pred = model.predict(X_val)\n    y_val_proba = model.predict_proba(X_val)[:,1]\n    \n    print(\"---F1 scores---\")\n    print(\"Train score:\", f1_score(y_train, y_train_pred)) \n    print(\"Validation score:\", f1_score(y_val, y_val_pred))\n    \n    print(\"---AUC scores---\")\n    print(\"Train score:\", roc_auc_score(y_train, y_train_proba)) \n    print(\"Validation score:\", roc_auc_score(y_val, y_val_proba))\n    \n    print(\"---confusion---\")\n    print(confusion_matrix(y_val, y_val_pred))\n    \nfull_evaluation(model)","3d7628ca":"from sklearn.ensemble import RandomForestClassifier\n\nmodel2_2 = RandomForestClassifier(max_depth=15, n_jobs=-1, random_state=3301)\nfull_evaluation(model2_2)","0ebfee22":"model2_3 = RandomForestClassifier(max_depth=25, n_jobs=-1, random_state=3301)\nfull_evaluation(model2_3)","acd4daa7":"model2_4 = RandomForestClassifier(max_depth=20, n_jobs=-1, random_state=3301)\nfull_evaluation(model2_4)","4ca3cf1c":"model2_5 = RandomForestClassifier(n_jobs=-1, random_state=3301)\nfull_evaluation(model2_5)","cbfdc490":"model2_5.feature_importances_","d98f6e64":"model2_6 = RandomForestClassifier(max_depth=30, \n                                  n_jobs=-1,\n                                  n_estimators=300,\n                                  random_state=3301)\n\nfull_evaluation(model2_6)","a025c45e":"model2_7 = RandomForestClassifier(max_depth=30, \n                                  n_jobs=-1,\n                                  n_estimators=1000,\n                                  random_state=3301)\n\nfull_evaluation(model2_7)","ac486f9f":"model2_8 = XGBClassifier(n_estimators=500, n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_8.fit(X_train, y_train,\n            verbose=True)\n\nfull_evaluation(model2_8, fit=False)","ca559f51":"model2_9 = XGBClassifier(n_estimators=1000, n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_9.fit(X_train, y_train,\n            verbose=True)\n\nfull_evaluation(model2_9, fit=False)","0e86c407":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_val, model2_8.predict(X_val)))","c2924b61":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = [1. if y_cont > threshold else 0. for y_cont in y]\n    return 'f1',-f1_score(t,y_bin)\n\nmodel2_9 = XGBClassifier(n_estimators=100, learning_rate=1,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_9.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_f1,\n            verbose=True)\n\nfull_evaluation(model2_9, fit=False)","301ef410":"model2_10 = XGBClassifier(n_estimators=400, learning_rate=0.8,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_10.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_f1,\n            early_stopping_rounds=30,\n            verbose=True)\n\nfull_evaluation(model2_10, fit=False)","b80fd0ee":"model2_11 = XGBClassifier(n_estimators=400, learning_rate=0.99,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_11.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_f1,\n            early_stopping_rounds=30,\n            verbose=True)\n\nfull_evaluation(model2_11, fit=False)","bb303f8c":"model2_12 = XGBClassifier(n_estimators=1000, learning_rate=0.99,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel2_12.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_f1,\n            early_stopping_rounds=30,\n            verbose=True)\n\nfull_evaluation(model2_12, fit=False)","8d5c905f":"model_14","14a8f647":"y_proba = model_14.predict(X_val)\ny_pred = np.array(y_proba > 0.3) # trying out different thresholds on the AUC optimized model","6290d9a7":"print(classification_report(y_val, y_pred))","e5043878":"confusion_matrix(y_val, y_pred) #this looks better","ca19e7ce":"f1_score(y_val, y_pred)","d70da32a":"y_pred = np.array(y_proba > 0.25) # trying out different thresholds on the AUC optimized model\nconfusion_matrix(y_val, y_pred) #this looks better","8b87b45e":"f1_score(y_val, y_pred)","de965fbd":"def xgb_auc(y, t, threshold=0.5):\n    t = t.get_label()\n    #y_bin = [1. if y_cont > threshold else 0. for y_cont in y]\n    return 'AUC', -roc_auc_score(t,y)\n\nmodel3_1 = XGBClassifier(n_estimators=500, learning_rate=0.01,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel3_1.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_auc,\n             early_stopping_rounds=30,\n            verbose=True)\n\nfull_evaluation(model3_1)","0798e6fb":"model3_2 = XGBClassifier(n_estimators=3000, learning_rate=0.01,\n                         n_jobs=-1, random_state=3301, \n                        objective='binary:logistic')\n\nmodel3_2.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=xgb_auc,\n             early_stopping_rounds=50,\n            verbose=True)\n\nfull_evaluation(model3_2)","eb1684ab":"model4_1 = XGBClassifier(n_estimators=30000, learning_rate=0.1, max_depth = 8,\n                         n_jobs=-1, random_state=3301, reg_lambda = 1.2, reg_alpha = 1.2,\n                        objective='binary:logistic',\n                        eval_metric='auc')\n\nmodel4_1.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n             early_stopping_rounds=50,\n            verbose=True)\n\nfull_evaluation(model4_1)","e3717978":"## Data Processing","a9390741":"This next cell took 1-2 minutes.","45b92afe":"I will try with a larger number of estimators.","6c318e92":"## Model Selection","9360c4e8":"## Results so far:\n### Optimizing for F1:\n* 0.24 validation score with boosting model2_12 (0.83 AUC)\n### Optimizing for AUC:\n* model_14 has 0.86 AUC, this is competitive with DNNs optimized for AUC that I've seen for this problem. The random forests also generally get good AUC (~0.85) but the threshold of 0.5 is too high so the F1-score suffers.\n\nIt seems that optimizing for F1 is better done through changing the threshold for predictions, (see below for 0.44 F1 score). I did see there are notebooks with ~0.95 AUC which seems possible since I have not really been optimizing for AUC that much.\n","4d4a5d5a":"As the dataset is quite large I will sample a single validation set. (I will use the hidden Kaggle test data for the testing.) Also note that the target variable is imbalanced so accuracy is not a good metric, we will need to use something else like f1_score or AUC.","6f40333e":"This was our simple logistic regression. Now the evaluation gives much more insight into model performance. Next I will try a random forest again.","b424eb2f":"## Fixed evaluation function\nIn continuation I am going to rewrite the evaluation function I had bacause I think I was doing some stupid mistakes with that. I guess auc only works on probabilistic precictions so I added that.","a7ad7b1f":"## Problem Statement\nInsurance companies provide customers a valuable form of hedging against low-probability\/high-impact risks by offering for example a large accident cover sum in exchange for a smaller premium paid upfront. To correctly set the premium\/cover ratio, the insurer must have analyze the probability that the customer will need to claim insurance. \n\nBeyond this principal business problem, the company can use models in many other ways for customer analysis. For example, in this project based on a public Kaggle [dataset](https:\/\/www.kaggle.com\/anmolkumar\/health-insurance-cross-sell-prediction) I will attempt to predict the chance a customer of a health insurance company will also be interested in vehicle insurance from the same company. This could be part of a solution tp estimate the revenues of the company on a longer timeframe.\n\nThe purpose is to obtain good prediction metrics and if possible to focus on models with well justified probabilistic interpretations.","fa9a2a2c":"Notice that accuracy score is high, but we know it is not a good indicator of performance. This prediction is actually null. We need a more complex model. I will try random forests next. It can give probabilities and insight into feature importances.","e3c069b9":"## EDA","ef655e0d":"## (Notes for Sara) (edit: this was a bit mistaken, random forests actually don't do that bad either but I was messing up the evaluation)\n\nThis was to try out the simple methods quickly. It seem random forests also struggle a lot with variance on this problem. Interestingly there is no disadvantage to letting the forest do its thing and overfit the training set, but i think this is because the actual performance is so low. My next steps then would be to try out boosting algorithms or deep learning. (A nonlinear SVM _might_ work if i take a smaller sample of the data I guess.)\n\nThe reason I picked this problem is that I worked before on a very similar seeming thing for the September Tabular Competition on Kaggle. In that competition I saw the best leaderboard teams were slightly above 0.80 AUC score. So far I am doing the same steps as in that. Next I will try an actually good boosting algortihm. In the competition I managed to reach 0.64 AUC with boosting, and I don't think I understand the algorithm in depth.\n\nAlso, I have not looked at other notebooks for this problem yet to see their approaches.","eed9df66":"## (Wrong use of AUC ahead) Experiments with random forests (probably skip running these, could take 10 minutes)\n","afd6260a":"There is no need to handle missing values. All the features seem possibly useful to the task of predicting Response. Driving license is mostly ones for obvious reasons.","072d4fdb":"This cell below seems to use accuracy even i I tell it to use AUC.","50940ea8":"The categorical features will be easy to encode into ints, an ordinal encoder will be needed for Vehicle_Age and it will also make sense for the other cat features since they are binary. The num columns come from different distributions so we could go with min\/max scaling. Id is an useless feature so we will drop it, Response is the target variable."}}