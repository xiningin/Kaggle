{"cell_type":{"8d68da99":"code","1078ce6c":"code","45399408":"code","86e12462":"code","1602fe40":"code","a2008f10":"code","ef06d4d7":"code","989e7841":"code","25e73507":"code","3f70b6e4":"code","f3c4b691":"code","a3a0dd01":"code","327520fa":"code","ee6d687b":"code","49374068":"code","ab33cb56":"code","5f687027":"code","8d037f89":"code","cc82119c":"code","b2bd6997":"code","e111fbf8":"code","8a836726":"code","4d984829":"markdown","03da2ae1":"markdown","de03d38d":"markdown","f42c7b7d":"markdown","566a4deb":"markdown","cd264fc6":"markdown","c0418c86":"markdown","52969594":"markdown","8cf9416e":"markdown","fde9b227":"markdown","748738a3":"markdown","58b66732":"markdown","4d20381d":"markdown","9493ae17":"markdown","d1b8bf68":"markdown","f5572718":"markdown","25f5bc54":"markdown"},"source":{"8d68da99":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom catboost import CatBoostClassifier, Pool\nimport random \n\nimport os\nfrom os import listdir\nfrom tqdm import tqdm\nfrom os.path import isfile\n\nimport sklearn\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\nfrom sklearn.externals import joblib\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.event import Events\nfrom bayes_opt.util import load_logs\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport time\nimport datetime\n\n#import shap\n# load JS visualization code to notebook\n#shap.initjs()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(os.listdir(\"..\/input\"))\nprint()\n\nprint(\"pandas:\", pd.__version__)\nprint(\"numpy:\", np.__version__)\nprint(\"sklearn:\", sklearn.__version__)\nprint()\nprint(\"lightgbm:\", lgb.__version__)\nprint(\"xgboost:\", xgb.__version__)\nprint(\"catboost:\", cb.__version__)","1078ce6c":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","45399408":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(f'Shape of train set: {train.shape}')\nprint(f'Shape of test set: {test.shape}')","86e12462":"sns.countplot(train['isFraud']) #Imbalanced Dataset\nplt.title('Target distribution');","1602fe40":"print(f'Number of fraud samples in train: {len(np.where(train[\"isFraud\"]==1)[0])}')\nprint(f'Percent of fraud samples in train: {round(100.0*len(np.where(train[\"isFraud\"]==1)[0])\/len(train[\"isFraud\"]),2)}')","a2008f10":"train = train.sample(frac=0.1, random_state=42) # comment if you want to run on entire set (takes longer time)\ntrain.reset_index(drop=True, inplace=True)","ef06d4d7":"y = train.isFraud.values\n\ntrain = train.drop('isFraud', axis=1)\ntest = test.copy()\ntrain = train.fillna(-1) #nan substitution could be done in a better way\ntest = test.fillna(-1) \ndel train_transaction, train_identity, test_transaction, test_identity","989e7841":"# Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))  ","25e73507":"cols = list(train.columns)\nlen(cols)","3f70b6e4":"scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","f3c4b691":"N = 50\n\nsvd = TruncatedSVD(n_components=N, random_state=42)\nX = svd.fit_transform(train[cols], y)  \nsvd.explained_variance_ratio_.sum()","a3a0dd01":"df = pd.DataFrame()\ndf[\"target\"] = y\n\nfor i in range(50):\n    df[i] = X[:,i]\n    \ndf.tail()","327520fa":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"reg\"\n\ntrain[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    reg = LogisticRegression(C=1,\n                             solver=\"newton-cg\", \n                             penalty=\"l2\", \n                             n_jobs=-1, \n                             max_iter=100).fit(X_train, y_train) \n    \n    y_pred = reg.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","ee6d687b":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"rfc\"\n\ntrain[arch] = 0\ntest[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    rfc = RandomForestClassifier(n_estimators=100,\n                                 criterion='gini',\n                                 n_jobs=-1).fit(X_train, y_train) \n    \n    y_pred = rfc.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","49374068":"%%time\n\narch = \"lgb\"\n\ntrain[arch] = 0\n\nrounds = 10000\nearly_stop_rounds = 300\n\nparams = {'objective': 'binary',\n          'boosting_type': 'gbrt',\n          'metric': 'auc',\n          'seed': 42,\n          'max_depth': -1,\n          'verbose': -1,\n          'n_jobs': -1}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n\n    d_train = lgb.Dataset(X_train, y_train)\n    d_valid = lgb.Dataset(X_valid, y_valid)    \n\n    model = lgb.train(params,\n                      d_train,\n                      num_boost_round=rounds,\n                      valid_sets=[d_train, d_valid],\n                      valid_names=['train','valid'],\n                      early_stopping_rounds=early_stop_rounds,\n                      verbose_eval=0) \n\n\n    y_pred = model.predict(X_valid)\n    train.loc[valid_index, arch] = y_pred\n    auc = roc_auc_score(y_valid, y_pred)\n    print(i, \"ROC AUC:\", round(auc, 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","ab33cb56":"%%time\n\narch = \"cat\"\n\ntrain[arch] = 0\n\nrounds = 10000\nearly_stop_rounds = 100\n\nparams = {'task_type': 'CPU', #GPU\n          'iterations': rounds,\n          'loss_function': 'Logloss',\n          'eval_metric':'AUC',\n          'random_seed': 42,\n          'learning_rate': 0.5,\n          'depth': 2}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    trn_data = Pool(X_train, y_train)\n    val_data = Pool(X_valid, y_valid)\n    \n    clf = CatBoostClassifier(**params)\n    clf.fit(trn_data,\n            eval_set=val_data,\n            use_best_model=True,\n            early_stopping_rounds=early_stop_rounds,\n            verbose=0)\n    \n    y_pred = clf.predict_proba(X_valid)[:, 1]\n    train.loc[valid_index, arch] = y_pred\n    auc = roc_auc_score(y_valid, y_pred)\n    print(i, \"ROC AUC:\", round(auc, 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","5f687027":"models = [\"cat\", \"lgb\", \"rfc\", \"reg\"] #\"nn\"\n\nfor model in models:\n    train[model] = train[model].rank()\/len(train)\n\ntrain[models].corr(method=\"spearman\")","8d037f89":"for arch in models:\n    print(arch, round(roc_auc_score(y, train[arch]), 5))","cc82119c":"train[\"avg\"] = train[models].mean(axis=1)\nprint(\"avg\", round(roc_auc_score(y, train[\"avg\"]), 5))","b2bd6997":"from scipy.stats.mstats import gmean\n\ndef power_mean(x, p=1):\n    if p==0:\n        return gmean(x, axis=1)\n    return np.power(np.mean(np.power(x,p), axis=1), 1\/p)","e111fbf8":"for power in [0,1,2,4,8]:\n    train[\"avg\"] = power_mean(train[models].values, power)\n    print(power, round(roc_auc_score(y, train[\"avg\"]), 5))","8a836726":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"stack\"\n\ntrain[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = train.loc[train_index, models]\n    X_valid = train.loc[valid_index, models]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    reg = LogisticRegression(C=1,\n                             solver=\"newton-cg\", \n                             penalty=\"l2\", \n                             n_jobs=-1, \n                             max_iter=100).fit(X_train, y_train) \n    \n    y_pred = reg.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","4d984829":"Based on 590k entries in train we should predict 506k for test set.\n\nLet's take a look on target distribution:","03da2ae1":"## LGBM","de03d38d":"## What is not covered (yet)\n\n- SVM\n- KNN\n- FastAI (because of low score)\n- H2O (that approach is already shown in Bojan's kernel: https:\/\/www.kaggle.com\/tunguz\/ieee-with-h2o-automl)","f42c7b7d":"### Reference: \n\nMost of the code is taken from https:\/\/gitlab.com\/ppleskov\/kaggle-days-dubai as I wrote in the beginning of the kernel.\n\nBut it's applies it for Fraud detection task and gives a hint about models which could be used in that competition.","566a4deb":"## Motivation\nThis kernel is dedicated to show simple models comparison, inspired by Pavel Pleskov [pipeline shared during Kaggle Days at Dubai](https:\/\/gitlab.com\/ppleskov\/kaggle-days-dubai). \n\nIn this kenel I am comparing standard algos used for classification task in tabular data. \n\nEvaluation metric is [ROC-AUC](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc). \n\n<left><img src='https:\/\/images.martechadvisor.com\/images\/uploads\/content_images\/frauddetectio_5b60873e86283.jpg'>`","cd264fc6":"## Catboost","c0418c86":"### Standard scaler preprocessing","52969594":"## NN \n\nI also checked FastAI implementation described in Pavel's pipeline, but it gives much less score than previous models.","8cf9416e":"### Loading data","fde9b227":"### Now, let's take a 10 % sample of the dataset to speed up all calculations","748738a3":"Maybe now you know which model to tune. ","58b66732":"## Stacking\n\nLet's stack predictions of previos model and learn LR on them. ","4d20381d":"## Logistic Regression","9493ae17":"Let's merge data as in [starter kernel](https:\/\/www.kaggle.com\/inversion\/ieee-simple-xgboost):","d1b8bf68":"## Correlation of the models","f5572718":"### Blending","25f5bc54":"## Random Forest"}}