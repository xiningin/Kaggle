{"cell_type":{"d62e52e2":"code","3ede4387":"code","a3c8eb02":"code","5f9b80b4":"code","cf474b54":"code","a2d79063":"code","0dece437":"code","569bf3e3":"code","b806b3bf":"code","25933fca":"code","ad6a51fa":"code","19559235":"code","5937222a":"code","06a4661f":"code","4d4ae6de":"code","1f45bdbf":"code","410d23e4":"code","fe48d890":"code","5d8f7c96":"code","4f4e201c":"code","90a58d6c":"code","5e52f50b":"code","67952f72":"code","90c99bc3":"markdown","9409cd29":"markdown","1bddefcf":"markdown","356d0281":"markdown","7a4842ab":"markdown","c786495f":"markdown","5eb5838a":"markdown","9093a0a3":"markdown","84c6bd66":"markdown","0dbe4f1b":"markdown","849e2f1a":"markdown"},"source":{"d62e52e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ede4387":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","a3c8eb02":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","5f9b80b4":"sns.countplot(df_train['target'])","cf474b54":"fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,4))\nsns.barplot(x=df_train[['keyword','location','text']].isnull().sum().index, y=df_train[['keyword','location','text']].isnull().sum().values, ax=ax1)\nsns.barplot(x=df_test[['keyword','location','text']].isnull().sum().index, y=df_train[['keyword','location','text']].isnull().sum().values, ax=ax2)\nax1.set_title('Train_data')\nax2.set_title('Test_data')","a2d79063":"df_train_tmp = df_train.copy()\ndf_train_tmp['len_text']=df_train_tmp['text'].str.len()\ndf_train_tmp['len_words'] = df_train_tmp['text'].str.split().map(lambda x:len(x))","0dece437":"def histplot(x):\n    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n    sns.histplot(data=df_train_tmp[df_train_tmp['target']==0][x],color='blue', ax=ax1)\n    ax1.set_title('Not disaster tweets: {}'.format(x))\n    sns.histplot(data=df_train_tmp[df_train_tmp['target']==1][x], color='red',ax=ax2)\n    ax2.set_title('Disaster tweets: {}'.format(x))","569bf3e3":"histplot('len_text')","b806b3bf":"histplot('len_words')","25933fca":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")","ad6a51fa":"df_train_tmp['text_nlp'] = df_train_tmp['text'].apply(lambda x: nlp(x))\n\ndf_train_tmp['stop_word_count'] = df_train_tmp['text_nlp'].apply(lambda x: sum([w.is_stop for w in x]))\ndf_train_tmp['punct_word_count'] = df_train_tmp['text_nlp'].apply(lambda x: sum([w.is_punct for w in x]))\ndf_train_tmp['hash_tag_count'] = df_train_tmp['text_nlp'].apply(lambda x: len([w.text for w in x if w.text=='#']))\ndf_train_tmp['POS'] = df_train_tmp['text_nlp'].apply(lambda x: [w.pos_ for w in x])","19559235":"histplot('stop_word_count')","5937222a":"histplot('hash_tag_count')","06a4661f":"import re\n\ndef clean_text(text):\n    cleaned_text = re.sub(r'#[a-zA-Z0-9_]+$','',text)\n    cleaned_text = re.sub(r'#([a-zA-Z0-9_]+)','',cleaned_text)\n    cleaned_text = re.sub(r'@[a-zA-Z0-9_]','',cleaned_text)\n    #cleaned_text = re.sub(r'\\n\\n \\n\\n \\n\\n\\n!\"-#$%&()--.+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ','',cleaned_text)\n    cleaned_text = re.sub(r'http[s?:\/\/\\w\/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',cleaned_text)  \n    cleaned_text = re.sub(r':[a-zA-Z0-9_]','',cleaned_text)\n    cleaned_text = re.sub(r'\\x89[\\w\/:%#\\$&\\?\\(\\)~\\.=\\+\\-_]','',cleaned_text)\n    cleaned_text = re.sub(r'[\\n\\t|!#$.-:;)(?]','',cleaned_text)\n    cleaned_text = re.sub(r'[=-><+-]','',cleaned_text)\n    cleaned_text = re.sub(r'\\[IR\\]','',cleaned_text)\n    cleaned_text = re.sub(r'_','',cleaned_text)\n    return cleaned_text","4d4ae6de":"df_train['text_clean_nlp'] = df_train['text'].apply(lambda x: nlp(clean_text(x)))","1f45bdbf":"df_train['text_clean_final'] = df_train['text_clean_nlp'].apply(lambda x: [w.lemma_ for w in x \n                                                         if w.is_stop != 1 \n                                                         and w.is_punct != 1\n                                                         and w.like_num != 1\n                                                        and w.like_url != 1\n                                                        and w.pos_ != 'SYM'\n                                                        and '@' not in x.text\n                                                         and w.text not in '#[a-zA-Z]'\n                                                         and w.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n '\n                                                        and len(w.text)!=1])\ndf_train['text_clean_final'] = df_train['text_clean_final'].apply(lambda x:\" \".join(x))","410d23e4":"df_train[df_train['target']==1].text_clean_final.str.split(expand=True).stack().value_counts()[:50].plot(kind='bar',figsize=(15,5),color='red')\nplt.title('Disaster tweeted words')","fe48d890":"df_train[df_train['target']==0].text_clean_final.str.split(expand=True).stack().value_counts()[:50].plot(kind='bar',figsize=(15,5),color='blue')\nplt.title('Non disaster tweeted words')","5d8f7c96":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nX_train,X_test,y_train,y_test = train_test_split(df_train['text_clean_final'],df_train['target'],test_size=0.3, random_state=42)","4f4e201c":"from sklearn.linear_model import LogisticRegression\npipeline_logi = Pipeline([('tfidf',TfidfVectorizer()),\n                     ('logi_reg',LogisticRegression())])\npipeline_logi.fit(X_train,y_train)\npredictions = pipeline_logi.predict(X_test)\n#print(f'accuracy: {accuracy_score(y_test,predictions)}')\nprint(classification_report(y_test,predictions))\n\nmatrix =confusion_matrix(y_test,predictions)\ndf = pd.DataFrame(matrix, index=['negative','positive'], columns=['predicted negative','predicted positive'])\nprint(df)","90a58d6c":"from sklearn.svm import SVC\npipeline_svc = Pipeline([('tfidf',TfidfVectorizer()),\n                     ('svc',SVC())])\npipeline_svc.fit(X_train,y_train)\npredictions = pipeline_svc.predict(X_test)\n#print(f'accuracy: {accuracy_score(y_test,predictions)}')\nprint(classification_report(y_test,predictions))\n\nmatrix =confusion_matrix(y_test,predictions)\ndf = pd.DataFrame(matrix, index=['negative','positive'], columns=['predicted negative','predicted positive'])\nprint(df)","5e52f50b":"from sklearn.ensemble import GradientBoostingClassifier\npipeline_GBC = Pipeline([('tfidf',TfidfVectorizer()),\n                     ('GBC',GradientBoostingClassifier(max_depth=2,n_estimators=100,learning_rate=1))])\npipeline_GBC.fit(X_train,y_train)\npredictions = pipeline_GBC.predict(X_test)\n#print(f'accuracy: {accuracy_score(y_test,predictions)}')\nprint(classification_report(y_test,predictions))\n\nmatrix =confusion_matrix(y_test,predictions)\ndf = pd.DataFrame(matrix, index=['negative','positive'], columns=['predicted negative','predicted positive'])\nprint(df)","67952f72":"my_predictions = pipeline_svc.predict(df_test['text'])\ndf_test['target'] = my_predictions\ndf_test[['id','target']].to_csv(\"submission01.csv\", index=False)","90c99bc3":"### Support vector machine","9409cd29":"Cleaning with regular expression.","1bddefcf":"### Logistic regression","356d0281":"# Vectorization, Classification, and Metrics","7a4842ab":"# Cleaning","c786495f":"## Null data","5eb5838a":"Cleaning with spaCy attributes. https:\/\/spacy.io\/api\/token#attributes","9093a0a3":"# Cleaned word counts","84c6bd66":"# Submission","0dbe4f1b":"# EDA","849e2f1a":"### Gradient boosting"}}