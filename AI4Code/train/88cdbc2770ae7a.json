{"cell_type":{"d9ddb65e":"code","e6a553e1":"code","d9b77b7f":"code","262468c4":"code","cd599d26":"code","5f2a56ad":"code","84acd72e":"code","5f0a79ef":"code","4fec5347":"code","b9b8d943":"code","82567353":"code","40b2d160":"code","a23b4781":"code","1e2d098d":"code","475bd021":"code","517e8bfd":"code","4383df06":"code","f2af6711":"code","bea72577":"code","43aeec82":"code","daf433c1":"code","130982bc":"code","61ed881d":"code","e4989cbd":"code","0466fa2f":"code","5b13a177":"code","8c02d3d8":"code","e09f985b":"code","7e089508":"code","a2da5668":"code","b601b3bf":"code","9b650f87":"code","615e2983":"code","161db06d":"code","0b0c2faf":"code","f4bf5a6f":"code","657a6ec6":"code","5e9106ad":"code","afac514c":"code","7c7bf950":"code","56fdf467":"code","e50ce74b":"code","31d2f814":"code","9845245e":"code","b034d8ce":"code","145f6ada":"markdown","09c66c7e":"markdown","370a0c8b":"markdown","574e7f44":"markdown","a2721349":"markdown","08fc1edb":"markdown","0bffbca7":"markdown"},"source":{"d9ddb65e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport joblib\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6a553e1":"import spacy\nfrom spacy import displacy\nfrom nltk.corpus import stopwords\nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","d9b77b7f":"allSpeeches = pd.read_json(\"\/kaggle\/input\/mann-ki-baat\/mann_ki_baat.json\")\nprint(allSpeeches.shape)\nallSpeeches.head()","262468c4":"%%time\n# Convert string data in \"date\" field to date format\nfrom datetime import datetime\n\nallSpeeches['date_format'] = pd.to_datetime(allSpeeches['date'], infer_datetime_format=False, errors='coerce')\nallSpeeches['weekday'] = allSpeeches['date_format'].apply(lambda x: x.weekday())\nallSpeeches['month'] = allSpeeches['date_format'].apply(lambda x: x.month)\nallSpeeches['year'] = allSpeeches['date_format'].apply(lambda x: x.year)\n\n# Map month values\nmonthValue ={1: \"January\", 2: \"February\", 3: \"March\", 4:\"April\", \n             5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\",\n            10:\"October\", 11:\"November\", 12:\"December\"}\n\nallSpeeches['monthValue'] = allSpeeches['month'].apply(lambda x: monthValue[x])\n\n# Map day of week\nday_of_week = {0: \"Monday\",1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\",4: \"Friday\" ,5: \"Saturday\",6: \"Sunday\"}\nallSpeeches['day_of_week'] = allSpeeches['weekday'].apply(lambda x: day_of_week[x])\n\n# Count number of lines \nallSpeeches['no_of_lines'] = allSpeeches['allText'].apply(lambda x: len(x.split('.'))) ## assuming sentence ends at '.'\n\n#Count words\nallSpeeches['total_words'] = allSpeeches['allText'].apply(lambda x: sum([1 for i in x.split() if i.isalpha()]))\nallSpeeches['total_words_remove_stopwords'] = allSpeeches['allText'].apply(lambda x: sum([1 for i in x.split() if i not in stopwords.words('english')]))\n\n","cd599d26":"allSpeeches.describe()","5f2a56ad":"allSpeeches.info()","84acd72e":"allSpeeches.head()","5f0a79ef":"# plotly\n# import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n#Import plotly expres\nimport plotly.express as px\n\n\n# word cloud library\nfrom wordcloud import WordCloud\n\n# matplotlib\nimport matplotlib.pyplot as plt","4fec5347":"tr1 = go.Bar(x=allSpeeches.monthValue.unique(),y=allSpeeches.monthValue.value_counts().values)\nlayout = dict(title='Distribution Over Different Months',yaxis_title='Number Of Speeches(Mann ki Baat)')\ngo.Figure(data=[tr1],layout=layout)","b9b8d943":"tr1 = go.Bar(x=allSpeeches.day_of_week.unique(),y=allSpeeches.day_of_week.value_counts().values)\nlayout = dict(title='Distribution Over Different Weekdays',yaxis_title='Number Of Speeches(Mann ki Baat)')\ngo.Figure(data=[tr1],layout=layout)","82567353":"allSpeeches[allSpeeches['day_of_week']!=\"Sunday\"]","40b2d160":"tr1 = go.Scatter(x=allSpeeches.date,y=allSpeeches.no_of_lines.values)\nlayout = dict(title='Lines per speech',yaxis_title='Number Of Lines\/speech')\ngo.Figure(data=[tr1],layout=layout)","a23b4781":"trace0 = go.Box(y=allSpeeches['total_words'],name=\"total_words\")\ntrace1 = go.Box(y=allSpeeches['total_words_remove_stopwords'],name=\"total_words_remove_stopwords\")\ndata = [trace0, trace1]\niplot(data)","1e2d098d":"px.pie(allSpeeches,'year',title='Years Of Mann ki Baat In Our Dataset')","475bd021":"%%time\ndf_dict = allSpeeches[['uuid','allText']].to_dict(orient='records')\nfor sub_dict in df_dict:\n    ent_ls = nlp(sub_dict['allText'])\n    ent_ls = [[ent.text,ent.label_] for ent in ent_ls.ents]\n    sub_dict['named_entity_dict'] = ent_ls","517e8bfd":"%%time\nall_entity_df = pd.DataFrame()\nfor i in range(len(df_dict)):\n    entity_type_df = pd.DataFrame(df_dict[i]['named_entity_dict'],columns=['value','label']).reset_index().groupby(['label'],as_index=False).value.count()\n    entity_type_df['uuid'] = df_dict[i]['uuid']\n    all_entity_df = all_entity_df.append(entity_type_df)\n\nall_entity_val_df = pd.DataFrame()\nfor i in range(len(df_dict)):\n    entity_val_type_df = pd.DataFrame(df_dict[i]['named_entity_dict'],columns=['value','label']).reset_index().groupby(['value','label'],as_index=False).count()\n    entity_val_type_df['uuid'] = df_dict[i]['uuid']\n    all_entity_val_df = all_entity_val_df.append(entity_val_type_df)","4383df06":"all_entity_df.head()","f2af6711":"all_entity_val_df.head()","bea72577":"speech_text_ls = allSpeeches[['uuid','allText']].drop_duplicates()['allText'].values","43aeec82":"tfidf = TfidfVectorizer(stop_words=stopwords.words('english'))\ntfidf_vec = tfidf.fit_transform(speech_text_ls)","daf433c1":"tfidf_df = pd.DataFrame(tfidf_vec.todense())\ntfidf_df.shape","130982bc":"tfidf_df.columns = tfidf.get_feature_names()\ntfidf_df['uuid'] = list(allSpeeches[['uuid']].drop_duplicates().uuid)\n\ntfidf_df.head()","61ed881d":"joblib.dump({\"df_dict\":df_dict,'df':allSpeeches,\n             'all_entity_df':all_entity_df,'all_entity_val_df':all_entity_val_df,\n             'tfidf_df':tfidf_df},\"data.pkl\")","e4989cbd":"data_saved = joblib.load(\"data.pkl\")","0466fa2f":"tfidf_df = data_saved['tfidf_df']\ndf = data_saved['df']\nall_entity_df = data_saved['all_entity_df']\nall_entity_val_df = data_saved['all_entity_val_df']\ndf_dict = data_saved['df_dict']\nall_entity_val_df_agg = all_entity_val_df.groupby(['value','label'],as_index=False)['index'].sum()","5b13a177":"tfidf_df.drop(columns=['uuid']).iloc[0].sort_values(ascending=False).head(20).plot(kind='bar')","8c02d3d8":"all_entity_df.groupby(['label'],as_index=False).value.sum().plot(x='label',y='value',kind='bar')","e09f985b":"# -*- coding: utf-8 -*-\nimport matplotlib\n\nmatplotlib.rc('font', family='Arial')\n\nplt.figure(figsize=(10,15))\nplt.subplots_adjust(wspace=0.8,hspace=1.5)\ncnt = 1\nfor label in all_entity_val_df_agg.label.unique():\n    plt.subplot(6,3,cnt)\n    cnt+=1\n    plt.barh('value','index',\n             data = all_entity_val_df_agg[all_entity_val_df_agg.label==label].sort_values(['index'],ascending=False).head(5))\n    plt.xticks(rotation=90)\n    plt.title(label)","7e089508":"## example of named entity tagging\ndoc = nlp(df_dict[0]['allText'][:1000])\ndisplacy.render(doc, style=\"ent\",jupyter=True)","a2da5668":"from spacy.lang.en import English\nimport string\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora\nimport gensim\nimport pyLDAvis.gensim","b601b3bf":"punctuations= string.punctuation\nparser = English()\nall_stopwords = nlp.Defaults.stop_words","9b650f87":"# This list has been updated on a hunch\n# Ran it till the next two code blocks & then updated the list\nexternal_stopwords = ['people','know','going','say','great','right','want','like','get','good','come','friends','think','thank','...','years','time',\n                     'tell','look','indian','country','lot','new','state','way','go','ok','sir','let','actually','india','little','okay','happen',\n                      'try','remember','hear','best','thing','numbers','pay','beautiful','take', 'countrymen', 'countryman','ji', 'ki', 'dear',\n                      '\"', \"`\", \"-\", \"mann\", \"baat\", \"friend\", \"_\"\n                     ]","615e2983":"def tokenize(txt):\n    tokens = parser(txt)\n    lda_tokens = []\n    for token in tokens :\n        if token.orth_.isspace(): continue\n        elif token.lower_ in all_stopwords : continue\n        elif token.lower_ in punctuations : continue\n        else :\n            lda_tokens.append(token.lower_)\n    return lda_tokens\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None: return word\n    else : return lemma\n\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)\n\n\ndef prepare_data_for_lda(txt):\n    tokens = tokenize(txt)\n    tokens = [get_lemma(word) for word in tokens]\n    tokens = [word for word in tokens if word not in external_stopwords]\n    return tokens","161db06d":"data_saved = joblib.load(\"data.pkl\")\ndf = data_saved['df']\n\ntext_data = [prepare_data_for_lda(txt) for txt in df[['allText','uuid']].drop_duplicates().allText]\n\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(txt) for txt in text_data]\n\nnum_topics = 5\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,num_topics=num_topics,id2word=dictionary,passes=200)\n","0b0c2faf":"topics = lda_model.print_topics(num_words=20)\nplt.figure(figsize=(25,4))\nplt.subplots_adjust(wspace=0.5,hspace=0.9)\nfor idx in range(len(topics)):\n    plt.subplot(1,num_topics,idx+1)\n    split_topics = pd.DataFrame([[float(i.split(\"*\")[0]),i.split(\"*\")[1]] for i in topics[idx][1].split(\"+\")],columns=['weight','word'])\n    plt.barh(\"word\",'weight',data=split_topics)\n    plt.xticks(rotation=90)\n    plt.title(f\"Topic Number {idx}\")","f4bf5a6f":"lda_display = pyLDAvis.gensim.prepare(lda_model,corpus,dictionary,sort_topics=False)\npyLDAvis.display(lda_display )","657a6ec6":"!pip install umap-learn\n!pip install sentence-transformers","5e9106ad":"from sentence_transformers import SentenceTransformer\nimport umap\nfrom sklearn.cluster import DBSCAN,KMeans\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nwarnings.filterwarnings(\"ignore\")","afac514c":"data = df[['allText','uuid']].drop_duplicates().allText.values\n\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\nembeddings = model.encode(data, show_progress_bar=True)","7c7bf950":"## creating clusters\ncluster_bert  =  KMeans(n_clusters = 4 ).fit(embeddings)\n\n## visualization\numap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\nresult = pd.DataFrame(umap_data, columns=['x', 'y'])\nresult['labels'] = cluster_bert.labels_\nplt.scatter(\"x\",\"y\",c='labels',data = result)","56fdf467":"docs_df = pd.DataFrame(data,columns =['doc'])\ndocs_df['labels'] = cluster_bert.labels_\ndocs_df['row_num'] = range(len(docs_df))\n\ndocs_per_id = docs_df.groupby(['labels'],as_index=False).agg({'doc':\" \".join})","e50ce74b":"def c_tf_idf(documents, m,external_stopwords, ngram_range=(1, 1)):\n    count = CountVectorizer(ngram_range=ngram_range, stop_words=list(all_stopwords)+external_stopwords).fit(documents)\n    t = count.transform(documents).toarray()\n    w = t.sum(axis=1)\n    tf = np.divide(t.T, w)\n    sum_t = t.sum(axis=0)\n    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n    tf_idf = np.multiply(tf, idf)\n\n    return tf_idf, count\n  ","31d2f814":"external_stopwords = external_stopwords + ['said','done','ve','got']","9845245e":"tf_idf, count = c_tf_idf(docs_per_id.doc.values,len(data),external_stopwords)\nn_words=20\ntf_idf_transposed = tf_idf.T\nwords = count.get_feature_names()\nlabels = docs_per_id.labels.values\n\nindices = np.argsort(tf_idf_transposed,axis=1)[:,:n_words]\n\ntop_n_words = {i:[(words[j],tf_idf_transposed[i,j]) for j in indices[i]] for i in labels}\n\ntopic_sizes = (docs_df.groupby(['labels'],as_index=False).doc.count()\n               .rename(columns={\"doc\":'size'})\n               .sort_values(by=['size'],ascending=False))","b034d8ce":"import seaborn as sns\n\nplt.figure(figsize=(15,10))\nfor i in range(len(top_n_words)):\n    plt.subplot(2,3,i+1)\n    sns.barplot(\"tfidf\",\"word\",data=pd.DataFrame(top_n_words[i],columns =['word','tfidf']))\n","145f6ada":"## Reading the file and exploring the dataset","09c66c7e":"## Basic EDA on the meta information","370a0c8b":"Large number of programs in October. The skew is because of a higher frequency at the start of the program in OCTOBER 2014","574e7f44":"\n\nHeavily influenced by the following two notebooks:\n\n[Neha Bansal](https:\/\/www.kaggle.com\/nehabansal\/whattrumpissaying-eda-lda-bert-topic-modelling)\n\n[Thomas](https:\/\/www.kaggle.com\/thomaskonstantin\/investigating-donald-trump-s-rallies)","a2721349":"## Extracting some date values (month, year,day)","08fc1edb":"## Extract entities ","0bffbca7":"# W*ork* I*n* P*rogress* ...\n\n![Image of men at work](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/8c\/GERD-Men-at-Work.jpg\/320px-GERD-Men-at-Work.jpg)"}}