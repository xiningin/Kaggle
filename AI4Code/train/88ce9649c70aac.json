{"cell_type":{"5051f81c":"code","28f428a0":"code","7637f8a4":"code","bacfd5d9":"code","4fc5c79d":"code","3cccc440":"code","970a34ae":"code","dd1a7541":"code","71837cbb":"code","e28a96ff":"code","88907c22":"code","d38dbdf4":"code","9cf75895":"code","a7221e7c":"code","cf1e9378":"code","e7bebe76":"code","7e03077d":"code","3d848081":"code","91f230dc":"code","135fa772":"code","9167e8b4":"code","0221d7cf":"code","a1043262":"code","0c8b6850":"code","c51bb88d":"code","7d80b98d":"code","e3d4eb0e":"code","e8f2903f":"code","b714f106":"markdown","a3efafa2":"markdown","3bb404ad":"markdown","80babecf":"markdown","6939243e":"markdown","d29cde70":"markdown","2d449c27":"markdown","8e46ac87":"markdown","07fe4e93":"markdown","d834b386":"markdown","3a12f33e":"markdown","4dd7bb9b":"markdown","bcdf2e9b":"markdown","ce91e66f":"markdown","3aa3a0f9":"markdown","b905734c":"markdown"},"source":{"5051f81c":"import numpy as np\nimport pandas as pd\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\ninit_notebook_mode(connected=True)\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport eli5\nimport shap\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n%env JOBLIB_TEMP_FOLDER=\/tmp","28f428a0":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n    \n\ndef missingData(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum())\/df.isnull().count().sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\n    return missing_data","7637f8a4":"folder_path = '..\/input\/ieee-fraud-detection\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","bacfd5d9":"print(f'Train dataset has {train.shape[0]} examples and {train.shape[1]} features.')\nprint(f'Test dataset has {test.shape[0]} examples and {test.shape[1]} features.')","4fc5c79d":"train_transaction.head(5)","3cccc440":"train_identity.head(5)","970a34ae":"del train_identity, train_transaction, test_identity, test_transaction;","dd1a7541":"import missingno as msno\n\nmissingdata_df = train.columns[train.isnull().any()].tolist()\nmsno.heatmap(train[missingdata_df], figsize=(20,20))","71837cbb":"missing_data = missingData(train)\nmissing_data.head(35)","e28a96ff":"cols_to_drop = missing_data[missing_data['Percent'] > 0.5].index\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","88907c22":"missing_data = missingData(train)\nnull_cols = missing_data[missing_data['Percent']>0].index\nmissing_data.head(20)","d38dbdf4":"for col in null_cols:\n    #print('Data in column {} has format {}.'.format(col, str(train[col].dtype)))\n    train[col] = train[col].replace(np.nan, train[col].mode()[0])\n    test[col] = test[col].replace(np.nan, train[col].mode()[0])\n    #print('Filled the null values of column {}'.format(col))\n    #print('-----------------------------')","9cf75895":"X = train.drop('isFraud', axis=1)\ny = train['isFraud']\n\nprint('The design has shape {}'.format(X.shape))\nprint('The target has shape {}'.format(y.shape))","a7221e7c":"X_cat = X.select_dtypes(include='object')\nX_num = X.select_dtypes(exclude='object')\n\ncat_cols = X_cat.columns.values\nnum_cols = X_num.columns.values\n\nprint('Categorical columns: ', cat_cols)\nprint('Numerical Columns: ', num_cols)","cf1e9378":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = y,\n        histnorm='probability')\n)\n\nfig.update_layout(height=450, width=400, title = 'Distribution of the target [isFraud]')\n\nfig.show()","e7bebe76":"n_rows, n_cols = 2, 5\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cat_cols): continue\n        feature = cat_cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=900, width=1500, title='Distributions of categorical variables')\n        \nfig.show()","7e03077d":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = train['TransactionDT'],\n        histnorm='probability',\n        name = 'Training set')\n)\n\nfig.add_trace(\n    go.Histogram(\n        x = test['TransactionDT'],\n        histnorm='probability',\n        name = 'Test set')\n)\n\nfig.update_layout(height=450, width=900, title = 'Distribution of transaction dates')\n\nfig.show()","3d848081":"X.head()","91f230dc":"cat_cols","135fa772":"encoder = OneHotEncoder(sparse=True)\nencoder.fit(X_cat)\nX_cat_e = encoder.transform(X_cat)\nX_cat_e","9167e8b4":"from scipy.sparse import coo_matrix\nX_num_sparse = coo_matrix(X_num)","0221d7cf":"X_num_sparse","a1043262":"import scipy\nfrom scipy.sparse import hstack\nX_sparse = scipy.sparse.hstack((X_cat_e, X_num_sparse))","0c8b6850":"scaler = RobustScaler(with_centering=False)\nXsc = scaler.fit_transform(X_sparse)\nprint(Xsc.shape)","c51bb88d":"feature_names = []\nfor arr in encoder.categories_:\n    feature_names += list(arr)\nfeature_names += list(num_cols)\nprint(feature_names)","7d80b98d":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n         }","e3d4eb0e":"# folds = TimeSeriesSplit(n_splits=5)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = feature_names\n\n# training_start_time = time.time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xsc, y)):\n#     start_time = time.time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(Xsc[trn_idx,:], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(Xsc[test_idx,:], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time.time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time.time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)","e8f2903f":"# feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n# feature_importances.to_csv('feature_importances.csv')\n\n# plt.figure(figsize=(16, 16))\n# sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n# plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","b714f106":"# EDA","a3efafa2":"# Models","3bb404ad":"### Import modules","80babecf":"## LGBoost Classifier","6939243e":"### Nullity matrix","d29cde70":"n_rows, n_cols =  7, 2\n\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = [ 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=2400, width=1500, title='Distributions of categorical variables')\n        \nfig.show()","2d449c27":"## Define categorical [X_cat] and numerical [X_num] sectors of the design","8e46ac87":"## Define design matrix [X] and target [y]","07fe4e93":"To get started I'm going to lift some functions from [Andrew Lukayenko's fantastic kernel](https:\/\/www.kaggle.com\/artgor\/eda-and-models).","d834b386":"### Imputations","3a12f33e":"n_rows, n_cols =  7, 2\n\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = [ 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=2400, width=1500, title='Distributions of categorical variables')\n        \nfig.show()","4dd7bb9b":"## Preprocess input for modelling","bcdf2e9b":"## Missing data","ce91e66f":"# Preprocessing","3aa3a0f9":"### Deletions","b905734c":"n_rows, n_cols = 2, 5\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=900, width=1500, title='Distributions of numerical variables')\n        \nfig.show()"}}