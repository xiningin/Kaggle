{"cell_type":{"2f2ee442":"code","a52d36d6":"code","2390d07a":"code","36ed0cb3":"code","79be8a76":"code","e5ceeeef":"code","df00e76b":"code","536e2168":"code","f9be8822":"code","dc730d7b":"code","47c7225b":"code","fbe0314f":"code","94178482":"code","a307bf9e":"code","1f0436e7":"code","dc814e01":"code","e4f849f9":"code","c70ab5c7":"code","547bbce1":"code","847c7540":"code","e478188d":"code","18d07e4d":"code","cb107f91":"code","71881224":"code","c0480d85":"code","8f88dd9f":"code","d63f2afc":"code","9148b668":"code","aa58e79b":"code","8f3d7f39":"code","f6a2a813":"code","766c59a7":"code","46c5dfb9":"code","5ecbbf50":"code","124c35cc":"code","80a3e981":"code","5d22ebe2":"code","64b2dc89":"code","0a63d01c":"code","a721fdb7":"code","ec9f0ade":"code","be4c07dc":"code","33a31d2e":"markdown","6b29fa69":"markdown","89692d6c":"markdown","bde5865e":"markdown","c5adc06d":"markdown","20cd11a9":"markdown","11b592cf":"markdown","6b99d614":"markdown","93f522d6":"markdown","be9149aa":"markdown","abb6828b":"markdown","1dca5300":"markdown","880c62d6":"markdown","ffa419f9":"markdown","b41c3999":"markdown","0b5b6c3a":"markdown","2f8af5dd":"markdown","598ed198":"markdown","13897459":"markdown","6201f464":"markdown","54b01092":"markdown","ca0fcab0":"markdown","d1e92076":"markdown","844c00ac":"markdown","945239b3":"markdown","94aad024":"markdown","42e50ace":"markdown","32f3ea56":"markdown","df7745fb":"markdown","3106368e":"markdown","16b9c0e0":"markdown","59a3fe81":"markdown","76257375":"markdown","fdf16df8":"markdown","f8aa8eac":"markdown","4f03ab98":"markdown","7e682dae":"markdown","a5e1769b":"markdown","d7afa6ef":"markdown","046fd97c":"markdown","c5719fd1":"markdown"},"source":{"2f2ee442":"import numpy as np\nimport pandas as pd\nimport seaborn as sbn\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a52d36d6":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_X = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()","2390d07a":"train.drop(\"PassengerId\", axis=1, inplace=True)\ntrain.head()","36ed0cb3":"train_X = train.loc[:,train.columns != \"Survived\"].copy()\ntrain_Y = train.loc[:,[\"Survived\"]].copy()\n\ntrain_X.info()\ntrain_Y.info()","79be8a76":"sum(train_Y.values) \/ len(train_Y)","e5ceeeef":"corr_matrix = train_X.corr()\nsbn.heatmap(corr_matrix, annot=True);","df00e76b":"train_X[\"Ticket\"].nunique()","536e2168":"train_X.drop(\"Ticket\", axis=1, inplace=True)\ntest_X.drop(\"Ticket\", axis=1, inplace=True)","f9be8822":"train_X.drop(\"Cabin\", axis=1, inplace=True)\ntest_X.drop(\"Cabin\", axis=1, inplace=True)","dc730d7b":"train_X.loc[train_X[\"Sex\"] == \"female\", \"Sex\"] = 1\ntrain_X.loc[train_X[\"Sex\"] == \"male\", \"Sex\"] = 0\ntrain_X[\"Sex\"] = train_X[\"Sex\"].astype(np.int64)\n\ntest_X.loc[test_X[\"Sex\"] == \"female\", \"Sex\"] = 1\ntest_X.loc[test_X[\"Sex\"] == \"male\", \"Sex\"] = 0\ntest_X[\"Sex\"] = test_X[\"Sex\"].astype(np.int64)","47c7225b":"train_X.head()","fbe0314f":"test_X.head()","94178482":"train_X.loc[train_X[\"Name\"].str.contains(\"Miss. \"), \"Name\"] = \"Miss\"\ntrain_X.loc[train_X[\"Name\"].str.contains(\"Mrs. \"), \"Name\"] = \"Mrs\"\ntrain_X.loc[train_X[\"Name\"].str.contains(\"Ms. \"), \"Name\"] = \"Ms\"\ntrain_X.loc[train_X[\"Name\"].str.contains(\"Master. \"), \"Name\"] = \"Master\"\ntrain_X.loc[train_X[\"Name\"].str.contains(\"Mr. |Mr \"), \"Name\"] = \"Mr\"\n\nprint(train_X[\"Name\"].value_counts())","a307bf9e":"train_X.loc[train_X[\"Name\"]. \\\n            str.contains(\"Ms\"),\"Name\"] = \"Miss\"\ntrain_X.loc[~train_X[\"Name\"]. \\\n            str.contains(\"Miss|Mrs|Master|Mr\"), \"Name\"] = \"NaN\"\nprint(train_X[\"Name\"].value_counts())","1f0436e7":"train_X.loc[(train_X[\"Name\"] == \"NaN\") &\n            (train_X[\"Sex\"] == 1), \"Name\"] = \"Miss\"\ntrain_X.loc[(train_X[\"Name\"] == \"NaN\") &\n            (train_X[\"Sex\"] == 0), \"Name\"] = \"Mr\"\nprint(train_X[\"Name\"].value_counts())","dc814e01":"n_encoder = OneHotEncoder(sparse=False)\n\n# fit encoder\n# If you run below line as \"n_encoder.fit(train_X[\"Name\"])\"\n# with one square parantheses, you will get an error.\nn_encoder.fit(train_X[[\"Name\"]])\n\n# transform Name column of train_X\nname_one_hot_np = n_encoder.transform(train_X[[\"Name\"]])\n# convert datatype to int64\nname_one_hot_np = name_one_hot_np.astype(np.int64)\n\n# output of transform is a numpy array, convert it to Pandas dataframe\nname_one_hot_df = pd.DataFrame(name_one_hot_np,\n                    columns=[\"Name_1\", \"Name_2\", \"Name_3\", \"Name_4\"])\n\n# concatenate name_one_hot_df to train_X\ntrain_X = pd.concat([train_X, name_one_hot_df], axis=1)\n\n# drop categorical Name column\ntrain_X.drop(\"Name\", axis=1, inplace=True)\n\ntrain_X.head()","e4f849f9":"test_X.loc[test_X[\"Name\"].str.contains(\"Miss. \"), \"Name\"] = \"Miss\"\ntest_X.loc[test_X[\"Name\"].str.contains(\"Mrs. \"), \"Name\"] = \"Mrs\"\ntest_X.loc[test_X[\"Name\"].str.contains(\"Ms. \"), \"Name\"] = \"Ms\"\ntest_X.loc[test_X[\"Name\"].str.contains(\"Master. \"), \"Name\"] = \"Master\"\ntest_X.loc[test_X[\"Name\"].str.contains(\"Mr. \"), \"Name\"] = \"Mr\"\n\nprint(test_X[\"Name\"].value_counts())","c70ab5c7":"test_X.loc[test_X[\"Name\"]. \\\n           str.contains(\"Ms\"), \"Name\"] = \"Miss\"\ntest_X.loc[~test_X[\"Name\"]. \\\n           str.contains(\"Miss|Mrs|Master|Mr\"), \"Name\"] = \"NaN\"\nprint(test_X[\"Name\"].value_counts())","547bbce1":"test_X.loc[(test_X[\"Name\"] == \"NaN\") &\n           (test_X[\"Sex\"] == 1), \"Name\"] = \"Miss\"\ntest_X.loc[(test_X[\"Name\"] == \"NaN\") &\n           (test_X[\"Sex\"] == 0), \"Name\"] = \"Mr\"\n\n# transform Name column of test_X\nname_one_hot_np = n_encoder.transform(test_X[[\"Name\"]])\n# convert datatype to int64\nname_one_hot_np = name_one_hot_np.astype(np.int64)\n\n# output of transform is a numpy array, convert it to Pandas dataframe\nname_one_hot_df = pd.DataFrame(name_one_hot_np,\n                    columns=[\"Name_1\", \"Name_2\", \"Name_3\", \"Name_4\"])\n\n# concatenate name_one_hot_df to test_X\ntest_X = pd.concat([test_X, name_one_hot_df], axis=1)\n\n# drop categorical Name column\ntest_X.drop(\"Name\", axis=1, inplace=True)\n\ntest_X.head()","847c7540":"train_X.info()\ntest_X.info()","e478188d":"print(train_X[\"Embarked\"].value_counts())","18d07e4d":"train_X[\"Embarked\"] = train_X[\"Embarked\"].fillna(\"S\")","cb107f91":"e_encoder = OneHotEncoder(sparse=False)\n\n# fit encoder\ne_encoder.fit(train_X[[\"Embarked\"]])\n\n# transform Embarked column of train_X\ne_one_hot_np = e_encoder.transform(train_X[[\"Embarked\"]])\n# convert datatype to int64\ne_one_hot_np = e_one_hot_np.astype(np.int64)\n\n# output of transform is a numpy array, convert it to Pandas dataframe\ne_one_hot_df = pd.DataFrame(e_one_hot_np,\n                            columns=[\"Emb_1\", \"Emb_2\", \"Emb_3\"])\n\n# concatenate e_one_hot_df to train_X\ntrain_X = pd.concat([train_X, e_one_hot_df], axis=1)\n\n# drop categorical Embarked column\ntrain_X.drop(\"Embarked\", axis=1, inplace=True)","71881224":"# transform Embarked column of test_X\ne_one_hot_np = e_encoder.transform(test_X[[\"Embarked\"]])\n# convert datatype to int64\ne_one_hot_np = e_one_hot_np.astype(np.int64)\n\n# output of transform is a numpy array, convert it to Pandas dataframe\ne_one_hot_df = pd.DataFrame(e_one_hot_np,\n                            columns=[\"Emb_1\", \"Emb_2\", \"Emb_3\"])\n\n# concatenate e_one_hot_df to test_X\ntest_X = pd.concat([test_X, e_one_hot_df], axis=1)\n\n# drop categorical Embarked column\ntest_X.drop(\"Embarked\", axis=1, inplace=True)","c0480d85":"train_X.info()\ntest_X.info()","8f88dd9f":"reg_age = train_X.copy()\n\n# get only rows with non-null age values\nreg_age_train_X = reg_age[reg_age[\"Age\"].notnull()].copy()\nreg_age_train_Y = reg_age_train_X[[\"Age\"]]\nreg_age_train_X.drop(\"Age\", axis=1, inplace=True)","d63f2afc":"reg = xgb.XGBRegressor(colsample_bylevel=0.7,\n                       colsample_bytree=0.5,\n                       learning_rate=0.3,\n                       max_depth=5,\n                       min_child_weight=1.5,\n                       n_estimators=18,\n                       subsample=0.9)\n\nreg.fit(reg_age_train_X, reg_age_train_Y);","9148b668":"age_all_predicted = reg.predict(train_X.loc[:, train_X.columns != \"Age\"])","aa58e79b":"train_X.loc[train_X[\"Age\"].isnull(),\n                    \"Age\"] = age_all_predicted[train_X[\"Age\"].isnull()]","8f3d7f39":"age_all_predicted_t = reg.predict(test_X.loc[:, (test_X.columns != \"Age\")&\n                                    (test_X.columns != \"PassengerId\")])\ntest_X.loc[test_X[\"Age\"].isnull(),\n                       \"Age\"] = age_all_predicted_t[test_X[\"Age\"].isnull()]","f6a2a813":"cls = xgb.XGBClassifier()\n\nparameters = {\n    \"colsample_bylevel\": np.arange(0.4, 1.0, 0.1),\n    \"colsample_bytree\": np.arange(0.7, 1.0, 0.1),\n    \"learning_rate\": [0.4, 0.45, 0.5, 0.55],\n    \"max_depth\": np.arange(4, 6, 1),\n    \"min_child_weight\": np.arange(1, 2, 0.5),\n    \"n_estimators\": [8, 10, 12],\n    \"subsample\": np.arange(0.6, 1.0, 0.1)\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\ngscv = GridSearchCV(\n    estimator=cls,\n    param_grid=parameters,\n    scoring=\"roc_auc\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=True,\n    refit=True,\n)\n\ngscv.fit(train_X, train_Y.values.ravel());\n","766c59a7":"prm = gscv.cv_results_[\"params\"] \nprint(len(prm))","46c5dfb9":"mean_test = gscv.cv_results_[\"mean_test_score\"]\nstd_test = gscv.cv_results_[\"std_test_score\"]\nprint(len(mean_test))\nprint(len(std_test))","5ecbbf50":"#for r in range(len(prm)):\nfor r in range(5):\n    print(\"Parameter set {}\".format(prm[r]))\n    print(\"Test score (mean {:.4f}, std {:.4f}))\". \\\n                  format(mean_test[r], std_test[r]))\n    print(\"\\n\")","124c35cc":"print(\"Best parameters {}\".format(gscv.best_params_))\nprint(\"Mean cross-validated score of best estimator {}\". \\\n                              format(gscv.best_score_))","80a3e981":"cls2 = xgb.XGBClassifier(**gscv.best_params_)\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = train_test_split(train_X, train_Y,\n                                                            test_size=0.2, random_state=0) \n\n# Fit classifier to new split\ncls2.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls2.predict_proba(valid_fold_X)","5d22ebe2":"# Compute ROC AUC score\nsplit_score = roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");","64b2dc89":"# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls2\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))","0a63d01c":"y_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))","a721fdb7":"plot_importance(cls2);","ec9f0ade":"train_fold_X3 = train_fold_X.copy()\nvalid_fold_X3 = valid_fold_X.copy()\n\ntrain_fold_X3.drop(\"Emb_1\", axis=1, inplace=True)\ntrain_fold_X3.drop(\"Emb_2\", axis=1, inplace=True)\n\nvalid_fold_X3.drop(\"Emb_1\", axis=1, inplace=True)\nvalid_fold_X3.drop(\"Emb_2\", axis=1, inplace=True)\n\ncls3 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls3.fit(train_fold_X3, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls3.predict_proba(valid_fold_X3)\n\n# Compute ROC AUC score\nsplit_score = roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls3\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \" with threshold {:.4f}\".\n                                   format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))","be4c07dc":"test_X.drop(\"Emb_1\", axis=1, inplace=True)\ntest_X.drop(\"Emb_2\", axis=1, inplace=True)\n\ntest_predict_proba = cls3.predict_proba(test_X.loc[:,\n                            test_X.columns != \"PassengerId\"])\n\n# Optimal threshold we found previously is opt_thr\ntest_pred = [1 if x > opt_thr else 0 for x in test_predict_proba[:,1]]\n\noutput = pd.DataFrame({\"PassengerId\": test_X[\"PassengerId\"],\n                                   \"Survived\": test_pred})\n\n#output.to_csv(\"my_submission.csv\", index=False)\n\noutput.head(20)","33a31d2e":"In our dataset, there are 5 numerical features (**Pclass**, **Age**, **SibSp**, **Parch** and **Fare**) and 5 categorical features (**Name**, **Sex**, **Ticket**, **Cabin** and **Embarked**). In **train_Y**, our labels are numerical. Let's see the class distribution.","6b29fa69":"In the same manner, convert **Embarked** column of **test_X**.","89692d6c":"So, **mean_test**, **std_test** all have similar length.","bde5865e":"Fit the regressor","c5adc06d":"Since our dataset is not imbalanced, we can use **ROC curve** to find the optimal threshold. Also we compute the area under the **ROC** curve (**ROC AUC**) to get an idea about the skill of the classifier. In case of highly imbalanced datasets, it is better to use precision-recall curves.\n\nWe use **Scikit-Learn** **roc_curve** to draw ROC curve and **roc_auc_score** to compute the area under curve.\n\nIn the code cell below,\n\n<code>split_pred_proba<\/code> holds probabilities for both classes.\n\n<code>split_pred_proba[:,0]<\/code> holds probabilities for class 0.\n\n<code>split_pred_proba[:,1]<\/code> holds probabilities for class 1 (Survived).\n\n","20cd11a9":"We get training samples and corresponding labels. Using **Pandas info** function, we inspect number of columns, column names, column data types and number of non-null values in **train_X** and **train_Y**.\n\nI used **copy()** to work on a separate copy instead of a view. Doing this prevents **SettingWithCopyWarning** in later stages.","11b592cf":"### Convert Categorical Features\n\nLet's start converting categorical features to numerical.\n\nWe convert **Sex** feature to numerical by assigning integers to each class. We do the conversion in both **train_X** and **test_X**. We also change the data type of the column to int64. We could have done this conversion also using Scikit-Learn LabelEncoder.","6b99d614":"We don't have a serious imbalance in our dataset. The distribution is approximately 0.4 to 0.6.\n\nIt is good practice to inspect the correlations between numerical features. When training an ML model, high correlation between features may cause instability and a small change in dataset may cause a big change in model parameters.\n\nWe use **Pandas corr()** function to compute correlations and **Seaborn heatmap** to plot the matrix. The semicolon at the end of seaborn command is used to suppress the outputs other than the plot.","93f522d6":"Do the same prediction and replacement also in **test_X**.","be9149aa":"In **Cabin** column, the number of null values is very high. There are only 204 non-null values in 891 rows. We can drop **Cabin** column from both **train_X** and **test_X**.","abb6828b":"Fill **NaN** values and apply one-hot-encoding.","1dca5300":"Now, we fill **NaN** rows with the help of **Sex** column. Most frequent categories are **Miss** and **Mr**. So we assign **Miss** to **NaN** if female and **Mr** to **NaN** if male.","880c62d6":"The most frequent class is **S**, so we can fill null values with **S**.","ffa419f9":"# Data Analysis and XGBoost Modeling\n\nWe perform exploratory data analysis and feature engineering on titanic dataset. Then we create a binary classifier to predict survived passengers.\n\n* **Data Analysis and Preparation**: We load train and test csv files, create two separate dataframe for training inputs and labels, inspect the features, check the number of null values in each feature, check for balance\/imbalance of the dataset, look at the correlations between numerical features and drop some features if needed. \n\n* **Convert Categorical Features**: We make cleaning and modifications on categorical features and convert them to numerical values using label-encoding and one-hot-encoding.\n\n* **Use Regression for Missing Age Values**: To fill missing values of Age column, we train an XGBoost regressor. Then using this regressor, we predict the missing age values in train and test data.\n\n* **Train a Binary Classifier with XGBoost**: Using the data we prepared in previous stages, we train an **XGBoost** classifier to predict the survived passengers on test set, draw ROC curve, find the optimal operating point and threshold, inspect the importance of features. After dropping some of the least important features, we train the final classifier.\n\nFirst, we import the necessary Python packages.","b41c3999":"Now, we can have a look at our **train_X** and **test_X** again.","0b5b6c3a":"Heatmap shows that **Parch** and **SibSp** has a cross-correlation of 0.41. We can say that there is not any serious correlation problem.\n\nIf we look at the **Ticket** column, we see that this column has large number of different values.","2f8af5dd":"Now we have the best parameters and the best estimator which gives the highest mean cross-validated score. But we don't have the threshold for our classifier. So instead of using best estimator, we will make a train-validation split on (train_X, train_Y). Then we will train and validate a new classifier on this new split with the best parameters found in grid search. The reason to do this is to compute the threshold for our classifier.","598ed198":"Then we put predicted age data to cells with only missing age values in **Age** column.","13897459":"As can be seen above, there are null values in the **Age** column of **train_X** and **test_X**. And there is 1 null value in **Fare** column of **test_X**. **XGBoost** can handle missing values. So we will leave null value in **Fare** column to **XGBoost**. But we will use a different method to impute the null values in **Age** feature.\n\n## Use Regression for Missing Age Values\n\nWe train an **XGBoost** regressor to predict the missing values of **Age** column. We will use **train_X** for the training process of regressor. **Age** column is output, other columns are input to the regressor. The rows with non-null age data are used to train regressor. \n\nFirst prepare the data for age regressor","6201f464":"## Train a Binary Classifier with XGBoost\n\nFinally, our data is ready to train our survival classifier. We train an **XGBoost** classifier using **train_X** and **train_Y**. There are a lot of parameters to tweak, so we use **GridSearchCV** of **Scikit-Learn** to find the best set of parameters.\n\nThe parameters in **param_grid** of **GridSearchCV** can be specified as lists or we can give start, stop and step values using **arange** of **Numpy**. We use **roc_auc** score to find the best parameter set.\n\nWe also create a **stratified k-fold** cross validator. This validator splits our training data to k stratified folds which means class distribution in each fold is similar.","54b01092":"If we want, we can print all scores with related parameters as shown below. Since printing all results will take lots of space, we can print the first 5 parameter combinations and corresponding scores as an example.","ca0fcab0":"There are 2 null values in **Embarked** columns of **train_X**. ","d1e92076":"\n\nSince we want high True Positive Rate and low False Positive Rate, we can set the point closest to (0,1) on **ROC** curve as the optimal operating point.\n\nWe can use a **lambda** function to compute the distance between **(fpr,tpr)** and **(0,1)**. **map** runs **lambda** for all **(fpr,tpr)** tuples. Then **map** object is converted to a list to run **Numpy argmin** on it. **Numpy argmin** returns the index of the point with lowest distance.","844c00ac":"As can be seen from above graph, **Age** and **Pclass** columns are the most important features in survival prediction. It is possible to drop some of the least important features and train a different classifier.","945239b3":"There is one **Ms**, so we assign **Ms** to **Miss** category and **NaN** to all remaining rows. In below code block, ~ negates the rule.","94aad024":"There is one **Ms**, so we assign **Ms** to **Miss** category and **NaN** to all remaining rows. ","42e50ace":"**GridSearchCV** also returns best parameters and best score. The output below is formatted to 4 decimal digits.","32f3ea56":"We drop **PassengerId** column from train data, because we won't use it in model creation.","df7745fb":"## Data Analysis and Preparation\n\nWe load our train and test csv files","3106368e":"**GridSearchCV** returns test scores. There will be 5 (because we use 5-fold CV) splits for each parameter combination in **param_grid**.\n\nFor each split, there will be 1 test score. **mean_test_score** and **std_test_score** are mean and standard deviation of test scores for each parameter combination. We can see how many parameter combinations are tried during grid search.","16b9c0e0":"Also, we can find the accuracy score at the optimal point. Using list comprehension, we convert predicted probabilities to class predictions. We simply compare each probability in <code>split_pred_proba[:,1]<\/code> against our threshold. If probability is higher than threshold, output is 1, otherwise 0.\n\nThen we compute the accuracy using **accuracy_score** of **Scikit-Learn**.\n\n**Remember that this accuracy is the accuracy of cls2 on valid_fold_X with the best parameters found during grid search.**","59a3fe81":"For **Name** column, we assign a new category to all rows. The names containing **Miss** are assigned **Miss** as the new category. Same is done for **Mrs**, **Ms**, **Master** and **Mr**. To differentiate **Mr** and **Mrs**, we use dot in substring search. These abbreviations end with a dot most of the time, but there is one row with **Mr** without dot. So we incorporate in <code>(\"Mr. |Mr \")<\/code> meaning <code>(\"Mr. \")<\/code> or <code>(\"Mr \")<\/code>.","76257375":"Now, we apply the same conversion process to **Name** column of **test_X**. First, we assign each row to one of 5 classes (**Miss**, **Mr**, **Mrs**, **Master** and **NaN**). Then we fill **NaN** values with **Miss** and **Mr**. Finally we apply one-hot-encoding using the encoder we fit above. ","fdf16df8":"Now let's see **train_X** and **test_X** info.","f8aa8eac":"Now, we can apply one-hot-encoding.","4f03ab98":"Now, **Name** column in **train_X** has four classes and no null values. Since we will model our data with **XGBoost** and **XGBoost** does not accept categorical input, we should convert **Name** column to numerical. Next step is to apply one hot encoding to **Name** column. For this purpose we will use **OneHotEncoder** from **Scikit-Learn**. We will fit encoder to **Name** column of **train_X** and then transform **Name** columns of **train_X** and **test_X** using the same encoder.\n\nNote that the output of one-hot-encoder transformation is a **Numpy** array. This **Numpy** array is converted to a **Pandas** dataframe. And during conversion, we give the column names as a list. Then dataframe is concatenated to **train_X**. Finally, categorical **Name** column is dropped. \n\n","7e682dae":"AUC is a measure of the classifier skill considering all different thresholds. But accuracy is computed using a single optimal threshold. Dropping 2 least important features\n\n* ROC AUC on (valid_fold_X,valid_fold_Y) increased from 0.8859 to 0.9007\n* Classification accuracy on (valid_fold_X,valid_fold_Y) increased from 0.8380 to 0.8603","a5e1769b":"So, in Ticket column, there are 681 different entries in 891 rows. This situation is named as **High Cardinality**. We can drop Ticket column from both train_X and test_X.","d7afa6ef":"We predict age data for all rows in **train_X**. We feed all columns except **Age** as input.","046fd97c":"One of the cool properties of **XGBoost** is the built-in function to plot feature importance. If we want, we can use semicolon to suppress the output other than the plot.","c5719fd1":"We should also convert **Embarked** column to numerical using one-hot-encoding. First, let's look if there are any null values."}}