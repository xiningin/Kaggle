{"cell_type":{"98ad343e":"code","b4999f9e":"code","ea221c0a":"code","cb12bf1a":"code","21b90f3a":"code","c1c327d9":"code","dba708d3":"code","c54c175b":"code","fe5028d3":"code","57bddd25":"code","fdfa213e":"code","88698e83":"code","78e1ddd6":"code","0645b182":"code","df1aa538":"code","a565e566":"code","89733748":"code","cd0838c3":"code","bcdd46a6":"code","10bd9471":"code","d6b8699f":"code","2e5ace99":"code","b3ad8d28":"code","d1b4f4b2":"code","37f8d508":"code","09999fe5":"code","489177b2":"code","bfc0bf21":"code","bce85958":"code","bc93452c":"code","fe005c8d":"code","bed6f6f6":"code","700fb65e":"code","2d82e96f":"markdown","933f93a8":"markdown","642acc9e":"markdown","939667a1":"markdown","bbf0851a":"markdown"},"source":{"98ad343e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b4999f9e":"!ls ..\/input\/forest-forest-dataset","ea221c0a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cb12bf1a":"df = pd.read_csv('..\/input\/forest-forest-dataset\/forestfires.csv')","21b90f3a":"# checking columns of dataset \ndf.columns","c1c327d9":"df.head()","dba708d3":"# checking null values \ndf.isnull().sum()","c54c175b":"plt.figure(figsize=(10,10))\ncorr= df.corr()\nsns.heatmap(corr,annot=True)","fe5028d3":"df.dtypes\n#month and days are object type hence applying labelencoder and onehotencoder  ","57bddd25":"# convering categorical data \ndf = pd.get_dummies(df, prefix=['month','day'],drop_first=True)","fdfa213e":"df.shape","88698e83":"y = df.iloc[:,[27]].values\nx = df.iloc[:,:-1].values","78e1ddd6":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y ,test_size = 0.25,random_state= 42)","0645b182":"# Fitting Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(x, y)","df1aa538":"print(y_test.shape, y_train.shape)","a565e566":"x.shape","89733748":"y_pred = regressor.predict(x_test)","cd0838c3":"from sklearn import metrics\nprint('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))","bcdd46a6":"# checking current parameter used in random forest regressor \nregressor.get_params","10bd9471":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","d6b8699f":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","2e5ace99":"print(random_grid)","b3ad8d28":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)","d1b4f4b2":"rf_random.fit(x,y)","37f8d508":"rf_random.best_params_","09999fe5":"hyper_regressor = RandomForestRegressor(n_estimators=800,min_samples_split=2,min_samples_leaf=2,max_features='sqrt',max_depth=20,bootstrap=False)\nhyper_regressor.fit(x, y)","489177b2":"y_pred = hyper_regressor.predict(x_test)","bfc0bf21":"# checking mean absolute error , mean square error , RMSE\nfrom sklearn import metrics\nprint('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, y_pred)))\nprint('root_mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))","bce85958":"# importing required librabries \nimport keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense ,Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LeakyReLU,PReLU,ELU","bc93452c":"#intilaizing the ANN\nann_regressor = Sequential()\n# adding input layer or first hiden layer to regressor \nann_regressor.add(Dense(output_dim=50,init = 'he_uniform',activation='relu',input_dim =27))\n# Adding the second hidden layer\nann_regressor.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n# Adding the third hidden layer\nann_regressor.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n# adding output layer \n# The Output Layer :\nann_regressor.add(Dense(1, init= 'he_uniform',activation='linear'))\n# Compile the network :\nann_regressor.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nann_regressor.summary()","fe005c8d":"model_hist= ann_regressor.fit(x_train,y_train,validation_split=0.20, batch_size = 10, nb_epoch = 50)","bed6f6f6":"ann_pred = ann_regressor.predict(x_test)","700fb65e":"print('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, ann_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, ann_pred)))\nprint('mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, ann_pred))))","2d82e96f":"**Hyper parameter tuning using RandomizedSearchCV**","933f93a8":"**Prediction using ANN**","642acc9e":"after hyper parameter tuning mse,mae has decreesd","939667a1":"here we can see that RH and temp are less corelated. \n","bbf0851a":"**Importing Librabries**"}}