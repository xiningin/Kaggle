{"cell_type":{"f108c317":"code","27e41fd5":"code","42273441":"code","8e84bdec":"code","c0e50520":"code","6657849c":"code","96bfda77":"code","9f77bb6f":"code","1a4c4826":"code","d87524c5":"code","0680b7fa":"code","d4d29890":"code","60c71640":"code","53098c7b":"code","c4d310f7":"code","b5acd199":"code","d5a98f8c":"code","134ddefb":"code","803cecba":"code","a6c36b1a":"code","d2425a64":"code","0a6ea817":"code","8b5be1a5":"code","ad62b5f1":"markdown","0a30ad07":"markdown","cb91a37a":"markdown","341e9bfd":"markdown","6c90decb":"markdown"},"source":{"f108c317":"# Importing Libraries and Data\nimport numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"..\/input\/craigslist-carstrucks-data\/vehicles.csv\")","27e41fd5":"# Get a quick glimpse of what I'm working with\nprint(df.shape)\nprint(df.columns)\ndf.head()","42273441":"# Want to better understand my numerical variables, specifically the min and max (range)\ndf.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","8e84bdec":"# Want to better understand categorical data\ndf.nunique(axis=0)","c0e50520":"df.dtypes","6657849c":"# Remove columns with more than 40% missing values\nNA_val = df.isna().sum()\n\ndef na_filter(na, threshold = .4): #only select variables that passees the threshold\n    col_pass = []\n    for i in na.keys():\n        if na[i]\/df.shape[0]<threshold:\n            col_pass.append(i)\n    return col_pass\n\ndf_cleaned = df[na_filter(NA_val)]\ndf_cleaned.columns","96bfda77":"### Getting rid of outliers for dependent variable ###\ndf_cleaned = df_cleaned[df_cleaned['price'].between(999.99, 250000)] # need to first get rid of unrealistic points to compute IQR more accurately\ndf_cleaned.describe()","9f77bb6f":"# Computing IQR\nQ1 = df_cleaned['price'].quantile(0.25)\nQ3 = df_cleaned['price'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Filtering Values between Q1-1.5IQR and Q3+1.5IQR\ndf_filtered = df_cleaned.query('(@Q1 - 1.5 * @IQR) <= price <= (@Q3 + 1.5 * @IQR)')\ndf_filtered.boxplot('price')","1a4c4826":"# Checking values again\ndf_filtered.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","d87524c5":"# Removing unrealistic outliers for independent variables\n\ndf_filtered = df_filtered[df_filtered['year'].between(1900, 2020)] # cant be newer than 2020\ndf_filtered = df_filtered[df_filtered['odometer'].between(0, 271431.5)] # = 140000 + 1.5 * (140000-52379)\nprint(df_filtered.shape)\nprint(df_filtered.columns)","0680b7fa":"# summary of NA values present\ndf_filtered.isna().sum()","d4d29890":"# Dropping last few columns\n\ndf_final = df_filtered.copy().drop(['id','url','region_url','image_url','region','description','model','state','paint_color'], axis=1) #removing region since lat\/long mean same thing\ndf_final.shape","60c71640":"# Dropping rows with null values\ndf_final = df_final.dropna(axis=0)\ndf_final.shape","53098c7b":"import matplotlib.pylab as plt\nimport seaborn as sns\n\n# calculate correlation matrix\ncorr = df_final.corr()\n# plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","c4d310f7":"# sns.pairplot(df_final)\n\n### Can also use the following if I want to narrow on specific variables ###\n\n# histogram: df_cleaned['price'].plot(kind='hist', bins=50, figsize=(12,6), facecolor='grey',edgecolor='black')\n# boxplot: df_cleaned.boxplot('odometer')\n# scatterplot: df_cleaned.plot(kind='scatter', x='year', y='price')","b5acd199":"df_final['manufacturer'].value_counts().plot(kind='bar')","d5a98f8c":"df_cleaned['type'].value_counts().plot(kind='bar')","134ddefb":"# Converting categorical variables into dummy variables\ndf_final = pd.get_dummies(df_final, drop_first=True)\nprint(df_final.columns)","803cecba":"# Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nX_head = df_final.iloc[:, df_final.columns != 'price']\n\nX = df_final.loc[:, df_final.columns != 'price']\ny = df_final['price']\nX = StandardScaler().fit_transform(X)","a6c36b1a":"# Creating the model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error as mae\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\nmodel = RandomForestRegressor(random_state=1)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","d2425a64":"# Checking accuracy of model\nprint(mae(y_test, pred))\nprint(df_final['price'].mean())","0a6ea817":"from sklearn.metrics import r2_score\nr2_score(y_test, pred)","8b5be1a5":"feat_importances = pd.Series(model.feature_importances_, index=X_head.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","ad62b5f1":"## 2) Data Modelling","0a30ad07":"# Table of Content\n## 1) Exploratory Date Analysis\n### a) Understanding data & cleaning dataset\n### b) Visualizing variables and relationships\n## 2) Data Modelling\n## 3) Feature Importance","cb91a37a":"## 3) Feature Importance","341e9bfd":"## b) Visualizing variables and relationships","6c90decb":"# 1) Exploratory Data Analysis\n\n## a) Understanding data & cleaning dataset\n"}}