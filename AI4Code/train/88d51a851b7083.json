{"cell_type":{"a089c21d":"code","44071079":"code","853d9fd8":"code","7e4f784e":"code","e3952bd0":"code","fe9da494":"code","3d58d7ef":"code","2cd51020":"code","8a8b3413":"code","ed09ba1e":"code","e1bff693":"code","b1aa840c":"code","3adf63c4":"code","cc1211b5":"code","d8a4cf0b":"code","46e9226f":"code","11a58048":"code","2197de9b":"code","0343c690":"code","fe0ba44d":"code","81987440":"code","f76423a4":"code","59644edb":"code","f6cfe3d1":"code","6f7ecb2a":"code","06e6c778":"code","d1267a48":"code","1eb8525e":"code","086ac1d7":"code","8156d043":"code","4df22a5b":"code","bccb75ad":"code","5b0dfac5":"code","03778f5e":"code","7e3eec01":"code","bd49cb5c":"code","64051c50":"code","5a51ada5":"code","4a7ade87":"code","b251bfee":"code","cd24a21b":"code","4f020bca":"markdown"},"source":{"a089c21d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44071079":"df=pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/car data.csv\")","853d9fd8":"df.head()","7e4f784e":"df.shape","e3952bd0":"print(df['Seller_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())","fe9da494":"df.isnull().sum()","3d58d7ef":"df.describe()","2cd51020":"df.columns","8a8b3413":"final_df=df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","ed09ba1e":"df['year']=2020","e1bff693":"df.head()","b1aa840c":"df['no_of_yrs']=df['year']-df['Year']","3adf63c4":"df.head()","cc1211b5":"df.drop(['Year'],axis=1,inplace=True)","d8a4cf0b":"df.head()","46e9226f":"df.drop(['year'],axis=1,inplace=True)","11a58048":"df.head()","2197de9b":"df.drop(['Car_Name'],axis=1,inplace=True)","0343c690":"final_df=pd.get_dummies(df,drop_first=True)","fe0ba44d":"final_df.head()","81987440":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\nsns.heatmap(final_df.corr())","f76423a4":"final_df.head()","59644edb":"x=final_df.iloc[:,1:]\ny=final_df.iloc[:,1]","f6cfe3d1":"y.head()","6f7ecb2a":"#Feature Importance\nfrom sklearn.ensemble import ExtraTreesRegressor\nmod=ExtraTreesRegressor()\nmod.fit(x,y)","06e6c778":"print(mod.feature_importances_)","d1267a48":"#plot graph of feature importances for better visualization \nfeat_importances = pd.Series(mod.feature_importances_, index=x.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","1eb8525e":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)","086ac1d7":"print(xtrain.shape)\nprint(xtest.shape)","8156d043":"from sklearn.ensemble import RandomForestRegressor\nrfreg=RandomForestRegressor()","4df22a5b":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nprint(n_estimators)","bccb75ad":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","5b0dfac5":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","03778f5e":"rf_random = RandomizedSearchCV(estimator = rfreg, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","7e3eec01":"rf_random.fit(xtrain,ytrain)","bd49cb5c":"rf_random.best_params_","64051c50":"rf_random.best_score_","5a51ada5":"predictions=rf_random.predict(xtest)","4a7ade87":"sns.distplot(ytest-predictions)","b251bfee":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(ytest, predictions))\nprint('MSE:', metrics.mean_squared_error(ytest, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(ytest, predictions)))","cd24a21b":"import pickle\n# open a file, where you ant to store the data\nfile = open('model.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","4f020bca":"# Hyperparameter tuning"}}