{"cell_type":{"23a16837":"code","c4a3b96d":"code","b925ce1a":"code","d9cf7ee9":"code","e88e5b57":"code","0dc539b2":"code","5749d1b7":"code","19dbf82c":"code","b5fef171":"code","475d5fc6":"code","1411e238":"code","c14d0c44":"code","39cd6be0":"code","1ce2a38f":"code","ff92ed5b":"code","32925aaa":"code","031554bc":"code","a65f9af8":"code","dc00b2c8":"code","fc18974f":"code","a94d4fd9":"code","31654981":"code","29232c5a":"code","c14b9a38":"code","720d5dd0":"code","69350943":"code","5e556bd0":"code","e4c549a1":"markdown","66ad8e8f":"markdown","49382ff4":"markdown","ddf5c11e":"markdown","eaf5c684":"markdown","a9b7bd39":"markdown","bc8c57d3":"markdown","19c15eea":"markdown","7cf16fd3":"markdown","890c1b1c":"markdown","895d5867":"markdown","e609f54f":"markdown","53f69a18":"markdown","4b337f1c":"markdown","939dfabf":"markdown","5bac0f79":"markdown","8d18c0b7":"markdown"},"source":{"23a16837":"    import h2o\n    from IPython import get_ipython\n    import jupyter\n    import matplotlib.pyplot as plt\n    from pylab import rcParams\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n    import os\n    from h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator\n\n    h2o.init(max_mem_size = 2) # initializing h2o server\n    h2o.remove_all()","c4a3b96d":"    creditData = pd.read_csv(\"..\/input\/creditcard.csv\") # read data using pandas\n    # creditData_df = h2o.import_file(r\"File_Path\\creditcard.csv\") # H2O method\n    creditData.describe()","b925ce1a":"    print(\"Few Entries: \")\n    print(creditData.head())\n    print(\"Dataset Shape: \", creditData.shape)\n    print(\"Maximum Transaction Value: \", np.max(creditData.Amount))\n    print(\"Minimum Transaction Value: \", np.min(creditData.Amount))","d9cf7ee9":"    # Turns python pandas frame into an H2OFrame\n    creditData_h2o  = h2o.H2OFrame(creditData)\n    # check if there is any null values\n    # creditData.isnull().sum() # pandas method\n    creditData_h2o.na_omit() # h2o method\n    creditData_h2o.nacnt() # no missing values found","e88e5b57":"        # Let's plot the Transaction class against the Frequency\n        labels = ['normal','fraud']\n        classes = pd.value_counts(creditData['Class'], sort = True)\n        classes.plot(kind = 'bar', rot=0)\n        plt.title(\"Transaction class distribution\")\n        plt.xticks(range(2), labels)\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Frequency\")","0dc539b2":"    fraud = creditData[creditData.Class == 1]\n    normal = creditData[creditData.Class == 0]","5749d1b7":"    # Amount vs Class\n    f, (ax1, ax2) = plt.subplots(2,1,sharex=True)\n    f.suptitle('Amount per transaction by class')\n\n    ax1.hist(fraud.Amount, bins = 50)\n    ax1.set_title('Fraud List')\n\n    ax2.hist(normal.Amount, bins = 50)\n    ax2.set_title('Normal')\n\n    plt.xlabel('Amount')\n    plt.ylabel('Number of Transactions')\n    plt.xlim((0, 10000))\n    plt.yscale('log')\n    plt.show()","19dbf82c":"    # time vs Amount\n    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n    f.suptitle('Time of transaction vs Amount by class')\n\n    ax1.scatter(fraud.Time, fraud.Amount)\n    ax1.set_title('Fraud List')\n\n    ax2.scatter(normal.Time, normal.Amount)\n    ax2.set_title('Normal')\n\n    plt.xlabel('Time (in seconds)')\n    plt.ylabel('Amount')\n    plt.show()","b5fef171":"    #plotting the dataset considering the class\n    color = {1:'red', 0:'yellow'}\n    fraudlist = creditData[creditData.Class == 1]\n    normal = creditData[creditData.Class == 0]\n    fig,axes = plt.subplots(1,2)\n\n    axes[0].scatter(list(range(1,fraudlist.shape[0] + 1)), fraudlist.Amount,color='red')\n    axes[1].scatter(list(range(1, normal.shape[0] + 1)), normal.Amount,color='yellow')\n    plt.show()","475d5fc6":"    features= creditData_h2o.drop(['Time'], axis=1)","1411e238":"    # 80% for the training set and 20% for the testing set\n    train, test = features.split_frame([0.8])\n    print(train.shape)\n    print(test.shape)\n    #train.describe()\n    #test.describe()","c14d0c44":"    train_df = train.as_data_frame()\n    test_df = test.as_data_frame()\n\n    train_df = train_df[train_df['Class'] == 0]\n    train_df = train_df.drop(['Class'], axis=1)\n\n    Y_test_df = test_df['Class']\n\n    test_df = test_df.drop(['Class'], axis=1)\n\n    train_df.shape","39cd6be0":"    train_h2o = h2o.H2OFrame(train_df) # converting to h2o frame\n    test_h2o = h2o.H2OFrame(test_df)\n    x = train_h2o.columns","1ce2a38f":"    anomaly_model = H2ODeepLearningEstimator(activation = \"Tanh\",\n                                   hidden = [14,7,7,14],\n                                   epochs = 100,\n                                   standardize = True,\n                                    stopping_metric = 'MSE', # MSE for autoencoders\n                                    loss = 'automatic',\n                                    train_samples_per_iteration = 32,\n                                    shuffle_training_data = True,     \n                                   autoencoder = True,\n                                   l1 = 10e-5)\n    anomaly_model.train(x=x, training_frame = train_h2o)","ff92ed5b":"    anomaly_model._model_json['output']['variable_importances'].as_data_frame()","32925aaa":"    # plotting the variable importance\n    rcParams['figure.figsize'] = 14, 8\n    #plt.rcdefaults()\n    fig, ax = plt.subplots()\n\n    variables = anomaly_model._model_json['output']['variable_importances']['variable']\n    var = variables[0:15]\n    y_pos = np.arange(len(var))\n\n    scaled_importance = anomaly_model._model_json['output']['variable_importances']['scaled_importance']\n    sc = scaled_importance[0:15]\n\n    ax.barh(y_pos, sc, align='center', color='green', ecolor='black')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(variables)\n    ax.invert_yaxis()\n    ax.set_xlabel('Scaled Importance')\n    ax.set_title('Variable Importance')\n    plt.show()","031554bc":"    # plotting the loss\n    scoring_history = anomaly_model.score_history()\n    %matplotlib inline\n    rcParams['figure.figsize'] = 14, 8\n    plt.plot(scoring_history['training_mse'])\n    #plt.plot(scoring_history['validation_mse'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')","a65f9af8":"    test_rec_error = anomaly_model.anomaly(test_h2o) \n    # anomaly is a H2O function which calculates the error for the dataset\n    test_rec_error_df = test_rec_error.as_data_frame() # converting to pandas dataframe\n\n    # plotting the testing dataset against the error\n    test_rec_error_df['id']=test_rec_error_df.index\n    rcParams['figure.figsize'] = 14, 8\n    test_rec_error_df.plot(kind=\"scatter\", x='id', y=\"Reconstruction.MSE\")\n    plt.show()","dc00b2c8":"    # predicting the class for the testing dataset\n    predictions = anomaly_model.predict(test_h2o)\n\n    error_df = pd.DataFrame({'reconstruction_error': test_rec_error_df['Reconstruction.MSE'],\n                            'true_class': Y_test_df})\n    error_df.describe()","fc18974f":"    # reconstruction error for the normal transactions in the testing dataset\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    rcParams['figure.figsize'] = 14, 8\n    normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n    _ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","a94d4fd9":"    # reconstruction error for the fraud transactions in the testing dataset\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    rcParams['figure.figsize'] = 14, 8\n    fraud_error_df = error_df[error_df['true_class'] == 1]\n    _ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)","31654981":"    from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                                 roc_curve, recall_score, classification_report, f1_score,\n                                 precision_recall_fscore_support)\n    fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n    roc_auc = auc(fpr, tpr)\n\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.001, 1])\n    plt.ylim([0, 1.001])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show();","29232c5a":"    precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n    plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n    plt.title('Recall vs Precision')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.show()","c14b9a38":"    plt.plot(th, precision[1:], label=\"Precision\",linewidth=5)\n    plt.plot(th, recall[1:], label=\"Recall\",linewidth=5)\n    plt.title('Precision and recall for different threshold values')\n    plt.xlabel('Threshold')\n    plt.ylabel('Precision\/Recall')\n    plt.legend()\n    plt.show()","720d5dd0":"    # plot the testing set with the threshold\n    threshold = 0.01\n    groups = error_df.groupby('true_class')\n    fig, ax = plt.subplots()\n\n    for name, group in groups:\n        ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n                label= \"Fraud\" if name == 1 else \"Normal\")\n    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n    ax.legend()\n    plt.title(\"Reconstruction error for different classes\")\n    plt.ylabel(\"Reconstruction error\")\n    plt.xlabel(\"Data point index\")\n    plt.show();","69350943":"    import seaborn as sns\n    LABELS = ['Normal', 'Fraud']\n    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n    conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n    plt.figure(figsize=(12, 12))\n    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n    plt.title(\"Confusion matrix\")\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()\n","5e556bd0":"    csr = classification_report(error_df.true_class, y_pred)\n    print(csr)","e4c549a1":"# Data Visualization","66ad8e8f":"We need to find a better threshold that can seperate the anomalies from normals. This can be done by getting the intersection of the **Precision\/Recall vs Threshold** graph","49382ff4":"## Variable Importance\nIn H2O there is a special way of analysing the variables which gave more impact on the model.","ddf5c11e":"# Autoencoders\nSo as the algorithm I chose **Autoencoders**, which is a deep learning, unsupervised ML algorithm. \n\"Autoencoding\" is a data compression algorithm, which takes the input and going through a compressed representation and gives the reconstructed output. \n","eaf5c684":"when  building the model, \n4 fully connected hidden layers were chosen with, [14,7,7,14] number of nodes for each layer.\nFirst two for the encoder and last two for the decoder.","a9b7bd39":"The *Time* variable is not giving an impact on the model prediction,. This can be figure out from data visualization. \nBefore moving on to the trainig part, we need to figure out which variables are important and which are not. \nSo we can drop the unwanted variables.","bc8c57d3":"# About H2O\nMachine Learning PLatform used in here is H2O, which is a Fast, Scalable, Open source application for machine\/deep learning. \nBig names such as PayPal, Booking.com, Cisco are using H2O as the ML platform.\nThe speciality of h2o is that it is using in-memory compression to handles billions of data rows in memory, even in a small cluster.\nIt is easy to use APIs with R, Python, Scala, Java, JSON as well as a built in web interface, Flow\nYou can find more information here: https:\/\/www.h2o.ai\n","19c15eea":"**True Positives** - Number of actual frauds predicted as frauds\n\n**False Positives** - Number of non-frauds predicted as frauds\n\n**False Negatives** - Number of frauds predicted as non-frauds.\n","7cf16fd3":"## About the Dataset\nThe Dataset contains 284,807 transactions in total. From that 492 are fraud transactions. So the data itself is highly imbalanced. It contains only numeric input variable. The traget variable is 'Class'","890c1b1c":"### Precision and Recall\nSince the data is highly unbalanced, it cannot be measured only by using accuracy.\nPrecision vs Recall was chosen as the matrix for the classification task.\n\n**Precision**: Measuring the relevancy of obtained results. \n[ True positives \/ (True positives + False positives)]\n\n**Recall**: Measuring how many relevant results are returned.\n[ True positives \/ (True positives + False negatives)]\n\n\n\n\n","895d5867":"### Classification Report\n      ","e609f54f":"### Confusion Matrix","53f69a18":"# Anomaly Detection\nI used an anomaly detection technique for the dataset. \nAnomaly detection is a technique to identify unusual patterns that do not confirm to the expected behaviors. Which is called outliers. \nIt has many applications in business from fraud detection in credit card transactions to fault detection in operating environments.\nMachine learning approaches for Anomaly detection;\n1.     K-Nearest Neighbour\n2.     Autoencoders - Deep neural network\n3.     K-means\n4.     Support Vector Machine\n5.     Naive Bayes\n","4b337f1c":"# Split the Frame","939dfabf":"### ROC Curve","5bac0f79":"# Loading the Dataset\nH2O also have a frame like pandas. So most of the data handling parts can be done using H2OFrame instead of DataFrame","8d18c0b7":"## Evaluating the Testing set\nTesting set has both normal and fraud transactions in it.\nFrom this training method, The model will learn to identify the pattern of the input data.\n If an anomalous test point does not match the learned pattern, the autoencoder will likely have a high error rate in reconstructing this data, indicating anomalous data.\nSo that we can identify the anomalies of the data.\nTo calculate the error, it uses **Mean Squared Error**(MSE)"}}