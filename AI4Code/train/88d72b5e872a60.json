{"cell_type":{"c898e77e":"code","aded6ba1":"code","29de9f36":"code","9d3e4698":"code","e2c4418a":"code","de51c663":"code","3c60db65":"code","0da7bb4c":"code","a70fef81":"code","afd152dc":"code","7a566394":"code","885dbda4":"code","dd7eaf1b":"code","a0a6209f":"markdown","c41cfb40":"markdown","723f2c9e":"markdown","e0b3106b":"markdown","090d6578":"markdown","2a5a09f2":"markdown","db964c87":"markdown","6198962e":"markdown","9570b55e":"markdown","e06332e7":"markdown","5cd9e1db":"markdown","02689541":"markdown"},"source":{"c898e77e":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add\n","aded6ba1":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last unique n_epochs epochs of) the training history\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else len(history['loss']) - n_epochs\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","29de9f36":"# Read the data\noriginal_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n\ngdp_df.set_index('year', inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\noriginal_train_df.head(2)","9d3e4698":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return tf.abs(y_true - y_pred) \/ (y_true + tf.abs(y_pred)) * 200\n","e2c4418a":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}),\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n                                      (df.country == 'Norway')\n                                      for d in list(range(18, 28))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n                                      (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) &\n                                      (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) +\n                                      list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = list(test_df.columns)\nprint(list(features))\n","de51c663":"train_df.iloc[7632 : 7650]","3c60db65":"train_df.shape","0da7bb4c":"train_df = train_df.drop(train_df.index[7632 : 7650])\ntrain_df.shape","a70fef81":"#%%time\nEPOCHS = 1000 # increase the number of epochs if the training curve indicates that a better result is possible\nEPOCHS_COSINEDECAY = 120\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nRUNS = 5 # set to 1 for quick experiments\nDIAGRAMS = True\nUSE_PLATEAU = True\nINFERENCE = False\n\n# We split the features into subsets so that we can apply different\n# regularization schemes for the subsets\nwd_features = [f for f in features if f.startswith('wd')]\nother_features = [f for f in features if f not in wd_features]\n\ndef tpsjan_model():\n    \"\"\"Linear model with flexible regularization\n    \n    The model is to be used with a log-transformed target.\n    \"\"\"\n    wd = Input(shape=(len(wd_features), ))\n    other = Input(shape=(len(other_features), ))\n    wd_contribution = Dense(1, use_bias=False)(wd) # no regularization\n    other_contribution = Dense(1, kernel_regularizer=tf.keras.regularizers.l2(1e-10),\n                               use_bias=True,\n                               bias_initializer=tf.keras.initializers.Constant(value=5.7))(other)\n    output = Add()([wd_contribution, other_contribution])\n    model = Model([wd, other], output)\n    return model\n\n\ndef fit_model(X_tr, X_va=None):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data (select columns and scale)\n    preproc = StandardScaler()\n    X_tr_f = pd.DataFrame(preproc.fit_transform(X_tr[features]), columns=features, index=X_tr.index)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = pd.DataFrame(preproc.transform(X_va[features]), columns=features, index=X_va.index)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n        validation_data = ([X_va_f[wd_features], X_va_f[other_features]], np.log(y_va))\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    if USE_PLATEAU and X_va is not None:\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=25, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else:\n        epochs = EPOCHS_COSINEDECAY\n        lr_start=0.01\n        lr_end=0.00001\n        def cosine_decay(epoch):\n            if epochs > 1:\n                w = (1 + math.cos(epoch \/ (epochs-1) * math.pi)) \/ 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = tpsjan_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n\n    # Train the model\n    history = model.fit([X_tr_f[wd_features], X_tr_f[other_features], ], np.log(y_tr), \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=512,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    \n    if X_va is not None:\n        # Inference for validation\n        y_va_pred = np.exp(model.predict([X_va_f[wd_features], X_va_f[other_features]]))\n        oof_list[run][val_idx] = y_va_pred\n        \n        # Evaluation: Execution time and SMAPE\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}\")\n        score_list.append(smape)\n        \n        if DIAGRAMS and fold == 0 and run == 0:\n            # Plot training history\n            plot_history(history_list[-1], title=f\"Validation SMAPE = {smape:.5f}\", plot_lr=True)\n\n            # Plot y_true vs. y_pred\n            plt.figure(figsize=(10, 10))\n            plt.scatter(y_va, y_va_pred, s=1, color='r')\n            #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n            plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n            plt.gca().set_aspect('equal')\n            plt.xlabel('y_true')\n            plt.ylabel('y_pred')\n            plt.title('OOF Predictions')\n            plt.show()\n\n    return preproc, model\n\n\n# Make the results reproducible\nnp.random.seed(2022)\n\ntotal_start_time = datetime.now()\nhistory_list, score_list, test_pred_list = [], [], []\noof_list = [np.full((len(train_df), 1), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    preproc, model = None, None\n    kf = GroupKFold(n_splits=4)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        print(f\"Fold {run}.{fold}\")\n        preproc, model = fit_model(X_tr, X_va)\n        if INFERENCE:\n            test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n            test_pred_list.append(np.exp(model.predict([test_df_f[wd_features], test_df_f[other_features]])))\n\n\nprint(f\"Average SMAPE: {sum(score_list) \/ len(score_list):.5f}\") # Average over all runs and folds\nwith open('oof.pickle', 'wb') as handle: pickle.dump(oof_list, handle) # for further analysis\n    \nif RUNS > 1:\n    y_va = train_df.num_sold\n    print(f\"Ensemble SMAPE: {np.mean(smape_loss(y_va, sum(oof_list).ravel() \/ len(oof_list))):.5f}\")\nprint(f\"Total time: {str(datetime.now() - total_start_time)[:-7]}\")\n","afd152dc":"# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df_f = pd.DataFrame(preproc.transform(demo_df[features]), columns=features, index=demo_df.index)\n    demo_df['num_sold'] = np.exp(model.predict([demo_df_f[wd_features], demo_df_f[other_features]]))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\nplot_five_years_combination(engineer)\n\n","7a566394":"# Retrain the network on the full training data several times\nRETRAIN_RUNS = 33\nif RETRAIN_RUNS > 0:\n    total_start_time = datetime.now()\n    test_pred_list = []\n    for run in range(RETRAIN_RUNS):\n        preproc, model = None, None\n        print(f\"Retraining {run}\")\n        preproc, model = fit_model(train_df)\n        print(f\"Loss:            {history_list[-1]['loss'][-1]:.6f}\")\n        test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n        test_pred_list.append(np.exp(model.predict([test_df_f[wd_features], test_df_f[other_features]])))\n    print(f\"Total time: {str(datetime.now() - total_start_time)[:-7]}\")\n","885dbda4":"# Ensemble the test predictions\nsub = None\nif len(test_pred_list) > 0:\n    # Create the submission file\n    print(f\"Ensembling {len(test_pred_list)} predictions...\")\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\n    sub.to_csv('submission_keras.csv', index=False)\n    \n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n\nsub","dd7eaf1b":"# Create a rounded submission file\nsub_rounded = None\nif sub is not None:\n    sub_rounded = sub.copy()\n    sub_rounded['num_sold'] = sub_rounded['num_sold'].round()\n    sub_rounded.to_csv('submission_keras_rounded.csv', index=False)\nsub_rounded\n","a0a6209f":"# Training and validation\n\nWe validate using a 4-fold GroupKFold with the years as groups. We show\n- The execution time and the SMAPE\n- The training and validation loss curves with the learning rate\n- A scatterplot y_true vs. y_pred (ideally all points should lie near the diagonal)\n","c41cfb40":"# Demonstration","723f2c9e":"### This notebook is just a fork of the following great notebook:\n\n#### https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-05-keras-quickstart\n\n### Thanks to: @ambrosm","e0b3106b":"# Submission","090d6578":"## **<span style=\"color:darkcyan;\">The leap year(2016)<\/span>**\n\n### **<span style=\"color:darkcyan;\">Delete February 29th<\/span>**","2a5a09f2":"<div class=\"alert alert-success\">    \n<\/div>","db964c87":"I just deleted Feb29th (2016) from the data set. Because 2016 is a leap year and 2019 does not have this day.\n\nYou can see that by deleting this day, especially in any notebook, the score of that notebook will be a little better. Be sure to try.","6198962e":"<div class=\"alert alert-success\">    \n<\/div>","9570b55e":"# Feature engineering","e06332e7":"<div class=\"alert alert-success\">    \n<\/div>","5cd9e1db":"<div class=\"alert alert-success\">    \n<\/div>","02689541":"# Keras quickstart model for the January TPS\n\nThis notebook shows how to use a Keras network in the January 2022 TPS competition.\n\nOn the first of January, I implemented a Keras network for the TPS competition. The network fluctuated between overfitting and divergence. This let me realize that I had to understand the data before implementing the network. I did an [EDA](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense), implemented a [linear model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) and a [LightGBM model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart). Now I'm returning to Keras.\n\nThe network consists of a single dense layer, i.e. it is a linear model. So what is the **advantage of a single-layer neural network** compared to the scikit-learn regressors? The main advantage is that the network is more flexible for experimentation and ready for future improvements: We can play with various regularization schemes or add a hidden layer.\n\nSome points to note:\n- Although Keras could handle SMAPE as a custom loss, I'm using an MSE loss and a log-transformed target. See [this post](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298473) and in particular [this post](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/300611#1649132) for an explanation why MSE with a log-transformed target is the best choice.\n- The network basically uses the same features as my linear model. This contrasts with the LightGBM model, which works with completely other features.\n- Initializing the bias to a suitable non-zero value reduces training time massively.\n- Cross-validation uses a 4-fold GroupKFold with the years as groups.\n- For good test predictions, we need to retrain the network on the full training data (all four years). In this retraining, we cannot use early stopping because there is no data left for validation. I train the network for a fixed number of epochs and use cosine learning rate decay.\n\nExperiment with the notebook - I wish you good luck!\n\nBug reports: Please report all bugs in the comments section of the notebook.\n\nRelease notes:\n- V2: Retrain on full data\n- V3: Feature engineering\n"}}