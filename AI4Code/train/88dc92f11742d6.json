{"cell_type":{"fe46f70a":"code","c0a05d9f":"code","cd1d1cef":"code","28991b5c":"code","22471f0d":"code","b9f4137d":"code","a7340ad1":"code","34340f11":"code","933e68a4":"code","a31c8467":"code","e61ff9a3":"code","0692b2e5":"code","a4c9e394":"code","ddd31eea":"code","7154e06e":"code","7bd55943":"code","8766b34b":"code","9503368d":"code","53c2fa07":"code","49b0314a":"code","0b00d6fe":"code","a8ff408a":"code","f348552f":"code","ec38f1b9":"code","1b8ef461":"code","b6edb833":"code","525083c4":"code","13e82112":"code","e9035b4a":"code","9952d105":"code","0ed80c09":"code","aeb06e9c":"code","839c6ef0":"code","1daa1d7d":"code","2e35194e":"markdown","5a7f0678":"markdown","258508b6":"markdown","feecbe00":"markdown","a8459b99":"markdown","fb5f62fd":"markdown","329a2989":"markdown","2556066a":"markdown","c0246286":"markdown","0c356f20":"markdown","2a459638":"markdown","9a13c11c":"markdown","7461390e":"markdown","cda4fe2d":"markdown","7678d291":"markdown","967d6379":"markdown","e6e00298":"markdown","b49336eb":"markdown","b397deb5":"markdown","89d4614b":"markdown","1bf810b4":"markdown","7484db27":"markdown","7ce4b949":"markdown","e99976b4":"markdown","44fff072":"markdown","47d73e58":"markdown","871d1705":"markdown","0ec59623":"markdown","e4157ff5":"markdown","85ea4a6f":"markdown","695c81e9":"markdown","93b1a1ce":"markdown","757445d0":"markdown","442ed1c0":"markdown","3b51433a":"markdown","8512869b":"markdown","543279b0":"markdown","1605d688":"markdown","8c1c6f96":"markdown","34c55fdd":"markdown","0bb9fafb":"markdown","d9709f23":"markdown","f2ca2075":"markdown","f1dc990e":"markdown","77c11ef7":"markdown","2e1ca191":"markdown","e43bb943":"markdown","af68b3ee":"markdown","d78b6d75":"markdown"},"source":{"fe46f70a":"import os\nfrom fnmatch import fnmatch\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, KFold, learning_curve\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","c0a05d9f":"df = pd.read_csv('..\/input\/appliances-energy-prediction-data-set\/energydata_complete.csv')","cd1d1cef":"df.shape","28991b5c":"df.info()","22471f0d":"df.describe()","b9f4137d":"df.isnull().sum()","a7340ad1":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(19, 17))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':8 }\n    )\n    \n    plt.title('Heatmap de corr\u00e9lation des Features', y=1.05, size=15)\n\ncorrelation_heatmap(df)","34340f11":"df.drop(['rv2', 'rv1', 'Visibility'], axis = 1)","933e68a4":"y = df['Appliances']\nX = df.drop(['Appliances', 'date'], axis = 1)\nX.head()","a31c8467":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","e61ff9a3":"print(\"la taille des donn\u00e9es d'entra\u00eenement est: \",y_train.shape[0])\nprint(\"la taille d'\u00e9chantillon du test est: \",y_valid.shape[0])","0692b2e5":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train) \nX_valid = scaler.transform(X_valid)","a4c9e394":"pca = PCA(n_components = 'mle', svd_solver = 'full')\nX_train = pca.fit_transform(X_train)\nX_valid = pca.transform(X_valid)","ddd31eea":"print(\"le nombre des Principal component analysis est: \",pca.n_components_)","7154e06e":"svm = SVR(kernel = 'rbf')\nsvm.fit(X_train, y_train)","7bd55943":"def evaluation(model):\n    ypred = model.predict(X_valid)\n    \n    rmse = np.sqrt(mean_squared_error(y_valid, ypred))\n    mae = mean_absolute_error(y_valid, ypred)\n    \n    print(\"la valeur du mean_squared_error est: \",rmse)\n    print(\"la valeur mean_absolute_error est: \",mae)\n\n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                              cv=4, scoring='neg_mean_squared_error',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()\n    plt.show()\n    \n    return rmse, mae","8766b34b":"evaluation(svm)","9503368d":"kernel_ridge = KernelRidge(kernel ='rbf', alpha=1.0)","53c2fa07":"kernel_ridge.fit(X_train, y_train)","49b0314a":"evaluation(kernel_ridge)","0b00d6fe":"nbreClasses = 10\ndef rbf(x, c, s):\n    distance = np.linalg.norm(np.array(x) - np.array(c))\n    return 1 \/ np.exp(-distance\/(2*s**2))\n    \ndef kmeans(X, k, itersmax = 100): \n    \n    clusters = X[np.random.choice(range(len(X)), k, replace=False)]\n    converged = False\n    \n    iter = 0\n    while (not converged) and (iter<itersmax):  \n        \n        cluster_list = [[] for i in range(len(clusters))]\n        for x in X:\n            distances = []\n            for c in clusters:\n                distances.append(np.linalg.norm(np.array(x) - np.array(c)))\n            cluster_list[int(np.argmin(distances))].append(x)\n            \n        cluster_list = list((filter(None, cluster_list)))\n        \n        prevClusters = clusters.copy()\n        clusters = []\n        \n        for j in range(len(cluster_list)):\n            clusters.append(np.mean(cluster_list[j], axis = 0))\n            \n        diff = np.abs(np.sum(prevClusters) - np.sum(clusters))\n        converged = (diff==0)\n        \n        iter += 1\n        \n    stds = [np.std(x) for x in cluster_list]\n    return np.array(clusters), stds","a8ff408a":"class RBFNet(object):\n    \n    \"\"\"Implementation of a Radial Basis Function Network\"\"\"\n    def __init__(self, k = 2, lr = 0.01, epochs = 100, rbf = rbf, inferStds = True):\n        self.k = k\n        self.lr = lr\n        self.epochs = epochs\n        self.rbf = rbf\n        self.inferStds = inferStds\n        self.w = np.random.randn(10, k)\n        self.b = np.random.randn(10, 1)\n    \n    def fit(self, X, y):\n        if self.inferStds:\n            # compute stds from data\n            self.centers, self.stds = kmeans(X, self.k)\n        else:\n            # use a fixed std \n            self.centers, _ = kmeans(X, self.k)\n            dMax = np.max([np.linalg.norm(np.array(c1) - np.array(c2)) for c1 in self.centers for c2 in self.centers])\n            self.stds = np.repeat(dMax \/ np.sqrt(2*self.k), self.k)\n        # training\n        for epoch in range(self.epochs):\n            for i in range(X.shape[0]):\n                # forward pass\n                a = np.array([self.rbf(X[i], c, s) for c, s, in zip(self.centers, self.stds)])\n\n                F = a.T.dot(self.w) + self.b\n                loss = (y[i] - F).flatten() ** 2\n\n                # backward pass\n                error = -(y[i] - F).flatten()\n                # online update\n                self.w = self.w - self.lr * a * error\n                self.b = self.b - self.lr * error\n                \n    def predict(self, X):\n        y_pred = []\n        for i in range(X.shape[0]):\n            a = np.array([rbf(X[i], c, s) for c, s in zip(self.centers, self.stds)])\n            F = self.w.dot(a.T) + self.b\n            y_pred.append(F)\n        return np.array(y_pred)","f348552f":"rbf_net = RBFNet(lr = 0.01, k = 15, epochs = 100)\n\nX_train = np.squeeze(np.asarray(X_train))\ny_train = np.squeeze(np.asarray(y_train))\n\n    \nrbf_net.fit(X_train, y_train)","ec38f1b9":"class RBF():\n    def __init__(self, k, epochs, lr=0.01):\n        self.k = k\n        self.lr = lr\n        self.epochs = epochs\n        self.w = np.random.randn(k)\n        self.b = np.random.randn(1)\n\n    def gaussian(self ,x, center, sig):\n        return np.exp((-1 \/ (2 * sig ** 2)) * (x - center) ** 2)\n\n    def sig_finder(self,clusters,X,k):\n        '''\n            computing spreads of clusters\n            finding clusters with 1 or 0 points and compute their spreads as mean of others` spreads\n        '''\n        sigma = np.zeros(k)\n        ideal_cluster = np.argmin(np.squeeze(np.abs(X[:, np.newaxis] - clusters[np.newaxis, :])), axis=1)\n        averg = []\n        outliers = []\n        for i in range(k):\n            pointsForCluster = X[ideal_cluster == i]\n            if len(pointsForCluster) < 2:\n                outliers.append(i)\n            else:\n                averg.append(X[ideal_cluster == i])\n                sigma[i] = np.std(X[ideal_cluster == i])\n        averg = np.concatenate(averg)\n        sigma[outliers] = np.mean(np.std(averg))\n        return sigma\n\n    def clustering(self ,X, k):\n        ''' uniform setting of initial centers '''\n        lx = len(X)\n        diff = lx \/ k\n        o = []\n        for i in range(0, k):\n            o.append(X[int(diff * i)])\n        clusters = np.array(o)\n        ''' random choice of initial centers '''\n        # clusters = np.random.choice(np.squeeze(X), size=k)\n        before = clusters.copy()\n        flag = True\n        while flag:\n            '''find the cluster that's closest to each point'''\n            ideal_cluster = np.argmin(np.squeeze(np.abs(X[:, np.newaxis] - clusters[np.newaxis, :])), axis=1)\n            '''updating each cluster by taking the mean of all of the points in it'''\n            for i in range(k):\n                Numbers = X[ideal_cluster == i]\n                if len(Numbers) > 0:\n                    clusters[i] = np.mean(Numbers, axis=0)\n            flag = np.average(clusters - before) > 0.000001\n            before = clusters.copy()\n        sigma = self.sig_finder(clusters,X,k)\n        return clusters, sigma\n\n    def fit(self, X, y):\n        self.centers, self.stds = self.clustering(X, self.k)\n        for epoch in range(self.epochs):\n            if epoch % 100 ==0:\n                print(\"epoch \"+str(epoch))\n            for i in range(len(X)):\n                # forward pass\n                sig=np.array(self.gaussian(X[i], self.centers, self.stds))\n                Y = np.dot(sig,self.w) + self.b\n                # backward pass\n                error = -(y[i] - Y)\n                self.w = self.w - self.lr * sig * error\n                self.b = self.b - self.lr * error\n\n    def predict(self, X):\n        y_pred = []\n        for i in range(len(X)):\n            sig = np.array(self.gaussian(X[i], self.centers, self.stds))\n            Y = sig.dot(self.w) + self.b\n            y_pred.append(Y)\n        return np.array(y_pred)","1b8ef461":"rbf_net = RBF(lr = 0.01, k = 15, epochs = 200)\n\nX_train = np.squeeze(np.asarray(X_train))\ny_train = np.squeeze(np.asarray(y_train))\n\n    \nrbf_net.fit(X_train, y_train)","b6edb833":"from keras.layers import Layer\nfrom keras import backend as K\n\nclass RBFLayer(Layer):\n    def __init__(self, units, gamma, **kwargs):\n        super(RBFLayer, self).__init__(**kwargs)\n        self.units = units\n        self.gamma = K.cast_to_floatx(gamma)\n\n    def build(self, input_shape):\n        self.mu = self.add_weight(name='mu',\n                                  shape=(int(input_shape[1]), self.units),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(RBFLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        diff = K.expand_dims(inputs) - self.mu\n        l2 = K.sum(K.pow(diff, 2), axis=1)\n        res = K.exp(-1 * self.gamma * l2)\n        return res\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)","525083c4":"X_train.shape","13e82112":"from keras.layers import Dense, Flatten\nfrom keras.models import Sequential\nfrom keras.losses import binary_crossentropy\n","e9035b4a":"RBF_Keras = Sequential()\nRBF_Keras.add(Flatten(input_shape=(26,)))\nRBF_Keras.add(RBFLayer(100, 0.5))\nRBF_Keras.add(RBFLayer(60, 0.5))\nRBF_Keras.add(Dense(1, activation='linear'))","9952d105":"RBF_Keras.compile(optimizer='rmsprop', loss = 'mean_absolute_error')","0ed80c09":"RBF_Keras.summary()","aeb06e9c":"history = RBF_Keras.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs = 100, batch_size = 8)","839c6ef0":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","1daa1d7d":"ypred = RBF_Keras.predict(X_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, ypred))\nmae = mean_absolute_error(y_valid, ypred)\nprint(\"la valeur du mean_squared_error est: \",rmse)\nprint(\"la valeur mean_absolute_error est: \",mae)","2e35194e":"<span style=\"color:grey; font-size:1.2em\">Nous pouvons donc simplement utiliser le code suivant pour <b>former et pr\u00e9dire<\/b> avec notre mod\u00e8le<span>","5a7f0678":"<span style=\"color:grey; font-size:1.2em\">On choisit les param\u00e8tres de la m\u00e9thode `.compile()`. <br>pour le param\u00e8tre <b>loss<\/b> est <i>mean_absolute_error<\/i> . pour <b>optimizer<\/b> on choisit: <i>rmsprop<\/i> ( Root Mean Squared Propagation ).<\/span>","258508b6":"<span style=\"color:grey; font-size:1.2em\">Cr\u00e9ons une <b>\u00e9valuation<\/b> de fonction pour tester nos mod\u00e8les et ploter <b>le score<\/b> sur les donn\u00e9es d'entra\u00eenement et de validation.<\/span>","feecbe00":"<span style=\"color:grey; font-size:1.2em\">Nous pouvons \u00e9galement cr\u00e9er <b>une classe<\/b> qui contiendra toutes <b>les fonctions requises<\/b> \u00e0 un seul object comme suit.<\/span>","a8459b99":"<center id=\"Analysis-section\">\n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Analyses et Transformations \ud83d\udd0e\n        <\/h2>\n<\/center>","fb5f62fd":"### Informations sur chaque feature\u00a0:\n\n- date heure ann\u00e9e-mois-jour heure:minute:seconde\n- Appliances, consommation d'\u00e9nergie en Wh (variable cible pour la pr\u00e9diction)\n- lights, consommation d'\u00e9nergie des appareils d'\u00e9clairage dans la maison en Wh\n- T1, Temp\u00e9rature dans le coin cuisine, en degr\u00e9s Celsius\n- RH_1, Humidit\u00e9 dans le coin cuisine, en %\n- T2, Temp\u00e9rature dans le s\u00e9jour, en Celsius\n- RH_2, Humidit\u00e9 dans le s\u00e9jour, en %\n- T3, Temp\u00e9rature dans la zone buanderie\n- RH_3, Humidit\u00e9 dans la buanderie, en %\n- T4, Temp\u00e9rature dans le bureau, en degr\u00e9s Celsius\n- RH_4, Humidit\u00e9 dans le bureau, en %\n- T5, Temp\u00e9rature dans la salle de bain, en degr\u00e9s Celsius\n- RH_5, Humidit\u00e9 dans la salle de bain, en %\n- T6, Temp\u00e9rature \u00e0 l'ext\u00e9rieur du b\u00e2timent (c\u00f4t\u00e9 nord), en degr\u00e9s Celsius\n- RH_6, Humidit\u00e9 \u00e0 l'ext\u00e9rieur du b\u00e2timent (c\u00f4t\u00e9 nord), en %\n- T7, Temp\u00e9rature dans la salle de repassage, en Celsius\n- RH_7, Humidit\u00e9 dans la salle de repassage, en %\n- T8, Temp\u00e9rature chambre ado 2, en Celsius\n- RH_8, Humidit\u00e9 chambre ado 2, en %\n- T9, Temp\u00e9rature dans la chambre des parents, en Celsius\n- RH_9, Humidit\u00e9 dans la chambre des parents, en %\n- To, Temp\u00e9rature ext\u00e9rieure (de la station m\u00e9t\u00e9o de Chi\u00e8vres), en degr\u00e9s Celsius\n- Pressure (de la station m\u00e9t\u00e9o de Chi\u00e8vres), en mm Hg\n- RH_out, Humidit\u00e9 ext\u00e9rieure (de la station m\u00e9t\u00e9o de Chi\u00e8vres), en %\n- Wind speed (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en m\/s\n- Visibility (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en km\n- Tdewpoint (de la station m\u00e9t\u00e9o de Chi\u00e8vres), \u00b0C\n- va1, Variable al\u00e9atoire 1, non dimensionnelle\n- va2, Variable al\u00e9atoire 2, non dimensionnelle","329a2989":"<span style=\"color:grey; font-size:1.2em\">Voici les r\u00e9sultats du <b>SVM<\/b> avec le noyau <b>RBF<\/b>.<\/span>","2556066a":"<span style=\"color:grey; font-size:1.2em\">Voyons un <b>summary<b> des param\u00e8tres de notre mod\u00e8le<\/span>","c0246286":"<div id=\"split-data\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Diviser les donn\u00e9es\n        <\/h3>\n<\/div>","0c356f20":"\n\n<span style=\"color:grey; font-size:1.2em\">Nous commen\u00e7ons par diviser notre jeu de donn\u00e9es <b>80%<\/b> pour l'entra\u00eenement et <b>20%<\/b> pour la validation.<\/span>\n","2a459638":"<span style=\"color:grey; font-size:1.2em\">Les donn\u00e9es n'ont pas de <b>valeurs manquantes<\/b>, nous n'effectuerons donc pas un <b>nettoyage des donn\u00e9es.<\/b> <\/span>","9a13c11c":"<div style=\"font-size:1.3em\">\n    <span>\n    R\u00e9alis\u00e9 par :\u00b6\n    <\/span>\n      <ul>\n         <li> ba-haddou Ayoub<\/li>\n         <li>Lazrek Imane<\/li>\n         <li>Salmi Ali<\/li>\n      <\/ul>\n    <span>\n    Encadr\u00e9 par :\u00b6\n    <\/span>\n      <ul>\n         <li>M.AIT KBIR Mohamed<\/li>\n      <\/ul>\n<\/div>","7461390e":"<h4 style=\"color:grey\"> Explorant notre jeu de donn\u00e9es <\/h4>","cda4fe2d":"<span style=\"color:grey; font-size:1.2em\">Dans le cadre de notre etude Ce projet qui vise a Utilisation des <b>r\u00e9seaux RBF<\/b> on va analyser, transformer notre jeu donnees et modeliser apr\u00e8s en utilisant plusieurs mod\u00e8les.\n<\/span>","7678d291":"<div id=\"RBF\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  R\u00e9seaux RBF\n        <\/h3>\n<\/div>","967d6379":"<div id=\"RBF-Keras\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  R\u00e9seaux RBF avec Keras\n        <\/h3>\n<\/div>","e6e00298":"<span style=\"color:grey; font-size:1.2em\">Voici les r\u00e9sultats du <b>kernel_ridge<\/b> avec <b>RBF<\/b> et une valeur d'<b>alphe = 1<\/b>.<\/span>","b49336eb":"<div id=\"eda\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Exploration de donn\u00e9es\n        <\/h3>\n<\/div>","b397deb5":"<span style=\"color:grey; font-size:1.2em\">Cr\u00e9ons un objet contenant <b>l'historique<\/b> de notre formation de mod\u00e8le:<\/span>","89d4614b":"<span style=\"color:grey; font-size:1.2em\">C'est temps d' <b>\u00e9valuer<\/b> notre mod\u00e8le \u00e0 la fois sur les donn\u00e9es d'entra\u00eenement et de test et d'obtenir <b>scores<\/b> de pr\u00e9cision.<\/span>","1bf810b4":"<span style=\"color:grey; font-size:1.2em\">Un aper\u00e7u des donn\u00e9es en utilisant les fonctions <b>info()<\/b> et <b>describe()<\/b> du pandas pour examiner les donn\u00e9es. <\/span>","7484db27":"<div id=\"KR\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Kernel Ridge\n        <\/h3>\n<\/div>","7ce4b949":"<div style=\"color:grey; font-size:1.2em\">Le travail que nous avons r\u00e9alis\u00e9 a consist\u00e9 \u00e0 exploirer le cycle de vie d'un project de Data Science en general et du Machine Learning et Deep Learning en particulier. On bien appris comment analyser et transformer les donn\u00e9es, dans la partie du Pr\u00e9-traitement on a diviser les donn\u00e9es et vectoriser les phrases, et derni\u00e8rement la construction des mod\u00e8les en essayant diff\u00e9rentes approches: SVF et RBF NET.\n<br>\n<br>\n<span style=\"color:black; font-size:1.2em; background-color:#FFFFA6\">Ce projet nous a permis d'acqu\u00e9rir les techniques d'utiliser des r\u00e9seaux RBF pour l\u2019analyse et pr\u00e9dire.<\/span>\n<\/div>","e99976b4":"<center id=\"model-building\">\n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Construction des mod\u00e8les \ud83d\udee0\ufe0f\n        <\/h2>\n<\/center>","44fff072":"<div id=\"overview\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Aper\u00e7u\n        <\/h3>\n<\/div>","47d73e58":"<div id=\"SVM\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  SVM\n        <\/h3>\n<\/div>","871d1705":"<center id=\"Conclusion\">\n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Conclusion \ud83d\udccc\n        <\/h2>\n<\/center>","0ec59623":"<div id=\"dataset\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Jeu de donn\u00e9es\n        <\/h3>\n<\/div>","e4157ff5":"<center id=\"Intro-section\">\n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Introduction \ud83d\udcd6\n        <\/h2>\n<\/center>","85ea4a6f":"<center><h1 style=\"color:#1a1a1a;\n                    font-size:3em\">\n        Projet: \ud83e\udd16 M\u00e9thodologies de l\u2019IA \n        <\/h1> \n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Exercice 2: Utilisation des r\u00e9seaux RBF.\n       <\/h2>\n<\/center>\n\n","695c81e9":"<span style=\"color:grey; font-size:1.2em\">m\u00eame chose pour <b>le modelling<\/b>.","93b1a1ce":"<center id=\"pre-processing\">\n        <h2 style=\"color:#1a1a1a;\n                    font-size:2em\">\n        Pr\u00e9-traitement \u2699\ufe0f\n        <\/h2>\n<\/center>","757445d0":"<span style=\"color:grey; font-size:1.2em\">Maintenant que nous avons cr\u00e9\u00e9 notre <b>fonction rbf<\/b> et une autre fonction pour <b>le clustering kmeans<\/b>, nous cr\u00e9ons ensuite <b>une class<\/b>e dans laquelle nous allons d\u00e9finir le processus <b>d'apprentissage et de pr\u00e9diction<\/b>.<span>","442ed1c0":"<span style=\"color:grey; font-size:1.2em\">Maintenant nous cr\u00e9ons un nouveau mod\u00e8le `Sequential()` puis nous ajoutons<b> 4 couches<\/b>. <br>\n    ---La premi\u00e8re couche est Flatten qui est utilis\u00e9e pour rendre l'entr\u00e9e multidimensionnelle unidimensionnelle,<br>\n    ---Les deuxi\u00e8me et troisi\u00e8me couches (Hidden layers) sont notre RBFLayer,<br>\n    ---La derni\u00e8re couche a un activation='linear' pour donner une valeur pr\u00e9dite d'Appliances.<br>\n<\/span>","3b51433a":"<span style=\"color:grey; font-size:1.2em\">Tout d'abord, nous allons essayer de pr\u00e9dire la valeur des 'Appliances' en utilisant une <b>SVM<\/b> (Support Vector Machine) avec un <b>kernel 'rbf'<\/b>.<\/span>","8512869b":"<span style=\"color:grey; font-size:1.2em\">Tra\u00e7ons une Heatmap de corr\u00e9lation des Features.\n<\/span>","543279b0":"<span style=\"color:grey; font-size:1.2em\">Nous supprimerons les colonnes <b>'rv2', 'rv1', 'Visibilit\u00e9'<\/b> car elles n'ont clairement aucun impact sur la valeur <b>'Appliances'<\/b>.\n<\/span>","1605d688":"<div id=\"trans\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  Transformation de donn\u00e9es\n        <\/h3>\n<\/div>","8c1c6f96":"<span style=\"color:grey; font-size:1.2em\">Principal Component Analysis, ou `PCA()`, est une m\u00e9thode de r\u00e9duction de la dimensionnalit\u00e9 qui est souvent utilis\u00e9e pour r\u00e9duire la dimensionnalit\u00e9 de grands ensembles de donn\u00e9es, en transformant un grand ensemble de variables en un plus petit qui contient toujours la plupart des informations dans le grand ensemble.<\/span>","34c55fdd":"\n<span style=\"color:grey; font-size:1.2em\"> L'ensemble de donn\u00e9es est \u00e0 10 min pendant environ 4,5 mois. Les conditions de temp\u00e9rature et d'humidit\u00e9 de la maison ont \u00e9t\u00e9 surveill\u00e9es avec un r\u00e9seau de capteurs sans fil ZigBee. Chaque n\u0153ud sans fil a transmis les conditions de temp\u00e9rature et d'humidit\u00e9 environ 3,3 min. Ensuite, les donn\u00e9es sans fil ont \u00e9t\u00e9 moyenn\u00e9es sur des p\u00e9riodes de 10 minutes. Les donn\u00e9es \u00e9nerg\u00e9tiques ont \u00e9t\u00e9 enregistr\u00e9es toutes les 10 minutes avec des compteurs d'\u00e9nergie m-bus. La m\u00e9t\u00e9o de la station m\u00e9t\u00e9orologique de l'a\u00e9roport la plus proche (a\u00e9roport de Chievres, Belgique) a \u00e9t\u00e9 t\u00e9l\u00e9charg\u00e9e \u00e0 partir d'un ensemble de donn\u00e9es publiques de Reliable Prognosis (rp5.ru) et fusionn\u00e9e avec les ensembles de donn\u00e9es exp\u00e9rimentaux \u00e0 l'aide de la colonne date et heure. Deux variables al\u00e9atoires ont \u00e9t\u00e9 incluses dans l'ensemble de donn\u00e9es pour tester les mod\u00e8les de r\u00e9gression et pour filtrer les attributs non pr\u00e9dictifs (param\u00e8tres).\n<\/span>","0bb9fafb":"<span style=\"color:grey; font-size:1.2em\">`StandardScaler` standardise une Feature en soustrayant la moyenne, puis en la mettant \u00e0 l'\u00e9chelle de la variance unitaire. La variance unitaire consiste \u00e0 diviser toutes les valeurs par l'\u00e9cart type.\n<\/span>","d9709f23":"<span style=\"color:grey; font-size:1.2em\">Important nos jeu de donn\u00e9es sous les fichier <b>energydata_complete.csv<\/b><\/span>","f2ca2075":"<span style=\"color:grey; font-size:1.2em\">Nous allons maintenant cr\u00e9er notre <b>r\u00e9seau RBF<\/b>.<br>\nEssayons d'abord d'en cr\u00e9er un '<b>from scratch<\/b>'\u00a0: pour ce faire, nous devons d'abord cr\u00e9er des <b>fonctions<\/b> qui nous aidera dans le processus.<\/span>","f1dc990e":"<span style=\"color:grey; font-size:1.2em\">Tra\u00e7ons l'historique de nos score de mod\u00e8le sur les donn\u00e9es d'entra\u00eenement et de validation pour chaque <b>Epoch<\/b>.<\/span>","77c11ef7":"<span style=\"color:grey; font-size:1.2em\">Et enfin, nous pouvons utiliser une fonction et une m\u00e9thode <b>pr\u00e9d\u00e9finies dans keras<\/b> pour rendre notre travail beaucoup plus <b>facile<\/b>.<span>","2e1ca191":"<span style=\"color:grey; font-size:1.2em\">Comme nous pouvons le voir, la <b>tol\u00e9rance<\/b> fournie par kernel_ridge donne de meilleurs r\u00e9sultats que SVM. Ceci est d\u00fb \u00e0 la diff\u00e9rence de <b>loss functions<\/b> (<b>ridge<\/b> vs <b>epsilon-insensitive loss<\/b>).<\/span>","e43bb943":"<div id=\"scale\">\n        <h3 style=\"color:#1a1a1a;\n                    font-size:2em\">\n         \u2b9e  \u00e9chelonner les donn\u00e9es\n        <\/h3>\n<\/div>","af68b3ee":"<h4 style=\"color:grey\"> Importation de biblioth\u00e8ques <\/h4>","d78b6d75":"<div style=\"font-size:1.5em\">\n    <p>\ud83d\udcdc Table des mati\u00e8res:<\/p>\n    <ul>\n       <li>\n          <a href=\"#Intro-section\">Introduction \ud83d\udcd6<\/a>\n          <ul>\n             <li><a href=\"#overview\">Aper\u00e7u<\/a><\/li>\n             <li><a href=\"#dataset\">Jeu de donn\u00e9es<\/a><\/li>\n          <\/ul>\n       <\/li>\n       <li>\n          <a href=\"#Analysis-section\">Analyses et Transformations \ud83d\udd0e<\/a>\n          <ul>\n             <li><a href=\"#eda\">Exploration de donn\u00e9es<\/a><\/li>\n             <li><a href=\"#trans\">Transformation de donn\u00e9es<\/a><\/li>\n          <\/ul>\n       <\/li>\n       <li>\n          <a href=\"#pre-processing\">Pr\u00e9-traitement \u2699\ufe0f<\/a>\n          <ul>\n             <li><a href=\"#split-data\">Diviser les donn\u00e9es<\/a><\/li>\n             <li><a href=\"#scale\">\u00e9chelonner les donn\u00e9es<\/a><\/li>\n          <\/ul>\n       <\/li>\n       <li>\n          <a href=\"#model-building\">Construction des mod\u00e8les \ud83d\udee0\ufe0f<\/a>\n          <ul>\n             <li><a href=\"#SVM\">Support vector machines<\/a><\/li>\n              <li><a href=\"#KR\">Kernel ridge<\/a><\/li>\n             <li><a href=\"#RBF\">R\u00e9seaux RBF<\/a><\/li>\n             <li><a href=\"#RBF-Keras\">R\u00e9seaux RBF avec Keras<\/a><\/li>\n          <\/ul>\n       <\/li>\n       <li><a href=\"#Conclusion\">Conclusion \ud83d\udccc<\/a><\/li>\n    <\/ul>\n<\/div>"}}