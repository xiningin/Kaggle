{"cell_type":{"03077609":"code","d222aa2c":"code","9c8f77ea":"code","c3f15530":"code","7086b8cf":"code","5af629b9":"code","6435e1bf":"code","6b3ce331":"code","487e1ddd":"code","39ed2558":"code","9936259e":"code","b98043b2":"code","08403710":"code","c68f9646":"code","8b2379d6":"code","f90003e9":"code","b65fcc2f":"code","1088878b":"code","d0d78876":"code","63517fec":"code","66bbe598":"code","850f9e77":"code","f1ed8cde":"code","ef9e7655":"code","8793ddab":"code","f61e81dd":"code","089f85e6":"markdown","a66573de":"markdown","7a182e2a":"markdown","10bc5e87":"markdown","55da5a33":"markdown","e764ab49":"markdown","415143d0":"markdown","6f8ed5be":"markdown","7ae4b3d6":"markdown","cb99db17":"markdown","261b1058":"markdown","854839ce":"markdown","c53db415":"markdown","c4c4709a":"markdown","f72e4b73":"markdown","7eea5395":"markdown","3b1294cf":"markdown","016f8b7a":"markdown","60806d85":"markdown","7ac69617":"markdown","99e8dc3d":"markdown","0f56efdd":"markdown","22c1ed88":"markdown","7a8965fa":"markdown"},"source":{"03077609":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d222aa2c":"plt.style.use(\"ggplot\")#plot style","9c8f77ea":"zoo = pd.read_csv(\"..\/input\/zoo.csv\")#read data\nzoo.head()","c3f15530":"zoo.info()","7086b8cf":"print(zoo.class_type.value_counts())\nplt.figure(figsize = (10,8))\nsns.countplot(zoo.class_type)\nplt.show()","5af629b9":"data = zoo.copy()\ndata.drop(\"animal_name\",axis = 1,inplace = True)","6435e1bf":"x = data.drop(\"class_type\",axis = 1)# input data\ny = data.class_type.values# target data","6b3ce331":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.4,random_state = 42)\nprint(\"x_train shape : \",x_train.shape)\nprint(\"x_test shape : \",x_test.shape)\nprint(\"y_train shape : \",y_train.shape)\nprint(\"y_test shape : \",y_test.shape)","487e1ddd":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42,kernel = \"linear\")\nsvm.fit(x_train,y_train)\ny_pred_svm = svm.predict(x_test)\nprint(\"Train Accurary : \",svm.score(x_train,y_train))\nprint(\"Test Accuray : \",svm.score(x_test,y_test))","39ed2558":"from sklearn.metrics import confusion_matrix,classification_report\ncm_svm = confusion_matrix(y_test,y_pred_svm)\ncr_svm = classification_report(y_test,y_pred_svm)\nprint(\"confusion matrix : \\n\",cm_svm)\nprint(\"classification report : \\n\",cr_svm)","9936259e":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_svm,annot = True,cmap = \"Blues\",xticklabels = np.arange(1,8),yticklabels = np.arange(1,8))\nplt.show()","b98043b2":"from sklearn.neighbors import KNeighborsClassifier\nscr_max = 0\nknn_test_score_list = []\nknn_train_score_list = []\n\nfor i in range(1,x_train.shape[0]+1):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(x_train,y_train)\n    knn_test_scr = knn.score(x_test,y_test)\n    knn_test_score_list.append(knn_test_scr)\n    knn_train_scr = knn.score(x_train,y_train)\n    knn_train_score_list.append(knn_train_scr)\n    if knn_test_scr >= scr_max:\n        scr_max = knn_test_scr\n        index = i\n\nprint(\"Best K value = \",index)\nprint(\"Best score = \",scr_max)\n\nplt.figure(figsize = (15,10))\nplt.plot(range(1,x_train.shape[0]+1),knn_test_score_list,label = \"test\")\nplt.plot(range(1,x_train.shape[0]+1),knn_train_score_list,label = \"train\")\nplt.legend()\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Scores\")\nplt.show()","08403710":"knn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\ncr_knn = classification_report(y_test,y_pred_knn)\ncm_knn = confusion_matrix(y_test,y_pred_knn)\nprint(\"confusion matrix : \\n\",cm_knn)\nprint(\"classification report : \\n\",cr_knn)","c68f9646":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_knn,annot = True,xticklabels = np.arange(1,8),yticklabels = np.arange(1,8))\nplt.show()","8b2379d6":"from sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier(random_state = 42)\ndec_tree.fit(x_train,y_train)\ny_pred_tree = dec_tree.predict(x_test)\nprint(\"Test Accurary : \",dec_tree.score(x_test,y_test))\nprint(\"Train Accurary : \",dec_tree.score(x_train,y_train))","f90003e9":"cm_tree = confusion_matrix(y_test,y_pred_tree)\ncr_tree = classification_report(y_test,y_pred_tree)\nprint(\"confusion matrix : \\n\",cm_tree)\nprint(\"classification report : \\n\",cr_tree)","b65fcc2f":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_tree,annot = True,xticklabels = np.arange(1,8),yticklabels = np.arange(1,8),cmap = \"Greens\")\nplt.show()","1088878b":"from sklearn.ensemble import RandomForestClassifier\ns_max = 0\nrf_train_score_list = []\nrf_test_score_list = []\n\nfor i in range(1,x_train.shape[0]+1):\n    rf = RandomForestClassifier(n_estimators = i,random_state = 42)\n    rf.fit(x_train,y_train)\n    test_score = rf.score(x_test,y_test)\n    rf_test_score_list.append(test_score)\n    train_score = rf.score(x_train,y_train)\n    rf_train_score_list.append(train_score)\n    if test_score >= s_max :\n        s_max = test_score\n        index = i\n\nprint(\"Best Score = \",s_max)\nprint(\"Best n_estimators = \",index)\n\nplt.figure(figsize = (10,8))\nplt.plot(range(1,x_train.shape[0]+1),rf_test_score_list,label = \"test\")\nplt.plot(range(1,x_train.shape[0]+1),rf_train_score_list,label = \"train\")\nplt.legend()\nplt.xlabel(\"n estimators\")\nplt.ylabel(\"Scores\")\nplt.show()","d0d78876":"rf = RandomForestClassifier(n_estimators = 60,random_state = 42)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\ncm_rf = confusion_matrix(y_test,y_pred_rf)\ncr_rf = classification_report(y_test,y_pred_rf)\nprint(\"confusion matrix : \\n\",cm_rf)\nprint(\"classification report : \\n\",cr_rf)","63517fec":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_rf,annot = True,xticklabels = np.arange(1,8),yticklabels = np.arange(1,8),cmap = \"Greens\")\nplt.show()","66bbe598":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)\ny_pred_lr = log_reg.predict(x_test)\nprint(\"Test Accurary : \",log_reg.score(x_test,y_test))\nprint(\"Train Accurary : \",log_reg.score(x_train,y_train))","850f9e77":"cm_lr = confusion_matrix(y_test,y_pred_lr)\ncr_lr = classification_report(y_test,y_pred_lr)\nprint(\"confusion matrix : \\n\",cm_lr)\nprint(\"classification report : \\n\",cr_lr)","f1ed8cde":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_lr,annot = True,xticklabels = np.arange(1,8),yticklabels = np.arange(1,8),cmap = \"Reds\")\nplt.show()","ef9e7655":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(\"Test Accurary : \",nb.score(x_test,y_test))\nprint(\"Train Accurary : \",nb.score(x_train,y_train))","8793ddab":"cm_nb = confusion_matrix(y_test,y_pred_nb)\ncr_nb = classification_report(y_test,y_pred_nb)\nprint(\"confusion matrix : \\n\",cm_nb)\nprint(\"classification report : \\n\",cr_nb)","f61e81dd":"plt.figure(figsize = (10,8))\nsns.heatmap(cm_nb,annot = True,xticklabels = np.arange(1,8),yticklabels = np.arange(1,8),cmap = \"Reds\")\nplt.show()","089f85e6":"# Classification Methods\n* Generally,methods in sklearn use same function:\n* fit : fittin data ,actually this all you need to do after that data is ready to predict\n* classification_method.fit(x,y)\n* predict : predict targets\n* classification_method.predict(x)\n* score : this is achivment rate our model.This is closes to 1,the better\n* classification_method.score(x,y)","a66573de":"Aim : Find best model by use supervised ML","7a182e2a":"* RFC take n estimators like KNN\n* So we can do same thing we do in KNN","10bc5e87":"# SVM Classification","55da5a33":"# Logistic Regression","e764ab49":"# Naive Bayes Classification","415143d0":"# KNN Classification","6f8ed5be":"* KNN classification take k value.\n* We can find best k value by use for loop.","7ae4b3d6":"* target 3 and 7 have false prediction\n* total false prediction is 3\n* Logistic Regression is not good according to others","cb99db17":"* target 3 and 7 have false prediction\n* Naive Bayes accurary score is same other classification\n* But again,it has different prediction","261b1058":"![image.png](attachment:image.png)","854839ce":"# Conclusion\n* All classification methods usually have false prediction to target 3\n* I think because of this target 3 number of sample is little according to others\n* We see that Naive Bayes,KNN,Random Forest and Decision Tree give similar and best results\n* I think we can choose one of them.","c53db415":"We have 7 target :\n1. Mammal\n1. Bird\n1. Reptile\n1. Fish\n1. Amphibian\n1. Bug\n1. Invertebrate","c4c4709a":"# Random Forest Classification","f72e4b73":"# Confusion Matrix\n* it will help to understand our model how good\n* it show us true and false predicts\n* it is easy with sklearn\n* confusion_matrix(y_true,y_predict)","7eea5395":"Let's see how many of each target in our data.","3b1294cf":"ML algorithms use math so we do not need string variables(animal names)","016f8b7a":"* target 3 and 7 have false prediction\n* Altough accurary score is 0.93,target 3 has no correct prediction and target 7 has one false prediction","60806d85":"# Decision Tree Classification","7ac69617":"# Classification Report\n* precision : What proportion of positive identifications was actually correct?\n* precision = TP\/(TP+FP)\n* recall : What proportion of actual positives was identified correctly?\n* recall = TP\/(TP+FN)\n* f1_score = 2*(precision*recall)\/(precision+recall)\n* support : number of acctual target","99e8dc3d":"* target 3 and 6 have false prediction\n* Decison Tree and KNN have same accurary score\n* but their prediction is not same","0f56efdd":"* target 3 and 7 have false prediction\n* RF ,KNN and Desicion Tree have same accurary score\n* but their prediction is different","22c1ed88":"* target 3 and 7 have false prediction\n* We can see KNN is better than SVM","7a8965fa":"# Introduction"}}