{"cell_type":{"afd04395":"code","40ea6702":"code","2e6dc0cf":"code","56dfe519":"code","505eba2d":"code","cb8a4451":"code","bc0b8fe7":"code","a160beed":"code","454cab9c":"code","8410394b":"code","31014d06":"code","82851252":"code","ef9eab13":"code","d844b122":"code","03c3b8d6":"code","47da08f8":"code","a160dfc3":"markdown","16f0d8b9":"markdown","0238bddc":"markdown","50dddc62":"markdown","abaa7bcd":"markdown","23c9518d":"markdown","323bdcbe":"markdown","8acf74c5":"markdown","f3f1a56f":"markdown","a2ce2ab4":"markdown","39e03a29":"markdown","47bd43a2":"markdown","e3775c19":"markdown","236783db":"markdown","0632e217":"markdown","abdcef21":"markdown","51f39a6d":"markdown","b6d326b8":"markdown"},"source":{"afd04395":"from IPython.display import YouTubeVideo\nYouTubeVideo('6vYJyOGKCHE', width=800, height=450)","40ea6702":"!pip install kaggle-environments --upgrade --quiet","2e6dc0cf":"from kaggle_environments import make, evaluate\nenv = make(\"halite\", debug=True)\nenv.run([\"random\", \"random\", \"random\", \"random\"])\nenv.render(mode=\"ipython\", width=800, height=600)","56dfe519":"from kaggle_environments import make\nfrom kaggle_environments.envs.halite.helpers import *\n\nboard_size = 5\nenvironment = make(\"halite\", \n                   configuration={\n                       \"size\": board_size, \n                       \"startingHalite\": 1000})\nagent_count = 2\nenvironment.reset(agent_count)\nstate = environment.state[0]\n\nboard = Board(state.observation, \n              environment.configuration)\n\ndef move_ships_north_agent(\n    observation, configuration):\n    board = Board(\n        observation, configuration)\n    current_player = board.current_player\n    for ship in current_player.ships:\n        ship.next_action = ShipAction.NORTH\n    return current_player.next_actions\n\nenvironment.reset(agent_count)\nenvironment.run([move_ships_north_agent, \"random\"])\nenvironment.render(mode=\"ipython\", width=500, height=450)","505eba2d":"%%writefile submission.py\nimport time\nimport copy\nimport sys\nimport math\nimport collections\nimport pprint\nimport numpy as np\nimport scipy.optimize\nimport scipy.ndimage\nfrom kaggle_environments.envs.halite.helpers import *\nimport kaggle_environments\nimport random\n\nCONFIG_MAX_SHIPS=20\nall_actions=[ShipAction.NORTH, ShipAction.EAST,ShipAction.SOUTH,ShipAction.WEST]\nall_dirs=[Point(0,1), Point(1,0), Point(0,-1), Point(-1,0)]\nstart=None\nnum_shipyard_targets=4\nsize=None\nship_target={}\nme=None\ndid_init=False\nquiet=False\nC=None\nclass Obj:\n  pass\nturn=Obj()\nturns_optimal=np.array(\n  [[0, 2, 3, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8],\n   [0, 1, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7],\n   [0, 0, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7],\n   [0, 0, 1, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6],\n   [0, 0, 0, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6],\n   [0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5],\n   [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n#### Functions\ndef print_enemy_ships(board):\n  print('\\nEnemy Ships')\n  for ship in board.ships.values():\n    if ship.player_id != me.id:\n      print('{:6}  {} halite {}'.format(ship.id,ship.position,ship.halite))\n      \ndef print_actions(board):\n  print('\\nShip Actions')\n  for ship in me.ships:\n    print('{:6}  {}  {} halite {}'.format(ship.id,ship.position,ship.next_action,ship.halite))\n  print('Shipyard Actions')\n  for sy in me.shipyards:\n    print('{:6}  {}  {}'.format(sy.id,sy.position,sy.next_action))\n\ndef print_none(*args):\n  pass\n\ndef compute_max_ships(step):\n  if step < 200:\n    return CONFIG_MAX_SHIPS\n  elif step < 300:\n    return CONFIG_MAX_SHIPS-2\n  elif step < 350:\n    return CONFIG_MAX_SHIPS-4\n  else:\n    return CONFIG_MAX_SHIPS-5\n\ndef set_turn_data(board):\n  turn.num_ships=len(me.ships)\n  turn.max_ships=compute_max_ships(board.step)\n  turn.total_halite=me.halite\n  turn.halite_matrix=np.reshape(board.observation['halite'], (board.configuration.size,board.configuration.size))\n  turn.num_shipyards=len(me.shipyards)\n  turn.EP,turn.EH,turn.ES=gen_enemy_halite_matrix(board)\n  turn.taken={}\n  turn.last_episode = (board.step == (board.configuration.episode_steps-2))\n  \ndef init(obs,config):\n  global size\n  global print\n  if hasattr(config,'myval') and config.myval==9 and not quiet:\n    pass\n  else:\n    print=print_none\n    pprint.pprint=print_none\n  size = config.size\n\ndef limit(x,a,b):\n  if x<a:\n    return a\n  if x>b:\n    return b\n  return x\n  \ndef num_turns_to_mine(C,H,rt_travel):\n  if C==0:\n    ch=0\n  elif H==0:\n    ch=turns_optimal.shape[0]\n  else:\n    ch=int(math.log(C\/H)*2.5+5.5)\n    ch=limit(ch,0,turns_optimal.shape[0]-1)\n  rt_travel=int(limit(rt_travel,0,turns_optimal.shape[1]-1))\n  return turns_optimal[ch,rt_travel]\n\ndef halite_per_turn(carrying, halite,travel,min_mine=1):\n  turns=num_turns_to_mine(carrying,halite,travel)\n  if turns<min_mine:\n    turns=min_mine\n  mined=carrying+(1-.75**turns)*halite\n  return mined\/(travel+turns), turns\n  \ndef move(pos, action):\n  ret=None\n  if action==ShipAction.NORTH:\n    ret=pos+Point(0,1)\n  if action==ShipAction.SOUTH:\n    ret=pos+Point(0,-1)\n  if action==ShipAction.EAST:\n    ret=pos+Point(1,0)\n  if action==ShipAction.WEST:\n    ret=pos+Point(-1,0)\n  if ret is None:\n    ret=pos\n  return ret % size\n\ndef dirs_to(p1, p2, size=21):\n  deltaX, deltaY=p2 - p1\n  if abs(deltaX)>size\/2:\n    #we wrap around\n    if deltaX<0:\n      deltaX+=size\n    elif deltaX>0:\n      deltaX-=size\n  if abs(deltaY)>size\/2:\n    #we wrap around\n    if deltaY<0:\n      deltaY+=size\n    elif deltaY>0:\n      deltaY-=size\n  ret=[]\n  if deltaX>0:\n    ret.append(ShipAction.EAST)\n  if deltaX<0:\n    ret.append(ShipAction.WEST)\n  if deltaY>0:\n    ret.append(ShipAction.NORTH)\n  if deltaY<0:\n    ret.append(ShipAction.SOUTH)\n  if len(ret)==0:\n    ret=[None]\n  return ret, (deltaX,deltaY)\n\ndef shipyard_actions():\n  for sy in me.shipyards:\n    if turn.num_ships < turn.max_ships:\n      if turn.total_halite >= 500 and sy.position not in turn.taken:\n        sy.next_action = ShipyardAction.SPAWN\n        turn.taken[sy.position]=1\n        turn.num_ships+=1\n        turn.total_halite-=500\n\ndef gen_enemy_halite_matrix(board):\n  EP=np.zeros((size,size))\n  EH=np.zeros((size,size))\n  ES=np.zeros((size,size))\n  for id,ship in board.ships.items():\n    if ship.player_id != me.id:\n      EH[ship.position.y,ship.position.x]=ship.halite\n      EP[ship.position.y,ship.position.x]=1\n  for id, sy in board.shipyards.items():\n    if sy.player_id != me.id:\n      ES[sy.position.y,sy.position.x]=1\n  return EP,EH,ES\n\ndef dist(a,b):\n  action,step=dirs_to(a, b, size=21) \n  return abs(step[0]) + abs(step[1])\n\ndef nearest_shipyard(pos):\n  mn=100\n  best_pos=None\n  for sy in me.shipyards:\n    d=dist(pos, sy.position)\n    if d<mn:\n      mn=d\n      best_pos=sy.position\n  return mn,best_pos\n  \ndef assign_targets(board,ships):\n  old_target=copy.copy(ship_target)\n  ship_target.clear()\n  if len(ships)==0:\n    return\n  halite_min=50\n  pts1=[]\n  pts2=[]\n  for pt,c in board.cells.items():\n    assert isinstance(pt,Point)\n    if c.halite > halite_min:\n      pts1.append(pt)\n  for sy in me.shipyards:\n    for i in range(num_shipyard_targets):\n      pts2.append(sy.position)\n  C=np.zeros((len(ships),len(pts1)+len(pts2)))\n  for i,ship in enumerate(ships):\n    for j,pt in enumerate(pts1+pts2):\n      d1=dist(ship.position,pt)\n      d2,shipyard_position=nearest_shipyard(pt)\n      if shipyard_position is None:\n        d2=1\n      my_halite=ship.halite\n      if j < len(pts1):\n        v, mining=halite_per_turn(my_halite,board.cells[pt].halite, d1+d2)\n      else:\n        if d1>0:\n          v=my_halite\/d1\n        else:\n          v=0\n      if board.cells[pt].ship and board.cells[pt].ship.player_id != me.id:\n        enemy_halite=board.cells[pt].ship.halite\n        if enemy_halite <= my_halite:\n          v = -1000\n        else:\n          if d1<5:\n            v+= enemy_halite\/(d1+1)\n      C[i,j]=v\n  print('C is {}'.format(C.shape))\n  row,col=scipy.optimize.linear_sum_assignment(C, maximize=True)\n  pts=pts1+pts2\n  for r,c in zip(row,col):\n    ship_target[ships[r].id]=pts[c]\n  print('\\nShip Targets')\n  print('Ship      position          target')\n  for id,t in ship_target.items():\n    st=''\n    ta=''\n    if board.ships[id].position==t:\n      st='MINE'\n    elif len(me.shipyards)>0 and t==me.shipyards[0].position:\n      st='SHIPYARD'\n    if id not in old_target or old_target[id] != ship_target[id]:\n      ta=' NEWTARGET'\n    print('{0:6}  at ({1[0]:2},{1[1]:2})  assigned ({2[0]:2},{2[1]:2}) h {3:3} {4:10} {5:10}'.format(\n      id, board.ships[id].position, t, board.cells[t].halite,st, ta))\n\n  return\n\ndef make_avoidance_matrix(myship_halite):\n  filter=np.array([[0,1,0],[1,1,1],[0,1,0]])\n  bad_ship=np.logical_and(turn.EH <= myship_halite,turn.EP)\n  avoid=scipy.ndimage.convolve(bad_ship, filter, mode='wrap',cval=0.0)\n  avoid=np.logical_or(avoid,turn.ES)\n  return avoid\n\ndef make_attack_matrix(myship_halite):\n  attack=np.logical_and(turn.EH > myship_halite,turn.EP)\n  return attack\n\ndef get_max_halite_ship(board, avoid_danger=True):\n  mx=-1\n  the_ship=None\n  for ship in me.ships:\n    x=ship.position.x\n    y=ship.position.y\n    avoid=make_avoidance_matrix(ship.halite)\n    if ship.halite>mx and (not avoid_danger or not avoid[y,x]):\n      mx=ship.halite\n      the_ship=ship\n  return the_ship\n\ndef remove_dups(p):\n  ret=[]\n  for x in p:\n    if x not in ret:\n      ret.append(x)\n  return ret\n\ndef matrix_lookup(matrix,pos):\n  return matrix[pos.y,pos.x]\n\ndef ship_converts(board):\n  if turn.num_shipyards==0 and not turn.last_episode:\n    mx=get_max_halite_ship(board)\n    if mx is not None:\n      if mx.halite + turn.total_halite > 500:\n        mx.next_action=ShipAction.CONVERT\n        turn.taken[mx.position]=1\n        turn.num_shipyards+=1\n        turn.total_halite-=500\n  for ship in me.ships:\n    if ship.next_action:\n      continue\n    avoid=make_avoidance_matrix(ship.halite)\n    z=[matrix_lookup(avoid,move(ship.position,a)) for a in all_actions]\n    if np.all(z) and ship.halite > 500:\n      ship.next_action=ShipAction.CONVERT\n      turn.taken[ship.position]=1\n      turn.num_shipyards+=1\n      turn.total_halite-=500\n      print('ship id {} no escape converting'.format(ship.id))\n    if turn.last_episode and ship.halite > 500:\n      ship.next_action=ShipAction.CONVERT\n      turn.taken[ship.position]=1\n      turn.num_shipyards+=1\n      turn.total_halite-=500\n      \ndef ship_moves(board):\n  ships=[ship for ship in me.ships if ship.next_action is None]\n  assign_targets(board,ships)\n  actions={}\n  for ship in ships:\n    if ship.id in ship_target:\n      a,delta = dirs_to(ship.position, ship_target[ship.id],size=size)\n      actions[ship.id]=a\n    else:\n      actions[ship.id]=[random.choice(all_actions)]\n      \n  for ship in ships:\n    action=None\n    x=ship.position\n    avoid=make_avoidance_matrix(ship.halite)\n    attack=make_attack_matrix(ship.halite)\n    action_list=actions[ship.id]+[None]+all_actions\n    for a in all_actions:\n      m=move(x,a)\n      if attack[m.y,m.x]:\n        print('ship id {} attacking {}'.format(ship.id,a))\n        action_list.insert(0,a)\n        break\n    action_list=remove_dups(action_list)\n    for a in action_list:\n      m=move(x,a)\n      if avoid[m.y,m.x]:\n        print('ship id {} avoiding {}'.format(ship.id,a))\n      if m not in turn.taken and not avoid[m.y,m.x]:\n        action=a\n        break\n    ship.next_action=action\n    turn.taken[m]=1\n    \ndef agent(obs, config):\n  global size\n  global start\n  global prev_board\n  global me\n  global did_init\n  #Do initialization 1 time\n  start_step=time.time()\n  if start is None:\n    start=time.time()\n  if not did_init:\n    init(obs,config)\n    did_init=True\n  board = Board(obs, config)\n  me=board.current_player\n  set_turn_data(board)\n  print('==== step {} sim {}'.format(board.step,board.step+1))\n  print('ships {} shipyards {}'.format(turn.num_ships,turn.num_shipyards))\n  print_enemy_ships(board)\n  ship_converts(board)\n  ship_moves(board)\n  shipyard_actions()\n  print_actions(board)\n  print('time this turn: {:8.3f} total elapsed {:8.3f}'.format(time.time()-start_step,time.time()-start))\n  return me.next_actions\n","cb8a4451":"env.run([\"\/kaggle\/working\/submission.py\", \"\/kaggle\/working\/submission.py\"])\n\nenv.run([\"\/kaggle\/working\/submission.py\", \"random\"])\nenv.render(mode=\"ipython\", width=800, height=600)","bc0b8fe7":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_curve\nimport matplotlib.pyplot as plt","a160beed":"# assuming we have times damaged, time alive and speed of clicking as Nx3 matrix\n# and we are given some simple feedback 0 to degrade difficulty, 1 to upgrade\n# we will skip scaling step for now\nseed = 13\nN = 1000\ngame_stats = np.random.randint(0, 10, N*3).reshape(N, 3)\ntarget = np.random.choice([0, 1], size=N).reshape(N, 1)\nplt.hist(target);","454cab9c":"X_train, X_test, y_train, y_test = train_test_split(\n    game_stats, target, test_size=0.25, random_state=seed)\n\nmodel = LogisticRegression()\nmodel = model.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy_score(y_test, preds)","8410394b":"plt.plot(roc_curve(y_test, preds)[0], label='false positive',);\nplt.plot(roc_curve(y_test, preds)[1], label='false negative');\nplt.legend();","31014d06":"import tensorflow as tf\nimport tensorflow.keras as K","82851252":"inputs = K.Input(shape=(3,))\nx = K.layers.Dense(32, activation=tf.nn.relu)(inputs)\nx = K.layers.Dense(64, activation=tf.nn.relu)(inputs)\noutputs = K.layers.Dense(1, activation=tf.nn.softmax)(x)\nmodel = K.Model(inputs=inputs, outputs=outputs)","ef9eab13":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nepochs = 5\nhistory = model.fit(X_train, y_train, epochs=epochs)","d844b122":"preds = model.predict(X_test)\naccuracy_score(y_test, preds)","03c3b8d6":"import torch\nimport torch.nn as nn\nimport torch.optim as optim","47da08f8":"class DQN(nn.Module):\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.bn3 = nn.BatchNorm2d(32)\n        self.head = nn.Linear(448, 2)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))","a160dfc3":"### Brutal reality: Developers from Mars, Data Scientists from Venera\n\nDifficulties arise when we go with the most convinient tools we know and love: data scientists largely prefer Python, R or Scala. Mobile developers primerely look towards Unity. Intergation between these tools is far from the smooth one.\n\nAlongside pitfalls we will build a few simple models but bare in mind: <br>1) we are bounded with implementation of those. <br>2) data is randomly generated and might have no patterns at all.","16f0d8b9":"<a href=\"https:\/\/ibb.co\/Xp7gxy0\"><img src=\"https:\/\/i.ibb.co\/J2v4jx6\/Screenshot-from-2020-08-08-23-14-21.png\" alt=\"Screenshot-from-2020-08-08-23-14-21\" border=\"0\"><\/a>","0238bddc":"That is how we would collect resources... But do we really want to type so much code? What about large scale games?\n\nFor our project we want to create pleasant experience for a user. The basic idea (at least for starters) would be to create responsive agents which will provide challanging enough game meanwhile not scaring the user off with too difficult enemies.\n\nFor beginning we can write some basic rules, say, if user recieves damage too often we can make agents weaker. But as our game grows we need to look for how responsive is the device, how often user pauses game, track his progress. That is too much to handle and our agents will grow accordingly.","50dddc62":"Machine Learning and Artificial Intelegence is integrating in our life. Ads on Google? Amazon or Netflix suggestions? Finally, better website and games user experience? Well, you know the answer. And we have already seen how AI beat the chess grandmaster Garry Kasparov. Following video gets better idea of this event.","abaa7bcd":"### Scenario 3. Diging deeper with pytorch and reinforcment learning","23c9518d":"### AI is not perfect ... but it learns\n\nThanks to kaggle we have easy way experiment with a few ideas around agent training. I will not bore you for too long but a few words about Halite game:\n> Halite by Two Sigma (\"Halite\") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.","323bdcbe":"**Pros:** Easy API, usually Deep Learning models can catch non-linear highly complex patterns in data<br>\n**Cons:** Supervised learning algorithm(needs labeled data), needs design, from DL perspective provides limited tools for customization (or complicates those)","8acf74c5":"To be continued","f3f1a56f":"### Our case study, beloved project: TimeHack","a2ce2ab4":"**Pros:** Fast algorithms, does not rely on GPU<br>\n**Cons:** Supervised learning algorithm(needs labeled data), unable to catch complex relationship in the data.","39e03a29":"A bit explanation is needed here but I will try to be consize. In this set up we will not need labeled data and instead we may use images directly from the game screen and teach network to strive towards desired state that we predefine. I will not bore with another details such as policy, states, but reward function is what is crutial here and that is what our network will be trying to obtain.","47bd43a2":"Basic set up of our game boils down to fighting computer agents with a few twists: the player has a chance to resurrect right before 5 seconds his death and change things for better. Here is another overhead to track.","e3775c19":"First let's take a look at the randomly playing game to get a better idea.","236783db":"Now we know how to cotrol agent. Feel free to skip all this code.","0632e217":"### Scenario 1. Use anaconda and classic machine learning","abdcef21":"### Scenario 2. Use tensorflow multilayer deep learning model and (maybe) mlagents will aid developers","51f39a6d":"## AI in Game Industry: what is the future... And why it is harder then it seems to deploy","b6d326b8":"#### Depending on the approach (classic or deep learning) the CUDA support is required for Tensorflow <a href='https:\/\/developer.nvidia.com\/cuda-toolkit-archive'> here is CUDA toolkit<\/a>"}}