{"cell_type":{"92f8d09b":"code","ed264f12":"code","c4d7d062":"code","43038e83":"code","c466c660":"code","43d90a72":"code","ff54182f":"code","6583a9f8":"code","c3a0e782":"code","bea70cf2":"code","1ac129da":"code","c63cc313":"code","d1a8c469":"code","db2f2cb3":"code","44179054":"code","481d5961":"code","cc4bd0b7":"code","beb04791":"code","3e50732b":"code","7d8b5554":"code","0bb5875e":"code","90c41d31":"code","0597be43":"code","679e9539":"code","916838de":"code","f2a9f379":"code","1b070561":"code","25a12082":"code","91b1b768":"code","1af7b179":"code","bd871c4e":"code","d276bc7c":"code","ab14e4d5":"code","07151449":"code","8f65d337":"code","f4f1d3f0":"code","d9885ebc":"code","3447e2d5":"code","31681f44":"code","65f1d00a":"code","074689e2":"code","07e2c04c":"code","8adbf4cd":"code","a00f80ca":"code","cea514d0":"code","30bf72a7":"code","cdda77f1":"code","ed96fa26":"code","8c960192":"code","f501e6ab":"code","17a06937":"code","6ee426b7":"code","74812f1d":"code","966623a1":"code","9f073c8f":"code","f2516892":"code","a1fe759a":"code","ca544588":"code","f9255e09":"code","43bf2579":"code","c3826f1c":"code","98dde5cc":"code","a64c5810":"code","c2c731cd":"code","ec2bde7a":"code","6daf7475":"code","74f7da10":"markdown","51456a2b":"markdown","ef09bff5":"markdown","95d14b3c":"markdown","8d3deb96":"markdown","afa816d2":"markdown","95306f34":"markdown","8d507045":"markdown","c6781923":"markdown","febeb0eb":"markdown","66b95bfa":"markdown","0492beba":"markdown","394aa72e":"markdown","7611bdeb":"markdown","38301396":"markdown","3da7cc2d":"markdown","496eba97":"markdown","35acf2dd":"markdown","87ca91e1":"markdown","e5a06e41":"markdown","48ff1a10":"markdown","095ac1ca":"markdown","8844a442":"markdown","8e0bcb4e":"markdown","107b22a5":"markdown","5501e1a1":"markdown","74230b79":"markdown","23bd3065":"markdown","6aa86a54":"markdown","3bd35651":"markdown","1c15bc96":"markdown","a6440d7f":"markdown","61af7db4":"markdown","c07c9a14":"markdown","b4b43e2a":"markdown","3e5c0675":"markdown","732c2f7f":"markdown","dc2fc0d6":"markdown","cab36cc4":"markdown","47d9cca5":"markdown","a220285d":"markdown","906b59b3":"markdown","4c1f5619":"markdown","1f56af24":"markdown","a6ead47a":"markdown","67e549fa":"markdown","0f699ac0":"markdown","998c9449":"markdown"},"source":{"92f8d09b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed264f12":"def survival_rate(groups):\n    '''\n    calculate a survival rate by the groups specified\n    groups: a list or string, columns to group by\n    '''\n    \n    output = data.groupby(groups).agg({'Survived':['sum','count']})\n    output['survival_rate'] = output.iloc[:,0]\/output.iloc[:,1]\n    return output","c4d7d062":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","43038e83":"data.info()","c466c660":"data.describe()","43d90a72":"fig, ax =plt.subplots(2,3,figsize=(15,10))\nsns.countplot(data['Survived'], ax=ax[0,0])\nsns.countplot(data['Sex'], ax=ax[1,0])\nsns.countplot(data['Pclass'], ax=ax[0,1])\nsns.countplot(data['SibSp'], ax=ax[1,1])\nsns.countplot(data['Parch'], ax=ax[0,2])\nsns.countplot(data['Embarked'], ax=ax[1,2])\n\nfig.show()","ff54182f":"sns.catplot(x='Survived',kind='count',data=data)","6583a9f8":"sns.catplot(x='Survived',kind='count',data=data,col='Sex')","c3a0e782":"sns.catplot(x='Survived',kind='count',data=data,col='Sex',row='Pclass')","bea70cf2":"# Look at the survival rate by sex & class\nsurvival_rate(['Sex','Pclass'])","1ac129da":"g = sns.catplot(x='Age',y='Survived',data=data,kind='violin',orient='h')\ng.set(xticks = np.arange(-10,100,10))","c63cc313":"g = sns.catplot(x='Age',y='Pclass',data=data,kind='boxen',orient='h', hue='Survived')\ng.set(xticks = np.arange(0,100,5))","d1a8c469":"# those under 15 - There are only 14 rows and all are 1 or under, however, most have survived\ndata[data['Age'] <= 15 & data['Pclass'].isin([1,2,3])]","db2f2cb3":"data['Child'] = (data['Age'] <= 15)*1","44179054":"g = sns.catplot(y='Fare',x='Survived',data=data,kind='boxen')\naxes = g.axes\n# axes[0,0].set_ylim(0,300)","481d5961":"g = sns.catplot(y='Fare',x='Pclass',data=data,kind='boxen',hue='Survived')\naxes = g.axes\naxes[0,0].set_ylim(0,300)","cc4bd0b7":"sns.catplot(x='Parch',data=data,kind='count',col='Survived',row='Sex')","beb04791":"data['Parch_0'] = (data['Parch'] == 0)*1","3e50732b":"survival_rate('Parch_0')","7d8b5554":"sns.catplot(x='SibSp',data=data,kind='count',col='Survived')","0bb5875e":"survival_rate('SibSp')","90c41d31":"sns.catplot(x='SibSp',data=data,kind='count',col='Pclass')","0597be43":"data['SibSp_0'] = (data['SibSp'] == 0)*1","679e9539":"survival_rate('SibSp_0')","916838de":"sum(data['Cabin'].isnull())\/data.shape[0]","f2a9f379":"data[~data['Cabin'].isnull()][['Name','Cabin']].tail(10)","1b070561":"# generate a cabin deck column\ndata['CabinDeck'] = data['Cabin'].str.extract('([ABCDEFG])[0-9]{1,3}')","25a12082":"data[['Name','Cabin','CabinDeck']][~data['Cabin'].isnull() & data['CabinDeck'].isnull()]","91b1b768":"sns.countplot(x='CabinDeck',data=data,order=['A','B','C','D','E','F','G'],hue='Survived')","1af7b179":"survival_rate('CabinDeck')","bd871c4e":"data['Name']","d276bc7c":"data['Title'] = data['Name'].str.extract('([A-Za-z]*)\\.')","ab14e4d5":"sns.catplot(x='Title',data=data,kind='count',hue='Survived')\nplt.xticks(rotation=90)","07151449":"survival_rate('Title').sort_values(by=('Survived','count'),ascending=False)","8f65d337":"sns.catplot(y='Age',x='Title',data=data,kind='boxen')\nplt.xticks(rotation=90)","f4f1d3f0":"sns.catplot(x='Title',data=data,kind='count',hue='Pclass')\nplt.xticks(rotation=90)","d9885ebc":"sns.catplot(x='Embarked',data=data,kind='count',hue='Survived')","3447e2d5":"survival_rate('Embarked')","31681f44":"sns.catplot(x='Embarked',data=data,hue='Pclass',kind='count')","65f1d00a":"data.describe()","074689e2":"from sklearn.preprocessing import StandardScaler\n\nstandard = StandardScaler()\npd.DataFrame(standard.fit_transform(data[['Age','Fare']]),columns = ['Age','Fare']).describe()\n# data.dtypes.values != object]].drop('PassengerId',axis=1","07e2c04c":"sns.heatmap(data.corr())","8adbf4cd":"X = data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Child','Parch_0','SibSp_0','CabinDeck','Title']]\n\ny = data['Survived']","a00f80ca":"# convert Pclass to an integer\nX['Pclass'] = X['Pclass'].astype(str)\n\n# # assign categorical features - objects or Pclass\n# X = pd.get_dummies(X,drop_first=True)","cea514d0":"sns.heatmap(X.corr())","30bf72a7":"# # Creating a pipeline\n# # We can add a function to the pipeline by using FeatureTransformer. It was inconvenient as I was adding columns on the numpy array, which doesn't have column names, hence I could risk adding feature off of the wrong column\n# def new_features(Y):\n#     Z = Y.copy()\n#     A = np.zeros((X.shape[0],Z.shape[1]+2))\n#     A[:1] = (Z[:,X.columns.get_loc(\"Parch\")] == 1)*1\n#     return A\n\n# Pipeline('ft',FunctionTransformer(new_features))\n    ","cdda77f1":"from sklearn.metrics import accuracy_score\naccuracy_score(y,(X.Sex == 'female')*1)","ed96fa26":"def model_udf(estimator_name,estimator,parameters,X,y):\n    '''\n    Return a fitted model on the X and y\n    estimator name : string for the name of the estimator step in the pipeline\n    estimator : sklearn estimator e.g. KNeighborsClassifier\n    parameters : dictionary, parameter grid for Grid Search\n    '''\n#     steps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='median')),\n#              ('scaler',StandardScaler()),\n#              ('onehot', OneHotEncoder(handle_unknown='ignore')),\n#             (estimator_name, estimator)]\n\n#     pipe = Pipeline(steps)\n\n    numeric_features = list(X._get_numeric_data().columns)\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())])\n\n    categorical_features = list(set(X.columns) - set(X._get_numeric_data().columns))\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)])\n\n    # Append classifier to preprocessing pipeline.\n    # Now we have a full prediction pipeline.\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          (estimator_name, estimator)])\n\n\n    parameters = parameters\n\n    clf = GridSearchCV(pipe, param_grid=parameters,return_train_score=True)\n\n    clf.fit(X,y)\n    \n    return clf\n","8c960192":"numeric_features = list(X._get_numeric_data().columns)\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = list(set(X.columns) - set(X._get_numeric_data().columns))\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', KNeighborsClassifier())])\n","f501e6ab":"clf = model_udf(\n    'knn'\n    ,KNeighborsClassifier()\n    ,parameters = {'knn__n_neighbors':np.arange(1,31)},X=X\n    ,y=y\n)","17a06937":"f'The Grid Search best score is {clf.best_score_:0.1%} and the best parameters are {clf.best_params_}'","6ee426b7":"cv_results = pd.DataFrame(clf.cv_results_)[['param_knn__n_neighbors','params','mean_score_time','mean_train_score','mean_test_score']]","74812f1d":"cv_results.head()","966623a1":"fig, ax = plt.subplots()\nax.plot(cv_results['param_knn__n_neighbors'],cv_results['mean_test_score'],label='mean_test_score')\nax.plot(cv_results['param_knn__n_neighbors'],cv_results['mean_train_score'],label='mean_train_score')\nax.legend()","9f073c8f":"from sklearn.linear_model import SGDClassifier\nclf = model_udf(\n    'sgd'\n    , SGDClassifier()\n    ,parameters = {'sgd__alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'sgd__loss':['hinge','log'], 'sgd__penalty':['l1','l2']},X=X\n    ,y=y\n)","f2516892":"clf.score(X,y)","a1fe759a":"f'The Grid Search best score is {clf.best_score_:0.1%} and the best parameters are {clf.best_params_}'","ca544588":"# # Needs review since using onehotencoder https:\/\/stackoverflow.com\/questions\/39043326\/computing-feature-importance-with-onehotencoded-features\n# def feature_importance(gridsearch_object,estimator_step):\n#     plt.figure(figsize=(10,10))\n#     sns.barplot(x='b',y='a',data=pd.DataFrame({'a':X.columns,'b':clf.best_estimator_.named_steps['sgd'].coef_[0]}).sort_values('b'))\n\n# feature_importance(clf,'sgd')\n\n# clf.best_estimator_.named_steps['preprocessor'].transformers[1][1].named_steps['onehot'].get_feature_names(categorical_features)","f9255e09":"from sklearn.ensemble import RandomForestClassifier\n\n\nclf = model_udf(\n    'rfc'\n    , RandomForestClassifier()\n    ,parameters = {'rfc__n_estimators':[100,350,500],'rfc__max_features':['log2','auto','sqrt'],'rfc__min_samples_leaf':[2,10,30]}\n    ,X=X\n    ,y=y\n)","43bf2579":"f'The Grid Search best score is {clf.best_score_:0.1%} and the best parameters are {clf.best_params_}'","c3826f1c":"clf.score(X,y)","98dde5cc":"def data_transform(data):\n    data['Title'] = data['Name'].str.extract('([A-Za-z]*)\\.')\n    data['CabinDeck'] = data['Cabin'].str.extract('([ABCDEFG])[0-9]{1,3}')\n    data['SibSp_0'] = (data['SibSp'] == 0)*1\n    data['Child'] = (data['Age'] <= 15)*1\n    data['Parch_0'] = (data['Parch'] == 0)*1\n    data = data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Child','Parch_0','SibSp_0','CabinDeck','Title']]\n    # convert Pclass to an integer\n    data['Pclass'] = data['Pclass'].astype(str)\n    return data","a64c5810":"X_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX_test = data_transform(X_test)","c2c731cd":"X_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata_transform(X_test)","ec2bde7a":"predictions = clf.predict(X_test)","6daf7475":"output = pd.DataFrame({'PassengerId':X_test.PassengerId, 'Survived':predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","74f7da10":"# Declare X and y","51456a2b":"Add column for children","ef09bff5":"## Define function for modelling\nTo avoid repetition, we will put our modelling code into a function that returns the trained grid search object","95d14b3c":"# Sex","8d3deb96":"Add a feature for those with a child\/parent","afa816d2":"Below, we can see that the majority of unmarried women were in third class, a higher share than for married women, which could contirbute to higher survival rate of married women","95306f34":"Create a column for the passenger's title","8d507045":"# EDA","c6781923":"## Parch\nFemales who survived were less likely to have a child\/parent with them than whose who didn't. Males who survived were more likely to have a child\/parent with them. Men may have been able to get on a lifeboat if they had a child with them. We ","febeb0eb":"# Correlations","66b95bfa":"## Fare\nPassengers with a higher fare were more likely to survive than those with lower fare. This is in line with passenger in First Class being more likely to survive","0492beba":"38% of passengers survived","394aa72e":"Breaking down by Class, we see the fare has larger impact if you're in first class","7611bdeb":"We see that the best Grid Search score is 83.7%, higher than SGD Classifier (82.8%) and KNN (82.9%)","38301396":"## KNN Grid Search Results (Overfitting)\nBy visualising the mean train and test scores during the grid search we can observe if we're not overfitting on the train set. We can obtain the results of each iteration of the grid search by using the cv_results_ attribute","3da7cc2d":"## Pclass and Sex\nPassengers are less likely to survive in the third class for both men and women. The discrepancy isn't as large between first and second class. Women in third class have ~ 50% chance of surviving vs 96% for women in first class. Men in First class have a 36% chance of surviving vs 13% for men in third class","496eba97":"### Visualise each iteration over the list of n_neigbors\nAs we can see in the graph, the model was overfitting at the beginning (where k < 4). As we increased K so that the model wasn't too dependent on the immediate neighbour, we can see the test results improving, with model finding the optimum of 17 neighbors. The test results stopped improving after about K = 17. If we continued increasing K we would probably start to see underfitting with both train and test scores falling","35acf2dd":"# Scoring on the test set","87ca91e1":"Below, we can see that Master refers to young boys, hence the higher change of survival. ","e5a06e41":"Note that 23% of the data has a cabin against it, so breaking down by 7 cabin decks may not provide much benefit","48ff1a10":"People in third class could have larger families (perhaps high mortality leading to more births). We can see larger families in third class below","095ac1ca":"# Embarked\nDoes chance of survival change by city embarked from? Passengers who embarked from Queensway and Southampton had the lowest rate of survival (38% and 33% respectively). Those who departed from Cherbourg had a 55% survival rate. Breaking Embarked by class we see that it's part driven by this. First class made up a higher share of Cherbourg and and there were barely any second and third class departing from Queensway.","8844a442":"# Read data","8e0bcb4e":"# Logistic Regression \/ SVM\nThe grid search best score is 82.7%, slightly lower than KNN which had 82.9%","107b22a5":"Scoring on the whole titanic dataset we get a score of 89%, 10 points higher than predict female as the survivors (79%)","5501e1a1":"## Age\nThere isn't much discrepency in age between those who survived vs didn't. However, we can see a bump in those under 10 surviving - 'women and children first'. We could potentially build a feature off of this.","74230b79":"we can see a survival rate of 46% for those with a sibling or spouse vs 34% for those without","23bd3065":"Breaking down by class, we can see the difference more clearly: within class, younger people are more likely to survive, particularly those under 15. (I have opted for the boxen plot because it won't show out of range values like the violin plot which uses KDE). We could add a feature for under 15","6aa86a54":"Observe the non-null cabins (284 out of 891 are non-null)","3bd35651":"# K Nearest Neighbour\nWe use a pipeline to impute missing data, scale the data and fit a KNN model.\nThere are more complex pipelines you can build that involve one hot encoding on the categorical features https:\/\/towardsdatascience.com\/extracting-feature-importances-from-scikit-learn-pipelines-18c79b4ae09a  \nKNN achieves an accuracy of 84% on the train set, an improvement on the benchmark of 79%","1c15bc96":"Add a column for lack of Sibling\/Spouse","a6440d7f":"Master has a higher survival rate of 58% vs 15% for Mr. This could be because Master refers to younger men. Married women (79% survival) are more likely to survive than unmarried women (70% survival)","61af7db4":"# Random Forest Comparison to Benchmark","c07c9a14":"# Random Forest Classifier\nNow we will run a gridsearch using Random Forest","b4b43e2a":"We can extract the title of the passengers to understand differences between married\/unmarried women","3e5c0675":"## Define Functions","732c2f7f":"Those with a child\/parent had a 51% chance of survival, vs 34% survival for those without","dc2fc0d6":"## How many Survived?\nFewer survived than didn't. About 300 survived and just over 500 didn't","cab36cc4":"## Counts of categorical variables\nPassengers in third class made up the largest share  \nSouthampton is the most popular city to embark from  \nThere were fewer women than men on board (~1\/3 women)","47d9cca5":"## Name","a220285d":"# Generate a Cabin Deck column\nThere could be patterns depending on the cabin deck. There are 7 lettered cabin decks A-G, descending in height. A is the highest deck. Being in a higher deck could increase chance of survival. There are also more first class passengers in higher decks","906b59b3":"### Cabins not classified with a cabin deck\nThere are four rows which wasn't class. From research, cabin T is the Boat Deck (top deck).","4c1f5619":"# Modelling","1f56af24":"## SGD Classifier Feature Importance\nWe can see from the featuer importance that the model is picking up the right signals i.e. married women the most likely to survive, the higher the fare, the higher the chance of survival, being male or in third class reducing the chance of survival","a6ead47a":"## SibSp\nThose who survived were more likely to have a sibling or spouse","67e549fa":"### Visualise the cabin deck\nMost passengers stayed in decks B-E. B, D and E have a similar survival rate of ~75%, C has a lower survival rate of 60% (Not sure why after research)","0f699ac0":"# Benchmark - predict female as survivor\nBefore modelling, let's set a benchamark, so we can see if our model is adding value.\nUsing the Kaggle default submission of predicting female passengers as survivors, the accuracy to beat is 78.7%","998c9449":"## Dummify Categorical Variables"}}