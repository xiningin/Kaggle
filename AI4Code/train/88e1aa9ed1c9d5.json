{"cell_type":{"7939d842":"code","fd06f4b6":"code","cdd4072f":"code","d35e2ec7":"code","d904c0dd":"code","e9828d7c":"code","cb2975fd":"code","caa3be1e":"code","3d92269d":"code","edd53f0c":"code","689c52c8":"markdown","e25eeb96":"markdown","d8b67202":"markdown","38665f2a":"markdown","56d15c41":"markdown"},"source":{"7939d842":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA       # Not used yet\n# Machine Learning Methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fd06f4b6":"train_dataset = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\ntest_dataset = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTest.csv')","cdd4072f":"train_dataset.head(2)","d35e2ec7":"# Checking the unique values of the labels\nprint('Train Dataset:')\nprint(train_dataset['LABEL'].value_counts())\nprint('Test Dataset:')\nprint(test_dataset['LABEL'].value_counts())","d904c0dd":"# Let's split the data into train and test sets and check each feature looking for anomalies \n# OBS: the LABEL feature is the only categorical variable in the dataset\nX_train, y_train = train_dataset.drop(columns=['LABEL'], axis=1), train_dataset['LABEL']\nX_test, y_test = test_dataset.drop(columns=['LABEL'], axis=1), test_dataset['LABEL']\n\n# Divide the test sets in two parts (interseption empty, similar size) in order to check variance of the classificators\nX_test_1, X_test_2 = X_test[:285], X_test[285:]\ny_test_1, y_test_2 = y_test[:285], y_test[285:]","e9828d7c":"# Let's apply a Standard Scaler function to both, X_train and X_test sets\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_1_scaled = scaler.transform(X_test_1)\nX_test_2_scaled = scaler.transform(X_test_2)","cb2975fd":"# Logistic Regression\nLR = LogisticRegression()\nLR.fit(X_train_scaled, train_dataset['LABEL'])\n\nscore_trainLR = LR.score(X_train_scaled, train_dataset['LABEL'])\nscore_test1LR = LR.score(X_test_1_scaled, y_test_1)\nscore_test2LR = LR.score(X_test_2_scaled, y_test_2)\n\nprint(\"Score train: \", score_trainLR)\nprint(\"Score test set 1: \", score_test1LR)\nprint(\"Score test set 2: \", score_test2LR)","caa3be1e":"# Random Forest Classifier\nRF = RandomForestClassifier()\nRF.fit(X_train_scaled, train_dataset['LABEL'])\n\nscore_trainRF = RF.score(X_train_scaled, train_dataset['LABEL'])\nscore_test1RF = RF.score(X_test_1_scaled, y_test_1)\nscore_test2RF = RF.score(X_test_2_scaled, y_test_2)\n\nprint(\"Score train: \", score_trainRF)\nprint(\"Score test set 1: \", score_test1RF)\nprint(\"Score test set 2: \", score_test2RF)","3d92269d":"# Gradient Boosting Classifier\nGB = GradientBoostingClassifier()\nGB.fit(X_train_scaled, train_dataset['LABEL'])\n\nscore_trainGB = RF.score(X_train_scaled, train_dataset['LABEL'])\nscore_test1GB = RF.score(X_test_1_scaled, y_test_1)\nscore_test2GB = RF.score(X_test_2_scaled, y_test_2)\n\nprint(\"Score train: \", score_trainGB)\nprint(\"Score test set 1: \", score_test1GB)\nprint(\"Score test set 2: \", score_test2GB)","edd53f0c":"plt.figure(figsize=(20, 7.5))\ndata_labels = ('LR Train', 'LR Test 1', 'LR Test 2',\n               'RF Train', 'RF Test 1', 'RF Test 2',\n               'GB Train', 'GB Test 1', 'GB Test 2') \n\ny_pos = np.arange(len(data_labels))\nperformances = [score_trainLR, score_test1LR, score_test2LR,\n                score_trainRF, score_test1RF, score_test2RF,\n                score_trainGB, score_test1GB, score_test2GB]\nplt.bar(y_pos, performances, align='center', alpha=.55)\nplt.xticks(y_pos, data_labels)\nplt.ylabel('Scores')\nplt.title('Scores of Logistic Regression (LR), Random Forest Classifier (RF) and Gradient Boosting Classifier (GB)')\nplt.show()","689c52c8":"# CONTENT\n\n1. [Preparing the data](#step1)\n2. [Machine Learning Methods](#MLM)\n3. [Conclusions and Visualizations](#conclusions)","e25eeb96":"<a name=\"step1\"><\/a>\n## (1) Preparing the data\n\nFirst we are going to explore the dataset and preprocessing it.","d8b67202":"Let's Check Normality of the values of each feature","38665f2a":"<a name='MLM'><\/a>\n## (2) Machine Learning Methods","56d15c41":"<a name=\"conclusions\"><\/a>\n## Conclusions and visualizations\n\nBefore trying more computationally demanding methods (deep learning or evolutive computing based ones, I will apply them in future notebooks) I just tried simpler ones based on Machine Learning and they are enough to reach very good performance with low variability."}}