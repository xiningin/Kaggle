{"cell_type":{"5bc2c419":"code","ba432485":"code","9a7b360c":"code","a38d13ba":"code","ae9ada15":"code","ed80b9dc":"code","17ff6d38":"code","ec0a8ed8":"code","5ac34e90":"code","ff6b5c42":"code","c07c9404":"code","582bac2d":"code","4735f896":"code","4338badd":"code","78b6560d":"code","a0c72ed4":"code","ef73f7d6":"code","200ef8dc":"code","225d5bb5":"code","4a419fc9":"code","144a964a":"code","ca4afeeb":"code","3913a358":"code","bb76d8e9":"code","4386376f":"code","7a39192d":"code","57dcde4b":"code","25aa4bed":"code","b2fc5dbe":"code","9132e690":"code","1268c9ab":"code","c2d42ef8":"code","2142a45c":"code","37dc6720":"code","32689578":"code","597dc70c":"code","9b18f632":"code","ba568ab7":"code","b1c96f74":"code","26a60c08":"code","b36dac8c":"code","374b7feb":"code","3e1c6722":"code","e9c18c9a":"code","2873b7b1":"code","6e9c0fcd":"code","aa00d946":"code","3e82e6ae":"code","66bee7d8":"code","5d9ad385":"code","6e06a37c":"code","361dbeb7":"code","bb859750":"code","911a885e":"code","51a218b6":"markdown","2a018d16":"markdown","fed28f65":"markdown","8af5161c":"markdown","81d25d76":"markdown","7f39a231":"markdown","393413df":"markdown","6e53f029":"markdown","a45158ad":"markdown","bcca22e6":"markdown","34830ea6":"markdown","b37023be":"markdown","e58c99d5":"markdown","04a2f715":"markdown","0c090674":"markdown","771aa21a":"markdown","93e249b3":"markdown","eb15c4a9":"markdown","f21ce17a":"markdown","b5aa2cf2":"markdown","66fc321b":"markdown","4ae1e51e":"markdown"},"source":{"5bc2c419":"# general imports\nimport pandas as pd\nimport numpy as np\n\n# plot imports\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport missingno as msno\n%matplotlib inline\n\n\n# sklearn imports\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import accuracy_score","ba432485":"# load data\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.head()","9a7b360c":"# Removing columns that **naturally** do not interfere with the outcome\n# Of course, the outcome is not related to the ticket name or number. The passenger Id will be removed later...\ndf = df.drop(columns=['Name', 'Ticket'])","a38d13ba":"df.info()","ae9ada15":" msno.bar(df)","ed80b9dc":"msno.matrix(df)","17ff6d38":"# creating the outcome column (temporary)... \n# only to be used in graphics\ndf['Outcome'] = df['Survived'].apply(lambda x: 'deceased' if x == 0 else 'survived')\nfig = px.pie(df, values='PassengerId', names='Outcome', labels=['deceased','survived'])\nfig.show()","ec0a8ed8":"vc = df['Outcome'].value_counts()\nd = vc[0]\ns = vc[1]\nprint(f'Deceased: {d}, Survived: {s}, proportion: {d\/s}')","5ac34e90":"df_outcome_by_gender = df.groupby(by=['Outcome', 'Sex']).count().reset_index()\ndf_outcome_by_gender = df_outcome_by_gender[['Outcome', 'Sex', 'PassengerId']]\ndf_outcome_by_gender","ff6b5c42":"d_values = df_outcome_by_gender.iloc[0:2]['PassengerId']\ns_values = df_outcome_by_gender.iloc[2:4]['PassengerId']\n\nlabels = ['female', 'male']\n\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=d_values), 1, 1)\nfig.add_trace(go.Pie(labels=labels, values=s_values), 1, 2)\n\nfig.update_traces(hole=.4)\nfig.update_layout(\n    annotations=[dict(text='Deceased', x=0.18, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Survived', x=0.80, y=0.5, font_size=20, showarrow=False)])\nfig.show()","c07c9404":"df_outcome_by_class = df.groupby(by=[ 'Pclass', 'Outcome']).count().reset_index()\ndf_outcome_by_class = df_outcome_by_class[['Pclass', 'Outcome', 'PassengerId']]\ndf_outcome_by_class","582bac2d":"f_values = df_outcome_by_class.iloc[0:2]['PassengerId']\ns_values = df_outcome_by_class.iloc[2:4]['PassengerId']\nt_values = df_outcome_by_class.iloc[4:6]['PassengerId']\n\nlabels = ['Deceased', 'Survived']\n\n\nfig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=f_values), 1, 1)\nfig.add_trace(go.Pie(labels=labels, values=s_values), 1, 2)\nfig.add_trace(go.Pie(labels=labels, values=t_values), 1, 3)\n\n\nfig.update_traces(hole=.4)\nfig.update_layout(\n    annotations=[dict(text='1st', x=0.12, y=0.5, font_size=20, showarrow=False),\n                 dict(text='2nd', x=0.50, y=0.5, font_size=20, showarrow=False),\n                 dict(text='3rd', x=0.87, y=0.5, font_size=20, showarrow=False),])\nfig.show()","4735f896":"# Passengers will be divided into the following age groups:\n# \n# Kids (under 12)\n# Young (between 12 and 18 years)\n# Adults (between 18 and 65 years)\n# Adults (over 65 years)\n\ndef calculate_age_group(age):\n  if age < 12:\n    return 'Kids'\n  elif age < 18:\n    return 'Young'\n  elif age < 65:\n    return 'Adults'\n  elif age >= 65:\n    return 'Adults (over 65)'\n  \n  # if age is Nan...\n  return '-'\n\ndf['AgeGroup'] = df['Age'].apply(calculate_age_group)\ndf.head()","4338badd":"# Some ages are missing values\n\ndf_outcome_by_age = df.groupby(by=[ 'AgeGroup', 'Outcome']).count().reset_index()\ndf_outcome_by_age","78b6560d":"# I will try to fill in the average age in a stratified way, by class and sex ...\ndf_mean_age = df.groupby(by=[ 'Pclass', 'Sex']).mean().reset_index()\ndf_mean_age = df_mean_age[['Pclass', 'Sex', 'Age']]\ndf_mean_age","a0c72ed4":"# input age (fill missing values)\n# I did hard coded, because there are few values to be filled (only 6)\n# PLEASE... forgive me for the warnings\n\n\n# 1st class\nage = df_mean_age.iloc[0]['Age']\ndf['Age'][(df['Pclass']==1) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==1) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[1]['Age']\ndf['Age'][(df['Pclass']==1) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==1) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\n# 2nd class\nage = df_mean_age.iloc[2]['Age']\ndf['Age'][(df['Pclass']==2) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==2) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[3]['Age']\ndf['Age'][(df['Pclass']==2) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==2) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\n# 3rd class\nage = df_mean_age.iloc[4]['Age']\ndf['Age'][(df['Pclass']==3) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==3) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[5]['Age']\ndf['Age'][(df['Pclass']==3) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==3) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\ndf['AgeGroup'] = df['Age'].apply(calculate_age_group)\ndf.head()","ef73f7d6":"df_outcome_by_age = df.groupby(by=[ 'AgeGroup', 'Outcome']).count().reset_index()\ndf_outcome_by_age","200ef8dc":"k_values = df_outcome_by_class.iloc[0:2]['PassengerId']\ny_values = df_outcome_by_class.iloc[2:4]['PassengerId']\na_values = df_outcome_by_class.iloc[4:6]['PassengerId']\na65_values = df_outcome_by_class.iloc[4:6]['PassengerId']\n\nlabels = ['Deceased', 'Survived']\n\n\nfig = make_subplots(rows=1, cols=4, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=k_values), 1, 1)\nfig.add_trace(go.Pie(labels=labels, values=y_values), 1, 2)\nfig.add_trace(go.Pie(labels=labels, values=a_values), 1, 3)\nfig.add_trace(go.Pie(labels=labels, values=a65_values), 1, 4)\n\n\nfig.update_traces(hole=.4)\nfig.update_layout(\n    annotations=[dict(text='Kids', x=0.1, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Young', x=0.36, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Adults', x=0.62, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Adults > 65', x=0.92, y=0.5, font_size=20, showarrow=False),])\nfig.show()","225d5bb5":"# \"# of siblings \/ spouses aboard the Titanic\"\ndf['SibSp'].unique()","4a419fc9":"# \"# of parents \/ children aboard the Titanic\"\ndf['Parch'].unique()","144a964a":"# Marking those who traveled alone\ndf['Alone'] = True\ndf['Alone'][(df['SibSp'] > 0) | (df['Parch'] > 0)] = False\n\ndf.head()","ca4afeeb":"df_outcome_alone = df.groupby(by=[ 'Alone', 'Outcome']).count().reset_index()\ndf_outcome_alone","3913a358":"ac_values = df_outcome_alone.iloc[0:2]['PassengerId']\nal_values = df_outcome_alone.iloc[2:4]['PassengerId']\n\nlabels = ['deceased', 'survived']\n\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=ac_values), 1, 1)\nfig.add_trace(go.Pie(labels=labels, values=al_values), 1, 2)\n\nfig.update_traces(hole=.4)\nfig.update_layout(\n    annotations=[dict(text='Accompanied', x=0.18, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Alone', x=0.80, y=0.5, font_size=20, showarrow=False)])\nfig.show()","bb76d8e9":"# Creating the column to separate those that have the cabin number information\ndf['CabinInformed'] = False\ndf['CabinInformed'][df['Cabin'].notnull()] = True\ndf.head()\n","4386376f":"df_outcome_cabin = df.groupby(by=['Pclass', 'CabinInformed']).count().reset_index()\ndf_outcome_cabin[['Pclass', 'CabinInformed', 'PassengerId']]","7a39192d":"# Checking the current columns\ndf.info()","57dcde4b":"X = df.copy()\nX.drop(columns=['Survived', 'Outcome'], inplace=True)\ny = df['Survived']","25aa4bed":"X.drop(columns=['PassengerId', 'Fare', 'Cabin', 'Embarked', 'SibSp', 'Parch'], inplace=True)\nX.info()","b2fc5dbe":"# Changing the order of the columns (first the numeric variables and \n# then those that have undergone some change). It will be helpful at this \n# time to check the importance of the features.\n\nX = X[['Age', 'Sex', 'AgeGroup', 'Alone', 'CabinInformed', 'Pclass']]\nX.head()","9132e690":"from sklearn.preprocessing import MinMaxScaler\n\ncat_feats = ['Sex', 'AgeGroup', 'Alone', 'CabinInformed', 'Pclass']\ncat_pipe = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\nnum_feats = ['Age']\nnum_pipe = Pipeline(steps=[\n    ('norm', MinMaxScaler())])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_pipe, num_feats),\n        ('cat', cat_pipe, cat_feats)])","1268c9ab":"preprocessor.fit_transform(X)[0]","c2d42ef8":"knn = KNeighborsClassifier(n_jobs=-1)\nparam_grid = [ \n     \n   { #k-nearest neighbors vote    \n     'estimator': [knn],\n     'estimator__n_neighbors':[3, 5, 7, 9, 11],\n     'estimator__weights':['uniform', 'distance'],\n     'estimator__weights':['uniform', 'distance'],       \n     'estimator__algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n     'estimator__p':[1, 2],       \n   },     \n   { #Decision Tree Classifier  \n     'estimator': [DecisionTreeClassifier(class_weight=\"balanced\")],\n     'estimator__max_depth':[2, 3, 5, 8, 13, 21],\n       \n   },    \n   { #Logistic Regression  \n     'estimator': [LogisticRegression(class_weight='balanced', random_state=0, max_iter=100, n_jobs=-1)],\n     'estimator__class_weight':['balanced', None],\n   },\n   { #Random forest classifier\n     'estimator': [RandomForestClassifier(random_state=0, class_weight='balanced', n_jobs=-1)],\n     'estimator__n_estimators':[2, 3, 5, 8, 13, 21],\n     'estimator__max_depth':[2, 3, 5, 8, 13, 21],\n     'estimator__class_weight':['balanced', None],       \n   },      \n   { #AdaBoost classifier\n     'estimator': [AdaBoostClassifier(random_state=0)],\n     'estimator__n_estimators':[2, 3, 5, 8, 13, 21],\n   },      \n   { #Bagging classifier\n     'estimator': [BaggingClassifier(n_jobs=-1, random_state=0)],\n     'estimator__n_estimators':[2, 3, 5, 8, 13, 21],\n   },      \n]","2142a45c":"pipe = Pipeline(steps=[('preprocessor',preprocessor),\n                       #('preprocScaler',StandardScaler()),\n                       ('estimator', knn) ])","37dc6720":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","32689578":"splits = 3\nkfold = StratifiedKFold(n_splits = splits, shuffle = True, random_state = 0)","597dc70c":"metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n\nrefit = 'accuracy'\ngscv = GridSearchCV(pipe, param_grid=param_grid, scoring=metrics, verbose=10, refit=refit, return_train_score=True, cv=kfold, n_jobs=-1)\ngscv.fit(X_train, y_train) ","9b18f632":"gscv.best_params_","ba568ab7":"y_pred = gscv.predict(X_test)","b1c96f74":"plot_confusion_matrix(gscv, X_test, y_test, normalize='true') ","26a60c08":"print(classification_report(y_test, y_pred))","b36dac8c":"plot_roc_curve(gscv, X_test, y_test)  ","374b7feb":"disp = plot_precision_recall_curve(gscv, X_test, y_test)","3e1c6722":"def best_threshold(y, y_p):\n    thresholds = np.arange(0.01, 0.99, 0.01)\n\n    pred_df = pd.DataFrame(y_p)\n    y_df = pd.DataFrame(y)\n\n    if pred_df.shape[1] == 1:\n        pred_df = pd.concat(\n            [1 - pred_df, pred_df], axis=1)\n\n    if y_df.shape[1] == 1:\n        y_df = pd.concat(\n            [1 - y_df*1, y_df*1], axis=1)\n        y_df.columns = ['Negative class', 'Positive class']\n\n    # filter mirror classes\n    if pred_df.shape[1] == 2:\n        pred_df = pred_df.iloc[:, -1:]\n        y_df = y_df.iloc[:, -1:]\n\n    tpr = []\n    tnr = []\n    accs = []\n    for thr in thresholds:\n        # use threshold to estimate classes\n        y_pred = pred_df.copy()\n        y_pred_prob = pred_df.copy()\n        y_pred[y_pred < thr] = 0\n        y_pred[y_pred >= thr] = 1\n        \n        acc = accuracy_score(y_df, y_pred)\n        accs.append(acc)        \n\n    idx = np.argmax(accs)\n    thr = thresholds[idx]\n    y_pred = pred_df.copy()\n    y_pred_prob = pred_df.copy()\n    y_pred[y_pred < thr] = 0\n    y_pred[y_pred >= thr] = 1\n    \n    return thr, y_pred\n    ","e9c18c9a":"y_p= gscv.predict_proba(X_test)\nthr, y_pred =  best_threshold(y_test, y_p)\nthr","2873b7b1":"print(classification_report(y_test, y_pred))","6e9c0fcd":"df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf","aa00d946":"# Removing columns that **naturally** do not interfere with the outcome\ndf = df.drop(columns=['Name', 'Ticket'])","3e82e6ae":"# Adjusting age (according to training base)\n# 1st class\nage = df_mean_age.iloc[0]['Age']\ndf['Age'][(df['Pclass']==1) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==1) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[1]['Age']\ndf['Age'][(df['Pclass']==1) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==1) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\n# 2nd class\nage = df_mean_age.iloc[2]['Age']\ndf['Age'][(df['Pclass']==2) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==2) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[3]['Age']\ndf['Age'][(df['Pclass']==2) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==2) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\n# 3rd class\nage = df_mean_age.iloc[4]['Age']\ndf['Age'][(df['Pclass']==3) & (df['Sex'] == 'female')] = \\\n  df[(df['Pclass']==3) & (df['Sex'] == 'female')]['Age'].fillna(age)\n\nage = df_mean_age.iloc[5]['Age']\ndf['Age'][(df['Pclass']==3) & (df['Sex'] == 'male')] = \\\n  df[(df['Pclass']==3) & (df['Sex'] == 'male')]['Age'].fillna(age)\n\ndf['AgeGroup'] = df['Age'].apply(calculate_age_group)\ndf.head()","66bee7d8":"# Creating the column to separate those that have the cabin number information\ndf['CabinInformed'] = False\ndf['CabinInformed'][df['Cabin'].notnull()] = True\ndf.head()","5d9ad385":"# Marking those who traveled alone\ndf['Alone'] = True\ndf['Alone'][(df['SibSp'] > 0) | (df['Parch'] > 0)] = False\n\ndf.head()","6e06a37c":"# Leaving only columns used in the model\n\npassenger_id = df['PassengerId']\n\ndf.drop(columns=['PassengerId', 'Fare', 'Cabin', 'Embarked', 'SibSp', 'Parch'], inplace=True)\ndf","361dbeb7":"df = df[['Age', 'Sex', 'AgeGroup', 'Alone', 'CabinInformed', 'Pclass']]\ndf.head()","bb859750":"y_pred = gscv.predict(df)","911a885e":"output = pd.DataFrame({'PassengerId': passenger_id, 'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)","51a218b6":"**Question 02: How many died and how many survived (by gender)?**\n\nSome thoughts:\n\n* It is possible to observe that, among those who died, the vast majority were male (about 85%)\n\n* Among those who survived, about 65% were female\n\n* This can cause some bias in the data prediction","2a018d16":"Some thoughts:\n\n* The 'age' column has about 20% of missing values. This will probably be \ndealt with in the pre-processing stage of the database (filled with mean, median, knn or something similar...).\n* The 'Cabin' column has a lot of missing data, but in any case, it should be eliminated from the predictor variables (or transformed)\n","fed28f65":"## Prediction execution","8af5161c":"# EDA - Exploratory Data Analysis","81d25d76":"## Ploting results","7f39a231":"**Question 00: How about missing values?**","393413df":"**Question 06: Those who have the booth number are what class?**\n\nSome thoughts:\n\n* Among those who are in the first class, the vast majority have the number of cabin informed, which does not occur in the other classes ...\n* I still don't know if that makes a difference in the prediction or the outcome ...","6e53f029":"As stated earlier, there are variables that naturally do not influence the outcome, so they will be removed\n\n\nThe variables SibSp and Parch will be removed, since they have already been encapsulated by the variable Alone","a45158ad":"**Question 03: How many died and how many survived (by class)?**\n\n\nSome thoughts:\n\n* First class passengers had a more favorable outcome (most were saved), while second and third class passengers had unfavorable outcomes (most died)","bcca22e6":"**Question 05: Did accompanied passengers have different chances of survival?**\n\n\nSome thoughts:\n\n* Being accompanied did not significantly increase the chance of survival (50% chance) ... probably this is due to the fact that, being accompanied, one was saved by the boat and another stayed on the ship\n* However, being alone significantly decreased the chance of survival","34830ea6":"### Choosing the best threshold for the test base","b37023be":"# PREDICTION OF TITANIC SURVIVORS USING MACHINE LEARNING TECHNIQUES \n\n## GOALS: Given a set of characteristics of titanic passengers, predicting the outcome between survivors or deceased","e58c99d5":"**Question 04: How many died and how many survived (by age group)?**\n\nSome thoughts:\n\n* The younger, the greater the chance of survival","04a2f715":"**Question 01: How many died and how many survived?**\n\nSome thoughts:\n\n* The database is slightly unbalanced (for every passenger who survived, 1.6 passengers died)\n* It may be necessary to use some artificial balancing technique","0c090674":"Separation of the training and testing set","771aa21a":"# Prediction on test base","93e249b3":"# Predictive model","eb15c4a9":"## Executing grid search","f21ce17a":"Configuration of prediction models","b5aa2cf2":"Including pre-processing for categorical variables (one hot encoder)","66fc321b":"**Now it's time to separate between predictor variables and the outcome variable**","4ae1e51e":"k-fold"}}