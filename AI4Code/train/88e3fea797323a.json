{"cell_type":{"db90fca0":"code","68358b12":"code","d858f030":"code","11b4684e":"code","6906d02a":"code","4c18928f":"code","dd14b805":"code","53905dfb":"code","20d939d7":"code","a911a48b":"code","874a86cc":"code","05224095":"code","bef10f21":"code","6a40a950":"code","983fc928":"code","43ed7917":"code","27d3e6ad":"code","5898749e":"code","dc3406a9":"code","f4c3d1ab":"code","8b08ee9f":"code","59b0cf3d":"code","eb6ee74c":"code","dc0d640a":"code","f7c62df8":"code","4fe64b0a":"code","2dcaa3a4":"code","3ad94b7a":"code","436c7d47":"code","3527a537":"code","4b23464c":"code","1e80f567":"code","bbfc5311":"code","643456f4":"code","55e209b0":"code","56d7f892":"code","4b684f7f":"code","13b72ffd":"code","71b1a815":"code","e87cf389":"code","8e258866":"code","cd3eb289":"code","b5be8c09":"code","a66509da":"code","763f75fd":"code","2b1328ee":"code","4ad1e20f":"code","a2eeb288":"markdown","4e6e89ec":"markdown","ae944401":"markdown"},"source":{"db90fca0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68358b12":"from matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize, WordNetLemmatizer\nimport nltk\nimport re \nnltk.download('wordnet')\n  ","d858f030":"## Load in the data\ndata = pd.read_csv(\"..\/input\/stockmarket-sentiment-dataset\/stock_data.csv\")","11b4684e":"## Read the data\ndata.head()","6906d02a":"data.shape","4c18928f":"data.info()","dd14b805":"data.describe()","53905dfb":"## Sentiment Value count \ndata[\"Sentiment\"].value_counts()","20d939d7":"## Plot the Sentiment value count \nsns.countplot(data[\"Sentiment\"])","a911a48b":"## Lenght of the Text using KDEplot\nlenght = data[\"Text\"].str.len()\nsns.kdeplot(lenght)","874a86cc":"## Checking for stopwords\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)","05224095":"word_list = list()\nfor i in range(len(data)):\n    lip = data.Text[i].split()\n    for k in lip:\n        word_list.append(k)","bef10f21":"from collections import Counter \nwordCounter = Counter(word_list)\ncountedWordDict = dict(wordCounter)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nsortedWordDict[0:20]","6a40a950":"from wordcloud import WordCloud\nwordList2 = \" \".join(word_list)\nstop_word_Cloud = set(stopwords.words(\"english\"))\nwordcloud = WordCloud(stopwords=stop_word_Cloud,max_words=2000,background_color=\"white\",min_font_size=3).generate_from_frequencies(countedWordDict)\nplt.figure(figsize=[20,10])\nplt.axis(\"off\")\nplt.imshow(wordcloud)\nplt.show()","983fc928":"## Replacing the negative one with zero so our model can predict well\ndata[\"Sentiment\"] = data[\"Sentiment\"].replace(-1,0)","43ed7917":"## Lets check our data again\ndata[\"Sentiment\"].value_counts()","27d3e6ad":"data.shape","5898749e":"## NlP Processing\nps = PorterStemmer()\nlemma = WordNetLemmatizer()\nstopwordSet = set(stopwords.words(\"english\"))","dc3406a9":"## Clean the text \ntext_reviews = list()\nfor i in range(len(data)):\n    text = re.sub('[^a-zA-Z]',\" \",data['Text'][i])\n    text = text.lower()\n    text = word_tokenize(text,language=\"english\")\n    text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet]\n    text = \" \".join(text)\n    text_reviews.append(text)","f4c3d1ab":"## Create the (B.O.W) bag of word model\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(text_reviews).toarray()\ny= data['Sentiment']\n\n## Split the dataset into Training and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state = 0)","8b08ee9f":"## Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)","59b0cf3d":"print(classification_report(y_test, Y_pred))","eb6ee74c":"print(confusion_matrix(y_test, Y_pred))","dc0d640a":"## Naives baye multinomial\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\nY_pred = clf.predict(X_test)","f7c62df8":"print(classification_report(y_test, Y_pred))","4fe64b0a":"print(confusion_matrix(y_test, Y_pred))","2dcaa3a4":"## Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)","3ad94b7a":"print(classification_report(y_test, Y_pred))","436c7d47":"print(confusion_matrix(y_test, Y_pred))","3527a537":"# Stock Data analysis","4b23464c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e80f567":"# Extracting bank data\nhdfc = pd.read_csv('\/kaggle\/input\/stock-market-india\/FullDataCsv\/HDFCBANK__EQ__NSE__NSE__MINUTE.csv',index_col='timestamp')\nnb = pd.read_csv('\/kaggle\/input\/stock-market-india\/FullDataCsv\/NIFTY_BANK__EQ__INDICES__NSE__MINUTE.csv',index_col='timestamp')\nkotak = pd.read_csv('\/kaggle\/input\/stock-market-india\/FullDataCsv\/KOTAKBANK__EQ__NSE__NSE__MINUTE.csv',index_col='timestamp')\nicici = pd.read_csv('\/kaggle\/input\/stock-market-india\/FullDataCsv\/ICICIBANK__EQ__NSE__NSE__MINUTE.csv',index_col='timestamp')","bbfc5311":"nb.head()","643456f4":"hdfc.head()","55e209b0":"fin_df = [hdfc,icici,nb,kotak]\nfor i in fin_df:\n    i['Difference'] = (i['open']-i['close'])\/i['open']*100\n    i.drop(['open','high','close','low','volume'],axis=1,inplace=True)","56d7f892":"c_df= pd.concat(fin_df,axis=1,ignore_index=True)\n","4b684f7f":"c_df.head()","13b72ffd":"c_df.columns = ['HDFC','ICICI','BNIFTY','KOTAK']\nc_df.head()","71b1a815":"c_df.shape","e87cf389":"c_df.isnull().sum()","8e258866":"import missingno as msg\nmsg.matrix(c_df)","cd3eb289":"c_df.dropna(inplace=True,axis=0)\nfrom sklearn.preprocessing import StandardScaler","b5be8c09":"ss = StandardScaler()\nc_df = ss.fit_transform(c_df)","a66509da":"df = pd.DataFrame(data=c_df)","763f75fd":"df.columns = ['HDFC','ICICI','BNIFTY','KOTAK']\ndf.head()","2b1328ee":"df.corr()","4ad1e20f":"import seaborn as sns \nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,12))\nsns.heatmap(df.corr(),annot=True)","a2eeb288":"# Data Preprocesing","4e6e89ec":"# NLP Processing","ae944401":"# Modelling and Predicting"}}