{"cell_type":{"a8b696ef":"code","ef39bd17":"code","dc53ceae":"code","90e3f05f":"code","4f070cca":"code","ff9469f3":"code","f0724021":"code","a0f25532":"code","a5893078":"code","1dc10340":"code","533ad6d9":"code","43392096":"code","f7fa8d45":"code","87a5efc7":"markdown"},"source":{"a8b696ef":"# libraries, classes etc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='ticks', color_codes=True)\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report","ef39bd17":"# loading data\ndf = pd.read_excel('..\/input\/covid19\/dataset.xlsx')\n\ndf = df.set_index('Patient ID') # patients as index\n\ndf = df.drop([\n    'Patient addmited to regular ward (1=yes, 0=no)',\n    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n    'Patient addmited to intensive care unit (1=yes, 0=no)'\n], axis=1)\n\n# first observations\ndf.head()","dc53ceae":"# dimension\ndf.shape","90e3f05f":"# numeric conversion\ndf['Urine - pH'] = df['Urine - pH'].replace('N\u00e3o Realizado', 0).astype('float')\ndf['Urine - Leukocytes'] = df['Urine - Leukocytes'].replace('<1000', 0).astype('float')\n\ndef func(x):\n    if x == 'negative':\n        return 0\n    elif x == 'positive':\n        return 1\n\ndf['SARS-Cov-2 exam result'] = df['SARS-Cov-2 exam result'].apply(func) ","4f070cca":"# object features\ndf['Patient age quantile'] = df['Patient age quantile'].astype('str')\n\nobject_columns = df.select_dtypes(include='object').columns\n\n# keep numeric features (aka hemogran)\ndf_num = df.drop(object_columns, axis=1)\ndf_num = df_num.dropna(thresh=len(df_num.columns) * 0.3) # keep rows with 30% of NaN\n\n# dimension\ndf_num.shape","ff9469f3":"print('Negative COVID-19 cases: ', len(df_num[df_num['SARS-Cov-2 exam result'] == 0]))\nprint('Positive COVID-19 cases: ', len(df_num[df_num['SARS-Cov-2 exam result'] == 1]))","f0724021":"# new dataframe\ndf = df_num.copy()\n\n# features with at least 1 observation filled \ndf = df.dropna(axis=1, how='all')\n\n# dimension\ndf.shape","a0f25532":"# predictors\nX = df.drop(['SARS-Cov-2 exam result'], axis=1)\n\n# fill missing values for RandomUnderSampler\nimputer = KNNImputer(n_neighbors=3)\nimputer.fit(X)\nX = pd.DataFrame(imputer.transform(X), index=X.index, columns=X.columns)\n\n# target\ny = df['SARS-Cov-2 exam result']\n\n# standardize features\nscaler = StandardScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns=X.columns, index=X.index)","a5893078":"# feature selection with ExtraTreesClassifier  \nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X, y)\n\n# feature selection and preparation for feature importance plot \nfeature_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['Feature importance'])\n\ntreshold = feature_importances['Feature importance'].quantile(q=0.5) # treshold (median)\n\nfeature_importances = feature_importances[feature_importances['Feature importance'] >= treshold] # cut above treshold\nfeatures = feature_importances.index\nfeature_importances = feature_importances.sort_values('Feature importance', ascending=False)\n\nplt.subplots(figsize=(10,10))\nsns.barplot(x=feature_importances['Feature importance'], y=feature_importances.index, data=feature_importances, color='b')\nplt.title('Feature Importance', size=15)\nplt.xlabel('%')\nplt.grid(axis='x')\nplt.show()","1dc10340":"# dataframe with best predictors\nX = X[list(features)]\n\n# dimension\nX.shape","533ad6d9":"# train\/test split\nmsk = np.random.rand(len(df)) < 0.6\nX_train, y_train = X[msk], y[msk]\nX_test, y_test = X[~msk], y[~msk]\n\n# RandomUnderSampler\nsampler = RandomUnderSampler(random_state=123)\nX_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\nX_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)","43392096":"# classifier name\nnames = [\n    'Random forest',\n    'XGBoost',\n    'Multi-layer Perceptron'\n]\n\n# classifier class\nclassifiers = [\n    RandomForestClassifier(n_jobs=-1, random_state=123),\n    XGBClassifier(n_jobs=-1, random_state=123),\n    MLPClassifier()\n]\n\n# hyperparameter space\nparameters = [\n           \n    # Random forest\n    {\n        'bootstrap' : Integer(0, 1),\n        'max_depth' : Integer(5, 15),\n        'max_features' : Categorical(['auto', 'sqrt']),\n        'min_samples_leaf' : Integer(1, 4),\n        'min_samples_split' : Integer(2, 10),\n        'n_estimators': Integer(50, 100)\n    },\n    \n    # XGBoost\n    {\n        'learning_rate' : Real(0.05, 0.31, prior='log-uniform'),\n        'max_depth' : Integer(5, 15),\n        'min_child_weight' : Integer(1, 8),\n        'colsample_bytree' : Real(0.3, 0.8, prior='log-uniform'),\n        'subsample' : Real(0.8, 1, prior='log-uniform'),\n        'n_estimators' : Integer(50, 100)\n    },\n    \n    # Multi-layer Perceptron\n    { \n        'activation' : Categorical(['identity', 'logistic', 'tanh', 'relu']),\n        'solver' : Categorical(['lbfgs', 'sgd', 'adam']),\n        'alpha' : Real(1e-6, 1e-2, prior='log-uniform'),\n        'learning_rate' : Categorical(['constant', 'invscaling', 'adaptive']),\n        'max_iter' : Integer(100, 500)\n    }\n]","f7fa8d45":"# classifier + bayesian optimization of hyperparameters\nfor name, clf, param in zip(names, classifiers, parameters):\n     \n    opt = BayesSearchCV(clf, param, scoring='precision', n_iter=50, cv=10, n_jobs=-1, refit=True, random_state=123)\n    opt.fit(X_train_resampled, y_train_resampled)\n    \n    cv_results = opt.cv_results_\n    best_params = opt.best_params_\n    \n    y_pred = opt.best_estimator_.predict(X_test)\n    \n    print(name)\n    \n    print('')\n    \n    plt.boxplot(cv_results['mean_test_score'])\n    plt.title('Precision - Train')\n    plt.show()\n    \n    print('')\n    \n    print('AUC score: ', roc_auc_score(y_test, y_pred))\n    \n    print('')\n    \n    print(classification_report(y_test, y_pred))\n    \n    print('')\n    print('-----------------------------------------------------------------------')\n    print('')","87a5efc7":"## Task 1\n\nI tried Random Forest, XGBoost and Multi-layer Perceptron models, all with Bayesian hyperparameter optimization. The best model among the three in training was the Random Forest, in test this model predicts negative cases very well but does not predict positive cases well."}}