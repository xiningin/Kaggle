{"cell_type":{"785997e8":"code","9e2767ef":"code","db83be66":"code","0fd7c7f1":"code","82f24b5e":"code","bdf4f512":"code","d82b5695":"code","8fd0de19":"code","98336e4c":"code","e3d70c84":"code","5ff48b48":"code","1ffa30bb":"code","5ec1a5b7":"code","ab462407":"code","818718b8":"code","c592f3d8":"code","5cfd8c30":"code","b97e8746":"code","bf3a71b6":"code","5ccef2ab":"code","1a0962da":"code","da7572c1":"code","a742f217":"markdown","9b9f35bb":"markdown","6052e5ff":"markdown","9a5ad006":"markdown","88f1051f":"markdown","0f04a9b6":"markdown","7211d674":"markdown"},"source":{"785997e8":"import pandas as pd\nfrom pprint import pprint\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndata = pd.read_csv('..\/input\/emotion-detection-from-text\/tweet_emotions.csv')\n\nprint('No. of Tweets: ',data.shape[0])\nprint(data.isnull().sum())\nprint()\nprint('-- Sentiment-Count --')\nprint()\nprint(data.sentiment.value_counts())\nprint()\nall_classes = data.sentiment.unique().tolist()\n\npprint(all_classes)\n\ndata.head()","9e2767ef":"''' Manual Splitting '''\nimport gc\n\nm = data.shape[0]\ntrain_split = 0.75 * m\nval_split = (1 - 0.75) * m\ntest_split = 0.20 * val_split\nval_split = (1 - 0.20) * val_split\n\nprint('Splitting ratio: ')\nprint()\nprint('train_split: ', train_split, ', val_split: ', val_split, ', test-split: ', test_split)\n\ntrain_data = data.iloc[:int(train_split)]\nval_data = data.iloc[int(train_split) + 1: int(train_split) + int(val_split)]\ntest_data = data.iloc[int(train_split) + int(val_split) + 1 : int(train_split) + int(val_split) + int(test_split)]\nprint()\nprint('All_Shapes: ')\nprint()\nprint(train_data.shape, val_data.shape, test_data.shape)\n\ndel val_split\ndel train_split\ndel test_split\ngc.collect()","db83be66":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nsns.set_style('darkgrid')\n\nplt.figure(figsize=(16,8))\nplt.title('Data-Distribution')\nsns.histplot(train_data['sentiment'], kde=True, color='black', stat='density')\nplt.show()\n\nprint(train_data['sentiment'].value_counts())","0fd7c7f1":"''' Resampling -- Another Approach to tackle Imbalance '''\n\nfrom sklearn.utils import resample\n\nmaxx = 3\ntarget_majority = train_data[train_data.sentiment==all_classes[maxx]]\n\n#min_class = [0, 2, 5, 7, 8, 10, 11, 12]\n\nfor cl in range(13):\n    train_minority = data[data.sentiment==all_classes[cl]]\n    train_minority_upsampled = resample(train_minority, replace=True, n_samples=len(target_majority), random_state=123)\n    if cl == 0:\n        train_upsampled = pd.concat([train_minority_upsampled, target_majority])\n        #train_upsampled = pd.concat([train_upsampled, ])\n    if cl>0 and cl!=maxx:\n        train_upsampled = pd.concat([train_minority_upsampled, train_upsampled])\n\ntrain_upsampled['sentiment'].value_counts() \n#'''","82f24b5e":"data = train_upsampled.sample(frac=1).reset_index(drop=True)\nplt.figure(figsize=(16,8))\nplt.title('Equal-Data-Distribution')\nsns.histplot(data['sentiment'], kde=False, color='black', stat='count')\nplt.show()","bdf4f512":"import string\nimport numpy as np\nimport tensorflow as tf\n\ncontent_text = train_data.content.tolist()\nall_classes = train_data.sentiment.unique().tolist()\n\n'''\n\ntoken_data = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ')\n\ntoken_data.fit_on_texts(content_text)\nind_text = token_data.texts_to_sequences(content_text)'''\n\n\ncontent_text = train_data.content.tolist()\nex_char = string.punctuation\nex_char = ex_char.replace('~', '')\nc_text = '~~~~~~'.join(content_text)\n\nx = c_text.translate(str.maketrans('', '', ex_char))\nc_text = x.split('~~~~~~')\n\nprint('Again_samples: ',len(c_text))\nprint('Some_Sentences: ')\nprint()\nprint(c_text[34])\nprint(c_text[21])\nprint()\n\ntoken_data = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ')\ntoken_data.fit_on_texts(c_text)\nind_text = token_data.texts_to_sequences(c_text)\n\nprint()\n\nprint('All_samples: ', len(ind_text))\nword_index = token_data.word_index\nprint('Diff. words: ', len(list(word_index)))\n","d82b5695":"label_token = tf.keras.preprocessing.text.Tokenizer(num_words=len(all_classes),\n    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ')\nclasses = ' '.join(all_classes)\nlabel_token.fit_on_texts([classes])\nlabel_index = label_token.word_index\nprint('No. of Labels: ',len(list(label_index)))\nprint()\nlabel_index = {key:value-1 for key, value in label_index.items()}\npprint(label_index)\ninv_label_index = {value:key for key, value in label_index.items()}\nprint()\npprint(inv_label_index)","8fd0de19":"y_train = []\nfor i in train_data.sentiment:\n    y_train.append(label_index.get(i))\ny_train = np.array(y_train)\nprint('Label_shape: ', y_train.shape)\npprint(y_train[:10])","98336e4c":"x_train = np.array(ind_text)\nmax_inp_len = len(x_train[0])\nfor step, i in enumerate(x_train):\n    if len(i) > max_inp_len:\n        max_inp_len = len(i)\nprint('max_input_length: ',max_inp_len)\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = max_inp_len, padding = 'post')\nprint('x_train_shape: ',x_train.shape, ', y_train_shape: ', y_train.shape)\n\n","e3d70c84":"''' Validation & Test Preprocessing '''\n\n#ind_text = token_data.texts_to_sequences(c_text)\n\n\ndef val_preprocess(val_data, token_data, label_index):\n    \n    content_text = val_data.content.tolist()\n    all_classes = val_data.sentiment.unique().tolist()\n\n    content_text = val_data.content.tolist()\n    ex_char = string.punctuation\n    ex_char = ex_char.replace('~', '')\n    c_text = '~~~~~~'.join(content_text)\n\n    x = c_text.translate(str.maketrans('', '', ex_char))\n    c_text = x.split('~~~~~~')\n\n    print('Again_Val_samples: ',len(c_text))\n    print('Some_Val_Sentences: ')\n    print()\n    print(c_text[34])\n    print(c_text[21])\n    print()\n\n    ind_text = token_data.texts_to_sequences(c_text)\n    x_val = np.array(ind_text)\n    print()\n\n    print('All_Val_samples: ', len(ind_text))\n    \n    y_val = []\n    for i in val_data.sentiment:\n        y_val.append(label_index.get(i))\n    y_val = np.array(y_val)\n    print('Label_shape: ', y_val.shape)\n    \n    return x_val, y_val\n    \ndef test_preprocess(test_data, token_data, label_index):\n    \n    content_text = test_data.content.tolist()\n    all_classes = test_data.sentiment.unique().tolist()\n\n    content_text = test_data.content.tolist()\n    ex_char = string.punctuation\n    ex_char = ex_char.replace('~', '')\n    c_text = '~~~~~~'.join(content_text)\n\n    x = c_text.translate(str.maketrans('', '', ex_char))\n    c_text = x.split('~~~~~~')\n\n    print('Again_Test_samples: ',len(c_text))\n    print('Some_Test_Sentences: ')\n    print()\n    print(c_text[34])\n    print(c_text[21])\n    print()\n\n    ind_text = token_data.texts_to_sequences(c_text)\n    x_test = np.array(ind_text)\n    print()\n    print('All_Test_samples: ', len(ind_text))\n    y_test = []\n    for i in test_data.sentiment:\n        y_test.append(label_index.get(i))\n    y_test = np.array(y_test)\n    print('Label_shape: ', y_test.shape)\n    \n    return x_test, y_test\n        \n\nprint('Val-Processing ...')\nx_val, y_val = val_preprocess(val_data, token_data, label_index)\nprint(x_val.shape, y_val.shape)\nprint('---'*20)\nprint('Test_Preprocessing ...')\nx_test, y_test = test_preprocess(test_data, token_data, label_index)\nprint(x_test.shape, y_test.shape)\n","5ff48b48":"x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen = max_inp_len, padding = 'post')\nprint('x_val_shape: ',x_val.shape, ', y_val_shape: ', y_val.shape)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = max_inp_len, padding = 'post')\nprint('x_test_shape: ',x_test.shape, ', y_test_shape: ', y_test.shape)","1ffa30bb":"''' Normal_RNN_Model '''\n\nimport keras_tuner as kt\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 2)\n\n\n    \ninput_1 = tf.keras.layers.Input(shape=(x_train.shape[-1],))\nembd_1 = tf.keras.layers.Embedding(input_dim = len(list(word_index)) + 1, output_dim=128)(input_1)\nbi_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, return_sequences=True, dropout=0.3, recurrent_dropout=0.5), merge_mode = 'concat')(embd_1)\nlstm_1 = tf.keras.layers.LSTM(units=64, dropout=0.3, recurrent_dropout=0.5)(bi_1)\ndense_1 = tf.keras.layers.Dense(len(all_classes), activation='softmax')(lstm_1)\n\nmodel = tf.keras.models.Model(inputs = input_1, outputs = dense_1, name='Basic_RNN')\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\ntf.keras.utils.plot_model(model, show_shapes =True)","5ec1a5b7":"his2 = model.fit(x_train, y_train, epochs=50, validation_data = (x_val, y_val), batch_size=64, callbacks=[stop_early]) #, sample_weight = sample_weights)\nprint()\nprint(' -- Model_Evaluation -- ')\n(model.evaluate(x_test, y_test))\ntf.keras.models.save_model(model, filepath = \".\/basic_rnn.h5\")","ab462407":"from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\nhis_2 = his2.history\n\nfig, axs = plt.subplots(1,2, figsize=(16,8))\n\naxs[0].set_title('Loss_Curve')\nep = range(len(his_2['loss']))\naxs[0].plot(ep, his_2['loss'],'o--r',label = 'Training_loss')\naxs[0].plot(ep, his_2['val_loss'],'o--b',label = 'Val_loss')\naxs[0].set_xlabel('epoch')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n\naxs[1].set_title('Acc_Curve')\nep = range(len(his_2['loss']))\naxs[1].plot(ep, his_2['accuracy'],'o--r',label = 'Training_acc')\naxs[1].plot(ep, his_2['val_accuracy'],'o--b',label = 'Val_acc')\naxs[1].set_xlabel('epoch')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\n\nplt.show()\n\nprint()\n\nypred = model.predict(x_test)\nypred = np.argmax(ypred, axis=-1)\nconf_matrix  = confusion_matrix(ypred, y_test)\n\nprint('--Confusion_Matrix--')\nprint()\nprint(conf_matrix)\nprint()\nprint('f1_score: ',f1_score(y_test, ypred, average='micro'))\nprint('Pr.: ',precision_score(y_test, ypred, average='micro'))\nprint('Re.: ',recall_score(y_test, ypred, average='micro'))","818718b8":"## inference\nimport random\n\nindex = {}\nfor val, key in enumerate(word_index.keys()):\n    index[val+1] = key \ninv_word_index = index\ninv_word_index[0] = ''\n\n\ndef reverser(inp):\n    st = ''\n    for i in inp:\n        st += ' ' + str(inv_word_index.get(i))\n    return st\n\ndef predict():\n    seed = random.randint(0, x_test.shape[0])\n    x_seed = x_test[seed]\n    act = y_test[seed]\n    st = reverser(x_seed)\n    print('Input_Sentence: ', st)\n    print('--'*20)\n    print('Actual_emotion: ', inv_label_index.get(act))\n    pred = (np.argmax(model.predict(x_seed.reshape(1,-1)), axis=-1).tolist())\n    print('--'*20)\n    print('Predicted_emotion: ', inv_label_index.get(pred[-1]))\n\nfor _ in range(5):\n    print('**'*50)\n    predict()\n    print('**'*50)\n    print()","c592f3d8":"## Transformer_architecture\nimport tensorflow as tf\n\nclass T_encoder(tf.keras.layers.Layer):\n    def __init__(self, num_heads, embd_dim, dense_dim, num_classes , **kwargs):\n        super(T_encoder, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.embd_dim = embd_dim\n        self.dense_dim = dense_dim\n        self.multi_head_layer = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embd_dim)\n        \n        self.dense_layer = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation='relu'),\n            tf.keras.layers.Dropout(0.5),\n            tf.keras.layers.Dense(embd_dim)])\n        \n        self.norm_layer_1 = tf.keras.layers.LayerNormalization()\n        self.norm_layer_2 = tf.keras.layers.LayerNormalization()\n        self.Final_Dense = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n        self.supports_mask = True\n    \n\n    def call(self, inputs, mask=None): ## we mask the padding in encoder model\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype='int32') ## because attention is 5-D tensor \n        att_output = self.multi_head_layer(query = inputs, value = inputs, key = inputs, attention_mask = padding_mask)# (all vals) after instantiating pass query, value and key\n        att_output = tf.keras.layers.Dropout(0.5)(att_output)\n        output_1 = self.norm_layer_1(att_output + inputs)\n        \n        dense_output = self.dense_layer(output_1)\n        output_2 = self.norm_layer_2(dense_output + output_1)\n        fl_output = tf.keras.layers.Flatten()(output_2)\n        encoder_output = self.Final_Dense(fl_output)\n        \n        return encoder_output\n        \nclass positional_encoding(tf.keras.layers.Layer):\n    def __init__(self, inp_dim , max_seq_len, embd_dim, **kwargs):\n        super(positional_encoding, self).__init__(**kwargs)\n        self.inp_dim = inp_dim\n        self.max_seq_len = max_seq_len\n        self.embd_dim = embd_dim\n        self.embd_layer = tf.keras.layers.Embedding(input_dim = inp_dim, output_dim = embd_dim)\n        self.pos_layer = tf.keras.layers.Embedding(input_dim = max_seq_len, output_dim = embd_dim)\n        \n    def call(self, inputs):\n        embd_output_1 = self.embd_layer(inputs)\n        pos_in = tf.range(start = 0, limit = self.max_seq_len, delta =1)\n        pos_output_1 = self.pos_layer(pos_in)\n        model_input = embd_output_1 + pos_output_1\n        return model_input\n    \n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0) ## for masking out the padding in encoder_input\n","5cfd8c30":"### AdamW is an optimizer which decays based on training steps \nimport tensorflow_addons as tfa\n'''\nstep = tf.Variable(0)\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n    [10000, 15000], [1e-0, 1e-1, 1e-2])\nlr = 1e-1 * schedule(step) ## learning_rate\nwd = lambda: 1e-4 * schedule(step) ## weight_deacays\n'''\n### Learning Rate Scheduler decays lr based on epochs\ndef sec(epoch, learning_rate):\n    if epoch <= 2:\n        return learning_rate\n    else:\n        return learning_rate * tf.math.exp(-0.9)\n\nlrs = tf.keras.callbacks.LearningRateScheduler(sec)\nstop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)","b97e8746":"import keras_tuner as kt\nembd_dim = 64\nnum_heads = 2\ndense_dim = 64\nnum_classes = len(all_classes)\n\ndef tuner_model(hp):\n    hp_embd_dim = hp.Int('embd_dim', min_value = 16, max_value = 512, step = 16)\n    hp_num_heads = hp.Int('num_heads', min_value = 2, max_value = 10, step = 2)\n    hp_dense_dim = hp.Int('dense_dim', min_value = 16, max_value = 512, step = 16)\n    hp_lr = hp.Choice('lr', values = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n    \n    inp_1  = tf.keras.layers.Input(shape=(x_train.shape[-1], ))\n    pos_op = positional_encoding(inp_dim = len(list(word_index))+1, max_seq_len = max_inp_len, embd_dim = hp_embd_dim )(inp_1)\n    enc_op = T_encoder(num_heads = hp_num_heads , embd_dim = hp_embd_dim, dense_dim = hp_dense_dim, num_classes = num_classes)(pos_op)\n\n    transformer = tf.keras.models.Model(inputs = inp_1, outputs = enc_op, name = 'Transformer')\n    transformer.compile(loss = \"sparse_categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(learning_rate = hp_lr), metrics = ['accuracy'])\n    return transformer\n\ntuner = kt.Hyperband(tuner_model, objective = 'loss', max_epochs = 3)\n\ntuner.search(x_train, y_train, epochs=5, callbacks=[stop_early, lrs])\n\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint('Best_embd_dim: ', best_hps['embd_dim'])\nprint('Best_num_heads: ', best_hps['num_heads'])\nprint('Best_dense_dim: ', best_hps['dense_dim'])\nprint('Best_Learning_Rate: ', best_hps['lr'])\n\nbest_transformer = tuner.hypermodel.build(best_hps)\n\nbest_transformer.summary()\ntf.keras.utils.plot_model(best_transformer, show_shapes = True)","bf3a71b6":"print(x_train.shape, y_train.shape)\nprint('-')\nhis2 = best_transformer.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), batch_size=32, callbacks = [stop_early, lrs]) ## Learining_rate_scheduler helps in controlling your loss\nprint()\nprint('-- Evaluation --')\nbest_transformer.evaluate(x_test, y_test)\nprint()\nhis_2 = his2.history\n\nfig, axs = plt.subplots(1,2, figsize=(16,8))\n\naxs[0].set_title('Loss_Curve')\nep = range(len(his_2['loss']))\naxs[0].plot(ep, his_2['loss'],'o--r',label = 'Training_loss')\naxs[0].plot(ep, his_2['val_loss'],'o--b',label = 'Val_loss')\naxs[0].set_xlabel('epoch')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n\naxs[1].set_title('Acc_Curve')\nep = range(len(his_2['loss']))\naxs[1].plot(ep, his_2['accuracy'],'o--r',label = 'Training_acc')\naxs[1].plot(ep, his_2['val_accuracy'],'o--b',label = 'Val_acc')\naxs[1].set_xlabel('epoch')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\n\nplt.show()","5ccef2ab":"from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\nypred = best_transformer.predict(x_test)\nypred = np.argmax(ypred, axis=-1)\nconf_matrix  = confusion_matrix(ypred, y_test)\n\nprint('--Confusion_Matrix--')\nprint()\nprint(conf_matrix)\nprint()\nprint('f1_score: ',f1_score(y_test, ypred, average='micro'))\nprint('Precsion: ',precision_score(y_test, ypred, average='micro'))\nprint('Recall: ',recall_score(y_test, ypred, average='micro'))","1a0962da":"import time\nfor i in range(50):\n    print('time: ', i)\n    time.sleep(30*60)","da7572c1":"\n## inference\nimport random\n\nindex = {}\nfor val, key in enumerate(word_index.keys()):\n    index[val+1] = key \ninv_word_index = index\ninv_word_index[0] = ''\n\n\ndef reverser(inp):\n    st = ''\n    for i in inp:\n        st += ' ' + str(inv_word_index.get(i))\n    return st\n\ndef predict():\n    seed = random.randint(0, x_test.shape[0])\n    x_seed = x_test[seed]\n    act = y_test[seed]\n    st = reverser(x_seed)\n    print('Input_Sentence: ', st)\n    print('--'*20)\n    print('Actual_emotion: ', inv_label_index.get(act))\n    pred = (np.argmax(best_transformer.predict(x_seed.reshape(1,-1)), axis=-1).tolist())\n    print('--'*20)\n    print('Predicted_emotion: ', inv_label_index.get(pred[-1]))\n\nfor _ in range(5):\n    print('**'*50)\n    predict()\n    print('**'*50)\n    print()","a742f217":"<img style = \"width:100%; height: 80%;\" src = \"https:\/\/s01.sgp1.cdn.digitaloceanspaces.com\/article\/107360-byxhzxvmlb-1544009839.jpg\" \/>\n<body bgcolor = \"black\">\n<img style=\"width:80px; height:80px;float:left;\" src = \"https:\/\/i.pinimg.com\/originals\/90\/c6\/69\/90c6698dc6f9e00bb32ffb3e21042474.gif\" \/><img style=\"width:80px; height:80px;float:right;\" src = \"https:\/\/acegif.com\/wp-content\/gif\/laughing-emoji-3.gif\" \/><center><img style=\"width:auto; height:auto;\" src = \"https:\/\/raw.githubusercontent.com\/AlikMon88\/FillerStack-repo\/ebb777f8f116d4a79e69c67705179b641328ed42\/extra\/transformers.gif\" \/><\/center> \n<\/body>","9b9f35bb":"<img style=\"width:80px; height:80px; float:left;\" src=\"https:\/\/i.pinimg.com\/originals\/0e\/3e\/e5\/0e3ee551876e1ad2a39f89e4adf9168a.gif\"><img style=\"width:80px; height:80px; float:right;\" src=\"https:\/\/i.pinimg.com\/originals\/0e\/3e\/e5\/0e3ee551876e1ad2a39f89e4adf9168a.gif\"><h2 style=\"background-color:black; color:white; text-align:center;\"><b> Model Creation & Training |<i> Basic_RNN <\/i><\/b><\/h2> ","6052e5ff":"<img style=\"width:70px; height:70px; float:left;\" src=\"https:\/\/cliply.co\/wp-content\/uploads\/2021\/03\/392103020_WOW_EMOJI_400px.gif\"><img style=\"width:70px; height:70px; float:right;\" src=\"https:\/\/cliply.co\/wp-content\/uploads\/2021\/03\/392103020_WOW_EMOJI_400px.gif\"><h2 style=\"background-color:black; color:white; text-align:center;\"><b><i> Data Balancing & Preprocessing <\/i><\/b><\/h2> ","9a5ad006":"**Its pretty evident that we have a very limited data, so this will definetely be a bottleneck in the learning process**","88f1051f":"<img style=\"width:80px; height:80px; float:left;\" src=\"https:\/\/www.icegif.com\/wp-content\/uploads\/icegif-5555.gif\"><img style=\"width:80px; height:80px; float:right;\" src=\"https:\/\/www.icegif.com\/wp-content\/uploads\/icegif-5555.gif\"><h2 style=\"background-color:black; color:white; text-align:center;\"><b>Inference |<i>  Basic_RNN <\/i><\/b><\/h2> ","0f04a9b6":"<img style=\"width:80px; height:80px; float:left;\" src=\"https:\/\/i.kym-cdn.com\/photos\/images\/original\/000\/715\/783\/7bf.gif\"><img style=\"width:80px; height:80px; float:right;\" src=\"https:\/\/i.kym-cdn.com\/photos\/images\/original\/000\/715\/783\/7bf.gif\"><h2 style=\"background-color:black; color:white; text-align:center;\"><b>Model Creation & Training |<i>  Transformer <\/i><\/b><\/h2> ","7211d674":"<img style=\"width:90px; height:90px; float:left;\" src=\"https:\/\/appstickers-cdn.appadvice.com\/1414013427\/830617990\/e65c9cff850712ff8fd3ff98c10dfa76-1.gif\"><img style=\"width:90px; height:90px; float:right;\" src=\"https:\/\/appstickers-cdn.appadvice.com\/1414013427\/830617990\/e65c9cff850712ff8fd3ff98c10dfa76-1.gif\"><h2 style=\"background-color:black; color:white; text-align:center;\"><b>Inference |<i>  Transformer <\/i><\/b><\/h2> "}}