{"cell_type":{"13b37fc9":"code","8d74d29d":"code","0f9ebec6":"code","67f20a42":"code","4148d20c":"code","8c280f45":"code","23bac056":"code","738cc26a":"code","ea43352b":"code","faf15711":"code","be57e7fe":"code","215dc5e1":"code","e9dc99dc":"code","4937e7a0":"code","7b4ddb42":"code","dd76b672":"code","aba40b7d":"code","08a58474":"code","695a752f":"code","f7c411f1":"markdown","de6360e5":"markdown","308a2f41":"markdown","4adae39a":"markdown","2b4bca8f":"markdown","492e9f0b":"markdown","087b80ad":"markdown","26fc0cc5":"markdown","c7284586":"markdown","49b7453a":"markdown","1ab281cb":"markdown"},"source":{"13b37fc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d74d29d":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","0f9ebec6":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","67f20a42":"train_features.shape","4148d20c":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=train_features, palette='pastel')\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=train_features, palette='Purples')\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","8c280f45":"plt.figure(figsize=(15,5))\nsns.distplot( train_features['cp_time'], color='red', bins=5)\nplt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","23bac056":"train_features.cp_type.value_counts(normalize=True).plot(kind='pie', figsize=(15, 5), fontsize=12,\n                                                         title='CP Type', autopct='%1.1f%%')\nplt.show()","738cc26a":"train_features.cp_time.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14,\n                                                         title='CP Time', xlabel='Time')\nplt.show()","ea43352b":"train_features.cp_dose.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14,\n                                                         title='CP Dose', xlabel='Dose')\nplt.show()","faf15711":"GENE_COLS = [c for c in train_features.columns if c[:2] == 'g-']\nCELL_COLS = [c for c in train_features.columns if c[:2] == 'c-']\nprint('Number of gene columns:', len(GENE_COLS))\nprint('Number of cell columns:', len(CELL_COLS))","be57e7fe":"ax = train_features.set_index('sig_id') \\\n    .sample(10)[GENE_COLS] \\\n    .T.plot(figsize=(15, 5))\nplt.suptitle('Gene Features for 10 Random Samples', fontsize=20)\nax.get_legend().remove()\nplt.show()","215dc5e1":"tmp_df = train_features.loc[:, ['g-0', 'g-1', 'g-2', 'c-97', 'c-98', 'c-99']]\n\nplt.figure(figsize=(8, 8))\nsns.heatmap(tmp_df.corr(), annot=True)\nplt.show()","e9dc99dc":"SEED = 42\nNFOLDS = 5\n\nnp.random.seed(SEED)","4937e7a0":"# drop id col\nX = train_features.iloc[:,1:].to_numpy()\nX_test = test_features.iloc[:,1:].to_numpy()\ny = train_targets_scored.iloc[:,1:].to_numpy() ","7b4ddb42":"clf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist')))\n               ])\n","dd76b672":"params = {'classify__estimator__colsample_bytree': 0.652231655518253,\n          'classify__estimator__gamma': 3.6975211709521023,\n          'classify__estimator__learning_rate': 0.05033414197773552,\n          'classify__estimator__max_delta_step': 2.070593162427692,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.579959348704868,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8638628715886625,\n          'encode__min_group_size': 0.4160029192647806}\n\nclf.set_params(**params)","aba40b7d":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test_features.shape[0], y.shape[1]))\nkf = KFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ NFOLDS","08a58474":"print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","695a752f":"# create the submission file\nsample_submission.iloc[:,1:] = test_preds\nsample_submission.to_csv('submission.csv', index=False)","f7c411f1":"# Importing Libraries","de6360e5":"Distribution of CP Time ","308a2f41":"# Reading Dataset","4adae39a":"Very Imbalance distribution of cp_type.","2b4bca8f":"# Exploratory Data Analysis","492e9f0b":"# XGBoost baseline - multilabel classification","087b80ad":"# Framing as a binary classification problem","26fc0cc5":"Gene Features","c7284586":"Distribution of CP Dose ","49b7453a":"Check if some features is correlated","1ab281cb":"# Train the model"}}