{"cell_type":{"0fd9fa57":"code","1a380878":"code","de79e8f0":"code","b287029b":"code","69874f16":"code","7c424ad6":"code","215683e3":"code","599f00e2":"code","a63a167c":"code","bd6f4052":"code","0e561216":"code","add6c0c2":"code","55496d50":"code","114847c0":"code","866c3677":"code","5a8df3d4":"code","0828c826":"code","2ca07c70":"code","9bd6556f":"code","0c69fb7a":"code","f63e91b7":"code","39ea084e":"code","077bc82d":"code","95c90dfe":"code","f679353d":"code","4336fd7e":"code","7064c52a":"code","c4e5e374":"code","d1db2774":"code","79bc6363":"code","582ce3f7":"code","e497a125":"code","05500516":"code","a924884b":"code","60bcbe86":"code","d264bfcc":"code","304badc0":"code","925c0a5f":"code","9d9b7b5a":"code","5c26c7a2":"code","7892d98c":"code","080b7bb5":"code","5316b3c7":"code","3d4165ce":"code","5a010e4f":"code","f5b6cefd":"code","d657cffe":"code","01fe680d":"code","2d8d138e":"code","b5d44b6f":"code","7265a2c4":"code","3a43060b":"code","55a97004":"code","4c87bbdd":"code","9b333aeb":"code","7651e228":"code","4efeb246":"code","0961bef2":"code","32a43ae8":"code","f6895cb1":"code","26a966de":"code","a1d05c79":"code","462cd4c1":"code","21b64194":"code","9b11d953":"code","d37a7457":"code","6b1718b9":"code","85685aac":"code","5a10e52f":"code","292742b6":"code","fb20c041":"code","72a70df9":"code","e7c7dc56":"code","417c8174":"code","5f45a48d":"code","70f323cd":"code","e091a04d":"code","2d7167a0":"code","87218d22":"code","a9e7f08e":"code","78abc45c":"code","e064a5bc":"code","dd7216da":"code","f498d3cc":"code","38b283cc":"code","d35db11e":"code","81de9254":"code","2c5483c9":"code","f14ee893":"code","281c9664":"code","f1454248":"code","00f2a234":"code","230a6947":"code","c9cd9569":"code","a1b644ae":"code","8ce5f52a":"code","b556ba77":"code","5bf06392":"code","dc9b6d69":"code","10b613fc":"code","fdb76ffc":"code","da644f4d":"code","aa24f519":"code","3e99a723":"code","d345ce30":"code","2ac4c108":"code","837a42ec":"code","dee11dcd":"code","6bb28ca8":"code","03c82b8f":"code","b1d5e92b":"code","70504f38":"code","9dffd1d9":"code","934f4b78":"code","50dde8ae":"code","6135781f":"code","296d742d":"code","e7020894":"code","58854211":"code","f6d28f7a":"code","39121334":"code","c10ac0b7":"code","b3e98e08":"code","2feeb0c0":"code","b3ae5b58":"code","b1769b00":"code","e70185fa":"code","3b4fb8a7":"code","7468600e":"code","ac3f32e6":"code","2b6f09af":"markdown","7e171c94":"markdown","3341c838":"markdown","9fb59872":"markdown","8a309fed":"markdown","76f9bd9b":"markdown","9f7d52da":"markdown","9ee66ce4":"markdown","ad930d0d":"markdown","6c97db97":"markdown","f64ed413":"markdown","be2dddf8":"markdown","66ae5f8a":"markdown","a44cf6e8":"markdown","06dd7dfa":"markdown","97aa4b80":"markdown","89f8f958":"markdown","3c738e0a":"markdown","79943bd0":"markdown","8c1f1256":"markdown","23a8c12f":"markdown","cd3e6e49":"markdown","02b6c27b":"markdown","4576a8d8":"markdown","0b0849ae":"markdown","6553f28c":"markdown","5b219577":"markdown","99400a02":"markdown","cc712686":"markdown","6fc25509":"markdown","a1f16d16":"markdown","d0a22d84":"markdown","1b0ec407":"markdown","864c6264":"markdown","7340c7f4":"markdown","7138e589":"markdown","3a6797df":"markdown","302ef3eb":"markdown","340f37d1":"markdown","b4384e61":"markdown","64019e81":"markdown","7d3eedb1":"markdown","a694d815":"markdown","be711b5c":"markdown","e6e2435a":"markdown","779d131e":"markdown","12d211a8":"markdown","2fb15535":"markdown","a98e0910":"markdown","192158b8":"markdown","08939496":"markdown","02362dcd":"markdown","d7240498":"markdown","27aba526":"markdown","8c76c428":"markdown","322f5819":"markdown","3e5f1254":"markdown","98b3ba62":"markdown","db74a633":"markdown","98d622e6":"markdown","05003241":"markdown","4b739a4f":"markdown","685d0d9f":"markdown","148f6b3c":"markdown","3c69218c":"markdown","47d4855c":"markdown","a843afcf":"markdown","3c6aadd5":"markdown","1d595b40":"markdown","aac40978":"markdown","f2f375cc":"markdown","5c6529b2":"markdown","0d0cc421":"markdown","8065045d":"markdown","35c28b10":"markdown","644d26e9":"markdown","90e12064":"markdown","9af14333":"markdown","a177aa1e":"markdown","14bf2fd7":"markdown","bfca7a68":"markdown","e9a63195":"markdown","d2803423":"markdown","647ae409":"markdown","1e2b4698":"markdown","400211bc":"markdown","036bb553":"markdown","9451f032":"markdown","2ca86f30":"markdown","17b73664":"markdown","de353e50":"markdown","316c338f":"markdown","bb124f72":"markdown","3492559c":"markdown","c3f0cbfa":"markdown","2d46099a":"markdown","175c05c2":"markdown","86e553e4":"markdown","cadf8b79":"markdown","343163e6":"markdown"},"source":{"0fd9fa57":"import warnings\nwarnings.filterwarnings('ignore')","1a380878":"# Importing python libraries :\n\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# import machine learning and stats libraries:\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc \nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, RepeatedKFold \n\n# For hyper-parameter tuning\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Logistic Regression\nfrom sklearn.linear_model import Ridge, Lasso, LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom xgboost.sklearn import XGBClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# SVM model\nfrom sklearn.svm import SVC\n\n# Classification algo metrics\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve, accuracy_score, recall_score\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n  \n#install scikit-optimize\n#!pip install scikit-optimize\nfrom skopt import BayesSearchCV\n\n# install imblearn\n#!conda install -c conda-forge imbalanced-learn\n#!pip install imbalanced-learn==0.6.2\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","de79e8f0":"# To display all the columns\npd.options.display.max_columns = None\n# To display all the rows\npd.options.display.max_rows = None\n#setting the maximum column width \npd.set_option('display.max_colwidth', 80)\n# change the cell width\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","b287029b":"pip list","69874f16":"# df = pd.read_csv('creditcard.csv') # importing the dataset from local file path. Please change the path accordingly.\n# df.head(5) #checking the top 5 rows","7c424ad6":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv') # importing the dataset from kaggle.\ndf.head(5) #checking the top 5 rows","215683e3":"#checking the bottom 5 rows\ndf.tail(5)","599f00e2":"# Data dimension\nprint(\"Data dimension     :\",df.shape)\nprint(\"Data size          :\",df.size)\nprint(\"Number of Row          :\",len(df.index))\nprint(\"Number of Columns      :\",len(df.columns))","a63a167c":"#info about the column types etc. \ndf.info(verbose = True)","bd6f4052":"#observe the different feature type present in the data\ndf.dtypes","0e561216":"#checking numerical columns statistics\ndf.describe()","add6c0c2":"#check if any null values\ndf.isnull().sum()","55496d50":"# Boxplot to understand the distribution of numerical attributes :\n\n# Selecting only numerical feature from the dataframe:\nnumeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n\n# Excluding BINARY target feature(Class) and Time variable as its not needed for transformation and Amount variable as its covered in previous plot: \nli_not_plot = ['Class', 'Time', 'Amount']\nli_transform_num_feats = [c for c in list(numeric_features) if c not in li_not_plot]\n\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(figsize=(22,34))\n# Using log scale:\n#ax.set_xscale(\"log\")\nax = sns.boxplot(data=df[li_transform_num_feats] , orient=\"h\", palette=\"Paired\")\nax.set(ylabel=\"Features\")\nax.set(xlabel=\"Values\")\nax.set(title=\"Distribution of numerical atributes\")\nsns.despine(trim=True,left=True)","114847c0":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100\n\nprint(normal_share)\nprint(fraud_share)","866c3677":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\nplt.figure(figsize=(15,6))\n\nplt.subplot(121)\nplt.title('Fraudulent BarPlot-Class Histogram', fontweight='bold',fontsize=14)\ncount_of_classes = pd.value_counts(df['Class'], sort = True).sort_index()\nax = count_of_classes.plot(kind = 'bar')\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.xticks([0,1],[\"Genuine\",\"Fraudulent\"])\n\ntotal = float(len(df))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height*100\/total),\n            ha=\"center\") \n\n\nplt.subplot(122)\nlabels = 'Genuine', 'Fraudalent'\ndf[\"Class\"].value_counts().plot.pie(autopct = \"%1.2f%%\", labels=labels, startangle=90)\nplt.show()","5a8df3d4":"# Time Distribution plot for transactions \nplt.figure(figsize=(15,7))\n\nplt.title('Distribution of Transaction Time')\nsns.distplot(df['Time'].values\/(60*60))","0828c826":"# Create a scatter plot to observe the distribution of classes with time\n\n#As time is given in relative fashion, we will need to use pandas.Timedelta which Represents a duration, the difference between two dates or times.\n\nDelta_Time = pd.to_timedelta(df['Time'], unit='s')\n#Create derived columns Mins and hours\ndf['Time_Day'] = (Delta_Time.dt.components.days).astype(int)\ndf['Time_Hour'] = (Delta_Time.dt.components.hours).astype(int)\ndf['Time_Min'] = (Delta_Time.dt.components.minutes).astype(int)","2ca07c70":"# Bivariate Analysis: Create a scatter plot to observe the distribution of classes with time\n\nfig = plt.figure(figsize=(14, 18))\ncmap = sns.color_palette('Set2')\n\n# PLot the relation between the variables:\n\nplt.subplot(3,1,1)\nsns.scatterplot(x=df['Time'], y='Class', palette=cmap, data=df)\nplt.xlabel('Time', size=18)\nplt.ylabel('Class', size=18)\nplt.tick_params(axis='x', labelsize=16)\nplt.tick_params(axis='y', labelsize=16) \nplt.title('Time vs Class Distribution', size=20, y=1.05)","9bd6556f":"# Storing Fraud and non-Fraud transactions \n\ndf_nonfraud = df[df.Class == 0]\ndf_fraud = df[df.Class == 1]","0c69fb7a":"#Scatter plot between Time and Amount\n\nfig = plt.figure(figsize = (8,8))\nplt.scatter(df_nonfraud.Amount, df_nonfraud.Time.values\/(60*60),alpha=0.5,label='Non Fraud')\nplt.scatter(df_fraud.Amount, df_fraud.Time.values\/(60*60),alpha=1,label='Fraud')\nplt.xlabel('Amount')\nplt.ylabel('Time')\nplt.title('Scatter plot between Amount and Time ')\nplt.legend()\nplt.show()","f63e91b7":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 50\n\nax1.hist(df.Time[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Time[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()","39ea084e":"# Plot of transactions in 48 hours\n\nbins = np.linspace(0, 48, 48)\nplt.hist((df_nonfraud.Time\/(60*60)), bins, alpha=1,label='Non-Fraud')\nplt.hist((df_fraud.Time\/(60*60)), bins, alpha=0.6,label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Percentage of transactions by hour\")\nplt.xlabel(\"Transaction time from first transaction in the dataset (hours)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","077bc82d":"# Box Plot of amount for both classes\nplt.figure(figsize = (15, 6))\nplt.subplot(121)\na = sns.boxplot(x = 'Class', y = 'Amount',hue='Class', data = df, showfliers=False) \nplt.setp(a.get_xticklabels(), rotation=45)\n\nplt.subplot(122)\n# KDE plot to visualize the distribution of Amount for both the classes\nplt.rcParams['figure.figsize'] = [10,6]\nsns.kdeplot(df.loc[df['Class'] == 0, 'Amount'], label = 'Non Fraud')\nsns.kdeplot(df.loc[df['Class'] == 1, 'Amount'], label = 'Fraud')\nplt.title('Distribution of Amount by Target Value')\nplt.xlabel('Amount')\nplt.ylabel('Density')","95c90dfe":"# Create a scatter plot to observe the distribution of classes with Amount\n#To clearly the data of frauds and no frauds\ndf_Fraud = df[df['Class'] == 1]\ndf_Regular = df[df['Class'] == 0]\n\n# Fraud Transaction Amount Statistics\nprint(df_Fraud[\"Amount\"].describe())","f679353d":"# Regular Transaction Amount Statistics\nprint(df_Regular[\"Amount\"].describe())","4336fd7e":"# Create a scatter plot to observe the distribution of classes with Amount\n\n# Bivariate Analysis: Create a scatter plot to observe the distribution of classes with Amount\n\n\nfig = plt.figure(figsize=(14, 18))\ncmap = sns.color_palette('Set1')\n\n# PLot the relation between the variables:\n\nplt.subplot(3,1,1)\nsns.scatterplot(x=df['Amount'], y='Class', palette=cmap, data=df)\nplt.xlabel('Amount', size=18)\nplt.ylabel('Class', size=18)\nplt.tick_params(axis='x', labelsize=16)\nplt.tick_params(axis='y', labelsize=16) \nplt.title('Amount vs Class Distribution', size=20, y=1.05)","7064c52a":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 30\n\nax1.hist(df.Amount[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Amount[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","c4e5e374":"# Let's try to understand the Amount variable as it is not PCA transformed variable :\n\nplt.figure(figsize=(24, 12))\n\nplt.subplot(2,2,1)\nplt.title('Amount Distribution')\ndf['Amount'].astype(int).plot.hist();\nplt.xlabel(\"variable Amount\")\n#plt.ylabel(\"Frequency\")\n\nplt.subplot(2,2,2)\nplt.title('Amount Distribution')\nsns.set()\nplt.xlabel(\"variable Amount\")\nplt.hist(df['Amount'],bins=100)\nplt.show()","d1db2774":"# Plot of high value transactions($200-$2000)\n\nbins = np.linspace(200, 2000, 100)\nplt.hist(df_nonfraud.Amount, bins, alpha=1, density=True, label='Non-Fraud')\nplt.hist(df_fraud.Amount, bins, alpha=1, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200-$2000)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","79bc6363":"plt.figure(figsize=(24, 12))\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()","582ce3f7":"# Understanding more on the correlation in data:\nprint(\"Most important features relative to target variable Class\")\n\ncorr_initial = df.corr()['Class']\n# convert series to dataframe so it can be sorted\ncorr_initial = pd.DataFrame(corr_initial)\n# correct column label from SalePrice to correlation\ncorr_initial.columns = [\"Correlation\"]\n# sort correlation\ncorr_initial2 = corr_initial.sort_values(by=['Correlation'], ascending=False)\ncorr_initial2.head(10)","e497a125":"# Lets plot the heatmap again for relatively strong correlation (i.e. >0.09) with the target variable:\n\ntop_feature = corr_initial2.index[abs(df.corr()['Class']>0.09)]\nplt.subplots(figsize=(8, 5))\ntop_corr = df[top_feature].corr()\nsns.heatmap(top_corr, annot=True, cmap=\"YlGnBu\")\nplt.show()","05500516":"#Check the fraud\/Non_Fraud related records\ndf['Class'].value_counts()","a924884b":"#Check the percentage of fraud\/Non_Fraud related records\ndf['Class'].value_counts(normalize=True)*100","60bcbe86":"print('The percentage without churn prediction is ', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('The percentage with churn prediction is ', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\nprint('The ratio of imbalance is', round(df['Class'].value_counts()[1]\/df['Class'].value_counts()[0] * 100,2))","d264bfcc":"#The fraus Vs normal trasaction by day\nplt.figure(figsize=(5,5))\nsns.distplot(df[df['Class'] == 0][\"Time_Day\"], color='green')\nsns.distplot(df[df['Class'] == 1][\"Time_Day\"], color='red')\nplt.title('Fraud Vs Normal Transactions by Day', fontsize=17)\nplt.show()","304badc0":"# The fraud Vs normal trasaction by hour\nplt.figure(figsize=(15,5))\nsns.distplot(df[df['Class'] == 0][\"Time_Hour\"], color='green')\nsns.distplot(df[df['Class'] == 1][\"Time_Hour\"], color='red')\nplt.title('Fraud Vs Normal Transactions by Hour', fontsize=17)\nplt.show()","925c0a5f":"# Drop unnecessary columns\n# As we have derived the Day\/Hour\/Minutes from the time column we will drop Time\ndf.drop('Time', axis = 1, inplace= True)\n#also day\/minutes might not be very useful as this is not time series data, we will keep only derived column hour\ndf.drop(['Time_Day', 'Time_Min'], axis = 1, inplace= True)","9d9b7b5a":"df.info()","5c26c7a2":"# Create X and y dataset for independent and dependent data\ny = df['Class']\nX = df.drop(['Class'], axis=1)\nX.head()","7892d98c":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.20)","080b7bb5":"print(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","5316b3c7":"# plot the histogram of a variable from the dataset to see the skewness\ncols = list(df.columns.values)\ncols.remove('Class')\nnormal_records = df.Class == 0\nfraud_records = df.Class == 1\n\nplt.figure(figsize=(20, 60))\nfor n, col in enumerate(cols):\n    plt.subplot(10,3,n+1)\n    sns.distplot(df[col][normal_records], color='green')\n    sns.distplot(df[col][fraud_records], color='red')\n    plt.title(col, fontsize=17)\nplt.show()","3d4165ce":"# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\npt = PowerTransformer()\npt.fit(X_train)                       ## Fit the PT on training data\nX_train_pt = pt.transform(X_train)    ## Then apply on all data\nX_test_pt = pt.transform(X_test)","5a010e4f":"# plot the histogram of a variable from the dataset again to see the result \ncols = list(df.columns.values)\ncols.remove('Class')\nnormal_records = df.Class == 0\nfraud_records = df.Class == 1\n\nplt.figure(figsize=(20, 60))\nfor n, col in enumerate(cols):\n    plt.subplot(10,3,n+1)\n    sns.distplot(df[col][normal_records], color='green')\n    sns.distplot(df[col][fraud_records], color='red')\n    plt.title(col, fontsize=17)\nplt.show()","f5b6cefd":"# Create a dataframe to store results\ndf_Results = pd.DataFrame(columns=['Model','Data_Imbalance_Handling','Accuracy on Test Dataset','Recall on Test Dataset','roc_value on Test Dataset','threshold'])","d657cffe":"# function to view model results - confusion matrix and classification report\ndef plot_model_results(y_test, y_pred_model):\n    fig = plt.figure(figsize=(18, 7))\n    ax1 = fig.add_subplot(121)\n    \n    # Plot Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred_model)\n    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG', linewidth=0.3,\n    xticklabels = [\"Non-Fraudalent\", \"Fraudalent\"] ,\n    yticklabels = [\"Non-Fraudalent\", \"Fraudalent\"] )\n    plt.title('Confusion Matrix of Test Dataset' , fontweight =\"bold\")\n    plt.ylabel('True labels')\n    plt.xlabel('Predicted labels')\n    \n    # Print classification Report\n    ax2 = fig.add_subplot(122)\n    # convert a classification report to a pandas Dataframe is by simply having the report returned as a dict\n    report = classification_report(y_test, y_pred_model, output_dict=True)\n    df = pd.DataFrame(report).transpose()\n    ax2.set_axis_off() \n    font_size=12\n    class_report_table = ax2.table(cellText = df.values, rowLabels = df.index, colLabels = df.columns, rowColours =[\"palegreen\"] * 10, colColours =[\"palegreen\"] * 10,  loc ='right')         \n    class_report_table.auto_set_font_size(False)\n    class_report_table.set_fontsize(font_size)\n    class_report_table.auto_set_column_width(col=list(range(len(df.columns)))) # Provide integer list of columns to adjust\n    ax2.set_title('Classification Report on Test Dataset', fontweight =\"bold\", loc=\"right\")    \n    plt.show()  ","01fe680d":"# function to calculate and view model evaluation metrics - ROC curve and Evaluation metrics such as Area under curve score, accuracy and recall\ndef plot_evaluation_metrics(y_test_model, y_pred_model, y_pred_prob, DataImbalance, model):\n    accuracy = round(accuracy_score(y_pred=y_pred_model, y_true=y_test_model),4)\n    recall = round(recall_score(y_pred_model, y_test_model),4)\n    roc_value = round(roc_auc_score(y_test_model, y_pred_prob),4)\n    fpr, tpr, thresholds = roc_curve(y_test_model, y_pred_prob)\n    threshold = round(thresholds[np.argmax(tpr-fpr)],4)\n    roc_auc = auc(fpr, tpr)\n    \n    fig = plt.figure(figsize=(18, 7))\n    # AUC-ROC Curve \n    ax1 = fig.add_subplot(121)\n    plt.plot(fpr, tpr, label='ROC curve for test dataset(area = %0.2f)' % roc_auc )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic', fontweight =\"bold\")\n    plt.legend(loc=\"lower right\")\n     \n    # Print Evaluation metrics\n    ax2 = fig.add_subplot(122)\n    ax2.axis('off')\n    font_size=12\n    \n    eval_df = pd.DataFrame({'Data_Imbalance_Handling': DataImbalance,'Model': model,'Accuracy on Test Dataset': accuracy, 'Recall on Test Dataset':recall, 'roc_value on Test Dataset': roc_value,'threshold': threshold}, index=[0])\n    df = eval_df.transpose()\n    ax2.axis('tight')\n    metric_table = ax2.table(cellText = df.values, rowLabels = df.index, colLabels = df.columns, rowColours =[\"palegreen\"] * 10, colColours =[\"palegreen\"] * 10,  loc ='right')         \n    metric_table.auto_set_column_width(col=list(range(len(eval_df.columns)))) # Provide integer list of columns to adjust\n    metric_table.auto_set_font_size(False)\n    metric_table.set_fontsize(font_size)\n    ax2.set_title('Evaluation Metrics on Test Dataset', fontweight =\"bold\", loc=\"right\")    \n    \n    plt.show()  \n    \n    return(eval_df)","2d8d138e":"# function to build Logistic regression model\ndef buildAndRunLogisticModels(df_Results, DataImbalance, X_train,y_train, X_test, y_test ): \n    start_time = time.time()\n    num_C = list(np.power(10.0, np.arange(-10, 10)))\n    cv_num = KFold(n_splits=10, shuffle=True, random_state=42)\n    \n    searchCV_l2 = LogisticRegressionCV(Cs= num_C, penalty='l2', scoring='roc_auc', cv=cv_num, random_state=42, max_iter=10000, fit_intercept=True, solver='newton-cg', tol=10)\n    searchCV_l2.fit(X_train, y_train)\n    print(\"Parameters for L2 regularisations: \")\n    print(searchCV_l2.coef_)\n    print(searchCV_l2.intercept_)  \n    # find predicted values\n    y_pred_l2 = searchCV_l2.predict(X_test)\n    # Find predicted probabilities\n    y_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:,1] \n    plot_model_results(y_test, y_pred_l2) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred_l2, y_pred_probs_l2, DataImbalance, \"Logistic Regression with L2 Regularization\") # Plot ROC curve and Evaluation metrics\n    df_Results = pd.concat([df_Results, eval_df])\n    l2_time = time.time()\n    print(\"Time Taken by Logistic Regression\/L2 RegModel: --- %s seconds ---\" % (l2_time - start_time))\n\n    searchCV_l1 = LogisticRegressionCV(Cs=num_C, penalty='l1', scoring='roc_auc', cv=cv_num, random_state=42, max_iter=10000, fit_intercept=True, solver='liblinear', tol=10)\n    searchCV_l1.fit(X_train, y_train)\n    print(\"Parameters for l1 regularisations: \")\n    print(searchCV_l1.coef_)\n    print(searchCV_l1.intercept_) \n    # find predicted values using l1 reg model\n    y_pred_l1 = searchCV_l1.predict(X_test)\n    # Find predicted probabilities\n    y_pred_probs_l1 = searchCV_l1.predict_proba(X_test)[:,1] \n        \n    plot_model_results(y_test, y_pred_l1) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred_l1, y_pred_probs_l1, DataImbalance, \"Logistic Regression with L1 Regularization\")\n    df_Results = pd.concat([df_Results, eval_df])\n    l1_time = time.time()\n    print(\"Time Taken by Logistic Regression\/L1 RegModel: --- %s seconds ---\" % (l1_time - start_time))\n\n    return df_Results","b5d44b6f":"# function to build KNN model\ndef buildAndRunKNNModels(df_Results,DataImbalance, X_train,y_train, X_test, y_test ):\n    #create KNN model and fit the model with train dataset\n    start_time = time.time()\n    knn = KNeighborsClassifier(n_neighbors = 5,n_jobs=16)\n    knn.fit(X_train,y_train)\n\n    # Predict on test dataset\n    y_pred = knn.predict(X_test)\n    knn_probs = knn.predict_proba(X_test)[:, 1]\n\n    plot_model_results(y_test, y_pred) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred, knn_probs, DataImbalance, \"KNN\") # Plot ROC curve and Evaluation metrics\n    df_Results = pd.concat([df_Results, eval_df])\n    \n    print(\"Time Taken by KNN Model: --- %s seconds ---\" % (time.time() - start_time))\n    return df_Results","7265a2c4":"# function to build SVM model\ndef buildAndRunSVMModels(df_Results, DataImbalance, X_train,y_train, X_test, y_test ):\n    #Evaluate SVM model with sigmoid kernel  model\n    start_time = time.time()\n    clf = SVC(kernel='sigmoid', random_state=42)\n    clf.fit(X_train,y_train)\n    y_pred_SVM = clf.predict(X_test)\n\n    # Run classifier\n    classifier = SVC(kernel='sigmoid' , probability=True)\n    svm_probs = classifier.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n\n    plot_model_results(y_test, y_pred_SVM) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred_SVM, svm_probs, DataImbalance, \"SVM\") # Plot ROC curve and Evaluation metrics\n    df_Results = pd.concat([df_Results, eval_df])\n    \n    print(\"Time Taken by SVM Model: --- %s seconds ---\" % (time.time() - start_time))\n    return df_Results","3a43060b":"# function to build Decision Tree model\ndef buildAndRunTreeModels(df_Results, DataImbalance, X_train,y_train, X_test, y_test ):\n    # Evaluate Decision Tree model with 'gini' & 'entropy'\n    criteria = ['gini', 'entropy'] \n    scores = {} \n\n    for c in criteria: \n        start_time = time.time()\n        dt = DecisionTreeClassifier(criterion = c, random_state=42) \n        dt.fit(X_train, y_train) \n        y_pred = dt.predict(X_test)\n        test_score = dt.score(X_test, y_test) \n        tree_preds = dt.predict_proba(X_test)[:, 1]\n        \n        plot_model_results(y_test, y_pred) # plot confusion matrix and classification report\n        eval_df = plot_evaluation_metrics(y_test, y_pred, tree_preds, DataImbalance, \"Tree model with {0} criteria\".format(c)) # Plot ROC curve and Evaluation metrics\n        df_Results = pd.concat([df_Results, eval_df])\n        \n        print(\"Time Taken by Tree Model with %s criteria: --- %s seconds ---\" % (c, time.time() - start_time))\n    return df_Results","55a97004":"# function to build Random Forest model\ndef buildAndRunRandomForestModels(df_Results, DataImbalance, X_train,y_train, X_test, y_test ):\n    start_time = time.time()\n    # Create the model with 100 trees\n    RF_model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=42)\n    # Fit on training data\n    RF_model.fit(X_train, y_train)\n    y_pred = RF_model.predict(X_test)\n\n    # Actual class predictions\n    rf_predictions = RF_model.predict(X_test)\n\n    # Probabilities for each class\n    rf_probs = RF_model.predict_proba(X_test)[:, 1]\n\n    plot_model_results(y_test, y_pred) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred, rf_probs, DataImbalance, \"Random Forest\") # Plot ROC curve and Evaluation metrics\n    df_Results = pd.concat([df_Results, eval_df])\n    \n    print(\"Time Taken by Random Forest Model: --- %s seconds ---\" % (time.time() - start_time))\n    return df_Results","4c87bbdd":"# function to build XGBoost model\ndef buildAndRunXGBoostModels(df_Results, DataImbalance,X_train,y_train, X_test, y_test ):\n    start_time = time.time()\n    # fit model no training data\n    XGBmodel = XGBClassifier(random_state=42)\n    XGBmodel.fit(X_train, y_train)\n    y_pred = XGBmodel.predict(X_test)\n\n    # Probabilities for each class\n    XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n\n    plot_model_results(y_test, y_pred) # plot confusion matrix and classification report\n    eval_df = plot_evaluation_metrics(y_test, y_pred, XGB_probs, DataImbalance, \"XGBoost\") # Plot ROC curve and Evaluation metrics\n    df_Results = pd.concat([df_Results, eval_df])\n    \n    print(\"Time Taken by XGBoost Model: --- %s seconds ---\" % (time.time() - start_time))\n    return df_Results","9b333aeb":"# Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","7651e228":"# Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","4efeb246":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","0961bef2":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","32a43ae8":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","f6895cb1":"#Run SVM Model with Sigmoid Kernel\nprint(\"SVM Model with Sigmoid Kernel\")\ndf_Results = buildAndRunSVMModels(df_Results,\"Power Transformer\",X_train_pt,y_train, X_test_pt, y_test)","26a966de":"df_Results","a1d05c79":"# Lets perfrom RepeatedKFold and check the results\nrkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n# X is the feature set and y is the target\nfor train_index, test_index in rkf.split(X,y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]","462cd4c1":"#Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results,\"RepeatedKFold Cross Validation\", X_train_cv,y_train_cv, X_test_cv, y_test_cv)","21b64194":"#Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)","9b11d953":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)","d37a7457":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)","6b1718b9":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)","85685aac":"#Run SVM Model with Sigmoid Kernel\nprint(\"SVM Model with Sigmoid Kernel\")\ndf_Results = buildAndRunSVMModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)","5a10e52f":"df_Results","292742b6":"# Evaluate XGboost model\n\n# fit model no training data\nXGBmodel = XGBClassifier(random_state=42)\nXGBmodel.fit(X_train_cv,y_train_cv)\n\ncoefficients = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(XGBmodel.feature_importances_))], axis = 1)\ncoefficients.columns = ['Feature','feature_importances']","fb20c041":"plt.figure(figsize=(20,5)) # to be deleted\nsns.barplot(x='Feature', y='feature_importances', data=coefficients)\nplt.title(\"XGBoost with Repeated KFold Feature Importance\", fontsize=18)\nplt.show()","72a70df9":"# Lets perfrom StratifiedKFold and check the results\nskf = StratifiedKFold(n_splits=5, random_state=None)\n# X is the feature set and y is the target\nfor train_index, test_index in skf.split(X,y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_SKF_cv, X_test_SKF_cv = X.iloc[train_index], X.iloc[test_index]\n    y_train_SKF_cv, y_test_SKF_cv = y.iloc[train_index], y.iloc[test_index]","e7c7dc56":"#Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results,\"StratifiedKFold Cross Validation\", X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","417c8174":"#Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","5f45a48d":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","70f323cd":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","e091a04d":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","2d7167a0":"#Run SVM Model with Sigmoid Kernel\nprint(\"SVM Model with Sigmoid Kernel\")\ndf_Results = buildAndRunSVMModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)","87218d22":"df_Results","a9e7f08e":"# Logistic Regression\n\nnum_C = list(np.power(10.0, np.arange(-10, 10)))\ncv_num = KFold(n_splits=10, shuffle=True, random_state=42)\n\nsearchCV_l2 = LogisticRegressionCV(\n          Cs= num_C\n          ,penalty='l2'\n          ,scoring='roc_auc'\n          ,cv=cv_num\n          ,random_state=42\n          ,max_iter=10000\n          ,fit_intercept=True\n          ,solver='newton-cg'\n          ,tol=10\n      )\n\nsearchCV_l2.fit(X_train, y_train)\nprint ('Max auc_roc for l2:', searchCV_l2.scores_[1].mean(axis=0).max())\n\nprint(\"Parameters for l2 regularisations\")\nprint(searchCV_l2.coef_)\nprint(searchCV_l2.intercept_) \nprint(searchCV_l2.scores_) \n\n#find predicted vallues\ny_pred_l2 = searchCV_l2.predict(X_test)\n\n#Find predicted probabilities\ny_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:,1] \n\n# Accuracy of L2\/L1 models\nAccuracy_l2 = accuracy_score(y_pred=y_pred_l2, y_true=y_test)\n\nprint(\"Accuracy of Logistic model with l2 regularisation : {0}\".format(Accuracy_l2))\n\nl2_roc_value = roc_auc_score(y_test, y_pred_probs_l2)\nprint(\"l2 roc_value: {0}\" .format(l2_roc_value))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs_l2)\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint(\"l2 threshold: {0}\".format(threshold))","78abc45c":"searchCV_l2.coef_","e064a5bc":"coefficients = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(searchCV_l2.coef_))], axis = 1)\ncoefficients.columns = ['Feature','Importance Coefficient']","dd7216da":"coefficients","f498d3cc":"plt.figure(figsize=(20,5))\nsns.barplot(x='Feature', y='Importance Coefficient', data=coefficients)\nplt.title(\"Logistic Regression with L2 Regularisation Feature Importance\", fontsize=18)\nplt.show()","38b283cc":"# Undersampling\nRUS = RandomUnderSampler(sampling_strategy=0.5)\n# fit and apply the transform\nX_Under, y_Under = RUS.fit_resample(X_train, y_train)\n#Create Dataframe\nX_Under = pd.DataFrame(data=X_Under,   columns=cols)","d35db11e":"print(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results,\"Random Undersampling\", X_Under, y_Under , X_test, y_test)","81de9254":"# Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results,\"Random Undersampling\",X_Under, y_Under , X_test, y_test)","2c5483c9":"# Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)","f14ee893":"# Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)","281c9664":"# Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)","f1454248":"#Run SVM Model with Sigmoid Kernel\nprint(\"SVM Model with Sigmoid Kernel\")\ndf_Results = buildAndRunSVMModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)","00f2a234":"df_Results","230a6947":"skf = StratifiedKFold(n_splits=5, random_state=None)\n\nfor fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n    X_train = X.loc[train_index]\n    y_train = y.loc[train_index] \n    X_test = X.loc[test_index]\n    y_test = y.loc[test_index]  \n    ROS = RandomOverSampler(sampling_strategy=0.5)\n    X_over, y_over= ROS.fit_resample(X_train, y_train)\n\nX_over = pd.DataFrame(data=X_over,   columns=cols) # Create Dataframe for X_over","c9cd9569":"Data_Imbalance_Handling = \"Random Oversampling with StratifiedKFold CV\"\n#Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results , Data_Imbalance_Handling , X_over, y_over, X_test, y_test)","a1b644ae":"#Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results , Data_Imbalance_Handling,X_over, y_over, X_test, y_test)","8ce5f52a":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with 'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results , Data_Imbalance_Handling,X_over, y_over, X_test, y_test)","b556ba77":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results , Data_Imbalance_Handling,X_over, y_over, X_test, y_test)","5bf06392":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results , Data_Imbalance_Handling,X_over, y_over, X_test, y_test)","dc9b6d69":"# # Run SVM Model with Sigmoid Kernel\n# print(\"SVM Model with Sigmoid Kernel\")\n# start_time = time.time()\n# df_Results = buildAndRunSVMModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n# print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))","10b613fc":"df_Results","fdb76ffc":"SMOTE_over = SMOTE(random_state=0)\n\nX_train_Smote, y_train_Smote= SMOTE_over.fit_resample(X_train, y_train) \n\n#Create dataframe\n#X_train_Smote = pd.DataFrame(data=X_train_Smote,   columns=cols)\n# Artificial minority samples and corresponding minority labels from SMOTE are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from SMOTE, we do\nX_train_smote_1 = X_train_Smote[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nn = X_train_1.shape[0]\nplt.scatter(X_train_smote_1.iloc[:n, 0], X_train_smote_1.iloc[:n, 1], label='Artificial SMOTE Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:n, 0], X_train_0[:n, 1], label='Actual Class-0 Examples')\nplt.legend()\n\n#Create dataframe\nX_train_Smote = pd.DataFrame(data=X_train_Smote,   columns=cols)","da644f4d":"skf = StratifiedKFold(n_splits=5, random_state=None)\n\nfor fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n    X_train = X.loc[train_index]\n    y_train = y.loc[train_index] \n    X_test = X.loc[test_index]\n    y_test = y.loc[test_index]  \n    SMOTE_skf = SMOTE(random_state=0)\n    X_train_Smote, y_train_Smote= SMOTE_skf.fit_resample(X_train, y_train)\n\nX_train_Smote = pd.DataFrame(data=X_train_Smote, columns=cols) # Create Dataframe for X_over","aa24f519":"Data_Imbalance_Handling = \"SMOTE Oversampling with StratifiedKFold CV \"\n#Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)","3e99a723":"# #Run KNN Model - takes long time; uncomment to train this model\n# print(\"KNN Model\")\n# df_Results = buildAndRunKNNModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)","d345ce30":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)","2ac4c108":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)","837a42ec":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)","dee11dcd":"# # Run SVM Model with Sigmoid Kernel\n# print(\"SVM Model with Sigmoid Kernel\")\n# start_time = time.time()\n# df_Results = buildAndRunSVMModels(df_Results, Data_Imbalance_Handling, X_train_Smote, y_train_Smote , X_test, y_test)\n# print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))","6bb28ca8":"df_Results","03c82b8f":"ADASYN_over = ADASYN(random_state=0)\nX_train_ADASYN, y_train_ADASYN = ADASYN_over.fit_resample(X_train, y_train) \n\n# Artificial minority samples and corresponding minority labels from ADASYN are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from ADASYN, we do\nX_train_adasyn_1 = X_train_ADASYN[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_adasyn_1.iloc[:X_train_1.shape[0], 0], X_train_adasyn_1.iloc[:X_train_1.shape[0], 1],\n            label='Artificial ADASYN Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()","b1d5e92b":"skf = StratifiedKFold(n_splits=5, random_state=None)\n\nfor fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n    X_train = X.loc[train_index]\n    y_train = y.loc[train_index] \n    X_test = X.loc[test_index]\n    y_test = y.loc[test_index]  \n    SMOTE_adasyn = SMOTE(random_state=0)\n    X_train_ADASYN, y_train_ADASYN= ADASYN_over.fit_resample(X_train, y_train)\n\nX_train_ADASYN = pd.DataFrame(data=X_train_ADASYN, columns=cols) # Create Dataframe for X_over","70504f38":"Data_Imbalance_Handling = \"ADASYN Oversampling with StratifiedKFold CV \"\n#Run Logistic Regression with L1 And L2 Regularisation\nprint(\"Logistic Regression with L1 And L2 Regularisation\")\ndf_Results = buildAndRunLogisticModels(df_Results, Data_Imbalance_Handling, X_train_ADASYN, y_train_ADASYN , X_test, y_test)","9dffd1d9":"#Run KNN Model\nprint(\"KNN Model\")\ndf_Results = buildAndRunKNNModels(df_Results, Data_Imbalance_Handling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)","934f4b78":"#Run Decision Tree Models with  'gini' & 'entropy' criteria\nprint(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\ndf_Results = buildAndRunTreeModels(df_Results, Data_Imbalance_Handling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)","50dde8ae":"#Run Random Forest Model\nprint(\"Random Forest Model\")\ndf_Results = buildAndRunRandomForestModels(df_Results, Data_Imbalance_Handling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)","6135781f":"#Run XGBoost Model\nprint(\"XGBoost Model\")\ndf_Results = buildAndRunXGBoostModels(df_Results, Data_Imbalance_Handling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)","296d742d":"# #Run SVM Model with Sigmoid Kernel\n# print(\"SVM Model with Sigmoid Kernel\")\n# start_time = time.time()\n# df_Results = buildAndRunSVMModels(df_Results, Data_Imbalance_Handling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n# print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))","e7020894":"df_Results","58854211":"df_Results.sort_values(['roc_value on Test Dataset', 'Recall on Test Dataset'], ascending=[False, False])","f6d28f7a":"# Run Base XGBoost Model before tuning hyperparameters\nprint(\"Base XGBoost Model before tuning hyperparameters\")\nXGBmodel = XGBClassifier(random_state=42)\nXGBmodel.fit(X_over, y_over)\ny_pred = XGBmodel.predict(X_test)\n\n# Probabilities for each class\nXGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n\nplot_evaluation_metrics(y_test, y_pred, XGB_probs, \"Random OverSampling\", \"XGBoost\")","39121334":"# # fit model on training data\n# XGBmodel = XGBClassifier(random_state=42)\n\n# #Lets tune XGBoost Model for max_depth and min_child_weight\n\n# param_test = {'max_depth':range(3,10,2), 'min_child_weight':range(1,6,2)}\n# gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n#               learning_rate=0.1, max_delta_step=0, max_depth=3,\n#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n#               nthread=None, objective='binary:logistic', random_state=42, \n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n#               silent=None, subsample=1, verbosity=1), param_grid = param_test, scoring='roc_auc',n_jobs=4,cv=5)\n# gsearch1.fit(X_over, y_over)\n# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","c10ac0b7":"# param_test = {'n_estimators':range(60,150,20)}\n# gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n#               learning_rate=0.1, max_delta_step=0, max_depth=9,\n#               min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n#               nthread=None, objective='binary:logistic', random_state=42,\n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n#               silent=None, subsample=1, verbosity=1), param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n# gsearch1.fit(X_over, y_over)\n# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","b3e98e08":"# param_test = { 'learning_rate':[0.05,0.1,0.125,0.15,0.2]}\n# gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n#               learning_rate=0.1, max_delta_step=0, max_depth=9,\n#               min_child_weight=5, missing=None, n_estimators=140, n_jobs=1,\n#               nthread=None, objective='binary:logistic', random_state=42,\n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n#               silent=None, subsample=1, verbosity=1), param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n# gsearch1.fit(X_over, y_over)\n# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","2feeb0c0":"# param_test = {'gamma':[i\/10.0 for i in range(0,5)]}\n# gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, gamma=0,\n#               learning_rate=0.2, max_delta_step=0, max_depth=9,\n#               min_child_weight=5, missing=None, n_estimators=140, n_jobs=1,\n#               nthread=None, objective='binary:logistic', random_state=42,\n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n#               silent=None, subsample=1, verbosity=1), param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n# gsearch1.fit(X_over, y_over)\n# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","b3ae5b58":"# Final XBGoost tuned model\nclf = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0.3,\n              learning_rate=0.2, max_delta_step=0, max_depth=9,\n              min_child_weight=5, missing=None, n_estimators=140, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1), param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\nclf.fit(X_over, y_over) # fit on training data created with random oversampling and stratified Kfold cross validation","b1769b00":"clf.cv_results_, clf.best_params_, clf.best_score_","e70185fa":"clf # Print the final model and see the parameters","3b4fb8a7":"# Use the above tuned XGBoost Model for prediction on test dataset\nprint(\"Prediction on Train data set using tuned XGBoost Model\")\n\n# Evaluation on train dataset\ny_train_pred = clf.predict(X_over)\n# Probabilities for each class\nXGB_train_probs = clf.predict_proba(X_over)[:, 1]\nplot_model_results(y_over, y_train_pred)\nplot_evaluation_metrics(y_over, y_train_pred, XGB_train_probs, \"Random OverSampling with Stratified KFold Cross Validation\", \"XGBoost - Train Dataset\")\n\nprint(\"Prediction on Test data set using tuned XGBoost Model\")\n# Evaluation on test dataset\ny_test_pred = clf.predict(X_test)\nXGB_test_probs = clf.predict_proba(X_test)[:, 1]\n\nplot_model_results(y_test, y_pred)\nplot_evaluation_metrics(y_test, y_pred, XGB_test_probs, \"Random OverSampling with Stratified KFold Cross Validation\", \"XGBoost - Test Dataset\")","7468600e":"#### Print the FPR, TPR & select the best threshold from the roc curve\ntrain_roc_value = round(roc_auc_score(y_over, XGB_train_probs),4)\ntest_roc_value = round(roc_auc_score(y_test, XGB_test_probs),4)\n\nfpr, tpr, thresholds = roc_curve(y_test, XGB_probs)\nthreshold = round(thresholds[np.argmax(tpr-fpr)],4)\n\nprint('Train auc =', train_roc_value)\nprint('Test auc =', test_roc_value)\n\nprint(\"False positive rate for the final model: \", fpr)\nprint(\"True positive rate for the final model: \", tpr)\nprint(\"Best threshold from the ROC curve: \", threshold)","ac3f32e6":"var_imp = []\nfor i in clf.best_estimator_.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(clf.best_estimator_.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(clf.best_estimator_.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(clf.best_estimator_.feature_importances_)[-3])+1)\n\n# Variable on Index-13 and Index-9 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(clf.best_estimator_.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(clf.best_estimator_.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index], label='Actual Class-0 Examples')\nplt.legend()\nplt.show()","2b6f09af":"<a id=\"evaluation\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           7.2 Model evaluation\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","7e171c94":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nIt is evident that V4, V11, v5 has + ve imporatnce whereas V14, V12, V10 seems to have -ve impact on the predictaions    <\/span>    \n<\/div>","3341c838":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will feed the raw data(imbalanced) which is created by cross validation and StratifiedKFold to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model    <\/span>\n<\/div>    ","9fb59872":"<a id=\"smoteoversampling\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.7 Model building - imbalanced handled by SMOTE Oversamplng\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","8a309fed":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nAs the results show Logistic Regression with L2 Regularisation for StratifiedKFold cross validation provided best results on imbalanced datasets with roc_auc:0.9834 and recall:0.9667\n    <\/span>    \n<\/div>","76f9bd9b":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Perform class balancing with : <\/b><br>\n\n- Random Oversampling\n- SMOTE\n- ADASYN\n <\/div>    ","9f7d52da":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Once we have train and test dataset we will feed the data to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model\n        \nWe did try SVM (support vector Machine) model , but due to extensive processive power requirement we avoided useing the model.\n<br>Once we get results for above model, we will compare the results and select model which provided best results for the Random oversampling techinique\n    <\/span>\n<\/div>    ","9ee66ce4":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will use ADASYN Oversampling method to handle the class imbalance<\/b><br>\n\n1. First we will display class distibution with and without the ADASYN Oversampling.\n2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n <\/div>    ","ad930d0d":"<a id=\"parameter\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           7.1 Hyper-Parameter Tuning for Final Model by Handling class imbalance\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","6c97db97":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\n         We got 284807 records and 31 columns in our dataset. \n    <\/span>    \n<\/div>","f64ed413":"<a id=\"imbalancedmodels\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.2 Model building on imbalanced dataset\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","be2dddf8":"<a id=\"distribution\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.1 Data Distribution\n            <\/span>   \n        <\/font>    \n<\/h2>\n\nHere we will observe the distribution of our classes\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","66ae5f8a":"<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Arial'>\n            <font color = blue >\n                <math> Accuracy = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">(TP + TN)<\/div>\n                    <div style=\"text-align: center;\">(TP + TN + FP + FN)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> Precision = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">TP<\/div>\n                    <div style=\"text-align: center;\">(TP + FP)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>    \n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> Recall = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">TP<\/div>\n                    <div style=\"text-align: center;\">(TP + FN)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> F Measure (F1) = 2 * <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\"> Precision * Recall <\/div>\n                    <div style=\"text-align: center;\">(Precision + Recall)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <ul>\n                    <li><b>TPR (True Positive Rate)<\/b> = TP\/(TP + FN)<\/li>\n                    <li><b>TNR (True Negative Rate)<\/b> = TN\/(TN + FP)<\/li>\n                    <li><b>FPR (False Positive Rate)<\/b> = FP\/(TN + FP)<\/li>\n                    <li><b>FNR (False Negative Rate)<\/b> = FN\/(TP + FN)<\/li>\n                <\/ul>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>    ","a44cf6e8":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nThis shows that most of the transactions occur for less than 5000 amount <\/span>    \n<\/div>","06dd7dfa":"<a id=\"import\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            3. Reading and Understanding Data\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","97aa4b80":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nIt seems XGBoost with Repeated KFold cross validation has provided us the best results with ROC_Value of 0.9768 and recall of 0.9651  <\/span>    \n<\/div>","89f8f958":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nAfter using Power transformer on the dataset, the dataset looks more Gaussian. We are good to build model on this dataset now<\/span>    \n<\/div>","3c738e0a":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight on choosing best evaluation metric: <\/b><br><br>\n        The plots show that the given dataset is imbalanced. In this scenario <b>accuracy<\/b> will not be the best metric to evaluate our classification algorithm. Just using accuracy as the evaluation metric will predit every case as 0 Non Fraud and hence it would be wrong. We can use either <b>recall or Area under ROC curve(roc-auc)<\/b> or the combination of recall and roc-auc to evaluate the models.\n    <\/span>    \n<\/div>","79943bd0":"<a id=\"modelselect\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            7. Final Model Selection:\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","8c1f1256":"<a id=\"imbalance\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.2 Imbalance Analysis\n            <\/span>   \n        <\/font>    \n<\/h2><br>\n\n<div>\n    <span style='font-family:Arial'>\n      We need to check the imbalance in Credit card fraud detection Data. If the data is highly imbalanced, we need to use proper methods to negate the effect of imbalance in our prediction models. \n    <\/span>\n<\/div>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","23a8c12f":"### If there is skewness present in the distribution use:\n- <b>Power Transformer<\/b> package present in the <b>preprocessing library provided by sklearn<\/b> to make distribution more gaussian","cd3e6e49":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Best Model considering various parameters and scenarios:<\/b><br>\n\nIn nutshell rather than aiming for overall accuracy on the entire dataset due to the dataset being imbalanced towards genuine transactions, we cared more about detecting most of the fraud cases (recall), whilst keeping the cost at which this is achieved under control (ROC).<br><br>\nWe got two best models as mentioned above. We prefer XGBoost over Logistic Regression in this case as this model had handled class imbalance and might be more stable and better approach to make future predictions and helpful to understand the influencing parameters.<br><br>\nSo we can try to tune the hyperparameters of the model <b>XGBoost on Random Oversampling with Stratified KFold Cross Validation<\/b> to get best results.","02b6c27b":"### Print the class distribution after applying SMOTE","4576a8d8":"<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Arial'>\n            Credit Card Fraud Detection:\n            <\/span>   \n        <\/font>    \n<\/h3>\n\nIn this project you will predict fraudulent credit card transactions with the help of Machine learning models. In this project, we will analyse customer-level data that has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group.","0b0849ae":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nMost of the fraud transactions seem to have less amount.     <\/span>    \n<\/div>","6553f28c":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nIt seems Undersampling has impacted the recall value of these models. There is an increase in the false positives. Lets try handling the class imbalance using other techniques. <\/span>    \n<\/div>","5b219577":"<a id=\"undersampling\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.5 Model building - imbalanced handled by undersampling\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","99400a02":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nMost of the fraud transactions seem to have taken in between time of 50000 seconds and 1000000 seconds    <\/span>    \n<\/div>","cc712686":"<a id=\"goal\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            1.2 Business Problem Overview\n            <\/span>   \n        <\/font>    \n<\/h2>\n<br>\n\n<div>\n    <span style='font-family:Arial'>\n        \nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\n\nIt has been estimated by Nilson Report that by 2020, banking frauds would account for $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing in new and different ways. \n\n \nIn the banking industry, credit card fraud detection using machine learning is not only a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees as well as denials of legitimate transactions.\n    <\/span>\n<\/div>\n<hr>","6fc25509":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight on Feature Importance: <\/b><br>\nWe can see that feature V17 has high impact followed by V14, V10.\n            <\/span>    \n<\/div>","a1f16d16":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nWe can see most of the variables have outliers and the distribution is not normal. <\/span>    \n<\/div>","d0a22d84":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Hyper Parameters Tuned:<\/b><br><br>\n        The above block gives the optimum value for n_estimators, Max_depth and min_child_weight. Kaggle takes long time for it to complete. So have commented it out.<br><br>\n        Lets use the <b>n_estimators : 140 , max_depth : 9, min_child_weight : 5<\/b> from the above results. We will now use these tuned parameters and check for the best learning rate<\/span>\n<\/div>    ","1b0ec407":"<span style='font-family:Arial'>\n    <font color = green><h2> Classification Model Evaluation Metrics: <\/h2><br>\n     <font color = purple>   <h3> Confusion Matrix : <\/h3>\n    <\/font>\n    <table>\n    <thead>\n    <tr><th><\/th><th>Predicted Negative(0)<\/th><th>Predicted Positive(1)<\/th><\/tr>\n    <\/thead>\n    <tbody>\n        <tr><td><b>Actual Negative(0)<\/b><\/td><td bgcolor=\"green\" ><font color = white>True Negative (TN)<\/font> <\/td><td bgcolor=\"red\"><font color = white>False Postive (FP)<\/font><\/td><\/tr>\n        <tr><td><b>Actual Positive(1)<\/b><\/td><td bgcolor=\"red\" ><font color = white>False Negative (FN)<\/font><\/td><td bgcolor=\"green\" ><font color = white>True Positive (TP)<\/font><\/td><\/tr>\n    <\/tbody>\n<\/table>\n<\/span>\n<hr>","864c6264":"<a id=\"split\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           5.1 Train - Test Split\n            <\/span>   \n        <\/font>    \n<\/h2>","7340c7f4":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Lets create generic functions for model evaluation and for building following models: <\/b><br>\n\n- Logistic Regression with L1\/L2 regularization\n- KNN models\n- SVM models\n- Decision Trees\n- Random Forest\n- XGBoost\n <\/div>    ","7138e589":"<a id=\"dataprep\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            5. Data Preparation\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","3a6797df":"<h3>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.1.3 Visualizing Amount Distribution\n            <\/span>   \n        <\/font>    \n<\/h3>","302ef3eb":"<a id=\"python\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            2.2 Import Python Libraries:\n            <\/span>   \n        <\/font>    \n<\/h2>","340f37d1":"<h1 style=\"text-align:center\">   \n      <font color = purple >\n            <span style='font-family:Arial'>\n                Credit Card Fraud Detection - Deep Leraning Capstone Project\n            <\/span>   \n        <\/font>    \n<\/h1>\n<h3 style=\"text-align:right\">   \n      <font color = gray >\n            <span style='font-family:Georgia'>\n                By Padma A.\n            <\/span>   \n        <\/font>    \n<\/h3>\n<hr style=\"width:100%;height:5px;border-width:0;color:gray;background-color:gray\">\n<center><img src=\"https:\/\/miro.medium.com\/max\/601\/1*xSqK9iS7nZAaB-Sdwiwjow.png\"><\/center>","b4384e61":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nFrom the above two plots, we can see that most of the fraud transactions seem to have amount less than 5000.     <\/span>    \n<\/div>","64019e81":"<a id=\"fraud\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            1.3 Understanding & Defining Fraud\n            <\/span>   \n        <\/font>    \n<\/h2>\n<br>\n<div>\n    <span style='font-family:Arial'>\n        Credit card fraud is any dishonest act or behaviour to obtain information without proper authorisation from the account holder for financial gain. Among different ways of committing frauds, skimming is the most common one, which is a way of duplicating information that is located on the magnetic strip of the card. Apart from this, the other ways are as follows:\n\n- Manipulation\/alteration of genuine cards\n- Creation of counterfeit cards\n- Stealing\/loss of credit cards\n- Fraudulent telemarketing\n    <\/span>\n<\/div>\n<hr>","7d3eedb1":"<a id=\"functions\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.1 Generic functions for model building and evaluation\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","a694d815":"<h3>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.1.2 Visualizing Time Distribution\n            <\/span>   \n        <\/font>    \n<\/h3>","be711b5c":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Once we have train and test dataset we will feed the data to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model\n        \nI tried SVM (support vector Machine) model, but due to extensive processive power requirement and time take, I have commented it out.\n<br>Once we get results for above model, we will compare the results and select model which provided best results for the Random oversampling techinique\n    <\/span>\n<\/div>    ","e6e2435a":"<a id=\"background\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            1.1 Problem Statement\n            <\/span>   \n        <\/font>    \n<\/h2>","779d131e":"#### So we have 492 fraudalent transactions out of 284807 total credit card transactions.\n\n\nTarget variable distribution shows that we are dealing with an highly imbalanced problem as there are many more genuine transactions class as compared to the fraudalent transactions. The model would achieve high accuracy as it would mostly predict majority class \u2014 transactions which are genuine in our example. To overcome this we will use other metrics for model evaluation such as ROC-AUC , precision and recall etc","12d211a8":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\n         The data types of the columns are float and integer. We have 30 columns with float values and one column with integer values. None of the columns have inconsistent datatype. Hence, no conversion is required. Let's inspect if any column has null values. \n    <\/span>    \n<\/div>","2fb15535":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nThe green plot shows the genuine transactions and the red plot shows the fraud transactions.<\/span>    \n<\/div>","a98e0910":"<a id=\"outlier\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           5.2 Handling Data Skewness\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","192158b8":"<a id=\"conclusion\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            8. Feature Importance and Conclusion\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","08939496":"<a id=\"crossvalidation\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.3 Model building on imbalanced dataset created by cross validation with RepeatedKFold\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","02362dcd":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Hyper Parameters Tuned:<\/b><br><br>\n        The above block gives the optimum value for learning_rate, n_estimators, Max_depth and min_child_weight. Kaggle takes long time for it to complete. So have commented it out.<br><br>\nLets use the <b>n_estimators : 140 , max_depth : 9, min_child_weight : 5, learning_rate : 0.2 <\/b>from the above results. We will now use these tuned parameters and check for the best gamma value\n<\/div>    ","d7240498":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Results for SMOTE Oversampling: <\/b><br>\nLooking at Recall and ROC value we have Random Forest which has provided best results for SMOTE oversampling technique<\/span>    \n<\/div>","27aba526":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will feed the raw data(imbalanced) which is created by cross validation with RepeatedKFold to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model    <\/span>\n<\/div>    ","8c76c428":"<a id=\"overstratified\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.6 Model building - imbalanced handled by Oversampling with RandomOverSampler and StratifiedKFold Cross Validation\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","322f5819":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Hyper Parameters Tuned:<\/b><br><br>\n        The above block gives the optimum value for Max_depth and min_child_weight. Kaggle takes long time for it to complete. So have commented it out.<br><br>\nLets use the <b>Max_depth : 9 and min_child_weight : 5<\/b> from the above results and tune XGBoost model further for n_estimators\n <\/div>    ","3e5f1254":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will feed the raw data(imbalanced) which is power transformed to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model    <\/span>\n<\/div>    ","98b3ba62":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nThe scatter plot is little weird since we are plotting time vs class where class is a categorical variable.     <\/span>    \n<\/div>","db74a633":"<a id=\"adaysnoversampling\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.8 Model building - imbalanced handled by ADASYN Oversampling\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","98d622e6":"<a id=\"intro\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            1. Introduction:\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","05003241":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Once we have train and test dataset we will feed the data to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model\n        \nWe did try SVM (support vector Machine) model , but due to extensive processive power requirement we avoided useing the model.\n<br>Once we get results for above model, we will compare the results and select model which provided best results for the Random oversampling techinique\n    <\/span>\n<\/div>    ","4b739a4f":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Results for ADASYN Oversampling: <\/b><br>\nLooking at Recall and ROC value we have XGBoost and Random forest which have provided best results for ADASYN oversampling technique<\/span>    \n<\/div>","685d0d9f":"<a id=\"warning\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            2.1 Suppress Warnings:\n            <\/span>   \n        <\/font>    \n<\/h2>","148f6b3c":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Lets tune the XGBoost Model for max_depth and min_child_weight on Random Oversampling with Stratified KFold Cross Validation Training Dataset<\/b><br>\n <\/div>    ","3c69218c":"<a id=\"delcol\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.3 Delete Unnecessary Columns\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","47d4855c":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b>\n        \n- In the long tail, fraud transaction happened more frequently.\n- It seems It would be hard to differentiate fraud from normal transactions by transaction amount alone.    <\/span>    \n<\/div>","a843afcf":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nDataset is split into train and test data\n    <\/span>    \n<\/div>","3c6aadd5":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\nThis is the evaluation results on the test dataset of the XGBoost model trained on Random oversampling with Stratified Kfold Cross Validation.\n        <\/span>    \n<\/div>","1d595b40":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nWe can see many variables have outliers and the skewness can be seen and handled in the section: <a href='#outlier'>5.2 Handling Data Skewness <\/a>\n    <\/span>    \n<\/div>","aac40978":"<a id=\"duplicate\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            3.3 Null Value Analysis\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","f2f375cc":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br><br>\nThe green plot shows the genuine transactions and the red plot shows the fraud transactions. <br><br> The plot shows that the fraud transactions are high at early time of the day and in the afternoon time<\/span>    \n<\/div>","5c6529b2":"<h3>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.1.4 Visualizing correlation\n            <\/span>   \n        <\/font>    \n<\/h3>","0d0cc421":"##### Preserve X_test & y_test to evaluate on the test data once you build the model","8065045d":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nHour zero corresponds to the hour the first transaction happened and not necessarily 12-1 AM. Given the heavy decrease in normal transactions from hours 1 to 8 and again roughly at hours 24 to 32, it seems fraud tends to occur at higher rates during the night.   \n<\/span>    \n<\/div>","35c28b10":"<a id=\"toc\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">\n    <span style='font-family:Arial'>\n    Table of Content\n    <\/span>\n<\/h1>\n    <br>\n<span style='font-family:Arial'>\n    <ul>\n        <li><a href='#intro'>1. Introduction<\/a><\/li>\n        <ul>\n            <li><a href='#background'>1.1 Problem Statement<\/a><\/li>\n            <li><a href='#goal'>1.2 Business Problem Overview<\/a><\/li>\n            <li><a href='#fraud'>1.3 Understanding and Defining Fraud<\/a><\/li>\n            <li><a href='#datadict'>1.4 Data Dictionary<\/a><\/li>\n        <\/ul>\n        <li><a href='#libraries'>2. Environment Setup<\/a><\/li>\n        <ul>\n            <li><a href='#warning'>2.1 Suppress Warnings<\/a><\/li>\n            <li><a href='#python'>2.2 Import Python Libraries<\/a><\/li>\n            <li><a href='#setup'>2.3 Setting up Jupyter View<\/a><\/li>  \n            <li><a href='#libversion'>2.4 Installed Python Library versions used in this project<\/a><\/li> \n        <\/ul>\n        <li><a href='#import'>3. Reading & Understanding the data<\/a><\/li>\n        <ul>\n            <li><a href='#input'>3.1 Importing the input file<\/a><\/li>\n            <li><a href='#inspect'>3.2 Inspecting the dataframe<\/a><\/li>\n            <li><a href='#duplicate'>3.3 Null Value Analysis<\/a><\/li>\n        <\/ul>\n        <li><a href='#eda'>4. Exploratory Data Analysis <\/a><\/li>\n        <ul>\n            <li><a href='#distribution'>4.1 Data Distribution <\/a><\/li>\n            <li><a href='#imbalance'>4.2 Imbalance Analysis <\/a><\/li>\n            <li><a href='#delcol'>4.3 Delete Unnecessary Columns <\/a><\/li>\n        <\/ul>\n        <li><a href='#dataprep'>5. Data Preparation <\/a><\/li>\n        <ul>\n            <li><a href='#split'>5.1 Train- Test Split <\/a><\/li>\n            <li><a href='#outlier'>5.2 Handling Data Skewness <\/a><\/li>\n        <\/ul>\n        <li><a href='#modelbuild'>6. Model Building<\/a><\/li>\n        <ul>   \n            <li><a href='#functions'>6.1 Generic functions for model building <\/a><\/li>\n            <li><a href='#imbalancedmodels'>6.2 Model building on imbalanced dataset <\/a><\/li>\n            <li><a href='#crossvalidation'>6.3 Model building on imbalanced dataset created by cross validation with RepeatedKFold<\/a><\/li>\n            <li><a href='#StratifiedKFold'>6.4 Model building on imbalanced dataset created by cross validation with StratifiedKFold<\/a><\/li>\n            <li><a href='#undersampling'>6.5 Model building - imbalance handled by undersampling<\/a><\/li>\n            <li><a href='#overstratified'>6.6 Model building - imbalance handled by Oversampling with RandomOverSampler and StratifiedKFold Cross Validation<\/a><\/li>\n            <li><a href='#smoteoversampling'>6.7 Model building - imbalance handled by SMOTE Oversamplng<\/a><\/li>\n            <li><a href='#adaysnoversampling'>6.8 Model building - imbalance handled by ADASYN Oversampling<\/a><\/li>\n        <\/ul>\n        <li><a href='#modelselect'>7. Final Model Selection <\/a><\/li>\n        <ul>\n            <li><a href='#parameter'>7.1 Hyper-Parameter Tuning for Final Model by Handling class imbalance<\/a><\/li>\n            <li><a href='#evaluation'>7.2 Model evaluation<\/a><\/li>\n        <\/ul>\n        <li><a href='#conclusion'>8. Feature Importance and Conclusion <\/a><\/li>\n        <\/ul>\n<\/span>","644d26e9":"<a id=\"StratifiedKFold\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n           6.4 Model building on imbalanced dataset created by cross validation with StratifiedKFold\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","90e12064":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will use Random Oversampling method to handle the class imbalance<\/b><br>\n\n1. First we will display class distibution with and without the Random Oversampling.\n2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n <\/div>    ","9af14333":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        After training and running all the models, we can see from the above metrics that the following models have provided best roc_auc value and recall. <br><br>\n<b>1. XGBOOST model with Random Oversampling with StratifiedKFold CV \/ RepeatedKFold Cross Validation<\/b> and <br><b>2. Logistic Regression with L2 Regularisation with StratifiedKFold Cross Validation <\/b> <\/span>    \n<\/div>","a177aa1e":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nThe distribution of amount for Fraud transactions is much higher than non-fradualent transactions.    <\/span>    \n<\/div>","14bf2fd7":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Results for Random Oversampling: <\/b><br><br>\nRandom Oversampling seems to have +ve change in prediction for XGBoost when compared to Undersampling. Its Recall has improved to 0.96 and the ROC value is holding good at 0.97\n<\/span>    \n<\/div>","bfca7a68":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Hyper parameter tuning results:<\/b><br><br>\n1. max_depth : 9 <br>2. min_child_weight : 5 <br>3. n_estimators : 140 <br>4. learning_rate : 0.2 <br>5. gamma: 0.3<br> <\/span>    \n<\/div>","e9a63195":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nTransaction time data seems to be high between 10 and 20; then it decreases to go low at 30 and then increases again\n    <\/span>    \n<\/div>","d2803423":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will feed the balanced data which is created by Random Undersampling(RUS) to below models:<\/b><br>\n        \n1. Logistic Regression with L2 Regularisation\n2. Logistic Regression with L1 Regularisation\n3. KNN\n4. Decision tree model with Gini criteria\n5. Decision tree model with Entropy criteria\n6. Random Forest\n7. XGBoost\n8. SVM Model    <\/span>\n<\/div>    ","647ae409":"<a id=\"eda\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            4. Exploratory Data Analysis\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","1e2b4698":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>We will use SMOTE Oversampling method to handle the class imbalance<\/b><br>\n\n1. First we will display class distibution with and without the SMOTE Oversampling.\n2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n <\/div>    ","400211bc":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Evaluation on the tuned XGBoost model which is trained on random oversampling with Stratified KFold Cross Validation<\/b><\/span><\/div>    ","036bb553":"### Print the class distribution after applying ADASYN","9451f032":"<a id=\"libversion\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            2.4 Installed Python Library versions used in this project\n            <\/span>   \n        <\/font>    \n<\/h2>","2ca86f30":"<a id=\"datadict\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            1.4 Data Dictionary\n            <\/span>   \n        <\/font>    \n<\/h2>\n<br>\n<div>\n    <span style='font-family:Arial'>\n<ul>\n<li>The data set includes credit card transactions made by European cardholders over a period of two days in September 2013. <\/li>\n<li>Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for 0.172% of the total transactions.<\/li> \n<li>The data set has also been modified with principal component analysis (PCA) to maintain confidentiality. Apart from \u2018time\u2019 and \u2018amount\u2019, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. <\/li>\n<li>The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. <\/li>\n    <li>The feature 'amount' is the transaction amount. <\/li>\n<li>The feature 'class' represents class labelling, and it takes the value of 1 in cases of fraud and 0 in others.<\/li>\n        <\/ul>\n    <\/span>\n<\/div>\n<hr>","17b73664":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Arial'>\n        <b>Project Pipeline: <\/b><br>\n\n- Data Understanding\n- EDA\n- Train\/test Split\n- Model building \n- Hyperparameter tuning\n- Model evaluation\n <\/div>    ","de353e50":"#### Feature Importance","316c338f":"<a id=\"libraries\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            2. Environment Setup:\n            <\/span>   \n        <\/font>    \n<\/h1>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","bb124f72":"<a id=\"input\"><\/a>\n<h2 name='libraries'>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            3.1 Importing the Input file\n            <\/span>   \n        <\/font>    \n<\/h2>","3492559c":"<a id=\"modelbuild\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Arial'>\n            6. Model Building\n            <\/span>   \n        <\/font>    \n<\/h1>\n\nBuild different models and see the results\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","c3f0cbfa":"<a id=\"setup\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            2.3 Setting up Jupyter View\n            <\/span>   \n        <\/font>    \n<\/h2>","2d46099a":"#### Feature Importance","175c05c2":"<a id=\"inspect\"><\/a>\n<h2 name='libraries'>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            3.2 Inspect Dataframe\n            <\/span>   \n        <\/font>    \n<\/h2>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","86e553e4":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Conclusion: <\/b><br><br>\n            <ul><li>We have built many models like logistic regression with L1\/L2 regularization, SVM, KNN, Decision Tree, Random forest based on the transaction data provided to us.\n                <li>The data provided to us was an <b>imbalanced data set<\/b>. <\/li>\n                <li>Hence, for building a proper logistic model on top of that we have used some balancing techniques like (Random oversampling, SMOTE, ADASYN etc) to balance the data and applied some of very popular logistic regression models like Random Forest, Logistic regression and some boosting techniques like XGBoost to catch any fraud transactions.<\/li><br><br> \n                <li>In this imbalanced scenario, Accuracy was not a good evaluation criteria and hence we focussed more on <b>Recall and AUC.<\/b><\/li>\n                <li>Finally we were able to build a proper model and predicted on test data and the results were satisfying.<br><br><\/li>\n                <li>We were also able to figure out the variables which will be important in detecting any fraud transactions: <b>V14, V10 and V4<\/b> which are the principal components and are not provided for us to maintain confidentiality<\/li>\n                <li>PCA components V14, V10 and V4 are the ones which can explain maximum variance in the model and hence has to be focused more<\/span>    \n<\/div>","cadf8b79":"<h3>   \n      <font color = purple >\n            <span style='font-family:Arial'>\n            4.1.1 Visualizing the Distribution of Fraudulent Variable\n            <\/span>   \n        <\/font>    \n<\/h3>","343163e6":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Arial'>\n        <b>Insight: <\/b><br>\nWe observed that the dataset has no null values and Hence, no Null treatment is required.\n    <\/span>    \n<\/div>"}}