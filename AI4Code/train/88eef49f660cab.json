{"cell_type":{"27769b87":"code","de9e1b75":"code","0dbbcd53":"code","ccc33f39":"code","b4a7d788":"code","4ec641ac":"code","3bd809ab":"code","b586dfee":"code","5a354914":"code","31540a03":"code","8ddb76db":"code","fde71354":"code","31be4ff5":"code","bd635503":"code","5952fd66":"code","d37cc133":"markdown","769f3dc4":"markdown","cd358917":"markdown","10562a1e":"markdown","d1f13572":"markdown","60b136f8":"markdown","c18d4aac":"markdown","92d2dc62":"markdown","152904ef":"markdown"},"source":{"27769b87":"## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","de9e1b75":"## defining constants\nPATH_TRAIN = \"\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv\"\nPATH_TEST = \"\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv\"\n\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 2357\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n","0dbbcd53":"## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)\n","ccc33f39":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n","b4a7d788":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n","4ec641ac":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_1_3_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_1_3_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"change_1_7_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 7}_cc\"]\n    df[\"change_1_7_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 7}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n\n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases - df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities - df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_1_3_cc\",\n        \"change_1_3_ft\",\n        \"change_1_7_cc\",\n        \"change_1_7_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]\n","3bd809ab":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200)) + test_lag_cc\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200)) + test_lag_ft\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n","b586dfee":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft\n","5a354914":"## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)\n","31540a03":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))\n","8ddb76db":"## feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature, title = \"Feature Importance of LGB Model 1\")\nv1.vbar(x = df_fimp_1_cc.feature, top = df_fimp_1_cc.importance, width = 0.81)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature, title = \"Feature Importance of LGB Model 14\")\nv14.vbar(x = df_fimp_14_cc.feature, top = df_fimp_14_cc.importance, width = 0.81)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature, title = \"Feature Importance of LGB Model 28\")\nv28.vbar(x = df_fimp_28_cc.feature, top = df_fimp_28_cc.importance, width = 0.81)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)\n","fde71354":"## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 ConfirmedCases over time\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases, color = \"green\", legend_label = \"CC (Train)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_lgb, color = \"blue\", legend_label = \"CC LGB (Val)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_mad, color = \"purple\", legend_label = \"CC MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"CC LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"CC MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","31be4ff5":"## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Fatalities over time\")\n    v.line(df_geography.Date, df_geography.Fatalities, color = \"green\", legend_label = \"FT (Train)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_lgb, color = \"blue\", legend_label = \"FT LGB (Val)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_mad, color = \"purple\", legend_label = \"FT MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"FT LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"FT MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","bd635503":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.13 * df_test.ConfirmedCases_test_lgb + 0.87 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.13 * df_test.Fatalities_test_lgb + 0.87 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these geographies well\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission\n","5952fd66":"## writing final submission and complete output\ndf_submission.to_csv(PATH_SUBMISSION, index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)\n","d37cc133":"## Validation\n* Validating LGB and MAD models using RMSLE metric.\n\n* Visualizing feature importance of 1st, 14th and 28th models to see which features are crucial to predict short-term, medium-term and long-term respectively.","769f3dc4":"## Prepare Data\n* Unlike many competitions, there are overlapping dates across train and test data. So it is important to be careful while handling the dates.\n\n* Some data issues are fixed using previous max values in case of discrepancy.\n\n* External data from [this dataset](https:\/\/www.kaggle.com\/rohanrao\/covid19-forecasting-metadata) is merged.","cd358917":"## MAD Model\n* This Moving Average with Decay (MAD) model is a simple heuristic using historic values that decays with time.\n\n* It is structured based on my EDA and intuitive feeling of how the Covid-19 trend is likely to move.","10562a1e":"## Feature Engineering\n* Lag features are created.\n\n* Several differences, ratios and averages of historic values are created and used.","d1f13572":"## LGB Model\n* Note that the target variable used is the change (or difference) in log values of ConfirmedCases \/ Fatalities from the last known value.\n\n* One model is built for each day in the private test data. So that's a total of 28 models.\n\n* A single set of parameters with less number of leaves along with regularization is used to control overfitting.","60b136f8":"## Modelling\n* One model is built for each date.\n\n* For dates already present in train data due to the overlap, no model is built.\n\n* Models are validated using the same framework.","c18d4aac":"## Visualizing Predictions\n* Viewing the actual, validation and test values together for each geography for ConfirmedCases as well as Fatalities.","92d2dc62":"## Submission\n* Combining the two approaches using weights for final submission.\n\n* LGB models don't seem to perform well for certain geographies and are replaced by MAD predictions.","152904ef":"## Covid-19 Forecasting (Week 3)\n![Coronavirusm.jpg](attachment:Coronavirusm.jpg)\n\nThe [covid-19 virus](https:\/\/en.wikipedia.org\/wiki\/Coronavirus_disease_2019) has created a pandemic in the early months of 2020. Kaggle has hosted a series of forecasting competitions for predict the number the spread of the virus. This is my solution for Week 3 of the series.\n\nMy solution for Week 2: https:\/\/www.kaggle.com\/rohanrao\/covid-19-w2-lgb-mad   \nMy solution for Week 4: https:\/\/www.kaggle.com\/rohanrao\/covid-19-w4-lgb-mad\n\nI am compiling external datasets in the competition format here: https:\/\/www.kaggle.com\/rohanrao\/covid19-forecasting-metadata   \nThey are structured such that it can directly be merged with the train and test datasets on Kaggle.\n"}}