{"cell_type":{"02ee6473":"code","3e1b0331":"code","eb26d70e":"code","1df2d117":"code","b27c154a":"code","ff255001":"code","9a83b7db":"code","3432a90e":"code","b39db4ed":"code","9af8942e":"code","edaf6882":"code","ac64a61d":"code","2bf2c738":"code","5a7c6dfc":"code","16da26d5":"code","313e5890":"code","96888af1":"code","c5253445":"code","0d27509b":"code","3c59d572":"code","b1140fdd":"code","e8af3d2b":"code","b333b0f9":"code","7adf2a06":"code","61a99f3e":"code","8167eefb":"code","70879b3f":"code","a22b1746":"code","c66ad850":"code","e9ee5131":"code","324dc03a":"code","c9aa0e0a":"code","d58bd801":"code","c510d836":"code","00c619c0":"markdown","2e5f689e":"markdown","e8642d6f":"markdown","f478b0e8":"markdown","2a9f7860":"markdown","0945b93a":"markdown","81c6d72d":"markdown","af06436c":"markdown"},"source":{"02ee6473":"!mkdir white_belt\/\n!mkdir blue_belt\/\n!mkdir black_belt\/","3e1b0331":"%%writefile white_belt\/all_rock.py\ndef constant_play_agent_0(observation, configuration):\n  \"\"\"Always plays \"Rock\" (0)\"\"\"\n  return 0","eb26d70e":"%%writefile white_belt\/all_paper.py\ndef constant_play_agent_1(observation, configuration):\n  \"\"\"Always plays \"Paper\" (1)\"\"\"\n  return 1","1df2d117":"%%writefile white_belt\/all_scissors.py\ndef constant_play_agent_2(observation, configuration):\n  \"\"\"Always plays \"Scissors\" (2)\"\"\"\n  return 2","b27c154a":"%%writefile white_belt\/reactionary.py\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","ff255001":"%%writefile white_belt\/counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","9a83b7db":"%%writefile white_belt\/mirror.py\ndef mirror_opponent_agent(observation, configuration):\n  if observation.step > 0:\n    return observation.lastOpponentAction\n  else:\n    return 0","3432a90e":"%%writefile white_belt\/mirror_shift_1.py\ndef mirror_shift_opponent_agent_1(observation, configuration):\n  if observation.step > 0:\n    return (observation.lastOpponentAction + 1) % 3\n  else:\n    return 0","b39db4ed":"%%writefile white_belt\/mirror_shift_2.py\ndef mirror_shift_opponent_agent_2(observation, configuration):\n  if observation.step > 0:\n    return (observation.lastOpponentAction + 2) % 3\n  else:\n    return 0","9af8942e":"%%writefile white_belt\/de_bruijn.py\nimport re\nimport random\nimport pydash\nfrom itertools import combinations_with_replacement\n\nactions = list(combinations_with_replacement([2,1,0,2,1,0],3)) * 18\n# random.shuffle(actions)\nprint('len(actions)',len(actions))\nprint(actions)\nactions = pydash.flatten(actions)\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef kaggle_agent(observation, configuration):    \n    action = actions[observation.step] % configuration.signs\n    return int(action)","edaf6882":"%%writefile white_belt\/statistical.py\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","ac64a61d":"%%writefile blue_belt\/not_so_markov.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","2bf2c738":"%%writefile blue_belt\/transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","5a7c6dfc":"%%writefile blue_belt\/stochastic_transition_matrix.py\nimport numpy as np\nimport json\n#import torch\n\nmatrix = np.ones((3,3,3)) * (1\/3) #so we can choose object based on what we chose and what the opponent chose transition matrix\nmatrix_freq = np.ones((3,3,3)) #frequency matrix\nprev_me = 0\nprev_op = 0\n#print(state_dict)\n\ndef copy_opponent_agent (observation, configuration):\n    \n    global prev_me, prev_op, matrix, matrix_freq\n        \n    if observation.step > 0:\n        #return (observation.lastOpponentAction + 1)%3\n        #prev_op = observation.lastOpponentAction #we store the last action of the opponent\n        \n        #from step > 1 we can update matrix because we know what we chose and what it chose\n        if observation.step > 1:\n            matrix_freq[prev_op, prev_me, observation.lastOpponentAction] += 1\n            matrix[prev_op, prev_me, :] = matrix_freq[prev_op, prev_me, :] \/ np.sum(matrix_freq[prev_op, prev_me, :]) \n            \n        \n        prev_op = observation.lastOpponentAction #we store the last action of the opponent  \n        \n        #choose the optimal choice based on the transition matrix\n        #choosing stochastically\n        prev_me = (np.random.choice(3, p=matrix[prev_op, prev_me, :]) + 1) % 3\n        \n        #print(matrix) \n        \n        state_dict = {\"transition tensor\" : matrix.tolist()}\n        with open('transition_matrix.json', 'a') as outfile:\n            json.dump(state_dict, outfile)\n            outfile.write(\"\\n\")\n        \n        return prev_me\n        \n              \n    else:\n        #prev_me = np.random.randint(0,3)\n        state_dict = {\"transition tensor\" : matrix.tolist()}\n        with open('transition_matrix.json', 'w') as outfile:\n            json.dump(state_dict, outfile)\n            outfile.write(\"\\n\")\n        prev_me = (np.random.choice(3, p=matrix[prev_op, prev_me, :]) + 1)%3\n        return prev_me\n        #json.dump(matrix_freq, outfile)","16da26d5":"%%writefile blue_belt\/decision_tree.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","313e5890":"%%writefile blue_belt\/xgboost.py\n\nimport random\nfrom pandas import DataFrame\nfrom xgboost import XGBClassifier\n\nnumTurnsPredictors = 5 #number of previous turns to use as predictors\nminTrainSetRows = 10 #only start predicting moves after we have enough data\nmyLastMove = None\nmySecondLastMove = None\nopponentLastMove = None\nnumDummies = 2 #how many dummy vars we need to represent a move\npredictors = DataFrame(columns=[str(x) for x in range(numTurnsPredictors * 2 * numDummies)])\npredictors = predictors.astype(\"int\")\nopponentsMoves = []\nroundHistory = [] #moves made by both players in each round\nclf = XGBClassifier(n_estimators=10)\n\ndef randomMove():\n    return random.randint(0,2)\n\n#converts my and opponents moves into dummy variables i.e. [1,2] into [0,1,1,0]\ndef convertToDummies(moves):\n    newMoves = []\n    dummies = [[0,0], [0,1], [1,0]]\n\n    for move in moves:\n        newMoves.extend(dummies[move])\n\n    return newMoves\n\ndef updateRoundHistory(myMove, opponentMove):\n    global roundHistory\n    roundHistory.append(convertToDummies([myMove, opponentMove]))\n\ndef flattenData(data):\n    return sum(data, [])\n\ndef updateFeatures(rounds):\n    global predictors\n    flattenedRounds = flattenData(rounds)\n    predictors.loc[len(predictors)] = flattenedRounds\n\ndef fitAndPredict(clf, x, y, newX):\n    df = DataFrame.from_records([newX], columns=[str(i) for i in range(numTurnsPredictors * 2 * numDummies)])\n    clf.fit(x, y)\n    return int(clf.predict(df)[0])\n\ndef makeMove(observation, configuration):\n    global myLastMove\n    global mySecondLastMove\n    global opponentLastMove\n    global predictors\n    global opponentsMoves\n    global roundHistory\n\n    if observation.step == 0:\n        myLastMove = randomMove()\n        return myLastMove\n\n    if observation.step == 1:\n        updateRoundHistory(myLastMove, observation.lastOpponentAction)\n        myLastMove = randomMove()\n        return myLastMove\n\n    else:\n        updateRoundHistory(myLastMove, observation.lastOpponentAction)\n        opponentsMoves.append(observation.lastOpponentAction)\n\n        if observation.step > numTurnsPredictors:\n            updateFeatures(roundHistory[-numTurnsPredictors - 1: -1])\n\n        if len(predictors) > minTrainSetRows:\n            predictX = flattenData(roundHistory[-numTurnsPredictors:]) #data to predict next move\n            predictedMove = fitAndPredict(clf, predictors,\n                                opponentsMoves[(numTurnsPredictors-1):], predictX)\n            myLastMove = (predictedMove + 1) % 3\n            return myLastMove\n        else:\n            myLastMove = randomMove()\n            return myLastMove","96888af1":"%%writefile blue_belt\/statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","c5253445":"%%writefile blue_belt\/rfind.py\n\nimport random\nhist = []  # history of your moves\ndict_last = {}\nmax_dict_key = 10\nlast_move = 0\n\n\ndef beat(x):\n    return (x + 1) % 3\n\n\ndef predict():\n    global dict_last\n    global max_dict_key\n    for i in reversed(range(min(len(hist), max_dict_key))):\n        t = tuple(hist[-i:])\n        if t in dict_last:\n            return dict_last[t]\n    return random.choice([0, 1, 2])\n\n\ndef update(move, op_move):\n    global hist\n    global dict_last\n    global max_dict_key\n    hist.append(move)\n    for i in reversed(range(min(len(hist), max_dict_key))):\n        t = tuple(hist[-i:])\n        dict_last[t] = op_move\n\n\ndef run(observation, configuration):\n    global last_move\n    if observation.step == 0:\n        last_move = random.choice([0, 1, 2])\n        return last_move\n    update(last_move, observation.lastOpponentAction)\n    move = beat(predict())\n\n    return move","0d27509b":"%%writefile black_belt\/multi_armed_bandit_v15.py\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.1),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.1),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.1),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.1),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.1),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.1),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.1),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.1),\n    \n    'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.01),\n    'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.01),\n    'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.01),\n    'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.01),\n    \n    'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.01),\n    'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.01),\n    'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.01),\n    'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.01),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.03 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","3c59d572":"%%writefile black_belt\/multi_armed_bandit_v32.py\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n        \n# looks for the same pattern in history and returns the best answer to the most possible counter strategy\nclass pattern_matching(agent):\n    def __init__(self, steps = 3, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        self.steps = steps\n        \n    def history_step(self, history):\n        if len(history) < self.steps + 1:\n            return self.initial_step()\n        \n        next_step_count = np.zeros(3) + self.init_value\n        pattern = [history[i][self.step_type] for i in range(- self.steps, 0)]\n        \n        for i in range(len(history) - self.steps):\n            next_step_count = (next_step_count - self.init_value)\/self.decay + self.init_value\n            current_pattern = [history[j][self.step_type] for j in range(i, i + self.steps)]\n            if np.sum([pattern[j] == current_pattern[j] for j in range(self.steps)]) == self.steps:\n                next_step_count[history[i + self.steps][self.step_type]] += 1\n        \n        if next_step_count.max() == self.init_value:\n            return self.initial_step()\n        \n        if  self.deterministic:\n            step = np.argmax(next_step_count)\n        else:\n            step = np.random.choice([0,1,2], p = next_step_count\/next_step_count.sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n        \n# if we add all agents the algorithm will spend more that 1 second on turn and will be invalidated\n# right now the agens are non optimal and the same computeations are repeated a lot of times\n# the approach can be optimised to run much faster\nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n    \n#     'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.001),\n#     'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.001),\n#     'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.001),\n#     'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.001),\n    \n#     'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.001),\n#     'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.001),\n#     'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.001),\n#     'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_1': pattern_matching(1, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_1': pattern_matching(1, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_1': pattern_matching(1, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_1': pattern_matching(1, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_2': pattern_matching(2, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_2': pattern_matching(2, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_2': pattern_matching(2, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_2': pattern_matching(2, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_3': pattern_matching(3, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_3': pattern_matching(3, False, True, decay = 1.001),\n    'determenistic_pattern_matching_decay_3': pattern_matching(3, True, False, decay = 1.001),\n    'determenistic_self_pattern_matching_decay_3': pattern_matching(3, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_4': pattern_matching(4, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_4': pattern_matching(4, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_4': pattern_matching(4, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_4': pattern_matching(4, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_5': pattern_matching(5, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_5': pattern_matching(5, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_5': pattern_matching(5, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_5': pattern_matching(5, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_6': pattern_matching(6, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_6': pattern_matching(6, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_6': pattern_matching(6, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_6': pattern_matching(6, True, True, decay = 1.001),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    # load history\n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    # we can use it for analysis later\n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","b1140fdd":"%%writefile black_belt\/memory_patterns_v20.py\n# start executing cells from here to rewrite submission.py\n\nimport random\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    \"\"\" \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent's memory\n    \"\"\"\n    pattern_group_index = previous_action[\"pattern_group_index\"]\n    pattern_index = previous_action[\"pattern_index\"]\n    pattern = groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    pattern[\"reward\"] += previous_step_result\n    # if pattern is inefficient\n    if pattern[\"reward\"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent's memory\n        del groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \ndef find_action(group, group_index):\n    \"\"\" if possible, find my_action in this group of memory patterns \"\"\"\n    if len(current_memory) > group[\"memory_length\"]:\n        this_step_memory = current_memory[-group[\"memory_length\"]:]\n        memory_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern[\"opp_next_actions\"]:\n                # if this opponent's action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action[\"amount\"] > my_action_amount or\n                        (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action[\"amount\"]\n                    my_action = action[\"response\"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern and its index in memory \"\"\"\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i][\"actions\"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    \"\"\" add my_agent's current step to current_memory \"\"\"\n    # if there's too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent's last action to agent's current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    \"\"\" if possible, update or add some memory pattern in this group \"\"\"\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group[\"memory_length\"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group[\"memory_length\"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                \"actions\": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                \"reward\": 0,\n                # list of observed opponent's actions after each occurrence of this pattern\n                \"opp_next_actions\": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of my_agent\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            group[\"memory_patterns\"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n    \n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 5\n# minimum steps in a memory pattern\nSTEPS_MIN = 3\n# lowest efficiency threshold of a memory pattern before being removed from agent's memory\nEFFICIENCY_THRESHOLD = -3\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n\n# current memory of the agent\ncurrent_memory = []\n# previous action of my_agent\nprevious_action = {\n    \"action\": None,\n    # action was taken from pattern\n    \"action_from_pattern\": False,\n    \"pattern_group_index\": None,\n    \"pattern_index\": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of my_agent\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    \n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # action of my_agent\n    my_action = None\n    \n    # forced random action\n    global steps_to_random\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # if it's not first step\n    if obs[\"step\"] > 0:\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_my_agent(current_memory[-2], current_memory[-1])\n        global reward\n        reward += previous_step_result\n        # if previous action of my_agent was taken from pattern\n        if previous_action[\"action_from_pattern\"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action's data\n                previous_action[\"action\"] = my_action\n                previous_action[\"action_from_pattern\"] = True\n                previous_action[\"pattern_group_index\"] = i\n                previous_action[\"pattern_index\"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # add my_agent's current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action","e8af3d2b":"%%writefile black_belt\/memory_patterns_v7.py\nimport random\n\n\n# maximum steps in the pattern\nsteps_max = 6\n# minimum steps in the pattern\nsteps_min = 3\n# maximum amount of steps until reassessment of effectiveness of current memory patterns\nmax_steps_until_memory_reassessment = random.randint(80, 120)\n\n\n# current memory of the agent\ncurrent_memory = []\n# list of 1, 0 and -1 representing win, tie and lost results of the game respectively\n# length is max_steps_until_memory_r_t\nresults = []\n# current best sum of results\nbest_sum_of_results = 0\n# memory length of patterns in first group\n# steps_max is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = steps_max * 2\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(steps_max, steps_min - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n\n    \ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    global results\n    global best_sum_of_results\n    # action of my_agent\n    my_action = None\n    # if it's not first step, add opponent's last action to agent's current memory\n    # and reassess effectiveness of current memory patterns\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        results.append(get_step_result_for_my_agent(current_memory[-2], current_memory[-1]))\n        # if there is enough steps added to results for memery reassessment\n        if len(results) == max_steps_until_memory_reassessment:\n            results_sum = sum(results)\n            # if effectiveness of current memory patterns has decreased significantly\n            if results_sum < (best_sum_of_results * 0.5):\n                # flush all current memory patterns\n                best_sum_of_results = 0\n                results = []\n                for group in groups_of_memory_patterns:\n                    group[\"memory_patterns\"] = []\n            else:\n                # if effectiveness of current memory patterns has increased\n                if results_sum > best_sum_of_results:\n                    best_sum_of_results = results_sum\n                del results[:1]\n    for group in groups_of_memory_patterns:\n        # if length of current memory is bigger than necessary for a new memory pattern\n        if len(current_memory) > group[\"memory_length\"]:\n            # get momory of the previous step\n            previous_step_memory = current_memory[:group[\"memory_length\"]]\n            previous_pattern = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            # if such pattern already exists\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                    action[\"amount\"] += 1\n            # delete first two elements in current memory (actions of the oldest step in current memory)\n            del current_memory[:2]\n            # if action was not yet found\n            if my_action == None:\n                pattern = find_pattern(group[\"memory_patterns\"], current_memory, group[\"memory_length\"])\n                # if appropriate pattern is found\n                if pattern != None:\n                    my_action_amount = 0\n                    for action in pattern[\"opp_next_actions\"]:\n                        # if this opponent's action occurred more times than currently chosen action\n                        # or, if it occured the same amount of times and this one is choosen randomly among them\n                        if (action[\"amount\"] > my_action_amount or\n                                (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                            my_action_amount = action[\"amount\"]\n                            my_action = action[\"response\"]\n    # if no action was found\n    if my_action == None:\n        my_action = random.randint(0, 2)\n    current_memory.append(my_action)\n    return my_action","b333b0f9":"%%writefile black_belt\/iocane_powder.py\n\nimport random\n\n\ndef recall(age, hist):\n    \"\"\"Looking at the last 'age' points in 'hist', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found.\"\"\"\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    \"\"\"Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold.\"\"\"\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    \"\"\"The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         \"best\" guess so far.\"\"\"\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        \"\"\"Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies' second-guesses.\"\"\"\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        \"\"\"Returns a nested array of predictor objects, of the given dimensions.\"\"\"\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        \"\"\"The main iocaine \"move\" function.\"\"\"\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set \"best\" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don't know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We've picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act","7adf2a06":"%%writefile black_belt\/greenberg.py\n# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http:\/\/webdocs.cs.ualberta.ca\/~darse\/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http:\/\/www.mathpuzzle.com\/greenberg.c\n\n# Python translation by Travis Erdman\n# https:\/\/github.com\/erdman\/roshambo\n\n\ndef player(my_moves, opp_moves):\n    import random\n    from operator import itemgetter\n    rps_to_text = ('rock','paper','scissors')\n    rps_to_num  = {'rock':0, 'paper':1, 'scissors':2}\n    wins_with = (1,2,0)      #superior\n    best_without = (2,0,1)   #inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in range(3):\n            for j in range(24):\n                for k in range(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in range(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in range(2):\n                for k in range(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in range(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in range(3)] for k in range(4)] for j in range(24)] for l in range(50)]\n        player.r_full_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(24)] for l in range(50)]\n        player.p_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.r_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in range(24)]\n        player.r_full = [[0,0] for _ in range(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(4)] for j in range(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in zip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in range(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in range(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) \/ float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) \/ float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) \/ float(T)] for i in range(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in range(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in range(2)]\n\n    f = [[[[0,0,0] for k in range(4)] for j in range(2)] for i in range(3)]\n    t = [[[0,0,0,0] for j in range(2)] for i in range(3)]\n\n    m_len = [[0 for _ in range(T)] for i in range(3)]\n\n    for i in range(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in range(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in range(T-1,0,-1):\n        for j in range(3):\n            for k in range(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in range(3):\n        for k in range(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in range(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in range(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_num[rps_to_text[player.p_len[max_index(player.s_len)]]]\n\nopponent_hist, my_hist = [], []\nact = None\n\ndef greenberg_agent(observation, configuration):\n    global opponent_hist, my_hist, act\n    \n    rps_to_text = ('rock','paper','scissors')\n    if observation.step > 0:\n        my_hist.append(rps_to_text[act])\n        opponent_hist.append(rps_to_text[observation.lastOpponentAction])\n        \n    act = player(my_hist, opponent_hist)\n    return act","61a99f3e":"%%writefile black_belt\/testing_please_ignore.py\ncode = compile(\n    \"\"\"\nfrom collections import defaultdict\nimport operator\nimport random\nif input == \"\":\n    score  = {'RR': 0, 'PP': 0, 'SS': 0, \\\n              'PR': 1, 'RS': 1, 'SP': 1, \\\n              'RP': -1, 'SR': -1, 'PS': -1,}\n    cscore = {'RR': 'r', 'PP': 'r', 'SS': 'r', \\\n              'PR': 'b', 'RS': 'b', 'SP': 'b', \\\n              'RP': 'c', 'SR': 'c', 'PS': 'c',}\n    beat = {'P': 'S', 'S': 'R', 'R': 'P'}\n    cede = {'P': 'R', 'S': 'P', 'R': 'S'}\n    rps = ['R', 'P', 'S']\n    wlt = {1: 0, -1: 1, 0: 2}\n\n    def counter_prob(probs):\n        weighted_list = []\n        for h in rps:\n            weighted = 0\n            for p in probs.keys():\n                points = score[h + p]\n                prob = probs[p]\n                weighted += points * prob\n            weighted_list.append((h, weighted))\n\n        return max(weighted_list, key=operator.itemgetter(1))[0]\n\n    played_probs = defaultdict(lambda: 1)\n    dna_probs = [\n        defaultdict(lambda: defaultdict(lambda: 1)) for i in range(18)\n    ]\n\n    wlt_probs = [defaultdict(lambda: 1) for i in range(9)]\n\n    answers = [{'c': 1, 'b': 1, 'r': 1} for i in range(12)]\n\n    patterndict = [defaultdict(str) for i in range(6)]\n\n    consec_strat_usage = [[0] * 6, [0] * 6,\n                          [0] * 6]  #consecutive strategy usage\n    consec_strat_candy = [[], [], []]  #consecutive strategy candidates\n\n    output = random.choice(rps)\n    histories = [\"\", \"\", \"\"]\n    dna = [\"\" for i in range(12)]\n\n    sc = 0\n    strats = [[] for i in range(3)]\nelse:\n    prev_sc = sc\n\n    sc = score[output + input]\n    for j in range(3):\n        prev_strats = strats[j][:]\n        for i, c in enumerate(consec_strat_candy[j]):\n            if c == input:\n                consec_strat_usage[j][i] += 1\n            else:\n                consec_strat_usage[j][i] = 0\n        m = max(consec_strat_usage[j])\n        strats[j] = [\n            i for i, c in enumerate(consec_strat_candy[j])\n            if consec_strat_usage[j][i] == m\n        ]\n\n        for s1 in prev_strats:\n            for s2 in strats[j]:\n                wlt_probs[j * 3 + wlt[prev_sc]][chr(s1) + chr(s2)] += 1\n\n        if dna[2 * j + 0] and dna[2 * j + 1]:\n            answers[2 * j + 0][cscore[input + dna[2 * j + 0]]] += 1\n            answers[2 * j + 1][cscore[input + dna[2 * j + 1]]] += 1\n        if dna[2 * j + 6] and dna[2 * j + 7]:\n            answers[2 * j + 6][cscore[input + dna[2 * j + 6]]] += 1\n            answers[2 * j + 7][cscore[input + dna[2 * j + 7]]] += 1\n\n        for length in range(min(10, len(histories[j])), 0, -2):\n            pattern = patterndict[2 * j][histories[j][-length:]]\n            if pattern:\n                for length2 in range(min(10, len(pattern)), 0, -2):\n                    patterndict[2 * j +\n                                1][pattern[-length2:]] += output + input\n            patterndict[2 * j][histories[j][-length:]] += output + input\n    played_probs[input] += 1\n    dna_probs[0][dna[0]][input] += 1\n    dna_probs[1][dna[1]][input] += 1\n    dna_probs[2][dna[1] + dna[0]][input] += 1\n    dna_probs[9][dna[6]][input] += 1\n    dna_probs[10][dna[6]][input] += 1\n    dna_probs[11][dna[7] + dna[6]][input] += 1\n\n    histories[0] += output + input\n    histories[1] += input\n    histories[2] += output\n\n    dna = [\"\" for i in range(12)]\n    for j in range(3):\n        for length in range(min(10, len(histories[j])), 0, -2):\n            pattern = patterndict[2 * j][histories[j][-length:]]\n            if pattern != \"\":\n                dna[2 * j + 1] = pattern[-2]\n                dna[2 * j + 0] = pattern[-1]\n                for length2 in range(min(10, len(pattern)), 0, -2):\n                    pattern2 = patterndict[2 * j + 1][pattern[-length2:]]\n                    if pattern2 != \"\":\n                        dna[2 * j + 7] = pattern2[-2]\n                        dna[2 * j + 6] = pattern2[-1]\n                        break\n                break\n\n    probs = {}\n    for hand in rps:\n        probs[hand] = played_probs[hand]\n\n    for j in range(3):\n        if dna[j * 2] and dna[j * 2 + 1]:\n            for hand in rps:\n                probs[hand] *= dna_probs[j*3+0][dna[j*2+0]][hand] * \\\n                               dna_probs[j*3+1][dna[j*2+1]][hand] * \\\n                      dna_probs[j*3+2][dna[j*2+1]+dna[j*2+0]][hand]\n                probs[hand] *= answers[j*2+0][cscore[hand+dna[j*2+0]]] * \\\n                               answers[j*2+1][cscore[hand+dna[j*2+1]]]\n            consec_strat_candy[j] = [dna[j*2+0], beat[dna[j*2+0]], cede[dna[j*2+0]],\\\n                                     dna[j*2+1], beat[dna[j*2+1]], cede[dna[j*2+1]]]\n            strats_for_hand = {'R': [], 'P': [], 'S': []}\n            for i, c in enumerate(consec_strat_candy[j]):\n                strats_for_hand[c].append(i)\n            pr = wlt_probs[wlt[sc] + 3 * j]\n            for hand in rps:\n                for s1 in strats[j]:\n                    for s2 in strats_for_hand[hand]:\n                        probs[hand] *= pr[chr(s1) + chr(s2)]\n        else:\n            consec_strat_candy[j] = []\n    for j in range(3):\n        if dna[j * 2 + 6] and dna[j * 2 + 7]:\n            for hand in rps:\n                probs[hand] *= dna_probs[j*3+9][dna[j*2+6]][hand] * \\\n                               dna_probs[j*3+10][dna[j*2+7]][hand] * \\\n                      dna_probs[j*3+11][dna[j*2+7]+dna[j*2+6]][hand]\n                probs[hand] *= answers[j*2+6][cscore[hand+dna[j*2+6]]] * \\\n                               answers[j*2+7][cscore[hand+dna[j*2+7]]]\n\n    output = counter_prob(probs)\n\"\"\", '<string>', 'exec')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg['input'] = inp\n    exec(code, gg)\n    return {'R': 0, 'P': 1, 'S': 2}[gg['output']]","8167eefb":"%%writefile black_belt\/IOU2.py\nimport random\n\nclass Strategy:\n  def __init__(self):\n    # 2 different self.lengths of history, 3 kinds of history, both, mine, yours\n    # 3 different self.limit self.length of reverse learning\n    # 6 kinds of strategy based on Iocaine Powder\n    self.num_predictor = 27\n\n\n    self.len_rfind = [20]\n    self.limit = [10,20,60]\n    self.beat = { \"R\":\"P\" , \"P\":\"S\", \"S\":\"R\"}\n    self.not_lose = { \"R\":\"PPR\" , \"P\":\"SSP\" , \"S\":\"RRS\" } #50-50 chance\n    self.my_his   =\"\"\n    self.your_his =\"\"\n    self.both_his =\"\"\n    self.list_predictor = [\"\"]*self.num_predictor\n    self.length = 0\n    self.temp1 = { \"PP\":\"1\" , \"PR\":\"2\" , \"PS\":\"3\",\n              \"RP\":\"4\" , \"RR\":\"5\", \"RS\":\"6\",\n              \"SP\":\"7\" , \"SR\":\"8\", \"SS\":\"9\"}\n    self.temp2 = { \"1\":\"PP\",\"2\":\"PR\",\"3\":\"PS\",\n                \"4\":\"RP\",\"5\":\"RR\",\"6\":\"RS\",\n                \"7\":\"SP\",\"8\":\"SR\",\"9\":\"SS\"} \n    self.who_win = { \"PP\": 0, \"PR\":1 , \"PS\":-1,\n                \"RP\": -1,\"RR\":0, \"RS\":1,\n                \"SP\": 1, \"SR\":-1, \"SS\":0}\n    self.score_predictor = [0]*self.num_predictor\n    self.output = random.choice(\"RPS\")\n    self.predictors = [self.output]*self.num_predictor\n\n\n  def prepare_next_move(self, prev_input):\n    input = prev_input\n\n    #update self.predictors\n    #\"\"\"\n    if len(self.list_predictor[0])<5:\n        front =0\n    else:\n        front =1\n    for i in range (self.num_predictor):\n        if self.predictors[i]==input:\n            result =\"1\"\n        else:\n            result =\"0\"\n        self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n    #history matching 1-6\n    self.my_his += self.output\n    self.your_his += input\n    self.both_his += self.temp1[input+self.output]\n    self.length +=1\n    for i in range(1):\n        len_size = min(self.length,self.len_rfind[i])\n        j=len_size\n        #self.both_his\n        while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[0+6*i] = self.your_his[j+k]\n            self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[0+6*i] = random.choice(\"RPS\")\n            self.predictors[1+6*i] = random.choice(\"RPS\")\n        j=len_size\n        #self.your_his\n        while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[2+6*i] = self.your_his[j+k]\n            self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[2+6*i] = random.choice(\"RPS\")\n            self.predictors[3+6*i] = random.choice(\"RPS\")\n        j=len_size\n        #self.my_his\n        while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[4+6*i] = self.your_his[j+k]\n            self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[4+6*i] = random.choice(\"RPS\")\n            self.predictors[5+6*i] = random.choice(\"RPS\")\n\n    for i in range(3):\n        temp =\"\"\n        search = self.temp1[(self.output+input)] #last round\n        for start in range(2, min(self.limit[i],self.length) ):\n            if search == self.both_his[self.length-start]:\n                temp+=self.both_his[self.length-start+1]\n        if(temp==\"\"):\n            self.predictors[6+i] = random.choice(\"RPS\")\n        else:\n            collectR = {\"P\":0,\"R\":0,\"S\":0} #take win\/lose from opponent into account\n            for sdf in temp:\n                next_move = self.temp2[sdf]\n                if(self.who_win[next_move]==-1):\n                    collectR[self.temp2[sdf][1]]+=3\n                elif(self.who_win[next_move]==0):\n                    collectR[self.temp2[sdf][1]]+=1\n                elif(self.who_win[next_move]==1):\n                    collectR[self.beat[self.temp2[sdf][0]]]+=1\n            max1 = -1\n            p1 =\"\"\n            for key in collectR:\n                if(collectR[key]>max1):\n                    max1 = collectR[key]\n                    p1 += key\n            self.predictors[6+i] = random.choice(p1)\n    \n    #rotate 9-27:\n    for i in range(9,27):\n        self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        \n    #choose a predictor\n    len_his = len(self.list_predictor[0])\n    for i in range(self.num_predictor):\n        sum = 0\n        for j in range(len_his):\n            if self.list_predictor[i][j]==\"1\":\n                sum+=(j+1)*(j+1)\n            else:\n                sum-=(j+1)*(j+1)\n        self.score_predictor[i] = sum\n    max_score = max(self.score_predictor)\n    #min_score = min(self.score_predictor)\n    #c_temp = {\"R\":0,\"P\":0,\"S\":0}\n    #for i in range (self.num_predictor):\n        #if self.score_predictor[i]==max_score:\n        #    c_temp[self.predictors[i]] +=1\n        #if self.score_predictor[i]==min_score:\n        #    c_temp[self.predictors[i]] -=1\n    if max_score>0:\n        predict = self.predictors[self.score_predictor.index(max_score)]\n    else:\n        predict = random.choice(self.your_his)\n    self.output = random.choice(self.not_lose[predict])\n    return self.output\n\n\nglobal GLOBAL_STRATEGY\nGLOBAL_STRATEGY = Strategy()\n\n\ndef agent(observation, configuration):\n  global GLOBAL_STRATEGY\n\n  # Action mapping\n  to_char = [\"R\", \"P\", \"S\"]\n  from_char = {\"R\": 0, \"P\": 1, \"S\": 2}\n\n  if observation.step > 0:\n    GLOBAL_STRATEGY.prepare_next_move(to_char[observation.lastOpponentAction])\n  action = from_char[GLOBAL_STRATEGY.output]\n  return action","70879b3f":"%%writefile black_belt\/dllu1.py\ncode = compile(\n    \"\"\"\n# see also www.dllu.net\/rps\n# remember, rpsrunner.py is extremely useful for offline testing, \n# here's a screenshot: http:\/\/i.imgur.com\/DcO9M.png\nimport random\nnumPre = 30\nnumMeta = 6\nif not input:\n    limit = 8\n    beat={'R':'P','P':'S','S':'R'}\n    moves=['','','','']\n    pScore=[[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre]\n    centrifuge={'RP':0,'PS':1,'SR':2,'PR':3,'SP':4,'RS':5,'RR':6,'PP':7,'SS':8}\n    centripete={'R':0,'P':1,'S':2}\n    soma = [0,0,0,0,0,0,0,0,0];\n    rps = [1,1,1];\n    a=\"RPS\"\n    best = [0,0,0];\n    length=0\n    p=[random.choice(\"RPS\")]*numPre\n    m=[random.choice(\"RPS\")]*numMeta\n    mScore=[5,2,5,2,4,2]\nelse:\n    for i in range(numPre):\n        pp = p[i]\n        bpp = beat[pp]\n        bbpp = beat[bpp]\n        pScore[0][i]=0.9*pScore[0][i]+((input==pp)-(input==bbpp))*3\n        pScore[1][i]=0.9*pScore[1][i]+((output==pp)-(output==bbpp))*3\n        pScore[2][i]=0.87*pScore[2][i]+(input==pp)*3.3-(input==bpp)*1.2-(input==bbpp)*2.3\n        pScore[3][i]=0.87*pScore[3][i]+(output==pp)*3.3-(output==bpp)*1.2-(output==bbpp)*2.3\n        pScore[4][i]=(pScore[4][i]+(input==pp)*3)*(1-(input==bbpp))\n        pScore[5][i]=(pScore[5][i]+(output==pp)*3)*(1-(output==bbpp))\n    for i in range(numMeta):\n        mScore[i]=0.96*(mScore[i]+(input==m[i])-(input==beat[beat[m[i]]]))\n    soma[centrifuge[input+output]] +=1;\n    rps[centripete[input]] +=1;\n    moves[0]+=str(centrifuge[input+output])\n    moves[1]+=input\n    moves[2]+=output\n    length+=1\n    for y in range(3):\n        j=min([length,limit])\n        while j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n            j-=1\n        i = moves[y].rfind(moves[y][length-j:length],0,length-1)\n        p[0+2*y] = moves[1][j+i] \n        p[1+2*y] = beat[moves[2][j+i]]\n    j=min([length,limit])\n    while j>=2 and not moves[0][length-j:length-1] in moves[0][0:length-2]:\n        j-=1\n    i = moves[0].rfind(moves[0][length-j:length-1],0,length-2)\n    if j+i>=length:\n        p[6] = p[7] = random.choice(\"RPS\")\n    else:\n        p[6] = moves[1][j+i] \n        p[7] = beat[moves[2][j+i]]\n        \n    best[0] = soma[centrifuge[output+'R']]*rps[0]\/rps[centripete[output]]\n    best[1] = soma[centrifuge[output+'P']]*rps[1]\/rps[centripete[output]]\n    best[2] = soma[centrifuge[output+'S']]*rps[2]\/rps[centripete[output]]\n    p[8] = p[9] = a[best.index(max(best))]\n    \n    for i in range(10,numPre):\n        p[i]=beat[beat[p[i-10]]]\n        \n    for i in range(0,numMeta,2):\n        m[i]=       p[pScore[i  ].index(max(pScore[i  ]))]\n        m[i+1]=beat[p[pScore[i+1].index(max(pScore[i+1]))]]\noutput = beat[m[mScore.index(max(mScore))]]\nif max(mScore)<0.07 or random.randint(3,40)>length:\n    output=beat[random.choice(\"RPS\")]\n\"\"\", '<string>', 'exec')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg['input'] = inp\n    exec(code, gg)\n    return {'R': 0, 'P': 1, 'S': 2}[gg['output']]","a22b1746":"%%writefile black_belt\/centrifugal_bumblepuppy_v4.py\ncode = compile(\n    \"\"\"\n#                         WoofWoofWoof\n#                     Woof            Woof\n#                Woof                      Woof\n#              Woof                          Woof\n#             Woof  Centrifugal Bumble-puppy  Woof\n#              Woof                          Woof\n#                Woof                      Woof\n#                     Woof            Woof\n#                         WoofWoofWoof\n\nimport random\n\nnumber_of_predictors = 60 #yes, this really has 60 predictors.\nnumber_of_metapredictors = 4 #actually, I lied! This has 240 predictors.\n\n\nif not input:\n\tlimits = [50,20,6]\n\tbeat={'R':'P','P':'S','S':'R'}\n\turmoves=\"\"\n\tmymoves=\"\"\n\tDNAmoves=\"\"\n\toutputs=[random.choice(\"RPS\")]*number_of_metapredictors\n\tpredictorscore1=[3]*number_of_predictors\n\tpredictorscore2=[3]*number_of_predictors\n\tpredictorscore3=[3]*number_of_predictors\n\tpredictorscore4=[3]*number_of_predictors\n\tnuclease={'RP':'a','PS':'b','SR':'c','PR':'d','SP':'e','RS':'f','RR':'g','PP':'h','SS':'i'}\n\tlength=0\n\tpredictors=[random.choice(\"RPS\")]*number_of_predictors\n\tmetapredictors=[random.choice(\"RPS\")]*number_of_metapredictors\n\tmetapredictorscore=[3]*number_of_metapredictors\nelse:\n\n\tfor i in range(number_of_predictors):\n\t\t#metapredictor 1\n\t\tpredictorscore1[i]*=0.8\n\t\tpredictorscore1[i]+=(input==predictors[i])*3\n\t\tpredictorscore1[i]-=(input==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 2: beat metapredictor 1 (probably contains a bug)\n\t\tpredictorscore2[i]*=0.8\n\t\tpredictorscore2[i]+=(output==predictors[i])*3\n\t\tpredictorscore2[i]-=(output==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 3\n\t\tpredictorscore3[i]+=(input==predictors[i])*3\n\t\tif input==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore3[i]=0\n\t\t#metapredictor 4: beat metapredictor 3 (probably contains a bug)\n\t\tpredictorscore4[i]+=(output==predictors[i])*3\n\t\tif output==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore4[i]=0\n\t\t\t\n\tfor i in range(number_of_metapredictors):\n\t\tmetapredictorscore[i]*=0.96\n\t\tmetapredictorscore[i]+=(input==metapredictors[i])*3\n\t\tmetapredictorscore[i]-=(input==beat[beat[metapredictors[i]]])*3\n\t\t\n\t\n\t#Predictors 1-18: History matching\n\turmoves+=input\t\t\n\tmymoves+=output\n\tDNAmoves+=nuclease[input+output]\n\tlength+=1\n\t\n\tfor z in range(3):\n\t\tlimit = min([length,limits[z]])\n\t\tj=limit\n\t\twhile j>=1 and not DNAmoves[length-j:length] in DNAmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = DNAmoves.rfind(DNAmoves[length-j:length],0,length-1) \n\t\t\tpredictors[0+6*z] = urmoves[j+i] \n\t\t\tpredictors[1+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\t\t\t\n\t\twhile j>=1 and not urmoves[length-j:length] in urmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = urmoves.rfind(urmoves[length-j:length],0,length-1) \n\t\t\tpredictors[2+6*z] = urmoves[j+i] \n\t\t\tpredictors[3+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\n\t\twhile j>=1 and not mymoves[length-j:length] in mymoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = mymoves.rfind(mymoves[length-j:length],0,length-1) \n\t\t\tpredictors[4+6*z] = urmoves[j+i] \n\t\t\tpredictors[5+6*z] = beat[mymoves[j+i]]\n\t#Predictor 19,20: RNA Polymerase\t\t\n\tL=len(mymoves)\n\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\twhile i==-1:\n\t\tj-=1\n\t\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\t\tif j<2:\n\t\t\tbreak\n\tif i==-1 or j+i>=L:\n\t\tpredictors[18]=predictors[19]=random.choice(\"RPS\")\n\telse:\n\t\tpredictors[18]=beat[mymoves[j+i]]\n\t\tpredictors[19]=urmoves[j+i]\n\n\t#Predictors 21-60: rotations of Predictors 1:20\n\tfor i in range(20,60):\n\t\tpredictors[i]=beat[beat[predictors[i-20]]] #Trying to second guess me?\n\t\n\tmetapredictors[0]=predictors[predictorscore1.index(max(predictorscore1))]\n\tmetapredictors[1]=beat[predictors[predictorscore2.index(max(predictorscore2))]]\n\tmetapredictors[2]=predictors[predictorscore3.index(max(predictorscore3))]\n\tmetapredictors[3]=beat[predictors[predictorscore4.index(max(predictorscore4))]]\n\t\n\t#compare predictors\noutput = beat[metapredictors[metapredictorscore.index(max(metapredictorscore))]]\nif max(metapredictorscore)<0:\n\toutput = beat[random.choice(urmoves)]\n\"\"\", '<string>', 'exec')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg['input'] = inp\n    exec(code, gg)\n    return {'R': 0, 'P': 1, 'S': 2}[gg['output']]","c66ad850":"my_agent = 'black_belt\/centrifugal_bumblepuppy_v4.py'","e9ee5131":"import os\nimport pandas as pd\nimport kaggle_environments\nfrom datetime import datetime\nimport multiprocessing as pymp\nfrom tqdm import tqdm\nimport ray.util.multiprocessing as raymp\n\n\n# function to return score\ndef get_result(match_settings):\n    start = datetime.now()\n    outcomes = kaggle_environments.evaluate(\n        'rps', [match_settings[0], match_settings[1]], num_episodes=match_settings[2])\n    won, lost, tie, avg_score = 0, 0, 0, 0.\n    for outcome in outcomes:\n        score = outcome[0]\n        if score > 0: won += 1\n        elif score < 0: lost += 1\n        else: tie += 1\n        avg_score += score\n    elapsed = datetime.now() - start\n    return match_settings[1], won, lost, tie, elapsed, float(avg_score) \/ float(match_settings[2])\n\n\ndef eval_agent_against_baselines(agent, baselines, num_episodes=10, use_ray=False):\n    df = pd.DataFrame(\n        columns=['wins', 'loses', 'ties', 'total time', 'avg. score'],\n        index=baselines\n    )\n    \n    if use_ray:\n        pool = raymp.Pool()\n    else:\n        pool = pymp.Pool()\n    matches = [[agent, baseline, num_episodes] for baseline in baselines]\n    \n    results = []\n    for content in tqdm(pool.imap_unordered(get_result, matches), total=len(matches)):\n        results.append(content)\n\n    for baseline_agent, won, lost, tie, elapsed, avg_score in results:\n        df.loc[baseline_agent, 'wins'] = won\n        df.loc[baseline_agent, 'loses'] = lost\n        df.loc[baseline_agent, 'ties'] = tie\n        df.loc[baseline_agent, 'total time'] = elapsed\n        df.loc[baseline_agent, 'avg. score'] = avg_score\n        \n    return df","324dc03a":"%%time\nwhite_belt_agents = [os.path.join('white_belt', agent) for agent in os.listdir('white_belt')]\neval_agent_against_baselines(my_agent, white_belt_agents)","c9aa0e0a":"%%time\nblue_belt_agents = [os.path.join('blue_belt', agent) for agent in os.listdir('blue_belt')]\neval_agent_against_baselines(my_agent, blue_belt_agents)","d58bd801":"%%time\nblack_belt_agents = [os.path.join('black_belt', agent) for agent in os.listdir('black_belt')]\neval_agent_against_baselines(my_agent, black_belt_agents)","c510d836":"!rm history.csv\n!rm bandit.json","00c619c0":"<h1 style='background:white; border:3px solid; color:black'><center>White Belt Baselines<\/center><\/h1>\n\n<br>\n\nThese agents are extremely simple and can be used as a minimum testing pool for your agent. A decent agent should beat all of the \"white belt\" baseline ones in 100% of matches. The following agents were added in the \"simple\" category:\n- **`white_belt\/all_rock.py`** &mdash; plays only Rock\n- **`white_belt\/all_paper.py`** &mdash; plays only Paper\n- **`white_belt\/all_scissors.py`** &mdash; plays only Scissors\n- **`white_belt\/reactionary.py`** &mdash; hit the last action of the opponent\n- **`white_belt\/counter_reactionary.py`** &mdash; counter strategy to reactionary agent\n- **`white_belt\/mirror.py`** &mdash; mirrors the opponent's last moves\n- **`white_belt\/mirror_shift_1.py`** &mdash; mirrors the opponent's last moves and shift by 1\n- **`white_belt\/mirror_shift_1.py`** &mdash; mirrors the opponent's last moves and shift by 2\n- **`white_belt\/de_bruijn.py`** &mdash; from the [Rock Paper Scissors - De Bruijn Sequence](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-de-bruijn-sequence) notebook","2e5f689e":"<h1 style='background:blue; border:3px solid; border-color: blue; color:white'><center>Blue Belt Baselines<\/center><\/h1>\n\n<br>\n\nYou also need to beat these in 100% of the time if you want any chance to rise to the \"bronze\" range, because the LB is probably already filled with those agents. The following agents were added in the \"Blue Belt\" category:\n\n- **`blue_belt\/statistical.py`** &mdash; monitors the move statistics and tries to counter the most frequent move\n- **`blue_belt\/not_so_markov.py`** &mdash; from [(Not so) Markov](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov) (v5)\n- **`blue_belt\/transition_matrix.py`** &mdash; from [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix) notebook (v2)\n- **`blue_belt\/stochastic_transition_matrix.py`** &mdash; from [RPS - Stochastic Transition Matrix](https:\/\/www.kaggle.com\/peternagymathe\/rps-stochastic-transition-matrix)\n- **`blue_belt\/decision_tree.py`** &mdash; from [Decision Tree Classifier](https:\/\/www.kaggle.com\/alexandersamarin\/decision-tree-classifier?scriptVersionId=46415861) (v4)\n- **`blue_belt\/xgboost`** &mdash; from [XGBoost For Predicting Opponent's Action](https:\/\/www.kaggle.com\/ollyattwood\/xgboost-for-predicting-opponent-s-action) (v1).\n- **`blue_belt\/statistical_prediction.py`** &mdash; from [Rock Paper Scissors - Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction) (v17)\n- **`blue_belt\/rfind.py`** &mdash; from [Running RPSContest bots](https:\/\/www.kaggle.com\/purplepuppy\/running-rpscontest-bots)","e8642d6f":"## Against Black Belt Agents\n\nTesting against black belt baselines is where the real fun begins...","f478b0e8":"<h1 style='background:black; border:3px solid; border-color: black; color:white'><center>Black Belt Baselines<\/center><\/h1>\n\n<br>\n\nThese are agents that have very nice standing on the leaderboard (near the bronze band and even above). The following agents were added to the \"Black Belt\" category:\n\n- **`black_belt\/multi_armed_bandit_v15.py`** &mdash; from [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents) (v15)\n- **`black_belt\/multi_armed_bandit_v32.py`** &mdash; from [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents) (v32)\n- **`black_belt\/memory_patterns_v20.py`** &mdash; from [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns) (v20)\n- **`black_belt\/memory_patterns_v7.py`** &mdash; from [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns) (v7)\n- **`black_belt\/iocane_powder.py`** &mdash; from [RPS: RoShamBo Comp - Iocaine Powder](https:\/\/www.kaggle.com\/group16\/rps-roshambo-comp-iocaine-powder)\n- **`black_belt\/greenberg`** &mdash; from [RPS: RoShamBo Competition - Greenberg](https:\/\/www.kaggle.com\/group16\/rps-roshambo-competition-greenberg)\n- **`black_belt\/testing_please_ignore.py`** &mdash; from [Running RPSContest bots](https:\/\/www.kaggle.com\/purplepuppy\/running-rpscontest-bots)\n- **`black_belt\/IOU2.py`** &mdash; from [RPSContest - IO2_fightinguuu](http:\/\/www.rpscontest.com\/entry\/885001)\n- **`black_belt\/dllu1.py`** &mdash; from [RPSContest - dllu1](http:\/\/www.rpscontest.com\/entry\/498002)\n- **`black_belt\/centrifugal_bumblepuppy_4.py`** &mdash; from [RPSContest - Centrifugal BumblePuppy 4](centrifugal_bumblepuppy_v4)\n\nNote that I added different versions of the same agent because they are all pretty strong, and ideally you would want your agent to be able to beat all of them if you are aiming for bronze+ places.\n\nPS: the `black_belt\/memory_patterns_v7.py` agent is **freaking strong** &mdash; I was able to beat other versions of \"memory patterns\" agents locally, except version 7. Well done, [@yegorbiryukov](https:\/\/www.kaggle.com\/yegorbiryukov). `black_belt\/multi_armed_bandit_v15.py` is also the one in the silver zone right now.","2a9f7860":"# Rock-Paper-Scissors Dojo\n\n![Dojo](https:\/\/i.imgur.com\/jeWU2Ea.png[\/img])\n\nIt is very important to keep a diverse pool of agents to test your new agent with. In this notebook, I collected a bunch of agents from public notebooks to form a \"Dojo\" where you can test your agent before submitting.\n\nI am also planning to add my own agents that are weaker than my flagship ones in the future. This notebook can be attached to your notebooks as a dataset. At the end of this notebook, you can also find a simple code for agents comparison.\n\n### Update 11\/12:\n\n- Added \"Centrifugal Bumblepuppy 4\", \"dllu1\", and \"IOU2\" bots from RPSContest. They're quite strong (one of them was actually the winner of the 2011 contest).\n- Added multi-processing evaluation (taken from [RPS - Multiprocessing Agent Comparisons](https:\/\/www.kaggle.com\/booooooow\/rps-multiprocessing-agent-comparisons) notebook)","0945b93a":"# Evaluation Example\n\nThis is an example of how you can evaluate your own model using the baseline agents in this Dojo. In this example, we will evaluate `black_belt\/testing_please_ignore.py` agent against the other ones.\n\nThe evaluation code uses **multi processing** (either with **ray** or python's built-in), as in [RPS - Multiprocessing Agent Comparisons](https:\/\/www.kaggle.com\/booooooow\/rps-multiprocessing-agent-comparisons) notebook.","81c6d72d":"## Against White Belt Baselines\n\nAs we can see, our \"black belt\" baseline beats the white belt ones in 100% of cases easily.","af06436c":"## Against Blue Belt Agents\n\nSame for blue belt ones &mdash; our black belt agent is still able to beat them in (almost) majority of cases."}}