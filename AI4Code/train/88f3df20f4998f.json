{"cell_type":{"5c7e9320":"code","62990151":"code","3242a62d":"code","ab52a082":"code","714c4787":"code","24ef8b4f":"code","dd2eb3b6":"code","8daf455d":"code","8bc2def5":"code","563415fb":"code","aca3ce20":"code","f32b80a0":"code","9991f5b1":"code","be1af387":"code","e5141040":"code","043184fc":"code","bf53b3a6":"code","930696d4":"code","e9435dd3":"code","40c900b3":"code","67023cf1":"code","af4200ed":"code","9aaff3ab":"code","ce7ecd32":"code","a08c5dae":"code","735b7f23":"code","da67fc58":"code","53658739":"code","28d392ba":"code","440292cb":"code","c021fe23":"code","33588072":"markdown","849bcd53":"markdown","6403fb46":"markdown","ff23b6b5":"markdown","06208823":"markdown","657ae117":"markdown","1240f4b0":"markdown","701c8b95":"markdown","94acc357":"markdown"},"source":{"5c7e9320":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn","62990151":"df=pd.read_csv('..\/input\/admission\/Admission_Predict_Ver1.1.csv')\ndf.head()","3242a62d":"# Checking for Null Values\ndf.drop('Serial No.',axis=1,inplace=True)\ndf.isnull().sum()","ab52a082":"df=df.rename(columns={'GRE Score':'gre_score'})\ndf=df.rename(columns={'TOEFL Score':'toefl_score'})\ndf=df.rename(columns={'University Rating':'uni_rating'})\ndf=df.rename(columns={'Chance of Admit ':'chance'})\ndf=df.rename(columns={'LOR ':'LOR'})\ndf.describe()\n#By looking at the minimum and maximum values for each feature we can conclude that there are no invalid entries.","714c4787":"sns.jointplot(x='gre_score',y='chance',data=df,kind='reg')\n# we can see a direct and linear relationship between GRE Scores and Chances of Admission","24ef8b4f":"sns.jointplot(x='toefl_score',y='chance',data=df,kind='reg')\n# we observe a similar relationship for TOEFL Scores","dd2eb3b6":"sns.jointplot(x='uni_rating',y='chance',data=df,kind='scatter')\n# this plot is kind of difficult to interpret, hence we try a different approach","8daf455d":"#computing the average chances for each university rating category\nfor i in set(df.uni_rating):\n    plt.bar(i,df[df.uni_rating==i].chance.mean()*100)\nplt.ylabel('% chance of admission')\nplt.xlabel('University Rating (Out of 5)')\nax=plt.gca()\nax.set_ylim([40,100])\n#We can say that the chance of admission is higher with higher university rating.","8bc2def5":"for i in set(df.SOP):\n    plt.bar(i,df[df.SOP==i].chance.mean()*100,width=0.45)\nplt.ylabel('% chance of admission')\nplt.xlabel('SOP Rating (Out of 5)')\nax=plt.gca()\nax.set_ylim([40,100])\n#We can say that the chance of admission is higher with higher SOP rating.","563415fb":"for i in set(df.LOR):\n    plt.bar(i,df[df.LOR==i].chance.mean()*100,width=0.45)\nplt.ylabel('% chance of admission')\nplt.xlabel('LOR Rating (Out of 5)')\nax=plt.gca()\nax.set_ylim([40,100])\n#We can say that the chance of admission is higher with higher LOR rating.","aca3ce20":"sns.jointplot(x='CGPA',y='chance',data=df,kind='scatter')\n# we can see a direct and linear relationship between CGPA Scores and Chances of Admission","f32b80a0":"for i in set(df.Research):\n    plt.bar(i,df[df.Research==i].chance.mean()*100)\nplt.ylabel('% chance of admission')\nplt.xlabel('LOR Rating (Out of 5)')\nax=plt.gca()\nax.set_ylim([50,100])","9991f5b1":"df.head(7)","be1af387":"#the GRE score rating starts from 260 (base score), so to get a more accurate measure of a student's performance we transform it\ndf['gre_score']=df['gre_score']-260\ndf.head(3)","e5141040":"#making the GRE score within the range of 0 to 5\ndf['gre_score']=df['gre_score']*5\/80\ndf.head(4)","043184fc":"#we take the minimum TOEFL score to be 60\ndf['toefl_score']=df['toefl_score']-60\ndf.head(3)","bf53b3a6":"df['toefl_score']=df['toefl_score']*5\/60\ndf.head(3)","930696d4":"df['CGPA']=df['CGPA']\/2\ndf.head(3)","e9435dd3":"df.describe()","40c900b3":"X=df.drop('chance',axis=1)\ny=df['chance']","67023cf1":"X_train=X.loc[0:399,:]\nX_test=X.loc[400:,:]\ny_train=y.loc[0:399]\ny_test=y.loc[400:]\n\nprint('X Train Data Count:',X_train.shape[0])\nprint('X Test Data Count:',X_test.shape[0])\nprint('y Train Data Count:',y_train.shape[0])\nprint('y Test Data Count:',y_test.shape[0])","af4200ed":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","9aaff3ab":"reg=linear_model.LinearRegression()\nreg.fit(X_train,y_train)\npredictions=reg.predict(X_test)\nprint('For Linear Regression model, RMSE=',sqrt(mean_squared_error(predictions,y_test)))\nplt.scatter(predictions,y_test)\nplt.plot([0,1],[0,1],color='red')\nax=plt.gca()\nax.set_ylim([0.35,1])\nax.set_xlim([0.35,1])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Actual vs Predicted (For Test Data)')\n# Closer the points to the red line, better the prediction of the model","ce7ecd32":"from sklearn.ensemble import RandomForestRegressor\nrf_random=RandomForestRegressor()\nrf_random.fit(X_train,y_train)\npredictions=rf_random.predict(X_test)\nprint('For RandomForest Regression model, RMSE=',sqrt(mean_squared_error(predictions,y_test)))\nplt.scatter(predictions,y_test)\nplt.plot([0,1],[0,1],color='red')\nax=plt.gca()\nax.set_ylim([0.35,1])\nax.set_xlim([0.35,1])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Actual vs Predicted (For Test Data)')","a08c5dae":"#We observe that the linear regression model is better. But, we can try to improve the performance using hyperparameter tuning.\nestimators=[]\nfor i in range(1,70,6):\n    estimators.append(i)\nfor i in estimators:\n    rf=RandomForestRegressor(n_estimators=i)\n    rf.fit(X_train,y_train)\n    print('For RandomForest Regression model with',i,'estimators,','RMSE=',sqrt(mean_squared_error(rf.predict(X_test),y_test)))","735b7f23":"from sklearn.linear_model import Ridge\nridge=Ridge()\nridge.fit(X_train,y_train)\npredictions=ridge.predict(X_test)\nprint('For Ridge Regression model, RMSE=',sqrt(mean_squared_error(predictions,y_test)))\nplt.scatter(predictions,y_test)\nplt.plot([0,1],[0,1],color='red')\nax=plt.gca()\nax.set_ylim([0.35,1])\nax.set_xlim([0.35,1])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Actual vs Predicted (For Test Data)')","da67fc58":"from sklearn.model_selection import GridSearchCV\nridge=Ridge()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,35,40,45,50,55]}\nridge_regressor=GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\nridge_regressor.fit(X_train,y_train)\npredictions=ridge_regressor.predict(X_test)\nprint('For Ridge Regression model, RMSE=',sqrt(mean_squared_error(predictions,y_test)))\nprint('Best Alpha:',ridge_regressor.best_params_)\nplt.scatter(predictions,y_test)\nplt.plot([0,1],[0,1],color='red')\nax=plt.gca()\nax.set_ylim([0.35,1])\nax.set_xlim([0.35,1])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Actual vs Predicted (For Test Data)')","53658739":"from sklearn.linear_model import Lasso\nlasso=Lasso()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,35,40,45,50,55]}\nlasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\nlasso_regressor.fit(X_train,y_train)\npredictions=lasso_regressor.predict(X_test)\nprint('For Lasso Regression model, RMSE=',sqrt(mean_squared_error(predictions,y_test)))\nprint('Best Alpha:',lasso_regressor.best_params_)\nplt.scatter(predictions,y_test)\nplt.plot([0,1],[0,1],color='red')\nax=plt.gca()\nax.set_ylim([0.35,1])\nax.set_xlim([0.35,1])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Actual vs Predicted (For Test Data)')","28d392ba":"results=pd.DataFrame(columns=['Actual Chance'],data=y_test)\nresults['Actual Chance']=y_test\nresults['Predicted Chance']=predictions\nresults.reset_index(inplace=True,drop=True)\nresults.head(4)","440292cb":"rows,columns=results.shape\nfor i in range(0,rows):\n    results.iloc[i,1]=float(format(results.iloc[i,1],'.2f'))\nresults.head(4)","c021fe23":"print('RMSE(Test):',sqrt(mean_squared_error(results['Actual Chance'],results['Predicted Chance'])))","33588072":"### Random Forest Regressor (with default parameters)","849bcd53":"## Feature scaling","6403fb46":"### Data dependencies and analysis","ff23b6b5":"### Ridge regression using GridSearchCV","06208823":"# RMSE: 0.04288","657ae117":"### Ridge regression (with default parameters)","1240f4b0":"#### So we can conclude that all the features have a direct and near-linear relationship with the chance of admission","701c8b95":"### Linear Regression model.","94acc357":"### Dataset splitting"}}