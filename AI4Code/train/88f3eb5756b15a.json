{"cell_type":{"5746ac09":"code","f6feda55":"code","9ce157a6":"code","74278e38":"code","c3b24101":"code","a49fce38":"code","96288bb9":"code","a98a0e28":"code","7434c110":"code","41d0f35d":"code","4bdcaaf6":"code","032ebd3d":"code","9f78f8d9":"markdown","50e71534":"markdown","55d4dbac":"markdown","f9e4760e":"markdown","43f6e32b":"markdown","bfa37c67":"markdown","95ddda07":"markdown","5740d55f":"markdown","a14af2a2":"markdown"},"source":{"5746ac09":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom xgboost import XGBClassifier\nimport pdb","f6feda55":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge\/')\ntraining_path = data_path \/ 'training'\nevaluation_path = data_path \/ 'evaluation'\ntest_path = data_path \/ 'test'","9ce157a6":"def plot_result(test_input, test_prediction,\n                input_shape):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 2, figsize=(15,15))\n    test_input = test_input.reshape(input_shape[0],input_shape[1])\n    axs[0].imshow(test_input, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Actual Target')\n    test_prediction = test_prediction.reshape(input_shape[0],input_shape[1])\n    axs[1].imshow(test_prediction, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Model Prediction')\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_test(test_prediction, task_name):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 1, figsize=(15,15))\n    axs.imshow(test_prediction, cmap=cmap, norm=norm)\n    axs.axis('off')\n    axs.set_title(f'Test Prediction {task_name}')\n    plt.tight_layout()\n    plt.show()","74278e38":"# https:\/\/www.kaggle.com\/inversion\/abstraction-and-reasoning-starter-notebook\ndef flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","c3b24101":"sample_sub = pd.read_csv(data_path\/'sample_submission.csv')\nsample_sub = sample_sub.set_index('output_id')\nsample_sub.head()","a49fce38":"def get_moore_neighbours(color, cur_row, cur_col, nrows, ncols):\n\n    if cur_row<=0: top = -1\n    else: top = color[cur_row-1][cur_col]\n        \n    if cur_row>=nrows-1: bottom = -1\n    else: bottom = color[cur_row+1][cur_col]\n        \n    if cur_col<=0: left = -1\n    else: left = color[cur_row][cur_col-1]\n        \n    if cur_col>=ncols-1: right = -1\n    else: right = color[cur_row][cur_col+1]\n        \n    return top, bottom, left, right\n\ndef get_tl_tr(color, cur_row, cur_col, nrows, ncols):\n        \n    if cur_row==0:\n        top_left = -1\n        top_right = -1\n    else:\n        if cur_col==0: top_left=-1\n        else: top_left = color[cur_row-1][cur_col-1]\n        if cur_col==ncols-1: top_right=-1\n        else: top_right = color[cur_row-1][cur_col+1]   \n        \n    return top_left, top_right","96288bb9":"def make_features(input_color, nfeat):\n    nrows, ncols = input_color.shape\n    feat = np.zeros((nrows*ncols,nfeat))\n    cur_idx = 0\n    for i in range(nrows):\n        for j in range(ncols):\n            feat[cur_idx,0] = i\n            feat[cur_idx,1] = j\n            feat[cur_idx,2] = input_color[i][j]\n            feat[cur_idx,3:7] = get_moore_neighbours(input_color, i, j, nrows, ncols)\n            feat[cur_idx,7:9] = get_tl_tr(input_color, i, j, nrows, ncols)\n            feat[cur_idx,9] = len(np.unique(input_color[i,:]))\n            feat[cur_idx,10] = len(np.unique(input_color[:,j]))\n            feat[cur_idx,11] = (i+j)\n            feat[cur_idx,12] = len(np.unique(input_color[i-local_neighb:i+local_neighb,\n                                                         j-local_neighb:j+local_neighb]))\n\n            cur_idx += 1\n        \n    return feat\n","a98a0e28":"def features(task, mode='train'):\n    num_train_pairs = len(task[mode])\n    feat, target = [], []\n    \n    global local_neighb\n    for task_num in range(num_train_pairs):\n        input_color = np.array(task[mode][task_num]['input'])\n        target_color = task[mode][task_num]['output']\n        nrows, ncols = len(task[mode][task_num]['input']), len(task[mode][task_num]['input'][0])\n\n        target_rows, target_cols = len(task[mode][task_num]['output']), len(task[mode][task_num]['output'][0])\n        \n        if (target_rows!=nrows) or (target_cols!=ncols):\n            print('Number of input rows:',nrows,'cols:',ncols)\n            print('Number of target rows:',target_rows,'cols:',target_cols)\n            not_valid=1\n            return None, None, 1\n\n        imsize = nrows*ncols\n        offset = imsize*task_num*3 #since we are using three types of aug\n        feat.extend(make_features(input_color, nfeat))\n        target.extend(np.array(target_color).reshape(-1,))\n            \n    return np.array(feat), np.array(target), 0","7434c110":"# mode = 'eval'\nmode = 'test'\nif mode=='eval':\n    task_path = evaluation_path\nelif mode=='train':\n    task_path = training_path\nelif mode=='test':\n    task_path = test_path\n\nall_task_ids = sorted(os.listdir(task_path))\n\nnfeat = 13\nlocal_neighb = 5\nvalid_scores = {}\n\nmodel_accuracies = {'ens': []}\npred_taskids = []\n\nfor task_id in all_task_ids:\n\n    task_file = str(task_path \/ task_id)\n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    feat, target, not_valid = features(task)\n    if not_valid:\n        print('ignoring task', task_file)\n        print()\n        not_valid = 0\n        continue\n\n    xgb =  XGBClassifier(n_estimators=25, n_jobs=-1)\n    xgb.fit(feat, target, verbose=-1)\n\n\n#     training on input pairs is done.\n#     test predictions begins here\n\n    num_test_pairs = len(task['test'])\n    for task_num in range(num_test_pairs):\n        cur_idx = 0\n        input_color = np.array(task['test'][task_num]['input'])\n        nrows, ncols = len(task['test'][task_num]['input']), len(\n            task['test'][task_num]['input'][0])\n        feat = make_features(input_color, nfeat)\n\n        print('Made predictions for ', task_id[:-5])\n\n        preds = xgb.predict(feat).reshape(nrows,ncols)\n        \n        if (mode=='train') or (mode=='eval'):\n            ens_acc = (np.array(task['test'][task_num]['output'])==preds).sum()\/(nrows*ncols)\n\n            model_accuracies['ens'].append(ens_acc)\n\n            pred_taskids.append(f'{task_id[:-5]}_{task_num}')\n\n            print('ensemble accuracy',(np.array(task['test'][task_num]['output'])==preds).sum()\/(nrows*ncols))\n            print()\n\n        preds = preds.astype(int).tolist()\n        plot_test(preds, task_id)\n        sample_sub.loc[f'{task_id[:-5]}_{task_num}',\n                       'output'] = flattener(preds)\n","41d0f35d":"if (mode=='train') or (mode=='eval'):\n    df = pd.DataFrame(model_accuracies, index=pred_taskids)\n    print(df.head(10))\n\n    print(df.describe())\n    for c in df.columns:\n        print(f'for {c} no. of complete tasks is', (df.loc[:, c]==1).sum())\n\n    df.to_csv('ens_acc.csv')\n","4bdcaaf6":"sample_sub.head()","032ebd3d":"sample_sub.to_csv('submission.csv')","9f78f8d9":"# Plotting functions","50e71534":"# In this notebook i show how to use decision trees for ARC Challenge. ","55d4dbac":"\n\nIn this notebook i apply the decision tree approach on the evaluation set and provide some details about this approach. The challenges of ARC Challenge:\n\n1. The tasks have variable sizes (ranging from 2x2 to 30x30).\n2. Each task has very few samples (on average around 2). So, supervised learning approaches particularly over parameterised neural networks are likely to overfit.\n3. It's not trivial to augment data to generate new samples. Most of the tasks involve abstract reasoning, so commonly used augmentation techniques are not suitable.\n\nThe decision tree approach addresses some of the problems. I flatten the input images and use each pixel as an observation. This helps handle variable task sizes (as long as input and output are same size). Flattening the image also has the advantage of giving more number of samples for training. It's also possible to control the number of estimators, tree depth, regularization which can help fight overfitting.\n\nThe problem with flattening the image is that it loses global structure information. So, i designed some features that can capture some global information about the environment of pixel like the moore neighbours, no. of unique colors in the row and column etc. For this approach, feature engineering is going to be very important.","f9e4760e":"# For flattening 2D numpy arrays","43f6e32b":"# Loading Libraries","bfa37c67":"# Training and Prediction","95ddda07":"# Make features for each train sample","5740d55f":"# Extract neighbourhood Features","a14af2a2":"# Set Paths"}}