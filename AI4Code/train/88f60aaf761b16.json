{"cell_type":{"23897724":"code","611426ae":"code","614cdff1":"code","bcbe6d40":"code","01789467":"code","e3a813b9":"code","87e2dfae":"code","9568992e":"code","1eae9962":"code","b486cff9":"code","79d3d70f":"code","c1728f8d":"code","6a6a17fd":"code","5feeb631":"code","8895f5ff":"code","2549feb5":"code","418c8aef":"code","4fc270b8":"code","c2b919d9":"code","08f3a6e5":"code","937ae464":"code","ab56f7d7":"code","dfd72327":"code","2e22a859":"code","4828cdd9":"code","d9ff1ae0":"code","3b0f8dcc":"code","9a88f6ae":"code","0a78b119":"code","0230ec29":"code","296bdcad":"markdown","fffa5b16":"markdown","b0f230a0":"markdown","5b86b41e":"markdown","b7670e85":"markdown","70a191f3":"markdown","f2139dd3":"markdown","63ae2717":"markdown","c5b3014d":"markdown","ac2c3502":"markdown","5a15eb68":"markdown","f7929205":"markdown","f7072ce9":"markdown","0eebeae2":"markdown","fe52ee96":"markdown","37c3064b":"markdown","a600f340":"markdown","0eed944a":"markdown","37dde9dd":"markdown","64b6afdb":"markdown","d2f77e25":"markdown","376e251d":"markdown","3c0bd27f":"markdown"},"source":{"23897724":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","611426ae":"df = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\ndf.head()","614cdff1":"df.describe()","bcbe6d40":"df.isna().sum()","01789467":"df.info()","e3a813b9":"df1 = df.drop(['id','date'], axis=1)\ndf1.head()","87e2dfae":"corr = df1.corr()\nplt.figure(figsize=(25,15))\nsns.heatmap(corr, annot=True)","9568992e":"from sklearn.model_selection import train_test_split","1eae9962":"x = df1.drop(['price'], axis=1)\ny = df1['price']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=3)","b486cff9":"plt.subplots(figsize=(7, 5))\nsns.countplot(df1[\"bedrooms\"])\nplt.show()","79d3d70f":"plt.subplots(figsize=(15, 5))\nsns.countplot(df1[\"bathrooms\"])\nplt.show()","c1728f8d":"sns.countplot(df1[\"waterfront\"])\nplt.show()","6a6a17fd":"sns.countplot(df1[\"floors\"])\nplt.show()","5feeb631":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics","8895f5ff":"lm = LinearRegression()\nlm.fit(x_train,y_train)            # Fitting model with x_train and y_train\nlm_pred = lm.predict(x_test)       # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, lm_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, lm_pred))\nprint(\"Accuracy :\",lm.score(x_test, y_test))","2549feb5":"labels = {'True Labels': y_test, 'Predicted Labels': lm_pred}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","418c8aef":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV","4fc270b8":"dtree_up = DecisionTreeRegressor()\ndtree_up.fit(x_train, y_train)               # Fitting model with x_train and y_train\ndtree_pred_up = dtree_up.predict(x_test)     # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, dtree_pred_up, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, dtree_pred_up))\nprint(\"Accuracy :\",dtree_up.score(x_test, y_test))","c2b919d9":"d = np.arange(1, 21, 1)\n\ndtree = DecisionTreeRegressor(random_state=5)\nhyperParam = [{'max_depth':d}]\n\ngsv = GridSearchCV(dtree,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train, y_train)                          # Fitting model with xtrain_scaler and y_train\ndtree_pred_mms = best_model.best_estimator_.predict(x_test)     # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\n\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, dtree_pred_mms, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, dtree_pred_mms))\nprint(\"Accuracy :\",best_model.score(x_test, y_test))","08f3a6e5":"labels = {'True Labels': y_test, 'Predicted Labels': dtree_pred_mms}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","937ae464":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics","ab56f7d7":"rf = RandomForestRegressor()\nrf.fit(x_train, y_train)             # Fitting model with x_train and y_train\nrf_pred = rf.predict(x_test)         # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred))\nprint(\"Accuracy :\",rf.score(x_test, y_test))","dfd72327":"nEstimator = [140,160,180,200,220]\ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\n\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(x_train, y_train)\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(x_train, y_train)        # Fitting model with x_train and y_train\n\n# Predicting the results:\nrf_pred_tune = model.predict(x_test)\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred_tune, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred_tune))\nprint(\"Accuracy :\",model.score(x_test, y_test))","2e22a859":"labels = {'True Labels': y_test, 'Predicted Labels': rf_pred_tune}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","4828cdd9":"import statsmodels.api as sm","d9ff1ae0":"x1 = sm.add_constant(x)\n# Results will contain output of Ordinary Least Squares(OLS). Fit will apply a technique to obtain the fit of the model.\nresults = sm.OLS(y,x1).fit() \nresults.summary()","3b0f8dcc":"print('R2: ', results.rsquared)","9a88f6ae":"# Removing floors from the Independent Variables because P > 0.05\nx2 = x.drop(['floors'], axis=1)","0a78b119":"x3 = sm.add_constant(x2)\n# Results will contain output of Ordinary Least Squares(OLS). Fit will apply a technique to obtain the fit of the model.\nresults1 = sm.OLS(y,x2).fit() \nresults1.summary()","0230ec29":"print('R2: ', results1.rsquared)","296bdcad":"First, Null Values need to be checked as it is \nvery important to remove Null values for \nMultiple Regression. However, we did not find \nany Null values in this dataset. Then 2 columns: \n\u2018id\u2019 and \u2018date\u2019 are removed from DataFrame as \nthese contain useless information.","fffa5b16":"**1st Plot:** Shows the bedrooms count, and it can be observed that most of the properties are having 3 bedrooms and 4 bedrooms.","b0f230a0":"## 4. StatsModel OLS:","5b86b41e":"**4th Plot:**  Shows how many \nfloors maximum properties have, and we can \nobserve that most of the properties are having 1 \nand 2 floors.\n","b7670e85":"Next, we used Decision Tree for our model. For \nthis, we used 2 variants of model unpruned \nsimple decision tree model and tuned regressor \nwith multiple max_depth. Results are:\n1. Decision Tree (Unpruned):\n* RMSE: 422.72\n* R2 Score: 0.76\n* Accuracy: 76.05 % \n2. Decision Tree (Pruned): which was pruned using max_depth for 1 to 20 range. This model is hyperparameter tuned using sklearn\u2019s GridSearchCV.\n* Max_depth: 11\n* RMSE: 406.80\n* R2 Score: 0.79\n* Accuracy: 79.46 %\n\nShows the lmplot which is a straight \nline and closer to 45 degrees. This plot turns out \nto be much better than the Multiple Linear \nRegression model.\n","70a191f3":"### HyperParameter Tuned Decision Tree Regressor:","f2139dd3":"## 2. Decision Tree:","63ae2717":"## 3. Random Forest: ","c5b3014d":"# **Conclusion:**\nThis dataset is House Sales in King \nCounty, USA, where we predicted \u2018price\u2019. This \ndataset had few variables which were removed \nduring data cleaning and the correlation of all \nvariables were good with target variables. We \nhave used 4 machine learning models for this \ndataset, Multiple Linear Regression produced \nan average result and accuracy of 70.78 %, \nhowever, hyperparameter tuned Decision Tree \nalso provided accuracy of around 79.46 %. \nRandom Forest worked well and for both \nsimple and hyperparameter tuned Random \nForest Model, accuracy came out to be 88.58 %. \nHowever, after using StatsModel OLS, we \nfound that the \u2018floors\u2019 variable has P values > \n0.05, so we removed that variable and received \na very good model with 90.50 % accuracy. \nStatsModel after removing the \u2018floors\u2019 variable \nturns out to be the best model for our dataset.","ac2c3502":"# Splitting data into train and test\nWe used train_test_split from sklearn library to \nsplit our data into 75% and 25% for train and \ntest sets respectively. We created x_train, \nx_test, y_train and y_test. The Random state for \ntrain and test is 3.\n","5a15eb68":"# Data Wrangling","f7929205":"# **Machine Learning models:**\n\n4 Machine Learning models are used:","f7072ce9":"# Visualization:","0eebeae2":"StatsModel is the last model we are using to get \nthe best \u2018price\u2019 prediction. First, we are using a \nbasic model and from (Fig. 23) we can observe \nthe P values of all Independent Variables. It is \nobserved that only the floor is having P > 0.05, \ni.e, 0.063. So, for the next model we will \nremove the \u2018floor\u2019 variable and run this model \nagain to get very good results.\n1. StatsModel OLS:\n* Accuracy = 70 %\n2. StatsModel OLS after removing \u2018floors\u2019 \n(P>0.05):\n* Accuracy = 90.50 %\n\nClearly shows that after removing the \n\u2018floor\u2019 variable we are getting 90.50 % \naccuracy which is the highest among all other \nmodels. Also, the F-Statistics value is very \nsmall and close to 0.","fe52ee96":"**2nd Plot:**  Shows the bathroom count, \nand it can be observed that most of the houses \nare having 2.5, 1, and 1.75 bathrooms.","37c3064b":"### Unpruned Tree","a600f340":"## 1. Multiple Linear Regression:","0eed944a":"**3rd Plot:**  Shows property with waterfront and we can \nobserve that the maximum of the houses is not \nhaving a waterfront and only a few have a \nwaterfront feature. ","37dde9dd":"### Simple Random Forest","64b6afdb":"### HyperParameter Tuned Random Forest Regressor:","d2f77e25":"# Correlation:\nWe can demonstrate that all variables \nare in good correlation with \u2018price\u2019. Only \n\u2018zipcode\u2019 has a negative correlation of -0.05 but \nare very near to 0 with the target variable. \n\u2018sqft_living\u2019, \u2018grades\u2019 and \u2018bathrooms\u2019 are \nhaving a positive strong correlation with the \ntarget variable \u2018price\u2019.\n","376e251d":"I have used first Multiple Linear Regression for \nthis dataset. This model provided an average \nresult. Below are the results:\n* RMSE: 444.30\n* R2 Score: 0.71\n* Accuracy: 70.78 % \n\nShows the lmplot for this multiple \nlinear regression model and it plots a straight \nline, but this is not much close to 45 degrees.\n","3c0bd27f":"We have used Random Forest for this dataset. \nWe have used 2 variants of Random Forest; 1st \nis normal Random Forest and 2nd is \nHyperparameter tuned, Random Forest. We are \nusing GridSearchCV from sklearn. For the 2nd \nmodel, we have used parameters like \n\u2018n_estimators\u2019 and \u2018max_depth\u2019. We will \niterate through all parameters and find the best \none. Results are:\n1. Random Forest (Simple):\n* RMSE: 351.26\n* R2 Score: 0.89\n* Accuracy: 88.58 % \n2. Random Forest (Tuned): n_estimators = [140,160,180,200,220] and max_depth = [10,15,20,25,30]\n* Best n_estimators: 180\n* Best max_depth: 30\n* RMSE: 351.30\n* R2 Score: 0.89\n* Accuracy: 88.58 %\n\nShows the lmplot and it can be \nobserved that this time we got a straight line \nwhich is close to 45 degrees. Random Forest \nwith tuned parameters looks very efficient for \nthis dataset."}}