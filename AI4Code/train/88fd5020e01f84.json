{"cell_type":{"46838449":"code","7dfb2dad":"code","28555dcb":"code","8cabb80a":"code","30ea85a9":"code","9543109e":"code","bc080716":"code","71850ce5":"code","fb58c735":"code","e9d8c294":"code","4b0c3ff8":"code","9742abc0":"code","c598b63a":"code","c22facc1":"code","d018cd96":"code","563fdeaf":"code","887e0a53":"code","f59b8eb0":"code","27432300":"code","7928aaca":"code","48423f4a":"code","4965fc9b":"code","a61234c1":"code","ffe7de47":"markdown","84a3275c":"markdown","091f20d1":"markdown","42376ad1":"markdown","89d02a7e":"markdown","e15e9c98":"markdown","1794c6b1":"markdown","9a159f5c":"markdown","00f824be":"markdown","b09d9fb9":"markdown","352fe9d0":"markdown","8c001d49":"markdown"},"source":{"46838449":"#Basic Sandbox\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#To generate a refid for each paper (dataset + bib_entries)\nimport hashlib #for sha1\n\n#To build network and compute pagerank\nimport networkx as nx\nimport math as math\n\n#For Data viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom datetime import timedelta","7dfb2dad":"#Weight parameters for Approach 2 and 3 :\n\nweights_InfluenceScore = [0.25, 0.25, 0.25, 0.25]","28555dcb":"#1. Get the data\ndatafiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n            ifile = os.path.join(dirname, filename)\n            if ifile.split(\".\")[-1] == \"json\":\n                datafiles.append(ifile)\n            \nprint(\"Number of Files Loaded: \", len(datafiles))\n\n\n#Loading metadata csv file to get the publish time\nmetadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\n\n#2. Creating of the two DataFrames:\n#dfPaperList = df of Research Papers.. Variables: paper_id, paper_title, paper_authors\n#dfCitationsFlat = df of all the citations . Variables: citationsId, paperId (where the citation is made),refid, title, year\n\nauthors = []\n\ncitationsFlat = []\ncitationsCount = 0\n\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    paper_id = doc['paper_id']\n    \n    paper_authors = []\n\n    for value in doc['metadata']['authors']:\n        if len(doc['metadata']['authors']) == 0:\n            paper_authors.append(\"NA\")\n        else:\n            last = value[\"last\"]\n            first = value[\"first\"]\n            paper_authors.append(first+\" \"+last)\n\n    authors.append({\"paper_id\": paper_id, \"authors\" : paper_authors})\n\n    for key,value in doc['bib_entries'].items():\n        refid = key\n        title = value['title'].lower()\n        year = value['year']\n        venue = value['venue'] \n        SHATitleCitation = hashlib.sha1(title.lower().encode()).hexdigest() #\n\n        if (len(title) == 0):\n            continue #there is noting we can do without any title\n\n        citationsFlat.append({\"citationId\":citationsCount,\\\n                          \"refid\" : SHATitleCitation,\\\n                          \"from\": paper_id,\\\n                          \"title\": title.lower(),\\\n                          \"year\": year})\n        citationsCount=citationsCount+1\n        \n#Conversion into DataFrame\ndfCitationsFlat = pd.DataFrame(citationsFlat)\nauthorsDf = pd.DataFrame(authors)\n\nmetadata_extract = metadata[[\"sha\", \"title\", \"abstract\", \"publish_time\"]].rename(columns = {\"sha\" : \"paper_id\"})\ndfPaperList = pd.merge(metadata_extract, authorsDf, on = \"paper_id\", how = \"left\")\n\ndfPaperList[\"year\"] = 0\ndfPaperList[\"refid\"] = \"\"\n\nfor i in range(len(dfPaperList)):\n    \n    dfPaperList[\"refid\"][i] =  hashlib.sha1(str(dfPaperList[\"title\"][i]).lower().encode()).hexdigest()\n     #NB: We are building a custom identifier based on papers titles to ensure identification will be consistent between the papers in the Research Dataset and the papers extracted from the bib entries.\n     #Unfortunately a paperId is not present for citations and doi is not provided for the whole dataset but title seem to be present for ~98% of the dataset. To enable and ease indexing capabilities we are hashing with SHA   \n    dfPaperList[\"year\"][i] = str(dfPaperList[\"publish_time\"][i])[:4]\n    \n    try:\n        dfPaperList[\"authors\"][i] = dfPaperList[\"authors\"][i].split(\";\")\n    except:\n        continue\n        \nquotationPapersFreq = pd.DataFrame({\"refid\" : dfCitationsFlat[\"refid\"].value_counts().index, \n                       \"nbQuotations\" : dfCitationsFlat[\"title\"].value_counts().values}) \n\npaperToScore = pd.merge(dfPaperList,quotationPapersFreq, on = \"refid\", how = \"left\")\npaperToScore[\"nbQuotations\"] = paperToScore[\"nbQuotations\"].fillna(0)\n\n\n#Adding list of references by papers according to the refid\nrefList = pd.DataFrame({\"references\" : dfCitationsFlat.groupby('from')['refid'].apply(list)}) \nrefList[\"paper_id\"] = refList.index; cols = [\"paper_id\",\"references\"] ; refList = refList[cols].reset_index(drop = True) #Reformatting the reflist by papers\ndatasetForScoring = pd.merge(paperToScore, refList, how='left', on = 'paper_id').reset_index(drop = True)\n\ndatasetForScoring = datasetForScoring[(datasetForScoring[\"authors\"].isna() == False)].reset_index(drop = True)","8cabb80a":"#3. A few stats regarding number of papers loaded\n\nprint(\"Number of Papers in the CORD-19 dataset :\",dfPaperList.shape[0])\n#(05\/14\/2020) Number of Papers in the covid dataset : 63,571\n\nprint(\"Number of Citations found in the CORD-19 dataset :\",dfCitationsFlat.shape[0])\n#(05\/14\/2020) Number of Citations made in the covid dataset : 4,208,974\n\nprint(\"Citations with no title: \",sum(1 if x == \"\" else 0 for x in dfCitationsFlat[\"title\"]))\n#(05\/14\/2020) Citations with no title:  0\n\n#How many duplicates? \nprint(\"Number of duplicated research paper titles: \",len(dfPaperList[\"title\"])-len(dfPaperList[\"title\"].drop_duplicates()))\n#(05\/14\/2020) Number of duplicated research paper titles:  1,421\n\nprint(\"Number of duplicated citations titles: \",len(dfCitationsFlat[\"title\"])-len(dfCitationsFlat[\"title\"].drop_duplicates()))\n#(05\/14\/2020) Number of duplicated citations titles:  2,543,820\n\n#Dataframe Visualization\nprint(\"Number of Papers that will be scored: \", datasetForScoring.shape[0])\ndatasetForScoring.head()\n","30ea85a9":"#1. Creating an author dataset + Computation of the author page rank using an author network\n#Variables for author dataset: id, name, co-authors, number of points linked to quotations, paper_count, citations, average citations,co_author_avg_citations,h-index\n\nauthor_data = {}\nauthor_id = {\n    'start': 1,\n    'curr': 1\n}\n\nassigned_ids = {}\n\ndef create_author_data(train_data, author_data, author_id, assigned_ids):\n    for i in range(len(train_data)):\n        authors = train_data.authors[i]\n    \n        try:\n            citations = train_data.nbQuotations[i]\/len(authors) #Number of times a paper have been quoted divided by len authors\n        except:\n            continue\n\n        for author in authors:\n            names = author.split(' ')\n            unique_name = names[0] + \"_\" + names[len(names)-1]\n            if unique_name not in author_data:\n                author_data[unique_name] = {\n                    'num_citations': citations,\n                    'paper_count': 1,\n                    'name': unique_name,\n                    'author_id': author_id['curr'],\n                    'co_authors': {},\n                    'citations': [train_data.nbQuotations[i]]\n                }\n                assigned_ids[unique_name] = author_id['curr']\n                author_id['curr'] += 1\n\n            else:\n                author_data[unique_name]['num_citations'] += citations\n                author_data[unique_name]['paper_count'] += 1\n                author_data[unique_name]['citations'].append(train_data.nbQuotations[i])\n\n            for co_author in authors:\n                co_author_names = co_author.split(' ')\n                co_author_unique_name = co_author_names[0] + \"_\" + co_author_names[len(co_author_names)-1]\n                if co_author_unique_name != unique_name:\n                    author_data[unique_name]['co_authors'][co_author_unique_name] = 1\n                        \n            \n            \n# call for each data file\ncreate_author_data(datasetForScoring, author_data, author_id, assigned_ids)\n\n# add average citations\nfor data in author_data:\n    author_data[data]['average_citations'] = author_data[data]['num_citations'] \/ author_data[data]['paper_count']\n    \n# adding h-index\ndef get_h_index(citations):\n    return ([0] + [i + 1 for i, c in enumerate(sorted(citations, reverse = True)) if c >= i + 1])[-1]\n\ndata_to_df = []\nfor data in author_data:\n    each_author = author_data[data]\n    co_authors = each_author['co_authors']\n    co_author_ids = []\n    co_author_avg_citations = 0\n    for co_author in co_authors:\n        co_author_avg_citations += author_data[co_author]['average_citations']\n        co_author_ids.append(assigned_ids[co_author])\n    each_author['co_authors'] = co_author_ids\n    each_author['co_author_avg_citations'] = co_author_avg_citations\/len(co_author_ids) if len(co_author_ids) != 0 else 0\n    data_to_df.append(each_author)\n    \nauthorsData = pd.DataFrame.from_dict(data_to_df, orient='columns')\n\nauthorsData['h_index'] = authorsData.apply(lambda x: get_h_index(x.citations), axis=1)","9543109e":"#2. Computation of authors page rank\n\n### AUTHOR PAGE RANK ###\n#Data Pre-processing: building the dataset on which the author network will be built\ntrain = authorsData.copy().drop(columns=['num_citations', 'h_index','paper_count', 'citations']).dropna(axis = 0, subset=['co_authors'])\ntrain = train[train.co_authors != '[]']\ntrain['author_id'] = pd.to_numeric(train['author_id'])\n\n# Building up the network to compute author page rank: \nG = nx.Graph()\nfor i in range(len(train)):\n    auth = train.iloc[i]['author_id']\n    for neighbor in train.iloc[i]['co_authors']:\n        if G.has_edge(auth, neighbor):\n            G.add_edge(auth, neighbor, weight = G[auth][neighbor]['weight']+1)\n        else:\n            G.add_edge(auth, neighbor, weight = 1)\n            \nscore_authors = nx.pagerank(G, alpha=0.55, max_iter=100, tol=1.0e-6, nstart=None, weight='weight', dangling=None)\n\n#Saving the page rank by author id\nauthorPRK = pd.DataFrame.from_dict(score_authors, orient = \"index\")\nauthorPRK[\"author_id\"] = authorPRK.index\nauthorPRK.columns = [\"pagerank_author\", \"author_id\"]\nauthorPRK.to_csv(\"pagerank_author.csv\",index = False)","bc080716":"#3. Computation of publication page rank\n\n# Building up the network to compute the pagerank for publication\nG1 = nx.Graph()\nfor i in range(len(datasetForScoring)):\n# for i in range(100): #Only on a sample\n    G1.add_node(datasetForScoring['refid'][i])\n    auth = datasetForScoring['refid'][i]\n    \n    for e in list(str(datasetForScoring[\"references\"][i]).lstrip(\"[\").rstrip(\"]\").replace(\" \",\"\").split(\",\")):\n        try:\n            if G1.has_edge(auth, e):\n                G1.add_edge(auth, e, weight = G[auth][e]['weight']+1)\n            else:\n                G1.add_edge(auth, e, weight = 1)\n        except:\n            continue\n        \nscore_publication = nx.pagerank(G1, alpha=0.85, tol=1.0e-6, nstart=None, weight=1, dangling=None)\n\n#Saving the page rank by paper id\npubliPRK = pd.DataFrame.from_dict(score_publication, orient = \"index\")\npubliPRK[\"publication_id\"] = publiPRK.index\npubliPRK.columns = [\"pageRankPublication\", \"publication_id\"]\npubliPRK[\"publication_id\"] = publiPRK[\"publication_id\"].str.replace(\"'\",\"\")\npubliPRK = publiPRK.reset_index(drop = True)\n\npubliPRK.to_csv(\"pagerank_publication.csv\",index = False)\n\n#Integration of the variable Page Rank for publication datasetForScoring\nenhancedDatasetForScoring = pd.merge(datasetForScoring,publiPRK, left_on = \"refid\", right_on = \"publication_id\", how = \"left\").drop(columns= [\"publication_id\"])\nenhancedDatasetForScoring = enhancedDatasetForScoring.drop_duplicates(subset='refid', keep=\"last\") #Temporary patch to manage the case where twice Page rank for some publications","71850ce5":"#4. Computation of Author Scoring\n\n#Dataset to consolidate Author Page Rank and Publication Rank in a way to compute authorP2\ndfAuthorP2 = pd.merge(authorsData[[\"author_id\",\"name\"]],authorPRK, on = \"author_id\", how = \"left\").reset_index(drop=True)\ndfAuthorP2[\"name\"] = dfAuthorP2[\"name\"].str.replace(\"_\",\" \")\n\n# Extract enhancedDatasetForScoring \"paper_refid\" &\"paper_authors\"\nauthorsfromDf = enhancedDatasetForScoring[[\"refid\",\"authors\"]].reset_index(drop = True)\n# authorsfromDf = authorsfromDf[(authorsfromDf[\"authors\"].isna() == False)]\nauthorsfromDf = pd.DataFrame(authorsfromDf.authors.tolist(), index = authorsfromDf.refid).stack().reset_index(level=1, drop=True).reset_index(name='authors')[['authors','refid']]\n\n#Computing the sum of publication page rank for each paper\ndfAuthorP2withPRPubli = pd.merge(authorsfromDf,publiPRK, left_on = \"refid\", right_on = \"publication_id\", how = \"left\").drop(columns = [\"refid\", \"publication_id\"]).groupby(\"authors\").sum()\ndfAuthorP2withPRPubli[\"authors\"] = dfAuthorP2withPRPubli.index #Reformatting\ndfAuthorP2withPRPubli = dfAuthorP2withPRPubli.reset_index(drop=True)\n\ndfAuthorP2Final = pd.merge(dfAuthorP2,dfAuthorP2withPRPubli, left_on = \"name\", right_on = \"authors\", how = \"left\").drop(columns = \"name\")\n\n# ######### Author Scoring #########\ndfAuthorP2Final[\"pagerank_author_norm\"] = (dfAuthorP2Final[\"pagerank_author\"]-dfAuthorP2Final[\"pagerank_author\"].mean())\/dfAuthorP2Final[\"pagerank_author\"].std()\ndfAuthorP2Final[\"pagerank_publication_norm\"] = (dfAuthorP2Final[\"pageRankPublication\"]-dfAuthorP2Final[\"pageRankPublication\"].mean())\/dfAuthorP2Final[\"pageRankPublication\"].std()\n\ndfAuthorP2Final[\"authorP2\"] = 0.25*dfAuthorP2Final[\"pagerank_author_norm\"] + 0.75*dfAuthorP2Final[\"pagerank_publication_norm\"]","fb58c735":"#1. Data Preparation\n\n    # Consolidate Author Score for each paper\nauthorP2Data = dfAuthorP2Final[[\"authors\",\"authorP2\"]]\n# enhancedDatasetForScoring = enhancedDatasetForScoring[(enhancedDatasetForScoring[\"authors\"].isna() == False)]\nauthorToPaper = pd.DataFrame(enhancedDatasetForScoring[[\"refid\",\"authors\"]].authors.tolist(), index=enhancedDatasetForScoring[[\"refid\",\"authors\"]].refid).stack().reset_index(level=1, drop=True).reset_index(name='authors')[['authors','refid']]\n\nauthorP2Conso = pd.merge(authorToPaper,authorP2Data, on = \"authors\", how = \"left\")\n\n# Consolidate AuthorP2 for each paper as followed: 0.5 * Max page rank + 0.5 * average of the page rank of all the authors\nmaxAuthorScore = authorP2Conso.groupby('refid').agg({'authorP2': 'max'})\nmeanAuthorScore = authorP2Conso.groupby('refid').agg({'authorP2': 'mean'})\n\nauthorScoring = pd.merge(maxAuthorScore,meanAuthorScore, on = \"refid\", how = \"inner\").rename(columns = {\"authorP2_x\" : \"maxAuthorScore\",\"authorP2_y\" : \"meanAuthorScore\"})\nauthorScoring[\"refid\"] = authorScoring.index\nauthorScoring = authorScoring.reset_index(drop = True)\n\nauthorScoring[\"authorP2\"] = 0.5*authorScoring[\"maxAuthorScore\"] + 0.5*authorScoring[\"meanAuthorScore\"]\nauthorScoring = authorScoring.drop(columns = [\"maxAuthorScore\",\"meanAuthorScore\"])\n\n#Integration of the variable authorP2 for datasetForScoring\nDatasetReadyForScoring = pd.merge(enhancedDatasetForScoring,authorScoring, on = \"refid\", how = \"left\")\n\n# Influence Score Computation Dataset Overview\nDatasetReadyForScoring.head()","e9d8c294":"#2. Data Exploration \n    #2.1. nbQuotations variable\nq25 = DatasetReadyForScoring[\"nbQuotations\"].quantile(.25); q50 = DatasetReadyForScoring[\"nbQuotations\"].quantile(.5)\nq75 = DatasetReadyForScoring[\"nbQuotations\"].quantile(.75); q100 = DatasetReadyForScoring[\"nbQuotations\"].quantile(1)\n\nprint(\"Max:\", q100 ) ; print(\"Top 25% above :\", q75 ); print(\"Top 50% above:\", q50); print(\"Top 75% above:\", q25 )","4b0c3ff8":"    #2.2. Pagerank Publication\npq25 = DatasetReadyForScoring[\"pageRankPublication\"].quantile(.25); pq50 = DatasetReadyForScoring[\"pageRankPublication\"].quantile(.5)\npq75 = DatasetReadyForScoring[\"pageRankPublication\"].quantile(.75); pq100 = DatasetReadyForScoring[\"pageRankPublication\"].quantile(1)\n\nprint(\"Max:\", pq100 ) ; print(\"Top 25% above :\", pq75 ); print(\"Top 50% above:\", pq50); print(\"Top 75% above:\", pq25 )","9742abc0":"    #2.3. Author P2\naq25 = DatasetReadyForScoring[\"authorP2\"].quantile(.25); aq50 = DatasetReadyForScoring[\"authorP2\"].quantile(.5)\naq75 = DatasetReadyForScoring[\"authorP2\"].quantile(.75); aq100 = DatasetReadyForScoring[\"authorP2\"].quantile(1)\n    \nprint(\"Max:\", pq100 ) ; print(\"Top 25% above :\", pq75 ); print(\"Top 50% above:\", pq50); print(\"Top 75% above:\", pq25 )","c598b63a":"#3. Score computation all data by approaches\n\n########### APPROACH ONE ########### \nnow = date.today() ; numberWeeks = 6\ndateThresold = now - timedelta(days = numberWeeks*7)\n\ninfluenceScoreData_A1 = DatasetReadyForScoring[(DatasetReadyForScoring[\"publish_time\"].isna() == False)].reset_index(drop = True)\n\ninfluenceScoreData_A1[\"Recency\"] = 0\n\n    #Assessing Recency of a paper\nfor i in range(len(influenceScoreData_A1)):\n    try:\n        if influenceScoreData_A1[\"publish_time\"][i] > dateThresold:\n            influenceScoreData_A1[\"Recency\"][i] = 1\n        else:\n            influenceScoreData_A1[\"Recency\"][i] = 0\n    except:\n        continue\n        \n    #Compute Final Score Approach 1          \ninfluenceScoreData_A1[\"influenceScore\"] = 0\n\nfor i in range(len(influenceScoreData_A1)):\n        if influenceScoreData_A1[\"Recency\"][i] == 1:\n            influenceScoreData_A1[\"influenceScore\"] = influenceScoreData_A1[\"authorP2\"]\n        else:\n            influenceScoreData_A1[\"influenceScore\"] = influenceScoreData_A1[\"pageRankPublication\"]\n\n\n            \n            \n            \n            \n            \n########### APPROACH TWO ########### \ninfluenceScoreData_A2 = DatasetReadyForScoring.copy()\n\n#Compute Score for Nb_quoted and year\n\ninfluenceScoreData_A2[\"nbQuotationsScore\"] = 0\ninfluenceScoreData_A2[\"yearScore\"] = 0\n\nfor i in range(len(influenceScoreData_A2)):\n    \n    #Nb_quoted\n    if influenceScoreData_A2[\"nbQuotations\"][i] < q25:\n        influenceScoreData_A2[\"nbQuotationsScore\"][i] = 0\n    elif influenceScoreData_A2[\"nbQuotations\"][i] < q50:\n        influenceScoreData_A2[\"nbQuotationsScore\"][i] = 0.25\n    elif influenceScoreData_A2[\"nbQuotations\"][i] < q75:\n        influenceScoreData_A2[\"nbQuotationsScore\"][i] = 0.5\n    else:\n        influenceScoreData_A2[\"nbQuotationsScore\"][i] = 1\n        \n    #Year\n    if influenceScoreData_A2[\"year\"][i] == 2020:\n        influenceScoreData_A2[\"yearScore\"][i] = 1\n    elif influenceScoreData_A2[\"year\"][i] == 2019:\n        influenceScoreData_A2[\"yearScore\"][i] = 0.5\n    else:\n        influenceScoreData_A2[\"yearScore\"][i] = 0\n\ninfluenceScoreData_A2[\"pageRankPublication_norm\"] = (influenceScoreData_A2[\"pageRankPublication\"] - influenceScoreData_A2[\"pageRankPublication\"].mean())\/ influenceScoreData_A2[\"pageRankPublication\"].std()\ninfluenceScoreData_A2[\"authorP2_norm\"] = (influenceScoreData_A2[\"authorP2\"] - influenceScoreData_A2[\"authorP2\"].mean())\/ influenceScoreData_A2[\"authorP2\"].std()\n\n\ninfluenceScoreData_A2[\"influenceScore\"] = 0.00\n\nfor i in range(len(influenceScoreData_A2)):\n    influenceScoreData_A2[\"influenceScore\"][i] = weights_InfluenceScore[0] * influenceScoreData_A2[\"nbQuotationsScore\"][i] + weights_InfluenceScore[1] * influenceScoreData_A2[\"yearScore\"][i] + weights_InfluenceScore[2] * influenceScoreData_A2[\"pageRankPublication_norm\"][i] + weights_InfluenceScore[3] * influenceScoreData_A2[\"authorP2_norm\"][i]\n\n        \ninfluenceScoreData_A2 = influenceScoreData_A2.sort_values(by = \"influenceScore\", ascending = False).reset_index(drop = True)            \n            \n        \n                        \n########### APPROACH THREE ########### \ninfluenceScoreData_A3 = DatasetReadyForScoring.copy()\n\ninfluenceScoreData_A3[\"influenceScore\"] = 0.00\n\nfor i in range(len(influenceScoreData_A3)):\n    \n    x1 = 0; x2 = 0; x3 = 0; x4 = 0\n    \n    #Nb_quoted\n    if influenceScoreData_A3[\"nbQuotations\"][i] < q25:\n        x1 = 0\n    elif influenceScoreData_A3[\"nbQuotations\"][i] < q50:\n        x1 = 0.25\n    elif influenceScoreData_A3[\"nbQuotations\"][i] < q75:\n        x1 = 0.5\n    else:\n        x1 = 1\n        \n    #Year\n    if influenceScoreData_A3[\"year\"][i] == 2020:\n        x2 = 1\n    elif influenceScoreData_A3[\"year\"][i] == 2019:\n        x2 = 0.5\n    else:\n        x2 = 0\n        \n    #PageRank Publication\n    if influenceScoreData_A3[\"pageRankPublication\"][i] < pq25:\n        x3 = 0\n    elif influenceScoreData_A3[\"pageRankPublication\"][i] < pq50:\n        x3 = 0.25\n    elif influenceScoreData_A3[\"pageRankPublication\"][i] < pq75:\n        x3 = 0.5\n    else:\n        x3 = 1\n        \n    #Author Scoring\n    if influenceScoreData_A3[\"authorP2\"][i] < aq25:\n        x4 = 0\n    elif influenceScoreData_A3[\"authorP2\"][i] < aq50:\n        x4 = 0.25\n    elif influenceScoreData_A3[\"authorP2\"][i] < aq75:\n        x4 = 0.5\n    else:\n        x4 = 1\n        \n    influenceScoreData_A3[\"influenceScore\"][i] = weights_InfluenceScore[0] * x1 + weights_InfluenceScore[1] * x2 + weights_InfluenceScore[2] * x3 + weights_InfluenceScore[3] * x4\n       \ninfluenceScoreData_A3 = influenceScoreData_A3.sort_values(by = \"influenceScore\", ascending = False).reset_index(drop = True)\n","c22facc1":"########### DATA EXPORT - EACH PAPER ID WITH SCORE ########### \n\n#Exporting influenceScore by paper id - FIRST APPROACH\nPaperScoring_A1 = influenceScoreData_A1[[\"paper_id\",\"influenceScore\"]]\nPaperScoring_A1.to_csv(\"PaperScoring_A1.csv\", index = False)\n\n#Exporting influenceScore by paper id - SECOND APPROACH\nPaperScoring_A2 = influenceScoreData_A2[[\"paper_id\",\"influenceScore\"]]\nPaperScoring_A2.to_csv(\"PaperScoring_A2.csv\", index = False)\n\n#Exporting influenceScore by paper id - THIRD APPROACH\nPaperScoring_A3 = influenceScoreData_A3[[\"paper_id\",\"influenceScore\"]]\nPaperScoring_A3.to_csv(\"PaperScoring_A3.csv\", index = False)\n\n#Exporting final dataset with all consolidated by paper id - THIRD APPROACH\nConsolidatedDfwithScore = pd.merge(DatasetReadyForScoring, PaperScoring_A1, on = \"paper_id\", how = \"left\") #Adding Score from approach 1\nConsolidatedDfwithScore = pd.merge(ConsolidatedDfwithScore, PaperScoring_A2, on = \"paper_id\", how = \"inner\") #Adding Score from approach 2\nConsolidatedDfwithScore = pd.merge(ConsolidatedDfwithScore, PaperScoring_A3, on = \"paper_id\", how = \"inner\") #Adding Score from approach 3\nConsolidatedDfwithScore = ConsolidatedDfwithScore.rename(columns = {\"influenceScore_x\" : \"ScoreApproach1\",\"influenceScore_y\" : \"ScoreApproach2\", \"influenceScore\" : \"ScoreApproach3\" })\n\nConsolidatedDfwithScore.to_csv(\"ConsolidatedDfwithScore.csv\", index = False)","d018cd96":"#Building up the function to compute the influence score given a paper_id (possiblity to choose the approach)\n\ndef articleScore(paper_id,approach = 2): #Default approach is 2\n    try:\n        if approach == 1: \n            x = influenceScoreData_A1[(influenceScoreData_A1[\"paper_id\"] == paper_id)]\n\n        elif approach == 2: \n            x = influenceScoreData_A2[(influenceScoreData_A2[\"paper_id\"] == paper_id)]\n\n        elif approach == 3: \n            x = influenceScoreData_A3[(influenceScoreData_A3[\"paper_id\"] == paper_id)]\n\n        return x.influenceScore.values[0]\n\n    except:\n        return 0\n    ","563fdeaf":"#1. Scoring Comparison\n    #1.1. Final Score Distribution \nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(influenceScoreData_A1[\"influenceScore\"], ax = ax1).set_title('Approach 1: Influence Score Distribution')\nsns.distplot(influenceScoreData_A2[\"influenceScore\"], ax = ax2).set_title('Approach 2: Influence Score Distribution')\nsns.distplot(influenceScoreData_A3[\"influenceScore\"], ax = ax3).set_title('Approach 3: Influence Score Distribution')\nplt.show()","887e0a53":"    #1.2. Top 1000 for each approaches - Deep diving the characteristics for each variables\n\ntop = 1000\n\ntop_A1 = influenceScoreData_A1[[\"title\",\"year\", \"nbQuotations\", \"pageRankPublication\", \"authorP2\", \"influenceScore\"]].sort_values(by = \"influenceScore\", ascending = False).head(top).reset_index(drop = True)\ntop_A2 = influenceScoreData_A2[[\"title\",\"year\", \"nbQuotations\", \"pageRankPublication\", \"authorP2\", \"influenceScore\"]].sort_values(by = \"influenceScore\", ascending = False).head(top).reset_index(drop = True)\ntop_A3 = influenceScoreData_A3[[\"title\",\"year\", \"nbQuotations\", \"pageRankPublication\", \"authorP2\", \"influenceScore\"]].sort_values(by = \"influenceScore\", ascending = False).head(top).reset_index(drop = True)\n\n        #Distribution Visualization\n\nfig, ax = plt.subplots(3,4,figsize=(30,10))\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n\n#Approach 1\nsns.distplot(top_A1[(top_A1[\"nbQuotations\"] != 0)][\"nbQuotations\"], ax = ax[0,0]).set_title('Approach 1 - Top '+str(top)+': nbQuotations Distribution') \nsns.distplot(top_A1[\"year\"], ax = ax[0,1]).set_title('Approach 1 - Top '+str(top)+': Year Distribution')\nsns.distplot(top_A1[\"pageRankPublication\"], ax = ax[0,2]).set_title('Approach 1 - Top '+str(top)+': Pagerank Publication Distribution')\nsns.distplot(top_A1[\"authorP2\"], ax = ax[0,3]).set_title('Approach 1 - Top '+str(top)+': Author Score Distribution')\n\n#Approach 2\nsns.distplot(top_A2[\"nbQuotations\"], ax = ax[1,0], color=\".2\").set_title('Approach 2 - Top '+str(top)+': nbQuotations Distribution')\nsns.distplot(top_A2[\"year\"], ax = ax[1,1], color=\".2\").set_title('Approach 2 - Top '+str(top)+': Year Distribution')\nsns.distplot(top_A2[\"pageRankPublication\"], ax = ax[1,2], color=\".2\").set_title('Approach 2 - Top '+str(top)+': Pagerank Publication Distribution')\nsns.distplot(top_A2[\"authorP2\"], ax = ax[1,3], color=\".2\").set_title('Approach 2 - Top '+str(top)+': Author Score Distribution')\n\n#Approach 3\nsns.distplot(top_A3[\"nbQuotations\"], ax = ax[2,0], color=\"orange\").set_title('Approach 3 - Top '+str(top)+': nbQuotations Distribution')\nsns.distplot(top_A3[\"year\"], ax = ax[2,1], color=\"orange\").set_title('Approach 3 - Top '+str(top)+': Year Distribution')\nsns.distplot(top_A3[\"pageRankPublication\"], ax = ax[2,2], color=\"orange\").set_title('Approach 3 - Top '+str(top)+': Pagerank Publication Distribution')\nsns.distplot(top_A3[\"authorP2\"], ax = ax[2,3], color=\"orange\").set_title('Approach 3 - Top '+str(top)+': Author Score Distribution')\n\nplt.show()","f59b8eb0":"    #1.3. Does the top1000 of each approaches have common papers in their top?\nprint(\"% of Common papers of Approach 1 with Approach 2 :\", round(len(pd.merge(top_A1, top_A2, how='inner', on=['title']))\/top*100,2))\nprint(\"% of Common papers of Approach 1 with Approach 3 :\", round(len(pd.merge(top_A1, top_A3, how='inner', on=['title']))\/top*100,2))\nprint(\"% of Common papers of Approach 2 with Approach 3 :\", round(len(pd.merge(top_A3, top_A2, how='inner', on=['title']))\/top*100,2))","27432300":"paper_id = \"a80c3e9dfde9824cb8f54f9f382d0d601743ffc0\" \n#this is find in CORD-19 dataset, e.g. \n\nscore = articleScore(paper_id)\n#return article Score, for default approach\nprint(score)\n\n\nscore = articleScore(paper_id,1)\n#return article Score, for approach 1\nprint(score)\n\nscore = articleScore(paper_id,2)\n#return article Score, for approach 2\nprint(score)\n\nscore = articleScore(paper_id,3)\n#return article Score, for approach 3\nprint(score)\n\n\npaper_id = \"DOES NOT EXIST\" \nscore = articleScore(paper_id)\n#return article Score, when paper is missing\nprint(score)\n","7928aaca":"#Variable NbQuotations\nsns.distplot(DatasetReadyForScoring[\"nbQuotations\"]).set_title('NbQuotations Distribution')","48423f4a":"#Variable year\nsns.distplot(DatasetReadyForScoring[\"year\"]).set_title('Year Distribution')","4965fc9b":"#Variable Page Rank Publication\nsns.distplot(DatasetReadyForScoring[\"pageRankPublication\"]).set_title('Page Rank Publication Distribution')","a61234c1":"#Variable authorP2\nsns.distplot(DatasetReadyForScoring[\"authorP2\"]).set_title('Author Score Distribution')","ffe7de47":"# Part I: Data Preparation","84a3275c":"## Librairies","091f20d1":"# Part V: Conclusions and further studies\n\nIn this notebook we shown how to evaluate a paper influence based on its number of quotations, recency, authors reputation and its page rank among a network of quoted publications.\nThis score can be used in a search engine to rank research papers according to their influence, in order to ease priorization.\n\nThree versions has been implemented, in further experiments we could evaluate their usability. Also influence score may be updated with new combinations based on data availability:\n\n* Publisher Reputation\n* Conference Reputation\n* Future Rank (probability of a paper being quoted)","42376ad1":"## How to use\n\n**Scoring computation with a paper_id as an input:**","89d02a7e":"# Appendix","e15e9c98":"# PART IV: Scoring Comparison","1794c6b1":"The notebook is organised as follow:   \n\n**Part I: Data Preparation **\n1. Get the data from the Kaggle Challenge dataset (json files + metadata CSV file)\n2. Consolidate all the data into dataframes: \n1) *dfPaperList*: dataframe that summarizes all the research papers of the CORD-19 dataset 2) *dfCitationsFlat*: dataframe that consolidates all the citations of the Research Papers\n3. Quick data exploration \n4. Consolidation into a final input dataset (*datasetForScoring*) to be used in Part II for page rank calculation (publication + authors)\n\n\n\n**Part II: Computation of Author Scoring and Publication pagerank**\n\nSource used: https:\/\/github.com\/urmilkadakia\/Ranking-of-academic-papers-and-authors\n1. Creating an author dataset (*authorsData*)that compiles all information about the authors and co-authors of the *datasetForScoring* dataframe. Will be used to compute author page rank\n2. Computation of the author page rank using *authorsData*\n3. Computation of the publication rank using *datasetForScoring*\n4. Computaion of Author Score using above computation  \n\n\n**Part III: Influence Score consolidation for all the subset of Research Papers**\n\n1. Data Preparation: consolidation of a dataframe with all the variables necessary to compute influence score\n2. Score consolidation on the sample defined above - All Approaches applied \n3. Function creation to compute the influence score given a paper_id (possiblity to choose the approach)\n\n\n**Part IV: Score comparison and Score Function Creation**\n\nComparison of the different influence score approaches\n\n\n**Part V: Conclusion**\n1. Discussing results\n2. Examples\n\n**Appendix**\n * Data Exploration of the different parameters: we identify the different parameters level to convert into points\n\n\n\nWhile running this notebook, the dataframe would be exported as a CSV file for re-use:\n* Page rank publication by paper refid\n* Page rank authors by author id\n* Paper Scoring detailling the Influence Score according for each paper ID for each approaches\n","9a159f5c":"## Variable Distribution","00f824be":"# Part II: Computation of Author Scoring and Publication pagerank","b09d9fb9":"## Introduction\n\n**In this notebook we are:  **\n* Proposing a definition for a metric the influence of a research paper\n* Building an algorithm to compute such influence scores, and propose three variations\n* Establishing a paper ranking based on the influence score\n\nWe deep dive three possible approaches to build an influence score.\nAll are based on an evaluation of author reputation.\nTherefore we are definining an *author P2 metric*:  \n\n\\begin{equation*}\nP^2(author) = (0.25) * P_{coautornetwork}(author) + (0.75) \\sum^{publications}{P_{citationnetwork}(publication)}\n\\end{equation*}\n\nSource: https:\/\/github.com\/urmilkadakia\/Ranking-of-academic-papers-and-authors \n\n\nAlso, in our method we are building a network of papers, and then leveraging page rank of each paper\ncf. https:\/\/networkx.github.io\/documentation\/networkx-1.10\/reference\/generated\/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n\n1. **First Approach** :\nif Publication in the last 6 weeks, use author scoring to rank papers. Otherwise, use page rank.  \n*NB: Only a subset of CORD-19 dataset papers that have a publish_time*\n\n\n\n2. **Second Approach**:  \n\n\\begin{equation*}\nInfluence Score = w_1 * NbQuotationsScore + w_2 * YearScore + w_3 * pageRankPublication + w_4 * authorP2\n\\end{equation*}\n\nWe are suggesting to reduce NbQuotations and Year to a few discretes values.  \nPage Range and Author P2 are normalized.  \n * **Nb_quotations Score**: +1 pts for Top 25%, + 0.50 pts for Top 50%, +0.25 pts for Top 75%. Otherwise 0 pt. \n * **Year Score**: +1 pts if published in 2020, +0.5 pts if published in 2019. Otherwise 0 pt.\n  \n\n3. **Third Approach**: discrete score based on a point system. \n\n\\begin{equation*}\nInfluence Score = w_1 * NbQuotationsScore + w_2 * YearScore + w_3 * pageRankPublicationScore + w_4 * authorP2Score\n\\end{equation*}\n\n\n * **Nb_quotations Score**: +1 pts for Top 25%, + 0.50 pts for Top 50%, +0.25 pts for Top 75%. Otherwise 0 pt. \n * **Year Score**: +1 pts if published in 2020, +0.5 pts if published in 2019. Otherwise 0 pt.\n * **PageRank Publication Score**: +1 pts for Top 25%, + 0.50 pts for Top 50%, +0.25 pts for Top 75%. Otherwise 0 pt. \n * **Author P2 Score**: +1 pts for Top 25%, + 0.50 pts for Top 50%, +0.25 pts for Top 75%. Otherwise 0 pt. \n","352fe9d0":"# Part III: Influence Score consolidation for all the subset Research Papers","8c001d49":"## Constants"}}