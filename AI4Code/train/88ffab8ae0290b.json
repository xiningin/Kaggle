{"cell_type":{"0b87c546":"code","62b1bcfc":"code","573d5fbf":"code","c50f5c21":"code","b036d48f":"code","03ca70a6":"code","91b70ba1":"code","d39f59a9":"code","02287456":"code","2c4f33ab":"code","9cfed947":"code","aa1cc776":"code","bafa30e8":"code","cf5ed6f0":"code","c2f57dba":"code","993e581f":"code","2a8549e2":"code","0eceb70a":"code","e4360e65":"code","f1174e70":"code","1a4ad5b8":"code","deb5989f":"code","e9ceb9f1":"code","411b09ff":"code","3fb79106":"code","84168942":"code","88776eb0":"code","81a89316":"code","a6df5df9":"code","aa7b73f5":"code","23afd1de":"code","2b8d3de8":"code","1148664e":"code","c714beb6":"code","3f5e482d":"code","9f1e0ced":"code","65b3d4a0":"code","f60b3b79":"code","0d749a3a":"code","97d554b7":"code","f786b32d":"code","0c4137d5":"code","6e0141ae":"code","f4404241":"code","58ffde9b":"code","a75a0454":"code","a5f4eb47":"markdown","90d7e584":"markdown","3f4792c2":"markdown","dd189b59":"markdown"},"source":{"0b87c546":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62b1bcfc":"cd \/kaggle\/input\/adult-income-dataset\/","573d5fbf":"import itertools\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.preprocessing import OneHotEncoder\nfrom pandas import get_dummies\nimport matplotlib as mpl\nfrom sklearn import neighbors\nfrom sklearn import mixture\nfrom matplotlib import pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import multivariate_normal\nfrom sklearn.decomposition import LatentDirichletAllocation as lda\n#Almost none of these are useful","c50f5c21":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pymc3 as pm\n#This is the most important module\nimport arviz as az\nimport matplotlib.lines as mlines\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import OrderedDict\nimport theano\nimport theano.tensor as tt\nimport itertools\nfrom IPython.core.pylabtools import figsize\npd.set_option('display.max_columns', 30)\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","b036d48f":"\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pymc3 as pm\nimport random\nimport matplotlib\nimport plotnine\nfrom plotnine import ggplot, aes, geom_point, geom_jitter, geom_smooth, geom_histogram, geom_line, geom_errorbar, stat_smooth, geom_ribbon\nfrom plotnine.facets import facet_wrap\nimport seaborn as sns\nimport theano.tensor as tt\nfrom scipy.special import expit\nfrom scipy.special import logit\nfrom scipy.stats import cauchy\nfrom sklearn.metrics import roc_auc_score\nfrom skmisc.loess import loess\nimport warnings\nwarnings.filterwarnings(\"ignore\") # plotnine is causing all kinds of matplotlib warnings","03ca70a6":"#The only relevant biases are heuristics I can think of are - the availability effect - that people only see very \n#successful people from certain professions of TV; and so, assume that those professions are very high paying\n\n\n%matplotlib inline","91b70ba1":"#!pip install scikit-misc\n\n\n\n## Define custom functions\n\ninvlogit = lambda x: 1\/(1 + tt.exp(-x))\n\ndef trace_predict(trace, X):\n    y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(trace['beta']) )) )\n    return(y_hat)\n\n\n# Define prediction helper function\n# for more help see: https:\/\/discourse.pymc.io\/t\/how-to-predict-new-values-on-hold-out-data\/2568\ndef posterior_predict(trace, model, n=1000, progressbar=True):\n    with model:\n        ppc = pm.sample_posterior_predictive(trace,n, progressbar=progressbar)\n    \n    return(np.mean(np.array(ppc['y_obs']), axis=0))\n\n\n## I much prefer the syntax of tidyr gather() and spread() to pandas' pivot() and melt()\ndef gather( df, key, value, cols ):\n    id_vars = [ col for col in df.columns if col not in cols ]\n    id_values = cols\n    var_name = key\n    value_name = value\n    return pd.melt( df, id_vars, id_values, var_name, value_name )\n\n\ndef spread( df, index, columns, values ):\n    return df.pivot(index, columns, values).reset_index(level=index).rename_axis(None,axis=1)\n\n\n## define custom plotting functions\n\ndef fit_loess(df, transform_logit=False):\n    l = loess(df[\"value\"],df[\"target\"])\n    l.fit()\n    pred_obj = l.predict(df[\"value\"],stderror=True)\n    conf = pred_obj.confidence()\n    \n    yhat = pred_obj.values\n    ll = conf.lower\n    ul = conf.upper\n    \n    df[\"loess\"] = np.clip(yhat,.001,.999)\n    df[\"ll\"] = np.clip(ll,.001,.999)\n    df[\"ul\"] = np.clip(ul,.001,.999)\n    \n    if transform_logit:\n        df[\"logit_loess\"] = logit(df[\"loess\"])\n        df[\"logit_ll\"] = logit(df[\"ll\"])\n        df[\"logit_ul\"] = logit(df[\"ul\"])\n    \n    return(df)\n\n\ndef plot_loess(df, features):\n    \n    z = gather(df[[\"id\",\"target\"]+features], \"feature\", \"value\", features)\n    z = z.groupby(\"feature\").apply(fit_loess, transform_logit=True)\n    z[\"feature\"] = pd.to_numerc(z[\"feature\"])\n\n    plot = (\n        ggplot(z, aes(\"value\",\"logit_loess\",ymin=\"logit_ll\",ymax=\"logit_ul\")) + \n        geom_point(alpha=.5) + \n        geom_line(alpha=.5) + \n        geom_ribbon(alpha=.33) + \n        facet_wrap(\"~feature\")\n    )\n    return(plot)","d39f59a9":"data = pd.read_csv('adult.csv')","02287456":"data.head()","2c4f33ab":"data['native-country'].unique()","9cfed947":"#It should be useful to map the income distribution by country.","aa1cc776":"#It seems like it would be very useful to separate this dataframe into several dataframes by country.\n#Just because it's too large for me though, doesn't mean it's too large for the computer\nimport seaborn as sns\n# Make one plot for each different location\nsns.kdeplot([data['native-country'] == 'United-States', 'income'], \n            label = 'US', shade = True)\nsns.kdeplot(data.ix[df['native-country'] == 'Peru', 'income'], \n            label = 'Peru', shade = True)\n# Add labeling\nplt.xlabel('income')\nplt.ylabel('Density')\nplt.title('Density Plot of Incomes by Country')","bafa30e8":"## Load data\ndata = pd.read_csv('\/kaggle\/input\/adult-income-dataset\/adult.csv')\ny = np.asarray(data.target)\nX = np.array(data.ix[:, 2:302])\n#df2 = pd.read_csv('..\/input\/adult.csv')\n#df2.head()\n#X2 = np.array(df2.ix[:, 1:301])\n\nprint(\"training shape: \", X.shape)\nprint(\"test shape: \", X2.shape)","cf5ed6f0":"#data.shape","c2f57dba":"#data.head()\n#data['race'].unique()\ndata['occupation'].unique()","993e581f":"data.info","2a8549e2":"data['education'].unique()","0eceb70a":"data.columns","e4360e65":"data.drop(columns={'fnlwgt','marital-status','capital-gain','capital-loss'},inplace=True)","f1174e70":"X = data","1a4ad5b8":"#My main question is how the Regression distinguishes between Cardinal and Ordinal value inputs","deb5989f":"enc = OneHotEncoder()","e9ceb9f1":"X.columns","411b09ff":"enc.fit(X)","3fb79106":"enc.categories_\n#What does this do?","84168942":"#enc.get_feature_names(['workclass'])","88776eb0":"X.columns\nX = data[['age', 'workclass', 'education', 'educational-num', 'occupation',\n       'relationship', 'race', 'gender', 'hours-per-week', 'native-country',\n       'income']]","81a89316":"y = data[['income']]\nrandom.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\ndfp = gather(df[[\"id\",\"target\"]+rand_feats], \"feature\", \"value\", rand_feats)\nsns.set(style=\"ticks\")\n\nsns.pairplot(spread(dfp, \"id\", \"feature\", \"value\").drop(\"id\",1))\n\nplotnine.options.figure_size = (12,9)\n\nrandom.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\nplot_loess(df,rand_feats)\n\ndef make_model(X, y, cauchy_scale):\n    model = pm.Model()\n\n    with model:\n\n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta = pm.Cauchy('beta', alpha=0, beta=cauchy_scale, shape=X.shape[1])\n        mu = pm.math.dot(X, beta)\n\n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=invlogit(alpha + mu),  observed=y)\n    \n    model.name = \"linear_c_\"+str(cauchy_scale)\n    return(model)\n\n\n\nshape_par = .0175\n\nprior_df = pd.DataFrame({\"value\": np.arange(-2,2,.01)})\nprior_df = prior_df.assign(dens = cauchy.pdf(prior_df[\"value\"],0,shape_par))\ncauchy_samples = cauchy.rvs(0, shape_par, 10000)\n\nprint(\"percent non-zero coefs :\", 1-np.mean((cauchy_samples < .1) & (cauchy_samples > -.1)))\n\nplotnine.options.figure_size = (8,6)\nggplot(prior_df, aes(x=\"value\", y=\"dens\")) + geom_line()\n\n\n\n\n","a6df5df9":"# Histogram of grades\nplt.hist(data['income'], bins = 14)\nplt.xlabel('income')\nplt.ylabel('Variables')\nplt.title('Distribution of Incomes')","aa7b73f5":"#data.corr()['income'].sort_values()","23afd1de":"#the get dummies variable separates it into a hundred columns\n","2b8d3de8":"#dummy_df = pd.get_dummies(X)","1148664e":"#dummy_df.head()\ndummy_df.columns","c714beb6":"#Select only categorical variables\ncategory_df = data.select_dtypes('object')\n# One hot encode the variables\ndummy_df = pd.get_dummies(category_df)\n# Put the income back in the dataframe\ndummy_df['income'] = data['income']\n# Find correlations with income\ndummy_df.corr()['income'].sort_values()\n","3f5e482d":"\"\"\"from sklearn.model_selection import train_test_split\n# df is features and labels are the targets \n# Split by putting 25% in the testing set\nX_train, X_test, y_train, y_test = train_test_split(df, labels, \n                                                   test_size = 0.25,\n                                                    random_state=42)\"\"\"","9f1e0ced":"\"\"\"# X_train is our training data, we will make a copy for plotting\nX_plot = X_train.copy()\n# Compare grades to the median\nX_plot['relation_median'] = (X_plot['Grade'] >= 12)\nX_plot['Grade'] = X_plot['Grade'].replace({True: 'above', \n                                          False: 'below'})\n# Plot all variables in a loop\nplt.figure(figsize=(12, 12))\nfor i, col in enumerate(X_plot.columns[:-1]):\n    plt.subplot(3, 2, i + 1)\n    subset_above = X_plot[X_plot['relation_median'] == 'above']\n    subset_below = X_plot[X_plot['relation_median'] == 'below']\n    sns.kdeplot(subset_above[col], label = 'Above Median')\n    sns.kdeplot(subset_below[col], label = 'Below Median')\n    plt.legend()\n    plt.title('Distribution of %s' % col)\n    \nplt.tight_layout()\"\"\"","65b3d4a0":"comp = pm.stats.compare(model_dict, ic=\"LOO\", method='BB-pseudo-BMA')\n\n# generate posterior predictions for original data\nfor i in range(0,len(traces)):\n    y_hat = trace_predict(traces[i], X)\n    print(\"scale = \",cauchy_scale_pars[i],\", training AUCROC:\",roc_auc_score(y,y_hat))\n    \n# print comparisons\ncomp","f60b3b79":"comp_abridged = pm.stats.compare(dict(zip(models[0:-2], traces[0:-2])), ic=\"LOO\", method='BB-pseudo-BMA')\ncomp_abridged\n","0d749a3a":"model1 = models[5]\ntrace1 = traces[5]\nmodel1.name\n'","97d554b7":"coefs = pm.summary(trace1, varnames=[\"beta\"], alpha=.10)\n\ntop_coefs = (coefs\n             .assign(abs_est = abs(coefs[\"mean\"]), non_zero = np.sign(coefs[\"hpd_5\"]) == np.sign( coefs[\"hpd_95\"]))\n             .sort_values(\"abs_est\", ascending=False)\n            ).head(20)\n\ntop_coefs","f786b32d":"plotnine.options.figure_size = (12,9)\n\nregex = re.compile(\"__(.*)\")\ntop_feats = [regex.search(x)[1] for x in list(top_coefs.index)]\n\nplot_loess(df, top_feats)","0c4137d5":"def generate_submission(trace, file_suffix=\"\"):\n\n    test_predictions = trace_predict(trace, X2)\n\n    submission  = pd.DataFrame({'id':df2.id, 'target':test_predictions})\n    submission.to_csv(\"submission_\"+file_suffix+\".csv\", index = False)\n    return(None)\n\nfor model in model_dict.keys():\n    generate_submission(model_dict[model], model.name)","6e0141ae":"comp_MA = pm.stats.compare(dict(zip(models[0:-3], traces[0:-3])), ic=\"LOO\", method='BB-pseudo-BMA')\n\n# do prediction from averaged model\nppc_w = pm.sample_posterior_predictive_w(traces[0:-3], 4000, [make_model(X2,np.zeros(19750),c) for c in cauchy_scale_pars[0:-3]],\n                        weights=comp_MA.weight.sort_index(ascending=True),\n                        progressbar=True)\n                        \ny_hatMA = np.mean(np.array(ppc_w['y_obs']), axis=0)\nsubmission  = pd.DataFrame({'id':df2.id, 'target':y_hatMA})\nsubmission.to_csv('submission_MA.csv', index = False)\n","f4404241":"non_lin_feats = [\"276\",\"91\",\"240\",\"246\",\"253\",\"255\",\"268\",\"118\",\"240\",\"7\",\"167\",\"65\",\"33\"]\n\nplot_loess(df, non_lin_feats)\n","58ffde9b":"def make_polymodel(X, y):\n    \n    with pm.Model() as model:\n        \n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta1 = pm.Cauchy('beta', alpha=0, beta=.07, shape=X.shape[1])\n        beta2 = pm.Cauchy('beta^2', alpha=0, beta=.07, shape=X.shape[1])\n        beta3 = pm.Cauchy('beta^3', alpha=0, beta=.07, shape=X.shape[1])\n        \n        mu1 = pm.math.dot(X, beta1)\n        mu2 = pm.math.dot(np.power(X,2), beta2)\n        mu3 = pm.math.dot(np.power(X,3), beta3)\n        \n        p = invlogit(alpha + mu1 + mu2 + mu3)\n        \n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=p,  observed=y)\n        \n    return(model)","a75a0454":"!pip install scikit-misc\n\n\n\n## Define custom functions\n\ninvlogit = lambda x: 1\/(1 + tt.exp(-x))\n\ndef trace_predict(trace, X):\n    y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(trace['beta']) )) )\n    return(y_hat)\n\n\n# Define prediction helper function\n# for more help see: https:\/\/discourse.pymc.io\/t\/how-to-predict-new-values-on-hold-out-data\/2568\ndef posterior_predict(trace, model, n=1000, progressbar=True):\n    with model:\n        ppc = pm.sample_posterior_predictive(trace,n, progressbar=progressbar)\n    \n    return(np.mean(np.array(ppc['y_obs']), axis=0))\n\n\n## I much prefer the syntax of tidyr gather() and spread() to pandas' pivot() and melt()\ndef gather( df, key, value, cols ):\n    id_vars = [ col for col in df.columns if col not in cols ]\n    id_values = cols\n    var_name = key\n    value_name = value\n    return pd.melt( df, id_vars, id_values, var_name, value_name )\n\n\ndef spread( df, index, columns, values ):\n    return df.pivot(index, columns, values).reset_index(level=index).rename_axis(None,axis=1)\n\n\n## define custom plotting functions\n\ndef fit_loess(df, transform_logit=False):\n    l = loess(df[\"value\"],df[\"target\"])\n    l.fit()\n    pred_obj = l.predict(df[\"value\"],stderror=True)\n    conf = pred_obj.confidence()\n    \n    yhat = pred_obj.values\n    ll = conf.lower\n    ul = conf.upper\n    \n    df[\"loess\"] = np.clip(yhat,.001,.999)\n    df[\"ll\"] = np.clip(ll,.001,.999)\n    df[\"ul\"] = np.clip(ul,.001,.999)\n    \n    if transform_logit:\n        df[\"logit_loess\"] = logit(df[\"loess\"])\n        df[\"logit_ll\"] = logit(df[\"ll\"])\n        df[\"logit_ul\"] = logit(df[\"ul\"])\n    \n    return(df)\n\n\ndef plot_loess(df, features):\n    \n    z = gather(df[[\"id\",\"target\"]+features], \"feature\", \"value\", features)\n    z = z.groupby(\"feature\").apply(fit_loess, transform_logit=True)\n    z[\"feature\"] = pd.to_numerc(z[\"feature\"])\n\n    plot = (\n        ggplot(z, aes(\"value\",\"logit_loess\",ymin=\"logit_ll\",ymax=\"logit_ul\")) + \n        geom_point(alpha=.5) + \n        geom_line(alpha=.5) + \n        geom_ribbon(alpha=.33) + \n        facet_wrap(\"~feature\")\n    )\n    return(plot)","a5f4eb47":"Income correlates strongly with intellegnce, age and nationality.  It make correlate less strongly with other factors.","90d7e584":"**Exploratory Data Analysis**","3f4792c2":"**Clearly State Priors**","dd189b59":"**Data Cleaning**"}}