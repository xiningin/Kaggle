{"cell_type":{"d715110c":"code","f961d3ff":"code","f280e439":"code","8e3e690e":"code","c040e676":"code","f57139c3":"code","b9da0a70":"code","d2fe1d3f":"code","481a72dd":"code","cd0a4650":"code","83bf1c7a":"code","78fcc040":"code","6c847d20":"code","94257b5f":"code","f0aa894c":"code","a3801c82":"code","b593ac8e":"code","00e3eeb1":"code","9ceaba70":"code","18b5f448":"code","7fdf75f6":"markdown","a8ff97ec":"markdown","43f1019e":"markdown","7474ba58":"markdown","b5d74128":"markdown","7f2845b4":"markdown","59dd8a5c":"markdown","f719699e":"markdown","09f8901e":"markdown","15015295":"markdown","0e379999":"markdown","957547b6":"markdown","9661e334":"markdown","11aa88d1":"markdown","e260ab98":"markdown","2ca77eff":"markdown","e2072064":"markdown"},"source":{"d715110c":"## importing required libraries\n\n# Linear algebra\nimport numpy as np\n\n# Data Precessing\nimport pandas as pd\n\n# Data Visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithmns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn import preprocessing\n\n# Metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,mean_squared_error\nfrom sklearn.model_selection import GridSearchCV","f961d3ff":"df = pd.read_csv('..\/input\/titanic\/train.csv') # For working on remotely on Kaggle \n#df = pd.read_csv('train.csv')  # For working locally on pc\ndf.head(10)","f280e439":"df.columns","8e3e690e":"df.rename(columns={'Pclass':'Ticket_Class','SibSp':'Siblings\/Spouses','Parch':'Parents\/Children'},inplace=True)","c040e676":"df = df[['PassengerId','Name','Sex','Age','Ticket_Class','Ticket','Fare','Embarked','Cabin','Siblings\/Spouses', 'Parents\/Children','Survived']]","f57139c3":"df.describe()","b9da0a70":"df.isnull().sum()","d2fe1d3f":"# Dealing with missing values\n\n# Replace missing values with the mean in the 'Age' column\ndf.Age = df.Age.replace(np.nan,df.Age.mean())\n\n# Replacing missing values with the most frequesnt value\ndf.Embarked.value_counts() # 'S' is the most common value\ndf.Embarked = df.Embarked.replace(np.nan,'S')\n\ntitanic_df = df.dropna(axis=1)","481a72dd":"# choosing the target variable\ny = titanic_df.Survived\n\n# choosing features\nfeatures = titanic_df[['Sex', 'Age', 'Ticket_Class','Embarked', 'Siblings\/Spouses', 'Parents\/Children']]\n\n# Preprocessing Categorical Variables \nx_ = pd.get_dummies(features) # Once hot encording\n\n# Normalizing data\nx = preprocessing.StandardScaler().fit_transform(x_)\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","cd0a4650":"parameters = {'criterion': ['gini', 'entropy'],\n     'splitter': ['best', 'random'],\n     'max_depth': [2*n for n in range(1,10)],\n     'max_features': ['auto', 'sqrt'],\n     'min_samples_leaf': [1, 2, 4],\n     'min_samples_split': [2, 5, 10]}\n\ntree = DecisionTreeClassifier()\n\ngscv = GridSearchCV(tree,parameters,scoring='accuracy',cv=10)\ntree_cv = gscv.fit(x_train,y_train)","83bf1c7a":"print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\nprint(\"accuracy :\",tree_cv.best_score_)","78fcc040":"Decision_Tree = DecisionTreeClassifier(criterion='entropy',max_depth=4,max_features='auto',min_samples_leaf=2,min_samples_split=2,splitter='best')\nDecision_Tree.fit(x_train,y_train)\nyhat_dt = Decision_Tree.predict(x_test)","6c847d20":"parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'p': [1,2]}\n\nKNN = KNeighborsClassifier()\ngscv = GridSearchCV(KNN,parameters,scoring='accuracy',cv=10)\nknn_cv = gscv.fit(x_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\nprint(\"accuracy :\",knn_cv.best_score_)","94257b5f":"rfd = RandomForestClassifier()\nrfd.fit(x_train,y_train)\npredictions = rfd.predict(x_test)\n\nprint(accuracy_score(y_test,predictions))\nprint(mean_squared_error(y_test,predictions))","f0aa894c":"#final model\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x_train,y_train)\nyhat_knn = knn.predict(x_test)\n\nprint(f'Accuracy Score is: {accuracy_score(y_test,yhat_knn)}')","a3801c82":"parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n\nlr=LogisticRegression()\ngscv = GridSearchCV(lr,parameters,scoring='accuracy',cv=10)\nlogreg_cv = gscv.fit(x_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","b593ac8e":"lr = LogisticRegression(C=0.1, penalty='l2', solver='lbfgs')\nlr.fit(x_train,y_train)\nyhat_lr = lr.predict(x_test)","00e3eeb1":"svm_model = svm.SVC(kernel='linear',C=1)\nsvm_model.fit(x_train,y_train)\nyhat_svm = svm_model.predict(x_test)\n\nprint(accuracy_score(y_test,yhat_svm))","9ceaba70":"dic = {'Decision Tree':0.83*100,'Random Forest':0.82*100,'KNN':0.81*100,'Logistic Regression':0.79*100,'Support Vector Machine':0.78*100}\nscores = pd.DataFrame(list(dic.items()))\nscores.rename(columns={0:'Algorithm',1:'Accuracy Score'},inplace=True)\n\nscores","18b5f448":"#df_test = pd.read_csv('test.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\ndf_test.rename(columns={'Pclass':'Ticket_Class','SibSp':'Siblings\/Spouses','Parch':'Parents\/Children'},inplace=True)\n\n# Dealing with missing values\ndf_test.Age = df_test.Age.replace(np.nan,df_test.Age.mean())\ndf_test.Embarked = df_test.Embarked.replace(np.nan,'S')\n#df_test.dropna(inplace=True,axis=1)\n\n#Converting Categorical data to numerica using One Hot Encording\nfeatures = df_test[['Sex', 'Age', 'Ticket_Class','Embarked', 'Siblings\/Spouses', 'Parents\/Children']]\nx_test_submission = pd.get_dummies(features)\n\n# Normalizing data\nx_test_submission_ = preprocessing.StandardScaler().fit_transform(x_test_submission)\n\nsubmissions_predictions = Decision_Tree.predict(x_test_submission_)\n\n#making a submission\nids = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': submissions_predictions })\n \n#saving the dataframe as a .csv file for submission\noutput.to_csv('titanic_second_prediction.csv', index = False)","7fdf75f6":"## 4.4 KNN Model","a8ff97ec":"## Final submission","43f1019e":"## 5. Final Algorithm\n","7474ba58":"## 4.5 Logistic Regression","b5d74128":"Now we will train several Machine Learning models and compare their results.We need to use the predictions on the training set to compare the algorithms with each other. Later on, we will use cross validation.","7f2845b4":"# 4. Building Machine Learning Models","59dd8a5c":"# 1. Importing the Libraries","f719699e":"# 2. Loading the Data","09f8901e":"## 4.3 Random Forest","15015295":"Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the \u2018Age\u2019 feature.","0e379999":"## 4.1 Data Preprocessing","957547b6":"# 3. Data Exploration","9661e334":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the \u2018Age\u2019 feature, which has 177 missing values. The \u2018Cabin\u2019 feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","11aa88d1":"### Decision Tree has the highest accuracy score ","e260ab98":"## 4.2 Decision Tree","2ca77eff":"I am going to build 4 Machine Learning Algorithmns and choose the one with the highest accuracy score as the best Algorithm ","e2072064":"## 4.6 Support Vector Machine"}}