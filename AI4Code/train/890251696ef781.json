{"cell_type":{"c18d6ac0":"code","8e543589":"code","6c8756a2":"code","16b39839":"code","8e999e6b":"code","db2ec438":"code","838b24c5":"code","d7a02553":"code","f71ea474":"code","6c51e3cf":"code","123ab7cd":"code","aa183642":"code","06c6dc85":"code","c01e33a5":"code","2a2246e7":"code","c0ba810b":"code","c34849e1":"code","c2790a4c":"code","d6afed79":"code","7decc585":"code","67f80a13":"code","a00c5533":"markdown","6c456148":"markdown","3bbcb24e":"markdown","53fb1f79":"markdown","48390360":"markdown","3b6a852d":"markdown","3de9b396":"markdown","0cfb0ef4":"markdown","43bba327":"markdown","d260853e":"markdown","d860288b":"markdown","27d64f0f":"markdown","0e761a97":"markdown","8d49da20":"markdown"},"source":{"c18d6ac0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e543589":"# Load Data\ndf = pd.read_csv('..\/input\/nlp-tweet-sentiment-analysis\/bitcointweets.csv', header=None)\ndf = df[[1,7]]\ndf.columns = ['tweet','label']\ndf.head()","6c8756a2":"# inspect sentiment\nsns.countplot(df['label'])","16b39839":"# text length\ndf['text_length'] = df['tweet'].apply(len)\ndf[['label','text_length','tweet']].head()","8e999e6b":"g = sns.FacetGrid(df,col='label')\ng.map(plt.hist,'text_length')","db2ec438":"\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport re\n\ndef clean_text(s):\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n    s = re.sub(r'@\\S+', '', s)\n    s = re.sub('&amp', ' ', s)\n    return s\ndf['clean_tweet'] = df['tweet'].apply(clean_text)\n\ntext = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)\n\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","838b24c5":"# Encode Categorical Variable\nX = df['clean_tweet']\n# y = pd.get_dummies(df['label']).values\nencode_cat = {\"label\":     {\"['neutral']\": 0, \"['positive']\": 1, \"['negative']\": 2},\n             }\ny_df = df.replace(encode_cat)\ny = y_df['label']\ny.value_counts()","d7a02553":"seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)","f71ea474":"# Split Train Test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","6c51e3cf":"vocab_size = 20000  # Max number of different word, i.e. model input dimension\nmaxlen = 80  # Max number of words kept at the end of each text","123ab7cd":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.pipeline import TransformerMixin\nfrom sklearn.base import BaseEstimator\n\nclass TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    \"\"\" Sklearn transformer to convert texts to indices list \n    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n    def __init__(self,  **kwargs):\n        super().__init__(**kwargs)\n        \n    def fit(self, texts, y=None):\n        self.fit_on_texts(texts)\n        return self\n    \n    def transform(self, texts, y=None):\n        return np.array(self.texts_to_sequences(texts))\n        \nsequencer = TextsToSequences(num_words=vocab_size)","aa183642":"class Padder(BaseEstimator, TransformerMixin):\n    \"\"\" Pad and crop uneven lists to the same length. \n    Only the end of lists longernthan the maxlen attribute are\n    kept, and lists shorter than maxlen are left-padded with zeros\n    \n    Attributes\n    ----------\n    maxlen: int\n        sizes of sequences after padding\n    max_index: int\n        maximum index known by the Padder, if a higher index is met during \n        transform it is transformed to a 0\n    \"\"\"\n    def __init__(self, maxlen=500):\n        self.maxlen = maxlen\n        self.max_index = None\n        \n    def fit(self, X, y=None):\n        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n        return self\n    \n    def transform(self, X, y=None):\n        X = pad_sequences(X, maxlen=self.maxlen)\n        X[X > self.max_index] = 0\n        return X\n\npadder = Padder(maxlen)","06c6dc85":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import make_pipeline\n\nbatch_size = 128\nmax_features = vocab_size + 1\n\nimport tensorflow as tf \ntf.random.set_seed(seed)\n\ndef create_model(max_features):\n    \"\"\" Model creation function: returns a compiled LSTM\"\"\"\n    model = Sequential()\n    model.add(Embedding(max_features, 128))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","c01e33a5":"# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n# required by Scikit-learn for the last step of a Pipeline\nsklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=batch_size, \n                               max_features=max_features, verbose=1)\n\n# Build the Scikit-learn pipeline\npipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n\npipeline.fit(X_train, y_train);","2a2246e7":"print('Computing predictions on test set...')\n\ny_preds = pipeline.predict(X_test)","c0ba810b":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\ndef model_evaluate(): \n    \n    print('Test Accuracy:\\t{:0.1f}%'.format(accuracy_score(y_test,y_preds)*100))\n    \n    #classification report\n    print('\\n')\n    print(classification_report(y_test, y_preds))\n\n    #confusion matrix\n    confmat = confusion_matrix(y_test, y_preds)\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()","c34849e1":"model_evaluate()","c2790a4c":"# We choose a sample from test set\nidx = 15\ntest_text = np.array(X_test)\ntest_class = np.array(y_test)\ntext_sample = test_text[idx]\nclass_names = ['neutral', 'positive', 'negative']\nprint(text_sample)\nprint('Probability =', pipeline.predict_proba([text_sample]).round(3))\nprint('True class: %s' % class_names[test_class[idx]])\n","d6afed79":"from lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=class_names)\nexp = explainer.explain_instance(text_sample, pipeline.predict_proba, num_features=6, top_labels=2)\nexp.show_in_notebook(text=text_sample)\n","7decc585":"text_sample2 = re.sub('successful', ' ', text_sample)\nprint(text_sample2)","67f80a13":"print('Probability =', pipeline.predict_proba([text_sample2]).round(3))","a00c5533":"Let's rerun the prediction:","6c456148":"We are going to clean up the tweets, remove special chars, stop words, URL links, etc..","3bbcb24e":"Ok...now to interpret! You can see all the words highlighted in orange which contributes to the positive classification. The decimal values show the proportion of how much that word contributes to the class label.","53fb1f79":"Wow, as you can see, when you remove 'successful', the LSTM model class probabilities have indeed changed!","48390360":"Now let's run LIME!","3b6a852d":"Reason is that to use LIME text explainer, we need to use a sklearn pipeline","3de9b396":"As expected, most tweets are very short in length.","0cfb0ef4":"Lime is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a probability for each class. Support for scikit-learn classifiers is built-in.","43bba327":"The word 'successful' is a major reason for the positive classification, let's see what happens when we remove it!","d260853e":"So far, so good. Our LSTM model predicts the correct class for this sample text.","d860288b":"Majority of tweets are neutral and positive. Looks like there are not much negative tweets on Bitcoin! No wonder the price is skyrocketing!","27d64f0f":"Good, our LSTM model seems very accurate on the test set. Now for the interesting part in using LIME.","0e761a97":"This notebook will show an example of text classification using a standard LSTM, followed by using LIME library((https:\/\/github.com\/marcotcr\/lime))","8d49da20":"From here, we need to create our LSTM model and use the KerasClassifier in keras.wrappers.scikit_learn."}}