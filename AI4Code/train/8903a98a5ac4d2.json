{"cell_type":{"a0e4f8a5":"code","2d923b90":"code","1c73b65f":"code","47b2fde2":"code","3b327929":"code","0433f263":"code","f8436811":"code","3b0caa88":"code","3f1979b7":"code","40ccfb3b":"code","801c4fbd":"code","6debab05":"code","7a1b97fd":"code","c9b810ce":"code","42594851":"code","1ced2dde":"code","81052c4d":"code","a55002af":"code","53b2ad03":"code","4940e578":"code","9f8204d2":"code","3e1db6a7":"code","3cea3125":"code","08c25f5b":"code","7ecc44ad":"markdown","6abd5d93":"markdown","a765ce10":"markdown","318d734a":"markdown","e60395bc":"markdown","65e026c6":"markdown","e048dac9":"markdown","776b458d":"markdown","ad34892c":"markdown","4f2ba764":"markdown","66771473":"markdown","464c7106":"markdown","79a44b01":"markdown","3c1b1468":"markdown","b8428c2c":"markdown"},"source":{"a0e4f8a5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","2d923b90":"cities = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCity.csv')\nrio = cities.loc[cities['City'] == 'Amsterdam', ['dt','AverageTemperature']]\nrio.columns = ['Date','Temp']\nrio['Date'] = pd.to_datetime(rio['Date'])\nrio.reset_index(drop=True, inplace=True)\nrio.set_index('Date', inplace=True)\n\nrio = rio.loc['1900':'2013-01-01']\nrio = rio.asfreq('M', method='bfill')\nrio.head()","1c73b65f":"plt.figure(figsize=(22,6))\nsns.lineplot(x=rio.index, y=rio['Temp'])\nplt.title('Temperature Variation in Amsterdam from 1900 until 2012')\nplt.show()","47b2fde2":"rio['month'] = rio.index.month\nrio['year'] = rio.index.year\npivot = pd.pivot_table(rio, values='Temp', index='month', columns='year', aggfunc='mean')\npivot.plot(figsize=(20,6))\nplt.title('Yearly Rio temperatures')\nplt.xlabel('Months')\nplt.ylabel('Temperatures')\nplt.xticks([x for x in range(1,13)])\nplt.legend().remove()\nplt.show()","3b327929":"monthly_seasonality = pivot.mean(axis=1)\nmonthly_seasonality.plot(figsize=(20,6))\nplt.title('Monthly Temperatures in Amsterdamo')\nplt.xlabel('Months')\nplt.ylabel('Temperature')\nplt.xticks([x for x in range(1,13)])\nplt.show()","0433f263":"year_avg = pd.pivot_table(rio, values='Temp', index='year', aggfunc='mean')\nyear_avg['10 Years MA'] = year_avg['Temp'].rolling(10).mean()\nyear_avg[['Temp','10 Years MA']].plot(figsize=(20,6))\nplt.title('Yearly AVG Temperatures in Amsterdam')\nplt.xlabel('Months')\nplt.ylabel('Temperature')\nplt.xticks([x for x in range(1900,2012,3)])\nplt.show()","f8436811":"train = rio[:-60].copy()\nval = rio[-60:-12].copy()\ntest = rio[-12:].copy()","3b0caa88":"baseline = val['Temp'].shift()\nbaseline.dropna(inplace=True)\nbaseline.head()","3f1979b7":"def measure_rmse(y_true, y_pred):\n    return sqrt(mean_squared_error(y_true,y_pred))\n\nrmse_base = measure_rmse(val.iloc[1:,0],baseline)\nprint(f'The RMSE of the baseline that we will try to diminish is {round(rmse_base,4)} celsius degrees')","40ccfb3b":"def check_stationarity(y, lags_plots=48, figsize=(22,8)):\n    \"Use Series as parameter\"\n    \n    y = pd.Series(y)\n    fig = plt.figure()\n\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)\n    ax2 = plt.subplot2grid((3, 3), (1, 0))\n    ax3 = plt.subplot2grid((3, 3), (1, 1))\n    ax4 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n\n    y.plot(ax=ax1, figsize=figsize)\n    ax1.set_title('Amsterdam Temperature Variation')\n    plot_acf(y, lags=lags_plots, zero=False, ax=ax2);\n    plot_pacf(y, lags=lags_plots, zero=False, ax=ax3);\n    sns.distplot(y, bins=int(sqrt(len(y))), ax=ax4)\n    ax4.set_title('Distribution Chart')\n\n    plt.tight_layout()\n    \n    print('Results of Dickey-Fuller Test:')\n    adfinput = adfuller(y)\n    adftest = pd.Series(adfinput[0:4], index=['Test Statistic','p-value','Lags Used','Number of Observations Used'])\n    adftest = round(adftest,4)\n    \n    for key, value in adfinput[4].items():\n        adftest[\"Critical Value (%s)\"%key] = value.round(4)\n        \n    print(adftest)\n    \n    if adftest[0].round(2) < adftest[5].round(2):\n        print('\\nThe Test Statistics is lower than the Critical Value of 5%.\\nThe serie seems to be stationary')\n    else:\n        print(\"\\nThe Test Statistics is higher than the Critical Value of 5%.\\nThe serie isn't stationary\")","801c4fbd":"check_stationarity(train['Temp'])","6debab05":"check_stationarity(train['Temp'].diff(12).dropna())","7a1b97fd":"def walk_forward(training_set, validation_set, params):\n    history = [x for x in training_set.values]\n    prediction = list()\n    \n    pdq, PDQS, trend = params\n    for week in range(len(validation_set)):\n        model = sm.tsa.statespace.SARIMAX(history, order=pdq, seasonal_order=PDQS, trend=trend)\n        result = model.fit(disp=False)\n        yhat = result.predict(start=len(history), end=len(history))\n        prediction.append(yhat[0])\n        history.append(validation_set[week])\n        \n    return prediction","c9b810ce":"val['Pred'] = walk_forward(train['Temp'], val['Temp'], ((3,0,0),(0,1,1,12),'c'))","42594851":"rmse_pred = measure_rmse(val['Temp'], val['Pred'])\n\nprint(f\"The RMSE of the SARIMA(3,0,0),(0,1,1,12),'c' model was {round(rmse_pred,4)} celsius degrees\")\nprint(f\"It's a decrease of {round((rmse_pred\/rmse_base-1)*100,2)}% in the RMSE\")","1ced2dde":"val['Error'] = val['Temp'] - val['Pred']","81052c4d":"def plot_error(data, figsize=(20,8)):\n    plt.figure(figsize=figsize)\n    ax1 = plt.subplot2grid((2,2), (0,0))\n    ax2 = plt.subplot2grid((2,2), (0,1))\n    ax3 = plt.subplot2grid((2,2), (1,0))\n    ax4 = plt.subplot2grid((2,2), (1,1))\n    \n    ax1.plot(data.iloc[:,0:2])\n    ax1.legend(['Real','Pred'])\n    ax1.set_title('Current and Predicted Values')\n    \n    ax2.scatter(data.iloc[:,1], data.iloc[:,2])\n    ax2.set_xlabel('Predicted Values')\n    ax2.set_ylabel('Errors')\n    ax2.set_title('Errors versus Predicted Values')\n    \n    sm.graphics.qqplot(data.iloc[:,2], line='r', ax=ax3)\n    plot_acf(data.iloc[:,2], lags=(len(data.iloc[:,2])-1),zero=False, ax=ax4)\n    plt.tight_layout()\n    plt.show()","a55002af":"val.drop(['month','year'], axis=1, inplace=True)\nval.head()","53b2ad03":"plot_error(val)","4940e578":"future = pd.concat([train['Temp'], val['Temp']])\nfuture.head()","9f8204d2":"model = sm.tsa.statespace.SARIMAX(future, order=(3,0,0), seasonal_order=(0,1,1,12), trend='c')\nresult = model.fit(disp=False)","3e1db6a7":"test['Pred'] = result.predict(start=(len(future)), end=(len(future)+13))","3cea3125":"test[['Temp', 'Pred']].plot(figsize=(22,6))\nplt.title('Current Values compared to the Extrapolated Ones')\nplt.show()","08c25f5b":"test_baseline = test['Temp'].shift()\n\ntest_baseline[0] = test['Temp'][0]\n\nrmse_test_base = measure_rmse(test['Temp'],test_baseline)\nrmse_test_extrap = measure_rmse(test['Temp'], test['Pred'])\n\nprint(f'The baseline RMSE for the test baseline was {round(rmse_test_base,2)} celsius degrees')\nprint(f'The baseline RMSE for the test extrapolation was {round(rmse_test_extrap,2)} celsius degrees')\nprint(f'That is an improvement of {-round((rmse_test_extrap\/rmse_test_base-1)*100,2)}%')","7ecc44ad":"Below I'll try to make a brief explanation about ARIMA models:\n\n# SARIMA Model (p, d, q)(P, D, Q, S):\nSARIMA stands for Seasonal Auto Regressive Integrated Moving Average, The name scares, but this is not as scary as it seems.\n\n## Non seasonal:\n\nWe can split the Arima term into three terms, AR, I, MA:\n\n * **AR(p)** stands for *autoregressive model*, the `p` parameter is an integer that confirms how many lagged series are going to be used to forecast periods ahead, example:\n     * The average temperature of yesterday has a high correlation with the temperature of today, so we will use AR(1) parameter to forecast future temperatures.\n     * The formula for the AR(p) model is: $\\hat{y}_{t} = \\mu + \\theta_{1}Y_{t-1} + ... + \\theta_{p}Y_{t-p}$ Where $\\mu$ is the constant term, the **p** is the periods to be used in the regression and $\\theta$ is the parameter fitted to the data.\n     \n * **I(d)** is the differencing part, the `d` parameter tells how many differencing orders are going to be used, it tries to make the series stationary, example:\n \n     * Yesterday I sold 10 items of a product, today I sold 14, the \"I\" in this case is just the first difference, which is +4, if you are using logarithm base this difference is equivalent to percentual difference. \n     * If d = 1: $y_{t} = Y_{t} - Y_{t-1}$ where $y_{t}$ is the differenced series and $Y_{t-period}$ is the original series\n     * If d = 2: $y_{t} = (Y_{t} - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) = Y_{t} - 2Y_{t-1} + Y_{t-2}$\n     * Note that the second difference is a change-in-change, which is a measure of the local \"acceleration\" rather than trend.\n\n* **MA(q)** stands for *moving average model*, the `q` is the number of lagged forecast errors terms in the prediction equation, example:\n     * It's strange, but this MA term takes a percentage of the errors between the predicted value against the real. It assumes that the past errors are going to be similar in future events.\n     * The formula for the MA(p) model is: $\\hat{y}_{t} = \\mu - \\Theta_{1}e_{t-1} + ... + \\Theta_{q}e_{t-q}$ Where $\\mu$ is the constant term, **q** is the period to be used on the $e$ term and $\\Theta$ is the parameter fitted to the errors\n     * The error equation is $ e_{t} = Y_{t-1} - \\hat{y}_{t-1}$\n     \n## Seasonal:\n\nThe **p, d, q** parameters are capitalized to differ from the non seasonal parameters.\n\n* **SAR(P)** is the seasonal autoregression of the series.\n    * The formula for the SAR(P) model is: $\\hat{y}_{t} = \\mu + \\theta_{1}Y_{t-s}$ Where P is quantity of autoregression terms to be added, usually no more than 1 term, **s** is how many periods ago to be used as base and $\\theta$ is the parameter fitted to the data.\n    * Usually when the subject is weather forecasting, 12 months ago have some information to contribute to the current period.\n    * Setting P=1 (i.e., SAR(1)) adds a multiple of $Y_{t-s}$ to the forecast for $y_{t}$\n    \n* **I(D)** the seasonal difference MUST be used when you have an strong and stable pattern.\n     * If d = 0 and D = 1: $y_{t} = Y_{t} - Y_{t-s}$ where $y_{t}$ is the differenced series and $Y_{t-s}$ is the original seasonal lag.\n     * If d =1 and D = 1: $y_{t} = (Y_{t} - Y_{t-1}) - (Y_{t-s} - Y_{t-s-1}) = Y_{t} - Y_{t-1} -Y_{t-s} + Y_{t-s-1}$\n     * D should never be more than 1, and d+D should never be more than 2. Also, if d+D =2, the constant term should be suppressed.\n     \n* **SMA(Q)** \n     * Setting Q=1 (i.e., SMA(1)) adds a multiple of error $e_{t-s}$ to the forecast for $y_{t}$\n\n\n* **S** It's the seasonal period where you are going to calculate the the P, D, Q terms. If there is a 52 week seasonal correlation this is the number to be used on the 'S' parameter\n\n## Trend:\n  \nWe will use SARIMAX to create a forecast, the following terms are a definition to the trend:\n\n * 'n' when there is no trend to be used (default).\n * \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial)\n * \u2018t\u2019 indicates a linear trend with time\n * \u2018ct\u2019 is both trend and constant. \n * Can also be specified as an iterable defining the polynomial as in numpy.poly1d, where [1,1,0,1] would denote a+bt+ct3","6abd5d93":"The series has an interesting behavior, there is a sequential significative negative autocorrelation starting at lag 6 and repeating each 12 months, it's because of the difference in the seasons, if today is winter with cold temperatures in 6 months we will have higher temperatures in the summer, that's why the negative autocorrelation occurs. These temperatures usually walk in opposite directions.\n\nAlso, from lag 12 and sequentially from every 12 lags there is a significant positive autocorrelation. The **PACF** shows a positive spike in the first lag and a drop to negative **PACF** in the following lags.\n\nThis behavior between the **ACF** and **PACF** plots suggests an AR(1) model and also a first seasonal difference ($Y_{t} - Y_{t-12}$). I'll plot the stationarity function again with the first seasonal difference to see if we will need some SAR(P) or SMA(Q) parameter:","a765ce10":"Also I'm going to create a function to use the [RMSE](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation) as a base to calculate the error, but you are free to use another parameter:","318d734a":"Now, let's plot the series and check how it behaves","e60395bc":"It seems that the SARIMA parameters were well fitted, the predicted values are following the real values and also the seasonal pattern.\n\nFinally I'll evaluate the model with the RMSE in the test set (baseline against the extrapolation):","65e026c6":"Now i'm going to check if there is some trend through the years in this Series:","e048dac9":"We can confirm that there is a constant increasing trend and that the average temperature increased from 23.5\u00ba to 24.5\u00ba, that's 4.25% in over100 years.\n\nBefore we go on, i'm going to split the data in training, validation and test set. After training the model, I will use the last 5 years to do the data validation and test, being 48 months to do a month by month validation (walk forward) and 12 months to make an extrapolation for the future and compare to the test set:","776b458d":"# Modeling\n\nThere are several things that are time dependent, I mean, today's values can have an effective relationship to values that have occurred in the past.\n\nSome examples related to the subject are demand of products during a certain period, harvest of commodities, stock prices and of course what we will try to predict, the climate change in Amsterdam.\n\nFirst we need to import the essential libraries:","ad34892c":"It's always important to check the residuals, I'm going to create a function to plot some important charts to help us visualize the residuals.\n\nI'm going to plot the following charts:\n* Current and Predicted values through the time.\n* Residuals vs Predicted values in an scatterplot.\n* QQ Plot showing the distribution of errors and its ideal distribution\n* Autocorrelation plot of the Residuals to see if there is some correlation left.","4f2ba764":"Analyzing the plots above we can see that the predictions fit very well on the current values.\n\nThe **Error vs Predicted values** has a linear distribution (the errors are between -1.5 and +1.5 while the temperature increases).\n\nThe QQ Plot shows a normal pattern with some little outliers and,\n\nThe autocorrelation plot shows a positive spike over the confidence interval just above the second lag, but I believe that there is no need for more changes.\n\nFinally it's time to extrapolate the prediction in the **test set** for the last 12 months","66771473":"The series clearly has some seasonality, the higher temperatures are around November and February and the lower are between July and September. Just to make the things clear, I'll merge these lines into just one line, averaging the monthly levels:","464c7106":"As the plots above showed, the first **ACF** lags have a gradual decay, while the **PACF** drops under the confidence interval after the third lag, this is an **AR** signature with a parameter of 3, so this is an **AR(3)** model.\n\nAs we used a first seasonal difference, the **ACF** and **PACF** showed a significative drop in the 12th lag, it means an **SMA** signature with a parameter of 1 lag, resuming this is an **SAR(1) with a first difference**.\n\nInitially i'm going to work with the following (p,d,q) orders: (3, 0, 0), and with the following seasonal (P, D, Q, S) orders (0,1,1,12) and as the series has a clear uptrend i'm going to use it in the model ('c'). \n \n To start forecasting the validation set, I'm going to create a function to use one-step-forecast in the whole validation set and measure the error:","79a44b01":"And before creating the forecasts we will create a baseline forecast in the validation set, in our simulation we will try to have a smaller error compared to this one.\n\nit will consider the previous month as a base forecast to the next month:","3c1b1468":"Now I'm going to create a new column on the test set with the predicted values and I will compare them against the real values","b8428c2c":"As we can see, the series has a small uptrend and it appears that there is some seasonality with higher temperatures at the begining and end of the year and lower temperatures around the middle of the year.\n\nTo create a time series forecast, the series must be stationary (constant mean, variance and autocorrelation).\n\nOne way to check if the series is stationary is using the **adfuller function**, if the P-Value is lower than 5% (usual number used for this kind of study) the series is stationary and you can start creating your model. \n\nIf the series isn't stationary you can do some data transformation like using natural logarithm, deflation, differencing, etc.\n\nBelow is the function that I used to check the stationarity, it plots: \n\n * The series itself;\n * The autocorrelation function **(ACF)**:\n      * It shows the correlation between the current temperatures versus the lagged versions of itself.\n * The partial autocorrelation **(PACF)**:\n     * It shows the correlation between the current temperatures versus the lagged version excluding the effects of earlier lags, for example, it show the effective influence of the lag 3 in the current temperatures excluding the effects of the lags 1 and 2.\n\nFor more interesting sources you can read the materials on this amazing website made by Mr. Robert Nau: [ Duke University](http:\/\/people.duke.edu\/~rnau\/411home.htm), also you can check [Jason Brownlee's](machinelearningmastery.com) website, which have a lot of time series content."}}