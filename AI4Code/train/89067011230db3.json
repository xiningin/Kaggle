{"cell_type":{"ffe16021":"code","4d1b77b8":"code","1f133f2e":"code","d626c2cd":"code","4004fbec":"code","6b3a4a21":"code","1f03023e":"code","54e93a3c":"code","6947f891":"code","73bf9279":"code","3cdbe6ae":"code","17bce70a":"code","76778ed4":"code","01d74fd7":"code","89f63216":"code","b5007631":"markdown","d8c5cd07":"markdown","4f13b32c":"markdown","7d3442c8":"markdown","21329cbc":"markdown","16048390":"markdown","3ef7fcc9":"markdown","e6a51800":"markdown","43c9ee4f":"markdown","f28fbdcd":"markdown","4476ac35":"markdown","fca9f589":"markdown","e26c5493":"markdown","2fdde6ec":"markdown","8daf0b76":"markdown","77046550":"markdown","8207c83c":"markdown","675310ef":"markdown"},"source":{"ffe16021":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom langdetect import detect\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk import ngrams\nfrom nltk.corpus import stopwords\n\nplt.style.use('fivethirtyeight') \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","4d1b77b8":"stocks=pd.read_csv(\"..\/input\/financial-tweets\/stocks_cleaned.csv\")\nstockerbot=pd.read_csv(\"..\/input\/financial-tweets\/stockerbot-export.csv\",error_bad_lines=False) #Some lines prevent the file from being loaded, we'll just skip them here\nstockerbot=stockerbot.drop(columns=['id'])\n","1f133f2e":"stockerbot[\"timestamp\"] = pd.to_datetime(stockerbot[\"timestamp\"])\nstockerbot[\"text\"] = stockerbot[\"text\"].astype(str)\nstockerbot[\"url\"] = stockerbot[\"url\"].astype(str)\n\nstockerbot[\"company_names\"] = stockerbot[\"company_names\"].astype(\"category\")\nstockerbot[\"symbols\"] = stockerbot[\"symbols\"].astype(\"category\")\nstockerbot[\"source\"] = stockerbot[\"source\"].astype(\"category\")\nstockerbot.dtypes","d626c2cd":"stockerbot['date'] = stockerbot['timestamp'].dt.date\nstockerbot['time'] = stockerbot['timestamp'].dt.time","4004fbec":"stockerbot.isnull().any() #company_namaes & url have missing values\nstockerbot[stockerbot['company_names'].isnull()] #1 line\nstockerbot[stockerbot['url'].isnull()] #6369 lines : yet they are verified\n\n#stockerbot  = stockerbot[stockerbot['verified'] == True] #Dropping unverified tweets (~10% of all tweets)\n\n","6b3a4a21":"ts = pd.Series(stockerbot['date'].values, index=stockerbot['date'])\nstockerbot['date'].value_counts().sort_values()  # Why is there such a spike on 2018-07-18? More than 90% of the tweets take place from 07-16 to 07-18.","1f03023e":"total_companies = stockerbot[\"symbols\"].value_counts() #Quoted companies\ntotal_sources = stockerbot[\"source\"].value_counts() #Different sources : \nprint(total_sources)\ntotal_sources.head(50).plot.bar() #Allows us to see the biggest sources and how they are leveled. \n#3 biggest sources are quoteed ~900 times, three nexts ~620 times, then it falls to ~350\n\n#Room for improvement: Check the sources confidence, see how much tweets are verified, etc.\n\n\n","54e93a3c":"print(total_companies)\ntotal_companies.head(50).plot.bar()","6947f891":"stop = stopwords.words('english')\nstop.append(\"RT\")\nurl_regex = \"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\nhandle_regex= \"^@?(\\w){1,15}$\"\n\n\nstockerbot['text']=stockerbot['text'].str.replace(url_regex, '')\nstockerbot['text']=stockerbot['text'].str.replace(handle_regex, '')\n\nstockerbot['text']=stockerbot['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","73bf9279":"stockerbot = stockerbot[stockerbot[\"source\"] != \"test5f1798\"]","3cdbe6ae":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(stockerbot['text'])\nfrequencies = sum(sparse_matrix).toarray()[0]","17bce70a":"top_2grams = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\ntop_2grams\ntop_2grams.head(50).plot.bar() #Allows us to see the biggest sources and how they are leveled. \n","76778ed4":"word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(stockerbot['text'])\nfrequencies = sum(sparse_matrix).toarray()[0]","01d74fd7":"top_3grams = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\ntop_3grams\ntop_3grams.head(50).plot.bar()\n# what is \"amp\" we had \"amp amp\" first and now \"amp amp amp\"","89f63216":"unique_companies=np.array(stockerbot[\"symbols\"].unique())\n\ncompanies_df={}\n\n\n\nfor company in unique_companies:\n    if (len(stockerbot[stockerbot[\"symbols\"] == company])>=65): # 50% of the companies has been quoted at least 65 times (thanks to total_companies.describe())\n        word_vectorizer = CountVectorizer(ngram_range=(2,3), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(stockerbot.loc[stockerbot[\"symbols\"] == company,['text']]['text'])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        \n        ### Now let's run bi\/tri gram analysis (for each company and keep the 10 most relevant for each.)\n        companies_df[company]=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False).head(10)    \n\n\ncompanies_df\n","b5007631":"## Casting types to work with defined types and not undefined objects","d8c5cd07":"### Tri-grams","4f13b32c":"##\u00a0Tweets by company names","7d3442c8":"# Univariate analysis","21329cbc":"# Text analysis\n## Most common n-grams","16048390":"## Create separate date & time columns for easier manipulation","3ef7fcc9":"###\u00a0Notes\nSo far it seems we can't make assumption about the bi\/tri grams. They give us information about what's happening but we don't have anything related to strong negative \/ positive feelings ","e6a51800":"## Tweets repartition in time","43c9ee4f":"## Reading files","f28fbdcd":"### Bi\/Tri grams based on company names","4476ac35":"\n## Removing meaningless terms\n### Stopwords\nLet's remove stopwords from text, like url, handle (@Author), or specific useless words (\"RT\")","fca9f589":"##\u00a0Tweets by source","e26c5493":"# Sentiment analysis","2fdde6ec":"###\u00a0Comments\nWe can get more sense out of the ngrams when genereated for given companies. Ex, for Texas Instrument we have \"code conduct violation\" that comes on top. For master card, we have information about a new patent. \nMost of the times, the ngrams are \"join us\", \"register bonus\" or assimilated, so we have limited informations about finance themselves.","8daf0b76":"## Useless tweets\nRemove tweets from source that doesn't provide anything","77046550":"## Missing values","8207c83c":"### This notebook is mainly for me, getting a hand on python and trying data cleaning \/ text analysis on real exemple","675310ef":"### Bi-grams"}}