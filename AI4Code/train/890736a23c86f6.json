{"cell_type":{"1285ce62":"code","72d495ce":"code","71aacc0a":"code","ee23477e":"code","74161bdc":"code","9facd221":"code","48979fa1":"code","d8c468f6":"code","1a1bd0f8":"code","a6b7e7bd":"code","64d8e2bb":"code","896fb0e6":"code","c4762d3a":"markdown","34c9157e":"markdown","b9318918":"markdown","bdeb5a83":"markdown","592a335f":"markdown","95e24eb0":"markdown","522ea6c7":"markdown"},"source":{"1285ce62":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nimport pandas_profiling\nfrom datetime import datetime\nfrom itertools import combinations, chain\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","72d495ce":"train_df = pd.read_csv('..\/input\/learn-together\/train.csv')\ntest_df = pd.read_csv('..\/input\/learn-together\/test.csv')\nID = test_df['Id']","71aacc0a":"\n\ndef main(train_df, test_df):\n    # this is public leaderboard ratio\n    start = datetime.now()\n    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n    \n    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n    \n    # Aspect\n    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]\/180)\n    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]\/180)\n    print(\"Aspect\", (datetime.now() - start).seconds)\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) \/ (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    \n    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n    print(\"Hillshade\", (datetime.now() - start).seconds)\n    \n    # Hydrology ** I forgot to add arctan\n    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) \/\n                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n    \n    # Holizontal\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) \/ (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    print(\"Horizontal\", (datetime.now() - start).seconds)\n    \n    \n    def categorical_post_mean(x):\n        p = (x.values)*type_ratio\n        p = p\/p.sum()*x.sum() + 10*type_ratio\n        return p\/p.sum()\n    \n    # Wilder\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n    wilder[\"one\"] = 1\n    piv = wilder.pivot_table(values=\"one\",\n                             index=\"Wilder_Type\",\n                             columns=\"Cover_Type\",\n                             aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    print(\"Wilder_type\", (datetime.now() - start).seconds)\n    \n    \n    # Soil type\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    print(\"Soil_type\", (datetime.now() - start).seconds)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    return total_df\n\ntotal_df = main(train_df, test_df)\none_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\ntotal_df = total_df.drop(one_col, axis=1)","ee23477e":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\ntest = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)","74161bdc":"from sklearn.ensemble import ExtraTreesClassifier\nimport statistics \nscores = []\n\nfor i in range(10):\n    x_train, x_test, y_train, y_test = train_test_split(X,y, train_size = 0.6, shuffle = True, random_state=1)\n\n    model = ExtraTreesClassifier()\n    model.fit(x_train,y_train)\n    scores.append((accuracy_score(y_test,model.predict(x_test) )))  \netc_preds = model.predict(test)\nprint(round(statistics.mean(scores),4))","9facd221":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nonehotencoder = OneHotEncoder()\ny2 = onehotencoder.fit_transform(y.reshape(-1,1)).toarray()","48979fa1":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\ntest = sc_X.transform(test)","d8c468f6":"optimizer = 'adam'\nkernel_initializer= 'Orthogonal'\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 47, kernel_initializer=kernel_initializer, activation = 'relu', input_dim = 47))\nclassifier.add(Dropout(rate = 0.1))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 200, kernel_initializer= kernel_initializer, activation = 'relu'))\nclassifier.add(Dropout(rate = 0.1))\n\n\n# Adding the third hidden layer\nclassifier.add(Dense(units = 50, kernel_initializer= kernel_initializer, activation = 'relu'))\nclassifier.add(Dropout(rate = 0.1))\n\n# Adding the output layer\nclassifier.add(Dense(units = 7, kernel_initializer= kernel_initializer, activation = 'softmax'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X,y2, batch_size = 1000, epochs = 10000, callbacks=[tf.keras.callbacks.TensorBoard('logs')])\n\n#Predicting values for classes [classes from 1-7, values predicted from 0-6 so add 1]\npreds = classifier.predict_classes(test) + 1","1a1bd0f8":"\n'''\nif you're running this on your own computer you can use the code below to see the model and loss curve etc\n\n# Load TENSORBOARD\n%load_ext tensorboard\n# Start TENSORBOARD\n%tensorboard --logdir logs --host localhost\n\n'''","a6b7e7bd":"# predcitions by NN\npd.Series(preds).value_counts()","64d8e2bb":"# predictions by extra trees classifier\npd.Series(etc_preds).value_counts()  # not scaled","896fb0e6":"submission = pd.DataFrame({ 'Id': ID,\n                            'Cover_Type': preds })\nsubmission.to_csv(\"submission_ann1.csv\", index=False)","c4762d3a":"### Now lets look at the results ","34c9157e":"### The model\n\nThis is not optimsed too heavily, I was mainly just playing around with random numbers and layers. This is a work in progress so I hope to learn more how to optimise these types of networks","b9318918":"## Now for the modeling\n\nI will use an untuned Extra trees classifer as a comparator for my NN as this  classifier performed best in my previous testing.","bdeb5a83":"Scale the data for the NN, It does not work without scaling unlike a decision tree. Not sure why.\n\nAlso, you need to have the target encoded which is what y2 is doing ","592a335f":"Please, if you have any suggestions of how I can improve these, please comment below.","95e24eb0":"My previous kernel is here - https:\/\/www.kaggle.com\/jakelj\/basic-ensemble-model\n\nI decided that although I enjoy decision trees, and I especially like the idea of a decision tree predicting the type of tree, I would try and learn a bit about Neural networks so here is my work in progress.\n\nInstead of starting over again with feature engineering I looked over some of the past notebooks and used there code -- stand on the shoulders of giants and all that.","522ea6c7":"Original code for feature engineering here 'https:\/\/www.kaggle.com\/nadare\/eda-feature-engineering-and-modeling-4th-359' by Nadare"}}