{"cell_type":{"787af93d":"code","fb29b2ec":"code","9f9aa3ba":"code","2696b008":"code","8ce0e779":"code","bb69db2d":"code","599d843f":"code","d907130c":"code","65449548":"code","916a0d66":"code","425a8117":"code","e991e207":"code","722644fe":"code","ca762218":"code","d4f3f07a":"code","9f49e19d":"code","cc1aab9d":"code","9743e45e":"code","1b64ad7a":"code","b2adc963":"code","1727eb80":"code","36499bf2":"code","794615d6":"code","73ff69a6":"code","55becfbc":"code","271ce746":"code","29ee7099":"code","b5d90bff":"code","fb2db508":"code","c57421d5":"code","9985c4b1":"code","c269a9d0":"code","7833b6a8":"code","0fce7343":"code","342999cc":"code","9c235f62":"code","f8ebcff5":"code","8ac721ee":"code","0a4208bd":"code","3c65d931":"code","2df5a003":"code","b9715989":"code","36f3ed37":"code","78fde57f":"code","5dc55606":"code","2cbb6849":"code","d05018eb":"code","9ba8f639":"code","18af611d":"code","4c916b86":"code","68bb8153":"code","3af1d2a0":"code","d4b3def4":"code","b942c37d":"code","a5f1978e":"code","847d25af":"code","e20fcb1b":"code","8e810298":"code","d9ca7dbf":"code","c28d7a09":"code","36c5acff":"code","0ec40f0a":"code","52d97fd4":"markdown","95893586":"markdown","ec049b2d":"markdown","6cfa01ac":"markdown","7dc49510":"markdown","30f5acd3":"markdown","8a2cf9a5":"markdown","e0f30083":"markdown","53a1e592":"markdown","3c852de3":"markdown","d6c44083":"markdown","d5d1830f":"markdown","c0428484":"markdown","5dd77276":"markdown","cc2ab582":"markdown","8961fe49":"markdown","4aa4c065":"markdown","1a6ae277":"markdown","8cc155b3":"markdown","e0109074":"markdown","c93ab804":"markdown","046d6a3e":"markdown","31b71e9a":"markdown","62868193":"markdown","3e24b6e9":"markdown","5b09aa75":"markdown","18b9438d":"markdown","0eb210d9":"markdown","6f557aa8":"markdown","0f971ade":"markdown","01feb0f9":"markdown","b3883ad8":"markdown","3cceb034":"markdown","1b6168e7":"markdown","59e4daa5":"markdown","7612c590":"markdown","1bf659c6":"markdown","97913be4":"markdown","44ae9559":"markdown","e3fdbccf":"markdown","3af81566":"markdown","d98532cd":"markdown","9a2678dc":"markdown","78f32256":"markdown","94a31e47":"markdown","0026771d":"markdown","e4352ce6":"markdown"},"source":{"787af93d":"# Libs to deal with tabular data\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\n\n# Plotting packages\nimport seaborn as sns\nsns.axes_style(\"darkgrid\")\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# Machine Learning\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom lightgbm import LGBMClassifier\n\n# Optimization\n!pip uninstall optuna -y\n!pip uninstall typing -y\n!pip install optuna==2.3.0\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour, plot_optimization_history\nfrom optuna.visualization import plot_param_importances, plot_slice\n\n\n# To display stuff in notebook\nfrom IPython.display import display, Markdown\n\n# Misc tqdm.notebook.tqdm\nfrom tqdm.notebook import tqdm\nimport time","fb29b2ec":"shots = pd.read_csv('..\/input\/nba-shot-logs\/shot_logs.csv')","9f9aa3ba":"shots.sample(5)","2696b008":"shots.shape","8ce0e779":"shots.columns = shots.columns.str.lower()\nshots.dtypes","bb69db2d":"shots.isnull().sum()","599d843f":"with pd.option_context('display.max_columns', None):\n    display(shots[shots['game_id'] == shots['game_id'].sample().iloc[0]].head(10))","d907130c":"shots['touch_time'].value_counts()","65449548":"(shots['touch_time'] < 0).sum()","916a0d66":"with pd.option_context('display.max_columns', None):\n    display(shots.loc[shots['touch_time'] < 0, :].head())","425a8117":"# Convert game clock to seconds\nshots['game_clock'] = shots['game_clock'].apply(\n    lambda x: 60*int(x.split(':')[0]) + int(x.split(':')[1])\n)\n\n# Replacing abnormal values with NaNs\nshots.loc[shots['touch_time'] < 0, 'touch_time'] = np.nan\n\n# Converting type of shot (2 or 3 points) to categorical\nshots['pts_type'] = (shots['pts_type'] == 3) * 1\n\n# Converting location\nshots['location'] = (shots['location'] == 'H') * 1\n\n# Renaming columns\nshots = shots.rename(columns = {\n    'fgm':'hit',\n    'pts_type':'3pts_shot',\n    'location':'home_match'\n})\n\n# Dropping informative columns (not useful to modelling) as well as \n# future variables which won't be available at predicting time\nshots = shots.drop(columns = [\n    'game_id',\n    'matchup',\n    'w',\n    'final_margin',\n    'closest_defender_player_id',\n    'player_id',\n    'shot_result',\n    'closest_defender',\n    'player_name',\n    'pts'\n])","e991e207":"shots.head(5)","722644fe":"shots.shape","ca762218":"def plot_distribution(col, bins=10):\n    shots[col].plot.hist(bins=bins)\n    plt.title(col, fontsize=16)\n    plt.show()\n    \ndef plot_relationship(x, y):\n    sns.boxplot(data = shots, y = y, x = x)\n    plt.title('{} vs {}'.format(x, y), fontsize=16)\n    plt.show()\n    \ndef show_frequency(x, y):\n    joint = pd.crosstab(shots[x], shots[y], margins = True)\n    joint = joint \/ joint.loc['All', 'All']\n    display(joint)\n    \ndef plot_scatter(x, y):\n    sns.scatterplot(data = shots_scaled, x = x, y = y)\n    plt.title('{} vs {}'.format(x, y), fontsize=16)\n    plt.show()","d4f3f07a":"ax = shots['hit'].replace({\n    0:'Miss',\n    1:'Hit'\n}).value_counts().plot.bar(rot=0)\nax.set_title('hit', fontsize=16)\nplt.show()","9f49e19d":"shots['hit'].replace({\n    0:'Miss',\n    1:'Hit'\n}).value_counts() \/ shots.shape[0]","cc1aab9d":"ax = shots['home_match'].replace({\n    0:'Away',\n    1:'Home'\n}).value_counts().plot.bar(rot=0)\nax.set_title('home_match', fontsize=16)\nplt.show()\n\nax = shots['period'].value_counts().plot.bar(rot=0)\nax.set_title('period', fontsize=16)\nplt.show()\n\nplot_distribution('game_clock', bins=10)","9743e45e":"ax = shots['3pts_shot'].replace({\n    0:'2 points',\n    1:'3 points'\n}).value_counts().plot.bar(rot=0)\nax.set_title('3pts_shot', fontsize=16)\nplt.show()","1b64ad7a":"plot_distribution('shot_dist', bins=25)","b2adc963":"plot_distribution('shot_clock', bins=20)","1727eb80":"plot_distribution('close_def_dist', bins=40)","36499bf2":"plot_distribution('shot_number', bins=10)\nplot_distribution('dribbles', bins=30)\nplot_distribution('touch_time', bins=20)","794615d6":"shots.describe()","73ff69a6":"show_frequency('3pts_shot', 'hit')","55becfbc":"plot_relationship('hit', 'close_def_dist')","271ce746":"plot_relationship('hit', 'shot_dist')\nplot_relationship('hit', 'touch_time')\nplot_relationship('hit', 'dribbles')","29ee7099":"plot_relationship('hit', 'shot_clock')\nplot_relationship('3pts_shot', 'close_def_dist')","b5d90bff":"shots_scaled = shots.copy()\n\n# Stardardization\nshots_scaled[['shot_clock']] = preprocessing.StandardScaler().fit_transform(shots[['shot_clock']].values)\n\n# Robust scaling\nskewed_cols = ['shot_number', 'dribbles', 'touch_time', 'close_def_dist']\nshots_scaled[skewed_cols] = preprocessing.RobustScaler().fit_transform(shots[skewed_cols].values)\n    \n# Min max transformation\nmin_max_cols = ['period', 'game_clock', 'shot_dist']\nshots_scaled[min_max_cols] = preprocessing.MinMaxScaler().fit_transform(shots[min_max_cols].values)\n\n# Filling NaNs with mean\nshots_scaled['shot_clock'] = shots_scaled['shot_clock'].fillna(shots_scaled['shot_clock'].mean())\nshots_scaled['touch_time'] = shots_scaled['touch_time'].fillna(shots_scaled['touch_time'].median())\nshots['shot_clock'] = shots['shot_clock'].fillna(shots['shot_clock'].mean())\nshots['touch_time'] = shots['touch_time'].fillna(shots['touch_time'].median())","fb2db508":"x_corr = shots_scaled[['shot_number', 'shot_clock', 'dribbles', 'touch_time', 'shot_dist', 'close_def_dist', 'period', 'game_clock']].corr()\ncorr_mask = np.zeros_like(x_corr, dtype=np.bool)\ncorr_mask[np.tril_indices_from(corr_mask, k=0)] = True\n\nsns.heatmap(x_corr, mask = corr_mask, annot=True)\nplt.show()","c57421d5":"plot_scatter('dribbles', 'touch_time')","9985c4b1":"plot_scatter('shot_dist', 'close_def_dist')","c269a9d0":"plot_scatter('period', 'shot_number')","7833b6a8":"# Fitting PCA and showing PVE\npca = PCA(random_state=42).fit(shots_scaled.drop('hit', axis=1).values)\npve = pca.explained_variance_ratio_","0fce7343":"plt.plot(range(1, len(pve) + 1), pve.cumsum())\nplt.title('Cumulative sum of explained variance', fontsize=16)\nplt.xlabel('Components')\nplt.ylabel('Percentage (%)')\nplt.show()","342999cc":"pca_components = pd.DataFrame(\n    pca.components_[:4,:].T,\n    index = shots.columns[:-1]\n)\nsns.heatmap(pca_components, annot=True)","9c235f62":"n_clusters = range(2, 41)\nkms, inertias = [], []\nfor n in tqdm(n_clusters):\n    ts = time.time()\n    km = KMeans(n_clusters=n, random_state=42)\n    km.fit(shots_scaled.drop('hit', axis=1).values)\n    kms.append(km)\n    inertias.append(km.inertia_)","f8ebcff5":"# Checking the within-cluster variation.\nplt.plot(range(2,41), inertias)\nplt.title('Within-cluster variance', fontsize=16)\nplt.xlabel('Number of clusters')\nplt.ylabel('Sum of squared distances')\n# plt.show()","8ac721ee":"# Splitting original dataset\nX = shots.drop('hit', axis = 1).values\ny = shots['hit'].values\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n\n# Splitting scaled and transformed by PCA dataset\nX_scaled = pca.transform(shots_scaled.drop('hit', axis = 1).values)[:,:4]\nx_train_scaled, x_test_scaled, _, _ = train_test_split(X_scaled, y, test_size = 0.2, random_state=42)","0a4208bd":"def decision_tree_cv(x, y, folds=5):\n    cv = KFold(folds, random_state=42, shuffle=True)\n    depths = list(range(1, 101))\n    scores = np.zeros((len(depths), folds, 2, 2)) #depth, fold, split, metric\n    \n    for id_split, array_idxs in tqdm(enumerate(cv.split(x))):\n        train_index, val_index = array_idxs[0], array_idxs[1]\n        x_train, x_val = x[train_index], x[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n        \n        for depth in depths:\n            clf = DecisionTreeClassifier(max_depth=depth, random_state=42).fit(x_train, y_train)\n            scores[depth - 1, id_split, 0, 0] = clf.score(x_train, y_train)\n            scores[depth - 1, id_split, 1, 0] = clf.score(x_val, y_val)\n            scores[depth - 1, id_split, 0, 1] = roc_auc_score(y_train, clf.predict_proba(x_train)[:,1])\n            scores[depth - 1, id_split, 1, 1] = roc_auc_score(y_val, clf.predict_proba(x_val)[:,1])\n            \n    return scores\n\ndef report_cv(scores):\n    sns.lineplot(data = pd.DataFrame(\n        scores.mean(1)[:,:, 1], \n        index = list(range(1, 101)), \n        columns = ['train', 'test']\n    ))\n    plt.show()\n    \n    val_scores = scores.mean(1)[:, 1]\n    \n    print('Best model')\n    print('***********')\n    print('Mean validation accuracy: ', scores.mean(1)[:, 1, 0].max())\n    print('Mean validation AUC: ', scores.mean(1)[:, 1, 1].max())\n    print('Depth of the best model: ', scores.mean(1)[:, 1, 1].argmax() + 1)","3c65d931":"scores = decision_tree_cv(x_train, y_train)","2df5a003":"report_cv(scores)","b9715989":"scores_pca = decision_tree_cv(x_train_scaled, y_train)","36f3ed37":"report_cv(scores_pca)","78fde57f":"depth_best_model = scores.mean(1)[:, 1, 1].argmax() + 1\nclf = DecisionTreeClassifier(max_depth=depth_best_model, random_state=42).fit(x_train, y_train)","5dc55606":"importances = pd.Series(clf.feature_importances_, index=shots.columns[:-1]).sort_values(ascending=False)\n\nsns.barplot(x = importances.values, y = importances.index, orient='h', palette='Reds_r')\nplt.title('Gini importance of features', fontsize=16)\nplt.show()","2cbb6849":"perm_importances = permutation_importance(\n    clf,\n    x_train,\n    y_train,\n    scoring = 'roc_auc',\n    n_repeats = 10,\n    n_jobs = -1\n)\ndf_perm_importances = pd.DataFrame(perm_importances.importances, index=shots.columns[:-1]).T\ndf_perm_importances = df_perm_importances.melt(\n    value_vars = df_perm_importances.columns,\n    var_name = 'feature',\n    value_name = 'importance'\n)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(data = df_perm_importances, x = 'importance', y = 'feature')\nplt.title('Permutation importance using AUC (train set)', fontsize=16)\nplt.show()","d05018eb":"plt.figure(figsize=(14,10))\nplot_tree(\n    clf,\n    feature_names = shots.columns[:-1], \n    class_names = ['Miss', 'Hit'],\n    filled = True,\n    impurity = False,\n    max_depth = 2\n)\nplt.show()","9ba8f639":"plot_tree(\n    clf,\n    feature_names = shots.columns[:-1], \n    class_names = ['Miss', 'Hit'],\n    filled = True,\n    impurity = False\n)\nplt.show()","18af611d":"param_grid = {\n    'n_estimators': [100],\n    'criterion': ['gini'],\n    'max_depth': [5, 10, 15, 20, 25, 50],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [2, 5, 10, 15],\n    'max_features': ['sqrt']\n}\n\ngrid_search_cv = GridSearchCV(\n    estimator = RandomForestClassifier(n_jobs = -1, random_state = 42),\n    param_grid = param_grid,\n    scoring = ['accuracy', 'roc_auc'],\n    n_jobs = -1,\n    refit = 'roc_auc',\n    cv = 5,\n    verbose = 3,\n    return_train_score = True    \n).fit(x_train, y_train)","4c916b86":"scores = pd.DataFrame(grid_search_cv.cv_results_).sort_values('rank_test_roc_auc')\n\nprint('Best model')\nprint('***********')\nprint('Mean validation accuracy: ', scores.iloc[0, :]['mean_test_accuracy'])\nprint('Mean validation AUC: ', grid_search_cv.best_score_, '\\n')\nprint('Best hyperparameters')\nprint('***********')\nfor param, val in grid_search_cv.best_params_.items():\n    print(param + ':', val)","68bb8153":"def report_train_test(scores, param, name):\n    df = scores.groupby(param).mean()[['mean_train_roc_auc', 'mean_test_roc_auc']]\n    df.columns = ['Train', 'Test']\n    df.index.name = name\n    sns.lineplot(data = df)\n    plt.ylabel('ROC AUC')\n    plt.title('Tuning AUC based on {}'.format(name.lower()), fontsize = 14)\n    plt.show()","3af1d2a0":"report_train_test(scores, 'param_max_depth', 'Max depth')\nreport_train_test(scores, 'param_min_samples_leaf', 'Min samples on leaves')\nreport_train_test(scores, 'param_min_samples_split', 'Min samples on internal nodes')","d4b3def4":"perm_importances = permutation_importance(\n    grid_search_cv.best_estimator_,\n    x_train,\n    y_train,\n    scoring = 'roc_auc',\n    n_repeats = 10,\n    n_jobs = -1\n)\ndf_perm_importances = pd.DataFrame(perm_importances.importances, index=shots.columns[:-1]).T\ndf_perm_importances = df_perm_importances.melt(\n    value_vars = df_perm_importances.columns,\n    var_name = 'feature',\n    value_name = 'importance'\n)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(data = df_perm_importances, x = 'importance', y = 'feature')\nplt.title('Permutation importance using AUC (train set)', fontsize=16)\nplt.show()","b942c37d":"class Light_GBM_CV:\n    def __init__(self, x, y, folds=5, random_state=42):\n        # Hold this implementation specific arguments as the fields of the class.\n        self.x = x\n        self.y = y\n        self.folds = folds\n        self.random_state = random_state\n\n    def __call__(self, trial):\n        cv = KFold(\n            self.folds, \n            random_state = self.random_state, \n            shuffle=True\n        )\n        \n        clf = LGBMClassifier(\n            boosting_type = 'gbdt',\n            objective = 'binary',\n            metric = 'auc',\n            random_state = self.random_state,\n            num_leaves = trial.suggest_int('num_leaves', 16, 256),\n            max_depth = trial.suggest_int('max_depth', 4, 8),\n            learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1.0),\n            min_child_samples = trial.suggest_int('min_child_samples', 5, 100),\n            n_estimators = trial.suggest_int('n_estimators', 10, 200),\n            lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1.0),\n            lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-5, 1.0),\n            max_bin = trial.suggest_int('max_bin', 50, 256)\n        )\n        \n        scores = []\n\n        for array_idxs in cv.split(self.x):\n            train_index, val_index = array_idxs[0], array_idxs[1]\n            x_train, x_val = self.x[train_index], self.x[val_index]\n            y_train, y_val = self.y[train_index], self.y[val_index]\n\n            clf.fit(\n                x_train, \n                y_train,\n                eval_set = (x_val, y_val),\n                early_stopping_rounds = 5,\n                verbose = False\n            )\n            scores.append(clf.best_score_['valid_0']['auc'])\n\n        return sum(scores) \/ self.folds","a5f1978e":"lgbm_cv = Light_GBM_CV(x_train, y_train)\nstudy = optuna.create_study(sampler=TPESampler(seed = 42), direction='maximize')\nstudy.optimize(lgbm_cv, n_trials=1000)","847d25af":"print('Best model')\nprint('***********')\nprint('Mean validation AUC: ', study.best_value, '\\n')\nprint('Best hyperparameters')\nprint('***********')\nfor param, val in study.best_params.items():\n    print(param + ':', val)","e20fcb1b":"plot_optimization_history(study)","8e810298":"plot_slice(study)","d9ca7dbf":"plot_contour(study, params=['num_leaves', 'min_child_samples', 'n_estimators'])","c28d7a09":"plot_param_importances(study)","36c5acff":"lgbm_final_clf = LGBMClassifier(\n    boosting_type = 'gbdt',\n    objective = 'binary',\n    metric = 'auc',\n    random_state = 42,\n    **study.best_params\n)\n\nlgbm_final_clf.fit(\n    x_train, \n    y_train,\n    eval_set = (x_test, y_test),\n    early_stopping_rounds = 5\n)","0ec40f0a":"perm_importances = permutation_importance(\n    lgbm_final_clf,\n    x_test,\n    y_test,\n    scoring = 'roc_auc',\n    n_repeats = 10,\n    n_jobs = -1\n)\ndf_perm_importances = pd.DataFrame(perm_importances.importances, index=shots.columns[:-1]).T\ndf_perm_importances = df_perm_importances.melt(\n    value_vars = df_perm_importances.columns,\n    var_name = 'feature',\n    value_name = 'importance'\n)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(data = df_perm_importances, x = 'importance', y = 'feature')\nplt.title('Permutation importance using AUC (train set)', fontsize=16)\nplt.show()","52d97fd4":"The rank of for the first four variables didn't change between methods, but the shot type (2 or 3 points) seems to have higher importance according to the permutation method.\n\n### Visualizing the tree\n\nBelow you can see a representation of the decision tree. Notice that, since the tree has 7 levels, it is too deep to be visualized. So we are only showing the first 2 levels (not counting the root node).","95893586":"## Conclusions\n\nAfter fitting three different, it's clear that Light GBM is the winner. I've got approximately 64.4% in the cross-validation AUC and in the test set (which wasn't used) we have about 63.8% AUC. It's important to remeber that 50% AUC is the threshold for random predictions, so after all our model isn't very good. But it's probably due to our restrictions on the variables used. If we kept and explored things such as player performance in the game or in the tournament, we could have a better result. \n\nOne takeaway from this study is that it doesn't matter how good or complex is the model we are using. Most of the accuracy comes from good predictive data, that is, if we don't have data with quality and predictive power, the gains from using more advanced models will be marginal.\n\n**If you found this notebook interesting, please give it an upvote. Thanks for reading!**","ec049b2d":"For me, the shot clock distribution is interesting. It is kind of normal and has a peak in the end, which can be shots originated from rebounds. ","6cfa01ac":"This plot above shows how after some time optune figured out the best parameters and kept exploring its neighborhood to try to improve the objective value (in our case the cross-validated AUC).","7dc49510":"## Scaling data and filling missing values\n\nNow we must scale the distributions and fill NaNs with placeholds in order to apply unsupervised learning techniques and futher explore the data. In the following sections, we are going to apply K-Means and PCA, which are sensitive to measurement units. In the ideal setting, each distribution should have the same unit so that the algorith gives them the importance. In other words, we don't want a PCA component being dominated by one feature just because its measument unit is greater than the others. \n\nBelow I will apply transformations to make variables variance more equal. Also, depndending on the distribution I filled missing values with the mean or the median, which are central statics.","30f5acd3":"Indeed, most leaves at the left side predict a hit, while the right side predicts a miss.\n\n## Random Forest\n\nNow that we created a baseline of our problem using a decision tree, it's time to use more powerful models. But before we explore random forests, it's important to understand why decision trees aren't very good. A very common problem is that they can easily overfit. We tried to avoid this behaviour by controlling the depth of the tree, but sometimes it's not enough. More sophisticated ways of controling the decision tree are using cost-complexity pruning or tuning other hyperparameters. The problem is that fitting a decision tree with this added complexity can become time consuming. Another problem is that slightly changes in the dataset can drastically change the structure of the tree. In other words, decision trees usually have high variance.\n\nRandom forests were proposed to address some of these issues. A random forest is actually composed of set of decision trees, each one independently fitted on a resampled version of the training set. The main ideia is to use bootstraping to decresase the variance of the prediction by \"averaging\" (actually in classification we use the majority vote) the result of the trees. In addition to that, in order to make each indidual tree even more different, random forests only consider a limited number of features in each split. To sum it up, random forests offer two advantages:\n\n- It creates variability by using a different dataset (resampled) in each tree.\n- Reduces the impact of very important or correlated features, allowing the trees to explore different ways of splitting the feature space.\n\nImportant hyperparameters in the model are the number of trees and the number of features, but it's always good to control the depth of the tree or the sample size in each split or leave to avoid overfitting. In this case, I'm going to set the number of trees to 100, which is probably high enough to get a good set of different trees, and I'm going to use $\\sqrt{11}$ variables in each split, which is a recommended number in the book Introduction to Statistical Learning. Finally, I'm going to employ grid search to figure out the best model configuration on the other hyperparameters.\n\nTo do so, I'm going to use scikit-learn's Grid Search class, which provides an easy interface to run a grid search over a cross-validation.","8a2cf9a5":"Notice that roughly one third to the shots are behind the three points line.","e0f30083":"### Gini importance\n\nFirst, let's take a look at the feature importances.","53a1e592":"As I expected, accuracy and AUC improved, althought not too much. Now let's see how our model performs in the train and test set. Since we fitted 3 hyperparameters, we can't visualize all combinations at the same time. Thus I'm going to inspect each one individually and average the scores to have one score by parameters value.  ","3c852de3":"#### Final dataset\n\nNotice that I dropped categorical features like player name and defender name. I did so because I want to create a model that predicts a miss or a hit regardless of the player making the shot. In other words, I want to use game features, not \"personal\" features. ","d6c44083":"Also, shots are more frequently converted when the defensive player is far from the shooter and the ofensive team has more shot time to develop the attack.","d5d1830f":"The shot distance is probably bimodal because of the three points field goal. It has a radius of 23 ft (7 m) to the basket and this distance is very close to the second peak. Also notice that there are some outliers above 30 ft, which probably happened due to the game clock.","c0428484":"## Visualizing data\n\nLet's first visualize variables individually and then inspect their relationships.","5dd77276":"Notice how the distribution of closest defensor distance when the shot is hit has much more outliers. A theory that can explain that are rebound and counter-attacks. ","cc2ab582":"## Decision Tree\n\nFirst I will use a decision tree due to its interpretability and ease of use. The algorithm's core ideia is to split the feature space in smaller regions so that each of these regions are as pure as possible. In other words, we want the regions to have mostly one class. These splits can be interpreted as rules that divide the space in two. For example, we can split the data whether the shot distance is greater or lower than 15 ft. \n\nSince this algorithm sequencially split the feature space in two regions, we can think of it as a binary tree where each node represents a split (i.e. a rule) and the leaves represents regions of the space where the purity should be higher. In classification tasks, the training set is used to figure out these rules and create good leaves. Then, to predict an instance we follow these rules and use the points contained in the leave to devise a prediction, which is usually the most frequent class among these points.\n\nIf we let the tree grow indefinitely, we would end up overfitting the training set. That's because each subsequent split will be more specific and it will end up fitting the noise of the data, instead of capturing the general patterns. So it is necessary to regularize it controlling the depth of tree, i.e., the number of splits used. Another option is to use cost-complexity pruning, but controlling depth is easier and more efficient.\n\nIn this part I'm going to implement a function to train and test the decision tree using cross-validation. Then I will save the results and report them using a graph to see if our model has overffited.","8961fe49":"The graph below is tipically the first option to assess the K-Means performance. The y-axis is the sum of the distances between each point and its respectively centroid. It will always descrease and we need to find a point where adding a new cluster won't lead to much better segmentation. In this case, that point can't be clearly seen.","4aa4c065":"## Principal component analysis\n\nPrincipal component analysis is a dimensionality reduction algorithm that tries to find linear combinations of variables that explain as much variance as possible. It is very useful when we have correlated variables and we want to reduce them into a single variable, which makes the dataset less redundant and complex.\n\nPCA works in steps, creating components sequentially. In each step it tries to find a linear combination of features, which creates a new feature, whose variance is the maximum possible. Starting from the second component, the components are required to be uncorrelated to preceding new features. An alternative interpretation is that when we find a component with maximum variance, we are actually finding a line in the feature space such that the projections over it have the maximum variance. And uncorrelated components actually mean perpendicular lines. So, when we apply PCA we find a new set of axis that better describes the dataset.","1a6ae277":"The slice plot show how each variable interacts with the objective value. Notice how in the latter trials the objective is closer to the maximum value.","8cc155b3":"## Clustering with K-means\n\nApply K-Means is usually useful because it can show if there are clusters of instances that we can analyze deeper. It an iterative technique that uses euclidian distance and works better with spherical clusters. It has a hyperparameter that is the number of clusters we want to find. Most of the time we don't know this, so I ran it with different numbers.","e0109074":"The importance rank didn't change, but now we can see that variable that had very little importance in the decision tree, such as home match, shot number and period, actually are more important in the random forest. Also, shuffling a variable has a greater AUC impact on the random forest than in the decision tree.\n\n## Gradient Boosting Machine\n\nInstead of fitting a number of trees in parallel, another way to improve decision trees is by stacking them. The core idea of a boosting algorithm is to use a sequence of weak learners to gradually, step by step, learn a function that best approximate our target. Generally these weak learners are decision trees with a small number of leaves (small depth) that constraint the number of interactions between variables. For example, if we have $d$ leaves, at most $d-1$ variables were used in the tree.\n\nIn addition to that, each tree is fitted based on the residuals of the approximated funcion up until that tree. Notice that for this to work in a classification task, the prediction of tree must be the odds. This allows the weaker learners to focus on specific areas where the previous learners didn't perform well. Thus, the final function will be the sum of the predictions of each tree. However, we can do two more things to further improve boosting algorithms. First, we can multiply each tree's prediction by a shrinkage parameter to slow even more the process. This parameter can be fixed and tuned using grid search, but in Grandient Boosting Machines this shrinkage parameter is found by using gradient descent. So in each boosting step, we solve a optimization problem where we want to minimize a loss function that depends on the shrinkage. Second, we can multiply the shrinkage parameter and the weak learner prediction by a learning rate. This learning rate is fixed and should be fine-tuned.\n\nTherefore, typically we have three essential parameters to fine-tune and avoid overfitting: number of tree, depth of leaves and learning rate. If we want to get the best perfomance out of GBM's, we could also search for the best parameters related to the tree construction. Notice that if let the algorithm build a very long stack of learners it will easily overfit. Instead of grid searching each value for the number of tree, we can use the test set to access when to stop stacking models.\n\nTo evaluate this model and figure out the best hyperparameter values, I'm going to use LightGMB and Optuna. The latter is a optimization package that implements a bayesian approach to make a guided search in the hyperparameter space. ","c93ab804":"Now that we ran hundreds of hyperparameters trials, we can use some visualizations provided by optuna to see how the optimization worked and to see the hyperparameters interactions. ","046d6a3e":"There are some coefficients higher than 0.5. Let's take a look at their scatter plot. The explanation for these relationships are kind of straightforward, for example the number of dribbles increase as the touch time (ball possession of the shooter) increases.","31b71e9a":"Notice that the dataset is almost balanced, so we can use metrics like accuracy and AUC to assess a model performance.","62868193":"The three plots above tell us that hits tipically have smaller touch times, shot distances and dribbles.","3e24b6e9":"#### Ten rows of a random match","5b09aa75":"#### Inspecting touch_time","18b9438d":"In these first 7 nodes, 6 of them split the feature space using the features shot distance and closest defensor distance, which are the most important features based on the graphs aboves. Also, it seems like the left side of the tree is predicting hit while the right side predicts miss. Let's visualize the entire tree to see the leave colors, not the content. ","0eb210d9":"Now it is also useful to inspect the coefficients of the linear combinations that generated each component. We can see that:\n\n- The first component is mostly about dribbles and touch time, which are correlated. We can interpret it as a variable describing how the player acted before the shoot.\n- The second one is dominated by shot clock. It tell us if the shot involved team work.\n- The third component can be describe as a mesure of the difficulty of the shot. It accounts for closest defensor distance, shot distance and shot type.\n- The last one describes the match situation, that is, if the player is warmed-up.","6f557aa8":"## Data preparation\n\nIn this section I will take a first look at the data and transform it into a dataframe more suitable to machine learning.\n\n#### Random sample","0f971ade":"### Dataset transformed by PCA","01feb0f9":"The closest player distance distribution tell us that most defensors tend to be up to 5 ft far from the shooter.","b3883ad8":"### Features and target relationships\n\nNow, let's visualize if there is a relation between explanatory variables and our target. Below I show only the relevant relations.","3cceb034":"- The distributions above are kind of uniform, except the period. In this case, if a game ends up in a tie, new periods are played until the match has a winner.\n- Notice that, while it not so significant, as the game progress, the number of shot attemps is reduced. Thus, in early game players tend to play faster and with more risk.","1b6168e7":"One way to select the number of components is to analyze the amount of variance a component explains compared to the original variances. In the graph above, I show how much variance is explained by adding a new component. Starting from the fourth component, the curve doesn't increases as much as before so that it creates an elbow. We can can stop here by arguing that adding more component won't lead to a much better result.  ","59e4daa5":"## Interpreting the decision tree\n\nIn this section we inspect which variables are the most important for the prediction and visualize the beggining of the decision tree to see how it works. Before we do that, I will fit a decision tree to the entire training set using the depth of the best model.","7612c590":"## Can we predict whether a shot will be a hit in NBA?\n\n![](https:\/\/betsulblog.wecontent.com.br\/media\/nba-melhores-temporadas-calouro-historiaj7h.jpg)\n\nNBA is the most prestigious basketall championship out there and only the world's best players have a chance to play in the league. Given that, can we accurately predict if a player will make a hit or a miss? In order to answer that, I'm going to use data from the 2014\/2015 season containing variables like shot distance and shot clock. In this kernel I will explore the dataset to visualize the feature distributions and their relationships, as well as, use unsupervised learning algorithms to figure out structure in the data. Then I will use tree algorithms to predict the shot and also understand which variables are the more important in the classification. **If you like the notebook, please give it an upvote!**\n\n## Import libs and data","1bf659c6":"These last histograms show how dynamic is a basketball match nowadays. Teams usually play together exchanging lots of passes and keeping low ball possession times. From the point of view of a defensive player, it is much harder to counter an attack if the balls keep changing directions all the time and you have to adjust your position frequently.\n\n### Descriptive statistics\n\nTo finalize, let's take a look in the means, percentiles and outliers of each distribution.","97913be4":"## Splitting the dataset\n\nTo start modelling, we first need to split the data in train and test sets. Also I will create two versions of the features, the original one and the PCA reduced one.","44ae9559":"### Distributions","e3fdbccf":"With this graph it's clear that adjusting the learning rate is extremely important. As a side note, these parameter importances are calculated by fitting a random forest using the hyperparameters as features and the objective value as target.\n\n## Interpreting the Light GBM\n\nAs with random forests, there is no easy way to visualize hundreds of stacked trees But again we can compute the permutation importance. Before that, I'm going to fit a GBM to the entire training set with the best parameters given by Optuna.","3af81566":"## Analyzing correlations\n\nLet's analyze our features based on pearson correlation. These coefficients range from -1 and 1 and tell us if pair of variables have a linear relationship. For two distributions X and Y:\n\n- 1 indicates that X increases as Y increases fitting perfectly a line.\n- -1 indicates that X increases as Y decreases also fitting a perfectly line.\n- 0 indicates that variables doesn't have a linear relationship.","d98532cd":"Using contour plots we can see how pairs of hyperparameters interact. It's interesting to know that there are large regions where we can get large AUC.  ","9a2678dc":"Touch time is defined as the amount of time that the player has the ball possession before makeing a shot. Thus, negative values aren't defined but, as we can see above, they are present.\n\n#### Fixing columns\n\nIn this part I convert time variables to seconds, transform categorical variables in booleans and remove redundant columns.","78f32256":"The frequency table above shows that, when analyzing shots according to their type, 2 points shots can be hit or a miss with the same probability, however 3 points shots are converted only roughly one third of the attempts.","94a31e47":"The Gini importance of a feature is the normalized amount of impurity reduced by using this variable to split a region. It is called Gini importance because the metric utilized to measure region impurity in the training phase is called Gini.\n\nAs we can see, there are 5 very important variable:\n\n- Shot distance\n- Closest defensor distance\n- Touch time\n- Shot clock\n- Game clock\n\nHowever, shot distance and closest defensor distance are by far the most important variables in order to make a good prediction. The remaining 5 variables reduced very little impurity, which indicates that we could in addition drop them and fit the model just with the first 5 variables. This could lead to faster predictions, less noise and maybe better performance (generalization).\n\n#### Permutation importance\n\nNow, let's evaluate the feature importances using a method called permutation importance. In order to assess the importance of a feature X:\n\n1. Choose a dataset to evaluate the model and a metric. In our case, I will use the training set and AUC.\n2. Evaluate the model performance on this dataset.\n3. Build a new feature set where X is randomly permuted and the other variables are kept unchanged.\n4. Evaluate again the model on the modified dataset.\n5. The importance is the difference of performance between the original and the modified dataset.\n\nThis procedure is repeated a number of times using bootstrap to generate a distribution of importance for each variable, so that we can have a confidence interval.\n\nThe intuition of this methodology is that permuting important features will cause a high damage to the model performance, while less important features wouldn't do the same. That is, high values mean higher importance.","0026771d":"Based on the graphs above, we can see that building deeper trees than 10 levels doesn't improve our model. Also, notice that increasing the minimum number of samples on the leaves slightly improve the test set metric while it decreases drastically the train set performance. When we increase this hyperparameter, we are actually also restricting the tree's height. The result of that is an increase in the model's generalization, which improves the test set AUC and underfit the training set.\n\nI probably could have better results if I expanded our grid search, but using the current settings it already took over 45 minutes, which is a lot. \n\n## Interpreting the random forest\n\nWhen using random forests, there is an obvious accuracy-explainability trade-off. There is no easy way to visualize hundreds of trees, so we lose an appealing feature of the decision tree. But, at least we can access which variables are the most important using gini importance or permutation importance.","e4352ce6":"To evaluate each model, I'm going to use accuracy, AUC and cross validation with 5 folds.\n\n### Original dataset"}}