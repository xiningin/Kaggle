{"cell_type":{"3dec699c":"code","f70cb5a1":"code","65df31a2":"code","a4ceacca":"code","9d3ef37b":"code","3753ed2a":"code","e8a41b77":"code","1da077ea":"code","89a82c83":"code","e53f113f":"code","dfe8c2e8":"code","f0e79cc1":"code","16cb7965":"code","6a262138":"code","25be31ab":"code","3558e72f":"code","8a0fb0ff":"code","ae164909":"code","5989f46c":"code","78450f54":"code","d14f1f4f":"code","f57d96bd":"code","d8c1bc8f":"code","3bb84fad":"markdown","ff6ab48e":"markdown","6bde01fe":"markdown","cb5ead2c":"markdown"},"source":{"3dec699c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier","f70cb5a1":"# Read data\n\nrow_data = pd.read_csv('..\/input\/orbitclassification\/classast - pha.csv')\nrow_data.head()","65df31a2":"# Checking for duplicate lines\n\nrow_data.duplicated().unique()","a4ceacca":"# Checking for empty cells in data\n\nrow_data.isnull().sum() ","9d3ef37b":"# \u0421lass balance check\n\nplt.figure(figsize=(20,5))\nsns.countplot(x = row_data['class'])\nplt.show()","3753ed2a":"# The classes are highly unbalanced. This must be taken into account.","e8a41b77":"# Numerical features exploration\n\nnumerical_features = row_data.iloc[:, :11].columns.to_list()\n\nfor column_name in numerical_features:\n    plt.figure(figsize=(15,10))\n    sns.distplot(x = row_data[column_name])\n    plt.xlabel(column_name)\n    plt.show()\n    \n    plt.figure(figsize=(15,3))\n    sns.boxplot(x = row_data[column_name])\n    plt.show()","1da077ea":"# The boxplot shows strong outliers from a (AU), Q (AU) and P (yr).\n# Let's Explore them in more detail","89a82c83":"outliers_a_AU = row_data.loc[row_data['a (AU)'] > 15]\nprint('Orbit class with a (AU) > 15:', ', '.join([str(i) for i in outliers_a_AU['class'].unique()]))\nprint(outliers_a_AU)","e53f113f":"# All outliers belong to the same object\n# Deleted outliers object\n\nrow_data = row_data.loc[row_data['a (AU)'] < 15]","dfe8c2e8":"# Renaming the target variable\n\nrow_data['class'] = row_data['class'].replace('APO*', 1)\nrow_data['class'] = row_data['class'].replace('ATE*', 2)\nrow_data['class'] = row_data['class'].replace('AMO*', 3)\nrow_data['class'] = row_data['class'].replace('APO', 4)\nrow_data['class'] = row_data['class'].replace('IEO*', 5)\nrow_data['class'] = row_data['class'].replace('ATE', 6)","f0e79cc1":"# Features correlation exploration\n\n# Pearson correlation\nplt.figure(figsize=(10,8))\ncorr = row_data.corr(method='pearson')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='RdBu', mask=mask)\nplt.show()","16cb7965":"# Spearman correlation\nplt.figure(figsize=(10,8))\ncorr = row_data.corr(method='spearman')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='RdBu', mask=mask)\nplt.show()","6a262138":"# The data show a strong correlation between a(AU) and Q(AU); a(AU) and P(yr);\n# Q(AU) and P(yr); e and Q(AU).\n\n# The correlation between a(AU) (Semi-major axis) and Q(AU) (Aphelion distance) is explained by\n# the fact that the semi-major axis (a) is the average of the aphelion (Q) and \n# perihelion (q) distances. Also Q can be calculated from Q = a(1+e).\n\n# The correlation between a(AU) (Semi-major axis) and P(yr) (Orbital period) is explained by \n# the fact that they are related by the relationship P = 2*Pi*sqrt(a^3\/\u03bc)\n\n# The correlation between Q(AU) (Aphelion distance) and P(yr) (Orbital period) is explained \n# by the fact that Q(AU) (as I wrote above) is related to a(AU), which is related to P(yr)\n\n# The correlation between e (Eccentricity) and Q(AU) (Aphelion distance) is explained by \n# the fact that they are related by the relationship Q = a(1+e).\n\n# From all of the above, it follows that in the work you can ignore such parameters as \n# P(yr) (Orbital period) and Q(AU) (Aphelion distance). Both of these parameters can \n# be calculated from a(AU) (Semi-major axis) and e (Eccentricity). The same can be \n# said about q(AU) (Perihelion), but it does not show a strong correlation with other \n# parameters, so it can be written.","25be31ab":"# Removing columns P(yr) and Q(AU)\n\nrow_data.drop(['Q (AU)', 'P (yr)'], axis=1, inplace=True)\nrow_data.head()","3558e72f":"data_X = row_data.iloc[:, :9]  # data without target variable\ndata_y = row_data['class']  # target variable\n\n# Balanced data\noversample = SMOTE(k_neighbors = 4)\ndata_X_balanced, data_y_balanced = oversample.fit_resample(data_X, data_y.ravel())\n\n# Split data in to train and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data_X_balanced, data_y_balanced, test_size=0.33, random_state=42, stratify=data_y_balanced)\n\n# Scaling data\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","8a0fb0ff":"models = []\n\nmodels.append(('KNN',KNeighborsClassifier(n_jobs=-1)))\nmodels.append(('LR',LogisticRegression(random_state=42,n_jobs=-1)))\nmodels.append(('DT',DecisionTreeClassifier(random_state=42)))\nmodels.append(('Bag_DT',BaggingClassifier(DecisionTreeClassifier(random_state=42), random_state=42, n_jobs=-1)))\nmodels.append(('RF',RandomForestClassifier(random_state=42, n_jobs=-1)))\nmodels.append(('GBC',GradientBoostingClassifier(random_state=42)))\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name, model in models:\n    scores = cross_val_score(model, X_train, y_train, scoring='f1_weighted', cv=kf, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean F1 {round(accuracy, 3)} STD:({round(std, 3)})\")","ae164909":"# The best results were shown by DecisionTreeClassifier, BaggingClassifier,\n# RandomForestClassifier and GradientBoostingClassifier.\n# Let's check them on the test set","5989f46c":"# Try DecisionTreeClassifier on test set\n\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\ny_predicted = dt.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","78450f54":"# Try BaggingClassifier on test set\n\nbg = BaggingClassifier(DecisionTreeClassifier(random_state=42))\nbg.fit(X_train, y_train)\ny_predicted_bg = bg.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_bg)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","d14f1f4f":"# Try RandomForestClassifier on test set\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_predicted_rf = rf.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_rf)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","f57d96bd":"# Try GradientBoostingClassifier on test set\n\ngb = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=50, random_state=42)\ngb.fit(X_train, y_train)\ny_predicted_gb = gb.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_gb)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","d8c1bc8f":"# Best result on test set shown by RandomForestClassifier","3bb84fad":"# Read data","ff6ab48e":"# Trying out different models using kFold cross-validation","6bde01fe":"# Exploring data","cb5ead2c":"# Preparing data"}}