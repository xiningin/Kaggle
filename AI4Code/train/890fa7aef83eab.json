{"cell_type":{"8de131dd":"code","382fe3d0":"code","8b154827":"code","e175bc6b":"code","49c875a0":"code","50e8b3d7":"code","1a18cfd9":"code","5792edaa":"code","c797846f":"code","7668784d":"code","e6711abe":"code","da325e6d":"code","a6aac32a":"markdown","bd82cab9":"markdown","e36ff953":"markdown","8c4aceac":"markdown","0d6c8bc7":"markdown"},"source":{"8de131dd":"import os\nimport numpy as np\nimport pandas as pd\n\n# Sklearn Vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb","382fe3d0":"submission_df = pd.read_csv(\"..\/input\/shopee-sentiment-analysis\/sampleSubmission.csv\")\ntest_df = pd.read_csv(\"..\/input\/shopee-sentiment-analysis\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/shopee-sentiment-analysis\/train.csv\")\nsubmission_df.shape, test_df.shape, train_df.shape","8b154827":"# Optional step, still same output 4=5\n\n# # Duplicated review but with different rating, if the mean is not integers\n# # then that review has different rating\n# diff_rating = train_df.groupby('review')['rating'].agg(['mean', 'count'])\n# to_removed = diff_rating[diff_rating['mean'] % 1 != 0.0].index.to_list()\n\n# # Since it is very small, we might just remove them\n# print(f\"Before removed shape: {train_df.shape}\")\n# print(f\"To be removed: {diff_rating['count'].sum()}\")\n# train_df = train_df[~train_df['review'].isin(to_removed)].reset_index(drop=True)\n# print(f\"After removed shape: {train_df.shape}\")","e175bc6b":"# Top n-gram correlation\ndef get_term_frequency(corpus, ngram_range=(1, 1)):\n    tokenizer_kwargs = dict(\n        analyzer='word',  # for many misspelled words, 'char_wb' is recommended\n        ngram_range=ngram_range,  # (1, 1) = unigram, unigram, (1, 2) = unigram, bigram, etc.\n        min_df=2,  # if integer, remove word that occurs less than this number\n    )\n    token_f = CountVectorizer(\n        input='content',\n        **tokenizer_kwargs,\n    )    \n    A_tokenized = token_f.fit_transform(corpus)\n    \n    term_count = np.array(A_tokenized.sum(axis=0)).flatten().tolist()\n    term_names = token_f.get_feature_names()\n    term_df = pd.DataFrame(list(zip(term_names, term_count)), columns=['name', 'count']).sort_values(by='count', ascending=False)\n    term_df = term_df.set_index('name')\n    \n    return term_df\n\ndef plot_side_by_side(first_df, second_df, n_show=50, top=True):\n    n_show = n_show\n    fig, ax = plt.subplots(1, 2, figsize=(14, n_show\/5))\n    if top:\n        first_df.head(n_show)[::-1].plot(kind='barh', ax=ax[0], legend=None, alpha=0.7)\n        second_df.head(n_show)[::-1].plot(kind='barh', ax=ax[1], legend=None, alpha=0.7)\n    else:\n        first_df.tail(n_show).plot(kind='barh', ax=ax[0], legend=None, alpha=0.7)\n        second_df.tail(n_show).plot(kind='barh', ax=ax[1], legend=None, alpha=0.7)\n    ax[0].set_title(f'Rating=4 top {n_show} Terms')\n    ax[1].set_title(f'Rating=5 top {n_show} Terms')\n    ax[0].set_ylabel('')\n    ax[1].set_ylabel('')\n    plt.tight_layout()\n    plt.show()","49c875a0":"train_term_df_4 = get_term_frequency(train_df[train_df['rating'] == 4]['review'], ngram_range=(1, 1))\ntrain_term_df_5 = get_term_frequency(train_df[train_df['rating'] == 5]['review'], ngram_range=(1, 1))\nplot_side_by_side(train_term_df_4, train_term_df_5)","50e8b3d7":"train_term_df_4 = get_term_frequency(train_df[train_df['rating'] == 4]['review'], ngram_range=(2, 2))\ntrain_term_df_5 = get_term_frequency(train_df[train_df['rating'] == 5]['review'], ngram_range=(2, 2))\nplot_side_by_side(train_term_df_4, train_term_df_5)","1a18cfd9":"train_term_df_4 = get_term_frequency(train_df[train_df['rating'] == 4]['review'], ngram_range=(3, 3))\ntrain_term_df_5 = get_term_frequency(train_df[train_df['rating'] == 5]['review'], ngram_range=(3, 3))\nplot_side_by_side(train_term_df_4, train_term_df_5)","5792edaa":"train_rating_check = train_df[train_df['rating'].isin([4, 5])].reset_index(drop=True)\ntrain_rating_check['rating'] = train_rating_check['rating'] - 4  # make it 0, 1 for binary classification\nbase_mask = np.random.rand(len(train_rating_check)) < 0.6\ntrain_mask = base_mask\nvalid_mask = ~base_mask\ncol_target = 'rating'","c797846f":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(train_rating_check['review'])\nX.shape","7668784d":"train_data = lgb.Dataset(X[train_mask],\n                         label=train_rating_check[train_mask][col_target],)\nvalid_data = lgb.Dataset(X[valid_mask],\n                         label=train_rating_check[valid_mask][col_target],)\nparams =  {\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 100,\n    \"colsample_bytree\": 0.75,\n    \"subsample\": 0.75,\n    \"subsample_freq\": 1,\n    \"max_depth\": 2,\n    \"nthreads\": 8,\n    \"verbose\": 1,\n    'metric': 'auc',\n    'objective': 'binary',\n    \"early_stopping_rounds\": 100,\n    \"reg_lambda\": 0.5,\n    \"num_boost_round\": 100000,\n    \"seed\": 1234,\n}\n\nbst = lgb.train(params,\n                train_data,\n                valid_sets=[train_data, valid_data])","e6711abe":"# Redo using our randomly assigned label data\ntrain_rating_check['rating'] = np.random.randint(2, size=train_rating_check.shape[0])\ntrain_rating_check['rating'].values","da325e6d":"train_data = lgb.Dataset(X[train_mask],\n                         label=train_rating_check[train_mask][col_target],)\nvalid_data = lgb.Dataset(X[valid_mask],\n                         label=train_rating_check[valid_mask][col_target],)\nparams =  {\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 100,\n    \"colsample_bytree\": 0.75,\n    \"subsample\": 0.75,\n    \"subsample_freq\": 1,\n    \"max_depth\": 2,\n    \"nthreads\": 8,\n    \"verbose\": 1,\n    'metric': 'auc',\n    'objective': 'binary',\n    \"early_stopping_rounds\": 100,\n    \"reg_lambda\": 0.5,\n    \"num_boost_round\": 100000,\n    \"seed\": 1234,\n}\n\nbst = lgb.train(params,\n                train_data,\n                valid_sets=[train_data, valid_data])","a6aac32a":"Hmm hmm.","bd82cab9":"Hmm hmm hmm.","e36ff953":"Aw.","8c4aceac":"Aw aw.","0d6c8bc7":"Hmm."}}