{"cell_type":{"3254861a":"code","8d0ec720":"code","0b5c7ff3":"code","9ac35703":"code","aa86ef29":"code","a374c511":"code","a7630590":"code","a11ffc33":"code","ffec7a8b":"code","b059380a":"code","12b58ade":"code","f00dee50":"code","c9e4b07f":"code","e0d27eb5":"code","f90aae83":"code","3caebbeb":"code","6c79f3f7":"code","ae9e6360":"code","b395b833":"code","410303de":"code","e03627e5":"markdown","698945d7":"markdown","bbd37039":"markdown","1b2b68f9":"markdown","f3e16178":"markdown","dd591449":"markdown","79b20179":"markdown","5cc0902b":"markdown","61fabd1f":"markdown","809ee4fd":"markdown","c7d98c07":"markdown","23f1ca87":"markdown","6c9fcd75":"markdown","9c491de6":"markdown","aee41442":"markdown"},"source":{"3254861a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8d0ec720":"#create dataset\n#np.random.normal(25,5,1000) =>\n#it's mean that 66%(666) of my data will be between 20(25-5) and 30(25+5)\n#This distribution is called Gaussian distribution.\n\n#for class1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n#for class2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n#for class3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3), axis=0)\ny = np.concatenate((y1,y2,y3), axis=0)","0b5c7ff3":"dictionary = {\"x\":x,\"y\":y}\ndata = pd.DataFrame(dictionary)","9ac35703":"#my data look like this\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","aa86ef29":"#but kmeans algorithm will see this way\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\nplt.show()","a374c511":"#K-MEANS\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    #kmeans.inertia_ =>find wcss for each key value and add to list\n    wcss.append(kmeans.inertia_)\n\n#as we can see,optimal point is k=3\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","a7630590":"plt.figure(figsize=(20,10))\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n#This is my model for k = 2.\n#fit_predict(data)=>it's mean that fit my data and create my clusters.\nkmeans2 = KMeans(n_clusters=1)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,1)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\n\n# drop labels to use for k=2\ndata.drop([\"label\"],axis=1,inplace=True)\n\nkmeans2 = KMeans(n_clusters=2)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,2)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\n\n# drop labels to use for k=3\ndata.drop([\"label\"],axis=1,inplace=True)\n\nkmeans2 = KMeans(n_clusters=3)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,3)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(data.x[data.label == 2],data.y[data.label == 2],color=\"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")","a11ffc33":"# dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data,method=\"ward\")\nplt.figure(figsize=(15,8))\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","ffec7a8b":"# HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\n\ndata[\"label\"] = cluster\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\nplt.show()","b059380a":"data = pd.read_csv(\"..\/input\/Iris.csv\")","12b58ade":"data.head()","f00dee50":"#we drop Id column because it's not a useful feature for us.\ndata.drop([\"Id\"],axis=1,inplace=True)","c9e4b07f":"data.info()","e0d27eb5":"sns.pairplot(data=data,hue=\"Species\",palette=\"Set1\")\nplt.show()","f90aae83":"features = data.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]","3caebbeb":"#K-MEANS\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    #kmeans.inertia_ =>find wcss for each key value and add to list\n    wcss.append(kmeans.inertia_)\n\n#as we can see,optimal point is k=3\nplt.figure(figsize=(10,8))\nplt.plot(range(1,15),wcss,\"-o\")#\"-o\"=> for marker(point)\nplt.title(\"WCSS-K Chart\", fontsize=18)\nplt.grid(True)\nplt.xlabel(\"Number of K (cluster) Value\")\nplt.ylabel(\"WCSS\")\nplt.xticks(range(1,15))\nplt.show()","6c79f3f7":"plt.figure(figsize=(20,20))\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(3,2,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(3,2,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# drop labels to use for k=3\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,3)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# drop labels to use for k=4\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,4)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# drop labels to use for k=5\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,5)\nplt.title(\"K = 5\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=5)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\nplt.scatter(features.PetalLengthCm[features.labels == 4],features.PetalWidthCm[features.labels == 4])\n\n# drop labels\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,6)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-setosa\"],data.PetalWidthCm[data.Species == \"Iris-setosa\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-versicolor\"],data.PetalWidthCm[data.Species == \"Iris-versicolor\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-virginica\"],data.PetalWidthCm[data.Species == \"Iris-virginica\"])\n\nplt.show()\n","ae9e6360":"from scipy.cluster.hierarchy import dendrogram, linkage\n\nmerge = linkage(features,method=\"ward\")\n\nplt.figure(figsize=(15,8))\ndendrogram(merge, leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()\n","b395b833":"# we draw a line on the longest vertical line between horizontal lines.\n# Then we count how many vertical lines the line passes.\n# As you can see,it's look like k=2 but we will see.","410303de":"from sklearn.cluster import AgglomerativeClustering\nplt.figure(figsize=(20,20))\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\n\nplt.subplot(3,2,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(3,2,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=2)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# drop labels to use for k=3\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,3)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# drop labels to use for k=4\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,4)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=4)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# drop labels to use for k=5\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,5)\nplt.title(\"K = 5\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=5)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\nplt.scatter(features.PetalLengthCm[features.labels == 4],features.PetalWidthCm[features.labels == 4])\n\n# drop labels\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,6)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-setosa\"],data.PetalWidthCm[data.Species == \"Iris-setosa\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-versicolor\"],data.PetalWidthCm[data.Species == \"Iris-versicolor\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-virginica\"],data.PetalWidthCm[data.Species == \"Iris-virginica\"])\n\nplt.show()","e03627e5":"# Hierarchical Clustering","698945d7":"**As we can see,we clearly seperate our classes using with Hierarchical Clustering Algorithm for k=3.**","bbd37039":"# Hierarchical Clustering","1b2b68f9":"**According to Elbow Rule,we will choose 2 or 3.Because we don't know which one is better to use it.**","f3e16178":"## Let's try this algorithm on our Iris data.","dd591449":"**According to Elbow Rule,we will choose 2 or 3.Because we don't know which one is better to use it.**","79b20179":"**We draw a line on the longest vertical line between horizontal lines.**<br><br>\n**Then we count how many vertical lines the line passes.**<br><br>\n**As you can see,it's look like k=3 but we will see.**","5cc0902b":"# K-Means Clustering","61fabd1f":"# INTRODUCTION","809ee4fd":"**As we can see,we clearly seperate our classes using with K-Means Clustering Algorithm for k=3.**","c7d98c07":"**As we can see,we clearly seperate our classes using with K-Means Clustering Algorithm for k=3.**","23f1ca87":"* Firstly, let's start implementing these two algorithms in our own data set to understand better.","6c9fcd75":"**In this kernel,we will see K-Means and Hierarchical Clustering algorithms.**","9c491de6":"# K-Means Clustering","aee41442":"# Conclusion<br>\n**If you like it, Please upvote my kernel.**<br>\n**If you have any question, I will happy to hear it**"}}