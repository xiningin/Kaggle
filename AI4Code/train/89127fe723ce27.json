{"cell_type":{"45ec61ed":"code","77fabb07":"code","9bdd4e90":"code","658b6470":"code","9a1fd1d5":"code","7da99ef3":"code","5ec0acd0":"code","368e2550":"code","cfc0dd36":"code","96b2cc94":"code","f670e8d1":"code","01f279da":"code","61fb9b3a":"code","cff1c981":"code","4818e1b7":"code","87603899":"code","760dadf9":"code","0600ca60":"code","4aa1e81a":"markdown","30f5302f":"markdown","fd747e42":"markdown","a9d3af2d":"markdown","a3602112":"markdown","0ebe2348":"markdown","fc3d03fc":"markdown","f9d0c548":"markdown","5ae1c821":"markdown","1bfdd315":"markdown","e629493d":"markdown","8a7d812b":"markdown","ea9a03de":"markdown","47e1e441":"markdown","4c8e0c98":"markdown","5925f249":"markdown","92f5ecac":"markdown","52f186e8":"markdown","cf43ffcd":"markdown","470800e5":"markdown","bca52aaa":"markdown","5b2e6c06":"markdown"},"source":{"45ec61ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77fabb07":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\ndf.head()","9bdd4e90":"df.dtypes","658b6470":"df.info()","9a1fd1d5":"df.shape","7da99ef3":"df.isnull().sum()","5ec0acd0":"df.columns.isna()","368e2550":"df.isin([' ?']).sum()","cfc0dd36":"#df = df.dropna()","96b2cc94":"\ncategorical_df = df.select_dtypes(include=['object'])\ncategorical_df.columns\n\n\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\n\ncategorical_df = categorical_df.apply(enc.fit_transform)\ncategorical_df.head()\n\n\ndf = df.drop(categorical_df.columns, axis=1)\ndf = pd.concat([df, categorical_df], axis=1)\ndf.head()\n","f670e8d1":"X = df.drop('class', axis=1)\ny = df['class']\n","01f279da":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","61fb9b3a":"from sklearn.linear_model import LogisticRegression\n\ndef_lr= LogisticRegression()\ndef_lr.fit(X_train, y_train)\n\nlr_pred = def_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, lr_pred))\n","cff1c981":"from sklearn import svm\ndef_svm = svm.SVC()\ndef_svm.fit(X_train, y_train)\n\nsvm_pred = def_svm.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(\"SVM accuracy: \", accuracy_score(y_test, svm_pred))\n","4818e1b7":"from sklearn.tree import DecisionTreeClassifier\n\ndef_dt= DecisionTreeClassifier()\ndef_dt.fit(X_train, y_train)\n\ndt_pred = def_dt.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Decision Tree accuracy\", accuracy_score(y_test, dt_pred))\n  ","87603899":"from sklearn.ensemble import RandomForestClassifier\n\ndef_rf = RandomForestClassifier()\ndef_rf.fit(X_train, y_train)\n\n\nrf_pred = def_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, rf_pred))\n\n","760dadf9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV , KFold\n\n\nlr= LogisticRegression()\n\n\ngs_grid  = {\n               'penalty': ['l1','l2','elasticnet'],\n               'class_weight': ['balanced', 'None'],\n               'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\n\nlr_CV = GridSearchCV(estimator = lr, param_grid=gs_grid, cv=5)\n\nresult = lr_CV.fit(X_train, y_train)\n\nprint(result.best_params_)\nprint(result.best_score_)\n","0600ca60":"\nfrom sklearn.linear_model import LogisticRegression\n\nfinal_lr=LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced')\nfinal_lr.fit(X_train, y_train)\n\nfinal_lr_pred = final_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, final_lr_pred))\n\n","4aa1e81a":"# 2.1 Encoding categorical variables","30f5302f":"SVM","fd747e42":"# 4. Building the Final Model","a9d3af2d":"Input Variables:\n\n-**classes**: edible=e, poisonous=p)\n\n-**cap-shape**: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\n-**cap-surface**: fibrous=f,grooves=g,scaly=y,smooth=s\n\n-**cap-color**: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\n-**bruises**: bruises=t,no=f\n\n-**odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\n-**gill-attachment**: attached=a,descending=d,free=f,notched=n\n\n-**gill-spacing**: close=c,crowded=w,distant=d\n\n-**gill-size**: broad=b,narrow=n\n\n-**gill-color**: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\n-**stalk-shape**: enlarging=e,tapering=t\n\n-**stalk-root**: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\n-**stalk-surface-above-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n\n-**stalk-surface-below-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n\n-**stalk-color-above-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n-**stalk-color-below-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n-**veil-type: partial**=p,universal=u\n\n-**veil-color: brown**=n,orange=o,white=w,yellow=y\n\n-**ring-number**: none=n,one=o,two=t\n\n-**ring-type**: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\n-**spore-print-color**: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\n-**population**: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\n-**habitat**: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","a3602112":"Using default model\n\n1. Logistic Regression\n\n2. SVM\n\n3. Decision Tree\n\n4. Random Forest","0ebe2348":"Split Data ","fc3d03fc":"Decision TreeClassifier","f9d0c548":"Encoding categorical variables numerically for classification","5ae1c821":"Comparing 4 Model's accuary scoure: \n\nI will choose ***Random Forest*** as Final Model","1bfdd315":"# 3. Classification Model","e629493d":"Understanding the data ","8a7d812b":"Checking null value","ea9a03de":"Logistic Regression","47e1e441":"Define X and y","4c8e0c98":"No need to use this code as this dataset no null value","5925f249":"\nThis Notebook covers the following aspects:\n\n~1. Overview\n\n~2. Data Preprocessing\n\n    ~2.1 Encoding categorical variables\n    \n    \n~3. Classification Model\n\n~4. Tuning Model\n\n~5. Building the Final Model","92f5ecac":"# **2. Data Preprocessing**","52f186e8":"Random Forest","cf43ffcd":"Logistic Regression","470800e5":"# **1. Overview**","bca52aaa":"Input those parameter into Logistic Regression Model","5b2e6c06":"# 4. Tuning Model"}}