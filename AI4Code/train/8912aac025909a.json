{"cell_type":{"7d39fe72":"code","c87d9384":"code","6a63d98b":"code","8af67882":"code","20eef68f":"code","a0d501e3":"code","2d6bb86e":"code","30debab7":"code","d2b66c8f":"code","e5af8be0":"code","9188c809":"code","6f9f0e3c":"code","e58107e2":"code","768426c3":"code","dc621c41":"code","a4bdcaf5":"code","86db14bc":"code","4d3be259":"code","a7603ce3":"code","98987e05":"code","3310ad29":"code","694f3a8f":"code","f7a51b24":"code","a40a439d":"code","81e0978e":"code","77ee1ebf":"code","b9aa7a0f":"markdown","25409431":"markdown","1e63ae94":"markdown","e85b2eb3":"markdown","0c754410":"markdown","3144360e":"markdown","c313063e":"markdown","aff55834":"markdown","8e85c378":"markdown","c92deeb6":"markdown","7bb718a6":"markdown","f928e21a":"markdown","e29979c4":"markdown","0e84fd65":"markdown","80d28c10":"markdown","c149898a":"markdown","317fcc82":"markdown"},"source":{"7d39fe72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c87d9384":"train_data = pd.read_csv(\"\/kaggle\/input\/gamedata\/train.csv\")\ntrain_data.head()","6a63d98b":"test = pd.read_csv(\"\/kaggle\/input\/gamedata\/eval.csv\")\ntest.head()","8af67882":"sample = pd.read_csv(\"\/kaggle\/input\/gamedata\/sample_submission.csv\")\nsample.head()","20eef68f":"train_data.info()","a0d501e3":"for i in train_data:\n    print(train_data[i].unique())","2d6bb86e":"Y = train_data[\"esrb_rating\"]\nfeatures = [\"console\",\"alcohol_reference\",\"animated_blood\",\"blood\",\"blood_and_gore\",\"cartoon_violence\",\"crude_humor\",\"drug_reference\",\"fantasy_violence\"\n             ,\"intense_violence\",\"language\",\"lyrics\",\"mature_humor\",\"mild_blood\",\"mild_cartoon_violence\",\"mild_fantasy_violence\",\"mild_language\",\"mild_lyrics\"\n              ,\"mild_suggestive_themes\",\"mild_violence\",\"no_descriptors\",\"nudity\",\"partial_nudity\",\"sexual_content\",\"sexual_themes\",\"simulated_gambling\",\"strong_janguage\"\n             ,\"strong_sexual_content\",\"suggestive_themes\",\"use_of_alcohol\",\"use_of_drugs_and_alcohol\",\"violence\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test[features])\nX.info()","30debab7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nlog_reg = LogisticRegression()\n\nsolvers = ['newton-cg','liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=log_reg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X,Y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d2b66c8f":"log_reg = LogisticRegression(C=100, penalty='l2', solver='newton-cg')\nlog_reg.fit(X,Y)\nreg_result = log_reg.predict(X_test)\nreg_result","e5af8be0":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=1111)\nsplits = kf.split(X)\nfor train_index, val_index in splits:\n    X_train, y_train = X.iloc[train_index], Y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], Y.iloc[val_index]\n    log_reg.fit(X_train,y_train)\n    print(\"Score:\",log_reg.score(X_train,y_train))  ","9188c809":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\n\npolynomial_svm_clf = Pipeline([\n   (\"poly_features\", PolynomialFeatures(degree=3)),\n   (\"scaler\", StandardScaler()),\n   (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n    \n])\npolynomial_svm_clf = LinearSVC(max_iter=5000)\npolynomial_svm_clf.fit(X,Y)\nsvm_result = polynomial_svm_clf.predict(X_test)\nsvm_result","6f9f0e3c":"kf = KFold(n_splits=5, shuffle=True, random_state=1111)\nsplits = kf.split(X)\nfor train_index, val_index in splits:\n    X_train, y_train = X.iloc[train_index], Y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], Y.iloc[val_index]\n    polynomial_svm_clf.fit(X_train,y_train)\n    print(\"Score:\",polynomial_svm_clf.score(X_train,y_train)) ","e58107e2":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(max_depth=10)\ngrid = {'max_depth': [2,3,5,10,20], 'criterion': [\"gini\",\"entropy\"],'min_samples_leaf': [5,10,20,50,100]}\ngrid = GridSearchCV(estimator=tree_clf, param_grid=grid, refit=True, verbose=2)\ngrid.fit(X,Y)\nprint(\"Best parameters:\",grid.best_params_)\n","768426c3":"tree_clf = DecisionTreeClassifier(max_depth=20,criterion='entropy',min_samples_leaf=5)\ntree_clf.fit(X,Y)\ntree_result = tree_clf.predict(X_test)\ntree_result","dc621c41":"kf = KFold(n_splits=5, shuffle=True, random_state=1111)\nsplits = kf.split(X)\nfor train_index, val_index in splits:\n    X_train, y_train = X.iloc[train_index], Y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], Y.iloc[val_index]\n    tree_clf.fit(X_train,y_train)\n    print(\"Score:\",tree_clf.score(X_train,y_train)) ","a4bdcaf5":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\nprint(rnd_clf.get_params().keys())\nrnd_clf.fit(X,Y)\nrandom_result = rnd_clf.predict(X_test)\nrandom_result\n","86db14bc":"kf = KFold(n_splits=5, shuffle=True, random_state=1111)\nsplits = kf.split(X)\nfor train_index, val_index in splits:\n    X_train, y_train = X.iloc[train_index], Y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], Y.iloc[val_index]\n    rnd_clf.fit(X_train,y_train)\n    print(\"Score:\",rnd_clf.score(X_train,y_train)) ","4d3be259":"from sklearn.neighbors import KNeighborsClassifier\nk_near = KNeighborsClassifier(n_neighbors=5,p=1,leaf_size=5)\nparam_grid = {'n_neighbors': [1,5,7,10,15], 'p': [1,2],'leaf_size': [5,10,15,25,50]}\ngrid = GridSearchCV(estimator=k_near,param_grid=param_grid,refit=True,verbose=2)\ngrid.fit(X,Y)\nprint(\"Best parameters:\",grid.best_params_)\n","a7603ce3":"k_near = KNeighborsClassifier(n_neighbors=5,p=1,leaf_size=10)\nk_near.fit(X,Y)\nk_result = k_near.predict(X_test)\nk_result","98987e05":"kf = KFold(n_splits=5, shuffle=True, random_state=1111)\nsplits = kf.split(X)\nfor train_index, val_index in splits:\n    X_train, y_train = X.iloc[train_index], Y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], Y.iloc[val_index]\n    k_near.fit(X_train,y_train)\n    print(\"Score:\",k_near.score(X_train,y_train)) ","3310ad29":"log_reg.fit(X,Y)\nlog_reg.score(X,Y)","694f3a8f":"polynomial_svm_clf.fit(X,Y)\npolynomial_svm_clf.score(X,Y)","f7a51b24":"tree_clf.fit(X,Y)\ntree_clf.score(X,Y)","a40a439d":"rnd_clf.fit(X,Y)\nrnd_clf.score(X,Y)","81e0978e":"k_near.fit(X,Y)\nk_near.score(X,Y)","77ee1ebf":"predictions = log_reg.predict(X_test)\noutput = pd.DataFrame({'id':test.id ,'esrb_rating':predictions})\noutput.to_csv('submission.csv', index=False)\nprint(output)","b9aa7a0f":"# **3-4 Random Forest**","25409431":"# **3-Preparing Data for models**","1e63ae94":"# **3-3 Decision Tree**","e85b2eb3":"**Validation scores**","0c754410":"# **2.1-Checking missing values**","3144360e":"#  **1-Loading data**","c313063e":"# **3-2 Support Vector machine**","aff55834":"**Validation scores**","8e85c378":"# **3-5 K Nearest Neighbor**","c92deeb6":"Tiuna Benito","7bb718a6":"# **5 Generating submission**","f928e21a":"**Validation scores**","e29979c4":"**Validation scores**","0e84fd65":"**validation scores**","80d28c10":"# **2.2-Checking any outliers**","c149898a":"# **3-1 Linear Regression**","317fcc82":"**Validating all models**"}}