{"cell_type":{"3e628288":"code","d1361c82":"code","df13d53f":"code","f6f3c765":"code","2868ff19":"code","9293770c":"code","e3f7c1a2":"code","f8f29f92":"code","47357236":"code","072367a2":"code","adac6a28":"code","7cd1c91a":"code","67752814":"code","e37bda12":"code","6bcb4051":"code","1a3b44a3":"code","002c94b6":"code","0df28aef":"code","7c2b0e1e":"code","2eab54b2":"code","df5ee505":"code","1077c1d0":"code","a533f488":"code","39441c9f":"code","270ebdba":"code","debb0989":"code","265ef806":"code","eb83f1f9":"code","5fff3843":"code","f2c5ea73":"code","62dc66fa":"markdown","3deb3182":"markdown","ce9f966c":"markdown","08be2080":"markdown","9564b411":"markdown","fc2a4bea":"markdown","e4f918af":"markdown","b053e332":"markdown","0fbd7074":"markdown","f988f080":"markdown","2587eace":"markdown","433c207e":"markdown","0cb75174":"markdown","71fac456":"markdown","4d4d0fae":"markdown","3ea9f21d":"markdown"},"source":{"3e628288":"! pip install ","d1361c82":"! pip install faker","df13d53f":"# https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/blob\/master\/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom faker import Faker\nimport random\nimport babel\nfrom babel.dates import format_date\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\nfrom sklearn import model_selection\nimport pytorch_lightning as pl\nfrom torch.nn import functional as F\nfrom torch.nn.functional import cross_entropy\nfrom torch.nn import CrossEntropyLoss\nfrom torchmetrics.functional import accuracy\nfrom torchvision import transforms\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping","f6f3c765":"fake = Faker()\nFaker.seed(12345)\nrandom.seed(12345)\n\n# Define format of the data we would like to generate\nFORMATS = ['short',\n           'medium',\n           'long',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'full',\n           'd MMM YYY', \n           'd MMMM YYY',\n           'dd MMM YYY',\n           'd MMM, YYY',\n           'd MMMM, YYY',\n           'dd, MMM YYY',\n           'd MM YY',\n           'd MMMM YYY',\n           'MMMM d YYY',\n           'MMMM d, YYY',\n           'dd.MM.YY']\n\n# change this if you want it to work with another language\nLOCALES = ['en_US']","2868ff19":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Config:\n    RANDOM_STATE_SEED = 42        \n    BATCH_SIZE = 4096\n    NUM_WORKERS = 2\n    NUM_EPOCHS = 75\n    PRECISION = 16\n    NUM_FOLDS = 5\n    FAST_DEV_RUN = False\n    DEVICE = device\n    PATIENCE = 15","9293770c":"def generate_date_data():\n    dt = fake.date_object()\n    human_readable_dt = None\n    machine_readable_dt = None\n    try:\n        human_readable_dt = format_date(dt, random.choice(FORMATS), \"en_US\")\n        human_readable_dt = human_readable_dt.replace(\",\", \"\")\n        machine_readable_dt = dt.isoformat()\n    except AttributeError as e:\n        return None, None, None\n    return human_readable_dt, machine_readable_dt, dt","e3f7c1a2":"def load_date_dataset(num_examples=100):    \n    dataset = []\n    for row in range(num_examples):\n        h_dt, m_dt, dt = generate_date_data()        \n        dataset.append([h_dt, m_dt])    \n    return np.array(dataset)","f8f29f92":"# the vocab for both source and target sequences needs to be generated from training data only\n# to prevent data leakage into the validation sets leading to inflated model accuracy in validation phase\ndef get_source_target_vocab(human_dates, machine_dates):\n    human_dt_vocab = set()\n    machine_dt_vocab = set()\n    for (h_dt, m_dt) in zip(human_dates, machine_dates):\n        human_dt_vocab.update(tuple(h_dt))\n        machine_dt_vocab.update(tuple(m_dt))\n    # char to index dictionary for source sequence        \n    human_dt_vocab = {value: index for index, value in enumerate(sorted(human_dt_vocab) + ['<unk>', '<pad>', '<sos>', '<eos>'])}\n    machine_dt_vocab = {value: index for index, value in enumerate(sorted(machine_dt_vocab) + ['<sos>'])}\n    inv_machine_dt_vocab = dict(enumerate(sorted(machine_dt_vocab)))  \n    # index to char dictionary for source sequence\n    inv_human_dt_vocab = dict(enumerate(sorted(human_dt_vocab)))       \n    return human_dt_vocab, machine_dt_vocab, inv_human_dt_vocab, inv_machine_dt_vocab ","47357236":"def stoi(str, length, vocab, add_sos_token=False):\n    \"\"\"\n    Converts all strings in the vocabulary into a list of integers representing the positions of the\n    input string's characters in the \"vocab\"\n    \n    Arguments:\n    string -- input string, e.g. 'Wed 10 Jul 2007'\n    length -- the number of time steps you'd like, determines if the output will be padded or cut\n    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n    \n    Returns:\n    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n    \"\"\"\n    #str = str.lower()\n    str = str.replace(\",\", \"\")\n    if len(str) > length:\n        str = str[:length]\n    unk_index = vocab.get(\"<unk>\")            \n    char_indexes = [vocab.get(char, unk_index) for char in str]\n    if add_sos_token:\n        sos_index = vocab.get(\"<sos>\")        \n        # We add index corresponding to <sos> token to the start of target date sequence\n        char_indexes.insert(0, sos_index)\n    return np.array(char_indexes)","072367a2":"# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n# on the data. We use stratified kfold if the target distribution is unbalanced\ndef strat_kfold_dataframe(df, target_col_name, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df = df.sample(frac=1, random_state=Config.RANDOM_STATE_SEED).reset_index(drop=True)\n    # get the target data\n    y = df[target_col_name].values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE_SEED)\n    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_index, \"kfold\"] = fold\n    return df        ","adac6a28":"dataset = load_date_dataset(120000)\n# Let us create a dates dataframe that will contain training data of human readable and machine\n# readable dates\ndf_dates = pd.DataFrame({\"h_dt\": dataset[:, 0], \"m_dt\": dataset[:, 1]})\ndf_dates = strat_kfold_dataframe(df_dates, target_col_name=\"m_dt\", num_folds=5)  \ndf_dates.head()","7cd1c91a":"# converts a string (sequence of chars) to a tensor of ints where each int is the \n# position of the corresponding char in the relevant vocab\nclass StoITensorTransform(object):\n    def __init__(self, vocab, max_seq_length, add_sos_token=False):\n        self.vocab = vocab\n        self.max_seq_length = max_seq_length\n        self.add_sos_token = add_sos_token\n\n    def __call__(self, X):\n        vectorized_str = stoi(X, self.max_seq_length, self.vocab, self.add_sos_token)        \n        return torch.from_numpy(vectorized_str)","67752814":"# batch is the return value of __getitem__ method of the dataset being used. For DateDataset it is h_dt, m_dt\ndef pad_collate(batch):\n    # we want to pad the h_dt sequences as these can be of variable length.\n    # h_dt is of shape len(h_dt)\n    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n    h_dt_sorted = [x[0] for x in sorted_batch]\n    h_dt_padded = pad_sequence(h_dt_sorted, batch_first = True, padding_value=0)\n    # the original length of the padded h_dt sequences\n    h_dt_len = torch.Tensor([len(x) for x in h_dt_sorted])\n    # unpadded m_dt sequences    \n    m_dt = torch.stack([x[1] for x in sorted_batch])        \n    return h_dt_padded, h_dt_len, m_dt","e37bda12":"class DateDataset(Dataset):\n    def __init__(self, human_fmt_dates, machine_fmt_dates, transform, target_transform):\n        super().__init__()\n        self.h_dts = human_fmt_dates\n        self.m_dts = machine_fmt_dates\n        self.transform = transform\n        self.target_transform = target_transform        \n\n    # Returns vectorized form of human format date and its corresponding machine format date\n    # with elements of the vectorized date being the index of the characters in the corresponding date vocab\n    def __getitem__(self, index):\n        h_dt = self.h_dts[index]\n        m_dt = self.m_dts[index]\n        if self.transform:\n            h_dt = self.transform(h_dt)\n        if self.target_transform:\n            m_dt = self.target_transform(m_dt)\n        return h_dt, m_dt\n\n    def __len__(self):                \n        return len(self.h_dts)","6bcb4051":"# Get the train and validation data loaders for a specific fold. \n# X: numpy array of input features\n# y: numpy array of target labels\n# fold: fold index for which to create data loaders                                     \n# kfolds: Array that marks each of the data items as belonging to a specific fold\ndef get_fold_dls(fold, df):\n    fold += 1                         \n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    val_df = df[df.kfold == fold].reset_index(drop=True)\n    h_dt_max_len = train_df.h_dt.apply(lambda x: len(x)).max()\n    h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_source_target_vocab(train_df.h_dt, train_df.m_dt)    \n    # transform to convert human_date and machine_date to one hot encoded forms\n    transform = StoITensorTransform(h_vocab, h_dt_max_len)\n    target_transform = StoITensorTransform(m_vocab, len(m_vocab), add_sos_token=True)\n    ds_train = DateDataset(train_df.h_dt, train_df.m_dt, transform=transform, target_transform=target_transform)\n    ds_val = DateDataset(val_df.h_dt, val_df.m_dt, transform=transform, target_transform=target_transform)\n    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, \n                        num_workers=Config.NUM_WORKERS, collate_fn=pad_collate)\n    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, \n                        collate_fn=pad_collate)\n    return dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab","1a3b44a3":"dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(0, df_dates)","002c94b6":"# https:\/\/discuss.pytorch.org\/t\/what-does-the-scatter-function-do-in-layman-terms\/28037\/3\ndef one_hot_encode(input, vocab_size):        \n    #print(f\"input.shape = {input.shape}, vocab_size = {vocab_size}\")\n    batch_size = input.shape[0]\n    seq_length = input.shape[1]\n    input = input.reshape(batch_size, seq_length, 1).to(Config.DEVICE)    \n    zeros_tensor = torch.zeros(batch_size, seq_length, vocab_size).to(Config.DEVICE)    \n    return zeros_tensor.scatter_(2, input, 1)","0df28aef":"class lstm_encoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, drop_out, is_bidirect=False):\n        super().__init__()\n        # input to lstm is a sequence (of words, of chars, of anything else). The dimensions being \n        # (batch_size, sequence_length, input_size) if batch_first = True with sequence_length = length of longest sequence in the batch\n        # where input_size = number of features(cols) in input X. If you use embedding layer, then each word in the\n        # the sequence is represented by an embedding vector, so input_size = size of the embedding vector. If one\n        # hot encoding representation is used then input_size = vocab_size with each word represented by a one hot\n        # vector with size = vocab_size        \n        self.input_size = input_size\n        # hidden_size = number of units in the hidden layer\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.is_bidirect = is_bidirect\n        self.num_directions = 2 if is_bidirect else 1\n        self.lstm_layer = nn.LSTM(\n            input_size = input_size, \n            hidden_size = hidden_size,\n            num_layers = num_layers,\n            batch_first = True,\n            dropout = drop_out,\n            bidirectional = is_bidirect\n            )                \n\n    def forward(self, inputs, input_lengths):       \n        # inputs = [batch_size, max_seq_length] \n        # we are going to use one hot encoding representation of the human dates data. The input data is\n        # vectorized form of human format date with elements of the vectorized date being the index of the \n        # characters in the corresponding date vocab (input_size = vocab_size)\n        # inputs_oh = F.one_hot(inputs.T.float(), self.input_size)\n        #print(f\"inputs.shape = {inputs.shape}\")\n        inputs_oh = one_hot_encode(inputs, self.input_size)\n        #print(f\"inputs_oh.shape = {inputs_oh.shape}\")\n        # inputs_oh = [batch_size, max_seq_length, vocab_size]\n        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n        # which elements of the sequence are padded ones and ignore them in computation.\n        packed_padded_inputs = pack_padded_sequence(inputs_oh, input_lengths.to(\"cpu\"), batch_first=True)\n        lstm_out_pack, (h_final, c_final) = self.lstm_layer(packed_padded_inputs)\n        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]        \n        return h_final, c_final\n\n    # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n    # hidden state for each element in the batch, c0 = initial cell state for each element in the batch\n    def init_state(self, batch_size):\n        return (\n            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n            torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n        )","7c2b0e1e":"class lstm_decoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, drop_out):\n        super().__init__()        \n        self.input_size = input_size        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers                \n        self.lstm_layer = nn.LSTM(\n            input_size = input_size, \n            hidden_size = hidden_size,\n            num_layers = num_layers,\n            batch_first = True,\n            dropout = drop_out,\n            bidirectional = False\n            )   \n        self.linear = nn.Linear(hidden_size, input_size)                        \n\n    def forward(self, input_oh, encoder_hidden, encoder_cell):\n        # input_oh = [batch_size, target_vocab_size]\n        # The input sequence length in decoder is always 1 as we feed in one character at a time\n        input_oh = input_oh.unsqueeze(1).to(Config.DEVICE)\n        # input_oh = [batch_size, 1, target_vocab_size]        \n        # inputs_oh = F.one_hot(inputs.T.long(), self.input_size)                \n        lstm_out, (h_final, c_final) = self.lstm_layer(input_oh, (encoder_hidden, encoder_cell))\n        #print(f\"decoder lstm_out.shape = {lstm_out.shape}\")\n        # lstm_out = [batch_size , seq_length , num_directions * hidden_size]\n        # h_final and c_final = [num_direction * num_layers, batch_size, hidden_size]\n        # seq_length and num_direction will always be 1 for decoder. Thus\n        # lstm_out = [batch_size, 1, hidden_size]\n        # h_final and c_final = [num_layers, batch_size, hidden_size]\n        pred = self.linear(lstm_out.squeeze(1))        \n        # pred = [batch_size, output_dim] where output_dim = vocab_size of target sequences ( machine dates in our case)\n        # For multi class classification the number of output nodes is equal to the number of classes to predict (vocab size)\n        return pred, h_final, c_final\n        ","2eab54b2":"class EncoderDecoderLitModel(pl.LightningModule):\n    def __init__(self, hparams, source_vocab_size, target_vocab_size):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = hparams[\"lr\"]\n        self.weight_decay = hparams[\"weight_decay\"]\n        # target_vocab_size = vocab_size for target sequence data (machine_date) as we are using one hot encoding\n        self.target_vocab_size = target_vocab_size\n        self.encoder = lstm_encoder(\n            input_size = source_vocab_size,\n            hidden_size = hparams[\"hidden_size\"],\n            num_layers = hparams[\"num_layers\"],\n            drop_out = hparams[\"enc_drop_out\"]\n            )\n        self.decoder = lstm_decoder(\n            input_size = target_vocab_size,\n            hidden_size = hparams[\"hidden_size\"],\n            num_layers = hparams[\"num_layers\"],\n            drop_out = hparams[\"dec_drop_out\"]\n        )            \n\n    def forward(self, src_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0.5):        \n        # src_seq = [batch_size, max_source_seq_length]\n        # target_seq = [batch_size, target_seq_length]\n        batch_size = target_seq_oh.shape[0]        \n        # target sequence length is 11 as it includes the <sos> token at the begining\n        target_seq_length = target_seq_oh.shape[1]            \n        # tensor to store decoder output\n        dec_outputs = torch.zeros((batch_size, target_seq_length, self.target_vocab_size))        \n        #print(f\"dec_outputs.shape = {dec_outputs.shape}\")\n        # first input to the decoder is the <sos> token which is the first character in all target sequences\n        input = target_seq_oh[:, 0, :].reshape(batch_size, -1)\n        # input = [batch_size, target_vocab_size]\n        #print(f\"decoder input.shape = {input.shape}\")\n        # last hidden and cell state of the encoder is used as initial hidden and cell state of the decoder\n        hidden, cell = self.encoder(src_seq, src_seq_lengths)\n        for t in range(1, target_seq_length):            \n            dec_output, hidden, cell = self.decoder(input, hidden, cell)\n            # dec_output = [batch_size, target_vocab_size]            \n            dec_outputs[:, t, :] = dec_output\n            # whether to use teacher forcing\n            teacher_forcing = False if np.random.random() < teacher_forcing_ratio else True            \n            # if teacher forcing use actual token at t as the input to t+1, otherwise use the prediction at t\n            # as the input to t+1\n            actual_t = target_seq_oh[:, t, :]\n            input = actual_t if teacher_forcing else dec_outputs[:, t, :]\n            input = input.reshape(batch_size, -1)\n        return dec_outputs            \n\n    def configure_optimizers(self):\n        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=self.weight_decay)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n        return {\n            \"optimizer\": model_optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                \"monitor\": \"val_loss\",\n                \"frequency\": 1\n            }\n        }\n\n    def training_step(self, batch, batch_idx):\n        # data loader batch doesn't perform one hot encoding of either source or target sequences\n        src_padded_seq, src_seq_lengths, target_seq = batch\n        # target_seq = [batch_size, target_seq_length]\n        # src_padded_seq = [batch_size, max_src_seq_length]\n        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n        # target_seq_oh = [batch_size, target_seq_length, target_vocab_size]\n        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh)\n        # pred_target_seq = [batch_size, target_seq_length, target_vocab_size]\n        # we will exclude the first character from both the predicted and actual target dates. The first character\n        # in target_dates in <sos> token while the first value in pred_target_dates is 0.         \n        #print(f\"target_seq.shape = {target_seq.shape}\")\n        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")\n        target_seq = target_seq[:, 1:]        \n        # flatten the target_seq to 1d \n        target_seq = target_seq.reshape(-1)\n        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n        # flatten the predicted target seq to 2d\n        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n        #print(f\"target_seq.shape = {target_seq.shape}\")\n        #print(f\"pred_target_seq.shape = {pred_target_seq.shape}\")                \n        train_loss = cross_entropy(pred_target_seq, target_seq)\n        train_perplexity = torch.exp(train_loss)\n        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        self.log(\"train_perplexity\", train_perplexity, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return train_loss\n\n    def validation_step(self, batch, batch_idx):        \n        src_padded_seq, src_seq_lengths, target_seq = batch\n        target_seq_oh = one_hot_encode(target_seq, self.target_vocab_size)\n        # Remember to turn teacher forcing off for validation\n        pred_target_seq = self(src_padded_seq, src_seq_lengths, target_seq_oh, teacher_forcing_ratio=0)\n        target_seq = target_seq[:, 1:].reshape(-1)\n        pred_target_seq = pred_target_seq[:, 1:, :].to(Config.DEVICE)\n        # flatten the predicted target seq to 2d\n        pred_target_seq = pred_target_seq.view(-1, self.target_vocab_size)\n        val_loss = cross_entropy(pred_target_seq, target_seq)\n        val_perplexity = torch.exp(val_loss)\n        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        self.log(\"val_perplexity\", val_perplexity, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return val_loss","df5ee505":"# For results reproducibility \n# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\npl.seed_everything(42, workers=True)\n\n# Best trial number = 10\n# Best trial params:\n# {'lr': 0.0009729811471218791, 'hidden_size': 218, 'drop_out': 0.20683771027057984, 'num_layers': 2, 'weight_decay': 1.0661805946346311e-06}\n\n# model hyperparameters\nmodel_params = {    \n    \"num_layers\": 2,    \n    \"hidden_size\": 218,\n    \"enc_drop_out\": 0.206,\n    \"dec_drop_out\": 0.206,\n    \"lr\": 0.00097,\n    \"weight_decay\": 1.066e-06\n    }","1077c1d0":"from pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning import LightningModule, Trainer\n# Monitor multiple metric values that are calculated either in training or validation step and return the\n# best metric values for each epoch\nclass MetricsAggCallback(Callback):\n    def __init__(self, metrics_to_monitor):\n        # dictionary with metric name as key and monitor mode (min, max) as the value\n        # ( the same names used to log metric values in training and validation step)\n        self.metrics_to_monitor = metrics_to_monitor\n        # dictionary with metric_name as key and list of metric value for each epoch\n        self.metrics = {metric: [] for metric in metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the best metric value for all epochs\n        self.best_metric = {metric: None for metric in metrics_to_monitor.keys()}\n        # dictionary with metric_name as key and the epoch number with the best metric value\n        self.best_metric_epoch = {metric: None for metric in metrics_to_monitor.keys()}     \n        self.epoch_counter = 0   \n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n        self.epoch_counter += 1\n        print(f\"For epoch {self.epoch_counter}\")            \n        for metric, mode in self.metrics_to_monitor.items():\n            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n            print(f\"{metric} = {metric_value}\")\n            self.metrics[metric].append(metric_value)\n            if mode == \"max\":\n                self.best_metric[metric] = max(self.metrics[metric])            \n            elif mode == \"min\":            \n                self.best_metric[metric] = min(self.metrics[metric])            \n            self.best_metric_epoch[metric] = self.metrics[metric].index(self.best_metric[metric])    ","a533f488":"def run_training(fold, fold_loss, fold_metrics, dl_train, dl_val, h_vocab, m_vocab, find_lr=True):\n    fold_str = f\"fold{fold}\"\n    print(f\"Running training for {fold_str}\")\n    seq2seq_model = EncoderDecoderLitModel(\n        hparams = model_params, \n        source_vocab_size = len(h_vocab),\n        target_vocab_size = len(m_vocab)\n        )\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n    chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"\n    metrics_to_monitor = {\n        \"val_loss\": \"min\",\n        \"val_perplexity\": \"min\",\n        }\n    loss_chkpt_callback = ModelCheckpoint(dirpath=\".\/model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n    metric_chkpt_callback = MetricsAggCallback(metrics_to_monitor = metrics_to_monitor)\n    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n    trainer = pl.Trainer(\n        gpus = 1,\n        deterministic = True,\n        auto_select_gpus = True,\n        progress_bar_refresh_rate = 20,\n        max_epochs = Config.NUM_EPOCHS,\n        logger = tb_logger,\n        auto_lr_find = True,    \n        precision = Config.PRECISION,   \n        fast_dev_run = Config.FAST_DEV_RUN, \n        gradient_clip_val = 1.0,\n        #resume_from_checkpoint = \"model\/best_model_epoch=71_val_loss=0.4772.ckpt\",\n        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n    )        \n    if find_lr:\n        trainer.tune(model=seq2seq_model, train_dataloaders=dl_train)\n        print(seq2seq_model.lr)\n    trainer.fit(seq2seq_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n    fold_min_loss = loss_chkpt_callback.best_model_score.cpu().detach().item()\n    fold_loss.append(fold_min_loss)\n    fold_metrics = {metric: (metric_chkpt_callback.best_metric[metric], metric_chkpt_callback.best_metric_epoch[metric]) \n                    for metric in metrics_to_monitor.keys()}\n    print(f\"Best metric value for {fold_str}\")\n    print(f\"val_loss  = {fold_loss[fold]}\")\n    print(fold_metrics)\n    del trainer, seq2seq_model, loss_chkpt_callback, metric_chkpt_callback ","39441c9f":"from optuna.integration import PyTorchLightningPruningCallback\n\ndef run_hparam_tuning(model_params, trial):\n    dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(0, df_dates)\n    early_stopping = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")    \n    seq2seq_model = EncoderDecoderLitModel(\n        hparams = model_params, \n        source_vocab_size = len(h_vocab),\n        target_vocab_size = len(m_vocab)\n        )    \n    trainer = pl.Trainer(\n        checkpoint_callback=False,        \n        gpus=1,\n        # For results reproducibility \n        deterministic=True,\n        auto_select_gpus=True,\n        progress_bar_refresh_rate=20,\n        max_epochs=Config.NUM_EPOCHS,        \n        precision=Config.PRECISION,   \n        weights_summary=None,         \n        gradient_clip_val = 1.0,            \n        callbacks=[early_stopping]\n    )      \n    trainer.fit(seq2seq_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n    loss = trainer.callback_metrics[\"val_loss\"].item()\n    del trainer, seq2seq_model, early_stopping, dl_train, dl_val\n    return loss ","270ebdba":"import optuna\n\nConfig.NUM_EPOCHS = 20        \ndef objective(trial):\n    params = {\n        \"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3),\n        \"hidden_size\": trial.suggest_int(\"hidden_size\", 32, 512),\n        \"enc_drop_out\": trial.suggest_uniform(\"enc_drop_out\", 0.2, 0.7),\n        \"dec_drop_out\": trial.suggest_uniform(\"dec_drop_out\", 0.2, 0.7),\n        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 2),\n        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2),\n    }    \n    loss = run_hparam_tuning(params, trial)\n    return loss\n\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"Seq2SeqModelTuning\")    \nstudy.optimize(objective, n_trials=20)\nprint(f\"Best trial number = {study.best_trial.number}\")\nprint(\"Best trial params:\")\nprint(study.best_params)","debb0989":"%load_ext tensorboard","265ef806":"find_lr = True\nfold_loss = []\nfold_metrics = []\n\nConfig.NUM_EPOCHS = 75\nfor fold in range(Config.NUM_FOLDS):\n    dl_train, dl_val, ds_train, ds_val, h_vocab, m_vocab, inv_h_vocab, inv_m_vocab = get_fold_dls(fold, df_dates)\n    run_training(fold, fold_loss, fold_metrics, dl_train, dl_val, h_vocab, m_vocab, find_lr=False)\n    break","eb83f1f9":"def get_key_from_value(dict, value_to_search):\n    for key, value in dict.items():\n        if value == value_to_search:\n            return key","5fff3843":"# inference on test set\n\ndef run_prediction():\n    test_dataset = load_date_dataset(num_examples=10)\n    Config.BATCH_SIZE = 1\n    df_dates_test = pd.DataFrame({\"h_dt\": test_dataset[:, 0], \"m_dt\": test_dataset[:, 1]})\n    h_dt_max_len = df_dates_test.h_dt.apply(lambda x: len(x)).max()\n    transform = StoITensorTransform(h_vocab, h_dt_max_len)\n    target_transform = StoITensorTransform(m_vocab, len(m_vocab), add_sos_token=True)\n    ds_test = DateDataset(df_dates_test.h_dt, df_dates_test.m_dt, transform=transform, target_transform=target_transform)    \n    dl_test = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, collate_fn=pad_collate)\n    seq2seq_model = EncoderDecoderLitModel.load_from_checkpoint(\n        checkpoint_path = \"..\/input\/datetranslationmodel\/best_model_epoch70_val_loss0.0010.ckpt\",\n        hparams = model_params, \n        source_vocab_size = len(h_vocab),\n        target_vocab_size = len(m_vocab)\n    ).to(Config.DEVICE)\n    pred_table = []\n    for src_seq, src_seq_len, target_seq in dl_test:        \n        src_seq = src_seq.to(Config.DEVICE)\n        src_seq_len = src_seq_len.to(Config.DEVICE)\n        target_seq = target_seq.to(Config.DEVICE)\n        target_seq_oh = one_hot_encode(target_seq, len(m_vocab))\n        outputs = seq2seq_model(src_seq, src_seq_len, target_seq_oh)\n        pred_target_seq = outputs.argmax(2)[:, 1:].reshape(-1).cpu().tolist()\n        target_seq = target_seq[:, 1:].reshape(-1).cpu().tolist()                \n        src_seq = src_seq.reshape(-1).cpu().tolist()\n        source_str = ''.join([get_key_from_value(h_vocab, index) for index in src_seq])                \n        target_str = ''.join([inv_m_vocab[index] for index in target_seq])        \n        pred_target_str = ''.join([get_key_from_value(m_vocab, index) for index in pred_target_seq])        \n        pred_table.append([source_str, target_str, pred_target_str])        \n    return pred_table","f2c5ea73":"from tabulate import tabulate\n\npred_table = run_prediction()    \nheader = [\"Source sequence\", \"Target Sequence\", \"Predicted Target Sequence\"]\nprint(tabulate(pred_table, headers=header))","62dc66fa":"## Encoder","3deb3182":"### Hyperparameter tuning using optuna (uncomment if you want to run)","ce9f966c":"## Date translation (sequence to sequence learning) using encoder decoder architecture with LSTMs. Simulates a language translation task \nSource sequence is date data in different formats\nTarget sequence is date data in machine readable \"yyyy-mm-dd\" format","08be2080":"### K fold CV","9564b411":"## Training loop","fc2a4bea":"## Get training and validation data for a fold","e4f918af":"## Vectorize the text data using text vocab","b053e332":"## Decoder","0fbd7074":"## Encoder decoder model with pytorch lightning","f988f080":"## Padding the source sequence","2587eace":"## Generate synthetic training dataset","433c207e":"### Custom lightning callback \nTo record validation metric values at each epoch and the best metric values across all epochs","0cb75174":"## Custom date dataset","71fac456":"## Inference","4d4d0fae":"## One hot encoding of data","3ea9f21d":"Model trained perfectly on local machine with GeForce 1060 but somehow on kaggle kernel, the loss refused to go down with exactly the same codebase. May be this has to do with different execution environment. I have uploaded the local model and using that for inference. It gives perfect predictions on test set."}}