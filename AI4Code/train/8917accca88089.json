{"cell_type":{"0501e05d":"code","0b33f9dc":"code","e64fc61f":"code","06be7643":"code","941d3a22":"code","fea05c3e":"code","3042fe33":"code","a3351145":"code","5c7f9cab":"code","68b8d392":"code","59a51e07":"code","941b3902":"code","1d922bc9":"code","e6c834f8":"code","ad30b6d4":"code","3cb12245":"code","f803c2ca":"code","56034e28":"code","884983f7":"code","bab1afac":"markdown","5320a5c1":"markdown","9a4cdca1":"markdown","7ddf6b4b":"markdown","e8c0062f":"markdown","e89efca8":"markdown","f17cd7c7":"markdown","fc306cc9":"markdown","d1d0b941":"markdown","dd99aa37":"markdown","66918b79":"markdown","65c1c234":"markdown","f0227b1e":"markdown"},"source":{"0501e05d":"import numpy as np\nimport pandas as pd\nfrom scipy import signal\n\nimport math, random\nimport gym\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","0b33f9dc":"import warnings\nwarnings.filterwarnings(\"ignore\")","e64fc61f":"ada_stock = pd.read_csv(\"..\/input\/binance-top-cryptocurrencies\/ADA.csv\")\nxrp_stock = pd.read_csv(\"..\/input\/binance-top-cryptocurrencies\/XRP.csv\")\n\nada_open = ada_stock[\"open\"].values\nada_close = ada_stock[\"close\"].values\nxrp_open = xrp_stock[\"open\"].values\nxrp_close = xrp_stock[\"close\"].values\n\nada_stock.head()","06be7643":"plt.figure(1, figsize=(15,5))\nada = plt.subplot(121)\nxrp = plt.subplot(122)\nada.title.set_text(\"ADA\")\nada.plot(range(0, len(ada_open)), ada_open)\nxrp.title.set_text(\"XRP\")\nxrp.plot(range(0, len(xrp_open)), xrp_open)","941d3a22":"# ADA\nada_open = signal.detrend(ada_open)\nada_close = signal.detrend(ada_close)\n\n# XRP\nxrp_open = signal.detrend(xrp_open)\nxrp_close = signal.detrend(xrp_close)","fea05c3e":"plt.figure(1, figsize=(15,5))\nada = plt.subplot(121)\nxrp = plt.subplot(122)\nada.title.set_text(\"ADA Open\")\nada.plot(range(0, len(ada_open)), ada_open)\nxrp.title.set_text(\"XRP Open\")\nxrp.plot(range(0, len(xrp_open)), xrp_open)","3042fe33":"# Equalize length\nada_open = ada_open[:len(xrp_open)]\nada_close = ada_close[:len(xrp_close)]","a3351145":"print(\"Min values: \\nADA_open:{:.4f}, ADA_close:{:.4f}, XRP_open:{:.4f}, XRP_close:{:.4f} \\n\"\n         .format(ada_open.min(), ada_close.min(), xrp_open.min(), xrp_close.min()))\n\nadd_nr = -1 * round(xrp_close.min(), 3)\nprint(\"Add {0:1.3f} to the open\/close\".format(add_nr))\n\nada_open += add_nr\nada_close += add_nr\nxrp_open += add_nr\nxrp_close += add_nr","5c7f9cab":"xrp_open = xrp_open[:len(ada_open)]\nxrp_close = xrp_close[:len(xrp_close)]","68b8d392":"class Environment():\n\n    def __init__(self, starting_cash_mean=200., max_stride=5, series_length=208, starting_point=1, randomize_cash_std=0, \\\n                 starting_shares_mean=0., randomize_shares_std=0., inaction_penalty=0.):\n        \n        self.starting_shares_mean = starting_shares_mean\n        self.randomize_shares_std = randomize_shares_std\n        self.starting_cash_mean = starting_cash_mean\n        self.randomize_cash_std = randomize_cash_std\n        \n        self.actions = {\"buy_ada\": 0, \"sell_ada\": 1, \"wait\": 2, \"buy_xrp\": 3, \"sell_xrp\": 4}\n        \n        self.state = torch.zeros(8, dtype=torch.float32)\n        \n        self.starting_cash = max(int(np.random.normal(self.starting_cash_mean, self.randomize_cash_std)), 0.)\n        \n        self.series_length = series_length\n        self.starting_point = starting_point\n        self.cur_timestep = self.starting_point\n        \n        self.state[0] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n        self.state[1] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n        self.starting_portfolio_value = self.portfolio_value()\n        self.state[2] = self.starting_cash\n        self.state[3] = ada_open[self.cur_timestep]\n        self.state[4] = xrp_open[self.cur_timestep]\n        self.state[5] = self.starting_portfolio_value\n        self.state[6] = self.five_day_window()[0]\n        self.state[7] = self.five_day_window()[1]\n        \n        self.max_stride = max_stride\n        self.stride = self.max_stride\n        \n        self.done = False\n        self.diversification_bonus = 1.\n        self.inaction_penalty = inaction_penalty\n    \n    def portfolio_value(self):\n        return (self.state[0] * ada_close[self.cur_timestep]) + (self.state[1] * xrp_close[self.cur_timestep]) + self.state[2]\n    \n    def next_opening_price(self):\n        step = self.cur_timestep + self.stride\n        return [ada_open[step], xrp_open[step]]\n    \n    def five_day_window(self):\n        step = self.cur_timestep\n        if step < 5:\n            return [ada_open[0], xrp_open[0]]\n        ada5 = ada_open[step-5:step].mean()\n        xrp5 = xrp_open[step-5:step].mean()\n        return [ada5, xrp5]\n    \n    def is_done(self, cur_timestep, cur_value, gain):\n        if cur_timestep >= self.starting_point + (self.series_length * self.stride):\n            new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n                        cur_value, *self.five_day_window()]\n            self.state = new_state\n            bonus = 0.\n            if self.state[0] > 0 and self.state[1] > 0:\n                bonus = self.diversification_bonus\n            return new_state, cur_value + bonus + gain, True, { \"msg\": \"done\"}\n        return None, cur_value + 0 + gain, False, { \"msg\": \"not done\"}\n    \n    \n    def wait(self, cur_value, ts_left, gain):\n        new_state = [self.state[0], self.state[1], self.state[2], * self.next_opening_price(), \\\n                    cur_value, *self.five_day_window()]\n        self.state = new_state\n        retval = new_state, -self.inaction_penalty - ts_left + gain, False, { \"msg\": \"wait\" }\n        return retval\n    \n    \n    def bankrupt(self, cur_value, ts_left, gain):\n        new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n                cur_value, *self.five_day_window()]\n        self.state = new_state\n        return new_state, -ts_left+gain\/2, True, { \"msg\": \"bankrupted self\"}\n    \n    \n    def sold_too_much(self, cur_value, ts_left, gain):\n        new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n                cur_value, *self.five_day_window()]\n        self.state = new_state\n        return new_state, -ts_left+gain\/2, True, { \"msg\": \"sold more than have\"}\n    \n        \n    def step(self, action):\n        action = [action, 1.]\n        cur_timestep = self.cur_timestep\n        ts_left = self.series_length - (cur_timestep - self.starting_point)\n        retval = None\n        cur_value = self.portfolio_value()\n        gain = cur_value - self.starting_portfolio_value\n        \n        retval = self.is_done(cur_timestep, cur_value, gain)\n        if retval[2]:\n            return retval\n        \n        if action[0] == self.actions[\"wait\"]:\n            return self.wait(cur_value, ts_left, gain)\n            \n        if action[0] == self.actions[\"buy_ada\"]:\n            if action[1] * ada_open[cur_timestep] > self.state[2]:\n                retval = self.bankrupt(self, cur_value, ts_left, gain)\n            else:\n                ada_shares = self.state[0] + action[1]\n                cash_spent = action[1] * ada_open[cur_timestep] * 1.1\n                new_state = [ada_shares, self.state[1], self.state[2] - cash_spent, *self.next_opening_price(), \\\n                       cur_value, *self.five_day_window()]\n                self.state = new_state\n                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"bought ADA\"}\n                \n        if action[0] == self.actions[\"buy_xrp\"]:\n            if action[1] * xrp_open[cur_timestep] > self.state[2]:\n                retval = self.bankrupt(self, cur_value, ts_left, gain)\n            else:\n                xrp_shares = self.state[1] + action[1]\n                cash_spent = action[1] * xrp_open[cur_timestep] * 1.1\n                new_state = [self.state[0], xrp_shares, self.state[2] - cash_spent, *self.next_opening_price(), \\\n                       cur_value, *self.five_day_window()]\n                self.state = new_state\n                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"bought XRP\"}  \n\n        if action[0] == self.actions[\"sell_ada\"]:\n            if action[1] > self.state[0]:\n                retval = self.sold_too_much(cur_value, ts_left, gain)\n            else:\n                ada_shares = self.state[0] - action[1]\n                cash_gained = action[1] * ada_open[cur_timestep] * 0.9\n                new_state = [ada_shares, self.state[1], self.state[2] + cash_gained, *self.next_opening_price(), \\\n                       cur_value, *self.five_day_window()]\n                self.state = new_state\n                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"sold ADA\"}\n                \n        if action[0] == self.actions[\"sell_xrp\"]:\n            if action[1] > self.state[1]:\n                retval = self.sold_too_much(cur_value, ts_left, gain)\n            else:\n                xrp_shares = self.state[1] - action[1]\n                cash_gained = action[1] * xrp_open[cur_timestep] * 0.9\n                new_state = [self.state[0], xrp_shares, self.state[2] + cash_gained, *self.next_opening_price(), \\\n                       cur_value, *self.five_day_window()]\n                self.state = new_state\n                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"sold XRP\"}\n                \n        self.cur_timestep += self.stride\n        return retval\n    \n    def reset(self):\n        self.state = torch.zeros(8)\n        self.starting_cash = max(int(np.random.normal(self.starting_cash_mean, self.randomize_cash_std)), 0.)\n        self.cur_timestep = self.starting_point\n        self.state[0] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n        self.state[1] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n        self.state[2] = self.starting_cash\n        self.state[3] = ada_open[self.cur_timestep]\n        self.state[4] = xrp_open[self.cur_timestep]\n        self.starting_portfolio_value = self.portfolio_value()\n        self.state[5] = self.starting_portfolio_value\n        self.state[6] = self.five_day_window()[0]\n        self.state[7] = self.five_day_window()[1]       \n        self.done = False\n        return self.state","59a51e07":"class Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.input_layer = nn.Linear(8, 128)\n        self.hidden_1 = nn.Linear(128, 128)\n        self.hidden_2 = nn.Linear(32,31)\n        self.hidden_state = torch.zeros(2,1,32)\n        self.rnn = nn.GRU(128, 32, 2)\n        self.action_head = nn.Linear(31, 5)\n        self.value_head = nn.Linear(31, 1)\n        self.saved_actions = []\n        self.rewards = []\n\n    def reset_hidden(self):\n        self.hidden_state = torch.zeros(2,1,32)\n        \n    def forward(self, x):\n        x = torch.as_tensor(x).type(torch.float32)\n        x = torch.sigmoid(self.input_layer(x))\n        x = torch.tanh(self.hidden_1(x))\n        x, self.hidden_state = self.rnn(x.view(1,-1,128), self.hidden_state.data)\n        x = F.relu(self.hidden_2(x.squeeze()))\n        action_scores = self.action_head(x)\n        state_values = self.value_head(x)\n        return F.softmax(action_scores, dim=-1), state_values\n    \n    def act(self, state):\n        probs, state_value = self.forward(state)\n        m = Categorical(probs)\n        action = m.sample()\n        if action == 1 and env.state[0] < 1: action = torch.LongTensor([2]).squeeze()\n        if action == 4 and env.state[1] < 1: action = torch.LongTensor([2]).squeeze()\n        self.saved_actions.append((m.log_prob(action), state_value))\n        return action.item()","941b3902":"series_length = int(len(ada_open) \/ 4) - 1\nenv = Environment(max_stride=4, series_length=series_length, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\nmodel = Policy()\noptimizer = optim.Adam(model.parameters(), lr=3e-4)","1d922bc9":"def end_episode():\n    R = 0\n    saved_actions = model.saved_actions\n    policy_losses = []\n    value_losses = []\n    rewards = []\n    for r in model.rewards[::-1]:\n        R = r + (gamma * R)\n        rewards.insert(0, R)\n    rewards = torch.tensor(rewards)\n    \n    epsilon = (torch.rand(1) \/ 1e4) - 5e-5\n\n    rewards += epsilon\n    \n    for (log_prob, value), r in zip(saved_actions, rewards):\n        reward = torch.tensor(r - value.item())\n        policy_losses.append(-log_prob * reward)\n        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))\n        \n    optimizer.zero_grad()\n    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n    loss = torch.clamp(loss, -1e-5, 1e5)\n    loss.backward()\n    optimizer.step()\n    del model.rewards[:]\n    del model.saved_actions[:]","e6c834f8":"env.reset()\ndel model.rewards[:]\ndel model.saved_actions[:]\n\ngamma = 0.9\nlog_interval = 100\n\nrunning_reward = 0\nfor episode in range(0, len(ada_open)):\n    state = env.reset()\n    reward = 0\n    done = False\n    msg = None\n    i = 0\n    while not done:\n        action = model.act(state)\n        state, reward, done, msg = env.step(action)\n        model.rewards.append(reward)\n        i += 1\n        if done:\n            break\n    running_reward = running_reward * (1 - 1\/log_interval) + reward * (1\/log_interval)\n    end_episode()\n\n    if msg[\"msg\"] == \"done\" and env.portfolio_value() > env.starting_portfolio_value * 1.1 and running_reward > 500:\n        print(\"Early Stopping: \" + str(int(reward)))\n        break\n        \n    if episode % log_interval == 0:\n        print(\"Episode: {:4d},  [Start: {:8.2f}],  [Finish: {:8.1f}],  [Status: {} at t={:4d}],  [Last_Reward: {:8.1f}],  [Running_Reward: {:8.1f}]\"\n              .format(episode, env.starting_portfolio_value, env.portfolio_value(), msg[\"msg\"], env.cur_timestep, reward, running_reward))","ad30b6d4":"env = Environment(max_stride=4, series_length=series_length, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\nenv.reset()\nprint(\"Action:{:12s}, Contains:[{} ADA][{} XPR][{:8.2f}$]\".format(\" -Starting-  \", env.state[0], env.state[1], env.portfolio_value()))\nfor i in range(0,env.series_length + 1):\n    action = model.act(env.state)\n    next_state, reward, done, msg = env.step(action)\n    if msg[\"msg\"] == 'bankrupted self':\n        print('bankrupted self by 1')\n        break\n    if msg[\"msg\"] == 'sold more than have':\n        print('sold more than have by 1')\n        break\n    print(\"Action: {:12s}, Contains:[{:6.2f} ADA][{:6.2f} XPR][{:8.2f}$]\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n    if msg[\"msg\"] == \"done\":\n        print(\"Total portfolio value {}\".format(env.portfolio_value()))\n        break","3cb12245":"ada_open_orig = ada_stock[\"open\"].values\nada_close_orig = ada_stock[\"close\"].values\nxrp_open_orig = xrp_stock[\"open\"].values\nxrp_close_orig = xrp_stock[\"close\"].values","f803c2ca":"bought_ada_at = []\nbought_xrp_at = []\nsold_ada_at = []\nsold_xrp_at = []\nbought_ada_at_orig = []\nbought_xrp_at_orig = []\nsold_ada_at_orig = []\nsold_xrp_at_orig = []\nnothing_at = []\nba_action_times = []\nbm_action_times = []\nsa_action_times = []\nsm_action_times = []\nn_action_times = []","56034e28":"env = Environment(max_stride=4, series_length=series_length, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\nenv.reset()\n\nstarting_val = env.starting_portfolio_value\nprint(\"Starting portfolio value: {}\".format(starting_val))\nfor i in range(0,env.series_length + 1):\n    action = model.act(env.state)\n    if action == 0:\n        bought_ada_at.append(ada_open[env.cur_timestep])\n        bought_ada_at_orig.append(ada_open_orig[env.cur_timestep])\n        ba_action_times.append(env.cur_timestep)\n    if action == 1:\n        sold_ada_at.append(ada_close[env.cur_timestep])\n        sold_ada_at_orig.append(ada_close_orig[env.cur_timestep])\n        sa_action_times.append(env.cur_timestep)\n    if action == 2:\n        nothing_at.append(35)\n        n_action_times.append(env.cur_timestep)\n    if action == 3:\n        bought_xrp_at.append(xrp_open[env.cur_timestep])\n        bought_xrp_at_orig.append(xrp_open_orig[env.cur_timestep])\n        bm_action_times.append(env.cur_timestep)\n    if action == 4:\n        sold_xrp_at.append(xrp_close[env.cur_timestep])\n        sold_xrp_at_orig.append(xrp_close_orig[env.cur_timestep])\n        sm_action_times.append(env.cur_timestep)\n    next_state, reward, done, msg = env.step(action)\n    if msg[\"msg\"] == 'bankrupted self':\n        env.reset()\n        break\n    if msg[\"msg\"] == 'sold more than have':\n        env.reset()\n        break\n    if msg[\"msg\"] == \"done\":\n        print(\"{}, have {} aapl and {} msft and {} cash\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n        val = env.portfolio_value()\n        print(\"Finished portfolio value {}\".format(val))\n        env.reset()\n        break","884983f7":"plt.figure(1, figsize=(15,5))\nada = plt.subplot(121)\nxrp = plt.subplot(122)\nada.plot(range(0, len(ada_open)), ada_open)\nxrp.plot(range(0, len(xrp_open)), xrp_open)\nada.plot(ba_action_times, bought_ada_at, \"ro\")\nada.plot(sa_action_times, sold_ada_at, \"go\")\nxrp.plot(bm_action_times, bought_xrp_at, \"ro\")\nxrp.plot(sm_action_times, sold_xrp_at, \"go\")","bab1afac":"# Prepare Data","5320a5c1":"# Environment","9a4cdca1":"# Examine Data","7ddf6b4b":"# Buy, Sell, Wait","e8c0062f":"## Equalize data","e89efca8":"<pre>\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2588\u2580\u2580\u2580\u2580\u2580\u2588\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2588\u2580\u2591\u2591\u2584\u2591\u2584\u2591\u2591\u2591\u2591\u2580\u2588\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2580\u2588\u2580\u2580\u2580\u2580\u2584\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2584\u2584\u2584\u2584\u2580\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2588\u2584\u2591\u2580\u2580\u2588\u2580\u2588\u2580\u2591\u2591\u2584\u2588\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2580\u2588\u2584\u2584\u2584\u2584\u2584\u2588\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n<\/pre>\n\n## Policy Reinforcement Learning Trading\nBy Alin Cijov","f17cd7c7":"# Reinforcement Learning Policy Model","fc306cc9":"# Plot the analysis","d1d0b941":"# Test on the original data","dd99aa37":"# Training","66918b79":"## Negative values","65c1c234":"# Load Data","f0227b1e":"## Signals"}}