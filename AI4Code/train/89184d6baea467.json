{"cell_type":{"e0da0baf":"code","b0a222f5":"code","8979c09e":"code","975d8c67":"code","d862ff92":"code","3362082e":"code","6f9048e8":"code","d93d0ba8":"code","18734b4a":"code","cfb4e808":"code","91a74e9e":"code","d518f89c":"code","15442171":"code","2c91d1a6":"code","95f93f0d":"code","d42992e2":"markdown","5df7c32a":"markdown","92d968a1":"markdown","9548a235":"markdown","80079735":"markdown"},"source":{"e0da0baf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b0a222f5":"# define dtypes on loading data  - will speed up and reduce memory use\ncategorical_dtypes = {\n    'event_type':'category', 'product_id':'category',\n    'category_id':'category',\n       'category_code':'category', 'brand':'category', \n    'user_id':'category', 'user_session':'category'\n}","8979c09e":"### setting the dtype on import to categorical doesn't work when concatenating.. \ndf = pd.concat([pd.read_csv(\"\/kaggle\/input\/ecommerce-events-history-in-cosmetics-shop\/2019-Oct.csv\",dtype=categorical_dtypes)\n                ,pd.read_csv(\"\/kaggle\/input\/ecommerce-events-history-in-cosmetics-shop\/2019-Nov.csv\",dtype=categorical_dtypes)])\n\ndf['event_time'] = pd.to_datetime(df['event_time'],infer_datetime_format=True)\n\n\n# add joint key for user + product (Could also add user + category)\ndf[\"user_product\"] = (df['user_id'].astype(str)+df['product_id'].astype(str)).astype('category').cat.codes\n\n\n## categorical\/label encoding of the IDs (instead of string - save memory\/file size):\n### we still need to define ats categoircal due to the concatenation of the input files\ndf['user_session'] = df['user_session'].astype('category').cat.codes.astype('category')\ndf['user_session'] = df['user_session'].astype('category').cat.codes.astype('category')\ndf['user_id'] = df['user_id'].astype('category').cat.codes.astype('category')\ndf['category_id'] = df['category_id'].astype('category').cat.codes.astype('category')\ndf['product_id'] = df['product_id'].astype('category').cat.codes.astype('category')\n\n\nprint(df.shape)\ndf.head()","975d8c67":"\n\n## categorical\/label encoding of the IDs (instead of string - save memory\/file size):\n### we still need to define ats categoircal due to the concatenation of the input files\ndf['user_session'] = df['user_session'].astype('category').cat.codes.astype('category')\ndf['user_session'] = df['user_session'].astype('category').cat.codes.astype('category')\ndf['user_id'] = df['user_id'].astype('category').cat.codes.astype('category')\ndf['category_id'] = df['category_id'].astype('category').cat.codes.astype('category')\ndf['product_id'] = df['product_id'].astype('category').cat.codes.astype('category')\n\n","d862ff92":"df[\"event_type\"].value_counts()","3362082e":"df[\"product_id\"].value_counts().describe()","6f9048e8":"df.columns","d93d0ba8":"df.drop([\"event_time\"],axis=1).nunique()","18734b4a":"## get all \"positive events\" , then later we'll add 0s\ndf_targets = df.loc[df[\"event_type\"].isin([\"cart\",\"purchase\"])].drop_duplicates(subset=['event_type', 'product_id',\n                                                                                        'price', 'user_id',\n                                                                                        'user_session'])\n\nprint(df_targets.shape)\ndf_targets.tail()","cfb4e808":"## not filtering by price (discount?) doesn't change much\ndf_targets.drop_duplicates(subset=['event_type', 'product_id', 'user_id', 'user_session']).shape[0]","91a74e9e":"## ## could also do this with np.where  ;  or stack + inindex ; or with a join (on filtered rows containing purhcase and fillna(0))\n\n# # df2[\"purchase\"] = (df2[\"event_type\"]==\"purchase\").astype(int)\n\n# df2[\"purchase\"] = np.where(df2[\"event_type\"]==\"purchase\",1,0)\n\n## https:\/\/stackoverflow.com\/questions\/48175172\/assign-a-pandas-series-to-a-groupby-operation","d518f89c":"## laziest option - add a row where purchased, groupby(max), then keep rows at time of added to cart\n\n# df_targets[\"purchased\"] = (df_targets[\"event_type\"]==\"purchase\").astype(int) #np.where() ## only kept 1s - bug ..\n\ndf_targets[\"purchased\"] = np.where(df_targets[\"event_type\"]==\"purchase\",1,0)\nprint(df_targets.shape)\ndf_targets[\"purchased\"].describe()","15442171":"df_targets[\"purchased\"] = df_targets.groupby([\"user_session\",\"product_id\"])[\"purchased\"].transform(\"max\")\ndf_targets[\"purchased\"].describe()","2c91d1a6":"# keep only rows with the time of addition to cart  = time of prediction\n## also, drop duplicates for cases of multiple purchases of same it or readding (only a small amount - a few hundred such cases)\ndf_targets = df_targets.loc[df_targets[\"event_type\"]==\"cart\"].drop_duplicates([\"user_session\",\"product_id\",\"purchased\"])\n\nprint(df_targets.shape)\ndf_targets[\"purchased\"].describe()","95f93f0d":"df.to_csv(\"ecom_cosmetics_timeSeries.csv.gz\",index=False,compression=\"gzip\")\n\n# output a sample of targets for modelling speed\ndf_targets.drop([\"event_type\"],axis=1).sample(frac=0.35).to_csv(\"ecom_cosmetics_cart_purchase_labels.csv.gz\",index=False,compression=\"gzip\")","d42992e2":"## PREPARE Data for classification\n* predict at time of view or addition tocart if user will purchase each product or not \n* this is about setting up the data & targets\n* This pipeline can also be applied to : https:\/\/www.kaggle.com\/mkechinov\/ecommerce-behavior-data-from-multi-category-store\n* Feature engineering will be done extnerally\n\n* another target possible - will an item be removed from cart ? ","5df7c32a":"# Export","92d968a1":"## optional: Feature engineering","9548a235":"# Target:\n* For each product the user viewed  or (depending on problem definition) added to cart , get combinations and predict if there was purchase.\n* Additional possible proxy target - did user add item to cart.\n\n\n* Q: Do we want to consider multiple user purchases of the same item? If so then we would want to not drop duplicates across sessions. For now, we will assume that the defined session seperates the data ok\n\n\n    * https:\/\/stackoverflow.com\/questions\/42854801\/including-missing-combinations-of-values-in-a-pandas-groupby-aggregation\n    \n    \n* get all conbinations then fillna 0 :\n    * https:\/\/stackoverflow.com\/questions\/31786881\/adding-values-for-missing-data-combinations-in-pandas","80079735":"## Target: For items added to basked: Predict if they will be purchased in that session\n\n* Different from predicting if they will be purchased in the future\n* Much less class imbalance.\n* Ignore possible target of items being deleted from session\n\n* Could also be modelled as recomender problem (+- explicit\/implicit (`lightFM`)) \/: 1 : added, -1: removed from shopping cart, 0: not purchased \"yet\" or not added to cart\n\n    * https:\/\/stackoverflow.com\/questions\/31786881\/adding-values-for-missing-data-combinations-in-pandas    * "}}