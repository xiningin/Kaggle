{"cell_type":{"0c49ece8":"code","76deb450":"code","3c6b5465":"code","35d7a2b6":"code","7317b756":"code","a0465cd6":"code","2a9677f5":"code","9a4f2aea":"code","30c90804":"code","96aefe7b":"code","aa6cd7bf":"code","d5917400":"code","a7b781c5":"code","d8390a9b":"code","8e233d89":"code","a0849a1b":"code","48bd27eb":"code","c27a28dc":"code","68332902":"code","b5ab7bc2":"code","85f3b279":"code","9f45a168":"code","2d918c02":"code","6f411724":"code","8e4b267c":"code","3083a879":"code","7a82012c":"code","b3ab003d":"code","a220f607":"code","9cbbe7e7":"code","cbd93b00":"code","2265c243":"code","3c6ab721":"code","69b79ecd":"code","c9c7f19f":"code","c4593577":"code","a5a59f7f":"code","42fb32c7":"code","70bf1edc":"code","2822ce7f":"code","cadf4461":"code","e9a424f1":"code","150512c9":"code","1a7a3a3d":"code","0902fe4b":"code","f5ab8a88":"code","6eeb24c6":"code","6a0e7589":"code","eee9aea0":"code","7d7d5a20":"code","3c6b9661":"code","328c228e":"code","6cf4411c":"code","7a5ea99c":"code","e0fc0907":"code","bd643c02":"code","c4b887d3":"code","9eed9d50":"code","a457984f":"code","810d432d":"code","910698d4":"code","81729a3e":"code","16561b8f":"code","fe069aeb":"code","37cf9de3":"code","fdd5338d":"code","085a7f76":"code","06d9a75c":"code","cc191a29":"code","4a2e9b89":"markdown","0c12559f":"markdown","4c2e3b7f":"markdown","9987f345":"markdown","766b6676":"markdown","b05bc9c3":"markdown","52055b16":"markdown","4730069f":"markdown","a3361896":"markdown","8612f7e2":"markdown","3734bb98":"markdown","5c39c74f":"markdown","92ecc3bb":"markdown","587954db":"markdown","6035053b":"markdown","4991eba7":"markdown","da364872":"markdown","b0e0964f":"markdown","9f6e3c3c":"markdown","21aecaa4":"markdown","8cd5b17c":"markdown","2473dbc4":"markdown","03c8f385":"markdown","24bda1b4":"markdown","0ac51c65":"markdown","b24c0d7d":"markdown","da30ef6a":"markdown","a92959ed":"markdown","c0b49569":"markdown","9b26b581":"markdown","eaa961ce":"markdown","e7117f98":"markdown","dad60d53":"markdown","3454bb3b":"markdown","115574cb":"markdown","494c5e96":"markdown","c2f870ba":"markdown","bbef7bbb":"markdown","5d643cdf":"markdown","4ca43332":"markdown","e327b556":"markdown","44780297":"markdown","146d9ffd":"markdown","5b73bee9":"markdown","2fb3b0d9":"markdown","68666740":"markdown","34ae6c8f":"markdown","dc2bea75":"markdown","9bf86000":"markdown","5d9a8aff":"markdown","3bfdcac4":"markdown","42c2bfbd":"markdown","91815401":"markdown","53b32144":"markdown","3dc835a8":"markdown","5caaaa95":"markdown","0144565b":"markdown","b4ee0836":"markdown","fd6a9ba2":"markdown","722e3f06":"markdown","0d74f475":"markdown","915ccbb7":"markdown","98b98b34":"markdown","6ceb5e6b":"markdown","4a6d3601":"markdown","4f0688e2":"markdown"},"source":{"0c49ece8":"#performing imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom scipy import stats\nimport warnings\nimport missingno as msno\nfrom scipy.stats import boxcox,skew,norm\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","76deb450":"#reading the trainset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","3c6b5465":"#Taking a look at the columns and shape of the trainset\nprint(\"Columns of trainset: \",train.columns)\nprint(\"Shape of the trainset: \",train.shape)","35d7a2b6":"#printing the head of the trainset\ntrain.head()","7317b756":"#listing the numeric features\nnumeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.columns","a0465cd6":"#listing the categorical features\ncategorical_features = train.select_dtypes(include=[np.object])\ncategorical_features.columns","2a9677f5":"#the ID column is only for indexing and won't help much in training the ML model.So it's safe to delete it\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain = train.drop(\"Id\",axis=1,inplace=False)\ntest = test.drop(\"Id\",axis=1,inplace=False)\n\n#printing the shapes of train set and test set just to verify that the ID column has been dropped\nprint(\"Trainset shape \",train.shape)\nprint(\"Testset shape \",test.shape)","9a4f2aea":"train['SalePrice'].describe()","30c90804":"#visualising a histogram of the target variable to see it's distribution\nsns.distplot(train['SalePrice'])","96aefe7b":"#calculating skewness and kutrosis for SalePrice\nprint(\"Skewness: \",train['SalePrice'].skew())\nprint(\"Kurtosis: \",train['SalePrice'].kurt())","aa6cd7bf":"#creating a correlation matrix\ncorr_mat = train.corr()\nprint(corr_mat['SalePrice'].sort_values(ascending=False),'\\n')","d5917400":"f, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corr_mat,square=True,vmax=0.8)","a7b781c5":"k= 11\ncols = corr_mat.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","d8390a9b":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\ndata.plot.scatter(x=var,y='SalePrice',ylim=(0,800000))","8e233d89":"#scatter plot garagecars\/saleprice. this is not a numerical variable but a dummy variable\nvar = 'GarageCars'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\ndata.plot.scatter(x=var,y='SalePrice',ylim=(0,800000))","a0849a1b":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\ndata.plot.scatter(x=var,y='SalePrice',ylim=(0,800000))","48bd27eb":"#bos plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\nf, ax = plt.subplots(figsize=(12,9))\nfig = sns.boxplot(x=var, y='SalePrice',data=data)\nfig.axis(ymin=0,ymax=800000)","c27a28dc":"#box plot yearbuilt\/saleprice\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\nf,ax = plt.subplots(figsize=(16,10))\nfig = sns.boxplot(x=var,y='SalePrice',data=data)\nfig.axis(ymin=0,ymax=800000)\nplt.xticks(rotation=90)","68332902":"#scatterplot\nsns.set()\ncols = ['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\nsns.pairplot(train[cols],size=3)\nplt.show()","b5ab7bc2":"fig, ax = plt.subplots(figsize=(16,9))\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","85f3b279":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index) #creating a virtual box to corner these\n                                                                                        #outliers\n\n#Check the graphic again\nfig, ax = plt.subplots(figsize=(16,9))\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9f45a168":"fig, ax = plt.subplots(figsize=(16,9))\nax.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","2d918c02":"#histogram and normal probability plot\nsns.distplot(train['SalePrice'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('SalePrice')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot=plt)\nplt.show()","6f411724":"#applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice']+1)","8e4b267c":"#transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('SalePrice')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot=plt)\nplt.show()","3083a879":"#histogram and normal probability plot\nsns.distplot(train['GrLivArea'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['GrLivArea'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('GrLivArea')\n\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'],plot=plt)\nplt.show()","7a82012c":"#applying log transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea']+1)","b3ab003d":"#transformed histogram and normal probability plot\nsns.distplot(train['GrLivArea'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['GrLivArea'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('GrLivArea')\n\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'],plot=plt)\nplt.show()","a220f607":"#histogram and normal probability plot\nsns.distplot(train['TotalBsmtSF'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['TotalBsmtSF'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('TotalBsmtSF')\n\nfig = plt.figure()\nres = stats.probplot(train['TotalBsmtSF'],plot=plt)\nplt.show()","9cbbe7e7":"#transformed histogram and normal probability plot\nsns.distplot(np.log(train['TotalBsmtSF']+1),fit=norm)\nfig = plt.figure()\nres = stats.probplot(np.log(train['TotalBsmtSF']+1),plot=plt)\n\n#Note:This doesn't seem well","cbd93b00":"train['TotalBsmtSF'],maxlog = boxcox(train['TotalBsmtSF']+1)\n\n#histogram and normal probability plot\nsns.distplot(train['TotalBsmtSF'],fit=norm)\n\n#getting fitted parameters used by the function\n(mu,sigma) = norm.fit(train['TotalBsmtSF'])\nprint('\\n mu = {0:.2f} and sigma = {1:.2f}'.format(mu,sigma))\n\n#plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {0:.2f} and $\\sigma = ${1:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.xlabel('TotalBsmtSF')\n\nfig = plt.figure()\nres = stats.probplot(train['TotalBsmtSF'],plot=plt)\nplt.show()","2265c243":"#working with the missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count())*100).sort_values(ascending=False)\nmissing_vals = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_vals.head(30)","3c6ab721":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\ndata_na = missing_vals[:20]\nsns.barplot(x=data_na.index, y=data_na.Percent)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","69b79ecd":"train['PoolQC'] = train['PoolQC'].fillna('None')\ntest['PoolQC'] = test['PoolQC'].fillna('None')","c9c7f19f":"train['MiscFeature'] = train['MiscFeature'].fillna('None')\ntest['MiscFeature'] = test['MiscFeature'].fillna('None')","c4593577":"train['Alley'] = train['Alley'].fillna('None')\ntest['Alley'] = test['Alley'].fillna('None')","a5a59f7f":"train['Fence'] = train['Fence'].fillna('None')\ntest['Fence'] = train['Fence'].fillna('None')","42fb32c7":"train['FireplaceQu'] = train['FireplaceQu'].fillna('None')\ntest['FireplaceQu'] = test['FireplaceQu'].fillna('None')","70bf1edc":"#filling the lot frontage with the median value. A good trick is to use the median value of the neighbourhood rather than\n#lotfrontage, so that it can be realistic in regards to other parameters as well.\ntrain['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#A thing to note is why train and test be treated like the same but kept different. This is because the test data doesn't leak\n#in the training set and your model doesn't 'know' your test data before testing","2822ce7f":"for col in ('GarageType','GarageFinish','GarageQual','GarageCond'):\n    train[col] = train[col].fillna('None')\n    test[col] = test[col].fillna('None')","cadf4461":"for col in ('GarageYrBlt','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath'):\n    train[col] = train[col].fillna(0)\n    test[col] = test[col].fillna(0)","e9a424f1":"train['MasVnrArea'] = train['MasVnrArea'].fillna(0)\ntrain['MasVnrType'] = train['MasVnrType'].fillna('None')","150512c9":"for col in('BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'):\n    train[col] = train[col].fillna('None')\n    test[col] = test[col].fillna('None')","1a7a3a3d":"train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntest['Electrical'] = test['Electrical'].fillna(test['Electrical'].mode()[0])","0902fe4b":"train.shape","f5ab8a88":"test.shape","6eeb24c6":"#Transforming some numerical variables that are really categorical\n\n#MSSubClass = The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\n\n#Changing OverallQual into categorical variable\ntrain['OverallQual'] = train['OverallQual'].astype(str)\ntest['OverallQual'] = test['OverallQual'].astype(str)\n\n#Changing OverallCond into categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\ntest['OverallCond'] = test['OverallCond'].astype(str)\n\n#Changing year sold and month sold into categorical features\ntrain['YrSold'] = train['YrSold'].astype(str)\ntest['YrSold'] = test['YrSold'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)\ntest['MoSold'] = test['MoSold'].astype(str)","6a0e7589":"#Label Encoding some categorical variables so that their information can be used\ncategorical_features = train.select_dtypes(include=[np.object])\ncols = list(categorical_features.columns)\n\n#process all the columns, apply label encoding\nfor c in cols:\n    lbl_train = LabelEncoder()\n    lbl_train.fit(list(train[c].values))\n    train[c] = lbl_train.transform(list(train[c].values))\n    \n    lbl_test = LabelEncoder()\n    lbl_test.fit(list(test[c].values))\n    test[c] = lbl_test.transform(list(test[c].values))    ","eee9aea0":"numeric_feats = train.dtypes[train.dtypes!=\"object\"].index\n\n#check skew of all features\nskewed_feats = train[numeric_feats].apply(lambda x:skew(x.dropna())).sort_values(ascending=False)\nprint(\"Skew Features\")\nskewness = pd.DataFrame({'Skew' : skewed_feats})\nprint(skewness)","7d7d5a20":"skewness = skewness[abs(skewness.Skew) > 0.65]\nprint(\"Features that can be skewed in the dataset are {0}\".format(skewness.shape[0]))\n\nskewed_features = skewness.index\nfor feat in skewed_features:\n    train[feat] = np.log(train[feat]+1)\n    #print(\"Lambda for maxlog for {0} is {1}. \".format(feat,maxlog))","3c6b9661":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train,test)).reset_index(drop=True)\nall_data.drop(['SalePrice'],axis=1,inplace=True)\nprint(all_data.shape)","328c228e":"all_data = pd.get_dummies(all_data)\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","6cf4411c":"train.shape","7a5ea99c":"test.shape","e0fc0907":"train.head()","bd643c02":"#Import libraries\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","c4b887d3":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,train.values,y_train,scoring='neg_mean_squared_error',cv=kf))\n    return(rmse)","9eed9d50":"lasso_reg = make_pipeline(RobustScaler(),Lasso(random_state=42))","a457984f":"enet_reg = make_pipeline(RobustScaler(),ElasticNet(random_state=42))","810d432d":"krr_reg = KernelRidge(kernel=\"polynomial\") #using krr instead of SVR because it's much faster on medium sized datasets","910698d4":"gboost_reg = GradientBoostingRegressor(loss='huber',random_state=42)","81729a3e":"xgb_reg = xgb.XGBRegressor(random_state=42)","16561b8f":"score = rmsle_cv(lasso_reg)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fe069aeb":"score = rmsle_cv(enet_reg)\nprint(\"\\nElastic Net score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","37cf9de3":"score = rmsle_cv(xgb_reg)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fdd5338d":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","085a7f76":"xgb_reg.fit(train,y_train)\nxgb_train_pred = xgb_reg.predict(train)\nxgb_pred = np.expm1(xgb_reg.predict(test))\nprint(rmsle(y_train,xgb_train_pred))","06d9a75c":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = xgb_pred\nsub.to_csv('submission.csv',index=False)","cc191a29":"xgb_pred.shape","4a2e9b89":"1.LASSO Regression","0c12559f":"First of all, just to be informed, SalePrice is our target variable. Let's see it's stats using the describe function","4c2e3b7f":"The SalePrice variable is dealt with","9987f345":"As such there exists skewness in many variables. They can be eliminated as well","766b6676":"### Getting dummy encoding features","b05bc9c3":"#### Plotting Correlations of Sale Price with some of the strong correlating variables","52055b16":"Some noteworthy things:<br>\n->'TotalBsmtSF' and '1stFlrSF' are highly correlated<br>\n->'GarageCars' and 'GarageArea' are highly correlated<br>\n->The Target Variable SalePrice is likely to be correlated to 'OverallQual','GrLivArea','TotalBsmtSF' and 'GarageCars'","4730069f":"It's tempting to delete the 3 values where GrLivArea>3000. But i think that it won't affect the regression function that much. So I'm keeping them as they are. Also one thing to note during outlier deletion is that it's a trade off. The records are being deleted and so are valuable observations for that record. Outlier deletion should only be done if it seems that the recorded observation is truly an outlier and not just an observation with slight more variance","a3361896":"I'm gonna fill up the missing values here. Although it should be noted that variables such as PoolQC and MiscFeature don't add much value here and can be probably removed as well.","8612f7e2":"<b>3.TotalBsmtSF<\/b>","3734bb98":"Lot Frontage","5c39c74f":"There is a positive skewness in the distribution. We already know the solution","92ecc3bb":"#### Creating correlation matrices both general and zoomed style","587954db":"5.XGBoost","6035053b":"#### ZoomedHeatMap","4991eba7":"### Define a cross-validation strategy","da364872":"2.Elastic Net Regression","b0e0964f":"### Base Models","9f6e3c3c":"### Dealing with Outliers","21aecaa4":"### Base model scores","8cd5b17c":"3.Kernel Ridge Regression","2473dbc4":"Now this is a tricky one to deal with. Why? because:<br>\n->Positive Skewness<br>\n->If you observe the distribution carefully, there are many 0 values. so? whats the problem right?<br>\n->No, because Log transformations can't happen on 0 values.<br>\n\n->Solution: As per my research, people usually use log(x+c), but as you can see below, it doesn't exactly solve the problem.","03c8f385":"Electrical has mostly 'SBrkr'. we can use that to fill it up","24bda1b4":"The maximum value of the correlation aside from itself is 0.79, so let's keep the vmax for the sns heatmap be 0.8 to have a more customised heatmap","0ac51c65":"<b>2.GrLivArea<\/b>","b24c0d7d":"Outliers are extreme observations in our data which can affect the accuracy of the trained model.If such observations are too less, it's safe to delete them.","da30ef6a":"4.Gradient Boosting Regression","a92959ed":"It's sensitive to outliers. So RobustScaler() method can be used on the make_pipeline()","c0b49569":"#### Normality","9b26b581":"### Data Transformation","eaa961ce":"## Data Analysis","e7117f98":"FireplaceQu","dad60d53":"### Submission","3454bb3b":"## Modelling","115574cb":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2. Filling them with None","494c5e96":"Some notes:<br>\n-> OverallQual,GrLiveArea,GarageCars, GarageArea and TotalBsmtSF are strongly correlated to SalePrice<br>\n-> GarageCars and GarageArea are so correlated to each other and their correlation matrix shows they can be replaced with each other. GarageCars has more correlation to SalePrice than GarageArea and GarageArea should be dropped<br>\n->TotalBsmtSF and 1stFlrSF can also be considered as twins<br>\n->TotRmsAbvGrd and GrLivArea are twins as well<br>","c2f870ba":"I'm filling all the garage and basement missing variables with 0 as no garage=no cars and also i think missing values for no basement are likely to be 0","bbef7bbb":"For the categorical variables MasVnrArea and MasVnrType. NA means most likely no mason as None is much on MasVnrType.For the area we can fill 0","5d643cdf":"PoolQC","4ca43332":"huber loss makes it robust to outliers","e327b556":"### Dealing with Missing Values","44780297":"I'm going to use cross_val_score of sklearn. This function doesn't have a shuffle attribute, one code of line can be added in order to shuffle the dataset prior to cross-validation","146d9ffd":"For an ideal distribution, skewness and kurtosis both should be zero","5b73bee9":"Some notes:<br>\n->The distribution is not good. Positive skewness and 'peakedness'.The probability plot doesn't follow the normal distribution diagnol<br>\n\n->Solution: When there is positive skewness, log transformations are really helpful","2fb3b0d9":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","68666740":"#### GrLivArea","34ae6c8f":"Let's create a zoomed heat map using the top 12 highly correlated features with SalePrice","dc2bea75":"## Imports","9bf86000":"To reduce the skewness here, I'm going to use the BoxCox transformation. But first of all, a threshold to check whether an observation is skewed or not has to be made. For the same, i'm assuming 0.65 as the threshold as I think it covers all the major variables fully and it would be sufficient for a proper training to the dataset","5d9a8aff":"Alley","3bfdcac4":"Fence","42c2bfbd":"GarageType, GarageFinish, GarageQual and GarageCond","91815401":"Lets plot some subplots which can be used to determine some outliers","53b32144":"#### TotalBsmtSF","3dc835a8":"#### Relationship with Categorical Features","5caaaa95":"We will be attending to:<br>\n<b>Histogram: <\/b> Kurtosis and skewness<br>\n<b>Normal probability plot: <\/b>Data Distribution should closely follow the diagnol that represents the normal distribution","0144565b":"#### Scatterplots between SalePrice and all other variables","b4ee0836":"As you can see, there are some things noteworthy:<br>\n-> There's a deviation from normal distribution<br>\n-> Having positive skewness<br>\n-> Having peakedness<br>","fd6a9ba2":"The limit on y can be set at 800000 as we have seen that the max value on y is 755000","722e3f06":"# This is a kernel for house price:advanced regression techniques dataset","0d74f475":"These are the plots with the highly correlated variables.Some noteworthy points can be:<br>\n->GrLivArea and SalePrice are having a linear relationship<br>\n->The GarageCars is likely to have outliers on its value 3<br>\n->TotalBsmtSF and SalePrice is likely to have an exponential relationship<br>\n->OverallQual and SalePrice are highly related<br>\n->The Price is high as the year passes, which can be seen from the yearbuilt graph<br>","915ccbb7":"Another solution is doing a boxcox transformation. But the boxcox transformation requires the data to be strictly positive (i.e >0). Hence combining the above approach and then doing the boxcox transformation.<br>\n<b>Note:<\/b> I'm not entirely sure about this being the best solution to deal with this problem","98b98b34":"MiscFeature","6ceb5e6b":"<b>1.SalePrice<\/b>","4a6d3601":"#### Relationship with Numerical Variables","4f0688e2":"#### Note:"}}