{"cell_type":{"3c5be630":"code","4f142bbd":"code","d767625d":"code","41ca948d":"code","9c66e522":"code","043656d9":"code","5b2f176b":"code","745e1871":"code","4e103911":"code","9077d70c":"code","8e50a5bb":"code","da24cc38":"code","8bce8a65":"code","3ec80b92":"markdown","f9055f93":"markdown","26ed5fe1":"markdown","52fb0fb2":"markdown","8e386b45":"markdown","17b93d7e":"markdown","9b89b46f":"markdown","20df3836":"markdown","c698a96a":"markdown","4002f441":"markdown","a9851cd5":"markdown","87a4ccf5":"markdown","92798d6f":"markdown"},"source":{"3c5be630":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pickle\nimport numpy as np\nimport progressbar\n\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom keras.models import Sequential\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import SimpleRNN, GRU, LSTM\nfrom keras.layers.core import Dense, Dropout\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers import Convolution1D, MaxPooling1D\nfrom keras import optimizers\nfrom keras.regularizers import l1_l2","4f142bbd":"with open('..\/input\/atis.pkl\/atis.pkl', 'rb') as f:\n    train_set, valid_set, test_set, dicts = pickle.load(f, encoding='latin1')","d767625d":"train_x, train_ne, train_label = train_set\nval_x, val_ne, val_label = valid_set\ntest_x, test_ne, test_label = test_set","41ca948d":"w2idx, ne2idx, labels2idx = dicts['words2idx'], dicts['tables2idx'], dicts['labels2idx']\n\n# Create index to word\/label dicts\nidx2w  = {w2idx[k]:k for k in w2idx}\nidx2ne = {ne2idx[k]:k for k in ne2idx}\nidx2la = {labels2idx[k]:k for k in labels2idx}","9c66e522":"print(train_x[0])\nprint([idx2w[i] for i in train_x[0]])","043656d9":"print([idx2la[i] for i in train_label[0]])","5b2f176b":"words_val = [ list(map(lambda x: idx2w[x], w)) for w in val_x]\ngroundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in val_label]\nwords_train = [ list(map(lambda x: idx2w[x], w)) for w in train_x]\ngroundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in train_label]","745e1871":"n_classes = len(idx2la)\nn_vocab = len(idx2w)\nn_examples = len(words_train)\nprint('#labels: ', n_classes, '\\t#distinct words: ', n_vocab, '\\t#examples: ', n_examples)","4e103911":"model = Sequential()\nmodel.add(Embedding(n_vocab,100))\n\n## Essas duas camadas s\u00e3o opcionais. Daremos mais detalhes nos pr\u00f3ximos laborat\u00f3rios\n#model.add(Convolution1D(64, 5, border_mode='same', activation='relu'))\n#model.add(Dropout(0.1))\n\nmodel.add(GRU(100, return_sequences=True, \n              kernel_regularizer=l1_l2(l1=0.0, l2=0.0), \n              recurrent_regularizer=l1_l2(l1=0.0, l2=0.0)\n             ))\n\nmodel.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n#optm = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n#optm = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\noptm = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(loss='categorical_crossentropy', optimizer=optm)","9077d70c":"ml = MultiLabelBinarizer(classes=list(idx2la.values())).fit(idx2la.values())\n\ndef conlleval( trues, preds ):\n    trues = ml.transform(trues)\n    preds = ml.transform(preds)\n    return score(trues, preds, beta=1, average='macro' )\n","8e50a5bb":"# Defina aqui o n\u00famero de \u00e9pocas\nn_epochs = 12\n\ntrain_f_scores = []\nval_f_scores = []\nbest_val_f1 = 0\ncon_dict = {}\n\nfor i in range(n_epochs):\n    print(\"\\nEpoch {}\".format(i))\n    \n    print(\"Training =>\")\n    train_pred_label = []\n    avgLoss = 0\n    \n    bar = progressbar.ProgressBar(max_value=len(train_x))\n    for n_batch, sent in bar(enumerate(train_x)):\n        label = train_label[n_batch]\n        label = np.eye(n_classes)[label][np.newaxis,:]\n        sent = sent[np.newaxis,:]\n        \n        if sent.shape[1] > 1: #some bug in keras\n            loss = model.train_on_batch(sent, label)\n            avgLoss += loss\n\n        pred = model.predict_on_batch(sent)\n        pred = np.argmax(pred,-1)[0]\n        train_pred_label.append(pred)\n\n    avgLoss = avgLoss\/n_batch\n    \n    predword_train = [ list(map(lambda x: idx2la[x], y)) for y in train_pred_label]\n    con_dict['p'], con_dict['r'], con_dict['f1'], _ = conlleval(groundtruth_train, predword_train)\n    train_f_scores.append(con_dict['f1'])\n    \n    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['p'], con_dict['r'], con_dict['f1']))\n    \n    print(\"\\n\\nValidating =>\")\n    \n    val_pred_label = []\n    avgLoss = 0\n    \n    bar = progressbar.ProgressBar(max_value=len(val_x))\n    for n_batch, sent in bar(enumerate(val_x)):\n        label = val_label[n_batch]\n        label = np.eye(n_classes)[label][np.newaxis,:]\n        sent = sent[np.newaxis,:]\n        \n        if sent.shape[1] > 1: #some bug in keras\n            loss = model.test_on_batch(sent, label)\n            avgLoss += loss\n\n        pred = model.predict_on_batch(sent)\n        pred = np.argmax(pred,-1)[0]\n        val_pred_label.append(pred)\n\n    avgLoss = avgLoss\/n_batch\n    \n    predword_val = [ list(map(lambda x: idx2la[x], y)) for y in val_pred_label]\n    con_dict['p'], con_dict['r'], con_dict['f1'], _ = conlleval(groundtruth_val, predword_val)\n    val_f_scores.append(con_dict['f1'])\n    \n    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['p'], con_dict['r'], con_dict['f1']))\n\n    if con_dict['f1'] > best_val_f1:\n        best_val_f1 = con_dict['f1']\n        open('model_architecture.json','w').write(model.to_json())\n        model.save_weights('best_model_weights.h5',overwrite=True)\n        print(\"Best validation F1 score = {}\".format(best_val_f1))\n\n","da24cc38":"print(\"Best epoch to stop = {}\".format(val_f_scores.index(max(val_f_scores))))\nprint(\"Best validation F1 score = {}\".format(best_val_f1))","8bce8a65":"idx = np.random.randint(len(words_val))\npreds = [idx2la[i[0]] for i in np.argmax(model.predict(val_x[idx]), -1)]\nfor k in zip(words_val[idx],groundtruth_val[idx], preds):\n    print('Word: %s\\t\\tLabel: %s\\t\\tPred: %s' % k)","3ec80b92":"Obs: Esse laborat\u00f3rio foi baseado [nesse artigo](https:\/\/chsasank.github.io\/spoken-language-understanding.html)\n\nO problema que vamos lidar aqui \u00e9 uma das tarefas t\u00edpicas da [compreens\u00e3o de linguagem natural](https:\/\/en.wikipedia.org\/wiki\/Natural_language_understanding). Nosso objetivo \u00e9 identificar elementos principais em uma frase. A tarefa ser\u00e1, dada uma pergunta feita em um sistema sobre v\u00f4os, identificar elementos como cidade de origem e destino, datas, hor\u00e1rios, etc.\n\nO conjunto de dados utilizado \u00e9 o Airline Travel Information System (ATIS). Esses dados foram coletados na d\u00e9cada de 90 pelo Departamento de defesa americano (DARPA). ATIS consiste em pares de perguntas e de labels para cada palavra da pergunta. \n\nAqui est\u00e1 um exemplo de pergunta e seus r\u00f3tulos, que est\u00e3o codificados usando a nota\u00e7\u00e3o [Inside Outside Beginning (IOB)](https:\/\/en.wikipedia.org\/wiki\/Inside_Outside_Beginning):\n","f9055f93":"No c\u00f3digo abaixo treinamos a nossa rede recorrente. O n\u00famero de \u00e9pocas pode ser ajustado. A cada \u00e9poca \u00e9 consolidado as m\u00e9tricas `precision`, `recall` e `f1-score`. Ver detalhes de como \u00e9 feito o c\u00e1lculo [aqui](https:\/\/en.wikipedia.org\/wiki\/F1_score). Obtendo 1 nestes scores temos um classificador perfeito.","26ed5fe1":"### Criando a RNN e realizando o treinamento e valida\u00e7\u00e3o","52fb0fb2":"Aqui escolhemos um exemplo aleat\u00f3rio dos dados de valida\u00e7\u00e3o para comparar os labels reais x previstos","8e386b45":"Veja os labels do exemplo 0.","17b93d7e":"Aqui est\u00e1 nosso c\u00f3digo que vai criar nosso modelo de uma rede neuronal recorrente. Usaremos a classe GRU do keras, que \u00e9 um tipo especial de rede recorrente.","9b89b46f":"A vari\u00e1vel dicts guarda os dicion\u00e1rios de palavras para \u00edndices. Aqui criamos os \u00edndices inversos (n\u00fameros para palavras).","20df3836":"Primeiro importamos as bibliotecas que vamos usar e os dados de um arquivo no formato [pickle](https:\/\/docs.python.org\/3\/library\/pickle.html).","c698a96a":"Aqui transformamos dados de treino e valida\u00e7\u00e3o que guardam os \u00edndices, em listas com as palavras.","4002f441":"Note que estamos lidando com um conjunto de dados muito pequeno. S\u00e3o apenas 4978 exemplos para treinamento e 572 palavras distintas nas perguntas.","a9851cd5":"Cada exemplo \u00e9 um vetor com os \u00edndices das palavras no dicion\u00e1rio. Para obter as palavras usamos o \u00edndice inverso, como ilustrado abaixo com o exemplo 0.","87a4ccf5":"# Laborat\u00f3rio 5 \u2013 Aplica\u00e7\u00e3o de RNN para Compreens\u00e3o de Linguagem Natural","92798d6f":"### Carregando e analisando os dados"}}