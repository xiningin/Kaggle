{"cell_type":{"7bae132c":"code","3df43484":"code","de013dfb":"code","bbc77b75":"code","8e5119fc":"code","f6750fb8":"code","f4b39248":"code","3b36483b":"code","4c962ca9":"code","871bf9f8":"code","080c7ca9":"code","cc5d0db9":"code","d3e69b39":"code","957303a2":"code","a7670736":"code","f0f9f1f7":"code","6070ebca":"code","406d527b":"code","95da5411":"code","2bde1f34":"code","df78819d":"code","7a381558":"code","970f657a":"code","f317aea2":"code","0d32da70":"code","140bb345":"code","278f0e65":"code","497bdeab":"code","a35223b0":"code","03585df2":"code","69b3a4c3":"code","e1fcbf7a":"code","caf9745b":"code","4337332d":"code","9453e742":"code","00bcec43":"code","58cc8bed":"code","c636fab1":"code","f0b6f4f6":"code","1a788949":"code","5b823c49":"code","c6ce4218":"code","43418bf2":"code","b521bbbf":"code","21968c93":"code","be7c6bf2":"code","95a27285":"code","92074c8b":"code","86cfe0f9":"code","b3133aa5":"code","7b965b29":"code","f5378038":"code","84d9820b":"code","cca5c25e":"code","e053abfc":"code","e5a5e090":"code","3daba40c":"code","1cc08efa":"code","b0f13803":"code","6d36859f":"code","14f4fde2":"code","9092d15d":"code","66c56ed4":"code","f74bafe6":"code","2c2dc6aa":"code","97273fdd":"code","6e7fecb2":"code","650ba546":"code","b088d5ab":"code","4cc5c78d":"code","977c379e":"code","cee7107e":"code","e8ebf883":"code","64882347":"code","9399002a":"code","219375de":"code","5193ad55":"code","10389059":"code","c9cd9bac":"code","f219abf1":"markdown","4c87e9c6":"markdown","ae22aedc":"markdown","9a1a0b59":"markdown","12f10c71":"markdown","e7889719":"markdown","94a4fd40":"markdown","60dea2c7":"markdown","e29972cd":"markdown","ffaf81fb":"markdown","cb750356":"markdown","dca96468":"markdown","db8be0e8":"markdown","64795d3f":"markdown","b14039fe":"markdown","62a81900":"markdown","54ed67f7":"markdown","0844edbf":"markdown","8bc79e31":"markdown","f9cb9f65":"markdown","3b0b6f34":"markdown","296338b7":"markdown","ed0b797c":"markdown","e4c8ece3":"markdown","b15ab2d4":"markdown","eef32956":"markdown","d4a3b2b7":"markdown","08b9d0e0":"markdown","9854f52d":"markdown","c0755d76":"markdown","0ba99647":"markdown","86206fd8":"markdown","b41c47b5":"markdown","0433459b":"markdown","060b0b03":"markdown","c9e75dcf":"markdown","4e4ea846":"markdown","fe1c853a":"markdown","852cf5f0":"markdown","b42776c4":"markdown","6419b060":"markdown","e6b3c6cd":"markdown","58fab0d2":"markdown","d7d0c4bc":"markdown","3158a739":"markdown","73b5b30b":"markdown","e179d75e":"markdown","a4d87d8b":"markdown","71a91610":"markdown","cb2d6f35":"markdown","25a08a98":"markdown","b1c5b4a1":"markdown"},"source":{"7bae132c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import (accuracy_score, f1_score, roc_curve, auc, precision_recall_curve, \n                             classification_report, confusion_matrix, roc_auc_score)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport optuna\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_columns', None)","3df43484":"df = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv', engine='python', sep='\\t')\ncopy = df.copy()\ndf.head()","de013dfb":"df.info()","bbc77b75":"# First, make the 'age' variable, by subtracting birth year from 2021\ndf['Age'] = (2021 - df['Year_Birth']).astype(np.int64)\ndf.drop('Year_Birth', axis=1, inplace=True)","8e5119fc":"print(df['Z_CostContact'].unique())\nprint(df['Z_Revenue'].unique())","f6750fb8":"df.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)","f4b39248":"inc_med = df['Income'].median()\ndf['Income'].fillna(inc_med, inplace=True)","3b36483b":"# Check Kid Home and Teen Home\ndf[['Kidhome', 'Teenhome']].hist()","4c962ca9":"df[df['Teenhome'] == 2].head()","871bf9f8":"df['Childhome'] = df['Kidhome'] + df['Teenhome']\ndf.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\ndf.head()","080c7ca9":"# check the maximum of the enrollment date\ndf['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'])\nprint(type(df['Dt_Customer'].iloc[0]))\nprint(df['Dt_Customer'].max())\ndf.head()","cc5d0db9":"# Let's count the days and years since enrollment till 2014-12-31\ndates = pd.to_datetime('2014-12-31') - df['Dt_Customer']\nyears = 2014 - df['Dt_Customer'].dt.year\n\ncorrdf = pd.DataFrame()\ncorrdf['Dates'] = dates.dt.days\ncorrdf['Years'] = years\ncorrdf['Response'] = df['Response']\n\npd.plotting.scatter_matrix(corrdf, figsize=(16, 12))","d3e69b39":"df['Enrolldates'] = dates.dt.days\n\n# add the month when each customer has enrolled\ndf['Enrollmonth'] = df['Dt_Customer'].dt.month\ndf.drop('Dt_Customer', axis=1, inplace=True)\ndf.head()","957303a2":"df[['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Response']].corr()","a7670736":"corr_test = df[['NumWebPurchases', 'NumCatalogPurchases', 'Response']].copy()\ncorr_test['NumWebcat'] = corr_test['NumWebPurchases'] + corr_test['NumCatalogPurchases']\ncorr_test['MulWebcat'] = corr_test['NumWebPurchases'] * corr_test['NumCatalogPurchases']\n\ncorr_test.corr()","f0f9f1f7":"df['MulWebcat'] = df['NumWebPurchases'] * df['NumCatalogPurchases']\ndf.drop(['NumWebPurchases', 'NumCatalogPurchases'], axis=1, inplace=True)\ndf.head()","6070ebca":"df['Income'].describe()","406d527b":"inc_mean = df['Income'].mean()\ninc_std = df['Income'].std()\nol_candidates = df[(df['Income'] < inc_mean - (3*inc_std)) | (df['Income'] > inc_mean + (3*inc_std))]\nol_candidates","95da5411":"df = df[df['Income'] < df['Income'].max()]\ndf.describe()\n\ndf['Income'] = np.log(df['Income'])","2bde1f34":"# labeling the categorical features\ncat_features = ['Education', 'Marital_Status']\nfor cat in cat_features:\n    le = LabelEncoder()\n    df[cat] = le.fit_transform(df[cat])\n\n# aggregate the total number of promotion acceptance\ndf['Totalacp'] = df['AcceptedCmp1'] + df['AcceptedCmp2'] + df['AcceptedCmp3'] + df['AcceptedCmp4'] + df['AcceptedCmp5']\ndf.drop(['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5'], axis=1, inplace=True)\n\ndf.head()","df78819d":"cat_feat = ['Education', 'Marital_Status', 'Complain', 'Enrollmonth']","7a381558":"cust_id = df['ID'].copy()\ndf.drop('ID', axis=1, inplace=True)","970f657a":"def pr_auc(y_true, y_score):\n    pr, re, th = precision_recall_curve(y_true, y_score)\n    return auc(re, pr)\n\ndef f1_manual(y_true, y_score):\n    pr, re, th = precision_recall_curve(y_true, y_score)\n    return (2 * pr * re) \/ (pr + re)","f317aea2":"X = df.drop('Response', axis=1)\ny = df['Response']\n\nfor col in X.columns:\n    if col != 'Income':\n        X[col] = X[col].map(lambda x: int(x))\n\nexp1 = X.copy()\ny = y.map(lambda x: int(x))","0d32da70":"X_train, X_test, y_train, y_test = train_test_split(exp1, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(X_train.shape, X_test.shape)","140bb345":"print(y_train.value_counts())","278f0e65":"rf = RandomForestClassifier(random_state=42)\nada = AdaBoostClassifier(random_state=42)\ngbc = GradientBoostingClassifier(random_state=42)\nlgb = LGBMClassifier(random_state=42)\ncat = CatBoostClassifier(random_state=42, silent=True, grow_policy='Lossguide')\nxgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n\nmodels = [rf, lgb, cat, xgb, ada, gbc,]","497bdeab":"for model in models:\n    name = model.__class__.__name__\n    rocauc = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n    \n    print('Name: %s, ROC AUC: %.4f' % (name, np.mean(rocauc)))\n    print('========================================')","a35223b0":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(X_tr.shape, X_val.shape)","03585df2":"#LGBM\ndef objective(trial):\n    param = {\n      'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 3, 20),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 30),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = LGBMClassifier(**param, n_jobs=-1, random_state=42)\n    model.fit(X_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nlgb_best = study.best_trial\nlgb_best_params = lgb_best.params\nprint('score: {0}, params: {1}'.format(lgb_best.value, lgb_best_params))","69b3a4c3":"# Catboost\ndef objective(trial):\n    param = {\n      'max_leaves': trial.suggest_int('max_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = CatBoostClassifier(**param, grow_policy='Lossguide', random_state=42, silent=True)\n    model.fit(X_tr, y_tr, verbose=False, cat_features=cat_feat)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","e1fcbf7a":"# XGB\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),      \n      'gamma': trial.suggest_uniform('gamma', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = XGBClassifier(**param, use_label_encoder=False, n_jobs=-1, eval_metric='logloss', random_state=42)\n    model.fit(X_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nxgb_best = study.best_trial\nxgb_best_params = xgb_best.params\nprint('score: {0}, params: {1}'.format(xgb_best.value, xgb_best_params))","caf9745b":"# GBC\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_samples_split': trial.suggest_uniform('min_samples_split', 0, 1),      \n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 11),\n      'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0, 0.5)\n    }\n\n    model = GradientBoostingClassifier(**param, random_state=42)\n    model.fit(X_tr, y_tr)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ngbc_best = study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","4337332d":"# Test models\nlgb_final = LGBMClassifier(**lgb_best_params, random_state=42)\ncat_final = CatBoostClassifier(**cat_best_params, random_state=42, silent=True, grow_policy='Lossguide')\nxgb_final = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\ngbc_final = GradientBoostingClassifier(**gbc_best_params, random_state=42)\n\nfinals = [lgb_final, cat_final, xgb_final, gbc_final]","9453e742":"result1 = pd.DataFrame(columns=['name', 'ROC AUC1'])\n\nfor final in finals:\n    name = final.__class__.__name__\n    if name != 'CatBoostClassifier':\n        final.fit(X_train, y_train)\n    else:\n        final.fit(X_train, y_train, cat_features=cat_feat)\n    \n    y_pred_proba = final.predict_proba(X_test)[:, 1]\n    rocauc = roc_auc_score(y_test, y_pred_proba)\n    \n    result1 = result1.append({'name': name, 'ROC AUC1': rocauc}, ignore_index=True)\n    \n    print('Model: %s, ROC AUC: %.4f' % (name, rocauc))\n    print('========================================================')\n    \nresult1","00bcec43":"X.info()","58cc8bed":"# split\nexp2 = X[cat_feat].copy()\n\nfor col in X.columns:\n    if col not in cat_feat:\n        med = X[col].median()\n        exp2[col] = (X[col] >= med).astype(np.int64)\n\nexp2.head()","c636fab1":"exp2_train, exp2_test, y_train, y_test = train_test_split(exp2, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(exp2_train.shape, exp2_test.shape)","f0b6f4f6":"print(y_train.value_counts())","1a788949":"exp2_tr, exp2_val, y_tr, y_val = train_test_split(exp2_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(exp2_tr.shape, exp2_val.shape)","5b823c49":"#LGBM\ndef objective(trial):\n    param = {\n      'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 3, 20),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 30),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = LGBMClassifier(**param, n_jobs=-1, random_state=42)\n    model.fit(exp2_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp2_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nlgb_best = study.best_trial\nlgb_best_params = lgb_best.params\nprint('score: {0}, params: {1}'.format(lgb_best.value, lgb_best_params))","c6ce4218":"# Catboost\ndef objective(trial):\n    param = {\n      'max_leaves': trial.suggest_int('max_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = CatBoostClassifier(**param, grow_policy='Lossguide', random_state=42, silent=True)\n    model.fit(exp2_tr, y_tr, verbose=False, cat_features=exp2_tr.columns.values)\n    y_pred_proba = model.predict_proba(exp2_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","43418bf2":"# XGB\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),      \n      'gamma': trial.suggest_uniform('gamma', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0, 1)\n    }\n\n    model = XGBClassifier(**param, use_label_encoder=False, n_jobs=-1, eval_metric='logloss', random_state=42)\n    model.fit(exp2_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp2_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nxgb_best = study.best_trial\nxgb_best_params = xgb_best.params\nprint('score: {0}, params: {1}'.format(xgb_best.value, xgb_best_params))","b521bbbf":"# GBC\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_samples_split': trial.suggest_uniform('min_samples_split', 0, 1),      \n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 11),\n      'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0, 0.5)\n    }\n\n    model = GradientBoostingClassifier(**param, random_state=42)\n    model.fit(exp2_tr, y_tr)\n    y_pred_proba = model.predict_proba(exp2_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ngbc_best = study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","21968c93":"# Test models\nlgb_final = LGBMClassifier(**lgb_best_params, random_state=42)\ncat_final = CatBoostClassifier(**cat_best_params, random_state=42, silent=True, grow_policy='Lossguide')\nxgb_final = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\ngbc_final = GradientBoostingClassifier(**gbc_best_params, random_state=42)\n\nfinals = [lgb_final, cat_final, xgb_final, gbc_final]","be7c6bf2":"exp2_train.columns.values","95a27285":"result2 = pd.DataFrame(columns=['name', 'ROC AUC2'])\n\nfor final in finals:\n    name = final.__class__.__name__\n    if name != 'CatBoostClassifier':\n        final.fit(exp2_train, y_train)\n    else:\n        final.fit(exp2_train, y_train, cat_features=exp2_train.columns.values)\n    \n    y_pred_proba = final.predict_proba(exp2_test)[:, 1]\n    rocauc = roc_auc_score(y_test, y_pred_proba)\n    \n    result2 = result2.append({'name': name, 'ROC AUC2': rocauc}, ignore_index=True)\n    \n    print('Model: %s, ROC AUC: %.4f' % (name, rocauc))\n    print('========================================================')\n    \nresult2","92074c8b":"exp3 = X[cat_feat].copy()\n\ndef quartile(series):\n    q1 = series.quantile(q=0.25)\n    q2 = series.median()\n    q3 = series.quantile(q=0.75)\n    \n    new_series = pd.Series(index=series.index)\n    zeros = series[series < q1].index\n    ones = series[(series >= q1) & (series < q2)].index\n    twos = series[(series >= q2) & (series < q3)].index\n    tres = series[series >= q3].index\n    \n    new_series.loc[zeros] = 0\n    new_series.loc[ones] = 1\n    new_series.loc[twos] = 2\n    new_series.loc[tres] = 3\n    \n    return new_series.astype(np.int64)\n\nfor col in X.columns:\n    if col not in cat_feat:\n        exp3[col] = quartile(X[col])\n\nexp3.head()","86cfe0f9":"exp3_train, exp3_test, y_train, y_test = train_test_split(exp3, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(exp3_train.shape, exp3_test.shape)","b3133aa5":"exp3_tr, exp3_val, y_tr, y_val = train_test_split(exp3_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(exp3_tr.shape, exp3_val.shape)","7b965b29":"#LGBM\ndef objective(trial):\n    param = {\n      'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 3, 20),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 30),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = LGBMClassifier(**param, n_jobs=-1, random_state=42)\n    model.fit(exp3_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp3_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nlgb_best = study.best_trial\nlgb_best_params = lgb_best.params\nprint('score: {0}, params: {1}'.format(lgb_best.value, lgb_best_params))","f5378038":"# Catboost\ndef objective(trial):\n    param = {\n        'max_leaves': trial.suggest_int('max_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = CatBoostClassifier(**param, grow_policy='Lossguide', random_state=42, silent=True)\n    model.fit(exp3_tr, y_tr, verbose=False, cat_features=exp3_train.columns.values)\n    y_pred_proba = model.predict_proba(exp3_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","84d9820b":"# XGB\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),      \n      'gamma': trial.suggest_uniform('gamma', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = XGBClassifier(**param, use_label_encoder=False, n_jobs=-1, eval_metric='logloss', random_state=42)\n    model.fit(exp3_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp3_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nxgb_best = study.best_trial\nxgb_best_params = xgb_best.params\nprint('score: {0}, params: {1}'.format(xgb_best.value, xgb_best_params))","cca5c25e":"# GBC\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_samples_split': trial.suggest_uniform('min_samples_split', 0, 1),      \n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 11),\n      'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0, 0.5)\n    }\n\n    model = GradientBoostingClassifier(**param, random_state=42)\n    model.fit(exp3_tr, y_tr)\n    y_pred_proba = model.predict_proba(exp3_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ngbc_best = study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","e053abfc":"# Test models\nlgb_final = LGBMClassifier(**lgb_best_params, random_state=42)\ncat_final = CatBoostClassifier(**cat_best_params, random_state=42, silent=True, grow_policy='Lossguide')\nxgb_final = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\ngbc_final = GradientBoostingClassifier(**gbc_best_params, random_state=42)\n\nfinals = [lgb_final, cat_final, xgb_final, gbc_final]","e5a5e090":"result3 = pd.DataFrame(columns=['name', 'ROC AUC3'])\n\nfor final in finals:\n    name = final.__class__.__name__\n    if name != 'CatBoostClassifier':\n        final.fit(exp3_train, y_train)\n    else:\n        final.fit(exp3_train, y_train, cat_features=exp3_train.columns.values)\n    \n    y_pred_proba = final.predict_proba(exp3_test)[:, 1]\n    rocauc = roc_auc_score(y_test, y_pred_proba)\n    \n    result3 = result3.append({'name': name, 'ROC AUC3': rocauc}, ignore_index=True)\n    \n    print('Model: %s, ROC AUC: %.4f' % (name, rocauc))\n    print('========================================================')\n    \nresult3","3daba40c":"X.describe()","1cc08efa":"exp4 = X[cat_feat].copy()\nexp4_cats = cat_feat\n\nfor col in X.columns:\n    if (col not in cat_feat) and (X[col].std() >= X[col].mean()):\n        exp4[col] = quartile(X[col])\n        exp4_cats.append(col)\n    else:\n        exp4[col] = X[col]\n\nexp4.head()","b0f13803":"print(exp4_cats)","6d36859f":"exp4_train, exp4_test, y_train, y_test = train_test_split(exp4, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(exp4_train.shape, exp4_test.shape)","14f4fde2":"exp4_tr, exp4_val, y_tr, y_val = train_test_split(exp4_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(exp4_tr.shape, exp4_val.shape)","9092d15d":"#LGBM\ndef objective(trial):\n    param = {\n      'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 3, 20),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 30),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = LGBMClassifier(**param, n_jobs=-1, random_state=42)\n    model.fit(exp4_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp4_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nlgb_best = study.best_trial\nlgb_best_params = lgb_best.params\nprint('score: {0}, params: {1}'.format(lgb_best.value, lgb_best_params))","66c56ed4":"# Catboost\ndef objective(trial):\n    param = {\n      'max_leaves': trial.suggest_int('max_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = CatBoostClassifier(**param, grow_policy='Lossguide', random_state=42, silent=True)\n    model.fit(exp4_tr, y_tr, verbose=False, cat_features=exp4_cats)\n    y_pred_proba = model.predict_proba(exp4_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","f74bafe6":"# XGB\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),      \n      'gamma': trial.suggest_uniform('gamma', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = XGBClassifier(**param, use_label_encoder=False, n_jobs=-1, eval_metric='logloss', random_state=42)\n    model.fit(exp4_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp4_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nxgb_best = study.best_trial\nxgb_best_params = xgb_best.params\nprint('score: {0}, params: {1}'.format(xgb_best.value, xgb_best_params))","2c2dc6aa":"# GBC\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_samples_split': trial.suggest_uniform('min_samples_split', 0, 1),      \n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 11),\n      'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0, 0.5)\n    }\n\n    model = GradientBoostingClassifier(**param, random_state=42)\n    model.fit(exp4_tr, y_tr)\n    y_pred_proba = model.predict_proba(exp4_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ngbc_best = study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","97273fdd":"# Test models\nlgb_final = LGBMClassifier(**lgb_best_params, random_state=42)\ncat_final = CatBoostClassifier(**cat_best_params, random_state=42, silent=True, grow_policy='Lossguide')\nxgb_final = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\ngbc_final = GradientBoostingClassifier(**gbc_best_params, random_state=42)\n\nfinals = [lgb_final, cat_final, xgb_final, gbc_final]","6e7fecb2":"result4 = pd.DataFrame(columns=['name', 'ROC AUC4'])\n\nfor final in finals:\n    name = final.__class__.__name__\n    if name != 'CatBoostClassifier':\n        final.fit(exp4_train, y_train)\n    else:\n        final.fit(exp4_train, y_train, cat_features=exp4_cats)\n    \n    y_pred_proba = final.predict_proba(exp4_test)[:, 1]\n    rocauc = roc_auc_score(y_test, y_pred_proba)\n    \n    result4 = result4.append({'name': name, 'ROC AUC4': rocauc}, ignore_index=True)\n    \n    print('Model: %s, ROC AUC: %.4f' % (name, rocauc))\n    print('========================================================')\n    \nresult4","650ba546":"exp5 = X[cat_feat].copy()\nexp5_cats = cat_feat\n\nfor col in X.columns:\n    if (col not in cat_feat) and (X[col].std() < X[col].mean()):\n        exp5[col] = quartile(X[col])\n        exp5_cats.append(col)\n    else:\n        exp5[col] = X[col]\n\nexp5.head()","b088d5ab":"print(exp5_cats)","4cc5c78d":"exp5_train, exp5_test, y_train, y_test = train_test_split(exp5, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(exp5_train.shape, exp5_test.shape)","977c379e":"exp5_tr, exp5_val, y_tr, y_val = train_test_split(exp5_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint(exp5_tr.shape, exp5_val.shape)","cee7107e":"#LGBM\ndef objective(trial):\n    param = {\n      'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 3, 20),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_samples': trial.suggest_int('min_child_samples', 2, 30),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = LGBMClassifier(**param, n_jobs=-1, random_state=42)\n    model.fit(exp5_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp5_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nlgb_best = study.best_trial\nlgb_best_params = lgb_best.params\nprint('score: {0}, params: {1}'.format(lgb_best.value, lgb_best_params))","e8ebf883":"# Catboost\ndef objective(trial):\n    param = {\n      'max_leaves': trial.suggest_int('max_leaves', 20, 150),\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = CatBoostClassifier(**param, grow_policy='Lossguide', random_state=42, silent=True)\n    model.fit(exp5_tr, y_tr, verbose=False, cat_features=exp5_cats)\n    y_pred_proba = model.predict_proba(exp5_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","64882347":"# XGB\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 1),      \n      'gamma': trial.suggest_uniform('gamma', 0, 1),\n      'subsample': trial.suggest_uniform('subsample', 0.1, 1)\n    }\n\n    model = XGBClassifier(**param, use_label_encoder=False, n_jobs=-1, eval_metric='logloss', random_state=42)\n    model.fit(exp5_tr, y_tr, verbose=False)\n    y_pred_proba = model.predict_proba(exp5_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\nxgb_best = study.best_trial\nxgb_best_params = xgb_best.params\nprint('score: {0}, params: {1}'.format(xgb_best.value, xgb_best_params))","9399002a":"# GBC\ndef objective(trial):\n    param = {\n      'max_depth': trial.suggest_int('max_depth', 5, 30),\n      'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n      'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n      'min_samples_split': trial.suggest_uniform('min_samples_split', 0, 1),      \n      'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 11),\n      'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0, 0.5)\n    }\n\n    model = GradientBoostingClassifier(**param, random_state=42)\n    model.fit(exp5_tr, y_tr)\n    y_pred_proba = model.predict_proba(exp5_val)[:, 1]\n    rocauc = roc_auc_score(y_val, y_pred_proba)\n\n    return rocauc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=100)\n\ngbc_best = study.best_trial\ngbc_best_params = gbc_best.params\nprint('score: {0}, params: {1}'.format(gbc_best.value, gbc_best_params))","219375de":"# Test models\nlgb_final = LGBMClassifier(**lgb_best_params, random_state=42)\ncat_final = CatBoostClassifier(**cat_best_params, random_state=42, silent=True, grow_policy='Lossguide')\nxgb_final = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\ngbc_final = GradientBoostingClassifier(**gbc_best_params, random_state=42)\n\nfinals = [lgb_final, cat_final, xgb_final, gbc_final]","5193ad55":"result5 = pd.DataFrame(columns=['name', 'ROC AUC5'])\n\nfor final in finals:\n    name = final.__class__.__name__\n    if name != 'CatBoostClassifier':\n        final.fit(exp5_train, y_train)\n    else:\n        final.fit(exp5_train, y_train, cat_features=exp5_cats)\n    \n    y_pred_proba = final.predict_proba(exp5_test)[:, 1]\n    rocauc = roc_auc_score(y_test, y_pred_proba)\n    \n    result5 = result5.append({'name': name, 'ROC AUC5': rocauc}, ignore_index=True)\n    \n    print('Model: %s, ROC AUC: %.4f' % (name, rocauc))\n    print('========================================================')\n    \nresult5","10389059":"print(len(exp4_cats))\nprint(len(exp5_cats))","c9cd9bac":"m1 = pd.merge(result1, result2, on='name', how='inner')\nm2 = pd.merge(m1, result3, on='name', how='inner')\nm3 = pd.merge(m2, result4, on='name', how='inner')\nmerged = pd.merge(m3, result5, on='name', how='inner')\n\nmerged","f219abf1":"## Train - Validation Split","4c87e9c6":"Well, whatever these variables mean, since there is no variation within each variable, they would not be helpful for prediction. So I will drop them.","ae22aedc":"# Data and Library Import","9a1a0b59":"Since the Random Forest and AdaBoost got lowest scores, I'm not going to use them anymore. For the others, since boosting algorithms can show much better performance through optimization, I'll try to optimize them. For optimization, I'm going to use the **Optuna**.","12f10c71":"## Train - Validation Split","e7889719":"# Continuous vs Categorical: Which type is better for classification?\n\nWhile studying machine learning, I've wondered if it is better to use **continuous** explanatory variables or to use **categorical** explanatory variables for better classification, especially when using the tree-based algorithms: Random Forest, Boosting algorithms and so on.\n\nTherefore, I've decided to conduct an experiment and compare which type of explanatory variables yields the better classification result - in terms of typical classification metrics such as accuracy, ROC-AUC or F1 score.\n\nI've already heard that aggregating continuous values into fewer categories could make a model perform worse, since aggregation reduces the variability of a variable. However, it seems to me that it would be much easier for tree-based algorithms to find a better or more clear criterion to split nodes. So I want to see the evidence with my own two eyes. Of course, the results might vary depending on the properties of each dataset, but I believe that this experiment would help me to take a glance!\n","94a4fd40":"Now, let's check how long the customer has enrolled. There may be two options: First, we can count the years they have enrolled, or second we can count the days of enrollment.","60dea2c7":"# Evaluation Metrics","e29972cd":"## Optimization","ffaf81fb":"## Optimization","cb750356":"## Train - Test Split","dca96468":"# III. With Multi-level Categories - 4 levels","db8be0e8":"## Train - Test Split","64795d3f":"## Optimization","b14039fe":"So, the Kidhome variable denotes the number of non-teen children of each household. Thus, let's aggregate them into the total number.","62a81900":"Oops, there are variables, Z_CostContact and Z_Revenue, not shown on the variable description. Let's take a glance at what these refer to.","54ed67f7":"# EDA and Feature Engineering","0844edbf":"## Optimization","8bc79e31":"## Optimization","f9cb9f65":"## Feature - Target Split","3b0b6f34":"Obviously, the 'days' variable has more variation than the years, thus let's use the days.","296338b7":"## Model Evaluation","ed0b797c":"## Train - Validation Split","e4c8ece3":"## Model Selection","b15ab2d4":"As we can see, the correlation of the (store purchases - response) is much less than that between the (web purchases - response), and that between (the catalog purchases - response). Thus, let's aggregate the number of web purchases and the number of catalog purchases.","eef32956":"## Train - Test Split","d4a3b2b7":"# I. With Numerical Features as itself","08b9d0e0":"## Train - Test Split","9854f52d":"## Model Evaluation","c0755d76":"Below part is the same as above, so I'm gonna hide the codes. Let's just skip to the result part.","0ba99647":"## Model Evaluation","86206fd8":"Let's move onto the income distribution now.","b41c47b5":"Now, let's move onto the purchase approaches. Customers who make purchases via Internet or catalogue may be more sensitive to promotion, especially to price promotion. Therefore, aggregate the number of purchases via web and catalog.","0433459b":"Let's convert numerical variables with small standard deviation into 4-level categorical variables here.","060b0b03":"Also, there seems to be 16 missing observations for the 'income' variable. There are two possible ways to treat missing variables: drop or fill. Since this is a small dataset with around 2000 data points, I will not drop those missing variables. Instead, I'm going to fill them with the median.","c9e75dcf":"## Model Evaluation","4e4ea846":"## Model Evaluation","fe1c853a":"# V. Converting only variables with small Standard deviation","852cf5f0":"We're done with EDA and data preprocessing! Let's dive into the main event now. Here, we have 4 categorical features: Education, Marital_Status, Complain, and Month. This implies that there are 19 numerical features. So, I'm going to train a model with the numerical features as itself first, then conduct experiments by converting those numerical ones to the categorical ones following some criteria.","b42776c4":"Finally, let's convert the categorical variables into numerical variables using the Label Encoder. Then, aggregate the total number of promotion acceptance for each customer.","6419b060":"## Train - Validation Split","e6b3c6cd":"## Result Merge","58fab0d2":"Assuming that the variable follows the normal distribution, observations 3 standard deviations away from the mean correspond to 2.6% of the entire sample. However, as you can see, the observation with the the income of $ \\$ $666,666 is much further from the mean than the other outlier candidates whose values are just around $ \\$ $160,000. So, I'm just going to drop the observation with the maximum income, and transform the income variable to logarithmic scale.","d7d0c4bc":"# II. With 2-level categorical variables\n\nHere, I'm going to split every numerical variable into 2 levels, and the cutoff for each variable will be set at its median.","3158a739":"The multiplied variable has a bit higher correlation with the target variable. So I will use the multiplied variable instead of the added one.","73b5b30b":"# IV. Converting only variables with large Standard deviation","e179d75e":"There seems to be an outlier, the customer with the income of $ \\$ $666,666. Let's see if there is other outliers, by using the standard deviation method.","a4d87d8b":"* As we can see here, except for the Gradient Boosting Classifier, ROC AUC scored the highest when none of continuous values is converted to categorical features. This implies that it is recommended not to convert any continuous variable into a categorical variable arbitrarily.\n    - Furthermore, it is also seen that the less categorical features, the higher score the model achieves.\n    - The reason seems to be that the underlying mechanism of classification has little difference with that of regression. In fact, classification is also about predicting continuous targets, the only difference is that the target for classification is restricted in range of $(0, 1)$, and a criterion called decision threshold is necessary to map those continuous estimates(called probabilities) to the integers 0 and 1.\n    \n\n* More interesting result is that even though the number of categorical features are the same, ROC AUC calculated from the experiment 5 is higher than that from the experiment 4. \n    - The difference between the experiments is just which variables are converted to categorical features: the ones whose standard deviation is larger than the mean are converted in the experiment 4, while the ones whose standard deviation is smaller than the mean are converted in the experiment 5.\n    - This is just one experiment so this result might be hard to be generalized, it gives us an insight that if we were to convert some continuous variables to categorical variables, it is recommended to convert the ones whose standard deviation is smaller than the mean.","71a91610":"# Data Explanation\n\nToday, I'm going to use the marketing campaign dataset, which contains around 30 variables such as the age, socio-economic status of each customer. With these variables, I'm going to predict whether a customer would response to a marketing campaign or not.\n\nContent\n\nPeople\n\n* ID: Customer's unique identifier\n* Year_Birth: Customer's birth year\n* Education: Customer's education level\n* Marital_Status: Customer's marital status\n* Income: Customer's yearly household income\n* Kidhome: Number of children in customer's household\n* Teenhome: Number of teenagers in customer's household\n* Dt_Customer: Date of customer's enrollment with the company\n* Recency: Number of days since customer's last purchase\n* Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\nProducts\n\n* MntWines: Amount spent on wine in last 2 years\n* MntFruits: Amount spent on fruits in last 2 years\n* MntMeatProducts: Amount spent on meat in last 2 years\n* MntFishProducts: Amount spent on fish in last 2 years\n* MntSweetProducts: Amount spent on sweets in last 2 years\n* MntGoldProds: Amount spent on gold in last 2 years\n\nPromotion\n\n* NumDealsPurchases: Number of purchases made with a discount\n* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\nPlace\n\n* NumWebPurchases: Number of purchases made through the company\u2019s web site\n* NumCatalogPurchases: Number of purchases made using a catalogue\n* NumStorePurchases: Number of purchases made directly in stores\n* NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month","cb2d6f35":"There are several numerical variables whose standard deviation is (much) larger than its mean. So I'll convert only those variables into 4-level categorical variables here.","25a08a98":"## Train - Validation Split","b1c5b4a1":"## Train - Test Split"}}