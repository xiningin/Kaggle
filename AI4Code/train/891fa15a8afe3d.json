{"cell_type":{"67bd736d":"code","9df80373":"code","ff9e0790":"code","d9f78d96":"code","1ded0cca":"code","aeb19dd1":"code","6bf16cd5":"code","1e17847b":"code","b9391c82":"code","d3595a4b":"code","26b51a87":"code","fbd70d14":"code","e571af03":"code","53535562":"code","d91d9b61":"code","35288a2f":"code","fbac0f22":"code","87f585ef":"code","482f5099":"code","0c5cefc6":"code","dc0c90d9":"code","df226390":"code","3a403e1f":"code","74d83fa7":"code","9c27e4b2":"code","5345879c":"code","2ae7c252":"code","08f16807":"code","5aae51da":"code","fc6a6573":"code","2dd5816c":"code","4daecc05":"code","be785541":"code","9f6ec090":"code","902b4d9d":"code","e4b5e333":"code","5e2e052f":"code","daf9a2e3":"code","44b897b2":"code","f33f9259":"code","b2366f45":"code","aa0009de":"code","244d8aae":"code","cfa956f9":"code","ba41242a":"code","b6e4ccfe":"code","4fab0acb":"code","4b9d574f":"code","4458010d":"markdown","e8473bf2":"markdown","02c693bb":"markdown","ee8d0da1":"markdown","ca519f76":"markdown","7f1fe4f8":"markdown","8b1f15b7":"markdown","3a28a026":"markdown","86fe718f":"markdown","2f2990f5":"markdown","341f382a":"markdown","3e4d32c1":"markdown","9068ef97":"markdown","f55d490e":"markdown","080b9645":"markdown","b2e54409":"markdown","2aa68407":"markdown","74157f0e":"markdown","88443f17":"markdown","b0c0188a":"markdown","ea5d02d2":"markdown","992713ae":"markdown","f4e2a5b0":"markdown","41f53100":"markdown","0080ed03":"markdown","05beb8d8":"markdown","3864d67c":"markdown","ce0b7e0f":"markdown"},"source":{"67bd736d":"#Main libraries to work with the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#set plots to show without the need of plt.show()\n%matplotlib inline\n\n#setting seaborn's plots styles\nsns.set_style(\"darkgrid\")\nsns.set_palette(\"colorblind\")\n\n#avoid showing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9df80373":"data = pd.read_csv(\"..\/input\/heart.csv\")\ndata.head() #5 first rows","ff9e0790":"data[\"sex_s\"] = data[\"sex\"].map({0: \"female\", 1: \"male\"})\ndata[\"cp_s\"] = data[\"cp\"].map({0: \"typical angina\", 1: \"atypical angina\", 2: \"non-anginal pain\", 3: \"asymptomatic\"})\ndata[\"fbs_s\"] = data[\"fbs\"].map({0: \"<= 120 mg\/dl\", 1: \"> 120 mg\/dl\"})\ndata[\"restecg_s\"] = data[\"restecg\"].map({0: \"normal\", 1: \"abnormal\", 2: \"dangerous\"})\ndata[\"exang_s\"] = data[\"exang\"].map({0: \"no\", 1: \"yes\"})\ndata[\"slope_s\"] = data[\"slope\"].map({0: \"upsloping\", 1: \"flat\", 2:\"downsloping\"})\ndata[\"target_s\"] = data[\"target\"].map({0: \"healthy\", 1: \"sick\"})\n\ndata[\"is_sick\"] = data[\"target\"]\ndata.drop([\"target\"],axis=1,inplace=True)","d9f78d96":"data.head(3)","1ded0cca":"#Columns infos\ndata.info()","aeb19dd1":"#General description of the numerical values, such as mean, median, std etc.\ndata.describe()","6bf16cd5":"#Correlation matrix\n#More about correlation:\n#https:\/\/stats.stackexchange.com\/questions\/18082\/how-would-you-explain-the-difference-between-correlation-and-covariance\nplt.figure(figsize=(16,8))\nsns.heatmap(data=data.corr(),annot=True,cmap=\"viridis\")\nplt.title(\"Correlation Matrix\")","1e17847b":"#Checking target distribution\nsns.countplot(x=\"target_s\",data=data)","b9391c82":"fig, ax = plt.subplots(3,2,figsize=(16,12))\nsns.boxplot(x=\"cp_s\",y=\"age\",data=data,ax=ax[0][0])\nsns.boxplot(x=\"cp_s\",y=\"age\",hue=\"target_s\",data=data,ax=ax[0][1])\nsns.boxplot(x=\"cp_s\",y=\"trestbps\",data=data,ax=ax[1][0])\nsns.boxplot(x=\"cp_s\",y=\"thalach\",data=data,ax=ax[2][0])\nsns.boxplot(x=\"cp_s\",y=\"trestbps\",hue=\"target_s\",data=data,ax=ax[1][1])\nsns.boxplot(x=\"cp_s\",y=\"thalach\",hue=\"target_s\",data=data,ax=ax[2][1])","d3595a4b":"fig, ax = plt.subplots(1,2,figsize=(16,3))\nsns.countplot(x=\"cp_s\",hue=\"target_s\",data=data,ax=ax[0])\nsns.barplot(x=\"cp_s\",y=\"is_sick\",data=data,ax=ax[1])","26b51a87":"#Engineering two new features\ngroup0 = [\"typical angina\"]\ndef group_pain(pain):\n    return int(pain not in group0)\n\ndata[\"cp_typ_x_rest\"] = data[\"cp_s\"].apply(group_pain)\ngroup0.append(\"asymptomatic\")\ndata[\"cp_typ_&_asymp_x_rest\"] = data[\"cp_s\"].apply(group_pain)","fbd70d14":"data.head(2) #checking if the two new columns were added","e571af03":"fig, ax = plt.subplots(2,2,figsize=(16,8))\nsns.distplot(data[\"thalach\"],ax=ax[0][0])\nax[0][0].set_title(\"Distribution over the dataset\")\nsns.kdeplot(data[data[\"target_s\"] == \"sick\"][\"thalach\"],ax=ax[0][1],color=\"red\",label=\"sick\")\nsns.kdeplot(data[data[\"target_s\"] == \"healthy\"][\"thalach\"],ax=ax[0][1],color=\"green\",label=\"healthy\")\nax[0][1].set_title(\"Distribution over the dataset separated by sick and healthy people\")\nsns.distplot(data[data[\"sex_s\"] == \"female\"][\"thalach\"],ax=ax[1][0],color=\"orange\")\nax[1][0].set_title(\"Distribution for women\")\nsns.distplot(data[data[\"sex_s\"] ==   \"male\"][\"thalach\"],ax=ax[1][1],color=\"blue\")\nax[1][1].set_title(\"Distribution for men\")\nplt.tight_layout()","53535562":"fig, ax = plt.subplots(1,2,figsize=(16,4))\nsns.countplot(x=\"exang\",data=data,ax=ax[0]) #0 - not induced \/ #1 - induced\nax[0].set_title(\"Distribution over the dataset\")\nsns.countplot(x=\"target_s\",hue=\"exang\",data=data,ax=ax[1])\nax[1].set_title(\"Distribution over the dataset separated by sick and healthy people\")","d91d9b61":"sns.kdeplot(data[data[\"target_s\"] ==    \"sick\"][\"oldpeak\"],color=\"red\",label=\"sick\")\nsns.kdeplot(data[data[\"target_s\"] == \"healthy\"][\"oldpeak\"],color=\"green\",label=\"healthy\")","35288a2f":"#Age is a continuous value, so a histogram is appropriate.\nsns.distplot(data[\"age\"])\ndata[\"age\"].describe()","fbac0f22":"data.loc[data[\"age\"] == 29]","87f585ef":"sns.countplot(x=\"sex_s\",hue=\"target_s\",data=data,palette=\"magma\")\nprint(\"Males   in dataset: {}\".format(data.loc[data[\"sex_s\"] == \"male\",:].shape[0]))\nprint(\"Females in dataset: {}\".format(data.loc[data[\"sex_s\"]==\"female\",:].shape[0]))","482f5099":"fig, ax = plt.subplots(2,2,figsize=(16,8))\n#0,0\nsns.kdeplot(\n    data[(data[\"sex_s\"] == \"female\") & (data[\"target_s\"] == \"sick\")][\"age\"],\n    color=\"yellow\",shade=True,ax=ax[0][0])\nsns.kdeplot(\n    data[(data[\"sex_s\"] == \"female\") & (data[\"target_s\"] == \"healthy\")][\"age\"],\n    color=\"violet\",shade=True,ax=ax[0][0])\nax[0][0].legend(labels=(\"female_sick\",\"female_healthy\"))\n#0,1\nsns.kdeplot(\n    data[(data[\"sex_s\"] == \"male\") & (data[\"target_s\"] == \"sick\")][\"age\"],\n    color=\"red\",shade=True,ax=ax[0][1])\nsns.kdeplot(\n    data[(data[\"sex_s\"] == \"male\") & (data[\"target_s\"] == \"healthy\")][\"age\"],\n    color=\"blue\",shade=True,ax=ax[0][1])\nax[0][1].legend(labels=(\"male_sick\",\"male_healthy\"))\n#1,0\ndata[\"age_cats\"] = pd.cut(data[\"age\"],bins=[28,40,50,60,100])\nsns.countplot(x=\"age_cats\",data=data,ax=ax[1][0])\n#1,1\nsns.barplot(x=\"age_cats\",y=\"is_sick\",data=data,ax=ax[1][1])\nax[1][1].set_title(\"Number of sick and healthy people for each age category\")","0c5cefc6":"from sklearn.ensemble import ExtraTreesClassifier\n\nprint(\"List of all features: {}\".format(data.columns))","dc0c90d9":"def feature_importances(dataset,features_list,target,test_size=.25,random_state=14):\n    \"\"\"\n    Wrap-up function to train an ExtraTreesClassifier and return a descending ordered feature importance list\n    \"\"\"\n    etc = ExtraTreesClassifier(n_estimators=50)\n    etc.fit(dataset[features_list],dataset[target])\n    fi = pd.DataFrame(data=etc.feature_importances_,index=X,columns=[\"Feature Importance\"])\n    return fi.sort_values(by=\"Feature Importance\",ascending=False)","df226390":"#Using the original 13 features\nX = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\ny = 'is_sick'\n\nfeature_importances(data,X,y)","3a403e1f":"#Using cp as the 1st engineered feature\nX = ['age','sex','cp_typ_x_rest','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\ny = 'is_sick'\n\nfeature_importances(data,X,y)","74d83fa7":"X = ['age','sex','cp_typ_&_asymp_x_rest','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope',\n     'ca','thal']\ny = 'is_sick'\n\nfeature_importances(data,X,y)","9c27e4b2":"X = ['age','sex','cp','cp_typ_x_rest','cp_typ_&_asymp_x_rest','trestbps','chol','fbs','restecg','thalach','exang',\n     'oldpeak','slope','ca','thal']\ny = 'is_sick'\n\nfeature_importances(data,X,y)","5345879c":"#Base algorithms, no ensembling for now\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#Cross validation\nfrom sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n\n#Scaling\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n\n#Metrics to evaluate models\n#General metrics for classifiers\nfrom sklearn.metrics import classification_report, confusion_matrix \n\n#Metrics for precision\/recall trade-off (more of this later in this notebook)\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, precision_score, recall_score\n\nimport time #built-in library to measure time","2ae7c252":"def autotrain(X,y,scoring=\"accuracy\",cv_split=5,title=\"\"):\n    \"\"\"\n    Performs cross validation of defined base models and presents results as a dataframe sorted by best test scores.\n    Adapted from LD Freeman's kernel:\n    https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n    \"\"\"\n    #define base training models.\n    models = [KNeighborsClassifier(), SVC(gamma=\"auto\"), LogisticRegression(solver=\"liblinear\"), \n              DecisionTreeClassifier(), GaussianNB()]\n    \n    #create a dataframe to store training information and display after all iterations are finished.\n    results = pd.DataFrame(columns=[\"Algorithm\",\"Base Estimator\",\n                                    \"Train Time\",\"Train Score\", \"Test Score\",\"Scaling Method\"])\n    \n    print(title) #title for the resulting dataframe\n    for i,model in enumerate(models):\n        #define scalers to try\n        scalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(),RobustScaler()]\n        results.loc[i,\"Algorithm\"] = model.__class__.__name__\n        training = cross_validate(model,X,y,cv=cv_split,scoring=\"accuracy\",return_train_score=True) #\n        results.loc[i,\"Base Estimator\"] = str(model)\n        results.loc[i,\"Train Time\"] = training[\"fit_time\"].sum()\n        results.loc[i,\"Train Score\"] = training[\"train_score\"].mean()\n        results.loc[i,\"Test Score\"] = training[\"test_score\"].mean()\n        results.loc[i,\"Scaling Method\"] = \"Unscaled\"\n        #print(\"Model: {}\".format(model.__class__.__name__))\n        #print(\"Testing Score (unscaled): {}\".format(training[\"test_score\"].mean()))    \n        for scaler in scalers:\n            X_scaled = scaler.fit_transform(X)\n            training = cross_validate(model,X_scaled,y,cv=cv_split,scoring=\"accuracy\",return_train_score=True)\n            #print(\"Testing Score ({}): {}\".format(scaler.__class__.__name__,training[\"test_score\"].mean()))\n            if training[\"test_score\"].mean() > results.loc[i,\"Test Score\"]:\n                results.loc[i,\"Train Score\"] = training[\"train_score\"].mean()\n                results.loc[i,\"Test Score\"] = training[\"test_score\"].mean()\n                results.loc[i,\"Scaling Method\"] = scaler.__class__.__name__\n        #print(\"*\"*50)\n        \n    return results.sort_values(by=\"Test Score\",ascending=False)","08f16807":"X = data[['age','sex','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','cp']]\ny = data['is_sick']\nautotrain(X,y,title=\"Model features with all 13 original features\")","5aae51da":"X = data[['age','sex','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal',\n          'cp_typ_x_rest']]\n\nautotrain(X,y,title=\"Model features including engineered feature for chest pain typical angina X rest\")","fc6a6573":"X = data[['thalach','exang','slope','ca','thal','cp']]\n\nautotrain(X,y,title=\"Model features with the best features (according to ExtraTreesClassifier) already existing\")","2dd5816c":"X = data[['thalach','slope', 'ca','thal','cp_typ_x_rest']]\n\nautotrain(X,y,title=\"Model features with the best features (according to ExtraTreesClassifier) and engineered cp\")","4daecc05":"X_svm1 = RobustScaler().fit_transform(data[['thalach','exang','slope','ca','thal','cp']])\nX_svm2 = RobustScaler().fit_transform(data[['thalach','slope', 'ca','thal','cp_typ_x_rest']])\nX_lreg = StandardScaler().fit_transform(data[['age','sex','trestbps','chol','fbs','restecg','thalach','exang',\n                                                'oldpeak','slope','ca','thal','cp']])\n\ny = data['is_sick']\n\nsvm1_X_train, svm1_X_test, y_train, y_test = train_test_split(X_svm1, y, test_size=.3, random_state=14)\nsvm2_X_train, svm2_X_test, y_train, y_test = train_test_split(X_svm2, y, test_size=.3, random_state=14)\nlreg_X_train, lreg_X_test, y_train, y_test = train_test_split(X_lreg, y, test_size=.3, random_state=14)\n\nsvm1 = SVC(gamma=\"auto\")\nsvm2 = SVC(gamma=\"auto\")\nlreg = LogisticRegression(solver=\"liblinear\")\n\nsvm1.fit(svm1_X_train,y_train)\nsvm2.fit(svm2_X_train,y_train)\nlreg.fit(lreg_X_train,y_train)\n\nprint(\"Support-vector classifiers and logistic regression trained with 70% of the dataset randomly chosen\")\nprint(\"Number of instances for training: {}\".format(lreg_X_train.shape[0]))\nprint(\"Number of instances for testing: {}\".format(lreg_X_test.shape[0]))","be785541":"fig, ax = plt.subplots(1,3,figsize=(16,4))\n\nsvm1_predict = svm1.predict(svm1_X_test)\nsvm1_conf_matrix = confusion_matrix(y_test,svm1_predict)\n\nsvm2_predict = svm2.predict(svm2_X_test)\nsvm2_conf_matrix = confusion_matrix(y_test,svm2_predict)\n\nlreg_predict = lreg.predict(lreg_X_test)\nlreg_conf_matrix = confusion_matrix(y_test,lreg_predict)\n\nlabels = (\"healthy\",\"sick\")\n\nsns.heatmap(svm1_conf_matrix,   annot=True,cmap=\"coolwarm_r\",xticklabels=labels,yticklabels=labels,ax=ax[0])\nax[0].set_title(\"Support-Vector Classifier #1\")\nax[0].set_ylabel(\"Actual Values\", fontsize=16)\nax[0].set_xlabel(\"Predicted Values\", fontsize=16)\n#(with RobustScaler & only best existing features)\n\nsns.heatmap(svm2_conf_matrix,   annot=True,cmap=\"coolwarm_r\",xticklabels=labels,yticklabels=labels,ax=ax[1])\nax[1].set_title(\"Support-Vector Classifier #2\")\nax[1].set_xlabel(\"Predicted Values\", fontsize=16)\n#(with RobustScaler & engineered cp and best features)\n\nsns.heatmap(lreg_conf_matrix, annot=True,cmap=\"coolwarm_r\",xticklabels=labels,yticklabels=labels,ax=ax[2])\nax[2].set_title(\"Logistic Regressor Classifier\")\nax[2].set_xlabel(\"Predicted Values\", fontsize=16)\n#(with StandardScaler & all 13 existing features)","9f6ec090":"print(\"Classification report for Support-Vector Classifier #1\".upper())\nprint(\"-\"*60)\nprint(classification_report(y_test,svm1_predict,target_names=labels))","902b4d9d":"#SVM Tuning\n\nparams_svm = {\n    \"kernel\": [\"rbf\", \"linear\"],\n    \"C\": np.logspace(-5,3,9),\n    \"gamma\": np.logspace(-4,-1,4),\n    \"decision_function_shape\": [\"ovo\", \"ovr\"],\n    \"random_state\": [41],\n}\n\n# In each fold, the dataset will be splitted 75\/25\ngrid1 = GridSearchCV(SVC(probability=True),iid=False,param_grid=params_svm,cv=4) \n\nstart = time.perf_counter()\ngrid1.fit(X_svm1,y)\nend = time.perf_counter()\n\nprint(\"SVM tuning amount of seconds elapsed: {:.2f}\".format(end-start))\nprint(\"Best parameters found for this model: {}\".format(grid1.best_params_))","e4b5e333":"svm_tuned = grid1.best_estimator_\n\nsvm_tuned_predict = svm_tuned.predict(X_svm1) #checking overall performance for the tuned model\nsvm_tuned_conf_matrix = confusion_matrix(y,svm_tuned_predict)\n\nsns.heatmap(svm_tuned_conf_matrix,annot=True,cmap=\"coolwarm_r\",xticklabels=labels,yticklabels=labels,fmt=\"1\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.title(\"SVM after hyperparameters tuning\")","5e2e052f":"print(\"Classification report for Support-Vector Classifier (Tuned) #1\".upper())\nprint(\"-\"*60)\nprint(classification_report(y,svm_tuned_predict,target_names=labels))","daf9a2e3":"#LogReg Tuning\n\nparams_lreg = {\n    \"C\": np.logspace(-5,3,9),\n    \"solver\": (\"liblinear\",\"lbfgs\",\"sag\",\"saga\",\"newton-cg\"),\n    \"fit_intercept\": (True,False),\n    \"random_state\": [41],\n}\n\n# In each fold, the dataset will be splitted 75\/25\ngrid2 = GridSearchCV(LogisticRegression(),iid=False,param_grid=params_lreg,cv=4)\n\nstart = time.perf_counter()\ngrid2.fit(X_lreg,y)\nend = time.perf_counter()\n\nprint(\"LogReg tuning amount of seconds elapsed: {:.2f}\".format(end-start))\nprint(\"Best parameters found for this model: {}\".format(grid2.best_params_))","44b897b2":"print(\"Classification report for Logistic Regression\".upper())\nprint(\"-\"*60)\nprint(classification_report(y_test,lreg_predict,target_names=labels))\nprint(\"\\n\")\nlreg_tuned = grid2.best_estimator_\nlreg_tuned_predict = lreg_tuned.predict(X_lreg)\nprint(\"Classification report for Logistic Regression (Tuned) #1\".upper())\nprint(\"-\"*60)\nprint(classification_report(y,lreg_tuned_predict,target_names=labels))","f33f9259":"#SVC\ny_preds = svm_tuned.predict(svm1_X_test)\ny_scores_svm = svm_tuned.predict_proba(svm1_X_test)[:,1]\n\npred_and_proba = pd.DataFrame(data={\"Final Prediction\": y_preds, \"Proba\": y_scores_svm})","b2366f45":"#Checking if threshold is .5\npred_and_proba.loc[(pred_and_proba[\"Proba\"] > .4) & (pred_and_proba[\"Proba\"] < .6),:]","aa0009de":"svm_prec, svm_rec, svm_t = precision_recall_curve(y_test,y_scores_svm)\ny_scores_lreg = lreg_tuned.predict_proba(lreg_X_test)[:,1]\nlreg_prec, lreg_rec, lreg_t = precision_recall_curve(y_test,y_scores_lreg)\n\nfig, ax = plt.subplots(1,2,figsize=(16,3))\nplt.sca(ax[0])\nplt.step(svm_rec,svm_prec,where=\"post\",alpha=.5,color=\"r\")\nplt.fill_between(svm_rec,svm_prec,step=\"post\",alpha=.2,color=\"r\")\nplt.xlim(.79,1.001)\nplt.xlabel(\"Recall\",fontsize=14)\nplt.ylabel(\"Precision\",fontsize=14)\nplt.title(\"Recall vs. Precision\",fontsize=18)\nplt.sca(ax[1])\nplt.step(lreg_rec,lreg_prec,where=\"post\",color=\"b\")\nplt.fill_between(lreg_rec,lreg_prec,step=\"post\",alpha=.2,color=\"b\")\nplt.xlim(.79,1.001)\nplt.xlabel(\"Recall\",fontsize=14)\nplt.ylabel(\"Precision\",fontsize=14)\nplt.title(\"Recall vs. Precision\",fontsize=18)","244d8aae":"plt.plot(np.arange(0,svm_t.shape[0]),svm_t,color=\"r\")\nplt.plot(np.arange(0,lreg_t.shape[0]),lreg_t,color=\"b\")","cfa956f9":"#SVM\nfor threshold in np.arange(0,1.05,.05):\n    y_adj = [1 if y >= threshold else 0 for y in y_scores_svm]\n    print(\"SVM: For threshold of {:.2f}, precision is {:.3f} and recall is {:.3f}\".format(\n    threshold, precision_score(y_test,y_adj), recall_score(y_test,y_adj)))","ba41242a":"plt.figure(figsize=(8, 8))\nplt.title(\"Precision and Recall Scores as a function of the decision threshold\")\nplt.plot(svm_t, svm_prec[:-1], \"b--\", label=\"Precision\")\nplt.plot(svm_t, svm_rec[:-1], \"g-\", label=\"Recall\")\nplt.ylabel(\"Score\",fontsize=14)\nplt.xlabel(\"Decision Threshold\",fontsize=14)\nplt.legend(loc='best')","b6e4ccfe":"for threshold in np.arange(.4,.46,.01):\n    y_adj = [1 if y >= threshold else 0 for y in y_scores_svm]\n    print(\"SVM: For threshold of {:.2f}, precision is {:.3f} and recall is {:.3f}\".format(\n    threshold, precision_score(y_test,y_adj), recall_score(y_test,y_adj)))","4fab0acb":"#Confusion matrix for threshold = .42\nthreshold = .42\ny_adj = [1 if y >= threshold else 0 for y in y_scores_svm]\nsns.heatmap(confusion_matrix(y_test,y_adj),annot=True,fmt=\"1\")","4b9d574f":"#Classification Report for threshold = .42\nprint(classification_report(y_test,y_adj))","4458010d":"SVC #1 predicted 75\/91 = 82.4% cases correctly. SVC #2 predicted 72\/91 = 79% cases correctly. LogReg predicted 76\/91 = 83.5% cases correctly. Although differences seem small, we're going to perform **hyperparameters tuning** only in SVC #1 and the Logistic Regressor. This is due the fact that SVC #2 is predicting almost twice as much **False Negatives (FN)** as the other two models. In plain english, this means that SVC #2 is telling twice as much sick patients that they are healthy. We want to be **conservative** and minimize this kind of error. It is better to predict that a healthy person is sick and perform more tests on them than send sick people home.\n\nAccording to sklearn's documentation:\nThe recall is the ratio $TP  \/ (TP + FN)$ where $TP$ is the number of **true positives** and $FN$ the number of **false negatives**. The recall is intuitively the **ability** of the classifier **to find all the positive samples**. <br>(Source: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html)\n","e8473bf2":"We can see that the recall curve decreases slower than the increase of precision. The optimal point is between .6 and .7, but the recall for it is around .85. Not good enough. I believe we can settle a decision threshold somewhere .41, .42 or .43. Let's test.","02c693bb":"# Heart Disease Study\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to \nthis date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\n## Importing libraries and reading dataset\n\nFirstly, we are going to import only library for manipulating data. Sklearn's modules won't be loaded for now, just on the second part.","ee8d0da1":"## General interpretation of dataset\n\nThe dataset contains the following features:\n1. age: in years\n2. sex: (1 = male; 0 = female)\n3. cp: chest pain type\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n5. chol: serum cholestoral in mg\/dl\n6. fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecg: resting electrocardiographic results\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak: ST depression induced by exercise relative to rest\n11. slope: the slope of the peak exercise ST segment\n12. ca: number of major vessels (0-3) colored by flourosopy\n13. thal: 1 = normal; 2 = fixed defect; 3 = reversable defect\n> The original info for this feature is: \"A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\" but the values in the dataset are 0 (unknown, 2 instances), 1, 2 and 3.\n14. target: 0 = no (healthy), 1 = yes (sick)\n\n### Stringifying Columns to Facilitate Analysis","ca519f76":"The values for the **oldpeak** tend to be **lower** to **sick people**.","7f1fe4f8":"### Exercise induced angina (exang)","8b1f15b7":"## Next steps \/ suggestions:\n1. Use sklearn.ensemble classifiers;\n2. Use sklearn.feature_selection tools to improve selecting features;\n3. Build, with sklearn.pipeline, a black-box model to fit and predict data with pre-fixed threshold.","3a28a026":"## Interpreting the Classification Report","86fe718f":"**Highest Test Scores (Accuracy)**\n1. SVC with (all) existing best RobustScaled features -> .8446\n2. SVC with engineered best RobustScaled features -> .8445\n3. Logistic Regression with (all) existing StandardScaled features -> .8349\n\nIt is important to notice that for these 3 results listed above, if we compare:\n* 1 with SVC w\/ RobustScaler and all existing features, the train score has **dropped**.\n* 2 with SVC w\/ RobustScaler and engineered cp and all the remaining features, the train score has **dropped**.\n\nSo, we clearly see that an increase of (not so useful) features led to **overfitting**, i.e., the models were performing excelent on the train set but failing to predict unseen data. The **Accuracy x Error** for training and testing can be seen below. We can see that an increase on the number of features (increase of model **complexity**) is good until certain point. After that, the model starts to (over)fit too much known data, which means that it is capturing **unwanted noise** instead of **generalizing**. The exclamation mark indicates the **optimal point**.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fc\/Overfitting.png\",width=30%>\n_Source: Wikipedia commons_","2f2990f5":"**Highest correlated factors to target:**\n* Chest pain (cp)\n* Maximum heart rate achieved (thalach)\n* Exercise induced angina (exang)\n* ST depression induced by exercise relative to rest (oldpeak)\n\n**Worth checking**\n* Colored vessels by flourosopy (ca)\n* Slope (slope)\n* Sex (sex)\n* Age (age)\n\n**Lowest correlated factors to target:**\n* Fasting blood sugar (fbs)\n* Serum cholesterol (chol)\n* Resting blood pressure (trestbps)","341f382a":"From the confusion matrix, we know that: \n* $TP = ActuallySick = 44$\n* $FP = PredictedSickButIsHealthy = 11$\n* $TN = ActuallyHealthy = 31$\n* $FN = PredictedHealthyButIsSick = 5$\n\n### Precision: $\\frac{TP}{TP+FP}$\n\n$Precision(sick) = \\frac{ActuallySick}{ActuallySick + PredictedSickButIsHealthy} = \\frac{44}{44+11} = 80.0\\%$\n<br><br>\n$Precision(healthy) = \\frac{ActuallyHealthy}{ActuallyHealthy + PredictedHealthyButIsSick} = \\frac{31}{31+5} = 86.1\\%$\n### Recall (or sensitivity): $\\frac{TP}{TP+FN}$\n$Recall(sick) = \\frac{ActuallySick}{ActuallySick + PredictedHealthyButIsSick} = \\frac{44}{44+5} = 89.8\\%$\n<br><br>\n$Recall(healthy) = \\frac{ActuallyHealthy}{ActuallyHealthy + PredictedSickButIsHealthy} = \\frac{31}{31+11} = 73.8\\%$\n___\nOur goal is to minimize the False Negatives (FN) for **safety** reasons. The _Predicted Healthy but is Sick_ in this test set is 5. Ideally pushing it to zero would lead to $Precision(healthy) = 1$ and $Recall(sick) = 1$.","3e4d32c1":"## Modeling\n### Feature Importance\nFirst step is to use an ensemble algorithm to check feature importance.\n\nReference: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html","9068ef97":"The threshold for Logistic Regression is a lot more sensitive, i.e., a small change in its value can modify considerably the outcome. This looks unstable. Let's proceed with SVC and find a suitable threshold.","f55d490e":"## Improving our Model: Tuning Hyperparameters\nFor this step, we're going to use the GridSearchCV function. We can create a dictionary with all the hyperparams. we wanna try and the grid search will try every possible combination of those. In the end, it returns the best performing model.","080b9645":"## First glance at the data and understanding the target\nAfter stringifying the categorical columns and renaming the target to \"is_sick\", let's see how the dataset is currently and first touch the \"is_sick\" column.","b2e54409":"## Testing different algorithms","2aa68407":"The **youngest** person in this dataset is 29 years old and already **has a heart disease**. I better call my doctor...","74157f0e":"## Train Test Splitting to check Recall","88443f17":"This graphs show that healthy usually have the chest pain value \"typical angina\". For the other cases, even asymptomatic, there are more sick people than healthy. The difference between sick and healthy for non-anginal pain and atypical angina are interesting to notice.\n\nWe'll work with three strategies and see which performs better: \n1. Cp as the three values we already have;\n2. Typical angina = 0, the rest = 1;\n3. Typical angina and asymptomatic = 0, non-anginal and atypical angina = 1.","b0c0188a":"Extra trees confirms chest pain as one of the most important features. Sex perhaps has low importance here due to the unbalanced proportion of men and women (70% are men approx.). We already had a clue that fbs, chol and trestbps had low impact. Age having only a mid weight surprises me though.","ea5d02d2":"## Exploratory Data Analysis (EDA)\n### Chest Pain (cp)","992713ae":"### Oldpeak","f4e2a5b0":"### Maximum Heart Rate Achieved (thalach)","41f53100":"As expected, the **maximum heart rate** achieved is **higher** for **sick people**. There isn't a significant difference for the values of men and women.","0080ed03":"We can see that in a recall of almost 1 (.98 or .97) we can achieve a precision of .78 for the SVC and .75 for LogReg. If a recall of .94 is acceptable, logreg has a better precision, of .81, against .79 from SVC. Let's try to find both threshold just to compare.","05beb8d8":"This is quite a surprise. Even though we have the majority of people beyond 50, these people are not the sickiest... At least not for this dataset","3864d67c":"## A new approach for improvement: Adjusting Threshold\nAccording to Kevin Arvai: \n>The **precision_recall_curve** and **roc_curve** are useful tools to visualize the **sensitivity-specificty tradeoff** in the classifier. They help inform a data scientist where to set the **decision threshold** of the model to maximize either sensitivity or specificity. This is called the \u201coperating point\u201d of the model.","ce0b7e0f":"## Quick look on features labeled as \"worth checking\"\n### Age and Sex"}}