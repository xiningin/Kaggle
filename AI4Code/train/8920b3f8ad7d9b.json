{"cell_type":{"a912ffef":"code","0701d1c3":"code","9fcc1b89":"code","00300ed3":"code","511ee222":"code","c8b31035":"code","843eecef":"code","7c0c8675":"code","adc24043":"code","ec9807cf":"code","0afa77d2":"code","7885122d":"code","f482ff7e":"code","e2894701":"code","5d594e5f":"code","ed3bf3d0":"code","f2f395f8":"code","02038b03":"code","5646f765":"code","67f88c00":"code","90ba1431":"code","083cf041":"code","c363f882":"code","e00ee791":"code","b4f1f1e2":"code","2a676f07":"code","6478571a":"markdown","4b9f7716":"markdown","9a06f06a":"markdown","02e31464":"markdown","386374b2":"markdown","ee7bbcb8":"markdown","930a2cdd":"markdown","f16ae169":"markdown","3ab0d743":"markdown","9aa8dd30":"markdown","1772c6a2":"markdown","556ba976":"markdown","34bf804b":"markdown"},"source":{"a912ffef":"import os, re, gc, warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom scipy.stats import wasserstein_distance as wd\nimport seaborn as sns\nwarnings.filterwarnings(\"ignore\")\ngc.collect()","0701d1c3":"DATA_DIR = '..\/input\/'\nFILES={}\nfor fn in os.listdir(DATA_DIR):\n    FILES[ re.search( r'[^_\\.]+', fn).group() ] = DATA_DIR + fn\n\nCAT_COL='wheezy-copper-turtle-magic'    \n\ntrain = pd.read_csv(FILES['train'],index_col='id')\ntest = pd.read_csv(FILES['test'],index_col='id')\nCATS = sorted(train[CAT_COL].unique())\n\n# I use id as the index column to make work easier when splitting or augmenting training data\n# You can waste a lot of time with poor pandas concatenation","9fcc1b89":"c=train[ train[CAT_COL]==0 ].corr().abs()\nhigh_c = c.where(np.triu(np.ones(c.shape), k=1).astype(bool)).stack().sort_values(ascending=False)\n# unpacking that statement\n# get indices for upper triangular matrix not including diagonal np.triu(np.ones(c.shape), k=1).astype(bool)\n# get the indexed values, and store them in a sorted list c.where().stack().sort_values(ascending=False)\nhigh_c","00300ed3":"high_c.hist(bins=100)\ndisplay(high_c.describe())\nprint(f'''# of feature pairs with correlation greater than:\\n\n      0.1 : {sum(high_c.gt(.1))} \\n\n      0.15: {sum(high_c.gt(.15))}''')","511ee222":"def col_sort_by_wd(df):\n    distances ={}\n    for c in tqdm_notebook( range(1,df.shape[1]-1) ):\n        a = df.loc[ df.target==0 ].iloc[:,c]\n        b = df.loc[ df.target==1 ].iloc[:,c]\n        distances[train.columns[c]]= wd(a,b)\n    w = pd.Series(distances)\n    return w.sort_values(ascending=False)","c8b31035":"w1 = col_sort_by_wd(train).drop(CAT_COL)\nsns.distplot(w1)\nw1","843eecef":"w2 = col_sort_by_wd(train[ train[CAT_COL]==0 ].drop(CAT_COL,axis=1))\nsns.distplot(w2)\nw2","7c0c8675":"w2[ w2.gt(.25) ]\n# top 45 features for separating by target, when 'wheezy-copper-turtle-magic'==0","adc24043":"sns.jointplot(w1,w2[w1.index],marker='1').set_axis_labels('Feature scores across whole dataset','Feature scores when restricted to wheezy-copper-turtle-magic==0')","ec9807cf":"from scipy.stats import wasserstein_distance as wd\n\nfeat='zippy-harlequin-otter-grandmaster'\nsub_idx_0 = train['target']==0\nsub_idx_1 = train['target']==1\n\nsns.distplot(train[sub_idx_0][feat],bins=16,color='red')\nsns.distplot(train[sub_idx_1][feat],bins=16,color='blue')\n\nprint(f'Wasserstein distance between distributions: {wd( train.loc[sub_idx_0][feat], train.loc[sub_idx_1][feat])}' )","0afa77d2":"sub_idx_0 = (train['target']==0) & (train[CAT_COL]==0)\nsub_idx_1 = (train['target']==1) & (train[CAT_COL]==0)\n\nsns.distplot(train[sub_idx_0][feat],bins=16,color='red')\nsns.distplot(train[sub_idx_1][feat],bins=16,color='blue')\n\nprint(f'Wasserstein distance between distributions: {wd( train.loc[sub_idx_0][feat], train.loc[sub_idx_1][feat])}' )","7885122d":"c=train[ (train[CAT_COL]==0) ].corr().abs()\nhigh_c = c.where(np.triu(np.ones(c.shape), k=1).astype(bool)).stack().sort_values(ascending=False)\nhigh_c.filter(like='zippy-harlequin-otter-grandmaster')\n# the correlations do shift when you isolate for target==0 or 1\n# ex: replace with c=train[ (train[CAT_COL]==0) & (train['target']==0) ].corr().abs()","f482ff7e":"# looking at top 2 and bottom 2 correlations with zippy-harlequin-otter-grandmaster under CAT_COL==0\ncols_of_interest=['skimpy-copper-fowl-grandmaster','flaky-chocolate-beetle-grandmaster','zippy-harlequin-otter-grandmaster','blurry-wisteria-oyster-master','crabby-carmine-flounder-sorted' ]\nfeats = train[sub_idx_0][cols_of_interest]\nsns.pairplot(train[ train[CAT_COL]==0 ],vars=cols_of_interest,markers=['1','2'],hue='target',palette='husl')\nprint(f'Multivariate distribution plot where {str(CAT_COL)}==0 and target==0')","e2894701":"names = list(train.drop([\"target\"],axis=1).columns.values)","5d594e5f":"first_names = []\nsecond_names = []\nthird_names = []\nfourth_names = []\n\nfor name in names:\n    words = name.split(\"-\")\n    first_names.append(words[0])\n    second_names.append(words[1])\n    third_names.append(words[2])\n    fourth_names.append(words[3])","ed3bf3d0":"fns = {x for x in first_names}\nsns = {x for x in second_names}\ntns = {x for x in third_names}\nfons = {x for x in fourth_names}\n\nans = [fns, sns, tns, fons]","f2f395f8":"for i in range(len(ans)):\n    for j in range(i+1,len(ans)):\n        print(f'{i+1} & {j+1}: {ans[i]&ans[j]}')","02038b03":"display( train.filter(like='-blue-').head() )\ntrain.filter(like='coral').head()","5646f765":"pd.Series(first_names).value_counts().hist()\npd.Series(first_names).value_counts()","67f88c00":"pd.Series(second_names).value_counts().hist()\npd.Series(second_names).value_counts()","90ba1431":"pd.Series(third_names).value_counts().hist()\npd.Series(third_names).value_counts()","083cf041":"pd.Series(fourth_names).value_counts().hist()\npd.Series(fourth_names).value_counts()","c363f882":"pd.Series(fourth_names).nunique()","e00ee791":"train.shape","b4f1f1e2":"np.log(262144)\/np.log(2)","2a676f07":"len('Instant Gratification')\n# Unfortunately this does not appear to be a cipher key","6478571a":"We've already split up training into 512 separate models.\nDoing something further starting with first names is an idea. There are 85 words in that position, with no more than 7 occurences of any given word.","4b9f7716":"Most significant in the poem to me are the non-rhyming lines,\n\n    Silly column names abound,\n\n    Careful how you pick and slice,\n    \n    \nBut perhaps there is a time-series element?\n\n    or be left behind by history.\n","9a06f06a":"`stinky-maroon-blue-kernel` and `greasy-sepia-coral-dataset` each have a word that occurs out of its expected position. I attribute this to chance, and not an intentional clue.","02e31464":"# Exploratory Data Analysis\nI have experimented with some different kinds of modelling, but most of my time has been spent on everything that comes before that. I have written my own small functions, and then rewritten them or replaced them with existing ones that are standard tools in the kaggle community. This notebook is a distillation of some preliminary work I did which was not fit to be shared","386374b2":"Ok, so restricting to when `wheezy-copper-turtle-magic` makes it easier to distinguish samples where target is 0 or 1.\n\nInterestingly, neither distribution is gaussian. In particular it looks like the red distribution (target==0) is the sum of 2 or more gaussian processes.\n\nTrying to untangle the features can drive you mad.\n* I haven't tried brute force generation of possible non-linear features that can be combined into a linear model.\n* I have tried isolating the insignificant features and reducing their dimensionality by transforming them with a non-linear manifold, and then fitting a classifier to the top features along with my synthetic ones\n* I did start to try a PyTorch implementation of a Neural Ordinary Differential Equation solver, but ran into difficulties loading the library on the Kaggle cloud to enable GPU computation -- I would be very interested to see someone pick that up","ee7bbcb8":"### Clues\nThis is an anonymized, binary classification dataset found on a USB stick that washed ashore in a bottle. There was no data dictionary with the dataset, but this poem was handwritten on an accompanying scrap of paper:\n\n    Silly column names abound,\n    but the test set is a mystery.\n    Careful how you pick and slice,\n    or be left behind by history.\n","930a2cdd":"Part of the reason why I think there are clues to be found is that the dimensions chosen feel like a puzzle constructed with obvious care to make the whole thing easier to grasp.\n\n* samples in the training set: 2^18 or 262144\n* subcategories: 2^9 or 512\n* features: 2^8 or 256\n\nWhat other ways are there to slice and dice the data before fitting a model.\nFor any given subcategory, only about 50 features are useful for any of the classifiers explored in the competition.\n\nIt seems like data augmentation by boost leaderboard scores by 1 or 2% by the end, but the bulk of missclassifications will remain even after doubling training time with pseudo-labelling as one example.","f16ae169":"Just repeating what we've seen in tabular form, after restricting to samples for a single value of `wheezy-copper-turtle-magic`, we see an uplift of signifiance of features across the board when trying to distinguish between in- and out-of-target distribtuions.\n\nNext, we'll see what this looks like for the feature `zippy-harlequin-otter-grandmaster` which was the strongest feature in the case where `wheezy-copper-turtle-magic` is 0.","3ab0d743":"It's hard to pick an angle which will enable greater separation for the least significant features.\nThe strong features have separation by slightly different mean values\nThe weak features are only separated by changes in variance, resulting in much more overlap and lower WD scores.\n\nI've seen a lot of people trim their dataset by applying a variance threshold, and I believe that is just another way of achieving a similar or the same result.\n\nI believe the features with low WD scores are features that have undergone a non-linear transformation.\nIt's so much easier to forget them, the models train faster and so you can get a better score by searching more hyperparameters.\n\n\nThe challenge is to invert these unknown non-linear transforms, and recover linearly useful features under within reasonable time and computing resources.\n\nWe were given a poem as a clue, and I think there is another trick to be uncovered.","9aa8dd30":"### Thanks to everyone for reading, especially if you have contributed to our understanding by sharing your own work early and often.\n","1772c6a2":"Just 26 words in the final position.\nOther than our useful `wheezy-copper-turtle-magic`, there are 5 to 19 features for any given word in that fourth position.\nThis seems like an even better prospect for further slicing and dicing.\nIs there a clue in `26`? It is an unsual number, the number of letters in the english alphabet.","556ba976":"Not shown: when varying the selection to different values of `wheezy-copper-turtle-magic` there will be about 40-50 features that are significantly more informative than the rest when considered individually. For linear models, that means you can drop all but the 50 strongest features for any given subset without significantly decreasing performance. That means you can train the same model faster, or fit a more detailed model.","34bf804b":"These are the correlations between features for the training dataset where 'wheezy-copper-turtle-magic'==0 (including target as a feature).\n\nAbout 1 in 40 feature pairs has a correlation greater than 0.1, and 1 in 624 have a correlation greater than 0.15\n\nNext I'll take another look at feature importance using the Wasserstein Distance metric.\nAlso known as the 'earth moving distance', it measures the separation between 2 different distributions.\nI'll be looking at the difference, for each feature, of the samples where target is 0 or 1\n\nNote: WD of in- and out-of-target distributions for feature 'wheezy-copper-turtle-magic' == 0.30391190887413444, far above any other feature. Having accepted the wisdom of considering 512 subsets of data, I'll be trying to continue the push to provide better inputs to models"}}