{"cell_type":{"3bee3792":"code","44f71bae":"code","fe4c73ea":"code","4937af6d":"code","df7af839":"code","17502020":"code","a1330c60":"code","202a466f":"code","7f59460b":"code","98f179e8":"code","f31706ff":"code","0859feaf":"code","d785b5f1":"code","6153034e":"code","df57095d":"code","9042fb63":"code","7982cd32":"code","fa33a42a":"code","240a98a4":"code","a12b9117":"code","11d4e222":"code","4513bfe7":"code","f98f5f09":"code","e0b6c79c":"code","b34bfb1b":"code","206fa522":"code","651e780a":"code","02acbd8f":"code","be266b24":"code","a6843406":"code","01d5be6c":"code","8e6383cc":"code","1e49d527":"code","edd1e090":"code","301a85c0":"code","96d022fe":"code","cbfd17ca":"code","520b38e3":"code","b1db27c1":"code","ef1f9d31":"code","17726b8d":"code","085bcdec":"code","15cfa1c2":"code","c66dc593":"code","7f4a3da5":"code","7657c917":"code","f0f411b8":"code","4dde5e13":"code","ebaa327b":"code","5d6ae4f0":"code","48303815":"code","4ee0a22a":"code","c608b840":"code","92f75a12":"code","defb504f":"code","0985674d":"code","4c556941":"code","5b9de9f2":"code","f43e4311":"markdown","57c74ef8":"markdown","0eb2e3ee":"markdown","3b961d7b":"markdown","032c58c5":"markdown","ccb155ac":"markdown","26104256":"markdown","9eaf01e0":"markdown","d27e51e2":"markdown","9e603b7a":"markdown","90dfaaa2":"markdown","8dd0742f":"markdown","75da5ef3":"markdown","025214c5":"markdown","9532377a":"markdown","c84946b5":"markdown","8b832ced":"markdown","c823d733":"markdown","c76b9aa5":"markdown"},"source":{"3bee3792":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score,recall_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);","44f71bae":"df = pd.read_csv(\"..\/input\/churndataset\/churn.csv\")","fe4c73ea":"df.head()","4937af6d":"df.shape","df7af839":"df.info()","17502020":"df.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","a1330c60":"df.isnull().sum()","202a466f":"categorical_variables = [col for col in df.columns if col in \"O\"\n                        or df[col].nunique() <=11\n                        and col not in \"Exited\"]\n\ncategorical_variables","7f59460b":"numeric_variables = [col for col in df.columns if df[col].dtype != \"object\"\n                        and df[col].nunique() >11\n                        and col not in \"CustomerId\"]\nnumeric_variables","98f179e8":"df[\"Exited\"].value_counts()","f31706ff":"churn = df.loc[df[\"Exited\"]==1]","0859feaf":"not_churn = df.loc[df[\"Exited\"]==0]","d785b5f1":"not_churn[\"Tenure\"].value_counts().sort_values()","6153034e":"churn[\"Tenure\"].value_counts().sort_values()","df57095d":"not_churn[\"NumOfProducts\"].value_counts().sort_values()","9042fb63":"churn[\"NumOfProducts\"].value_counts().sort_values()","7982cd32":"not_churn[\"HasCrCard\"].value_counts()","fa33a42a":"churn[\"HasCrCard\"].value_counts()","240a98a4":"not_churn[\"IsActiveMember\"].value_counts()","a12b9117":"churn[\"IsActiveMember\"].value_counts()","11d4e222":"not_churn[\"CreditScore\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","4513bfe7":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('CreditScore')\npyplot.hist(not_churn[\"CreditScore\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","f98f5f09":"churn[\"CreditScore\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","e0b6c79c":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('CreditScore')\npyplot.hist(churn[\"CreditScore\"],bins=15, alpha=0.8, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","b34bfb1b":"sns.catplot(\"Exited\", \"CreditScore\", data = df)","206fa522":"not_churn[\"Age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","651e780a":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('Age')\npyplot.hist(not_churn[\"Age\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","02acbd8f":"churn[\"Age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","be266b24":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('Age')\npyplot.hist(churn[\"Age\"],bins=15, alpha=0.7, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","a6843406":"sns.catplot(\"Exited\", \"Age\", data = df)","01d5be6c":"not_churn[\"Balance\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","8e6383cc":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('Balance')\npyplot.hist(not_churn[\"Balance\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","1e49d527":"churn[\"Balance\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","edd1e090":"pyplot.figure(figsize=(8,6))\npyplot.xlabel('Balance')\npyplot.hist(churn[\"Balance\"],bins=15, alpha=0.7, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","301a85c0":"sns.catplot(\"Exited\", \"Balance\", data = df)","96d022fe":"k = 10 \ncols = df.corr().nlargest(k, 'Exited')['Exited'].index\ncm = df[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis')","cbfd17ca":"df.isnull().sum()","520b38e3":"def outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):\n    quantile_one = dataframe[variable].quantile(low_quantile)\n    quantile_three = dataframe[variable].quantile(up_quantile)\n    interquantile_range = quantile_three - quantile_one\n    up_limit = quantile_three + 1.5 * interquantile_range\n    low_limit = quantile_one - 1.5 * interquantile_range\n    return low_limit, up_limit","b1db27c1":"def has_outliers(dataframe, numeric_columns, plot=False):\n   # variable_names = []\n    for col in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n            print(col, \" : \", number_of_outliers, \"outliers\")\n            #variable_names.append(col)\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n    #return variable_names","ef1f9d31":"for var in numeric_variables:\n    print(var, \"has \" , has_outliers(df, [var]),  \"Outliers\")","17726b8d":"df[\"NewTenure\"] = df[\"Tenure\"]\/df[\"Age\"]\ndf[\"NewCreditsScore\"] = pd.qcut(df['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\ndf[\"NewAgeScore\"] = pd.qcut(df['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\ndf[\"NewBalanceScore\"] = pd.qcut(df['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\ndf[\"NewEstSalaryScore\"] = pd.qcut(df['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\ndf[\"NewCreditsScore\"] = df[\"NewCreditsScore\"].astype(int)\ndf[\"NewAgeScore\"] = df[\"NewAgeScore\"].astype(int)\ndf[\"NewBalanceScore\"] = df[\"NewBalanceScore\"].astype(int)\ndf[\"NewEstSalaryScore\"] = df[\"NewEstSalaryScore\"].astype(int)\n","085bcdec":"list = [\"Gender\", \"Geography\"]\ndf = pd.get_dummies(df, columns =list, drop_first = True)","15cfa1c2":"df.head()","c66dc593":"df = df.drop([\"CustomerId\",\"Surname\"], axis = 1)","7f4a3da5":"def robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.01)\n            quartile3 = variable.quantile(0.99)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) \/ interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)","7657c917":"new_cols_ohe = [\"Gender_Male\",\"Geography_Germany\",\"Geography_Spain\"]\nlike_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) <= 10]\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Exited\"\n                   and col not in like_num]\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])","f0f411b8":"df.head()","4dde5e13":"from xgboost import XGBClassifier\nX = df.drop(\"Exited\",axis=1)\ny = df[\"Exited\"]\n# Train-Test Separation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123456)\n# Models for Classification\nmodels = [('LR', LogisticRegression(random_state=123456)),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier(random_state=123456)),\n          ('RF', RandomForestClassifier(random_state=123456)),\n          ('SVR', SVC(gamma='auto',random_state=123456)),\n          ('XGB', GradientBoostingClassifier(random_state = 123456)),\n          (\"LightGBM\", LGBMClassifier(random_state=123456))]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","ebaa327b":"# GB Confusion Matrix\nmodel_XGB = GradientBoostingClassifier(random_state=123456)\nmodel_XGB.fit(X_train, y_train)\ny_pred = model_XGB.predict(X_test)\nconf_mat = confusion_matrix(y_pred,y_test)\nconf_mat","5d6ae4f0":"print(\"True Positive : \", conf_mat[1, 1])\nprint(\"True Negative : \", conf_mat[0, 0])\nprint(\"False Positive: \", conf_mat[0, 1])\nprint(\"False Negative: \", conf_mat[1, 0])","48303815":"print(classification_report(model_XGB.predict(X_test),y_test))","4ee0a22a":"def generate_auc_roc_curve(clf, X_test):\n    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n    pass","c608b840":"generate_auc_roc_curve(model_XGB, X_test)","92f75a12":"X.info()","defb504f":"# LightGBM: \nlgb_model = LGBMClassifier()\n# Model Tuning\nlgbm_params = lgbm_params = {\"learning_rate\": [0.01, 0.5, 1],\n                             \"n_estimators\": [200, 500, 1000],\n                             \"max_depth\": [6, 8, 10],\n                             \"colsample_bytree\": [1, 0.5, 0.4]}\nlgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X, y)\nlgbm_cv_model.best_params_\n# Final Model\nlgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X, y)","0985674d":"# XGB\nXGB_model = XGBClassifier()\n# Model Tuning\nxgb_params = {\n    \"learning_rate\": [0.1, 0.2, 1],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n    \"max_depth\":[3,5,8],\n    \"subsample\":[0.5, 1.0],\n    \"n_estimators\": [100,500]}\nxgb_cv_model  = GridSearchCV(XGB_model,\n                             xgb_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose = 2).fit(X,y)\nxgb_cv_model.best_params_\nxgb_tuned = XGBClassifier(**xgb_cv_model.best_params_).fit(X,y)","4c556941":"models = [(\"LightGBM\", lgbm_tuned),\n          (\"GB\",gbm_tuned)]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","5b9de9f2":"for name, model in models:\n        base = model.fit(X_train,y_train)\n        y_pred = base.predict(X_test)\n        acc_score = accuracy_score(y_test, y_pred)\n        feature_imp = pd.Series(base.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\n        sns.barplot(x=feature_imp, y=feature_imp.index)\n        plt.xlabel('De\u011fi\u015fken \u00d6nem Skorlar\u0131')\n        plt.ylabel('De\u011fi\u015fkenler')\n        plt.title(name)\n        plt.show()","f43e4311":"CreditScore","57c74ef8":"Modeling","0eb2e3ee":"* RowNumber: corresponds to the record (row) number and has no effect on the output.\n* CustomerId: contains random values and has no effect on customer leaving the bank.\n* Surname: the surname of a customer has no impact on their decision to leave the bank.\n* CreditScore: can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n* Geography: a customer\u2019s location can affect their decision to leave the bank.\n* Gender: it\u2019s interesting to explore whether gender plays a role in a customer leaving the bank.\n* Age: this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n* Tenure: refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n* Balance: also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n* NumOfProducts: refers to the number of products that a customer has purchased through the bank.\n* HasCrCard: denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n* IsActiveMember: active customers are less likely to leave the bank.\n* EstimatedSalary: as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n* Exited: whether or not the customer left the bank. (0=No,1=Yes)","3b961d7b":"Exited (DEpendent Variable)","032c58c5":"Correlation Matrix","ccb155ac":"Feature Engineering","26104256":"Tenure","9eaf01e0":"Missing Value","d27e51e2":"4 - Model Tuning","9e603b7a":"Numerical Variables","90dfaaa2":"Scalling ","8dd0742f":"# **Dataset**","75da5ef3":"2 - Data Preprocessing","025214c5":"Categorical Variables","9532377a":"# ****1-EDA****","c84946b5":"Modelleme","8b832ced":"One Hot Encoding ","c823d733":"Outliers","c76b9aa5":"Is Active Member "}}