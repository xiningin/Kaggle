{"cell_type":{"03028110":"code","beeede18":"code","2ad4df6a":"code","20b5b224":"code","054164d2":"code","e1397445":"code","6bdc448e":"code","407c35f4":"code","89fa1f6e":"code","102a696d":"code","097d4a83":"code","767c41bc":"code","420b5a64":"code","afe3bfc5":"code","b4b13119":"code","4e7a396f":"code","77d38feb":"code","c7671f15":"code","fb5078c2":"code","b0f70fe9":"code","a4429743":"code","762eb936":"code","95d1309d":"code","39b85e8d":"code","8dffb5f3":"code","6ba1273b":"code","14fc7ff4":"code","6e301cb4":"code","4e77003a":"code","025be791":"code","db732a17":"code","48c48b47":"code","c2129b52":"code","7b878ca4":"code","714f0159":"code","db1755c8":"code","c20a52db":"code","18d6c304":"code","5a9bf101":"code","702832cf":"code","1a4e561d":"code","ee40f15b":"code","7f08fedd":"code","a428407a":"code","0b3212d5":"code","533dd4f6":"code","d73332bf":"code","282873be":"code","970f1ee1":"code","8af7ad3c":"code","8d7c836a":"code","93394f06":"code","a924225d":"code","25ce2fba":"code","2779a3ad":"code","8596b9a2":"code","d474a917":"code","3d8426b5":"code","c269740b":"code","3c932855":"code","3f53c32d":"code","5b95359b":"code","66f94c84":"code","aa300459":"code","de71b09b":"code","6925c588":"code","3032f044":"code","80eb3348":"code","f08c3fe8":"code","34d4487f":"code","70319977":"code","18be85f3":"code","694dc5cc":"code","bfdf9bd1":"code","c2934c3b":"code","a40a84c7":"code","43fc265f":"code","357b6782":"code","2d27c505":"code","2454356c":"code","4742bd47":"code","95b178bd":"code","01d4b9c9":"code","ba3533bf":"code","a7951224":"code","c10d8e50":"code","03233d20":"code","05fe2ed4":"code","d393e1b8":"code","38ec443b":"code","f820db03":"code","9ade66e5":"code","b65dba6d":"code","773e87c8":"code","b0afad20":"code","4b22b521":"code","da1fb483":"code","8a93fb94":"code","552096f6":"code","f2f3f784":"code","771b0385":"code","c233e361":"code","9330796a":"code","2fcb67e0":"code","b251bafe":"code","af317841":"code","f72fbb93":"code","0b9b6936":"code","6908a9ea":"code","230655de":"code","75c9a4be":"code","50069324":"code","bd6efac0":"markdown","28fc763b":"markdown","13f1a5d3":"markdown","a48cf997":"markdown","e3557980":"markdown","251684a2":"markdown","3e090369":"markdown","4266f924":"markdown","a677d4de":"markdown","51664181":"markdown","e46de079":"markdown","6c074614":"markdown","f30ad041":"markdown","d4808b6e":"markdown","ee77c651":"markdown","5d9c9909":"markdown","5cc834fa":"markdown","9a95ec14":"markdown","5686fba5":"markdown","f951fb43":"markdown","ae3e42f5":"markdown","bf25662e":"markdown","4df95448":"markdown","87a1776a":"markdown","cd387c20":"markdown","2433d0d2":"markdown","ae33b421":"markdown","b9632028":"markdown","89d0b84e":"markdown","a5b40dc6":"markdown","37c58fca":"markdown","d3105167":"markdown","e183a476":"markdown","2d595b7f":"markdown","266454d9":"markdown","0bead6c7":"markdown","b16af30a":"markdown","1a6d547f":"markdown","7424caab":"markdown","f061d671":"markdown","0e2e3c49":"markdown","2b816244":"markdown","d3bc16ac":"markdown","ef5a9800":"markdown","6948959b":"markdown","dfcd33fc":"markdown","3844f24b":"markdown","adc6f501":"markdown","78c96489":"markdown","ea4f11e3":"markdown","568314db":"markdown","c4003445":"markdown","d903bc47":"markdown","58764e04":"markdown","b3c4dcdf":"markdown","19b37405":"markdown","8cca9ff0":"markdown","d46ac0e0":"markdown","3a13f901":"markdown"},"source":{"03028110":"# Importing libraries\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')","beeede18":"import warnings\nwarnings.filterwarnings('ignore')","2ad4df6a":"# Loading the train data\ndf = pd.read_csv('\/kaggle\/input\/customer\/Train.csv')\n\n# Looking top 10 rows\ndf.head(10)","20b5b224":"# Looking the bigger picture\ndf.info()","054164d2":"# Checking the number of missing values in each column\ndf.isnull().sum()","e1397445":"# Removing all those rows that have 3 or more missing values\ndf = df.loc[df.isnull().sum(axis=1)<3]","6bdc448e":"# Looking random 10 rows of the data\ndf.sample(10)","407c35f4":"print('The count of each category\\n',df.Var_1.value_counts())","89fa1f6e":"# Checking the count of null values\ndf.Var_1.isnull().sum()","102a696d":"# Filling the missing values w.r.t other attributes underlying pattern \ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'Yes'),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'No'),\"Var_1\"] = 'Cat_4'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & ((df['Profession'] == 'Lawyer') | (df['Profession'] == 'Artist')),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & (df['Age'] > 40),\"Var_1\"] = 'Cat_6'","097d4a83":"# Counting Var_1 in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Var_1\"].value_counts().unstack().round(3)\n\n# Percentage of category of Var_1 in each segment\nax2 = df.pivot_table(columns='Var_1',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","767c41bc":"print('The count of gender\\n',df.Gender.value_counts())","420b5a64":"# Checking the count of missing values\ndf.Gender.isnull().sum()","afe3bfc5":"# Counting male-female in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Gender\"].value_counts().unstack().round(3)\n\n# Percentage of male-female in each segment\nax2 = df.pivot_table(columns='Gender',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","b4b13119":"print('Count of married vs not married\\n',df.Ever_Married.value_counts())","4e7a396f":"# Checking the count of missing values\ndf.Ever_Married.isnull().sum()","77d38feb":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & ((df['Spending_Score'] == 'Average') | (df['Spending_Score'] == 'High')),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Spending_Score'] == 'Low'),\"Ever_Married\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Age'] > 40),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Profession'] == 'Healthcare'),\"Ever_Married\"] = 'No'","c7671f15":"# Counting married and non-married in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Ever_Married\"].value_counts().unstack().round(3)\n\n# Percentage of married and non-married in each segment\nax2 = df.pivot_table(columns='Ever_Married',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","fb5078c2":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","b0f70fe9":"# Checking the count of missing values\ndf.Age.isnull().sum()","a4429743":"# Looking the distribution of column Age\nplt.figure(figsize=(10,5))\n\nskewness = round(df.Age.skew(),2)\nkurtosis = round(df.Age.kurtosis(),2)\nmean = round(np.mean(df.Age),0)\nmedian = np.median(df.Age)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Age)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(1,2,2)\nsns.distplot(df.Age)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","762eb936":"# Looking the distribution of column Age w.r.t to each segment\na = df[df.Segmentation =='A'][\"Age\"]\nb = df[df.Segmentation =='B'][\"Age\"]\nc = df[df.Segmentation =='C'][\"Age\"]\nd = df[df.Segmentation =='D'][\"Age\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Age\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","95d1309d":"# Converting the datatype from float to int\ndf['Age'] = df['Age'].astype(int)","39b85e8d":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","8dffb5f3":"# Divide people in the 4 age group\ndf['Age_Bin'] = pd.cut(df.Age,bins=[17,30,45,60,90],labels=['17-30','31-45','46-60','60+'])","6ba1273b":"# Counting different age group in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Age_Bin\"].value_counts().unstack().round(3)\n\n# Percentage of age bins in each segment\nax2 = df.pivot_table(columns='Age_Bin',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(3)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","14fc7ff4":"print('Count of each graduate and non-graduate\\n',df.Graduated.value_counts())","6e301cb4":"# Checking the count of missing values\ndf.Graduated.isnull().sum()","4e77003a":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Spending_Score'] == 'Average'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Profession'] == 'Artist'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Age'] > 49),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Var_1'] == 'Cat_4'),\"Graduated\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Ever_Married'] == 'Yes'),\"Graduated\"] = 'Yes'\n\n# Replacing remaining NaN with previous values\ndf['Graduated'] = df['Graduated'].fillna(method='pad')","025be791":"# Counting graduate and non-graduate in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Graduated\"].value_counts().unstack().round(3)\n\n# Percentage of graduate and non-graduate in each segment\nax2 = df.pivot_table(columns='Graduated',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","db732a17":"print('Count of each profession\\n',df.Profession.value_counts())","48c48b47":"# Checking the count of missing values\ndf.Profession.isnull().sum()","c2129b52":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Work_Experience'] > 8),\"Profession\"] = 'Homemaker'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Age'] > 70),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Family_Size'] < 3),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'Average'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Graduated'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'No'),\"Profession\"] = 'Healthcare'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'High'),\"Profession\"] = 'Executives'","7b878ca4":"# Count of segments in each profession\nax1 = df.groupby([\"Profession\"])[\"Segmentation\"].value_counts().unstack().round(3)\n\n# Percentage of segments in each profession\nax2 = df.pivot_table(columns='Segmentation',index='Profession',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (16,5))\nlabel = ['Artist','Doctor','Engineer','Entertainment','Executives','Healthcare','Homemaker','Lawyer','Marketing']\nax[0].set_xticklabels(labels = label,rotation = 45)\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (16,5))\nax[1].set_xticklabels(labels = label,rotation = 45)\n\nplt.show()","714f0159":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","db1755c8":"# Checking the count of missing values\ndf.Work_Experience.isnull().sum()","c20a52db":"# Filling NaN with previous values\ndf['Work_Experience'] = df['Work_Experience'].fillna(method='pad')","18d6c304":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Work_Experience.skew(),2)\nkurtosis = round(df.Work_Experience.kurtosis(),2)\nmean = round(np.mean(df.Work_Experience),0)\nmedian = np.median(df.Work_Experience)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Work_Experience)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Work_Experience)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","5a9bf101":"# Looking the distribution of column Work_Experience w.r.t to each segment\na = df[df.Segmentation =='A'][\"Work_Experience\"]\nb = df[df.Segmentation =='B'][\"Work_Experience\"]\nc = df[df.Segmentation =='C'][\"Work_Experience\"]\nd = df[df.Segmentation =='D'][\"Work_Experience\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Work_Experience\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Work Experience')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","702832cf":"# Changing the data type\ndf['Work_Experience'] = df['Work_Experience'].astype(int)","1a4e561d":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","ee40f15b":"# Dividing the people into 3 category of work experience \ndf['Work_Exp_Category'] = pd.cut(df.Work_Experience,bins=[-1,1,7,15],labels=['Low Experience','Medium Experience','High Experience'])","7f08fedd":"# Counting different category of work experience in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Work_Exp_Category\"].value_counts().unstack().round(3)\n\n# Percentage of work experience in each segment\nax2 = df.pivot_table(columns='Work_Exp_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(3)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","a428407a":"print('Count of spending score\\n',df.Spending_Score.value_counts())","0b3212d5":"# Checking the count of missing values\ndf.Spending_Score.isnull().sum()","533dd4f6":"# Counting different category of spending score in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Spending_Score\"].value_counts().unstack().round(3)\n\n# Percentage of spending score in each segment\nax2 = df.pivot_table(columns='Spending_Score',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(3)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","d73332bf":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","282873be":"# Checking the count of missing values\ndf.Family_Size.isnull().sum()","970f1ee1":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Ever_Married'] == 'Yes'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Var_1'] == 'Cat_6'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Graduated'] == 'Yes'),\"Family_Size\"] = 2.0\n\n# Fill remaining NaN with previous values\ndf['Family_Size'] = df['Family_Size'].fillna(method='pad')","8af7ad3c":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Family_Size.skew(),2)\nkurtosis = round(df.Family_Size.kurtosis(),2)\nmean = round(np.mean(df.Family_Size),0)\nmedian = np.median(df.Family_Size)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Family_Size)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Family_Size)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","8d7c836a":"# Looking the distribution of column Family Size w.r.t to each segment\na = df[df.Segmentation =='A'][\"Family_Size\"]\nb = df[df.Segmentation =='B'][\"Family_Size\"]\nc = df[df.Segmentation =='C'][\"Family_Size\"]\nd = df[df.Segmentation =='D'][\"Family_Size\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Family_Size\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Family Size')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","93394f06":"# Changing the data type\ndf['Family_Size'] = df['Family_Size'].astype(int)","a924225d":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","25ce2fba":"# Divide family size into 3 category\ndf['Family_Size_Category'] = pd.cut(df.Family_Size,bins=[0,4,6,10],labels=['Small Family','Big Family','Joint Family'])","2779a3ad":"# Counting different category of family size in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Family_Size_Category\"].value_counts().unstack().round(3)\n\n# Percentage of family size in each segment\nax2 = df.pivot_table(columns='Family_Size_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","8596b9a2":"print('Count of each category of segmentation\\n',df.Segmentation.value_counts())","d474a917":"segments = df.loc[:,\"Segmentation\"].value_counts()\nplt.xlabel(\"Segment\")\nplt.ylabel('Count')\nsns.barplot(segments.index , segments.values).set_title('Segments')\nplt.show()","3d8426b5":"df.reset_index(drop=True, inplace=True)\ndf.info()","c269740b":"# number of unique ids\ndf.ID.nunique()","3c932855":"df.describe(include='all')","3f53c32d":"df = df[['ID','Gender', 'Ever_Married', 'Age', 'Age_Bin', 'Graduated', 'Profession', 'Work_Experience', 'Work_Exp_Category',\n         'Spending_Score', 'Family_Size', 'Family_Size_Category','Var_1', 'Segmentation']]\ndf.head(10)","5b95359b":"# Saving the file\n#df.to_csv('cleaned_train.csv')","66f94c84":"df1 = df.copy()\ndf1.head()","aa300459":"# Separating dependent-independent variables\nX = df1.drop('Segmentation',axis=1)\ny = df1['Segmentation']","de71b09b":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf1_trainX, df1_testX, df1_trainY, df1_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","6925c588":"# converting binary variables to numeric\ndf1_trainX['Gender'] = df1_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf1_trainX['Ever_Married'] = df1_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_trainX['Graduated'] = df1_trainX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_trainX['Spending_Score'] = df1_trainX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_trainX.Profession,prefix='Profession')\ndf1_trainX = pd.concat([df1_trainX,pf],axis=1)\n\nvr = pd.get_dummies(df1_trainX.Var_1,prefix='Var_1')\ndf1_trainX = pd.concat([df1_trainX,vr],axis=1)\n\n# scaling continuous variables\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1_trainX[['Age','Work_Experience','Family_Size']] = scaler.fit_transform(df1_trainX[['Age','Work_Experience','Family_Size']])\n\ndf1_trainX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","3032f044":"# converting binary variables to numeric\ndf1_testX['Gender'] = df1_testX['Gender'].replace(('Male','Female'),(1,0))\ndf1_testX['Ever_Married'] = df1_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_testX['Graduated'] = df1_testX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_testX['Spending_Score'] = df1_testX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_testX.Profession,prefix='Profession')\ndf1_testX = pd.concat([df1_testX,pf],axis=1)\n\nvr = pd.get_dummies(df1_testX.Var_1,prefix='Var_1')\ndf1_testX = pd.concat([df1_testX,vr],axis=1)\n\n# scaling continuous variables\ndf1_testX[['Age','Work_Experience','Family_Size']] = scaler.transform(df1_testX[['Age','Work_Experience','Family_Size']])\n\ndf1_testX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","80eb3348":"df1_trainX.shape, df1_trainY.shape, df1_testX.shape, df1_testY.shape","f08c3fe8":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df1_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","34d4487f":"df2 = df.copy()\ndf2.head()","70319977":"# Separating dependent-independent variables\nX = df2.drop('Segmentation',axis=1)\ny = df2['Segmentation']","18be85f3":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf2_trainX, df2_testX, df2_trainY, df2_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","694dc5cc":"# Converting binary to numeric\ndf2_trainX['Gender'] = df2_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf2_trainX['Ever_Married'] = df2_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_trainX['Graduated'] = df2_trainX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_trainX.Age_Bin,prefix='Age_Bin')\ndf2_trainX = pd.concat([df2_trainX,ab],axis=1)\n\npf = pd.get_dummies(df2_trainX.Profession,prefix='Profession')\ndf2_trainX = pd.concat([df2_trainX,pf],axis=1)\n\nwe = pd.get_dummies(df2_trainX.Work_Exp_Category,prefix='WorkExp')\ndf2_trainX = pd.concat([df2_trainX,we],axis=1)\n\nsc = pd.get_dummies(df2_trainX.Spending_Score,prefix='Spending')\ndf2_trainX = pd.concat([df2_trainX,sc],axis=1)\n\nfs = pd.get_dummies(df2_trainX.Family_Size_Category,prefix='FamilySize')\ndf2_trainX = pd.concat([df2_trainX,fs],axis=1)\n\nvr = pd.get_dummies(df2_trainX.Var_1,prefix='Var_1')\ndf2_trainX = pd.concat([df2_trainX,vr],axis=1)\n\ndf2_trainX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","bfdf9bd1":"# Converting binary to numeric\ndf2_testX['Gender'] = df2_testX['Gender'].replace(('Male','Female'),(1,0))\ndf2_testX['Ever_Married'] = df2_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_testX['Graduated'] = df2_testX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_testX.Age_Bin,prefix='Age_Bin')\ndf2_testX = pd.concat([df2_testX,ab],axis=1)\n\npf = pd.get_dummies(df2_testX.Profession,prefix='Profession')\ndf2_testX = pd.concat([df2_testX,pf],axis=1)\n\nwe = pd.get_dummies(df2_testX.Work_Exp_Category,prefix='WorkExp')\ndf2_testX = pd.concat([df2_testX,we],axis=1)\n\nsc = pd.get_dummies(df2_testX.Spending_Score,prefix='Spending')\ndf2_testX = pd.concat([df2_testX,sc],axis=1)\n\nfs = pd.get_dummies(df2_testX.Family_Size_Category,prefix='FamilySize')\ndf2_testX = pd.concat([df2_testX,fs],axis=1)\n\nvr = pd.get_dummies(df2_testX.Var_1,prefix='Var_1')\ndf2_testX = pd.concat([df2_testX,vr],axis=1)\n\ndf2_testX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","c2934c3b":"df2_trainX.shape, df2_trainY.shape, df2_testX.shape, df2_testY.shape","a40a84c7":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df2_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","43fc265f":"train_ovr1_x = df1_trainX.copy()\ntrain_ovr1_x.head()","357b6782":"train_ovr1_y = df1_trainY.copy()\ntrain_ovr1_y.head()","2d27c505":"# Importing the library\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\n# Creating OvR object\novr1 = OneVsRestClassifier(LinearSVC(random_state=0))\n\n# Train model\nmodel_ovr1 = ovr1.fit(train_ovr1_x, train_ovr1_y)\n\n# Predicting the classes\nyhat1 = ovr1.predict(train_ovr1_x)\n\n# Looking at the coefficients of variables \n#print('-------Coefficient of variables obtained from each of the 4 models ------')\n#print(model_ovr1.coef_)\n\n# Looking at the intercepts \n#print('-------Intercept of each of the 4 models ------')\n#print(model_ovr1.intercept_)\n\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(train_ovr1_y.values, yhat1, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('\\n\\n-------The confusion matrix for this model is-------')\nprint(cm1)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of the model-------')\nprint(classification_report(train_ovr1_y.values, yhat1))","2454356c":"test_ovr1_x = df1_testX.copy()\ntest_ovr1_x.head()","4742bd47":"test_ovr1_y = df1_testY.copy()\ntest_ovr1_y.head()","95b178bd":"y_ovr1 = ovr1.predict(test_ovr1_x)\ny_ovr1","01d4b9c9":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_ovr1_y.values, y_ovr1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_ovr1_y.values, y_ovr1))","ba3533bf":"pd.Series(y_ovr1).value_counts()","a7951224":"train_ovr2_x = df2_trainX.copy()\ntrain_ovr2_x.head()","c10d8e50":"train_ovr2_y = df2_trainY.copy()\ntrain_ovr2_y.head()","03233d20":"# Importing the library\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\n# Creating OvR object\novr2 = OneVsRestClassifier(LinearSVC(random_state=0))\n\n# Train model\nmodel_ovr2 = ovr2.fit(train_ovr2_x, train_ovr2_y)\n\n# Predicting the classes\nyhat2 = ovr2.predict(train_ovr2_x)\n\n# Looking at the coefficients of variables \n#print('-------Coefficient of variables obtained from each of the 4 models------')\n#print(model_ovr2.coef_)\n\n# Looking at the intercepts \n#print('\\n-------Intercept of each of the 4 models ------')\n#print(model_ovr2.intercept_)\n\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(train_ovr2_y.values, yhat2, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('\\n\\n-------The confusion matrix for this model is-------')\nprint(cm2)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_ovr2_y.values, yhat2))","05fe2ed4":"test_ovr2_x = df2_testX.copy()\ntest_ovr2_x.head()","d393e1b8":"test_ovr2_y = df2_testY.copy()\ntest_ovr2_y.head()","38ec443b":"y_ovr2 = ovr2.predict(test_ovr2_x)\ny_ovr2","f820db03":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_ovr2_y.values, y_ovr2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_ovr2_y.values, y_ovr2))","9ade66e5":"pd.Series(y_ovr2).value_counts()","b65dba6d":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_ovr1_y.values, yhat1))\nprint('\\nTest data')\nprint(classification_report(test_ovr1_y.values, y_ovr1))","773e87c8":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_ovr2_y.values, yhat2))\nprint('\\nTest data')\nprint(classification_report(test_ovr2_y.values, y_ovr2))","b0afad20":"train_ovo1_x = df1_trainX.copy()\ntrain_ovo1_x.head()","4b22b521":"train_ovo1_y = df1_trainY.copy()\ntrain_ovo1_y.head()","da1fb483":"# Importing the library\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import LinearSVC\n\n# Creating OvO object\novo1 = OneVsOneClassifier(LinearSVC(random_state=0))\n\n# Train model\nmodel_ovo1 = ovo1.fit(train_ovo1_x, train_ovo1_y)\n\n# Predicting the classes\nyhat3 = ovo1.predict(train_ovo1_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(train_ovo1_y.values, yhat3, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm3)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_ovo1_y.values, yhat3))","8a93fb94":"test_ovo1_x = df1_testX.copy()\ntest_ovo1_x.head()","552096f6":"test_ovo1_y = df1_testY.copy()\ntest_ovo1_y.head()","f2f3f784":"y_ovo1 = ovo1.predict(test_ovo1_x)\ny_ovo1","771b0385":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_ovo1_y.values, y_ovo1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_ovo1_y.values, y_ovo1))","c233e361":"pd.Series(y_ovo1).value_counts()","9330796a":"train_ovo2_x = df2_trainX.copy()\ntrain_ovo2_x.head()","2fcb67e0":"train_ovo2_y = df2_trainY.copy()\ntrain_ovo2_y.head()","b251bafe":"# Importing the library\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import LinearSVC\n\n# Creating OvO object\novo2 = OneVsOneClassifier(LinearSVC(random_state=0))\n\n# Train model\nmodel_ovo2 = ovo2.fit(train_ovo2_x, train_ovo2_y)\n\n# Predicting the classes\nyhat4 = ovo2.predict(train_ovo2_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(train_ovo2_y.values, yhat4, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm4)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_ovo2_y.values, yhat4))","af317841":"test_ovo2_x = df2_testX.copy()\ntest_ovo2_x.head()","f72fbb93":"test_ovo2_y = df2_testY.copy()\ntest_ovo2_y.head()","0b9b6936":"y_ovo2 = ovo2.predict(test_ovo2_x)\ny_ovo2","6908a9ea":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_ovo2_y.values, y_ovo2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_ovo2_y.values, y_ovo2))","230655de":"pd.Series(y_ovo2).value_counts()","75c9a4be":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_ovo1_y.values, yhat3))\nprint('\\nTest data')\nprint(classification_report(test_ovo1_y.values, y_ovo1))","50069324":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_ovo2_y.values, yhat4))\nprint('\\nTest data')\nprint(classification_report(test_ovo2_y.values, y_ovo2))","bd6efac0":"###### Age","28fc763b":"#### Building the model with `first_type` of dataframe(df_type1)","13f1a5d3":"###### Graduated","a48cf997":"#### Is ACCURACY everything? \nIn general, there is no general best measure. The best measure is derived from your needs. `In a sense, it is not a machine learning question, but a business question`. It is common that two people will use the same data set but will choose different metrics due to different goals.\n<br><br>\nAccuracy is a great metric. Actually, most metrics are great and I like to evaluate many metrics. However, at some point you will need to decide between using model A or B. There you should use a single metric that best fits your need.<br><br>\nRead more: https:\/\/towardsdatascience.com\/is-accuracy-everything-96da9afd540d","e3557980":"###### Preprocessing in test data","251684a2":"---\n---\n### II. One-Vs-One ","3e090369":"<font color='blue'>Segment D has people with relatively more experienced than other segments while Segment C has people with low experience","4266f924":"**Ways to treat missing values**<br>\nCheck here:https:\/\/www.datasciencenovice.com\/2020\/08\/5-ways-to-treat-missing-values.html","a677d4de":"###### Preprocessing in train data","51664181":"---\n## <font color='orange'>Step II: Model Building","e46de079":"<font color='blue'>Now all the data has been cleaned. There is no missing value and columns are in right format. <br>\n<font color='blue'>All the ids are unique that is there is no duplicate entry.<br>\n<font color='blue'>Created new column: 'Age_Bin', 'Work_Exp_Category' and 'Family_Size_Category'. <br> \n<font color='blue'>Delete only 0.2% of rows. ","6c074614":"<font color='blue'>In each of the segment the count of cat_6 or proportion of cat_6 is very high i.e. most of the entries in the given data belongs to cat_6.","f30ad041":"###### Spending Score","d4808b6e":"#### Observation:\n<font color='blue'>1. In train the model gave accuracy of 0.52 while in test it is 0.50, hardly a difference.<br>\n<font color='blue'>2. This model is good only for segment C and D as their recall is very good and close in both train and test.<br>\n<font color='blue'>3. But this second model is better than the previous model as this predicts better results for segment B.<br>\n<font color='blue'>4. So we can say that `model-2 is better` than model-1 in OvR technique.","ee77c651":"###### Family Size","5d9c9909":"#### Building the model using `second type` dataframe(df_type2)","5cc834fa":"###### Gender","9a95ec14":"---\n## <font color='orange'>Step III: Model Evaluation","5686fba5":"<font color='blue'>1. We have seen that there are `missing values` in the dataset. So we will work on data cleaning.<br>\n<font color='blue'>2. `Create some new attributes` based upon given data\/domain knowledge\/prior experience.<br>\n<font color='blue'>3. Create graphs and `performs EDA` and write observations.","f951fb43":"<font color='blue'>All the 4 segments have around same number of male-female distribution. In all segment male are more than female. <br> \n<font color='blue'>But segment D has highest male percentage as compared to other segments.","ae3e42f5":"### Variables Description\n\n           \n| Variable\t            | Definition                                                        |\n|---------------------- |-------------------------------------------------------------------|\n| ID\t                | Unique ID                                                         |\n| Gender\t            | Gender of the customer                                            |\n| Ever_Married\t        | Marital status of the customer                                    |\n| Age\t                | Age of the customer                                               |\n| Graduated\t            | Is the customer a graduate?                                       |\n| Profession\t        | Profession of the customer                                        |\n| Work_Experience\t    | Work Experience in years                                          |\n| Spending_Score\t    | Spending score of the customer                                    |\n| Family_Size\t        | Number of family members for the customer(including the customer) |\n| Var_1\t                | Anonymised Category for the customer                              |\n| Segmentation(target)  | Customer Segment of the customer                                  |","bf25662e":"###### Var_1","4df95448":"![image.png](attachment:image.png)","87a1776a":"###### Preprocessing on train data","cd387c20":"### I. One-Vs-Rest","2433d0d2":"---\n## <font color='orange'>Step I: Importing, Cleaning and EDA","ae33b421":"###### Preprocessing on test data","b9632028":"#### Predicting on test set","89d0b84e":"<font color='blue'>Segment C has most number of customers who are graduated while segment D has lowest number of graduate customers.","a5b40dc6":"- One-vs-rest (OvR for short, also referred to as One-vs-All or OvA) is a heuristic method for using binary classification algorithms for multi-class classification.\n- It involves splitting the multi-class dataset into multiple binary classification problems. A binary classifier is then trained on each binary classification problem and predictions are made using the model that is the most confident.\n- For example, here we are given a multi-class classification problem with examples for each class 'A', 'B', 'C' and 'D'. This could be divided into four binary classification datasets as follows:\n    - Binary Classification Problem 1: A vs [B,C,D]\n    - Binary Classification Problem 2: B vs [A,C,D]\n    - Binary Classification Problem 3: C vs [A,B,D]\n    - Binary Classification Problem 4: D vs [A,B,C]\n- A possible downside of this approach is that it requires one model to be created for each class. For example, three classes requires three models. This could be an issue for large datasets (e.g. millions of rows), slow models (e.g. neural networks), or very large numbers of classes (e.g. hundreds of classes).\n- This approach requires that each model predicts a class membership probability or a probability-like score. The argmax of these scores (class index with the largest score) is then used to predict a class. This approach is commonly used for algorithms that naturally predict numerical class membership probability or score, such as:\n    - Logistic Regression\n    - Perceptron","37c58fca":"#### Predicting on test set","d3105167":"## Multi Class Classification \n- A classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-class classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. \n- Common examples include image classification (is it a cat, dog, human, etc) or handwritten digit recognition (classifying an image of a handwritten number into a digit from 0 to 9).\n- In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).\n- Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.","e183a476":"<font color='blue'>The mean age of segment D is 33 and we can say that people in this segment are belong to 30s i.e. they are younger and also from 'ever_married' distribution it is seen that segment D has maximum number of customers who are singles indicating they are younger.<br>\n<font color='blue'>Also segment C has mean age of 49 and we also seen that most cutomers in this segment are married. ","2d595b7f":"## Algorithms Covered\n\n1. In this notebook, we are going to use **One vs Rest(OvR)** and **One vs One(OvO)** algorithms to solve the problem.\n\n2. In second notebook, we are going to use **Decision Tree** and **Random Forest** algorithms to solve the same problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c102?scriptVersionId=43468547)\n\n3. In the last notebook, we are going to use **k-NN** and **Naive Bayes** algorithms to solve the same problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c103?scriptVersionId=43468336)\n\n**Note**:The EDA process is same in all the three notebooks. The only change is in algorithm to solve the problem.","266454d9":"- One-vs-One (OvO for short) is another heuristic method for using binary classification algorithms for multi-class classification. Like one-vs-rest, one-vs-one splits a multi-class classification dataset into binary classification problems. Unlike one-vs-rest that splits it into one binary dataset for each class, the one-vs-one approach splits the dataset into one dataset for each class versus every other class.\n- For example, this problem is a multi-class classification problem with four classes: A, B, C and D. This could be divided into six binary classification datasets as follows:\n    - Binary Classification Problem 1: A vs B\n    - Binary Classification Problem 2: A vs C\n    - Binary Classification Problem 3: A vs D\n    - Binary Classification Problem 4: B vs C\n    - Binary Classification Problem 5: B vs D\n    - Binary Classification Problem 6: C vs D\n- When predicting new points, each classifier votes on the class of the point, and the class with the most votes is chosen as the winner. In the event of a tie, you may select the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. \n- The formula for calculating the number of binary datasets, and in turn, models, is as follows:`(NumClasses * (NumClasses \u2013 1)) \/ 2`\n- One-versus-one classifiers are both more computationally expensive, requiring far more classifiers to be trained, and less immediately interpretable. However, if the classifier being used scales poorly, and the dataset is sufficiently large, training this many two-class classifiers may be faster or provide better results than classification in the one-versus-rest scheme, which considers every point.","0bead6c7":"#### Training data","b16af30a":"###### Work Experience","1a6d547f":"###### Segmentation","7424caab":"<font color='blue'>Segment A,B and C have major customers from profession:**Artist** while Segment D have major customers from profession:**Healthcare**. <br>\n**Homemaker** is least in all the four segment.","f061d671":"![image.png](attachment:image.png)","0e2e3c49":"#### Predicting on test set","2b816244":"### Problem Statement\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n<br><br>\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n<br><br>\nYou are required to help the manager to predict the right group of the new customers.<br><br>\nYou can check this link: https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/","d3bc16ac":"<font color='blue'>1. Both the reports are very similar in terms of accuracy, precision, recall and f1-score.<br>\n<font color='blue'>2. We can see `model-2 is better` as to model-1 because it has better results of segment A. While we can see `model-1 is better` as to model-2 because it has better results of segment B.<br>\n<font color='blue'>3. So we can't really say which model is best for model building in OvO technique. ","ef5a9800":"<font color='blue'>Segment D has maximum number of people with low spending score while in Segment C average spending people are more.","6948959b":"#### Building the model with `second type` of dataframe (df_type2)","dfcd33fc":"##### Observation:\n1. `Age` and `Ever_Married` has a positive correlation of 0.6 which means that people who are married have more age as compared to those who are unmarried.\n2. `Age` and `Profession_Healthcare` has a negative correlation of 0.5 which means all those people whose profession is healthcare are younger in age to those who of other professions people.\n3. `Profession_Healthcare` and `Ever_Married` has negative correlation of 0.42 which means all those peoples whose profession is healthcare are unmarried.(only 13% of healthcare professionals are married).\n4. `Age` and `Profession_Lawyer` has a positive correlation of 0.42 which means all those people whose profession is lawyer are older in age to those of other professions people.\n5. `Ever_Married` and `Spending_Average` has a positive correlation which means those who are married spend averagely.(around 42% married people spent averagely)\n6. `Ever_Married` and `Spending_High` has a little positive correlation which means those who are married spend high.(around 25% of married people spent high)\n7. `Ever_Married` and `Spending_Low` has a negative correlation of 0.67 which means those who are unmarried spent low.(round 99% of unmarried people spent low )\n8. `Age` and `Spending_Score` has a positive correlation of 0.42 which means as age increase the spending power also increase.\n9. `Profession_Executives` and `Spending_High` has positive correlation of 0.40 which means all those peoples whose profession is executive spent high.(around 66% of executives spent high).","3844f24b":"#### <font color='red'>Making two different dataframes\n<font color='red'>Now we consider\/make two different dataframes apart from the above main dataframe (namely df) <br>\n- `df1`: Spending Score(ranking), Age(normalise), Work_Experience(normalise), Family Size(normalise)\n- `df2`: Spending Score(dummy variables), Age Bin(dummy variables), Work_Exp_Category(dummy variables), Family_Size_Category(dummy variables)\n","adc6f501":"<font color='blue'>We seen that most of the customers in segment C are married while segment D has least number of married customers. It means segment D is a group of customers that are mostly singles and maybe younger in age. ","78c96489":"---\n## <font color='orange'>Step III: Model Evaluation","ea4f11e3":"#### Observation:\n<font color='blue'>1. In train the model gave accuracy of 0.5 while in test it is 0.49, hardly a difference.<br>\n<font color='blue'>2. This model is good only for segment C and D as their recall is very good and close in both train and test. ","568314db":"<font color='blue'>In the given data it is observed that most of the people have family size of 1 or 2 (i.e. they have small family).<br> But Segment D has more number of  big families as compared to other segments. ","c4003445":"###### Ever Married","d903bc47":"## <font color='orange'>Step II: Model Building","58764e04":"#### Building the model with `first type` of dataframe (df_type1)","b3c4dcdf":"#### Predicting on test set","19b37405":"###### Profession","8cca9ff0":"## <font color = 'blue'>Topics Covered in this notebook<\/font>\n1. Basic cleaning and EDA\n2. One vs Rest Classifier\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm\n3. One vs One Classifier\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm","d46ac0e0":"##### Why Spearman?\nCheck this: https:\/\/idkwhoneedstohearthis.blogspot.com\/2020\/05\/correlation-why-spearmans.html","3a13f901":"<font color='blue'>`Recall` - what proportion of actual Positives is correctly classified <br>\n`Precision` - what proportion of predicted Positives is truly Positive <br>\n`Accuracy` - what proportion of both Positive and Negative were correctly classified <br>\n`f1-score` - Now imagine that you have two classifiers \u2014 classifier A and classifier B \u2014 each with its own precision and recall. One has a better recall score, the other has better precision. We would like to say something about their relative performance. In other words, we would like to summarize the models\u2019 performance into a single metric. That\u2019s where F1-score are used. It\u2019s a way to combine precision and recall into a single number. <br>\n    **F1-score = 2 \u00d7 (precision \u00d7 recall)\/(precision + recall)**"}}