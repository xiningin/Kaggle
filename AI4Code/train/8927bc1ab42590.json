{"cell_type":{"e53e153e":"code","c5629967":"code","da777f5a":"code","9f0b7eac":"code","9e6a574f":"code","39f087c5":"code","7b35b0d7":"code","8de64311":"code","a048d6ea":"code","7df6298b":"markdown","9b73fc7d":"markdown","e998b365":"markdown","fbf5df48":"markdown","449ae1bc":"markdown","67ad66b3":"markdown","5cb27d96":"markdown","b64828dd":"markdown","ba7bf0c0":"markdown"},"source":{"e53e153e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os","c5629967":"class TimeSeriesData():\n    def __init__(self, num_points, xmin, xmax):\n        # TimeSeries Data being built with Sine function.\n        self.xmin = xmin\n        self.xmax = xmax\n        self.num_points = num_points\n        self.resolution = (xmax- xmin) \/ num_points\n        self.x_data = np.linspace(xmin, xmax, num_points)\n        self.y_true = np.sin(self.x_data)\n    \n    def return_truth (self, xseries):\n        return np.sin(xseries)\n    # The next_batch function is used to return data from the data generated by TimeSeriesData constructor.The ts_start\n    # values are brought to the timeseries data range. This is used down the kernel.\n    def next_batch(self, batch_size, steps, return_batch_ts = False):\n        rand_start = np.random.rand(batch_size, 1)\n        ts_start = rand_start * (self.xmax - self.xmin - (steps*self.resolution))\n        batch_ts = ts_start + np.arange(0, steps + 1 ) * self.resolution\n        y_batch = np.sin(batch_ts)\n        if return_batch_ts:\n            return(y_batch[:,:-1].reshape(-1, steps, 1), y_batch[:,1:].reshape(-1, steps, 1), batch_ts)\n        else:\n            return(y_batch[:,:-1].reshape(-1, steps, 1), y_batch[:,1:].reshape(-1, steps, 1))","da777f5a":"ts_data = TimeSeriesData(250,0,10) # Creating 250 equally spaced points between 0 and 10\nplt.plot(ts_data.x_data, ts_data.y_true) # Looking at the data generated above.","9f0b7eac":"num_time_steps = 30 \ntrain_inst =np.linspace(5, 5 + ts_data.resolution*(num_time_steps+1), num_time_steps + 1)\nplt.title = \"A Training Instance\"\nplt.plot(train_inst[:-1,], ts_data.return_truth(train_inst[:-1]), 'bo', markersize = 15, label = \"training instance\")\nplt.plot(train_inst[1:], ts_data.return_truth(train_inst[1:]), 'ko', markersize = 5, label = \"test instance\")\nplt.legend()","9e6a574f":"tf.reset_default_graph()\nnum_inputs = 1\nnum_outputs = 1\nnum_neurons = 100\nlearning_rate = 0.001\nnum_train_iterations = 200\nbatch_size = 1\nX = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\ny = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs]) \nseq_length = tf.placeholder(tf.int32, [None])\ncell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, activation = tf.nn.relu), output_size = num_outputs)\noutputs, states = tf.nn.dynamic_rnn( cell, X,  dtype=tf.float32)\n# Mean Squared Error \nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain = optimizer.minimize(loss)","39f087c5":"# Session \ninit = tf.global_variables_initializer()\n#saver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(init)\n    for iteration in range(num_train_iterations):\n        X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps)\n        sess.run(train, feed_dict = {X:X_batch, y:y_batch})\n        if iteration % 100 == 0:\n            mse = loss.eval(feed_dict = {X:X_batch, y:y_batch})\n            print(iteration, \"\\tMSE:\", mse)\n#saver.save(sess, \".\/testrnn\")\n    # USING THE SAME SESSION WHICH WAS OPENED ABOVE.\n    X_new = np.sin(np.array(train_inst[:-1].reshape(-1, num_time_steps, num_inputs)))\n    y_pred = sess.run(outputs, feed_dict = {X:X_new})\n    print(\"done\")\n","7b35b0d7":"# TESTING THE MODEL\n#plt.title(\"Model Testing\")\nplt.plot(train_inst[:-1], np.sin(train_inst[:-1]), \"bo\", markersize = 15, alpha = 0.3, label = \"training Instance\")\n# Prediction\nplt.plot(train_inst[1:], np.sin(train_inst[1:]), \"ko\", markersize = 15, alpha = 0.3, label = \"target\")\n# Model Prediction\nplt.plot(train_inst[1:], y_pred[0,:,0],'r.', markersize =10, label = \"PREDICTIONS\")\nplt.legend()\nplt.tight_layout()","8de64311":"tf.reset_default_graph()\nnum_inputs = 1\nnum_outputs = 1\nnum_neurons = 100\nlearning_rate = 0.001\nnum_train_iterations = 1000\nbatch_size = 1\nX = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\ny = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs]) \nseq_length = tf.placeholder(tf.int32, [None])\ncell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, activation = tf.nn.relu), output_size = num_outputs)\noutputs, states = tf.nn.dynamic_rnn( cell, X,  dtype=tf.float32)\n# Mean Squared Error \nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain = optimizer.minimize(loss)\n# Mean Squared Error \nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain = optimizer.minimize(loss)\n# Session \ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(init)\n    for iteration in range(num_train_iterations):\n        X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps)\n        sess.run(train, feed_dict = {X:X_batch, y:y_batch})\n        if iteration % 100 == 0:\n            mse = loss.eval(feed_dict = {X:X_batch, y:y_batch})\n            print(iteration, \"\\tMSE:\", mse)\n#saver.save(sess, \".\/testrnn\")\n    # USING THE SAME SESSION WHICH WAS OPENED ABOVE.\n    X_new = np.sin(np.array(train_inst[:-1].reshape(-1, num_time_steps, num_inputs)))\n    y_pred = sess.run(outputs, feed_dict = {X:X_new})\n    print(\"Training done.\")\n\n    # TESTING THE MODEL\nplt.plot(train_inst[:-1], np.sin(train_inst[:-1]), \"bo\", markersize = 15, alpha = 0.3, label = \"training Instance\")\n# Prediction\nplt.plot(train_inst[1:], np.sin(train_inst[1:]), \"ko\", markersize = 12, alpha = 0.3, label = \"target\")\nplt.plot(train_inst[1:], y_pred[0,:,0],'r.', markersize =10, label = \"PREDICTIONS\")\nplt.legend()\nplt.tight_layout()","a048d6ea":"tf.reset_default_graph()\nnum_inputs = 1\nnum_outputs = 1\nnum_neurons = 100\nlearning_rate = 0.001\nnum_train_iterations = 1000\nbatch_size = 1\nX = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\ny = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs]) \nseq_length = tf.placeholder(tf.int32, [None])\ncell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.GRUCell(num_units = num_neurons, activation = tf.nn.relu), output_size = num_outputs)\noutputs, states = tf.nn.dynamic_rnn( cell, X,  dtype=tf.float32)\n# Mean Squared Error \nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain = optimizer.minimize(loss)\n# Mean Squared Error \nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain = optimizer.minimize(loss)\n# Session \ninit = tf.global_variables_initializer()\n#saver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(init)\n    for iteration in range(num_train_iterations):\n        X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps)\n        sess.run(train, feed_dict = {X:X_batch, y:y_batch})\n        if iteration % 100 == 0:\n            mse = loss.eval(feed_dict = {X:X_batch, y:y_batch})\n            print(iteration, \"\\tMSE:\", mse)\n#saver.save(sess, \".\/testrnn\")\n    # USING THE SAME SESSION WHICH WAS OPENED ABOVE.\n    X_new = np.sin(np.array(train_inst[:-1].reshape(-1, num_time_steps, num_inputs)))\n    y_pred = sess.run(outputs, feed_dict = {X:X_new})\n    print(\"Training done.\")\n\n# TESTING THE MODEL\n\nplt.plot(train_inst[:-1], np.sin(train_inst[:-1]), \"bo\", markersize = 15, alpha = 0.3, label = \"training Instance\")\n# Prediction\nplt.plot(train_inst[1:], np.sin(train_inst[1:]), \"ko\", markersize = 12, alpha = 0.3, label = \"target\")\n# Model Prediction\nplt.plot(train_inst[1:], y_pred[0,:,0],'r.', markersize = 10, label = \"PREDICTIONS\")\nplt.legend()\nplt.tight_layout()\n#mse_pred = (y_pred - np.sin(train_inst[1:])) ^2","7df6298b":"###  Generating data and basic exploration","9b73fc7d":"### Result\n\nWe see that with number of iterations as only 200, the prediction done is not pretty good. We also observe here that first few instances of batch, the prediction was off the mark by a great distance, but later on, it became better.","e998b365":"\n## Model Building \n\n### Parameters and Structure\nVarious parameters are used when building a model in Tensorflow.\n\n* num of inputs - Is the number of input given to a Neural Network unit.\n* number of outputs - Number of points which the NN is supposed to output.\n* number of neurons : number of cells required in the neural network.\n* Iterations - Number of training steps the model will take.\n* batch size : number of data points fed in a batch.\n* learning rate : The rate at which algorithm will adjust the parameters. Too small and training time will increase, too big and we might miss the optimum values.\n* cell - Various kind of cells are provided by Tensorflow, Here we will use **BasicRNNCell** and later **GRUCell**.\n* Optimizer - We are using Adam Optimizer here. \n\nNote: Learning_rate, Type of cell and Optimizer along with Number of Neurons are some of the parameters which can be changed while training a neural network.\n","fbf5df48":"### Training with number of iterations - 2000 \nWe will do another training with increased number of iterations and see the effect.","449ae1bc":"Above concludes this kernel on Basic RNN.  Hope you enjoyed!","67ad66b3":"# Objective\n\nThe objective of this notebook is to use **RNN - Recurrent Neural Networks in Tensorflow** for predicting a series value when input is in series.\nThe structure of this program is such that first a data generator class is written which creates sequential data. Sine function is used for creation of training data. Later on, a neural network containing RNN Cell is created and it is used to predict next sequence. \nPlease remember, it is not the next item which is predicted, rather it is the whole sequence.\n\nRNNs have following properties:\n1. They remember the state and context, which is sometimes very important in prediction related tasks.\n2. They are represented in layers which are unrolled through time , each steps can be called a timesteps.\n\nRecurrent Neural Networks have witnessed remarkable success when working with sequential data and they are popular in Text Related modelling.\n\n**A note on Tensorflow: ** \n\nTensorFlow\u2122 is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.","5cb27d96":"**This time the model has performed much better.** But again we see that initial predictions were not coming so nice as they turned out to be later. Lets do one experiment again and test this with a GRU cell. It becomes lot easier in Tensorflow to change the architecture due to its scalable framework. I have kept the numbe of iterations 200 so that we can compare this performance with the first model created above.","b64828dd":"### Test and Train instances\nThe points in blue are the training instance sequence and black points are the target ones. So, from Machine learning perspective, we have the input and outputs.","ba7bf0c0":"## Data Generator Class\nFollowing class is a data generator which generates a 2 dimensional array of data.  x_data is input data or X axis data, where as Y axis is the output. The sine function is used to generate the data.\nThe ultimate objective is to learn this function with help of RNN. RNNs are used for sequential modelling, so if we are given n points then we should be able to predict next set of points in the series."}}