{"cell_type":{"959cf128":"code","189d2e32":"code","61075210":"code","2794ae66":"code","0df58048":"code","c2efce24":"code","7a7ffc39":"code","0954837c":"code","7d8df6dd":"code","a1f076b6":"code","86a17e1f":"code","8a51d032":"code","24d16688":"code","62026da3":"code","9a848891":"code","9e71dd51":"code","ba481dd2":"code","ebf22340":"code","9b99b14b":"code","68ac6501":"code","c10f8c5e":"code","c9fab704":"code","04257834":"code","873a029c":"code","6c8f7883":"markdown","c9496330":"markdown","fbb18198":"markdown","a21e5d4a":"markdown","fa00e73e":"markdown","75712bd6":"markdown","691de5db":"markdown","5f8cdc4a":"markdown","bba953f8":"markdown","da08fa64":"markdown","a8accf01":"markdown","ede58a75":"markdown","44d52b5c":"markdown","fcaab89a":"markdown","ede07633":"markdown","bdca23e8":"markdown","dbc8b1a3":"markdown","6c3630cf":"markdown","d8a931f9":"markdown","c5307f89":"markdown","8472a766":"markdown","a3947e23":"markdown"},"source":{"959cf128":"import pandas as pd\npd.set_option('display.max_rows', None)\nimport numpy  as np\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","189d2e32":"# Read in the csv data using pandas \ntrain  = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv',index_col=0)\ntest   = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv', index_col=0)\nsample = pd.read_csv('..\/input\/santander-customer-satisfaction\/sample_submission.csv')","61075210":"train.dtypes.value_counts()","2794ae66":"train.select_dtypes(include=['int64']).nunique()","0df58048":"features_to_drop = train.nunique()\nfeatures_to_drop = features_to_drop.loc[features_to_drop.values==1].index\n# now drop these columns from both the training and the test datasets\ntrain = train.drop(features_to_drop,axis=1)\ntest  = test.drop(features_to_drop,axis=1)","c2efce24":"train.isnull().values.any()","7a7ffc39":"X = train.iloc[:,:-1]\ny = train['TARGET']","0954837c":"y.value_counts().to_frame().T","7d8df6dd":"from imblearn.over_sampling import SMOTE\nX_resampled, y_resampled = SMOTE().fit_resample(X, y)","a1f076b6":"y_resampled.value_counts().to_frame().T","86a17e1f":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, \n                                                  train_size=0.5,\n                                                  test_size=0.2, \n                                                  random_state=42, \n                                                  shuffle=True)","8a51d032":"from sklearn.preprocessing import MinMaxScaler\nscaler  = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_val   = scaler.transform(X_val)\ntest    = scaler.transform(test)","24d16688":"model = keras.Sequential(\n    [\n        keras.layers.Dense(units=9, activation=\"relu\", input_shape=(X_train.shape[-1],) ),\n        # randomly delete 30% of the input units below\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(units=9, activation=\"relu\"),\n        # the output layer, with a single neuron\n        keras.layers.Dense(units=1, activation=\"sigmoid\"),\n    ]\n)\n\n# save the initial weights for later\ninitial_weights = model.get_weights()","62026da3":"model.summary()","9a848891":"keras.utils.plot_model(model, show_shapes=True)","9e71dd51":"learning_rate = 0.001\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n              loss=\"binary_crossentropy\", \n              metrics=keras.metrics.AUC()\n             )","ba481dd2":"history = model.fit(X_train, y_train, \n          epochs=500, \n          batch_size=1000, \n          validation_data=(X_val, y_val),\n          verbose=0)","ebf22340":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[5:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[5:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[5:,\"auc\"], lw=2, label='training ROC AUC score')\nplt.plot(logs.loc[5:,\"val_auc\"], lw=2, label='validation ROC AUC score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC AUC\")\nplt.legend(loc='lower right')\nplt.show()","9b99b14b":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    min_delta = 0.0002, # minimium amount of change to count as an improvement\n    patience  = 20,     # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","68ac6501":"model.set_weights(initial_weights)","c10f8c5e":"history = model.fit(X_train, y_train, \n          epochs=500, \n          batch_size=1000, \n          validation_data=(X_val, y_val),\n          verbose=0,\n          # add in our early stopping callback\n          callbacks=[early_stopping]\n        )","c9fab704":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[5:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[5:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[5:,\"auc\"], lw=2, label='training ROC AUC score')\nplt.plot(logs.loc[5:,\"val_auc\"], lw=2, label='validation ROC AUC score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC AUC\")\nplt.legend(loc='lower right')\nplt.show()","04257834":"sample['TARGET'] = model.predict(test)","873a029c":"sample.to_csv('submission.csv',index=False)","6c8f7883":"The first thing we shall do is take a look at what types of data we have to train with. Categorical features can be handled via [embedding](https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/).","c9496330":"much better. \n\nAs this is a large dataset we shall only use 50% of the data for training, and 20% for validation","fbb18198":"also, neural networks like to have data all in the same range, for example [0,1] to improve stability. We shall do this using the [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html)\nNote: One should really remove any outliers before doing this as a single outlier can have a large influence on the results","a21e5d4a":"# Tabular binary classification with neural networks: keras\nHere we create a fully-connected artificial neural network based on the kaggle [\"Intro to Deep Learning\"](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning) course written by [Ryan Holbrook](https:\/\/www.kaggle.com\/ryanholbrook). We shall be using [Keras](https:\/\/keras.io\/), the python deep learning API.\nFor our data we shall be using the [Santander Customer Satisfaction](https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction) dataset.","fa00e73e":"# Related reading\nIt is well known that, when it comes to tabular data, neural networks have a difficult time competing with the likes of [XGBoost](https:\/\/www.kaggle.com\/carlmcbrideellis\/very-simple-xgboost-regression). However, progress is constantly being made in that respect, and the following papers make for some very interesting reading\n* [\"*Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data*\"](https:\/\/arxiv.org\/pdf\/1909.06312.pdf) (2019)\n* [\"*TabNet: Attentive Interpretable Tabular Learning*\"](https:\/\/arxiv.org\/pdf\/1908.07442.pdf) (2020)\n* [\"*Tabular Data: Deep Learning is Not All You Need*\"](https:\/\/arxiv.org\/pdf\/2106.03253.pdf) (2021)\n* [\"*Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data*\"](https:\/\/arxiv.org\/pdf\/2106.11189.pdf) (2021)\n* [\"*Deep Neural Networks and Tabular Data: A Survey*\"](https:\/\/arxiv.org\/pdf\/2110.01889.pdf) (2021)","75712bd6":"let us take a look at our model","691de5db":"We shall use the [Adam](https:\/\/keras.io\/api\/optimizers\/adam\/) (Adaptive Moment Estimation) optimizer, a form of [Stochastic gradient descent](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent) (SGD). For more on the subject see the review [\"*An overview of gradient descent optimization algorithms*\"](https:\/\/arxiv.org\/pdf\/1609.04747.pdf), written by Sebastian Ruder.\nAs our problem is binary classification our [loss function](https:\/\/en.wikipedia.org\/wiki\/Loss_function) will be the [binary cross entropy](https:\/\/keras.io\/api\/losses\/probabilistic_losses\/). Finally, as per the [competition evaluation criteria](https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction\/overview\/evaluation), we shall calculate the area under the curve (AUC) of the [ROC](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic).","5f8cdc4a":"Now let us check for any missing values, as these do not sit well with neural networks","bba953f8":"indeed there is a large class imbalance, in view of this we shall resample the minority class using [SMOTE](https:\/\/imbalanced-learn.org\/stable\/references\/generated\/imblearn.over_sampling.SMOTE.html) (Synthetic Minority Over-sampling Technique) from the [imbalanced-learn](https:\/\/imbalanced-learn.org\/stable\/) library","da08fa64":"wonderful, there are no missing values at all. Now let us divide the training data into the features and the target","a8accf01":"Now write out a `submission.csv` file for submission to the competition for scoring. The scores for this model are given at the top of this notebook. As a reference point the winning score for this competition had a Private Leaderboard score of `0.82907`.","ede58a75":"Now let us delete our old training and start anew. We do this by restoring the initial weights of our neural network:","44d52b5c":"or create a more aesthetically pleasing representation via","fcaab89a":"Take a look at how the training went, checking for either [overfitting or underfitting](https:\/\/www.kaggle.com\/ryanholbrook\/overfitting-and-underfitting), by plotting the so-called *learning curves*","ede07633":"let us take a look","bdca23e8":"Check to see whether the data is [highly imbalanced or not](https:\/\/www.kaggle.com\/carlmcbrideellis\/classification-how-imbalanced-is-imbalanced)","dbc8b1a3":"we have now finished training our neural network. Now let us use our model on the `test` data to predict the values, when it comes to neural networks the prediction stage is more commonly referred to  as *inference*:","6c3630cf":"we can see that a good many of the integer features have one single value. Such columns have zero variance and thus have no predictive value, so we shall drop these columns from the train, as well as the test data to maintain consistency","d8a931f9":"we have no strings, so we shall not be using the aforementioned embedding. However, we do have a large number of integer columns. Let us take a look at how many different values each of these integer columns have (this is a long list, so it has been hidden. Click on \"Show hidden\" to take a look)","c5307f89":"we shall now train the model, providing 1000 rows of training data at a  time (`batch_size`), the whole process repeated 500 times (these are the `epochs`).\nIf the `batch_size` is  large, it will take more `epochs` for the neural net to converge. For an interesting article on the subject see [\"*Effect of Batch Size on Neural Net Training*\"](https:\/\/medium.com\/deep-learning-experiments\/effect-of-batch-size-on-neural-net-training-c5ae8516e57).","8472a766":"We are now ready to create our neural network, note that we insert a [dropout layer](https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/) as a form of regularization which will  help reduce overfitting by randomly setting (here 30%) of the input unit values to zero.","a3947e23":"we can see that towards the end the validation ROC AUC score has leveled off and is not improving. We can also see that despite the validation ROC AUC score stagnating, the training ROC AUC score is still steadily rising; this is an indication that we are overfitting. Ideally we would like to watch out for this and call a halt to the training so as not to waste time and CPU\/GPU. To do this we can set up a \"callback\":"}}