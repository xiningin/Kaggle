{"cell_type":{"780f3e08":"code","72fbcade":"code","a871b4c1":"code","c682ad17":"code","0333f3d5":"code","8b71877f":"code","84d8b513":"code","490c6e2c":"code","da742616":"code","ead4da62":"code","171549b9":"code","31b40a82":"code","430d5274":"code","55c953fb":"code","c4bb2e1a":"code","948d72dd":"code","c0b841ee":"code","364eadf2":"code","3b60cac1":"code","f9f275f3":"code","e38c21a2":"code","90057a50":"code","721a4d22":"code","21bc6ab6":"code","bc38199b":"code","138f0835":"code","4b453a38":"code","a944bc18":"code","1b80a86b":"code","f77fcf9e":"code","33ef6b6f":"code","4c8fe20a":"code","492aa2b8":"code","acf28248":"code","cb270bf4":"code","9d15d04f":"code","ec2315e2":"code","de47a30d":"code","2b586657":"code","aa036913":"code","0f398c61":"code","d7a3d0a5":"code","55899dda":"code","d7c54b90":"code","7e2e7e3f":"code","03cad700":"code","0b63e5d1":"code","9cd0e81f":"code","b1179404":"code","280405fd":"code","fd9dc613":"code","c3325b60":"markdown","5b2a3451":"markdown","6635d4b6":"markdown","0b97710b":"markdown","9dff9b50":"markdown","8489a491":"markdown","901baab8":"markdown","4f3dcc93":"markdown","3922d134":"markdown","2c0c529f":"markdown","70ea5a6c":"markdown","1ed2660a":"markdown","f72378a4":"markdown","496ab07c":"markdown","e4a24551":"markdown","434848f5":"markdown","74fddd39":"markdown","156a7881":"markdown","7ca8ac2c":"markdown","2617cecf":"markdown","8f12c698":"markdown","633bde1a":"markdown","7156a01d":"markdown","9de0cdf4":"markdown","9bffc6a8":"markdown","f4030def":"markdown","a0a6a2a0":"markdown","6132686a":"markdown","17d364c9":"markdown","bb3bb83e":"markdown","7fe7d88a":"markdown","d12327b5":"markdown","ee92ff1e":"markdown","f7b648b8":"markdown","e0854b48":"markdown","f9d6d851":"markdown","9aefd9e5":"markdown","a46eaca9":"markdown","4b6b385b":"markdown"},"source":{"780f3e08":"import pandas as pd","72fbcade":"daily_activity = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/dailyActivity_merged.csv\")","a871b4c1":"daily_activity.groupby(['Id']).mean()","c682ad17":"daily_activity.query('Id == 1927972279')","0333f3d5":"daily_activity.query('TotalSteps == 0').groupby(['Id'])['ActivityDate'].count()","8b71877f":"records_count = daily_activity.query('TotalSteps != 0').groupby(['Id'])['ActivityDate'].count(). \\\n                                rename('records_count').reset_index()\nprint(records_count.query('records_count == 31').shape)\nprint(records_count.query('records_count >= 20').shape)","84d8b513":"heartrate_seconds = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/heartrate_seconds_merged.csv\")\nheartrate_seconds.groupby('Id')['Value'].describe()","490c6e2c":"hourly_cal = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/hourlyCalories_merged.csv\")\nhourly_intense = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/hourlyIntensities_merged.csv\")\nhourly_steps = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/hourlySteps_merged.csv\")\nhourly_cal_intense = pd.merge(hourly_cal, hourly_intense, how=\"inner\",on=['Id','ActivityHour'])\nhourly_cal_intense_steps = pd.merge(hourly_cal_intense, hourly_steps, how=\"inner\", on=['Id','ActivityHour'])\nprint(hourly_cal_intense_steps.head(2))","da742616":"print(hourly_cal_intense_steps['ActivityHour'].dtype)\nhourly_cal_intense_steps['ActivityHour'] = pd.to_datetime(\n                                                hourly_cal_intense_steps['ActivityHour'], \n                                                infer_datetime_format=True\n                                            )","ead4da62":"hourly_cal_intense_steps['ActivityDate'] = hourly_cal_intense_steps['ActivityHour'].dt.date\ndaily_cal_intense_steps = hourly_cal_intense_steps.groupby(['Id','ActivityDate'])[['Calories','TotalIntensity','StepTotal']].sum().reset_index()\ndaily_activity['ActivityDate'] = pd.to_datetime(daily_activity['ActivityDate'], infer_datetime_format=True).dt.date\nmerge_df = pd.merge(daily_activity[['Id','ActivityDate','TotalSteps','Calories']],\n                    daily_cal_intense_steps.drop(columns=['TotalIntensity'])\n                    , on=['Id','ActivityDate'])\nmerge_df.groupby(['Id']).mean()","171549b9":"merge_df.query('Id == 4319703577')","31b40a82":"minute_cal = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/minuteCaloriesNarrow_merged.csv\")\nminute_intense = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/minuteIntensitiesNarrow_merged.csv\")\nminute_mets = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/minuteMETsNarrow_merged.csv\")\nminute_steps = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/minuteStepsNarrow_merged.csv\")\nminute_cal_intense = pd.merge(minute_cal, minute_intense, on=['Id','ActivityMinute'])\nminute_cal_intense_mets = pd.merge(minute_cal_intense, minute_mets, on=['Id','ActivityMinute'])\nminute_cal_intense_mets_steps = pd.merge(minute_cal_intense_mets, minute_steps, on = ['Id', 'ActivityMinute'])","430d5274":"minute_cal_intense_mets_steps['ActivityMinute'] = pd.to_datetime(minute_cal_intense_mets_steps['ActivityMinute'], \n                                                                 infer_datetime_format=True)\nminute_cal_intense_mets_steps['ActivityDate'] = minute_cal_intense_mets_steps['ActivityMinute'].dt.date\nday_cal_steps_from_minute = minute_cal_intense_mets_steps.groupby(['Id','ActivityDate'])[['Calories','Steps']].sum().reset_index()\nmerge2 = pd.merge(merge_df, day_cal_steps_from_minute, on=['Id','ActivityDate'])\nmerge2.columns = ['Id','ActivityDate','CaloriesDay','TotalStepsDay','TotalStepsDayFromHour','CaloriesDayFromHour',\n                 'TotalStepsDayFromMinute','CaloriesDayFromMinute']\nmerge2 = merge2[['Id','ActivityDate','TotalStepsDay','TotalStepsDayFromMinute','TotalStepsDayFromHour','CaloriesDay',\n                 'CaloriesDayFromMinute','CaloriesDayFromHour']]\nmerge2.groupby(['Id']).mean()","55c953fb":"minute_sleep = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/minuteSleep_merged.csv\")","c4bb2e1a":"minute_sleep['Id'].nunique()","948d72dd":"pd.set_option('display.max_rows',9999)\nminute_sleep['date'] = pd.to_datetime(minute_sleep['date'], infer_datetime_format=True)\nminute_sleep['day'] = minute_sleep['date'].dt.date\nminute_sleep.query('value == 1').groupby(['Id','day'])['value'].sum()","c0b841ee":"minute_sleep[(minute_sleep['Id'] == 4319703577) & (minute_sleep['day'] == pd.to_datetime(\"2016-05-05\", format=\"%Y-%m-%d\"))]","364eadf2":"pd.set_option('display.max_rows',9999)\nminute_sleep.query('value == 1').groupby(['Id','day'])['value'].sum()","3b60cac1":"sleep_day = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/sleepDay_merged.csv\")\nsleep_day['SleepDay'] = pd.to_datetime(sleep_day['SleepDay'], infer_datetime_format=True)\nsleep_day['day'] = sleep_day['SleepDay'].dt.date","f9f275f3":"# transform the minutely data to daily sleep record\nday_total_time_in_bed_from_minute = minute_sleep.groupby(['Id','day'])['value'].sum().rename('TotalMinutesInBedFromMinute').reset_index()\nday_total_time_as_sleep_from_minute = minute_sleep.query('value == 1').groupby(['Id','day'])['value'].sum().rename('TotalMinutesAsleepFromMinute').reset_index()\nsleep_day_from_minute = pd.merge(day_total_time_in_bed_from_minute, day_total_time_as_sleep_from_minute, on = ['Id','day'])","e38c21a2":"sleep_day_compare = pd.merge(sleep_day[['Id','day','TotalMinutesAsleep','TotalTimeInBed']], sleep_day_from_minute, on = ['Id','day'], how='right')\nsleep_day_compare.describe()","90057a50":"sleep_day_compare[sleep_day_compare['TotalMinutesAsleep'] - sleep_day_compare['TotalMinutesAsleepFromMinute'] != 0].describe()","721a4d22":"weight_log = pd.read_csv(\"..\/input\/fitbit\/Fitabase Data 4.12.16-5.12.16\/weightLogInfo_merged.csv\")\nweight_log['Date'] = pd.to_datetime(weight_log['Date'], infer_datetime_format=True)","21bc6ab6":"weight_log.groupby('Id').count()","bc38199b":"selected_participants_for_daily_activity = list(records_count.query('records_count >= 20')['Id'])\ndaily_activity_cleaned = daily_activity[daily_activity['Id'].isin(selected_participants_for_daily_activity)].query('TotalSteps != 0')","138f0835":"hourly_cal_intense_steps_cleaned = hourly_cal_intense_steps[hourly_cal_intense_steps['Id'].isin(selected_participants_for_daily_activity)]\nminute_cal_intense_mets_steps_cleaned = minute_cal_intense_mets_steps[minute_cal_intense_mets_steps['Id'].isin(selected_participants_for_daily_activity)]","4b453a38":"sleep_period = minute_sleep.groupby(['Id','logId']).agg(\n    sleepstart = pd.NamedAgg('date', aggfunc='min'),\n    wakeup = pd.NamedAgg('date', aggfunc='max'),\n    total_time_asleep = pd.NamedAgg('value', aggfunc=lambda x:sum(x==1)),\n    total_time_inbed = pd.NamedAgg('value', aggfunc='sum') \n).reset_index().sort_values('Id')","a944bc18":"import seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","1b80a86b":"daily_activity_mean = daily_activity_cleaned.groupby('Id').mean()\n# drop these 2 columns because of too many zeros, which doesn't add value to clustering\ndaily_activity_mean = daily_activity_mean.drop(columns=['LoggedActivitiesDistance', 'SedentaryActiveDistance'])","f77fcf9e":"def cluster(df, n_components_test, n_components_chose, n_cluster_chose):\n    ## let's find out how many components should we use\n    pca = PCA(n_components=n_components_test)\n    pca.fit(df)\n    figure = plt.figure(figsize=(10,4))\n    ax1 = figure.add_subplot(121)\n    pca_components = pd.DataFrame({'features':list(range(n_components_test)), 'ratio':pca.explained_variance_ratio_})\n    sns.barplot(x='features',y='ratio',data=pca_components, ax=ax1)\n    \n    ## By checking the explained_variance_ratio_, the activity features can be explained by the first 2 components. \n    ## Next, we can perform clustering and decide the number of cluster we should use\n    pca = PCA(n_components=n_components_chose)\n    df_pca = pd.DataFrame(pca.fit_transform(df),columns=['components%d' % (i) for i in list(range(n_components_chose))], index=df.index)\n    print(\"pca explained variance_ratio_:\", pca.explained_variance_ratio_)\n    score_list = []\n    for n in range(2,10):\n        clustering = KMeans(n_clusters=n, random_state=100).fit(df_pca)\n        score = silhouette_score(df_pca, clustering.labels_)\n        score_list.append(score)\n    score_eval = pd.DataFrame({'cluster':list(range(2,10)), 'score':score_list})\n    ax2 = figure.add_subplot(122)\n    sns.barplot(x='cluster', y='score',data=score_eval, ax=ax2)\n    \n    clustering = KMeans(n_clusters=n_cluster_chose, random_state=100).fit(df_pca)\n    df_pca['cluster'] = clustering.labels_\n    df_pca = df_pca.reset_index()\n    return df_pca, clustering","33ef6b6f":"daily_activity_pca, clustering = cluster(daily_activity_mean, 4, 2, 3)\nplt.figure(figsize=(5,5))\nsns.scatterplot(x='components0', y='components1', hue='cluster', data=daily_activity_pca)","4c8fe20a":"daily_activity_mean['cluster'] = clustering.labels_\ndaily_activity_cluster_mean = daily_activity_mean.groupby(['cluster']).mean().reset_index()\ndaily_activity_mean = daily_activity_mean.reset_index()\nfigure = plt.figure(figsize=(20,15))\nfor i, feature in enumerate(daily_activity_cluster_mean.columns[1:]):\n    ax = figure.add_subplot(3,4,i+1)\n    sns.barplot(x='cluster',y=feature,data=daily_activity_cluster_mean)\n    ax.set_title(feature)","492aa2b8":"customers_in_cluster = pd.DataFrame(daily_activity_mean.cluster.value_counts().reset_index())\ncustomers_in_cluster.columns = ['cluster','number_of_customer']\nsns.barplot(x='cluster', y='number_of_customer', data=customers_in_cluster)","acf28248":"hourly_cal_intense_steps_cleaned['hour'] = hourly_cal_intense_steps_cleaned['ActivityHour'].dt.hour","cb270bf4":"hourly_cal_intense_steps_mean = hourly_cal_intense_steps_cleaned.groupby(['Id','hour'])[['Calories','TotalIntensity','StepTotal']].mean().reset_index()\n# hourly_cal_intense_steps_mean = hourly_cal_intense_steps_mean.merge(customer_group, on = ['Id'], how='left')","9d15d04f":"hourly_cal_intense_steps_cleaned.loc[(hourly_cal_intense_steps_cleaned['hour'] >= 5) & (hourly_cal_intense_steps_cleaned['hour'] < 12), 'day_part'] = \"morning\"\nhourly_cal_intense_steps_cleaned.loc[(hourly_cal_intense_steps_cleaned['hour'] >= 12) & (hourly_cal_intense_steps_cleaned['hour'] < 17), 'day_part'] = \"afternoon\"\nhourly_cal_intense_steps_cleaned.loc[(hourly_cal_intense_steps_cleaned['hour'] >= 17) & (hourly_cal_intense_steps_cleaned['hour'] < 21), 'day_part'] = \"evening\"\nhourly_cal_intense_steps_cleaned.loc[(hourly_cal_intense_steps_cleaned['hour'] >= 21) | (hourly_cal_intense_steps_cleaned['hour'] < 4), 'day_part'] = \"night\"","ec2315e2":"day_part_cal_intense_steps = hourly_cal_intense_steps_cleaned.groupby(['Id', 'ActivityDate', 'day_part'])[['Calories','TotalIntensity','StepTotal']].sum().reset_index()\nday_part_cal_intense_steps = day_part_cal_intense_steps[day_part_cal_intense_steps['Id'].isin(daily_activity_mean.query('cluster == 1 | cluster == 2')['Id'])]","de47a30d":"day_part_cal_intense_steps_mean = day_part_cal_intense_steps.groupby(['Id','day_part'])[['Calories','TotalIntensity','StepTotal']].mean().reset_index()","2b586657":"day_part_cal_intense_steps_mean_pt = day_part_cal_intense_steps_mean.pivot(index=['Id'],columns=['day_part'], values=['Calories','TotalIntensity','StepTotal'])","aa036913":"day_part_cal_intense_steps_pca, clustering = cluster(day_part_cal_intense_steps_mean_pt, 5, 4, 3)","0f398c61":"day_part_cal_intense_steps_mean = day_part_cal_intense_steps_mean.merge(day_part_cal_intense_steps_pca[['Id','cluster']], on=['Id'], how='left')","d7a3d0a5":"day_part_cal_intense_steps_mean['day_part'] = day_part_cal_intense_steps_mean['day_part'].astype('category').cat.set_categories(['morning', 'afternoon', 'evening', 'night'], ordered=True)","55899dda":"figure = plt.figure(figsize=(20,5))\nax1 = figure.add_subplot(131)\nsns.lineplot(x='day_part', y='StepTotal', data=day_part_cal_intense_steps_mean, hue=\"cluster\", ax=ax1)\nax1.set_title(\"Total Steps\")\nax2 = figure.add_subplot(132)\nsns.lineplot(x='day_part', y='TotalIntensity', data=day_part_cal_intense_steps_mean, hue=\"cluster\", ax=ax2)\nax2.set_title(\"Total Intensity\")\nax3 = figure.add_subplot(133)\nsns.lineplot(x='day_part', y='Calories', data=day_part_cal_intense_steps_mean, hue=\"cluster\", ax=ax3)\nax3.set_title(\"Calories\")","d7c54b90":"customers_in_cluster = pd.DataFrame(day_part_cal_intense_steps_pca.cluster.value_counts().reset_index())\ncustomers_in_cluster.columns = ['cluster','number_of_customer']\nsns.barplot(x='cluster', y='number_of_customer', data=customers_in_cluster)","7e2e7e3f":"daily_activity_cleaned['ActivityDate'] = pd.to_datetime(daily_activity_cleaned['ActivityDate'], infer_datetime_format=True)\nregular_exerciser = daily_activity_cleaned[daily_activity_cleaned['Id'].isin(day_part_cal_intense_steps_mean.query('cluster == 0 | cluster == 2')['Id'])]\nregular_exerciser['weekday'] = regular_exerciser['ActivityDate'].dt.weekday\nregular_exerciser['week'] = regular_exerciser['ActivityDate'].dt.week\n\nregular_exerciser['exercise_today'] = daily_activity_cleaned['TotalSteps'] >= 10000\nexercise_freq = regular_exerciser.groupby(['Id']).agg(\n    record_days=pd.NamedAgg('ActivityDate', aggfunc='count'),\n    exercise_days=pd.NamedAgg('exercise_today', aggfunc='sum')\n)\nexercise_freq['freq'] = round(exercise_freq['record_days'] \/ exercise_freq['exercise_days'],0)\nsns.histplot(x='freq', data=exercise_freq, binwidth=1, discrete=True)\nplt.title('Exercise frequency')","03cad700":"minute_cal_intense_mets_steps_cleaned['is_exercising'] = minute_cal_intense_mets_steps_cleaned['Intensity'] > 1","0b63e5d1":"exercise_length = minute_cal_intense_mets_steps_cleaned.groupby(['Id','ActivityDate'])['is_exercising','Steps'].sum().reset_index()\nexercise_length = exercise_length[exercise_length['Id'].isin(day_part_cal_intense_steps_mean.query('cluster == 0 | cluster == 2')['Id'])]\nexercise_length = exercise_length[exercise_length['Id'].isin(exercise_freq.query('freq <= 3').index)]\nexercise_length = exercise_length.groupby('Id')['is_exercising'].mean().rename('average_exercise_length').reset_index()\nax = sns.histplot(x='average_exercise_length', data=exercise_length, binwidth=30)\nax.set_xlim((0,150))","9cd0e81f":"sleep_period['sleepstart_hour'] = sleep_period['sleepstart'].dt.hour\nsleep_start_freq = sleep_period.query('total_time_inbed >= 300').groupby(['Id','sleepstart_hour'])['logId'].count().rename('frequency').reset_index()\nsleep_start_freq['freq_rank'] = sleep_start_freq.groupby(['Id'])['frequency'].rank(method='first', ascending=False)\nsleep_start_most_freq = sleep_start_freq[sleep_start_freq.Id.isin(sleep_start_freq.query('freq_rank > 2')['Id'])].query('freq_rank == 1')\nsns.histplot(x='sleepstart_hour', data=sleep_start_most_freq, discrete=True)","b1179404":"sleep_period_over_300 = sleep_period.query('total_time_inbed >= 300')\nsleep_period_over_300['asleep_pct'] = sleep_period_over_300['total_time_asleep'] \/ sleep_period_over_300['total_time_inbed']\nsleep_period_over_300_mean = sleep_period_over_300.groupby(['Id'])[['total_time_asleep', 'total_time_inbed', 'asleep_pct']].mean()\nfigure = plt.figure(figsize=(20,5))\nax1 = figure.add_subplot(131)\nsns.histplot(x='total_time_asleep', data=sleep_period_over_300_mean,ax=ax1, binwidth=60)\nax2 = figure.add_subplot(132)\nsns.histplot(x='total_time_inbed', data=sleep_period_over_300_mean,ax=ax2, binwidth=60)\nax3 = figure.add_subplot(133)\nsns.histplot(x='asleep_pct', data=sleep_period_over_300_mean,ax=ax3, binwidth=0.1)","280405fd":"figure = plt.figure(figsize=(25,15))\nax1 = figure.add_subplot(211)\nplt.title('BMI')\nsns.lineplot(x='Date', y='BMI', data=weight_log.query('Id == 6962181067'),ax=ax1)\nax1.set_ylim((23,25))\nax2 = figure.add_subplot(212)\nplt.title(\"WeightKg\")\nsns.lineplot(x='Date', y='WeightKg', data=weight_log.query('Id == 6962181067'),ax=ax2)\nax2.set_ylim((60,65))","fd9dc613":"figure = plt.figure(figsize=(25,15))\nax1 = figure.add_subplot(211)\nplt.title('BMI')\nsns.lineplot(x='Date', y='BMI', data=weight_log.query('Id == 8877689391'),ax=ax1)\nax1.set_ylim((25,26))\nax2 = figure.add_subplot(212)\nplt.title(\"WeightKg\")\nsns.lineplot(x='Date', y='WeightKg', data=weight_log.query('Id == 8877689391'),ax=ax2)\nax2.set_ylim((81,86))","c3325b60":"**5.2** By inspecting how long the participants sleep each day, I found that:\n* Only 24 participants have sleep data, and few of them have full records (31 records), which indicates they are not used to wearing device during sleep\n* The total minutes of sleeping in some records are abnormally small","5b2a3451":"<a id=\"Introduction\"><\/a>\n# <font color=#4e79a7>Bellabeat case study <\/font>\nThis is a capstone project from the [Google data analysis specialization](https:\/\/www.coursera.org\/professional-certificates\/google-data-analytics) hosted on coursera, meaning to practice what I learned from the course. So, this project will be divided into 5 parts according to the analysis steps lectured in the course (exclude the **act** step), including: **ask**, **prepare**, **process**, **analysis**, **share**. Since I am more familiar with the python language, I will finish this project using Python.\n\n<div>\n    <h1 id=\"Ask\" style=\"color:#4e79a7\">\n        Ask\n    <\/h1>\n <\/div>\n\n### Business task\nGiving recommendations for marketing strategies by analyzing how users use their smart device\n\n### Questions that may relate to the task\n* How many groups of customer can we identify based on their activities?\n* What are the features of different customer groups?\n* How do these results have influence on marketing strategy?\n\n### Stakeholders\n* Ur\u0161ka Sr\u0161en, cofounder and Chief Creative Officer\n* Sando Mur, cofounder, key member of the Bellabeat executive team\n* Head of the marketing department\n\n<div>\n    <h1 id=\"Prepare\" style=\"color:#4e79a7\">\n        Prepare\n    <\/h1>\n <\/div>\n\n### Description of the data\n* The data records several kinds of health data, such as sleep, steps, heartrate, calories consumed in three time dimensions as second, minute and hour\n* Some data is already merged together, for example, dailyActivity_merged.csv is merged from dailyCalories_merged.csv, dailyIntensities_merged.csv, dailySteps_merged.csv\n* Some data is provided as both long and wide format, such as minuteCalories\n\n\n### Problems of the data\n* The data is stored as .csv files, and each of them contain so many records that it may be more practical to conduct analysis using R\/Python than using spreadsheet.\n* The data is not collected directly from Bellabeat but from a simliar product, so it is a second party data.\n* It will be great if we have the demographics data about the partifipants.\n* Lacking of metadata, which brings obstacles for interpreting the data. However, thanks for the kaggler [Laimis Andrijauskas](https:\/\/www.kaggle.com\/laimisandrijauskas), you can find it [here](https:\/\/www.kaggle.com\/arashnic\/fitbit\/discussion\/281341)\n#### Let's deep dive into the specific datasets\nFirst of all, let's import the pandas package","6635d4b6":"For merged hourly data (**hourly_cal_intense_steps**, dataframe object created in the prepare step) and minutely data (**minute_cal_intense_mets_steps**, dataframe object created in the prepare step), I selected participants that were used above","0b97710b":" **1.2** Next, I want to see how many invalid records exist for each individual. It turns out that half of the participants have invalid records, 4 of whom have **more than 10** invalid records","9dff9b50":"**1.4** According to these findings, I think the invalid records in this dataset should not be included in the analysis, which will cause bias to the outcome. Regarding to not half of the participants have full records, to make this analysis more meaningful, I will try including participants with at least 20 records.  \n               ","8489a491":"Since dailyActivity_merged.csv contains data from other daily files, I will skip these files.","901baab8":"Next, I want to take a further step to identify customer groups for customers in cluster 1 and 2 above based on the time (morning, afternoon, evening, night) when they do exercise and exercise intentisy","4f3dcc93":"<div>\n    <h1 id=\"Process\" style=\"color:#4e79a7\">\n        Process\n    <\/h1>\n <\/div>","3922d134":"<div>\n    <h1 id=\"Analysis\" style=\"color:#4e79a7\">\n        Analysis\n    <\/h1>\n <\/div>","2c0c529f":"#### 5. minuteSleep_merged.csv\n**5.1** According to the [metadata](https:\/\/www.kaggle.com\/arashnic\/fitbit\/discussion\/281341), value = **3** means **awake**, value = **2** means **restless**, value = **1** means **asleep**","70ea5a6c":" **1.1** By checking the mean of each attributes for each participants, I found that the total steps of Id 1927972279 is abnormally small. Then I filterd out this participant from the dataset and found that many records of he (she) are zero, which may indicate the device failed to collect the data. I define these records as **invalid records**.","1ed2660a":"By inspecting the trend of total steps, intensity and calories from morning to night for each cluster, we can rougly describe the features of each cluster:\n* Cluster 0: customers in this group may exercise twice each day, generaly in the afternnon and evening, and the exercise intensity is moderately high\n* Cluster 1: Total steps in this group distributes more evenly than in other groups, which may indicate customers in this group don't exercise too much, it's just their job require them to walk more\n* Cluster 2: customers in this group exercise in the morning, and the exercise intensity is fairly high","f72378a4":"For **minuteSleep_merged.csv**, I think it may be useful to transform it to a data frame recording each entire sleep for each participant, which includes the time when participants go to bed, the time when they get up, total minutes asleep and total time in bed. In this case, each entire sleep record is not splitted into different days. \n  We can use the **logId** to separate each sleep. ","496ab07c":"Next I want to know the exercise frequency of regular exercisers. It seems that most of them do exercise every day","e4a24551":"Among these 3 clusters, althoug they have clear differences in exercise intensity, especially for the **VeryActiveDistance** attribute, the **SedentaryMinutes** attribute among the groups doesn't vary too much. This means our customers may have the occupation that need them sitting a lot during work. Based on this observation, the differences seen in exercise intensity can be explained by customers' attidude towards sports. In summary, these 3 clusters represents: \n* Cluster 0: seldom exercise customers\n* Cluster 1: High-intensity exercise customers\n* Cluster 2: Moderate-intensity exercise customer","434848f5":"We can also know that:\n* Total asleep time of most customers is between 7-8 hours\n* Total inbed time of most customers is between 8-9 hours\n* The asleep time percentage of most customers is between 80%-90%","74fddd39":"For **sleepDay_merged.csv**, I don't do much cleaning except for changing the data type of \"SleepDay\" attribute to pandas datetime data type, which was already done in the proceess step. \nAlso, there is not too much we can do for the **weightLogInfo_merged.csv** dataset, except for changing the \"Date\" attribute to the right data type, which was already done in the process step. \nAfter taking all the efforts, we can now walk into the analysis part!","156a7881":"**6.1** By comparing the calculated data to the provided, I found that:\n* Total minutes in bed from calculation is different from the one in sleepDay_merged, may be is because of the different definition according to the metadata. \n* Records in the calculated data are more than those in sleepDay_merged.csv, which may indicate some missing records from the provided daily data\n* The total minutes alseep is not matched for more than 300 records, and the gap between is not a small one","7ca8ac2c":"Finally, I want to see wether the weight and BMI of our customers change during wearing the devices, based on the hypothesis that the smart devices can prompt them to lead a more healthier style. However, it seems that the index doesn't vary a lot. But this result is limited to the number of customers who have enough data for analysis and the tracking length. ","2617cecf":"#### 6. sleepDay_merged.csv\nThis file seems as a sleep record in daily dimension, so I transformed the minuteSleep_merged.csv to the one in daily dimension and compared it with sleepDay_merged.csv to find potential problems.","8f12c698":"  #### 1. dailyActivity_merged.csv","633bde1a":"Next, I want to see the sleeping habits of our customers. First I want to see when our customers usually go to bed. To make sure the result is plausible, I only use the the records with total time in bed more than 300 minutes and choose customer with at least 3 records after filtering based on that. **The result shows that most of our customers go to bed between 10 pm ~ 12 pm**.","7156a01d":"#### 2. heartrate_seconds_merged.csv\n Different participants have different count of heart rate records, the smallest count is 2490. In addition, the interval between each record for the same person have differences. However, I don't think these will have a great influence on the analysis, but I will keep these points in mind.","9de0cdf4":" **1.3** In addition, it is possible that they dont'wear the devices every day. By counting the records for each participant after filtering the invalid ones, I found that **only 14 participants had full records**, and 26 participants had at least 20 records.","9bffc6a8":"It turns out that we only need 2 components to explain the daily activity features. When the cluster number is between 2 and 5, the scores don't vary too much. Considering too much clusters is not good to precision marketing, I choose to use 3 clusters. ","f4030def":"Most of our customers are moderate-intensity exercise customers","a0a6a2a0":"According to the problems stated above, for **dailyActivity_merged.csv**, I selected participants with at least 20 records, and excluded the invalid records.","6132686a":"It turns out that 4 components are enough to represent the activity features, and **the customers can be divided into 3 groups**","17d364c9":"By counting the number of customers in each cluster, the number of regular exercisers (customer in cluster 0 or cluster 2) is 12 out of 26, about 50%.","bb3bb83e":"**5.3** By inspecting some abnormal samples, I found that:\n* Some sleep records are splitted into 2 days, for example, 2016-04-12 11:50 pm - 2016-04-13 6:00 am, which causes the length of sleeping in one day is very small\n* Some sleep records are generated only in the daytime, maybe are collected during a nap or are attributed to the life style of the participant (like taking a night shift and sleeping in the daytime)\n* Some noises may exist, for example, some 2s or 3s suddenly appear in consecutive 1s in a specific day for a specific participant, like 1,1,1,1,1,1,2,2,3,3,1,1,1,1","7fe7d88a":"## weightLogInfo_merged.csv\n* Only 8 participants have data, and only 2 of them have a record more then 20 days","d12327b5":"#### 3. hourlyCalories, hourlyIntensities, hourlySteps\n**3.1** These three datasets has the same number of records with matched Id and ActivityHour, so I will merge them before further processing","ee92ff1e":"#### 4. minuteCaloriesNarrow_merged.csv, minuteIntensitiesNarrow_merged.csv, minuteMETsNarrow_merged.csv, minuteStepsNarrow_merged.csv\nThese four datasets has the same number of records and matched id and timestamp, so they can be merged together. Here I chose to use the long format version. I found that the minutely data can matched with the hourly one, which means it has the same problems too.","f7b648b8":"<div>\n    <h1 id=\"Share (PPT included)\" style=\"color:#4e79a7\">\n        Share (PPT included)\n    <\/h1>\n <\/div>\n \nSo far,we have found many interesting things from the data, but not all the points are valuable to the target, which is giving recommendations for marketing strategies. For me, I think the following points may be useful.<\/font>\n* Our customers can be separated into 2 major groups, customers who are regular exercisers or not fond of exercising. The ratio between the two groups is 50% to 50%. So I recommend the advertisement content should be group specific, since the customers who don't like sports may be less sensitive to the content relating to sports.\n* Most of our customers go to bed after 10 pm, so the advertisement resources should be arranged before this time   \n\nThese 2 points might be a bit shallow, any other ideas are very welcome!\n**Here is my [PPT](https:\/\/docs.google.com\/presentation\/d\/e\/2PACX-1vSNPw4xQ_0z-O-lyAKmQif1iG15xwANrkMP-WIxrAUYufNbGu-sxn-QgreirteOneuBz9pgPq4rSnhE\/pub?start=true&loop=false&delayms=3000&slide=id.g10f94bb5a10_2_112) for presentation.** \n\nThat's the end! Thank you!","e0854b48":"**3.2** The ActivityHour attribute is not a datetime data type, we need to convert it to the right data type before further investigating","f9d6d851":"First, let's answer the first question, **how many customer groups can we identify from the data?** This question may be helpful for precision marketing.","9aefd9e5":"**3.3** I calculated the daily values from the merged dataframe, and by comparing it to the given dailyActivity_merged.csv, I found that although data of the most participants from both datasets matched, some had a clear gap between the two datasets, such as Id 4319703577. Then I looked into the hourly dataset and found that some data was missing, for example, total steps of Id 4319703577 on 2016-04-12. So if we need to analyze the data from the hourly dimension, we need to exclude these abnormal records.","a46eaca9":"Next, I want to know how much time the regular exercisers spend on exericising. By inspecting the data, I find that it is very possible that the intensity value larger than 1 indicates the exercising moment.","4b6b385b":"By extracting the regular exercisers, I find that most of these people spend more than 60 minutes on exercising"}}