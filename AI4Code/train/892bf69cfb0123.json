{"cell_type":{"6b557346":"code","801b0ef7":"code","600991b2":"code","3a6aa8c0":"code","b4309f5d":"code","550d7efc":"code","0a738d0e":"code","4dfe29c3":"code","3abb7925":"code","ee4b641c":"code","aa42dec9":"code","ef3f084b":"code","7477bdf0":"code","6a87503d":"code","8b80e510":"code","d1229397":"code","abbee212":"code","0c141a1f":"code","7fe75e76":"code","b1bbd318":"code","ffa0d1dd":"code","bdb162aa":"code","16791d33":"code","84228c8b":"code","bcd179af":"code","3957cb53":"code","d76ca434":"code","d8541452":"code","d3b953fe":"code","78a11452":"code","5d44c655":"code","41850e25":"code","888b597f":"code","254ec77c":"code","7e31551a":"code","ff09c945":"code","b198a12d":"code","d61f4083":"code","c6916c04":"code","b711d3cd":"code","c45761d3":"code","ce10f745":"markdown","23b7918a":"markdown","18fff3f2":"markdown","9ce0ee5a":"markdown","ff0ee0ca":"markdown","1de9de8e":"markdown","c3e79c86":"markdown","42b605db":"markdown","98f64228":"markdown","b1c0f719":"markdown","b6e9bb94":"markdown","0d20330a":"markdown","11db4557":"markdown","901e05b1":"markdown","230a7458":"markdown","8168a635":"markdown","c25a7ee6":"markdown","8990ce43":"markdown","0e4744bf":"markdown","64f535f3":"markdown","66ec421f":"markdown","f3e7636f":"markdown","d626a8c2":"markdown","33b98798":"markdown","c3349d2d":"markdown","880a9e02":"markdown","947325fd":"markdown","e6665d67":"markdown","de0670dc":"markdown","7f6bd802":"markdown","700d1960":"markdown","36166150":"markdown","a56d9ba1":"markdown","d2442c82":"markdown","f5d90518":"markdown","77179838":"markdown","494dae87":"markdown","e9580d6d":"markdown","1b91ba3b":"markdown","cc78aa18":"markdown","80b17a55":"markdown","1610eab4":"markdown","d5312ac7":"markdown","51c429d8":"markdown","c8b3144a":"markdown","469bfdbd":"markdown","b37f94ed":"markdown","64b5a68e":"markdown","23bf2885":"markdown","6842732a":"markdown","6795b5ac":"markdown","5ab72a8d":"markdown","a9a5caef":"markdown","0425c696":"markdown","aae517f2":"markdown","269b5599":"markdown","b285fbbe":"markdown","96492970":"markdown"},"source":{"6b557346":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/bn8rVBuIcFg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","801b0ef7":"import os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","600991b2":"INPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncalendar = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\nselling_prices = pd.read_csv(f'{INPUT_DIR}\/sell_prices.csv')\nsample_submission = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\nsales_train_val = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')","3a6aa8c0":"ids = sorted(list(set(sales_train_val['id'])))\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[2]].set_index('id')[d_cols].values[0]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[1]].set_index('id')[d_cols].values[0]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[17]].set_index('id')[d_cols].values[0]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales\")\nfig.show()","b4309f5d":"ids = sorted(list(set(sales_train_val['id'])))\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[0]].set_index('id')[d_cols].values[0][:90]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0][1300:1400]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[65]].set_index('id')[d_cols].values[0][350:450]\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales snippets\")\nfig.show()","550d7efc":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1\/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')","0a738d0e":"y_w1 = denoise_signal(x_1)\ny_w2 = denoise_signal(x_2)\ny_w3 = denoise_signal(x_3)\n\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"mediumaquamarine\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color=\"purple\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"lightskyblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color=\"navy\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) sales\")\nfig.show()","4dfe29c3":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_w1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_w2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_w3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()","3abb7925":"def average_smoothing(signal, kernel_size=3, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample)","ee4b641c":"y_a1 = average_smoothing(x_1)\ny_a2 = average_smoothing(x_2)\ny_a3 = average_smoothing(x_3)\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"lightskyblue\"), showlegend=False,\n               name=\"Original sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_a1, mode='lines', marker=dict(color=\"navy\"), showlegend=False,\n               name=\"Denoised sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_a2, mode='lines', marker=dict(color=\"indigo\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"mediumaquamarine\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_a3, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) signals\")\nfig.show()","aa42dec9":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_a1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_a2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_a3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()","ef3f084b":"past_sales = sales_train_val.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nstore_list = selling_prices['store_id'].unique()\nmeans = []\nfig = go.Figure()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    means.append(np.mean(past_sales[store_items].sum(axis=1)))\n    fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\")","7477bdf0":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name \")","6a87503d":"df = pd.DataFrame(np.transpose([means, store_list]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\")","8b80e510":"greens = [\"mediumaquamarine\", \"mediumseagreen\", \"seagreen\", \"green\"]\nstore_list = selling_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"ca\" in s or \"CA\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=greens[i])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (California)\")","d1229397":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n    if \"ca\" in s or \"CA\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s, marker=dict(color=greens[i])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name (California)\")","abbee212":"df = pd.DataFrame(np.transpose([means, stores]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\", color_continuous_scale=greens)\n\n\nfig = go.Figure(data=[\n    go.Bar(name='', x=stores, y=means, marker={'color' : greens})])\n\nfig.update_layout(title=\"Mean sales vs. Store name (California)\", yaxis=dict(title=\"Mean sales\"), xaxis=dict(title=\"Store name\"))\nfig.update_layout(barmode='group')\nfig.show()","0c141a1f":"purples = [\"thistle\", \"violet\", \"purple\", \"indigo\"]\nstore_list = selling_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"wi\" in s or \"WI\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=purples[i%len(purples)])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (Wisconsin)\")","7fe75e76":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n    if \"wi\" in s or \"WI\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s, marker=dict(color=purples[i%len(purples)])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name (Wisconsin)\")","b1bbd318":"df = pd.DataFrame(np.transpose([means, stores]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\", color_continuous_scale=greens)\n\n\nfig = go.Figure(data=[\n    go.Bar(name='', x=stores, y=means, marker={'color' : purples})])\n\nfig.update_layout(title=\"Mean sales vs. Store name (Wisconsin)\", yaxis=dict(title=\"Mean sales\"), xaxis=dict(title=\"Store name\"))\nfig.update_layout(barmode='group')\nfig.show()","ffa0d1dd":"blues = [\"skyblue\", \"dodgerblue\", \"darkblue\"]\nstore_list = selling_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"tx\" in s or \"TX\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=blues[i%len(blues)])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (Texas)\")","bdb162aa":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n    if \"tx\" in s or \"TX\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s, marker=dict(color=blues[i%len(blues)])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name (Texas)\")","16791d33":"df = pd.DataFrame(np.transpose([means, stores]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\", color_continuous_scale=greens)\n\n\nfig = go.Figure(data=[\n    go.Bar(name='', x=stores, y=means, marker={'color' : blues})])\n\nfig.update_layout(title=\"Mean sales vs. Store name (Texas)\", yaxis=dict(title=\"Mean sales\"), xaxis=dict(title=\"Store name\"))\nfig.update_layout(barmode='group')\nfig.show()","84228c8b":"train_dataset = sales_train_val[d_cols[-100:-30]]\nval_dataset = sales_train_val[d_cols[-30:]]","bcd179af":"fig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Train (blue) vs. Validation (orange) sales\")\nfig.show()","3957cb53":"predictions = []\nfor i in range(len(val_dataset.columns)):\n    if i == 0:\n        predictions.append(train_dataset[train_dataset.columns[-1]].values)\n    else:\n        predictions.append(val_dataset[val_dataset.columns[i-1]].values)\n    \npredictions = np.transpose(np.array([row.tolist() for row in predictions]))\nerror_naive = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","d76ca434":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Naive approach\")\nfig.show()","d8541452":"predictions = []\nfor i in range(len(val_dataset.columns)):\n    if i == 0:\n        predictions.append(np.mean(train_dataset[train_dataset.columns[-30:]].values, axis=1))\n    if i < 31 and i > 0:\n        predictions.append(0.5 * (np.mean(train_dataset[train_dataset.columns[-30+i:]].values, axis=1) + \\\n                                  np.mean(predictions[:i], axis=0)))\n    if i > 31:\n        predictions.append(np.mean([predictions[:i]], axis=1))\n    \npredictions = np.transpose(np.array([row.tolist() for row in predictions]))\nerror_avg = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","d3b953fe":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Moving average\")\nfig.show()","78a11452":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = Holt(row).fit(smoothing_level = 0.3, smoothing_slope = 0.01)\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_holt = np.linalg.norm(predictions - val_dataset.values[:len(predictions)])\/len(predictions[0])","5d44c655":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Holt linear\")\nfig.show()","41850e25":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = ExponentialSmoothing(row, seasonal_periods=3).fit()\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_exponential = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","888b597f":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Exponential smoothing\")\nfig.show()","254ec77c":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/Y2khrpVo6qI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","7e31551a":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit()\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","ff09c945":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","b198a12d":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/95-HMzxsghY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","d61f4083":"dates = [\"2007-12-\" + str(i) for i in range(1, 31)]\npredictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    df = pd.DataFrame(np.transpose([dates, row]))\n    df.columns = [\"ds\", \"y\"]\n    model = Prophet(daily_seasonality=True)\n    model.fit(df)\n    future = model.make_future_dataframe(periods=30)\n    forecast = model.predict(future)[\"yhat\"].loc[30:].values\n    predictions.append(forecast)\npredictions = np.array(predictions).reshape((-1, 30))\nerror_prophet = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","c6916c04":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Prophet\")\nfig.show()","b711d3cd":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\ntime_series_data = sales_train_val[time_series_columns]\nforecast = pd.DataFrame(time_series_data.iloc[:, -28:].mean(axis=1))\nforecast = pd.concat([forecast] * 28, axis=1)\nforecast.columns = [f'F{i}' for i in range(1, forecast.shape[1] + 1)]\nvalidation_ids = sales_train_val['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\nids = np.concatenate([validation_ids, evaluation_ids])\npredictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast] * 2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)\npredictions.to_csv('submission.csv', index=False)","c45761d3":"error = [error_naive, error_avg, error_holt, error_exponential, error_arima, error_prophet]\nnames = [\"Naive approach\", \"Moving average\", \"Holt linear\", \"Exponential smoothing\", \"ARIMA\", \"Prophet\"]\ndf = pd.DataFrame(np.transpose([error, names]))\ndf.columns = [\"RMSE Loss\", \"Model\"]\npx.bar(df, y=\"RMSE Loss\", x=\"Model\", color=\"Model\", title=\"RMSE Loss vs. Model\")","ce10f745":"## Holt linear <a id=\"3.4\"><\/a>\n\n**Holt linear** \ub294 \uccab \ub450 \ubaa8\ub378\uacfc \uc644\uc804\ud788 \ub2e4\ub985\ub2c8\ub2e4. Holt linear\ub294 \uc120\ud615 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uacc4\uc5f4\uc5d0\uc11c \ub192\uc740 \ub808\ubca8\uc758 \uacbd\ud5a5\uc744 \ucc3e\uc73c\ub824 \ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \ub2e4\uc74c\ucc98\ub7fc \uc694\uc57d\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.: \n\n<!--\nThe **Holt linear** is completely different from the first two methods. Holt linear attempts to capture the high-level trends in time series data using a linear function. The method can be summarized as follows:\n-->\n\n### Forecast, level, and trend equations respectively","23b7918a":"\uc704 \uadf8\ub798\ud504\ub294 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac01 \uc0c1\uc815\uc5d0 \ub300\ud55c \ud310\ub9e4 \ubd84\ud3ec\ub97c \ube44\uad50\ud55c\ub2e4. \uce98\ub9ac\ud3ec\ub2c8\uc544\uc758 \uc0c1\uc810\uc740 \ud310\ub9e4\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \ubd84\uc0b0\uc744 \uac00\uc9c4 \uac83\ucc98\ub7fc \ubcf4\uc778\ub2e4. \uadf8\ub9ac\uace0 \uadf8\uac83\uc740 \uce98\ub9ac\ud3ec\ub2c8\uc544\uc5d0\uc11c\uc758 \uc77c\ubd80 \uc7a5\uc18c\uac00 \ub2e4\ub978 \uacf3\ubcf4\ub2e4 \uc0c1\ub2f9\ud788 \ube60\ub974\uac8c \uc131\uc7a5\ud55c\ub2e4\ub294 \uac83\uc744 \uac00\ub9ac\ud0ac \uc9c0\ub3c4 \ubaa8\ub978\ub2e4. *\uc608)* \uac1c\ubc1c \ubd88\uade0\ud615\uc774 \uc788\ub2e4. \ubc18\uba74\uc5d0, \uc704\uc988\ucf54\uc2e0\uacfc \ud14d\uc0ac\uc2a4\ub294 \ub9ce\uc740 \ubd84\uc0b0\uc774 \uc5c6\uc774 \uaf64 \uc548\uc815\uc801\uc73c\ub85c \ubcf4\uc778\ub2e4. \uc774\uac83\uc740 \uac1c\ubc1c\uc774 \uc8fc \uc548\uc5d0\uc11c \ub354 \ub3d9\uc77c\ud560 \uac83\uc774\ub77c\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uac8c\ub2e4\uac00, \uce98\ub9ac\ud3ec\ub2c8\uc544 \uc0c1\uc810\uc740 \ub610\ud55c \uac00\uc7a5 \ub192\uc740 \uc804\uccb4 \ud3c9\uade0 \ud310\ub9e4\ub97c \uac00\uc9c4 \uac83 \uac19\ub2e4.\n\n<!--\nThe above plot compares the sales distribution for each store in the dataset. The stores in California seem to have the highest variance in sales, which might indicate that some places in California grow significantly faster than others, *i.e.* there is development disparity. On the other hand, the Wisconsin and Texas sales seem to be quite consistent among themselves, without much variance. This indicates that development might be more uniform in these states. Moreover, the California stores also seem to have the highest overall mean sales.\n-->","18fff3f2":"# Modeling <a id=\"3\"><\/a>\n\n\uc774\uc81c \uc800\ub294 \uc5b4\ub5bb\uac8c \ud310\ub9e4\uac00 \ub2e4\uc591\ud55c \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud574\uc11c \uc608\uce21\ub420 \uc218 \uc788\ub294 \uc9c0 \uc99d\uba85\ud560 \uac83\uc785\ub2c8\ub2e4, \ub2e4\uc2dc \ub9d0\ud574\uc11c: **naive approach, moving average, Holt linear, exponetial smoothing, ARIMA, and Prophet**\n\n<!--\nNow, I will demonstrate how sales can be forecasted using various methods, namely: **naive approach, moving average, Holt linear, exponential smoothing, ARIMA, and Prophet**\n-->\n\n## Train\/Val split <a id=\"3.1\"><\/a>\n\n\uba3c\uc800, \uc6b0\ub9ac\ub294 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \uac80\uc99d\ud558\uae30 \uc704\ud574 \uc791\uc740 \ud2b8\ub808\uc774\ub2dd\uacfc \uc14b\uc744 \ub9cc\ub4e4 \ud544\uc694\uac00 \uc788\ub2e4. \uc800\ub294 \uc720\ud6a8 \ub370\uc774\ud130\ub85c \ub9c8\uc9c0\ub9c9 30\uc77c\uc758 \ud310\ub9e4\ub97c \uc0ac\uc6a9\ud560 \uac83\uc774\uace0, \uadf8 \uc804\uc758 70\uc77c\uc758 \ud310\ub9e4\ub97c \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub85c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0\uc11c \ud310\ub9e4\ub7c9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc720\ud6a8 \ub370\uc774\ud130\uc5d0\uc11c \ud310\ub9e4\ub97c \uc608\uce21\ud560 \ud544\uc694\uac00 \uc788\ub2e4.\n\n<!--\nFirst, we need to create miniature training and validation sets to train and validate our models. I will use the last 30 days' sales as the validation data and the sales of the 70 days before that as the training data. We need to predict the sales in the validation data using the sales in the training data.\n-->","9ce0ee5a":"\"M5 Forecasting - Accuracy\" competition\uc5d0 \uc624\uc2e0 \uac78 \ud658\uc601\ud569\ub2c8\ub2e4! \uc774 competition\uc5d0\uc120, \ucc38\uac00\uc790\ub4e4\uc774 \uce98\ub9ac\ud3ec\ub2c8\uc544, \ud14d\uc0ac\uc2a4, \uc704\uc2a4\ucf58\uc2e0 \uc8fc\uc5d0\uc11c\uc758 \uacc4\uce35\uc801\uc778 \ud310\ub9e4\ub97c \uae30\ubc18\uc73c\ub85c \uc6d6\ub9c8\ud2b8\uc5d0\uc11c \ubbf8\ub798 \ud310\ub9e4\ub97c \uc608\uce21\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ud310\ub9e4, \uc218\uc775, \uc7ac\uace0 \uac00\uaca9\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774 \uacbd\uc81c\ud559\uc5d0\uc11c \uace0\uc804\uc801\uc778 \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4, \uadf8\ub9ac\uace0 \uc774 \uac83\uc740 \ud22c\uc790\uc790\ub4e4\uc774 \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c4 \uc608\uce21\uc5d0 \uae30\ubc18\ud55c \uacb0\uc815\uc744 \ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc911\uc694\ud569\ub2c8\ub2e4.\n\n\uc774 \ucee4\ub110\uc5d0\uc11c, \uc800\ub294 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c \uac04\ub7b5\ud788 \uc18c\uac1c\ud560 \uac83\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub098\uc11c, \uc800\ub294 Matploylib\uacfc Plotly\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc744 \uc2dc\uac01\ud654\ud560 \uac83\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc800\ub294 \uc5b4\ub5bb\uac8c \uc774 \ubb38\uc81c\uac00 \uc608\uce21 \uc54c\uace0\ub9ac\uc998\uc758 \ub2e4\uc591\uc131\uc5d0 \uc811\uadfc\ub420 \uc218 \uc788\ub294 \uc9c0 \uc99d\uba85\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n<font size=3 color=\"red\">\ub9cc\uc57d \uc774 \ucee4\ub110\uc774 \ub9c8\uc74c\uc5d0 \ub4e0\ub2e4\uba74, \ud22c\ud45c\ud574\uc8fc\uc138\uc694. \uc81c\uac00 \ub354 \uc88b\uc740 \ud004\ub9ac\ud2f0\uc758 \ucf58\ud150\uce20\ub97c \ub9cc\ub4dc\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. :)<\/font>\n\n<!--\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin. Forecasting sales, revenue, and stock prices is a classic application of machine learning in economics, and it is important because it allows investors to make guided decisions based on forecasts made by algorithms. \n\nIn this kernel, I will briefly explain the structure of dataset. Then, I will visualize the dataset using Matplotlib and Plotly. And finally, I will demonstrate how this problem can be approached with a variety of forecasting algorithms.\n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>\n-->","ff0ee0ca":"\uc544\ub798 \uadf8\ub9bc\uc740 \uc774 \uadf8\ub798\ud504\ub4e4\uc744 \uc606\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd08\ub85d \uadf8\ub798\ud504\ub294 \uc6d0\ubcf8 \ud310\ub9e4\ub97c \ub098\ud0c0\ub0b4\uace0 \ube68\uac04 \uadf8\ub798\ud504\ub294 denoised \ud310\ub9e4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\n<!--\nThe below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.\n-->","1de9de8e":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ub450 \uac1c\uc758 smoothing \ubc29\uc2dd: moving average and exponential smoothing\uc774 \ucd5c\uace0\uc758 \uc810\uc218\ub97c \ubc1b\uc740 \ubaa8\ub378\uc774\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. Holt linear\ub294 \uadf8\ub807\uac8c \ub5a8\uc5b4\uc9c0\uc9c4 \uc54a\uc2b5\ub2c8\ub2e4. \ub0a8\uc740 models: naive approach, ARIMA, Prophet\uc740 \ucd5c\uc545\uc758 \uc810\uc218\ub97c \ubc1b\uc740 \ubaa8\ub378\uc785\ub2c8\ub2e4. \ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc800\ub294 ARIMA\uc640 Prophet\uc758 \uc815\ud655\ub3c4\uac00 hyperparameters\ub97c \uc870\uc808\ud558\uba74 \uc0c1\ub2f9\ud788 \uc0c1\uc2b9\ub420 \uc218 \uc788\ub2e4\uace0 \ubbff\uc2b5\ub2c8\ub2e4.\n\n<!--\nFrom the above graph, we can see that the two smoothing methods: moving average and exponential smoothing are the best-scoring models. Holt linear is not far behind. The remaining models: naive approach, ARIMA, and Prophet are the worst-scoring models. I believe that the accuracy of ARIMA and Prophet can be boosted significantly by tuning the hyperparameters.\n-->","c3e79c86":"\ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc784\uc758\ub85c \uc120\ud0dd\ub41c \uc0c1\uc810\uc5d0\uc11c\uc758 \ud310\ub9e4 \ub370\uc774\ud130\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc608\uc0c1\ucc98\ub7fc, \uc815\ud574\uc9c4 \ub0a0\uc758 \ud310\ub9e4\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ub9e4\uc6b0 \ub9ce\uc740 \uc694\uc18c\ub4e4\uc774 \uc788\ub2e4\ub294 \uc0ac\uc2e4\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0, \ud310\ub9e4 \ub370\uc774\ud130\ub294 \ub9e4\uc6b0 \uc624\ub958\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \ud2b9\uc815\ud55c \ub0a0\ub9c8\ub2e4, \ud310\ub9e4\ub7c9\uc774 0\uc774 \ub418\ub294\ub370 \uadf8\uac83\uc740 \uadf8 \ub0a0\uc5d0 \ud2b9\uc815 \uc0c1\ud488\uc744 \uc774\uc6a9\ud560 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (Rob\uc758 \ucee4\ub110\uc5d0\uc11c \uae30\ub85d\ub41c \uac83 \ucc98\ub7fc)\n\n<!--\nThese are sales data from randomly selected stores in the dataset. As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day (as noted by Rob in his kernel).\n-->","42b605db":"### Average smoothing\n\nAverage smooting\uc740 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc5d0\uc11c denoise \ud558\ub294\ub370 \ube44\uad50\uc801 \uac04\ub2e8\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uace0\uc815\ub41c \ud06c\uae30\uc758 (10 \ucc98\ub7fc) \"window\"\ub97c \uac00\uc838\uc635\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uba3c\uc800 \uc2dc\uacc4\uc5f4 \ucd08\ubc18\uc5d0 window\ub97c \ub193\uc2b5\ub2c8\ub2e4. (\uccab 10\uac1c\uc758 \uc6d0\uc18c) \uadf8\ub9ac\uace0 \uadf8 \ubd80\ubd84\uc758 \ud3c9\uade0\uc744 \uad6c\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774\uc81c \ud2b9\uc815\ud55c \"stride\"\uc5d0 \uc758\ud574 \uc55e\ucabd\uc73c\ub85c \uc2dc\uacc4\uc5f4\uc744 \ub530\ub77c window\ub97c \uc6c0\uc9c1\uc774\uace0, \uc0c8 window\uc758 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud558\uace0, \uc774 \uacfc\uc815\uc744 \ubaa8\ub4e0 \uc2dc\uacc4\uc5f4\uc758 \ub05d\uae4c\uc9c0 \ub3c4\ub2ec\ud560 \ub584\uae4c\uc9c0 \ubc18\ubcf5\ud569\ub2c8\ub2e4.  \uc6b0\ub9ac\uac00 \uacc4\uc0b0\ud55c \ubaa8\ub4e0 \ud3c9\uade0\uac12\uc740 \uc0c8\ub85c\uc6b4 \uc2dc\uacc4\uc5f4\ub85c \uc5f0\uad00\ub429\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uadf8\uac83\uc740 denoised \ud310\ub9e4 \ub370\uc774\ud130\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.\n\n<!--\nAverage smooting is a relatively simple way to denoise time series data. In this method, we take a \"window\" with a fixed size (like 10). We first place the window at the beginning of the time series (first ten elements) and calculate the mean of that section. We now move the window across the time series in the forward direction by a particular \"stride\", calculate the mean of the new window and repeat the process, until we reach the end of the time series. All the mean values we calculated are then concatenated into a new time series, which forms the denoised sales data.\n-->","98f64228":"\uc774\uc81c \uc6b0\ub9ac\uc758 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc5d0 Prophet\uc774 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub294 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font>, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font> \uadf8\ub9ac\uace0 \uc608\uce21\uce58\ub294 <font color=\"green\">green<\/font> \uc785\ub2c8\ub2e4.\n\n<!--\nNow let us see how Prophet performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","b1c0f719":"# The dataset <a id=\"1\"><\/a>\n\n\ub370\uc774\ud130\uc14b\uc740 5\uac1c\uc758 .csv \ud30c\uc77c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.\n\n* <code>calendar.csv<\/code> - \uc5b4\ub5a4 \uc0c1\ud488\uc774 \ud310\ub9e4\ub418\uc5c8\ub294 \uc9c0\uc5d0 \ub300\ud55c \ub0a0\uc9dc\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \ub0a0\uc9dc\ub294 <code>yyyy\/dd\/mm<\/code> \ud615\uc2dd\uc73c\ub85c \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\n* <code>sales_train_validation.csv<\/code> - \uc0c1\ud488 \ubcc4\ub85c \ub9e4\uc77c \uae30\ub85d\ub41c \uac1c\ub2f9 \ud310\ub9e4 \uc815\ubcf4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 <code>[d_1 - d_1913]<\/code> \ub85c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\n* <code>submission.csv<\/code> - competition\uc5d0\uc11c \uc81c\ucd9c\uc744 \uc704\ud55c \uc62c\ubc14\ub978 \ud3ec\ub9f7\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n* <code>sell_prices.csv<\/code> - \uc0c1\uc810\uacfc \ub0a0\uc9dc\ub9c8\ub2e4 \ud310\ub9e4\ub41c \uc0c1\ud488\uc758 \uac00\uaca9\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.\n\n* <code>sales_train_evaluation.csv<\/code> - competition \ub9c8\uac10 \uc804\uc5d0 \ud55c\ub2ec \ub3d9\uc548 \uc774\uc6a9\uac00\ub2a5\ud569\ub2c8\ub2e4. <code>[d_1 - d_1941]<\/code>\uc5d0 \ub300\ud55c \ud310\ub9e4\ub97c \ud3ec\ud568\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n\uc774 competition\uc5d0\uc11c \uc800\ud76c\ub294 <code>[d_1942 - d_1969]<\/code>\uc5d0 \ub300\ud55c \ud310\ub9e4\ub97c \uc608\uce21\ud574\uc57c \ud569\ub2c8\ub2e4. \ud589\ub4e4\uc740 evaluation set\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \ud589 <code>[d_1914 - d_1941]<\/code> \ub294 validation set\uc774\uace0, \ub098\uba38\uc9c0 \ud589\ub4e4\uc740 training set\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc774\uc81c \ub370\uc774\ud130\uc14b\uc744 \uc774\ud574\ud558\uace0, \ubb34\uc5c7\uc744 \uc608\uce21\ud560 \uc9c0 \uc54c\uc544\ubcf4\uace0, \ub370\uc774\ud130\uc14b\uc744 \uc2dc\uac01\ud654\ud569\uc2dc\ub2e4.\n\n<!--\nThe dataset consists of five .csv files.\n\n* <code>calendar.csv<\/code> - Contains the dates on which products are sold. The dates are in a <code>yyyy\/dd\/mm<\/code> format.\n\n* <code>sales_train_validation.csv<\/code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]<\/code>.\n\n* <code>submission.csv<\/code> - Demonstrates the correct format for submission to the competition.\n\n* <code>sell_prices.csv<\/code> - Contains information about the price of the products sold per store and date.\n\n* <code>sales_train_evaluation.csv<\/code> - Available one month before the competition deadline. It will include sales for <code>[d_1 - d_1941]<\/code>.\n\nIn this competition, we need to forecast the sales for <code>[d_1942 - d_1969]<\/code>. These rows form the evaluation set. The rows <code>[d_1914 - d_1941]<\/code> form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.\n-->","b6e9bb94":"\uc6b0\ub9ac\ub294 naive approach\uc5d0 \uc758\ud574 \ub9cc\ub4e4\uc5b4\uc9c4 \uc608\uce21\uc774 \uc815\ud655\ud558\uc9c0 \uc54a\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uace0, \uadf8\uac83\uc740 \ub2e8\uc21c\ud55c \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \uc608\uce21\ub41c\ub2e4\uace0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc608\uce21 \ud558\uae30 \uc704\ud574 \uc5ec\ub7ec time stamps \ub97c \uc0ac\uc6a9\ud558\ub294 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n<!--\nWe can see that the forecasts made by the naive approach are not accurate and it is to be expected of such a simple algorithm. We need more complex models which use several time stamps to make forecasts.\n-->","0d20330a":"ARIMA\ub294 \ub0ae\uc740 \ub808\ubca8\uacfc \ub192\uc740 \ub808\ubca8\uc758 \uacbd\ud5a5\uc744 \ub3d9\uc2dc\uc5d0 \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub4e4 \uc911 \uc624\uc9c1 \ud558\ub098\ub9cc \ucc3e\uc744 \uc218 \uc788\ub294 \ub300\ubd80\ubd84\uc758 \ub2e4\ub978 \ubaa8\ub378\uacfc\ub294 \ub2ec\ub9ac. \uc774\uac83\uc740 \uac01 \uc0d8\ud50c\uc744 \uc704\ud574 \uc8fc\uae30\uc801\uc778 \ud568\uc218\ub97c \uc608\uce21\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc774 \ud568\uc218\ub4e4\uc740 \uaf64 \uc815\ud655\ud55c \uac83\ucc98\ub7fc \ubcf4\uc785\ub2c8\ub2e4. (\ub450 \ubc88\uc9f8 \uc0d8\ud50c\uc744 \uc81c\uc678\ud558\uace0)\n\n<!--\nARIMA is able to find low-level and high-level trends simultaneously, unlike most other models which can only find one of these. It is able to predict a periodic function for each sample, and these functions seem to be pretty accurate (except for the second sample).\n-->","11db4557":"\uc6b0\ub9ac\ub294 \uc774 \ubaa8\ub378\uc774 naive approach \ubcf4\ub2e4 \ub354 \uc798 \uc218\ud589\ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \uc77c\uc77c \ud310\ub9e4 \ub370\uc774\ud130\uc5d0 \ubcc0\ub355\uc5d0 \ub35c \ubbfc\uac10\ud558\uace0, \uc57d\uac04 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uac00\uc9c0\ub294 \uacbd\ud5a5\uc744 \uc120\ud0dd\ud558\ub3c4\ub85d \uad00\ub9ac\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uc544\uc9c1\ub3c4 \ud310\ub9e4\uc5d0\uc11c \ub192\uc740  \ub808\ubca8\uc758 \uacbd\ud5a5\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n<!--\nWe can see that this model performs better than the naive approach. It is less susceptible to the volatility in day-to-day sales data and manages to pick up trends with slightly higher accuracy. However, it is still unable to find high-level trends in the sales. \n-->","901e05b1":"\uc6b0\ub9ac\ub294 Holt linear\uac00 \ub9e4\uc6b0 \uc77c\uad00\ub418\uac8c \ud310\ub9e4\ub7c9\uc758 \ub192\uc740 \uc218\uc900\uc744 \uacbd\ud5a5\uc744 \uc608\uce21\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098, \ub2e4\ub978 \ud568\uc218\ub4e4\ucc98\ub7fc \uc815\ud655\ud558\uac8c \ud310\ub9e4\ub7c9\uc5d0\uc11c \uc9e7\uc740 \uc2dc\uac04\uc758 \ubcc0\ub3d9\uc744 \uc7a1\uc744 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \uc544\ub9c8\ub3c4 \uc774 \ud568\uc218\ub294 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c \ub2e4\ub978 \ub0ae\uc740 \ub808\ubca8\uc758 \uc608\uce21\uacfc \uc870\ud569\ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4.  \n\n<!--\nWe can see that Holt linear is able to predict high-level trends in the sales very consistently. But, it is not able to capture the short-term volatility in the sales as accurately as other methods. Maybe this method can be combined with other low-level forecasters to produce better results.\n-->","230a7458":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc5b4\ub450\uc6b4 \uc120\uadf8\ub798\ud504\ub294 denoised\ub41c \ud310\ub9e4\ub97c \ub098\ud0c0\ub0b4\uace0 \ubc1d\uc740 \uc120\uadf8\ub798\ud504\ub294 \uc6d0\ubcf8 \ud310\ub9e4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 Wavelet denoising\uc774 noise\uc5d0 \uc758\ud574 \ud750\ud2b8\ub7ec\uc9c4 \uac83 \uc5c6\uc774 \ud310\ub9e4 \ub370\uc774\ud130\uc5d0\uc11c \uc131\uacf5\uc801\uc73c\ub85c \"general trend\"\ub97c \ucc3e\uc544\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud310\ub9e4\uc5d0\uc11c \uc774 \ub192\uc740 \uacbd\ud5a5\uc774\ub098 \ud328\ud134\uc744 \ubc1c\uacac\ud558\ub294 \uac83\uc740 \ubaa8\ub378\uc744 \ud2b8\ub808\uc774\ub2dd \ud558\ub294\ub370 feature\uc744 \uc0dd\uc131\ud558\ub294\ub370 \uc720\uc6a9\ud560 \uc9c0\ub3c4 \ubaa8\ub985\ub2c8\ub2e4.\n\n<!--\nIn the above graphs, the dark lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that Wavelet denoising is able to successfully find the \"general trend\" in the sales data without getting distracted by the noise. Finding these high- trends or patterns in the sales may be useful in generating features to train a model.\n-->","8168a635":"<img src=\"https:\/\/i.imgur.com\/YB551eX.png\" width=\"600px\"> ","c25a7ee6":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uc6cc\uc2f1\ud134 \uc0c1\uc810 \uc911\uc5d0\uc11c \ub9e4\uc6b0 \ub0ae\uc740 \ucc28\uc774\ub97c \ubcfc \uc218 \uc788\ub2e4. \ud310\ub9e4 \uace1\uc120\uc740 \ub9e4\uc6b0 \uc790\uc8fc \uc11c\ub85c \uacb9\uce5c\ub2e4. \uc774\uac83\uc740 \uc704\uc2a4\ucf54\uc2e0\uc758 \ub300\ubd80\ubd84\uc774 \ube44\uc2b7\ud55c \"development curve\" \uac16\uace0 \uc788\uace0, \uc8fc\ub97c \uac74\ub108\uba74\uc11c \uac1c\ubc1c\uc5d0\uc11c\ub294 \ub354 \ub300\ub2e8\ud55c \uacf5\ud3c9\ud568\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uac00\ub9ac\ud0a4\ub294 \uac83 \uac19\ub2e4. \uac1c\ubc1c\uc5d0\uc11c \uad6c\uccb4\uc801\uc778 \"hotspots\" \uc774\ub098 \"hubs\"\uac00 \uc5c6\ub2e4. \uac10\uc18c\ud558\ub294 \uc21c\uc11c\ub85c \ud3c9\uade0 \ud310\ub9e4\ub294 <code>WI_2, WI_3, WI_1<\/code> \uc774\ub2e4. \uc0c1\uc810 <code>WI_2<\/code>\ub294 \ucd5c\ub300 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\uace0, \ubc18\uba74\uc5d0 \uc0c1\uc810 <code>WI_2<\/code>\ub294 \ucd5c\ub300 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\uace0 \ubc18\uba74\uc5d0 <code>WI_1<\/code> \ucd5c\uc18c \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nIn the above graph, we can see a very low disparity in sales among Wisconsin stores. The sales curves intersect each other very often. This may indicate that most parts of Wisconsin have a similar \"development curve\" and that there is a greater equity in development across the state. There are no specific \"hotspots\" or \"hubs\" of development. The average sales in descending order are <code>WI_2, WI_3, WI_1<\/code>. The store <code>WI_2<\/code> has maximum sales while the store <code>WI_1<\/code> has minimum sales. \n-->","8990ce43":"# EDA <a id=\"2\"><\/a>\n\n\uc774\uc81c, \ud310\ub9e4 \ub370\uc774\ud130\ub4e4\uc744 \uc2dc\uac01\ud654\ud558\uace0 \ub370\uc774\ud130\uc5d0\uc11c \uba87\uba87\uc758 insight\ub97c \uc5bb\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n<!--\nNow, I will try to visualize the sales data and gain some insights from it.\n-->","0e4744bf":"## Stores and states <a id=\"2.4\"><\/a>\n\uc774\uc81c \uc800\ub294 \uba87\uba87\uc758 \uc720\uc6a9\ud55c \ud1b5\ucc30\uc744 \uc5bb\uae30 \uc704\ud574 \ub2e4\ub978 \uc0c1\uc810\uacfc \uc8fc \ucabd\uc73c\ub85c \ud310\ub9e4 \ub370\uc774\ud130\ub97c \ubcfc \uac83\uc785\ub2c8\ub2e4.\n\n<!--\nNow, I will look at the sales data across different stores and states in order to gain some useful insights.\n-->","64f535f3":"# Takeaways\n\n* \uc8fc\ub9c8\ub2e4 \ud310\ub9e4\ub7c9\uc5d0 \ub300\ud574 \ub2e4\ub978 \ud3c9\uade0\uacfc \ubd84\uc0b0\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \uc8fc\ub9c8\ub2e4 \uac1c\ubc1c \ubd84\ud3ec\uac00 \ub2e4\ub978 \uac83\uc744 \uac00\ub9ac\ud0b5\ub2c8\ub2e4.\n\n* \ub300\ubd80\ubd84\uc758 \ud310\ub9e4\ub7c9\uc740 \uc120\ud615\uc801\uc778 \uacbd\ud5a5\uc758 \uc0ac\uc778 \ud568\uc218 \ubaa8\uc591\uc785\ub2c8\ub2e4. \uac70\uc2dc\uc801\uc778 \uacbd\uc81c \uc21c\ud658\uc744 \uc5f0\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.\n\n* \uba87\uba87\uc758 \ube44ML \ubc29\uc2dd\uc740 \uc2dc\uacc4\uc5f4\uc744 \uc608\uce21\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. moving average\uc640 exponential smoothing\uc774 \ub9e4\uc6b0 \uc88b\uc740 \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\n* ARIMA\uc640 Prophet\uc758 \uc131\ub2a5\uc740 \ub354 \ub9ce\uc740 hyperparameter \uc870\uc808\uc744 \ud1b5\ud574 \ud5a5\uc0c1\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\n* Different states have different mean and variance of sales, indicating differences in the distribution of development in these states.\n\n* Most sales have a linearly trended sine wave shape, reminiscent of the macroeconomic business cycle.\n\n* Several non-ML models can be used to forecast time series data. Moving average and exponential smoothing are very good models.\n\n* ARIMA and Prophet's performance can be boosted with more hyperparamter tuning.\n-->","66ec421f":"### Rolling Average Price vs. Time for each store","f3e7636f":"### Load the data","d626a8c2":"# Introduction","33b98798":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc800\ub294 \ub370\uc774\ud130\uc14b\uc758 \ubaa8\ub4e0 \uc0c1\uc810\uc744 \ub530\ub77c rolling sales \ub97c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uac70\uc758 \ubaa8\ub4e0 \ud310\ub9e4 \uace1\uc120\uc740 \uac70\uc2dc\uc801\uc778 \ub808\ubca8\uc5d0\uc120 \"linear oscillation\" \uacbd\ud5a5\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c, \ud310\ub9e4\ub7c9\uc740 \ud2b9\uc815 \ud3c9\uade0 \uac12\uc5d0 \ub300\ud574 \uc0ac\uc778 \uace1\uc120\ucc98\ub7fc \uc9c4\ub3d9\ud55c\ub2e4. \uadf8\ub7ec\ub098 \uc774\uac83\uc740 \uc704\ucabd\uc73c\ub85c \uc120\ud615 \ucd94\uc138\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4. \uc774\uac83\uc740 \ud310\ub9e4\ub7c9\uc774 \uba87\ub2ec\ub9c8\ub2e4 \uc810\uc810 \ub354 \ucee4\uc9c0\ub294 \ub808\ubca8\uc5d0\uc11c \uc9c4\ub3d9\ud55c\ub2e4\ub294 \uac83\uc744 \ub0b4\ud3ec\ud55c\ub2e4.\n\n\uc774\uac83\uc740 **business cycle**\uc744 \uc5f0\uc0c1\uc2dc\ud0a8\ub2e4. \uadf8\uac83\uc740 \uacbd\uc81c\ud559\uc774 \uc9e7\uc740 \uae30\uac04\uc758 \uc9c4\ub3d9\ud558\ub294 \ubcc0\ub3d9\uc744 \uac00\uc9c0\uace0 \uc788\uc73c\ub098 \uae34 \ubc94\uc704\uc5d0\uc11c\ub294 \uc120\ud615\uc801\uc73c\ub85c \uc99d\uac00\ud55c\ub2e4\ub294 \uac83 \ub098\ud0c0\ub0b8\ub2e4. \uc544\ub9c8\ub3c4, \uc0c1\uc810 \ub808\ubca8\uc5d0\uc11c \uadf8\ub7f0 \uc791\uc740 \uaddc\ubaa8\uc758 \uacbd\ud5a5\uc774 \uac70\uc2dc\uc801\uc778 \ub808\ubca8\uc5d0\uc11c \ubcf8 \uacbd\ud5a5\uc744 \uacb0\uc815\ud558\uae30 \uc704\ud574 \uc62c\ub77c\uac04\ub2e4. \uc544\ub798\ub294 \uac70\uc2dc\uc801\uc778 \ube44\uc9c0\ub2c8\uc2a4 \uc0ac\uc774\ud074\uc758 \uadf8\ub9bc\uc774\ub2e4.\n\n<!--\nIn the above graph, I have plotted rolling sales across all stores in the dataset. Almost every sales curve has \"linear oscillation\" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nThis trend is reminiscent of the **business cycle**, where economies have short-term oscillatory fluctuations but grow linearly in the long run. Maybe, such small-scale trends at the level of stores add up to decide trends we see at the macroeconomic level. Below is an illustration of the macroeconomic business cycle:\n-->","c3349d2d":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac19\uc740 \uad00\uacc4\ub97c \uc54c \uc218 \uc788\ub2e4. \uc0c1\uc810 <code>TX_2<\/code> \ucd5c\ub300 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\uace0, \ubc18\uba74\uc5d0 \uc0c1\uc810 <code>TX_1<\/code> \ucd5c\uc18c \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nIn the above plots, we can see the same relationship. The store <code>TX_2<\/code> has maximum sales while the store <code>TX_1<\/code> has minimum sales. \n-->","880a9e02":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ud14d\uc0ac\uc2a4 \uc0c1\uc810 \uc911\uc5d0\uc11c \ud310\ub9e4 \ucc28\uc774\uac00 \uac00\uc7a5 \ub0ae\uc740 \uac83\uc744 \ub2e4\uc2dc \ud55c \ubc88  \ubcfc \uc218 \uc788\ub2e4. \ud310\ub9e4 \ucee4\ube0c\ub294 \uc885\uc885 \uc11c\ub85c \uacb9\uce5c\ub2e4, \ube44\ub85d \uc704\uc2a4\ucf58\uc2e0 \ucc98\ub7fc \uc790\uc8fc\ub294 \uc544\ub2c8\uc9c0\ub9cc. \uc774\uac83\uc740 \ud14d\uc0ac\uc2a4\uc758 \ub300\ubd80\ubd84\uc758 \uc9c0\uc5ed\uacfc \ube44\uc2b7\ud55c \"development curve\"\ub97c \uac00\uc9c0\uace0 \uc788\uace0 \uc8fc\ub97c \ub118\uc5b4\uc11c \uac1c\ubc1c\uc5d0 \ub354 \ub3d9\ub4f1\ud568\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \ub2e4\uc2dc \ud55c \ubc88 \uac00\ub9ac\ud0a8\ub2e4. \ubd84\uc0b0\uc740 \uc704\uc2a4\ucf58\uc2e0 \ubcf4\ub2e4 \ub354 \ub192\ub2e4 \uadf8\ub7ec\ub098 \ud14d\uc0ac\uc2a4\uc5d0\uc11c\uc758 \uac1c\ubc1c \"hubs\" \uc77c \uc9c0\ub3c4 \ubaa8\ub978\ub2e4. \uce98\ub9ac\ud3ec\ub2c8\uc544\uc5d0\uc11c \ub9cc\ud07c \ud45c\ud604\ub418\uc9c0 \uc54a\uc744 \uc9c0 \ubaa8\ub978\ub2e4. \uac10\uc18c\ud558\ub294 \uc21c\uc11c\uc758 \ud3c9\uade0 \ud310\ub9e4\ub294 <code>TX_2, TX_3, TX_1<\/code> \uc774\ub2e4. \uc0c1\uc810 <code>TX_2<\/code>\ub294 \ucd5c\ub300 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\uace0, \uc0c1\uc810 <code>TX_1<\/code>\ub294 \ucd5c\uc18c \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nIn the above graph, we can once again see that a very low disparity in sales among Texas stores. The sales curves intersect each other often, albeit not as often as in Wisconsin. This might once again indicate that most parts of Texas have a similar \"development curve\" and that there is a greater equity in development across the state. The variance here is higher than in Wisconsin though, so there might be \"hubs\" of development in Texas as well, but not as pronounced as in California. The average sales in descending order are <code>TX_2, TX_3, TX_1<\/code>. The store <code>TX_2<\/code> has maximum sales while the store <code>TX_1<\/code> has minimum sales.\n-->","947325fd":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac19\uc740 \ud2b8\ub80c\ub4dc\ub97c \ubcfc \uc218 \uc788\ub2e4: \uce98\ub9ac\ud3ec\ub2c8\uc544 \uc0c1\uc810\uc740 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubaa8\ub4e0 \uc0c1\uc810 \uc911\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \ubd84\uc0b0\uacfc \ud3c9\uade0 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nFrom the above graph, we can see the same trends: California stores have the highest variance and mean sales among all the stores in the dataset.\n-->","e6665d67":"<img src=\"https:\/\/i.imgur.com\/iemn4xo.jpg\" width=\"400px\">","de0670dc":"## Sales data <a id=\"2.2\"><\/a>\n\n\n### Sample sales data","7f6bd802":"# Contents\n\n* [<font size=4>The dataset<\/font>](#1)\n\n\n* [<font size=4>EDA<\/font>](#2)\n    * [Preparing the ground](#2.1)\n    * [Sales data](#2.2)\n    * [Denoising](#2.3)\n    * [Stores and sales](#2.4)\n\n    \n* [<font size=4>Modeling<\/font>](#3)\n    * [Train\/Val split](#3.1)\n    * [Naive approach](#3.2)\n    * [Moving average](#3.3)\n    * [Holt linear](#3.4)\n    * [Exponential smoothing](#3.5)\n    * [ARIMA](#3.6)\n    * [Prophet](#3.7)\n    * [Loss for each model](#3.8)\n\n\n* [<font size=4>Takeaways<\/font>](#4)\n\n\n* [<font size=4>Ending Note<\/font>](#5)","700d1960":"\uc544\ub798\ub294 \uc138 \uac1c\uc758 \uc0d8\ud50c \ub370\uc774\ud130 \ubd80\ubd84\uc5d0\uc11c\uc758 \ud310\ub9e4\ub7c9\uc785\ub2c8\ub2e4. \uc800\ub294 \ubaa8\ub378\uc758 \uc791\ub3d9\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574 \uc774 \uc0d8\ud50c\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uac83 \uc785\ub2c8\ub2e4.\n\n<!--\nBelow are the sales from three sample data points. I will use these samples to demonstrate the working of the models.\n-->","36166150":"<img src=\"https:\/\/i.imgur.com\/MHgcgGo.png\" width=\"180px\">\n<img src=\"https:\/\/i.imgur.com\/3ImRHEO.png\" width=\"300px\">\n<img src=\"https:\/\/i.imgur.com\/XExnvMX.png\" width=\"300px\">\n\n\uc704 \uc2dd\uc5d0\uc11c $\\alpha$ \uc640 $\\beta$ \ub294 \uad6c\uc131\ub41c \uc0c1\uc218\uc785\ub2c8\ub2e4. *l<sub>t<\/sub>* \uc640 b<sub>t<\/sub> \uc758 \uac12\uc740 **level** \uacfc **trend** \uac12\uc744 \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\ntrend \uac12\uc740 \uc120\ud615 \uc608\uce21\ud568\uc218\uc758 \uae30\uc6b8\uae30\uc774\uace0, level \uac12\uc740 \uc120\ud615\uc608\uce21\ud568\uc218\uc758 *y*-intercept \uc785\ub2c8\ub2e4. \uae30\uc6b8\uae30\uc640 *y*-intercept \uac12\uc740 \ub450\ubc88\uc9f8\uc640 \uc138\ubc88\uc9f8 \uc218\uc815 \uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc18d \uc218\uc815\ub429\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uae30\uc6b8\uae30\uc640 *y*-intercept \uac12\uc740 \uc608\uce21 *y<sub>t+h<\/sub>* (equation 1) \uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc2dd\uc5d0\uc11c *h* time steps \ub294 \ud604\uc7ac time step\ubcf4\ub2e4 \uc55e\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc774 \ubaa8\ub378\uc774 \uc5b4\ub5bb\uac8c \uc6b0\ub9ac\uc758 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc744 \uc218\ud589\ud558\ub294 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font>, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font>\uc774\uace0, \uc608\uce21\uc740 <font color=\"green\">green<\/font> \uc785\ub2c8\ub2e4.\n\n<!--\nIn the above equations, $\\alpha$ and $\\beta$ are constants which can be configured. The values *l<sub>t<\/sub>* and *b<sub>t<\/sub>* represent the **level** and **trend** values repsectively. The trend value is the slope of the linear forecast function and the level value is the *y*-intercept of the linear forecast function. The slope and *y*-intercept values are continuously updated using the second and third update equations. Finally, the slope and *y*-intercept values are used to calculate the forecast, *y<sub>t+h<\/sub>* (in equation 1), which is *h* time steps ahead of the current time step. Now let us see how this model performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","a56d9ba1":"## Exponential smoothing <a id=\"3.5\"><\/a>\n\n**exponential smoothing** \ubc29\uc2dd\uc740 \ud3c9\uade0 smoothing \uacfc \ub2e4\ub978 \uc885\ub958\uc758 smoothing\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\uc804 time steps \uc740 \uae30\ud558\uae09\uc218\uc801\uc778 \uac00\uc911\uce58\ub97c \ubc1b\uc558\uace0, \uc608\uce21\ud558\uae30 \uc704\ud574 \ucd94\uac00\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc2dc\uac04\uc774 \ub354 \ub4a4\ub85c \uac08 \uc218\ub85d \uac00\uc911\uce58 \uac10\uc18c\uac00 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ub2e4\uc74c \ucc98\ub7fc \uc694\uc57d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\nThe **exponential smoothing** method uses a different type of smoothing which differs from average smoothing. The previous time steps are exponentially weighted and added up to generate the forecast. The weights decay as we move further backwards in time. The model can be summarized as follows:\n-->\n\n<img src=\"https:\/\/i.imgur.com\/IqqjOFc.png\" width=\"520px\">\n<img src=\"https:\/\/i.imgur.com\/GiyHyZf.png\" width=\"255px\">\n\n\uc704 \uc2dd\uc5d0\uc11c, $\\alpha$ \ub294 smoothing parameter\uc774\uace0, \uc608\uce21 *y<sub>t+1<\/sub>* \ub294 *y<sub>1<\/sub>, ~ , y<sub>t<\/sub>* \uc5d0\uc11c \ubaa8\ub4e0 \uad00\uce21\uc758 \uac00\uc911\uce58\uac00 \uc801\uc6a9\ub41c \ud3c9\uade0\uc785\ub2c8\ub2e4. \uac00\uc911\uce58 \uac10\uc18c \ube44\uc728\uc740 \ud30c\ub77c\ubbf8\ud130 *a*\uc5d0 \uc758\ud574 \uc870\uc808\ub429\ub2c8\ub2e4. \uc774 \ubc29\uc2dd\uc740 \ubaa8\ub4e0 time steps\uc5d0 \uac19\uc740 \uac00\uc911\uce58\ub97c \uc8fc\ub294 \uac83 (moving average \ubc29\uc2dd\ucc98\ub7fc) \ub300\uc2e0\uc5d0 \ub2e4\ub978 time step\uc5d0 \ub2e4\ub978 \uac00\uc911\uce58\ub97c \uc90d\ub2c8\ub2e4. \uc774\uac83\uc740 \ucd5c\uadfc \ub370\uc774\ud130\uac00 \uc608\uce21\ud560 \ub54c \uc624\ub798\ub41c \ud310\ub9e4\ub7c9\ubcf4\ub2e4 \ub354 \uc911\uc694\ud558\ub2e4\ub294 \uac83\uc73c\ub85c \ud655\uc2e4\ud788 \ud569\ub2c8\ub2e4. \uc774\uc81c \uc6b0\ub9ac\ub294 \uc774 \uc0c8\ub85c\uc6b4 smoothing \ubc29\uc2dd\uc774 \uc5b4\ub5bb\uac8c \uc6b0\ub9ac\uc758 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9\ud558\ub294 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font> \uc774\uace0, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font> \uc774\uace0, \uc608\uce21\uce58\ub294 <font color=\"green\">green<\/font>\n\n<!--\nIn the above equations, $\\alpha$ is the smoothing parameter. The forecast *y<sub>t+1<\/sub>* is a weighted average of all the observations in the series *y<sub>1<\/sub>, \u2026 ,y<sub>t<\/sub>*. The rate at which the weights decay is controlled by the parameter *\u03b1*. This method gives different weightage to different time steps, instead of giving the same weightage to all time steps (like the moving average method). This ensures that recent sales data is given more importance than old sales data while making the forecast. Now let us see how this new smoothing method performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","d2442c82":"## Naive approach <a id=\"3.2\"><\/a>\n\n\uccab \uc811\uadfc\uc740 \ub9e4\uc6b0 \uac04\ub2e8\ud55c **naive approach** \uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \ub2e8\uc9c0 \ud604\uc7ac \ud310\ub9e4\ub85c \ub2e4\uc74c \ud310\ub9e4\ub97c \uc608\uce21\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ub2e4\uc74c\uc73c\ub85c \uc694\uc57d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.:\n\n<!--\nThe first approach is the very simple **naive approach**. It simply forecasts the next day's sales as the current day's sales. The model can be summarized as follows:\n-->\n\n<img src=\"https:\/\/i.imgur.com\/r8wjrzk.png\" width=\"110px\">\n\n\uc704 \uc2dd\uc5d0\uc11c *y<sub>t+1<\/sub>* \ub294 \ub2e4\uc74c \ub0a0\uc758 \ud310\ub9e4\ub7c9\uc744 \uc704\ud55c \uc608\uce21 \uac12\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 *y<sub>t<\/sub>* \ub294 \uc624\ub298\uc758 \ud310\ub9e4\ub7c9\uc785\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc624\ub298 \ud310\ub9e4\ub7c9\uc73c\ub85c \ub0b4\uc77c \ud310\ub9e4\ub7c9\uc744 \uc608\uce21\ud569\ub2c8\ub2e4. \uc9c0\uae08 \uc6b0\ub9ac\uac00 \uc5b4\ub5bb\uac8c \uc774 \uc0d8\ud519 \ubaa8\ub378\uc774 \uc6b0\ub9ac\uc758 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc218\ud589\ud558\ub294 \ubd05\uc2dc\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font> \uc774\uace0, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font>, \uc608\uce21\uc740 <font color=\"green\">green<\/font> \uc785\ub2c8\ub2e4. \n\n<!--\nIn the above equation, *y<sub>t+1<\/sub>* is the predicted value for the next day's sales and *y<sub>t<\/sub>* is today's sales. The model predicts tomorrow's sales as today's sales. Now let us see how this simple model performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","f5d90518":"## ARIMA <a id=\"3.6\"><\/a>\n\n**ARIMA** \ub294 **A**uto **R**egressive **I**ntegrated **M**oving **A**verage \ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. exponential smoothing \ubaa8\ub378\uc774 trend\uc758 \uc774\ub8f9\uc5d0 \uae30\ubc18\ud558\uace0 \ub370\uc774\ud130\uc5d0 \uacc4\uc808\uc815 \ubcc0\ub3d9\uc5d0 \uae30\ubc18\ud55c \ubc18\uba74\uc5d0, ARIMA \ubaa8\ub378\uc740 \uc2dc\uacc4\uc5f4\uc5d0\uc11c \uc0c1\uad00\uad00\uacc4\ub97c \ubb18\uc0ac\ud558\ub294 \ub370 \ucd08\uc810\uc744 \ub461\ub2c8\ub2e4. \uc544\ub798 \ube44\ub514\uc624\ub294 ARIMA\ub97c \uc798 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n<!--\n**ARIMA** stands for **A**uto **R**egressive **I**ntegrated **M**oving **A**verage. While exponential smoothing models were based on a description of trend and seasonality in data, ARIMA models aim to describe the correlations in the time series. The video below explains ARIMA very well:\n-->","77179838":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc5b4\ub450\uc6b4 \uc120 \uadf8\ub798\ud504\ub294 denoised \ud310\ub9e4\ub97c \ub098\ud0c0\ub0b4\uace0, \ubc1d\uc740 \uc120 \uadf8\ub798\ud504\ub294 \uc6d0\ubcf8 \ud310\ub9e4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 average smoothing\uc774 \ub370\uc774\ud130\uc5d0\uc11c \uac70\uc2dc\uc801\uc778 \uacbd\ud5a5\uc774\ub098 \ud328\ud134\uc744 \ucc3e\ub294 \ub370 Wavelet denoising \ub9cc\ud07c \ud6a8\uacfc\uc801\uc774\uc9c0 \uc54a\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc6d0\ubcf8 \ud310\ub9e4\uc5d0\uc11c \ub9ce\uc740 noise\ub294 \uc2ec\uc9c0\uc5b4 denoising \ud6c4\uc5d0\ub3c4 \uacc4\uc18d\ub429\ub2c8\ub2e4. \uadf8\ub7ec\ubbc0\ub85c, wavelet denoising\uc774 \ud310\ub9e4 \ub370\uc774\ud130\uc5d0\uc11c \ud2b8\ub80c\ub4dc\ub97c \ucc3e\ub294 \ub370 \ubd84\uba85\ud788 \ub354 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, average smoothing\uc774\ub098 \"rolling mean\"\uc740 \ubaa8\ub378\ub9c1\uc5d0\uc11c \uc720\uc6a9\ud55c feature\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\n<!--\nIn the above graphs, the dark lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that average smoothing is not as effective as Wavelet denoising at finding macroscopic trends and pattersns in the data. A lot of the noise in the original sales persists even after denoising. Therefore, wavelet denoising is clearly more effective at finding trends in the sales data. Nonetheless, average smoothing or \"rolling mean\" can also be used to calculate useful features for modeling.\n-->","494dae87":"## Moving average <a id=\"3.3\"><\/a>\n**moving average** \ubc29\ubc95\uc740 naive approach \ubcf4\ub2e4 \ub354 \ubcf5\uc7a1\ud569\ub2c8\ub2e4. \uadf8\uac83\uc740 \uc774\uc804 30\uc77c (\ub610\ub294 \ub2e4\ub978 \uc218) \ub3d9\uc548\uc758 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud558\uace0 \ub2e4\uc74c\ub0a0\uc758 \ud310\ub9e4\ub7c9\uc744 \uc608\uce21\ud569\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \uc774\uc804 30 timestamps\ub97c \uace0\ub824\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 naive approach \ubcf4\ub2e4 \uc9e7\uc740 \uae30\uac04 \ub3d9\uc548\uc758 \ubcc0\ub3d9\uc774 \ub35c \uc0dd\uae30\ub294 \uacbd\ud5a5\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ub2e4\uc74c\ucc98\ub7fc \uc694\uc57d\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.:\n\n<!--\nThe **moving average** method is more complex than the naive approach. It calculates the mean sales over the previous 30 (or any other number)  days and forecasts that as the next day's sales. This method takes the previous 30 timesteps into consideration, and is therefore less prone to short term fluctuations than the naive approach. The model can be summarized as follows:\n-->\n\n<img src=\"https:\/\/i.imgur.com\/5uJvt7H.png\" width=\"200px\">\n\n\uc704 \uc2dd\uc5d0\uc11c *y<sub>t+1<\/sub>* \ub0b4\uc77c \ud310\ub9e4\ub7c9\uc785\ub2c8\ub2e4. \uc624\ub978\ucabd \ud56d\uc5d0\uc11c, \uc774\uc804 30\uc77c\uc758 \ubaa8\ub4e0 \ud310\ub9e4\ub7c9\uc774 \ub354\ud574\uc9c0\uace0, \ud3c9\uade0\uc744 \ucc3e\uae30 \uc704\ud574 30\uc73c\ub85c \ub098\ub215\ub2c8\ub2e4. \uc774\uac83\uc740 \ubaa8\ub378\uc758 \uc608\uce21 *y<sub>t+1<\/sub>* \ub97c \ud615\uc131\ud569\ub2c8\ub2e4. \uc774\uc81c \uc6b0\ub9ac\uac00 \ucd5c\uc18c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc744 \uc5b4\ub5bb\uac8c \uc218\ud589\ud560 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font>, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font>, \uc608\uce21\uc740 <font color=\"green\">green<\/font> \uc785\ub2c8\ub2e4.\n\n<!--\nIn the above equation, *y<sub>t+1<\/sub>* is tomorrow's sales. On the right hand side, all the sales for the previous 30 days are added up and divided by 30 to find the average. This forms the model's prediction, *y<sub>t+1<\/sub>*. Now let us see how this new model performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","e9580d6d":"Prophet\uc740 ARIMA\uc640 \ub9e4\uc6b0 \ube44\uc2b7\ud55c \ubaa8\uc591\uc758 \uc608\uce21 \uacb0\uacfc\ub85c \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc790\uc138\ud788 \ubcf4\uba74, \uc6b0\ub9ac\ub294 ARIMA\uc5d0\ub294 \uc5c6\ub294 \uac70\uc2dc\uc801\uc778 \uc0c1\uc2b9 \uacbd\ud5a5\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. ARIMA \uc608\uce21\uc5d0\uc120, \uc815\ud655\ud788 \uac19\uc740 \ud328\ud134\uc774 \ubc18\ubcf5\ub429\ub2c8\ub2e4. \uadf8\ub7ec\ub098 Prophet \uc608\uce21\uc5d0\uc120, \uac01 \uc9c4\ub3d9\uc5d0\uc11c \uc218\uc9c1\uc73c\ub85c \uc6c0\uc9c1\uc778 \uac19\uc740 \ud328\ud134\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 ARIMA \ubcf4\ub2e4 high-level\uc758 \uacbd\ud5a5\uc744 \ub354 \uc798 \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n<!--\nProphet appears to output very similar-shaped predictions to ARIMA. But on closer observation, we can see that the there is a macroscopic upward trend which was absent in ARIMA. In the ARIMA predictions, the exact same pattern was repeated. But in the Prophet predictions, the same pattern is shifted vertically at each oscillation. This shows that is able to capture high-level trends better than ARIMA.\n-->","1b91ba3b":"### Rolling Average Price vs. Time (WI)","cc78aa18":"\uc6b0\ub9ac\ub294 \uae30\ud558\uae09\uc218\uc801\uc778 smoothing\uc774 \ub9e4 \uc2dc\uac04\ub9c8\ub2e4 \uc218\ud3c9\uc120\uc744 \ub9cc\ub4e4\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 time stemp\uc5d0 \uba40\uc218\ub85d \ub0ae\uc740 \uac00\uc911\uce58\ub97c \uc8fc\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \uc608\uce21\uc774 \ud3c9\ud3c9\ud574\uc9c0\uac70\ub098 \uc0c1\uc218\ub85c \ub0a8\uac8c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\uac83\uc740 \ud6cc\ub96d\ud55c \uc608\uce21\uc73c\ub85c \ud3c9\uade0 \ud310\ub9e4\ub7c9\uc744 \uc608\uce21\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\nWe can see that exponential smoothing is generating a horizontal line every time. This is because it gives very low weightage to faraway time steps, causing the predictions to flatten out or remain constant. However, it is able to predict the mean sales with excellent accuracy.\n-->","80b17a55":"\uc704 \uadf8\ub9bc\uc5d0\uc11c, \uc800\ub294 \ub2e8\uc9c0 \ud310\ub9e4 \ub370\uc774\ud130\uc5d0\uc11c \uc791\uc740 \uc815\ubcf4\ub97c \ucd94\ucd9c\ud558\uae30 \uc704\ud574 \ud655\ub300\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc2dc\uc791\ud558\uae30 \uc55e\uc11c, \uc6b0\ub9ac\ub294 \ubd84\uba85\ud788 \ud310\ub9e4 \ub370\uc774\ud130\uac00 \ub9e4\uc6b0 \ubd88\uaddc\uce59\uc801\uc774\uace0 \ubd88\uc548\uc815\ud558\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub54c\ub54c\ub85c, \ud310\ub9e4\ub7c9\uc774 \ud55c \ud589\uc5d0\uc11c \uba70\uce60\ub3d9\uc548 0\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub2e4\ub978 \ub584\uc5d4, \uba70\uce60\ub3d9\uc548 \ucd5c\uace0\uac12\uc73c\ub85c \ub0a8\uc544\uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \uc6b0\ub9ac\ub294 \ud310\ub9e4 \ub370\uc774\ud130\uc5d0 \uadfc\ubcf8\uc801\uc778 \ud2b8\ub80c\ub4e4\uc744 \uc54c\uace0 \uc608\uce21\uc744 \ud558\uae30 \uc704\ud574 \uc77c\uc885\uc758 'denoising' \uae30\ubc95\uc774 \ud544\uc694\ud569\ub2c8\ub2e4\n\n<!--\nIn the above plots, I simply zoom in to sample snippets in the sales data. As stated earlier, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of \"denoising\" techniques to find the underlying trends in the sales data and make forecasts.\n-->","1610eab4":"### Rolling Average Price vs. Time (TX)","d5312ac7":"### Rolling Average Price vs. Time (CA)","51c429d8":"\uc774\uc81c ARIMA\uac00 \uc6b0\ub9ac\uc758 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc5d0 \uc5b4\ub5bb\uac8c \uc218\ud589\ud558\ub294 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub294 <font color=\"blue\">blue<\/font>, \uc720\ud6a8 \ub370\uc774\ud130\ub294 <font color=\"darkorange\">orange<\/font>, \uadf8\ub9ac\uace0 \uc608\uce21\uce58\ub294 <font color=\"green\">green<\/font> \uc785\ub2c8\ub2e4.\n\n<!--\nNow let us see how ARIMA performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.\n-->","c8b3144a":"\uc2dc\uc791\ud558\uae30 \uc804\uc5d0, \uc2dc\uacc4\uc5f4\uc744 \uc608\uce21\ud558\ub294 \ubc95\uc5d0 \ub300\ud55c \ud6cc\ub96d\ud55c \ube44\ub514\uc624\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\nTo get started, here is an excellent video about how to approach time series forecasting:\n-->","469bfdbd":"# Acknowledgements\n\n1. [M5 Forecasting - Starter Data Exploration](https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration) ~ by Rob Mulla\n2. [EDA and Baseline Model](https:\/\/www.kaggle.com\/rdizzl3\/eda-and-baseline-model) ~ by RDizzl3\n3. [How to Create an ARIMA Model for Time Series Forecasting in Python](https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/) ~ by Machine Learning Mastery\n4. [7 methods to perform Time Series forecasting (with Python codes)](https:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/time-series-forecasting-methods\/) ~ by Analytics Vidhya\n5. [Economics for the IB Diploma](https:\/\/www.cambridge.org\/core\/books\/economics-for-the-ib-diploma\/1918CF16A8FC979AAB19951A487DCB1C) ~ by Ellie Tragakes\n6. [Prophet](https:\/\/facebook.github.io\/prophet\/) ~ by Facebook","b37f94ed":"# Ending note\n<font color=\"red\" size=4>\uc774 \uacb0\ub860\uc774 \uc81c \ucee4\ub110\uc785\ub2c8\ub2e4. \ub9cc\uc57d \uc88b\uc558\ub2e4\uba74 \ud22c\ud45c\ud574\uc8fc\uc138\uc694. \uc774\uac83\uc740 \uc81c\uac00 \ub354 \uc88b\uc740 \uc9c8\uc758 \ucee8\ud150\uce20\ub97c \ub9cc\ub4dc\ub294 \ub370 \ub3d9\uae30\ubd80\uc5ec\uac00 \ub429\ub2c8\ub2e4. :)<\/font>\n\n<!--\n<font color=\"red\" size=4>This concludes my kernel. Please upvote if you like it. It motivates me to produce more quality content :)<\/font>\n-->","64b5a68e":"### Wavelet denoising\n\nWavelet denoising (\ubcf4\ud1b5 \uc804\uae30 \uc2e0\ud638\ub85c \uc0ac\uc6a9\ub428) \uc740 \uc2dc\uacc4\uc5f4\uc5d0\uc11c \ubd88\ud544\uc694\ud55c \ub178\uc774\uc988\ub97c \uc81c\uac70\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \"wavelet cefficients\"\ub77c \ubd88\ub9ac\ub294 coefficients\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\uc774 coefficients\ub294 \uc815\ubcf4\uc758 \uc5b4\ub290 \ubd80\ubd84\uc744 \uc720\uc9c0\ud560 \uac83\uc778\uc9c0 (signal) \uc640 \uc5b4\ub290 \uac83\uc744 \ubc84\ub9b4 \uac83\uc778\uc9c0 (noise) \ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \n\n\uc6b0\ub9ac\ub294 \uc2dc\uacc4\uc5f4\uc5d0\uc11c \ud310\ub9e4\uc5d0\uc11c\uc758 \uc784\uc758\uc131\uc744 \uc774\ud574\uace0 \uadf8\uc5d0 \ub9de\ucdb0 wavelet coefficients\ub97c \uc704\ud55c \ucd5c\uc18c threshold\ub97c \uacb0\uc815\ud558\uae30 \uc704\ud574 MAD (mean absolute deviation) \uac12\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 wavelets\uc5d0\uc11c \ub0ae\uc740 coefficients \ub97c \uac78\ub7ec\ub0b4\uace0 \ub0a8\uc544\uc788\ub294 coefficients\uc5d0\uc11c \ud310\ub9e4 \ub370\uc774\ud130\ub97c \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uadf8\uac83\uc774 \uc6b0\ub9ac\uac00 \uc131\uacf5\uc801\uc73c\ub85c \ud310\ub9e4 \ub370\uc774\ud130\uc5d0\uc11c \ub178\uc774\uc988\ub97c \uc81c\uac70\ud55c \uac83\uc785\ub2c8\ub2e4.\n\n<!--\nWavelet denoising (usually used with electric signals) is a way to remove the unnecessary noise from a time series. This method calculates coefficients called the \"wavelet coefficients\". These coefficients decide which pieces of information to keep (signal) and which ones to discard (noise).\n\nWe make use of the MAD (mean absolute deviation) value to understand the randomness in the sales and accordingly decide the minimum threshold for the wavelet coefficients in the time series. We filter out the low coefficients from the wavelets and reconstruct the sales data from the remaining coefficients and that's it; we have successfully removed noise from the sales data.\n-->","23bf2885":"### Import libraries","6842732a":"## Preparing the ground <a id=\"2.1\"><\/a>","6795b5ac":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac19\uc740 \uad00\uacc4\ub97c \uc54c \uc218 \uc788\ub2e4. \uc0c1\uc810 <code>CA_3<\/code>\ub294 \ucd5c\ub300 \ud310\ub9e4\ub7c9\uc744 \uac00\uc9c0\uace0 \uc788\uace0 \uc0c1\uc810 <code>CA_4<\/code> \ucd5c\uc18c \ud310\ub9e4\ub7c9\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nIn the above plots, we can see the same relationship. The store <code>CA_3<\/code> has maximum sales while the store <code>CA_4<\/code> has minimum sales.\n-->","5ab72a8d":"## Denoising <a id=\"2.3\"><\/a>\n\uc774\uc81c, \uc800\ub294 \uc774 \ubd88\uc548\uc815\ud55c \ud310\ub9e4 \uac00\uaca9\uc774 \uadfc\ubcf8\uc801\uc778 \ud2b8\ub80c\ub4dc\ub97c \ucd94\ucd9c\ud558\uae30 \uc704\ud574 denoise\ub420 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc904 \uac83\uc785\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \uc6d0\ub798 \uc2dc\uac8c\uc5f4\uc5d0\uc11c \uc77c\ubd80 \uc815\ubcf4\ub97c \uc783\uc744 \uc9c0\ub3c4 \ubaa8\ub985\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uadf8\uac83\uc740 \uc2dc\uacc4\uc5f4\uc5d0\uc11c \ud2b8\ub80c\ub4dc\ub97c \uace0\ub824\ud558\uae30 \uc704\ud55c \ud2b9\uc815 feature\ub97c \ucd94\ucd9c\ud558\ub294\ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.\n\n<!--\nNow, I will show how these volatile sales prices can be denoised in order to extract underlying trends. This method may lose some information from the original time series, but it may be useful in extracting certain features regarding the trends in the time series.\n-->","a9a5caef":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uce98\ub9ac\ud3ec\ub2c8\uc544 \uc0c1\uc810 \uc911 \ud310\ub9e4\ub7c9\uc5d0\uc11c \uac00\uc7a5 \ud070 \ucc28\uc774\ub97c \ubcfc \uc218 \uc788\ub2e4. \uc774 \ud310\ub9e4 \uace1\uc120\uc740 \uac70\uc758 \uc11c\ub85c \uacb9\uce58\uc9c0 \uc54a\ub294\ub2e4. \uc774\uac83\uc740 \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcc0\ud558\uc9c0 \uc54a\ub294 \uce98\ub9ac\ud3ec\ub2c8\uc544\uc758 \uac1c\ubc1c \uc911 \ud2b9\uc815\ud55c \"hubs\"\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uac00\ub9ac\ud0ac \uc9c0\ub3c4 \ubaa8\ub978\ub2e4. \uadf8\ub9ac\uace0 \ub2e4\ub978 \uc9c0\uc5ed\ub4e4\uc740 \ud56d\uc0c1 \"hubs\" \ub4a4\uc5d0 \ub0a8\uaca8\uc838 \uc788\ub2e4. \uac10\uc18c\ud558\ub294 \uc21c\uc11c\ub85c \ud3c9\uade0 \ud310\ub9e4\ub7c9\uc740 <code>CA_3, CA_1, CA_2, CA_4<\/code> \uc774\ub2e4. \uc0c1\uc810 <code>CA_3<\/code>\uc740 \ucd5c\ub300 \ud310\ub9e4\ub7c9\uc774\uace0 \ubc18\uba74\uc5d0 \uc0c1\uc810 <code>CA_4<\/code> \ub294 \ucd5c\uc18c \ud310\ub9e4\ub7c9\uc774\ub2e4.\n\n<!--\nIn the above graph, we can see the large disparity in sales among California stores. The sales curves almost never intersect each other. This may indicate that there are certain \"hubs\" of development in California which do not change over time. And other areas always remain behind these \"hubs\". The average sales in descending order are <code>CA_3, CA_1, CA_2, CA_4<\/code>. The store <code>CA_3<\/code> has maximum sales while the store <code>CA_4<\/code> has minimum sales. \n-->","0425c696":"## Loss for each model <a id=\"3.8\"><\/a>","aae517f2":"\uc544\ub798 \uadf8\ub9bc\uc740 \uc774 \uadf8\ub798\ud504\ub4e4\uc744 \uc606\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub179\uc0c9 \uadf8\ub798\ud504\ub294 \uc6d0\ubcf8 \ud310\ub9e4\ub97c \ubcf4\uc5ec\uc8fc\uace0, \ube68\uac04 \uadf8\ub798\ud504\ub294 denoised \ud310\ub9e4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n<!--\nThe below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales.\n-->","269b5599":"## Prophet <a id=\"3.7\"><\/a>\n\nProphet\uc740 <font color=\"darkblue\">Facebook<\/font>\uc758 \uc2dc\uacc4\uc5f4 \uc608\uce21 \uc624\ud508\uc18c\uc2a4 \ud504\ub85c\uc81d\ud2b8\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \ube44\uc120\ud615 \uacbd\ud5a5\uc744 \ub9e4\ub144, \ub9e4\uc8fc, \ub9e4\uc77c, \uacc4\uc808, \ud734\uc77c\uc5d0 \ub9de\ub3c4\ub85d \ucca8\uac00\ud55c \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uac15\ud55c \uacc4\uc808\uc801 \uc601\ud5a5\uacfc \uba87\uba87\uc758 \uacc4\uc808\uc131\uc758 \uc5ed\uc0ac\uc801 \ub370\uc774\ud130\ub97c \uac00\uc9c0\ub294 \uc2dc\uacc4\uc5f4\uc5d0 \ucd5c\uace0\ub85c \uc88b\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \ub610\ud55c \ub2e4\ub978 \ubaa8\ub378\uc5d0 \ube44\uad50\ud558\uc5ec \uacb0\uce21\uce58\uc640 \ud2b8\ub80c\ub4dc \ubcc0\ud654\uc5d0 \ub354 \ud2bc\ud2bc\ud558\ub3c4\ub85d \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \ube44\ub514\uc624\ub294 Prophet \uc54c\uace0\ub9ac\uc998\uc744 \ub9e4\uc6b0 \uc798 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n<!--\nProphet is an opensource time series forecasting project by <font color=\"darkblue\">Facebook<\/font>. It is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, including holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. It is also supposed to be more robust to missing data and shifts in trend compared to other models. The video below explains the Prophet algorithm very well:\n-->","b285fbbe":"### Sample sales snippets","96492970":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac19\uc740 \uad00\uacc4\ub97c \uc54c \uc218 \uc788\ub2e4. \uc0c1\uc810 <code>W1_2<\/code> \ucd5c\ub300 \ud310\ub9e4\uc774\uace0 \ubc18\uba74\uc5d0, \uc0c1\uc810 <code>W1_1<\/code> \ucd5c\uc18c \ud310\ub9e4\uc774\ub2e4.\n\n<!--\nIn the above plots, we can see the same relationship. The store <code>W1_2<\/code> has maximum sales while the store <code>W1_1<\/code> has minimum sales. \n-->"}}