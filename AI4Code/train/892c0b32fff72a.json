{"cell_type":{"8efae39c":"code","832a3721":"code","b6bf3686":"code","e57867f5":"code","9ff316c5":"code","414cc360":"code","34fe2e64":"code","b682424c":"code","77908d6b":"code","e9c7daa9":"code","60cba27e":"code","56fbb2d5":"code","bf15ff0b":"code","76f2bcef":"code","24acc19f":"code","95cdeb5e":"code","fc6f2a2b":"code","b6858556":"code","eee8c2e0":"code","29d51b91":"code","144e57a3":"code","e4bc0034":"code","d1e9fdc7":"code","74dd4458":"code","840cac8d":"code","fa9a6085":"code","a365afb1":"code","360d30ce":"markdown"},"source":{"8efae39c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","832a3721":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Bidirectional, Dense,Dropout,LSTM,Activation, RepeatVector, SimpleRNN\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\n\nimport os, glob","b6bf3686":"years = ['2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\ncompanies = ['stedin', 'liander','enduris', 'enexis','westland-infra','rendo','coteq'] ","e57867f5":"path = r'..\/input\/dutch-energy\/Electricity\/' \n\nstedin=[]\nliander = []\nenduris = []\nenexis = []\nwestland_infra = []\nrendo = []\ncoteq = []\n\nfor company in companies:\n    all_files = glob.glob(f\"{path}\/{company}*.csv\")\n    for file in all_files:\n        print(company, file)\n        for year in years:\n            if year in file:\n                print(f\"adding column year {year} to {file}\")\n        \n                comp_df = company\n                print(f\"processing for : {comp_df}\")\n                comp_df = pd.read_csv(file, index_col=None, header=0)\n                comp_df['year'] = year\n\n                if company == companies[0]:\n                    stedin.append(comp_df)\n\n                elif company == companies[1]:\n                    liander.append(comp_df)\n                    \n                elif company == companies[2]:\n                        enduris.append(comp_df)\n                        \n                elif company == companies[3]:\n                    enexis.append(comp_df)\n\n                elif company == companies[4]:\n                    westland_infra.append(comp_df)\n\n                elif company == companies[5]:\n                    rendo.append(comp_df)\n\n                elif company == companies[6]:\n                    coteq.append(comp_df)  \n\n\n                \n        print('-------------------------------')\n","9ff316c5":"stedin_df = pd.concat(stedin, axis=0, ignore_index=True)\n# liander_df = pd.concat(liander, axis=0, ignore_index=True)\n# enduris_df = pd.concat(enduris, axis=0, ignore_index=True)\n# enexis_df = pd.concat(enexis, axis=0, ignore_index=True)\n# westland_infra_df = pd.concat(westland_infra, axis=0, ignore_index=True)\n# rendo_df = pd.concat(rendo, axis=0, ignore_index=True)\n# coteq_df = pd.concat(coteq, axis=0, ignore_index=True)","414cc360":"stedin_df.info()\n","34fe2e64":"stedin_df.isna().sum()\n","b682424c":"stedin_df = stedin_df.drop('STANDAARDDEVIATIE', 1)\n","77908d6b":"stedin_df.head()","e9c7daa9":"def normalization_mx(data):\n    \n    \"\"\"takes data and scales it \n    between 0 to 1\n    \"\"\"\n    \n    dataset = data.values.reshape(-1,1)\n\n    sclar = MinMaxScaler(feature_range=(0,1))\n    dataset = sclar.fit_transform(dataset)\n    return dataset\n\n\n\n\ndef normilization(data):\n    \n    \"\"\"takes data and scales it \n    between 0 to 1\n    \"\"\"\n    \n    dataset = data.iloc[:,11].astype('float32')\n    \n    max_value = np.max(dataset)\n    min_value = np.min(dataset)\n    \n    scalar = max_value - min_value\n    dataset = list(map(lambda x: (x-min_value) \/ scalar, dataset))\n    return np.array(dataset)","60cba27e":"norm_data = normilization(stedin_df)","56fbb2d5":"def load_data(dataset, seq_len):\n    X_train = []\n    y_train = []\n    split_size = int(0.8 * len(dataset))\n    \n    for i in range(seq_len, len(dataset)):\n        X_train.append(dataset[i - seq_len: i, 0])\n        y_train.append(dataset[i, 0])\n\n   \n    X_test = X_train[split_size:]\n    y_test = y_train[split_size:]\n\n   \n    X_train = X_train[:split_size]\n    y_train = y_train[:split_size]\n\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n\n    return [X_train, y_train, X_test, y_test]","bf15ff0b":"seq_len = 120 #choose sequence length\nlabel_len = 10 #choose labellen\n\nX_train, y_train, X_test, y_test = load_data(tf.expand_dims(norm_data, axis = 1), seq_len)","76f2bcef":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","24acc19f":"\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling1D, TimeDistributed, LSTM, BatchNormalization, Input, concatenate, Conv1D\n\ndef cnn_lstm(train_x, train_y, test_x):\n    \n    model=Sequential()\n    model.add(TimeDistributed(Conv1D(16, 2, padding = \"same\", strides = 1, activation = \"relu\"),input_shape=(None, train_x.shape[1], 1)))\n    model.add(TimeDistributed(MaxPooling1D(2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(50, return_sequences = True))\n    model.add(LSTM(10))\n    model.add(Dense(1, activation = \"relu\"))\n    model.compile(optimizer = \"adam\",loss = \"mse\")\n    \n    model.summary()\n    # reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n    history = model.fit(train_x.reshape(len(train_x), 1, train_x.shape[1], 1) ,train_y, batch_size=32, epochs=50, verbose=1, validation_split=0.3)\n    #prediction\n    pred = model.predict(test_x.reshape(len(test_x),1, test_x.shape[1], 1))\n    return model, pred, history","95cdeb5e":"cnnlstm_model, cnnlstm_pred, cnn_history = cnn_lstm(X_train, y_train, X_test)","fc6f2a2b":"y_test","b6858556":"import matplotlib.pyplot as plt\ndef plot_curve(history):\n    plt.figure(figsize=(10, 6))\n\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 -\n    # Retrieve a list of list results on training and test data\n    # sets for each training epoch\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 -\n    loss=history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs=range(len(loss)) # Get number of epochs\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \n    # Plot training and validation loss per epoch\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \n    plt.plot(epochs, loss, 'b', label = ' loss')\n    plt.plot(epochs, val_loss, 'r', label = 'val loss')\n     \n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.figure()\n    zoomed_loss = loss[25:]\n    zoomed_val_loss = val_loss[25:]\n    zoomed_epochs = range(25,50)\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \n    # Plot training and validation loss per epoch\n    # \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \n    plt.figure(figsize=(10, 6))\n\n    plt.plot(zoomed_epochs, zoomed_loss, 'b', label = 'loss')\n    plt.plot(zoomed_epochs, zoomed_val_loss, 'r', label = 'val loss')\n    plt.title('Training loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.figure()\n    \nplot_curve(cnn_history)","eee8c2e0":"def plot_predictions(test, predicted, title):\n    plt.figure(figsize=(16, 4))\n   \n    plt.plot(test, color='blue', label='Actual power consumption data')\n    plt.plot(predicted, alpha=0.7, color='red', label='Predicted power consumption data')\n    \n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Normalized power consumption scale')\n    \n    plt.legend()\n    plt.show()\n\n\nplot_predictions(y_test[:1000], tf.squeeze(cnnlstm_pred[:1000]), \"Predictions made by model\")","29d51b91":"y_test[:10] - tf.squeeze(cnnlstm_pred[:10])","144e57a3":"tf.squeeze(cnnlstm_pred[:10])","e4bc0034":"from sklearn.metrics import mean_squared_error,mean_absolute_error\ndef mean_absolute_percentage_error(y_true, y_pred):\n    \"\"\"MAPE\"\"\"\n    error=np.array(np.abs((y_true - y_pred) \/ y_true))\n    mse=np.mean(error,axis=0) * 100\n    return mse","d1e9fdc7":"# mean_absolute_percentage_error(y_test, cnnlstm_pred)","74dd4458":"def eval_score(y_test, y_true):\n    mae = mean_absolute_error(y_test, y_true)\n    mse = mean_squared_error(y_test, y_true)\n     \n    \n    return f\"mae : {mae}, mse : {mse}\"","840cac8d":"eval_score(y_test, cnnlstm_pred)","fa9a6085":"def evluation(real,pred,data,data_name):\n    \"\"\"Real: the true value\n       Pred: predicted value\n       Data: the name of data\"\"\"\n    dataset = data.iloc[:,11].astype('float32')\n    max_value = np.max(dataset)\n    min_value = np.min(dataset)\n    scalar = max_value - min_value\n    pred=pd.DataFrame(list(map(lambda x: x*scalar+min_value ,pred)))\n    real=pd.DataFrame(list(map(lambda x: x*scalar+min_value ,real)))\n    mse=mean_squared_error(real,pred)\n    print(\"MSE of %s is %f\"%(data_name,mse))\n    mae=mean_absolute_error(real,pred)\n    print(\"MAE of %s is %f\"%(data_name,mae))\n    rmse=np.sqrt(mean_squared_error(real,pred))\n    print(\"RMSE of %s is %f\"%(data_name,rmse))\n#     mape=mean_absolute_percentage_error(real,pred)\n#     print(\"MAPE of %s is %f\"%(data_name,mape))","a365afb1":"evluation(y_test, tf.squeeze(cnnlstm_pred), stedin_df, 'stedin-data')","360d30ce":"# Link to the EDA and naive time-series model\n## [Energy consumption in NL](https:\/\/www.kaggle.com\/raaavan\/bi-directional-lstm-ts)\n\n(still under construction, I work in this whenever I can)"}}