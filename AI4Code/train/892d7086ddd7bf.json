{"cell_type":{"db0c718d":"code","affbd707":"code","913fd6b4":"code","e62bbd8f":"code","636aa0c4":"code","27983d95":"code","445b5682":"code","26b5d2fa":"code","88d06f94":"code","f5ea47e5":"code","c5c4c1f9":"code","1539c7a2":"code","6201060c":"code","c058f770":"code","99807ba0":"code","580fdca6":"code","926b3756":"code","b4270296":"code","06ed3231":"markdown","638d3bd9":"markdown","2a1bf4b2":"markdown","f29a0f37":"markdown","0f2138f9":"markdown","e6b695df":"markdown","ecd19045":"markdown","fb577cca":"markdown","f9b8a665":"markdown","2375014d":"markdown"},"source":{"db0c718d":"import pandas as pd\nimport datatable as dt\nimport numpy as np\nimport xgboost as xgb\nimport optuna\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\n#from optuna.visualization import plot_edf\n#from optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice","affbd707":"#No idea who is the creator of the function, very grateful. do let me know if you know the OG.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","913fd6b4":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","e62bbd8f":"#Cover_type=5 has only one example, can drop it\ntrain = train[train.Cover_Type != 5]                                                                       ","636aa0c4":"#Thanks to this thread https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n# remove unuseful features\ntrain = train.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\ntest = test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n\n# extra feature engineering\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df\n\ntrain = fe(train)\ntest = fe(test)\n\n# Summed features pointed out by @craigmthomas (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823)\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)\n","27983d95":"y = train.Cover_Type.values - 1\nX = reduce_mem_usage(train.drop(\"Cover_Type\", axis=1)).set_index(\"Id\")\nXtest = reduce_mem_usage(test).set_index(\"Id\")","445b5682":"import gc\ndel([train, test])\n_ = [gc.collect() for i in range(5)]","26b5d2fa":"X","88d06f94":"def objective(trial):\n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.15,random_state=42)\n        \n    params = {\n        'objective': trial.suggest_categorical('objective',['multi:softprob']), \n        'num_class': trial.suggest_categorical('num_class',[6]), \n        'tree_method': trial.suggest_categorical('tree_method',['gpu_hist']),\n        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3,1.0),\n        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n        'max_depth': trial.suggest_categorical('max_depth', [3,5,7,9,11,13,15,17,20]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n        'eval_metric': trial.suggest_categorical('eval_metric',['mlogloss']),\n\n    }\n    model = xgb.XGBClassifier(**params)\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=300,verbose=False)\n    predictions = model.predict(test_x)\n    acc = accuracy_score(test_y, predictions)\n    \n    return acc","f5ea47e5":"%%time\nstudy = optuna.create_study(direction = 'maximize' , study_name = 'xgbclassifier')\nstudy.optimize(objective, n_trials=200)\n","c5c4c1f9":"df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))","1539c7a2":"df","6201060c":"df.to_csv('df_xgb_params_200_trials.csv', index=False)","c058f770":"study.best_trial.params","99807ba0":"#plots optimization history of all trials as well as the best score at each point.\nplot_optimization_history(study)","580fdca6":"#plots the parameter relationship as slice also we can see which part of search space were explored more.\nplot_slice(study)","926b3756":"#plots the interactive visualization of the high-dimensional parameter relationship in study and scores.\nplot_parallel_coordinate(study)","b4270296":"plot_param_importances(study)","06ed3231":"<a id=\"1\"><\/a>\n## Imports","638d3bd9":"<a id=\"7\"><\/a>\n## Dataframe of trails","2a1bf4b2":"<a id=\"5\"><\/a>\n## Optuna objective function","f29a0f37":"<a id=\"8\"><\/a>\n## Best params Optuna objective function","0f2138f9":"<a id=\"6\"><\/a>\n### Params Explnation\n![1_K7eB1EDtwpT3TSqFX2ZKkQ.png](attachment:faf7351e-73d0-4cdd-ae6e-fd24ec601321.png)","e6b695df":"# TPS-DEC2021[TPS-12\/21[Xgb-optuna-tutorial+(study vis)]]\n\n## Dataset\n\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition\n\n# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Imports](#1)\n- [2. Redcuce memory function](#2)\n- [3. Load Data](#3)\n- [4. Feature Eng..](#4)\n- [5. Optuna objective function](#5)\n- [6. Params Explanation](#6)\n- [7. Dataframe of trails](#7)\n- [8. Best params on 20 trails](#8)\n- [9. Study Visualizations](#9)\n\n.\n## Observations\n- num_classes = 6( dropped 1)\n- Highly Imbalanced dataset, with soil type 5 having only 1 example\n- Saw some correlations among the features with Elavation having a high Correlation with the target variable\n- No of trails - 20\n\n### Contact\n\n- [Click here ](https:\/\/twitter.com\/bambose_) - to connect me on twitter\n- [14 baseline models](https:\/\/www.kaggle.com\/datastrophy\/tps-end-of-the-year-14-baseline-models\/edit\/run\/81296449) - to visit the previous notebook\n\n\n### PS - Ensemble notebook and training with best params notebook out soon\n\n\n\n\n","ecd19045":"<a id=\"4\"><\/a>\n## Feature Eng..","fb577cca":"<a id=\"3\"><\/a>\n## Load data","f9b8a665":"<a id=\"2\"><\/a>\n## Reduce memory function","2375014d":"<a id=\"9\"><\/a>\n## Study visualizations"}}