{"cell_type":{"a9f3f2db":"code","3bb5d44a":"code","14b0b76c":"code","f6858ab3":"code","cf1f4ff2":"code","d3071701":"code","211429a8":"code","05913518":"code","e3af3819":"code","c5715ab6":"code","d6793591":"code","55e2f6cd":"markdown","6ca38eaf":"markdown","dc06af6c":"markdown","c46b10e5":"markdown","2da77877":"markdown","f36d5f9e":"markdown","92094ad7":"markdown","6028b2a4":"markdown"},"source":{"a9f3f2db":"import nltk\nnltk.download('reuters')\nfrom nltk.corpus import reuters\nreuters_corpora = [reuters.raw(fid) for fid in reuters.fileids()]","3bb5d44a":"# !python -m spacy download en_core_web_sm","14b0b76c":"import spacy\nfrom spacy.symbols import DET, X, NUM, PRON\nfrom tqdm import tqdm\nfrom gensim import corpora\nfrom gensim.models.ldamulticore import LdaMulticore\n\nnlp = spacy.load('en_core_web_sm')","f6858ab3":"processed_corpora = [\n    [token.lemma_ for token in nlp(doc_text) \\\n        if not(token.is_punct or token.is_stop or token.is_space or token.pos in [DET, NUM, PRON])]\n    for doc_text in tqdm(reuters_corpora)]","cf1f4ff2":"# Create Dictionary\ngensim_dict = corpora.Dictionary(processed_corpora)\n\n# gensim_dict.token2id = reuters_vocab\nprocessed_corpora = [gensim_dict.doc2bow(text) for text in processed_corpora]","d3071701":"lda_model = LdaMulticore(processed_corpora,\n                        id2word=gensim_dict,\n                        num_topics=10,\n                        workers= 2)\nlda_model","211429a8":"lda_model.print_topics()","05913518":"from gensim.test.utils import datapath\n\nmodel_file = datapath(\"gensim_model\")\nlda_model.save(model_file)\n\n# You can load it using:\n# from gensim.models.ldamulticore import LdaModel\n# lda_model = LdaModel.load(model_file)","e3af3819":"import pyLDAvis\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, processed_corpora, gensim_dict)\nvis","c5715ab6":"from spacy.tokens import Doc\ntopics = lambda doc: lda_model[gensim_dict.doc2bow([token.lemma_ for token in doc])][0]\nDoc.set_extension('topics', getter=topics)\n","d6793591":"doc = nlp(u'The decisive factor now is the behavior of the U.S. president, who basically told the crown prince, we are giving you free rein as long as you buy enough weapons and other things from us')\nprint(doc._.topics)","55e2f6cd":"Lets start with tokenizing each text and removing non-important words. I'm going to use word lemmas instead of words. This improves the model output. I also drop some words, including stop-words, numbers, determiners and pronouns.","6ca38eaf":"I'm going to generate topic models on a corpora using gensim and spacy.\nTopic modelling aims to genrate discover different sources of document theme, a.k.a topics. In topic modeling we suppose each document is a combination of multiple topics. Topic Models helps to find out how much a document is related to a topic. Automatically extracting key topics in a collection of articles, emails, discussion texts, etc. Topic Modeling is also useful in many nlp tasks, including information retrieval, classification and summarization. \n\nLet's do this with reuters news dataset. This dataset contains ~10K documents and 1.3 Million words.","dc06af6c":"**OPTIONAL:** you can visalize topics using pyLDAvis module","c46b10e5":"Now, lets prepare this data to feed gensim LDA model.","2da77877":"I'm going to use spacy to process texts in this corpora. Processing texts is more straigh-forward in spacy.\nIn order to use pos tags, you should download a spacy model file. Make sure you have \"en_core_web_sm\" installed on your machine. To do so, run below command:","f36d5f9e":"Now, we can add this model as an attribute to spacy Doc objects. Just add an extension attribute to Doc Objects and you have access to 5 key topic of each document:","92094ad7":"In LDA model, each document is supposed as a bag of different words. Each topic is a probability distribution on words and Each document is a probabilistic combinations of topics. Suppose we want to generate a Document of lenght N (this is an old-school algorithm and each document is just a bag-of-words, forget about the order! this is not deep learning). Anyway, for each N word we choose a topic (with different probabilities) and use this topic to generate a word. Simple and Easy :)\nLDA algorithm aims to estimate these probablitlies.\n","6028b2a4":"let's save this model to disk first"}}