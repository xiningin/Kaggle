{"cell_type":{"2adcf51a":"code","0f125644":"code","c8b8406e":"code","90298752":"code","b192434c":"code","8fc259d7":"code","feed9fe8":"code","9202b4b7":"code","03efd253":"code","21978ace":"code","ec742593":"code","329b077e":"code","a289bf16":"code","c22e04cd":"code","702d840c":"code","6ea46592":"code","0b4e2c4b":"code","401c4d5e":"code","2a6f2025":"code","098558a3":"code","e803c91f":"markdown","8a5df8fc":"markdown","7c0508ec":"markdown","f50d770f":"markdown","0dc15baf":"markdown","ed3d73c8":"markdown","9594803c":"markdown"},"source":{"2adcf51a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f125644":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport re\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c8b8406e":"from keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\n\n#set random seed for the session and also for tensorflow that runs in background for keras\nimport random\nimport tensorflow as tf\ntf.random.set_seed(123)\nrandom.seed(123)","90298752":"train = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip\", sep='\\t')\ntest = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip\", sep='\\t')\ntrain.head()","b192434c":"def clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['Phrase']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #lemmatize each word to its lemma\n        lemmatizer = WordNetLemmatizer()\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(lemma_words)\n\n    return(reviews)","8fc259d7":"%%time\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\nprint(len(train_sentences))\nprint(len(test_sentences))","feed9fe8":"target=train.Sentiment.values\ny_target=to_categorical(target)\n# number of numerical values exist in y_traget's column\nnum_classes=y_target.shape[1]","9202b4b7":"X_train,X_val,y_train,y_val = train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)","03efd253":"unique_words = set()\nlen_max = 0\n\nfor sent in tqdm(X_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \n#length of the list of unique_words \nprint(len(list(unique_words)))\nprint(\"Max length of text is : \",len_max)","21978ace":"tokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(X_train))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(test_sentences)","ec742593":"X_train = sequence.pad_sequences(X_train, maxlen=len_max)\nX_val = sequence.pad_sequences(X_val, maxlen=len_max)\nX_test = sequence.pad_sequences(X_test, maxlen=len_max)","329b077e":"print(\"X_training shape   : \",X_train.shape)\nprint(\"X_validation shape : \",X_val.shape)\nprint(\"X_testing shape    : \",X_test.shape)","a289bf16":"model=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=False))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(num_classes,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel.summary()","c22e04cd":"early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]","702d840c":"%%time\nhistory = model.fit( X_train,\n                    y_train, \n                    validation_data = (X_val, y_val),\n                    epochs = 20, \n                    verbose = 1,\n                    batch_size = 256, \n                    callbacks = callback)","6ea46592":"import matplotlib.pyplot as plt\nfig1 = plt.gcf()\nepoch_count = range(1, len(history.history['accuracy']) + 1)\nplt.plot(epoch_count, history.history['accuracy'], 'r--')\nplt.plot(epoch_count, history.history['val_accuracy'], 'b-')\nplt.legend(['Training Accuracy', 'Validation Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.show()\nfig1.savefig('moviereviewprediction_accuracy.png')","0b4e2c4b":"import matplotlib.pyplot as plt\nfig1 = plt.gcf()\nepoch_count = range(1, len(history.history['loss']) + 1)\nplt.plot(epoch_count, history.history['loss'], 'r--')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()\nfig1.savefig('moviereviewprediction_loss.png', bbox_inches='tight')","401c4d5e":"test_id = test['PhraseId']","2a6f2025":"y_pred=model.predict_classes(X_test)","098558a3":"submission = pd.DataFrame({'PhraseId': test_id, 'Sentiment': y_pred})\nsubmission.to_csv('movie_review_prediction_submission.csv', index=False)\nsubmission.head()","e803c91f":"* padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n* Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.","8a5df8fc":"A word embedding is a class of approaches for representing words and documents using a dense vector representation.","7c0508ec":"**Embedding Layer** requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n\nThe Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.","f50d770f":"[<center><h1>**Kaggle: Movie Review Prediction**<\/h1><\/center>](https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews)","0dc15baf":"# Function for cleaning the reviews, tokenize and lemmatize them.\n\nThis function will take each phrase iteratively and it will \n    \n        remove html content\n        remove non-alphabetic characters\n        tokenize the sentences\n        lemmatize each word to its lemma\nand then return the result in the list named reviews\n\n**Tokenization**\n\n    Tokenization is the process of breaking up the given text into \n    units called tokens.\n\n    e.g.: - Hello Friends, Welcome to the world of Natural \n    Language Processing\n\n    Word Token in Sent1 are as follows\n\n    \u2018Hello\u2019 \u2018Friends\u2019 \u2018,\u2019 \u2018Welcome\u2019 \u2018to\u2019 \u2018the\u2019 \u2018world\u2019 \u2018of\u2019 \n    \u2018Natural\u2019 \u2018Language\u2019 \u2018Processing\u2019\n\n    Total Number of Tokens: - 11\n\n# Stemming and Lematizing\n\n**Stemming**\n\n    Stemming is a process of reducing words to its root form even \n    if the root has no dictionary meaning. For eg: beautiful \n    and beautifully will be stemmed to beauti which has no\n    meaning in English dictionary.\n\n**Lemmatisation**\n\n    Lemmatisation is a process of reducing words into their lemma \n    or dictionary. It takes into account the meaning of the word in \n    the sentence. For eg: beautiful and beautifully are lemmatised \n    to beautiful and beautifully respectively without changing the \n    meaning of the words. But, good, better and best are lemmatised \n    to good since all the words have similar meaning.\n","ed3d73c8":"Tqdm is used for showing progressbar, as we don't get any verbose here. TQdm can also be used in model fitting insetead of verbose using callback function.","9594803c":"We can use 300 dimensions to represent over 1 million words. Which is why, most of the renowned organizations use 300 dimension as an output dimension in embedding."}}