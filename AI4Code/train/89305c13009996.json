{"cell_type":{"0fd483c8":"code","d0205aa8":"code","57d6da64":"code","76d1e0b6":"code","de6cd277":"code","dd3615ca":"code","b752105c":"code","e99a5d49":"code","14a84504":"code","bea16e3f":"code","cfb8c830":"code","92fd87bd":"code","c9a7667f":"code","c62f490d":"code","4bbff239":"code","e1652fbc":"code","58f5f3dd":"markdown","97d7f071":"markdown","1547ba62":"markdown","26b6d23b":"markdown","073e1e90":"markdown","5f537f08":"markdown","0fa7d617":"markdown","4f8898ba":"markdown","f7183c24":"markdown","ec0bea2b":"markdown","6e3b0e71":"markdown","9138b073":"markdown","212d1f98":"markdown","d52cdc37":"markdown","6e566501":"markdown","cabc3c59":"markdown"},"source":{"0fd483c8":"%%capture\n!wget https:\/\/www.dropbox.com\/s\/3j76hc0q63it4iu\/dataset.zip\n!unzip dataset.zip","d0205aa8":"from PIL import Image\n\nsample_img = Image.open(\"test\/SODA\/SODA0066.png\")\nsample_img","57d6da64":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets","76d1e0b6":"train_dir = \"train\/\"\ntest_dir = \"test\/\"\ntrain_transform = transforms.Compose([\n    transforms.Resize(150),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize(150),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\ntrain_img = datasets.ImageFolder(train_dir, transform=train_transform)\ntest_img = datasets.ImageFolder(test_dir, transform=test_transform)","de6cd277":"class Net(nn.Module):\n   \n    def __init__(self, kernel_size=4, feature_maps=[16, 32], input_size=(3,150,150), dense=512):\n        super(Net, self).__init__()\n        self.input_size = (1, *input_size) # add batch dim for a single data input (Channel x Width X Height) \n        self.conv1 = nn.Conv2d(3, feature_maps[0], kernel_size=kernel_size, stride=1, padding=0)\n        self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(feature_maps[0], feature_maps[1], kernel_size=kernel_size, stride=1, padding=0)\n        \n        self.fc1 = nn.Linear(self.__get_shape(), dense) # get_shape find the correct size after conv. See below\n        self.fc2 = nn.Linear(dense, 5)\n        self.do = nn.Dropout()\n        \n    def extract_feat(self, x): # this function calculate forward propagation in extraction part\n        x = self.pool(F.relu(self.conv1(x))) \n        x = self.pool(F.relu(self.conv2(x))) \n        x = x.reshape(x.shape[0], -1) \n        return x\n    \n    def forward(self, x):\n        x = self.extract_feat(x)\n        x = self.do(F.relu(self.fc1(x))) \n        x = self.fc2(x) \n        return x\n    \n    def __get_shape(self):\n        \"\"\"\n        This function is used to calculate the number of features after convolutional process dynamically.\n        It is called before dense layer. The idea is by run a simple input and find the correct size before \n        creating self.fc1\n        \"\"\"\n        x = torch.rand(*self.input_size, requires_grad=False)\n        with torch.no_grad():\n            x = self.extract_feat(x)\n        return x.shape[1]\n        \n    ","dd3615ca":"!pip install wandb","b752105c":"import wandb\n\nwandb.login()","e99a5d49":"config = {\n    \"kernel_size\" : 4,\n    \"epochs\" : 20,\n    \"dense\" : 512,\n    \"feature_maps\" : [16, 16],\n    \"batch_size\" : 32,\n    \"learning_rate\": 0.01\n}","14a84504":"run = wandb.init(project=\"pt-cnn\", reinit=\"True\", config=config)","bea16e3f":"model = Net(kernel_size=config[\"kernel_size\"],\n            feature_maps=config[\"feature_maps\"],\n            dense=config[\"dense\"])\nmodel = model.to(\"cuda\")\nmodel","cfb8c830":"trainloaders = torch.utils.data.DataLoader(train_img, batch_size=config[\"batch_size\"], shuffle=True)\ntestloaders = torch.utils.data.DataLoader(test_img, shuffle=True)","92fd87bd":"criterion = nn.CrossEntropyLoss() # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=config[\"learning_rate\"]) # algoritma backprop","c9a7667f":"# This function have similar part with the train cell, see the explanation in the cell below\n# In this function we use data test (testloaders) to validate the model result for each epoch, \n# and return an average loss and accuracy\n\ndef validate(model):\n    model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        total_sample = 0    \n        total_correct = 0\n        for image, label in testloaders:\n            image = image.to(\"cuda\")\n            label = label.to(\"cuda\")\n            out = model(image)\n            loss = criterion(out, label)\n            total_loss += loss.item()\n            total_sample += len(label)\n            total_correct += torch.sum(torch.max(out, 1)[1] == label).item()*1.0\n            \n    return  total_loss\/total_sample, total_correct\/total_sample","c62f490d":"wandb.watch(model, criterion, log=\"all\", log_freq=10) # WANDB WATCH\n \nfor i in range(config[\"epochs\"]):\n    model.train()\n    \n    total_loss = 0\n    total_sample = 0\n    total_correct = 0\n    \n    for image, label in trainloaders: \n        image = image.to('cuda') \n        label = label.to('cuda') \n        \n        out = model(image) # STEP 1: forward propagation\n        loss = criterion(out, label) # STEP 2: calculate loss\n        optimizer.zero_grad() # STEP 3: zero out the gradient. see: https:\/\/stackoverflow.com\/q\/48001598\/2147347\n        loss.backward() # STEP 4: backpropagation\n        optimizer.step() # STEP 5: update the model\n        \n        total_loss += loss # sum of losses for this epoch\n        total_sample += len(label) # number seen images in this epoch\n        total_correct += torch.sum(torch.max(out,1)[1]==label)*1.0 # sum of the correct prediction\n        \n    loss = total_loss\/total_sample # averaging loss\n    acc = total_correct\/total_sample # averaging accuracy\n    \n    val_loss, val_acc = validate(model) # validate using data \"test\" at the end of an epoch\n    \n    wandb.log({\"loss\":loss, \"acc\":acc, \"val_loss\": val_loss, \"val_acc\":val_acc}, step=i) # WANDB LOG\n    \n    print(f\"epoch {i} loss:{total_loss\/total_sample}  acc:{total_correct\/total_sample}\") # our personal log","4bbff239":"# Save the model, save it to wandb server\ntorch.save(model.state_dict(), \"model.pt\")\nwandb.save('model.pt')","e1652fbc":"run.finish()","58f5f3dd":"### 6.1 Optional, you can also save the model for each run","97d7f071":"## 3. Data Transformation\n\nWe will resize the images to from 256x256 to 150x150 (for faster training), transform image to tensor, and do a simple normalization (set mean and standard deviation of each channel to 0.5).","1547ba62":"## 5. Weights and Biases\n\n<img src=\"http:\/\/wandb.me\/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" \/>\n\nin the section below, we'll start using Weights and Biases.\n\nFirst of all, let's install it","26b6d23b":"### 5.3 Init Weights and Biases","073e1e90":"You will see the log in your W&B account:\n\n![](https:\/\/structilmy.com\/wp-content\/uploads\/2022\/01\/image.png)","5f537f08":"## 2. Pytorch setup\n\nIn Kaggle Notebook, pytorch is already installed. If you want to run locally install Pytorch by [following the instruction here](https:\/\/pytorch.org\/)","0fa7d617":"### 5.4 Setup the training process\n\n1. create a model based on hyperparameters on `config`\n2. create a [data loader](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html) using `config[\"batch_size\"]`\n3. define the loss function and the optimizer","4f8898ba":"## 6. Train the model\n\nWhen training the model, we use `wandb.watch` to track the model weights and biases and use `wandb.log` to track other variables that you want to track","f7183c24":"### 6.2 Finish the tracking\n\nIf you want to make another run, marks the current run as finished. W&B will finish uploading the data and log others information such a packages installed, wandb config files, etc.","ec0bea2b":"### 1.1 Inspect image","6e3b0e71":"## 1. Download dataset","9138b073":"### 5.1 login W&B account\n\nSuper easy login to connect this project with our W&B account","212d1f98":"> **You can re-run the training to find the best parameters, update your parameters in 5.2 and re-run from that cell**","d52cdc37":"### 5.2 Define hyperparameters value\n\nwrite in a simple dictionary `config`. This dictionary contains hyperparameters used in the training.","6e566501":"## 4. The Architecture\n\nWe will use CNN with two convolutional and pooling layers, and a fully-connected layer. We also apply a dropout layer in the fully-connected layers.\n\nTo demonstrate the capabilities of W&B, the architecture below is made more dynamic. User can provide the following parameters to shape the architecture:\n- `kernel_size (default=4)`: kernel size for ALL convolution and pooling layers\n- `feature_maps (default=[16,32])`: It is written as a list containing two elements. The first element is the number of feature maps for the first convolution, and the second element for the second convolution\n- `input_size (default=(3,150,150))`: the shape of input image (channel x  width x height)\n- `dense (default=512)`: the number of neurons in the hidden layer after all the convolution processes have been passed.\n\n**Note**: in the cell below we haven't used Weight and Biases, the code below is pure Pytorch code to build the architecture","cabc3c59":"# Pytorch Image Classification + Weights and Biases\n\nDataset used in this tutorial are part of the [groceries dataset by Univ. Freiburg](https:\/\/github.com\/PhilJd\/freiburg_groceries_dataset). Instead using all 25 classes, this notebook will only use 5 classes. The tutorial is also written for this notebook using Bahasa Indonesia in [my blog](https:\/\/structilmy.com)."}}