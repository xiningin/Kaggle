{"cell_type":{"9e4fe4e6":"code","d88bd415":"code","191de20f":"code","953ebaae":"code","e567beac":"code","7d835cf2":"code","b5fcda94":"code","d047106f":"code","0046fda5":"code","d047bb14":"code","3cb8a5c5":"code","359e33e0":"code","f09f39cc":"code","77a1f2b9":"code","849e1ab3":"code","780b8496":"code","26b5b2e5":"code","b9fb61bb":"code","a74bcfbb":"code","b0857141":"code","d730a92f":"code","6bc787e5":"code","1e4b631e":"code","e1d81d50":"code","328c1097":"code","9583010f":"code","b1ba079a":"code","0be2fd91":"code","83ba6d4f":"code","d5ae8205":"code","d1a0296a":"code","992fb460":"code","9d13b81b":"code","241a43c0":"code","502a9792":"code","918f2cd0":"code","143df584":"code","0e1f6dfb":"code","fad59644":"code","244e3d23":"code","6cde9f04":"code","64c8004e":"code","9393e1c4":"code","79753285":"code","e8c726c7":"code","e1b34430":"code","6ff6cd5f":"code","8e6ed39f":"code","19725d41":"code","51a635e2":"code","149f274e":"code","122cf045":"code","e9a49c27":"code","d8b618c4":"code","8d308e4f":"code","9a96c6ab":"code","d03d81fd":"code","0f8f68a5":"code","63c8eb4b":"code","a7701934":"code","d0deae65":"code","02860e67":"code","a75cdfef":"code","44b14c94":"code","fd6b9123":"code","3370ff96":"code","6cca358f":"code","ac7f4792":"code","cfc3d03e":"code","0617a8c9":"code","ab12b2b5":"code","d04c0ae8":"code","8830c7ff":"code","e9331f54":"code","f3c64953":"code","65efbcf8":"code","44977248":"code","f04e51f8":"code","d39d15b5":"code","99c013eb":"code","4b59d528":"code","e52f7a42":"code","76fba2a8":"code","0b84934e":"code","6aaba0da":"code","8621b821":"code","5ad696bf":"code","ed6a689e":"code","6e25e072":"code","3af3a800":"code","9fb9fb75":"code","c266b93b":"code","15dfd9de":"code","04ee03db":"code","ed645ca1":"code","bfc108b5":"code","6ab7f445":"code","8244a0c2":"code","4ac5c10a":"code","f74610d1":"markdown","a9f4ff68":"markdown","c229c08b":"markdown","af6b80c8":"markdown","732edc0e":"markdown","dd679d10":"markdown","c55c8ece":"markdown","a82e02a5":"markdown","29add935":"markdown","1990afae":"markdown","d6cbe3f3":"markdown","f940421e":"markdown","74afc53e":"markdown","c2ec2eb5":"markdown","93a0a9ef":"markdown","f6fad74f":"markdown","aaf73120":"markdown","25e69880":"markdown","01d40cd6":"markdown","674c7419":"markdown","07cc5c07":"markdown","a5121ecd":"markdown","83a27026":"markdown","06b8f6af":"markdown","6a36af01":"markdown","f028db9c":"markdown","1dc12c51":"markdown","7ce803a4":"markdown","cea6e03a":"markdown","9a25c40a":"markdown","618caa0a":"markdown","a5d78d28":"markdown","21a13ea5":"markdown","ad16d1e8":"markdown","2d731943":"markdown","456b8aa1":"markdown","d7848856":"markdown","ee0fc761":"markdown","564075c8":"markdown","91129ae1":"markdown"},"source":{"9e4fe4e6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d88bd415":"!pip install -q -U keras-tuner","191de20f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport keras_tuner as kt\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense, Dropout\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score, auc\n\nimport timeit\n\nseed=42 #define seed for reproducibility\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","953ebaae":"def num_plot(df, col, title, symb):\n    fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8,5),gridspec_kw={\"height_ratios\": (.2, .8)})\n    ax[0].set_title(title,fontsize=18)\n    sns.boxplot(x=col, data=df, ax=ax[0])\n    ax[0].set(yticks=[])\n    sns.histplot(x=col, data=df, ax=ax[1])\n    ax[1].set_xlabel(col, fontsize=16)\n    plt.axvline(df[col].mean(), color='darkgreen', linewidth=2.2, label='mean=' + str(np.round(df[col].mean(),1)) + symb)\n    plt.axvline(df[col].median(), color='red', linewidth=2.2, label='median='+ str(np.round(df[col].median(),1)) + symb)\n    plt.axvline(df[col].mode()[0], color='purple', linewidth=2.2, label='mode='+ str(np.round(df[col].mode()[0],1)) + symb)\n    plt.legend(bbox_to_anchor=(1, 1.03), ncol=1, fontsize=17, fancybox=True, shadow=True, frameon=True)\n    plt.tight_layout()\n    plt.show()","e567beac":"def get_scores(y, y_pred, col):\n    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n    'Precision':np.round(precision_score(y, y_pred),2),\n    'Recall':np.round(recall_score(y, y_pred),2),\n    'F1':np.round(f1_score(y, y_pred),2),\n    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n    scores_df = pd.Series(data).to_frame(col)\n    return scores_df","7d835cf2":"df = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","b5fcda94":"df.head()","d047106f":"df.drop(['RowNumber','CustomerId','Surname'], axis=1, inplace=True)","0046fda5":"df.info()","d047bb14":"num_plot(df, 'Balance', 'Balance Distribution' , '$' )","3cb8a5c5":"plt.figure(figsize=(8,6))\nsns.heatmap(df.corr(), cmap='RdBu', annot=True, vmin=-1, vmax=1)\nplt.title('Correlation Matrix');","359e33e0":"num_plot(df, 'Age', 'Age Distribution' , 'yo' )","f09f39cc":"df['Age_log'] = np.log(df['Age'])","77a1f2b9":"num_plot(df, 'Age_log', 'Age Distribution (log transformed)' , '')","849e1ab3":"df.drop('Age', axis=1, inplace=True)","780b8496":"df['Gender'].value_counts()","26b5b2e5":"df['Gender'] = df['Gender'].replace(['Female','Male'],[1,0])","b9fb61bb":"df['Geography'].value_counts()","a74bcfbb":"encoded_df = pd.get_dummies(df, drop_first=True)","b0857141":"encoded_df['Exited'].value_counts()","d730a92f":"X = encoded_df.drop('Exited', axis=1)\ny = encoded_df['Exited']","6bc787e5":"y.value_counts()","1e4b631e":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = seed)","e1d81d50":"y_test.value_counts()","328c1097":"y_train.value_counts()","9583010f":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","b1ba079a":"smote = SMOTE(random_state=seed)\n\nX_train, y_train = smote.fit_resample(X_train, y_train)\n\ny_train.value_counts()","0be2fd91":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state = seed)","83ba6d4f":"y_train.value_counts()","d5ae8205":"BATCH_SIZE=32\n\ndef create_model(hp):\n    model = keras.Sequential()\n    \n    for i in range(hp.Choice('hidden_layers', values=[1])):\n        model.add(keras.layers.Dense(units = hp.Int(\"units_{}\".format(i+1),min_value=2,max_value=40,step=1), activation='relu')),\n        model.add(keras.layers.Dropout(hp.Choice(\"dropout_{}\".format(i+1), values=[0.3, 0.4, 0.5]))),\n\n    model.add(keras.layers.Dense(1, activation='sigmoid')),\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]) ),\n                  metrics=['accuracy'])\n    \n    return model","d1a0296a":"tuner_hb = kt.Hyperband(\n        create_model,\n        objective=\"val_accuracy\",\n        factor=3,\n        max_epochs=20,\n        hyperband_iterations=1,\n        seed=seed,\n        directory='my_dir',\n        project_name='kt',\n        overwrite=True)","992fb460":"early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)","9d13b81b":"start = timeit.default_timer()","241a43c0":"tuner_hb.search(X_train, y_train,validation_data=(X_val, y_val), callbacks=[early_stop])","502a9792":"tuner_hb.results_summary(num_trials=1)","918f2cd0":"stop = timeit.default_timer()","143df584":"time_hb = stop-start","0e1f6dfb":"best_hps_hb=tuner_hb.get_best_hyperparameters()[0]","fad59644":"hypermodel_hb = tuner_hb.hypermodel.build(best_hps_hb)","244e3d23":"history = hypermodel_hb.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stop])","6cde9f04":"hypermodel_hb.summary()","64c8004e":"val_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","9393e1c4":"hypermodel_hb = tuner_hb.hypermodel.build(best_hps_hb)","79753285":"# retrain the model\nhypermodel_hb.fit(X_train, y_train, epochs=best_epoch, batch_size=BATCH_SIZE,validation_data=(X_val, y_val), callbacks=[early_stop])","e8c726c7":"y_pred_prob_hb = hypermodel_hb.predict(X_test)\ny_pred_hb = y_pred_prob_hb.round()","e1b34430":"print(classification_report(y_test, y_pred_hb, target_names = ['No','Yes']))","6ff6cd5f":"hb_scores = get_scores(y_test, y_pred_hb, 'HyperBand')\nhb_scores","8e6ed39f":"tuner_rs = kt.RandomSearch(\n        create_model,\n        objective=\"val_accuracy\",\n        max_trials=150,\n        seed=seed,\n        directory='my_dir',\n        project_name='kt',\n        overwrite=True)","19725d41":"early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)","51a635e2":"start = timeit.default_timer()","149f274e":"tuner_rs.search(X_train, y_train,validation_data=(X_val, y_val))","122cf045":"stop = timeit.default_timer()","e9a49c27":"time_rs = stop - start","d8b618c4":"tuner_rs.results_summary(num_trials=1)","8d308e4f":"best_hps_rs=tuner_rs.get_best_hyperparameters()[0]","9a96c6ab":"hypermodel_rs = tuner_rs.hypermodel.build(best_hps_rs)","d03d81fd":"history = hypermodel_rs.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stop])","0f8f68a5":"val_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","63c8eb4b":"hypermodel_rs = tuner_rs.hypermodel.build(best_hps_rs)\nhistory = hypermodel_rs.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_val, y_val), callbacks=[early_stop])","a7701934":"eval_result = hypermodel_rs.evaluate(X_test, y_test)\nprint(\"[test loss, test accuracy]:\", eval_result)","d0deae65":"y_pred_prob_rs = hypermodel_rs.predict(X_test)\ny_pred_rs = y_pred_prob_rs.round()","02860e67":"print(classification_report(y_test, y_pred_rs, target_names = ['No','Yes']))","a75cdfef":"rs_scores = get_scores(y_test, y_pred_rs, 'Random')\nrs_scores","44b14c94":"tuner_bo = kt.BayesianOptimization(\n        create_model,\n        objective=\"val_accuracy\",\n        max_trials=100,\n        seed=seed,\n        directory='my_dir',\n        project_name='kt',\n        overwrite=True)","fd6b9123":"early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)","3370ff96":"start = timeit.default_timer()","6cca358f":"tuner_bo.search(X_train, y_train,validation_data=(X_val, y_val), callbacks=[early_stop])","ac7f4792":"stop = timeit.default_timer()","cfc3d03e":"time_bo = stop - start","0617a8c9":"tuner_bo.results_summary(num_trials=1)","ab12b2b5":"best_hps_bo=tuner_bo.get_best_hyperparameters()[0]","d04c0ae8":"hypermodel_bo = tuner_bo.hypermodel.build(best_hps_bo)","8830c7ff":"history = hypermodel_bo.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stop])","e9331f54":"hypermodel_bo.summary()","f3c64953":"val_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","65efbcf8":"hypermodel_bo = tuner_bo.hypermodel.build(best_hps_bo)","44977248":"history = hypermodel_bo.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_val, y_val), callbacks=[early_stop])","f04e51f8":"eval_result = hypermodel_bo.evaluate(X_test, y_test)","d39d15b5":"print(\"[test loss, test accuracy]:\", eval_result)","99c013eb":"y_pred_prob_bo = hypermodel_bo.predict(X_test)\ny_pred_bo = y_pred_prob_bo.round()","4b59d528":"print(classification_report(y_test, y_pred_bo, target_names = ['No','Yes']))","e52f7a42":"bo_scores = get_scores(y_test, y_pred_bo, 'Baesian')\nbo_scores","76fba2a8":"EPOCHS=30\nBATCH_SIZE = 32\n\ndef objective(trial):\n\n    model = keras.Sequential()\n\n    in_feat = X_train.shape[0]\n\n    for i in range(1):\n        out_feat = trial.suggest_int(\"n_units_l{}\".format(i), 1, 40)\n        model.add(keras.layers.Dense(units=out_feat, activation='relu'))\n        model.add(keras.layers.Dropout(trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)))\n        in_feat=out_feat\n \n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(trial.suggest_float(\"lr\", 5e-5, 1e-2, log=True)),\n                  metrics=['accuracy'])\n    model.fit(X_train, y_train,\n              validation_data = (X_val, y_val),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n              verbose = False )\n    \n    score = model.evaluate(X_val, y_val, verbose=0)\n                  \n    return score[1]","0b84934e":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nparams = []\n\nfor key, value in trial.params.items():\n    params.append(value)\n    print(\"    {}: {}\".format(key, value))","6aaba0da":"units_1 = params[0]\ndropout_1 = np.round(params[1],2)\nlr = np.round(params[2],5)","8621b821":"model= keras.Sequential()\n\nmodel.add(keras.layers.Dense(units=units_1, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_1))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(learning_rate=lr),\n              metrics=['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n\nmodel.fit(X_train, y_train,\n              validation_data = (X_val, y_val),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n)","5ad696bf":"score = model.evaluate(X_test, y_test, verbose=0)  ","ed6a689e":"y_pred_prob = model.predict(X_test)\ny_pred = y_pred_prob.round()","6e25e072":"print(classification_report(y_test, y_pred_bo, target_names = ['No','Yes']))","3af3a800":"opt_scores = get_scores(y_test, y_pred, 'OPTUNA')\nopt_scores","9fb9fb75":"hb_params = [best_hps_hb.get('units_1'),\n             best_hps_hb.get('dropout_1'),\n             best_hps_hb.get('learning_rate'),\n             np.round(time_hb,2)]","c266b93b":"rs_params = [best_hps_rs.get('units_1'),\n             best_hps_rs.get('dropout_1'),\n             best_hps_rs.get('learning_rate'),\n             np.round(time_rs,2)]","15dfd9de":"bo_params = [best_hps_bo.get('units_1'),\n             best_hps_bo.get('dropout_1'),\n             best_hps_bo.get('learning_rate'),\n             np.round(time_bo,2)]","04ee03db":"params_all = pd.DataFrame(zip(hb_params, rs_params, bo_params, params),\n                          index =['NEURONS LAYER 1 ','DROPOUT LAYER 1 ','LEARNING RATE', 'Time(s)'] ,\n                          columns = ['HyperBand','Random','Baesian', 'OPTUNA'])\nparams_all","ed645ca1":"fig, ax = plt.subplots(1, 4, figsize = (12, 4))\n\nplt.suptitle('ROC Curve', fontsize=20)\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_hb)\nroc_auc = auc(fpr, tpr)\nax[0].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[0].plot([0, 1], [0, 1],'r--')\nax[0].set_title('Hyperband OPT',fontsize=20)\nax[0].set_ylabel('True Positive Rate',fontsize=15)\nax[0].set_xlabel('False Positive Rate',fontsize=15)\nax[0].legend(loc = 'lower right', prop={'size': 15},handlelength=0)\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_rs)\nroc_auc = auc(fpr, tpr)\nax[1].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[1].plot([0, 1], [0, 1],'r--')\nax[1].set_title('RandomSearch OPT',fontsize=20)\nax[1].set_ylabel('True Positive Rate',fontsize=15)\nax[1].set_xlabel('False Positive Rate',fontsize=15)\nax[1].legend(loc = 'lower right', prop={'size': 15},handlelength=0)\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_bo)\nroc_auc = auc(fpr, tpr)\nax[2].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[2].plot([0, 1], [0, 1],'r--')\nax[2].set_title('Baesian OPT',fontsize=20)\nax[2].set_ylabel('True Positive Rate',fontsize=15)\nax[2].set_xlabel('False Positive Rate',fontsize=15)\nax[2].legend(loc = 'lower right', prop={'size': 15},handlelength=0)\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\nax[3].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[3].plot([0, 1], [0, 1],'r--')\nax[3].set_title('OPTUNA',fontsize=20)\nax[3].set_ylabel('True Positive Rate',fontsize=15)\nax[3].set_xlabel('False Positive Rate',fontsize=15)\nax[3].legend(loc = 'lower right', prop={'size': 15},handlelength=0)\n\nplt.tight_layout()\nplt.show()","bfc108b5":"fig, ax = plt.subplots(1,4,figsize=(10,3))\n\nlabels=['No','Yes']\n\nplt.suptitle('Confusion Matrix', fontsize=20)\n\nax[0].set_title('Hyperband OPT')\nsns.heatmap(confusion_matrix(y_test, y_pred_hb), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20},ax=ax[0])\nax[0].set_xticklabels(labels)\nax[0].set_yticklabels(labels)\nax[0].set_ylabel('Test')\nax[0].set_xlabel('Predicted')\n\nax[1].set_title('RandomSearch OPT')\nsns.heatmap(confusion_matrix(y_test, y_pred_rs), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20},ax=ax[1])\nax[1].set_xticklabels(labels)\nax[1].set_yticklabels(labels)\nax[1].set_ylabel('Test')\nax[1].set_xlabel('Predicted')\n\nax[2].set_title('Baesian OPT')\nsns.heatmap(confusion_matrix(y_test, y_pred_bo), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20},ax=ax[2])\nax[2].set_xticklabels(labels)\nax[2].set_yticklabels(labels)\nax[2].set_ylabel('Test')\nax[2].set_xlabel('Predicted')\n\nax[3].set_title('OPTUNA')\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20},ax=ax[3])\nax[3].set_xticklabels(labels)\nax[3].set_yticklabels(labels)\nax[3].set_ylabel('Test')\nax[3].set_xlabel('Predicted')\n\nplt.tight_layout()\nplt.show()","6ab7f445":"fig, ax = plt.subplots(1,4,figsize=(10,3))\n\nlabels=['No','Yes']\nplt.suptitle('Confusion Matrix (normalized)', fontsize=20)\n\nax[0].set_title('Hyperband OPT')\ncm = confusion_matrix(y_test, y_pred_hb)\ncmn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cmn, annot=True, cmap=\"Blues\", fmt='.2f', cbar=False, annot_kws={\"size\":20},ax=ax[0])\nax[0].set_xticklabels(labels)\nax[0].set_yticklabels(labels)\nax[0].set_ylabel('Test')\nax[0].set_xlabel('Predicted')\n\nax[1].set_title('RandomSearch OPT')\ncm = confusion_matrix(y_test, y_pred_rs)\ncmn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cmn, annot=True, cmap=\"Blues\", fmt='.2f', cbar=False, annot_kws={\"size\":20},ax=ax[1])\nax[1].set_xticklabels(labels)\nax[1].set_yticklabels(labels)\nax[1].set_ylabel('Test')\nax[1].set_xlabel('Predicted')\n\nax[2].set_title('Baesian OPT')\ncm = confusion_matrix(y_test, y_pred_bo)\ncmn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cmn, annot=True, cmap=\"Blues\", fmt='.2f', cbar=False, annot_kws={\"size\":20},ax=ax[2])\nax[2].set_xticklabels(labels)\nax[2].set_yticklabels(labels)\nax[2].set_ylabel('Test')\nax[2].set_xlabel('Predicted')\n\nax[3].set_title('OPTUNA')\ncm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cmn, annot=True, cmap=\"Blues\", fmt='.2f', cbar=False, annot_kws={\"size\":20},ax=ax[3])\nax[3].set_xticklabels(labels)\nax[3].set_yticklabels(labels)\nax[3].set_ylabel('Test')\nax[3].set_xlabel('Predicted')\n\nplt.show()","8244a0c2":"all_scores = hb_scores.join(rs_scores).join(bo_scores).join(opt_scores)\nall_scores","4ac5c10a":"params_all","f74610d1":"# Hyperband Optimization","a9f4ff68":"![image.png](attachment:775baffa-cb09-475d-a2d3-e5e786ae6872.png)","c229c08b":"# Baesian Optimization","af6b80c8":"The following projects aims to use Optimization algorithms such as HyperBand, RandomSearch and Baesian from Keras Tuner library and OPTUNA to optimize a Neural Network Hyperparameters.<br>\n**The Neural network will be used to predict customer churn** based on some features, which first need to preprocessed.\nIn particular, the Neural Net hyperparameters that will be tuned are the number of neurons on each layer, the dropout values and the learning rate.<br>","732edc0e":"In the following, OPTUNA algorithm will be used for the hyperparameter optimziation.","dd679d10":"# Random Search Optimization","c55c8ece":"## Balance column preprocessing","a82e02a5":"Awesome, by performing a log transformation the distribution looks normal, with mode = median = mean.","29add935":"We can drop the columns 'RowNumber','CustomerId','Surname' since they are not useful for the analysis.","1990afae":"# Confusion Matrix comparison","d6cbe3f3":"The Gender column can be encoded by 0 and 1.","f940421e":"# Main results Summary","74afc53e":"It looks like the majority of values for 'balance' is 0. Are they missing values? Excluding the 0 values, the distrbution looks normal.","c2ec2eb5":"## Age column preprocessing","93a0a9ef":"## Normalized Confusion Matrix","f6fad74f":"# Neural Network Hypermodel by Keras Tuner","aaf73120":"**Overall, all the optimization algorithms performed very similarly in terms of F1 score, and chose very similar values for the units, dropout and learning rate**<br>","25e69880":"# Optimal parameters comparison","01d40cd6":"**Thank you for reading my notebook! Let me know if you have some questions or if you want me to check out your works too ! :)**","674c7419":"# Oversampling with SMOTE ","07cc5c07":"**Overall, all the optimization algorithms performed very similarly in terms of F1 score, and chose very similar values for the units, dropout and learning rate**<br>","a5121ecd":"<img src=\"https:\/\/i.imgur.com\/GaZqHQd.png\" width=\"1000px\">","83a27026":"# Categorical feature encoding","06b8f6af":"## Custom functions definition","6a36af01":"# Conclusions","f028db9c":"The training set now is balanced.","1dc12c51":"After performing few tests, we could see that a Neural Net with just one hidden layer performed better than a 2-3 layers nerual net, where overfitting was more evident.<br>\nFor this reason, **we will define a Neural Network with one hidden layer where we will optimize the number neurons, dropout value and the learning rate.**","7ce803a4":"Then, re-instantiate the hypermodel and train it with the optimal number of epochs from above.","cea6e03a":"# Neural Network Hypermodel by OPTUNA","9a25c40a":"There is no order among these categories, so the Geography column should be encoded by one hot encoding.","618caa0a":"# Dataset loading","a5d78d28":"## Train test split","21a13ea5":"## Train - Validation split","ad16d1e8":"Bad luck, It looks like the linear correlations between 'Balance' and the other features are really low.<br>\n**EDIT: When using KNNImputer I achieved slightly lower scores in terms of f1 and accuracy. For this reason, these 0 values will be kept as is.**","2d731943":"# ROC curves comparison","456b8aa1":"The dataset is unbalanced with respect to the target variable 'Exited'. During the predicive model training, the unbalance in the dataset would case a bias towards the majority class (Churn=0), leading to very bad performance when predicting the minority class (Churn=1). <br>\n**After performing some tests, oversampling with SMOTE performed better over the original unbalanced dataset, and so will be applied to the training set.** <br>","d7848856":"It looks like this distribution can be potentially normalized using log transformation!","ee0fc761":"# Hyperparameter Optimization for Neural Networks with Keras Tuner and OPTUNA","564075c8":"Then, we re-instantiate the hypermodel and train it with the optimal number of epochs.","91129ae1":"The training set will be further splitted into a new train and validation sets to monitor the validation accuracy and so prevent overfitting."}}