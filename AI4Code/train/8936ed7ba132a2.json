{"cell_type":{"4ededf9b":"code","1a8612ab":"code","870ebcdc":"code","c3acb1da":"code","606b61e2":"code","22a4cb02":"code","77f240cb":"code","cca9953d":"code","e4ea8771":"code","17062cf5":"code","8f2b7b30":"code","ec2c4ca0":"code","a0cfe01d":"code","8fc34333":"code","24e87a42":"code","63d9e591":"code","034c38fe":"code","ccf5afbd":"code","0049e139":"code","0b87d45a":"code","96eacc0c":"code","4ce37e3f":"code","cf34cc2f":"code","23fb9fc5":"code","76fea43a":"code","591586c3":"code","c02167fe":"code","7c17fca4":"code","3a498b54":"code","4b16f95a":"code","5fdfcd50":"code","80ff6819":"code","40af1f10":"code","da80da84":"code","6f1f23b9":"code","02b7946c":"code","42d7a233":"code","0b0d7b10":"markdown","503182b3":"markdown","f9a956ed":"markdown","4cd9f874":"markdown","14145d2b":"markdown","ddfa3178":"markdown","00fe8309":"markdown","f9b9a751":"markdown","7a66b1a6":"markdown","bd5ea8da":"markdown","a2ee91d2":"markdown","656bb834":"markdown","9e57119e":"markdown","07dcaf69":"markdown","f6dbf134":"markdown","787b868f":"markdown","58ec0ae2":"markdown","bb5e365a":"markdown","fb3b8899":"markdown","743e9c2f":"markdown","c1550fee":"markdown","83509a88":"markdown"},"source":{"4ededf9b":"#sklearn\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\nfrom sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#load package\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from math import sqrt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","1a8612ab":"## Read in file\ntrain_original = pd.read_csv('..\/input\/train.csv')\ntest_original = pd.read_csv('..\/input\/test.csv')\ntrain_original.sample(10)\ntotal = [train_original,test_original]\n","870ebcdc":"#Retrive the salutation from 'Name' column\nfor dataset in total:\n    dataset['Salutation'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)    ","c3acb1da":"pd.crosstab(train_original['Salutation'], train_original['Sex'])","606b61e2":"pd.crosstab(test_original['Salutation'], test_original['Sex'])","22a4cb02":"for dataset in total:\n    dataset['Salutation'] = dataset['Salutation'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Salutation'] = dataset['Salutation'].replace('Mlle', 'Miss')\n    dataset['Salutation'] = dataset['Salutation'].replace('Ms', 'Miss')\n    dataset['Salutation'] = dataset['Salutation'].replace('Mme', 'Mrs')\n    dataset['Salutation'] = pd.factorize(dataset['Salutation'])[0]\n    \n\n\n#total.Salutation = pd.factorize(total.Salutation)[0]   \n","77f240cb":"pd.crosstab(train_original['Salutation'], train_original['Sex'])","cca9953d":"pd.crosstab(test_original['Salutation'], test_original['Sex'])","e4ea8771":"#clean unused variable\ntrain=train_original.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntest=test_original.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntotal = [train,test]\n\ntrain.shape, test.shape","17062cf5":"#Detect the missing data in 'train' dataset\ntrain.isnull().sum()","8f2b7b30":"## Create function to replace missing data with the median value\ndef fill_missing_age(dataset):\n    for i in range(1,4):\n        median_age=dataset[dataset[\"Salutation\"]==i][\"Age\"].median()\n        dataset[\"Age\"]=dataset[\"Age\"].fillna(median_age)\n        return dataset\n\ntrain = fill_missing_age(train)","ec2c4ca0":"## Embarked missing cases \ntrain[train['Embarked'].isnull()]","a0cfe01d":"train[\"Embarked\"] = train[\"Embarked\"].fillna('C')","8fc34333":"test.isnull().sum()","24e87a42":"test[test['Age'].isnull()].head()","63d9e591":"#apply the missing age method to test dataset\ntest = fill_missing_age(test)","034c38fe":"test[test['Fare'].isnull()]","ccf5afbd":"#filling the missing 'Fare' data with the  median\ndef fill_missing_fare(dataset):\n    median_fare=dataset[(dataset[\"Pclass\"]==3) & (dataset[\"Embarked\"]==\"S\")][\"Fare\"].median()\n    dataset[\"Fare\"]=dataset[\"Fare\"].fillna(median_fare)\n    return dataset\n\ntest = fill_missing_fare(test)","0049e139":"## Re-Check for missing data\ntrain.isnull().any()","0b87d45a":"## Re-Check for missing data\ntest.isnull().any()","96eacc0c":"\nfor dataset in total:\n    dataset.loc[dataset[\"Age\"] <= 9, \"Age\"] = 0\n    dataset.loc[(dataset[\"Age\"] > 9) & (dataset[\"Age\"] <= 19), \"Age\"] = 1\n    dataset.loc[(dataset[\"Age\"] > 19) & (dataset[\"Age\"] <= 29), \"Age\"] = 2\n    dataset.loc[(dataset[\"Age\"] > 29) & (dataset[\"Age\"] <= 39), \"Age\"] = 3\n    dataset.loc[(dataset[\"Age\"] > 29) & (dataset[\"Age\"] <= 39), \"Age\"] = 3\n    dataset.loc[dataset[\"Age\"] > 39, \"Age\"] = 4\n","4ce37e3f":"pd.qcut(train[\"Fare\"], 8).value_counts()","cf34cc2f":"for dataset in total:\n    dataset.loc[dataset[\"Fare\"] <= 7.75, \"Fare\"] = 0\n    dataset.loc[(dataset[\"Fare\"] > 7.75) & (dataset[\"Fare\"] <= 7.91), \"Fare\"] = 1\n    dataset.loc[(dataset[\"Fare\"] > 7.91) & (dataset[\"Fare\"] <= 9.841), \"Fare\"] = 2\n    dataset.loc[(dataset[\"Fare\"] > 9.841) & (dataset[\"Fare\"] <= 14.454), \"Fare\"] = 3   \n    dataset.loc[(dataset[\"Fare\"] > 14.454) & (dataset[\"Fare\"] <= 24.479), \"Fare\"] = 4\n    dataset.loc[(dataset[\"Fare\"] >24.479) & (dataset[\"Fare\"] <= 31), \"Fare\"] = 5   \n    dataset.loc[(dataset[\"Fare\"] > 31) & (dataset[\"Fare\"] <= 69.487), \"Fare\"] = 6\n    dataset.loc[dataset[\"Fare\"] > 69.487, \"Fare\"] = 7","23fb9fc5":"for dataset in total:\n    dataset['Sex'] = pd.factorize(dataset['Sex'])[0]\n    dataset['Embarked']= pd.factorize(dataset['Embarked'])[0]\ntrain.head()","76fea43a":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","591586c3":"x = train.drop(\"Survived\", axis=1)\ny = train[\"Survived\"]","c02167fe":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.25,random_state=1)","7c17fca4":"\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model. RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n   #tree.ExtraTreeClassifier(),\n    \n    ]\n","3a498b54":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(x_train, y_train).predict(x_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(x_train, y_train), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(x_test, y_test), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","4b16f95a":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA Train Accuracy\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Train Accuracy Comparison')\nplt.show()","5fdfcd50":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA Test Accuracy\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Test Accuracy Comparison')\nplt.show()","80ff6819":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA Precission\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Precission Comparison')\nplt.show()","40af1f10":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA Recall\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Recall Comparison')\nplt.show()","da80da84":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA AUC\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA AUC Comparison')\nplt.show()","6f1f23b9":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(x_train, y_train).predict(x_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","02b7946c":"tunealg = ensemble.ExtraTreesClassifier() #Select the algorithm to be tuned\ntunealg.fit(x_train, y_train)\n\nprint('BEFORE tuning Parameters: ', tunealg.get_params())\nprint(\"BEFORE tuning Training w\/bin set score: {:.2f}\". format(tunealg.score(x_train, y_train))) \nprint(\"BEFORE tuning Test w\/bin set score: {:.2f}\". format(tunealg.score(x_test, y_test)))\nprint('-'*10)\n\n","42d7a233":"#tune parameters\nparam_grid = {#'bootstrap': [True, False],\n              'class_weight': ['balanced' , None],\n              #'max_depth': [1, 2,3,4, None],\n              #'max_features': ['log2', 'auto'],\n              #'max_leaf_nodes': [0,1,2,3,4, None],\n              #'min_impurity_decrease': [True, False, None],\n              #'min_impurity_split': [True, False],\n              #'min_samples_leaf': [1, 2,3,4,5],\n              #'min_samples_split': [1,2,3,4,5],\n              #'min_weight_fraction_leaf': [0.0,1.0,2.0,3.0,4.0,5.0], \n              #'n_estimators': [10,15,25,35,45], \n              'n_jobs':  [1,2,3,4,5], \n              #'oob_score': [True, False], \n              'random_state': [0,1, 2,3,4, None], \n              #'verbose': [0,1, 2,3,4, 5], \n              'warm_start': [True, False]\n             }\n# So, what this GridSearchCV function do is finding the best combination of parameters value that is set above.\ntune_model = model_selection.GridSearchCV(linear_model.PassiveAggressiveClassifier(), param_grid=param_grid, scoring = 'roc_auc')\ntune_model.fit (x_train, y_train)\n\nprint('AFTER tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER tuning Training w\/bin set score: {:.2f}\". format(tune_model.score(x_train, y_train))) \nprint(\"AFTER tuning Test w\/bin set score: {:.2f}\". format(tune_model.score(x_test, y_test)))\nprint('-'*10)","0b0d7b10":"Checking the correlation between features","503182b3":"# 3. Spliting the data","f9a956ed":"# 4. Performance Comparison","4cd9f874":"# Introduction","14145d2b":"discretize Age feature","ddfa3178":"# 1. Load the library and data\n\nIn this section the library and the data used are loaded into the sytem\n\n## 1.1 Load the library","00fe8309":"Factorized 2 of the column whic are 'Sex' and 'Embarked'","f9b9a751":"Discretize Fare","7a66b1a6":"## 2.2 Detect and fill the missing data","bd5ea8da":"Detecting the missing data in 'test' dataset is done to get the insight which column consist missing data. as it is shown below, there are 2 column which have missing value. they are 'Age' and 'Fare' column. The same function is used in order to filled the missing 'Age' value. missing 'Fare' value is filled by finding the median of 'Fare' value in the 'Pclass' = 3 and 'Embarked' = S.","a2ee91d2":"Split the data into training and validation sets","656bb834":"# 4. Tuning the algorithm","9e57119e":"The next step is deletin column that will not be used in our models.","07dcaf69":"As it is shown above, there are 2 columns which have missing data. the way I'm handling missing 'Age' column is by filling them by the median of age in every passenger class. there are only two data missing in 'Embarked' column. Considering Sex=female and Fare=80, Ports of Embarkation (Embarked) for two missing cases can be assumed to be Cherbourg (C).","f6dbf134":"I have deided to work with the Titanic dataset again. this kernel is focusing on comparing the performance of several machine learning algorithms. I use several clasification model to create a model predicting survival on the Titanic. \nI am hoping to learn a lot from this site, so feedback is very welcome! This kernel is always improving because of your feedback!!!\n\nThere are three parts to my script as follows:\n\n1. Load the library and data\n2. Data cleaning\n3. Data spliting\n4. Training,testing, and Peformance comparison\n5. Tuning the algorithm\n\nIf you like this work and want to see my other works, you can check it here:\n\nhttps:\/\/www.onclick360.com\n","787b868f":"Seperate input features from target feature","58ec0ae2":"## 1.2 Load the data","bb5e365a":"Train the data into the model and calculate the performance","fb3b8899":"# 2. Data Cleaning\n## 2.1 Retrive the salutation and Eliminating unused variable\n\n'Salutation' variable can be retrieved from 'Name' column by taking the string between space string and '.' string.","743e9c2f":"List of Machine Learning Algorithm (MLA) used","c1550fee":"## 2.3 Re-Check for missing data","83509a88":" afterward, 'Salutation' column should be factorized to be fit in our future model"}}