{"cell_type":{"f3beb628":"code","8f479212":"code","a9044cb5":"code","f87b9040":"code","468fa8ff":"code","d6df9573":"code","4c8b8860":"code","02face36":"code","14e62a89":"code","2e6174ec":"code","f51fc72e":"code","3e47e634":"code","b664536b":"code","86fe5040":"code","edf4b2a1":"code","c218bba2":"code","4f52f595":"code","62a10a27":"code","2ea0b7b0":"code","62a98315":"code","cb1eba82":"code","b8881268":"code","79e1c08d":"code","d01ad86c":"code","ed2ac661":"code","1dc9cbb6":"code","fafca80f":"code","0425c365":"code","44a420db":"code","fdbe3699":"markdown","a3bca1a6":"markdown","de4f2a4a":"markdown","b1123503":"markdown","0e307755":"markdown","51745c31":"markdown","55bb9ec4":"markdown","3509e0d1":"markdown","adb1d9b3":"markdown","5c829ed9":"markdown","1e838842":"markdown","b351f134":"markdown","f0228f29":"markdown","61301f47":"markdown","461e24a2":"markdown","6c19bad7":"markdown","7ea08bc9":"markdown","cb7feb62":"markdown","04f712a1":"markdown","a22dc139":"markdown","ca6918e5":"markdown","d5bc7ada":"markdown","fa140dc4":"markdown","c23427ef":"markdown","95774e29":"markdown","05195593":"markdown","cdceccd5":"markdown","ba3d3c6c":"markdown","ab72278a":"markdown","b046fb35":"markdown","1e1821e1":"markdown","445eedfb":"markdown","71c82112":"markdown","4b8f9fb2":"markdown","2886c4e4":"markdown","904f6371":"markdown","41a2038e":"markdown","8508e4d8":"markdown","43ae470a":"markdown","7221cf0a":"markdown","a879dc7d":"markdown","2e3312c3":"markdown","90969697":"markdown"},"source":{"f3beb628":"import numpy as np \nimport pandas as pd \nimport os\nimport itertools\n\n#plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom nltk.util import ngrams\n\n\nimport re\nfrom collections import Counter\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport requests\nimport json\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n","8f479212":"social=pd.read_csv('..\/input\/the-social-dilemma-tweets\/TheSocialDilemma.csv')\nsocial.info()","a9044cb5":"import missingno as mno\nmno.matrix(social)","f87b9040":"missed = pd.DataFrame()\nmissed['column'] = social.columns\n\nmissed['percent'] = [round(100* social[col].isnull().sum() \/ len(social), 2) for col in social.columns]\nmissed = missed.sort_values('percent',ascending=False)\nmissed = missed[missed['percent']>0]\n\nfig = sns.barplot(\n    x=missed['percent'], \n    y=missed[\"column\"], \n    orientation='horizontal'\n).set_title('Missed values percent for every column')","468fa8ff":"ds = social['Sentiment'].value_counts().reset_index()\nds.columns = ['Sentiment', 'Count']\nds = ds.sort_values(['Count'],ascending=False)\n#social = pd.merge(social, ds, on='user_name')\n\nfig = sns.barplot( \n    x=ds[\"Sentiment\"], \n    y=ds[\"Count\"], \n    orientation='vertical'\n).set_title('Sentiment of the tweets') ","d6df9573":"ds = social['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'],ascending=False)\nsocial = pd.merge(social, ds, on='user_name')\n\nfig = sns.barplot( \n    x=ds.head(20)[\"tweets_count\"], \n    y=ds.head(20)[\"user_name\"], \n    orientation='horizontal'\n).set_title('Top 20 users by number of tweets') \n\n","4c8b8860":"social[social['user_name']=='OurPact'][['text','Sentiment']]","02face36":"social[social['user_name']=='OurPact']['Sentiment'].value_counts()","14e62a89":"social['user_created'] = pd.to_datetime(social['user_created'])\nsocial['year_created'] = social['user_created'].dt.year\ndata = social.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[data['year_created']>1970]\ndata = data['year_created'].value_counts().reset_index()\ndata.columns = ['year', 'number']\n\nfig = sns.barplot( \n    x=data[\"year\"], \n    y=data[\"number\"], \n    orientation='vertical'\n    #title='', \n).set_title('User created year by year')","2e6174ec":"ds = social['user_location'].value_counts().reset_index()\nds.columns = ['user_location', 'count']\nds = ds[ds['user_location']!='NA']\nds = ds.sort_values(['count'],ascending=False)\n\nfig = sns.barplot(\n    \n    x=ds.head(20)[\"count\"], \n    y=ds.head(20)[\"user_location\"], \n    orientation='horizontal'\n).set_title('Top 20 user locations by number of tweets')","f51fc72e":"from plotly.offline import init_notebook_mode, iplot\ndef pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]\/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n    \n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(social, 'user_location', 0.5, 'Number of tweets per location')","3e47e634":"ds = social['source'].value_counts().reset_index()\nds.columns = ['source', 'count']\nds = ds.sort_values(['count'],ascending=False)\n\nfig = sns.barplot(\n    x=ds.head(10)[\"count\"], \n    y=ds.head(10)[\"source\"], \n    orientation='horizontal', \n).set_title('Top 10 user sources by number of tweets')","b664536b":"social['hashtags'] = social['hashtags'].fillna('[]')\nsocial['hashtags_count'] = social['hashtags'].apply(lambda x: len(x.split(',')))\nsocial.loc[social['hashtags'] == '[]', 'hashtags_count'] = 0\nfig = sns.scatterplot( \n    x=social['hashtags_count'], \n    y=social['tweets_count']\n).set_title('Total number of tweets for users and number of hashtags in every tweet')","86fe5040":"ds = social['hashtags_count'].value_counts().reset_index()\nds.columns = ['hashtags_count', 'count']\nds = ds.sort_values(['count'],ascending=False)\nds['hashtags_count'] = ds['hashtags_count'].astype(str) + ' tags'\nfig = sns.barplot( \n    x=ds[\"count\"], \n    y=ds[\"hashtags_count\"], \n    orientation='horizontal'\n).set_title('Distribution of number of hashtags in tweets')","edf4b2a1":"social['date'] = pd.to_datetime(social['date']) \ndf = social.sort_values(['date'])\ndf['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\ndf['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\ndf.head()\n\nds = df.groupby(['day', 'user_name'])['hashtags_count'].count().reset_index()\nds = ds.groupby(['day'])['user_name'].count().reset_index()\nds.columns = ['day', 'number_of_users']\nds['day'] = ds['day'].astype(str)\nfig = sns.barplot( \n    x=ds['day'], \n    y=ds[\"number_of_users\"], \n    orientation='vertical',\n    #title='Number of unique users per day', \n    #width=800, \n    #height=800\n).set_title('Number of unique users per day')\n#fig.show()\nplt.xticks(rotation=90)","c218bba2":"ds = df['day'].value_counts().reset_index()\nds.columns = ['day', 'count']\nds = ds.sort_values('count',ascending=False)\nds['day'] = ds['day'].astype(str)\nfig = sns.barplot( \n    x=ds['count'], \n    y=ds[\"day\"], \n    orientation=\"horizontal\",\n).set_title('Tweets distribution over days present in dataset')","4f52f595":"social['tweet_date']=pd.to_datetime(social['date']).dt.date\ntweet_date=social['tweet_date'].value_counts().to_frame().reset_index().rename(columns={'index':'date','tweet_date':'count'})\ntweet_date['date']=pd.to_datetime(tweet_date['date'])\ntweet_date=tweet_date.sort_values('date',ascending=False)","62a10a27":"fig=go.Figure(go.Scatter(x=tweet_date['date'],\n                                y=tweet_date['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(\n    title_text='Tweets per Day : ({} - {})'.format(social['tweet_date'].sort_values()[0].strftime(\"%d\/%m\/%Y\"),\n                                                       social['tweet_date'].sort_values().iloc[-1].strftime(\"%d\/%m\/%Y\")),template=\"plotly_dark\",\n    title_x=0.5)\n\nfig.show()","2ea0b7b0":"social['hour'] = social['date'].dt.hour\nds = social['hour'].value_counts().reset_index()\nds.columns = ['hour', 'count']\nds['hour'] = 'Hour ' + ds['hour'].astype(str)\nfig = sns.barplot( \n    x=ds[\"hour\"], \n    y=ds[\"count\"], \n    orientation='vertical', \n).set_title('Tweets distribution over hours')\nplt.xticks(rotation='vertical')\n","62a98315":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ntweets_df = social.copy()\ntweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\ntweets_df = tweets_df.explode('hashtag')\ntweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ntweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\n#tweets_df","cb1eba82":"ds = tweets_df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'],ascending=False)\nfig = sns.barplot(\n    x=ds.head(10)[\"count\"], \n    y=ds.head(10)['hashtag'], \n    orientation='horizontal', \n).set_title('Top 10 hashtags')","b8881268":"def build_wordcloud(df, title):\n    wordcloud = WordCloud(\n        background_color='black',colormap=\"Oranges\", \n        stopwords=set(STOPWORDS), \n        max_words=50, \n        max_font_size=40, \n        random_state=666\n    ).generate(str(df))\n\n    fig = plt.figure(1, figsize=(14,14))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","79e1c08d":"build_wordcloud(social['text'], 'Prevalent words in tweets for all dataset')","d01ad86c":"india_df = social.loc[social.user_location==\"India\"]\nbuild_wordcloud(india_df['text'], title = 'Prevalent words in tweets from India')","ed2ac661":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","1dc9cbb6":"social['refine_text']=social['text'].str.lower()\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_tag(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_mention(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_hash(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_newline(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_url(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_number(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_punct(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:text_strip(x))\n\nsocial['text_length']=social['refine_text'].str.split().map(lambda x: len(x))","fafca80f":"fig = go.Figure(data=go.Violin(y=social['text_length'], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='royalblue', opacity=0.6,\n                               x0='Tweet Text Length'))\n\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Text length\",template='ggplot2')\nfig.show()","0425c365":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(social['refine_text'],(1,1),20)\nbigram_df=ngram_df(social['refine_text'],(2,2),20)\ntrigram_df=ngram_df(social['refine_text'],(3,3),20)","44a420db":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \",\n                  showlegend=False,title_x=0.5,height=1200,template=\"plotly_dark\")\nfig.show()","fdbe3699":"\n<h2 style='background:green; border:0; color:white'><center> PART 2: Twitter sentiment classification models and techniques! Coming soon! Stay tuned! <\/center><h2>\n\n","a3bca1a6":"<a id=\"4\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Tweets text analysis<\/center><h2>","de4f2a4a":"* Most of the tweets are positive in nature, which denotes a wide appreciation of the documentary among the users!","b1123503":"## Let's visualize some missing values!","0e307755":"### User created year by year","51745c31":"## Tweet distribution - hourly","55bb9ec4":"### Top 10 hastags used in the tweet!","3509e0d1":"The user name OurPact has created nearly 218 tweets with 213(majority) tweets in positive sentiment! Looks like Ourpact loved the documentary!","adb1d9b3":"#### Let's check the sentiment of the tweets made by the user name OurPact","5c829ed9":"* social, watch, and media are the most used unigrams\n* social media, social dilemma, just watched are the most used bigrams in the social dilemma tweets\n* social dilemma neflix, wach social dilemma, keepin kids safe, kids safe online are the most used trigrams!","1e838842":"## Prevalent words in tweets from India","b351f134":"## The average length for a IPL2020 Tweet using violin plot","f0228f29":"## Listing below the top N-gram sequential words used in IPL2020 tweets","61301f47":"### The interesting factor is that the documentary was intially released on jan, 2020! Once the documentary was released in one of the major OTT platform(Netflix) on sept 9th, 2020! It started creating a quite a buzz in social media platforms like twitter, where people started to voice out their concerns on the impact of social media! \n\n### This is the reason where you identiy an steady increase in the number of tweets after sept 9th, 2020 but after a weeks time, the hype among people gradually decreased!","461e24a2":"## Refining the text (Important step)","6c19bad7":"### Let's visualize the sentiment of the tweets!","7ea08bc9":"* Most users use 1 hastag followed by 2 hashtag, where certain population uses no hastag while tweeting\n* Very less amount of people use more than 2 hashtags in their post ","cb7feb62":"<a id=\"1\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Required Libraries<\/center><h2><a id=\"1\"><\/a>\n","04f712a1":"## Lets Visualize the top 20 users by number of tweets\n","a22dc139":"## Top 20 Users location based on the number of tweets","ca6918e5":"## N-GRAM","d5bc7ada":"### The user \"OurPact\" has made the highest number of tweets! No let's look into OurPact's tweets alone!","fa140dc4":"## Tweets per day","c23427ef":"* 2009 has the highest number of users followed by the year 2011, who tweeted about the social dilemma documentary\n* The amount of users who joined in 2008 tweeted very less about the social dilemma documentary comapred to the other users who joined in the later years!\n","95774e29":"<a id=\"3\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Tweets EDA<\/center><h2>","05195593":"### The dataset consists of 18,252 tweets with 14 columns!","cdceccd5":"### Most number of tweets were made during 6 and 8 PM in the evenings and majorly most of the tweets were made during the evenings!","ba3d3c6c":"![](https:\/\/www.thenews.com.pk\/assets\/uploads\/tns\/2019-07-07\/568081_5906029_tns.jpg)","ab72278a":"## Total number of tweets for users and number of hashtags in every tweet","b046fb35":"* Average length of the ipl2020 tweet: 13.06\n* Median lenght of the ipl 2020 tweet:15\n* Interquartile lie between : 10 and 18\n* Min: 0\n* Max: 27","1e1821e1":"<h2 style='background:green; border:0; color:white'><center>Kindly upvote the notebook and the dataset!<\/center><h2>","445eedfb":"## Tweets distribution over days present in dataset","71c82112":"## Number of hashtags used in each tweet","4b8f9fb2":"## Top 10 user sources by number of tweets","2886c4e4":"### The most twitter tweets are made through the iphone! Followed by the android and very less people prefer web app comapred to iphone or android!","904f6371":"## Visualizing the number of tweets per location!!","41a2038e":"<a id=\"2\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Dataset Quick Overview<\/center><h2>","8508e4d8":"## This notebook analyses the tweets with the trending #TheSocialDilemma hashtag made by the users of twitter!","43ae470a":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\" color=green><center>Quick navigation<\/center><\/h3>\n\n* [1. Required Libraries](#1)\n* [2. Dataset Quick Overview](#2)\n* [3. Tweets EDA](#3)\n* [4. Tweets text analysis](#4)   \n\n    Kindly, Upvote the notebook!","7221cf0a":"#### India holds the most number of tweets by location followed by San diego and Los angeles, California!","a879dc7d":"## Prevalent words in tweets ","2e3312c3":"[Dataset Link with 10,000 and more tweets with #TheSocialDilemma hashtag](https:\/\/www.kaggle.com\/kaushiksuresh147\/the-social-dilemma-tweets)","90969697":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0' role=\"tab\" aria-controls=\"home\" color=green><center>The Social Dilemma, a documentary-drama hybrid explores the dangerous human impact of social networking as the tech experts sound the alarm on the dangerous human impact of social networking.<\/center><\/h3>"}}