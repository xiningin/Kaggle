{"cell_type":{"87208b53":"code","2b3f9fd6":"code","f5164b98":"code","16de4e76":"code","a27747df":"code","79642067":"code","400d76e6":"code","ed4a6dcc":"code","91d5d721":"code","4c921e24":"code","9a3a318c":"code","b0c3c592":"code","d93c1044":"code","57bb1200":"code","1941888a":"code","49c023ce":"code","a7ed0995":"code","e070f2cf":"code","c939e206":"code","ff76dc35":"markdown","2f39ed10":"markdown","48720778":"markdown","716310b2":"markdown","d648f283":"markdown","e92b7916":"markdown","9e7bb351":"markdown","69e1aab4":"markdown","d50c4e6e":"markdown","0cbb07dd":"markdown","6c05efa7":"markdown"},"source":{"87208b53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b3f9fd6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import precision_score, classification_report, confusion_matrix","f5164b98":"df = pd.read_csv(\"..\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv\")\ndf.head()","16de4e76":"df.info()","a27747df":"df.describe()","79642067":"df.isnull().mean()*100","400d76e6":"df.targets.value_counts()","ed4a6dcc":"# first columns is redundant\ndf = df.drop('Unnamed: 0', axis=1)","91d5d721":"scaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)","4c921e24":"df_scaled","9a3a318c":"X = df.drop('targets', axis=1)\ny = df['targets']","b0c3c592":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","d93c1044":"xg_clf = XGBClassifier()\nxg_clf.fit(X_train, y_train,\n           eval_set=[(X_train, y_train), (X_test, y_test)],\n        eval_metric='logloss',\n        verbose=True)\ny_pred= xg_clf.predict(X_test)\nprecision = precision_score(y_pred, y_test)\nevals_result = xg_clf.evals_result()\n","57bb1200":"precision_score(y_pred, y_test)","1941888a":"lgbm_clf = LGBMClassifier(class_weights = [1, 2.76])\nlgbm_clf.fit(X_train, y_train)\nypred = lgbm_clf.predict(X_test)\nprecision_score(ypred, y_test)","49c023ce":"cat_boost =  CatBoostClassifier(class_weights = [1, 2.76388378])\ncat_boost.fit(X_train, y_train)\ny_pred = cat_boost.predict(X_test)","a7ed0995":"precision_score(y_pred, y_test)","e070f2cf":"print(confusion_matrix(y_pred, y_test))","c939e206":"print(classification_report(y_pred, y_test))","ff76dc35":"## 1. Importing Libraries and Loading Data","2f39ed10":"## 3.1 XGBClassifier","48720778":"## 3.3 CatBoostClassifier","716310b2":"## 3.2 LGBMClassifier","d648f283":"## 2. Data preprocesing","e92b7916":"**EDA is of no use. Will take too much time. **","9e7bb351":"****So, we have 91.30% precision and we predict Negative cases with 95% precision. This model can be even better if we use ensemble techniques like doing some stacking or blending. That's for another day.","69e1aab4":"**We will use boosting tree methods startting with XGBoost.**","d50c4e6e":"## 3. Splitting the dataset","0cbb07dd":"**Hurray!!, 91.30% precision.**","6c05efa7":"**Used the class_weights parameter to undo the imbalance in classes**"}}