{"cell_type":{"bd18fe6b":"code","ea010004":"code","5498e71b":"code","99c54eb0":"code","6d1bf19b":"code","6eaecd34":"code","f0bba0a5":"code","79cac865":"code","5953cee5":"code","649124d3":"code","a83795be":"code","3d5a4b29":"code","5f661e52":"markdown","9509c248":"markdown","6bcd74ba":"markdown","ecc95888":"markdown","6a19f83d":"markdown","a578c782":"markdown","89bda162":"markdown"},"source":{"bd18fe6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea010004":"!pip install pyspark","5498e71b":"from pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import SparkSession\nimport re\n\nss = SparkSession.builder \\\n    .config(\"spark.driver.memory\", \"10g\") \\\n    .getOrCreate()\nsc = ss.sparkContext\ns  = SQLContext(sc)","99c54eb0":"tweets = sc.textFile(\"\/kaggle\/input\/emojifydata-en\/dev.txt\")","6d1bf19b":"tweets_clean = tweets.flatMap(lambda x: x.split(' ')).filter(lambda x:x!='O').filter(lambda x:x!='').filter(lambda x:x!='<STOP>')","6eaecd34":"tweet_list = tweets_clean.collect()","f0bba0a5":"tweets = []\nglobal counter\ncounter = -1\n\ndef wordsToTweets(x):\n    global counter\n    if(x=='<START>'):\n        counter+=1\n        tweets.append('')\n    else:\n        tweets[counter]+=(x+\" \")","79cac865":"for word in tweet_list:\n    wordsToTweets(word)","5953cee5":"tweet_rdd = sc.parallelize(tweets)\ntweets = tweet_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))","649124d3":"text_rdd = tweets.map(lambda x: (x[0],re.sub(\":.*?:\",\"\",x[1])))\nemoji_rdd = tweets.map(lambda x: (x[0],re.findall(\":.*?:\",x[1])))\nemoji_rdd1 = emoji_rdd.map(lambda x:(x[0],x[1][0]))\nmax_emoji = emoji_rdd.map(lambda x: len(x[1])).max()","a83795be":"for i in range(1,max_emoji):\n    emoji_rdd2 = emoji_rdd.filter(lambda x:len(x[1])>i).map(lambda x: (x[0],x[1][i]))\n    if i==1:\n        emoji_rdd3 = emoji_rdd1.union(emoji_rdd2)\n    else:\n        emoji_rdd3 = emoji_rdd3.union(emoji_rdd2)\n\nrdd_for_df = text_rdd.leftOuterJoin(emoji_rdd3).map(lambda x:(x[1][0][:-1],x[1][1][1:-1]))","3d5a4b29":"df = s.createDataFrame(rdd_for_df, ['text','emoji']).distinct()\ndf.show()","5f661e52":"### create dataframe ","9509c248":"### read dev.txt as rdd","6bcd74ba":"### get emoji out of text","ecc95888":"### join text rdd with emoji rdd","6a19f83d":"* remove '0' from every word\n* remove empty words\n* remove < STOP> tag","a578c782":"### convert back to rdd","89bda162":"### convert to python list to concatenate words into tweets\n\n> I don't think this is possible in rdd\n\n- removes < START> tag"}}