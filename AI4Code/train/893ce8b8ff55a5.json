{"cell_type":{"0fc6c621":"code","28d5a131":"code","f6815ca0":"code","32175f93":"code","d1ef2fc5":"code","474751bf":"code","be8365ff":"code","45cd1188":"code","5c7d1eef":"code","4d688556":"code","8046fe46":"code","f4797bf6":"code","47a3dd13":"code","8586021e":"code","5f5c2ec6":"code","dcfc3b38":"code","75f8f214":"code","bf529957":"code","af45bc16":"code","4a252187":"code","21316658":"code","9783093d":"code","142aac68":"code","10f1bc80":"code","fa9ad9fa":"code","341813f2":"code","28f606f0":"code","eb944417":"code","5061b822":"code","1cbb3f80":"code","1b07a065":"code","dc4a055f":"code","49734220":"code","88f4029a":"code","3bb4b3e4":"code","baa8d9cf":"markdown","9db4aaf0":"markdown","c173f34a":"markdown","896c3d16":"markdown","55932c75":"markdown","7f5573bf":"markdown","c0a29a0a":"markdown","b29276f8":"markdown","10ee970a":"markdown","29325b84":"markdown","9f70c2ec":"markdown","9f11d9c7":"markdown","d3ac090d":"markdown","663613a4":"markdown","47aafcda":"markdown"},"source":{"0fc6c621":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","28d5a131":"data = pd.read_csv('..\/input\/financial-distress\/Financial Distress.csv')","f6815ca0":"data.info()","32175f93":"data.head()","d1ef2fc5":"df = data.copy()","474751bf":"def missing_values_table(df):\n    total_missing = df.isnull().sum().sort_values(ascending=False)\n    percentage_missing = (100*df.isnull().sum()\/len(df)).sort_values(ascending=False)\n    missing_table = pd.DataFrame({'missing values':total_missing,'% missing':percentage_missing})\n    return missing_table","be8365ff":"missing_values_table(df)","45cd1188":"df.shape","5c7d1eef":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n# Drop features \ndf.drop(to_drop, axis=1, inplace=True)","4d688556":"df.shape","8046fe46":"Y = df.iloc[:,2].values\nfor y in range(0,len(Y)):\n       if Y[y] > -0.5:\n              Y[y] = 0\n       else:\n              Y[y] = 1\nX = df.iloc[:,3:].values","f4797bf6":"print(df['Financial Distress'].value_counts())\ndf['Financial Distress'].value_counts().plot(kind='bar')","47a3dd13":"X = pd.DataFrame(X)\nY = pd.DataFrame(Y)","8586021e":"X.head()","5f5c2ec6":"Y.head()","dcfc3b38":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)","75f8f214":"print(f\"Shape of X_train is :{X_train.shape},\\nShape of X_test is :{X_test.shape},\\nShape of y_train is :{y_train.shape},\\nShape of y_test is :{y_test.shape}\")","bf529957":"#Importing Evaluation metrics.\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","af45bc16":"from sklearn.naive_bayes import BernoulliNB","4a252187":"BNB = BernoulliNB()","21316658":"BNB.fit(X_train,y_train)","9783093d":"BNB_pred = BNB.predict(X_test)","142aac68":"accuracy_score(BNB_pred,y_test)","10f1bc80":"BNB_CM = pd.DataFrame(confusion_matrix(BNB_pred,y_test), index = ['Actual No','Actual Yes'], columns=['Predicted No','Predicted Yes'])","fa9ad9fa":"BNB_CM","341813f2":"print(classification_report(BNB_pred,y_test))","28f606f0":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","eb944417":"LDA = LinearDiscriminantAnalysis()","5061b822":"LDA.fit(X_train,y_train)","1cbb3f80":"LDA_pred = LDA.predict(X_test)","1b07a065":"LDA_pred = pd.DataFrame(LDA_pred)","dc4a055f":"accuracy_score(LDA_pred,y_test)","49734220":"LDA_CM = pd.DataFrame(confusion_matrix(LDA_pred,y_test), index = ['Actual No','Actual Yes'], columns=['Predicted No','Predicted Yes'])\nLDA_CM","88f4029a":"LDA_pred.head(60)","3bb4b3e4":"LDA_pred.tail(60)","baa8d9cf":"Checking for highly correlated features.","9db4aaf0":"We can see that number of **1**'s in predictions are lower compared **0**'s because of low number of **1**'s in our training data.\nSince the target variable is highly imbalanced, we see such problems.","c173f34a":"Let's take a look at the distribution of our target variable.****","896c3d16":"Feel free to try out with other models as well.","55932c75":"After trying with different models, I felt that LDA worked better compared to others as it's confusion matrix looks more balanced compared to other models,hence it looks like a more robust model.","7f5573bf":"It looks like there aren't any missing values.","c0a29a0a":"Let's check if there are any missing values.","b29276f8":"Let's make a copy of our main dataframe.\nAlways follow this step, so that if something gets messed up, you have a back-up.","10ee970a":"## Bernoulli Naive Bayes","29325b84":"Splitting the data into train and test set.","9f70c2ec":"Now let's look at our dataframe's shape.","9f11d9c7":"Now that everything looks good, let's get to training.","d3ac090d":"It looks like about 8 features were highly correlated, and were removed.","663613a4":"Now let's convert target variables into binary form, i.e, 0 and 1.\n\n1 - Company is bankrupt.\n0 - Company is healthy.\n\nAlso removing ***Company*** and ***Time*** features.","47aafcda":"## LDA"}}