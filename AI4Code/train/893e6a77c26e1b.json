{"cell_type":{"3d83fc04":"code","19e9a26d":"code","77dffed8":"code","af84c774":"code","0157299c":"code","d3dcf3e1":"code","807b9f83":"code","dacefd64":"code","033ec5d8":"code","b77351ff":"code","393da949":"code","25ade148":"code","cd340c8b":"code","6ba4b409":"code","eab5d788":"code","73036769":"code","82a49f8c":"code","ec1709c9":"code","5fe3fef7":"code","82804348":"code","2486894c":"code","222ad650":"code","04831651":"code","30ac5d78":"code","8fc5f5d7":"code","6c1bfdc6":"code","6202a67e":"code","316361fd":"code","bd9bcd01":"code","063e52ca":"code","dc87759b":"code","aa094aba":"code","8d3ef5dd":"code","0a7e59e8":"code","6481bd22":"code","3415b9b0":"code","2d7c202d":"code","41040c47":"code","bb3e9652":"markdown","2ce9256f":"markdown","363f5add":"markdown","8968a62d":"markdown","5456822f":"markdown","14632385":"markdown","d4b62de4":"markdown","34c89bce":"markdown","89eb3996":"markdown","4eeb2a94":"markdown","80f4bc02":"markdown","4dfffc73":"markdown","ae4ef82b":"markdown","bfc7d403":"markdown","a22af97c":"markdown","fa281591":"markdown","cf0233c4":"markdown","2c358111":"markdown","a03bc2da":"markdown","32b1c816":"markdown","b06d523f":"markdown","02b6dac5":"markdown","9dd931b6":"markdown","e6a6fdc1":"markdown","0a459ee2":"markdown","bfa47da9":"markdown","a5a3ff9b":"markdown","25b7a577":"markdown","1ee8b242":"markdown","7ea0e63d":"markdown","e3f7e752":"markdown","72094fee":"markdown","f8c6a758":"markdown","61bc83dc":"markdown","0e1db39a":"markdown","34d0682e":"markdown"},"source":{"3d83fc04":"import numpy as np\nfrom pandas import DataFrame, Series, read_csv","19e9a26d":"titles = read_csv(\"\/kaggle\/input\/clickbait-dataset\/clickbait_data.csv\")\ntitles_len = len(titles)\nclckbt_ratio = len(titles[titles[\"clickbait\"]==0])\/titles_len\nprint(\"Database lenght : {} \\nClickbait ratio: {}\".format(titles_len, clckbt_ratio))\ntitles.head()","77dffed8":"for head in titles['headline'][titles['clickbait']==1][:10]:\n    print(head)","af84c774":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(titles['headline'], titles[\"clickbait\"],\n                                                    test_size=.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)","0157299c":"!pip install contractions\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom contractions import contractions_dict\nfrom nltk import word_tokenize\nfrom nltk.tag import pos_tag\nimport contractions\nimport string\n\n_lem = WordNetLemmatizer()\ncontractions_set = set(contr.lower() for contr in contractions_dict)\n\n\ndef remove_contractions(string):\n    ''' Expand and count the contractions in a given string.\n        Return string and the contractions number '''\n    string = string.lower()\n    contr_num = sum(1 for contr in contractions_set if contr in string)\n    parsed_string = ' '.join(contractions.fix(word) for word in string.split())\n    return parsed_string, contr_num\n\n\ndef lemmatise_sentence(sentence):\n    \n    # remove contarctions and convert to lower case\n    sentence, contr_num = remove_contractions(sentence.lower())\n    \n    # remove punctuation\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation+'\u2019\u2018'))  \n    \n    # lemmatize words\n    lemm_str = \"\"\n    for word, tag in pos_tag(word_tokenize(sentence.lower())):\n        if tag.startswith('NN'):\n            word_1 = word \n            pos = 'n'\n        elif tag.startswith('VB'):\n            word_1 = word\n            pos = 'v'\n        elif tag.startswith('CD'):\n            word_1 = 'NUM'\n            pos = 'a'\n        else:\n            word_1 = word\n            pos = 'a'\n        lemm_str += ' '+_lem.lemmatize(word_1, pos)\n    \n    return lemm_str, contr_num","d3dcf3e1":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ParseString(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self = True\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_prep, contr_list = [], []\n        for string in X:\n            lemm_str, contr_num = lemmatise_sentence(string)\n            X_prep.append(lemm_str)\n            contr_list.append(contr_num)\n        return DataFrame({\"headline\": X_prep, \"contr num\":contr_list})","807b9f83":"X_train_prep = ParseString().fit_transform(X=X_train)","dacefd64":"import warnings\nfrom sklearn.feature_extraction.text import CountVectorizer\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')    # shut up bs4 URL warning\n\n\ndef make_vocabulary(X, length, rm_words=False, to_del_words_list=None):\n    vectorizer = CountVectorizer(max_features=length)\n    vectorizer.fit(X)\n    vocab = vectorizer.get_feature_names()\n    if rm_words:\n        for word in to_del_words_list:\n            if word in vocab:\n                vocab.remove(word)\n    return vocab\n\n\ndef save_vocabulary(words_list, txt_file):\n    file = open(txt_file, 'w+')\n    for word in words_list:\n            file.write(str(key)+'\\n')\n    file.close()\n    \n    print('Vocabulary stored in \"{}\"'.format(txt_file))\n    \n\ndef load_vocabulary(length, txt_file, to_del=None):\n    file = open(txt_file, 'r')\n    vocab = np.array([file.readline().rstrip().lower() for line in range(length)])\n    file.close()\n    print('Dictionary loaded.')\n    return vocab\n    \n\nto_del = ['trump','donald','christmas','obama','president','america','harry','russian','russia','china',\n          'american']","033ec5d8":"X_train_clckb = X_train_prep.headline.values[y_train==1]\nX_train_noclckbt = X_train_prep.headline.values[y_train==0]\n\nfull_vocab = make_vocabulary(X_train_prep.headline, length=20)\nclckbt_vocab = make_vocabulary(X_train_clckb, length=21, rm_words=True, to_del_words_list=to_del)\nno_clckbt_vocab = make_vocabulary(X_train_noclckbt, length=20)","b77351ff":"common_words = DataFrame({'No Clickbait': no_clckbt_vocab[:20], \n                          'Clickbait': clckbt_vocab[:20], \n                          'Full': full_vocab[:20]})\ncommon_words.transpose()","393da949":"from scipy.sparse import coo_matrix, hstack\nfrom sklearn.pipeline import Pipeline\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n\nclass PreProcess(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.vectorizer = CountVectorizer(vocabulary=self.vocabulary)\n        self.stopwords_set = set(stopwords.words('english'))\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # bag of words\n        X_bag = self.vectorizer.transform(X.headline)\n        # meta data\n        meta_arr = []\n        for i in range(len(X)):\n            d = Counter(X.headline.iloc[i].split())\n            num_flag = 1 if list(d)[0]=='NUM' else 0\n            n_of_words = sum(d.values())\n            contr_r = X['contr num'].iloc[i]\/n_of_words\n            stop_r = sum(d[key] for key in set(d.keys())&self.stopwords_set) \/ n_of_words\n            meta_arr.append([num_flag, contr_r, stop_r, n_of_words])\n        meta_arr = coo_matrix(meta_arr)\n        return hstack([X_bag, meta_arr])\n\n    \n    \nfull_pipeline = Pipeline([\n    (\"parse text\", ParseString()),\n    (\"gen features\", PreProcess(vocabulary=clckbt_vocab))\n])\n","25ade148":"X_train_prep = full_pipeline.fit_transform(X_train)\nX_train_prep.shape","cd340c8b":"X_train_mini = X_train_prep.toarray()[:1000]\ny_train_mini = y_train[:1000]","6ba4b409":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n\nscorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score)\n}\n\n\ndef print_scores(clf_cv, acc=True, prec=True, rec=True):\n    '''Print cross validation results.\n       Valid only for CrossValidation.\n    '''\n    if acc:\n        print('accuracy: %.3f' % clf_cv['test_accuracy_score'].mean())\n    if prec:\n        print('precision: %.3f' % clf_cv['test_precision_score'].mean())\n    if rec:\n        print('recall: %.3f' % clf_cv['test_recall_score'].mean())\n    ","eab5d788":"from sklearn.naive_bayes import MultinomialNB\n\nmnb_clf = MultinomialNB()\n\nmnb_clf_cv = cross_validate(mnb_clf, X_train_mini[:,:200], y_train_mini, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(mnb_clf_cv)","73036769":"mnb_clf_cv = cross_validate(mnb_clf, X_train_prep.toarray()[:,:200], y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(mnb_clf_cv)","82a49f8c":"mnb_clf.fit(X_train_prep.toarray()[:,:200], y_train)\nprobabilities = mnb_clf.predict_proba(X_train_prep.toarray()[:,:200])[:, 1]\nprobabilities = probabilities.reshape((len(probabilities), 1))","ec1709c9":"X_train_forest = np.concatenate([probabilities, X_train_prep.toarray()[:, 200:]], axis=1)","5fe3fef7":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\n\nforest_cv = cross_validate(forest_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(forest_cv)","82804348":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n\nrnd_srch_forest = RandomizedSearchCV(forest_clf, param_distributions=param_distrib,\n                                     cv=5, scoring='accuracy', random_state=42,\n                                     n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_forest.fit(X_train_forest[:1000], y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_forest.best_score_)\nprint('Best params:', rnd_srch_forest.best_params_)","2486894c":"forest_clf = rnd_srch_forest.best_estimator_\n\nforest_cv = cross_validate(forest_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(forest_cv)","222ad650":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsvc_clf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\nsvc_cv = cross_validate(svc_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(svc_cv)","04831651":"param_distrib = {'svm__kernel': ['rbf', 'poly'],\n                 'svm__C': uniform(1,20),\n                 'svm__gamma': reciprocal(.0001, .1),\n                }\n                 \n\nrnd_srch_svc = RandomizedSearchCV(svc_clf, param_distributions=param_distrib,\n                                  cv=5, scoring='accuracy', n_iter=1000, verbose=5, n_jobs=-1)\n\nrnd_srch_svc.fit(X_train_forest[:1000], y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_svc.best_score_)\nprint('Best params:', rnd_srch_svc.best_params_)","30ac5d78":"svc_clf = rnd_srch_svc.best_estimator_\n\nsvc_clf_cv = cross_validate(svc_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(svc_clf_cv)","8fc5f5d7":"forest_clf_v1 = RandomForestClassifier(random_state=42)\n\nforest_cv_v1 = cross_validate(forest_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(forest_cv_v1)","6c1bfdc6":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n\nrnd_srch_forest_v1 = RandomizedSearchCV(forest_clf_v1, param_distributions=param_distrib,\n                                     cv=5, scoring='accuracy', random_state=42,\n                                     n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_forest_v1.fit(X_train_mini, y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_forest_v1.best_score_)\nprint('Best params:', rnd_srch_forest_v1.best_params_)","6202a67e":"forest_clf_v1 = rnd_srch_forest_v1.best_estimator_\n\nforest_cv_v1 = cross_validate(forest_clf_v1, X_train_prep, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(forest_cv_v1)","316361fd":"svc_clf_v1 = Pipeline([\n    ('scaler', StandardScaler(with_mean=False)),\n    ('svm', SVC())\n])\n\nsvc_cv_v1 = cross_validate(svc_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(svc_cv_v1)","bd9bcd01":"param_distrib = {'svm__kernel': ['rbf', 'poly'],\n                 'svm__C': uniform(1,20),\n                 'svm__gamma': reciprocal(.0001, .1),\n                }\n\nrnd_srch_svc_v1 = RandomizedSearchCV(svc_clf_v1, param_distributions=param_distrib,\n                                  cv=5, scoring='accuracy', n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_svc_v1.fit(X_train_mini, y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_svc_v1.best_score_)\nprint('Best params:', rnd_srch_svc_v1.best_params_)","063e52ca":"svc_clf_v1 = rnd_srch_svc_v1.best_estimator_\n\nsvc_clf_cv_v1 = cross_validate(svc_clf_v1, X_train_prep.toarray(), y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(svc_clf_cv_v1)","dc87759b":"X_val_prep = full_pipeline.transform(X_val)","aa094aba":"y_val_pred = mnb_clf.predict(X_val_prep.toarray()[:,:200])\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","8d3ef5dd":"y_val_proba = mnb_clf.predict_proba(X_val_prep.toarray()[:,:200])[:,1]\ny_val_proba = y_val_proba.reshape((len(y_val), 1))\nX_val_forest = np.concatenate([y_val_proba, X_val_prep.toarray()[:, 200:]], axis=1)","0a7e59e8":"forest_clf.fit(X_train_forest, y_train)\ny_val_pred = forest_clf.predict(X_val_forest)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","6481bd22":"svc_clf.fit(X_train_forest, y_train)\ny_val_pred = svc_clf.predict(X_val_forest)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","3415b9b0":"forest_clf_v1.fit(X_train_prep, y_train)\ny_val_pred = forest_clf_v1.predict(X_val_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","2d7c202d":"svc_clf_v1.fit(X_train_prep, y_train)\ny_val_pred = svc_clf_v1.predict(X_val_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","41040c47":"X_test_prep = full_pipeline.transform(X_test)\ny_test_pred = svc_clf_v1.predict(X_test_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_test, y_test_pred))\nprint('Precision: %.3f' % precision_score(y_test, y_test_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_test_pred))","bb3e9652":"### 4.2 Train Random Forest on Naive Bayes probabilities and non-word features","2ce9256f":"Now let's train a blind SVC on all the 204 features. Before doing that do not forget to __rescale the features__!","363f5add":"Fine tune","8968a62d":"Fine tune","5456822f":"... then on the full rain set","14632385":"### 4.5 Train naive SVM on all the features","d4b62de4":"Make a mini train set for exploratory analysis","34c89bce":"## 2. Preprocessing and Analysis\n\nBefore doing any analysis let's split the dataset into train and test sets. \nThe test size is the 10% of the total set (about 6000 headlines).\nWee further split the train set into train plus validation (10% of the initial train set). ","89eb3996":"We have defined all the parsing functions we need.\nLet's build a class that implement them on the database.","4eeb2a94":"Here the 20 most common clickbait words (in alphabetical order):","80f4bc02":"And now use __Cross Validation__ on the full train set to get a estimate of the performances.","4dfffc73":"### Random Forest on 5 features","ae4ef82b":"Cross Validation on train_set","bfc7d403":"### Naive Bayes","a22af97c":"## 5 Test on Validation set\n\nEvaluate the 5 classifiers we've built on the Validation set to see which performs better\n\n### Prepare titles","fa281591":"### Random Forest on 204 features","cf0233c4":"### 4.1 Train Naive Bayes only on the Bag of Words\n\nFirst we try it on Train_mini","2c358111":"## 4. Train some classifier","a03bc2da":"Let's train some classifier and see which one performs the best.","32b1c816":"Then we fine tune some hyperparameters.","b06d523f":"# Clickbait Classifier\n\nLet's build a simple ClickBait Classifier.\nWe'll try several algorithms and select the one that performs b \nWe'll use the [ClicBait Dataset](https:\/\/www.kaggle.com\/amananandrai\/clickbait-dataset) created by Aman Anand. \nIt contains ...","02b6dac5":"Here some clickbait headlines","9dd931b6":"### 6. Final evaluation on the Test Set\n\nChoose the best algorithm: __SVC classifier on 204 features__.","e6a6fdc1":"Make the vocabulary","0a459ee2":"## 3. Create the Pipeline\n\nTime to create a Pipeline that generates all the features. Together with the __words count__ we add 4 additional features \n- headline length (number of words)\n- stopwords ratio \n- contractions ratio\n- a flag if the headline starts with a number","bfa47da9":"### 4.5 Train naive Random Forest on all the features\n\nNow let's try a blind Random forest on all the 204 features. \n\nFirst on train_mini","a5a3ff9b":"#### Bag of Words\n\nTime to make the features.We create a bag of words that counts 200 most common clickbait words. \nTo better generalize, we then remove some common words related to actuality, as Trum, Donald, Obama, ... ","25b7a577":"Cross validation on train_set","1ee8b242":"### Parse the text\n\nImplement the following operations on each string:\n\n- converts to lower-case,\n- expand contractions,\n- remove punctuation,\n- lemmatize words\n\nWe also count the __number of contractions__ for each headline. This is because, usually clickbait headlines have an higer ratio of contracted words. ","7ea0e63d":"#### SVC on 5 features","e3f7e752":"### 4.3 Train SVM on Naive Bayes probabilities and non-word features\n\nLet's do the same thing of Sec.4.2 but using a SVM instead.\n\nExploratory analysis on train_mini.","72094fee":"Now we fit the model on the full train set. We'll use it's output on as a feature for a SVC and Random Forest.","f8c6a758":"## 1. Get the data","61bc83dc":"Fine tune","0e1db39a":"### SVC on 204 features","34d0682e":"First we train it on train_mini"}}