{"cell_type":{"dca910c8":"code","c7bf6b5f":"code","58199f9a":"code","9527de65":"code","9c864183":"code","d9e2d6c6":"code","53636ce9":"code","f2513c71":"code","2c13aba4":"code","af4fed9b":"code","2a04986a":"code","ee812107":"code","617ed293":"code","9c6456f0":"code","f9de1c08":"code","cf397ed5":"code","75d8db25":"code","3199ce5b":"code","694ab643":"code","ba0777a0":"code","5a565672":"code","a4aae34a":"code","83d69696":"code","f8f1e134":"code","6638ed5e":"code","e61c2293":"code","6126929f":"code","28039b52":"code","123025ce":"code","e378372f":"code","80bf8f24":"code","3c52af9e":"code","8ec87553":"code","535fc0ec":"code","7595d1c1":"markdown","51b920ce":"markdown","dcdd11a6":"markdown","d8e13a32":"markdown","b0cbec44":"markdown","841aff8a":"markdown"},"source":{"dca910c8":"import nltk\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.tokenize import RegexpTokenizer\n","c7bf6b5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","58199f9a":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadssecondcsv\/second.csv', delimiter=';', encoding = \"utf8\", nrows = nRowsRead)\ndf.dataframeName = 'second.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","9527de65":"#We need only three columns Score, Summary & Text. Let 's remove all the other columns.\n\n#df=df[['Study Type','Factors', 'Excerpt', 'Sample Text', 'Matches']]\n#df.head()","9c864183":"#calculating length of lists in pandas dataframe column \n#ref. https:\/\/stackoverflow.com\/questions\/41340341\/pythonic-way-for-calculating-length-of-lists-in-pandas-dataframe-column\ndate=df['Date'].str.len()\nprint(date)","d9e2d6c6":"# let's check the length of summaries, the average length is 20 characters.\ndf['date length'] = df['Date'].apply(len)\ndf['date length'].describe()","53636ce9":"import seaborn as sns\nsns.boxplot(x='Study Type', y=df['date length'], data=df)","f2513c71":"# let's do the same to the text, the avearage length is 302 characters.\ndf['study length'] = df['Study'].apply(len)\ndf['study length'].describe()","2c13aba4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(x='Study Type', y=df['study length'], data=df)\nplt.ylim(0, 900)","af4fed9b":"# Let 's keep the max length of Date to 30 characters and Study to 300 characters. \ndf= df[df['date length'] <=30]\ndf= df[df['study length'] <=300]\n\nlen(df)\ndf.head()","2a04986a":"df=df[['Date', 'Study Type','Study']]\ndf.head()\n#len(df) the length of updated dataset is 239868","ee812107":"df = df.reset_index(drop=True)# we need to reindex the dataset after removed some rows\ndf.head()","617ed293":"import requests\nfrom bs4 import BeautifulSoup \n\ndef preprocess_text(text):\n    \"\"\" Apply any preprocessing methods\"\"\"\n    text = BeautifulSoup(text).get_text()\n    text = text.lower()\n    text = text.replace('[^\\w\\s]','')\n    return text\n\ndf[\"Study\"] = df.Study.apply(preprocess_text)\ndf[\"Date\"] = df.Date.apply(preprocess_text)","9c6456f0":"field_length = df.Study.astype(str).map(len)\nprint()\nprint(\"###This is the longest text###\")\nprint (df.loc[field_length.idxmax(), 'Study'])\nprint()\nprint(\"###This is the shortest text###\")\nprint()\nprint (df.loc[field_length.idxmin(), 'Study'])","f9de1c08":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n#not is in the stopword list. W\/O adding whitelist, the summary would change from \"do not recommend\" to \"recommend\". This solution was borrowed from bertcarremans https:\/\/github.com\/bertcarremans\/TwitterUSAirlineSentiment\nwhitelist = [\"n't\", \"not\", \"no\"]\n\ndf['Study_after_removed_stopwords'] = df['Study'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint()\nprint('###Study after removed stopwords###'+'\\n'+df['Study_after_removed_stopwords'][1])\nprint()\nprint('###Study before removed stopwords###'+'\\n'+ df['Study'][1])\nprint()\ndf['Date_after_removed_stopwords'] = df['Date'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint('###Date after removed stopwords###'+ '\\n'+df['Date_after_removed_stopwords'][1])\nprint()\nprint('###Date before removed stopwords###'+'\\n'+df['Date'][1])","cf397ed5":"# A list of contractions from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","75d8db25":"#This two blocks of code was refered from https:\/\/www.kaggle.com\/currie32\/summarizing-text-with-amazon-reviews\ndef clean_text(text):\n\n    # Replace contractions with longer forms in the above list\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)","3199ce5b":"\nclean_studies = []\nfor study in df.Study:\n    clean_studies.append(clean_text(study))\nprint(\"Studies are complete.\")\n\nprint(len(clean_studies))\n\n\nclean_texts = []\nfor text in df.Date:\n    clean_texts.append(clean_text(text))\nprint(\"Texts are complete.\")\nprint(len(clean_texts))\nclean_text","694ab643":"df.head()","ba0777a0":"r'\/amazon_clean","5a565672":"df.to_csv(r'\/cusersmarildownloadssecondcsv_clean.csv',index=False)\nlen(df)","a4aae34a":"df1=df.sample(frac=0.100, replace=True, random_state=1)\nlen(df1)","83d69696":"df1 = df1.reset_index(drop=True)# we need to reindex the dataset after removed some rows","f8f1e134":"df1.tail()","6638ed5e":"df2=df1[['Study','Date']]\ndf2.head()\ndocs=df2.apply(lambda x: \" \".join(x), axis=1)\ndocs.head()","e61c2293":"from sklearn.feature_extraction.text import CountVectorizer\n#create a vocabulary of words, \n#ignore words that appear in 85% of documents, \n#eliminate stop words\ndocs=df2.apply(lambda x: \" \".join(x), axis=1).tolist()\n\ncv=CountVectorizer(max_df=0.85,max_features=10000)\nword_count_vector=cv.fit_transform(docs)\n \nlist(cv.vocabulary_.keys())[:10]\n ","6126929f":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","28039b52":"word_count_vector.shape","123025ce":"\"\"\"\nhttps:\/\/kavita-ganesan.com\/tfidftransformer-tfidfvectorizer-usage-differences\/#.XYtStuczbOQ\nneed the term frequency (term count) vectors for different tasks, use Tfidftransformer.\nneed to compute tf-idf scores on documents within\u201ctraining\u201d dataset, use Tfidfvectorizer\nneed to compute tf-idf scores on documents outside \u201ctraining\u201d dataset, use either one, both will work.\n\n\"\"\"\n\n# print idf values\ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_idf.sort_values(by=['idf_weights'])","e378372f":"df3=df[['Study','Date']]\n\ndf3.tail()","80bf8f24":"#https:\/\/github.com\/kavgan\/nlp-in-practice\/blob\/master\/tf-idf\/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)","3c52af9e":"def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","8ec87553":"# you only needs to do this once, this is a mapping of index to \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfeature_names=cv.get_feature_names()\n \n# get the document that we want to extract keywords from\ndoc=df['Date'][8]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n \n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n \n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\n=====Doc=====\")\nprint(doc)\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n    \nprint(\"\\n===original study===\")\nprint(df['Study'][8])\n \n","535fc0ec":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke was Here' )","7595d1c1":"I wanted to choose another feature though its length must result INTEGER. ","51b920ce":"The value of frac was frac=0.001  That resulted in 0 and df.tail was empty. So I changed to 0.100","dcdd11a6":"In February, we were  \"simulating the infected population and spread of Covid-19\". Curves, curves, and  more curves from the whole World.","d8e13a32":"#Code by Latong https:\/\/www.kaggle.com\/latong\/food-review-text-summarization","b0cbec44":"#I have no clue if I made anything wrong. Probably, yes. Though the Programm didn't return any error.\n\n#Let me know if you read what I should correct. OK?","841aff8a":"###According to this link https:\/\/stackoverflow.com\/questions\/54891464\/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\nand other papers, I will do the tokenization to only train dataset. As I don't have enough memory, I will first export the cleaned dataset into csv format; \nSecond subsample the dataset and split the toy dataset into train and test sets; Third, tokenize the train set. \n"}}