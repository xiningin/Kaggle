{"cell_type":{"51181045":"code","3f64e5c1":"code","3d024306":"code","951fca52":"code","e33234ea":"code","8b3fbc5f":"code","cac6b254":"code","0a96b44e":"code","25aab7bb":"code","42604ca3":"code","e8e63c5f":"code","b6be19d3":"code","bbcb77a2":"code","9cd65938":"code","54609b35":"code","f9f54356":"code","1edcd35a":"code","be5233d1":"code","288be8c3":"code","e3937663":"code","77a0ca19":"code","e7768852":"code","1a6d9278":"code","6f609509":"code","0efa5f61":"code","7bf18373":"code","e5806e09":"code","9d0d61a4":"code","e99a912b":"code","3d4edd96":"code","ff2f7fe0":"code","644b5c98":"code","1c156578":"code","da5338bb":"code","c16e3c27":"code","2aaa9bb1":"code","c6367034":"code","acfcc4c6":"code","4a665cd8":"code","e1ea501c":"code","addd5a46":"code","8a2a698e":"code","1d273459":"code","0dcbc35b":"code","d13bb9b3":"code","56223c54":"code","0604ea49":"code","cbd9ee4b":"code","43ff9348":"code","1777cf59":"code","3830f0df":"code","5da43efa":"code","711d1d23":"code","aaf571dd":"code","dac61cca":"code","d5fb4cf9":"code","ab45f841":"code","8d3d365e":"code","023ed0e1":"code","ce5493c0":"code","07099c39":"code","f801720f":"code","325a069c":"code","069883e6":"code","38b986c6":"code","83cd8a1a":"code","ec8e85b4":"code","70d2c6ba":"code","69c68879":"code","ff5f4a4c":"code","18009f7d":"code","d3b59559":"code","a94a52c6":"code","81c0d18f":"code","386d5719":"code","13e89add":"code","bc85bb3b":"code","9e540bc0":"code","1dc96643":"code","a2c2251e":"code","89c53d4a":"code","ad1803df":"code","201bc021":"code","b52e5aea":"code","bb73aa4b":"code","9fd4b1e5":"code","e546beb1":"code","49e24433":"code","30b2de10":"code","4ab2b1df":"markdown","9791c33b":"markdown","fa043ccf":"markdown","eecc8162":"markdown","a89b625c":"markdown","7e4495e4":"markdown","cad84542":"markdown","50d1deeb":"markdown","a91f3863":"markdown","ecec4cc8":"markdown","99c07c13":"markdown","85e83ff0":"markdown","dd13d9d3":"markdown","1b1b01b2":"markdown","60691d88":"markdown","1e458f8c":"markdown","eb8e8690":"markdown","0fc09227":"markdown","51a171b5":"markdown","66cd76cb":"markdown","cdbaf839":"markdown","65e7f8be":"markdown"},"source":{"51181045":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f64e5c1":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","3d024306":"train.head(10)","951fca52":"test.head(10)","e33234ea":"train[\"Ticket_first\"] = [i[0] for i in train[\"Ticket\"]]","8b3fbc5f":"train.dtypes","cac6b254":"train.groupby(\"Ticket_first\").mean()[\"Survived\"].plot.bar()","0a96b44e":"test[\"Ticket_first\"] = [i[0] for i in test[\"Ticket\"]]\ntest[\"Ticket_first\"].value_counts()","25aab7bb":"def group_as_per_ticket(ticket):\n    a=''\n    if ticket == '1':\n        a=\"High\"\n    elif ticket == '2':\n        a=\"Med\"\n    elif ticket == '3':\n        a=\"Low\"\n    elif ticket == '4':\n        a=\"Low\"\n    elif ticket == '5':\n        a='Low'\n    elif ticket == '6':\n        a='Low' \n    elif ticket == '7':\n        a='Low'  \n    elif ticket == '8':\n        a='Low'   \n    elif ticket == '9':\n        a='Low' \n    elif ticket == 'A':\n        a='Low'\n    elif ticket == 'C':\n        a='Med'\n    elif ticket == 'F':\n        a='High'\n    elif ticket == 'L':\n        a='Low'\n    elif ticket == 'P':\n        a='High'\n    elif ticket == 'S':\n        a='Med'\n    elif ticket == 'W':\n        a='Low'\n    return a\n\ntrain[\"Ticket_type\"] = train[\"Ticket_first\"].map(group_as_per_ticket)\ntest[\"Ticket_type\"] = test[\"Ticket_first\"].map(group_as_per_ticket)","42604ca3":"train.groupby('Ticket_type').mean()[\"Survived\"]","e8e63c5f":"#print(train.info())\n#print('*'*40)\n#print(test.info())","b6be19d3":"train[train[\"Embarked\"].isnull()]","bbcb77a2":"train.describe()","9cd65938":"train[train[\"Pclass\"]==1].mean()[\"Fare\"]","54609b35":"train[(train[\"Sex\"]=='female')&(train[\"Pclass\"]==1)&(train[\"Survived\"]==1)&(train[\"Fare\"]>80)][\"Embarked\"].value_counts()","f9f54356":"train[(train[\"Sex\"]=='female')&(train[\"Pclass\"]==1)&(train[\"Survived\"]==1)&(train[\"Fare\"]==80)]","1edcd35a":"#train.info()","be5233d1":"train = train.drop(index=[61,829])","288be8c3":"#train.info()","e3937663":"train[train[\"Age\"].isnull()][\"Survived\"].value_counts()","77a0ca19":"train.corr()[\"Age\"]","e7768852":"train.plot.hexbin(x=\"Age\",y=\"SibSp\",gridsize=15)","1a6d9278":"new_data_train = train.copy()\nnew_data_test = test.copy()\n\n# make new columns indicating what will be imputed\ncols_with_missing_train = (col for col in new_data_train.columns if new_data_train[col].isnull().any())\ncols_with_missing_test = (col for col in new_data_test.columns if new_data_test[col].isnull().any())\n\nfor col in cols_with_missing_train:\n    new_data_train[col + '_was_missing'] = new_data_train[col].isnull()\n\nfor col in cols_with_missing_test:\n    new_data_test[col + '_was_missing'] = new_data_test[col].isnull()\n    \ntrain[\"Age_was_missing\"] = new_data_train[\"Age_was_missing\"]\ntest[\"Age_was_missing\"] = new_data_test[\"Age_was_missing\"]","6f609509":"train.info()","0efa5f61":"#print(train.info())\n#print('*'*40)\n#print(test.info())","7bf18373":"test[test[\"Fare\"].isnull()]","e5806e09":"test[(test[\"Pclass\"]==3) & (test[\"Sex\"]=='male') & (test[\"Embarked\"]=='S')].mean()[\"Fare\"]","9d0d61a4":"values = {'Fare':12.71887}\ntest[test[\"Fare\"].isnull()] = test[test[\"Fare\"].isnull()].fillna(value=values)","e99a912b":"#print(train.info())\n#print('*'*40)\n#print(test.info())","3d4edd96":"train[\"Cabin\"] = train[\"Cabin\"].fillna(\"N\")\ntest[\"Cabin\"] = test[\"Cabin\"].fillna(\"N\")","ff2f7fe0":"train.Cabin = [i[0] for i in train.Cabin]\ntest.Cabin = [i[0] for i in test.Cabin]","644b5c98":"train[\"Cabin\"].value_counts()","1c156578":"train.groupby(\"Cabin\").mean()[\"Survived\"]","da5338bb":"def group_as_per_cabin(cabin):\n    a=''\n    if cabin == 'B':\n        a=\"High\"\n    elif cabin == 'D':\n        a=\"High\"\n    elif cabin == 'E':\n        a=\"High\"\n    elif cabin == 'A':\n        a=\"Medium\"\n    elif cabin == 'C':\n        a='Med'\n    elif cabin == 'F':\n        a='Med' \n    elif cabin == 'G':\n        a='Med'  \n    elif cabin == 'N':\n        a='Low'   \n    elif cabin == 'T':\n        a='Low'\n    else:\n        a='New'\n    return a\n\ntrain[\"Cabin\"] = train[\"Cabin\"].map(group_as_per_cabin)\ntest[\"Cabin\"] = test[\"Cabin\"].map(group_as_per_cabin)","c16e3c27":"train.groupby(\"Cabin\").mean()[\"Survived\"]","2aaa9bb1":"train = pd.get_dummies(train,columns=[\"Cabin\",\"Embarked\",\"Age_was_missing\",\"Sex\",\"Ticket_type\",\"Pclass\"])\ntest = pd.get_dummies(test,columns=[\"Cabin\",\"Embarked\",\"Age_was_missing\",\"Sex\",\"Ticket_type\",\"Pclass\"])","c6367034":"train = train.drop(columns=['Ticket','Ticket_first'])\ntest = test.drop(columns=['Ticket','Ticket_first'])","acfcc4c6":"train['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1","4a665cd8":"def accompanied(size):\n    a = ''\n    if size < 2:\n        a=\"alone\"\n    elif size < 3:\n        a=\"couple\"\n    elif size < 4:\n        a=\"small_family\"\n    elif size < 5:\n        a=\"family\"\n    elif size < 6:\n        a=\"large_family\"\n    elif size < 7:\n        a=\"extended_family\"\n    elif size < 8:\n        a=\"joint_family\"\n    else:\n        a=\"all\"\n    return a\n    \ntrain[\"accompanied\"] = train[\"family_size\"].map(accompanied)\ntest[\"accompanied\"] = test[\"family_size\"].map(accompanied)","e1ea501c":"train = pd.get_dummies(train,columns=[\"accompanied\"])\ntest = pd.get_dummies(test,columns=[\"accompanied\"])","addd5a46":"TrainData = train.copy()\nTestData = test.copy()","8a2a698e":"TrainData[\"title\"] = [i.split(',')[1].split('.')[0] for i in TrainData.Name]\nTestData[\"title\"] = [i.split(',')[1].split('.')[0] for i in TestData.Name]\nTrainData[\"title\"].value_counts()","1d273459":"TrainData.groupby('title').mean()","0dcbc35b":"TestData[\"title\"].value_counts()","d13bb9b3":"TrainData[\"title\"].dtypes","56223c54":"TrainData[\"title\"] = [i.replace('Ms', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Mlle', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Mme', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Dr', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Col', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Major', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Don', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Jonkheer', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Sir', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Lady', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Capt', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('the Countess', 'rare') for i in TrainData.title]\nTrainData[\"title\"] = [i.replace('Rev', 'rare') for i in TrainData.title]\n\nTestData[\"title\"] = [i.replace('Ms', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Mlle', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Mme', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Dr', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Col', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Major', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Dona', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Jonkheer', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Sir', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Lady', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Capt', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('the Countess', 'rare') for i in TestData.title]\nTestData[\"title\"] = [i.replace('Rev', 'rare') for i in TestData.title]\n\nTrainData[\"title\"].value_counts()\nTestData[\"title\"].value_counts()","0604ea49":"TrainData = pd.get_dummies(TrainData,columns=[\"title\"])\nTestData = pd.get_dummies(TestData,columns=[\"title\"])","cbd9ee4b":"TrainData = TrainData.drop(columns=['PassengerId','Name','Survived'])\nTestData = TestData.drop(columns=['PassengerId','Name'])","43ff9348":"print(TrainData.info())\nprint('*'*40)\nprint(TestData.info())","1777cf59":"#print(TrainData.info())\n#print('*'*40)\n#print(TestData.info())","3830f0df":"age_train = TrainData[TrainData[\"Age\"].notnull()]\nage_impute_TrainData = TrainData[TrainData[\"Age\"].isnull()]\nY_age_train = age_train[\"Age\"]\nX_age_train = age_train.drop(columns=[\"Age\"])\n\nage_test_train = TestData[TestData[\"Age\"].notnull()]\nage_impute_TestData = TestData[TestData[\"Age\"].isnull()]\nY_age_test = age_test_train[\"Age\"]\nX_age_test = age_test_train.drop(columns=[\"Age\"])","5da43efa":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_age_train[[\"Fare\",\"family_size\"]] = sc.fit_transform(X_age_train[[\"Fare\",\"family_size\"]])\nage_impute_TrainData[[\"Fare\",\"family_size\"]] = sc.transform(age_impute_TrainData[[\"Fare\",\"family_size\"]])\n\nX_age_test[[\"Fare\",\"family_size\"]] = sc.fit_transform(X_age_test[[\"Fare\",\"family_size\"]])\nage_impute_TestData[[\"Fare\",\"family_size\"]] = sc.transform(age_impute_TestData[[\"Fare\",\"family_size\"]])","711d1d23":"from sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 100],'gamma':(0.001,'auto')}\nsvr = SVR()\nclf1 = GridSearchCV(svr, parameters,cv=5)\nclf2 = GridSearchCV(svr, parameters,cv=5)\n\nclf1.fit(X_age_train,Y_age_train)\nclf2.fit(X_age_test,Y_age_test)","aaf571dd":"age_impute_TrainData = age_impute_TrainData.drop(columns=[\"Age\"])\nage_impute_TestData = age_impute_TestData.drop(columns=[\"Age\"])","dac61cca":"predicted_train_age = clf1.predict(age_impute_TrainData)\npredicted_test_age = clf2.predict(age_impute_TestData)\nTrainData.loc[TrainData.Age.isnull(), \"Age\"] = predicted_train_age\nTestData.loc[TestData.Age.isnull(), \"Age\"] = predicted_test_age","d5fb4cf9":"print(TrainData.info())\nprint('*'*40)\nprint(TestData.info())","ab45f841":"def age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 5: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\nTrainData['age_group'] = TrainData['Age'].map(age_group_fun)\nTestData['age_group'] = TestData[\"Age\"].map(age_group_fun)","8d3d365e":"TrainData[\"is_child\"] = [True if i<18 else False for i in TrainData[\"Age\"]]\nTestData[\"is_child\"] = [True if i<18 else False for i in TestData[\"Age\"]]","023ed0e1":"#TrainData.info()","ce5493c0":"TrainData = pd.get_dummies(TrainData,columns=[\"age_group\", \"is_child\"])\nTestData = pd.get_dummies(TestData,columns=[\"age_group\", \"is_child\"])","07099c39":"#print(TrainData.info())\n#print('*'*40)\n#print(TestData.info())","f801720f":"train.plot.hexbin(x=\"Fare\",y=\"Survived\",gridsize=15)","325a069c":"train.groupby(\"Survived\")[\"Fare\"].describe()","069883e6":"train['Fare'].describe()","38b986c6":"def fare_groups(fare):\n    a=''\n    if fare <= 125:\n        a=\"low\"\n    elif fare <= 250:\n        a=\"middle\"\n    elif fare <= 375:\n        a=\"upper middle\"\n    elif fare <= 500:\n        a=\"upper\"\n    else:\n        a=\"luxury\"\n    return a\n\nTrainData[\"Fare_status\"] = TrainData[\"Fare\"].map(fare_groups)\nTestData[\"Fare_status\"] = TestData[\"Fare\"].map(fare_groups)","83cd8a1a":"TrainData = pd.get_dummies(TrainData, columns=[\"Fare_status\"])\nTestData = pd.get_dummies(TestData, columns=[\"Fare_status\"])","ec8e85b4":"print(TrainData.info())\nprint('*'*40)\nprint(TestData.info())","70d2c6ba":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nTrainData[[\"Age\",\"Fare\",\"family_size\"]] = sc.fit_transform(TrainData[[\"Age\",\"Fare\",\"family_size\"]])\nTestData[[\"Age\",\"Fare\",\"family_size\"]] = sc.transform(TestData[[\"Age\",\"Fare\",\"family_size\"]])","69c68879":"#print(TrainData.info())\n#print('*'*40)\n#print(TestData.info())","ff5f4a4c":"from sklearn.decomposition import PCA\npca = PCA(n_components = 15)\n\npca.fit_transform(TrainData)\npca.transform(TestData)","18009f7d":"from sklearn.model_selection import train_test_split\nX_train_tot, X_test, y_train_tot, y_test = train_test_split(TrainData, train[\"Survived\"], test_size=0.1, random_state=42)","d3b59559":"X_train, X_val, y_train, y_val = train_test_split(X_train_tot, y_train_tot, test_size=0.2, random_state=42)","a94a52c6":"from sklearn.metrics import accuracy_score","81c0d18f":"##############################  Logistic Regression W\/O GridSearchCV   ###############################\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train,y_train)\nlog_reg.fit(X_train,y_train)\n\nY_test_Pred = log_reg.predict(X_test)\nacc_log_reg = accuracy_score(Y_test_Pred,y_test)\nprint(acc_log_reg)","386d5719":"##############################   Ridge Regression    #####################################\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\nridge.fit(X_train,y_train)\n\nY_test_Pred = ridge.predict(X_test)\nY_test_Pred[Y_test_Pred >= 0.5] = 1\nY_test_Pred[Y_test_Pred < 0.5] = 0\nacc_ridge = accuracy_score(Y_test_Pred,y_test)\nprint(acc_ridge)","13e89add":"###################################  Logistic Regression using GridSearchCV  ######################################\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'solver':('lbfgs','newton-cg','sag','liblinear','saga')}\nlogR = LogisticRegression(random_state = 0,multi_class = 'ovr')\ncross_validation = StratifiedKFold(n_splits=5)\nlogRGCV = GridSearchCV(logR, parameters,cv = cross_validation)\nlogRGCV.fit(X_train,y_train)\n\nY_test_Pred = logRGCV.predict(X_test)\nacc_log_reg_grid = accuracy_score(Y_test_Pred,y_test)\nprint(acc_log_reg_grid)","bc85bb3b":"##################################################### K Neighbours #############################################################\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'n_neighbors':[1,100]}\nneigh = KNeighborsClassifier()\ncross_validation = StratifiedKFold(n_splits=5)\nknn = GridSearchCV(neigh, parameters,cv=cross_validation)\nknn.fit(X_train,y_train)\n\nY_test_Pred = knn.predict(X_test)\nacc_knn = accuracy_score(Y_test_Pred,y_test)\nprint(acc_knn)","9e540bc0":"#################################################### Random Forest ###################################################\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nparameter_grid = {\n             'max_depth' : [4, 6, 8],\n             'n_estimators': [10, 50,100],\n             'max_features': ['sqrt', 'auto', 'log2'],\n             'min_samples_split': [0.001,0.003,0.01],\n             'min_samples_leaf': [1, 3, 10],\n             'bootstrap': [True,False],\n             }\nforest = RandomForestClassifier()\ncross_validation = StratifiedKFold(n_splits=5)\nrdclf = GridSearchCV(forest,scoring='accuracy',param_grid=parameter_grid,cv=cross_validation)\nrdclf.fit(X_train,y_train)\n\nY_test_Pred = rdclf.predict(X_test)\nacc_forest = accuracy_score(Y_test_Pred,y_test)\nprint(acc_forest)","1dc96643":"#################################################### AdaBoost ########################################\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(random_state = 0)\nparameters= {'n_estimators':[10,1000]}\ncross_validation = StratifiedKFold(n_splits=5)\nadaB = GridSearchCV(ada, parameters, cv = cross_validation)\nadaB.fit(X_train,y_train)\n\nY_test_Pred = adaB.predict(X_test)\nacc_adaboost = accuracy_score(Y_test_Pred,y_test)\nprint(acc_adaboost)","a2c2251e":"########################### Decision Tree ######################################\nfrom sklearn.tree import DecisionTreeClassifier\ndct = DecisionTreeClassifier(random_state=0)\nparameter_grid = {\n             'max_depth' : [4, 6, 8],\n             'max_features': ['sqrt', 'auto', 'log2'],\n             'min_samples_split': [0.001,0.003,0.01],\n             'min_samples_leaf': [1, 3, 10],\n             }\ncross_validation = StratifiedKFold(n_splits=5)\ndcT = GridSearchCV(dct,scoring='accuracy',param_grid=parameter_grid,cv=cross_validation)\ndcT.fit(X_train,y_train)\n\nY_test_Pred = dcT.predict(X_test)\nacc_dTree = accuracy_score(Y_test_Pred,y_test)\nprint(acc_dTree)","89c53d4a":"############################ SVC ##############################################\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 100],'gamma':(0.001,'auto')}\nsvc = SVC()\ncross_validation = StratifiedKFold(n_splits=5)\nsvcClf = GridSearchCV(svc, parameters,cv=cross_validation)\nsvcClf.fit(X_train,y_train)\n\nY_test_Pred = svcClf.predict(X_test)\nacc_svc = accuracy_score(Y_test_Pred,y_test)\nprint(acc_svc)","ad1803df":"########################### Gradient Boosting Classifier################################################\nfrom sklearn.ensemble import GradientBoostingClassifier\ngdb = GradientBoostingClassifier()\nclf= GradientBoostingClassifier()\nparameter_grid = {\n             'max_depth' : [4, 6, 8],\n             'n_estimators': [10, 50,100],\n             'max_features': ['sqrt', 'auto', 'log2'],\n             'min_samples_split': [0.001,0.003,0.01],\n             'min_samples_leaf': [1, 3, 10]\n             }\ncross_validation = StratifiedKFold(n_splits=5)\ngdB = GridSearchCV(gdb,scoring='accuracy',param_grid=parameter_grid,cv=cross_validation)\ngdB.fit(X_train,y_train)\n\nY_test_Pred = gdB.predict(X_test)\nacc_gboost = accuracy_score(Y_test_Pred,y_test)\nprint(acc_gboost)","201bc021":"########################### XG Boost ###########################################\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\n\nY_test_Pred = xgb.predict(X_test)\nacc_xgB = accuracy_score(Y_test_Pred,y_test)\nprint(acc_xgB)","b52e5aea":"########################## Extra Trees Classifier #################################\nfrom sklearn.ensemble import ExtraTreesClassifier\ntrees = ExtraTreesClassifier()\nparameter_grid = {\n             'max_depth' : [4, 6, 8],\n             'n_estimators': [10, 50,100],\n             'max_features': ['sqrt', 'auto', 'log2'],\n             'min_samples_split': [0.001,0.003,0.01],\n             'min_samples_leaf': [1, 3, 10],\n             'bootstrap': [True,False],\n             }\ncross_validation = StratifiedKFold(n_splits=5)\nextT = GridSearchCV(trees,scoring='accuracy',param_grid=parameter_grid,cv=cross_validation)\nextT.fit(X_train,y_train)\n\nY_test_Pred = extT.predict(X_test)\nacc_exTree = accuracy_score(Y_test_Pred,y_test)\nprint(acc_exTree)","bb73aa4b":"########################## Voting Classifier ####################################\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nsvc = SVC()\nlogreg = LogisticRegression()\nrandomforest = RandomForestClassifier()\ngradient = GradientBoostingClassifier()\ndectree = DecisionTreeClassifier()\nknn = KNeighborsClassifier()\nXGBClassifier = XGBClassifier()\nExtraTreesClassifier = ExtraTreesClassifier()\n\nvotClf = VotingClassifier(estimators=[\n   ('logreg',logreg), \n  ('random_forest', randomforest),\n   ('gradient_boosting', gradient),\n  ('decision_tree',dectree),  \n   ('knn',knn),\n   ('XGB Classifier', XGBClassifier),\n   ('ExtraTreesClassifier', ExtraTreesClassifier)], voting='soft')\n\nvotClf.fit(X_train,y_train)\n\nY_test_Pred = votClf.predict(X_test)\nacc_voting = accuracy_score(Y_test_Pred,y_test)\nprint(acc_voting)","9fd4b1e5":"#y_test_pred = clf.predict(X_val)\n#y_test_pred[y_test_pred >= 0.5] = 1\n#y_test_pred[y_test_pred < 0.5] = 0","e546beb1":"#from sklearn.metrics import accuracy_score\n#accuracy_score(y_test_pred,y_val)","49e24433":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Logistic Regression with Grid', 'XGBoost', \n              'Gradient Boosting', 'AdaBoost', 'ExtraTrees Classifier', \n               'Ridge Regression', 'Voting Classifier', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log_reg, \n              acc_forest, acc_log_reg_grid, acc_xgB, \n              acc_gboost, acc_adaboost, acc_exTree, acc_ridge, acc_voting, acc_dTree]})\nmodels.sort_values(by='Score', ascending=False)","30b2de10":"Y_pred = svcClf.predict(TestData)\nY_pred = pd.Series(Y_pred)\nFINAL = pd.concat([test[\"PassengerId\"],Y_pred],axis=1)\nFINAL.to_csv(\"submission.csv\", encoding='utf-8', index=False)","4ab2b1df":"Now, lets figure out how to impute the values of Age","9791c33b":"Handling the missing values in Fare column.","fa043ccf":"**Taking the first letter our of the ticket number to make some sense out of it and grouping them to form a categorical feature**","eecc8162":"**Using the selected model to predict the survival of the passengers in the test set.** \n\nHere, we select the SVC model.","a89b625c":"**Splitting the dataset into Train, Validation and Test Set**","7e4495e4":"We know that children were given priority in the evacuation of the ship at the event of this tragedy. So, it seems logical to have a feature which represents whether the passenger is a child or not. It can be a binary variable, derived from the Age Variable.","cad84542":"We see that the cases where the age is not present, i.e, NaN value, **the probability of surviving the disaster is more than 2 times the probability of not surviving**. So we are going to create a separate column to keep track of whether the age was initially present with us or whether we are using a replaced\/imputed value,  as that might prove to be crucial.","50d1deeb":"**Handling Null values in the \"Embarked\" column.**","a91f3863":"**Now, we can create another feature out of the fare paid by the passengers. The division can be done on the basis of the fare amount.**","ecec4cc8":"**FITTING THE MODEL AND SELECTING THE ONE WITH HIGHEST SCORE ON THE TEST SET CREATED.**","99c07c13":"This difference in the mean fare amount paid by the passengers is a clear indicator that more the fare paid by the passengers, more was the chance of survival.","85e83ff0":"Lets try and extract a bit more meaning out of the feature Name... Logically, it could give us a hint about the social status of a passenger and hence, gives us an important clue about whether saving that person was the priority or not.","dd13d9d3":"So now we try to see if we can find the fare related to a Pclass of 3 for a male and we can fill in the mean.","1b1b01b2":"**FEATURE SCALING**","60691d88":"We try to make sense out of the cabin feature. Those with missing values of cabin are put together into a separate category.","1e458f8c":"Since these 2 rows are quite identical and even after trying all the preliminary methods, we are unable to reach a conclusion about how to fill the missing 'embarked' value for these 2 rows, so, we finally decide to drop these 2 rows as better have less data than have the wrong data. Also, there is no point in treating NaN as different value since such a situation never occurs again the test set.","eb8e8690":"Since we are unable to see any strong direct connection between the age and the other features, we would rather fill the NaN values using the mean of the ages of the other passengers.","0fc09227":"Now lets fill in the age values using a SVM Regressor.","51a171b5":"**DIMENSIONALITY REDUCTION**","66cd76cb":"**Another feature, the number of members in a family, i.e, whether a person is travelling alone or in group.**","cdbaf839":"Since we know that the rescue strategy applied was one of \"Women and Children First\", so, it seems logical to have a column representing whether the passenger was a child or not.","65e7f8be":"Now lets work on filling in the missing age values, i.e, imputation of the age column.\nage_group, is_child"}}