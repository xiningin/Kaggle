{"cell_type":{"4d6a5739":"code","d2023aea":"code","9232937f":"code","3d0020cc":"code","6ceb83e6":"code","4df59e6e":"code","6ef8f83f":"code","c12eaa07":"code","a4c0e37f":"code","ffb5109f":"code","37d4a287":"code","257c45c1":"code","43515fb2":"code","0d88402f":"code","23dac66d":"code","215d7ed9":"markdown","8071707c":"markdown","4758c559":"markdown","7fe84b0a":"markdown","88a0a4c7":"markdown","51f0772b":"markdown","007f8b5c":"markdown","bea1da0e":"markdown","a7c3c175":"markdown","9ffe7092":"markdown","8eac7b79":"markdown","f4c63ce6":"markdown","f9e9b2f5":"markdown","75fc416c":"markdown","e3e66fb3":"markdown"},"source":{"4d6a5739":"import numpy as np\nimport tensorflow as tf\nimport keras \nfrom keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Dropout, Input\nfrom keras.preprocessing.image import img_to_array\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm \nimport os\nimport re","d2023aea":"# to get the files in proper order\ndef sorted_alphanumeric(data):  \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n    return sorted(data,key = alphanum_key)\n\n\n# defining the size of image \nSIZE = 256\n\nimage_path = '..\/input\/cuhk-face-sketch-database-cufs\/photos'\nimg_array = []\n\nsketch_path = '..\/input\/cuhk-face-sketch-database-cufs\/sketches'\nsketch_array = []\n\nimage_file = sorted_alphanumeric(os.listdir(image_path))\nsketch_file = sorted_alphanumeric(os.listdir(sketch_path))\n\n\nfor i in tqdm(image_file):\n    image = cv2.imread(image_path + '\/' + i,1)\n    \n    # as opencv load image in bgr format converting it to rgb\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # resizing images \n    image = cv2.resize(image, (SIZE, SIZE))\n    \n    # normalizing image \n    image = image.astype('float32') \/ 255.0\n    \n    #appending normal normal image    \n    img_array.append(img_to_array(image))\n    # Image Augmentation\n    \n    # horizontal flip \n    img1 = cv2.flip(image,1)\n    img_array.append(img_to_array(img1))\n     #vertical flip \n    img2 = cv2.flip(image,-1)\n    img_array.append(img_to_array(img2))\n     #vertical flip \n    img3 = cv2.flip(image,-1)\n    # horizontal flip\n    img3 = cv2.flip(img3,1)\n    img_array.append(img_to_array(img3))\n    # rotate clockwise \n    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n    img_array.append(img_to_array(img4))\n    # flip rotated image \n    img5 = cv2.flip(img4,1)\n    img_array.append(img_to_array(img5))\n     # rotate anti clockwise \n    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    img_array.append(img_to_array(img6))\n    # flip rotated image \n    img7 = cv2.flip(img6,1)\n    img_array.append(img_to_array(img7))\n  \n    \nfor i in tqdm(sketch_file):\n    image = cv2.imread(sketch_path + '\/' + i,1)\n    \n    # as opencv load image in bgr format converting it to rgb\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # resizing images \n    image = cv2.resize(image, (SIZE, SIZE))\n    \n    # normalizing image \n    image = image.astype('float32') \/ 255.0\n    # appending normal sketch image\n    sketch_array.append(img_to_array(image))\n    \n    #Image Augmentation\n    # horizontal flip \n    img1 = cv2.flip(image,1)\n    sketch_array.append(img_to_array(img1))\n     #vertical flip \n    img2 = cv2.flip(image,-1)\n    sketch_array.append(img_to_array(img2))\n     #vertical flip \n    img3 = cv2.flip(image,-1)\n    # horizontal flip\n    img3 = cv2.flip(img3,1)\n    sketch_array.append(img_to_array(img3))\n    # rotate clockwise \n    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n    sketch_array.append(img_to_array(img4))\n    # flip rotated image \n    img5 = cv2.flip(img4,1)\n    sketch_array.append(img_to_array(img5))\n     # rotate anti clockwise \n    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    sketch_array.append(img_to_array(img6))\n    # flip rotated image \n    img7 = cv2.flip(img6,1)\n    sketch_array.append(img_to_array(img7))\n    \n    \n    \n   \n\n\n\n    ","9232937f":"print(\"Total number of sketch images:\",len(sketch_array))\nprint(\"Total number of images:\",len(img_array))","3d0020cc":"# defining function to plot images pair\ndef plot_images(image, sketches):\n    plt.figure(figsize=(7,7))\n    plt.subplot(1,2,1)\n    plt.title('Image', color = 'green', fontsize = 20)\n    plt.imshow(image)\n    plt.subplot(1,2,2)\n    plt.title('Sketches ', color = 'black', fontsize = 20)\n    plt.imshow(sketches)\n   \n    plt.show()","6ceb83e6":"ls = [i for i in range(0,65,8)]\nfor i in ls:\n    plot_images(img_array[i],sketch_array[i])\n","4df59e6e":"train_sketch_image = sketch_array[:1400]\ntrain_image = img_array[:1400]\ntest_sketch_image = sketch_array[1400:]\ntest_image = img_array[1400:]\n# reshaping\ntrain_sketch_image = np.reshape(train_sketch_image,(len(train_sketch_image),SIZE,SIZE,3))\ntrain_image = np.reshape(train_image, (len(train_image),SIZE,SIZE,3))\nprint('Train color image shape:',train_image.shape)\ntest_sketch_image = np.reshape(test_sketch_image,(len(test_sketch_image),SIZE,SIZE,3))\ntest_image = np.reshape(test_image, (len(test_image),SIZE,SIZE,3))\nprint('Test color image shape',test_image.shape)","6ef8f83f":"def downsample(filters, size, apply_batch_normalization = True):\n    downsample = tf.keras.models.Sequential()\n    downsample.add(keras.layers.Conv2D(filters = filters, kernel_size = size, strides = 2, use_bias = False, kernel_initializer = 'he_normal'))\n    if apply_batch_normalization:\n        downsample.add(keras.layers.BatchNormalization())\n    downsample.add(keras.layers.LeakyReLU())\n    return downsample","c12eaa07":"def upsample(filters, size, apply_dropout = False):\n    upsample = tf.keras.models.Sequential()\n    upsample.add(keras.layers.Conv2DTranspose(filters = filters, kernel_size = size, strides = 2, use_bias = False, kernel_initializer = 'he_normal'))\n    if apply_dropout:\n        upsample.add(tf.keras.layers.Dropout(0.1))\n    upsample.add(tf.keras.layers.LeakyReLU()) \n    return upsample","a4c0e37f":"def model():\n    encoder_input = keras.Input(shape = (SIZE, SIZE, 3))\n    x = downsample(16, 4, False)(encoder_input)\n    x = downsample(32,4)(x)\n    x = downsample(64,4,False)(x)\n    x = downsample(128,4)(x)\n    x = downsample(256,4)(x)\n   \n    encoder_output = downsample(512,4)(x)\n    \n    decoder_input = upsample(512,4,True)(encoder_output)\n    x = upsample(256,4,False)(decoder_input)\n    x = upsample(128,4, True)(x)\n    x = upsample(64,4)(x)\n    x = upsample(32,4)(x)\n    x = upsample(16,4)(x)\n    x = tf.keras.layers.Conv2DTranspose(8,(2,2),strides = (1,1), padding = 'valid')(x)\n    decoder_output = tf.keras.layers.Conv2DTranspose(3,(2,2),strides = (1,1), padding = 'valid')(x)\n    \n  \n    return tf.keras.Model(encoder_input, decoder_output)\n\n        \n    \n    \n","ffb5109f":"# to get summary of model\nmodel = model()\nmodel.summary()","37d4a287":"# ## Model\n# Here I have defined two blocks of networks. Encoder network takes 256 by 256 image and downsample it to 16 by 16 latent vector\n# by passing our image via series of Convolution and Maxpooling layer. This downsampled 16 by 16 latent vector is upsampled by passing \n# through series of Convolution and UpSampling layer. The final decoder output is same as our encoder input. This upsamples output of decoder\n# is compared with our sketches and reconstruction loss is calculated. This loss is minimized by updating weight and bias of network through\n# backpropagation.\n# encoder_input = keras.Input(shape=(SIZE,SIZE, 3), name=\"img\")\n# x = Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu', padding = 'same')(encoder_input)\n# x = MaxPool2D(pool_size = (2,2))(x)\n\n# x = Conv2D(filters = 32,kernel_size = (3,3),strides = (2,2), activation = 'relu', padding = 'valid')(x)\n# x = Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), activation = 'relu', padding = 'same')(x)\n# x = MaxPool2D(pool_size = (2,2))(x)\n\n# x = Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# x = Conv2D(filters = 256 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x) \n# encoder_output = Conv2D(filters = 512 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x) \n# encoder = tf.keras.Model(encoder_input, encoder_output)\n\n# decoder_input = Conv2D(filters = 512 ,kernel_size = (3,3), activation = 'relu', padding = 'same')(encoder_output)\n# x = UpSampling2D(size = (2,2))(decoder_input)\n# x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# x = Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# x = UpSampling2D(size = (2,2) )(x)\n\n# x = Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# x = UpSampling2D(size = (2,2) )(x)\n# x = Conv2D(filters = 32 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# x = UpSampling2D(size = (2,2) )(x)\n \n# x = Conv2D(filters = 16  , kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n# decoder_output = Conv2D(filters = 3, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n\n# # final model\n# model = keras.Model(encoder_input, decoder_output)\n# model.summary()","257c45c1":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n              metrics = ['acc'])\n\nmodel.fit(train_image, train_sketch_image, epochs = 100, verbose = 0)","43515fb2":"prediction_on_test_data = model.evaluate(test_image, test_sketch_image)\nprint(\"Loss: \", prediction_on_test_data[0])\nprint(\"Accuracy: \", np.round(prediction_on_test_data[1] * 100,1))","0d88402f":"def show_images(real,sketch, predicted):\n    plt.figure(figsize = (12,12))\n    plt.subplot(1,3,1)\n    plt.title(\"Image\",fontsize = 15, color = 'Lime')\n    plt.imshow(real)\n    plt.subplot(1,3,2)\n    plt.title(\"sketch\",fontsize = 15, color = 'Blue')\n    plt.imshow(sketch)\n    plt.subplot(1,3,3)\n    plt.title(\"Predicted\",fontsize = 15, color = 'gold')\n    plt.imshow(predicted)","23dac66d":"ls = [i for i in range(0,95,8)]\nfor i in ls:\n    predicted =np.clip(model.predict(test_image[i].reshape(1,SIZE,SIZE,3)),0.0,1.0).reshape(SIZE,SIZE,3)\n    show_images(test_image[i],test_sketch_image[i],predicted)","215d7ed9":"# Introduction \nAutoencoder is special type of deep learning architecture consisting of two networks a) encoder b)decoder. Encoder can be fully connected dense neural network or Convolution neural network. Encoder is used to downsample our original sample image into latent vector by passing image through Convolution layers and maxpool layer.Similary,decoder also can be fully connected neural network or Convolution neural network, decoder is used to upsample the latent vector downsampled by encoder. This upsampled latent vector is compared with the original input and reconstruction loss is calculated. Backpropagation is used to minimized this reconstruction loss. Simple autoencoder can be used for Domain transformation, denoising images, image colorization, anamoly detection etc. Here I am going to train my autoencoder model to generate sketch of the input image.  I don't have enough training images so my model mightnot generate very good sketch of image.","8071707c":"## Objective:\nTo convert image to sketch using autoencoder","4758c559":"## Downsample layer","7fe84b0a":"# Visualizing images\nHere i have plotted all augmented images and its augmented sketches","88a0a4c7":"## Load data\nAt first, I would like to thank the person who created this amazing dataset. This dataset consist of 188 image and their corresponding sketches. As these images aren't enough for training our autoencoder model, I have augmented them using open cv library. After Augmentation I have got around 1500 images, these 1500 images also might not be enough but let's give a try. These images are converted into array and are stored in the list.","51f0772b":"# Plotting our predicted sketch along with real sketch","007f8b5c":"# Model \nHere i have use sequence of downsample layer for encoder and upsample layer for decoder","bea1da0e":"### Compiling and Fitting our model\nHere i have used Adam optimizer and mean_squared_error as loss and have trained model for 100 epochs","a7c3c175":"<img src = 'https:\/\/www.researchgate.net\/profile\/Chitralekha_Bhat\/publication\/317559243\/figure\/fig2\/AS:531269123805186@1503675837486\/Deep-Autoencoder-DAE.png'>","9ffe7092":"\n## Slicing and reshaping\n\nOut of 1504 images I have sliced them to two part. train images consist 1400 images while test images contains 104 images. After slicing image array, I reshaped them so that images can be fed directly into our encoder network\n","8eac7b79":"## Import Necessary Libraries","f4c63ce6":"## Any Suggestion to Improve this model is really appreciated.\n## Feel free to comment\n# Thanks For Your Visit","f9e9b2f5":"<img src = 'https:\/\/www.seoclerk.com\/pics\/254019-1TRhz71407611170.jpg'>","75fc416c":"## Upsample Layer","e3e66fb3":"### Evaluating our model"}}