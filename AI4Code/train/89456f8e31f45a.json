{"cell_type":{"e4e108d7":"code","e601bbcc":"code","e6aced49":"code","d1e9f3fd":"code","7e9e9a8c":"code","5a7610a6":"code","2d8c1707":"code","fd30935e":"code","f4f680f0":"code","e5b39f79":"code","af8d4d5d":"code","742d61b3":"code","e2dd18f5":"code","e4b43002":"code","f8f8ca2b":"code","00efaf8c":"code","88441a9f":"code","322b917d":"code","9059f588":"code","ddfa9df9":"code","90de9e13":"code","b28b376f":"code","b2d1ed70":"code","4f58c76c":"code","d57641d1":"code","06425ca1":"code","922731a6":"code","3e51d23d":"code","9c606a63":"code","20abd895":"markdown","55ae4461":"markdown","f17f9705":"markdown","8ae78d1d":"markdown","450d6298":"markdown","d85d9281":"markdown","c682e3aa":"markdown","aa53de57":"markdown","51b73770":"markdown","8b5469fc":"markdown","e900c05b":"markdown","2abe3987":"markdown","7dc0ea6d":"markdown","0d2aa2e5":"markdown","18ca62d3":"markdown","27f8ae84":"markdown","2690a6d5":"markdown","b030f6af":"markdown","8463a763":"markdown","2d149fe8":"markdown","ea7e3395":"markdown","cb04caf2":"markdown","6a838181":"markdown","521a4b1b":"markdown","0fda3729":"markdown","0c99abaf":"markdown","026c4113":"markdown","9dae6526":"markdown","01e4a8f6":"markdown","6fc64400":"markdown","87516691":"markdown","acc89579":"markdown","ffa89fd5":"markdown"},"source":{"e4e108d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e601bbcc":"data = pd.read_csv(\"\/kaggle\/input\/80-cereals\/cereal.csv\")\ndata.head()","e6aced49":"data.info()","d1e9f3fd":"data.describe()","7e9e9a8c":"data.mfr.value_counts()","5a7610a6":"data.type.value_counts()","2d8c1707":"data.cups.value_counts()","fd30935e":"data.weight.value_counts()","f4f680f0":"index = data.loc[data.mfr == 'A'].index[0]\ndata = data.drop([index])\ndata = data.reset_index()\n\ndata['mfr'].value_counts()","e5b39f79":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit_data = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_set = []\ntest_set = []\nfor train_index, test_index in split_data.split(data, data['mfr']):\n    train_set = data.loc[train_index]\n    test_set = data.loc[test_index]","af8d4d5d":"train_set_copy = train_set.copy()\ntrain_set_copy.head()","742d61b3":"attribute_names = ['calories', 'protein', 'fat', 'carbo', 'sodium', 'fiber', 'sugars', 'potass', 'vitamins']\nnew_attribute_names = []\n\nfor name in attribute_names:\n    new_name = name + '_per_cup'\n    train_set_copy[new_name] = train_set_copy[name] \/ train_set_copy['cups']\n    new_attribute_names.append(new_name)\n    \ntrain_set_copy = train_set_copy.drop(columns=attribute_names)\n\ntrain_set_copy.head()","e2dd18f5":"corr_matrix = train_set_copy.corr()\ncorr_matrix['rating'].sort_values(ascending=False)","e4b43002":"from pandas.plotting import scatter_matrix\n\n# Features that display a strong correlation that we want to confirm\nstrong_features = ['rating', 'fiber_per_cup', 'sugars_per_cup']\nscatter_matrix(train_set_copy[strong_features], figsize=(12,8))\n\n# Features we want to gain more insight into\nother_features = ['rating', 'protein_per_cup', 'potass_per_cup', 'fat_per_cup', 'sodium_per_cup']\nscatter_matrix(train_set_copy[other_features], figsize=(12,8))","f8f8ca2b":"train_set_copy = train_set.drop(columns=['rating']) # drop() makes a copy\ntarget_values = train_set['rating'].copy()","00efaf8c":"train_set_copy = train_set_copy.drop(columns=['shelf', 'weight', 'index', 'name'])","88441a9f":"from sklearn.preprocessing import OneHotEncoder","322b917d":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    \n    attribute_names = ['calories', 'protein', 'fat', 'carbo', 'sodium', 'fiber', 'sugars', 'potass', 'vitamins']\n    \n    def __init__(self, calories=True, protein=True, \n                 fat=True, carbo=True, sodium=True, \n                 fiber=True, potass=True, vitamins=True):\n        self.calories = calories\n        self.protein = protein\n        self.fat = fat\n        self.carbo = carbo\n        self.sodium = sodium\n        self.fiber = fiber\n        self.potass = potass\n        self.vitamins = vitamins\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        for name in attribute_names:\n            new_name = name + '_per_cup'\n            X[new_name] = X[name] \/ X['cups']\n            \n        X = X.drop(columns=attribute_names)\n        \n        return X","9059f588":"train_set_numerical = train_set_copy.drop(columns=['type', 'mfr'])\ntrain_set_categorical = train_set_copy[['type', 'mfr']]","ddfa9df9":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# numerical data pipeline\nnum_pipeline = Pipeline([\n    ('attrib_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])","90de9e13":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(train_set_numerical)\ncat_attribs = list(train_set_categorical)\n\n# combining pipelines for both numerical and categorical\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])","b28b376f":"train_set_copy.drop(columns=['cups'])\n\ntrain_set_prepared = full_pipeline.fit_transform(train_set_copy)","b2d1ed70":"from sklearn.linear_model import LinearRegression\n\nlin_reg_model = LinearRegression()","4f58c76c":"from sklearn.model_selection import cross_val_score\n\ndef cross_validate_model(model):\n    \n    # 10 folds, negative cost function b\/c cross_val_score expects a utility function\n    model_scores = cross_val_score(model, train_set_prepared, target_values,\n                                     scoring=\"neg_mean_squared_error\", cv=10)\n    model_rsme = np.sqrt(-model_scores)\n    \n    return model_rsme\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","d57641d1":"lin_reg_rsme = cross_validate_model(lin_reg_model)    \ndisplay_scores(lin_reg_rsme)","06425ca1":"# extracting some data from the training set to test against: validation set\ntest_data = train_set_copy.iloc[:5] # takes the first five records\ntest_labels = target_values.iloc[:5]\ntest_data_prepared = full_pipeline.transform(test_data)\n\nfrom sklearn.metrics import mean_squared_error\n\ndef validate_model(model):\n    model.fit(train_set_prepared, target_values)\n    print(\"Predictions:\", model.predict(test_data_prepared))\n    print(\"Labels:\", list(test_labels))\n\nvalidate_model(lin_reg_model)","922731a6":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree_model = DecisionTreeRegressor()\n\ndec_tree_rsme = cross_validate_model(dec_tree_model)\ndisplay_scores(dec_tree_rsme)","3e51d23d":"validate_model(dec_tree_model)","9c606a63":"from sklearn.ensemble import RandomForestRegressor\n\nrand_forest_model = RandomForestRegressor()\n\nrand_forest_rsme = cross_validate_model(rand_forest_model)\ndisplay_scores(rand_forest_rsme)","20abd895":"We can use `ColumnTransformer` to combine the different processing pipelines for numerical and categorical data.","55ae4461":"The first scatter matrix confirm our insight about `sugars_per_gram` were right. `fiber_per_gram` also has a slightly clear correlation.\n\nWith the second matrix, there is visually no clear correlation but there appears to be some correlation between `rating` and `protein_per_cup`.","f17f9705":"From this frequency distribution, we can assume that the `type` feature would not be a large factor in determining the `rating`, so we can make this a *hyperparameter* to see if it affects the predictions in any meaningful way.","8ae78d1d":"### Random Forest\n\nWe can use a stronger model such as a RandomForest and see if it produces a better RSME score.","450d6298":"# Select and Train Model","d85d9281":"- We can look at the different data value for `cups` because from first glance, it appears that they all have the same value.","c682e3aa":"# Get the data\n\n## Get the data from the csv file with Pandas.","aa53de57":"We see here that the linear model is still the best model, at least with default hyperparameters. \n\n**Further work can be done by experimenting with other models, tuning more hyperparameters, more data cleanup by removing\/adding features.**","51b73770":"2. Decompose categorical features\n\nThe categorical features include:\n- `mfr`\n- `type`\n\n`OneHotEncoder` can be use to convert these features into numerical data, which we can add to our pipeline.","8b5469fc":"**There are several insights from these correlation values:**\n- The strongest correlation is from `sugars_per_cup` (-0.627593). \n- No correlation from `shelf`, confirming our initial hypothesis.\n- Another strong correlation might include `fiber_per_cup`.\n\nWe can use a *scatter matrix* to visually confirm these insight and gain a clearer picture of the other features.","e900c05b":"## Custom Transformer\n\nTo automate the attribute combinations into a pipeline, a custom transformer can be made:","2abe3987":"## Correlations\n\nWe can use a *correlation matrix* to view the individual correlations and their values.","7dc0ea6d":"# Exploring the Data\n\nNow we need to explore our training set data, i.e. look for any\n- correlation between the target feature and the other features\n- attribute combinations that would yield more valuable data insight","0d2aa2e5":"Before transforming the data, we can drop the `cups` feature because we do not need it anymore.","18ca62d3":"## Creating pipelines\nWe first need to separate the **numerical data** and the **categorical data** because they will be prepared differently.","27f8ae84":"The pipeline for numerical data is:\n1. Adding the new attribute combinations and dropping the old ones.\n2. Feature scaling by standardizing the features.","2690a6d5":"1. Drop attributes that provide no useful information (**feature selection**)\n\n    From our exploration, there are several features we can drop:\n    - `shelf`\n    - `weight`\n    \n    Other features we can drop:\n    - `index` (original index before splitting)\n    - `name`\n\n    We can also drop the other features with weak correlations, but we can also make them hyperparameters when creating our model.","b030f6af":"## Linear Model\n\nTo start, linear regression is the simpliest model we can test on.","8463a763":"## Cleaning the Data","2d149fe8":"## Other Models\n\n### Decision Tree","ea7e3395":"From this simple observation, we see that:\n- There are **no missing values**: each feature has 77 records.\n- **`rating` is positively skewed**, being that the 75% of the data is less than 51 (on the assumption that `rating` is on a 0-100 scale)\n- There are **several features that *might* not be of use**, such as `name`, `shelf`, `weight`, and `cup` but further exploration is needed before removal.\n- From the dataset description, we know that the features are scaled in different units (sugars in grams, sodium in milligrams, etc.) To ensure that each feature is equally contributing to the prediction results, **we will need to do feature scaling**.\n\n## Future Surface Level Inspection\n\n- The categorical features (`manufacturer`, `type`) will be need to be converted to numerical features.\n    - We can view what kind of values these categorical features have:\n","cb04caf2":"# Prepare Data for Machine Learning","6a838181":"# Frame the problem and look at the big picture\n\n## 1. Defining the Objective\n\nThe objective is to accurately predict a cereal's rating based on its other attributes.\n\n## 2. Frame the problem\n\nThe machine learning model is a supervised offline model since the dataset has all the provided instances and all contain a 'rating' label which would be use to train the model. This is a multiple regression model because we are using multiple features to derive a prediction and because we are working with a continuous value.\n\n## 3. Performance measure\n\nThe model will be judged based on the **root mean squared error** metric, which will give us the *average magnitude of the differences between the predictions and the actual values*, giving more weight to outliers\/large errors. Here is the equation: \n$\\sqrt{\\frac{1}{N} \\sum_{(x,y)\\in D}^{} (y-prediction(x))^{2}}$\nwhere $N$ is the number of instances, $D$ is the dataset of labeled examples, $(x,y)$ is the input and output value.\n\n## 4. Assumptions\n\n- The model assumes that there has been no upstream components have manipulated the data, so it is getting raw data from the source dataset.\n- All current numerical features will remain numerical, meaning the model will still use regression.\n\n---","521a4b1b":"### Validate the model with test data from the training set\n\nHere we can see the actual predicted values and their corresponding target values:","0fda3729":"To prepare the data for machine learning, we must revert back to a clean training set and separate the predictor features from the target feature (`rating`):","0c99abaf":"### Using K-fold cross-validation\n\nUsing **K-fold cross-validation**, we can split the training set into subsets called folds. Each fold is selected as a *evaluation (validation) set* and training on the other folds.","026c4113":"## The next step is to sample out a test set.\n\nFrom our observation of the distribution of manufacturers, we will need a *stratified* sample, i.e the sample must have roughly the same distribution as the population. **But before we split, we must remove the record from American Home Food Products because it only has one member and hence you cannot create a stratified sample with that.**","9dae6526":"## View the basic metadata and properties of the data.","01e4a8f6":"As expected from my inspection of the `cups` feature, `weight` also differs between cereals. However we can still drop `weight` from our list of important features considering no other feature depends on it.","6fc64400":"## Attribute Combinations\n\nOur first step in exploring the data is to make the attribute combinations between `cups` and its related features. Such features are\n- calories\n- Macronutrients\n    - protein\n    - fat\n    - carbo\n- Micronutrients\/Other\n    - sodium\n    - fiber\n    - sugars\n    - potass\n    - vitamins","87516691":"We see that there is a substantial amount of cereals manufactured by **Kelloggs** and **General Mills**. We can assume that *the brand of the cereal can play a bit role in determining the `rating` given the brand's popularity and familiarity*. Since we are keeping this feature, this means that *when sampling out a test set, we want to make sure that the set is representative* of this skewed population.","acc89579":"From further inspection, the `cups` feature does differ from each cereal. This reveals that `cups` is in fact an important attribute since the other features are based on the serving size (which is measured in `cups`). This means that we need to do **attribute combinations** to ensure fair and equally weighed data.","ffa89fd5":"From `validate_model()`, it seems like DecisionTree got it spot on! But this is only with a single validation test set of 5 records. From the cross validation, where we used multiple validation test (10 to be precise), DecisionTree actually does worse than LinearRegression."}}