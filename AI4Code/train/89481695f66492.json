{"cell_type":{"dd9c2cca":"code","f171e8ca":"code","afed29e0":"code","5bf5811d":"code","014f7b54":"code","fc2886da":"code","0ed83ada":"code","d118f000":"code","6a70601f":"code","a440e9eb":"code","0f03cee5":"code","f146448e":"code","95be31a5":"code","5c6d1fc6":"code","da4ec3d8":"code","736306b3":"code","f6f76e15":"code","da882d77":"code","45b12fc0":"code","fc4e918f":"code","859316ad":"code","cfde174c":"code","a6474483":"code","5d771dcc":"code","0bb3a362":"code","5f621df1":"code","b30a3ce3":"code","0fdb3b7a":"code","58ac5786":"code","5a3ee7fb":"code","99215905":"code","8ca3a5a1":"code","573fdeb3":"code","9329bab3":"code","be23d3f8":"markdown","543025a0":"markdown","dcb8cbb0":"markdown","eb3fb131":"markdown","06a73f4f":"markdown","22b76b50":"markdown","31be8329":"markdown","afc70e81":"markdown","26080e24":"markdown","a4069522":"markdown","b3eec061":"markdown","42b7a29c":"markdown","3542c6eb":"markdown","9811e7a6":"markdown","beb08b03":"markdown","eda17757":"markdown","ff49c591":"markdown","58b14de9":"markdown","e02464e6":"markdown","d0726dcb":"markdown","d85647c2":"markdown","a3ef238b":"markdown","90781100":"markdown"},"source":{"dd9c2cca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport numpy # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport string\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()\nfrom string import punctuation\nimport re\n\n\nimport gc\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f171e8ca":"train = pd.read_csv(\"..\/input\/train.tsv\",sep=\"\\t\")\ntest = pd.read_csv(\"..\/input\/test.tsv\",sep=\"\\t\")","afed29e0":"sub = pd.read_csv('..\/input\/sampleSubmission.csv', sep=\",\")","5bf5811d":"train.head()","014f7b54":"test.head()","fc2886da":"def clean_review(reviews):\n    reviews_clean = []\n    for i in range(0, len(reviews)):\n        review = str(reviews[i])\n        review = re.sub('[^a-zA-Z]', ' ', review) # regular expression\n        review = [\n            lemma.lemmatize(w) \n            for w in word_tokenize(str(review).lower())\n        ]\n        review=' '.join(review)\n        reviews_clean.append(review)\n        \n    return reviews_clean","0ed83ada":"train['CleanedPhrase'] = clean_review(train.Phrase.values)\ntrain.head()","d118f000":"test['CleanedPhrase'] = clean_review(test.Phrase.values)\ntest.head()","6a70601f":"train['WordCount'] = train['CleanedPhrase'].apply(lambda x: len(TextBlob(x).words))\ntest['WordCount'] = test['CleanedPhrase'].apply(lambda x: len(TextBlob(x).words))","a440e9eb":"train = train[train['WordCount'] >= 1]\ntrain = train.reset_index(drop = True)","0f03cee5":"train.head()","f146448e":"test.head()","95be31a5":"sentiment_overview = train.groupby('Sentiment')['WordCount'].describe().reset_index()\nsentiment_overview","5c6d1fc6":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport matplotlib as plt\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\nimport cufflinks as cf\ncf.go_offline()","da4ec3d8":"min_length = go.Bar(\n    x = sentiment_overview['Sentiment'],\n    y = sentiment_overview['min'],\n    name = 'Min zinslength'\n)\n\naverage_length = go.Bar(\n    x = sentiment_overview['Sentiment'],\n    y = sentiment_overview['mean'],\n    name = 'Average Sentence length'\n)\n\nmax_length = go.Bar(\n    x = sentiment_overview['Sentiment'],\n    y = sentiment_overview['max'],\n    name = 'Max Sentence length'\n)\n\ndata = [min_length, average_length, max_length]\nlayout = go.Layout(\n    barmode = 'group',\n    title = 'Lengte van de zin per sentiment'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='grouped-bar')","736306b3":"from sklearn.cluster import KMeans\nimport numpy as np\n\nx = np.array(train['WordCount'])\nkm = KMeans(n_clusters = 4)\nkm.fit(x.reshape(-1,1))  \ntrain['cluster'] = list(km.labels_)","f6f76e15":"y = np.array(test['WordCount'])\nkm = KMeans(n_clusters = 4)\nkm.fit(y.reshape(-1,1))  \ntest['cluster'] = list(km.labels_)","da882d77":"cluster = train.groupby(['Sentiment','cluster'])['WordCount'].describe().reset_index()\ncluster","45b12fc0":"train.groupby(['Sentiment','cluster'])['WordCount'].count().unstack().plot(kind='bar', stacked=False)\ntrain.groupby(['Sentiment','cluster'])['WordCount'].mean().unstack().plot(kind='bar', stacked=False)\ntrain.groupby(['Sentiment','cluster'])['WordCount'].min().unstack().plot(kind='bar', stacked=False)\ntrain.groupby(['Sentiment','cluster'])['WordCount'].max().unstack().plot(kind='bar', stacked=False)","fc4e918f":"gc.collect()","859316ad":"train_text = train.filter(['CleanedPhrase','cluster'])\ntest_text = test.filter(['CleanedPhrase','cluster'])\ntarget = train.Sentiment.values\ny = to_categorical(target)\n\nprint(train_text.shape,target.shape,y.shape)","cfde174c":"X_train_text, X_val_text, y_train, y_val = train_test_split(train_text, y, test_size = 0.2, stratify = y, random_state = 123) # split train + validation\n\nprint(X_train_text.shape, y_train.shape)\nprint(X_val_text.shape, y_val.shape)","a6474483":"all_words = ' '.join(X_train_text.CleanedPhrase.values)\nall_words = word_tokenize(all_words)\ndist = FreqDist(all_words)\nunique_word_count = len(dist)\nunique_word_count","5d771dcc":"review_length = []\nfor text in X_train_text.CleanedPhrase.values:\n    word = word_tokenize(text)\n    l = len(word)\n    review_length.append(l)\n    \nmax_review_length = np.max(review_length)\nmax_review_length","0bb3a362":"max_features = unique_word_count\nmax_words = max_review_length\nbatch_size = 128\nepochs = 3\nnum_classes=5","5f621df1":"tokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(list(X_train_text.CleanedPhrase.values))\nX_train = tokenizer.texts_to_sequences(X_train_text.CleanedPhrase.values)\nX_val = tokenizer.texts_to_sequences(X_val_text.CleanedPhrase.values)\nX_test = tokenizer.texts_to_sequences(test.CleanedPhrase.values)","b30a3ce3":"X_train = sequence.pad_sequences(X_train, maxlen = max_words)\nX_val = sequence.pad_sequences(X_val, maxlen = max_words)\nX_test = sequence.pad_sequences(X_test, maxlen = max_words)\nprint(X_train.shape, X_val.shape, X_test.shape)","0fdb3b7a":"X_train = numpy.insert(X_train, 48, numpy.array([X_train_text.cluster.values]), axis = 1)\nX_val = numpy.insert(X_val, 48, numpy.array([X_val_text.cluster.values]), axis = 1)\nX_test = numpy.insert(X_test, 48, numpy.array([test.cluster.values]), axis=1)\nprint(X_train.shape, X_val.shape, X_test.shape)","58ac5786":"gc.collect()","5a3ee7fb":"model = Sequential()\nmodel.add(Embedding(max_features, 100, mask_zero = True))\nmodel.add(LSTM(64, dropout = 0.4, recurrent_dropout = 0.4, return_sequences = True))\nmodel.add(LSTM(32, dropout = 0.5, recurrent_dropout = 0.5, return_sequences = False))\nmodel.add(Dense(num_classes, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = 0.001), metrics = ['accuracy'])\nmodel.summary()","99215905":"%%time\nhistory = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = epochs, batch_size = batch_size, verbose=1)","8ca3a5a1":"prediction = model.predict_classes(X_test, verbose = 1)\n","573fdeb3":"sub.Sentiment = prediction\nsub.to_csv('sub.csv', index = False)\nsub.head()","9329bab3":"unique, counts = numpy.unique(prediction, return_counts = True)\ndict(zip(unique, counts))","be23d3f8":"### Het trainen van de A.I.\nNu kan de A.I. trainen en zich valideren!","543025a0":"#### Genereer het Prediction bestand","dcb8cbb0":"# Clustering Analyse van elk woord per sentiment","eb3fb131":"## Het voorbereiden van een model voor LSTM en Clustering\n#### Het verdelen van de Train Set tot train set en validation set","06a73f4f":"### Clustering in X_train, X_test en X_val","22b76b50":"### Read de Train en Test Sets","31be8329":"Er zijn veel meer woorden met een \"neutrale (2)\" sentiment: bijna 80.000. Terwijl de \u00e9\u00e9n na hoogste aantal woorden zijn bij een \"lichtelijk positieve (3)\" sentiment, bijna 33.000. Het verschil tussen deze twee is dus best groot.","afc70e81":"#### Tokenize text\n\nTokenizer zet strings om in token objects. Deze tokens kunnen heel makkelijk worden opgeteld of andere handelingen toegepast.","26080e24":"Het bestand is onderaan de notebook te bekijken.","a4069522":"### Aantal per sentiment grafiek\nHet is misschien ook interessant om deze data in een grafiek te doen. \n\nDit kunnen we doen door gebruik te maken van de libraries plotly en matplot.","b3eec061":"In sub komen de uiteindelijke resultaten. Deze komen in de vorm van (phraseId, sentiment).\n","42b7a29c":"### Tel de woorden in CleanedPhrase d.m.v. TextBlob library\n","3542c6eb":"#### Zoek de max. lengte van een review in train set","9811e7a6":"#### Zoek het aantal unieke woorden in train set","beb08b03":"#### Maak een model ervan","eda17757":"# Plan van aanpak\n### Import de data sets\nImport de gegeven data sets\n\n### Visualiseer de data\nDoor het visualiseren van de data kan ik een beeld vormen ervan. Ik heb dan alvast een idee en ik kan een verwachting hebben van het resultaat.\n\n### Cluster de data \nVia deze clusters kan ik een model maken waarop ik LSTM op kan toepassen.\nhttp:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\nhttps:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n\nLSTM maakt het mogelijk om een voorspelling te doen op een grote stuk tekst, zoals een beoordeling van een film. Hiervoor slaat het tijdelijk relevante data op en \"vergeet\" het minder belangrijke data.\n\n### Het maken van het model voor LSTM met Clustering\nVolgens https:\/\/towardsdatascience.com\/how-to-build-a-data-set-for-your-machine-learning-project-5b3b871881ac moet je de train set verdelen tot een train set en een validation set.\nHier moet ik bepalen welke data relevant is en wat onthouden moet worden door de computer.\n\n### Output\nUiteindelijk moet de prediction in de sampleSubmission file komen te staan, die gegeven is door Kaggle.","ff49c591":" ### Clean de zinnen d.m.v. leestekens en trefwoorden (lemma)","58b14de9":"Weer is te zien dat sentiment 2 een hoog aantal heeft. Een groot deel van de woorden zijn dus van neutrale sentiment.","e02464e6":"#### Sequence padding\n\nSequence padding zodat we de maximum lengte kunnen weten per data set.","d0726dcb":"### Bar graph of min, mean and max sentence lenght of each sentiment wise\nBar graph represent all sentence length are eqully distribute for each sentiment wise","d85647c2":"### Een overzicht van de hoeveelheid woorden per sentiment","a3ef238b":"Project Minor A.I.\nHogeschool Rotterdam\nby Rogier Mangoensentono\n\n\n# Movie Review Sentiment Analysis via LSTM + Clustering\n\n## Introductie\n\n**@Tony Busker: Zie hier de bijbehorende Word document indien u deze niet in mijn e-mail hebt gezien: https:\/\/rogierr2.stackstorage.com\/s\/rngyQ5ADf3TOtGD**\n\n### Beschrijving\nDe opdracht is om een A.I. te maken die de aard van een film beoordeling ziet. Dit doe ik d.m.v. de algoritmes **LSTM (Long Short-Term Memory)** en **Clustering**.\n\nHieronder staan de vijf verschillende sentimenten:\n\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive\n\nDit is mijn allereerste keer dat ik zo'n data analyse ga doen. Via Kaggle kan ik ge\u00fcploade datasets gebruiken en meteen online ermee aan de slag gaan, wat het werk een stuk toegankelijker en makkelijker maakt. ","90781100":"De A.I. kan dus al de verschillende sentimenten zien."}}