{"cell_type":{"c237e64d":"code","94e7482e":"code","224a2ee4":"code","a9671c09":"code","c4a989fc":"code","735d3700":"code","dd6a56d6":"code","d1411e12":"code","b1b20f7e":"code","742bb2a1":"code","66d8fc36":"code","042c8573":"code","b4a968cf":"code","eb3a166e":"code","3b05a4af":"code","dd03861b":"code","347f0ceb":"code","4958fcdd":"code","6d1fa684":"code","89754c5f":"code","22dc162e":"code","d1619f3c":"code","d72aaf5d":"code","92133dcb":"code","03f7fd7f":"code","7e46fb23":"code","a5367e8e":"code","d95723f8":"code","2a343553":"code","fc632423":"code","4f97567b":"code","ea7d7e3b":"code","de034432":"code","d5bb001b":"code","b1652c2b":"code","59e1495c":"code","61fbe00d":"code","581f8845":"code","c3f5d2a5":"markdown","117db88f":"markdown","94129743":"markdown","02a4aaf4":"markdown","71a04ae2":"markdown","50402d31":"markdown","b8fe0260":"markdown","f6311cbf":"markdown","74a2c572":"markdown","6decb2b3":"markdown","88804204":"markdown","9ba10fe7":"markdown","eacb1fbb":"markdown","b48de020":"markdown","d6f2fff7":"markdown","ae4e2206":"markdown","060574eb":"markdown","9b9c284d":"markdown","b936a1e4":"markdown","e3af271f":"markdown","a7e0986b":"markdown","39a703a9":"markdown","9ec4732e":"markdown","274f0d65":"markdown","2e7052bd":"markdown","03ed3186":"markdown","1886a767":"markdown","ea9e0627":"markdown","96595132":"markdown"},"source":{"c237e64d":"# List of extra libraries used in the project that requires installation\n\n# 1. Beautiful Soup\n!pip install bs4","94e7482e":"import torch\n\nimport numpy as np \nimport pandas as pd \nimport re\nfrom collections import Counter\n\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","224a2ee4":"df = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\n\ndf.shape","a9671c09":"df['review_len'] = df['review'].str.len()\ndf['review_num_words'] = df['review'].str.split().str.len()\ndf","c4a989fc":"df['review'][0]","735d3700":"df['review'][1]","dd6a56d6":"df['review'][2]","d1411e12":"#df = df.iloc[0:1000,:]\n#df.shape","b1b20f7e":"def pre_process_text(text):\n    \n    # Remove HTML tags\n    soup = BeautifulSoup(text, \"html.parser\")\n    text = soup.get_text()\n    \n    # Convert to lower case\n    text = text.lower()\n    \n    # Remove punctuations\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text","742bb2a1":"df['review'] = df['review'].apply(pre_process_text)","66d8fc36":"df['review'][1]","042c8573":"df['review_len'] = df['review'].str.len()\ndf['review_num_words'] = df['review'].str.split().str.len()\ndf","b4a968cf":"fig, ax = plt.subplots(figsize = (14,6))\nplt.hist(df['review_len'], bins=40 )\nplt.show()","eb3a166e":"df['review_len'].describe(percentiles = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100])\/100)","3b05a4af":"fig, ax = plt.subplots(figsize = (14,6))\nplt.hist(df['review_num_words'], bins=40 )\nplt.show()","dd03861b":"# Create List of reviews\nreviews_list = list(df['review'])\nlen(reviews_list)\n\n# From list of reviews, we need to get to list of words\nall_words = ' '.join(reviews_list)\nlist_words = all_words.split()\n\n# Count all the words using Counter Method\ncount_words = Counter(list_words)\ntotal_words = len(list_words)\nsorted_words = count_words.most_common(total_words)\n\n# Create vocab_to_in dictionary\nvocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}\n\n# tokenize reviews\nreviews_int = []\nfor review in reviews_list:\n    r = [vocab_to_int[w] for w in review.split()]\n    reviews_int.append(r)\n    \n# Encode Labels\nlabels_list = list(df['sentiment'])\nlabels_int = [1 if label =='positive' else 0 for label in labels_list]\nlabels_int = np.array(labels_int)","347f0ceb":"print(total_words)","4958fcdd":"sorted_words[0:10]","6d1fa684":"dict(list(vocab_to_int.items())[0:10])","89754c5f":"print (reviews_int[0:3])","22dc162e":"print(labels_int[0:3])","d1619f3c":"def pad_features(reviews_int, seq_length):\n    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n    \n    for i, review in enumerate(reviews_int):\n        review_len = len(review)\n        \n        if review_len <= seq_length:\n            zeroes = list(np.zeros(seq_length-review_len))\n            new = zeroes+review\n        elif review_len > seq_length:\n            new = review[0:seq_length]\n        \n        features[i,:] = np.array(new)\n    \n    return features","d72aaf5d":"seq_length = 2500\nfeatures = pad_features(reviews_int, seq_length)","92133dcb":"features[0:3]","03f7fd7f":"len_feat = len(features)\nsplit_frac = 0.8\ntrain_x = features[0:int(split_frac*len_feat)]\ntrain_y = labels_int[0:int(split_frac*len_feat)]\n\nremaining_x = features[int(split_frac*len_feat):]\nremaining_y = labels_int[int(split_frac*len_feat):]\n\nvalid_x = remaining_x[0:int(len(remaining_x)*0.5)]\nvalid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n\ntest_x = remaining_x[int(len(remaining_x)*0.5):]\ntest_y = remaining_y[int(len(remaining_y)*0.5):]","7e46fb23":"print(f'Shape of Train X, {len(train_x)}, Shape of Train y, {len(train_y)}')\nprint(f'Shape of Train X, {len(valid_x)}, Shape of Train y, {len(valid_y)}')\nprint(f'Shape of Train X, {len(test_x)}, Shape of Train y, {len(test_y)}')","a5367e8e":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# dataloaders\nbatch_size = 50\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","d95723f8":"# Obtain one batch of training data\ndataiter = iter(train_loader)\nx, y = dataiter.next()\nprint('Sample input size: ', x.size()) # batch_size, seq_length\nprint('Sample input: \\n', x)\n","2a343553":"print('Sample label size: ', y.size()) # batch_size\nprint('Sample label: \\n', y)","fc632423":"from torch import nn\n\nvocab_size = len(all_words)\nembedding_dim = 30\n\nembeds = nn.Embedding(vocab_size, embedding_dim)\nprint ('Shape of Embedding layer is ', embeds)\n","4f97567b":"print ('Embedding layer weights ', embeds.weight.shape)","ea7d7e3b":"embeds_out = embeds(x)\n\nprint ('Embedding layer output shape', embeds_out.shape)\nprint ('Embedding layer output ', embeds_out)","de034432":"# initializing the hidden state to 0\nhidden=None\nhidden_units = 512\n\nlstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_units, num_layers=1, batch_first=True)\nlstm_out, h = lstm(embeds_out, hidden)\nprint ('LSTM layer output shape', lstm_out.shape)\nprint ('LSTM layer output ', lstm_out)","d5bb001b":"fc = nn.Linear(in_features=hidden_units, out_features=1)\n\nfc_out = fc(lstm_out.contiguous().view(-1, hidden_units))\n\nprint ('FC layer output shape', fc_out.shape)\nprint ('FC layer output ', fc_out)","b1652c2b":"50*2500","59e1495c":"sigm = nn.Sigmoid()\nsigm_out = sigm(fc_out)\nprint ('Sigmoid layer output shape', sigm_out.shape)\nprint ('Sigmoid layer output ', sigm_out)","61fbe00d":"out = sigm_out.view(batch_size, -1)\nprint ('Output layer output shape', out.shape)\nprint ('Output layer output ', out)","581f8845":"final_output = out[:,-1]\nprint ('Final Output Shape , ', final_output.shape)\nprint ('Final sentiment prediction, ', final_output)","c3f5d2a5":"## We will set up the pipeline for only 1000 reviews. \n\nTip: Generally a good idea to work with smaller dataset as the code runs faster and it is easier to debug. However, some of the issues may only surface when you use the complete data so you have to mindfull.","117db88f":"# After tokenization this is how our data looks like\n![image.png](attachment:0f30d1f3-6e0f-42c3-afa1-c976bcfa17a0.png)","94129743":"![image.png](attachment:46363838-8f2f-40fa-a70a-ce1b5cecf030.png)\n","02a4aaf4":"# Visualize one batch of data ","71a04ae2":"# Define the sequence length\n\n### This sequence length is same as number of time steps for LSTM layer.\n\n* Too long reviews --> Truncate\n* Too short reviews --> Delete\n* Remaining reviews --> Padding","50402d31":"![image.png](attachment:1f795684-7084-46c1-8c91-dfb23ec10e99.png)","b8fe0260":"![image.png](attachment:47ab4e66-68b5-4ab9-ad4a-8e90f504c514.png)","f6311cbf":"# Training \/ Validation \/ Test Split\n\ntrain= 80% | valid = 10% | test = 10%\n","74a2c572":"# Sigmoid Activation Layer","6decb2b3":"# Now Data looks like this\n![image.png](attachment:789ad25d-73f6-4ff0-81a5-16778a3d2c9d.png)","88804204":"# Final Output :\n### This includes 2 steps\n\n### Step 1) Reshape the output so that rows = batch_size","9ba10fe7":"### Analyze Number of words in reviews","eacb1fbb":"## Word Embeddings:\n\n* In NLP, your features are words. But how should you represent a word in a computer?\n    - ASCI Value (This tells you what word is; doesn't tell you anything about what word means)\n    - One Hot Vector (Massive dimensional space)\n* Fundamental linguistic assumption: that words appearing in similar contexts are related to each other semantically.\n* How could we actually encode semantic similarity in words?\n    - There can be thousands of semantic attributes that might be relevant. How on earth can you set the value of these attributes?\n\n### Here comes the Central Idea of Deep Learning\n* You let Neural Networks learn these representations of the feature i.e. let word embeddings be parameters in the model and then be updated during the training.\n\n\nReading reference:\n1. [PyTorch Documentation](https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html)\n1. [What the heck is word embedding?](https:\/\/towardsdatascience.com\/what-the-heck-is-word-embedding-b30f67f01c81)","b48de020":"## Look at few review samples to understand what kind of data is present","d6f2fff7":"# LSTM based Network Architecture for Binary Classification\n(IMDB Movie Dataset)\n\n![image.png](attachment:039fc0be-d353-4c3e-9475-f1e9b2234ae1.png)\n\n### Various Steps Involved are:\n1. Input Data\n1. Tokenization\n1. Sequence Batching and Data Loaders\n1. Embedding Layer\n1. LSTM Layer\n1. Fully Connected (FC) layer\n1. Sigmoid\n1. Final Output","ae4e2206":"![image.png](attachment:bb050196-dcc5-4be3-8ec2-b41131673028.png)","060574eb":"### Analyze Review Lengths","9b9c284d":"# Add Embedding Layer","b936a1e4":"## Tokenize the reviews\n\n### Create word to index mapping dictionary\n\nWe will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library.\n","e3af271f":"![image.png](attachment:3967484c-39fd-4fe0-856f-66f3cb58d67c.png)","a7e0986b":"![image.png](attachment:06607270-93d6-4964-b3a5-b9ad77d7ff4d.png)","39a703a9":"## This is how our data looks like \n\n![image.png](attachment:38c5265a-9a1f-4676-9bfa-18eca3e7007f.png)","9ec4732e":"## We observe that the review text would require some cleaning before we use it further.\n### Some of the cleaning steps required are :\n1. Removing HTML tags\n1. Converting into lower case\n1. Removing special characters","274f0d65":"# Add LSTM Layer","2e7052bd":"# Fully Connected Layer","03ed3186":"![image.png](attachment:3769ed4b-38e0-4b71-912b-14918a5971d0.png)","1886a767":"# Data Loaders and Batching","ea9e0627":"### Step 2) Output from the last timestep","96595132":"![image.png](attachment:e8e64b7b-f958-4815-a1b4-7fabd4a51534.png)"}}