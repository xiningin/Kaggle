{"cell_type":{"02698c54":"code","0f6a7f00":"code","0e47cb1e":"code","45252640":"code","cc993f52":"code","186ed258":"code","699b6258":"code","f48b234a":"code","e1cb668b":"code","270ff22a":"code","de7f52ad":"code","096d003b":"code","5c601cc0":"code","49dbdb60":"code","955609e9":"code","b750a864":"code","9fa692fa":"code","e7435271":"code","485719f2":"code","d7e39991":"code","2b08e7d7":"code","df962f26":"code","170adc80":"code","cddb0b74":"code","9d495aa6":"markdown"},"source":{"02698c54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nyse'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f6a7f00":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\nfrom tqdm import tqdm\n\nprint(os.listdir(\"..\/input\/nyse\"))","0e47cb1e":"df = pd.read_csv(\"..\/input\/nyse\/prices-split-adjusted.csv\", header=0)\ndf.head(200)","45252640":"symbols = list(set(df.symbol))\nlen(symbols)                        #extracting symbols","cc993f52":"symbols[:20]","186ed258":"#taking a particular symbol for evaluating\ngoogle = df[df.symbol == 'GOOGL']\ngoogles = df[df.symbol == 'GOOGL']\ngoogle = google.close.values.astype('float32')\ngoogle = google.reshape(1762, 1)\ngoogle.shape","699b6258":"import plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots","f48b234a":"plt.plot(google)\nplt.show()\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ngoogle = scaler.fit_transform(google)","e1cb668b":"fig = make_subplots(rows=1, cols=2, column_widths=[0.6, 0.4])\n\nfig.add_trace(go.Scatter(x=googles.date, y=googles.open.diff(), name='l1'),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=googles['open'].diff(), name='h1', histnorm='probability density'),\n              row=1, col=2)\nfig.update_layout( height=550, width=1130, title_text=\"Consecutive difference between opening stock price of Google shares\")\n\nfig.update_xaxes(title_text=\"Time\", row=1, col=1);   fig.update_xaxes(title_text=\"Value\", row=1, col=2)\nfig.update_yaxes(title_text=\"Value\", row=1, col=1);   fig.update_yaxes(title_text=\"Prob. Density\", row=1, col=2)\n\nfig.show()","270ff22a":"f, axes= plt.subplots(2,2, figsize=(20,14))\nsns.regplot(x=googles.open, y=googles.close, color=\"g\", ax=axes[0][0])\nsns.regplot(x=googles.open, y=googles.volume, ax=axes[0][1])\nsns.regplot(x=googles.low, y=googles.high, color=\"b\", ax=axes[1][0])\nsns.regplot(x=googles.volume, y=googles.close, color=\"g\", ax=axes[1][1])","de7f52ad":"f, axes= plt.subplots(1,2, figsize=(20,6))\nsns.regplot(x=googles.open.diff(), y=googles.close.diff(), color=\"g\", ax=axes[0])\nsns.regplot(x=googles.low.diff(), y=googles.high.diff(), color=\"b\", ax=axes[1])\nplt.suptitle('Consecutive variation correlations', size=16)","096d003b":"corr= googles.corr()\nplt.figure(figsize=(8,5))\nsns.heatmap(corr, annot=True, cmap=\"Greens_r\",linewidth = 3, linecolor = \"white\")","5c601cc0":"train_size = int(len(google) * 0.80)\ntest_size = len(google) - train_size\ntrain, test = google[0:train_size,:], google[train_size:len(google),:]\nprint(len(train), len(test))","49dbdb60":"train = train.reshape(len(train) , 1)\ntest = test.reshape(len(test) , 1)","955609e9":"print(train.shape , test.shape)","b750a864":"def process_data(data , n_features):\n    dataX, dataY = [], []\n    for i in range(len(data)-n_features-1):\n        a = data[i:(i+n_features), 0]\n        dataX.append(a)\n        dataY.append(data[i + n_features, 0])\n    return np.array(dataX), np.array(dataY)","9fa692fa":"n_features = 2\n\ntrainX, trainY = process_data(train, n_features)\ntestX, testY = process_data(test, n_features)","e7435271":"print(trainX.shape , trainY.shape , testX.shape , testY.shape)","485719f2":"trainX = trainX.reshape(trainX.shape[0] , 1 ,trainX.shape[1])\ntestX = testX.reshape(testX.shape[0] , 1 ,testX.shape[1])","d7e39991":"import numpy\nimport matplotlib.pyplot as plt\nimport pandas\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense , BatchNormalization , Dropout , Activation\nfrom keras.layers import LSTM , GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.optimizers import Adam , SGD , RMSprop","2b08e7d7":"filepath=\"stock_weights.hdf5\"\nfrom keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0.0001, patience=1, verbose=1)\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')","df962f26":"#building the model\n\nmodel = Sequential()\nmodel.add(GRU(256 , input_shape = (1 , n_features) , return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(256))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64 ,  activation = 'relu'))\nmodel.add(Dense(1))\nprint(model.summary())\n# model = Sequential()\n\n# model.add(LSTM(units=1, output_dim=50,return_sequences=True))\n# model.add(Dropout(0.2))\n\n# model.add(LSTM(100,return_sequences=False))\n# model.add(Dropout(0.2))\n\n# model.add(Dense(output_dim=1))\n# model.add(Activation('linear'))\n\n# start = time.time()\n# model.compile(loss='mse', optimizer='rmsprop')\n# print ('compilation time : ', time.time() - start)","170adc80":"model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0005), metrics=['mean_squared_error'])","cddb0b74":"history = model.fit(trainX, trainY, epochs=100 , batch_size = 128 , \n          callbacks = [checkpoint , lr_reduce] , validation_data = (testX,testY))","9d495aa6":"defining callbacks"}}