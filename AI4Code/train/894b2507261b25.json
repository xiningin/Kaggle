{"cell_type":{"e7727ff0":"code","1e04b6a8":"code","7fc78ae4":"code","518e8797":"code","f42ebb17":"code","8bffbb19":"code","3f6eb511":"code","c479507b":"code","f3b0ead4":"code","bfe81b71":"code","c54534e3":"code","29feeaee":"code","88f72e32":"code","d4870dfc":"code","d423a8c2":"code","28f71268":"code","398c4f5a":"code","a707536a":"code","fc867166":"code","128f21f8":"code","15004592":"code","d2c6c689":"code","a740ca36":"code","2eb9c662":"code","ac0aea72":"code","2f7158fd":"code","5209146c":"code","e56ac1b0":"code","6e623d3d":"code","7103dbf0":"code","6bfc64f6":"code","595b57e8":"code","94c33676":"code","96b042ab":"code","42845a71":"code","4962798d":"code","7f1850d6":"code","9cceb419":"code","3c186279":"code","386afeb9":"code","8864dee6":"code","2084f0d9":"code","acace2fc":"code","94a32cac":"code","7d0a8486":"code","5223d389":"code","0f3c2ad4":"code","55082a4f":"code","c13a5101":"code","dabe7265":"code","024f78c4":"code","31416209":"code","732c1a9c":"code","3d4cec1c":"code","74943f92":"code","7b619b13":"code","d465d028":"code","a8df1ba3":"code","7d029e4b":"code","3fa65ee4":"code","69ab9372":"code","5fb8527b":"code","5f954554":"code","59fe1bfc":"code","76f12835":"code","e5c659c2":"code","2039e703":"markdown","6c86d977":"markdown","780e446d":"markdown","8813244c":"markdown","f6a6f74a":"markdown","2948bf02":"markdown","6a00ac98":"markdown","4d423216":"markdown","e07148c1":"markdown","1842287b":"markdown","dde91f37":"markdown","8f52b857":"markdown","c5fa26af":"markdown","b1da531c":"markdown","8bb820c1":"markdown","9b6fe7e4":"markdown","98cd19fb":"markdown","19f836f0":"markdown","0ceef414":"markdown","4e7b963b":"markdown","666817c6":"markdown","aab67056":"markdown","3f34b072":"markdown","4a16c308":"markdown","8ffc1f7c":"markdown","7896d87e":"markdown","893804fb":"markdown","dbd57a56":"markdown","717a0d07":"markdown","d9b0a5a5":"markdown","347ca19e":"markdown","be88d498":"markdown","dfa88067":"markdown","21b530be":"markdown","15f84319":"markdown","18993bcb":"markdown","5c316704":"markdown","46141ba8":"markdown","042757e3":"markdown","48b2068e":"markdown","417beed0":"markdown","0ff7776a":"markdown","3453884a":"markdown","ac45b697":"markdown","827efb71":"markdown","99ad20b3":"markdown","eaefe018":"markdown","8737201c":"markdown","c6133daa":"markdown","524098d2":"markdown","df82297b":"markdown","e5eaf9e6":"markdown","e177dd0a":"markdown","74cd716a":"markdown","578be23b":"markdown","1998fc78":"markdown","f55d222e":"markdown","5d37a232":"markdown","d6d999dc":"markdown","0f6d2160":"markdown","edf6e51c":"markdown","a8091797":"markdown","7c910aa4":"markdown","c5761019":"markdown","c39107a5":"markdown","68e86dea":"markdown","decf78a2":"markdown","e83f81bf":"markdown","db34399b":"markdown","f64b4100":"markdown","eccdb6f9":"markdown","5bacce69":"markdown","b865525b":"markdown","74e83c2b":"markdown","9703cfa0":"markdown","bee30fe9":"markdown","0b9fa1b6":"markdown","9f9c73d9":"markdown","cc07731a":"markdown","917a2cd9":"markdown","b4c9ac00":"markdown","35d48e44":"markdown","339f961b":"markdown","58d4c471":"markdown","5610db43":"markdown","4835d978":"markdown","9e969356":"markdown","0c4a12ef":"markdown","3a02e53f":"markdown","05a3eb2b":"markdown","60af2fb0":"markdown","e83c84c5":"markdown","e28e065f":"markdown","cbf4a7ae":"markdown"},"source":{"e7727ff0":"\"\"\"\n   Author : Kenil Shah\n   Github: Data-Science-Analytics\/Datasets\/UCLA Graduate Admission Prediction\/Graduate_Admission_Prediction.ipynb\n   \n\"\"\"   \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\n\n%matplotlib inline","1e04b6a8":"dataset = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')\ndataset.head() ","7fc78ae4":"dataset.info()","518e8797":"dataset.drop('Serial No.',axis = 1,inplace = True)\ndataset.columns = ['GRE', 'TOEFL', 'University Rating','SOP','LOR','CGPA','Research','Chance of Admit']","f42ebb17":"plt.rcParams['figure.figsize'] = 15,10\ndataset['GRE'].plot(kind = 'kde')\ndataset['GRE'].plot(kind = 'hist',density = True,color = 'g',alpha = 0.25)\nplt.xlabel('GRE Score',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Histogram and Distribution Plot of GRE Score',fontsize = 20)\nplt.text(308, 0.013, 'Average Score %d' %(int(np.mean(dataset['GRE']))), fontsize=15)\nplt.show()","8bffbb19":"dataset[dataset['Chance of Admit'] > 0.75]['GRE'].plot(kind = 'hist',x = 'GRE',color = 'g',alpha = 0.25)\nplt.text(320, 20, 'Average Score %d' %(int(np.mean(dataset[dataset['Chance of Admit'] > 0.75]['GRE']))), fontsize=15)\nplt.xlabel('GRE Score',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Histogram and Distribution Plot of GRE Score with Chances higher than 75%',fontsize = 20)\nplt.show()","3f6eb511":"# add SNS plot here with hue = 'Research'\nplt.rcParams['figure.figsize'] = 10,20\nsns.lmplot(x = 'GRE' , y = 'Chance of Admit',hue = 'Research',data = dataset,fit_reg = False)\nplt.axvline(x = 300,ymin = 0,ymax = 1)\nplt.axvline(x = 320,ymin = 0,ymax = 1,color = 'orange')\nplt.axhline(y = 0.6,color = 'green')\nplt.title('Impact of Research')\nplt.show()","c479507b":"plt.rcParams['figure.figsize'] = 15,10\ndataset['TOEFL'].plot(kind = 'kde')\ndataset['TOEFL'].plot(kind = 'hist',density = True,color = 'r',alpha = 0.25)\nplt.text(100, 0.02, 'Average Score %d' %(int(np.mean(dataset['TOEFL']))), fontsize=15)\nplt.xlabel('TOEFL Score',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Histogram and Distribution Plot of TOEFL Score',fontsize = 20)\nplt.show()","f3b0ead4":"dataset[dataset['Chance of Admit'] > 0.75]['TOEFL'].plot(kind = 'hist',x = 'TOEFL',color = 'r',alpha = 0.25)\nplt.text(110, 15, 'Average Score %d' %(int(np.mean(dataset[dataset['Chance of Admit'] > 0.75]['TOEFL']))), fontsize=15)\nplt.xlabel('TOEFL Score',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Histogram and Distribution Plot of TOEFL Score with chances higher than 75%',fontsize = 20)\nplt.show()","bfe81b71":"# add SNS plot here with hue = 'Research'\nplt.rcParams['figure.figsize'] = 10,20\nsns.lmplot(x = 'TOEFL' , y = 'Chance of Admit',hue = 'Research',data = dataset,fit_reg = False)\nplt.axvline(x = 100,ymin = 0,ymax = 1)\nplt.axvline(x = 110,ymin = 0,ymax = 1,color = 'orange')\nplt.axhline(y = 0.6,color = 'green')\nplt.title('Impact of Research')\nplt.show()","c54534e3":"dataset['University Rating'].unique() # We have 5 different Ratings by the University","29feeaee":"plt.rcParams['figure.figsize'] = 15,5\nsns.swarmplot(x = 'University Rating', y = 'Chance of Admit', hue = 'Research',data = dataset)\nplt.title('Impact of Research')","88f72e32":"dataset[(dataset['University Rating'] >= 4) & (dataset['Research'] == 1)].sort_values(by = ['Chance of Admit']).head(5)","d4870dfc":"plt.rcParams['figure.figsize'] = 10,5\ndataset['CGPA'].plot(kind = 'kde')\ndataset['CGPA'].plot(kind = 'hist',density = True,color = 'y',alpha = 0.25)\nplt.text(7.88, 0.2, 'Average CGPA %d' %(int(np.mean(dataset['CGPA']))), fontsize=15)\nplt.xlabel('CGPA',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Histogram and Distribution Plot of CGPA',fontsize = 20)\nplt.show()","d423a8c2":"dataset[dataset['Chance of Admit'] > 0.75]['CGPA'].plot(kind = 'hist',x = 'CGPA',color = 'y',alpha = 0.25)\nplt.text(8.75, 20, 'Average Score %d' %(int(np.mean(dataset[dataset['Chance of Admit'] > 0.75]['CGPA']))), fontsize=15)\nplt.xlabel('CGPA',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.xlim(8,10)\nplt.title('Histogram and Distribution Plot of CGPA with chances higher than 75%',fontsize = 20)\nplt.show()","28f71268":"# add SNS plot here with hue = 'Research'\nplt.rcParams['figure.figsize'] = 10,20\nsns.lmplot(x = 'CGPA' , y = 'Chance of Admit',hue = 'Research',data = dataset,fit_reg = False)\nplt.axvline(x = 7,ymin = 0,ymax = 1)\nplt.axvline(x = 9,ymin = 0,ymax = 1,color = 'orange')\nplt.axhline(y = 0.6,color = 'green')\nplt.title('Impact of Research')\nplt.show()","398c4f5a":"print('Average SOP :', int(np.mean(dataset['SOP'])))\nprint('Average LOR :', int(np.mean(dataset['LOR'])))\nplt.rcParams['figure.figsize'] = 10,5\n\ndataset['SOP'].plot(kind = 'kde')\ndataset['LOR'].plot(kind = 'kde')\nplt.legend(['SOP','LOR'])\n#dataset['CGPA'].plot(kind = 'hist',density = True,color = 'y',alpha = 0.25)","a707536a":"# Swarmplot for SOP and LOR values with hue Reasearch and y Chance of Admit\nplt.rcParams['figure.figsize'] = 15,5\nsns.swarmplot(x = 'SOP', y = 'Chance of Admit', hue = 'Research',data = dataset)\nplt.title('Impact of Research')\n","fc867166":"plt.rcParams['figure.figsize'] = 15,5\nsns.swarmplot(x = 'LOR', y = 'Chance of Admit', hue = 'Research',data = dataset)\nplt.title('Impact of Research')\n","128f21f8":"sns.pairplot(dataset,vars = ['GRE','TOEFL','University Rating','SOP','LOR','CGPA','Chance of Admit'],\n             kind = 'reg',diag_kind = 'kde',palette=\"husl\")","15004592":"#Dividing it into Independent and Dependent Variables\n\nX = dataset.iloc[:,:-1].values # Independent Variables\nY = dataset.iloc[:,7].values # Dependent Variables","d2c6c689":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set\n","a740ca36":"model_simple = LinearRegression()\nmodel_simple.fit(train_X,train_Y)","2eb9c662":"pred = model_simple.predict(test_X)","ac0aea72":"print('Mean Square Error is: ', mean_squared_error(test_Y,pred))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred))","2f7158fd":"plt.scatter(model_simple.predict(train_X),model_simple.predict(train_X) - train_Y, c = 'b')\nplt.hlines(y = 0,xmin = min(model_simple.predict(train_X)),xmax = max(model_simple.predict(train_X)))","5209146c":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","e56ac1b0":"polynomial = PolynomialFeatures(degree = 2)   # Degree 2\npolynomial_x = polynomial.fit_transform(train_X)\npolynomial.fit(polynomial_x, train_Y)\n\npolynomial_3 = PolynomialFeatures(degree = 3)   # Degree 3\npolynomial_x_3 = polynomial_3.fit_transform(train_X)\npolynomial_3.fit(polynomial_x_3, train_Y)","6e623d3d":"model_poly = LinearRegression()  # Degree 2\nmodel_poly.fit(polynomial_x,train_Y)\n\nmodel_poly_3 = LinearRegression() # Degree 3\nmodel_poly_3.fit(polynomial_x_3,train_Y)","7103dbf0":"pred_2 = model_poly.predict(polynomial.fit_transform(test_X)) # Degree 2\npred_3 = model_poly_3.predict(polynomial_3.fit_transform(test_X)) # Degree 3","6bfc64f6":"print('Mean Square Error for Polynomial degree 2 is: ', mean_squared_error(test_Y,pred_2))\nprint('Model Accuracy Score for Polynomial degree 2 is : ',r2_score(test_Y,pred_2))\nprint('Mean Square Error for Polynomial degree 3 is: ', mean_squared_error(test_Y,pred_3))\nprint('Model Accuracy Score for Polynomial degree 3 is : ',r2_score(test_Y,pred_3))","595b57e8":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred[:49], ':o', label='Predicted',color = 'red',linewidth = 1)\nplt.plot(x, pred_3[:49], ':x', label='Predicted',color = 'blue',linewidth = 1)\nplt.legend();","94c33676":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","96b042ab":"model_svr_rbf = SVR(kernel = 'rbf',C = 10,gamma = 0.01) # Gaussian Kernel\nmodel_svr_rbf.fit(train_X,train_Y)\n\nmodel_svr_linear = SVR(kernel = 'linear', C = 1) #Linear kernel\nmodel_svr_linear.fit(train_X,train_Y)","42845a71":"pred_svr_rbf = model_svr_rbf.predict(test_X)\npred_svr_linear = model_svr_linear.predict(test_X)","4962798d":"print('Mean Square Error for Gaussian(Radial) kernel is: ', mean_squared_error(test_Y,pred_svr_rbf))\nprint('Model Accuracy Score for Gaussian(Radial) kernel is : ',r2_score(test_Y,pred_svr_rbf))\nprint('Mean Square Error for Linear kernel is: ', mean_squared_error(test_Y,pred_svr_linear))\nprint('Model Accuracy Score for Linear kernel is : ',r2_score(test_Y,pred_svr_linear))","7f1850d6":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred_svr_rbf[:49], ':o', label='Predicted',color = 'red',linewidth = 1)\nplt.plot(x, pred_svr_linear[:49], ':x', label='Predicted',color = 'blue',linewidth = 1)\nplt.legend();","9cceb419":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","3c186279":"model_dtree = DecisionTreeRegressor(random_state = 0,max_depth = 5,max_features = 5,min_samples_split = 10)\nmodel_dtree.fit(train_X,train_Y)","386afeb9":"pred_dtree = model_dtree.predict(test_X)","8864dee6":"print('Mean Square Error : ', mean_squared_error(test_Y,pred_dtree))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_dtree))","2084f0d9":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred_dtree[:49], ':o', label='Predicted',color = 'red',linewidth = 2)\nplt.legend();","acace2fc":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","94a32cac":"model_rforest = RandomForestRegressor(n_estimators = 500,random_state = 0,max_depth = 7\n                                      ,max_features = 5,min_samples_split = 10)\nmodel_rforest.fit(train_X,train_Y)","7d0a8486":"pred_rforest = model_rforest.predict(test_X)","5223d389":"print('Mean Square Error : ', mean_squared_error(test_Y,pred_rforest))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_rforest))","0f3c2ad4":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred_rforest[:49], ':o', label='Predicted',color = 'red',linewidth = 2)\nplt.legend();","55082a4f":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","c13a5101":"pca = PCA(n_components = None)\ntrain_X_pca = pca.fit_transform(train_X)\ntest_X_pca = pca.fit(test_X)\nexplained_variance = pca.explained_variance_ratio_\n","dabe7265":"for x in explained_variance:\n    print(round(x,2))","024f78c4":"pca = PCA(n_components = 4)\ntrain_X_pca = pca.fit_transform(train_X)\ntest_X_pca = pca.transform(test_X)\nmodel_simple_pca = LinearRegression()\nmodel_simple_pca.fit(train_X_pca,train_Y)","31416209":"pred_pca = model_simple_pca.predict(test_X_pca)","732c1a9c":"print('Mean Square Error : ', mean_squared_error(test_Y,pred_pca))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_pca))","3d4cec1c":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred_pca[:49], ':o', label='Predicted',color = 'red',linewidth = 2)\nplt.legend();","74943f92":"#Splitting it into train and test dataset\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.2,random_state = 0) \n# Splitting it into 400 train and 100 test data set","7b619b13":"model_knn = KNeighborsRegressor(n_neighbors =10, metric = 'minkowski' , p = 2)\nmodel_knn.fit(train_X,train_Y)","d465d028":"pred_knn = model_knn.predict(test_X)","a8df1ba3":"print('Mean Square Error : ', mean_squared_error(test_Y,pred_knn))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_knn))","7d029e4b":"plt.figure(figsize=(15,5))\nx = np.arange(1,50)\nplt.plot(x,test_Y[:49], '-o', label='Actual',color ='green')\nplt.plot(x, pred_knn[:49], ':o', label='Predicted',color = 'red',linewidth = 2)\nplt.legend();","3fa65ee4":"cv = cross_val_score(estimator = model_rforest,X = X,y = Y,cv = 10)  # 10 parts","69ab9372":"for accuracy in cv:\n    print(accuracy)    ","5fb8527b":"print('Accuracy mean:',cv.mean())\nprint('Accuracy Standard Deviation:',cv.std())","5f954554":"from sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\ntrain_X = minmax.fit_transform(train_X)\ntest_X = minmax.transform(test_X)","59fe1bfc":"model_simple_normalized = LinearRegression()\nmodel_simple_normalized.fit(train_X,train_Y)\npred_normalized = model_simple_normalized.predict(test_X)\nprint('Mean Square Error is: ', mean_squared_error(test_Y,pred_normalized))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_normalized))","76f12835":"model_rforest_normalized = RandomForestRegressor(n_estimators = 500,random_state = 0,max_depth = 7\n                                      ,max_features = 5,min_samples_split = 10)\nmodel_rforest_normalized.fit(train_X,train_Y)\npred_rforest_normalized = model_rforest_normalized.predict(test_X)\nprint('Mean Square Error is: ', mean_squared_error(test_Y,pred_rforest_normalized))\nprint('Model Accuracy Score : ',r2_score(test_Y,pred_rforest_normalized))","e5c659c2":"index = ['Linear','Polynomial_2','Polynomial_3','SupportVectorGuassin','SupportVectorLinear',\n         'DecisionTree','RandomForest','PCR','KNearest','RandomForestNormalized','LinearNormalized']\nmse = [[mean_squared_error(test_Y,pred),r2_score(test_Y,pred)],\n       [mean_squared_error(test_Y,pred_2),r2_score(test_Y,pred_2)],\n       [mean_squared_error(test_Y,pred_3),r2_score(test_Y,pred_3)],\n       [mean_squared_error(test_Y,pred_svr_rbf),r2_score(test_Y,pred_svr_rbf)],\n       [mean_squared_error(test_Y,pred_svr_linear),r2_score(test_Y,pred_svr_linear)],\n       [mean_squared_error(test_Y,pred_dtree),r2_score(test_Y,pred_dtree)],\n       [mean_squared_error(test_Y,pred_rforest),r2_score(test_Y,pred_rforest)],\n       [mean_squared_error(test_Y,pred_pca),r2_score(test_Y,pred_pca)],\n       [mean_squared_error(test_Y,pred_knn),r2_score(test_Y,pred_knn)],\n       [mean_squared_error(test_Y,pred_rforest_normalized),r2_score(test_Y,pred_rforest_normalized)],\n       [mean_squared_error(test_Y,pred_normalized),r2_score(test_Y,pred_normalized)]]\ndata = pd.DataFrame(data = mse,index = index)\ndata.columns = ['MSE','Accuracy']\ndata.sort_values(by = 'Accuracy',ascending = False)\n","2039e703":"The Serial Number just looks like an index to me and we can remove that column as it wont have any affect on our predictive model.","6c86d977":"### **Decision Tree Regression**","780e446d":"#### *Residual vs Fitted Curve*","8813244c":"We will now look at the distribution of Chance of Admit for each value of University Rating","f6a6f74a":"#### *Training the Model*","2948bf02":"Lets first divide the dataset into train and test models.","6a00ac98":"### **Support Vector Regression**","4d423216":"### **Polynomial Regression**","e07148c1":"WE see that the students who applied at the university had on an average 8 CGPA.","1842287b":"### **Multiple Linear Regression**","dde91f37":"#### *Predicting the outcome*","8f52b857":"#### *Predicting the Outcome*","c5fa26af":"#### *Actual and Predicted*","b1da531c":"### **Principal Component Regression**","8bb820c1":"This tells us that there are no missing values in the dataset which is a plus point for us as we do not need to worry about missing values and what to do with it.","9b6fe7e4":"### **Correlation of Variables**","98cd19fb":"## **Regression** ","19f836f0":"#### *Training the Model*","0ceef414":"Now lets look at the distribution plot of each of this variables and the average score.","4e7b963b":"Now lets look at what each column indicates in the Dataset.\nGRE Score : Tells us about the GRE Score of each person.\nTOEFL Score : Tells us about the TOEFL Score of each person.\nUniversity Rating : The Rating which the University has given to the Student\nSOP : The rating which is given to the Statement of Purpose for the Student\nLOR : The rating which is given to the Letter of Recommendation of the Student\nCGPA : Tells us about the CGPA of the student in the previous college\nResearch : Tells us whether the studetn has done research or not\nChance of Admit : Is the probability the student will get into the University considering all the parameters above","666817c6":"#### *Predicting the outcome*","aab67056":"#### *Actual vs Predicted*","3f34b072":"So we finally got a model that predicts better than the Linear Regression used above. And it RandomForest has been the best algorithm till now.","4a16c308":"# **UCLA Admission Predictor**","8ffc1f7c":"####* Checking for Accuracy*","7896d87e":"#### *Splitting the Dataset*","893804fb":"Now lets look at the distribution of CGPA for all the students who had changes of admit  > 0.75. This will give us an idea about the CGPA you need for higher chances","dbd57a56":"#### *Fitting Polynomial to the Dataset*","717a0d07":"#### *Splitting the Dataset*","d9b0a5a5":"Now let us see if doing research has some effect on the chances and the average TOEFL Score than.","347ca19e":"#### *Actual and Predicted*","be88d498":"So to have a chance of 75% or more we need to have a CGPA of 9+. We can say that the University is very CGPA oriented and this could be a big factor in determining the chance of admission in an university.","dfa88067":"#### *Checking for Accuracy*","21b530be":"### ***TOEFL Scores***","15f84319":"The next thing we can do to further improve the model is Normalization of all its columns . We will take the top 2 algorithms i.e Random Forest and Linear Regression for this.","18993bcb":"Since we now got the best model as Random Forest lets try Cross validation and see if we can improve the model further","5c316704":"So this does not perform as good as the Linear Regression model.","46141ba8":"Now let us see if doing research has some effect on the chances and the average CGPA Score than.","042757e3":"As we know this is a linear dataset we expect the linear kernel to give a better result then the gaussia kernel. But by tweaking the parameters of the guassian kernel we are able to get a model that gives a similar result like the linear kernel. That being said we still have the Multiple Linear Regression as the best model till now.","48b2068e":"Now let us see if doing research has some effect on the chances and the average GRE Score than.","417beed0":"#### *Training the Dataset*","0ff7776a":"#### *Checking for Accuracy*","3453884a":"As we can the variance explained by the first two components is enough and the other components can be ignored. Now using this components to create the new linear regression model","ac45b697":"#### *Checking for Accuracy*","827efb71":"So on an average student has a TOEFL score of 107 who applies to this university.","99ad20b3":"We can see that we are further able to increase the accuracy of the model by using the cross validation approach and taking the value of k as 10 meaning dividing the dataset into 10 parts using 9 to train the model and 1 to test the model.","eaefe018":"A possible explanation for this is that someone who has done research will have lots to write in the SOP and hence will have a better rating for that SOP as they would be able to explain alot of things about their research interests work done by them etc.","8737201c":"Now we can see that to have a chance of admit just based on GRE Scores we need a score aorund 325+ to have a chance greter than 75%","c6133daa":"### **K Nearest Neighbor Regressor**","524098d2":"#### *Splitting the Dataset*","df82297b":"As expected we can see that they have a low score on any other column of their profile.","e5eaf9e6":"#### *Checking for different Accuracy*","e177dd0a":"So that is the end of the Regression model and let us now look at how each model has performed.","74cd716a":"We will try to fit the model with less number of independent variables using Principal Components Analysis by setting up a threshold of the variance we want and then using that number ot train our model.","578be23b":"Now we can see that as we increase the degree of the polynomial our mean square error increases and R-squared value decreases. This is because we may be overfitting the data and hence we might be predicting correct values for the training set but not the test set. Since we saw in the Residual vs Fitted value graph for Multiple Linear Regression that Linear assumption for the model is indeed a correct assumption , we would stick to that and assume the model to be linear and hence would perform better with degree 1.","1998fc78":"#### *Predicting the Outcome*","f55d222e":"This is kind of self explanatory that someone who has done research will have higher chance of admit and a better university rating. It also shows how important it is to do research to get a university rating of 4 or 5. And even in university rating 3 we see that if you have done research you have a higher chance. ","5d37a232":"Now lets look at the distribution of GRE Score for all the students who had changes of admit  > 0.75. This will give us an idea about the GRE Score you need for higher chances","d6d999dc":"#### *Actual and Predicted*","0f6d2160":"And as you can see we need a score of 112+ to have a admission chance of around 75%","edf6e51c":"#### *Checking for Accuracy*","a8091797":" The presence of rare cases like research is there but still not a good rating is becuase there is some other factor on their profile which is not that good and hence their chances are reduced.","7c910aa4":"#### *Predicting Outcome*","c5761019":"This shows that almost all the variables has good correlation with each other and a positive relationship to be precise. The all seem to have a positive relation with the Chance of Admit as well. Well enough of exploration now. Lets move to the Prediction part of the dataset","c39107a5":"#### *Splitting the Dataset*","68e86dea":"### ***SOP and LOR***","decf78a2":"#### *Predicting the Outcome*","e83f81bf":"Again showing the same thing, if you have done research and have a lower CGPA you still have a higher chance. Though one thing as mentioned above, the university being CGPA centric, higer CGPA continues to have more affect than Research. Another strange thing is that in the higher regions of GRE,TOEFL and CGPA there were many student who has done research and many few who has not. This shows people would better grades tend to do research as well. But Causation does not mean Correlation.","db34399b":"#### *Checking for Accuracy*","f64b4100":"### ***GRE Scores***","eccdb6f9":"#### *Training the Model*","5bacce69":"#### *Training the model*","b865525b":"#### *Splitting the Dataset*","74e83c2b":"> ### ***University Rating***","9703cfa0":"We can see that the points are randomly distributed around the line y = 0 and hence we cna say that the assumption of a linear fit on the data is actually true and this model can be used.","bee30fe9":"#### *Splitting the Dataset*","0b9fa1b6":"### **Random Forest Regression**","9f9c73d9":"## **Exploratory Data Analysis**","cc07731a":"### **Cross Validation on Random Forest**","917a2cd9":"Now lets look at the distribution of TOEFL Score for all the students who had changes of admit  > 0.75. This will give us an idea about the TOEFL Score you need for higher chances","b4c9ac00":"#### *Finding the Principal Components*","35d48e44":"#### *Training the Dataset*","339f961b":"#### *Predicting Outcome*","58d4c471":"This shows similiar results to GRE score and student will lower TOEFL scores can also increase their chances if they have some research goin on or have done some research.","5610db43":"#### *Checking for Accuracy*","4835d978":"#### *Actual vs Predicted*","9e969356":"A similar explantation to the one given above for SOP, someone who has done research will know many professors who would write a good LOR for him\/her and so will have a higher rating on LOR.Infact it is not strange to see that for Ratings of SOP and LOR above 4 we have more research students and only a few non-research students. Those exceptional ones would have something really interesting in thei SOPs to get such high scores.","0c4a12ef":"#### *Splitting the Dataset*","3a02e53f":"This tells us that if you have score around 300~320 and you do research you can have a chance higher than 60% to get into the university. You can also see for the higher scores in that range and having research they have more chances of an admit. Also if you do research and you have a more than decent score you have given yourself a good chance.","05a3eb2b":"#### *Training the Model*","60af2fb0":"> ### ***CGPA*** ","e83c84c5":"Well CGPA is said to be the most important factor in the decision making of a university and lets see how much impact does it have here.","e28e065f":"SO we see that on an average they have and LOR and SOP value of 3. There is not much to look into this. But we will look at how research value impact the chances for SOP and LOR.","cbf4a7ae":"#### *Predicted and Actual*"}}