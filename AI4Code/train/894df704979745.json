{"cell_type":{"999d6781":"code","9d73773c":"code","0cf419a8":"code","3f29444e":"code","fbce9bfe":"code","cdeaa2d1":"code","0d1cdcc0":"code","2a17839a":"code","b81e2d72":"code","a0149ae2":"code","90f93d38":"code","e92d17d4":"code","fc3413aa":"code","4fe30e93":"code","69dc9d96":"code","fe1c65c2":"code","0cbd3927":"code","7848a011":"code","89b447de":"code","3a3c5e13":"code","6186975c":"code","959cf181":"code","e963d481":"markdown","be54e3d4":"markdown","034c8a4f":"markdown","5bfaa011":"markdown","ce30064d":"markdown","fefbf082":"markdown","84a8314c":"markdown","2be8303c":"markdown","4da3ee21":"markdown","a060f693":"markdown"},"source":{"999d6781":"!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","9d73773c":"import sys\nsys.path.append('\/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle')\n! pip install -e \/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle -q","0cf419a8":"import os\nfrom glob import glob\nimport shutil\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport math","3f29444e":"debug=False\nIMG_SIZES = [[512, 512]]\nDIM=IMG_SIZES[0]\nTTA = 1\ndim = DIM[0]\naspect_ratio = False\nclass_labels = ['0', '1', '2', '3']\n\nsat  = (0.7, 1.3)\ncont = (0.8, 1.2)\nbri  =  0.1\nROT_    = 0.0\nSHR_    = 2.0\nHZOOM_  = 8.0\nWZOOM_  = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0","fbce9bfe":"filepaths = glob('\/kaggle\/input\/siim-covid19-detection\/test\/**\/*dcm',recursive=True)\ntest_df = pd.DataFrame({'filepath':filepaths,})\ntest_df['image_id'] = test_df.filepath.map(lambda x: x.split('\/')[-1].replace('.dcm', '')+'_image')\ntest_df['study_id'] = test_df.filepath.map(lambda x: x.split('\/')[-3].replace('.dcm', '')+'_study')\ntest_df.head()","cdeaa2d1":"os.makedirs('\/kaggle\/working\/test', exist_ok = True)","0d1cdcc0":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# credit @raddar\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_and_save(file_path):\n    split = 'train' if 'train' in file_path else 'test'\n    base_dir = f'\/kaggle\/working\/{split}'\n    img = read_xray(file_path)\n    h, w = img.shape[:2]  # orig hw\n    if aspect_ratio:\n        r = dim \/ max(h, w)  # resize image to img_size\n        interp = cv2.INTER_AREA if r < 1 else cv2.INTER_LINEAR\n        if r != 1:  # always resize down, only resize up if training with augmentation\n            img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=interp)\n    else:\n        img = cv2.resize(img, (dim, dim), cv2.INTER_AREA)\n    filename = file_path.split('\/')[-1].split('.')[0]\n    cv2.imwrite(os.path.join(base_dir, f'{filename}.jpg'), img)\n    return filename.replace('dcm','')+'_image',w, h\n","2a17839a":"filepaths = test_df.filepath.iloc[:100 if debug else test_df.shape[0]]\ninfo = []\nfor filepath in tqdm(filepaths):\n    info.append(resize_and_save(filepath))","b81e2d72":"image_id, width, height = list(zip(*info))\ndf = pd.DataFrame({'image_id':image_id,\n                   'width':width,\n                   'height':height})\ndf['image_path'] = '\/kaggle\/working\/test\/'+df.image_id.map(lambda x: x.replace('_image',''))+'.jpg'\ntest_df = pd.merge(test_df, df, on = 'image_id', how = 'left')\ntest_df.loc[:,class_labels] = 0\ntest_df.head()","a0149ae2":"import tensorflow.keras.backend as K\nimport math\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform(image, DIM=IMG_SIZES[0]):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    \n    # fixed for non-square image thanks to Chris Deotte\n    \n    if DIM[0]!=DIM[1]:\n        pad = (DIM[0]-DIM[1])\/\/2\n        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n        \n    NEW_DIM = DIM[0]\n    \n    XDIM = NEW_DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(NEW_DIM\/\/2, -NEW_DIM\/\/2,-1), NEW_DIM)\n    y   = tf.tile(tf.range(-NEW_DIM\/\/2, NEW_DIM\/\/2), [NEW_DIM])\n    z   = tf.ones([NEW_DIM*NEW_DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -NEW_DIM\/\/2+XDIM+1, NEW_DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([NEW_DIM\/\/2-idx2[0,], NEW_DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n    \n    if DIM[0]!=DIM[1]:\n        image = tf.reshape(d,[NEW_DIM, NEW_DIM,3])\n        image = image[:, pad:DIM[1]+pad,:]\n    image = tf.reshape(image, [*DIM, 3])\n        \n    return image","90f93d38":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n\n\ndef build_decoder(with_labels=True, target_size=(300, 300), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32)\n        img = tf.image.resize(img, target_size, method='area')\n#         img = tf.image.resize(img, target_size)\n        img = img\/255.0\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = transform(img, DIM = DIM)\n        img = tf.image.random_flip_left_right(img)\n#         img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, sat[0], sat[1])\n        img = tf.image.random_contrast(img, cont[0], cont[1])\n        img = tf.image.random_brightness(img, bri)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n    \n    return dset","e92d17d4":"strategy = auto_select_accelerator()\nBATCH_SIZE = strategy.num_replicas_in_sync * 32","fc3413aa":"# IMSIZE = (256, 384, 512, 640, 768, 1024)\n\ntest_paths = test_df.image_path.iloc[:100 if debug else test_df.shape[0]]\n# Get the multi-labels\n# label_cols = sub_df.columns[1:]\ntest_decoder = build_decoder(with_labels=False, target_size=DIM)\ndtest = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=True, \n    shuffle=False, augment=True, cache=False,\n    decode_fn=test_decoder\n)","4fe30e93":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n\ndef build_model(dim=IMG_SIZES[0], ef=0):\n    inp = tf.keras.layers.Input(shape=(*dim,3))\n    base = EFNS[ef](input_shape=(*dim,3),weights='imagenet',include_top=False)\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n    x = tf.keras.layers.Dense(4,activation='softmax')(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n#     opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n#     loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01) \n# #     acc = tf.keras.metrics.CategoricalAccuracy()\n# #     f1  = tfa.metrics.F1Score(num_classes=4,average='macro',threshold=None)\n#     model.compile(optimizer=opt,loss=loss,metrics=['AUC', acc, f1])\n    return model","69dc9d96":"base_dir = '\/kaggle\/input\/siim-covid-19-study-level-train-tpu'\nmodel_paths = sorted(glob(os.path.join(base_dir, '*h5')))# preds = np.zeros((count_data_items(files_test),1))\npreds=[]\nmodel = build_model(dim=IMG_SIZES[0], ef=7)\nfor fold, model_path in enumerate(tqdm(model_paths)):\n    print(f'Fold: {fold+1}')\n    with strategy.scope():\n        print('Loading Model...')\n        #model = tf.keras.models.load_model(model_path, compile=False)\n        model.load_weights(model_path)\n    print('Predicting...')\n    pred = model.predict(dtest, steps = TTA*len(test_paths)\/BATCH_SIZE, verbose=1)[:TTA*len(test_paths),:]\n    pred = np.mean(pred.reshape(TTA, len(test_paths), -1), axis=0)\n    preds.append(pred)\npreds = np.mean(preds, axis=0)","fe1c65c2":"name2label = { \n    'negative': 0,\n    'indeterminate': 1,\n    'atypical': 2,\n    'typical': 3}\nlabel2name  = {v:k for k, v in name2label.items()}","0cbd3927":"test_df.loc[:99 if debug else test_df.shape[0],class_labels] = preds\nstudy_df = test_df.groupby(['study_id'])[class_labels].mean().reset_index()\nstudy_df.rename(columns={'study_id':'id'}, inplace=True)\nstudy_df.head()","7848a011":"def get_PredictionString(row, thr=0):\n    string = ''\n    for idx in range(4):\n        conf =  row[str(idx)]\n        if conf>thr:\n            string+=f'{label2name[idx]} {conf:0.2f} 0 0 1 1 '\n    string = string.strip()\n    return string","89b447de":"study_df['PredictionString'] = study_df.progress_apply(get_PredictionString, axis=1)\nstudy_df = study_df.drop(class_labels, axis=1)\nstudy_df.head()","3a3c5e13":"image_df = pd.DataFrame({'id':test_df.image_id.tolist(),\n                         'PredictionString':[\"none 1 0 0 1 1\"]*len(test_df.image_id.tolist())})\nimage_df.head()","6186975c":"sub_df = pd.concat([study_df, image_df])\nsub_df.to_csv('\/kaggle\/working\/submission.csv',index=False)\nprint(sub_df.shape)\nsub_df.head()","959cf181":"import shutil\nshutil.rmtree('\/kaggle\/working\/test')","e963d481":"# Augmentation\n* credit: [@chris](https:\/\/www.kaggle.com\/cdeotte\/)","be54e3d4":"# Overview:\n* Basic idea was to use **classification** model for **Study-Level** & **detection** model for **Image-Level**,\n\n# Notebooks:\n\n#### Study-Level:\n* **train**: [SIIM-COVID-19: Study-Level [train] TPU\ud83e\ude7a](https:\/\/www.kaggle.com\/awsaf49\/siim-covid-19-study-level-train-tpu\/)\n* **infer**: [SIIM-COVID-19: Study-Level [infer]\ud83e\ude7a](https:\/\/www.kaggle.com\/awsaf49\/siim-covid-19-study-level-infer) [LB: **0.360**]\n* **data**: [SIIM-COVID-19: 512x512 tfrec Data](https:\/\/www.kaggle.com\/awsaf49\/siim-covid-19-512x512-tfrec-data)\n\n#### Image-Level:\n* **train**: [SIIM-COVID-19: YOLOv5 Image-Level [train]](https:\/\/www.kaggle.com\/awsaf49\/siim-covid-19-yolov5-image-level-train)\n* **infer**: [SIIM-COVID-19: YOLOv5 Image-Level [infer]](https:\/\/www.kaggle.com\/awsaf49\/siim-covid-19-yolov5-image-level-infer) **placeholder**, seems someting is wrong with `image-level` data, gives very small score `0.051`.\n\n# Dataset:\n\n#### JPEG\n* [1024x1024](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-1024-jpg-image-dataset)\n* [512x512](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-512-jpg-image-dataset)\n* [256x256](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-256-jpg-image-dataset)\n\n#### TFRECORD\n* [1024x1024](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-1024x1024-tfrec-dataset)\n* [512x512](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-512x512-tfrec-dataset)\n* [256x256](https:\/\/www.kaggle.com\/awsaf49\/siimcovid19-256x256-tfrec-dataset)","034c8a4f":"# DataLoader","5bfaa011":"# Commit or Not","ce30064d":"# Packages","fefbf082":"## Load model and submit","84a8314c":"# Process Prediction","2be8303c":"# [SIIM-FISABIO-RSNA COVID-19 Detection](https:\/\/www.kaggle.com\/c\/siim-covid19-detection)\n> Identify and localize COVID-19 abnormalities on chest radiographs\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26680\/logos\/header.png)","4da3ee21":"# Data Pipeline \n* credit : [@xhlulu](https:\/\/www.kaggle.com\/xhlulu)","a060f693":"# Install **gdcm** & **libjpeg** without internet"}}