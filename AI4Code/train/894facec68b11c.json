{"cell_type":{"f8d0619e":"code","2cba7831":"code","6932afa3":"code","81f83de9":"code","8c5fa9b8":"code","cc2c6b90":"code","08ebb58a":"code","7dd70c2f":"code","7110e960":"code","44b6c586":"code","b9fd99a4":"code","15981b37":"code","2fd9c823":"code","d0e0be6f":"code","74e08650":"code","70c8be7c":"code","963644dd":"code","14aefdd5":"code","af6fc53e":"code","d60161c4":"code","48990e75":"code","0f409981":"code","97aa2cf3":"code","048ca53c":"code","ceafb46a":"markdown","c6c815b9":"markdown","a3439d8f":"markdown","6429e0ba":"markdown","6cc71f8d":"markdown","cbd9b0a5":"markdown","a45c4356":"markdown","6770b688":"markdown","955c0012":"markdown","2b8c6e18":"markdown","852813b4":"markdown","62c20471":"markdown","d0e471e9":"markdown","204ad936":"markdown","6a03dad7":"markdown"},"source":{"f8d0619e":"import pandas as pd\nimport numpy as np\nimport matplotlib as nlp\ntweets=pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin', \n                   names = ['sentiment','id','date','query','user','tweet'])\ntweets","2cba7831":"tweets = tweets.sample(frac=1)\ntweets = tweets[:200000]\nprint(\"Dataset shape:\", tweets.shape)","6932afa3":"tweets['sentiment'].unique()","81f83de9":"tweets['sentiment']=tweets['sentiment'].replace(4,1)\ntweets","8c5fa9b8":"tweets.drop(['date','query','user'], axis=1, inplace=True)\ntweets.drop('id', axis=1, inplace=True)\ntweets.head(10)","cc2c6b90":"(tweets.isnull().sum() \/ len(tweets))*100","08ebb58a":"#converting pandas object to a string type\ntweets['tweet'] = tweets['tweet'].astype('str')","7dd70c2f":" positives = tweets['sentiment'][tweets.sentiment == 1 ]\nnegatives = tweets['sentiment'][tweets.sentiment == 0 ]\n\nprint('Total length of the data is:         {}'.format(tweets.shape[0]))\nprint('No. of positve tagged sentences is:  {}'.format(len(positives)))\nprint('No. of negative tagged sentences is: {}'.format(len(negatives)))","7110e960":"# nltk\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n#Stop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) \n#that a search engine has been programmed to ignore,\n#both when indexing entries for searching and when retrieving them as the result of a search query.\nnltk.download('stopwords')\nstopword = set(stopwords.words('english'))\nprint(stopword)","44b6c586":"import warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport string\nimport pickle\nurlPattern = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\nuserPattern = '@[^\\s]+'\nsome = 'amp,today,tomorrow,going,girl'\ndef process_tweets(tweet):\n  # Lower Casing\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)  \n    \n    tweet = re.sub(r\"some1\", \"someone\", tweet)\n    tweet = re.sub(r\"yrs\", \"years\", tweet)\n    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n    tweet = re.sub(r\"2day\", \"today\", tweet)\n    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n    tweet = re.sub(r\"goood\", \"good\", tweet)\n    tweet = re.sub(r\"some1\", \"someone\", tweet)\n    tweet = re.sub(r\"some1\", \"someone\", tweet)\n    tweet = tweet.lower()\n    tweet=tweet[1:]\n    # Removing all URls \n    tweet = re.sub(urlPattern,'',tweet)\n    # Removing all @username.\n    tweet = re.sub(userPattern,'', tweet) \n    #remove some words\n    tweet= re.sub(some,'',tweet)\n    #Remove punctuations\n    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n    #tokenizing words\n    tokens = word_tokenize(tweet)\n    #tokens = [w for w in tokens if len(w)>2]\n    #Removing Stop Words\n    final_tokens = [w for w in tokens if w not in stopword]\n    #reducing a word to its word stem \n    wordLemm = WordNetLemmatizer()\n    finalwords=[]\n    for w in final_tokens:\n      if len(w)>1:\n        word = wordLemm.lemmatize(w)\n        finalwords.append(word)\n    return ' '.join(finalwords)","b9fd99a4":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", \n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n     \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","15981b37":"def convert_abbrev_in_text(tweet):\n    t=[]\n    words=tweet.split()\n    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n    return ' '.join(t)  ","2fd9c823":"tweets['processed_tweets'] = tweets['tweet'].apply(lambda x: process_tweets(x))\ntweets['processed_tweets'] = tweets['processed_tweets'].apply(lambda x: convert_abbrev_in_text(x))\nprint('Text Preprocessing complete.')\ntweets","d0e0be6f":"#removing shortwords\ntweets['processed_tweets']=tweets['processed_tweets'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\ntweets.head(5)","74e08650":"from sklearn.utils import shuffle\ntweets =  shuffle(tweets).reset_index(drop=True)\n","70c8be7c":"tokenized_tweet=tweets['processed_tweets'].apply(lambda x: x.split())\ntokenized_tweet.head(5)","963644dd":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\ntoken = RegexpTokenizer(r'[a-zA-Z0-9]+')\ncv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts = cv.fit_transform(tweets['processed_tweets'].values.astype('U'))","14aefdd5":"from sklearn.model_selection import train_test_split\nX=text_counts\ny=tweets['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=19)\n","af6fc53e":"from sklearn.naive_bayes import ComplementNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom math import *\ncnb = ComplementNB()\ncnb.fit(X_train, y_train)\ncross_cnb = cross_val_score(cnb, X, y,n_jobs = -1)\nprint(\"Cross Validation score = \",cross_cnb)                \nprint (\"Train accuracy ={:.2f}%\".format(cnb.score(X_train,y_train)*100))\nprint (\"Test accuracy ={:.2f}%\".format(cnb.score(X_test,y_test)*100))\ntrain_acc_cnb=cnb.score(X_train,y_train)\ntest_acc_cnb=cnb.score(X_test,y_test)","d60161c4":"#plotting the best parameters\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport matplotlib.pyplot as plt\ndata_cnb = [train_acc_cnb,test_acc_cnb]\nlabels = ['Train Accuracy','Test Accuracy']\nplt.xticks(range(len(data_cnb)), labels)\nplt.ylabel('Accuracy')\nplt.title('Accuracy plot with best parameters')\nplt.bar(range(len(data_cnb)), data_cnb,color=['blue','darkorange']) \nTrain_acc = mpatches.Patch(color='blue', label='Train_acc')\nTest_acc = mpatches.Patch(color='darkorange', label='Test_acc')\nplt.legend(handles=[Train_acc, Test_acc],loc='best')\nplt.gcf().set_size_inches(8, 8)\nplt.show()","48990e75":" from sklearn.metrics import *\n#Predict test data set\ny_pred_cnb =cnb.predict(X_test)\n\n#This is the confusion matrix :\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test,y_pred_cnb))","0f409981":"#Checking performance our model with classification report\nprint(classification_report(y_test, y_pred_cnb))\n\n#Checking performance our model with ROC Score\nroc_score_cnb=roc_auc_score(y_test, y_pred_cnb)\nprint(\"Area Under the Curve = \",roc_score_cnb)","97aa2cf3":"from sklearn.metrics import *\n\nprint(\"F1 score ={:.2f}%\".format(f1_score(y_test, y_pred_cnb, average=\"macro\")*100))\nf1_cnb=f1_score(y_test, y_pred_cnb, average=\"macro\")\nprint(\"Precision score ={:.2f}%\".format(precision_score(y_test, y_pred_cnb, average=\"macro\")*100))\nprecision_cnb=precision_score(y_test, y_pred_cnb, average=\"macro\")\nprint(\"Recall score ={:.2f}%\".format(recall_score(y_test, y_pred_cnb, average=\"macro\")*100))  \nrecall_cnb=recall_score(y_test, y_pred_cnb, average=\"macro\")","048ca53c":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport numpy as np\nfpr_dt_1, tpr_dt_1,_=roc_curve(y_test,cnb.predict_proba(X_test)[:,1])\nplt.plot(fpr_dt_1,tpr_dt_1,label=\"ROC curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.gcf().set_size_inches(8, 8)\nplt.show()","ceafb46a":"## Calculating F1, prescision and recall scores","c6c815b9":"# Twitter Sentiment Analysis\n![twitter2.jpg](http:\/\/blog.datumbox.com\/wp-content\/uploads\/2013\/09\/twitter2.jpg)","a3439d8f":"## Authors of this notebook:\n## Himanshu Goyal\n## Ravindra Dhakar\n## Pushpa Sushma Aitha\n## Ishan Kotian\n## Abhinav Bhardwaj\n\n","6429e0ba":"## Step 1 : Currently (0=negative,4=Positive) changing the notation to (0=Negative,1=Positive) \n### So that we can understand the data","6cc71f8d":"## Tokenization","cbd9b0a5":"## Drawing the ROC curve","a45c4356":"## Naive Bayes","6770b688":"## Check the number of positive vs. negative tagged sentences","955c0012":"## Train\/Test Split","2b8c6e18":"## Confusion Matrix","852813b4":"## Data Cleaning","62c20471":"## Checking if any null values present","d0e471e9":"## Text Processing Completed","204ad936":"## Importing Library","6a03dad7":"## Removing the unnecessary columns."}}