{"cell_type":{"66ea1572":"code","496e230f":"code","61fe3c38":"code","2f09f7a3":"code","db3fe96b":"code","50ef84ea":"code","9ffc37d8":"code","13b9d72e":"code","a017d90e":"code","543cd8e9":"code","ae13182a":"code","91f7c78d":"code","12325fbe":"code","7a5f5b99":"code","849d14a0":"code","d5fbc1e4":"code","1f276f1c":"code","fd59b500":"code","1c821412":"code","8b180157":"code","d5eb6614":"code","a7a56944":"code","bbcb4dc1":"code","a6e13bdd":"code","8dcc2c60":"code","e45ba598":"code","932ce169":"code","5fc393e8":"code","4e93173d":"code","e163bf11":"code","a6b7c8e4":"code","d2459f8a":"markdown","5430fc1f":"markdown","3aed052b":"markdown","6c3c4eca":"markdown","4b4e9378":"markdown","cc07811c":"markdown","8951ff76":"markdown","0df65046":"markdown"},"source":{"66ea1572":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#Make sure Internet is toggled on in settings to retrieve data from Gutenberg\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport nltk\nfrom nltk.corpus import gutenberg\nfrom nltk import word_tokenize\nfrom nltk.collocations import *\n\n#Removes all pre-existing image files\nfrom os import path\nf = os.listdir()\nfor each in f:\n    if ((\"png\" in each) or (\"jpg\" in each)):\n        os.remove(each)","496e230f":"commonNames = pd.read_csv('..\/input\/commonnames\/USCensusNameData - Sheet2.csv')\nfemaleEnglishNames = commonNames['FirstnameF']\nmaleEnglishNames = commonNames['FirstnameF.1']\nlastEnglishNames = commonNames['Lastname']\n\n#convert lists to lower case for list comprehension later\ntempNames = []\nfor each in femaleEnglishNames:\n    tempNames.append(each.lower())\nfemaleEnglishNames = tempNames\n\ntempNames = []\nfor each in maleEnglishNames:\n    tempNames.append(each.lower())\n    #print(each.lower())\nmaleEnglishNames = tempNames\n\ntempNames = []\nfor each in lastEnglishNames:\n    tempNames.append(each.lower())\nlastEnglishNames = tempNames","61fe3c38":"from nltk.corpus import brown\nword_list = brown.words()\nword_set = set(word_list)","2f09f7a3":"#Pulls file from Gutenberg, tokenizes & POS tags\nimport urllib3\nurl = \"https:\/\/www.gutenberg.org\/files\/16\/16-0.txt\" #insert url of book here\nraw = urlopen(url).read()\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\nstart = raw.find(\"START OF THIS PROJECT GUTENBERG EBOOK\")\nend = raw.find(\"END OF THIS PROJECT GUTENBERG EBOOK\")\nraw = raw[start:end]\ntokens = word_tokenize(raw)\ntext = nltk.Text(tokens)\ntaggedText = nltk.pos_tag(text)","db3fe96b":"#Remove irrelevant parts of speech\nremovablePOS = [\"CC\", \"PRON\", \"PRP\", \"MD\", \"VBD\", \"VB\", \"DT\", \"RB\", \"IN\", \"PRP$\", \"WP\", \"VBP\", \"CD\"]\ntaggedText = [s for s in taggedText if (len(s[0]) > 3)]\nfor each in removablePOS:\n    taggedText = [s for s in taggedText if s[1] != each]\nbigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\n\n#Import 100 most common nouns in English\n#Added numbers and time of day to end\nmostFrequentNouns = [\"time\",\"year\",\"people\",\"way\",\"day\",\t\"man\",\t\"thing\",\t\"woman\",\t\"life\",\t\"child\",\t\"world\",\t\"school\",\t\"state\",\t\"family\",\t\"student\",\t\"group\",\t\"country\",\t\"problem\",\t\"hand\",\t\"part\",\t\"place\",\t\"case\",\t\"week\",\t\"company\",\t\"system\",\t\"program\",\t\"question\",\t\"work\",\t\"government\",\t\"number\",\t\"night\",\t\"point\",\t\"home\",\t\"water\",\t\"room\",\t\"mother\",\t\"area\",\t\"money\",\t\"story\",\t\"fact\",\t\"month\",\t\"lot\",\t\"right\",\t\"study\",\t\"book\",\t\"eye\",\t\"job\",\t\"word\",\t\"business\",\t\"issue\",\t\"side\",\t\"kind\",\t\"head\",\t\"house\",\t\"service\",\t\"friend\",\t\"father\",\t\"power\",\t\"hour\",\t\"game\",\t\"line\",\t\"end\",\t\"member\",\t\"law\",\t\"car\",\t\"city\",\t\"community\",\t\"name\",\t\"president\",\t\"team\",\t\"minute\",\t\"idea\",\t\"kid\",\t\"body\",\t\"information\",\t\"back\",\t\"parent\",\t\"face\",\t\"others\",\t\"level\",\t\"office\",\t\"door\",\t\"health\",\t\"person\",\t\"art\",\t\"war\",\t\"history\",\t\"party\",\t\"result\",\t\"change\",\t\"morning\",\t\"reason\",\t\"research\",\t\"girl\",\t\"guy\",\t\"moment\",\t\"air\",\t\"teacher\",\t\"force\",\t\"education\", \"something\", \"nothing\", \"anything\", \"everything\", \"voice\", \"manner\", \"matter\", \"course\", \"heart\", \"thing\", \"earth\",\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\", \"thousand\", \"round\", \"table\", \"morning\", \"afternoon\", \"night\", \"evening\", \"when\", \"dear\", \"hours\", \"months\", \"hour\", \"year\", \"month\", \"towards\", \"letter\", \"which\", \"when\", \"things\", \"that\"]\nfrequentTitles = [\"mister\", \"miss\", \"mrs.\", \"mr.\", \"madam\", \"mademoiselle\", \"madame\", \"signor\", \"sir\", \"count\", \"dear\"]\n\n#Function repeatedly lowers the minimum number of occurrences the bigram has to appear in the text to be counted\n#Once ten eligible bigrams have been found, the function stops. \nminimumFrequency = 50\nnumValid = 0\n\nwhile (numValid < 20 and minimumFrequency > 2):\n    #print(minimumFrequency)\n    finder = BigramCollocationFinder.from_words(taggedText)\n    finder.apply_freq_filter(minimumFrequency)\n    numValid = len(finder.nbest(bigram_measures.pmi, 20))\n    minimumFrequency-=1\n\ncandidates = finder.nbest(bigram_measures.pmi, 20)\n\n#Remove any bigrams that:\nfor each in candidates:\n    #print(each)\n    if ((each[1][1].startswith(\"N\") == False) #don't end with a noun\n        or (each[1][0].lower() in mostFrequentNouns) #end with one of 100 most common nouns + additional common words (from list above)\n        or (len(each[1][0]) > 9) #are over nine characters long (to avoid excessive names)\n        or (each[0][0].lower() in frequentTitles)\n       ):\n        #print(each)\n        candidates.remove(each)\n        \n#Check original text to confirm words appear adjacent to each other\ncurrent = 0\napproved = []\n\nwhile current < len(candidates):\n    confirmed = False\n    count = 0\n    while ((count < len(text)) and (confirmed == False)):\n        if text[count] == candidates[current][0][0]: #later replace with variable, then list\n            if text[(count + 1)] == candidates[current][1][0]:\n                approved.append((candidates[current][0][0].capitalize()) + \" \" + (candidates[current][1][0].capitalize()))\n                confirmed = True\n        count+=1\n    current+=1\n    \n#Remove any bigrams that share a word with a higher ranked bigram\n#Also removes names\nsinglesList = []\npairsList = []\n\nfor each in approved:\n    #print(each)\n    test = each.split()\n    if test[0] not in singlesList:\n        if test[0].lower() not in maleEnglishNames and test[0].lower() not in femaleEnglishNames and test[0].lower() not in frequentTitles:\n            #print(test[0])\n            if test[1] not in singlesList:\n                if test[1].lower() not in maleEnglishNames and test[0].lower() not in femaleEnglishNames:\n                    if test[1].lower() not in mostFrequentNouns:\n                        if test[1].lower() in word_set:\n                            if test[0].lower() in word_set:\n                                #print(each)\n                                singlesList.append(test[0])\n                                singlesList.append(test[1])\n                                pairsList.append(each)\n                                #print(pairsList)\n\n#print(pairsList)            \n\n#Take the top five remaining    \ntwoWordCards = pairsList[:5]\n#In the event there are under five eligible bigrams, save this value to create more one word cards\nnumTwoWordCards = len(twoWordCards)\ntwoWordCards","50ef84ea":"#Find the five most commonly occurring words\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer() \n\nabridgedText = []\n\nfor each in taggedText:\n    #remove any words under four characters long, capital words, and non-nouns\n    if((len(each[0]) > 4) and (each[0][0].islower()) and each[1].startswith(\"N\")):\n        #remove any words whose lemmas are part of 100 most common nouns\n        if (lemmatizer.lemmatize(each[0]) not in mostFrequentNouns):\n            abridgedText.append(each[0])\n#abridgedText =            \nfdist = FreqDist(abridgedText)\n\n#Place the ten most commonly occurring words into a list\nmostCommon = fdist.most_common(10)\n\n#Remove any words with the same lemmas (especially plurals)\nfor each in mostCommon:\n    for every in mostCommon:        \n        if lemmatizer.lemmatize(each[0]) == lemmatizer.lemmatize(every[0]):\n            if each != every:\n                mostCommon.remove(every)\n\n#Remove any words that already occur in a two word card\nfor each in mostCommon:\n    for every in twoWordCards:\n        #print(every)\n        if each[0].capitalize() in every:\n            #print(each[0])\n            #print(every)\n            mostCommon.remove(each)\n            \n#Remove any words that share the same first four characters (when lemmas don't matchup)\nshortWords = []\n\nfor each in mostCommon:\n    short = each[0][:4]\n    if short not in shortWords:\n        shortWords.append(each[0][:4])\n    else:\n        mostCommon.remove(each)         \n\n#Take the number of entries necessary to complete a list of ten\nmostCommon = mostCommon[:(10-numTwoWordCards)]\noneWordCards = []\n\nfor each in mostCommon:\n    oneWordCards.append(each[0].capitalize())\n\n#create final cardlist from text\ntextCardList = oneWordCards + twoWordCards\n\n#Place all kingdom cards into list\ndf = pd.read_csv('..\/input\/dominion\/DominionCardInfo.csv')\ndominionCardList = df['Name']\n#dominionCardList = dominionCardList.tolist()\ntextCardList","9ffc37d8":"dominionCardSynsets = []\n\nfor each in dominionCardList:\n    words = each.split()\n    for word in words:\n        #Remove apostrophe mark for cards such as Philosopher's stone\n        word = word.replace(\"\\'\", \"\")\n        #Check whether word exists in sysnet prepositions such as 'of' do not\n        if wordnet.synsets(word):\n            dominionCardSynsets.append([wordnet.synsets(word)[0], word, each])\n            #dominionSynsetReference.append(word)","13b9d72e":"from nltk.corpus import wordnet\nimport random\ncomparisonData = []\ntakenCards = []\n\n#Takes a string of text and returns list where each entry is a word in the string\ndef convertToSynset(cardName):\n    cardSynsets = []\n    words = cardName.split()\n    for word in words:\n        try:\n            cardSynsets.append(wordnet.synsets(word)[0])\n        except:\n            pass\n    return cardSynsets\n\nfor textCard in textCardList:\n    mostSimilar = [\"\", 0]\n    textSynset = convertToSynset(textCard)\n    for textWord in textSynset:\n        for dominionWord in dominionCardSynsets:\n            semanticSimilarity = textWord.wup_similarity(dominionWord[0])\n            if semanticSimilarity:\n                if semanticSimilarity > mostSimilar[1]:\n                    mostSimilar[1] = semanticSimilarity\n                    mostSimilar[0] = dominionWord[2]\n            else:\n                semanticSimilarity = ((random.randint(1, 40))\/100)\n                if semanticSimilarity > mostSimilar[1]:\n                    mostSimilar[1] = semanticSimilarity\n                    mostSimilar[0] = dominionWord[2]\n    result = [textCard, mostSimilar[0], mostSimilar[1]]\n    print(result)\n    takenCards.append(mostSimilar[0])\n    comparisonData.append(result)\n#for each in comparisonData:\n#    print(each)\n    #print(textCard + \" --- \" + mostSimilar[0])","a017d90e":"comparisonData","543cd8e9":"#Takes a list of string and returns same list with all duplicates removed\ndef removeDuplicates(someList):\n    newList = []\n    for each in someList:\n        if each not in newList:\n            newList.append(each)\n    return newList\n#print(takenCards)\n\ntakenCards = removeDuplicates(takenCards)\n#print(takenCards)\n\nwhile len(takenCards) < 10:\n    #Remove all duplicates with lower similarity scores\n    for x in comparisonData:\n        for y in comparisonData:\n            if ((x[0] != y[0]) and x[1] == y[1]):\n                if (x[2] >= y[2]):\n                    y[1] = \"\"\n                    y[2] = 0\n                else:\n                    x[1] = \"\"\n                    x[2] = 0\n    #for each in comparisonData:\n        #print(each)\n\n    #Find a new card for any words that lost one\n    for x in comparisonData:\n        if (x[1] == \"\"):\n            textSynset = convertToSynset(x[0])\n            for textWord in textSynset:\n                for dominionWord in dominionCardSynsets:\n                    #print(dominionWord[2])\n                    if(dominionWord[2]) not in takenCards:\n                        semanticSimilarity = textWord.wup_similarity(dominionWord[0])\n                        if semanticSimilarity:\n                            if semanticSimilarity > x[2]:\n                                x[2] = semanticSimilarity\n                                x[1] = dominionWord[2]\n\n    for each in comparisonData:\n        #print(each)\n        if each[1] not in takenCards:\n            takenCards.append(each[1])\n    #print(takenCards)\nfor x in comparisonData:\n    print((x[0]) + \" --- \" + (x[1]))","ae13182a":"import statistics\n\ndef standardDeviation(numberList):\n    deviationList = []\n    if len(numberList) > 1:\n        standardDeviation = (statistics.stdev(numberList) + 1)\n        average = statistics.mean(numberList)\n        for x in numberList:\n            deviationList.append(round((x - average)\/standardDeviation))\n        return deviationList\n    else:\n        for x in numberList:\n            deviationList.append(0)\n        return deviationList\n\n#Calculate rounded standard deviation for one word cards\nappearances = []\n\nfor each in mostCommon:\n    appearances.append(each[1])\n    \noneWordCostChanges = standardDeviation(appearances)\n\n#Calculate rounded standard deviation for two word cards\ntwoWordFrequencies = []\nappearances = []\n\nfor x in twoWordCards:\n    words = x.split()\n    firstWord = words[0]\n    secondWord = words[1]\n    count = 0\n    occurrence = 0\n    while count < len(text):\n        if firstWord.lower() == text[count] or firstWord == text[count]:\n            if (secondWord.lower() == text[(count+1)]) or (secondWord == text[(count+1)]):\n                #print(text[count] + \" \" + text[(count+1)] + str(count))\n                occurrence+=1\n                #print(words[0] + str(count)):\n        count+=1\n    total = [x,occurrence]\n    #print(total)\n    twoWordFrequencies.append(total)\n    appearances.append(occurrence)\n    \ntwoWordCostChanges = standardDeviation(appearances)\ncostChanges = oneWordCostChanges + twoWordCostChanges\ncostChanges\ncount = 0\nwhile count < len(comparisonData):\n    print(comparisonData[count][0] + \" ... \"+ comparisonData[count][1] + \" ... \" + str(costChanges[count]))\n    count+=1\n#for x in comparisonData:\n #   print((x[0]) + \" --- \" + (x[1]))","91f7c78d":"wordCount = len(taggedText)\n#print(mostCommon)\nfor each in mostCommon:\n    percent = ((each[1]\/wordCount)*100)\n    print( (each[0]) + \": \" + str(percent))\n","12325fbe":"#Be sure that Internet toggle is turned on in settings for dependencies to successfully install\n!pip install google-api-python-client\n!pip install python-resize-image\n!pip install Google-Images-Search\n\nfrom apiclient.discovery import build\n#what to do when image function is used from two different libraries?\n#from IPython.display import Image\nfrom IPython.core.display import HTML \nfrom IPython.display import Markdown as md\nfrom resizeimage import resizeimage\nimport PIL\nfrom PIL import Image","7a5f5b99":"#!pip install piexif","849d14a0":"#import piexif\n#from IPython.display import Image","d5fbc1e4":"cardList","1f276f1c":"api_key = \"AIzaSyBcRQ5XGx042vZh8itsNhg6QS39WdDq_ZE\" #from personal email Google Developers Account\nresource = build(\"customsearch\", \"v1\", developerKey=api_key).cse()\n\noneWordCards = ['Children', 'Course', 'Moment', 'Night']\nbase_set = [\"cellar\", \"market\", \"merchant\", \"militia\", \"mine\", \"moat\", \"remodel\", \"bandit\", \"village\", \"workshop\"]\n\ncount = 0\nfor each in cardList:\n    \n    #locates url of google image search result\n    search_term = each\n    search_term = search_term + \" clip art\"\n    result = resource.list(q=search_term, cx='003240139077527252035:bj4glzhiden', searchType='image').execute()\n    url = result['items'][0]['link']\n    \n    #defines filename\n    #filename = each + \".jpg\"\n    filename = \"image\" + str(count) + \".png\"\n    #saves url as image\n    urllib.request.urlretrieve(url, filename)\n    \n    #removes exif metadata\n   # img = Image.open(filename)\n    #piexif.transplant(filename, \"remodel.jpg\")\n    #image = Image.new(image.mode, image.size)\n    \n    #resize image\n   # basewidth = 200\n    #wpercent = (basewidth \/ float(img.size[0]))\n    #hsize = int((float(image.size[1]) * float(wpercent)))\n    #img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n    \n    #save image so markdown can consistently pick it up\n    #resized_filename = \"image\" + str(count) + \".png\"\n    #new_filename = \"image\" + str(count) + \".png\"\n    #img.save(new_filename)\n    count = count + 1","fd59b500":"print(PIL.PILLOW_VERSION)","1c821412":"#piexif.transplant(\"Mother.jpg\", \"Night.jpg\")","8b180157":"exif_dict = \"\"\n\nexif_bytes = piexif.dump(exif_dict)\npiexif.insert(exif_bytes, filename)","d5eb6614":"cardNames = cardNames.values.tolist()","a7a56944":"print(cardNames)","bbcb4dc1":"print(dominionSynsetReference)","a6e13bdd":"print(dominionCardSynsets[0])","8dcc2c60":"from nltk.corpus import wordnet\n\n#Takes a string of text and returns list where each entry is a word in the string\ndef convertToSynset(cardName):\n    cardSynsets = []\n    words = cardName.split()\n    for word in words:\n        cardSynsets.append(wordnet.synsets(word)[0])\n    return cardSynsets\n\ntest = 'villager'\ntestx = wordnet.synsets(test)[0]\n\nfor textCard in textCardList: \n    mostSimilar = [\"\", 0]\n    textSynset = convertToSynset(textCard)\n    for textWord in textSynset:\n        for dominionWord in dominionCardSynsets:\n            #print(textWord)\n            #print(dominionWord[1])\n            semanticSimilarity = textWord.wup_similarity(dominionWord[0])\n            #print(semanticSimilarity)\n            #print(mostSimilar[1])\n            if semanticSimilarity:\n                if semanticSimilarity > mostSimilar[1]:\n                    mostSimilar[1] = semanticSimilarity\n                    mostSimilar[0] = dominionWord[2]\n    print(textCard + \" is most similar to \" + mostSimilar[0] + \" \" + str(mostSimilar[1]))\n    #semanticSimilarity = synset.wup_similarity('villager')\n    #print(semanticSimilarity)\n    #print(synset)\n    #for each in synset:\n     #   semanticSimilarity = each.wup_similarity('villager')\n      #  print(semanticSimilarity)\n       # print('\\n')\n    #words = textCard.split()\n    #for word in words:\n       # wordx = wordnet.synsets(word)[0]\n        #print(wordx)\n        #for dominionCard in dominionCardList:\n            ","e45ba598":"dominionCardSynsets = []\ndominionSynsetReference = []\n\nfor each in dominionCardList:\n    words = each.split()\n    for word in words:\n        #Remove apostrophe mark for cards such as Philosopher's stone\n        word = word.replace(\"\\'\", \"\")\n        #Check whether word exists in sysnet prepositions such as 'of' do not\n        if wordnet.synsets(word):\n            dominionCardSynsets.append([wordnet.synsets(word)[0], word, each])\n            #dominionSynsetReference.append(word)","932ce169":"from nltk.corpus import wordnet\n\nfor card in textCardList: \n    mostSimilar = [\"\", 0]\n    words = card.split()\n    print(words)\n    for each in cardNames:\n        wordx = wordnet.synsets(word)[0]\n        words = word.split\n        if len(each.split()) == 1:\n            if wordnet.synsets(each):\n                card = wordnet.synsets(each)[0]\n                semanticSimilarity = wordx.wup_similarity(card)\n                if semanticSimilarity:\n                    #print(str(word) + \" has a \" + str((round(wordx.wup_similarity(card) * 100))) + \"% similarity to \" + each)\n                    if semanticSimilarity > mostSimilar[1]:\n                        #print(each + \" is currently the most semantically similar\")\n                        mostSimilar[1] = semanticSimilarity\n                        mostSimilar[0] = each\n                    elif semanticSimilarity == mostSimilar[1]:\n                        #print(each + \" is tied for most semantically similar\")\n                        mostSimilar[0] = (mostSimilar[0] + \", \" + each)\n                #else:\n                    #print(each + \" has no semantic relationship with \" + word)\n    print(mostSimilar[0] + \" is\/are the most semantically similar kingdom card(s) to \" + word) \n\n#for each in words:\n #   word = wordnet.synsets(each)\n  #  print(each)\n   # print(word)","5fc393e8":"from nltk.corpus import wordnet\n\nfor word in cardList: \n    mostSimilar = [\"\", 0]\n    for each in cardNames:\n        wordx = wordnet.synsets(word)[0]\n        words = word.split\n        if len(each.split()) == 1:\n            if wordnet.synsets(each):\n                card = wordnet.synsets(each)[0]\n                semanticSimilarity = wordx.wup_similarity(card)\n                if semanticSimilarity:\n                    #print(str(word) + \" has a \" + str((round(wordx.wup_similarity(card) * 100))) + \"% similarity to \" + each)\n                    if semanticSimilarity > mostSimilar[1]:\n                        #print(each + \" is currently the most semantically similar\")\n                        mostSimilar[1] = semanticSimilarity\n                        mostSimilar[0] = each\n                    elif semanticSimilarity == mostSimilar[1]:\n                        #print(each + \" is tied for most semantically similar\")\n                        mostSimilar[0] = (mostSimilar[0] + \", \" + each)\n                #else:\n                    #print(each + \" has no semantic relationship with \" + word)\n    print(mostSimilar[0] + \" is\/are the most semantically similar kingdom card(s) to \" + word) \n\n#for each in words:\n #   word = wordnet.synsets(each)\n  #  print(each)\n   # print(word)","4e93173d":"from nltk.corpus import wordnet\n\nword = wordnet.synsets('oasis')[0] \n\naction = wordnet.synsets('action')[0]\nattack = wordnet.synsets('attack')[0]\ntreasure = wordnet.synsets('treasure')[0]\nevent = wordnet.synsets('event')[0]\nvictory = wordnet.synsets('victory')[0]\nreaction = wordnet.synsets('reaction')[0]\nduration = wordnet.synsets('duration')[0]\nruins = wordnet.synsets('ruins')[0]\nshelter = wordnet.synsets('shelter')[0]\nreserve = wordnet.synsets('reserve')[0]\n\nprint(word)\nprint(str(word.wup_similarity(action)) + \" action\")\nprint(str(word.wup_similarity(attack)) + \" attack\")\nprint(str(word.wup_similarity(treasure)) + \" treasure\")\nprint(str(word.wup_similarity(event)) + \" event\")\nprint(str(word.wup_similarity(victory)) + \" victory\")\nprint(str(word.wup_similarity(reaction)) + \" reaction\")\nprint(str(word.wup_similarity(duration)) + \" duration\")\nprint(str(word.wup_similarity(ruins)) + \" ruins\")\nprint(str(word.wup_similarity(shelter)) + \" shelter\")\nprint(str(word.wup_similarity(reserve)) + \" reserve\")","e163bf11":"word = \"feodum\"\nif wordnet.synsets(word):\n    print(wordnet.synsets(\"feodum\"))","a6b7c8e4":"from nltk.corpus import wordnet\n#example output from Alice & Wonderland\n#note that string concatenation can be done in Sheets with =char(34)&A1&char(34)\nwords = [\"cheshire cat\", \"queen\", \"golden key\", \"rabbit-hole\", \"caterpillar\", \"mad hatter\"]\nwordTest = [\"cardinal\", \"cavalry\", \"fair\", \"fisherman\", \"supplies\", \"sanctuary\", \"mastermind\"]\ndominionCardTitles = [\"Envoy\", \"Governor\", \"Prince\", \"Stash\", \"Summon\", \"Garden\", \"Black Market\", \t\"Walled Village\", \t\"Adventurer\", \t\"Bureaucrat\", \t\"Cellar\", \t\"Chancellor\", \t\"Chapel\", \t\"Council Room\", \t\"Feast\", \t\"Festival\", \t\"Gardens\", \t\"Laboratory\", \t\"Library\", \t\"Market\", \t\"Militia\", \t\"Mine\", \t\"Moat\", \t\"Moneylender\", \t\"Remodel\", \t\"Smithy\", \t\"Spy\", \t\"Thief\", \t\"Throne Room\", \t\"Village\", \t\"Witch\", \t\"Woodcutter\", \t\"Workshop\", \t\"Baron\", \t\"Bridge\", \t\"Conspirator\", \t\"Coppersmith\", \t\"Courtyard\", \t\"Duke\", \t\"Great Hall\", \t\"Harem\", \t\"Ironworks\", \t\"Masquerade\", \t\"Mining Village\", \t\"Minion\", \t\"Nobles\", \t\"Pawn\", \t\"Saboteur\", \t\"Scout\", \t\"Secret Chamber\", \t\"Shanty Town\", \t\"Steward\", \t\"Swindler\", \t\"Torturer\", \t\"Trading Post\", \t\"Tribute\", \t\"Upgrade\", \t\"Wishing Well\", \t\"Ambassador\", \t\"Bazaar\", \t\"Caravan\", \t\"Cutpurse\", \t\"Embargo\", \t\"Explorer\", \t\"Fishing Village\", \t\"Ghost Ship\", \t\"Haven\", \t\"Island\", \t\"Lighthouse\", \t\"Lookout\", \t\"Merchant Ship\", \t\"Native Village\", \t\"Navigator\", \t\"Outpost\", \t\"Pearl Diver\", \t\"Pirate Ship\", \t\"Salvager\", \t\"Sea Hag\", \t\"Smugglers\", \t\"Tactician\", \t\"Treasure Map\", \t\"Treasury\", \t\"Warehouse\", \t\"Wharf\", \t\"Alchemist\", \t\"Apothecary\", \t\"Apprentice\", \t\"Familiar\", \t\"Golem\", \t\"Herbalist\", \t\"Philosopher's Stone\", \t\"Possession\", \t\"Scrying Pool\", \t\"Transmute\", \t\"University\", \t\"Vineyard\", \t\"Bank\", \t\"Bishop\", \t\"City\", \t\"Contraband\", \t\"Counting House\", \t\"Expand\", \t\"Forge\", \t\"Goons\", \t\"Grand Market\", \t\"Hoard\", \t\"King's Court\", \t\"Loan\", \t\"Mint\", \t\"Monument\", \t\"Mountebank\", \t\"Peddler\", \t\"Quarry\", \t\"Rabble\", \t\"Royal Seal\", \t\"Talisman\", \t\"Trade Route\", \t\"Vault\", \t\"Venture\", \t\"Watchtower\", \t\"Worker's Village\", \t\"Bag of Gold\", \t\"Diadem\", \t\"Fairgrounds\", \t\"Farming Village\", \t\"Followers\", \t\"Fortune Teller\", \t\"Hamlet\", \t\"Harvest\", \t\"Horn of Plenty\", \t\"Horse Traders\", \t\"Hunting Party\", \t\"Jester\", \t\"Menagerie\", \t\"Princess\", \t\"Remake\", \t\"Tournament\", \t\"Trusty Steed\", \t\"Young Witch\", \t\"Border Village\", \t\"Cache\", \t\"Cartographer\", \t\"Crossroads\", \t\"Develop\", \t\"Duchess\", \t\"Embassy\", \t\"Farmland\", \t\"Fool's Gold\", \t\"Haggler\", \t\"Highway\", \t\"Ill-Gotten Gains\", \t\"Inn\", \t\"Jack of All Trades\", \t\"Mandarin\", \t\"Margrave\", \t\"Noble Brigand\", \t\"Nomad Camp\", \t\"Oasis\", \t\"Oracle\", \t\"Scheme\", \t\"Silk Road\", \t\"Spice Merchant\", \t\"Stables\", \t\"Trader\", \t\"Tunnel\", \t\"Colony\", \t\"Copper\", \t\"Curse\", \t\"Duchy\", \t\"Estate\", \t\"Gold\", \t\"Platinum\", \t\"Potion\", \t\"Province\", \t\"Silver\", \t\"Abandoned Mine\", \t\"Altar\", \t\"Armory\", \t\"Band of Misfits\", \t\"Bandit Camp\", \t\"Beggar\", \t\"Catacombs\", \t\"Count\", \t\"Counterfeit\", \t\"Cultist\", \t\"Dame Anna\", \t\"Dame Josephine\", \t\"Dame Molly\", \t\"Dame Natalie\", \t\"Dame Sylvia\", \t\"Death Cart\", \t\"Feodum\", \t\"Forager\", \t\"Fortress\", \t\"Graverobber\", \t\"Hermit\", \t\"Hovel\", \t\"Hunting Grounds\", \t\"Ironmonger\", \t\"Junk Dealer\", \t\"Madman\", \t\"Marauder\", \t\"Market Square\", \t\"Mercenary\", \t\"Mystic\", \t\"Necropolis\", \t\"Overgrown Estate\", \t\"Pillage\", \t\"Poor House\", \t\"Procession\", \t\"Rats\", \t\"Rebuild\", \t\"Rogue\", \t\"Ruined Library\", \t\"Ruined Market\", \t\"Ruined Village\", \t\"Sage\", \t\"Scavenger\", \t\"Sir Bailey\", \t\"Sir Destry\", \t\"Sir Martin\", \t\"Sir Michael\", \t\"Sir Vander\", \t\"Spoils\", \t\"Squire\", \t\"Storeroom\", \t\"Survivors\", \t\"Urchin\", \t\"Vagrant\", \t\"Wandering Minstrel\", \t\"Advisor\", \t\"Baker\", \t\"Butcher\", \t\"Candlestick Maker\", \t\"Doctor\", \t\"Herald\", \t\"Journeyman\", \t\"Masterpiece\", \t\"Merchant Guild\", \t\"Plaza\", \t\"Soothsayer\", \t\"Stonemason\", \t\"Taxman\", \t\"Alms\", \t\"Amulet\", \t\"Artificer\", \t\"Ball\", \t\"Bonfire\", \t\"Borrow\", \t\"Bridge Troll\", \t\"Caravan Guard\", \t\"Champion\", \t\"Coin of the Realm\", \t\"Disciple\", \t\"Distant Lands\", \t\"Dungeon\", \t\"Duplicate\", \t\"Expedition\", \t\"Ferry\", \t\"Fugitive\", \t\"Gear\", \t\"Giant\", \t\"Guide\", \t\"Haunted Woods\", \t\"Hero\", \t\"Hireling\", \t\"Inheritance\", \t\"Lost Arts\", \t\"Lost City\", \t\"Magpie\", \t\"Messenger\", \t\"Miser\", \t\"Mission\", \t\"Page\", \t\"Pathfinding\", \t\"Peasant\", \t\"Pilgrimage\", \t\"Plan\", \t\"Port\", \t\"Quest\", \t\"Raid\", \t\"Ranger\", \t\"Ratcatcher\", \t\"Raze\", \t\"Relic\", \t\"Royal Carriage\", \t\"Save\", \t\"Scouting Party\", \t\"Seaway\", \t\"Soldier\", \t\"Storyteller\", \t\"Swamp Hag\", \t\"Teacher\", \t\"Trade\", \t\"Training\", \t\"Transmogrify\", \t\"Travelling Fair\", \t\"Treasure Hunter\", \t\"Treasure Trove\", \t\"Warrior\", \t\"Wine Merchant\"]\n\n#Find the dominion card that most closely resembles word from text\nfor word in words:\n    topScore = 0\n    test = word.split()\n    for each in test:\n        print(each)","d2459f8a":"The following code block removes any cases where multiple word phrases mapped onto the same dominion card. \nPreference is given to the word phrase with the closest semantic similarity, with tie going to whichever word phrase appears earlier in the list. ","5430fc1f":"TODO: Figure out a way to include the instructions for each card here. \nCurrent thought: explore ways to combine images in Python, so that any combination (+x actions\/cards\/etc) can be concatenated into a single image, saved, and then displayed in the subsequent row in the above markdown. \nMay be necessary to split into two rows of 5 cards each.","3aed052b":"TASK\nWrite a simple script that takes an array of words and for each returns the title of an existing dominion card that is the most semantically related based on Wu-Palmer similarity","6c3c4eca":"| Cellar | Market | Merchant | Militia | Mine | Moat | Remodel | Bandit | Village | Workshop |\n| :---: | :---: | :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| ![alt](image0.png) | ![alt](image1.png) | ![alt](image2.png) | ![alt](image3.png) | ![alt](image4.png) | ![alt](image5.png) | ![alt](image6.png) | ![alt](image7.png) | ![alt](image8.png) | ![alt](image9.png) |\n","4b4e9378":"testing of wordnet below","cc07811c":"Code below is related to image creation. Leave aside for now.","8951ff76":"These next two code blocks (and following markdown) retrieve and display clip art for the ten kingdom card names provided. \n","0df65046":"The next task is to work out card buffs and costs based on how frequently a word phrase appears in the text\n\nmostCommon (single words, includes frequency already)\ntwoWordCards (bigrams)"}}