{"cell_type":{"eeedcdcc":"code","5c832383":"code","7420ff0e":"code","bbc0606d":"code","537fefa7":"code","7f238a35":"code","9722bf72":"code","88e57fd2":"code","b1f5f69b":"code","30950446":"code","c19d8348":"code","3014e51e":"code","2ecce2e9":"code","1a08d4b1":"code","8c5a6227":"code","66630c81":"code","ae1fbe93":"code","05103a3b":"code","0b0f9206":"code","1fe3bf04":"code","b8ec9655":"code","26b285c1":"code","fab6b8f1":"code","ec3f65bb":"code","8b22db74":"code","1d646053":"code","d05eb9ff":"code","7f23344b":"code","9ba0d8c1":"code","6d6e78ab":"code","7354870f":"code","ad2a73ae":"code","3f7828f0":"code","7d1b405f":"code","9316f623":"code","8679eb00":"code","a525f88f":"code","db9a5da4":"code","cb8fd623":"code","44624e9d":"code","ed5d5790":"code","74e87448":"markdown","a4518815":"markdown","036b60ee":"markdown","29f6007d":"markdown","7f61e2e1":"markdown","c4e80f30":"markdown","df5e214e":"markdown","8a967b58":"markdown","be445df2":"markdown","84805c0f":"markdown","cdf72bd8":"markdown","8203c766":"markdown","abf61686":"markdown","2d14668e":"markdown","4488ec21":"markdown","ededbd8e":"markdown","c50cc50c":"markdown","31f94d90":"markdown","38fad137":"markdown","bc6dfc5f":"markdown","dbfce27a":"markdown","60b5ce35":"markdown","26fe1c00":"markdown","6fb0f3b8":"markdown","34ef7621":"markdown","9e5c876b":"markdown","d9453271":"markdown","71317e60":"markdown","7fa9811b":"markdown","a756490d":"markdown","866a73d7":"markdown"},"source":{"eeedcdcc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport re\nimport string \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.preprocessing import text,sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","5c832383":"real_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","7420ff0e":"real_data.head()","bbc0606d":"fake_data.head()","537fefa7":"#add column \nreal_data['target'] = 1\nfake_data['target'] = 0 ","7f238a35":"real_data.tail()","9722bf72":"#Merging the 2 datasets\ndata = pd.concat([real_data, fake_data], ignore_index=True, sort=False)\ndata.head()","88e57fd2":"data.isnull().sum()","b1f5f69b":"print(data[\"target\"].value_counts())\nfig, ax = plt.subplots(1,2, figsize=(19, 5))\ng1 = sns.countplot(data.target,ax=ax[0],palette=\"pastel\");\ng1.set_title(\"Count of real and fake data\")\ng1.set_ylabel(\"Count\")\ng1.set_xlabel(\"Target\")\ng2 = plt.pie(data[\"target\"].value_counts().values,explode=[0,0],labels=data.target.value_counts().index, autopct='%1.1f%%',colors=['SkyBlue','PeachPuff'])\nfig.show()","30950446":"print(data.subject.value_counts())\nplt.figure(figsize=(10, 5))\n\nax = sns.countplot(x=\"subject\",  hue='target', data=data, palette=\"pastel\")\nplt.title(\"Distribution of The Subject According to Real and Fake Data\")","c19d8348":"data['text']= data['subject'] + \" \" + data['title'] + \" \" + data['text']\ndel data['title']\ndel data['subject']\ndel data['date']\ndata.head()","3014e51e":"first_text = data.text[10]\nfirst_text","2ecce2e9":"pip install bs4","1a08d4b1":"from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(first_text, \"html.parser\")\nfirst_text = soup.get_text()\nfirst_text","8c5a6227":"first_text = re.sub('\\[[^]]*\\]', ' ', first_text)\nfirst_text = re.sub('[^a-zA-Z]',' ',first_text)  # replaces non-alphabets with spaces\nfirst_text = first_text.lower() # Converting from uppercase to lowercase\nfirst_text","66630c81":"nltk.download(\"stopwords\")   \nfrom nltk.corpus import stopwords  \n\n# we can use tokenizer instead of split\nfirst_text = nltk.word_tokenize(first_text)","ae1fbe93":"first_text = [ word for word in first_text if not word in set(stopwords.words(\"english\"))]","05103a3b":"lemma = nltk.WordNetLemmatizer()\nfirst_text = [ lemma.lemmatize(word) for word in first_text] \n\nfirst_text = \" \".join(first_text)\nfirst_text","0b0f9206":"#Removal of HTML Contents\ndef remove_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removal of Punctuation Marks\ndef remove_punctuations(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removal of Special Characters\ndef remove_characters(text):\n    return re.sub(\"[^a-zA-Z]\",\" \",text)\n\n#Removal of stopwords \ndef remove_stopwords_and_lemmatization(text):\n    final_text = []\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    \n    for word in text:\n        if word not in set(stopwords.words('english')):\n            lemma = nltk.WordNetLemmatizer()\n            word = lemma.lemmatize(word) \n            final_text.append(word)\n    return \" \".join(final_text)\n\n#Total function\ndef cleaning(text):\n    text = remove_html(text)\n    text = remove_punctuations(text)\n    text = remove_characters(text)\n    text = remove_stopwords_and_lemmatization(text)\n    return text\n\n#Apply function on text column\ndata['text']=data['text'].apply(cleaning)","1fe3bf04":"data.head()","b8ec9655":"from wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.target == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","26b285c1":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.target == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","fab6b8f1":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=data[data['target']==0]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='SkyBlue')\nax1.set_title('Fake news text')\ntext_len=data[data['target']==1]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='PeachPuff')\nax2.set_title('Real news text')\nfig.suptitle('Words in texts')\nplt.show()","ec3f65bb":"texts = ' '.join(data['text'])","8b22db74":"string = texts.split(\" \")","1d646053":"def draw_n_gram(string,i):\n    n_gram = (pd.Series(nltk.ngrams(string, i)).value_counts())[:15]\n    n_gram_df=pd.DataFrame(n_gram)\n    n_gram_df = n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head())\n    plt.figure(figsize = (16,9))\n    return sns.barplot(x='count',y='word', data=n_gram_df)","d05eb9ff":"draw_n_gram(string,1)","7f23344b":"draw_n_gram(string,2)","9ba0d8c1":"draw_n_gram(string,3)","6d6e78ab":"X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], random_state=0)","7354870f":"max_features = 10000\nmaxlen = 300","ad2a73ae":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","3f7828f0":"tokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","7d1b405f":"batch_size = 256\nepochs = 10\nembed_size = 100","9316f623":"model = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","8679eb00":"model.summary()","a525f88f":"history = model.fit(X_train, y_train, validation_split=0.3, epochs=10, batch_size=batch_size, shuffle=True, verbose = 1)","db9a5da4":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","cb8fd623":"plt.figure()\nplt.plot(history.history[\"accuracy\"], label = \"Train\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Test\")\nplt.title(\"Accuracy\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","44624e9d":"plt.figure()\nplt.plot(history.history[\"loss\"], label = \"Train\")\nplt.plot(history.history[\"val_loss\"], label = \"Test\")\nplt.title(\"Loss\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","ed5d5790":"pred = model.predict_classes(X_test)\nprint(classification_report(y_test, pred, target_names = ['Fake','Real']))","74e87448":"<a id = 7><\/a>\n<h2><font color = MidnightBlue>Removal of Stopwords<\/font><\/h2>","a4518815":"**1.Count of Fake and Real Data**","036b60ee":"<a id = 14><\/a>\n<h2><font color = MidnightBlue>Modeling<\/font><\/h2>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","29f6007d":"<a id = 12><\/a>\n<h2><font color = MidnightBlue>Bigram Analysis<\/font><\/h2>","7f61e2e1":"<a id = 10><\/a>\n<h2><font color = MidnightBlue>N-Gram Analysis<\/font><\/h2>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">\n<center><img style = \"height:450px;\" src=\"https:\/\/devopedia.org\/images\/article\/219\/7356.1569499094.png\"><\/center>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","c4e80f30":"<a id = 9><\/a>\n<h2><font color = MidnightBlue>Perform it for all the examples<\/font><\/h2>\n<b>We performed the steps for a single example. Now let's perform it for all the examples in the data.<\/b>","df5e214e":"<h3><b>Number of words in each text<\/b><\/h3>","8a967b58":"<a id = 3><\/a>\n<h1><font color = MidnightBlue>Visualization<\/font><\/h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","be445df2":"**Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.**","84805c0f":"<a id = 5><\/a>\n<h2><font color = MidnightBlue>Removal of HTML Contents<\/font><\/h2>","cdf72bd8":"<h2><b>Let's make some visualization with new data.<\/b><\/h2>","8203c766":"<a id = 6><\/a>\n<h2><font color = MidnightBlue>Removal of Punctuation Marks and Special Characters<\/font><\/h2>","abf61686":"<a id = 15><\/a>\n<h2><font color = MidnightBlue>Train Test Split<\/font><\/h2>","2d14668e":"**2.Distribution of The Subject According to Real and Fake Data**","4488ec21":"<a id = 2><\/a>\n<h1><font color = MidnightBlue>Load and Check Data<\/font><\/h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","ededbd8e":"**First, let's remove HTML content.**","c50cc50c":"<a id = 18><\/a>\n<h2><font color = MidnightBlue>Analysis After Training <\/font><\/h2>","31f94d90":"<h3><b>2.WordCloud for Fake News <\/b><\/h3>","38fad137":"**Let's now remove everything except uppercase \/ lowercase letters using Regular Expressions.**","bc6dfc5f":"<a id = 1><\/a>\n<h1><font color = MidnightBlue>Import Libraries<\/font><\/h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","dbfce27a":"**The number of words seems to be a bit different. 500 words  are most common in real news category while around 250 words are most common in fake news category.**","60b5ce35":"<a id = 8><\/a>\n<h2><font color = MidnightBlue>Lemmatization<\/font><\/h2>","26fe1c00":"**Let's remove stopwords like is,a,the... Which do not offer much insight.**","6fb0f3b8":"<h3><b>1.WordCloud for Real News <\/b><\/h3>","34ef7621":"<a id = 4><\/a>\n<h1><font color = MidnightBlue>Data Cleaning<\/font><\/h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","9e5c876b":"<a id = 11><\/a>\n<h2><font color = MidnightBlue>Unigram Analysis<\/font><\/h2>","d9453271":"<a id = 17><\/a>\n<h2><font color = MidnightBlue>Training LSTM Model<\/font><\/h2>","71317e60":"* **Tokenizing Text -> Repsesenting each word by a number**\n\n* **Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n<h3><b>Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones <\/b><\/h3>","7fa9811b":"<hr style=\"width:100%;height:3px;border-width:0;background-color:silver\">\n<h1 style=\"text-align:center\">   \n      <font color = MidnightBlue >\n                Fake News Detection with NLP and LSTM \n        <\/font>    \n<\/h1>   \n<hr style=\"width:100%;height:3px;border-width:0;background-color:silver\">\n<center><img style = \"height:550px;\" src=\"https:\/\/images.livemint.com\/rf\/Image-621x414\/LiveMint\/Period2\/2018\/05\/05\/Photos\/Processed\/fakereal-k2QC--621x414@LiveMint.jpg\"><\/center>\n<br>\n<h2><font color = MidnightBlue>What is \"Fake News\"?<\/font><\/h2>\n<p>\u201cFake news\u201d is a term that has come to mean different things to different people. At its core, we are defining \u201cfake news\u201d as those news stories that are false: the story itself is fabricated, with no verifiable facts, sources or quotes. Sometimes these stories may be propaganda that is intentionally designed to mislead the reader, or may be designed as \u201cclickbait\u201d written for economic incentives (the writer profits on the number of people who click on the story). In recent years, fake news stories have proliferated via social media, in part because they are so easily and quickly shared online.<\/p>\n<h2><font color = MidnightBlue>About Dataset<\/font><\/h2>\n<p>This data set consists of 40000 fake and real news. Our goal is to train our model to accurately predict whether a particular piece of news is real or fake. Fake and real news data are given in two separate data sets, with each data set consisting of approximately 20000 articles.<\/p>\n<h2><font color = MidnightBlue>Content:<\/font><\/h2>\n<br>\n \n1. [Import Libraries](#1)\n1. [Load and Check Data](#2)\n1. [Visualization](#3)\n1. [Data Cleaning](#4)\n    * [Removal of HTML Contents](#5)\n    * [Removal of Punctuation Marks and Special Characters](#6)\n    * [Removal of Stopwords](#7)\n    * [Lemmatization](#8)\n    * [Perform it for all the examples](#9)  \n1. [N-Gram Analysis](#10)\n    * [Unigram Analysis](#11)\n    * [Bigram Analysis](#12)\n    * [Trigram Analysis](#13)\n1. [Modeling](#14)\n    * [Train - Test Split](#15)\n    * [Tokenizing](#16)\n    * [Training LSTM Model](#17)\n    * [Analysis After Training](#18) ","a756490d":"<a id = 13><\/a>\n<h2><font color = MidnightBlue>Trigram Analysis<\/font><\/h2>","866a73d7":"<a id = 16><\/a>\n<h2><font color = MidnightBlue>Tokenizing<\/font><\/h2>"}}