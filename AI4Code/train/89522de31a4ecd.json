{"cell_type":{"75aff449":"code","0927c47d":"code","ca20002f":"code","081d0e1c":"code","4e4a4b22":"code","67137c8a":"code","16fc728b":"code","bdbedd0a":"code","74d909a6":"code","80140d41":"code","cca752c6":"code","255ae120":"markdown","b40039de":"markdown","4f9ce639":"markdown","a4669909":"markdown","0e6476c5":"markdown","6e06527c":"markdown","a4a46b50":"markdown","98e6e220":"markdown"},"source":{"75aff449":"import os\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm._tqdm_notebook import tqdm_notebook\nfrom matplotlib import pyplot as plt","0927c47d":"train_metadata_df = pd.read_csv(\"\/kaggle\/input\/birdclef-2021\/train_metadata.csv\")","ca20002f":"label_dic = {v:i for i, v in enumerate(train_metadata_df[\"primary_label\"].unique())}\nn_labels = len(label_dic)\ny_train = train_metadata_df[\"primary_label\"].map(label_dic).values","081d0e1c":"def check_year(year):\n    if year[:2] in [\"19\", \"20\"]:\n        return int(year)\n    else:\n        # there are mistake label like 0000, 0201, 0199, ...\n        return -1","4e4a4b22":"date_df = pd.DataFrame(train_metadata_df[\"date\"].str.split(\"-\").tolist(), columns=[\"year\", \"month\", \"day\"])\ndate_df[\"year\"] = date_df[\"year\"].map(check_year)\ndate_df[\"month\"] = date_df[\"month\"].astype(int)\ndate_df[\"day\"] = date_df[\"day\"].astype(int)\ndate_df.head()","67137c8a":"def check_hhmm(m):\n    if m is None:\n        return -1\n    m = m.lower().replace(\"am\", \"\").replace(\"pm\", \"\")\n    if m in [\"?\", \"??\", \"x\", \"xx\", \".\", \"\", \"night\", \"xx.xx\", \"dawn\", \"xx;xx\"]:\n        return -1\n    return int(m)","16fc728b":"time_df = pd.DataFrame(train_metadata_df[\"time\"].str.split(\":\").tolist(), columns=[\"hour\", \"minute\", \"second\"])\ntime_df[\"hour\"] = time_df[\"hour\"].map(check_hhmm)\ntime_df[\"minute\"] = time_df[\"minute\"].map(check_hhmm)\ntime_df = time_df.drop(\"second\", axis=1)\ntime_df.head()","bdbedd0a":"author_counts = train_metadata_df[\"author\"].value_counts()\nfrequent_author = {v: i for i, v in enumerate(author_counts[author_counts > 100].index)}\nauthor_df = pd.DataFrame(train_metadata_df[\"author\"].map(frequent_author).fillna(-1).values, columns=[\"author_id\"])\nauthor_df.head()","74d909a6":"org_features = [\"latitude\", \"longitude\", \"rating\"]\nfeature_df = pd.concat([train_metadata_df[org_features], date_df, time_df, author_df], axis=1)\nfeature_df.head()","80140d41":"features = feature_df.columns.tolist()\nX_train = feature_df.values","cca752c6":"oof = np.zeros(len(y_train))\nskf = StratifiedKFold(n_splits=5,  shuffle=True, random_state=416)\nfor train_index, valid_index in skf.split(X_train, y_train):\n    \n    dtrain = lgb.Dataset(X_train[train_index, :], label=y_train[train_index])\n    d_eval = lgb.Dataset(X_train[valid_index, :], label=y_train[valid_index])\n    \n    param = {\n        'objective': 'multiclass',\n        'metric': 'multi_logloss',\n        'num_class': n_labels,\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n    }\n\n    model = lgb.train(param, \n              dtrain,\n              valid_sets=d_eval,\n              early_stopping_rounds=10)\n    pred_y = model.predict(X_train[valid_index, :])\n    oof[valid_index] = pred_y.argmax(1)\n    \noof = oof.astype(int)\nprint(\"---------------------------\")\nscore = f1_score(y_train, oof, average='micro')\nprint(f\"F1 micro = {score:0.4}\")","255ae120":"load meta data","b40039de":"conversion year data.\n\nit has mistake labels like '0199'.\n","4f9ce639":"Frequent authors are featured.\n\nInfrequent authors are grouped into -1","a4669909":"make target data","0e6476c5":"Train by LGBM","6e06527c":"In this notebook I will try to predict birds only with metadata.\n\nThis is an experiment I did for my interest, so it doesn't contribute to raising the ranking of LB.","a4a46b50":"make feature values","98e6e220":"conversion year data.\n\nit has mistake labels like 'xx'."}}