{"cell_type":{"23843829":"code","2a940c3b":"code","de4f56ea":"code","8fccce33":"code","aa60c1f6":"code","76d6b898":"code","bd0b0bca":"code","fdcb13ac":"code","0bad5cfa":"code","aec58688":"code","bf0834e8":"code","269e2f21":"code","52221c27":"code","581961af":"code","fcc249bb":"code","b0f3ad5a":"code","25f31e71":"code","f0771d0e":"code","7ac9fc41":"code","316017ed":"code","d6977820":"code","398437c5":"code","a2909cbe":"code","e2d4b6ff":"code","44acd579":"code","2d6d0c33":"code","44f69602":"code","806a2d85":"code","6c08efa4":"code","406829c4":"code","8ee75fdb":"code","4756bf75":"code","724928c6":"code","933748f4":"code","78cee497":"code","55ff21e1":"code","99a68519":"code","f8b853e3":"code","7216cffb":"code","2a21757e":"code","fc21bf0c":"code","2aa45798":"code","fe987ded":"code","db0ef19d":"code","5b9f846d":"code","7b8c9437":"code","4c81349b":"code","628138e7":"code","cac02bd5":"code","34478f5a":"code","c0907b1a":"code","3e61f92c":"code","67945c99":"code","307a398b":"code","0569a879":"code","0c1a973c":"code","164785ae":"code","ec06b24e":"code","8e38c538":"code","9a0db1bc":"code","f5fe204b":"code","0184ec1a":"code","2816cc9a":"markdown","7ad55019":"markdown","e8f60b31":"markdown","0068a3b4":"markdown","e8c680df":"markdown","34937912":"markdown","ac31eb71":"markdown"},"source":{"23843829":"from fastai.text import *\nimport pandas\nimport fastText as ft\nimport pickle","2a940c3b":"path = Config().data_path()","de4f56ea":"with open ('..\/input\/final-data\/finallist.pickle', 'rb') as my_file:\n    en_pt = pickle.load(my_file)","8fccce33":"labels = ['en', 'pt']\n\ndf =  pd.DataFrame.from_records(en_pt, columns=labels)\ndf.head","aa60c1f6":"df['en'] = df['en'].apply(lambda x:x.lower())\ndf['pt'] = df['pt'].apply(lambda x:x.lower())","76d6b898":"def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n    \"Function that collect samples and adds padding. Flips token order if needed\"\n    samples = to_data(samples)\n    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n    if backwards: pad_first = not pad_first\n    for i,s in enumerate(samples):\n        if pad_first: \n            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n        else:         \n            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n    return res_x,res_y","bd0b0bca":"class Seq2SeqDataBunch(TextDataBunch):\n    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n    @classmethod\n    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n               pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n        val_bs = ifnone(val_bs, bs)\n        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs\/\/2)\n        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n        dataloaders = [train_dl]\n        for ds in datasets[1:]:\n            lengths = [len(t) for t in ds.x.items]\n            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)","fdcb13ac":"class Seq2SeqTextList(TextList):\n    _bunch = Seq2SeqDataBunch\n    _label_cls = TextList","0bad5cfa":"src = Seq2SeqTextList.from_df(df, path = path, cols='pt').split_by_rand_pct().label_from_df(cols='en', label_cls=TextList)","aec58688":"np.percentile([len(o) for o in src.train.x.items] + [len(o) for o in src.valid.x.items], 90)","bf0834e8":"np.percentile([len(o) for o in src.train.y.items] + [len(o) for o in src.valid.y.items], 90)","269e2f21":"src = src.filter_by_func(lambda x,y: len(x) > 50 or len(y) > 50)","52221c27":"data = src.databunch()","581961af":"data.show_batch()","fcc249bb":"pt_vecs = ft.load_model('..\/input\/fasttext_pt\/cc.pt.300.bin')","b0f3ad5a":"def create_emb(vecs, itos, em_sz=300, mult=1.):\n    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n    wgts = emb.weight.data\n    vec_dic = {w:vecs.get_word_vector(w) for w in vecs.get_words()}\n    miss = []\n    for i,w in enumerate(itos):\n        try: wgts[i] = tensor(vec_dic[w])\n        except: miss.append(w)\n    return emb\n","25f31e71":"emb_enc = create_emb(pt_vecs, data.x.vocab.itos)","f0771d0e":"torch.save(emb_enc, 'pt_emb.pth')","7ac9fc41":"del pt_vecs","316017ed":"en_vecs = ft.load_model('..\/input\/fasttext-english\/cc.en.300.bin')","d6977820":"emb_dec = create_emb(en_vecs, data.y.vocab.itos)","398437c5":"torch.save(emb_enc, 'en_emb.pth')","a2909cbe":"del en_vecs","e2d4b6ff":"from fastai.text.models.qrnn import QRNN, QRNNLayer","44acd579":"class Seq2SeqQRNN(nn.Module):\n    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n        super().__init__()\n        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n        self.emb_enc = emb_enc\n        self.emb_enc_drop = nn.Dropout(p_inp)\n        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)\n        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)\n        self.hid_dp  = nn.Dropout(p_hid)\n        self.emb_dec = emb_dec\n        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)\n        self.out_drop = nn.Dropout(p_out)\n        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))\n        self.out.weight.data = self.emb_dec.weight.data\n        \n    def forward(self, inp):\n        bs,sl = inp.size()\n        self.encoder.reset()\n        self.decoder.reset()\n        hid = self.initHidden(bs)\n        emb = self.emb_enc_drop(self.emb_enc(inp))\n        enc_out, hid = self.encoder(emb, hid)\n        hid = self.out_enc(self.hid_dp(hid))\n\n        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n        outs = []\n        for i in range(self.max_len):\n            emb = self.emb_dec(dec_inp).unsqueeze(1)\n            out, hid = self.decoder(emb, hid)\n            out = self.out(self.out_drop(out[:,0]))\n            outs.append(out)\n            dec_inp = out.max(1)[1]\n            if (dec_inp==self.pad_idx).all(): break\n        return torch.stack(outs, dim=1)\n    \n    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)","2d6d0c33":"\ndef seq2seq_loss(out, targ, pad_idx=1):\n    bs,targ_len = targ.size()\n    _,out_len,vs = out.size()\n    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n    return CrossEntropyFlat()(out, targ)","44f69602":"def seq2seq_acc(out, targ, pad_idx=1):\n    bs,targ_len = targ.size()\n    _,out_len,vs = out.size()\n    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n    out = out.argmax(2)\n    return (out==targ).float().mean()","806a2d85":"class NGram():\n    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n    def __eq__(self, other):\n        if len(self.ngram) != len(other.ngram): return False\n        return np.all(np.array(self.ngram) == np.array(other.ngram))\n    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))","6c08efa4":"def get_grams(x, n, max_n=5000):\n    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]","406829c4":"def get_correct_ngrams(pred, targ, n, max_n=5000):\n    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)","8ee75fdb":"class CorpusBLEU(Callback):\n    def __init__(self, vocab_sz):\n        self.vocab_sz = vocab_sz\n        self.name = 'bleu'\n    \n    def on_epoch_begin(self, **kwargs):\n        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        last_output = last_output.argmax(dim=-1)\n        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n            self.pred_len += len(pred)\n            self.targ_len += len(targ)\n            for i in range(4):\n                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n                self.corrects[i] += c\n                self.counts[i]   += t\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        precs = [c\/t for c,t in zip(self.corrects,self.counts)]\n        len_penalty = exp(1 - self.targ_len\/self.pred_len) if self.pred_len < self.targ_len else 1\n        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n        return add_metrics(last_metrics, bleu)","4756bf75":"\nemb_enc = torch.load('pt_emb.pth')\nemb_dec = torch.load('en_emb.pth')","724928c6":"model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)\nlearn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))])","933748f4":"learn.lr_find()","78cee497":"learn.recorder.plot()","55ff21e1":"learn.fit_one_cycle(8, 1e-2)","99a68519":"def get_predictions(learn, ds_type=DatasetType.Valid):\n    learn.model.eval()\n    inputs, targets, outputs = [],[],[]\n    with torch.no_grad():\n        for xb,yb in progress_bar(learn.dl(ds_type)):\n            out = learn.model(xb)\n            for x,y,z in zip(xb,yb,out):\n                inputs.append(learn.data.train_ds.x.reconstruct(x))\n                targets.append(learn.data.train_ds.y.reconstruct(y))\n                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))\n    return inputs, targets, outputs","f8b853e3":"inputs, targets, outputs = get_predictions(learn)","7216cffb":"for i in range(705,710):\n    print(inputs[i], targets[i], outputs[i], sep = '\\n')\n    print('\\n')","2a21757e":"class TeacherForcing(LearnerCallback):\n    \n    def __init__(self, learn, end_epoch):\n        super().__init__(learn)\n        self.end_epoch = end_epoch\n    \n    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        if train: return {'last_input': [last_input, last_target]}\n    \n    def on_epoch_begin(self, epoch, **kwargs):\n        self.learn.model.pr_force = 1 - 0.5 * epoch\/self.end_epoch","fc21bf0c":"class Seq2SeqQRNN(nn.Module):\n    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n        super().__init__()\n        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n        self.emb_enc = emb_enc\n        self.emb_enc_drop = nn.Dropout(p_inp)\n        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)\n        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)\n        self.hid_dp  = nn.Dropout(p_hid)\n        self.emb_dec = emb_dec\n        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)\n        self.out_drop = nn.Dropout(p_out)\n        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))\n        self.out.weight.data = self.emb_dec.weight.data\n        self.pr_force = 0.\n        \n    def forward(self, inp, targ=None):\n        bs,sl = inp.size()\n        hid = self.initHidden(bs)\n        emb = self.emb_enc_drop(self.emb_enc(inp))\n        enc_out, hid = self.encoder(emb, hid)\n        hid = self.out_enc(self.hid_dp(hid))\n\n        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n        res = []\n        for i in range(self.max_len):\n            emb = self.emb_dec(dec_inp).unsqueeze(1)\n            outp, hid = self.decoder(emb, hid)\n            outp = self.out(self.out_drop(outp[:,0]))\n            res.append(outp)\n            dec_inp = outp.data.max(1)[1]\n            if (dec_inp==self.pad_idx).all(): break\n            if (targ is not None) and (random.random()<self.pr_force):\n                if i>=targ.shape[1]: break\n                dec_inp = targ[:,i]\n        return torch.stack(res, dim=1)\n    \n    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)","2aa45798":"emb_enc = torch.load('pt_emb.pth')\nemb_dec = torch.load('en_emb.pth')","fe987ded":"model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)\nlearn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],\n                callback_fns=partial(TeacherForcing, end_epoch=8))","db0ef19d":"learn.fit_one_cycle(8, 1e-2)","5b9f846d":"inputs, targets, outputs = get_predictions(learn)","7b8c9437":"for i in range(705,710):\n    print(inputs[i], targets[i], outputs[i], sep = '\\n')\n    print('\\n')","4c81349b":"class Seq2SeqQRNN(nn.Module):\n    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n        super().__init__()\n        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n        self.emb_enc = emb_enc\n        self.emb_enc_drop = nn.Dropout(p_inp)\n        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc, bidirectional=True)\n        self.out_enc = nn.Linear(2*n_hid, emb_enc.weight.size(1), bias=False)\n        self.hid_dp  = nn.Dropout(p_hid)\n        self.emb_dec = emb_dec\n        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)\n        self.out_drop = nn.Dropout(p_out)\n        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))\n        self.out.weight.data = self.emb_dec.weight.data\n        self.pr_force = 0.\n        \n    def forward(self, inp, targ=None):\n        bs,sl = inp.size()\n        hid = self.initHidden(bs)\n        emb = self.emb_enc_drop(self.emb_enc(inp))\n        enc_out, hid = self.encoder(emb, hid)\n        \n        hid = hid.view(2,self.n_layers, bs, self.n_hid).permute(1,2,0,3).contiguous()\n        hid = self.out_enc(self.hid_dp(hid).view(self.n_layers, bs, 2*self.n_hid))\n\n        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n        res = []\n        for i in range(self.max_len):\n            emb = self.emb_dec(dec_inp).unsqueeze(1)\n            outp, hid = self.decoder(emb, hid)\n            outp = self.out(self.out_drop(outp[:,0]))\n            res.append(outp)\n            dec_inp = outp.data.max(1)[1]\n            if (dec_inp==self.pad_idx).all(): break\n            if (targ is not None) and (random.random()<self.pr_force):\n                if i>=targ.shape[1]: break\n                dec_inp = targ[:,i]\n        return torch.stack(res, dim=1)\n    \n    def initHidden(self, bs): return one_param(self).new_zeros(2*self.n_layers, bs, self.n_hid)","628138e7":"emb_enc = torch.load('pt_emb.pth')\nemb_dec = torch.load('en_emb.pth')","cac02bd5":"model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)\nlearn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],\n                callback_fns=partial(TeacherForcing, end_epoch=8))","34478f5a":"learn.lr_find()","c0907b1a":"learn.recorder.plot()","3e61f92c":"learn.fit_one_cycle(8, 1e-2)","67945c99":"inputs, targets, outputs = get_predictions(learn)","307a398b":"for i in range(705,710):\n    print(inputs[i], targets[i], outputs[i], sep = '\\n')\n    print('\\n')","0569a879":"class Seq2SeqQRNN(nn.Module):\n    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n        super().__init__()\n        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n        self.emb_enc = emb_enc\n        self.emb_enc_drop = nn.Dropout(p_inp)\n        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)\n        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)\n        self.hid_dp  = nn.Dropout(p_hid)\n        self.emb_dec = emb_dec\n        emb_sz = emb_dec.weight.size(1)\n        self.decoder = QRNN(emb_sz + n_hid, emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)\n        self.out_drop = nn.Dropout(p_out)\n        self.out = nn.Linear(emb_sz, emb_dec.weight.size(0))\n        self.out.weight.data = self.emb_dec.weight.data #Try tying\n        self.enc_att = nn.Linear(n_hid, emb_sz, bias=False)\n        self.hid_att = nn.Linear(emb_sz, emb_sz)\n        self.V =  init_param(emb_sz)\n        self.pr_force = 0.\n        \n    def forward(self, inp, targ=None):\n        bs,sl = inp.size()\n        hid = self.initHidden(bs)\n        emb = self.emb_enc_drop(self.emb_enc(inp))\n        enc_out, hid = self.encoder(emb, hid)\n        hid = self.out_enc(self.hid_dp(hid))\n\n        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n        res = []\n        enc_att = self.enc_att(enc_out)\n        for i in range(self.max_len):\n            hid_att = self.hid_att(hid[-1])\n            u = torch.tanh(enc_att + hid_att[:,None])\n            attn_wgts = F.softmax(u @ self.V, 1)\n            ctx = (attn_wgts[...,None] * enc_out).sum(1)\n            emb = self.emb_dec(dec_inp)\n            outp, hid = self.decoder(torch.cat([emb, ctx], 1)[:,None], hid)\n            outp = self.out(self.out_drop(outp[:,0]))\n            res.append(outp)\n            dec_inp = outp.data.max(1)[1]\n            if (dec_inp==self.pad_idx).all(): break\n            if (targ is not None) and (random.random()<self.pr_force):\n                if i>=targ.shape[1]: break\n                dec_inp = targ[:,i]\n        return torch.stack(res, dim=1)\n    \n    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)","0c1a973c":"emb_enc = torch.load('pt_emb.pth')\nemb_dec = torch.load('en_emb.pth')","164785ae":"model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)\nlearn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],\n                callback_fns=partial(TeacherForcing, end_epoch=8))","ec06b24e":"learn.lr_find()","8e38c538":"learn.recorder.plot()","9a0db1bc":"learn.fit_one_cycle(8, 1e-2)","f5fe204b":"inputs, targets, outputs = get_predictions(learn)","0184ec1a":"for i in range(705,710):\n    print(inputs[i], targets[i], outputs[i], sep = '\\n')\n    print('\\n')","2816cc9a":"Now we'll try with a bidirectional model.","7ad55019":"The kernel will crash if we try to have pt_vecs and en_vecs loaded at the same time, which is why we need to do this sequentially. ","e8f60b31":"Now we try with attention","0068a3b4":"We finally have some data. Let's filter out the really long stuff.","e8c680df":"Convert the list of tuples into a dataframe object","34937912":"Lowercase the text","ac31eb71":"Now let's uplad the pickle file we're working with, and set a default filepath"}}