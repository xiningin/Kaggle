{"cell_type":{"e0977e35":"code","1daf4cde":"code","38a2fbac":"code","43255a36":"code","c14e4671":"code","69a1139e":"code","3b71d6ee":"code","6db98feb":"code","0c5d4657":"code","7a0c5e34":"code","135bd265":"code","9484b06d":"code","30f4c13d":"code","095aea10":"code","d747a9f6":"code","8170285f":"code","cd61bae2":"code","04e9d1de":"code","f069f621":"code","77efcf20":"code","b306e33a":"code","253a3a29":"code","d077c55a":"code","53910a49":"code","b9204bdf":"code","51db07a4":"code","bd90366c":"code","ed6fbc4c":"code","85db8c44":"code","f9f4dfbb":"code","a4b159b8":"code","8b58eb2d":"code","7791ca03":"code","5d13291d":"code","0607cb41":"code","d89bbaaf":"code","fc7f5a7d":"code","ea0eda02":"code","408674eb":"code","b603cdf6":"code","078f5fb1":"code","5829d36d":"code","eb036724":"code","ebb25f44":"code","91689753":"code","b4ef915f":"code","de2b19eb":"code","c5672cb9":"code","eb2d8338":"code","1ef17295":"code","907a6f2c":"code","599ce546":"code","785d2f6f":"code","6de86d53":"code","303f4f24":"code","99930eba":"code","efeb626c":"code","448f5797":"code","bf93b5f9":"code","41779bf9":"code","bfda8987":"code","16f9ac90":"code","989086a8":"code","f768a5f0":"code","04e6948d":"code","302acc3e":"code","75ee0d50":"code","ede65f2c":"code","024dfee3":"code","e6d837e5":"code","76dc7449":"code","587159ae":"code","6fe9a499":"code","5ee9d431":"code","5a16e8e8":"code","ed86bfc9":"code","83dc0d32":"markdown","b6d6b555":"markdown","cc3cd0ed":"markdown","539393ab":"markdown","1d31562f":"markdown","affea78a":"markdown","1c3162c0":"markdown","035db94a":"markdown","573affbb":"markdown","d9336adf":"markdown","985cc049":"markdown","7de1d40a":"markdown","e97256db":"markdown","01f2a500":"markdown","e4b4ac6a":"markdown","dc1b7e29":"markdown","32cf2e29":"markdown","3993eca1":"markdown","1dc9b8f5":"markdown","84dac3d5":"markdown","89da67fe":"markdown","2f94dce1":"markdown","a36948a3":"markdown","6ccc56ad":"markdown","e5594f03":"markdown","fd16dd93":"markdown","c5789637":"markdown","2fb05b28":"markdown","4a07fccb":"markdown","0e1f35f0":"markdown","cc0276fb":"markdown","ee143687":"markdown","ad38b75d":"markdown","1b3ceb15":"markdown","ae3fea8c":"markdown","e10b984b":"markdown","2af294ae":"markdown","3a22edf2":"markdown","5ed9e9f6":"markdown","cc00d3ca":"markdown","1fc6b1fc":"markdown","11c81522":"markdown","edf261ca":"markdown","48ad1b4d":"markdown","d1e1ec62":"markdown","1d76386a":"markdown","58520a51":"markdown","10d179fd":"markdown","97c44669":"markdown","439c4ca4":"markdown","639c504d":"markdown","55ea485e":"markdown","8c034dc0":"markdown","6da10517":"markdown","b98d7f13":"markdown","175530e5":"markdown","1af188f7":"markdown","6bc35f30":"markdown","180b5065":"markdown","02d4422f":"markdown","c362186d":"markdown","b60057e9":"markdown","4f18636e":"markdown","c771c072":"markdown","484eff23":"markdown","5aa40077":"markdown"},"source":{"e0977e35":"\nimport requests\nimport tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2\nprint (\"all imported\")","1daf4cde":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())\napi_url = 'https:\/\/api.calorieninjas.com\/v1\/nutrition?query='\nquery = 'french fries'\nresponse = requests.get(api_url + query, headers={'X-Api-Key': 's9S3fiY9BWYZtpCebS71Rg==Qk1nbauJKhKE9k8R'})\nif response.status_code == requests.codes.ok:\n    print(response.text)\nelse:\n    print(\"Error:\", response.status_code, response.text)\n","38a2fbac":"%cd \/kaggle\/input\/food-101\/","43255a36":"# Helper function to download data and extract\ndef get_data_extract():\n  if \"food-101\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http:\/\/data.vision.ee.ethz.ch\/cvl\/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")","c14e4671":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()","69a1139e":"# Check the extracted dataset folder\n!ls food-101\/","3b71d6ee":"os.listdir('food-101\/images')","6db98feb":"os.listdir('food-101\/meta')","0c5d4657":"!head food-101\/meta\/train.txt","7a0c5e34":"!head food-101\/meta\/classes.txt","135bd265":"# Visualize the data, showing one image per class from 101 classes\nrows = 17\ncols = 6\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\ndata_dir = \"food-101\/images\/\"\nfoods_sorted = sorted(os.listdir(data_dir))\nfood_id = 0\nfor i in range(rows):\n  for j in range(cols):\n    try:\n      food_selected = foods_sorted[food_id] \n      food_id += 1\n    except:\n      break\n    if food_selected == '.DS_Store':\n        continue\n    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n    ax[i][j].imshow(img)\n    ax[i][j].set_title(food_selected, pad = 10)\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()\n# https:\/\/matplotlib.org\/users\/tight_layout_guide.html\n","9484b06d":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('\/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","30f4c13d":"# Prepare train dataset by copying images from food-101\/images to food-101\/train using the file train.txt\n%cd \/\nprint(\"Creating train data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/meta\/train.txt', '\/kaggle\/input\/food-101\/food-101\/images', 'train')","095aea10":"# Prepare test data by copying images from food-101\/images to food-101\/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/meta\/test.txt', '\/kaggle\/input\/food-101\/food-101\/images', 'test')","d747a9f6":"# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c","8170285f":"# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","cd61bae2":"# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list","04e9d1de":"foods_sorted","f069f621":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ","77efcf20":"# picking 3 food items and generating separate data folders for the same\nfood_list = ['apple_pie','pizza','omelette']\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'","b306e33a":"print(\"Creating train data folder with new classes\")\ndataset_mini(food_list, src_train, dest_train)","253a3a29":"print(\"Total number of samples in train folder\")\n\n!find train_mini -type d -or -type f -printf '.' | wc -c","d077c55a":"print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)","53910a49":"print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c","b9204bdf":"K.clear_session()\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 2250 #75750\nnb_validation_samples = 750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_3class.hdf5')\n","51db07a4":"class_map_3 = train_generator.class_indices\nclass_map_3","bd90366c":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","ed6fbc4c":"plot_accuracy(history,'FOOD101-Inceptionv3')\nplot_loss(history,'FOOD101-Inceptionv3')","85db8c44":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_3class.hdf5',compile = False)","f9f4dfbb":"def predict_class(model, images, show = True):\n  for img in images:\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255.                                      \n\n    pred = model.predict(img)\n    index = np.argmax(pred)\n    food_list.sort()\n    pred_value = food_list[index]\n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.title(pred_value)\n        plt.show()\n        print(pred_value)\n        \n    api_url = 'https:\/\/api.calorieninjas.com\/v1\/nutrition?query='\n    query = pred_value.replace(\"_\",\" \")\n    response = requests.get(api_url + query, headers={'X-Api-Key': 's9S3fiY9BWYZtpCebS71Rg==Qk1nbauJKhKE9k8R'})\n    if response.status_code == requests.codes.ok:\n        print(response.text)\n    else:\n        print(\"Error:\", response.status_code, response.text)\n        \n    ","a4b159b8":"# Downloading images from internet using the URLs\n!wget -O samosa.jpg http:\/\/veggiefoodrecipes.com\/wp-content\/uploads\/2016\/05\/lentil-samosa-recipe-01.jpg\n!wget -O applepie.jpg https:\/\/acleanbake.com\/wp-content\/uploads\/2017\/10\/Paleo-Apple-Pie-with-Crumb-Topping-gluten-free-grain-free-dairy-free-15.jpg\n!wget -O pizza.jpg https:\/\/media.glassdoor.com\/l\/78\/17\/b3\/69\/pizza.jpg\n!wget -O omelette.jpg https:\/\/www.seriouseats.com\/2020\/06\/20200602-western-denver-omelette-daniel-gritzer-8.jpg\n\n# If you have an image in your local computer and want to try it, uncomment the below code to upload the image files\n\n# from google.colab import files\n# image = files.upload()","8b58eb2d":"# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('applepie.jpg')\nimages.append('pizza.jpg')\nimages.append('omelette.jpg')\nimages.append('samosa.jpg')\npredict_class(model_best, images, True)\n\n\n\n\n","7791ca03":"# Helper function to select n random food classes\ndef pick_n_random_classes(n):\n  food_list = []\n  random_food_indices = random.sample(range(len(foods_sorted)),n) # We are picking n random food classes\n  for i in random_food_indices:\n    food_list.append(foods_sorted[i])\n  food_list.sort()\n  return food_list\n  ","5d13291d":"# Lets try with more classes than just 3. Also, this time lets randomly pick the food classes\nn = 11\nfood_list = pick_n_random_classes(n)\nfood_list = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', 'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara', 'strawberry_shortcake']\nprint(\"These are the randomly picked food classes we will be training the model on...\\n\", food_list)","0607cb41":"# Create the new data subset of n classes\nprint(\"Creating training data folder with new classes...\")\ndataset_mini(food_list, src_train, dest_train)","d89bbaaf":"print(\"Total number of samples in train folder\")\n!find train_mini -type d -or -type f -printf '.' | wc -c","fc7f5a7d":"print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)","ea0eda02":"print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c","408674eb":"# Let's use a pretrained Inceptionv3 model on subset of data with 11 food classes\nK.clear_session()\n\nn_classes = n\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 8250 #75750\nnb_validation_samples = 2750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_11class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_11class.log')\n\nhistory_11class = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_11class.hdf5')\n","b603cdf6":"class_map_11 = train_generator.class_indices\nclass_map_11","078f5fb1":"plot_accuracy(history_11class,'FOOD101-Inceptionv3')\nplot_loss(history_11class,'FOOD101-Inceptionv3')","5829d36d":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_11class.hdf5',compile = False)\n\n    ","eb036724":"# Downloading images from internet using the URLs\n!wget -O fries.jpg https:\/\/wallpapercave.com\/wp\/wp3031767.jpg\n!wget -O springrolls.jpg https:\/\/howtofeedaloon.com\/wp-content\/uploads\/2016\/02\/bibimbap-1.jpg\n!wget -O pizza.jpg https:\/\/i.pinimg.com\/originals\/43\/0f\/83\/430f83bfa304c69f4f6c96abbb38223e.jpg\n!wget -O garlicbread.jpg https:\/\/tableagent.s3.amazonaws.com\/media\/crumbs\/xl\/229_93.jpg\n\n# If you have an image in your local computer and want to tr\n\n\n# from google.colab import files\n# image = files.upload()","ebb25f44":"# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('fries.jpg')\nimages.append('pizza.jpg')\nimages.append('springrolls.jpg')\nimages.append('garlicbread.jpg')\npredict_class(model_best, images, True)","91689753":"# Load the saved model trained with 3 classes\nK.clear_session()\nprint(\"Loading the model..\")\nmodel = load_model('best_model_3class.hdf5',compile = False)\nprint(\"Done!\")\n","b4ef915f":"model.summary()","de2b19eb":"def deprocess_image(x):\n    # normalize tensor: center on 0., ensure std is 0.1\n    x -= x.mean()\n    x \/= (x.std() + 1e-5)\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n","c5672cb9":"def generate_pattern(layer_name, filter_index, size=150):\n    # Build a loss function that maximizes the activation\n    # of the nth filter of the layer considered.\n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n\n    # Compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, model.input)[0]\n\n    # Normalization trick: we normalize the gradient\n    grads \/= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n\n    # This function returns the loss and grads given the input picture\n    iterate = K.function([model.input], [loss, grads])\n    \n    # We start from a gray image with some noise\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n\n    # Run gradient ascent for 40 steps\n    step = 1.\n    for i in range(40):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)","eb2d8338":"def get_activations(img, model_activations):\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255. \n    plt.imshow(img[0])\n    plt.show()\n    return model_activations.predict(img)\n    ","1ef17295":"def show_activations(activations, layer_names):\n    \n    images_per_row = 16\n\n    # Now let's display our feature maps\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # This is the number of features in the feature map\n        n_features = layer_activation.shape[-1]\n\n        # The feature map has shape (1, size, size, n_features)\n        size = layer_activation.shape[1]\n\n        # We will tile the activation channels in this matrix\n        n_cols = n_features \/\/ images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # We'll tile each filter into this big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                 :, :,\n                                                 col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,\n                             row * size : (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    plt.show()\n\n","907a6f2c":"len(model.layers)","599ce546":"# We start with index 1 instead of 0, as input layer is at index 0\nlayers = [layer.output for layer in model.layers[1:11]]\n# We now initialize a model which takes an input and outputs the above chosen layers\nactivations_output = models.Model(inputs=model.input, outputs=layers)","785d2f6f":"layers","6de86d53":"layer_names = []\nfor layer in model.layers[1:11]:\n    layer_names.append(layer.name)\nprint(layer_names)","303f4f24":"food = 'applepie.jpg'\nactivations = get_activations(food,activations_output)","99930eba":"show_activations(activations, layer_names)","efeb626c":"food = 'pizza.jpg'\nactivations = get_activations(food,activations_output)","448f5797":"show_activations(activations, layer_names)","bf93b5f9":"# Get the index of activation_1 layer which has sparse activations\nind = layer_names.index('activation_1')\nsparse_activation = activations[ind]\na = sparse_activation[0, :, :, 13]\na","41779bf9":"all (np.isnan(a[j][k])  for j in range(a.shape[0]) for k in range(a.shape[1]))","bfda8987":"# Get the index of batch_normalization_1 layer which has sparse activations\nind = layer_names.index('batch_normalization_1')\nsparse_activation = activations[ind]\nb = sparse_activation[0, :, :, 13]\nb","16f9ac90":"first_convlayer_activation = activations[0]\nsecond_convlayer_activation = activations[3]\nthird_convlayer_activation = activations[6]\nf,ax = plt.subplots(1,3, figsize=(10,10))\nax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[0].axis('OFF')\nax[0].set_title('Conv2d_1')\nax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[1].axis('OFF')\nax[1].set_title('Conv2d_2')\nax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[2].axis('OFF')\nax[2].set_title('Conv2d_3')\n","989086a8":"def get_attribution(food):\n    img = image.load_img(food, target_size=(299, 299))\n    img = image.img_to_array(img) \n    img \/= 255. \n    f,ax = plt.subplots(1,3, figsize=(15,15))\n    ax[0].imshow(img)\n    \n    img = np.expand_dims(img, axis=0) \n    \n    preds = model.predict(img)\n    class_id = np.argmax(preds[0])\n    ax[0].set_title(\"Input Image\")\n    class_output = model.output[:, class_id]\n    last_conv_layer = model.get_layer(\"mixed10\")\n    \n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    \n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n    ax[1].imshow(heatmap)\n    ax[1].set_title(\"Heat map\")\n    \n    \n    act_img = cv2.imread(food)\n    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n    cv2.imwrite('classactivation.png', superimposed)\n    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n    ax[2].imshow(img_act)\n    ax[2].set_title(\"Class Activation\")\n    plt.show()\n    return preds","f768a5f0":"print(\"Showing the class map..\")\nprint(class_map_3)","04e6948d":"pred = get_attribution('applepie.jpg')\nprint(\"Here are softmax predictions..\",pred)","302acc3e":"pred = get_attribution('pizza.jpg')\nprint(\"Here are softmax predictions..\",pred)","75ee0d50":"!wget -O piepizza.jpg https:\/\/raw.githubusercontent.com\/theimgclist\/PracticeGround\/master\/Food101\/piepizza.jpg\n!wget -O piepizzas.png https:\/\/raw.githubusercontent.com\/theimgclist\/PracticeGround\/master\/Food101\/piepizzas.png\n!wget -O pizzapie.jpg https:\/\/raw.githubusercontent.com\/theimgclist\/PracticeGround\/master\/Food101\/pizzapie.jpg\n!wget -O pizzapies.png https:\/\/raw.githubusercontent.com\/theimgclist\/PracticeGround\/master\/Food101\/pizzapies.png    ","ede65f2c":"food = 'piepizza.jpg'\nactivations = get_activations(food,activations_output)","024dfee3":"show_activations(activations, layer_names)","e6d837e5":"pred = get_attribution('piepizza.jpg')\nprint(\"Here are softmax predictions..\",pred)","76dc7449":"food = 'pizzapie.jpg'\nactivations = get_activations(food,activations_output)","587159ae":"pred = get_attribution('pizzapie.jpg')\nprint(\"Here are softmax predictions..\",pred)","6fe9a499":"food = 'pizzapies.png'\nactivations = get_activations(food,activations_output)","5ee9d431":"pred = get_attribution('pizzapies.png')\nprint(\"Here are softmax predictions..\",pred)","5a16e8e8":"food = 'piepizzas.png'\nactivations = get_activations(food,activations_output)","ed86bfc9":"pred = get_attribution('piepizzas.png')\nprint(\"Here are softmax predictions..\",pred)","83dc0d32":"* **Human lives and Technology are blending more and more together**\n* **The rapid advancements in technology over the past few years can be attributed to how Neural Networks have evolved**\n* **Neural Networks and Deep Learning are now being used in so many fields and industries - healthcare, finance, retail, automative etc**\n* **Thanks to the Deep Learning libraries which enable us to develop applications\/models with few lines of code, which a decade ago only those with a lot of expertise and research could do**\n* **All of this calls for the need to understand how neural networks do what they do and how they do it**\n* **This has led to an active area of research - Neural Network Model Interpretability and Explainability**","b6d6b555":"*  **All the values in the above activation map from the layer batch_normalization_1 are negative**\n* **This activation in batch_normalization_1 is passed to the next layer activation_1 as input**\n* **As the name says, activation_1 is an activation layer and ReLu is the activation function used**\n* **ReLu takes an input value, returns 0 if its negative, the value otherwise**\n* **Since the input to activation array contains all negative values, the activation layer fills its activation map with all zeros for the index**\n* **Now we know why we have those 2 sparse activations in activation_1 layer**","cc3cd0ed":"### **Split the image data into train and test using train.txt and test.txt**","539393ab":"### **Model Explainability**","1d31562f":"**images** folder contains 101 folders with 1000 images  each  \nEach folder contains images of a specific food class","affea78a":"### Let us now check the model we trained and understand how it sees and classifies","1c3162c0":"### **Visualize the accuracy and loss plots**","035db94a":"**Visualize the activations of intermediate layers from layer 1 to 10**","573affbb":"* **Well, the model flipped its output too!**\n* **The model now thinks its an apple pie with 49.7% confidence and a pizza with 31.9%**","d9336adf":"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**\n* **Validation accuracy has been on the higher side than training accuracy for many epochs**\n* **This could be for several reasons:**\n  * We used a pretrained model trained on ImageNet which contains data from a variety of classes\n  * Using dropout can lead to a higher validation accuracy\n\n \n","985cc049":"* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**","7de1d40a":"* **Can we visualize the outputs of all the layers?**\n* **Yes, we can. But that gets too tedious**\n* **So, let's choose a few layers to visualize**","e97256db":"* **Yes!!! The model got them all right!!**","01f2a500":"### **Summary of the things I tried**\n* **This notebook is the refactored and organised version of all the experiments and training trials I made**\n* **I used this very useful Keras blog - https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html for reference**\n* **I spent considerable amount of time in fixing things even before getting to the model training phase**\n* **For example, it took some time to get the image visualization plots aligned withouth any overlap**\n* **It is easier to go through a notebook and understand code someone else has taken hours to finish**\n* **I started with VGG16 pretrained model. It did give good validation accuracy after training for few epochs**\n* **I then tried Inceptionv3. VGG was taking more time for each epoch and since inception was also giving good validation accuracy, I chose Inception over VGG**\n* **For data augmentation, I sticked to the transformations used in the above blog**\n* **I didnt use TTA except rescaling test images**\n* **Model explainability helped me understand many new things**\n* **Once we train a model, it is also necessary to try it in a way we know it may not work(like giving images with multiple classes of objects in it). This will lead to new observations, insights and maybe new conclusions if not inventions!**","e4b4ac6a":"![](https:\/\/images.deepai.org\/publication-preview\/visualizing-and-understanding-convolutional-networks-page-4-medium.jpg)","dc1b7e29":"* **Summary of the model gives us the list of all the layers in the network along with other useful details**","32cf2e29":"**The dataset being used is [Food 101](https:\/\/www.vision.ee.ethz.ch\/datasets_extra\/food-101\/)**\n* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**\n* **Each type of food has 750 training samples and 250 test samples**\n* **Note found on the webpage of the dataset :  **  \n***On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.***  \n* **The entire dataset is 5GB in size**","3993eca1":"* Keras and other Deep Learning libraries provide pretrained models  \n* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n* This helps in faster convergance and saves time and computation when compared to models trained from scratch","1dc9b8f5":"### **Visualize random image from each of the 101 classes**","84dac3d5":"* **Given an image with pizza and applepie, the model thinks its a pizza with 75.4% confidence and an applie pie with 18% confidence**\n* **Now let's flip the image vertically and see what the model does**","89da67fe":"**meta** folder contains the text files - train.txt and test.txt  \n**train.txt** contains the list of images that belong to training set  \n**test.txt** contains the list of images that belong to test set  \n**classes.txt** contains the list of all classes of food","2f94dce1":"* We now have train and test data ready  \n* But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  \n* To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  \n* Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2","a36948a3":"### **Predicting classes for new images from internet using the best trained model**","6ccc56ad":"* **We trained a model on 3 classes and tested it using new data**\n* ** The model was able to predict the classes of all three test images correctly**\n* **Will it be able to perform at the same level of accuracy for more classes?**\n* **FOOD-101 dataset has 101 classes of data**\n* ** Even with fine tuning using a pre-trained model, each epoch was taking more than an hour when all 101 classes of data is used(tried this on both Colab and on a Deep Learning VM instance with P100 GPU on GCP)**\n* **But to check how the model performs when more classes are included, I'm using the same model to fine tune and train on 11 randomly chosen classes**\n","e5594f03":"* **Defining some helper functions**","fd16dd93":"**Provide an input to the model and get the activations of all the 10 chosen layers**","c5789637":"###  **See how the class activation map looks for a different image**","2fb05b28":"![](https:\/\/s3-media2.fl.yelpcdn.com\/bphoto\/7BlRoSOG3AsAWHMPOaG7ng\/ls.jpg)","4a07fccb":"### **Fine tune Inceptionv3 model with 11 classes of data**","0e1f35f0":"### **References**\n* **Deep Learning with Python by Francois Cholett - must read really!**\n* [Building Powerful Image Classification Models](https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html)\n* [How Convolutional Neural Networks See the World](https:\/\/blog.keras.io\/how-convolutional-neural-networks-see-the-world.html)\n* [The Building Blocks of Interpretability](https:\/\/distill.pub\/2018\/building-blocks\/)\n* [Feature Visualization](https:\/\/distill.pub\/2017\/feature-visualization\/)","cc0276fb":"* **The above image is taken from the paper - [Visualizing and Understanding Convolutional Networks](https:\/\/arxiv.org\/abs\/1311.2901)**\n* **The image contains the features of a trained model along with the kind of objects they would detect**\n* **In the first row and first column, we have a grid of edge detecting features in layer 1 and some curve detectors in layer 2 in the 2nd column**\n* **The last column in 1st row are the kind of objects that get detected using those curvy features**\n* **With layer three in 2nd row, the model starts looking for patterns with edges and curves**\n* **The second column in second row contains examples of patterns that are detected in layer 3 of the model**\n* **With layer 4, the model starts detecting parts of object specific features and in layer 5 the model knows what's in the image**","ee143687":"**Check how many layers are in the trained model(this includes the 1st input layer as well)**","ad38b75d":"**Get the activations for a different input \/ food**","1b3ceb15":"### **Further Improvements**\n* **Try more augmentation on test images**\n* **Fine tune the model on the entire dataset(for a few epochs atleast)**\n* **Play with hyper parameters, their values and see how it impacts model performance**\n* **There is currently no implementation to handle out of distribution \/ no class scenario. Can try below methods:**\n    * Set a threshold for the class with highest score. When model gives prediction score below the threshold for its top prediction, the prediction can be classified as NO-CLASS \/ UNSEEN\n    * Add a new class called **NO-CLASS**, provide data from different classes other than those in the original dataset. This way the model also learns how to classify totally unseen\/unrelated data\n    * I am yet to try these methods and not sure about the results\n* **Recently published paper - [Rethinking ImageNet Pretraining](https:\/\/arxiv.org\/abs\/1811.08883 ), claims that training from random initialization instead of using pretrained weights is not only robust but also gives comparable results**\n* **Pre-trained models are surely helpful. They save a lot of time and computation. Yet, that shouldn't be the reason to not try to train a model from scratch**\n* **Time taking yet productive experiment would be to try and train a model on this dataset from scratch**\n* **Do more experiments with Model Interpretability and see what can be observed**","ae3fea8c":"### **Overview** \n* **Download and extract Food 101 dataset**\n* **Understand dataset structure and files** \n* **Visualize random image from each of the 101 classes**\n* **Split the image data into train and test using train.txt and test.txt**\n* **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**\n* **Fine tune Inception Pretrained model using Food 101 dataset**\n* **Visualize accuracy and loss plots**\n* **Predicting classes for new images from internet**\n* **Scale up and fine tune Inceptionv3 model with 11 classes of data**\n* **Model Explainability**\n* **Summary of the things I tried**\n* **Further improvements**\n* **Feedback**","e10b984b":"### **Look into the sparse activations in the layer activation_1**","2af294ae":"* **No surprise from model this time. We flipped the image but the model didnt flip its output**\n* **It's an apple pie again with 52% confidence**\n\n","3a22edf2":"* **Setting compile=False and clearing the session leads to faster loading of the saved model**\n* **Withouth the above addiitons, model loading was taking more than a minute!**","5ed9e9f6":"**Show the activation outputs of 1st, 2nd and 3rd Conv2D layer activations to compare how layers get abstract with depth**","cc00d3ca":"### **Lets see if we can break the model or see what it does when we surpise it with different data!**","1fc6b1fc":"* **This time it's applie pie with 73% and a pizza with 19% confidence**\n* **Let's try one last horizontal flip, this is the last really!**","11c81522":"### **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**","edf261ca":"* **We can see that the activation has all nan values(it was all zeros when executed outside Kaggle, Im yet to figure out why its showing all nan values here**\n* **To know why we have all zero\/nan values for this activation, lets visualize the activation at same index 13 from previous layer**","48ad1b4d":"* **Load the saved model and a test image**","d1e1ec62":"### **Understand dataset structure and files**","1d76386a":"* **The feature maps in the above activations are for a different input image**\n* **We see the same patterns discussed for the previous input image**\n* **It is interesting to see the blank\/sparse activations in the same layer(activation_1) and for same filters when a different image is passed to the network**\n* **Remember we used a pretrained Inceptionv3 model. All the filters that are used in different layers come from this pretrained model**","58520a51":"* **activations** contain the outputs of all the 10 layers which can be plotted and visualized","10d179fd":"### ANSWER \n* **Once we train a model, it is also necessary to try it in a way we know it may not work(like giving images with multiple classes of objects in it). This will lead to new observations, insights and maybe new conclusions if not inventions!**\n* **Flipping the image vertically has flipped the outputs whereas doing a horizontal flip didnt cause any change in output class**\n* **As a next step, we can try with more images, from different classes and check if the same pattern exists**","97c44669":"**Get the names of all the selected layers**","439c4ca4":"* We currently have a subset of dataset with 3 classes - samosa, pizza and omelette  \n* Use the below code to finetune Inceptionv3 pretrained model","639c504d":"### **Fine tune Inception Pretrained model using Food 101 dataset**","55ea485e":"* **In the above plot, we see on left the input image passed to the model, heat map in the middle and the class activation map on right**  \n* **Heat map gives a visual of what regions in the image were used in determining the class of the image**  \n* **Now it's clearly visible what a model looks for in an image if it has to be classified as an applepie!**","8c034dc0":"### **More surprise data to the model...**","6da10517":"### **Attribution**\n* **So far we were doing activation maps visualization**  \n* **This helps in understanding how the input is transformed from one layer to another as it goes through several operations**  \n* **At the end of training, we want the model to classify or detect objects based on features which are specific to the class**  \n* **For example, when training a dogs vs cats classifier, the model should detect dogs based on features relevant to dog but not cats**  \n* **To validate how model attributes the features to class output, we can generate heat maps using gradients to find out which regions in the input images were instrumental in determining the class**  ","b98d7f13":"# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**\n![alt text](https:\/\/www.vision.ee.ethz.ch\/datasets_extra\/food-101\/static\/img\/food-101.jpg)","175530e5":"* **What we see in the above plots are the activations or the outputs of each of the 11 layers we chose**  \n* **The activations or the outputs from the 1st layer(conv2d_1) don't lose much information of the original input**\n* **They are the results of applying several edge detecting filters on the input image**\n* **With each added layer, the activations lose visual\/input information and keeps building on the class\/ouput information**\n* **As the depth increases, the layers activations become less visually interpretabale and more abstract**\n* **By doing so, they learn to detect more specific features of the class rather than just edges and curves**\n* **We plotted just 10 out of 314 intermediate layers. We already have in these few layers, activations which are blank\/sparse(for ex: the 2 blank activations in the layer activation_1)**\n* **These blank\/sparse activations are caused when any of the filters used in that layer didn't find a matching pattern in the input given to it**\n* **By plotting more layers(specially those towards the end of the network), we can observe more of these sparse activations and how the layers get more abstract**\n","1af188f7":"* **Using feature visualization, we can know what a neural network layer and its features are looking for**\n* **Using attribution, we can understand how the features impact the output and what regions in the image led the model to the generated output**","6bc35f30":"### QUESTION!\n* **But hey, we did not train our model to do multi label classification. So why are we giving it input images with multiple classes?**","180b5065":"* **We can see how the heat map is different for a different image i.e the model looks for a totally different features\/regions if it has to classify it as a pizza**","02d4422f":"* **We trained our model to perform multi class classification and it seems to be doing well with >95% of accuracy**\n* **What will the model do when we give it an image which has more than one object that model is trained to classify?**","c362186d":"### **Feedback**\n* **Did you find any issues with the above code or have any suggestions or corrections?**\n* **There must be many ways to improve the model, its architecture, hyperparameters..**\n* **Please do let me know!**\n* **[Avinash Kappa](https:\/\/theimgclist.github.io\/)**\n* **[Twitter](https:\/\/twitter.com\/avinashso13)**\n* **[Linkedin](https:\/\/www.linkedin.com\/in\/avinash-kappa)**","b60057e9":"* **We have two blank\/sparse activations in layer 6**\n* **Below cell displays one of the sparse activations**","4f18636e":"**As seen below, the 10 chosen layers contain 3 convolution, 3 batch normalization, 3 activation and 1 max pooling layers**","c771c072":"* **The model did well even when the number of classes are increased to 11**\n* **Model training on all 101 classes takes some time**\n* **It was taking more than an hour for one epoch when the full dataset is used for fine tuning**","484eff23":"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**\n* **Validation accuracy has been on the higher side than training accuracy for many epochs**\n* **This could be for several reasons:**\n  * We used a pretrained model trained on ImageNet which contains data from a variety of classes\n  * Using dropout can lead to a higher validation accuracy \n","5aa40077":"* **Neural Networks learn incrementally**\n* **How does a neural network know what is in the image and how does it conclude that its a dog?**\n* **The best analogy to understand the incremental learning of the model here is to think about how we would hand sketch the dog**\n* **You can't start right away by drawing eyes, nose, snout etc**\n* **To have any of those dogly features, you need a lot of edges and curves**\n* **You start with edges\/lines, put many of them together**\n* **Use edges with curves to sketch patterns**\n* **The patterns with more finer details will help us draw the visible features of a dog like eyes, ears, snout etc**\n* **Neural networks adopt a very similar process when they are busy detecting what's in the provided data examples**"}}