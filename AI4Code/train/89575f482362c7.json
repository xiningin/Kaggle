{"cell_type":{"21edbe26":"code","80cadd95":"code","b54078a0":"code","e87018fa":"code","f05600ef":"code","44ef46e2":"code","c5f8fa14":"code","4e84b1d7":"code","92ffe33e":"code","36ee5a75":"code","0bc9a843":"code","56cdf0e3":"code","1978d3cc":"markdown","a58b808a":"markdown","54994ef5":"markdown","3ab596a9":"markdown","b2806de0":"markdown","aeb1bf04":"markdown","52a0065d":"markdown","98ee2d7e":"markdown","e857d381":"markdown","a4777743":"markdown","7c154034":"markdown","cc565b50":"markdown","0826d934":"markdown"},"source":{"21edbe26":"import tensorflow as tf\n## slim l\u00e0 package \u0111i k\u00e8m v\u1edbi tensorflow, gi\u00fap \u0111\u1ecbnh ngh\u0129a nhanh c\u00e1c lo\u1ea1i m\u00f4 h\u00ecnh deep learning\nimport tensorflow.contrib.slim as slim\nimport tensorflow.contrib.slim.nets\nfrom tensorflow.contrib.slim.nets import vgg \n## sklearn l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n r\u1ea5t ph\u1ed5 bi\u1ebfn trong ML, ch\u00fang ta ch\u1ec9 s\u1eed d\u1ee5ng tran_test_split \u0111\u1ec3 chia data th\u00e0nh 2 t\u1eadp\nfrom sklearn.model_selection import train_test_split\nimport json\n## th\u01b0 vi\u1ec7n t\u00ednh to\u00e1n tr\u00ean matrix\nimport numpy as np\nimport cv2\n# th\u01b0 vi\u1ec7n hi\u1ec3n th\u1ecb bi\u1ec3u \u0111\u1ed3\nimport matplotlib.pyplot as plt\nimport time","80cadd95":"# k\u00edch th\u01b0\u1edbc grid system \ncell_size = 7 \n# s\u1ed1 boundary box c\u1ea7n d\u1ef1 \u0111o\u00e1n m\u1ed7i \u00f4 vu\u00f4ng\nbox_per_cell = 2\n# k\u00edch th\u01b0\u1edbc \u1ea3nh \u0111\u1ea7u v\u00e0o\nimg_size = 224\n# s\u1ed1 lo\u1ea1i nh\u00e3n\nclasses = {'circle':0, 'triangle':1,  'rectangle':2}\nnclass = len(classes)\n\nbox_scale = 5.0\nnoobject_scale = 0.5\nbatch_size = 128\n# s\u1ed1 l\u1ea7n hu\u1ea5n luy\u1ec7n\n# th\u1eed thay \u0111\u1ed5i s\u1ed1 l\u1ea7n hu\u1ea5n luy\u1ec7n \u0111\u1ec3 cho k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n\nepochs = 100\n# learning c\u1ee7a ch\u00fang ta\nlr = 1e-3","b54078a0":"def load():\n    labels = json.load(open('..\/input\/train\/train\/labels.json'))\n    # s\u1ed1 l\u01b0\u01a1ng \u1ea3nh\n    N = len(labels)\n    # matrix ch\u1ee9a \u1ea3nh\n    X = np.zeros((N, img_size, img_size, 3), dtype='uint8')\n    # matrix ch\u1ee9a nh\u00e3n c\u1ee7a \u1ea3nh t\u01b0\u01a1ng \u1ee9ng\n    y = np.zeros((N,cell_size, cell_size, 5+nclass))\n    for idx, label in enumerate(labels):\n        img = cv2.imread(\"..\/input\/train\/train\/{}.png\".format(idx))\n        X[idx] = img\n        for box in label['boxes']:\n            x1, y1 = box['x1'], box['y1']\n            x2, y2 = box['x2'], box['y2']\n            # one-hot vector c\u1ee7a nh\u00e3n object\n            cl = [0]*len(classes)\n            cl[classes[box['class']]] = 1\n            # t\u00e2m c\u1ee7a boundary box\n            x_center, y_center, w, h = (x1+x2)\/2.0, (y1+y2)\/2.0, x2-x1, y2-y1\n            \n            # TODO \n            # t\u00ecm \u00f4 vu\u00f4ng tr\u00ean matrix 7x7 m\u00e0 t\u00e2m object thu\u1ed9c v\u1ec1\n            x_idx, y_idx =  int(x_center*cell_size\/img_size), int(y_center*cell_size\/img_size)\n                                                                  \n            # g\u00e1n nh\u00e3n v\u00e0o matrix \n            y[idx, y_idx, x_idx] = 1, x_center, y_center, w, h, *cl\n    \n    return X, y","e87018fa":"# ! wget --quiet --no-check-certificate 'https:\/\/docs.google.com\/uc?export=download&id=1rZufpopTpqjeMNGD1bHgORTw7WF4mmug' -O train.zip\n","f05600ef":"X, y = load()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)","44ef46e2":"def vgg16(inputs, is_training):\n    \"\"\"\u0111\u1ecbnh ngh\u0129a CNN\n    Args:\n      inputs: 5-D tensor [batch_size, width, height, 3]\n    Return:\n      iou: 4-D tensor [batch_size, 7, 7, 5*nbox + nclass]\n    \"\"\"\n    # kh\u00e1i b\u00e1o scope \u0111\u1ec3 c\u00f3 th\u00ea group nh\u1eefng bi\u1ebfn li\u00ean quan cho vi\u1ec7c visualize tr\u00ean tensorboard.\n    with tf.variable_scope(\"vgg_16\"):\n        with slim.arg_scope(vgg.vgg_arg_scope()):\n            # h\u00e0m repeat c\u00f3 t\u00e1c d\u1ee5ng l\u1eb7p l\u1ea1i t\u1ea7ng conv2d n l\u1ea7n m\u00e0 kh\u00f4ng ph\u1ea3i \u0111\u1ecbnh ngh\u0129a ph\u1ee9c t\u1ea1p. thank for slim package\n            # TODO\n            # s\u1eed d\u1ee5ng slim.repeat  v\u00e0 slim.max_pool2d \u0111\u1ec3 \u0111\u1ecbnh ngh\u0129a ph\u1ea7n c\u00f2n l\u1ea1i c\u1ee7a m\u00f4 h\u00ecnh CNN\n            # l\u01b0u \u00fd k\u00edch th\u01b0\u1edbc matrix output ph\u1ea3i l\u00e0 [batch_size, 7, 7, 5*nbox + nclass]\n            # thay \u0111\u1ed5i s\u1ed1 l\u01b0\u1ee3ng t\u1ea7ng c\u0169ng nh\u01b0 tham s\u1ed1 \u0111\u1ec3 \u0111\u1ea1t k\u1ebft qu\u1ea3 iou tr\u00ean t\u1eadp valid > 0.92\n            net = slim.repeat(inputs, 1, slim.conv2d, 16, [3, 3], scope='conv1')\n            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n            \n            \n            # thay v\u00ec s\u1eed d\u1ee5ng 2 t\u1ea7ng fully connected t\u1ea1i \u0111\u00e2y, \n            # ch\u00fang ta s\u1eed d\u1ee5ng conv v\u1edbi kernel_size = (1,1) c\u00f3 t\u00e1c d\u1ee5ng gi\u1ed1ng h\u1ec7t t\u1ea7ng fully conntected\n#             net = slim.conv2d(net, 512, [1, 1], scope='fc8')\n            net = slim.conv2d(net, 13, [1, 1], activation_fn=None, scope='fc9')\n    return net","c5f8fa14":"def compute_iou(boxes1, boxes2, scope='iou'):\n    \"\"\"calculate ious\n    Args:\n      boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n      boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n    Return:\n      iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n    \"\"\"\n    with tf.variable_scope(scope):\n        # transform (x_center, y_center, w, h) to (x1, y1, x2, y2)\n        boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] \/ 2.0,\n                             boxes1[..., 1] - boxes1[..., 3] \/ 2.0,\n                             boxes1[..., 0] + boxes1[..., 2] \/ 2.0,\n                             boxes1[..., 1] + boxes1[..., 3] \/ 2.0],\n                            axis=-1)\n\n        boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] \/ 2.0,\n                             boxes2[..., 1] - boxes2[..., 3] \/ 2.0,\n                             boxes2[..., 0] + boxes2[..., 2] \/ 2.0,\n                             boxes2[..., 1] + boxes2[..., 3] \/ 2.0],\n                            axis=-1)\n\n        # calculate the left up point & right down point\n        lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n        rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n\n        # intersection\n        intersection = tf.maximum(0.0, rd - lu)\n        inter_square = intersection[..., 0] * intersection[..., 1]\n\n        # calculate the boxs1 square and boxs2 square\n        square1 = boxes1[..., 2] * boxes1[..., 3]\n        square2 = boxes2[..., 2] * boxes2[..., 3]\n\n        union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n\n    return tf.clip_by_value(inter_square \/ union_square, 0.0, 1.0)    ","4e84b1d7":"def loss_layer(predicts, labels, scope='loss_layer'):\n    \"\"\"calculate loss function\n    Args:\n      predicts: 4-D tensor [batch_size, 7, 7, 5*nbox+n_class] \n      labels: 4-D tensor [batch_size, 7, 7, 5+n_class]\n    Return:\n      loss: scalar\n    \"\"\"\n    with tf.variable_scope(scope):\n        offset = np.transpose(np.reshape(np.array(\n            [np.arange(cell_size)] * cell_size * box_per_cell),\n            (box_per_cell, cell_size, cell_size)), (1, 2, 0))\n        offset = offset[None, :]\n        offset = tf.constant(offset, dtype=tf.float32)\n        offset_tran = tf.transpose(offset, (0, 2, 1, 3))\n        \n        # 2 ph\u1ea7n t\u1eed \u0111\u1ea7u c\u1ee7a vector d\u1ef1 \u0111o\u00e1n t\u1ea1i m\u1ed9t \u00f4 vu\u00f4ng l\u00e0 confidence score\n        predict_object = predicts[..., :box_per_cell]\n        \n        # 8 ph\u1ea7n t\u1eed ti\u1ebfp theo l\u00e0 d\u1ef1 \u0111o\u00e1n offset c\u1ee7a boundary box v\u00e0 width height\n        predict_box_offset = tf.reshape(predicts[...,box_per_cell:5*box_per_cell], (-1, cell_size, cell_size, box_per_cell, 4))\n        \n        # c\u00e1c ph\u1ea7n t\u1eed cu\u1ed1i l\u00e0 d\u1ef1 \u0111o\u00e1n l\u1edbp c\u1ee7a object\n        predict_class = predicts[...,5*box_per_cell:]\n        \n        # chuy\u1ec3n v\u1ecb tr\u00ed offset v\u1ec1 to\u1ea1 \u0111\u1ed9 normalize tr\u00ean kho\u1ea3ng [0-1]\n        predict_normalized_box = tf.stack(\n                                    [(predict_box_offset[..., 0] + offset) \/ cell_size,\n                                     (predict_box_offset[..., 1] + offset_tran) \/ cell_size,\n                                     tf.square(predict_box_offset[..., 2]),\n                                    tf.square(predict_box_offset[..., 3])], axis=-1)\n\n        # l\u1ea5y c\u00e1c nh\u00e3n t\u01b0\u01a1ng \u1ee9ng \n        true_object = labels[..., :1]\n        true_box = tf.reshape(labels[..., 1:5], (-1, cell_size, cell_size, 1, 4))\n        \n        # \u0111\u1ec3 normalize t\u1ecda \u0111\u1ed9 pixel v\u1ec1 \u0111o\u1ea1n [0-1] ch\u00fang ta chia cho img_size (224)\n        true_normalized_box = tf.tile(true_box, (1, 1, 1, box_per_cell, 1))\/img_size\n        true_class = labels[..., 5:]\n        \n        # t\u00ednh v\u1ecb tr\u00ed offset t\u1eeb nh\u00e3n \n        true_box_offset =  tf.stack(\n                                    [true_normalized_box[..., 0] * cell_size - offset,\n                                     true_normalized_box[..., 1] * cell_size - offset_tran,\n                                     tf.sqrt(true_normalized_box[..., 2]),\n                                     tf.sqrt(true_normalized_box[..., 3])], axis=-1)\n        \n        # t\u00ednh iou\n        predict_iou = compute_iou(true_normalized_box, predict_normalized_box)\n        \n        # mask ch\u1ee9a v\u1ecb tr\u00ed c\u00e1c \u00f4 vu\u00f4ng ch\u1ee9a object\n        object_mask = tf.reduce_max(predict_iou, 3, keepdims=True)  \n        \n        # t\u00ednh metric \u0111\u1ec3 monitor \n        iou_metric = tf.reduce_mean(tf.reduce_sum(object_mask, axis=[1,2,3])\/tf.reduce_sum(true_object, axis=[1,2,3]))\n        \n        object_mask = tf.cast((predict_iou>=object_mask), tf.float32)*true_object\n\n        noobject_mask = tf.ones_like(object_mask) - object_mask\n        \n        ## TODO t\u00ednh classification loss\n        ## class_delta l\u00e0 \u0111\u1ed9 ch\u00eanh l\u1ec7ch so v\u1edbi nh\u00e3n tr\u01b0\u1edbc khi b\u00ecnh ph\u01b0\u01a1ng, \n        ## l\u01b0u \u00fd ch\u1ec9 object_mask \u0111\u1ec3 ignore nh\u1eefng box kh\u00f4ng quan t\u00e2m\n        class_delta = true_object*(predict_class - true_class)\n        class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1,2,3]), name='class_loss')\n        \n        ## object loss\n        object_delta = object_mask*(predict_object - predict_iou)\n        \n        ## TODO t\u00ednh object loss t\u1eeb object_delta\n        object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1,2,3]), name='object_loss')\n        \n        ## TODO t\u00ednh noobject loss\n        noobject_delta = noobject_mask*predict_object\n        noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1,2,3]), name='noobject_loss')\n        \n        ## TODO t\u00ednh localization loss\n        box_mask = tf.expand_dims(object_mask, 4)\n        box_delta = box_mask*(predict_box_offset - true_box_offset)\n        box_loss = tf.reduce_mean(tf.reduce_sum(tf.square(box_delta), axis=[1,2,3]), name='box_loss')\n        \n        loss = 0.5*class_loss + object_loss + 0.1*noobject_loss + 10*box_loss\n        \n        return loss, iou_metric, predict_object, predict_class, predict_normalized_box","92ffe33e":"lr = 1e-3\nepochs = 150\nbatch_size = 128\ndef vgg16(inputs, is_training):\n    \"\"\"\u0111\u1ecbnh ngh\u0129a CNN\n    Args:\n      inputs: 5-D tensor [batch_size, width, height, 3]\n    Return:\n      iou: 4-D tensor [batch_size, 7, 7, 5*nbox + nclass]\n    \"\"\"\n    # kh\u00e1i b\u00e1o scope \u0111\u1ec3 c\u00f3 th\u00ea group nh\u1eefng bi\u1ebfn li\u00ean quan cho vi\u1ec7c visualize tr\u00ean tensorboard.\n    with tf.variable_scope(\"vgg_16\"):\n        with slim.arg_scope(vgg.vgg_arg_scope()):\n            net = slim.repeat(inputs, 2, slim.conv2d, 32, [3, 3], scope='conv1')\n            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n            \n            net = slim.repeat(net, 2, slim.conv2d, 64, [3, 3], scope='conv2')\n            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n            \n            net = slim.repeat(net, 3, slim.conv2d, 128, [3, 3], scope='conv3')\n            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n            \n            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv4')\n            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n            \n            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n            \n            net = slim.conv2d(net, 1024, [1, 1], scope='fc8')\n            net = slim.conv2d(net, 1024, [1, 1], scope='fc8')\n            net = slim.conv2d(net, 13, [1, 1], activation_fn=None, scope='fc9')\n    return net\n\ngraph = tf.Graph()\nwith graph.as_default():    \n    # None \u0111\u1ea1i di\u1ec7n cho batch_size, gi\u00fap batch_size c\u00f3 th\u1ec3 thay \u0111\u1ed5i linh ho\u1ea1t\n    images = tf.placeholder(\"float\", [None, img_size, img_size, 3], name=\"input\")\n    labels = tf.placeholder('float', [None, cell_size, cell_size, 8], name='label')\n    is_training = tf.placeholder(tf.bool)\n\n    logits = vgg16(images, is_training)\n    loss, iou_metric, predict_object, predict_class, predict_normalized_box = loss_layer(logits, labels)\n    \n    # \u0111\u1ecbnh ngh\u0129a adam optimizer, \u0111\u1ec3 t\u1ed1i \u01b0u h\u00e0m loss\n    optimizer = tf.train.AdamOptimizer(lr)\n    train_op = optimizer.minimize(loss)\n    \nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    # \u0111\u1ecbnh ngh\u0129a saver \u0111\u1ec3 l\u01b0u l\u1ea1i tr\u1ecdng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh, d\u00f9ng trong test c\u00e1c \u1ea3nh m\u1edbi\n    saver = tf.train.Saver(max_to_keep=1)\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        for batch in range(len(X_train)\/\/batch_size):\n            \n            # TODO \n            # l\u1ea5y t\u1eebng batch, forward, backward, c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 theo adam optimizer\n            \n            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n            train_total_loss, train_iou_m,_ = sess.run([loss, iou_metric, train_op], {images:X_batch, labels:y_batch, is_training:True})            \n        end_time = time.time()\n        \n        # t\u00ednh to\u00e1n loss, iou tr\u00ean t\u1eadp validation\n        val_loss = []\n        val_iou_ms = []\n        for batch in range(len(X_test)\/\/batch_size):\n            val_X_batch = X_test[batch*batch_size:(batch+1)*batch_size]\n            val_y_batch = y_test[batch*batch_size:(batch+1)*batch_size]\n            total_val_loss, val_iou_m, val_predict_object, val_predict_class, val_predict_normalized_box = sess.run([loss, iou_metric, predict_object, predict_class, predict_normalized_box], \n                                                 {images:val_X_batch, labels:val_y_batch, is_training:False})\n            val_loss.append(total_val_loss)\n            val_iou_ms.append(val_iou_m)\n            \n        saver.save(sess, '.\/model\/yolo', global_step=epoch)\n        print('epoch: {} - running_time: {:.0f}s - train_loss: {:.3f} - train_iou: {:.3f} - val_loss: {:.3f} - val_iou: {:.3f}'.format(epoch, end_time - start_time, train_total_loss, train_iou_m, np.mean(val_loss), np.mean(val_iou_ms)))","36ee5a75":"def iou(box1, box2):\n    \"\"\" t\u00ednh iou b\u1eb1ng numpy \n    Args:\n      box1: [center_x, center_y, w, h] \n      box2: [center_x, center_y, w, h] \n    Return:\n      iou: iou\n    \"\"\"\n    tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - \\\n        max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\n    lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - \\\n        max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\n    inter = 0 if tb < 0 or lr < 0 else tb * lr\n    return inter \/ (box1[2] * box1[3] + box2[2] * box2[3] - inter)\n    \ndef interpret_output(predict_object, predict_class, predict_normalized_box):\n    # nh\u1eadn l\u1ea1i img-size \u0111\u1ec3 ra kh\u00f4ng gian pixel\n    predict_box= predict_normalized_box*img_size\n    predict_object = np.expand_dims(predict_object, axis=-1)\n    predict_class = np.expand_dims(predict_class, axis=-2)\n    # x\u00e1c su\u1ea5t \u00f4 boundary ch\u1ee9a class b\u1eb1ng boundary ch\u1ee9a object * x\u00e1c su\u1ea5t c\u00f3 \u0111i\u1ec1u ki\u1ec7n c\u1ee7a l\u1edbp \u0111\u00f3 m\u00e0 \u00f4 vu\u00f4ng ch\u1ee9a object\n    class_probs = predict_object*predict_class\n    \n    # gi\u1eef c\u00e1c boundary box m\u00e0 c\u00f3 x\u00e1c su\u1ea5t ch\u1ee9a l\u1edbp >= 0.2\n    filter_mat_probs = np.array(class_probs >= 0.2, dtype='bool')\n    filter_mat_boxes = np.nonzero(filter_mat_probs)\n    boxes_filtered = predict_box[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n    class_probs_filtered = class_probs[filter_mat_probs]\n    \n    # ch\u1ecdn index c\u1ee7a l\u1edbp c\u00f3 x\u00e1c xu\u1ea5t l\u1edbp nh\u1ea5t l\u1ea1i m\u1ed7i boundary box\n    classes_num_filtered = np.argmax(\n        filter_mat_probs, axis=3)[\n        filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n\n    # gi\u1eef l\u1ea1i boundary box d\u1ef1 \u0111o\u00e1n c\u00f3 x\u00e1c xu\u1ea5t l\u1edbp nh\u1ea5t\n    argsort = np.array(np.argsort(class_probs_filtered))[::-1]\n    boxes_filtered = boxes_filtered[argsort]\n    class_probs_filtered = class_probs_filtered[argsort]\n    classes_num_filtered = classes_num_filtered[argsort]\n\n    # thu\u1eadt to\u00e1n non-maximun suppression\n    for i in range(len(boxes_filtered)):\n        if class_probs_filtered[i] == 0:\n            continue\n        for j in range(i + 1, len(boxes_filtered)):\n            if iou(boxes_filtered[i], boxes_filtered[j]) > 0.5:\n                class_probs_filtered[j] = 0.0\n                \n    # filter b\u01b0\u1edbc cu\u1ed1i b\u1ecf nh\u1eefng boundary overlap theo thu\u1eadt to\u00e1n tr\u00ean\n    filter_iou = np.array(class_probs_filtered > 0.0, dtype='bool')\n    boxes_filtered = boxes_filtered[filter_iou]\n    class_probs_filtered = class_probs_filtered[filter_iou]\n    classes_num_filtered = classes_num_filtered[filter_iou]\n\n    result = []\n    for i in range(len(boxes_filtered)):\n        result.append(\n            [classes_num_filtered[i],\n             boxes_filtered[i][0],\n             boxes_filtered[i][1],\n             boxes_filtered[i][2],\n             boxes_filtered[i][3],\n             class_probs_filtered[i]])\n\n    return result\n\ndef draw_result(img, result):\n    \"\"\" hi\u1ec3n th\u1ecb k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n\n    Args:\n      img: \u1ea3nh      \n      result: gi\u00e1 tr\u1ecb sinh ra \u1edf h\u00e0m tr\u00ean    \n    \"\"\"\n    plt.figure(figsize=(10,10), dpi=40)\n    img = np.pad(img, [(50,50), (50,50), (0,0)], mode='constant', constant_values=255)\n    for i in range(len(result)):\n        x = int(result[i][1])+50\n        y = int(result[i][2])+50\n        w = int(result[i][3] \/ 2)\n        h = int(result[i][4] \/ 2)\n        cv2.rectangle(img, (x - w, y - h), (x + w, y + h), (231, 76, 60), 2)\n        cv2.rectangle(img, (x - w, y - h - 20),\n                      (x -w + 50, y - h), (46, 204, 113), -1)\n        cv2.putText(\n            img, '{} : {:.2f}'.format(result[i][0] ,result[i][5]),\n            (x - w + 5, y - h - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.3,\n            (0, 0, 0), 1, cv2.LINE_AA)\n\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])","0bc9a843":"img_idx = 15\nresult = interpret_output(val_predict_object[img_idx], val_predict_class[img_idx], val_predict_normalized_box[img_idx])","56cdf0e3":"draw_result(val_X_batch[img_idx]*255, result)            ","1978d3cc":"Hi\u1ec3n th\u1ecb k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n c\u00f9ng boundary c\u1ee7a d\u1ef1 \u0111o\u00e1n m\u1edbi m\u00f4 h\u00ecnh","a58b808a":"### Download t\u1eadp d\u1eef li\u1ec7u tr\u00ean","54994ef5":"## Loss function\n\nCh\u00fang ta \u0111\u00e3 \u0111\u1ecbnh ngh\u0129a \u0111\u01b0\u1ee3c nh\u1eefng th\u00f4ng tin m\u00e0 m\u00f4 h\u00ecnh c\u1ea7n ph\u1ea3i d\u1ef1 \u0111o\u00e1n, v\u00e0 ki\u1ebfn tr\u00fac c\u1ee7a m\u00f4 h\u00ecnh CNN. B\u00e2y gi\u1edd l\u00e0 l\u00fac m\u00e0 ch\u00fang ta s\u1ebd \u0111\u1ecbnh ngh\u0129a h\u00e0m l\u1ed7i.\n\nYOLO s\u1eed d\u1ee5ng h\u00e0m \u0111\u1ed9 l\u1ed7i b\u00ecnh ph\u01b0\u01a1ng gi\u1eef d\u1ef1 \u0111o\u00e1n v\u00e0 nh\u00e3n \u0111\u1ec3 t\u00ednh \u0111\u1ed9 l\u1ed7i cho m\u00f4 h\u00ecnh. C\u1ee5 th\u1ec3, \u0111\u1ed9 l\u1ed7i t\u1ed5ng c\u1ee7a ch\u00fang ta s\u1ebd l\u00e0 t\u1ed5ng c\u1ee7a 3 \u0111\u1ed9 l\u1ed7i con sau:\n* \u0110\u1ed9 l\u1ed7i c\u1ee7a vi\u1ec7c d\u1eef \u0111o\u00e1n lo\u1ea1i nh\u00e3n c\u1ee7a object - Classifycation loss\n* \u0110\u1ed9 l\u1ed7i c\u1ee7a d\u1ef1 \u0111o\u00e1n t\u1ea1o \u0111\u1ed9 c\u0169ng nh\u01b0 chi\u1ec1u d\u00e0i, r\u1ed9ng c\u1ee7a boundary box - Localization loss\n* \u0110\u1ed9 l\u1ed7i c\u1ee7a \u00f4 vu\u00f4ng c\u00f3 ch\u1ee9a object n\u00e0o hay kh\u00f4ng - Confidence loss\n\nCh\u00fang ta mong mu\u1ed1n h\u00e0m l\u1ed7i c\u00f3 ch\u1ee9c n\u0103ng sau. Trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n, m\u00f4 h\u00ecnh s\u1ebd nh\u00ecn v\u00e0o nh\u1eefng \u00f4 vu\u00f4ng c\u00f3 ch\u1ee9a object. T\u0103ng classification score l\u1edbp \u0111\u00fang c\u1ee7a object \u0111\u00f3 l\u00ean. Sau \u0111\u00f3, c\u0169ng nh\u00ecn v\u00e0o \u00f4 vu\u00f4ng \u0111\u00f3, t\u00ecm boundary box t\u1ed1t nh\u1ea5t trong 2 boxes \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n. T\u0103ng localization score c\u1ee7a boundary box \u0111\u00f3 l\u00ean, thay \u0111\u1ed5i th\u00f4ng tin boundary box \u0111\u1ec3 g\u1ea7n \u0111\u00fang v\u1edbi nh\u00e3n. \u0110\u1ed1i v\u1edbi nh\u1eefng \u00f4 vu\u00f4ng kh\u00f4ng ch\u1ee9a object, gi\u1ea3m confidence score v\u00e0 ch\u00fang ta s\u1ebd kh\u00f4ng quan t\u00e2m \u0111\u1ebfn classification score v\u00e0 localization score c\u1ee7a nh\u1eefng \u00f4 vu\u00f4ng n\u00e0y.\n\nTi\u1ebfp theo, ch\u00fang ta s\u1ebd \u0111i l\u1ea7n l\u01b0\u1ee3t v\u00e0o chi ti\u1ebft \u00fd ngh\u0129a c\u1ee7a c\u00e1c \u0111\u1ed9 l\u1ed7i tr\u00ean.\n\n### Classification Loss\nCh\u00fang ta ch\u1ec9 t\u00ednh classification loss cho nh\u1eefng \u00f4 vu\u00f4ng \u0111\u01b0\u1ee3c \u0111\u00e1nh nh\u00e3n l\u00e0 c\u00f3 object. Classification loss t\u1ea1i nh\u1eefng \u00f4 vu\u00f4ng \u0111\u00f3 \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng \u0111\u1ed7 l\u1ed7i b\u00ecnh ph\u01b0\u01a1ng gi\u1eefa nh\u00e3n \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n v\u00e0 nh\u00e3n \u0111\u00fang c\u1ee7a n\u00f3.\n\n![classification_loss](https:\/\/github.com\/pbcquoc\/yolo\/raw\/master\/image\/classification_loss.png)\n\n![loss](https:\/\/pbcquoc.github.io\/images\/yolo_classification_loss.png)\n\nV\u00ed d\u1ee5, trong h\u00ecnh minh h\u1ecda \u1edf tr\u00ean, ch\u00fang ta c\u00f3 2 object t\u1ea1i \u00f4 vu\u00f4ng (d\u00f2ng,c\u1ed9t) l\u00e0 (2,1) v\u00e0 (3,4), ch\u1ee9a object l\u00e0 h\u00ecnh tam gi\u00e1c v\u00e0 h\u00ecnh t\u1ee9c gi\u00e1c \u0111\u1ec1u. \u0110\u1ed9 l\u1ed7i classification loss ch\u1ec9 t\u00ednh cho 2 object n\u00e0y m\u00e0 ko quan t\u00e2m \u0111\u1ebfn nh\u1eefng \u00f4 vu\u00f4ng kh\u00e1c. L\u00fac c\u00e0i \u0111\u1eb7t ch\u00fang ta c\u1ea7n l\u01b0u \u00fd ph\u1ea3i nh\u00e2n v\u1edbi m\u1ed9t mask \u0111\u1ec3 tri\u1ec7t ti\u00eau gi\u00e1 tr\u1ecb l\u1ed7i t\u1ea1i nh\u1eefng \u00f4 vu\u00f4ng ko quan t\u00e2m.\n\n### Localization Loss\nLocalization loss d\u00f9ng \u0111\u1ec3 t\u00ednh gi\u00e1 tr\u1ecb l\u1ed7i cho boundary box \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n bao g\u1ed3m offset x,y v\u00e0 chi\u1ec1u d\u00e0i, r\u1ed9ng so v\u1edbi nh\u00e3n ch\u00ednh x\u00e1c c\u1ee7a ch\u00fang ta. C\u00e1c b\u1ea1n n\u00ean l\u01b0u \u00fd r\u1eb1ng, ch\u00fang ta kh\u00f4ng t\u00ednh to\u00e1n tr\u1ef1c ti\u1ebfp gi\u00e1 tr\u1ecb l\u1ed7i n\u00e0y tr\u00ean k\u00edch th\u01b0\u1edbc c\u1ee7a \u1ea3nh m\u00e0 c\u1ea7n chu\u1ea9n d\u01b0\u1edbi k\u00ednh th\u01b0\u1edbc \u1ea3nh v\u1ec1 \u0111o\u1ea1n [0-1] \u0111\u1ed1i v\u1edbi t\u1ecda \u0111\u1ed9 \u0111i\u1ec3m t\u00e2m, v\u00e0 kh\u00f4ng d\u1eef \u0111o\u00e1n tr\u1ef1c ti\u1ebfp \u0111i\u1ec3m t\u00e2m m\u00e0 ph\u1ea3i d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb l\u1ec7ch offset x,y so v\u1edbi \u00f4 vu\u00f4ng t\u01b0\u01a1ng \u1ee9ng. Vi\u1ec7c chu\u1ea9n h\u00f3a k\u00edch th\u01b0\u1edbc \u1ea3nh v\u00e0 d\u1ef1 \u0111o\u00e1n offset l\u00e0m cho m\u00f4 h\u00ecnh nhanh h\u1ed9i t\u1ee5 h\u01a1n so v\u1edbi vi\u1ec7c d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb m\u1eb7c \u0111\u1ecbnh.\n![localization_loss](https:\/\/github.com\/pbcquoc\/yolo\/raw\/master\/image\/localization_loss.png)\n\n\n\u0110\u1ed9 l\u1ed7i localization loss \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng t\u1ed5ng \u0111\u1ed7 l\u1ed7i b\u00ecnh ph\u01b0\u01a1ng c\u1ee7a offsetx, offsety v\u00e0 chi\u1ec1u d\u00e0i, r\u1ed9ng tr\u00ean t\u1ea5t c\u1ea3 c\u00e1c \u00f4 vu\u00f4ng c\u00f3 ch\u1ee9a object. T\u1ea1i m\u1ed7i \u00f4 vu\u00f4ng \u0111\u00fang,ta ch\u1ecdn 1 boundary box c\u00f3 IOU (Intersect over union) t\u1ed1t nh\u1ea5t, r\u1ed3i sau \u0111\u00f3 t\u00ednh \u0111\u1ed9 l\u1ed7i theo c\u00e1c boundary box n\u00e0y. Theo h\u00ecnh m\u00ecnh h\u1ecda tr\u00ean ch\u00fang ta c\u00f3 4 boundary box t\u1ea1i \u00f4 vu\u00f4ng \u0111\u00fang c\u00f3 vi\u1ec1n m\u00e0u \u0111\u1ecf, ch\u00fang ta ch\u1ecdn 1 box t\u1ea1i m\u1ed7i \u00f4 vu\u00f4ng \u0111\u1ec3 t\u00ednh \u0111\u1ed9 l\u1ed7i. C\u00f2n box xanh \u0111\u01b0\u1ee3c b\u1ecf qua.\n\nLocalization loss l\u00e0 \u0111\u1ed9 l\u1ed7i quan tr\u1ecdng nh\u1ea5t trong 3 lo\u1ea1i \u0111\u1ed9 l\u1ed7i tr\u00ean. Do \u0111\u00f3, ta c\u1ea7n \u0111\u1eb7t tr\u1ecdng s\u1ed1 cao h\u01a1n cho \u0111\u1ed9 l\u1ed7i n\u00e0y.\n\n### Confidence Loss\nConfidence loss th\u1ec3 hi\u1ec7n \u0111\u1ed9 l\u1ed7i gi\u1eefa d\u1ef1 \u0111o\u00e1n boundary box \u0111\u00f3 ch\u1ee9a object so v\u1edbi nh\u00e3n th\u1ef1c t\u1ebf t\u1ea1i \u00f4 vu\u00f4ng \u0111\u00f3. \u0110\u1ed9 l\u1ed7i n\u00e0y t\u00ednh n\u00ean c\u1ea3 nh\u1eefng \u00f4 vu\u00f4ng ch\u1ee9a object v\u00e0 kh\u00f4ng ch\u1ee9a object.\n\n![confidence_loss](https:\/\/github.com\/pbcquoc\/yolo\/raw\/master\/image\/confidence_loss.png)\n\n\u0110\u1ed9 l\u1ed7i n\u00e0y l\u00e0 \u0111\u1ed9 l\u1ed7i b\u00ecnh ph\u01b0\u1eddng c\u1ee7a d\u1ef1 \u0111o\u00e1n boundary \u0111\u00f3 ch\u1ee9a object v\u1edbi nh\u00e3n th\u1ef1c t\u1ebf c\u1ee7a \u00f4 vu\u00f4ng t\u1ea1i v\u1ecb tr\u00ed t\u01b0\u01a1ng \u1ee9ng, ch\u00fang ta l\u01b0u \u00fd r\u1eb1ng, \u0111\u1ed9 l\u1ed7i t\u1ea1i \u00f4 vu\u00f4ng m\u00e0 nh\u00e3n ch\u1ee9a object quan tr\u1ecdng h\u01a1n l\u00e0 \u0111\u1ed9 l\u1ed7i t\u1ea1i \u00f4 vu\u00f4ng kh\u00f4ng ch\u1ee9a object, do \u0111\u00f3 ch\u00fang ta c\u1ea7n s\u1eed d\u1ee5ng h\u1ec7 s\u1ed1 lambda \u0111\u1ec3 c\u00e2n b\u1eb1ng \u0111i\u1ec1u n\u00e0y.\n\n### Total loss\nT\u1ed5ng k\u1ebft l\u1ea1i, t\u1ed5ng l\u1ed7i c\u1ee7a ch\u00fang ta s\u1ebd b\u1eb1ng t\u1ed5ng c\u1ee7a 3 lo\u1ea1i \u0111\u1ed9 l\u1ed7i tr\u00ean\n![total_loss](https:\/\/github.com\/pbcquoc\/yolo\/raw\/master\/image\/total_loss.png)\n","3ab596a9":"### Grid System\nM\u1ed9t trong nh\u1eefng kh\u00e1i ni\u1ec7m quan tr\u1ecdng nh\u1ea5t c\u1ee7a YOLO l\u00e0 grid System. C\u1ee5 th\u1ec3, \u1ea3nh \u0111\u01b0\u1ee3c chia th\u00e0nh ma tr\u1eadn \u00f4 vu\u00f4ng 7x7, m\u1ed7i \u00f4 vu\u00f4ng bao g\u1ed3m m\u1ed9t t\u1eadp c\u00e1c th\u00f4ng tin m\u00e0 m\u00f4 h\u00ecnh ph\u1ea3i d\u1eef \u0111o\u00e1n.\n![grid system](https:\/\/pbcquoc.github.io\/images\/yolo_grid_system.png)\n\n* \u0110\u1ed1i t\u01b0\u1ee3ng duy nh\u1ea5t m\u00e0 \u00f4 vu\u00f4ng \u0111\u00f3 ch\u1ee9a. T\u00e2m c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng c\u1ea7n x\u00e1c \u0111\u1ecbnh n\u1eb1m trong \u00f4 vu\u00f4ng n\u00e0o th\u00ec \u00f4 vu\u00f4ng \u0111\u00f3 ch\u1ee9a \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u00f3. V\u00ed d\u1ee5 t\u00e2m c\u1ee7a c\u00f4 g\u00e1i n\u1eb1m trong \u00f4 vu\u00f4ng m\u00e0u xanh, do \u0111\u00f3 m\u00f4 h\u00ecnh ph\u1ea3i d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c nh\u00e3n c\u1ee7a \u00f4 vu\u00f4ng \u0111\u00f3 l\u00e0 c\u00f4 g\u00e1i. L\u01b0u \u00fd, cho d\u00f9 ph\u1ea7n \u1ea3nh c\u00f4 g\u00e1i c\u00f3 n\u1eb1m \u1edf \u00f4 vu\u00f4ng kh\u00e1c m\u00e0 t\u00e2m kh\u00f4ng thu\u1ed9c \u00f4 vu\u00f4ng \u0111\u00f3 th\u00ec v\u1eabn kh\u00f4ng t\u00ednh l\u00e0 ch\u1ee9a c\u00f4 g\u00e1i, ngo\u00e0i ra, n\u1ebfu c\u00f3 nhi\u1ec1u t\u00e2m n\u1eb1m trong m\u1ed9t \u00f4 vu\u00f4ng th\u00ec ch\u00fang ta v\u1eabn ch\u1ec9 g\u00e1n m\u1ed9t nh\u00e3n cho \u00f4 vu\u00f4ng \u0111\u00f3 th\u00f4i. Ch\u00ednh r\u00e0ng bu\u1ed9t m\u1ed7i \u00f4 vu\u00f4ng ch\u1ec9 ch\u1ee9a m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng l\u00e0 nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh n\u00e0y. N\u00f3 l\u00e0m cho ta kh\u00f4ng th\u1ec3 detect nh\u1eefng object c\u00f3 t\u1ea7m n\u1eb1m c\u00f9ng m\u1ed9t \u00f4 vu\u00f4ng. Tuy nhi\u00ean ch\u00fang ta c\u00f3 th\u1ec3 t\u0103ng grid size t\u1eeb 7x7 l\u00ean k\u00edch th\u01b0\u1edbc l\u1edbn h\u01a1n \u0111\u1ec3 c\u00f3 th\u1ec3 detect \u0111\u01b0\u1ee3c nhi\u1ec1u object h\u01a1n. Ngo\u00e0i ra, k\u00edch th\u01b0\u1edbc c\u1ee7a \u1ea3nh \u0111\u1ea7u v\u00e0o ph\u1ea3i l\u00e0 b\u1ed9i s\u1ed1 c\u1ee7a grid size.\n* M\u1ed7i \u00f4 vu\u00f4ng ch\u1ecbu tr\u00e1ch nhi\u1ec7m d\u1ef1 \u0111o\u00e1n 2 boundary box c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng. M\u1ed7i boundary box d\u1eef \u0111o\u00e1n c\u00f3 ch\u1ee9a object hay kh\u00f4ng v\u00e0 th\u00f4ng tin v\u1ecb tr\u00ed c\u1ee7a boundary box g\u1ed3m trung t\u00e2m boundary box c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 chi\u1ec1u d\u00e0i, r\u1ed9ng c\u1ee7a boundary box \u0111\u00f3. V\u00ed v\u1ee5 \u00f4 vu\u00f4ng m\u00e0u xanh c\u1ea7n d\u1ef1 \u0111o\u00e1n 2 boundary box ch\u1ee9a c\u00f4 g\u00e1i nh\u01b0 h\u00ecnh minh h\u1ecda \u1edf d\u01b0\u1edbi. M\u1ed9t \u0111i\u1ec1u c\u1ea7n l\u01b0u \u00fd, l\u00fac c\u00e0i \u0111\u1eb7t ch\u00fang ta kh\u00f4ng d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb pixel m\u00e0 c\u1ea7n ph\u1ea3i chu\u1ea9n h\u00f3a k\u00edch th\u01b0\u1edbc \u1ea3nh v\u1ec1 \u0111o\u1ea1n t\u1eeb [0-1] v\u00e0 d\u1ef1 \u0111o\u00e1n \u0111\u1ed9 l\u1ec7ch c\u1ee7a t\u00e2m \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u1ebfn box ch\u1ee9a \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u00f3. V\u00ed d\u1ee5, ch\u00fang ta thay v\u00ec d\u1eef \u0111o\u00e1n v\u1ecb tr\u00ed pixel c\u1ee7a \u0111i\u1ec3m m\u00e0u \u0111\u1ecf, th\u00ec c\u1ea7n d\u1ef1 \u0111o\u00e1n \u0111\u1ed9 l\u1ec7ch a,b trong \u00f4 vu\u00f4ng ch\u1ee9a t\u00e2m object.\n\n![2box](https:\/\/pbcquoc.github.io\/images\/yolo_2box.png)\n\nT\u1ed5ng h\u1ee3p l\u1ea1i, v\u1edbi m\u1ed7i \u00f4 vu\u00f4ng ch\u00fang ta c\u1ea7n d\u1eef \u0111o\u00e1n c\u00e1c th\u00f4ng tin sau :\n\n* \u00d4 vu\u00f4ng c\u00f3 ch\u1ee9a \u0111\u1ed1i t\u01b0\u1ee3ng n\u00e0o hay kh\u00f4ng?\n* D\u1ef1 \u0111o\u00e1n \u0111\u1ed9 l\u1ec7ch 2 box ch\u1ee9a object so v\u1edbi \u00f4 vu\u00f4ng hi\u1ec7n t\u1ea1i\n* L\u1edbp c\u1ee7a object \u0111\u00f3\nNh\u01b0 v\u1eady v\u1edbi m\u1ed7i \u00f4 vu\u00f4ng ch\u00fang ta c\u1ea7n d\u1eef \u0111o\u00e1n m\u1ed9t vector c\u00f3 (nbox+4*nbox+nclass) chi\u1ec1u. V\u00ed d\u1ee5, ch\u00fang ta c\u1ea7n d\u1ef1 \u0111o\u00e1n 2 box, v\u00e0 3 l\u1edbp \u0111\u1ed1i v\u1edbi m\u1ed7i \u00f4 vu\u00f4ng th\u00ec ch\u00fang s\u1ebd c\u00f3 m\u1ed9t ma tr\u1eadn 3 chi\u1ec1u 7x7x13 ch\u1ee9a to\u00e0n b\u1ed9 th\u00f4ng tin c\u1ea7n thi\u1ebft.\n\n![label](https:\/\/pbcquoc.github.io\/images\/yolo_predict_vector.png)","b2806de0":"## \u0110\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh CNN\nCh\u00fang ta \u0111\u00e3 c\u1ea7n bi\u1ebft ph\u1ea3i d\u1ef1 \u0111o\u00e1n nh\u1eefng th\u00f4ng tin n\u00e0o \u0111\u1ed1i v\u1edbi m\u1ed7i \u00f4 vu\u00f4ng, \u0111i\u1ec1u quan tr\u1ecdng ti\u1ebfp theo l\u00e0 x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh CNN c\u00f3 cho ra ouput v\u1edbi shape ph\u00f9 h\u1ee3p theo y\u00eau c\u1ea7u c\u1ee7a ch\u00fang ta, t\u1ee9c l\u00e0 gridsize x gridsize x (nbox+4*nbox+nclass). V\u00ed d\u1ee5 v\u1edbi gridsize l\u00e0 7x7 l\u00e0 m\u1ed7i \u00f4 vu\u00f4ng d\u1ef1 \u0111o\u00e1n 2 boxes, v\u00e0 c\u00f3 3 lo\u1ea1i object t\u1ea5t c\u1ea3 th\u00ec ch\u00fang ta ph\u1ea3i c\u1ea7n output c\u00f3 shape 7x7x13 t\u1eeb m\u00f4 h\u00ecnh CNN\n\n![cnn](https:\/\/pbcquoc.github.io\/images\/yolo_cnn.jpeg)\n\nYOLO s\u1eed d\u1ee5ng linear regression \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n c\u00e1c th\u00f4ng tin \u1edf m\u1ed7i \u00f4 vu\u00f4ng. Do \u0111\u00f3, \u1edf layer cu\u1ed1i c\u00f9ng ch\u00fang ta s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng b\u1ea5t k\u00ec h\u00e0m k\u00edch ho\u1ea1t n\u00e0o c\u1ea3. V\u1edbi \u1ea3nh \u0111\u1ea7u v\u00e0o l\u00e0 448x448, m\u00f4 h\u00ecnh CNN c\u00f3 6 t\u1ea7ng max pooling v\u1edbi size 2x2 s\u1ebd gi\u1ea3m 64 l\u1ea7n k\u00edch th\u01b0\u1edbc \u1ea3nh xu\u1ed1ng c\u00f2n 7x7 \u1edf output \u0111\u1ea7u ra. \u0110\u1ed3ng th\u1eddi thay v\u00ec s\u1eed d\u1ee5ng t\u1ea7ng full connected \u1edf c\u00e1c t\u1ea7ng cu\u1ed1i c\u00f9ng, ch\u00fang ta c\u00f3 th\u1ec3 thay th\u1ebf b\u1eb1ng t\u1ea7ng 1x1 conv v\u1edbi 13 feature maps \u0111\u1ec3 output shape d\u1ec5 d\u00e0ng cho ra 7x7x13.","aeb1bf04":"## Training\nPh\u1ea7n n\u00e0y ch\u00fang ta s\u1ebd b\u1eaft \u0111\u1ea7u training, v\u00e0 t\u00ednh iou metric \u0111\u1ec3 quan s\u00e1t, iou nh\u1eadn gi\u00e1 tr\u1ecb t\u1eeb [0-1], s\u1ea5p x\u1ec9 1 l\u00e0 r\u1ea5t t\u1ed1t","52a0065d":"## Bi\u00ean d\u1ecbch graph\nPh\u1ea7n kh\u00f3 \u0111\u00e3 qua, ch\u00fang m\u1eebng c\u00e1c b\u1ea1n!!\n\nB\u00e2y gi\u1edd l\u00e0 bi\u00ean d\u1ecbch graph v\u00e0 \u0111\u1ecbnh ngh\u0129a optimizer","98ee2d7e":"## Load t\u1eadp d\u1eef li\u1ec7u\nCh\u00fang ta c\u00f3 25k m\u1eabu d\u1eef li\u1ec7u, m\u1ed7i \u1ea3nh c\u00f3 k\u00edch th\u01b0\u1edbc 224x224, v\u00e0 m\u1ed9t file json ch\u1ee9a nh\u00e3n c\u1ee7a c\u00e1c \u1ea3nh, g\u1ed3m t\u1eadp c\u00e1c object t\u01b0\u01a1ng \u1ee9ng v\u1edbi v\u1ecb tr\u00ed t\u1ea1o \u0111\u1ed9 c\u1ee7a boundary box ch\u1ee9a object.\n\n![dataset](https:\/\/raw.githubusercontent.com\/pbcquoc\/yolo\/master\/image\/dataset.png)\n\n\u0110\u1ec3 chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u ch\u00fang ta c\u1ea7n load t\u1ea5t c\u1ea3 c\u00e1c \u1ea3nh v\u00e0o b\u1ed9 nh\u1edb, chuy\u1ec3n th\u00f4ng tin t\u1ea1o \u0111\u1ed9 c\u1ee7a boundary box th\u00e0nh th\u00f4ng tin nh\u00e3n t\u01b0\u01a1ng \u1ee9ng v\u1edbi quy \u0111\u1ecbnh \u1edf tr\u00ean. ","e857d381":"Ch\u1ecdn m\u1ed9t t\u1eeb t\u1eadp validation, r\u1ed3i hi\u1ec3n th\u1ecb ","a4777743":"## H\u00e0m t\u00ednh IOU \nl\u00e0 h\u00e0m r\u1ea5t quan tr\u1ecdng c\u1ee7a m\u00f4 h\u00ecnh YOLO, gi\u00fap ch\u00fang ta \u0111\u00e1nh gi\u00e1 c\u00e1c boundary box \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n c\u00f3 ch\u00ednh x\u00e1c hay kh\u00f4ng\n\n![iou](https:\/\/pbcquoc.github.io\/images\/yolo_iou.png)","7c154034":"# H\u01b0\u1edbng d\u1eabn c\u00e0i \u0111\u1eb7t YOLO v1\n## Gi\u1edbi thi\u1ec7u\nTrong notebook n\u00e0y, m\u00ecnh s\u1ebd h\u01b0\u1edbng d\u1eabn c\u00e1c b\u1ea1n c\u00e0i \u0111\u1eb7t m\u00f4 h\u00ecnh YOLO v1 cho t\u1eadp d\u1eef li\u1ec7u m\u1eabu \u0111\u01b0\u1ee3c ph\u00e1t sinh. Sau khi l\u00e0m xong, c\u00e1c b\u1ea1n s\u1ebd n\u1eafm \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh CNN, v\u00e0 \u00fd ngh\u0129a h\u00e0m loss, c\u00e1ch c\u00e0i \u0111\u1eb7t, c\u0169ng nh\u01b0 qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n b\u1eb1ng tensorflow\n\nYou only look once (YOLO) l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh CNN \u0111\u1ec3 detect object m\u00e0 \u01b0u \u0111i\u1ec3m n\u1ed5i tr\u1ed9i l\u00e0 nhanh h\u01a1n nhi\u1ec1u so v\u1edbi nh\u1eefng m\u00f4 h\u00ecnh c\u0169. Th\u1eadm ch\u00ed c\u00f3 th\u1ec3 ch\u1ea1y t\u1ed1t tr\u00ean nh\u1eefng thi\u1ebft b\u1ecb IOT c\u00f3 c\u1ea5u h\u00ecnh y\u1ebfu nh\u01b0 raspberry pi.\n\n\u0110\u1ea7u v\u00e0o m\u00f4 h\u00ecnh l\u00e0 m\u1ed9t b\u1ee9c \u1ea3nh, \u0111\u1ed1i v\u1edbi b\u00e0i to\u00e1n object detection, ch\u00fang ta kh\u00f4ng ch\u1ec9 ph\u1ea3i ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c object tr\u00ean b\u1ee9c \u1ea3nh m\u00e0 c\u00f2n ph\u1ea3i \u0111\u1ecbnh v\u1ecb \u0111\u01b0\u1ee3c v\u1ecb tr\u00ed c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u00f3. Object Detection c\u00f3 kh\u00e1 nhi\u1ec1u \u1ee9ng d\u1ee5ng, v\u00ed d\u1ee5 nh\u01b0 h\u1ec7 th\u1ed1ng theo d\u00f5i ng\u01b0\u1eddi d\u1eabn c\u1ee7a Trung Qu\u1ed1c, t\u1eeb \u0111\u00f3 c\u00f3 th\u1ec3 gi\u00fap ch\u00ednh quy\u1ec1n x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c t\u1ed9i ph\u1ea1m l\u1eabn tr\u1ed1n \u1edf \u0111\u00f3 hay kh\u00f4ng, ho\u1eb7c h\u1ec7 th\u1ed1ng xe t\u1ef1 l\u00e1i, c\u0169ng ph\u1ea3i x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c ng\u01b0\u1eddi \u0111i \u0111\u01b0\u1eddng \u1edf \u0111\u00e2u t\u1eeb \u0111\u00f3 \u0111\u01b0a ra quy\u1ebft \u0111\u1ecbnh di chuy\u1ec3n ti\u1ebfp theo\n\n![yolo](https:\/\/pbcquoc.github.io\/images\/yolo_example.png)\n\nC\u00f3 m\u1ed9t s\u1ed1 h\u01b0\u1edbng ti\u1ebfp c\u1eadn \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1, \u0111\u1ed3ng th\u1eddi m\u1ed7i l\u1ea7n ch\u1ea1y t\u1ed1n r\u1ea5t nhi\u1ec1u th\u1eddi gian. M\u00ecnh s\u1ebd li\u1ec7t k\u00ea ra \u0111\u1ec3 c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 n\u1eafm \u0111\u01b0\u1ee3c \u00fd t\u01b0\u1edfng \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e0n object detection.\n\n* Chia \u1ea3nh th\u00e0nh nhi\u1ec1u box, m\u1ed7i box c\u00e1c b\u1ea1n s\u1ebd detect object trong box \u0111\u00f3. V\u1ecb tr\u00ed c\u1ee7a object ch\u00ednh l\u00e0 t\u1ea1o \u0111\u1ed9 c\u1ee7a box \u0111\u00f3.\n* Thay v\u00ec chia th\u00e0nh t\u1eebng box, ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng m\u1ed9t thu\u1eadt to\u00e1n \u0111\u1ec3 l\u1ef1a ch\u1ecdn nh\u1eefng region \u1ee9ng vi\u00ean (v\u00ed d\u1ee5 nh\u01b0 l\u00e0 thu\u1eadt to\u00e1n Selective Search), c\u00e1c v\u00f9ng \u1ee9ng vi\u00ean n\u00e0y c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 t\u01b0\u1edfng nh\u01b0 l\u00e0 nh\u1eefng v\u00f9ng li\u00ean th\u00f4ng v\u1edbi nhau tr\u00ean k\u00eanh m\u00e0u RGB, sau \u0111\u00f3 v\u1edbi m\u1ed7i v\u00f9ng \u1ee9ng vi\u00ean n\u00e0y, ch\u00fang ta d\u00f9ng model \u0111\u1ec3 ph\u00e2n lo\u1ea1i object. Ch\u00fang ta c\u00f3 m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh x\u00e2y d\u1ef1ng theo ki\u1ec3u n\u00e0y nh\u01b0 RCNN, Fast RCNN v\u00e0 Faster RCNN.\n\nR\u1ea5t r\u00f5 r\u00e0ng, nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a c\u00e1c ph\u01b0\u01a1ng ph\u00e1p tr\u00ean l\u00e0 t\u1ed1n r\u1ea5t nhi\u1ec1u t\u00e0i nguy\u00ean \u0111\u1ec3 t\u00ednh to\u00e1n cho m\u1ecdi v\u00f9ng tr\u00ean m\u1ed9t b\u1ee9c \u1ea3nh,v\u00e0 do \u0111\u00f3 kh\u00f4ng th\u1ec3 ch\u1ea1y realtime tr\u00ean c\u00e1c thi\u1ebft b\u1ecb y\u1ebfu.\n\n## Import c\u00e1c th\u01b0 vi\u1ec7n\nThay v\u00ec khai b\u00e1o c\u00e1c bi\u1ebfn, \u0111\u1ecbnh ngh\u0129a c\u00e1c layer b\u1eb1ng tensorflow m\u1ed9t c\u00e1ch chi ti\u1ebft, ch\u00fang ta s\u1eed d\u1ee5ng high level api c\u1ee7a trong g\u00f3i slim \u0111\u1ec3 \u0111\u1ecbnh g\u1ecdi CNN m\u1ed9t c\u00e1ch nhanh g\u1ecdn h\u01a1n.","cc565b50":"## Chia t\u1eadp d\u1eef li\u1ec7u th\u00e0nh train\/val\ntrain_test_split gi\u00fap chia d\u1eef li\u1ec7u th\u00e0nh 2 t\u1eadp train v\u00e0 test trong m\u1ed9t n\u1ed1t nh\u1ea1c. T\u1eadp train d\u00f9ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh, t\u1eadp test d\u00f9ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh b\u1eb1ng c\u00e1c metric v\u00ed d\u1ee5 nh\u01b0 accuracy, iou, \u0111\u1ed3ng th\u1eddi quan s\u00e1t qu\u00e1 tr\u00ecnh overfit. random_state gi\u00fap reproduce k\u1ebft qu\u1ea3 c\u1ee7a nh\u1eefng lu\u1ea5n hu\u1ea5n luy\u1ec7n ti\u1ebfp theo.","0826d934":"## Hi\u1ec3n th\u1ecb k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n\nFilter t\u1ea5t c\u1ea3 c\u00e1c box kh\u00f4ng th\u1ecfa \u0111i\u1ec1u ki\u1ec7n nh\u01b0 ko ch\u1ee9a object, merge c\u00e1c box overlap nhi\u1ec1u"}}