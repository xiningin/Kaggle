{"cell_type":{"4d5ec70f":"code","1166376a":"code","b25f5bca":"code","7ef5c065":"code","fdf9392b":"code","eb01d496":"code","88ff9171":"code","f82b9688":"code","4a0ce427":"code","ea85d1f9":"code","26060869":"code","cf46b3f0":"code","5e73a902":"code","a63e5169":"code","4b9480f5":"code","222d445d":"code","4269453d":"code","2d8cf2f4":"code","6384af9a":"code","cdeb3da2":"code","3031d6fd":"code","0205088e":"code","71490cbd":"code","8a4df8c9":"code","cee0ede4":"code","dd0a217f":"code","e3823e66":"code","e9623b2a":"code","c22ee783":"code","a4d1c5dc":"code","88dbe1c3":"code","9a66b148":"code","722f57e0":"code","586c1669":"code","f515f0a2":"code","5d5c13d3":"code","d9aa1232":"code","b6709424":"code","39dece21":"code","5ef7acc7":"code","b50fa7a0":"code","a60f53da":"code","c1ba2427":"code","8c66ff22":"code","06c50451":"code","6a11f71f":"code","420477b3":"code","269f6241":"code","1c818a43":"code","c8966cfb":"code","5df1bd09":"code","2a12cc96":"code","97a03577":"code","d3824c09":"code","d728c54a":"code","c14bf493":"code","9b14922b":"code","36ff6b86":"code","b3a4ebb1":"code","f3ce25b9":"code","ac4b932b":"code","64feb66d":"code","72627d80":"code","9c9a619c":"code","1b895c08":"code","44bd8710":"code","1dd97a87":"code","b129eb8d":"code","2ecf229a":"code","aa033682":"code","041a8ee1":"code","95fb5188":"markdown","3347c6de":"markdown","32fc7a7c":"markdown","4c9f2589":"markdown","0c2044df":"markdown","8dfefa58":"markdown","41f303c8":"markdown","d816c48e":"markdown","ff658d80":"markdown","41b5a12a":"markdown","ff7d04b4":"markdown","8f5a6155":"markdown","5002f2cf":"markdown","1972ce1a":"markdown"},"source":{"4d5ec70f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1166376a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","b25f5bca":"train.head()","7ef5c065":"print(train.dtypes)","fdf9392b":"train.shape","eb01d496":"train.columns","88ff9171":"print(train.nunique())","f82b9688":"train.describe()","4a0ce427":"train.info()","ea85d1f9":"import seaborn as sns\nimport matplotlib.pyplot as plt","26060869":"numeric_variables = [\"PassengerId\", \"Pclass\", \"Survived\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncategorical_variables = [\"Name\" , \"Sex\", \"Ticket\", \"Embarked\"]","cf46b3f0":"sns.barplot(x = \"Pclass\" , y = \"Survived\" , data = train)\n#Passenger class 1 people has survived more","5e73a902":"Pclass1=train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize=True)[1]*100\nPclass2=train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize=True)[1]*100\nPclass3=train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize=True)[1]*100\nPclass_Values=[Pclass1,Pclass2,Pclass3]\nplt.pie(Pclass_Values, labels=('Pclass1','Pclass2','Pclass3') ,explode=(0.1,0.0,0.0), autopct='%1.1f%%')","a63e5169":"sns.barplot(x = \"Sex\" , y = \"Survived\" , data = train)\n#females has survived more","4b9480f5":"Females=train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize=True)[1]*100\nmales=train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize=True)[1]*100\nSex_values=[males,Females]\nplt.pie(Sex_values, labels=('males','Females'),explode=(0.0,0.1),autopct='%1.1f%%')","222d445d":"sns.barplot(x = \"SibSp\" , y = \"Survived\" , data = train)\n#people with 1 siblings survived more","4269453d":"a = train[\"Survived\"][train['SibSp'] == 0].value_counts(normalize=True)[1]*100\nb = train[\"Survived\"][train['SibSp'] == 1].value_counts(normalize=True)[1]*100\nc = train[\"Survived\"][train['SibSp'] == 2].value_counts(normalize=True)[1]*100\nd = train[\"Survived\"][train['SibSp'] == 3].value_counts(normalize=True)[1]*100\ne = train[\"Survived\"][train['SibSp'] == 4].value_counts(normalize=True)[1]*100\nSibling_values=[a,b,c,d,e]\nplt.pie(Sibling_values, labels=('SibSp0','SibSp1','SibSp2','SibSp3','SibSp4'),explode=(0.1,0.1,0.1,0.1,0.1), autopct='%1.1f%%')","2d8cf2f4":"sns.barplot(x = \"Parch\" , y = \"Survived\" , data = train)\n#people with parch = 3 survived more","6384af9a":"P0=train[\"Survived\"][train[\"Parch\"] == 0].value_counts(normalize=True)[1]*100\nP1=train[\"Survived\"][train[\"Parch\"] == 1].value_counts(normalize=True)[1]*100\nP2=train[\"Survived\"][train[\"Parch\"] == 2].value_counts(normalize=True)[1]*100\nP3=train[\"Survived\"][train[\"Parch\"] == 3].value_counts(normalize=True)[1]*100\nP5=train[\"Survived\"][train[\"Parch\"] == 5].value_counts(normalize=True)[1]*100\nParch_values=[P0,P1,P2,P3,P5]\nplt.pie(Parch_values, labels=('Parch0','Parch1','Parch2','Parch3','Parch5'),explode=(0.0,0.0,0.0,0.1,0.0),autopct='%1.1f%%')","cdeb3da2":"train.isnull().sum()","3031d6fd":"missing_value_percentage = print(train.isnull().sum()\/len(train))\nmissing_value_percentage","0205088e":"train = train.drop([\"Cabin\", \"Name\", \"Fare\", \"Ticket\"], axis = 1)","71490cbd":"train.head()","8a4df8c9":"print(train.isnull().sum())\n\n#lets impute other missing values ","cee0ede4":"#impute age with mean as it is numeric \n#impute embarked with mode as it is categorical\ntrain['Age'] = train['Age'].fillna(train['Age'].mean())\ntrain[\"Embarked\"] = pd.Categorical(train[\"Embarked\"])\ntrain[\"Embarked\"] = train[\"Embarked\"].cat.codes\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(train[\"Embarked\"].mode())\nprint(train.isnull().sum())\n","dd0a217f":"train[\"Embarked\"] = train[\"Embarked\"].astype(\"object\")\nprint(train.dtypes)","e3823e66":"train.head()","e9623b2a":"train[\"Sex\"] = pd.Categorical(train[\"Sex\"])\ntrain[\"Sex\"] = train[\"Sex\"].cat.codes\ntrain[\"Sex\"] = train[\"Sex\"].astype(\"object\")\n\n#male = 1, female = 0","c22ee783":"train.head()","a4d1c5dc":"train.shape","88dbe1c3":"train.describe()","9a66b148":"numeric_variables2 = [\"Pclass\", \"Survived\", \"Age\",]\ncategorical_variables2 = [ \"Sex\", \"Embarked\"]","722f57e0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in numeric_variables2 :\n    print(i)\n    sns.boxplot(y = train[i])\n    plt.xlabel(i)\n    plt.ylabel(\"Values\")\n    plt.title(\"Boxplot of \" + i)\n    plt.show()","586c1669":"# Identify outliers\n#calculate Inner Fence, Outer Fence, and IQR\n\nfor i in numeric_variables2:\n    print(i)\n    q75, q25 = np.percentile(train.loc[:,i], [75, 25])\n    iqr = q75 - q25\n    Innerfence = q25 - (iqr*1.5)\n    Upperfence = q75 + (iqr*1.5)\n    print(\"Innerfence= \"+str(Innerfence))\n    print(\"Upperfence= \"+str(Upperfence)) \n    print(\"IQR =\"+str(iqr))\n    \n\n# replace outliers with NA\n\n    train.loc[train[i]<Innerfence, i] = np.nan\n    train.loc[train[i]>Upperfence, i] = np.nan","f515f0a2":"print(train.isnull().sum()\/len(train))","5d5c13d3":"#impute age with mean\ntrain['Age'] = train['Age'].fillna(train['Age'].mean())\nprint(train.isnull().sum())        ","d9aa1232":"train.head()","b6709424":"train.dtypes","39dece21":"numeric_variables3 = [\"PassengerId\", \"Survived\", \"Pclass\", \"Age\", \"SibsSp\", \"Parch\"]","5ef7acc7":"Correlation = train.loc[:, numeric_variables3]\ncorrelation_result = Correlation.corr()\nprint(correlation_result)\n    ","b50fa7a0":"heatmap =  sns.heatmap(correlation_result)\n\n#No collinearity is found","a60f53da":"#Data Distribution\n\nx = train.drop([\"Survived\"], axis = 1)\ny = train[\"Survived\"]","c1ba2427":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split \n\n#divide the data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=0)","8c66ff22":"from sklearn.ensemble import RandomForestClassifier\n\nRM_model = RandomForestClassifier(n_jobs = 100, n_estimators = 100, random_state = 123)\nRM_model.fit(x_train, y_train)","06c50451":"RM_model.score(x_test, y_test)","6a11f71f":"y_predrm = RM_model.predict(x_test)\n\n#Accuracy\n\naccuracy_rm = round(accuracy_score(y_predrm, y_test)*100,2)\nprint(accuracy_rm)","420477b3":"from sklearn.metrics import classification_report  \nprint(classification_report(y_predrm, y_test))","269f6241":"from sklearn.metrics import confusion_matrix \nprint(confusion_matrix(y_predrm, y_test))","1c818a43":"from sklearn.linear_model import LogisticRegression\nLR_Model = LogisticRegression()\nLR_Model.fit(x_train, y_train)","c8966cfb":"LR_Model.score(x_test,y_test)","5df1bd09":"y_predlr = LR_Model.predict(x_test)\n\n#Accuracy\naccuracy_lr = round(accuracy_score(y_predlr, y_test) * 100, 2)\nprint(\"Accuracy:\",accuracy_lr)","2a12cc96":"print(classification_report(y_predlr, y_test))","97a03577":"print(confusion_matrix(y_predlr, y_test))","d3824c09":"test.head()","d728c54a":"#Drop unnecessary columns \ntest = test.drop(['Name','Ticket','Fare','Cabin'], axis=1)\ntest.head()","c14bf493":"#Convert datatype\ntest[\"Sex\"] = pd.Categorical(test[\"Sex\"])\ntest[\"Sex\"] = test[\"Sex\"].cat.codes\ntest[\"Sex\"] = test[\"Sex\"].astype(\"object\")\n\ntest[\"Embarked\"] = pd.Categorical(test[\"Embarked\"])\ntest[\"Embarked\"] = test[\"Embarked\"].cat.codes\ntest[\"Embarked\"] = test[\"Embarked\"].astype(\"object\")\n\ntest.head()\n\n","9b14922b":"#Check for NA values\ntest.isnull().sum()","36ff6b86":"test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())","b3a4ebb1":"test.isnull().sum()","f3ce25b9":"test.head()","ac4b932b":"test[\"Survived\"] = RM_model.predict(test)","64feb66d":"test.head()","72627d80":"predicted_test = test.drop(['Pclass','Sex','Age','SibSp', \"Parch\", \"Embarked\"], axis=1)\n","9c9a619c":"y_output = predicted_test[\"Survived\"]\n","1b895c08":"given_output = pd.read_csv(\"..\/input\/gender_submission.csv\")","44bd8710":"y_given= given_output[\"Survived\"]","1dd97a87":"accuracy_final = round(accuracy_score(y_output, y_given) * 100, 2)\nprint(\"Accuracy:\",accuracy_final)","b129eb8d":"print(confusion_matrix(y_output, y_given))","2ecf229a":"sns.countplot(x=\"Survived\",data=test , )","aa033682":"test['Survived'].value_counts()","041a8ee1":"predicted_test.to_csv('submission.csv', index=False)","95fb5188":"MODELLING","3347c6de":"**CORRELATION ANALYSIS**","32fc7a7c":"DATA UNDERSTANDING","4c9f2589":"Outlier analysis","0c2044df":"Predict in Test data","8dfefa58":"**there is lot of outliers in the data so lets replace those outliers with NA and impute it **","41f303c8":"**EXPLORATORY DATA ANALYSIS - Initial Investigation to understand the Data**","d816c48e":"got 77.03 % accuracy with resembelence to the actual output, the model can be hypertuned , variable engineered etc can be done to improve the accuracy of the model ","ff658d80":"It is found that in normal cases Random Forest is a better model for the given dataset, hypertuning can be done to improve the accuracy far more better","41b5a12a":"MISSING VALUE ANALYSIS","ff7d04b4":"RANDOM FOREST","8f5a6155":"**FEATURE SELECTION**","5002f2cf":"drop cabin as it has a huge number of missing values and few other variables which are not important for the model","1972ce1a":"LOGISTIC REGRESSION"}}