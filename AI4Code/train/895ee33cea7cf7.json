{"cell_type":{"c7635ef4":"code","6c46779f":"code","93dfe4a9":"code","79ffd3e5":"code","b34a4d4e":"code","cf295b13":"code","ec3d72e9":"code","83ba2d88":"code","cd2221a7":"code","f8a09ba6":"code","36a10c0f":"code","0c6eba5c":"code","795b7301":"code","63b894b1":"code","900227e2":"code","e603f2ff":"code","1135b1da":"code","10203746":"code","e11c303b":"code","4b9098c2":"code","336d570b":"code","eb5a5aeb":"code","b2437b11":"code","4e47adaa":"code","e5b98b51":"code","b601ce7e":"code","e0800ec9":"code","d0f72d16":"code","1dc872f7":"code","5e9a4890":"code","5b841a2e":"code","9bc391e3":"code","aa2d592c":"code","f7749f26":"code","ffcf4d5c":"code","7b43d96b":"code","66277ce7":"code","a682330b":"code","9d08aa38":"code","fae5eaea":"code","19574883":"code","21691252":"code","42e5ce86":"code","54136af4":"code","9a91a351":"code","b33b756d":"code","b198ffe9":"code","65248393":"code","7166b759":"code","8f1c585a":"code","26e08ab3":"code","1a51b3be":"code","2575afc5":"code","73e66051":"code","b49d1659":"code","4f005aa9":"code","18248dd6":"code","b0d5dbdb":"code","067f03cf":"code","18c28c49":"code","c69c6f75":"code","3aa5a18a":"code","af2d9cdf":"code","7dd2cc46":"code","9252ad86":"code","dbaebab5":"code","5d3dabbf":"code","dba056d7":"code","256ded8a":"code","30d473e0":"code","5836471d":"code","c0beaa9d":"code","a172feb5":"code","dc2d453b":"code","66432f98":"code","3f3872b8":"code","feaaf5dd":"code","7ac9f4c6":"code","f9305784":"code","05a8f699":"code","67db9576":"code","bdf26ab1":"code","3f9c8f90":"code","98984bf8":"code","16615fcb":"code","13b77796":"code","f757998f":"code","f9d5187b":"code","1b5f6501":"code","3bd54dba":"code","9361c7b7":"code","7c54a78a":"code","d11493d8":"code","1d85e356":"code","a1db3905":"code","cdde110d":"code","05c9e719":"code","8ef868f0":"code","ed38170d":"code","ebf45048":"code","34924ce2":"code","eb813e2d":"code","6ddd6cab":"code","2083c71d":"code","523d099e":"code","b193e452":"code","1aac3221":"code","9d55dba1":"code","23723e06":"code","df00172e":"code","55ed437d":"code","837632e0":"code","d916202a":"code","ed5d7939":"code","a1773dc8":"code","cd2a93e2":"code","cd01ebdf":"code","52a92043":"code","3b48c14e":"code","4c2c2557":"code","a12ed23a":"code","10724828":"code","d14a6db8":"code","a2e9d303":"code","e2b557cc":"code","a87ddc2e":"code","16d840d3":"code","b9fe3436":"code","b0f1a860":"code","e9bf24e7":"code","6a9f09e2":"code","7861179a":"code","26f41c16":"code","fc592234":"code","bb53a20f":"code","85a98618":"code","f6b8322a":"code","25bd3f4b":"code","c5ec3e94":"code","2e98c5f5":"code","1000628f":"code","3ca7c221":"code","07b6873f":"markdown","3bf71c4f":"markdown","6c681e13":"markdown","1f13dad8":"markdown","7ca64200":"markdown","95b117ac":"markdown","77175b4a":"markdown","7cf04e6a":"markdown","45a497b2":"markdown","931d828b":"markdown","7ac903e2":"markdown","b3ad16a4":"markdown","6aaa8f8e":"markdown","1589e45e":"markdown","c956c4e9":"markdown","9b174476":"markdown","aa5cde19":"markdown","3f0fb6ec":"markdown","7019cd73":"markdown"},"source":{"c7635ef4":"# Importing the required packages\n#! pip install -U imbalanced-learn\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import table\nimport seaborn as sns\n\nfrom scipy.stats import zscore\n\n#matplotlib Packages\nimport matplotlib.pyplot as matplot\nfrom matplotlib import pyplot as plt \n\n\n\n\n#sklearn packages\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\n\n# SMOTE for up sampling\nfrom imblearn.combine import SMOTETomek\n\n# setting for better readability of the output\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)\npd.options.display.float_format = '{:.3f}'.format\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6c46779f":"# Creating a function to find the distribution of the dataset, to check distribution before and after imputation \ndef distribution(Source):\n        print(\"Columns that are int32,int64 = \",Source.select_dtypes(include=['int32','int64']).columns)\n        print(\"Columns that are flaot32,float64 = \",Source.select_dtypes(include=['float64']).columns)\n        print(\"Columns that are objects = \",Source.select_dtypes(include=['object']).columns)\n        a = pd.Series(Source.select_dtypes(include=['int32','int64']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            f, axes = matplot.subplots(1, 2, figsize=(10, 10))\n            sns.boxplot(Source[a[j]].value_counts(), ax = axes[0])\n            sns.distplot(Source[a[j]].value_counts(), ax = axes[1])\n            matplot.subplots_adjust(top =  1.5, right = 10, left = 8, bottom = 1)\n\n        a = pd.Series(Source.select_dtypes(include=['float64']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            matplot.Text('Figure for float64')\n            f, axes = matplot.subplots(1, 2, figsize=(10, 10))\n            sns.boxplot(Source[a[j]].value_counts(), ax = axes[0])\n            sns.distplot(Source[a[j]].value_counts(), ax = axes[1])\n            matplot.subplots_adjust(top =  1.5, right = 10, left = 8, bottom = 1)\n\n        a = pd.Series(Source.select_dtypes(include=['object']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            matplot.subplots()\n            sns.countplot(Source[a[j]])","93dfe4a9":"# Function to check the Quartile \ndef quartile_check(Source):\n    a = pd.Series(Source.select_dtypes(include=['int32','int64','float64','float32']).columns)\n    leng = len(a)\n    for j in range(0,len(a)):\n        print(\"Quantiles for {}\".format(a[j]))\n        print(Source[a[j]].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))\n        print(\"-----------------------------------------------------------------\")","79ffd3e5":"# Function for univariate plots\ndef univariate_plots(Source):\n        print(\"Columns that are int32,int64 = \",Source.select_dtypes(include=['int32','int64']).columns)\n        print(\"Columns that are flaot32,float64 = \",Source.select_dtypes(include=['float64']).columns)\n        print(\"Columns that are objects = \",Source.select_dtypes(include=['object']).columns)\n        a = pd.Series(Source.select_dtypes(include=['int32','int64']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            f, axes = plt.subplots(1, 2, figsize=(10, 10))\n            sns.boxplot(Source[a[j]], ax = axes[0])\n            sns.distplot(Source[a[j]], ax = axes[1])\n            plt.subplots_adjust(top =  1.5, right = 10, left = 8, bottom = 1)\n\n        a = pd.Series(Source.select_dtypes(include=['float64']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            plt.Text('Figure for float64')\n            f, axes = plt.subplots(1, 2, figsize=(10, 10))\n            sns.boxplot(Source[a[j]], ax = axes[0])\n            sns.distplot(Source[a[j]], ax = axes[1])\n            plt.subplots_adjust(top =  1.5, right = 10, left = 8, bottom = 1)\n\n        a = pd.Series(Source.select_dtypes(include=['object']).columns)\n        leng = len(a)\n        for j in range(0,len(a)):\n            plt.subplots()\n            sns.countplot(Source[a[j]])","b34a4d4e":"# importing the data\n#identity_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n#transaction_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\n#identity_test_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n#transaction_test_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n\nidentity_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntransaction_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\nidentity_test_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntransaction_test_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n","cf295b13":"transaction_df.head(10)","ec3d72e9":"identity_df.head(10)","83ba2d88":"print(identity_df.shape)","cd2221a7":"print(transaction_df.shape)\n# The total Number of rows in Transaction dataset does not match with the total number of identity dataset. \n# There is high chance we mightnot have identity for few of the transactions.\n","f8a09ba6":"identity_df.dtypes\n# The data types seems to be a mixer of float, int and object","36a10c0f":"transaction_df.dtypes","0c6eba5c":"identity_df.isnull().sum()","795b7301":"transaction_df.isnull().sum()","63b894b1":"identity_test_df.isnull().sum()","900227e2":"transaction_test_df.isnull().sum()","e603f2ff":"# Finding the distribution of all the features which seems to be a categorical variable also checking if any numerical value has typo which led to datatype object\nfor col in identity_df.columns:\n    if identity_df[col].dtypes == \"object\":\n        print(\"----------\")\n        print(col)\n        print(\"----------\")\n        print(identity_df[col].value_counts(dropna=False))\n        print(\"----------\")\n\n# There is no numerical column misclassified as object because of incoorect value","1135b1da":"# Getting the uniques values of each of the categorical variables  \nfor col in identity_df.columns:\n    if identity_df[col].dtypes == \"object\":\n        print(\"----------\")\n        print(col)\n        print(\"----------\")\n        print(identity_df[col].nunique())\n        print(\"----------\")","10203746":"# Finding the distribution of all the features which seems to be a categorical variable. -- Transaction Dataset\nfor col in transaction_df.columns:\n    if transaction_df[col].dtypes == \"object\":\n        print(\"----------\")\n        print(col)\n        print(\"----------\")\n        print(transaction_df[col].value_counts(dropna=False))\n        print(\"----------\")\n        \n# There is no numerical column misclassified as object because of incoorect value","e11c303b":"# Getting the uniques values of each of the categorical variables  \nfor col in transaction_df.columns:\n    if transaction_df[col].dtypes == \"object\":\n        print(\"----------\")\n        print(col)\n        print(\"----------\")\n        print(transaction_df[col].nunique())\n        print(\"----------\")","4b9098c2":"id_corr_mat = identity_df.corr()\ntran_corr_mat = transaction_df.corr()","336d570b":"#id_corr_mat.reset_index(inplace=True)\n#tran_corr_mat.reset_index(inplace=True)","eb5a5aeb":"#id_corr_mat.columns","b2437b11":"#id_corr_mat","4e47adaa":"#id_corr_mat[(id_corr_mat['id_01']>0.95) | (id_corr_mat['id_01']<-0.95)]['index']","e5b98b51":"#for i in id_corr_mat.columns:\n #   if i=='index':\n  #      continue\n   # else:\n    #    for i in id_corr_mat\n     #   print(\"highly correlated columns for {}\".format(i))\n      #  print(\"-----------------------------\")\n       # id_corr_mat[(id_corr_mat[i]>0.95) | (id_corr_mat[i]<-0.95)]['index']\n        #print(\"-----------------------------\")\n    #print(j)\n    #break","b601ce7e":"#id_corr_mat.head()","e0800ec9":"# Checking for null values in the identity dataset. From the below observation we can see there are a huge null values. We have to handle these null values. \nidentity_df.isnull().sum()","d0f72d16":"# Creating a Dataframe with column name and the number of null values, so that we can drop the columns which has more than 75% \n# of data as null. \nis_null = identity_df.isnull().sum()\nis_null = is_null.to_frame()\nis_null[\"col_name\"]=is_null.index\nis_null.columns = [\"null_count\",\"col_name\"]\n","1dc872f7":"# Creating a Dataframe with column name and the number of null values, so that we can drop the columns which has more than 75% \n# of data as null. \ntest_is_null = identity_test_df.isnull().sum()\ntest_is_null = test_is_null.to_frame()\ntest_is_null[\"col_name\"]=test_is_null.index\ntest_is_null.columns = [\"null_count\",\"col_name\"]\n","5e9a4890":"# Getting the list of columns that are not to be removed. This is used to get the distribution for these columns before and after imputation\nidentity_non_removed_columns = []\ninc = 0\nfor i,col in is_null.values:\n    \n    if (i\/144233) <= 0.75:\n        identity_non_removed_columns.append(col)","5b841a2e":"# distribution of data before the imputation is made. \n#distribution(identity_df[identity_non_removed_columns])\n\n# Th distribution shows there are outliers","9bc391e3":"# before the imputation is made. \n#quartile_check(identity_df[identity_non_removed_columns]),file=open(\"identity_quartile.csv\",\"a\")\n","aa2d592c":"# Dropping the columns that has more than 75% of data as null.\ninc = 0\nfor i,col in is_null.values:\n    \n    if (i\/144233) > 0.75:\n        identity_df.drop(labels=col,axis=1,inplace=True)\n        print(\"column dropped {} with {} null values\".format(col,i))\n        inc = inc + 1\nprint(\"total number of columns dropped {}\".format(inc))\n# all the columns dropped has 96% of missing data.","f7749f26":"identity_test_df.shape","ffcf4d5c":"# Dropping the columns that has more than 75% of data as null.\ninc = 0\nfor i,col in test_is_null.values:\n    \n    if (i\/141907) > 0.75:\n        identity_test_df.drop(labels=col,axis=1,inplace=True)\n        print(\"column dropped {} with {} null values\".format(col,i))\n        inc = inc + 1\nprint(\"total number of columns dropped {}\".format(inc))\n# all the columns dropped has 96% of missing data.","7b43d96b":"identity_test_df.columns = identity_df.columns\n# The column names in the train and test datasets have slight variation, keeping the same name will avoid confusion ","66277ce7":"# repeating the same for transaction dataset\nis_null = transaction_df.isnull().sum()\nis_null = is_null.to_frame()\nis_null[\"col_name\"]=is_null.index\nis_null.columns = [\"null_count\",\"col_name\"]","a682330b":"# Getting the list of columns that are not to be removed\ntransaction_non_removed_columns = []\ninc = 0\nfor i,col in is_null.values:\n    \n    if (i\/590540) <= 0.75:\n        transaction_non_removed_columns.append(col)","9d08aa38":"#quartile_check(transaction_df[transaction_non_removed_columns])","fae5eaea":"# distribution of data before the imputation is made. \n#distribution(transaction_df[transaction_non_removed_columns])\n# The distribution shows there are presence of outliier is the data. Since the categorical variabes are masked and representedd as int, we are not treating those columns","19574883":"# repeating the same for transaction dataset\nis_null = transaction_df.isnull().sum()\nis_null = is_null.to_frame()\nis_null[\"col_name\"]=is_null.index\nis_null.columns = [\"null_count\",\"col_name\"]","21691252":"# Dropping the columns that has more than 75% of data as null.\ninc = 0\nremoved_columns = []\nfor i,col in is_null.values:\n    \n    if (i\/590540) > 0.75:\n        transaction_df.drop(labels=col,axis=1,inplace=True)\n        print(\"column dropped {} with {} null values\".format(col,i))\n        inc = inc + 1\n        removed_columns.append(col)\nprint(\"total number of columns dropped {}\".format(inc))","42e5ce86":"# Dropping the columns that has more than 75% of data as null.\ninc = 0\nremoved_columns = []\nfor i,col in is_null.values:\n    \n    if (i\/590540) > 0.75:\n        transaction_test_df.drop(labels=col,axis=1,inplace=True)\n        print(\"column dropped {} with {} null values\".format(col,i))\n        inc = inc + 1\n        removed_columns.append(col)\nprint(\"total number of columns dropped {}\".format(inc))","54136af4":"sample = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv\")","9a91a351":"print(transaction_df.shape)\nprint(transaction_test_df.shape)","b33b756d":"#impute using measures of central tendency. Let us impute with mean for\n\n#creating a list with categorical variable( as described in the data definition)\ncat = [\"DeviceType\",\"DeviceInfo\",\"id_12\",\"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_31\",\"id_32\",\"id_33\",\"id_34\",\"id_35\",\"id_36\",\"id_37\",\"id_38\"]\n\n# Imputing in iteration \nfor col in identity_df.columns:\n    if identity_df[col].isnull().sum() > 0:\n        if col in cat:\n            identity_df[col].fillna(identity_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            \n            identity_df[col].fillna(identity_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))          \n            ","b198ffe9":"identity_test_df.columns = identity_df.columns","65248393":"#impute using measures of central tendency. Let us impute with mean for\n\n#creating a list with categorical variable( as described in the data definition)\ncat = [\"DeviceType\",\"DeviceInfo\",\"id_12\",\"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_31\",\"id_32\",\"id_33\",\"id_34\",\"id_35\",\"id_36\",\"id_37\",\"id_38\"]\n\n# Imputing in iteration \nfor col in identity_test_df.columns:\n    if identity_test_df[col].isnull().sum() > 0:\n        if col in cat:\n            identity_test_df[col].fillna(identity_test_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            \n            identity_test_df[col].fillna(identity_test_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))          \n            ","7166b759":"cat = [\"ProductCD\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\"addr1\", \"addr2\",\"P_emaildomain\", \"R_emaildomain\",\"M1\",\"M2\",\"M3\",\"M4\",\"M5\",\"M6\",\"M7\",\"M8\",\"M9\",]\nfor col in transaction_df.columns:\n    if transaction_df[col].isnull().sum() > 0:\n        if col in cat:\n            #print(col)\n            transaction_df[col].fillna(transaction_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            #print(col)\n            transaction_df[col].fillna(transaction_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))","8f1c585a":"cat = [\"ProductCD\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\"addr1\", \"addr2\",\"P_emaildomain\", \"R_emaildomain\",\"M1\",\"M2\",\"M3\",\"M4\",\"M5\",\"M6\",\"M7\",\"M8\",\"M9\",]\nfor col in transaction_test_df.columns:\n    if transaction_test_df[col].isnull().sum() > 0:\n        if col in cat:\n            #print(col)\n            transaction_test_df[col].fillna(transaction_test_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            #print(col)\n            transaction_test_df[col].fillna(transaction_test_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))","26e08ab3":"# distribution of data after the imputation is made. \n#distribution(identity_df[identity_non_removed_columns])\n\n#Imputation does not seem to have a much impact to the distribution","1a51b3be":"#print(quartile_check(identity_df[identity_non_removed_columns]),file=open(\"identity_quartile.csv\",\"a\"))","2575afc5":"#quartile_check(transaction_df[transaction_non_removed_columns])","73e66051":"# distribution of data before the imputation is made. \n#distribution(transaction_df[transaction_non_removed_columns])\n\n# Imputation has no much impact to the distribution","b49d1659":"# Distribution of Transaction Amout for Fraud and Legitimate transaction\n\nplt.figure(figsize=(50,15))\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(transaction_df[transaction_df[\"isFraud\"]==1][\"TransactionAmt\"], bins = bins,histtype=\"stepfilled\")\nax1.set_title('Fraud')\nax2.hist(transaction_df[transaction_df[\"isFraud\"]==0][\"TransactionAmt\"], bins = bins,histtype=\"stepfilled\")\nax2.set_title('Legit')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 33000))\nplt.yscale('log')\nplt.show();\n\n# The distribution of fraud transaction is in the lower range and legitimate transaction hass higher values","4f005aa9":"# getting the number of unique values from the card columns to understand the column values.  ","18248dd6":"print(\"Unique value present in card1 {}\".format(transaction_df[\"card1\"].nunique()))\nprint(\"Unique value present in card2 {}\".format(transaction_df[\"card2\"].nunique()))\nprint(\"Unique value present in card3 {}\".format(transaction_df[\"card3\"].nunique()))\nprint(\"Unique value present in card4 {}\".format(transaction_df[\"card4\"].nunique()))\nprint(\"Unique value present in card5 {}\".format(transaction_df[\"card5\"].nunique()))\nprint(\"Unique value present in card6 {}\".format(transaction_df[\"card6\"].nunique()))","b0d5dbdb":"transaction_df[transaction_df[\"card4\"]==\"discover\"][\"card3\"].value_counts()","067f03cf":"transaction_df[transaction_df[\"card4\"]==\"discover\"][\"card4\"].value_counts()\n","18c28c49":"transaction_df[transaction_df[\"card4\"]==\"discover\"][\"card3\"].value_counts()","c69c6f75":"c11= transaction_df.groupby([\"card1\"])[\"TransactionAmt\"].max()\nc12= transaction_df.groupby([\"card1\"])[\"TransactionAmt\"].min()\nc13= transaction_df.groupby([\"card1\"])[\"TransactionAmt\"].mean()\nc1 = pd.merge(c11,c12,on=\"card1\")\nc1 = pd.merge(c1,c13,on=\"card1\")\nc1 = c1.reset_index()\nc1.columns = [\"card1\",\"Max\",\"Min\",\"Avg\"]\nc1.index = c1[\"card1\"]\nc1.drop(\"card1\",axis=1,inplace=True)","3aa5a18a":"c1.sort_values(by=\"Max\",ascending=False).head(10).plot(kind=\"bar\")\n# Checking the distribution for top 10 category, majority of the transaction has occured in the 16075 category ","af2d9cdf":"c21= transaction_df.groupby([\"card2\"])[\"TransactionAmt\"].max()\nc22= transaction_df.groupby([\"card2\"])[\"TransactionAmt\"].min()\nc23= transaction_df.groupby([\"card2\"])[\"TransactionAmt\"].mean()\nc2 = pd.merge(c21,c22,on=\"card2\")\nc2 = pd.merge(c2,c23,on=\"card2\")\nc2 = c2.reset_index()\nc2.columns = [\"card2\",\"Max\",\"Min\",\"Avg\"]\nc2.index = c2[\"card2\"]\nc2.drop(\"card2\",axis=1,inplace=True)","7dd2cc46":"c2.sort_values(by=\"Max\",ascending=False).head(10).plot(kind=\"bar\")\n# Checking the distribution for top 10 category, majority of the transaction has occured in the 514 category ","9252ad86":"c31= transaction_df.groupby([\"card3\"])[\"TransactionAmt\"].max()\nc32= transaction_df.groupby([\"card3\"])[\"TransactionAmt\"].min()\nc33= transaction_df.groupby([\"card3\"])[\"TransactionAmt\"].mean()\nc3 = pd.merge(c31,c32,on=\"card3\")\nc3 = pd.merge(c3,c33,on=\"card3\")\nc3 = c3.reset_index()\nc3.columns = [\"card3\",\"Max\",\"Min\",\"Avg\"]\nc3.index = c3[\"card3\"]\nc3.drop(\"card3\",axis=1,inplace=True)","dbaebab5":"c3.sort_values(by=\"Max\",ascending=False).head(10).plot(kind=\"bar\")\n# Checking the distribution for top 10 category, majority of the transaction has occured in the 150 category ","5d3dabbf":"c51= transaction_df.groupby([\"card5\"])[\"TransactionAmt\"].max()\nc52= transaction_df.groupby([\"card5\"])[\"TransactionAmt\"].min()\nc53= transaction_df.groupby([\"card5\"])[\"TransactionAmt\"].mean()\nc5 = pd.merge(c51,c52,on=\"card5\")\nc5 = pd.merge(c5,c53,on=\"card5\")\nc5 = c5.reset_index()\nc5.columns = [\"card5\",\"Max\",\"Min\",\"Avg\"]\nc5.index = c5[\"card5\"]\nc5.drop(\"card5\",axis=1,inplace=True)","dba056d7":"c5.sort_values(by=\"Max\",ascending=False).head(10).plot(kind=\"bar\")\n# Checking the distribution for top 10 category, majority of the transaction has occured in the 102 category ","256ded8a":"card_detail = transaction_df[[\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\"]]","30d473e0":"IQR = transaction_df[\"TransactionAmt\"].quantile(0.75) - transaction_df[\"TransactionAmt\"].quantile(0.25)\nupper_limit = transaction_df[\"TransactionAmt\"].quantile(0.75) + (IQR * 1.5)\nlower_limit = transaction_df[\"TransactionAmt\"].quantile(0.25) - (IQR * 1.5)\nprint(\"Upper Limit :{}\".format(upper_limit))\nprint(\"Lower Limit :{}\".format(lower_limit))\n#Any thing lower than the upper limit or lower than the lower limit will be treated as outlier, \n#We are not handling the outlier and just observing the distribution.","5836471d":"# Distribution excluding the outlier limit\nsns.distplot(transaction_df[transaction_df[\"TransactionAmt\"] < 247.000000][\"TransactionAmt\"])","c0beaa9d":"# Positive Outlier\nsns.distplot(transaction_df[transaction_df[\"TransactionAmt\"] > 247.000000][\"TransactionAmt\"])","a172feb5":"# Transaction Amount cannot be negative. There is no negative outlier \ntransaction_df[transaction_df[\"TransactionAmt\"] < -79].shape","dc2d453b":"#univariate_plots(transaction_df)","66432f98":"#univariate_plots(identity_df)","3f3872b8":"total = len(transaction_df)\ntotal_amt = transaction_df.groupby(['isFraud'])['TransactionAmt'].sum().sum()\n\nprint(transaction_df[\"isFraud\"].value_counts())\nprint(transaction_df.groupby([\"isFraud\"])[\"TransactionAmt\"].sum().astype(int))\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ntransaction_df[\"isFraud\"].value_counts().plot(kind=\"bar\",ax=ax1,title=\"Over all number of Legit and Fraudelent Transaction\")\n\nfor i in ax1.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax1.text(i.get_x()-.03, i.get_height()+.5, \n            str(round(i.get_height()\/total*100, 2))+'%', fontsize=15,\n                color='dimgrey')\n\n\nax2 = plt.subplot(122)\ntransaction_df.groupby([\"isFraud\"])[\"TransactionAmt\"].sum().plot(kind=\"bar\",ax=ax2,title=\"Over all Legit and Fraudelent Transaction Amount\")\n\n\nfor i in ax2.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax2.text(i.get_x()-.03, i.get_height()+.5, \n            str(round(i.get_height()\/total_amt*100, 2))+'%', fontsize=15,\n                color='dimgrey')\n\n\n\nplt.show()","feaaf5dd":"transaction_df[\"Day\"] = transaction_df[\"TransactionDT\"]\/86400","7ac9f4c6":"transaction_df[\"Day\"] = transaction_df[\"Day\"].astype(int)","f9305784":"transaction_df[\"Day of the week\"] = ((transaction_df[\"Day\"] - 1)%7)+ 1","05a8f699":"transaction_df[\"Month\"] = ((transaction_df[\"Day\"]\/\/30) % 30) + 1","67db9576":"transaction_df[\"Day of Month\"] = transaction_df[\"Day\"] % 30 ","bdf26ab1":"transaction_df[\"Day of Month\"] = transaction_df[\"Day of Month\"].replace(0,30)","3f9c8f90":"transaction_df[\"Hour of the Day\"] = (transaction_df[\"TransactionDT\"] \/\/ 3600) % 24","98984bf8":"transaction_test_df[\"Day\"] = transaction_df[\"TransactionDT\"]\/86400","16615fcb":"transaction_test_df[\"Day\"] = transaction_df[\"Day\"].astype(int)","13b77796":"transaction_test_df[\"Day of the week\"] = ((transaction_df[\"Day\"] - 1)%7)+ 1","f757998f":"transaction_test_df[\"Month\"] = ((transaction_df[\"Day\"]\/\/30) % 30) + 1","f9d5187b":"transaction_test_df[\"Day of Month\"] = transaction_df[\"Day\"] % 30 ","1b5f6501":"transaction_test_df[\"Day of Month\"] = transaction_df[\"Day of Month\"].replace(0,30)","3bd54dba":"transaction_test_df[\"Hour of the Day\"] = (transaction_df[\"TransactionDT\"] \/\/ 3600) % 24","9361c7b7":"#transaction_df.drop(\"Day\",axis=1,inplace=True)\n#transaction_test_df.drop(\"Day\",axis=1,inplace=True)\n#transaction_df.drop(\"Month\",axis=1,inplace=True)\n#transaction_test_df.drop(\"Month\",axis=1,inplace=True)","7c54a78a":"#transaction_df.drop(\"TransactionDT\",axis=1,inplace=True)\n#transaction_test_df.drop(\"TransactionDT\",axis=1,inplace=True)","d11493d8":"transaction_df.dtypes","1d85e356":"trans_amt_df1 = transaction_df.groupby([\"Month\",\"isFraud\"])[\"TransactionAmt\"].sum().astype(int).to_frame()\ntrans_amt_df1 = trans_amt_df1.reset_index()\n","a1db3905":"tmp1 = trans_amt_df1.groupby([\"Month\"])[\"TransactionAmt\"].sum().to_frame().reset_index()\ntrans_amt_df1 = pd.merge(trans_amt_df1,tmp1,on=\"Month\")\ntrans_amt_df1[\"Percent Contribution\"] = (trans_amt_df1[\"TransactionAmt_x\"]\/trans_amt_df1[\"TransactionAmt_y\"])*100\ntrans_amt_df11= trans_amt_df1[trans_amt_df1[\"isFraud\"]==1][[\"Month\",\"Percent Contribution\"]]\ntrans_amt_df11","cdde110d":"transaction_df[\"P_emaildomain\"].nunique()","05c9e719":"# For better understanding grouping all the mail domain that has less tha 3% fraud recorded as others\ntransaction_df.loc[transaction_df.P_emaildomain.isin(transaction_df.P_emaildomain.value_counts()[(transaction_df[transaction_df[\"isFraud\"]==1].P_emaildomain.value_counts())\/transaction_df.P_emaildomain.value_counts() < 0.035].index), 'P_emaildomain'] = \"Others\"","8ef868f0":"transaction_test_df[\"P_emaildomain\"].nunique()","ed38170d":"# Grouping the emaildomain similar to train dataset\ncond = transaction_test_df['P_emaildomain'].isin(transaction_df['P_emaildomain']) == False\ntransaction_test_df.loc[transaction_test_df[cond].index,'P_emaildomain'] = \"Others\"\n\n","ebf45048":"print(transaction_df[\"P_emaildomain\"].nunique())\nprint(transaction_test_df[\"P_emaildomain\"].nunique())","34924ce2":"\nfor col in transaction_df.columns:\n    listv = []\n    if transaction_df[col].dtypes == \"object\":\n        for var in transaction_df[col].unique():\n            listv.append(var)\n        per = pd.crosstab(transaction_df[col], transaction_df['isFraud'], normalize='index') * 100\n        per = per.reset_index()\n        per.rename(columns={0:'Legit', 1:'Fraud'}, inplace=True)\n\n\n        plt.figure(figsize=(15,6))\n\n        ax1 = plt.subplot(121)\n\n        g1= sns.countplot(x=col, hue='isFraud', data=transaction_df,order=listv,palette=\"Set2\")\n        gt = g1.twinx()\n        gt = sns.pointplot(x=col, y='Fraud', data=per, color='black', legend=False,order=listv)\n        gt.set_ylabel(\"% of Fraud Transactions\", fontsize=12)\n\n        g1.set_title(\"Number of Legit and Fraudelent Transaction and Fraud contributio\", fontsize=14)\n        g1.set_ylabel(\"Count\", fontsize=12)\n\n\n# set individual bar lables using above list\n        for i in ax1.patches:\n    # get_x pulls left or right; get_height pushes up or down\n            ax1.text(i.get_x()-.03, i.get_height()+.5, \n                    str(round(i.get_height(), 2)), fontsize=15,\n                        color='dimgrey')\n    \n\n        plt.show()\n\n# Observing the distribution of the categorical variables and checkin the % contribution of fraudulent transaction \n# by each of its class.\n\n        \n","eb813e2d":"per = pd.crosstab(transaction_df['Month'], transaction_df['isFraud'], normalize='index') * 100\nper = per.reset_index()\nper.rename(columns={0:'Legit', 1:'Fraud'}, inplace=True)\n\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Month', hue='isFraud', data=transaction_df,palette=\"Set2\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Month', y='Fraud', data=per, color='red', legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=12)\n\ng1.set_title(\"Number of Legit and Fraudelent Transaction Every Month\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n# set individual bar lables using above list\nfor i in ax1.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax1.text(i.get_x()-.03, i.get_height()+.5, \n            str(round(i.get_height(), 2)), fontsize=15,\n                color='dimgrey')\n    \n\nplt.show()\n\n","6ddd6cab":"p_cont1 = pd.crosstab(transaction_df['Day of the week'], transaction_df['isFraud'], normalize='index') * 100\np_cont1 = p_cont1.reset_index()\np_cont1.rename(columns={0:'Legit', 1:'Fraud'}, inplace=True)\n\n\nlistv = []\nfor var in transaction_df['Day of the week'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Day of the week', hue='isFraud', data=transaction_df,order=listv,palette=\"Set2\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Day of the week', y='Fraud', data=p_cont1, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=12)\n\ng1.set_title(\"Number of Legit and Fraudelent Transaction for Day of Week\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","2083c71d":"per = pd.crosstab(transaction_df['Hour of the Day'], transaction_df['isFraud'], normalize='index') * 100\nper = per.reset_index()\nper.rename(columns={0:'Legit', 1:'Fraud'}, inplace=True)\n\n\nlistv = []\nfor var in transaction_df['Hour of the Day'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(20,6))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='Hour of the Day', hue='isFraud', data=transaction_df,order=listv,palette=\"Set2\")\ngt = g1.twinx()\ngt = sns.pointplot(x='Hour of the Day', y='Fraud', data=per, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=12)\n\ng1.set_title(\"Number of Legit and Fraudelent Transaction on Hour of Day basis\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()\n\n# Number of Fraudelent Transactions seems close enough every month","523d099e":"transaction_df[\"card_cat_type\"] = transaction_df[\"card4\"] + \" \" + transaction_df[\"card6\"]","b193e452":"transaction_test_df[\"card_cat_type\"] = transaction_test_df[\"card4\"] + \" \" + transaction_test_df[\"card6\"]","1aac3221":"per = pd.crosstab(transaction_df['card_cat_type'], transaction_df['isFraud'], normalize='index') * 100\nper = per.reset_index()\nper.rename(columns={0:'Legit', 1:'Fraud'}, inplace=True)\n\n\nlistv = []\nfor var in transaction_df['card_cat_type'].unique():\n    listv.append(var)\n\nplt.figure(figsize=(45,8))\n\nax1 = plt.subplot(121)\n\ng1= sns.countplot(x='card_cat_type', hue='isFraud', data=transaction_df,order=listv,palette=\"Set2\")\ngt = g1.twinx()\ngt = sns.pointplot(x='card_cat_type', y='Fraud', data=per, color='black', legend=False,order=listv)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=12)\n\ng1.set_title(\"Number of Legit and Fraudelent Transaction Every Month\", fontsize=14)\ng1.set_ylabel(\"Count\", fontsize=12)\n\n\n \n\nplt.show()","9d55dba1":"transaction_df.shape","23723e06":"\ntransaction_df.drop([\"Day\",\"Day of the week\",\"Month\",\"Day of Month\",\"Hour of the Day\"],axis=1,inplace=True)\ntransaction_test_df.drop([\"Day\",\"Day of the week\",\"Month\",\"Day of Month\",\"Hour of the Day\"],axis=1,inplace=True)\n# removing these columns after required Analysis","df00172e":"# Dropping card4 and card6 as we have created a column concatinating both\ntransaction_df.drop([\"card4\",\"card6\"],axis=1,inplace=True)\ntransaction_test_df.drop([\"card4\",\"card6\"],axis=1,inplace=True)","55ed437d":"# dropping this columns intuitivly as they might not have any correlation with the dependent variables. \nidentity_df.drop([\"id_30\",\"id_31\",\"id_33\",\"DeviceInfo\"],axis=1,inplace=True)\nidentity_test_df.drop([\"id_30\",\"id_31\",\"id_33\",\"DeviceInfo\"],axis=1,inplace=True)","837632e0":"print(transaction_df.shape)\nprint(transaction_test_df.shape)","d916202a":"print(identity_df.shape)\nprint(identity_test_df.shape)","ed5d7939":"new_paymentcard_df = pd.merge(transaction_df,identity_df,on=\"TransactionID\",how=\"left\")\n# Doing an inner join as doind a left join would leave all the columns from transaction dataset null and imputing it will be of no use..","a1773dc8":"test_new_paymentcard_df = pd.merge(transaction_test_df,identity_test_df,on=\"TransactionID\",how=\"left\")\n# Doing an inner join as doind a left join would leave all the columns from transaction dataset null and imputing it will be of no use..","cd2a93e2":"print(new_paymentcard_df.shape)\nprint(test_new_paymentcard_df.shape)","cd01ebdf":"new_paymentcard_df.isnull().sum()","52a92043":"#impute using measures of central tendency. Let us impute with mean for\n\n#creating a list with categorical variable( as described in the data definition)\ncat = [\"DeviceType\",\"DeviceInfo\",\"id_12\",\"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_31\",\"id_32\",\"id_33\",\"id_34\",\"id_35\",\"id_36\",\"id_37\",\"id_38\"]\n\n# Imputing in iteration \nfor col in new_paymentcard_df.columns:\n    if new_paymentcard_df[col].isnull().sum() > 0:\n        if col in cat:\n            new_paymentcard_df[col].fillna(new_paymentcard_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            \n            new_paymentcard_df[col].fillna(new_paymentcard_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))          \n            ","3b48c14e":"#impute using measures of central tendency. Let us impute with mean for\n\n#creating a list with categorical variable( as described in the data definition)\ncat = [\"DeviceType\",\"DeviceInfo\",\"id_12\",\"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_31\",\"id_32\",\"id_33\",\"id_34\",\"id_35\",\"id_36\",\"id_37\",\"id_38\"]\n\n# Imputing in iteration \nfor col in test_new_paymentcard_df.columns:\n    if test_new_paymentcard_df[col].isnull().sum() > 0:\n        if col in cat:\n            test_new_paymentcard_df[col].fillna(test_new_paymentcard_df[col].mode()[0],inplace=True)\n            print(\"column {} has been imputed with mode\".format(col))\n        else:\n            \n            test_new_paymentcard_df[col].fillna(test_new_paymentcard_df[col].median(),inplace=True)\n            print(\"column {} has been imputed with mean\".format(col))          \n            ","4c2c2557":"col = []\nfor c in new_paymentcard_df.columns:\n    if new_paymentcard_df[c].dtypes=='object':\n        col.append(c)\n        \n\nnew_paymentcard_dummies = pd.get_dummies(new_paymentcard_df , columns=col, drop_first=True)","a12ed23a":"X = new_paymentcard_dummies.drop([\"isFraud\",'TransactionID'],axis=1)\nX_id = new_paymentcard_dummies[\"TransactionID\"]\ny = new_paymentcard_dummies[\"isFraud\"]","10724828":"X_Scaled = StandardScaler().fit_transform(X)\ncov_matrix = np.cov(X_Scaled.T)\n#print('Covariance Matrix \\n%s', cov_matrix)\n\ne_vals, e_vecs = np.linalg.eig(cov_matrix)\ne_vals, e_vecs = np.linalg.eig(cov_matrix)\n\n#print('Eigenvectors \\n%s' %e_vecs)\n#print('\\nEigenvalues \\n%s' %e_vals)\n\ntot = sum(e_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(e_vals, reverse=True)]\n\ncum_var_exp = np.cumsum(var_exp)\n#print(\"Cumulative Variance Explained\", cum_var_exp)\n\nmatplot.figure(figsize=(20 , 15))\nmatplot.bar(range(1, e_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nmatplot.step(range(1, e_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nmatplot.ylabel('Explained Variance Ratio')\nmatplot.xlabel('Principal Components')\nmatplot.legend(loc = 'best')\nmatplot.tight_layout()\nmatplot.show()\n\nPricipal_comp_composition = (pd.DataFrame(cum_var_exp).reset_index())\nPricipal_comp_composition.columns = ['Pricipal Components', '% info retained']","d14a6db8":"Pricipal_comp_composition","a2e9d303":"from sklearn.decomposition import PCA as PCA\npca = PCA(n_components=143)\nprincipalComponents = pca.fit_transform(X)","e2b557cc":"principalComponents_df = pd.DataFrame(principalComponents)","a87ddc2e":"X_trainval, X_test, y_trainval, y_test = train_test_split(principalComponents_df,y , test_size=0.20, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.30, random_state=1)","16d840d3":"lab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\nval_scores_encoded = lab_enc.fit_transform(y_val)\ntest_scores_encoded = lab_enc.fit_transform(y_test)","b9fe3436":"rfcl = RandomForestClassifier(n_estimators = 75 , random_state=1234,max_depth=25\n                              ,criterion='gini',max_features='sqrt')\nrfcl = rfcl.fit(X_train, y_train)\ny_predict_rfcl = rfcl.predict(X_val)\nprint(rfcl.score(X_train , y_train))\nprint(rfcl.score(X_val, y_val))\nprint(metrics.confusion_matrix(y_val, y_predict_rfcl))\nprint(metrics.classification_report(y_val, y_predict_rfcl))","b0f1a860":"y_predict_rfcl = rfcl.predict(X_test)\nprint(rfcl.score(X_test, y_test))\nprint(metrics.confusion_matrix(y_test, y_predict_rfcl))\nprint(metrics.classification_report(y_test, y_predict_rfcl))","e9bf24e7":"# calculate the fpr and tpr for all thresholds of the classification\n\n# Firstly, calculate the probabilities of predictions made\nprobs = rfcl.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nmetrics\n# method to plot\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0,1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","6a9f09e2":"from lightgbm import LGBMClassifier\nlgb_clf = LGBMClassifier(random_state=17)","7861179a":"lgb_clf.fit(X_train,y_train)","26f41c16":"y_predict_lgb = lgb_clf.predict(X_val)","fc592234":"print(lgb_clf.score(X_train , y_train))\nprint(metrics.accuracy_score(y_val,y_predict_lgb))\nprint(metrics.confusion_matrix(y_val,y_predict_lgb))\nprint(metrics.classification_report(y_val,y_predict_lgb))","bb53a20f":"y_predict_lgb = lgb_clf.predict(X_test)\nprint(metrics.accuracy_score(y_test,y_predict_lgb))\nprint(metrics.confusion_matrix(y_test,y_predict_lgb))\nprint(metrics.classification_report(y_test,y_predict_lgb))","85a98618":"# calculate the fpr and tpr for all thresholds of the classification\n\n# Firstly, calculate the probabilities of predictions made\nprobs = lgb_clf.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nmetrics\n# method to plot\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0,1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","f6b8322a":"from xgboost import XGBClassifier","25bd3f4b":"xgb_clf = XGBClassifier(random_state=17,n_estimators=175,max_depth=7)\nxgb_clf.fit(X_train, y_train)","c5ec3e94":"# make predictions for test data\nprint(xgb_clf.score(X_train , y_train))\ny_predict = xgb_clf.predict(X_val)\nprint(metrics.accuracy_score(y_val,y_predict))\nprint(metrics.confusion_matrix(y_val,y_predict))\nprint(metrics.classification_report(y_val,y_predict))","2e98c5f5":"y_predict = xgb_clf.predict(X_test)","1000628f":"print(metrics.accuracy_score(y_test,y_predict))\nprint(metrics.confusion_matrix(y_test,y_predict))\nprint(metrics.classification_report(y_test,y_predict))","3ca7c221":"# calculate the fpr and tpr for all thresholds of the classification\n\n# Firstly, calculate the probabilities of predictions made\nprobs = lgb_clf.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nmetrics\n# method to plot\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0,1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","07b6873f":"********WIP*******\n\n1. Upsample the data\n2. Look for the improvement in recall value\n3. If improvement is noticed and if there is still a scope for improvement, Step back a little and use feature selection techniques to pick the best feature.\n\n\n","3bf71c4f":"# Starting with data cleaning. Looking for null values in the dataset","6c681e13":"<font color=\"blue\"> In the transaction dataset we have six columns which feature card details like card type, card\ncategory, issue bank, country, etc. Among these six card 4 is card category(Mastercard,\nVisa,american express, discover) and card6 refers to card type(debit\/Credit). Other columns are also categorical variables but are encoded due to security.\n\nAmong the remaining columns card1, card2, card3, card 5; card1 has the highest number of unique\nrecords which indicates it is a granular level detail among the card details hierarchy. \n\nOn analysing the column card4, discover is one of the card categories. Discover is primarily\nused in America. Since one of the card details holds the country detail of the card, analysing the\ncolumns by filtering \u2018card4 = discover\u2019 and see if we get one distinct value for any one column.\nThe qualified column to do such analysis would be card3 and card5 since the total number of\ncountries in the world is around 195.\n\nAs expected card3 column have one distinct value and card5 has 9 distinct values. So\nhypothetically we can say card3 column describes the country of the card used for transaction\nand the value 150 in card3 refers to the USA. <\/font>","1f13dad8":"Observation:\n\nThough the above models seems to ato have a very good accuracy score with a good AUC score too, but the recall value seems to be very low leading to Type II errors. Fraud detection is a use case where we should focus more on Type II error, which would be the main objective. \n\nThe main reason could be due to the distribution of the data, as observed the data is highy imbalanced, which makes the model to be highly biased. \n\nTo fix this we should go with SMOTE to upsample the data which would give a better performance. \n","7ca64200":"<font color=\"blue\"> # Univariant, Bivariant and Multivariant Analysis <\/font>","95b117ac":"# Model Implementation","77175b4a":"# The number of fraudulent transaction seems to be higher in the fourth month but still the percentage to fraud transaction to the total seems to be higer in 7th month followed by 2nd month. Another interesting thing to be noted is the month 7 has only fewer transaction but still the fraud transaction conntribution is higher. May be most of the number of fraud transaction occur in the begining of the month.  Month 1 has the most number of transaction wich can be due to some festive season.\n","7cf04e6a":"References \n\nhttps:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n","45a497b2":"Analysis of Transaction Date, the values are given in time delta in reference to some starting point. \nEarliest delta time available 86400. Which is 1 day from reference point\nEarliest delta time available 15811131. which is 182 days from referencce point\nTotal duration available 182 Days. Which is 6 months of total observation.","931d828b":"we have a huge number of NaN","7ac903e2":"The correlation matrix created had interesting observations. There were multi collinearity between many independent variables, we can drop it but I  would like to hold it till performing PCA and to get the maximum information after dropping Columns ","b3ad16a4":"We notice there are outliers present but we are skipping for now as we are planning to go with random Forest and Decision Tree ","6aaa8f8e":"<font color=\"blue\"> These two dataset needs to be joined based on TransactionID, but before we do that lets see if every feature provides a worthy information, check for null values and outliers and handle it and join both the dataset <\/font>\n","1589e45e":"# Data Description as provided - Transaction Datset\n1. TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n2. TransactionAMT: transaction payment amount in USD\n3. ProductCD: product code, the product for each transaction\n4. card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n5. addr: address\n6. dist: distance\n7. P_ and (R__) emaildomain: purchaser and recipient email domain\n8. C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n9. D1-D15: timedelta, such as days between previous transaction, etc.\n10. M1-M9: match, such as names on card and address, etc.\n11. Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n# Categorical Features:\n1. ProductCD\n2. card1 - card6\n3. addr1, addr2\n4. Pemaildomain Remaildomain\n5. M1 - M9","c956c4e9":"From theobservation made above we can say that there are few columns where more than 75% of the values in the respective columns have null values. Handling such columns by imputing by measures of central tendency may not yield good result. Other option we can opt for is to predict the missing values using machine learning algorithm, but it is also observed that other columns which can be used for predicting the columns with null values also has null value mostly which would not help us in getting a good prediction for imputaion purpose. \nThus we are handling  the null values in two ways.\n1. Dropping the columns with more than 75% of null values\nIt does not make a good sense or would it be helpful if we impute such columns with measure of central tendency. Intuitively  we can say that these variables could probably be some new observations that has come in to practice and hence may not hold value for old customers. Hence it is better to drop these columns rather than imputing them which would be of no use.\n2. Imputing the remaining columns with Measures of central tendency \nThe best option we can go with is to impute thhe columns with measures of central tendency. \n","9b174476":"Payment card fraud is one of the major ethical issues in the banking industry. There are different types of payment card frauds, \nPayment card fraud is a serious and long-term threat to society with a huge economic impact. Frauds are increasing dramatically with the progression of modern technology and global communication. As a result, fighting fraud has become an important issue to be explored.\n\nBuilding an efficient Machine Learning model to dynamically flag a fraudulent transaction requires the following steps\n1. Acquiring data\n2. Data Preprocessing.\n3. Exploratory Data Analysis.\n4. Principal Component Analysis.\n5. Model building and training.\n\n**Acquiring data**\nThe data comes from Vesta's real-world e-commerce transactions. It comes under 2 categories which has Transaction ID as the common feature between them.\n          Transaction - This dataset holds the data for all the real time transactions that were recorded\n          Identity -        Variables in this table are identity information \u2013 network connection         \t         information  (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, \t         etc) associated with transactions\n\n**Data Preprocessing.**\nThe data is cleansed by handling the missing values and treating outliers.\n\n**Missing Value:**\n\nThe dataset has a huge proportion of null values, I have handled the missing values using two approaches\n1. Dropping columns that has more than 75% of null values.\n2. Imputing the null values with appropriate measures of central tendency.\n\nNote:  I made required analysis to impute the missing values for by predicting the values using suitable Machine Learning algorithms. But my observations showed all the required columns which could be used to predict and impute also had null values. So I opted for imputing by measures of central tendency. \n\n**Exploratory Data Analysis.**\n\nDetailed EDA with code and explanation can be found below.\n\n**Principal Component Analysis.**\n\nPrincipal component analysis (PCA) is a technique for reducing the dimensionality of the dataset, increasing interpretability but at the same time minimizing information loss. This helps in reducing the complexity and helps the model to perform with better performance.  The entire process of model building has been taken care with different trial and error basis. The approaches are as follows\n\n1. Without Outlier treatment and with imbalanced data\n2. Without Outlier treatment and with balanced data (By performing Upsampling)\n3. With Outlier treatment and with imbalanced data\n4. With Outlier treatment and with balanced data (By performing Upsampling).\n\nEach of these approaches has its own impact on the number of principal components required. \nWithout Outlier treatment and with imbalanced data\nNumber of columns retained  for 99% of info retained- 143.\nWithout Outlier treatment and with balanced data (By performing Upsampling)\nNumber of columns retained  for 99% of info retained- 142\nWith Outlier treatment and with imbalanced data\nNumber of columns retained  for 99% of info retained- 98\nWith Outlier treatment and with balanced data (By performing Upsampling)\nNumber of columns retained  for 99% of info retained- 99\n\n**Model building and training.**\n\nModels considered to use\n\nRandom Forest - Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. In two dimensional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n\n\nXGBoost - Extreme Gradient Boosting is similar to gradient boosting framework but more efficient. It  has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine.\n\n\nLight GBM - LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency; lower memory usage; better accuracy; support of parallel and GPU learning; capable of handling large-scale data.\n\n\n\n\n\n\n","aa5cde19":"# The number of transaction between legit and fraudulent transaction shows the dataset is an imbalance dataset which has only 3.6 % of data for the class of interest. when checking for the transaction amount also we notice the same.","3f0fb6ec":"# To further make our analysis we group the data into Day, Day of Week, Month, Day of Month, Hour of Day to understand the distribution of data","7019cd73":"# Data Description as provided - Identity Dataset\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\nThey're collected by Vesta\u2019s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n# Categorical Features:\n1. DeviceType\n2. DeviceInfo\n3. id12 - id38"}}