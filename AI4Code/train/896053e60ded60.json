{"cell_type":{"0784e558":"code","c1a800ab":"code","685b2047":"code","18edf3d6":"code","17f8eb81":"code","2edcafd9":"code","699211fb":"code","e995ba6e":"code","144c9d16":"code","a6e85f1d":"code","f8a04c94":"code","a578252d":"code","e10d0c30":"code","ca68e5fa":"code","397031b2":"code","5289b4d9":"code","c7aabf2b":"code","df924a81":"code","85986e79":"code","10abe8c3":"code","074c2ec4":"code","936b0ba9":"code","941b3d6b":"code","5579503a":"code","395dc0db":"code","bcd9195c":"code","5bc136b3":"code","ceff7770":"code","7e6d8b32":"code","75d1fdc3":"code","b218c2a7":"code","6c343a20":"code","679b1144":"code","3768f32f":"code","cdf1b215":"code","ee6e462e":"code","010824d1":"code","7758c8d5":"code","36d344c3":"code","51478e4c":"code","5b8ca466":"code","b4f91aaf":"code","0a141165":"code","220c5b2c":"code","a548a1f3":"code","6da61315":"code","80479812":"code","4fc864ab":"code","32f42724":"code","9fdf8498":"code","17276712":"code","00ae3abe":"code","19e21aa5":"markdown","c2b8ab94":"markdown","6f7ed926":"markdown","5a5a39d0":"markdown","23b02f25":"markdown","48efddf7":"markdown","613c36b5":"markdown","b12035ed":"markdown"},"source":{"0784e558":"#import necessary libaries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n#we want the plots to appear inside the notebook\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n#evaluation\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score,recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')","c1a800ab":"#load dataset\ndf=pd.read_csv(\"..\/input\/heartdisease\/heart-disease.csv\")\ndf.shape #rows,colulns","685b2047":"df.head()","18edf3d6":"#check if dataset is balanced or unbalanced\ndf.target.value_counts()","17f8eb81":"#plot value counts\nax=df.target.value_counts(normalize=True).plot(kind=\"bar\",color=[\"#FF7F60\",\"#3EB2A2\"],figsize=(10,6),);\nax.bar_label(ax.containers[0],fmt='%.2f',fontsize=15)","2edcafd9":"#Check for nulls\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis',vmax=1,vmin=-1)","699211fb":"df[df.target==1].describe()","e995ba6e":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=[\"#FF7F60\",\"#3EB2A2\"])\n# Add some attributes to it\nplt.title(\"Heart Disease Frequency By Gender\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Count\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0); # keep the labels on the x-axis vertical\nplt.show()","144c9d16":"# Create another figure\nplt.figure(figsize=(10,6))\n# Start with positve examples\nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], alpha=1,\n            c=\"#FF7F60\") \n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], alpha=0.3,\n            c=\"#3EB2A2\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","a6e85f1d":"#histogram of age\nsns.displot(df[df['target'] == 1]['age'],kde=False)","f8a04c94":"#heart disease per chest pain type\n# Create a new crosstab and base plot\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"#3EB2A2\",\"#FF7F60\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"0: Typical angina, 1: Atypical angina, 2: Non-anginal pain, 3: Asymptomatic\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","a578252d":"#Correlation Matrix\n# Let's make it look a little prettier\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 8))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"Spectral\");  #YlGnBu","e10d0c30":"df.columns","ca68e5fa":"X= df[['age', 'sex', 'cp', 'trestbps', 'restecg', 'thalach','chol',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal']]\ny=df.target.values","397031b2":"X.head()","5289b4d9":"# Random seed for reproducibility\nnp.random.seed(42)\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","c7aabf2b":"y_train,len(y_train)","df924a81":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(solver='liblinear'), \n          \"XGBoost\": XGBClassifier(),\n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train,y_test ):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","85986e79":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test\n                            )\nmodel_scores\n","10abe8c3":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar(color=\"#3EB2A2\")","074c2ec4":"# Create a list of train scores\ntrain_scores = []\n# Create a list of test scores\ntest_scores = []\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n# Setup algorithm\nknn = KNeighborsClassifier()\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n      # Fit the algorithm\n    knn.fit(X_train, y_train)\n        # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n        # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))\n    ","936b0ba9":"train_scores","941b3d6b":"# Plot Scores\nplt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","5579503a":"#Tuning models with with RandomizedSearchCV\n# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","395dc0db":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model\nrs_log_reg.fit(X_train, y_train);","bcd9195c":"rs_log_reg.best_params_","5bc136b3":"rs_log_reg.score(X_test, y_test)","ceff7770":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","7e6d8b32":"# Find the best parameters\nrs_rf.best_params_","75d1fdc3":"# Evaluate the randomized search random forest model\nrs_rf.score(X_test, y_test)","b218c2a7":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","6c343a20":"# Check the best parameters\ngs_log_reg.best_params_","679b1144":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","3768f32f":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)","cdf1b215":"# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","ee6e462e":"# Display confusion matrix\n#print(confusion_matrix(y_test, y_preds))\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_confusion_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_confusion_mat(y_test, y_preds)","010824d1":"# Show classification report\nprint(classification_report(y_test, y_preds))","7758c8d5":"# Check best hyperparameters\ngs_log_reg.best_params_","36d344c3":"# Instantiate best model with best hyperparameters (found with GridSearchCV)\nlog_reg_model = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","51478e4c":"# Cross-validated accuracy score\ncv_acc = cross_val_score(log_reg_model,\n                         X,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\") # accuracy as scoring\ncv_acc","5b8ca466":"cv_acc = np.mean(cv_acc)\ncv_acc","b4f91aaf":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(log_reg_model,\n                                       X,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision","0a141165":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(log_reg_model,\n                                    X,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","220c5b2c":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(log_reg_model,\n                                X,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","a548a1f3":"cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.head()","6da61315":"# Visualizing cross-validated metrics\ncv_metrics.T.plot.bar(title=\"Cross Validated Metrics\", legend=False,color=\"#3EB2A2\");\n","80479812":"#Visualize\nsns.barplot(data=cv_metrics,palette = 'Paired',)\nplt.title('Cross Validated Scores')","4fc864ab":"# Fit an instance of LogisticRegression (taken from above)\nlog_reg_model.fit(X_train, y_train);","32f42724":"# Check coef_\nlog_reg_model.coef_","9fdf8498":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(log_reg_model.coef_[0])))\nfeatures_dict","17276712":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False,color=\"#0477BF\",figsize=(12,8));","00ae3abe":" #Joblib\nfrom joblib import dump,load\n#save to file\ndump(log_reg_model,filename=\"Logistic_Regression_Heart_Disease_Model2.joblib\")","19e21aa5":"# 2. EXPL0RATORY DATA ANALYSIS  ","c2b8ab94":"# Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand","6f7ed926":"# Evaluating a classification model, beyond accuracy","5a5a39d0":"# 4. Feature importance","23b02f25":"# 3.  MODELLING","48efddf7":"# Tuning a model with GridSearchCV","613c36b5":"# Model Comparison","b12035ed":"<h1><center><font size=80 color=#F07167>PREDICTING HEART DISEASE<\/font><\/center><\/h1>\n\n# 1. PROBLEM DEFINITION \n<h1><font color=#00AFB9> Given clinical parameters about a patient, can we predict whether or not they have heart disease?<\/font><\/center><\/h1>\n\n## The following are the features used to predict our target variable (heart disease or no heart disease).\n\n1.\t**age** - age in years\n2.\t**sex** - (1 = male; 0 = female)\n3.\t**cp** - chest pain type\n<ol>0: Typical angina: chest pain related decrease blood supply to the heart<\/ol>\n<ol>1: Atypical angina: chest pain not related to heart<\/ol>\n<ol>2: Non-anginal pain: typically esophageal spasms (non heart related)<\/ol>\n<ol>3: Asymptomatic: chest pain not showing signs of disease<\/ol>\n4.\t**trestbps** - resting blood pressure (in mm Hg on admission to the hospital)\n\u2022\tanything above 130-140 is typically cause for concern\n5.\t**chol** - serum cholestoral in mg\/dl\n*\tserum = LDL + HDL + .2 * triglycerides\n*\tabove 200 is cause for concern\n6.\t**fbs** - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n*\t'>126' mg\/dL signals diabetes\n7.\t**restecg** - resting electrocardiographic results\n<ol>0: Nothing to note<\/ol>\n<ol>1: ST-T Wave abnormality<\/ol>\n\tcan range from mild symptoms to severe problems\tsignals non-normal heart beat\n<ol>2: Possible or definite left ventricular hypertrophy<\/ol>\n\uf0a7\tEnlarged heart's main pumping chamber\n8.\t**thalach** - maximum heart rate achieved\n9.\t**exang** - exercise induced angina (1 = yes; 0 = no)\n10.\t**oldpeak** - ST depression induced by exercise relative to rest\n<ol>looks at stress of heart during excercise<\/ol>\n<ol>unhealthy heart will stress more<\/ol>\n11.\t**slope** - the slope of the peak exercise ST segment\n<ol>0: Upsloping: better heart rate with excercise (uncommon)<\/ol>\n<ol>1: Flatsloping: minimal change (typical healthy heart)<\/ol>\n<ol>2: Downslopins: signs of unhealthy heart<\/ol>\n12.\t**ca** - number of major vessels (0-3) colored by flourosopy\n<ol>colored vessel means the doctor can see the blood passing through<\/ol>\n<ol>the more blood movement the better (no clots)<\/ol>\n13.\t**thal** - thalium stress result\n<ol>1,3: normal<\/ol>\n<ol>6: fixed defect: used to be defect but ok now<\/ol>\n<ol>7: reversable defect: no proper blood movement when excercising<\/ol>\n14.\t**target** - have disease or not \n<ol>1=yes<\/ol>\n<ol>0=no)<\/ol>\n(= the predicted attribute)\n"}}