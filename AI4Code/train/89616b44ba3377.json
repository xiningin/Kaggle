{"cell_type":{"4a5c82de":"code","d23cbeed":"code","8102af6a":"code","cf10b0e0":"code","44b38df1":"code","98f028b1":"code","209ed076":"code","d43c0f89":"code","79375544":"code","0fc3ae97":"code","652f5d87":"code","a49d3651":"code","e5398ea8":"code","d423f977":"code","93865476":"code","b4f011c4":"code","bb11f9f3":"code","4a46b4c8":"markdown","c26fd997":"markdown","b23052cb":"markdown"},"source":{"4a5c82de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d23cbeed":"df=pd.read_csv(\"..\/input\/heart-disease-prediction\/Heart_Disease_Prediction.csv\")","8102af6a":"df.head(15)","cf10b0e0":"df.isnull().sum()","44b38df1":"label=df[\"Heart Disease\"]\ndf.drop(\"Heart Disease\",axis=1,inplace=True)","98f028b1":"label.value_counts().plot(kind=\"bar\")","209ed076":"df.dtypes","d43c0f89":"categorical_features=[\"Sex\",\"Chest pain type\",\"FBS over 120\",\"EKG results\",\"Exercise angina\",\"Slope of ST\",\"Number of vessels fluro\",\"Thallium\"]\n\ndf[categorical_features]=df[categorical_features].astype(\"category\")","79375544":"df.dtypes","0fc3ae97":"continuous_features=set(df.columns)-set(categorical_features)\nscaler=StandardScaler()\ndf_norm=df.copy()\ndf_norm[list(continuous_features)]=scaler.fit_transform(df[list(continuous_features)])\n#df_norm=pd.DataFrame(df_norm,columns=list(continuous_features))","652f5d87":"df_norm_dummies=pd.get_dummies(df_norm)","a49d3651":"df_norm_dummies","e5398ea8":"X_train,X_test,y_train,y_test=train_test_split(df_norm_dummies,label,test_size=0.15,stratify=label,random_state=10)","d423f977":"models=[SVC(),LogisticRegression(),GaussianNB(),DecisionTreeClassifier()]\n\nfor model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap=plt.cm.Blues)\n    plt.show()\n    print(classification_report(y_test,y_pred))","93865476":"plt.figure(figsize=(15,15))\nsns.heatmap(df_norm.corr(),cmap=plt.cm.Blues,annot=True)","b4f011c4":"sns.pairplot(df_norm[continuous_features])","bb11f9f3":"n_dimensions=[i for i in range(2,len(df.columns))]\n\nfor dim in n_dimensions:\n    pca=PCA(dim)\n    df_norm_reduc=pd.DataFrame(pca.fit_transform(df_norm),columns=[i for i in range(dim)])\n    \n    X_train,X_test,y_train,y_test=train_test_split(pd.get_dummies(df_norm_reduc),label,test_size=0.15,stratify=label,random_state=10)\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap=plt.cm.Blues)\n    plt.show()\n    print(classification_report(y_test,y_pred))","4a46b4c8":"###### We can see from the above <strong>**confusion matrix**<\/strong> and <strong>**classification report**<\/strong> that the model which reached the best result is the **Support Vector Machine Classifier** with an accuracy of <strong>**93%**<\/strong>\n\nLet us now try to explore and view the data from another angle and perspective.","c26fd997":"After running Principal Component Analysis on the dataset, trying different dimensions and training the SVC model which performed best on all set of features. We can observe that the accuracy score did not get better or even get closer to the optimal accuracy we reached so far which is **93%**.","b23052cb":"From the correlation matrix and the pair-element plotting in seaborn we can see that some features are somehow correlated ( eventhough it is a weak correlation ) like **Age-Cholesterol** | **BP-ST Depression**. \n\nWe will try to perform some Dimensionality reduction on the dataset and evaluate the model for each dimension."}}