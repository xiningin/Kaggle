{"cell_type":{"6f70b441":"code","b180695a":"code","d9f6640d":"code","254c4ed2":"code","a2f35ba9":"code","752f557d":"code","a548cb42":"code","fe3e74fb":"code","7915eeae":"code","cf5c961a":"code","7578019a":"code","23670706":"code","b165d3ac":"code","04b85cb6":"code","77841d90":"code","edce09bb":"code","ffc586fc":"code","7929e47d":"code","42935077":"code","63856e1b":"code","32fad537":"code","cdf8ba97":"code","e4079adf":"code","77c57fd0":"code","38adb5ae":"code","89fbab41":"code","09c42312":"code","a8629fb9":"code","5fd317c1":"code","0e30c2cb":"code","00826382":"code","8f81e4f9":"code","72983809":"code","03079a68":"code","fd3ecdb3":"code","31cf69d9":"code","2d2498d9":"code","6413e33d":"code","948a706c":"code","9eb9d752":"code","270bc10e":"code","8a6bfdf3":"code","e5982582":"code","12f13d91":"code","2386ccf8":"code","1ad7a565":"markdown","7c95bf3b":"markdown","42f33929":"markdown","81a327c9":"markdown","e42f7ae6":"markdown","f8432de2":"markdown","84649acc":"markdown","df3a2880":"markdown","66a469c3":"markdown","6abd293b":"markdown","84106a77":"markdown","63000f4a":"markdown","8b17c650":"markdown","107cd87d":"markdown","ea9bec7b":"markdown","ebb2dd38":"markdown","5978e27c":"markdown","f4128697":"markdown","ee7aab20":"markdown","9c3c5a4d":"markdown","a574e8e1":"markdown","5ccd0005":"markdown","5795a184":"markdown","9d4aebee":"markdown","31dc6b8a":"markdown","b8e65ebf":"markdown","4f9c0fd5":"markdown","ba9e7a02":"markdown","cd733119":"markdown","7e5bb360":"markdown","957f1f93":"markdown","d5f625f0":"markdown","89951ea6":"markdown","418529b5":"markdown","a39d8208":"markdown","3d22ae11":"markdown","e3d0cd62":"markdown","b346b1e5":"markdown","2ea7ff45":"markdown","774aee3c":"markdown"},"source":{"6f70b441":"from IPython.display import HTML\n\nHTML('<center><iframe width=\"1000\" height=\"450\" src=\"https:\/\/www.youtube.com\/embed\/sK-HqhT6nA0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","b180695a":"HTML('<center><iframe width=\"1000\" height=\"450\" src=\"https:\/\/www.youtube.com\/embed\/3_jjS3x3oC0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","d9f6640d":"# basic \nimport glob\nimport os, gc\nimport warnings\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","254c4ed2":"# directory\nprint('Competition Data\/Files')\nROOT = '..\/input\/optiver-realized-volatility-prediction\/'\nos.listdir(ROOT)","a2f35ba9":"# Weighted Average Price\ndef WAP(dataframe):\n    numerator = dataframe['bid_price1'] * dataframe['ask_size1'] + dataframe['ask_price1'] * dataframe['bid_size1']\n    denominator = dataframe['bid_size1']+ dataframe['ask_size1']\n    return numerator \/ denominator\n\n# Log Return\ndef log_return(wap_prices):\n    return np.log(wap_prices).diff()\n\n# Realized Volatility\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","752f557d":"book_train = pd.read_parquet(os.path.join(ROOT, 'book_train.parquet'))\nbook_train.head()","a548cb42":"print(f'We have {book_train.shape[0]\/1e6} million rows and {book_train.shape[1]} features in book_train.parquet.')","fe3e74fb":"book_train.info()","7915eeae":"print('Unique Values in each column of book_train.parquet')\nprint('##########################################')\nfor col in book_train:\n    print(f'{col}: {book_train[col].nunique()}')","cf5c961a":"print(f'Missing values in book_train.parquet in each columns:\\n{book_train.isnull().sum()}')","7578019a":"book_test = pd.read_parquet(os.path.join(ROOT, 'book_test.parquet'))\ngc.collect()","23670706":"print(\"Unique 'stock_id' book_test.parquet.\")\nprint('##########################################')\nprint(f\"{'Stock ID'}: {book_test.stock_id.nunique()}\")\nprint(f\"{'Time ID'}: {book_test.time_id.nunique()}\")","b165d3ac":"book_train_stats = book_train.groupby(\"stock_id\").agg({\n    \"bid_price1\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"ask_price1\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"bid_price2\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"ask_price2\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"bid_size1\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"ask_size1\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"bid_size2\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"ask_size2\": [\"min\", \"max\", \"mean\", \"var\"]\n}).reset_index()\nbook_train_stats.sample(4)","04b85cb6":"trade_train = pd.read_parquet(os.path.join(ROOT, 'trade_train.parquet'))\ngc.collect()\ntrade_train.head(3)","77841d90":"print(f'We have {trade_train.shape[0]\/1e6} million rows and {trade_train.shape[1]} features in trade_train.parquet.')","edce09bb":"print('Unique Values in each column of trade_train.parquet')\nprint('###################################################')\nfor col in trade_train:\n    print(f'{col}: {trade_train[col].nunique()}')","ffc586fc":"print(f'Missing values in trade_train.parquet in each columns:\\n{trade_train.isnull().sum()}')","7929e47d":"trade_train_stats = trade_train.groupby(\"stock_id\").agg({\n    \"price\": [\"min\", \"max\", \"mean\", \"var\"],\n    \"order_count\": [\"min\", \"max\", \"mean\", \"var\"]\n}).reset_index()\ntrade_train_stats.sample(4)","42935077":"trade_test = pd.read_parquet(os.path.join(ROOT, 'trade_test.parquet'))\ngc.collect()\ntrade_test","63856e1b":"train = pd.read_csv(os.path.join(ROOT, 'train.csv'))\ntrain.head()","32fad537":"test = pd.read_csv(os.path.join(ROOT, 'test.csv'))\ntest","cdf8ba97":"submission = pd.read_csv(os.path.join(ROOT, 'sample_submission.csv'))\nsubmission","e4079adf":"minn = book_train['time_id'].min()\nmean = book_train['time_id'].mean()\nmaxx = book_train['time_id'].max()\n\nmin_2 = book_train['seconds_in_bucket'].min()\nmean_2 = book_train['seconds_in_bucket'].mean()\nmax_2 = book_train['seconds_in_bucket'].max()","77c57fd0":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    book_train['time_id'].hist(bins = 50,color='green')\n    ax.axvline(minn, color='r', linestyle='--')\n    ax.axvline(mean, color='y', linestyle='-')\n    ax.axvline(maxx, color='b', linestyle='-')\n    plt.legend({'Min':minn,'Mean':mean,'Max':maxx})\n    plt.title(\"Time ID Distribution\")\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    book_train['seconds_in_bucket'].hist(bins = 50,color='gold')\n    ax.axvline(min_2, color='r', linestyle='--')\n    ax.axvline(mean_2, color='y', linestyle='-')\n    ax.axvline(max_2, color='b', linestyle='-')\n    plt.legend({'Min':min_2,'Mean':mean_2,'Max':max_2})\n    plt.title(\"Secounds in bucket Distribution\")","38adb5ae":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    book_train['bid_price1'].hist(bins = 60,color='tomato')\n    plt.title(\"Bid Price 1 Distribution\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    book_train['ask_price1'].hist(bins = 60,color='orange')\n    plt.title(\"Ask Price 1 Distribution\")","89fbab41":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    book_train['bid_price2'].hist(bins = 60,color='blueviolet')\n    plt.title(\"Bid Price 2 Distribution\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    book_train['ask_price2'].hist(bins = 60,color='powderblue')\n    plt.title(\"Ask Price 2 Distribution\")","09c42312":"book_train_5_7 = book_train[(book_train.stock_id == 7) & (book_train.time_id == 5)]\nbook_train_5_7.head()","a8629fb9":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['bid_price1'])\n    plt.title(\"Most competitive buy level per second\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['ask_price1'])\n    plt.title(\"Most competitve sell level per second\")","5fd317c1":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['bid_price2'])\n    plt.title(\"Second most competitive buy level per second\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['ask_price2'])\n    plt.title(\"Second most competitve sell level per second\")","0e30c2cb":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['bid_price1'])\n    ab = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['ask_price1'])\n    plt.title(\"Most competitive buy\/sell level per second\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['bid_price2'])\n    ab = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['ask_price2'])\n    plt.title(\"Second most competitve buy\/sell level per second\")","00826382":"gc.collect()","8f81e4f9":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 3)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    trade_train['time_id'].hist(bins = 50,color='green')\n    plt.title(\"Time ID Distribution\")\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    trade_train['seconds_in_bucket'].hist(bins = 50,color='gold')\n    plt.title(\"Secounds in bucket Distribution\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 2])\n    trade_train['price'].hist(bins = 50,color='slategrey')\n    plt.title(\"Average price of executed transactions per second\")","72983809":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    train['target'].hist(bins = 50,color='cyan')\n    plt.title(\"Target Distribution\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    np.log10(train['target']).hist(bins = 50,color='olive')\n    plt.title(\"Logarithmic Distribution of Target\")","03079a68":"gc.collect()","fd3ecdb3":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    ay = sns.countplot(y = book_train_5_7['bid_size1'], order=book_train_5_7.bid_size1.value_counts().index[:20], palette=\"crest\")\n    plt.title(\"Top 20 Shares On Competitive Buy Level\")\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.countplot(y = book_train_5_7['ask_size1'], order=book_train_5_7.ask_size1.value_counts().index[:20], palette=\"tab20c\")\n    plt.title(\"Top 20 Shares On Competitive Sell Level\")","31cf69d9":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.countplot(y = book_train_5_7['bid_size2'], order=book_train_5_7.bid_size2.value_counts().index[:20], palette=\"twilight_shifted\")\n    plt.title(\"Top 20 Shares On Second Most Competitive Buy Level\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.countplot(y = book_train_5_7['ask_size2'], order=book_train_5_7.ask_size2.value_counts().index[:20], palette=\"RdYlBu\")\n    plt.title(\"Top 20 Shares On Second Most Competitive Sell Level\")","2d2498d9":"trade_train_5_7 = trade_train[(trade_train.stock_id == 7) & (trade_train.time_id == 5)]\ntrade_train_5_7.head()","6413e33d":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.countplot(y = trade_train_5_7['size'], palette=\"bone\")\n    plt.title(\"Number Of Shares Traded\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.countplot(trade_train_5_7['order_count'], palette=\"flag\")\n    plt.title(\"Number Of Unique Trade Order\")","948a706c":"book_train_5_7['wap'] = WAP(book_train_5_7)\nbook_train_5_7['log_return'] = log_return(book_train_5_7['wap'])\nbook_train_5_7 = book_train_5_7[~book_train_5_7['log_return'].isnull()]","9eb9d752":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['wap'])\n    plt.title(\"Weighted Average Price of stock_id 7 at time_id 5\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.lineplot(book_train_5_7['seconds_in_bucket'], book_train_5_7['log_return'])\n    plt.title(\"Log Return of stock_id 7 at time_id 5\")","270bc10e":"print(f\"Realized Volatility for stock_id 0 at time_id 5 is: {realized_volatility(book_train_5_7['log_return'])}\")","8a6bfdf3":"book_train_7 = book_train[book_train.stock_id == 7]\nbook_train_7.sample(5)","e5982582":"book_train_7['wap'] = WAP(book_train_7)\nbook_train_7.groupby(['time_id'])\nbook_train_7['log_return'] = log_return(book_train_7['wap'])\nbook_train_7 = book_train_7[~book_train_7['log_return'].isnull()]\ndf_realized_vol_per_stock =  book_train_7.groupby(['time_id'])['log_return'].agg(realized_volatility).reset_index()\ndf_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':'pred'})","12f13d91":"df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{7}-{x}')\ndf_realized_vol_per_stock.drop('time_id', axis=1).head()","2386ccf8":"f = plt.figure(figsize=(16, 8))\ngs = f.add_gridspec(1, 2)\n\nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 0])\n    aa = sns.lineplot(book_train_7['seconds_in_bucket'], book_train_7['wap'])\n    plt.title(\"Weighted Average Price of stock_id 7 at time bucket\")\n    \nwith sns.axes_style(\"whitegrid\"):\n    ax = f.add_subplot(gs[0, 1])\n    aa = sns.lineplot(book_train_7['seconds_in_bucket'], book_train_7['log_return'])\n    plt.title(\"Log Return of stock_id 7 at each time bucket\")","1ad7a565":"## 5.2 <a id='5.2'>Categorial Features<\/a>\n[Tables of contents](#0.1)\n\n### 5.2.1 <a id='5.2.1'>Book Parquet<\/a>","7c95bf3b":"**\ud83d\udccc Points to note :**\n   * We can see the ditribution of **stock_id** and **time_id** is same it is above in **[book_train.parquet](#5.1.1)**.\n   * The average **price** are normalized as we can see in the graph. ","42f33929":"### Missing Values","81a327c9":"**\ud83d\udccc Points to note :**\n\nFor stock_id of 7 and time_id of 5,\n   * 10 shares at second most competitive buy level.\n   * 131 shares at second most competitive sell level.","e42f7ae6":"# 6 <a id='6'>Realized Volatility Calculation<\/a>\n[Table of contents](#0.1)\n\nOur target in this competition is to predict short-term **realized volatility**. The realized volatility is defined as the squared root of the sum of squared log returns. \n\n$$\n\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\n$$\n\nWhere, \n\n$$r_{t-1, t}$$ is called **Log Returns**. \n\nCalculating **realized volatility** involves certain steps. We will see step by step how it is done. \n   * We will calculate Weighted Averaged Price (WAP), to calculate the instantaneous stock valuation and calculate realized volatility as our target. WAP is calculated using top level price and volume (number of shares) into account. We are provided with top two levels of book data (order book). An order book lists the number of shares being bid on or offered at each price point.\n   \n$$ WAP = \\frac{BidPrice_{1}*AskSize_{1} + AskPrice_{1}*BidSize_{1}}{BidSize_{1} + AskSize_{1}} $$\n\n   * In simple terms **Log Return** is a way to compare the price difference. Also, log returns are preferred whenever some mathematical modelling is required. Let $S_t$ the price of the stock $S$ at time $t$, we can define the log return between $t_1$ and $t_2$ as:\n  \n$$\nr_{t_1, t_2} = \\log \\left( \\frac{S_{t_2}}{S_{t_1}} \\right)\n$$\n\nFor more information check [here](https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data#Order-book-statistics). Below we will calculate **\"Realized Volatility\" for stock_id 7 at time_id 5**. Let us first calculate WAP. ","f8432de2":"### 5.2.2 <a id='5.2.2'>Trade Parquet<\/a>\n[Table of contents](#0.1)\n\nSame as above we will only take subset of data with same **stock_id** and **time_id**.","84649acc":"## What is Market Maker ?\n\n[Table of contents](#0.1)\n\nA market maker (MM) is a firm or individual who actively quotes two-sided markets(A two-sided market exists when both buyers and sellers meet to exchange a product or service) in a security, providing bids and offers (known as asks) along with the market size of each.\n\n[source](https:\/\/www.investopedia.com\/terms\/m\/marketmaker.asp#)\n\n## What is Volatility ?\n\n[Table of contents](#0.1)\n\nIn most simple term **Volatility is associated with the amount of risk or uncertainty concerning how the stock markets will move.**\n\nVolatility is typically measured using either **standard deviation** or **variance**. In either case, the higher the value, the more volatile are the prices or the returns.  It means that a high standard deviation value suggests that prices are spread across a wide spectrum. Conversely, a low standard deviation value indicates that prices are closely knit across a narrow range.\n\n[source](https:\/\/groww.in\/p\/volatility\/)","df3a2880":"### Trade Train Statistics","66a469c3":"### Unique Values","6abd293b":"# WORK IN PROGRESS","84106a77":"**\ud83d\udccc Points to note :**\n   * We have **167 million rows** and  **11** features in book_train.parquet file.\n   * We have **112 unique stocks in book_train and only 1 in book_test**.\n   * Great, We don't have any missing values in any of file.\n   * Lastly, we have generated some meaningful statistics (min, max, mean and variance) column wise.","63000f4a":"## 4.5 <a id='4.5'>Submission<\/a>\n[Table of contents](#0.1)\n\nThis is how our submission file should be. It has **row_id** and **target** columns.","8b17c650":"# 2. <a id='2'>Import Packages<\/a>\n[Tabale of contents](#0.1)","107cd87d":"<center><img src=\"data:image\/png;base64,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\" width=\"600\" height=\"400\"><\/center>\n\n\n## <center>Optiver Realized Volatility Prediction<\/center>\n### <center>\ud83d\udcc8Apply your data science skills to make financial markets better\ud83d\udcc9<\/center>","ea9bec7b":"**\ud83d\udccc Points to note :**\n\nFor stock_id of 7 and time_id of 5,\n   * We see that most often 1 share traded.\n   * There are 7 unique trade orders","ebb2dd38":"### Unique Values","5978e27c":"## 6.1 <a id = '6.1'>Naive prediction: Stock ID 7<\/a>\n[Table of contents](#0.1)\n\nLet us calculate **Realized Volatility** per stock. In thi case we will consider for **stock_id = 7** at each **time_id**. We will understand this step by step to develop better understanding of how things are working under the hood. \n\n### STEPS TO CALCULATE REALIZED VOLATILITY PER STOCK\n   * First we will filter out samples where the **stock_id** is 7 from **book order** data. ","f4128697":"Since the file is very huge and consists of 167 million rows we will only select few cases where **stock_id** is 7 and **time_id** is 5 for better understanding. ","ee7aab20":"Since we are only dealing with **time_id=5** so we will only get single value. ","9c3c5a4d":"## 4.3 <a id='4.3'>Train Data<\/a>\n[Table of contents](#0.1)\n\nThis file has three columns both **stock_id** and  **time_id** are same as in above files and there is **target** column which can be linked with above files using same **stock_id** and  **time_id**. ","a574e8e1":"# 4. <a id='4'>Data Overview<\/a>\n[Table of contents](#0.1)\n\n## 4.1 <a id='4.1'>Book Parquet Files<\/a>\n**book_train.parquet\/book_test.parquet** - Both these files provide the raw order book data which can be **linked with the \"target\" in train\/test.csv** using **time_id and stock_id** columns. These parquet files are paritioned by **stock_id**. Let's quickly check how many **unique \"stock_id and time_id\"** are there in book_train\/test files. Let us have a glimpse of data.","5ccd0005":"# 5. <a id='5'>Individual Feature Distribution<\/a>\n[Table of contents](#0.1)\n\nWe are now going to start our EDA. We will start with developing intuition for continous feature distribution across all the available data first. \n\n## 5.1 <a id='5.1'>Continuous Feature Distribution<\/a>\n## 5.1.1 <a id='5.1.1'>Book Train Feature Distribution<\/a>","5795a184":"**\ud83d\udccc Points to note :**\n\nFor stock_id of 7 and time_id of 5,\n   * Most and second most competitive buy\/sell level at span of 600 seconds. ","9d4aebee":"**\ud83d\udccc Points to note :**\n\nFor stock_id of 7 and time_id of 5,\n   * The price fluctuations at most\/second most competitive buy\/sell level. We are provided with top two level in order book data.\n   * **seconds_in_bucket** range from 0-600.","31dc6b8a":"   * We will calculate **Weighted Aerage Price (WAP)**. The formula for clculating **WAP** is [here](#6). \n   * After that we will group data on basis of **time_id** and then we will calculate **Log Returns**.","b8e65ebf":"   * Group the data again using **time_id** and calculate **Realized Volatility** at each **time_id** for particular **stock**.\n   * Finally, as seen below we can generate our **submission file** with **row_id** and **pred** columns. The **row_id** is unique identifier for the submission row. We have one row for each existing time ID\/stock ID pair. In our case stock_id is 7 and time_id includes all available time bucket.","4f9c0fd5":"## 5.1.2 <a id='5.1.2'>Train Feature Distribution<\/a>\n[Tables of contents](#0.1)","ba9e7a02":"### Trade Test\n\nTest trade data.","cd733119":"## 4.2 <a id='4.2'>Trade Parquet Files<\/a>\n[Table of contents](#0.1)\n\n**trade_train.parquet\/trade_test.parquet** - These files contain trade data and are more sparse than the order book since it contains data on trade that actually executed. Usually, in the market, there are more passive buy\/sell intention updates (book updates) than actual trades. These files have same **time_id and stock_id** and also contains the same unique **stock_id** as in book_train\/test.parquet files. ","7e5bb360":"**\ud83d\udccc Points to note :**\n   * We see that our **target** column values are right skewed (positive) and values ranges from 0.00 to 0.02.","957f1f93":"# 3. <a id='3'>Utility<\/a>\n[Tabale of contents](#0.1)","d5f625f0":"### Missing Values","89951ea6":"Let's quickly plot **WAP** and **Log Returns** per second to visualize the results. ","418529b5":"## Table of contents <a id='0.1'><\/a>\n1. [Introduction](#1)\n2. [Import Packages](#2)\n3. [Utility](#3)\n4. [Data Overview](#4)\n    * [Book Order Data](#4.1)\n    * [Trade Execution Data](#4.2)\n    * [Train](#4.3)\n    * [Test](#4.4)\n    * [Submission](#4.5)\n5. [Individual Feature Distribution](#5)\n    * [Continuous Feature Distribution](#5.1)\n    * [Categorical Feature Distribution](#5.2)\n6. [Weighted Average Price](#6)\n    * [Naive prediction: Stock ID 7](#6.1)","a39d8208":"**\ud83d\udccc Points to note :**\n\nFor stock_id of 7 and time_id of 5,\n   * There are 100 shares at most competitive buy level.\n   * Only 1 share at most competitive sell level.","3d22ae11":"# 1. <a id='1'>Introduction<\/a>\n[Table of contents](#0.1)\n\nThis competiton is hosted by [Optiver](https:\/\/www.optiver.com\/) a leading trading firm. Optiver is a proprietary trading firm with nine locations across Europe, Asia-Pacific and North America. **The goal in this compeition is to predict the short-term volatility for stocks. We need to design a model which can forcast volatility over 10 minutes period** to create better access and prices for [options](#0.2). We are dealing time-series data. \n\n## About Competition Host\n\nOptiver, one of the world\u2019s largest electronic market makers. Optiver is dedicated to continuously improving financial markets, creating better access and prices for options, ETFs, cash equities, bonds and foreign currencies on numerous exchanges around the world.","e3d0cd62":"## What is Option ? <a id='0.2'><\/a>\n\n[Table of contents](#0.1)\n\nAn option is a contract that is written by a seller that conveys to the buyer the right \u2014 but not an obligation to buy (for a call option) or to sell (for a put option) a particular asset, at a specific price (strike price\/exercise price) in future.\n\nIn return for granting the option, the seller collects a payment (known as a premium) from the buyer.\n\n[source](https:\/\/groww.in\/p\/option-trading\/)\n\n## About Competition Data\n\n[Table of contents](#0.1)\n\nThe data includes **order book snapshots and executed trades.**\n\nWe are provided with the following files:\n   * book_train.parquet\/book_test.parquet - This provides most competitive **buy** and **sell** data entered in the market. These parquet files consists of following columns - \n   \n      * stock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the       * categorical data type when loaded; you may wish to convert it to int8.\n      * time_id - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n      * seconds_in_bucket - Number of seconds from the start of the bucket, always starting from 0.\n      * bid_price[1\/2] - Normalized prices of the most\/second most competitive buy level.\n      * ask_price[1\/2] - Normalized prices of the most\/second most competitive sell level.\n      * bid_size[1\/2] - The number of shares on the most\/second most competitive buy level.\n      * ask_size[1\/2] - The number of shares on the most\/second most competitive sell level.\n      \n      \n   * trade_train.parquet\/trade_test.parquet - This file is more sparse than the order book since it contains data on trade that actually executed. Usually, in the market, there are more passive buy\/sell intention updates (book updates) than actual trades. These files have following columns - \n   \n      * stock_id - Same as above.\n      * time_id - Same as above.\n      * seconds_in_bucket - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n      * price - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n      * size - The sum number of shares traded.\n      * order_count - The number of unique trade orders taking place.\n      \n  * train.csv The ground truth values for the training set.\n\n     * stock_id - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n     * time_id - Same as above.\n     * target - The realized volatility computed over the 10 minute window following the feature data under the same stock\/time_id. There is no overlap between feature and target data. You can find more info in our tutorial notebook.\n     \n   * test.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n\n    * stock_id - Same as above.\n    * time_id - Same as above.\n    * row_id - Unique identifier for the submission row. There is one row for each existing time ID\/stock ID pair. Each time window is not necessarily containing every individual stock.\n    \n* sample_submission.csv - A sample submission file in the correct format.\n\n   * row_id - Same as in test.csv.\n   * target - Same definition as in train.csv. The benchmark is using the median target value from train.csv.\n   \n## Competition Goal\n\nWe need to generate series of short term-signals to predict the realized volatality of the next 10-minute window for stocks using the book and trade data of a fixed 10-minute window.","b346b1e5":"**\ud83d\udccc Points to note :**\n   * We have **38 million rows** and  **11** features in trade_train.parquet file.\n   * Since the **stock_id** and **time_id** are same in trade_train.parquet file the number of **unique stock_id is 112**.\n   * We don't have any missing values.\n   * Same as **book order** parquet file we are generating statistics for **trade order** data as well. ","2ea7ff45":"## Book Train Statistics\n\nBelow are some useful statistics of **book_train.parquet** group by **stock_id**. Useful in **Feature Engineering**. ","774aee3c":"## 4.4 <a id='4.4'>Test Data<\/a>\n[Table of contents](#0.1)"}}