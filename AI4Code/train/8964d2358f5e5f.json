{"cell_type":{"915351be":"code","e640b0d4":"code","22eab028":"code","2eacec62":"code","9f44c092":"code","e167be41":"code","56097616":"code","e2a8d316":"code","21d1f98d":"code","2748652a":"code","326edc97":"code","ac7b072c":"code","0268f67f":"code","1ea0efa6":"code","00b85007":"code","ac49bd3d":"code","03d1a3f8":"code","8c4ead7f":"code","ca319307":"code","8195d925":"code","fa99ccd1":"code","e815278e":"code","dc97e7ce":"code","7ad0dab4":"code","7aabe38f":"code","58de2d50":"code","03db06bd":"code","40809078":"code","8fbf4808":"code","d1cfd9cc":"code","0066e07f":"code","0a6158de":"code","8741af05":"markdown","6b9c56af":"markdown","37dabe81":"markdown","47c7db4b":"markdown"},"source":{"915351be":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.decomposition import PCA","e640b0d4":"hp = pd.read_csv(r\"..\/input\/houseprice-train\/train.csv\")\ntest = pd.read_csv(r\"..\/input\/houseprice-test\/test.csv\")","22eab028":"# searching for outliers\nsns.boxplot(hp[\"SalePrice\"])","2eacec62":"# excluding outliers\nhp = hp.drop(hp[hp[\"SalePrice\"]>350000].index)","9f44c092":"hp.index = range(len(hp))","e167be41":"hp[hp.describe().columns]","56097616":"# replace non values by means of their column\nhp[\"GarageYrBlt\"].fillna(value =hp[\"GarageYrBlt\"].mean(), inplace=True)\nhp[\"MasVnrArea\"].fillna(value = hp[\"MasVnrArea\"].mean(), inplace=True)","e2a8d316":"# observe further non values in numerical columns\nplt.figure(figsize=(12,8))\nsns.heatmap(hp[hp.describe().columns].isnull())","21d1f98d":"# observing for non values (in common numerical columns of hp and test) in the test data frame\nfor col in hp.describe().columns.drop(\"SalePrice\"):\n    print(col, \" : \", test[col].isnull().sum())","2748652a":"# replacement of non values by column means \nfor col in [\"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \n           \"GarageYrBlt\", \"GarageCars\", \"GarageArea\"]:\n    test[col].fillna(value =test[col].mean(), inplace=True)","326edc97":"# non values in test df\nplt.figure(figsize=(12,8))\nsns.heatmap(test[hp.describe().columns.drop(\"SalePrice\")].isnull())","ac7b072c":"# list of numerical columns without non-values in hp \nhp_col_zero=[]\nfor col in hp.describe().columns:\n    if hp[col].isnull().sum() == 0:\n        hp_col_zero.append(col)","0268f67f":"# list of common numerical columns without non-values \nges_col_zero=[]\nfor col in hp.describe().columns.drop(\"SalePrice\"):\n    if test[col].isnull().sum() == 0:\n        if col in hp_col_zero:\n            ges_col_zero.append(col)","1ea0efa6":"# creation of new data frames consisting of categorial columns\n\nhpc = hp.drop(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice'], axis=\"columns\")\n\ntestc = test.drop(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold'], axis=\"columns\")","00b85007":"# creation of list of columns with too many non-values\ndroplist = []\nfor col in hpc.columns:\n    if hpc.isnull().sum()[col] > 600:\n        print(col, hpc.isnull().sum()[col])\n        droplist.append(col)","ac49bd3d":"# delete columns with too many non-values\nhpc.drop(droplist, axis=\"columns\", inplace=True)\ntestc.drop(droplist, axis=\"columns\", inplace=True)","03d1a3f8":"# replace non-values by \"no\"\nhpc.fillna(value = \"no\", inplace = True)","8c4ead7f":"# no non-values left in hpc\nplt.figure(figsize=(12,8))\nsns.heatmap(hpc.isnull())","ca319307":"# replace non-values by \"no\"\ntestc.fillna(value = \"no\", inplace = True)","8195d925":"# no non-values left in testc\nplt.figure(figsize=(12,8))\nsns.heatmap(testc.isnull())","fa99ccd1":"# function that compares two arrays by length and elements \n\ndef array_comparison(arr1, arr2):\n    \n    arr1 = sorted(arr1) \n    arr2 = sorted(arr2) \n\n    array_comp=[]\n\n    array_comp.append(len(arr1)==len(arr2))\n    \n    if len(arr1)==len(arr2):\n        for i in range(0, len(arr1)):\n            array_comp.append(arr1[i]==arr2[i])\n\n    return array_comp","e815278e":"# show columns with differences in length and\/or elements between hpc and testc\n\ncols_with_different_unique_values = []\nfor col in hpc.columns:\n    if False in array_comparison(hpc[col].unique() , testc[col].unique()):\n        print(col, array_comparison(hpc[col].unique() , testc[col].unique()))\n        cols_with_different_unique_values.append(col)","dc97e7ce":"# delete columns with difference in length\/elements\nhpc.drop(cols_with_different_unique_values, axis=\"columns\", inplace=True)\ntestc.drop(cols_with_different_unique_values, axis=\"columns\", inplace=True)","7ad0dab4":"# prepare two lists of columns and values of hpc\n\nvalue_list=[]\ncol_value_list = []\nfor col in hpc.columns:\n    for value in hpc[col].unique():\n        value_list.append(value) \n        col_value_list.append([col, value])","7aabe38f":"# show values in columns of hpc that occur several times \n# put these values into a set and convert that set into a list\n\nset = {1}\nfor el in value_list:\n    if value_list.count(el) > 1:\n        print(el, value_list.count(el))\n        set.add(el)\nlist_rep_values = list(set)","58de2d50":"# function that replaces values that occur several times by changing its name by adding their column\n# result: no value will occur several times\n\ndef rep_values_replacement(value):    \n    for i in range(0, len(col_value_list)):\n        if value == col_value_list[i][1]:\n            for j in range(0, len(testc)):\n                if testc.at[j, col_value_list[i][0]] == value:\n                    testc.at[j, col_value_list[i][0]] = str(col_value_list[i][0] + \"_\" + col_value_list[i][1])\n\n    for i in range(0, len(col_value_list)):\n        if value == col_value_list[i][1]:\n            for j in range(0, len(hpc)):\n                if hpc.at[j, col_value_list[i][0]] == value:\n                    hpc.at[j, col_value_list[i][0]] = str(col_value_list[i][0] + \"_\" + col_value_list[i][1])","03db06bd":"# chenge value names by using the former function\nfor value in list_rep_values:\n    rep_values_replacement(value)","40809078":"collist = hpc.columns","8fbf4808":"# convert all the columns into dummie variables\n\nfor i in range(0, len(collist)):\n    hpc = pd.concat([hpc, pd.get_dummies(hpc[collist[i]])], axis=\"columns\")\nfor i in range(0, len(collist)):\n    testc = pd.concat([testc, pd.get_dummies(testc[collist[i]])], axis=\"columns\")\n    \nhpc.drop(collist, axis=\"columns\", inplace = True)\ntestc.drop(collist, axis=\"columns\", inplace = True)","d1cfd9cc":"hp = pd.concat([hpc, hp[ges_col_zero], hp[\"SalePrice\"]], axis=\"columns\")\ntest = pd.concat([testc, test[ges_col_zero]], axis=\"columns\")","0066e07f":"# find the best parameter for a Ridge Regression\n\nx = hp.drop(\"SalePrice\", axis=\"columns\")\ny = hp[\"SalePrice\"]\n\nscaler = MinMaxScaler()\nscaler.fit(x)\nx = scaler.transform(x)\n\nridge = Ridge()\nparameters = {\"alpha\" : [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1,5,10,20,30,35,40,45,50,55,100, 500, 1000]}  \nridge_regressor = GridSearchCV(ridge, parameters, scoring=\"neg_root_mean_squared_error\", cv=4)\nridge_regressor.fit(x,y)\n\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)","0a6158de":"# prediction\n\nx = hp.drop(\"SalePrice\", axis=\"columns\")\ny = hp[\"SalePrice\"]\n\nxtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.3)\n    \nscaler = MinMaxScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n    \n\nridge = Ridge(alpha=5)\nridge.fit(xtrain, ytrain)\npredictions = ridge.predict(xtest)\n\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(ytest, predictions)))\n\nplt.scatter(np.log(predictions), np.log(ytest))","8741af05":"- reunion of the numerical and the dummysized categorical columns ","6b9c56af":"- prediction:\n- I tried the Linear Regression, the Ridge Regression and the Lasso Regression for the prediction of the House Sale Prices\n- I also tried to use only some independent variables like the ones highly correlated with the Sale Price\n- I got the best results by using all the independent variables in a Ridge Regression","37dabe81":"- handling the numerical values:","47c7db4b":"- handling the categorical columns:"}}