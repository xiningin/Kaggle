{"cell_type":{"7b46e2d6":"code","b668e274":"code","fe35760a":"code","ea958451":"code","3dd2c6d7":"code","b6ed5607":"code","7b61f73e":"code","db0983cc":"code","a290191f":"code","b2551b60":"markdown","799dc43b":"markdown"},"source":{"7b46e2d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom bayes_opt import BayesianOptimization\n\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b668e274":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","fe35760a":"raws_features = train_data.columns[2:]\ntrain_X, train_y = train_data[raws_features], train_data['target']\ntest_X = test_data[raws_features]","ea958451":"n_splits = 10\nnum_round = 77777\nseed = 7777","3dd2c6d7":"class CVClassifier():\n    def __init__(self, estimator, n_splits=5, stratified=True, num_round=77777, **params):\n        self.n_splits_ = n_splits\n        self.scores_ = []\n        self.clf_list_ = []\n        self.estimator_ = estimator\n        self.stratified_ = stratified\n        self.num_round_ = num_round\n        if params:\n            self.params_ = params\n        \n    def cv(self, train_X, train_y):\n        if self.stratified_:\n            folds = StratifiedKFold(self.n_splits_, shuffle=True, random_state=seed)\n        else:\n            folds = KFold(self.n_splits_, shuffle=True, random_state=seed)\n        oof = np.zeros(len(train_y))\n        for fold, (train_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n            print('fold %d' % fold)\n            trn_data, trn_y = train_X.iloc[train_idx], train_y[train_idx]\n            val_data, val_y = train_X.iloc[val_idx], train_y[val_idx]\n            if self.estimator_ == 'lgbm':\n                train_set = lgb.Dataset(data=trn_data, label=trn_y)\n                val_set = lgb.Dataset(data=val_data, label=val_y)\n                clf = lgb.train(params=params, train_set=train_set, num_boost_round=num_round, \n                                valid_sets=[train_set, val_set], verbose_eval=100, early_stopping_rounds=200)\n                oof[val_idx] = clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)\n                \n            elif self.estimator_ == 'xgb':\n                train_set = xgb.DMatrix(data=trn_data, label=trn_y)\n                val_set = xgb.DMatrix(data=val_data, label=val_y)\n                watchlist = [(train_set, 'train'), (val_set, 'valid')]\n                clf = xgb.train(self.params_, train_set, self.num_round_, watchlist, \n                               early_stopping_rounds=200, verbose_eval=100)\n                oof[val_idx] = clf.predict(val_set, ntree_limit=clf.best_ntree_limit)\n            \n            elif self.estimator_ == 'cat':\n                clf = CatBoostClassifier(self.num_round_, task_type='GPU', early_stopping_rounds=500, **self.params_)\n                clf.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=500)\n                oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n\n            # sk-learn model\n            else:\n                clf = self.estimator_.fit(trn_data, trn_y)\n                try:\n                    oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n                except AttributeError:\n                    oof[val_idx] = clf.decision_function(val_data)\n            \n            self.clf_list_.append(clf)\n            fold_score = roc_auc_score(train_y[val_idx], oof[val_idx])\n            self.scores_.append(fold_score)\n            print('Fold score: {:<8.5f}'.format(fold_score))\n        self.oof_ = oof\n        self.score_ = roc_auc_score(train_y, oof)\n        print(\"CV score: {:<8.5f}\".format(self.score_))\n        \n    def predict(self, test_X):\n        self.predictions_ = np.zeros(len(test_X))\n        \n        if self.estimator_ == 'lgbm':\n            self.feature_importance_df_ = pd.DataFrame()\n            for fold, clf in enumerate(self.clf_list_):\n                fold_importance_df = pd.DataFrame()\n                fold_importance_df[\"feature\"] = features\n                fold_importance_df[\"importance\"] = clf.feature_importance()\n                fold_importance_df[\"fold\"] = fold + 1\n                self.feature_importance_df_ = pd.concat([self.feature_importance_df_, fold_importance_df], axis=0)\n                \n                self.predictions_ += clf.predict(test_X, num_iteration=clf.best_iteration) * (self.scores_[fold] \/ sum(self.scores_))\n        elif self.estimator_ == 'xgb':\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict(xgb.DMatrix(test_X), ntree_limit=clf.best_ntree_limit) \\\n                * (self.scores_[fold] \/ sum(self.scores_))\n        elif self.estimator_ == 'cat':\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] \/ sum(self.scores_))\n        else:\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] \/ sum(self.scores_))","b6ed5607":"# Class for Bayesian Optimisation\nclass CVForBO():\n    def __init__(self, model, train_X, train_y, test_X, base_params, int_params=[], n_splits=5, num_round=77777):\n        self.oofs_ = []\n        self.params_ = []\n        self.predictions_ = []\n        self.cv_scores_ = []\n        self.model_ = model\n        self.train_X_ = train_X\n        self.train_y_ = train_y\n        self.test_X_ = test_X\n        self.base_params_ = base_params\n        self.int_params_ = int_params\n        self.n_splits_ = n_splits\n        self.num_round_ = num_round\n        \n    def cv(self, **opt_params):\n        for p in self.int_params_:\n            if p in opt_params:\n                opt_params[p] = int(np.round(opt_params[p]))\n        self.base_params_.update(opt_params)\n        \n        cv_model = CVClassifier(self.model_, n_splits=self.n_splits_, num_round=self.num_round_, **self.base_params_)\n        cv_model.cv(self.train_X_, self.train_y_)\n        cv_model.predict(self.test_X_)\n        \n        self.oofs_.append(cv_model.oof_)\n        self.predictions_.append(cv_model.predictions_)\n        self.params_.append(self.base_params_)\n        self.cv_scores_.append(cv_model.score_)\n\n        return cv_model.score_\n    \n    def post_process(self, model_type=None, oof_path='inter_oofs.csv', pred_path='inter_preds.csv', params_path='inter_params.csv'):\n        if not model_type:\n            model_type=self.model_\n        cols = ['{}_{}_{}'.format(model_type, str(self.cv_scores_[k]).split('.')[-1][:5], k) for k in range(len(self.cv_scores_))]\n        self.oof_df = pd.DataFrame(np.array(self.oofs_).T, columns=cols)\n        self.pred_df = pd.DataFrame(np.array(self.predictions_).T, columns=cols)\n        self.params_df = pd.DataFrame(self.params_).T.rename(columns={c_old: c_new for c_old, c_new in enumerate(cols)})\n        \n        self.oof_df.to_csv(oof_path)\n        self.pred_df.to_csv(pred_path)\n        self.params_df.to_csv(params_path)","7b61f73e":"cat_params = {\n    'eval_metric': 'AUC',\n    'bootstrap_type': 'Bernoulli',\n    'objective': 'Logloss',\n    'od_type': 'Iter',\n    'random_seed': seed,\n    'allow_writing_files': False}\n\ncv_cat_for_BO = CVForBO('cat', train_X, train_y, test_X, cat_params, ['depth'])\ncat_BO = BayesianOptimization(cv_cat_for_BO.cv, {\n    'depth': (2, 4), \n    'l2_leaf_reg': (37, 97), \n    'random_strength': (5, 17), \n    'eta': (0.01, 0.1)\n    }, random_state=seed)\n\ncat_BO.maximize(init_points=2, n_iter=15, acq='ei')","db0983cc":"print(cat_BO.max)\ncv_cat_for_BO.post_process()","a290191f":"max_idx = cv_cat_for_BO.cv_scores_.index(cat_BO.max['target'])\n\nsub_df = pd.DataFrame({'ID_code': test_data['ID_code'], \n                      'target': cv_cat_for_BO.predictions_[max_idx]})\nsub_df.to_csv('submissions.csv', index=False)","b2551b60":"Many thanks to [u1234x1234](https:\/\/www.kaggle.com\/u1234x1234) who shared me the idea of how to save intermediate results in this [discussion](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82621)","799dc43b":"Creating classes fo CV to automate the process"}}