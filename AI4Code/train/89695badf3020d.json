{"cell_type":{"e69fb3e0":"code","80c671f6":"code","7d641a30":"code","1f49741e":"code","473101f5":"code","9eeb7e55":"code","d377d440":"code","3d1af844":"code","674c0c2d":"code","0d47f6fa":"code","f8cd39cb":"code","24f3a180":"code","c218e1b4":"code","e50d8021":"code","34c5e506":"code","5cde118f":"code","635e9935":"code","13065380":"code","a3b113eb":"code","f7a7a496":"code","9d5616e8":"code","8b976c71":"code","f684ffe2":"code","3e85544b":"code","7b9430e4":"code","0b92db67":"code","d207bad1":"code","c257c4b3":"code","76ca2520":"code","72319299":"code","fb09c018":"code","b48a1c7b":"code","1a61d208":"code","6ea6a0a2":"code","7ace74e3":"code","a271e017":"code","708f3572":"code","383d34b7":"code","22bff434":"code","a34fc850":"code","ac5189c2":"code","28cad174":"markdown","1f0afbfd":"markdown","61e5a5ad":"markdown","86179c95":"markdown","ca696be2":"markdown"},"source":{"e69fb3e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport time\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80c671f6":"#Code by https:\/\/www.kaggle.com\/brsdincer\/human-evolution-process-dcgan\/notebook\n\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport zipfile\nfrom io import BytesIO\nfrom nibabel import FileHolder\nfrom nibabel.analyze import AnalyzeImage\nimport PIL\nfrom IPython import display","7d641a30":"#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score","1f49741e":"#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras","473101f5":"#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","9eeb7e55":"Video_Path = \"..\/input\/parler\/capitol_vids\/8fsC1EfPO70k.mp4\"","d377d440":"Video_IMG_List = []\n\nVideo_Cap = cv2.VideoCapture(Video_Path)\n\n\nwhile Video_Cap.isOpened():\n\n    \n    ret,frame = Video_Cap.read()\n    \n    if ret != True:\n        break\n        \n    if Video_Cap.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resize_IMG = cv2.resize(Transformation_IMG,(180,180))\n        Video_IMG_List.append(Resize_IMG)\n        \n        \nVideo_Cap.release()","3d1af844":"print(np.shape(np.asarray(Video_IMG_List)))","674c0c2d":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = Video_IMG_List[1]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","0d47f6fa":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = Video_IMG_List[500]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","f8cd39cb":"#Original code was video_img_list [3410] - 2nd line\n\nfigure = plt.figure(figsize=(8,8))\n\nPick_IMG = Video_IMG_List[30]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","24f3a180":"figure,axis = plt.subplots(4,4,figsize=(10,10))\n\nfor i,ax in enumerate(axis.flat):\n    \n    IMG_From_List = Video_IMG_List[i]\n    ax.set_xlabel(IMG_From_List.shape)\n    ax.imshow(IMG_From_List)\n    \nplt.tight_layout()\nplt.show()","c218e1b4":"X_Train = np.asarray(Video_IMG_List)","e50d8021":"print(X_Train.shape)","34c5e506":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = X_Train[410]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG);","5cde118f":"#Original code was X-train [3534]\n\nfigure = plt.figure(figsize=(8,8))\n\nPick_IMG = X_Train[353]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG);","635e9935":"iterations = 50\nvector_noise_dim = 180\ncount_example = 16\nbatch_size=12\ncount_buffer = 60000","13065380":"X_Train = X_Train \/ 255.","a3b113eb":"Train_Data = tf.data.Dataset.from_tensor_slices(X_Train).shuffle(count_buffer).batch(batch_size)","f7a7a496":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,128)))\n    #\n    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Conv2DTranspose(3,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    \n    return Model","9d5616e8":"Generator = Generator_Model()","8b976c71":"print(Generator.summary())","f684ffe2":"def Discriminator_Model():\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,3]))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    \n    model.add(Conv2D(128,(3,3),padding=\"same\"))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","3e85544b":"Discriminator = Discriminator_Model()","7b9430e4":"print(Discriminator.summary())","0b92db67":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","d207bad1":"Generator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)","c257c4b3":"seed = tf.random.normal([count_example, vector_noise_dim])","76ca2520":"def Discriminator_Loss(real_out,fake_out):\n    \n    real_loss_function = Loss_Function(tf.ones_like(real_out),real_out)\n    fake_loss_function = Loss_Function(tf.zeros_like(fake_out),fake_out)\n    total_loss = real_loss_function + fake_loss_function\n    \n    return total_loss","72319299":"def Generator_Loss(fake_out):\n    return Loss_Function(tf.ones_like(fake_out),fake_out)","fb09c018":"def generate_and_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(10, 10))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(6, 6, i+1)\n        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","b48a1c7b":"def Train_Step(images):\n    \n    random_noise_vector = tf.random.normal([batch_size,vector_noise_dim])\n    \n    with tf.GradientTape() as Generator_Tape, tf.GradientTape() as Discriminator_Tape:\n        \n        Generator_Fake_IMG = Generator(random_noise_vector,training=False)\n        \n        real_out = Discriminator(images,training=True)\n        fake_out = Discriminator(Generator_Fake_IMG,training=True)\n        \n        Generator_Loss_Out = Generator_Loss(fake_out)\n        Discriminator_Loss_Out = Discriminator_Loss(real_out,fake_out)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Out,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Out,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","1a61d208":"def Train(dataset,iterations):\n    \n    for epoch in range(iterations):\n        \n        start = time.time()\n        \n        for image_batch in dataset:\n            Train_Step(image_batch)\n            \n        display.clear_output(wait=True)\n        generate_and_save_images(Generator,epoch+1,seed)\n    \n    display.clear_output(wait=True)\n    generate_and_save_images(Generator,epoch,seed)","6ea6a0a2":"Train(Train_Data, iterations)","7ace74e3":"Predict_Noise = tf.random.normal(shape=[30,vector_noise_dim])","a271e017":"Generator_Predict_Noise = Generator(Predict_Noise)","708f3572":"figure, axes = plt.subplots(nrows=5,ncols=6,figsize=(10,10))\n\nfor i,ax in enumerate(axes.flat):\n    IMG_Random = Generator_Predict_Noise[i]\n    ax.imshow(IMG_Random)\n    ax.set_xlabel(Generator_Predict_Noise[i].shape)\nplt.tight_layout()\nplt.show()","383d34b7":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_Noise[25])\nplt.show()","22bff434":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_Noise[9])\nplt.show()","a34fc850":"figure = plt.figure(figsize=(14,14))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_Noise[15])\nplt.show()","ac5189c2":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Baris Dincer, @brsdincer for the code.' )","28cad174":"#US Capitol Attack 2021\n\nOn January 6, 2021, the United States Capitol in Washington, D.C., was stormed during a riot and violent attack against the U.S. Congress. A mob of supporters of President Donald Trump attempted to overturn his defeat in the 2020 presidential election by disrupting the joint session of Congress assembled to count electoral votes to formalize Joe Biden's victory. The Capitol complex was locked down and lawmakers and staff were evacuated while rioters occupied and vandalized the building for several hours. Five people died either shortly before, during, or after the event: one was shot by Capitol Police, one died of a drug overdose, and three succumbed to natural causes. More than 140 people were injured in the storming.\n\nhttps:\/\/en.wikipedia.org\/wiki\/2021_United_States_Capitol_attack","1f0afbfd":"#Code by Baris Dincer https:\/\/www.kaggle.com\/brsdincer\/human-evolution-process-dcgan\/notebook","61e5a5ad":"#The original code was Video_IMG_List[1000] I changed to 500","86179c95":"#Since that snippet. takes a long time. And trying to send me to Google Cloud. I commented all the rest.","ca696be2":"#DATA ENGINEERING BEFORE DC-GAN TRAINING"}}