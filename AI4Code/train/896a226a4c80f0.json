{"cell_type":{"0aee1baa":"code","4b321846":"code","761d671a":"code","3462f9ba":"code","822e5120":"code","15c04a3e":"code","b1ef59c4":"code","86745ae7":"code","d6c0a6ad":"code","98318433":"code","fc957946":"code","3759912c":"code","4c384d40":"code","4cf61db1":"code","382bd31e":"code","c791b67d":"code","3fe93b2b":"code","8145b411":"code","06e83637":"code","a4811438":"code","55235fc2":"code","37c8f33a":"code","e9085f0f":"code","8f6ac378":"code","5fb71e96":"code","81dae4f8":"code","207e975d":"code","213e7dd7":"code","c2b0ad10":"code","a3822666":"code","6aaf1671":"code","63b54858":"code","14eea158":"code","afffad73":"code","9e86ee30":"code","aca0362c":"code","8721b10d":"code","a52056be":"code","291ccf4c":"code","6b6d8ca8":"markdown","6bccc4ce":"markdown","e5082527":"markdown","1b249084":"markdown","cb0d1b48":"markdown","a55da822":"markdown","d58e310d":"markdown","3b1b4fbb":"markdown","953ffecb":"markdown","fbc758fe":"markdown","3f425ca4":"markdown","7abd2405":"markdown","14115eea":"markdown","8cebd8ba":"markdown","786f846c":"markdown","070987da":"markdown","f85b8a90":"markdown","22e1ac91":"markdown"},"source":{"0aee1baa":"# some of the more general imports needed for the script\nimport numpy as np\nimport pandas as pd\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = test['PassengerId']\n\n# showing overview of the train dataset\ntrain.head(10)","4b321846":"original_train = train.copy() \nfull_data = [train, test]","761d671a":"# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","3462f9ba":"# let's start by exploring age\n\ntrain[['Age']].describe()","822e5120":"train[['Age']].isnull().sum()","15c04a3e":"# Here we remove all NULLS in the 'Age' column and replace with pseudorandom ages derived from the mean\n# and standard deviation of the data belonging to the 'Age' column\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    # Next line has been improved to avoid warning\n    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(float)\n    \ntrain[['Age']].describe()","b1ef59c4":"# let's split this into quantiles and see what we get\n\ntrain[['Age']].quantile([0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,0.95,0.98])","86745ae7":"# taking a look at the distribution\n\nsns.distplot(train['Age'], color='Red')","d6c0a6ad":"import pylab\nfrom scipy.stats import probplot\n\n\nmeasurements = train['Age']   \nprobplot(measurements, dist=\"norm\", plot=pylab)\npylab.show()","98318433":"from scipy.stats import shapiro\n\n# some tests for normality, we need to decide if we are going to use a gaussian approach\n\n# normality test\nstat, p = shapiro(train['Age'])\nprint('Shapiro-Wilk Statistic=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","fc957946":"for dataset in full_data:\n    dataset['age_quantile'] = pd.qcut(dataset['Age'], 10, labels=False)\n    dataset['age_quantile'] \n    \ntrain['age_quantile'].describe()","3759912c":"for dataset in full_data:\n    dataset['age_quantile'] = dataset['age_quantile'].fillna(dataset['age_quantile'].mean())\n    \ntrain['age_quantile'].isnull().sum()","4c384d40":"train['age_quantile'].value_counts().sort_index()","4cf61db1":"from statistics import mode\nprint('Embarked Mode: ' +  str(mode(train['Embarked'])))","382bd31e":"print('Fare Mean: ' +  str(np.mean(train['Fare'])))","c791b67d":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(float)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(float)\n    dataset['Embarked'] = dataset['Embarked'].fillna(0)\n    \n    # Mapping Fare\n    dataset['Fare'] = dataset['Fare'].fillna(32.20)\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(float).dropna(axis=0, how='any')\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] ;\n    \n    ","3fe93b2b":"train.isnull().sum()","8145b411":"# Feature selection: remove variables no longer containing relevant information\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","06e83637":"train.head(10)","a4811438":"colormap = plt.cm.plasma\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","55235fc2":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since \"Survived\" is a binary class (0 or 1), these metrics grouped by the Title feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n\n# title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} ","37c8f33a":"xlab = ['N\/A', 'Mr', 'Ms', 'Mrs', 'Master', 'Rare']\ndp = sns.distplot(train['Title'], color='Red')\ndp.set(xticks=range(0,6), xticklabels=xlab)\n#dp.set_xticks([0,1,2,3,4,5,6])\n#dp.set_xticklabels(xlab)\nplt.show()","e9085f0f":"# normality test\nstat, p = shapiro(train['Title'])\nprint('Shapiro-Wilk Statistic=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","8f6ac378":"# normality test\nstat, p = shapiro(train['Sex'])\nprint('Shapiro-Wilk Statistic=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","5fb71e96":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since Survived is a binary feature, this metrics grouped by the Sex feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n    \n# sex_mapping = {{'female': 0, 'male': 1}} ","81dae4f8":"# Let's use our 'original_train' dataframe to check the sex distribution for each title.\n# We use copy() again to prevent modifications in out original_train dataset\ntitle_and_sex = original_train.copy()[['Name', 'Sex']]\n\n# Create 'Title' feature\ntitle_and_sex['Title'] = title_and_sex['Name'].apply(get_title)\n\n# Map 'Sex' as binary feature\ntitle_and_sex['Sex'] = title_and_sex['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n# Table with 'Sex' distribution grouped by 'Title'\ntitle_and_sex[['Title', 'Sex']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n\n# Since Sex is a binary feature, this metrics grouped by the Title feature represent:\n    # MEAN: percentage of men\n    # COUNT: total observations\n    # SUM: number of men","207e975d":"def get_gini_impurity(survived_count, total_count):\n    survival_prob = survived_count\/total_count\n    not_survival_prob = (1 - survival_prob)\n    random_observation_survived_prob = survival_prob\n    random_observation_not_survived_prob = (1 - random_observation_survived_prob)\n    mislabelling_survided_prob = not_survival_prob * random_observation_survived_prob\n    mislabelling_not_survided_prob = survival_prob * random_observation_not_survived_prob\n    gini_impurity = mislabelling_survided_prob + mislabelling_not_survided_prob\n    return gini_impurity","213e7dd7":"# Gini Impurity of starting node\ngini_impurity_starting_node = get_gini_impurity(342, 891)\ngini_impurity_starting_node","c2b0ad10":"# Gini Impurity decrease of node for 'male' observations\ngini_impurity_men = get_gini_impurity(109, 577)\ngini_impurity_men","a3822666":"# Gini Impurity decrease if node split for 'female' observations\ngini_impurity_women = get_gini_impurity(233, 314)\ngini_impurity_women","6aaf1671":"# Gini Impurity decrease if node split by Sex\nmen_weight = 577\/891\nwomen_weight = 314\/891\nweighted_gini_impurity_sex_split = (gini_impurity_men * men_weight) + (gini_impurity_women * women_weight)\n\nsex_gini_decrease = weighted_gini_impurity_sex_split - gini_impurity_starting_node\nsex_gini_decrease","63b54858":"# Gini Impurity decrease of node for observations with Title == 1 == Mr\ngini_impurity_title_1 = get_gini_impurity(81, 517)\ngini_impurity_title_1","14eea158":"# Gini Impurity decrease if node split for observations with Title != 1 != Mr\ngini_impurity_title_others = get_gini_impurity(261, 374)\ngini_impurity_title_others","afffad73":"# Gini Impurity decrease if node split for observations with Title == 1 == Mr\ntitle_1_weight = 517\/891\ntitle_others_weight = 374\/891\nweighted_gini_impurity_title_split = (gini_impurity_title_1 * title_1_weight) + (gini_impurity_title_others * title_others_weight)\n\ntitle_gini_decrease = weighted_gini_impurity_title_split - gini_impurity_starting_node\ntitle_gini_decrease","9e86ee30":"del train['Age']\ndel test['Age']","aca0362c":"try:\n    train['Age']\nexcept KeyError:\n    var_exists = False\nelse:\n    var_exists = True\n    \nprint('Age feature exists in train: ' + str(var_exists))\n\ntry:\n    test['Age']\nexcept KeyError:\n    var_exists = False\nelse:\n    var_exists = True\n    \nprint('Age feature exists in test: ' + str(var_exists))","8721b10d":"cv = KFold(n_splits=10)            # Desired number of Cross Validation folds\naccuracies = list()\nmax_attributes = len(list(test))\ndepth_range = range(1, max_attributes + 1)\n\n# Testing max_depths from 1 to max attributes\n# Uncomment prints for details about each Cross Validation pass\nfor depth in depth_range:\n    fold_accuracy = []\n    tree_model = tree.DecisionTreeClassifier(max_depth = depth)\n    # print(\"Current max depth: \", depth, \"\\n\")\n    for train_fold, valid_fold in cv.split(train):\n        f_train = train.loc[train_fold] # Extract train data with cv indices\n        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n\n        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n        fold_accuracy.append(valid_acc)\n\n    avg = sum(fold_accuracy)\/len(fold_accuracy)\n    accuracies.append(avg)\n    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n    # print(\"Average accuracy: \", avg)\n    # print(\"\\n\")\n    \n# Just to show results conveniently\ndf = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\ndf = df[[\"Max Depth\", \"Average Accuracy\"]]\nprint(df.to_string(index=False))","a52056be":"# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\ny_train = train['Survived']\nx_train = train.drop(['Survived'], axis=1).values \nx_test = test.values\n\n# Create Decision Tree with max_depth = 3\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(x_train, y_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(x_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\n\n\n#make sure you have graphviz executables in your system path\nfrom graphviz import Source\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = Source(tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 3,\n                              impurity = True,\n                              feature_names = list(train.drop(['Survived'], axis=1)),\n                              class_names = ['Died', 'Survived'],\n                              rounded = True,\n                              filled= True ))\n     \n# Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.truetype('\/usr\/share\/fonts\/truetype\/droid\/DroidSans.ttf', 26)\ndraw.text((10, 0), # Drawing offset (position)\n          '\"Title <= 1.5\" corresponds to \"Mr.\" title', # Text to draw\n          (0,0,255), # RGB desired color\n          font=font) # ImageFont object with desired font\nimg.save('sample-out.png')\nPImage('sample-out.png')","291ccf4c":"acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","6b6d8ca8":"The data shows that less 'Mr' survived (15.67%) than men in general (18.89%): Title seems therefore to be more useful than Sex for our purpose. This may be because Title implicitly includes information about Sex in most cases. To verify this, we can use the copy we made of the original training data without mappings and check the distribution of Sex grouped by Title.","6bccc4ce":"We find that the Title feature is slightly better at reducing the Gini Impurity than Sex. This confirms our previous analysis, and we're now sure that Title will be used for the first split. Sex will therefore be neglected since the information is already included in the Title feature.\n\n[For more on how decision trees work...](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/10169)","e5082527":"If we split by Title == 1 (== Mr), we'll have the two following nodes:\n\n--Node with only Mr: 517 observations with only 81 survived\n\n--Node with other titles: 374 observations with 261 survived","1b249084":"The best max_depth parameter seems therefore to be 3 (80.5% average accuracy across the 10 folds), and feeding the model with more data results in worse results probably due to over-fitting. We'll therefore use 3 as the max_depth parameter for our final model.","cb0d1b48":"### Gini Impurity\nBefore start working with Decision Trees, let's briefly explain how they work. The goal of their learning algorithms is always to find the best split for each node of the tree. But measuring the \"goodness\" of a given split is a subjective question so, in practice, different metrics are used for evaluating splits. One commonly used metric is Information Gain. The sklearn library we're gonna use implements Gini Impurity, another common measure, so let\u2019s explain it.\n\nGini Impurity measures the disorder of a set of elements. It is calculated as the probability of mislabelling an element assuming that the element is randomly labelled according to the distribution of all the classes in the set. Decision Trees will try to find the split which decreases Gini Impurity the most across the two resulting nodes. For the titanic example it can be calculated as follows (code should be explicit enough):","a55da822":" ### Age Normality and EDA\nSome tests for normality and miscellaneous EDA. We need to decide if we are going to use a Gaussian approach or not and exploring some of the features will help us to do this.","d58e310d":"Maybe if we had gone a few more levels down we would see the impact of the age quantile feature, but our test for best max depth parameter suggest we should leave it at 3. Slightly less accurate than the 82.38 from the tree in the original notebook, but now I have my first submission ready!","3b1b4fbb":"### Finding the best tree depth with the help of cross-validation\n\nAfter exploring the data, we're going to find of much of it can be relevant for our decision tree. This is a critical point for every Data Science project, since too much train data can easily result in bad model generalisation (accuracy on test\/real\/unseen observations). Over-fitting (a model excessively adapted to the train data) is a common reason. In other cases, too much data can also hide meaningful relationships either because they evolve with time or because highly correlated features prevent the model from capturing properly the value of each single one.\n\nIn the case of decision trees, the 'max_depth' parameter determines the maximum number of attributes the model is going to use for each prediction (up to the number of available features in the dataset). A good way to find the best value for this parameter is just iterating through all the possible depths and measure the accuracy with a robust method such as Cross Validation.\n\nCross Validation is a model validation technique that splits the training dataset in a given number of \"folds\". Each split uses different data for training and testing purposes, allowing the model to be trained and tested with different data each time. This allows the algorithm to be trained and tested with all available data across all folds, avoiding any splitting bias and giving a good idea of the generalisation of the chosen model. The main downside is that Cross Validation requires the model to be trained for each fold, so the computational cost can be very high for complex models or huge datasets.","953ffecb":"More than half of the <b>Cabin<\/b> data is missing, so we are going to drop it from our model, along with variables that no longer contain relevant information.","fbc758fe":"We're now going to simulate both splits, calculate the impurity of resulting nodes and then obtain the weighted Gini Impurity after the split to measure how much each split has actually reduced impurity.\n\nIf we split by Sex, we'll have the two following nodes:\n\n--Node with men: 577 observations with only 109 survived\n\n--Node with women: 314 observations with 233 survived","3f425ca4":"Let's use our Sex and Title features as an example and calculate how much each split will decrease the overall weighted Gini Impurity. First, we need to calculate the Gini Impurity of the starting node including all 891 observations in our train dataset. Since only 342 observations survived, the survival probability is around 38,38% (342\/891).","7abd2405":"Rather than use the age feature as defined by the author of the original notebook, let's drop it and use the quantile variable that we created earlier. We will compare the results of this model with those of the model in the original notebook at the end.","14115eea":"### Title vs Sex\nYou can easily compare features and their relationship with the class by grouping them and calculating some basic statistics for each group. The code below does exactly this in one line, and explains the meaning of each metric when working with a binary class.","8cebd8ba":"This heatmap is very useful as an initial observation because you can easily get an idea of the predictive value of each feature. In this case, Sex and Title show the highest correlations (in absolute terms) with the class (Survived): 0.54 and 0.49 respectively. But the absolute correlation between both is also very high (0.86, the highest in our dataset), so they are probably carrying the same information and using the two as inputs for the same model wouldn't be a good idea. High chances are one of them will be used for the first node in our final decision tree, so let's first explore further these features and compare them.","786f846c":"We find that, excepting for a single observation (a female with 'Dr' title), all the observations for a given Title share the same Sex. Therefore the feature Title is capturing all the information present in Sex. In addition, Title may be more valuable to our task by capturing other characteristics of the individuals like age, social class, personality, ...\n\nIt's true that by regrouping rare titles into a single category, we are losing some information regarding Sex. We could create two categories \"Rare Male\" and \"Rare Female\", but the separation will be almost meaningless due to the low occurrence of \"Rare\" Titles (2.6%, 23 out of 891 samples).\n\nThanks to this in-depth analysis of the Sex and Title features we've seen that, even if the correlation of the feature Sex with the class Survived was higher, Title is a richer feature because it carries the Sex information but also adds other characteristics. Therefore is very likely that Title is going to be the first feature in our final decision tree, making Sex useless after this initial split.","070987da":" <b>Age<\/b> is not normally distributed a if we think that a Gaussian approach could be effective, our tests tell us otherwise. Intuition tells us that age could be a strong predictor of survivability. Let's take a look at the formal titles held by passengers (Mrs., Mr., Dr., etc.) and Sex. First, we need to create and map some clever features.","f85b8a90":"This is my first notebook on Kaggle! I follow the general framework laid out in [this notebook by Diego Milla](https:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset) with some additional exploratory data analysis and some slight changes in the way the data was organized. His model was slightly more accurate because of the changes I made to the <b>Age<\/b> feature.","22e1ac91":"### Visualizing Processed Data"}}