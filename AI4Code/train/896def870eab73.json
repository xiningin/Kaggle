{"cell_type":{"78e652fc":"code","73e10718":"code","1e38e383":"code","33ca2c6f":"code","643e8211":"code","8838928d":"code","e2b918dd":"code","3c63e545":"code","405b42c7":"code","d5a8d115":"code","0cecd91b":"code","92a3d8e7":"code","69f55ffb":"code","5f20a7b0":"code","a93eebb2":"code","255ef35a":"code","3e9b9a6e":"code","5fada85e":"code","780ab562":"code","afd732b9":"code","bc0dfa20":"code","bb965ea6":"code","e4a55acd":"code","0567733d":"code","dc047244":"code","9027a654":"code","a6dbe9d0":"code","ebeca66f":"code","fa9205ba":"code","3e414e9c":"code","24db7ee0":"code","c47daaaf":"code","b42c26ea":"code","a7007ce0":"code","7d34d336":"code","1edd79e7":"code","781ace46":"code","06eee0b8":"code","a7f42e5b":"code","40ab53e5":"code","38b593ef":"code","ce870caf":"code","688e3032":"code","a6dfb028":"code","b3a40251":"code","dffd4b51":"code","b09372bd":"code","4c4fa2e1":"code","1de6a123":"code","0a157c36":"code","cfb9d35f":"code","74e463c7":"code","b7aa1fa1":"code","55310f00":"code","51fb351b":"code","c1a09d1a":"code","60783905":"code","a9bb9f0c":"code","fabefce8":"code","37b5a375":"code","a2707c0e":"code","aac6db3f":"code","727d444c":"code","0d78e8b2":"code","80d89382":"code","0cdfe997":"code","f979dca7":"code","542b6a8b":"code","329bec21":"code","e9578038":"code","2afa7f1c":"code","bf4f0f1b":"code","386db5c7":"code","6d2226aa":"code","b0b663ee":"code","3ed6232b":"code","973a2305":"code","ba51d072":"markdown","b97fd849":"markdown","e534428b":"markdown","754ef93c":"markdown","dcf77d8d":"markdown","772540fc":"markdown","714ae335":"markdown","8cc96c10":"markdown","786813e2":"markdown","8d65f408":"markdown","01ce9a82":"markdown","f11a442e":"markdown","c2d83ff1":"markdown","a4859667":"markdown","1834b0e8":"markdown","fa6d9ea2":"markdown","6b7841ab":"markdown","91a74286":"markdown","56214cb4":"markdown","e3cbf59f":"markdown","115035a5":"markdown","0e11bbe7":"markdown","0418678f":"markdown","2401386e":"markdown"},"source":{"78e652fc":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\npd.options.display.max_columns = 1000","73e10718":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.head()","1e38e383":"X = df['question_text']\ny = df['target']\nX.shape, y.shape","33ca2c6f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","643e8211":"df.info()","8838928d":"df['question_text'][df['question_text'] == \"\"].sum()","e2b918dd":"df['question_text'].shape, df['target'].shape","3c63e545":"df['target'].unique()","405b42c7":"sns.countplot(df['target'])\nplt.xlabel('Predictions');","d5a8d115":"purcent_of_sincere = len(df['question_text'][df['target'] == 0]) \/ len(df['question_text']) * 100\npurcent_of_unsincere = len(df['question_text'][df['target'] == 1]) \/ len(df['question_text']) * 100\n\nsincere_len = len(df['question_text'][df['target'] == 0])\nunsincere_len = len(df['question_text'][df['target'] == 1])\n\nprint(\"Purcent of sincere: {:.2f}%, {} questions\".format(purcent_of_sincere, sincere_len))\nprint(\"Purcent of unsincere: {:.2f}%, {} questions\".format(purcent_of_unsincere, unsincere_len))","0cecd91b":"sincere_lst_len = [len(df['question_text'][i]) for i in range(0, len(df['question_text'][df['target'] == 0])) if df['target'][i] == 0]\nsincere_len_mean = np.array(sincere_lst_len).mean()\nprint(\"Mean of sincere questions: {:.0f} characters\".format(sincere_len_mean))","92a3d8e7":"unsincere_lst_len = [len(df['question_text'][i]) for i in range(0, len(df['question_text'][df['target'] == 1])) if df['target'][i] == 1]\nunsincere_len_mean = np.array(unsincere_lst_len).mean()\nprint(\"Mean of unsincere questions: {:.0f} characters\".format(unsincere_len_mean))","69f55ffb":"s1 = df[df['target'] == 0]['question_text'].str.len()\nsns.distplot(s1, label='sincere')\ns2 = df[df['target'] == 1]['question_text'].str.len()\nsns.distplot(s2, label='unsincere')\nplt.title('Lenght Distribution')\nplt.legend();","5f20a7b0":"first_word_unsincere = []\nfor sentence in df[df['target'] == 1]['question_text']:\n    first_word_unsincere.append(sentence.split()[0])","a93eebb2":"from collections import Counter\ncounter_unsincere = Counter(first_word_unsincere)\ncounter_unsincere.most_common(10)","255ef35a":"first_word_sincere = []\nfor sentence in df[df['target'] == 0]['question_text']:\n    first_word_sincere.append(sentence.split()[0])","3e9b9a6e":"from collections import Counter\ncounter_sincere = Counter(first_word_sincere)\ncounter_sincere.most_common(10)","5fada85e":"tokenized_docs = [word_tokenize(doc.lower()) for doc in X_train]\ntokenized_docs[0]","780ab562":"alpha_tokens = [[t for t in doc if t.isalpha() == True] for doc in tokenized_docs]\nalpha_tokens[0]","afd732b9":"stop_words = stopwords.words('english')","bc0dfa20":"no_stop_tokens = [[t for t in doc if t not in stop_words] for doc in alpha_tokens]\nno_stop_tokens[0]","bb965ea6":"stemmer = PorterStemmer()","e4a55acd":"stemmed_tokens = [[stemmer.stem(t) for t in doc] for doc in no_stop_tokens]\nstemmed_tokens[0]","0567733d":"X_temp = X_train.reset_index()\nX_temp['temp'] = stemmed_tokens\nX_temp.set_index('index', inplace=True)\nX_temp.head()","dc047244":"X_temp = pd.concat([X_temp, y_train], axis=1, sort=False)\nX_temp.head()","9027a654":"np_X_temp_index = np.array(X_temp.index)","a6dbe9d0":"lst = []\nfor idx in np_X_temp_index:\n    lst.append(len(X_temp['temp'][idx]))","ebeca66f":"X_temp['count'] = lst\nX_temp.head()","fa9205ba":"mean_count_sincere = X_temp['count'][X_temp['target'] == 0].mean()","3e414e9c":"print(\"Mean of preprocessed sincere words: {:.0f}\".format(mean_count_sincere))","24db7ee0":"mean_count_unsincere = X_temp['count'][X_temp['target'] == 1].mean()","c47daaaf":"print(\"Mean of preprocessed unsincere words: {:.0f}\".format(mean_count_unsincere))","b42c26ea":"X_train_clean = [\" \".join(x_t) for x_t in stemmed_tokens]\nX_train_clean","a7007ce0":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","7d34d336":"from sklearn.pipeline import Pipeline\nvectorizer = TfidfVectorizer(stop_words='english')\nsvd = TruncatedSVD(random_state=42)\npreprocessing_pipe = Pipeline([('vectorizer', vectorizer), ('svd', svd)])","1edd79e7":"lsa_train = preprocessing_pipe.fit_transform(X_train_clean)\nlsa_train.shape","781ace46":"sns.scatterplot(x=lsa_train[:10000, 0], y=lsa_train[:10000, 1], hue=y_train[:10000]);","06eee0b8":"components = pd.DataFrame(data=svd.components_, columns=preprocessing_pipe.named_steps['vectorizer'].get_feature_names(), index=['component_0', 'component_1'])\ncomponents","a7f42e5b":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\nfor i, ax in enumerate(axes.flat):\n    components.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)","40ab53e5":"def cleaning(df):\n    tokenized_docs = [word_tokenize(doc.lower()) for doc in df]\n    alpha_tokens = [[t for t in doc if t.isalpha() == True] for doc in tokenized_docs]\n    no_stop_tokens = [[t for t in doc if t not in stop_words] for doc in alpha_tokens]\n    stemmed_tokens = [[stemmer.stem(t) for t in doc] for doc in no_stop_tokens]\n    df_clean = [\" \".join(x_t) for x_t in stemmed_tokens]\n    return df_clean","38b593ef":"X_test_clean = cleaning(X_test)\nX_test_clean","ce870caf":"cvec_unigram = CountVectorizer(stop_words='english').fit(X_train_clean)","688e3032":"mb = MultinomialNB()","a6dfb028":"pipe = make_pipeline(cvec_unigram, mb)","b3a40251":"pipe.fit(X_train_clean, y_train)","dffd4b51":"pipe.score(X_train_clean, y_train)","b09372bd":"pipe.score(X_test_clean, y_test)","4c4fa2e1":"y_pred = pipe.predict(X_test_clean)","1de6a123":"confusion_matrix(y_test, y_pred)","0a157c36":"print(classification_report(y_test, y_pred))","cfb9d35f":"scores = cross_val_score(pipe, X_train_clean, y_train, cv=5, scoring='f1')","74e463c7":"scores","b7aa1fa1":"print(\"mean: {}\".format(scores.mean()))\nprint(\"std: {}\".format(scores.std()))","55310f00":"cvec_bigram = CountVectorizer(stop_words='english', ngram_range=(2, 2)).fit(X_train_clean)","51fb351b":"mb = MultinomialNB()","c1a09d1a":"pipe_bi = make_pipeline(cvec_bigram, mb)","60783905":"pipe_bi.fit(X_train_clean, y_train)","a9bb9f0c":"pipe_bi.score(X_train_clean, y_train)","fabefce8":"pipe_bi.score(X_test_clean, y_test)","37b5a375":"y_pred_bi = pipe_bi.predict(X_test_clean)","a2707c0e":"confusion_matrix(y_test, y_pred_bi)","aac6db3f":"print(classification_report(y_test, y_pred_bi))","727d444c":"scores_bi = cross_val_score(pipe_bi, X_train_clean, y_train, cv=5, scoring='f1')","0d78e8b2":"scores_bi","80d89382":"print(\"mean: {}\".format(scores_bi.mean()))\nprint(\"std: {}\".format(scores_bi.std()))","0cdfe997":"cvec_trigram = CountVectorizer(stop_words='english', ngram_range=(3, 3)).fit(X_train_clean)","f979dca7":"mb = MultinomialNB()","542b6a8b":"pipe_tri = make_pipeline(cvec_trigram, mb)","329bec21":"pipe_tri.fit(X_train_clean, y_train)","e9578038":"pipe_tri.score(X_train_clean, y_train)","2afa7f1c":"pipe_tri.score(X_test_clean, y_test)","bf4f0f1b":"y_pred_tri = pipe_tri.predict(X_test_clean)","386db5c7":"confusion_matrix(y_test, y_pred_tri)","6d2226aa":"print(classification_report(y_test, y_pred_tri))","b0b663ee":"scores_tri = cross_val_score(pipe_tri, X_train_clean, y_train, cv=5, scoring='f1')","3ed6232b":"scores_tri","973a2305":"print(\"mean: {}\".format(scores_tri.mean()))\nprint(\"std: {}\".format(scores_tri.std()))","ba51d072":"### Word Tokenize on lower docs","b97fd849":"**NO conclusion here**  \n**Too much different words**","e534428b":"# Repartition of sincere\/unsincere","754ef93c":"NO empty strings","dcf77d8d":"# Quick EDA","772540fc":"# Difference of Lenght Distribution questions","714ae335":"## First word sincere","8cc96c10":"**NO conclusion here**  \n**Too much different words**","786813e2":"# Machine Learning","8d65f408":"# Count stemmed_tokens unsincere\/sincere","01ce9a82":"## CountVectorizer-bigrams","f11a442e":"# Importation datas","c2d83ff1":"**Best score with ngram_range(1, 1): f1_score = 0.54**","a4859667":"### Stop_words","1834b0e8":"# Latent semantic analysis","fa6d9ea2":"## CountVectorizer-Unigrams","6b7841ab":"## Countvectorizer","91a74286":"### Alpha Tokenize","56214cb4":"## First word unsincere","e3cbf59f":"# Preprocessing","115035a5":"### Stemmer","0e11bbe7":"0 -> sincere\n1 -> unsincere","0418678f":"## CountVectorizer-trigrams","2401386e":"NO NAN"}}