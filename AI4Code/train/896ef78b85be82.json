{"cell_type":{"1161d9d3":"code","b6d9b32b":"code","1eb130a6":"code","0b0e4334":"code","5c1d9995":"code","61cf0a4a":"code","30924416":"code","bc379741":"code","d96fdc4d":"code","d8ad10c5":"code","6f64bc7d":"code","530e56db":"code","dd56a8b0":"code","bd987274":"code","01dc9750":"code","6a5ed463":"code","cbdebd85":"code","d397b5f0":"code","de29f9f5":"code","c1a13ad8":"code","5c9dc28d":"markdown","6d302391":"markdown","252e0bfd":"markdown","73d03c92":"markdown","b372c502":"markdown","7a41602e":"markdown","9bbcdd7b":"markdown","f11ba27b":"markdown","efe2f320":"markdown","3175f645":"markdown","630965c4":"markdown","553c7754":"markdown"},"source":{"1161d9d3":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\nimport scipy.stats                # statistics\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"..\/input\"))","b6d9b32b":"df = pd.read_csv(\"..\/input\/pulsar_stars.csv\")\n\n# Print the head of df\nprint(df.head())\n\n# Print the info of df\nprint(df.info())\n\n# Print the shape of df\nprint(df.shape)","1eb130a6":"plt.figure(figsize=(12,8))\nsns.heatmap(df.describe()[1:].transpose(),\n            annot=True,linecolor=\"w\",\n            linewidth=2,cmap=sns.color_palette(\"Blues\"))\nplt.title(\"Data summary\")\nplt.show()","0b0e4334":"# Display the histogram to undestand the data\nf, axes = plt.subplots(2,4, figsize=(20, 12))\nsns.distplot( df[\" Mean of the integrated profile\"], ax=axes[0,0])\nsns.distplot( df[\" Standard deviation of the integrated profile\"], ax=axes[0,1])\nsns.distplot( df[\" Excess kurtosis of the integrated profile\"], ax=axes[0,2])\nsns.distplot( df[\" Skewness of the integrated profile\"], ax=axes[0,3])\nsns.distplot( df[\" Mean of the DM-SNR curve\"], ax=axes[1,0])\nsns.distplot( df[\" Standard deviation of the DM-SNR curve\"], ax=axes[1,1])\nsns.distplot( df[\" Excess kurtosis of the DM-SNR curve\"], ax=axes[1,2])\nsns.distplot( df[\" Skewness of the DM-SNR curve\"], ax=axes[1,3])","5c1d9995":"# Compute the correlation matrix\ncorr=df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.color_palette(\"Blues\")\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","61cf0a4a":"sns.pairplot(df,hue=\"target_class\")\nplt.title(\"pair plot for variables\")\nplt.show()","30924416":"df_scale = df.copy()\ncolumns =df.columns[:-1]\ndf_scale[columns] = power_transform(df.iloc[:,0:8],method='yeo-johnson')\ndf_scale.head()","bc379741":"# Display the histogram to undestand the data\nf, axes = plt.subplots(2,4, figsize=(20, 12))\nsns.distplot( df_scale[\" Mean of the integrated profile\"], ax=axes[0,0])\nsns.distplot( df_scale[\" Standard deviation of the integrated profile\"], ax=axes[0,1])\nsns.distplot( df_scale[\" Excess kurtosis of the integrated profile\"], ax=axes[0,2])\nsns.distplot( df_scale[\" Skewness of the integrated profile\"], ax=axes[0,3])\nsns.distplot( df_scale[\" Mean of the DM-SNR curve\"], ax=axes[1,0])\nsns.distplot( df_scale[\" Standard deviation of the DM-SNR curve\"], ax=axes[1,1])\nsns.distplot( df_scale[\" Excess kurtosis of the DM-SNR curve\"], ax=axes[1,2])\nsns.distplot( df_scale[\" Skewness of the DM-SNR curve\"], ax=axes[1,3])","d96fdc4d":"# Create feature and target arrays\nX = df_scale.iloc[:,0:8]\ny = df_scale.iloc[:,-1]\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123, stratify=y)\nprint('X_train:', X_train.shape)\nprint('X_test:', X_test.shape)","d8ad10c5":"# Seleccion of the model to run\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","6f64bc7d":"# Prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='lbfgs')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='scale')))\nmodels","530e56db":"# Evaluate each model by Accuracy\nresults = []\nnames = []\nseed = 123\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","dd56a8b0":"plt.figure(figsize=(12,8))\nsns.boxplot(x=names, y=results, palette=\"Set3\")\nplt.title(\"Models Accuracy\")\nplt.show()","bd987274":"# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the Grid Search\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5, scoring=scoring)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","01dc9750":"from sklearn.model_selection import cross_val_score\n\n# Fit it to the data with new hyper-parameters\nnew_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, \n                                  max_features = 7, min_samples_leaf = 3)\nnew_cv = cross_val_score(new_tree, X_train, y_train, cv=5, scoring=scoring)","6a5ed463":"# Merging the results with the old group of model to compare results\nnew_results = list(np.vstack((results, new_cv)))\nnames.append('CART_T')","cbdebd85":"plt.figure(figsize=(12,8))\nsns.boxplot(x=names, y=new_results, palette=\"Set3\")\nplt.title(\"Models Accuracy\")\nplt.show()","d397b5f0":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Instantiate the model\nlg = LogisticRegression(solver='lbfgs')\n\n# Fit the classifier to the training data\nlg.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = lg.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred),cbar=False,annot=True,cmap=cmap,fmt=\"d\")","de29f9f5":"from sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = lg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","c1a13ad8":"# Import necessary modules\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = lg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(lg, X, y, cv=5, scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))","5c9dc28d":"The dataset has 17,898 candidates with 8 variables. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency . The remaining four variables are similarly obtained from the DM-SNR curve. Finally we have the label of 1 is the candidate is a pulsar or just caused by RFI\/noise.","6d302391":"## Final Model","252e0bfd":"At the end the best model is the logistic regression compare to the traditional clasification models. ","73d03c92":"## Objective\n\nUsing **supervised machine learning**  the idea is to label pulsar stars to facilitate rapid analysis. This notebook follows the standard procedure to execute a successful classification problem. First, an exploratory data analysis should be done, then some cleaning a feature transformation, then to choose a model from several option and make some hyper-tuning optimization so at the end we have the most accurate model.","b372c502":"Now we know the best hyper-parameters for the tree model. Let's train again this model with these hyper-parameters.","7a41602e":"## Exploratory Data Analysis\n\n[Basic Exploratory Data Analysis](https:\/\/www.kaggle.com\/camiloemartinez\/lucky-charms-lovers) in order to identify if there is a relationship between the dependent variables (pulsar\/noise) and the all the independent variables.","9bbcdd7b":"As the graph shows the CART_T perform much better than CART with just little effort. However, the logistic regression is still better so we choose this model to perform the last steps in this classification problem.","f11ba27b":"> HTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey. A pulsar is a highly magnetized rotating neutron star that emits a beam of electromagnetic radiation. Much like the way a lighthouse can be seen only when the light is pointed in the direction of an observer. (wikipedia)","efe2f320":"## Fine-tuning\nIt looks like logistic regression perform the best. However let\u2019s explore a little with hyper parameter tuning to check if we can make for instance a decision tree classifier perform better.","3175f645":"The dataset has some variables very Skewed. Those are susceptible to make a transformation for modeling.","630965c4":"## Data Transformations\n\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Given the skewed variables and negative values in the data it uses the yeo-johnson transformation.","553c7754":"## Modeling\nThe idea is to perform different standard classification algorithms in order to choose which one is better for the dataset. Later over the most promising model perform some tuning in order to find the best model possible."}}