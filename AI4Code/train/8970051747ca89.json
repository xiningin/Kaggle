{"cell_type":{"7aca76bc":"code","e8fb02ab":"code","6b247b66":"code","e56e3d65":"code","98582630":"code","e24bd9f7":"code","37420386":"code","04b20309":"code","fd556e48":"code","921f5be2":"code","b7ba5346":"code","8495c4e8":"code","7516f869":"code","3f25f189":"code","9c1e094d":"code","18506fb3":"code","1e316640":"code","5d4ac2e9":"code","ac186859":"code","71c50828":"code","4f06c38b":"markdown","6f6395d6":"markdown","be2d37df":"markdown","6b6c47dc":"markdown","e402db0f":"markdown","2efe38e0":"markdown","02b9340a":"markdown","ee9ad966":"markdown","235a7850":"markdown"},"source":{"7aca76bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8fb02ab":"df=pd.read_csv('\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv',encoding ='ISO-8859-1',sep=\";\")\ndf.head()","6b247b66":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# lets create a feature matrix\nfeature_names =    ['Creditability','Account_Balance', 'Duration_of_Credit_monthly', 'Payment_Status_of_Previous_Credit',  'Credit_Amount', 'Value_Savings_Stocks',  'Length_of_current_employment',\n   'Most_valuable_available_asset', 'Age_years', 'No_of_Credits_at_this_Bank', 'Occupation', 'Foreign_Worker']","e56e3d65":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# define function for creating y\ndef yfromX(X):\n    y = X['Creditability'] + X['Duration_of_Credit_monthly']**2 + np.sin(3 * X['Account_Balance']) + (X['Payment_Status_of_Previous_Credit'] * X['Credit_Amount'] * X['Age_years'])\n    return y","98582630":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom sklearn.model_selection import train_test_split\n\n# create x and y\nnp.random.seed(0)\n\nX = pd.DataFrame(np.random.normal(size = (20000, len(feature_names))), columns = feature_names)\ny = yfromX(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ","e24bd9f7":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom eli5.sklearn import PermutationImportance","37420386":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nlr_train_preds = lr.predict(X_train)\nlr_test_preds = lr.predict(X_test)\n\nlr_train_mae = mean_absolute_error(y_train, lr_train_preds)\nlr_test_mae = mean_absolute_error(y_test, lr_test_preds)\n\nlr_fi = PermutationImportance(lr, cv = 'prefit', n_iter = 3).fit(X_train, y_train).feature_importances_","04b20309":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# KNN\n#knn = KNeighborsRegressor(n_neighbors = int(np.sqrt(len(X_train))))\n#knn.fit(X_train, y_train)\n\n#knn_train_preds = knn.predict(X_train)\n#knn_test_preds = knn.predict(X_test)\n\n#knn_train_mae = mean_absolute_error(y_train, knn_train_preds)\n#knn_test_mae = mean_absolute_error(y_test, knn_test_preds)\n\n#knn_fi = PermutationImportance(knn).fit(X_train, y_train).feature_importances_","fd556e48":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# Support Vector Regression \n#svr = SVR(C = .1)\n#svr.fit(X_train, y_train)\n\n#svr_train_preds = svr.predict(X_train)\n#svr_test_preds = svr.predict(X_test)\n\n#svr_train_mae = mean_absolute_error(y_train, svr_train_preds)\n#svr_test_mae = mean_absolute_error(y_test, svr_test_preds)\n\n#svr_fi = PermutationImportance(svr).fit(X_train, y_train).feature_importances_","921f5be2":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# Random Forest \nrf = RandomForestRegressor(max_depth=5)\nrf.fit(X_train, y_train)\n\nrf_train_preds = rf.predict(X_train)\nrf_test_preds = rf.predict(X_test)\n\nrf_train_mae = mean_absolute_error(y_train, rf_train_preds)\nrf_test_mae = mean_absolute_error(y_test, rf_test_preds)\n\nrf_fi = rf.feature_importances_","b7ba5346":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# XGBOOST \n#xgb = XGBRegressor(max_depth=5)\n#xgb.fit(X_train, y_train)\n\n#xgb_train_preds = xgb.predict(X_train)\n#xgb_test_preds = xgb.predict(X_test)\n\n#xgb_train_mae = mean_absolute_error(y_train, xgb_train_preds)\n#xgb_test_mae = mean_absolute_error(y_test, xgb_test_preds)\n\n#xgb_fi = xgb.feature_importances_","8495c4e8":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# Light GBM\nlgb = LGBMRegressor(max_depth=5)\nlgb.fit(X_train, y_train)\n\nlgb_train_preds = lgb.predict(X_train)\nlgb_test_preds = lgb.predict(X_test)\n\nlgb_train_mae = mean_absolute_error(y_train, lgb_train_preds)\nlgb_test_mae = mean_absolute_error(y_test, lgb_test_preds)\n\nlgb_fi = lgb.feature_importances_","7516f869":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# create a dataframe for feature importances and mean absolute error\n#mae_df = pd.DataFrame(columns=['Train','Test'])\n\n# add mae's to the mae_df\n#mae_df.loc['Linear Regression','Train'] =  lr_train_mae\n#mae_df.loc['Linear Regression','Test'] =  lr_test_mae\n\n#mae_df.loc['KNN','Train'] =  knn_train_mae\n#mae_df.loc['KNN','Test'] =  knn_test_mae\n\n#mae_df.loc['Support Vector Regression','Train'] =  svr_train_mae\n#mae_df.loc['Support Vector Regression','Test'] =  svr_test_mae\n\n#mae_df.loc['Random Forest','Train'] =  rf_train_mae\n#mae_df.loc['Random Forest','Test'] =  rf_test_mae\n\n#mae_df.loc['XGBoost','Train'] =  xgb_train_mae\n#mae_df.loc['XGBoost','Test'] =  xgb_test_mae\n\n#mae_df.loc['Light GBM','Train'] =  lgb_train_mae\n#mae_df.loc['Light GBM','Test'] =  lgb_test_mae\n\n#mae_df['Model'] = mae_df.index\n#mae_df['Train'] = mae_df['Train'].astype(float)\n#mae_df['Test'] = mae_df['Test'].astype(float)\n\n#mae_df = pd.melt(mae_df, id_vars=['Model'], value_vars=['Train','Test'])","3f25f189":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter \nimport matplotlib.ticker as mtick","9c1e094d":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# plot mae \n#fig,ax = plt.subplots(figsize=(10,4))\n#ax = sns.barplot(x='value',y='Model', hue='variable',data=mae_df.sort_values(by='value',ascending=False))\n#plt.xlabel('Mean Absolute Error')\n#plt.ylabel('')\n#plt.title('Mean Absolute Error Comparison')\n#plt.tight_layout()","18506fb3":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n# create feature importance dataframe \n#fi_df = pd.DataFrame(columns=['LR', 'KNN','SVR','RF','XGB','LGBM','Features'])\n\n#fi_df['Features'] = feature_names\n#fi_df['LR'] = lr_fi\n#fi_df['KNN'] = knn_fi\n#fi_df['SVR'] = svr_fi\n#fi_df['RF'] = rf_fi\n#fi_df['XGB'] = xgb_fi\n#fi_df['LGBM'] = lgb_fi\/1000","1e316640":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\n#fig = plt.figure(figsize=(18,8))\n\n#plt.subplot(2, 3, 1)\n#ax = sns.barplot(x='LR',y='Features',data=fi_df.sort_values(by='LR',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Linear Regression')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 2)\n#ax = sns.barplot(x='KNN',y='Features',data=fi_df.sort_values(by='KNN',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('K-Nearest Neighbors')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 3)\n#ax = sns.barplot(x='SVR',y='Features',data=fi_df.sort_values(by='SVR',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Support Vector Regression')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 4)\n#ax = sns.barplot(x='RF',y='Features',data=fi_df.sort_values(by='RF',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Random Forest')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 5)\n#ax = sns.barplot(x='XGB',y='Features',data=fi_df.sort_values(by='XGB',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('XGBoost')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 6)\n#ax = sns.barplot(x='LGBM',y='Features',data=fi_df.sort_values(by='LGBM',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Light GBM')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()","5d4ac2e9":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom boruta import BorutaPy\n\nnew_rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n\nboruta_selector = BorutaPy(new_rf, n_estimators = 'auto', random_state = 0)\nboruta_selector.fit(np.array(X_train), np.array(y_train))\n\nboruta_ranking = boruta_selector.ranking_\nselected_features = np.array(feature_names)[boruta_ranking <= 2]","ac186859":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nboruta_ranking = pd.DataFrame(data=boruta_ranking, index=X_train.columns.values, columns=['values'])\nboruta_ranking['Variable'] = boruta_ranking.index\nboruta_ranking.sort_values(['values'], ascending=True, inplace=True)","71c50828":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n\nfig,ax = plt.subplots(figsize=(8,4))\nax = sns.barplot(x='values',y='Variable',data=boruta_ranking, color='b')\nplt.title('Boruta Feature Ranking')\nplt.xlabel('')\nplt.ylabel('')\nplt.tight_layout()","4f06c38b":"#That will take some time","6f6395d6":"#Code by Ajay Sampath https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection","be2d37df":"#Thank you A.J. (Ajay Sampath) https:\/\/www.kaggle.com\/ajaysamp\/boruta-beats-em-all-new-look-at-feature-selection\n","6b6c47dc":"#Discarding FeaturesAccount_Balance\n\nSo, only the first 6 features should be picked by the model and the rest should be discarded. Now, y should be a complex function of the dependent variables and should include linear and non_linear relationships along with interaction effects between the features.\n\nThe function below will be used to determine y.\n\ny = X_linear + (X_nonlinear_square)^2 + sin(3 * X_nonlinear_sin) + (X_interaction_1 + X_interaction_2 + X_interaction_3)","e402db0f":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQIqEpVVea3zN0yAD0CIUFsV6aBpejsv68qPw&usqp=CAU)ushisky.com","2efe38e0":"#Boruta's ranking output (Rank 1: confirmed, Rank 2: some influence, Above Rank 3: Rejected \n\nThe output from Boruta is a ranking number. So we can divide them into categories are need. All features with a rank 1 mean that these have a confirmed effect on the target variable. A rank 2 indicates that there is some influence. Anything above 3 can be rejected as it does not have any infuence.\n\nUsing Boruta, we picked all the correct dependent variables and rejected the noise(??).\n\nBoruta can be a very powerful tool to use and check feature importances from traditional methods.","02b9340a":"#Feature Selection - The Usual Way\n\nGet the features the usual way by running a few popular and unpopular models.","ee9ad966":"#Create X and Y datasets.","235a7850":"#Got it now? It's Boruta and Python the Universal language in Kaggle. \n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcS7zrJJafcYRJZVjD9pommXfSOhTpTgIdXIIA&usqp=CAU)listendata.com"}}