{"cell_type":{"ba74cf10":"code","8cec1a90":"code","9a53079e":"code","aa36083c":"code","2280514b":"code","037b1510":"code","d36fd215":"code","f43b83ba":"code","cd1fc397":"code","d70fea17":"code","86886795":"code","8f6170a9":"code","8f28b75b":"code","498de6e9":"code","6b2f3012":"code","d78f32be":"code","65f2c237":"code","986293fb":"code","947bd659":"code","647a68e2":"code","d0de0ecd":"code","956aad72":"code","0b0acb3e":"code","b2bcecdb":"code","a620b07e":"code","c6eda623":"code","e3a71a7a":"code","e972b955":"code","caa3456e":"code","f08ee821":"code","20a97fb2":"code","4b73f9cc":"code","08734b5d":"code","6dcbc81c":"code","916ed7ca":"code","e721484a":"code","e9893e2c":"code","c0887196":"code","3610d1b6":"code","55c8d875":"code","18b73dd5":"code","bf1025a7":"code","e41748f6":"code","22d9ce5e":"code","51834c49":"code","418d8fd9":"code","d0e7f1c9":"code","59dc20e8":"code","bb1ca9a9":"code","3256a7ba":"code","d8c353b3":"code","efb0c325":"code","30065833":"code","d7a912b8":"code","228b65e8":"code","b38734f2":"code","9a617b99":"code","4cb71e99":"code","2244c785":"code","de43a1d6":"markdown","8c0d630d":"markdown","db7dfd8c":"markdown","472de7d7":"markdown","54c54657":"markdown","52f5d888":"markdown","93be5f95":"markdown","7cdd563b":"markdown","62f20cd4":"markdown","95860545":"markdown","b69ef0e8":"markdown","33583a2a":"markdown","4ff9818c":"markdown","f157e548":"markdown","087bcc49":"markdown","69983436":"markdown","4591d399":"markdown","add6930a":"markdown","1aaaa223":"markdown","87401fa3":"markdown","d82e24c1":"markdown","582fd551":"markdown","e905616e":"markdown","2d5179eb":"markdown","30b768ad":"markdown","667f4a62":"markdown","13a13f0b":"markdown","025955e4":"markdown","06e8ba68":"markdown","5306ace5":"markdown","642ee62f":"markdown","0acbde18":"markdown","a8f74fa3":"markdown","1e62c3cb":"markdown","a5b19a06":"markdown","f337810a":"markdown","18c6d5cb":"markdown","8c10a020":"markdown","0252f32a":"markdown"},"source":{"ba74cf10":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None \n%matplotlib inline","8cec1a90":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col=\"PassengerId\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col=\"PassengerId\")","9a53079e":"print(train_data.info())\nprint(train_data.isna().sum())","aa36083c":"print(test_data.info())\nprint(test_data.isna().sum())","2280514b":"print(train_data[\"Pclass\"].unique())\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","037b1510":"print(train_data[\"Name\"])","d36fd215":"train_data.Name[1].split()","f43b83ba":"train_data = train_data.assign(fname = train_data.Name.str.split(\",\").str[0])\ntrain_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in train_data.Name], index=train_data.index)","cd1fc397":"test_data = test_data.assign(fname = test_data.Name.str.split(\",\").str[0])\ntest_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in test_data.Name], index=test_data.index)\ntrain_data.drop(\"Name\", axis=1, inplace=True)\ntest_data.drop(\"Name\", axis=1, inplace=True)","d70fea17":"print(test_data.fname.nunique())\nprint(test_data.title.nunique())","86886795":"ts = sns.countplot(x=\"title\",data=train_data)\nts = plt.setp(ts.get_xticklabels(), rotation=45)\nprint(train_data[\"title\"].unique())\nprint(test_data[\"title\"].unique())\nother_titles = [title\n                for title in train_data[\"title\"]\n                if title not in [\"Mr\", \"Miss\", \"Mme\", \"Mlle\", \"Mrs\", \"Ms\"]]\nother_titles.append(\"Dona\")","8f6170a9":"train_data[\"title\"] = train_data['title'].replace(other_titles, 'Other')\ntrain_data[\"title\"] = train_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})\ntest_data[\"title\"] = test_data['title'].replace(other_titles, 'Other')\ntest_data[\"title\"] = test_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})","8f28b75b":"print(train_data.title)\nprint(test_data.title.isna().sum()) # No NaNs left","498de6e9":"from sklearn.preprocessing import OneHotEncoder\noh = OneHotEncoder(handle_unknown=\"ignore\", sparse = False)\n\nsurvivors = [x for x in train_data[train_data[\"Survived\"] == 1][\"fname\"]]\nothers = [x for x in train_data[train_data[\"Survived\"] == 0][\"fname\"]]\ntrain_data[\"fname\"] = train_data['fname'].replace(survivors, 'survivor')\ntrain_data[\"fname\"] = train_data['fname'].replace(others, 'other')\ntrain_data[\"fname\"] = train_data[\"fname\"].map({\"survivor\":1, \"other\":0})\n\nother_test = [x for x in test_data[\"fname\"] if x not in survivors]\ntest_data[\"fname\"] = test_data['fname'].replace(survivors, 'survivor')\ntest_data[\"fname\"] = test_data['fname'].replace(other_test, 'other_test')\ntest_data[\"fname\"] = test_data[\"fname\"].map({\"survivor\":1, \"other_test\":0})","6b2f3012":"train_data.drop(\"fname\", axis = 1, inplace = True)\ntest_data.drop(\"fname\", axis = 1, inplace = True)","d78f32be":"print(train_data[\"Sex\"].unique())\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","65f2c237":"interactions = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ninteractions[['sex_class', 'Survived']].groupby(['sex_class'], as_index=False).mean().sort_values(by='Survived', ascending=False)","986293fb":"train_data = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ntest_data = test_data.assign(sex_class = test_data['Sex'] + \"_\" + test_data['Pclass'].astype(\"str\"))","947bd659":"#train_data = train_data.join(pd.get_dummies(train_data['Pclass'], prefix=\"Pclass\"))\n#test_data = test_data.join(pd.get_dummies(test_data['Pclass'], prefix=\"Pclass\"))","647a68e2":"train_data[\"Sex\"] = train_data[\"Sex\"].map({\"female\":0, \"male\":1})\ntest_data[\"Sex\"] = test_data[\"Sex\"].map({\"female\":0, \"male\":1})","d0de0ecd":"train_data[\"sex_class\"] = train_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})\ntest_data[\"sex_class\"] = test_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})","956aad72":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Age\")","0b0acb3e":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b2bcecdb":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a620b07e":"train_data[\"fsize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"fsize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","c6eda623":"train_data[['fsize', 'Survived']].groupby(['fsize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e3a71a7a":"print(train_data.Ticket.nunique())\nprint(train_data.Ticket.tail())","e972b955":"train_data[\"ticket_prefix\"] = pd.Series([len(i.split()) > 1 for i in train_data.Ticket], index=train_data.index)","caa3456e":"train_data[['ticket_prefix', 'Survived']].groupby(['ticket_prefix'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","f08ee821":"train_data.drop(\"ticket_prefix\", axis=1, inplace=True)\ntrain_data.drop(\"Ticket\", axis=1, inplace=True)\ntest_data.drop(\"Ticket\", axis=1, inplace=True)","20a97fb2":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","4b73f9cc":"import numpy as np\ntrain_data[\"Fare\"] = train_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ntest_data[\"Fare\"] = test_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ng = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","08734b5d":"train_data.drop(\"Cabin\", axis=1, inplace=True)\ntest_data.drop(\"Cabin\", axis=1, inplace=True)","6dcbc81c":"#train_data = train_data.join(pd.get_dummies(train_data['Embarked'], prefix=\"Embarked_\"))\n#test_data = test_data.join(pd.get_dummies(test_data['Embarked'], prefix=\"Embarked_\"))\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].map({\"S\":0, \"Q\":1, \"C\":2, \"NaN\": np.nan})\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].map({\"S\":0, \"Q\":1, \"C\":2, \"NaN\": np.nan})","916ed7ca":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_y = train_data[\"Survived\"]\ntrain_data.drop(\"Survived\", axis=1, inplace=True)\n\ncombined = pd.concat([train_data,test_data],keys=[0,1])\n\nimp = IterativeImputer(max_iter = 20, random_state = 42)\ncombined = pd.DataFrame(imp.fit_transform(combined), index = combined.index, columns = combined.columns)\n\ntrain_data = combined.xs(0)\ntest_data = combined.xs(1)\n\ntrain_data[\"baby\"] =  pd.cut(train_data[\"Age\"], bins=[-1,5,100], labels=[1,0]).astype(\"int64\")\ntest_data[\"baby\"] = pd.cut(test_data[\"Age\"], bins=[-1,5,100], labels=[1,0]).astype(\"int64\")\n","e721484a":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\ntrain_scaled = pd.DataFrame(ss.fit_transform(train_data), index = train_data.index)\ntest_scaled = pd.DataFrame(ss.transform(test_data), index = test_data.index)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\n\ncombined = pd.DataFrame(poly.fit_transform(combined), index = combined.index)\n\ntrain_data_poly = combined.xs(0)\ntest_data_poly = combined.xs(1)\n\nss = StandardScaler()\n\ntrain_scaled_poly = pd.DataFrame(ss.fit_transform(train_scaled), index = train_scaled.index)\ntest_scaled_poly = pd.DataFrame(ss.transform(test_scaled), index = test_scaled.index)","e9893e2c":"print(train_data.isna().sum())\nprint(test_data.isna().sum())","c0887196":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\n\ntest_params ={\n    'penalty': [\"l1\", \"l2\", \"none\"],\n    'C': [x for x in np.linspace(0, 10, 500)]\n}\n\nlr_gs = RandomizedSearchCV(lr_model, test_params, cv=5, n_jobs=4, n_iter =500)","3610d1b6":"lr_gs.fit(train_data, train_y)","55c8d875":"print(lr_gs.best_params_)\nprint(lr_gs.best_score_)","18b73dd5":"lr_gs.fit(train_scaled_poly, train_y)","bf1025a7":"print(lr_gs.best_params_)\nprint(lr_gs.best_score_)","e41748f6":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=42)\n\nrf_params ={\n    'bootstrap': [True, False],\n    'max_depth': [int(x) for x in np.linspace(1, 50, 50)],\n    'max_features': ['auto', 'sqrt', \"none\"],\n    'min_samples_leaf': [int(x) for x in np.linspace(1, 8, 8)],\n    'min_samples_split': [int(x) for x in np.linspace(2, 30, 30)],\n    'n_estimators': [int(x) for x in np.linspace(2, 50, 50)]}\n\nrf_gs = RandomizedSearchCV(rf_model, rf_params, cv=5, n_jobs=4, n_iter =500)","22d9ce5e":"rf_gs.fit(train_data, train_y)","51834c49":"print(rf_gs.best_params_)\nprint(rf_gs.best_score_)","418d8fd9":"rf_gs.fit(train_scaled_poly, train_y)","d0e7f1c9":"print(rf_gs.best_params_)\nprint(rf_gs.best_score_)","59dc20e8":"from sklearn.svm import SVC\n\nsvc_model = SVC(random_state=42, max_iter=1000, probability = True)\n\ntest_params ={\n    \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n    \"degree\": [2],\n    \"C\": [x for x in np.linspace(0, 10, 500)]\n}\n\nsvc_gs = RandomizedSearchCV(svc_model, test_params, cv=5, n_jobs=4, n_iter =500)","bb1ca9a9":"svc_gs.fit(train_scaled, train_y)","3256a7ba":"print(svc_gs.best_params_)\nprint(svc_gs.best_score_)","d8c353b3":"svc_gs.fit(train_scaled_poly, train_y)","efb0c325":"print(svc_gs.best_params_)\nprint(svc_gs.best_score_)","30065833":"print(lr_gs.best_score_)\nprint(rf_gs.best_score_)\nprint(svc_gs.best_score_)","d7a912b8":"from sklearn.ensemble import VotingClassifier\n\nvc = VotingClassifier(estimators = [\n    (\"lr\", lr_gs.best_estimator_),\n    (\"rf\", rf_gs.best_estimator_),\n    (\"svc\", svc_gs.best_estimator_)\n], voting = \"soft\")","228b65e8":"vc.fit(train_scaled_poly, train_y)","b38734f2":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","9a617b99":"np.mean(evaluate_model(vc, train_scaled_poly, train_y))","4cb71e99":"preds = vc.predict(test_scaled_poly)","2244c785":"output = pd.DataFrame({'PassengerId': test_data.index,\n                       'Survived': preds})\n\noutput.to_csv('submission.csv', index=False)","de43a1d6":"### 5 & 6) SibSp and Parch:\n\nAs these both relate to family size it's probably best to tackle them together. Many of the notebooks I've\nlooked at combined these into a \"family sizes\" metric.\n\nSibSp: The number of siblings or spouses aboard the titanic.\nParch: The number of parents\/children aboard the titanic.\n\nMaybe I'm a bit slow but this is taking a while to understand.. an example might help.\nSo for a family of 4 (two parents and two kids), the parents would each have a value of 1 for SibSp\nand the kids would also have a value of one (for their spouse and sibling respectively). The parents have 2 kids\non board and each kid has 2 parents on board. So in this case everyone would have a Parch value of 2.\n\nTo get the total size of the family from any one of the members you need 1 (from SibSp) + 2 (from Parch) + 1 (the passenger themself).\nThat makes sense, but just to be sure I'll write out another example with a different size.\n\nA man, his wife and his three kids are on board. The family size is 5 so we should be able to get this from summing SibSp and Parch.\n\nSibSp_Man: 1 (his wife) + Parch_Man: 3 (his kids) + Man: 1 = Total: 5\n\nSibSp_Child_1: 2 (their siblings) + Parch_Child_1: 2 (both parents) + Child_1 = Total: 5\n\n\nOkay this seems to work... although I can see a problem. If the man in the above example has a sibling on board,\nI would intuitively say that the family size is now 6. But you could only get 6 by summing the mans SibSP and Parch values.\nIf you tried to sum the man's brother's columns, you would only end up with a family size of 2 or 3 (the man and possibly his wife).\nI'm very likely reading into this too much but it's something to consider. By summing these two columns you end up in a\nsituation where two people can be in the same \"family\" but have different family sizes. I might come back to this\nbut for now I think I'll just sum the columns. First I'll check the association with survival:","8c0d630d":"And again checking poly + scaled features, making sure to scale AFTER you add polynomial features:","db7dfd8c":"It looks better to me. We can probably leave it as is for now.","472de7d7":"And now seeing if polynomial features make a difference...","54c54657":"### Random Forest Model","52f5d888":"## First step is to load up the data and describe the columns and check for differences.","93be5f95":"### 11) Imputing Age\n\nAdding in the code from above to do the imputing of missing data. I am also then adding in a baby feature, which I could only do after filling in missing age. I think I want to train the imputer on both sets, but first I want to set and drop the Survived column. This way the imputer doesn't try to impute the survival values for the test set. Although I wonder what results that would give...","7cdd563b":"### 9) Cabin:\n\nFrom earlier we saw that many cabin entries were missing. We could probably do something to impute the data but I'll leave that for another iteration. For now I'm just going to drop it.","62f20cd4":"List out all the best scores. Try a voting classifier to merge the three models votes. Support vector machines don't output a probability by default, so if I want to use soft voting (an average of prediction probabilities) I need to go back and enable it.","95860545":"There are three classes (1, 2 and 3) representing first, second or third class tickets on the boat.\n\nIt seems like the passenger's class has a strong association with survival, with the higher class passengers having a higher survival rate.\nIt makes sense to include this in the model, likely without much change.\n\n### 2) Name:","b69ef0e8":"They seem to be numbers, with some having letter prefixes. There are only 681 unique ones in the training dataset\nand with no missing values, it means that some tickets have multiple people on them.\nI'll do the same trick as with the family name and titles, use string split to separate prefixes.","33583a2a":"This looks okay. Small families (4 or less) survived better than people who were alone or in bigger familiess\nWe can throw this in the model. On to the next one...\n\n### 7) Ticket:\n\nLet's take a look at what values tickets take on:","4ff9818c":"Something else that just stood out to me is that I'm not quite sure about how important encoding variables is.\nI've read some places that many models need everything to be encoded as numbers.\n\nThis seems straight forward but the more I think about it, the more confused I get.\nTake Pclass for example. This is encoded numerically and I'm pretty sure it most models would\nhappily take it and not throw out any errors. But if it's left as is, it would be treated the same as\nsomething like Age. While Pclass is ordinal, and having it encoded as 1, 2 and 3 doesn't seem too outrageous,\nI have an uneasy feeling about encoding something with discrete levels the same as a continguous variable (like Age).\n\nI don't know enough about machine learning to actually justify this feeling but just in case I will encode\nanything discrete using dummy variables\/one-hot encoding.\n","f157e548":"I'm still pretty new to python so I'm not sure what the cannonical way of doing this is, but using\na string split seems like the way to go.\n\nAfter fiddling around with google, I think I want to use the .assign method for a pandas dataframe.\n\nIf I split by comma, the first and second entry will give the family name and title respectively.","087bcc49":"I'm going to use a function from the model_selection module in sklearn. This lets me supply a grid of possible values for the parameters and it will test all possible combinations, storing the best result. As I don't want this \"best result\" to be overfitted, I'm going to set the cross-validate (cv) parameter to 10, so it will do 10-fold validation. ","69983436":"In the training set we are missing 177 ages, 2 embarked and 687 cabins.\nIn the test set we're missing 86 ages, 1 fare and 327 cabins.\n\nWith a lot of missing cabins it might make sense to drop the column but we will keep it for now.\n\n## Visualising the Data\nI want to see how each of the variables is associated with survival. Lets go in order:\n\n### 1) Pclass:","4591d399":"### 10) Embarked:\n\nNot much to do here, theres a few missing values which we can fill in.","add6930a":"I think we can drop the name columns now as we won't need it.\nWe'll also need to repeat the above for the test set.\n\n\n(Edit: I originally didn't have the index=train_data.index and all of my pd.Series list comprehensions were coming up\none value short. The joys of 0 indexing vs 1 indexing!)","1aaaa223":"### 8) Fare","87401fa3":"## Modelling\n\nAs it is my first notebook I want to just run something simple that I am comfortable with (logistic regression) as well as two models I'm just starting to learn about (SVM and RF). Features need to be scaled for SVM to work so I'll do that now. I also want to try and see if adding polynomial features will improve LR or SVM.\n\n","d82e24c1":"## Preamble\nHello Kaggle! I'm new-ish to machine learning and wanted to contribute something to Kaggle. I've read a few other people's notebooks for ideas on the titanic competition and wanted to throw my hat in the ring. On paper I have a few years experience in data science but really it's just been fitting linear models. Recently I decided I wanted to learn more so I started a few of the courses on here. Along with my data manipulation I've included some thoughts that popped into my head while I was working through the data. It may be useful, it may not but I thought I'd include it anyways.\n\nI'd appreciate any feedback you have to give. I'm still not sure on which models really need data to be setup numerically with a gaussian distribution or whether it's okay to have ordinal data encoded as integers, but here's my first attempt at a notebook. Enjoy!","582fd551":"#### Family name:\nAs I was thinking about what to do with this, I realised my original idea was probably breaking some rules of data sciencce.\nI was going to encode families as \"had family that survived\". So if a passenger was a part of a family that had survivors,\nmark them down. That way, in the test set, if people had shared family names from the training set, they might be more or\nless likely to have survived.\n\nSo I think while this could actually improve our submission score, it is incorporating some \"future\" knowledge\ninto the model, by creating a variable that is dependant on knowing who survived after the fact.\n\nI guess that leaves me a decision to make: am I trying to write a notebook that maximises my score\n or am I making one to implement a model with actual predictive (i.e. at the time of sinking) power?\n\n As I'm new to all of this and I want to learn techniques that are transferrable, I will focus on only\n using data that doesn't depend on knowing survival. With that said, I think I'll drop family name for now.\n\nI am curious however, is there much of a discussion about this type of \"hacking\" features on Kaggle?\nI can imagine we could come up with other examples (such as cabin) where we could potentially improve a submission\nscore by making a model will less predictive power. Anyways back to the analysis...\n\nUpdate: After reading more on the topic, I don't think there is anything wrong with doing this, I might have just ended up confusing myself. The idea of \"predicting\" who survived could be seen as predicting who would survive if the titanic were to sail again with the same (or similar) population. In that sense, picking a family name as a feature is no different to picking age as a feature. As such I'm going to update the model to include this and see if it makes a difference!\n","e905616e":"While I'm at it, I'll encode it and Sex as numeric using the map method.","2d5179eb":"### Support Vector Machine\nGoing to use the scaled features for this one.","30b768ad":"While the picture isn't super clear, you can see that survivors had more expensive fares and a wider spread of fare prices.\nThere is at least one outlier with a fare of >500 so I might drop him.\n\nApart from that the data is pretty skewed. I'll take a log transformation to reduce the skew and to\ndecrease the massive range in fares.","667f4a62":"I can think of a few things we could do with the names.\n\nFirstly, we could match up surnames to group families together.\nI could imagine that whole families either survived or died together.\n\nSecondly, we can get the titles of the names. As well as there being common ones such as\nMr and Miss, it seems like there are rare\/unique ones such as Rev (reverend). If\nsomeone is important enough to have their own title they might have been more likely to survive.","13a13f0b":"Thanks for your attention :) ","025955e4":"We have two labels for Sex, with females having a much higher survival rate.\nIt makes sense to include Sex in the model. It is possible that we could use sex to create a new feature by combining\nit with other features. For example, what about Sex and Pclass that we looked at earlier?\n\nAcross all males and females, females have a much higher survival rate. But what if wealthy males have a higher survival\nthan poor females? It might make sense to segment this out explicity.","06e8ba68":"There are a lot of uniques so  I think it makes sense to group them.\n\n#### Titles:\nFor now we will stick to headings that representing male, female, child and other.\nI'll then encode them as numerical.\nI will use the pandas dataframe replace and map functions for this:","5306ace5":"### 4) Age\n\nFirst thing's first, let's look at the distribution of age and see if there is any association with survival.","642ee62f":"Well that doesn't look promising. I would have thought the prefixes had some sort of importance.\nThere probably is something I could do with this but for now I'll just drop it.","0acbde18":"Now to look at what we've made:","a8f74fa3":"After yet MORE reading I've decided that any model that includes family name will massively overfit the data. It has a near perfect correlation with survival in the training set which almost certainly doesn't hold for the test set. \n\nThere may be something I could do to include family name but for now I will stick with my original decision to drop it.","1e62c3cb":"Since I have to use scaled data for the svm, and the polynomial features worked best for LR and SVM I'll use the scaled_poly set. ","a5b19a06":"### 1) Logistic Regression:\n\nFirst I will split the dataframes up into the independant variables (usually denoted as the matrix X) and the dependant variable (the vector y). I'll do one last check to make sure I have no NAs. ","f337810a":"Both stories tell a similar story, that smaller families tended to survive more than larger families.","18c6d5cb":"### 3) Sex:","8c10a020":"It certainly seems like this interaction feature adds something...\n\nAs I'm still new to this I don't yet know if the models will pick up this interaction without me\nexplicitly adding it as a feature. If I  DO include this column, it will be pretty highly associated with\nboth sex and class so again I'm not sure if that is something that can interfere with modeling.\n\nFor now, my ignorance will let me add it to the dataset and deal with any issues that arise later on.","0252f32a":"While the distributions look similar-ish, we can see that a bigger chunk of the survivors\nwere very young (10 or under) and not many survivors were over 60.\n\nAs a novice, my only real experience with machine learning is fitting linear models. Linar models\ndo pretty poorly when the underlying data isn't normally distributed. It can really only pick up the general trend of\n\"being older is better\" or \"being younger is better\". If a linear model were used here, it might pick up that being\nyounger is better for survival but it will miss the details. As this is the only reference point I have,\nI'll try to transform the data as if I was going to fit a linear model to it.\n\nBut first, there are some missing values that need to be dealt with.\nThere are (at least) three ways we can deal with this, each one being slightly more effort.\n\nFirstly, we can just drop the rows with missing data. While this might be tempting, dropping a row with around 14\nother entries just because of one missing value doesn't sound like the brightest idea.\n\nSecondly, we can replace the missing data with the average age (whether it's median\/mode\/mean) of the data set.\nThis would be a good first pass method and it would let us get the models up and running.\n\nThirdly, we can replace the missing data with the average from similar passengers. For example,\nif we're missing the age of a 1st class passenger, who is female, who embarked from C etc. we could substitute in the\nage of other passengers who fit that description.\n\nAs I want to get a bit more experience using Python\/Pandas I'm going to go with option 3.\nMy rough idea is to rank all the passengers based on their similarity to the missing passenger.\nI think the title and Pclass features will be good for this. I want to have a heirarchical approach.\nTry to replace with someone of the same title and Pclass, then of sex and class, then mean of just sex.\n\nUpdate: After reading around on the sklearn user guide,there is an experimental multivariate feature imputer that I am going to use to fill in all the missing data to see how it fares. I'll add this at the end as I need to drop the target feature (Survived) before I use the imputer (so it doesn't try to impute missing survived data in the test set). To see this, skip to section 11) Imputing Age."}}