{"cell_type":{"f4244479":"code","a36feab7":"code","a6d5df38":"code","8bba60c8":"code","49cd36da":"code","edac1139":"code","4b110cc4":"code","2618c116":"code","9cc1a455":"code","40952d31":"code","f8da62a5":"code","2684e68b":"code","6ea784e8":"code","181cde8d":"code","9317aaa5":"code","c7ac4135":"code","0e1d50e4":"code","17354245":"code","d556512f":"code","8c2490c9":"code","2eb81055":"code","1cf9a908":"code","d098f87a":"code","095bc695":"code","9ee40b2c":"code","7adadcc6":"code","b667fa1f":"code","a1dafacc":"code","d2d5ffe8":"code","71a56ea5":"code","f3bbb1cf":"code","103f66db":"code","a9bc61f9":"code","77f230e5":"code","a3d9b626":"code","fdc6f37d":"code","4804017f":"code","cde83eed":"code","d514dd5c":"code","2aeb7c32":"code","324df233":"code","9d992519":"code","b8346c15":"code","698bc7fa":"code","77a7f69a":"code","17e50af8":"code","6c6a6ea3":"code","ba822f8b":"code","61b603ce":"code","f8e01115":"code","8bcc3c8f":"code","68121a62":"code","3b6bd8f0":"code","d7c58dcc":"code","1580aac4":"code","6c101bc6":"code","ccd7be22":"code","857b65b5":"code","9a38372b":"code","f9dca317":"code","a9589580":"code","a8b1e0e6":"code","d55f0df2":"code","ed508d2b":"code","82b4896d":"code","5c60cc04":"code","83fbfb47":"code","74fcf902":"code","aa761421":"code","5ee0a71b":"code","d31bbeb1":"code","2db4a8b0":"code","1431f3aa":"code","f6cb2f0e":"code","d9cdd540":"code","fc8bb2da":"code","1825fb28":"code","c5293281":"code","1c716e3d":"code","90ef90a2":"code","2baefa3d":"code","9dc0eba5":"code","1424fe08":"code","e32b39bc":"code","1adc1e88":"code","1632fcda":"code","4aee4bcd":"code","519451fa":"code","4fe5264a":"code","9d604e39":"code","f3478285":"code","f25bc044":"code","3311895b":"code","68e5817b":"code","48cc59e0":"code","e0a2114b":"code","fb97089c":"code","d262f961":"code","dadef684":"code","2f0797e9":"code","43a7675b":"code","98c26e11":"code","f28946f7":"code","8e3fdb8c":"code","a5c44f7d":"code","4aa7af11":"code","c3061058":"code","810b0882":"code","dc89c052":"code","98d328aa":"code","b9d063ca":"code","59f7acff":"code","e9750b7d":"code","898cfd0d":"code","9e30cea0":"code","d64a2108":"code","4a9e9c8d":"code","ca1cf6e7":"code","2014bbba":"code","da0c0152":"code","aef60a4e":"code","09d4089e":"code","f3042006":"code","63296525":"code","91c78e06":"code","c0d585f2":"code","7321d9ee":"code","1f50b892":"code","d6010f8d":"code","673834b2":"code","11eefda1":"code","11bb8693":"code","1e634f59":"code","d4a96786":"code","b115fb4a":"code","5f586699":"code","bc85699b":"code","dff40e4e":"code","b23e76f8":"code","a1c45ec7":"code","848052b9":"code","a2d60ffd":"code","182c738f":"code","2beda124":"code","813f74ce":"code","fdc3dc11":"code","8281a259":"code","68b496a8":"code","2f7323c5":"code","63f8bec0":"code","d71f3afb":"code","5fd04df9":"code","15af2d57":"code","5ffd14d3":"code","21645fc3":"code","1bc63284":"code","81270e08":"code","d2d0dfbb":"code","0ac94bc8":"code","dd192ba7":"code","3a9ee1d9":"code","ada61259":"code","9f12ad16":"code","0d8c1502":"code","1cb060bc":"code","c42cdab0":"code","8630ef76":"code","43f2fec4":"code","9b60a2e3":"code","2d1106e7":"code","9adca959":"code","1fd9302f":"code","6e380309":"code","7c03a9ab":"code","e4ea9789":"code","4ec00227":"code","bb34cf5a":"code","41776476":"code","90450d6a":"code","7675bb83":"code","ecb4d87a":"code","6168ecd9":"code","e8f30fce":"code","07938eab":"code","32615375":"code","f49bb61d":"code","9b1cf963":"code","b9206378":"markdown","001cdc57":"markdown","fdacb1f1":"markdown","650e2e75":"markdown","c5d7eda1":"markdown","575c2c3a":"markdown","f87a2629":"markdown","4f669ac3":"markdown","f5767e34":"markdown","ccdbb046":"markdown","d53e559e":"markdown","34c95f27":"markdown","b86e5fb9":"markdown","89605b96":"markdown","7e8be85b":"markdown","1be3c526":"markdown","1a0ad8be":"markdown","f7dd6bd3":"markdown","764109e5":"markdown","1ab6382b":"markdown","7a0c9f97":"markdown","095f235a":"markdown","8e2cf614":"markdown","3964bc9b":"markdown","dd1485e2":"markdown","09a73581":"markdown","92092961":"markdown","aa1488fe":"markdown","4dd60cff":"markdown","51353a52":"markdown","181c735e":"markdown","f582b436":"markdown","98ec3bb7":"markdown","cbd11b73":"markdown","021babb6":"markdown","084ca1f6":"markdown"},"source":{"f4244479":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a36feab7":"# installing the third party packages\n\n!pip install -q feature-engine","a6d5df38":"# the basic python libraries\n\nimport warnings\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\nwarnings.filterwarnings('ignore')","8bba60c8":"# Importing the featuretools library\n\nimport featuretools as ft","49cd36da":"# Importing the feature-engine library\n\nfrom feature_engine.imputation import MeanMedianImputer\nfrom feature_engine.encoding import RareLabelEncoder\nfrom feature_engine.encoding import MeanEncoder\nfrom feature_engine.encoding import CountFrequencyEncoder","edac1139":"# Importing sklearn dependencies\n\n# To build the models\nfrom sklearn import metrics\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression","4b110cc4":"# importing the statsmodel library \n\nimport statsmodels.api as sm","2618c116":"# display all the columns of the dataframe\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","9cc1a455":"# Reading the price datasets\n\n# define the columns along with the datatypes\nprices_columns = {\n    'date': np.object,\n    'symbol': np.object,\n    'open': np.float32,\n    'close': np.float32,\n    'low': np.float32,\n    'high': np.float32,\n    'volume': np.float32\n}\n\n# reading the .csv file and converting into dataframe\nprices_dataframe = pd.read_csv('\/kaggle\/input\/nyse\/prices.csv', dtype=prices_columns)","40952d31":"# Reading the fundamentals datasets\n\nfundamentals_dataframe = pd.read_csv('\/kaggle\/input\/nyse\/fundamentals.csv', index_col=['Unnamed: 0'])","f8da62a5":"# Reading the securities datasets\n\nsecurities_dataframe = pd.read_csv('\/kaggle\/input\/nyse\/securities.csv')","2684e68b":"# sample records from prices dataset\n\nprices_dataframe.head(n=5)","6ea784e8":"# sample records from securities dataset\n\nsecurities_dataframe.head(n=5)","181cde8d":"# sample records from fundamentals data\n\nfundamentals_dataframe.head(n=5)","9317aaa5":"# On Prices dataset\n\nprices_dataframe.info()","c7ac4135":"# On fundamentals dataset\n\nfundamentals_dataframe.info()","0e1d50e4":"# On securities dataset\n\nsecurities_dataframe.info()","17354245":"# proportation of missing values in securities dataframe\n\nround(securities_dataframe.isnull().sum(axis=0) \/ len(securities_dataframe), 2)","d556512f":"# proportion of missing values in fundamentals dataframe\n\nround(fundamentals_dataframe.isnull().sum(axis=0) \/ len(fundamentals_dataframe), 2)","8c2490c9":"# Missing values in securities dataframe\n\nsecurities_dataframe['Date first added'] = securities_dataframe['Date first added'].fillna(method='backfill')\n\n# Note:\n# 1. Found the missing values in securities dataset Date first Added column \n# 2. As the date represented as np.object column we're filling the missing values by using backfill method of pandas.`fillna` method.","2eb81055":"# filling the missing values of the fundamentals dataframe\n\nmissing_columns = ['Current Ratio', 'Cash Ratio', 'Quick Ratio', 'For Year', 'Earnings Per Share', 'Estimated Shares Outstanding']\nfor column in missing_columns:\n    median_value = fundamentals_dataframe[column].median()\n    fundamentals_dataframe[column] = fundamentals_dataframe[column].fillna(median_value)\n\n# Note:\n# 1. Found missing values in the fundamentals dataset for the above columns.\n# 2. All the missing columns are numeric data columns so filling the data with it;s median value of that column using pandas.`fillna` method.","1cf9a908":"# converting the prices dataset date column into timestamp\n\nprices_dataframe['date'] = pd.to_datetime(prices_dataframe['date'])","d098f87a":"# Total number of unique stock symbols available in the dataset\n\nprint(f\"Total number of unique stock symbols available in dataset: {len(np.unique(prices_dataframe['symbol']))}\")","095bc695":"# distribution of price open value in the dataframe\n\nplt.figure(figsize=(6, 4))\nplt.hist(prices_dataframe['open'], bins=10)\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. Most of the stock are have their opening prices values between 0 and 400\n# 2. Only few stocks opening prices are higher values","9ee40b2c":"# distribution of price close value in the dataframe\n\nplt.figure(figsize=(6, 4))\nplt.hist(prices_dataframe['close'], bins=10)\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. Most of the stock are have their closing prices values between 0 and 400\n# 2. Only few stocks closing prices are higher values","7adadcc6":"# distribution of price low value in the dataframe\n\nplt.figure(figsize=(6, 4))\nplt.hist(prices_dataframe['low'], bins=10)\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. Most of the stock are have their lowest prices values between 0 and 400\n# 2. Only few stocks lowest prices are higher values","b667fa1f":"# distribution of price high value in the dataframe\n\nplt.figure(figsize=(6, 4))\nplt.hist(prices_dataframe['high'], bins=10)\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. Most of the stock are have their highest prices values between 0 and 400\n# 2. Only few stocks highest prices are higher values","a1dafacc":"# As the data on prices dataset is moving with time let's do \/ capture trend of the dataset.\n# Making the date column of prices dataset as an index\n\nprices_dataframe_indexed = prices_dataframe.set_index('date')","d2d5ffe8":"# Trend on opening prices\n\nplt.figure(figsize=(6,4))\nplt.plot(prices_dataframe_indexed['open'])\nplt.tight_layout()\nplt.show()","71a56ea5":"# Trend on closing prices\n\nplt.figure(figsize=(6,4))\nplt.plot(prices_dataframe_indexed['close'])\nplt.tight_layout()\nplt.show()","f3bbb1cf":"# Trend on Lowest price values\n\nplt.figure(figsize=(6,4))\nplt.plot(prices_dataframe_indexed['low'])\nplt.tight_layout()\nplt.show()","103f66db":"# Trend on Highest Prices values\n\nplt.figure(figsize=(6,4))\nplt.plot(prices_dataframe_indexed['high'])\nplt.tight_layout()\nplt.show()","a9bc61f9":"# Trend on prices dataset volumn column\n\nplt.figure(figsize=(6,4))\nplt.plot(prices_dataframe_indexed['volume'])\nplt.tight_layout()\nplt.show()","77f230e5":"# Total number SEC filling available in the securities dataset\n\nsecurities_dataframe['SEC filings'].value_counts()","a3d9b626":"# Total number of GICS Sectors available the securities dataset\n\nplt.figure(figsize=(6, 4))\nsecurities_dataframe['GICS Sector'].value_counts().sort_values(ascending=False).plot(kind='bar')\nplt.show()","fdc6f37d":"# Numerical vs Numerical\n# Relation between open and close prices\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['open'], y=prices_dataframe['close'])\nplt.tight_layout()\nplt.show()\n\n# Note:\n# There is highly positive correlation between open and close column.","4804017f":"# Relation between Low and Close columns\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['low'], y=prices_dataframe['close'])\nplt.tight_layout()\nplt.show()\n\n# Note: \n# There is highly positive correlation between Low and Closing Price columns.","cde83eed":"# Relation between high and close prices columns\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['high'], y=prices_dataframe['close'])\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. Positive or High correlation between dataset","d514dd5c":"# Relation between close and volume prices\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['volume'], y=prices_dataframe['close'])\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. No  correlation between the columns.","2aeb7c32":"# Relation between open and volume\n\n# Relation between close and volume prices\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['volume'], y=prices_dataframe['open'])\nplt.tight_layout()\nplt.show()\n\n# Note:\n# No correlation between the columns.","324df233":"# Relation between high and volume\n\n# Relation between close and volume prices\n\nplt.figure(figsize=(6,4))\nplt.scatter(x=prices_dataframe['volume'], y=prices_dataframe['high'])\nplt.tight_layout()\nplt.show()\n\n# Note:\n# No correlation between the columns.","9d992519":"# correlation between the columns\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(prices_dataframe.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. From the above heatmap it's look like the prices dataset columns are highly correlated with each other.","b8346c15":"# converting the `date first added` into timestamp datatype\n\nsecurities_dataframe['Date first added'] = pd.to_datetime(securities_dataframe['Date first added'])","698bc7fa":"plt.figure(figsize=(6,4))\nplt.hist(securities_dataframe['CIK'], bins=10)\nplt.tight_layout()\nplt.show()\n\n# Note:\n# 1. From the below histogram we can say that the data is not normally distributed.\n# 2. The below distribution has more than one mode.","77a7f69a":"# converting all date categorical column into timestamp columns\n\nfundamentals_dataframe['Period Ending'] = pd.to_datetime(fundamentals_dataframe['Period Ending'])","17e50af8":"# Taking a portion of the data only to avoid memory issues.\n\nprices_dataframe = prices_dataframe.sample(frac=0.75, replace=False, random_state=7)","6c6a6ea3":"# converting the date column year, month and day\n\nprices_dataframe['year'] = prices_dataframe['date'].dt.year\nprices_dataframe['month'] = prices_dataframe['date'].dt.month\nprices_dataframe['hour'] = prices_dataframe['date'].dt.day","ba822f8b":"# Assigning a holyday based on date given\n\n# creating the calendar instance\ncalendar = calendar()","61b603ce":"# Holiday Calculation\n\nstart = prices_dataframe['date'].min()\nend = prices_dataframe['date'].max()\nholidays = calendar.holidays(start=start, end=end)\nprices_dataframe['is_holiday'] = prices_dataframe['date'].apply(lambda date: True if date in holidays else False)","f8e01115":"# Converting the date column of Securities dataset as Year, Month and Day column\n\nsecurities_dataframe['Year First Added'] = securities_dataframe['Date first added'].dt.year\nsecurities_dataframe['Month First Added'] = securities_dataframe['Date first added'].dt.month\nsecurities_dataframe['Day First Added'] = securities_dataframe['Date first added'].dt.day","8bcc3c8f":"# Looking for whether first added stock date is holiday or not\n\nstart = securities_dataframe['Date first added'].min()\nend = securities_dataframe['Date first added'].max()\nholidays = calendar.holidays(start=start, end=end)\nsecurities_dataframe['is_registered_holiday'] = securities_dataframe['Date first added'].apply(lambda date: True if date in holidays else False)","68121a62":"# Splitting the string into city and state column\n\nsecurities_dataframe[['City', 'State']] = securities_dataframe['Address of Headquarters'].str.split(',', n=1, expand=True)","3b6bd8f0":"# dropping the redudant Address column from securities dataframe\n\nsecurities_dataframe.drop(labels=['Address of Headquarters'], axis=1, inplace=True)","d7c58dcc":"# Converting the Fundamentals dataset Period Ending column into Year, Month and day\n\nfundamentals_dataframe['Period Ending Year'] = fundamentals_dataframe['Period Ending'].dt.year\nfundamentals_dataframe['Period Ending Month'] = fundamentals_dataframe['Period Ending'].dt.month\nfundamentals_dataframe['Period Ending Day'] = fundamentals_dataframe['Period Ending'].dt.day","1580aac4":"# Looking for whether for whether Period Ending is fall on holiday or not\n\nstart = fundamentals_dataframe['Period Ending'].min()\nend = fundamentals_dataframe['Period Ending'].max()\nholidays = calendar.holidays(start=start, end=end)\nfundamentals_dataframe['Is Period Ended on Holiday'] = fundamentals_dataframe['Period Ending'].apply(lambda date: True if date in holidays else False)","6c101bc6":"# Creating Entities for the above dataframes\n# Creating the entityset\n\nes = ft.EntitySet(id='nyse')\nes","ccd7be22":"# Creating the Securities Entity from securities dataframe\n# The securities dataframe also doesn't have an unique index\n\nes = es.entity_from_dataframe(entity_id='securities', \n                              dataframe=securities_dataframe,\n                              variable_types={'Ticker symbol': ft.variable_types.Categorical, 'Security': ft.variable_types.Categorical, 'SEC filings': ft.variable_types.Categorical,\n                                              'GICS Sector': ft.variable_types.Categorical, 'GICS Sub Industry': ft.variable_types.Categorical,\n                                              'is_registered_holiday': ft.variable_types.Boolean, 'City': ft.variable_types.Categorical, 'State': ft.variable_types.Categorical}, \n                              index='Ticker symbol', \n                              time_index='Date first added')\nes","857b65b5":"# Creating Prices Entity from prices dataframe\n# The prices dataframe doesn't have an unique index \n\nes = es.entity_from_dataframe(entity_id='prices', \n                              dataframe=prices_dataframe,\n                              variable_types={'symbol': ft.variable_types.Categorical, 'is_holiday': ft.variable_types.Boolean}, \n                              make_index=True, \n                              index='prices_id', \n                              time_index='date')\nes","9a38372b":"# Creating the fundamentals Entity for fundamentals dataframe\n# The fundamentals dataframe also dosn't have an unique index\n\nes = es.entity_from_dataframe(entity_id='fundamentals', \n                              dataframe=fundamentals_dataframe,\n                              variable_types={'Ticker Symbol': ft.variable_types.Categorical,'Is Period Ended on Holiday': ft.variable_types.Boolean}, \n                              make_index=True, \n                              index='fundamentals_id', \n                              time_index='Period Ending')\nes","f9dca317":"# Creating Relationships for the above created entities\n# Creating a Relationship between securities and prices dataset\n\nr_prices = ft.Relationship(es['securities']['Ticker symbol'], es['prices']['symbol'])\n\n# Adding the above created relationship to entity set\nes = es.add_relationship(r_prices)","a9589580":"# Creating a Relationship between securites and fundamentals dataset\n\nr_fundamentals = ft.Relationship(es['securities']['Ticker symbol'], es['fundamentals']['Ticker Symbol'])\n\n# Adding the above created relationship to entity set\nes = es.add_relationship(r_fundamentals)","a8b1e0e6":"# Entityset with all the dependencies and its relationships.\n# entityset with all the entities and their relatioship\n\nes","d55f0df2":"# creating new features\n\ndataframe, names = ft.dfs(entityset=es, target_entity='prices', max_depth=2)","ed508d2b":"print(f\"Total number of features found using featuretools: {len(names)}\")","82b4896d":"# output dataframe shape\n\ndataframe.shape","5c60cc04":"# sample records \/ first five records from the target dataframe\n\ndataframe.head(n=5)","83fbfb47":"# Missing Value Imputation on above generated dataset\n\n# Looking for percentage of missing values in new dataframe\n\nna_columns = dataframe.columns[dataframe.isnull().any()]","74fcf902":"print(f\"Total number of columns which contains one or more missing values: {len(na_columns)}\")","aa761421":"# Filling all missing values using it's median or mean\n\nimputer = MeanMedianImputer(imputation_method='median', variables=list(na_columns))","5ee0a71b":"imputer.fit(dataframe)","d31bbeb1":"# Imputing the data\n\ndataframe = imputer.transform(dataframe)","2db4a8b0":"# Getting all the categorical columns from the master dataframe for encoding the categorical columns to numerical columns\n\ncat_columns = [column for column in dataframe if dataframe[column].dtype == np.object]","1431f3aa":"print(f\"Total number of categorical columns found in the dataset: {len(cat_columns)}\")","f6cb2f0e":"# Encoding RareLabel using RareLabelEncoder\n\nrare_encoder = RareLabelEncoder(tol=0.05, n_categories=2, variables=cat_columns, replace_with=\"Rare\")","d9cdd540":"rare_encoder.fit(dataframe)","fc8bb2da":"# Eliminating all rare variables in the dataset by group them as one category as 'Rare'\n\ndataframe = rare_encoder.transform(dataframe)","1825fb28":"# Encoding all the categorical columns into numeric using MeanTargetEncoding\n\nlabel_encoder = CountFrequencyEncoder(encoding_method='frequency', variables=cat_columns)","c5293281":"# Fitting the Rare Label Encoder for eliminating all the Rare categories of the column to one Category as Rare\n\nlabel_encoder.fit(dataframe)","1c716e3d":"dataframe = label_encoder.transform(dataframe)","90ef90a2":"# Getting all the boolean columns from the master dataframe for converting the boolean columns to numerical column TRUE -> 1 to FALSE -> 0\n\nbool_columns = [column for column in dataframe.columns if dataframe[column].dtype == np.bool]","2baefa3d":"print(f\"Total number of boolean columns found in the dataset: {len(bool_columns)}\")","9dc0eba5":"for column in bool_columns:\n    dataframe[column] = dataframe[column].map(lambda boolValue: 1 if boolValue else 0)","1424fe08":"# sample records after performing all types of encoding\n\ndataframe.head(n=5)","e32b39bc":"# adding utility functions\n\n# Creating a utility function to standardize \/ normalize the data and bring into -4 to +4 form\ndef normalize (X): \n    # return ( (x-np.mean(x))\/ (max(x) - min(x)))\n    return (X - np.mean(X)) \/ (np.std(X))\n\n# adding standardization function","1adc1e88":"# adding vif calculation function\n# UDF for calculating vif value\ndef vif_cal(input_data, dependent_col):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared  \n        vif=round(1\/(1-rsq),2)\n        vif_df.loc[i] = [xvar_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)","1632fcda":"# Splitting the dataframe into X and y\n\nX = dataframe.drop(['close'], axis=1)\ny = dataframe['close']","4aee4bcd":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.001 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.001, random_state=7))","519451fa":"# Training the above algorithm\n\nselector.fit(X=X, y=y)","4fe5264a":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","9d604e39":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","f3478285":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","f25bc044":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","3311895b":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","68e5817b":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","48cc59e0":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","e0a2114b":"lm.summary()","fb97089c":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","d262f961":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","dadef684":"# Looking for correlation in the training data\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(X_train.corr(), annot=False, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","2f0797e9":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.005 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.005, random_state=7))","43a7675b":"# Training the above Model\n\nselector.fit(X=X, y=y)","98c26e11":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","f28946f7":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","8e3fdb8c":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","a5c44f7d":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","4aa7af11":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","c3061058":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","810b0882":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","dc89c052":"lm.summary()","98d328aa":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","b9d063ca":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","59f7acff":"# Looking for correlation in the training data\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","e9750b7d":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.010 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.010, random_state=7))","898cfd0d":"# Training the above Model\n\nselector.fit(X=X, y=y)","9e30cea0":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","d64a2108":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","4a9e9c8d":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","ca1cf6e7":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","2014bbba":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","da0c0152":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","aef60a4e":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","09d4089e":"lm.summary()","f3042006":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","63296525":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","91c78e06":"# Looking for correlation in the training data\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","c0d585f2":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.025 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.025, random_state=7))","7321d9ee":"# Training the above Model\n\nselector.fit(X=X, y=y)","1f50b892":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","d6010f8d":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","673834b2":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","11eefda1":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","11bb8693":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","1e634f59":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","d4a96786":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","b115fb4a":"lm.summary()","5f586699":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","bc85699b":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","dff40e4e":"# Looking for correlation in the training data\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","b23e76f8":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.050 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.050, random_state=7))","a1c45ec7":"# Training the above Model\n\nselector.fit(X=X, y=y)","848052b9":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","a2d60ffd":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","182c738f":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","2beda124":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","813f74ce":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","fdc3dc11":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","8281a259":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","68b496a8":"lm.summary()","2f7323c5":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","63f8bec0":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","d71f3afb":"# Looking for correlation in the training data\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","5fd04df9":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.100 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.100, random_state=7))","15af2d57":"# Training the above Model\n\nselector.fit(X=X, y=y)","5ffd14d3":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","21645fc3":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","1bc63284":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","81270e08":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","d2d0dfbb":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","0ac94bc8":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","dd192ba7":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","3a9ee1d9":"lm.summary()","ada61259":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","9f12ad16":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","0d8c1502":"# Looking for correlation in the training data\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","1cb060bc":"# Using Lasso Regression for selecting K best features for model building\n# alpha \/ premium = 0.500 adding to the Lasso or L1 regularization parameter.\n\nselector = SelectFromModel(Lasso(alpha=0.500, random_state=7))","c42cdab0":"# Training the above Model\n\nselector.fit(X=X, y=y)","8630ef76":"# Select the K best features for the above trained model\n\nfeatures_selected = X.columns[selector.get_support()]","43f2fec4":"# getting the number of selected features\n\nprint(f\"Total number of features selectef from the Lasso regression: {len(features_selected)}\")","9b60a2e3":"# creating X_train and y_train from the selected dataframes\n\nX_train, y_train = X[features_selected], y","2d1106e7":"# Normalization of the data\n# Applying normalization to rescale every variables\n\nX_train = X_train.apply(normalize)","9adca959":"# splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","1fd9302f":"# Building a Linear Model by adding a constant to the dataset\n\nX_train = sm.add_constant(X_train)","6e380309":"# Ordinary Least Square Model\n\nlm = sm.OLS(y_train, X_train).fit()","7c03a9ab":"lm.summary()","e4ea9789":"# Callinf the above vif_call() function to see any multicolineary exists in the dataset\n\ntmp_dataframe = pd.concat([X_train, y_train], axis=1)","4ec00227":"# Calculating the VIF score for the above generated model.\n\nvif_cal(tmp_dataframe, dependent_col='close')","bb34cf5a":"# Looking for correlation in the training data\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(X_train.corr(), annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()","41776476":"# dropping the constant\n\nX_train = X_train.drop(['const'], axis=1)","90450d6a":"# # splitting the input dataset into traning and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=7, shuffle=True, test_size=0.10)","7675bb83":"### Building Linear Regression Model\n\nlr_reg = LinearRegression(n_jobs=-1)","ecb4d87a":"# Training the above Linear Regression Model\n\nlr_reg.fit(X_train, y_train)","6168ecd9":"### Getting the predictions for the given test dataset\n\ny_pred = lr_reg.predict(X_test)","e8f30fce":"# Plotting the Ground truth and Actuval Value\n\nc = [idx for idx in range(1, 57462, 1)]\nfig = plt.figure(figsize=(12, 8))\nplt.plot(c, y_test, color='blue', linewidth=2.5, linestyle='-')\nplt.plot(c, y_pred, color='red', linewidth=2.5, linestyle='-')\nfig.suptitle('Actuval and Predicted', fontsize=18)\nplt.xlabel('Index \/ Observations', fontsize=18)\nplt.ylabel('Stock Price', fontsize=20)\nplt.tight_layout()\nplt.show()","07938eab":"# Plotting the Error terms or Residuals\n\nc = [i for i in range(1, 57462, 1)]\nfig = plt.figure(figsize=(12, 8))\nplt.plot(c, y_test - y_pred, color='blue', linewidth=2.5, linestyle='-')\nfig.suptitle('Error Terms', fontsize=18)\nplt.xlabel('Index \/ Observations', fontsize=18)\nplt.ylabel('y_test - y_pred', fontsize=20)\nplt.tight_layout()\nplt.show()","32615375":"# Plotting y_test and y_pred to understand the spread.\n\nfig = plt.figure(figsize=(12, 8))\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                                 # X-label\nplt.ylabel('y_pred', fontsize=16)                               # Y-label\nplt.tight_layout()\nplt.show()","f49bb61d":"# Plotting the error terms to understand the distribution.\n\nfig = plt.figure(figsize=(12, 8))\nsns.distplot((y_test - y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test - y_pred', fontsize=18)                # X-label\nplt.ylabel('Index', fontsize=16)    \nplt.tight_layout()\nplt.show()","9b1cf963":"# calculating metrics\n\nmse = metrics.mean_squared_error(y_test, y_pred)\nr_squared = metrics.r2_score(y_test, y_pred)\nprint('Mean square error value: ', mse)\nprint('R Square Value: ', r_squared)","b9206378":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.005","001cdc57":"##### Getting the predictions for test dataset","fdacb1f1":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.001","650e2e75":"##### Splitting the dataframe into training and testing dataset.","c5d7eda1":"##### Looking for VIF Scores","575c2c3a":"##### Splitting the data into train and test dataset","f87a2629":"##### Looking VIF Score","4f669ac3":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.100","f5767e34":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.050","ccdbb046":"##### Splitting the data into train and test dataset","d53e559e":"##### Data Imputation on above generated dataset","34c95f27":"##### Encoding Boolean Variables","b86e5fb9":"Creating new Features using Featuretools - Deep Feature Synthesis\n\nNote: A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature.","89605b96":"##### Looking for VIF Score","7e8be85b":"### Feature Engineering and EDA on Prices, Securities and Fundamentals datasets","1be3c526":"##### Looking VIF Scores","1a0ad8be":"##### Splitting the data into train and test dataset","f7dd6bd3":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.025","764109e5":"##### Missing Value Imputation on Securities and Fundamentals dataset","1ab6382b":"##### Splitting the data into train and test dataset","7a0c9f97":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.010","095f235a":"### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.001","8e2cf614":"### Building Linear Regression Model using above generated dataset.","3964bc9b":"### EDA on Prices dataset, Securities dataset and fundamentals dataset","dd1485e2":"##### Meta data of the Prices, securities and fundamentals dataset","09a73581":"##### Looking VIF Score","92092961":"##### Generating new features using Featuretools","aa1488fe":"##### Looking VIF Score","4dd60cff":"##### Splitting the data into train and test dataset","51353a52":"##### Looking for NULL \/ NAN Values in the Prices, Securities and Fundamentals dataset","181c735e":"### Standardizing \/ Normalizing the dataframe using the below method.","f582b436":"##### Splitting the data into train and test dataset","98ec3bb7":"##### Looking for VIF Score","cbd11b73":"##### Encoding all categorical columns into numerical columns","021babb6":"##### Selecting the Best features by using Lasso Regression with regulariztion parameter alpha as 0.500","084ca1f6":"### Splitting the dataframe into indepedent and dependent datasets"}}