{"cell_type":{"44f5a385":"code","5cd4f4f0":"code","0b88df25":"code","65b5463c":"code","dd23c023":"code","fedc60e5":"code","7a79ff36":"code","39ac1a42":"code","c5aac54c":"code","74324a32":"code","60b81f93":"code","fdddd30b":"code","36145fad":"code","b63a6c84":"code","afc90642":"code","31451345":"code","78de8b9b":"code","85867d0c":"code","72bef62b":"code","d7fbe17e":"code","30194af9":"code","3ebb6d87":"code","340d78a8":"code","8b3de560":"code","4cfd56e1":"code","aaa84cf1":"code","d61a2f67":"code","ff6e44c4":"code","96f94c8d":"code","e1ac3d8c":"code","769bb5ab":"code","ca5cb544":"code","d4f25e20":"code","5d94141e":"code","e92d33ce":"code","693ac8ba":"code","84efc8ba":"code","a833bf8d":"code","f21eb05e":"code","71ddc12b":"code","a6fc67e0":"code","0d72d2f9":"code","da99c3e4":"code","5a277468":"code","e1592080":"code","71147ebe":"code","e0877a2a":"code","516834ad":"code","15b417e1":"code","5c6825c9":"markdown","fef84ca5":"markdown","cf18e6f6":"markdown","62f4092b":"markdown","205d9ca0":"markdown","f94c86ce":"markdown","4c955e77":"markdown","42159223":"markdown","de38bd89":"markdown","ccd7323c":"markdown","08395c89":"markdown","166231fd":"markdown","a500db27":"markdown","6a850fcf":"markdown","77c2a2da":"markdown","77d3b104":"markdown","476a94b2":"markdown","26313bda":"markdown","dd6dae36":"markdown","6ec09bfd":"markdown","a07337d3":"markdown","75753d24":"markdown","7d3f7614":"markdown","a170edfd":"markdown","e5473f79":"markdown","9dba7e93":"markdown","857438d8":"markdown"},"source":{"44f5a385":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5cd4f4f0":"d=pd.read_csv(\"..\/input\/data.csv\", encoding='ISO-8859-1')\nd.head()\n","0b88df25":"d.info()","65b5463c":"miss = []\nfor col in d.columns:\n    i=d[col].isnull().sum()\n    miss_v_p = i*100\/d.shape[0]\n    miss.append(miss_v_p)\n    print ('{} -----> {}%'.format(col, 100-i*100\/d.shape[0]))\n\ndico = {'columns': d.columns, 'filling rate': 100-np.array(miss), 'taux nan': miss}\n#print(miss, dico['taux de remplissage'])\ntr=pd.DataFrame(dico)\nr = range(tr.shape[0])\nbarWidth=0.85\nplt.figure(figsize=(20,8))\nplt.bar(r, tr['filling rate'], color='#a3acff', edgecolor='white', width=barWidth)\nplt.bar(r, tr['taux nan'], bottom=tr['filling rate'], color ='#b5ffb9', edgecolor= 'white', width=barWidth)\nplt.title('fill rate representation')\nplt.xticks(r, tr['columns'], rotation='vertical')\nplt.xlabel('columns')\nplt.ylabel('filling rate')\nplt.margins(0.01)","dd23c023":"def count_lab(d, lab='Country'):\n    \"\"\"Compte le nombre d'\u00e9l\u00e9ments identique dans la colonne 'lab'\"\"\"\n    r=d.groupby(lab).count()\n    r['nb']=r.iloc[:,0]\n    r.sort_values(by= 'nb', ascending=False, inplace=True)\n    dico={lab: r.index, 'nb':r['nb']}\n    return pd.DataFrame(dico)","fedc60e5":"label='Description'\ng1=count_lab(d, lab=label).head(30)\n#display(g1)\nplt.figure(figsize=(8,30))\nplt.barh(g1['Description'].apply(str), g1['nb'])\nplt.gca().invert_yaxis()\nplt.axvline(x=2000, color='b')\nplt.text(2082, -1, '>2000', color='b')\nplt.axvline(x=1000, color='r')\nplt.text(780, -1, '<1000', color='r')\nplt.grid(True)\nplt.title('the 30 Most Present Products')","7a79ff36":"desc_price=d.loc[:,['Description', 'UnitPrice']].groupby('Description').mean()\nimport seaborn as sns\n\ndesc_price.boxplot('UnitPrice')\nplt.title('unit price distribution')\nplt.ylim((0,70))","39ac1a42":"print('Rq: Missing Customer ID and Negative Quantities transactions seem to be related to delivery issues:')\nd[(d['CustomerID'].isnull())&(d['Quantity']<0)].sample(8)","c5aac54c":"print(d[d['CustomerID'].isnull()].shape, d.shape, )\nd.dropna(axis=0, subset=['CustomerID'], inplace=True)","74324a32":"#the price of the transaction for a product\nd['TotalPrice']=d['UnitPrice']*d['Quantity']\n\n#The total of the purchase invoice\nInvoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\nInvoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\nd=d.merge(Invoice, on='InvoiceNo')#rq: Here, merge remove any missing values from the pivot category.\n\n# the total amount spent in the DataFrame per customer\ntotal_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\ntotal_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\nd=d.merge(total_y, on='CustomerID')\n\n\nd.sample(5)\n\n","60b81f93":"\"\"\"Fonctions relatives au variables de co\u00fbts\"\"\"\ndef cost_info(d):\n    \"\"\"Add to 'd' informative variables on cost. In case these variables already exist in 'd', they are reset.\"\"\"\n    for col in ['TotalPrice', 'nb_inv', 'TotalYear', 'InvoiceTotal']:\n        try:\n            d.drop(columns=[col], inplace=True)\n            print('Resetting the cost variable {} succeeded'.format(col))\n        except:\n            print('Column {} not present'.format(col))\n    \n    #prix du produit * quantit\u00e9 achet\u00e9e\n    d['TotalPrice']=d['UnitPrice']*d['Quantity']\n    \n    \n    if type(d)==pd.Series:\n        d=pd.DataFrame([d])\n        \n    \n    #total par commande\n    Invoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\n    Invoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\n    d=d.merge(Invoice, on='InvoiceNo')\n        \n    #Total d\u00e9pens\u00e9 par le client\n    total_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\n    total_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\n    d=d.merge(total_y, on='CustomerID')\n        \n    #nombre de commandes sur l'ann\u00e9e\n    invoices=d.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\n    nb_invoices = invoices.groupby('CustomerID').count()\n    nb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n    d=d.merge(nb_invoices, on='CustomerID')\n        \n    \n    return d\n","fdddd30b":"\"\"\"Fonctions relatives au variables de co\u00fbts\"\"\"\ndef cost_info(d):\n    \"\"\"Add to 'd' informative variables on cost. In case these variables already exist in 'd', they are reset.\"\"\"\n    for col in ['TotalPrice', 'nb_inv', 'TotalYear', 'InvoiceTotal']:\n        try:\n            d.drop(columns=[col], inplace=True)\n            print('Resetting the cost variable {} succeeded'.format(col))\n        except:\n            print('Column {} not present'.format(col))\n    \n    #prix du produit * quantit\u00e9 achet\u00e9e\n    d['TotalPrice']=d['UnitPrice']*d['Quantity']\n    \n    \n    if type(d)==pd.Series:\n        d=pd.DataFrame([d])\n        \n    \n    #total par commande\n    Invoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\n    Invoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\n    d=d.merge(Invoice, on='InvoiceNo')\n        \n    #Total d\u00e9pens\u00e9 par le client\n    total_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\n    total_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\n    d=d.merge(total_y, on='CustomerID')\n        \n    #nombre de commandes sur l'ann\u00e9e\n    invoices=d.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\n    nb_invoices = invoices.groupby('CustomerID').count()\n    nb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n    d=d.merge(nb_invoices, on='CustomerID')\n        \n    \n    return d\n","36145fad":"import datetime as dt\nd['InvoiceDate']=pd.to_datetime(d['InvoiceDate'])\nnow=d['InvoiceDate'].max()\nnow","b63a6c84":"d['date']=d['InvoiceDate'].apply(lambda x : x.date())\nd['hour']=d['InvoiceDate'].apply(lambda x : x.time())","afc90642":"d_POS = d[d['Quantity']>0]#To not consider the refunds as transactions","31451345":"invoices=d_POS.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\nnb_invoices = invoices.groupby('CustomerID').count()\nnb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n#display(nb_invoices)\nnb_invoices.sort_values(by='nb_inv', ascending=False, inplace=True)","78de8b9b":"plt.hist(nb_invoices['nb_inv'], bins=200)#plt.plot(nb_invoices, '.')\nplt.title('Number of customers for each number of orders')\nplt.xlim((0,50))","85867d0c":"d=d.merge(nb_invoices, on='CustomerID')\nd.head()","72bef62b":"\nfirst_date = d.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').min().rename(columns={ 'InvoiceDate' : 'first_date'})\nlast_date = d_POS.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').max().rename(columns={ 'InvoiceDate' : 'last_date'})# we use d_POS to not consider refunds\ndisplay(last_date.info())\n\ndate = first_date.merge(last_date, on='CustomerID')\n#display(second_last)\n#display(date)\ndate['since_last']=date['last_date'].apply(lambda x: x.date()-now.date())\ndate['since_first']=date['first_date'].apply(lambda x: x.date()-now.date())\ndate.head(2)","d7fbe17e":"not_unique=date['since_last']-date['since_first']>dt.timedelta(days=0)\n\n\nnot_u=date.loc[not_unique,:]\n\nd_not_u = pd.DataFrame(index=not_u.index)\n#d_not_u['CustomerID']=not_u.index\n\n\n\ninvoice=d[d['Quantity']>0].groupby('InvoiceNo').nth(0)#We do not count refunds\ninvoice['InvoiceNo']=invoice.index\n\n\nd_not_u=d_not_u.merge(invoice, on='CustomerID', how='inner')\nd_not_u=d_not_u.sort_values(['CustomerID','InvoiceDate'], ascending=False)\nsecond_last=d_not_u.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').nth(1)\n\ndate=pd.concat([date, second_last.rename(columns={ 'InvoiceDate' : 'second_last'})], axis=1)\ndate['since_sec_last']=date['second_last'].apply(lambda x: x.date()-now.date())\ndate.head(2)","30194af9":"#Converting the number of days to integers\nfor col in ['since_last', 'since_first', 'since_sec_last']:\n    date[col]=date[col].apply(lambda x: pd.to_timedelta(x).days)\ndate.head()   ","3ebb6d87":"nb_unique = date['second_last'].isnull().sum()\nnb_cust= date.shape[0]\nprint(\"{}% of customers only made one involved during the year, {} customers on {}\".format(round(nb_unique*100\/nb_cust,2),\n                                                                                                               nb_unique,\n                                                                                                               nb_cust\n                                                                                                              ))","340d78a8":"nb_t_p=d_POS['Description'].nunique()\nprint('total number of products : ',nb_t_p)","8b3de560":"#quantity of each product per customers\nquantity = d_POS.loc[:,['CustomerID', 'Description', 'Quantity']].groupby(['CustomerID', 'Description']).sum()\ncol_ind = quantity.index.get_level_values(0)\nquantity.reset_index(level=[0,1], inplace=True)\n\n# Let add the Unit price per product\nUnitPrice=d_POS.loc[:, ['Description', 'UnitPrice']].groupby('Description').mean()\nquant_price = quantity.merge(UnitPrice, on='Description')\n\ndisplay(quant_price.head())\n\n","4cfd56e1":"\"\"\"Becouse we focus on positive quantity, we must used the total monetary value per customer ('TotalYear') only on positive quantity : \nRefunds can skew our entropy, so we have to recalculate TotalYear on positive quantity only\"\"\"\nfor c in quantity['CustomerID'].unique():\n    \n    c_quanti = quant_price[quant_price['CustomerID']==c]\n    date.loc[c,'quantity_t']=c_quanti.loc[:,'Quantity'].sum()\n    date.loc[c,'nb_prod_diff']=c_quanti.shape[0]\n    \"\"\"cancellations can skew our entropy, so we have to recalculate TotalYear\"\"\"\n    moyT = (c_quanti['Quantity']*c_quanti['UnitPrice']).sum()\/date.loc[c,'nb_prod_diff']\n    \n    date.loc[c, 'entropy_corrige']=np.sqrt(np.square(np.log(1+c_quanti['Quantity']*c_quanti['UnitPrice'])-np.log(1+moyT)).sum()\/date.loc[c,'nb_prod_diff'])\n","aaa84cf1":"date['ind_div']=((date['nb_prod_diff'])\/date['quantity_t'])","d61a2f67":"date.head()","ff6e44c4":"def entropie_prod(d):\n    d_POS=d[d['Quantity']>0]#Here we avoid the case where customer have a negative total quantity.\n    \n    #quantity of each product per customers\n    quantity = d_POS.loc[:,['CustomerID', 'Description', 'Quantity']].groupby(['CustomerID', 'Description']).sum()\n    col_ind = quantity.index.get_level_values(0)\n    quantity.reset_index(level=[0,1], inplace=True)\n    \n    # Let add the Unit price per product\n    UnitPrice=d_POS.loc[:, ['Description', 'UnitPrice']].groupby('Description').mean()\n    quant_price = quantity.merge(UnitPrice, on='Description')\n    \n    # the DataFrame we 'll return\n    date=pd.DataFrame(index=col_ind.unique(), columns=['quantity_t'])\n    \n    #Entropy Produit\n    \"\"\"Becouse we focus on positive quantity, we must used the total monetary value per customer ('TotalYear') only on positive quantity : \nRefunds can skew our entropy, so we have to recalculate TotalYear on positive quantity only\"\"\"\n    for c in quantity['CustomerID'].unique():\n    \n        c_quanti = quant_price[quant_price['CustomerID']==c]\n        date.loc[c,'quantity_t']=c_quanti.loc[:,'Quantity'].sum()\n        date.loc[c,'nb_prod_diff']=c_quanti.shape[0]\n        \n        \n        \"\"\"cancellations can skew our entropy, so we have to recalculate TotalYear\"\"\"\n        moyT = (c_quanti['Quantity']*c_quanti['UnitPrice']).sum()\/date.loc[c,'nb_prod_diff']\n    \n        date.loc[c, 'entropy_corrige']=np.sqrt(np.square(np.log(1+c_quanti['Quantity']*c_quanti['UnitPrice'])-np.log(1+moyT)).sum()\/date.loc[c,'nb_prod_diff'])\n    \n    #Diversity index\n    date['ind_div']=((date['nb_prod_diff'])\/date['quantity_t'])\n    \n    return date","96f94c8d":"from collections import Counter\nimport nltk","e1ac3d8c":"d_POS.head() #data restrict to positive quantity (not a refund or cancellation...)","769bb5ab":"\ndef weights_words(d, label='Description', sep=\" \", nb=None, min_occ=None):\n    \"\"\"\n    Associate with each word present in the description of a product, a score that corresponds to the money spent by the customer in this product (quantity purchased * Unit price).\n\u00a0\u00a0\u00a0\u00a0 Rq: In practice d will be the DataFrame for a client only.\n     \n    Associe \u00e0 chaque mots presents dans la description d'un produit, un score qui correspond \u00e0 l'argent d\u00e9penser du client dans ce produit ( quantit\u00e9 achet\u00e9 * prix Unitaire).\n    Rq: En pratique d sera le DataFrame pour un client uniquement.\n    \"\"\"\n    #count=dict()\n    words=[]\n    for ind in d.index:\n        #price=d.loc[ind, 'TotalPrice']\n        #if\n        words+=str(d.loc[ind, label]).split(sep)*int(d.loc[ind, 'TotalPrice']*10)#le rapport sera arrondie \u00e0 10 centimes pr\u00e8s\n        #else:\n        #for w_neg in str(d.loc[ind, label]).split(sep)*int((-price)*10):\n                #words=words.remove(w_neg)\n       \n    count=Counter(words)        \n    if nb==None:\n        if min_occ==None:\n            return count\n        else:\n            c_words=pd.Series(count)\n            rare = c_words[c_words<min_occ].index\n            c_words.drop(index=rare, inplace=True)\n            return dict(c_words)\n    else:\n        return(dict(count.most_common(nb)))","ca5cb544":"def df_cust_them(d):\n    \"\"\"\n    Returns a Dataframe that each client associates a proportion for each keyword.\n\u00a0\u00a0\u00a0\u00a0 This proportion represents the presence of the word on the customer's market share:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Each product has a description in which keywords are extracted.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 These words are then weighted by the (price of the product) * (quantity purchased) \/ (total of the customer's expenses)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 I thus obtain a score between 0 and 1. which corresponds to the basis of the proportion of purchases of a product for the customer.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 As the products are broken down into words, I implicitly add up the scores (proportions) of the words that\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 are found in several products.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 We have an indication of the interest (a posteriori) converted from the customer.\n\u00a0\u00a0\u00a0\u00a0 Rq: The hypothetical case where score> 1 is theoretically possible if the word appears several times in the description of the same product\n    \n    \n    Retourne un Dataframe qui a chaque client associe une proportion pour chaque mots clefs.\n    Cette proportion repr\u00e9sente la presence du mot sur la part de march\u00e9 du client:\n        A chaque produit correspond une description dans laquelle on extrait des mots clefs. \n        Ces mots sont ensuite pond\u00e9r\u00e9s par le (prix du produit)*(quantit\u00e9 achet\u00e9e)\/(total des d\u00e9penses du client)\n        J'obtiens donc un score entre 0 et 1. qui correspond \u00e0 la base aux proportions d'achats d'un produit pour le client.\n        Les produits \u00e9tant d\u00e9compos\u00e9s en mots, j'additionne implicitement les scores (proportions) des mot qui \n        se retrouvent dans plusieurs produit.\n        On a une indication de l'interet (\u00e0 posteriori) converti du client.\n    Rq: Le cas hypoth\u00e9tique o\u00f9 score >1 est th\u00e9oriquement possible ssi le mot apparait plusieur fois dans la description d'un meme produit.\n    \"\"\"\n    dt=d[d['Quantity']>0]\n    print('We keep positive quantities : ')\n    dt=cost_info(dt)\n    cust_them_dft = pd.DataFrame(index=dt['CustomerID'].unique())\n    \n    #Associate for each customer an affinity score on the most present words in the product description\n    #Associe pour chaque client un score d'affinit\u00e9 sur les mots les plus pr\u00e9sent dans la description des produits.\n    for c in dt['CustomerID'].unique():\n        d_cust = dt[dt['CustomerID']==c]\n        y_total=d_cust.loc[:, 'TotalYear'].mean()\n        \n        l=weights_words(d_cust, min_occ=2)#None\n        \n        \n        for lab in l.keys():\n            cust_them_dft.loc[c, lab]=l[lab]\n        cust_them_dft.loc[c, :]=cust_them_dft.loc[c, :]\/(y_total*10)\n        # Here I divide by the total quantities purchased from the customer\n        #rq: The factor 10 has been added to get an accuracy of 10 cents.\n        #Indeed the prices have been converted in integer * 10\n    return cust_them_dft\n\n\ndef cust_them_clean(cust_them, w_all_min=16, affiner=True, keep_max=False):\n    \"\"\"Cleans the DataFrame 'cust_them' by removing unnecessary words (columns).\n\u00a0\u00a0\u00a0\u00a0 it also removes columns with a total weight less than 'w_all_min'.\n    \"\"\"\n    try:\n        cust_them.drop(columns=[''], inplace=True)\n    except:\n        print(\"Column '' is not in the DataFrame\")\n        \n    #deletes words that are not nouns\n    is_not_noun = lambda pos: pos[:2]!='NN'\n    not_noun=[]\n    cust_them.columns = map(str.lower, cust_them.columns)\n    l_w_pos = nltk.pos_tag(cust_them.columns)\n    not_noun=[w for (w, pos) in l_w_pos if is_not_noun(pos)]    \n    \n    if affiner:\n        #list of arbitrary words to add:\n        l_words_int = ['photo' , 'girl', 'ceramic', 't-shirt', \n                   'origami', 'xmas', 'garden', 'gift', 'lantern', 'paint', 'marmalade', \n                   'poncho', 'bonbon', 'ivy', 'guitar', 'laser', 'boys', 'halloween', 'cloth', 't-light', 'baby', 'doormat']\n\n    for a in l_words_int:\n        try:\n        \n            not_noun.remove(a)\n            \n        except:\n            print(a, 'n is not in the list')\n    #List of words to remove (This corresponds to a retro active setting of words creating clusters when they are not significant enough (in the interpretation of the generated cluster)\n    if keep_max==False:\n        l_w_not_int=[]#'heart', 'retrospot', 'metal', 'design', 'holder', 'polkadot', 'regency','vintage',\n        for r in l_w_not_int:\n            not_noun.append(r)\n    #commit:\n    try:\n        cust_them = cust_them.drop(columns=not_noun)\n    except:\n        print('word not remove')\n    \n    cust_them.fillna(0, inplace=True)\n    \n    #Nous retirons les mots les moins pr\u00e9sent:\n    w_importance = cust_them.sum().sort_values(ascending=False)\n    w_importance = w_importance[w_importance>w_all_min]\n    cust_them=cust_them.loc[:, w_importance.index]\n    \n    return cust_them\n\n\ndef for_cust_them(d, w_all_min=16, keep_max=False):\n    them = df_cust_them(d)\n    them_clean = cust_them_clean(them, w_all_min=w_all_min)\n    return them_clean","d4f25e20":"%%time\ncust_them = for_cust_them(d)","5d94141e":"from sklearn import cluster, metrics","e92d33ce":"nb_clu = [i for i in range(3, 13)]\nsilh=[]\nfor n in nb_clu:\n    km_init = cluster.KMeans(n_clusters=n, random_state=0)\n    km_init.fit(cust_them)\n    s=metrics.silhouette_score(cust_them, km_init.labels_)\n    silh.append(s)\n    print('OK',n)\n    \nplt.plot(range(3, 13), silh)\nprint(silh.index(max(silh))+3, max(silh))\nprint('7 clu :',silh[7-3])","693ac8ba":"\nn=8\nkm_init = cluster.KMeans(n_clusters=n, random_state=0)#4ou7 rds 6 8clu ou 10 clusters\nkm_init.fit(cust_them)\n\nclusKM = pd.DataFrame(km_init.labels_, cust_them.index, columns=['km_them_t'])\n#display(clusKM)\nthem_clu=pd.concat([clusKM, cust_them], axis=1)\n#clusKM['CustomerID']=cust_them.index\n#d=d.merge(clusKM, on='CustomerID')\n\nthem_clu.head()","84efc8ba":"dico_cl = dict()\nfor k in range(n):\n    print('cluster ',k)\n    temp=them_clu[them_clu['km_them_t']==k].describe().T\n    temp=temp.iloc[1:, :]\n    temp=temp[temp['50%']>0]\n    display(temp)\n    dico_cl[k]=temp\n    l1 = plt.plot(temp['mean'])\n    l2 = plt.plot(temp['50%'])\n    l3 = plt.plot(temp['25%'])\n    plt.legend(['mean','median', '25%'])\n    plt.xticks(rotation=40)\n    #plt.legend((l1, l2, l3), ('mean', 'median', '25%'))\n    plt.show()","a833bf8d":"\"\"\"Let name these Clusters\"\"\"\n#dico_them={6:'MetalSign', 1: 't-Light', 3:'Thermos', 2:'Bags', 0:'ChismasTime', 5:'Unspecified', 4:'RegencyTea', 7:'Doormat'}\ndico_them={0:'RegencyTea', 1:'MetalSign', 2:'Unspecified', 3:'thermos', 4:'t-Light', 5:'Bags', 6:'HeartDeco', 7:'ChrismasVintage'}","f21eb05e":"from sklearn import decomposition","71ddc12b":"#n = nb of clusters\nnmf=decomposition.NMF(n_components=n, random_state=0)\nw=nmf.fit_transform(cust_them)\nw_clean = pd.DataFrame(w, index=cust_them.index)","a6fc67e0":"ordered_c=pd.DataFrame(km_init.predict(cust_them), index=cust_them.index, columns=['km_them'])\n#display(ordered_c)\nref_nmf = pd.concat([ordered_c ,w_clean], axis=1, sort=True)\nref_nmf.head(1)","0d72d2f9":"ref_nmf['legend']=ref_nmf['km_them'].apply(lambda x: dico_them[x])\n\nimport seaborn as sns\nfor i in range(7):\n    plt.figure(figsize=(20,20))\n    \n    sns.pairplot(ref_nmf.loc[:,['km_them', 'legend', i, i+1]] , hue='legend', \n                 palette=sns.color_palette())  #\"husl\", 8             \n    plt.title('plan factoriel {} et {}'.format(i+1, i+2))    \n    plt.xlim((-0.001,0.5))\n    plt.ylim((-0.001,0.5))\n    plt.show()\n    ","da99c3e4":"l=['HeartDeco', 'Bags', 'ChrismasVintage', 'Thermos', 'Unspecified', 't-Light', 'MetalSign', 'RegencyTea']\n#l=['Unspecified','t-Light', 'Bags', 'Thermos','RegencyTea', 'MetalSign','ChismasTime','DoormatChismas']\nfor i in range(8):\n    ref_nmf.rename(columns={i : l[i]},inplace=True)\nref_nmf.head()   ","5a277468":"d_clu = pd.concat([date.loc[:, ['entropy_corrige','ind_div']], ref_nmf.loc[:,l]], axis=1, sort=True)#'quantity_t','nb_prod_diff',","e1592080":"d_clu.head()","71147ebe":"from sklearn.preprocessing import StandardScaler\nscale_v=StandardScaler()\nd_clu.loc[:,:]=scale_v.fit_transform(d_clu.loc[:,:])\n","e0877a2a":"print(d_clu.shape)\nnb_clu = [i for i in range(3, 23)]\nsilh=[]\nfor n in nb_clu:\n    km_gen= cluster.KMeans(n_clusters=n, random_state=1)\n    km_gen.fit(d_clu)\n    s=metrics.silhouette_score(d_clu, km_gen.labels_)\n    silh.append(s)\n    #print('OK pour ',n)\n    \nplt.plot(nb_clu, silh)\nprint(silh.index(max(silh))+3, 'silhouette :', max(silh))","516834ad":"n_cl=14\nkm_gen = cluster.KMeans(n_clusters=n_cl, random_state=1)#4ou6\nkm_gen.fit(d_clu)\n\n#clusKM = pd.DataFrame(np.array([km_gen.labels_, d_clu.index]).T, columns=['categ', 'CustomerID'])\n#them_clu=clusKM.merge(d_clu, on='CustomerID')\n\nclusKM = pd.DataFrame(km_gen.labels_, index=d_clu.index, columns=['categ'])\nthem_clu=pd.concat([clusKM, d_clu], axis=1, sort=True)\n#clusKM['CustomerID']=cust_them.index\n#d=d.merge(clusKM, on='CustomerID')\n\nthem_clu.head()","15b417e1":"for k in range(n_cl):\n    print('cluster No.',k)\n    temp=them_clu[them_clu['categ']==k].describe().T\n    #temp.drop('CustomerID', inplace=True)\n    temp=temp.iloc[1:, :]\n    #temp=temp[temp['50%']>0]\n    display(temp)\n    plt.figure(figsize=(10,4))\n    l1 = plt.plot(temp['mean'])\n    l2 = plt.plot(temp['50%'])\n    l3 = plt.plot(temp['25%'])\n    l4 = plt.plot(temp['75%'])\n    plt.axhline(y=0, c='black', ls='--')\n    plt.xticks(rotation=40)\n    plt.title('No.{} (nb = {})'.format(k, temp.iloc[0,0]))\n    plt.legend(['mean', 'median', '25%', '75%'])\n    plt.show()","5c6825c9":"# 3 Thematic Clustering\n## 3.1 Vectorization:\n### New Dataframe'cust_them' : Distribution of words on the customer's market share\nHere, to simplify word processing, we will restrict the data to positive quantities.\n\n","fef84ca5":"# 1 Preview and Cleaning","cf18e6f6":"Let choose 8 clusters :","62f4092b":"## 2.1.2 DateTime variables:\n","205d9ca0":" ## 2.2 New DataFrame : 'Date' which characterizing the customers\n ### 2.2.1 Time data:\n#### First acquisition, last acquisition","f94c86ce":"\n## Variable 'nb_inv': the number of transactions per customer","4c955e77":"**Acknowledgement:** Many thank to Fabien Daniel for his advices and encouragements.","42159223":"## Cleaning","de38bd89":"We will be particularly interested in **product** types","ccd7323c":"## 3.2 First approach with Kmeans:","08395c89":"## 2.2.1 Customer diversity index 'ind_div' and entropy 'product_entropy'\nWe create these two features to better characterize customer behavior","166231fd":"This clutsering is a first step to dinsting thematics. But it's not enough. In add, one cluster (no.2) covers most of the data.<br>\nLet try to understand the Kmeans impact with visualisation with dimensions reduced:\n## 3.3 Cluster representation and switch to decomposition NMF:\nwe'll see that the decomposition NMF give very interesting results... \n","a500db27":"# 2 Feature engineering\nIn this part, we will create some interesting features to characterize customers. In thematic clustering, we will use only 'entropy' and 'diversity index'. Other features can be use for RFM segmentation.\n## 2.1 On the Dataset : ","6a850fcf":"With the gaphs aboves, we can rename our factorials plans :","77c2a2da":"#### Above, Our clusters are fitted on the decomposition NMF. On each factor plane, a cluster is distinguished by high values. Only plan number 4 (the 5th factorial plan) does not correspond to a specific cluster.","77d3b104":"Products are mainly cheap.","476a94b2":"****Let us introduce 'quant_price' : the DataFrame of the quantity of each product per customers:","26313bda":"\n** - Entropy Products ** identifies the distribution of products purchased by the customer. Thus, it is close to zero if this distribution is uniform and increases as much as the probability of having a non-uniform distribution is large.<br>\n** note 1 **: We could define entropy products only on ** product quantities **, but it is ** better to weight by the price of the product.** Indeed a customer will tend to buy in larger quantities cheap products than expensive products. (That is why we add UnitPrice in our DataFrame 'quant_price')<br>\n![](http:\/\/)** note 2 **: The formula of entropy product is inspired by the RMSLE.","dd6dae36":"![](http:\/\/)** - The Diversity Index ** mesur the diversity of product bought by one custumer therfore, identifies customers who are still buying the same products from those who buy a lot of different products. <br>\nFor tha same number of bought product, a customer who buys a lot of diffents products has a higher diversity index than a customer who would buy only one item. \n** note **: The formula of diversity index is simply the ratio : <br>  \n(the number of differents products purchased by the customer) \/ (the total number of products purchased by that consumer).","6ec09bfd":"### Automation function of the above code :","a07337d3":"Let scale the data to perform a clustering:","75753d24":"\n##### Automation function of the code above :","7d3f7614":"**Finaly, the NMF decomposition offers a better characterization of customer themes.** Now, customers have a score in each theme discovered. \n## 3.4 Final thematic clustering\nTo better characterize their relationship to themes, we will reinject our indices of diversity and entropy product.","a170edfd":"### Penultimate acquisition:","e5473f79":"We have 8 clusters. <br> * note: Taking into account the ** proportion of words on the budget invested per customer ** (rather than the quantity only), allowed to reveal a new category ('bottle' 'water ',' tea '..) which seems to correspond to rather expensive products (Thermos, tea service ...) *\n<br>\n** Rq **: The graphs above are easily interpretable: <br>\nThey correspond to the ** ownership of a word on the customer's market share **. <br>\n** For example **: In the ** cluster 3 **, 75% of customers have the words 'bottle' present on more than 25% of their market share. (similarly for the word 'water' etc ...) <br>\nOn the other hand, on average, these words (bottle and water) are each present in 41% of the market share of cluster customers.\nA little check seems to indicate that the thematic interest of these customers would be ** hot drink utensils ** ('thermos', 'hot chocolates', 'tea service' ...)","9dba7e93":"We have reduce the size of the main cluster 'Unspecific' (cluster no.5)<br>\nNow, we can identify customers who have specific affinities with themes we didn't discorer (cluster no.7)<br>\nWe can also notice the correlation of themes among the clusters. For example, in cluster no.8, customers love home decorations, but they prefer hearts rather than regency style.<br>\n<br>\nThis completes the thematic clustering. <br>\n<br>\n\n We can use RFM segmentation to get more information about customer behavior (Frequency, Recency, Monetary Value). (The feature engineering is already done)<br>\nAnd finally, we can train a classifier to predict the customer behavior.\n### to be continued...","857438d8":"I choose n_components = number of cluster"}}