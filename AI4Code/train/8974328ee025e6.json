{"cell_type":{"f0cbfddf":"code","227f2d15":"code","f36e1530":"code","63cfe3fe":"code","6fc78c12":"code","fb6dd909":"code","0cbe2b32":"code","d5f98f5f":"code","58fadcc0":"code","ebf5c155":"code","d5419772":"code","e41d0a1c":"code","e73d793f":"code","291659e1":"code","9c9fd5f5":"code","589908b1":"code","23cac6bd":"code","3f20cebd":"code","beb770b7":"code","aa6a3d1f":"code","0c71f632":"code","15b2e37e":"code","b0feade9":"code","a865aa84":"code","dababeac":"code","cac79285":"code","544efd4f":"code","2a30c8ac":"code","1a178c01":"code","c4fec718":"code","6b647fe3":"code","143eed86":"code","f41a8174":"code","7af7038f":"code","428635f7":"code","bdf2c0ae":"code","663fe18b":"code","6902ba27":"code","faf5f715":"code","bd3b6b90":"code","1ddbc4a9":"code","46f7157d":"code","6fd1e2a0":"code","360b7781":"code","872647ff":"code","d68ca40a":"code","5cde6e58":"code","89b9f739":"markdown"},"source":{"f0cbfddf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","227f2d15":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(train.shape)\nprint(test.shape)","f36e1530":"df = train.append(test).reset_index(drop=True)\nprint(df.shape)","63cfe3fe":"df.columns","6fc78c12":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","fb6dd909":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","0cbe2b32":"df[\"Neighborhood\"].value_counts()","d5f98f5f":"# Kategorik De\u011fi\u015fken Analizi\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\nfor col in cat_cols:\n    cat_summary(df, col)","58fadcc0":"for col in cat_but_car:\n    cat_summary(df, col)","ebf5c155":"# Say\u0131sal De\u011fi\u015fken Analizi\ndf[num_cols].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.99]).T","d5419772":"#Target Analizi\ndf[\"SalePrice\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T","e41d0a1c":"def find_correlation(dataframe, numeric_cols, corr_limit=0.60):\n    high_correlations = []\n    low_correlations = []\n    for col in numeric_cols:\n        if col == \"SalePrice\":\n            pass\n        else:\n            correlation = dataframe[[col, \"SalePrice\"]].corr().loc[col, \"SalePrice\"]\n            print(col, correlation)\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \": \" + str(correlation))\n            else:\n                low_correlations.append(col + \": \" + str(correlation))\n    return low_correlations, high_correlations\n\nlow_corrs, high_corrs = find_correlation(df, num_cols)","e73d793f":"# t\u00fcm de\u011fi\u015fkenler korelasyon\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot = True, figsize=(20,15), fmt=\".2f\" )\nplt.title(\"Correlation Between Features\")\nplt.show()","291659e1":"threshold = 0.60\nfilter = np.abs(corr_matrix[\"SalePrice\"]) > threshold\ncorr_features = corr_matrix.columns[filter].tolist()\nsns.clustermap(df[corr_features].corr(), annot = True, fmt = \".2f\")\nplt.title(\"Correlation Between Features w\/ Corr Threshold 0.60)\")\nplt.show()","9c9fd5f5":"def high_correlated_cols(dataframe, plot=False, corr_th=0.60):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\nhigh_correlated_cols(df)","589908b1":"# FEATURE ENGINEERING\n\ndf[\"SqFtPerRoom\"] = df[\"GrLivArea\"] \/ (df[\"TotRmsAbvGrd\"] +\n                                                       df[\"FullBath\"] +\n                                                       df[\"HalfBath\"] +\n                                                       df[\"KitchenAbvGr\"])\n\ndf['Total_Home_Quality'] = df['OverallQual'] + df['OverallCond']\n\ndf['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                                 df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\ndf[\"HighQualSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]","23cac6bd":"# Converting non-numeric predictors stored as numbers into string\n\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].apply(str)\ndf['MoSold'] = df['MoSold'].apply(str)","3f20cebd":"# RARE ENCODING\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) < 0.01).sum()>1]\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\nrare_analyser(df, \"SalePrice\", cat_cols)","beb770b7":"df = rare_encoder(df, 0.01, cat_cols)\n\ndrop_list = [\"Street\", \"SaleCondition\", \"Functional\", \"Condition2\", \"Utilities\", \"SaleType\", \"MiscVal\",\n             \"Alley\", \"LandSlope\", \"PoolQC\", \"MiscFeature\", \"Electrical\", \"Fence\", \"RoofStyle\", \"RoofMatl\",\n             \"FireplaceQu\"]\n\ncat_cols = [col for col in cat_cols if col not in drop_list]\n\nfor col in drop_list:\n    df.drop(col, axis=1, inplace=True)\n\nrare_analyser(df, \"SalePrice\", cat_cols)","aa6a3d1f":"useless_cols = [col for col in cat_cols if df[col].nunique() == 1 or\n                (df[col].nunique() == 2 and (df[col].value_counts() \/ len(df) <= 0.01).any(axis=None))]\n\ncat_cols = [col for col in cat_cols if col not in useless_cols]\n\n\nfor col in useless_cols:\n    df.drop(col, axis=1, inplace=True)\n\nrare_analyser(df, \"SalePrice\", cat_cols)","0c71f632":"# Label Encoding & ONE-HOT ENCODING\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\ncat_cols = cat_cols + cat_but_car\ndf = one_hot_encoder(df, cat_cols, drop_first=True)\n\ncheck_df(df)","15b2e37e":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nrare_analyser(df, \"SalePrice\", cat_cols)\n\nuseless_cols_new = [col for col in cat_cols if (df[col].value_counts() \/ len(df) <= 0.01).any(axis=None)]\n\ndf[useless_cols_new].head()\n\nfor col in useless_cols_new:\n    cat_summary(df, col)\n\nrare_analyser(df, \"SalePrice\", useless_cols_new)","b0feade9":"# Missing Values\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)","a865aa84":"test.shape","dababeac":"missing_values_table(train)","cac79285":"na_cols = [col for col in df.columns if df[col].isnull().sum() > 0 and \"SalePrice\" not in col]\n\ndf[na_cols] = df[na_cols].apply(lambda x: x.fillna(x.median()), axis=0)","544efd4f":"# Outliers\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name, q1=0.25, q3=0.75):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    print(col, check_outlier(df, col, q1=0.01, q3=0.99))","2a30c8ac":"# Model\n\ndf.shape","1a178c01":"train_df = df[df['SalePrice'].notnull()]\ntest_df = df[df['SalePrice'].isnull()].drop(\"SalePrice\", axis=1)\n\ntrain_df.shape","c4fec718":"test_df.shape","6b647fe3":"y = np.log1p(train_df['SalePrice'])\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n\nX.shape","143eed86":"# Base Models\n##################\n\nmodels = [('LR', LinearRegression()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor())]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=3, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","f41a8174":"lgbm_model = LGBMRegressor(random_state=46)","7af7038f":"# modelleme \u00f6ncesi hata:\nrmse = np.mean(np.sqrt(-cross_val_score(lgbm_model,\n                                        X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nrmse","428635f7":"lgbm_params = {\"learning_rate\": [0.01, 0.005],\n               \"n_estimators\": [15000, 20000],\n               \"colsample_bytree\": [0.5, 0.3]}","bdf2c0ae":"lgbm_gs_best = GridSearchCV(lgbm_model,\n                            lgbm_params,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=False).fit(X, y)\n\nfinal_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X, y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(rmse)","663fe18b":"#hiperparametrelerin default kendi de\u011feriyle rmse 0.1305858 idi.\n#optimizasyonlarla 0.12328 e indirdik","6902ba27":"# Feature Selection\n\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(final_model, X, 20)","faf5f715":"X.shape","bd3b6b90":"feature_imp = pd.DataFrame({'Value': final_model.feature_importances_, 'Feature': X.columns})\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\nnum_summary(feature_imp, \"Value\", True)","1ddbc4a9":"feature_imp[feature_imp[\"Value\"] > 0].shape\n\nfeature_imp[feature_imp[\"Value\"] < 1].shape\n\nzero_imp_cols = feature_imp[feature_imp[\"Value\"] < 1][\"Feature\"].values\n\nselected_cols = [col for col in X.columns if col not in zero_imp_cols]","46f7157d":"# Hyperparameter Optimization with Selected Features\n\nlgbm_model = LGBMRegressor(random_state=46)\n\nlgbm_params = {\"learning_rate\": [0.01, 0.005],\n               \"n_estimators\": [15000, 20000],\n               \"colsample_bytree\": [0.5, 0.3]}\n\nlgbm_gs_best = GridSearchCV(lgbm_model,\n                            lgbm_params,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=True).fit(X[selected_cols], y)","6fd1e2a0":"y = np.log1p(train_df['SalePrice'])\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n\nfinal_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X[selected_cols], y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(final_model, X[selected_cols], y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(rmse)","360b7781":"# SONUCLARIN YUKLENMESI\n#######################################\n\nsubmission_df = pd.DataFrame()\n\nsubmission_df['Id'] = test_df[\"Id\"].astype(\"Int32\")\nsubmission_df.head()","872647ff":"y_pred_sub = final_model.predict(test_df[selected_cols])\ntest_df.head()","d68ca40a":"y_pred_sub = np.expm1(y_pred_sub)\n\nsubmission_df['SalePrice'] = y_pred_sub\n\nsubmission_df.to_csv('submission.csv', index=False)","5cde6e58":"submission_df","89b9f739":"**Hyperparameter Optimization**"}}