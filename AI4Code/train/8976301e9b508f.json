{"cell_type":{"a8b6afa0":"code","de80c232":"code","be14e900":"code","850ddfc3":"code","632b502f":"code","bc9b93e6":"code","de9af679":"code","afbb1731":"code","8ec0eeeb":"code","409cbe36":"code","36b6890f":"code","752ff142":"code","29d028c8":"code","87533871":"code","63e69e3e":"code","9cdf82e8":"code","dbe9911a":"code","e8ed314a":"code","fe237dce":"code","5ee56359":"code","6c4e8382":"code","4713f373":"code","53aaff65":"code","f36d9e31":"code","5b7004b0":"code","5098f48f":"code","925f9df4":"code","90ed9979":"code","e1404a69":"code","1462037e":"markdown","15bdfdeb":"markdown","9e9a62fa":"markdown","fd72b26e":"markdown","c3a485df":"markdown","f2d8cfc9":"markdown","2d4b1cdf":"markdown","5a2240aa":"markdown","9f0a6fda":"markdown","951e30b4":"markdown","6303db79":"markdown"},"source":{"a8b6afa0":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\niris = datasets.load_iris()  #Load dataset iris","de80c232":"X = iris.data[:, :2]\ny = (iris.target != 0) * 1","be14e900":"plt.figure(figsize=(4, 3))\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\nplt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\nplt.legend()","850ddfc3":"class myLogisticRegression:\n    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n        self.lr = lr  # learning rate\n        self.num_iter = num_iter\n        self.fit_intercept = fit_intercept\n        self.verbose = verbose\n    \n    def __add_intercept(self, X):\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n    \n    def __sigmoid(self, z):\n        return 1 \/ (1 + np.exp(-z))\n    \n    def __loss(self, h, y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n    \n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)\n        \n        #weights initialization\n        self.weight = np.zeros(X.shape[1])\n        \n        for i in range(self.num_iter):\n            #forward propagation\n            z = np.dot(X, self.weight)\n            h = self.__sigmoid(z)\n            \n            #calculate the gradient from h-y\n            gradient = np.dot(X.T, (h - y)) \/ y.size\n            \n            #update weights using gradient and learning rate\n            self.weight -= self.lr * gradient\n            \n            #update the prediction h, and calculate the loss with latest weights\n            z = np.dot(X, self.weight)\n            h = self.__sigmoid(z)\n            loss = self.__loss(h, y)\n            \n            #if verbose is True, display training info    \n            if(self.verbose ==True and i % 10000 == 0):\n                print(f'loss: {loss} at {i} iteration\\t')\n                \n    def predict_prob(self, X):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)    \n        return self.__sigmoid(np.dot(X, self.weight))\n    \n    def predict(self, X):\n        return self.predict_prob(X).round()","632b502f":"model = myLogisticRegression(lr=0.1, num_iter=300000, fit_intercept=False, verbose=True)","bc9b93e6":"%time history = model.fit(X, y)\nmodel.weight","de9af679":"preds = model.predict(X)\n(preds == y).mean()","afbb1731":"plt.figure(figsize=(10, 6))\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\nplt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\nplt.legend()\nx1_min, x1_max = X[:,0].min(), X[:,0].max()\nx2_min, x2_max = X[:,1].min(), X[:,1].max()\n#xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\nxx1, xx2 = np.meshgrid(np.linspace(0, x1_max), np.linspace(0, x2_max))\ngrid = np.c_[xx1.ravel(), xx2.ravel()]\nprobs = model.predict_prob(grid).reshape(xx1.shape)\nplt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black')","8ec0eeeb":"model_2 = myLogisticRegression(lr=0.1, num_iter=300000, fit_intercept=True, verbose=True)","409cbe36":"%time model_2.fit(X, y)\nmodel_2.weight","36b6890f":"preds = model_2.predict(X)\n(preds == y).mean()","752ff142":"plt.figure(figsize=(10, 6))\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\nplt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\nplt.legend()\nx1_min, x1_max = X[:,0].min(), X[:,0].max()\nx2_min, x2_max = X[:,1].min(), X[:,1].max()\n#xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\nxx1, xx2 = np.meshgrid(np.linspace(0, x1_max), np.linspace(0, x2_max))\ngrid = np.c_[xx1.ravel(), xx2.ravel()]\nprobs = model_2.predict_prob(grid).reshape(xx1.shape)\nplt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black')","29d028c8":"import seaborn as sns\nimport pandas as pd\nimport sys\nimport os\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\n\ncol=['sepal_length','sepal_width','petal_length','petal_width','type']\n\n# convert a Scikit-learn dataset to a Pandas dataset\niris=datasets.load_iris()\n\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)","87533871":"y = (iris.target != 0) * 1\ndf['type'] = y\n\ndf_name=df.columns\n\ndf.head()","63e69e3e":"df.describe()","9cdf82e8":"g = sns.pairplot(df, hue=\"type\", palette=\"husl\")","dbe9911a":"def plotHist(df,nameOfFeature):\n    cls_train = df[nameOfFeature]\n    data_array = cls_train\n    hist_data = np.histogram(data_array)\n    binsize = .5\n    \n    '''\n\n    trace1 = go.Histogram(\n        x=data_array,\n        histnorm='count',\n        name=nameOfFeature,\n        autobinx=False,\n        xbins=dict(\n            start=df[nameOfFeature].min()-1,\n            end=df[nameOfFeature].max()+1,\n            size=binsize\n        )\n    )\n    '''\n    trace1 = go.Histogram(\n        x = data_array,\n        name = nameOfFeature,\n        autobinx = False)\n\n    trace_data = [trace1]\n    layout = go.Layout(\n        bargroupgap=0.3,\n         title='The distribution of ' + nameOfFeature,\n        xaxis=dict(\n            title=nameOfFeature,\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        ),\n        yaxis=dict(\n            title='Number of labels',\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        )\n    )\n    fig = go.Figure(data=trace_data, layout=layout)\n    py.iplot(fig)","e8ed314a":"plotHist(df,'sepal length (cm)')","fe237dce":"plotHist(df,'sepal width (cm)')","5ee56359":"from scipy.stats import skew\nfrom scipy.stats import kurtosis\ndef plotBarCat(df,feature,target):\n    \n    \n    \n    x0 = df[df[target]==0][feature]\n    x1 = df[df[target]==1][feature]\n\n    trace1 = go.Histogram(\n        x=x0,\n        opacity=0.75\n    )\n    trace2 = go.Histogram(\n        x=x1,\n        opacity=0.75\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay',\n                      title=feature,\n                       yaxis=dict(title='Count'\n        ))\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='overlaid histogram')\n    \n    def DescribeFloatSkewKurt(df,target):\n        \"\"\"\n            A fundamental task in many statistical analyses is to characterize\n            the location and variability of a data set. A further\n            characterization of the data includes skewness and kurtosis.\n            Skewness is a measure of symmetry, or more precisely, the lack\n            of symmetry. A distribution, or data set, is symmetric if it\n            looks the same to the left and right of the center point.\n            Kurtosis is a measure of whether the data are heavy-tailed\n            or light-tailed relative to a normal distribution. That is,\n            data sets with high kurtosis tend to have heavy tails, or\n            outliers. Data sets with low kurtosis tend to have light\n            tails, or lack of outliers. A uniform distribution would\n            be the extreme case\n        \"\"\"\n        print('-*-'*25)\n        print(\"{0} mean : \".format(target), np.mean(df[target]))\n        print(\"{0} var  : \".format(target), np.var(df[target]))\n        print(\"{0} skew : \".format(target), skew(df[target]))\n        print(\"{0} kurt : \".format(target), kurtosis(df[target]))\n        print('-*-'*25)\n    \n    DescribeFloatSkewKurt(df,target)","6c4e8382":"plotBarCat(df,df_name[0],'type')","4713f373":"plotBarCat(df,df_name[1],'type')","53aaff65":"def PlotPie(df, nameOfFeature):\n    labels = [str(df[nameOfFeature].unique()[i]) for i in range(df[nameOfFeature].nunique())]\n    values = [df[nameOfFeature].value_counts()[i] for i in range(df[nameOfFeature].nunique())]\n\n    trace=go.Pie(labels=labels,values=values)\n\n    py.iplot([trace])","f36d9e31":"PlotPie(df, 'type')","5b7004b0":"model = myLogisticRegression(lr=0.1, num_iter=300000, fit_intercept=True, verbose=True)","5098f48f":"X =  df[df_name[0:4]]\nY = df[df_name[4]]\nX_train, X_test, y_train, y_test =train_test_split(X,Y,\n                                                   test_size=0.25,\n                                                   random_state=0,\n                                                   stratify=df['type'])","925f9df4":"X_train.head()","90ed9979":"y_train.head()","e1404a69":"%time model.fit(X_train, y_train)\nmodel.weight","1462037e":"Here, the activation function is Sigmoid $a =\\sigma  \\left( z\\right)$ , where $ z = w x $\n\n$ a=\\sigma \\left( z\\right) =\\dfrac{1}{1+e^{-z}} $\n\n\n\n\n\n\n\n### Loss function\n\nIn Machine Learning, the function used to evaluate a model (or, the current weights during training) is referred as the loss function. We want to minimize the loss function by adjusting the weights. The weights are tuned using gradient descend. In Logistic Regression, the loss function is Logistic Loss, or Log Loss. In fact, Log Loss is the negative of the log of the likelihood function. \n\nProduct of the individual likelihoods: $L(a|y) = a^{y} \\cdot (1-a)^{1-y}$\n\n\n\nLog of Likelihood: $ll(w) = ln(a^{y} \\cdot(1-a)^{1-y}) = ln(a^{y}) + ln((1-a)^{1-y}) = y\\cdot ln(a) + (1-y)\\cdot ln(1-a)$\n\nSo the Log Loss is: \n\n$  loss(w) = - [y\\cdot ln(\\sigma(wx)) + (1-y)\\cdot ln(1-\\sigma(wx))] $\n\n### Gradient\n\nIn each training step, $loss(w)$ is calculated by the formular above. Next, the gradient is calculated for back propagation. This gradient is used to adjust the weights in each iteration.\n\n$\\dfrac{da}{dz}=\\left( \\dfrac{1}{1+e^{-z}}\\right) '=-1\\cdot \\dfrac{1}{\\left( 1+e^{-z}\\right) ^{2}}\\cdot e^{-z}\\cdot \\left( -1\\right)  =\\dfrac{e^{-z}}{\\left( 1+e^{-z}\\right) ^{2}} =\\dfrac{1}{1+e^{-z}}\\cdot \\dfrac{e^{-z}}{1+e^{-z}}\n$\n\n$\\dfrac{da}{dz}=\\dfrac{1}{1+e^{-z}}\\cdot \\dfrac{e^{-z}+1-1}{1+e^{-z}} = \\dfrac{1}{1+e^{-z}}\\left( 1-\\dfrac{1}{1+e^{-z}}\\right)\n$\n\n$\\dfrac{da}{dz}=a\\left( 1-a\\right)$\n\n$\\dfrac{dz}{dw}=x $\n\n$ a'=\\dfrac{da}{dz}\\cdot \\dfrac{dz}{dw} =a\\left( 1-a\\right)x$\n\n      \nThe derivative of the Log Loss is:\n\n$ loss'(w) = -[y \\dfrac{1}{\\sigma(wx)}\\sigma(wx)' + (1-y)\\dfrac{1}{1-\\sigma(wx)}(-1) \\sigma(wx)'] $\n\n$ = -[y \\dfrac{1}{\\sigma(wx)}\\sigma(wx)(1-\\sigma(wx)) x + (1-y)\\dfrac{1}{1-\\sigma(wx)}(-1) \\sigma(wx)(1-\\sigma(wx)) x] $\n\n$ = -\\sigma(wx)(1-\\sigma(wx)) x (y\\dfrac{1}{\\sigma(wx)} - (1-y)\\dfrac{1}{1-\\sigma(wx)} ) $\n\n$ = -\\sigma(wx)(1-\\sigma(wx)) x \\dfrac{y(1-\\sigma(wx)) - (1-y)\\sigma(wx)}{\\sigma(wx)(1-\\sigma(wx))}$\n\n$ = -\\sigma(wx)(1-\\sigma(wx)) x \\dfrac{y - \\sigma(wx)}{\\sigma(wx)(1-\\sigma(wx))}$\n\n$ = - x (y - \\sigma(wx))$\n\nIn the code below, $loss'(w)$ is the gradient:\n\n#calculate the gradient from h-y\n\ngradient = np.dot(X.T, (h - y)) \/ y.size\n\n\n\n### Backpropogation (Update weights)\n\n\n\nSo, in each iteration, we need to update the weights using the learning rate x -gradient:\n\n#update weights using gradient and learning rate\n\nself.weight -= self.lr * gradient","15bdfdeb":"Split into training and test set.","9e9a62fa":"Add the label y into the dataframe.","fd72b26e":"<h1><center>Understanding Logistic Regression<\/center><\/h1>\n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\".\n\nFrom the neural network point of view, logistic regression is a single layer neural network (a neural network with input layer and output layer, but no hidden layer). \n![LogisticRegression.png](attachment:LogisticRegression.png)","c3a485df":"The accuracy of training set is:","f2d8cfc9":"Create a logistic regression model using the Logistic Regression class defined at the beginning.","2d4b1cdf":"If fit_intercept is set to True, 1 and w0 will be added in the model's input layer. If not, input layer only has x1, x2, ..., xm, it will force the intercept to the origin (0, 0).","5a2240aa":"First create a logistic regression model. By setting fit_intercept to False, the intercept is forced to the origin (0, 0). This model will find the optimal values of  \ud835\udc640  and  \ud835\udc641  in the function  \ud835\udc67=\ud835\udc641\u22c5\ud835\udc651+\ud835\udc642\u22c5\ud835\udc652 , so to minimize the error between  \ud835\udf0e(\ud835\udc67)  and the ground truth  \ud835\udc66 .","9f0a6fda":"Logistic regression can handle high dimensional input. For ease of visualization, only first two columns of iris data are utilized as input X.","951e30b4":"This time, load the full dataset. Each input vector has 4 dimensions, x1 (Sepal length), x2 (Sepal width), x3 (Petal length) and x4 (Petal width).","6303db79":"Now create another model, and set fit_intercept to True, so the intercept is NOT forced to the origin (0, 0). It means we are estimating  \ud835\udc640,\ud835\udc641  and  \ud835\udc642  in function  \ud835\udc67=\ud835\udc640+\ud835\udc641\u22c5\ud835\udc651+\ud835\udc642\u22c5\ud835\udc652 ."}}