{"cell_type":{"76a3fc34":"code","971e0bb4":"code","b916f9c2":"code","315fa9cf":"code","a20e8d04":"code","88a08405":"code","4d0ed757":"markdown"},"source":{"76a3fc34":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# torch\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision.utils as utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torch.autograd import Variable\nfrom torchvision.models.vgg import vgg16\nfrom torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n\nimport argparse\nimport time\nfrom math import log10\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nimport xml.etree.ElementTree as ET # for parsing XML\nimport math\nfrom math import exp\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n\nimport shutil # used for making zip archives of output\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","971e0bb4":"class SRGenerator(nn.Module):\n    def __init__(self, scale_factor):\n        upsample_block_num = int(math.log(scale_factor, 2))\n\n        super(SRGenerator, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.PReLU()\n        )\n        self.block2 = ResidualBlock(64)\n        self.block3 = ResidualBlock(64)\n        self.block4 = ResidualBlock(64)\n        self.block5 = ResidualBlock(64)\n        self.block6 = ResidualBlock(64)\n        self.block7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64)\n        )\n        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n        self.block8 = nn.Sequential(*block8)\n\n    def forward(self, x):\n        block1 = self.block1(x)\n        block2 = self.block2(block1)\n        block3 = self.block3(block2)\n        block4 = self.block4(block3)\n        block5 = self.block5(block4)\n        block6 = self.block6(block5)\n        block7 = self.block7(block6)\n        block8 = self.block8(block1 + block7)\n\n        return (torch.tanh(block8) + 1) \/ 2\n\n\nclass SRDiscriminator(nn.Module):\n    def __init__(self):\n        super(SRDiscriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(512, 1024, kernel_size=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(1024, 1, kernel_size=1)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        return torch.sigmoid(self.net(x).view(batch_size))\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        return x + residual\n\n\nclass UpsampleBLock(nn.Module):\n    def __init__(self, in_channels, up_scale):\n        super(UpsampleBLock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.prelu(x)\n        return x","b916f9c2":"class GeneratorLoss(nn.Module):\n    def __init__(self):\n        super(GeneratorLoss, self).__init__()\n        vgg = vgg16(pretrained=True)\n        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n        for param in loss_network.parameters():\n            param.requires_grad = False\n        self.loss_network = loss_network\n        self.mse_loss = nn.MSELoss()\n        self.tv_loss = TVLoss()\n\n    def forward(self, out_labels, out_images, target_images):\n        # Adversarial Loss\n        adversarial_loss = torch.mean(1 - out_labels)\n        # Perception Loss\n        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n        # Image Loss\n        image_loss = self.mse_loss(out_images, target_images)\n        # TV Loss\n        tv_loss = self.tv_loss(out_images)\n        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n\n\nclass TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size = x.size()[0]\n        h_x = x.size()[2]\n        w_x = x.size()[3]\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv \/ count_h + w_tv \/ count_w) \/ batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]\n\n\nif __name__ == \"__main__\":\n    g_loss = GeneratorLoss()\n    print(g_loss)","315fa9cf":"def is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n\n\ndef calculate_valid_crop_size(crop_size, upscale_factor):\n    return crop_size - (crop_size % upscale_factor)\n\n\ndef train_hr_transform(crop_size):\n    return Compose([\n        RandomCrop(crop_size),\n        ToTensor(),\n    ])\n\n\ndef train_lr_transform(crop_size, upscale_factor):\n    return Compose([\n        ToPILImage(),\n        Resize(crop_size \/\/ upscale_factor, interpolation=Image.BICUBIC),\n        ToTensor()\n    ])\n\n\ndef display_transform():\n    return Compose([\n        ToPILImage(),\n        Resize(400),\n        CenterCrop(400),\n        ToTensor()\n    ])\n\n\nclass TrainDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, crop_size, upscale_factor):\n        super(TrainDatasetFromFolder, self).__init__()\n        self.image_filenames = [os.path.join(dataset_dir, x) for x in os.listdir(dataset_dir) if is_image_file(x)]\n        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n        self.hr_transform = train_hr_transform(crop_size)\n        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n\n    def __getitem__(self, index):\n        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n        lr_image = self.lr_transform(hr_image)\n        return lr_image, hr_image\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\nclass ValDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, upscale_factor):\n        super(ValDatasetFromFolder, self).__init__()\n        self.upscale_factor = upscale_factor\n        self.image_filenames = [os.path.join(dataset_dir, x) for x in os.listdir(dataset_dir) if is_image_file(x)]\n\n    def __getitem__(self, index):\n        hr_image = Image.open(self.image_filenames[index])\n        w, h = hr_image.size\n        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n        lr_scale = Resize(crop_size \/\/ self.upscale_factor, interpolation=Image.BICUBIC)\n        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n        hr_image = CenterCrop(crop_size)(hr_image)\n        lr_image = lr_scale(hr_image)\n        hr_restore_img = hr_scale(lr_image)\n        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\nclass TestDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, upscale_factor):\n        super(TestDatasetFromFolder, self).__init__()\n        self.lr_path = dataset_dir + '\/SRF_' + str(upscale_factor) + '\/data\/'\n        self.hr_path = dataset_dir + '\/SRF_' + str(upscale_factor) + '\/target\/'\n        self.upscale_factor = upscale_factor\n        self.lr_filenames = [os.path.join(self.lr_path, x) for x in os.listdir(self.lr_path) if is_image_file(x)]\n        self.hr_filenames = [os.path.join(self.hr_path, x) for x in os.listdir(self.hr_path) if is_image_file(x)]\n\n    def __getitem__(self, index):\n        image_name = self.lr_filenames[index].split('\/')[-1]\n        lr_image = Image.open(self.lr_filenames[index])\n        w, h = lr_image.size\n        hr_image = Image.open(self.hr_filenames[index])\n        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n        hr_restore_img = hr_scale(lr_image)\n        return image_name, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n\n    def __len__(self):\n        return len(self.lr_filenames)","a20e8d04":"def ssim(img1, img2, window_size = 11, size_average = True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    \n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    \n    return _ssim(img1, img2, window, window_size, channel, size_average)\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size\/\/2)**2\/float(2*sigma**2)) for x in range(window_size)])\n    return gauss\/gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average = True):\n    mu1 = F.conv2d(img1, window, padding = window_size\/\/2, groups = channel)\n    mu2 = F.conv2d(img2, window, padding = window_size\/\/2, groups = channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1*mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size\/\/2, groups = channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size\/\/2, groups = channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding = window_size\/\/2, groups = channel) - mu1_mu2\n\n    C1 = 0.01**2\n    C2 = 0.03**2\n\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))\/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)","88a08405":"#for gen_img_nr in range(100, 800):\n    UPSCALE_FACTOR = 4\n    TEST_MODE = True\n    IMAGE_NAME = '..\/input\/generated-image\/1_gen_img.png'\n    MODEL_NAME = '..\/input\/srgan-trained\/netG_epoch_4_100.pth'\n\n    model = SRGenerator(UPSCALE_FACTOR).eval()\n    if TEST_MODE:\n        model.cuda()\n        model.load_state_dict(torch.load(MODEL_NAME))\n    else:\n        model.load_state_dict(torch.load(MODEL_NAME, map_location=lambda storage, loc: storage))\n\n    image = Image.open(IMAGE_NAME).convert('RGB')\n    image = Variable(ToTensor()(image), volatile=True).unsqueeze(0)\n    if TEST_MODE:\n        image = image.cuda()\n\n    start = time.clock()\n    out = model(image)\n    elapsed = (time.clock() - start)\n    print('cost' + str(elapsed) + 's')\n    out_img = ToPILImage()(out[0].data.cpu())\n    plt.imshow(out_img)\n    out_img.save(\"generatedimage.png\")    ","4d0ed757":"# Challenge: Generative Adversarial Network\n### Generative Dog Images"}}