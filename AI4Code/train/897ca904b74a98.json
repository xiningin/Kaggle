{"cell_type":{"93dc2884":"code","586ea133":"code","c8f11428":"code","09a360ad":"code","e05102e4":"code","1ecb76cd":"code","3b1cf27e":"code","c9a2c3da":"code","38cbf7c5":"code","59457f5f":"code","7b9253ae":"code","099d2944":"code","4781a8da":"code","d2817291":"code","9226d750":"code","fa7c9dfb":"code","1952f32f":"code","42a6b0e5":"code","6477ca1a":"code","06123e38":"code","dcb23faa":"code","058c5380":"code","a5d0120c":"code","c682e51e":"code","63b73515":"code","98e21fd4":"code","50a5677c":"code","5dd5b81d":"code","ecd6b79d":"code","eae7d186":"code","40dee113":"code","099aebad":"code","38a3b7f7":"code","816ea3fe":"code","a8ef011d":"code","d334e208":"code","88133e82":"code","bd919608":"code","81e39605":"code","d584bd87":"code","3f564a7a":"code","d7cacf3f":"code","714c05dd":"code","ed77f84d":"code","b9b3199c":"code","59c6cf3b":"code","0e9bc6b3":"code","349b6bba":"code","e5073689":"code","bf844aed":"code","6aa59a42":"markdown","2c503c23":"markdown","9a6f2309":"markdown","fe4e2003":"markdown","bb4188bf":"markdown","a000b0e5":"markdown","cf094bd3":"markdown","82a539a8":"markdown","8679a6d5":"markdown","9bbd24b4":"markdown","54e87871":"markdown","365c1bf8":"markdown","3f25ce70":"markdown","df50ef91":"markdown","da2b40b9":"markdown","4673f205":"markdown","5c06c76e":"markdown","b1feb2d5":"markdown","3e7daad4":"markdown","de42d6f5":"markdown","20d1a805":"markdown","f3428d5c":"markdown","ad68b6c9":"markdown","bb26fc13":"markdown","225da5d8":"markdown","7b70bfb6":"markdown","a9feb5f2":"markdown","d519e20a":"markdown"},"source":{"93dc2884":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","586ea133":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncovid19_dataset = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')","c8f11428":"pd.set_option('display.max_row', 111)\npd.set_option('display.max_column', 111)","09a360ad":"df = covid19_dataset.copy()\ndf.shape","e05102e4":"df.dtypes.value_counts().plot.pie()","1ecb76cd":"plt.figure(figsize=(20,10))\nsns.heatmap(df.isna(), cbar=False)","3b1cf27e":"(df.isna().sum()\/df.shape[0]).sort_values(ascending=True)","c9a2c3da":"df = df[df.columns[df.isna().sum()\/df.shape[0] <0.9]]\ndf.head()","38cbf7c5":"plt.figure(figsize=(20,10))\nsns.heatmap(df.isna(), cbar=False)","59457f5f":"df['SARS-Cov-2 exam result'].value_counts(normalize=True)","7b9253ae":"for col in df.select_dtypes('float'):\n    plt.figure()\n    sns.distplot(df[col])","099d2944":"sns.distplot(df['Patient age quantile'], bins=20)","4781a8da":"for col in df.select_dtypes('object'):\n    print(f'{col :-<50} {df[col].unique()}')","d2817291":"for col in df.select_dtypes('object'):\n    plt.figure()\n    df[col].value_counts().plot.pie()","9226d750":"positive_df = df[df['SARS-Cov-2 exam result'] == 'positive']\nnegative_df = df[df['SARS-Cov-2 exam result'] == 'negative']\n\n# Blood and viral data creation \nmissing_rate = df.isna().sum()\/df.shape[0]\nblood_columns = df.columns[(missing_rate < 0.9) & (missing_rate >0.88)]\nviral_columns = df.columns[(missing_rate < 0.88) & (missing_rate > 0.75)]","fa7c9dfb":"for col in blood_columns:\n    plt.figure()\n    sns.distplot(positive_df[col], label='positive')\n    sns.distplot(negative_df[col], label='negative')\n    plt.legend()","1952f32f":"sns.countplot(x='Patient age quantile', hue='SARS-Cov-2 exam result', data=df)","42a6b0e5":"for col in viral_columns:\n    plt.figure()\n    sns.heatmap(pd.crosstab(df['SARS-Cov-2 exam result'], df[col]), annot=True, fmt='d')","6477ca1a":"sns.pairplot(df[blood_columns])","06123e38":"sns.clustermap(df[blood_columns].corr())","dcb23faa":"# The blood varaibles and viral variables have different missing rates \nmissing_rate = df.isna().sum()\/df.shape[0]\n\nblood_columns = list(df.columns[(missing_rate < 0.9) & (missing_rate >0.88)])\nviral_columns = list(df.columns[(missing_rate < 0.80) & (missing_rate > 0.75)])","058c5380":"key_columns = ['Patient age quantile', 'SARS-Cov-2 exam result']","a5d0120c":"df = df[key_columns + blood_columns + viral_columns]\ndf.head()","c682e51e":"from sklearn.model_selection import train_test_split\ntrainset, testset = train_test_split(df, test_size=0.2, random_state=0)","63b73515":"\ndef encoding(df):\n    code = {'negative':0,\n            'positive':1,\n            'not_detected':0,\n            'detected':1}\n    \n    for col in df.select_dtypes('object').columns:\n        df.loc[:,col] = df[col].map(code)\n        \n    return df","98e21fd4":"def feature_engineering(df):\n    df['is_sick'] = df[viral_columns].sum(axis=1) >= 1\n    df = df.drop(viral_columns, axis=1)\n    return df","50a5677c":"# We simlpy drop the null values\ndef imputation(df):\n    #df['is na'] = (df['Parainfluenza 3'].isna()) | (df['Leukocytes'].isna())\n    #df = df.fillna(-999)\n    df = df.dropna(axis=0)\n    return  df","5dd5b81d":"\ndef preprocessing(df):\n    \n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('SARS-Cov-2 exam result', axis=1)\n    y = df['SARS-Cov-2 exam result']\n    \n    print(y.value_counts())\n    \n    return X, y","ecd6b79d":"X_train, y_train = preprocessing(trainset)","eae7d186":"X_test, y_test = preprocessing(testset)","40dee113":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler","099aebad":"preprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False), SelectKBest(f_classif, k=10))","38a3b7f7":"# We will test 4 different models\nRandomForest = make_pipeline(preprocessor, RandomForestClassifier(random_state=0))\nAdaBoost = make_pipeline(preprocessor, AdaBoostClassifier(random_state=0))\nSVM = make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\nKNN = make_pipeline(preprocessor, StandardScaler(), KNeighborsClassifier())","816ea3fe":"dict_of_models = {'RandomForest': RandomForest,\n                  'AdaBoost' : AdaBoost,\n                  'SVM': SVM,\n                  'KNN': KNN\n                 }","a8ef011d":"from sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve","d334e208":"# Train\/Test f1 score comparison\ndef evaluation(model):\n    \n    model.fit(X_train, y_train)\n    ypred = model.predict(X_test)\n    \n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                              cv=4, scoring='f1',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","88133e82":"for name, model in dict_of_models.items():\n    print(name)\n    evaluation(model)","bd919608":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","81e39605":"SVM","d584bd87":"hyper_params = {'svc__gamma':[1e-3, 1e-4, 0.0005],\n                'svc__C':[1, 10, 100, 1000, 3000], \n               'pipeline__polynomialfeatures__degree':[2, 3],\n               'pipeline__selectkbest__k': range(45, 60)}","3f564a7a":"grid = RandomizedSearchCV(SVM, hyper_params, scoring='recall', cv=4,\n                          n_iter=40)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\n\ny_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","d7cacf3f":"evaluation(grid.best_estimator_)","714c05dd":"from sklearn.metrics import precision_recall_curve","ed77f84d":"precision, recall, threshold = precision_recall_curve(y_test, grid.best_estimator_.decision_function(X_test))","b9b3199c":"plt.plot(threshold, precision[:-1], label='precision')\nplt.plot(threshold, recall[:-1], label='recall')\nplt.legend()","59c6cf3b":"def model_final(model, X, threshold=0):\n    return model.decision_function(X) > threshold","0e9bc6b3":"y_pred = model_final(grid.best_estimator_, X_test, threshold=-1)","349b6bba":"from sklearn.metrics import recall_score","e5073689":"f1_score(y_test, y_pred)","bf844aed":"recall_score(y_test, y_pred)","6aa59a42":"## Shape Analysis","2c503c23":"# Exploratory Data Analysis","9a6f2309":"##### Blood Variables","fe4e2003":"To optimize the model performances, we will use a Grid search CV to test multiple hyperparameter combinations ","bb4188bf":"#### Variable \/ Variable relations ","a000b0e5":"### Continuous variables histograms","cf094bd3":"We will try to optimize the SVM model with a gread search","82a539a8":"## Sub variable categories creation ","8679a6d5":"## Train \/ Test Split","9bbd24b4":"The SVM model seems to be the best model ","54e87871":"## Evaluation","365c1bf8":"#### Target vs viral variables","3f25ce70":"### Remove useless columns","df50ef91":"#### Relation target \/ Age","da2b40b9":"## Preprocessing","4673f205":"## Feature engineering","5c06c76e":"### Target column Analysis","b1feb2d5":"## Encode categorical variables","3e7daad4":"## Optimisation","de42d6f5":"### Discrete variables analysis","20d1a805":"## Imputation","f3428d5c":"We will check the precision\/recall curves in respect of the threshold and keep the best threshold","ad68b6c9":"# Modeling","bb26fc13":"## Precision \/ Recall Curve","225da5d8":"# Preprocessing","7b70bfb6":"## Deep Analysis","a9feb5f2":"### Target\/Variables relations","d519e20a":"#### Target vs blood variables"}}