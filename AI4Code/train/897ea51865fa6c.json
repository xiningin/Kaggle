{"cell_type":{"008dae9c":"code","89241264":"code","185cccb8":"code","27a1f26e":"code","18b842ba":"code","2ba64f9a":"code","35035795":"code","80fa2af0":"code","60bba089":"code","8b3d7463":"code","69979a0e":"code","ae89c7c6":"code","905365f1":"code","c020a84f":"code","270fc36d":"code","97a76788":"code","2e544bf1":"code","a13bba3c":"code","3d91a1d7":"code","143aab9d":"code","23c81242":"code","52ca6b6b":"code","53391fa8":"code","c4fdd9cd":"code","bb3e4e68":"code","312754b6":"code","f03355ca":"code","8c75a949":"code","d75bbfed":"code","5b4bfa58":"code","805191f0":"code","9bc710eb":"code","cb532d43":"code","a3aec879":"code","dc9a6715":"code","c37e6bb8":"code","7ce93251":"code","dcee498a":"code","de9b89a5":"code","5a665674":"code","83bfc645":"code","cd1033cd":"code","c6c67f08":"code","6d776300":"code","2f890988":"code","a244af3a":"code","e48c638b":"code","7dfb93bf":"code","44d7809b":"code","c2dcb586":"code","949dbbc0":"code","4d4e7274":"code","7983cfec":"code","bc40a4dd":"code","fa55d51e":"code","21307402":"code","dc20e867":"code","35c8837e":"code","16777ed3":"code","7688b977":"markdown","cf32fbce":"markdown","08864de5":"markdown","faa3f76e":"markdown","41cdcdf2":"markdown","e8634370":"markdown","afd22b6c":"markdown","0851a72c":"markdown","8b628d80":"markdown","bc4b2719":"markdown","64172249":"markdown","9a26149f":"markdown","22d16955":"markdown","1a8839b7":"markdown","80de33b8":"markdown","4fa0af2d":"markdown","2a422dfe":"markdown","6fa4b148":"markdown","5ff20522":"markdown","d3a1b852":"markdown","c724ad5b":"markdown","80a05d6d":"markdown","0b311e8b":"markdown","5bb2753d":"markdown","2a2ae793":"markdown","cee7fffb":"markdown","93b22b13":"markdown","f628d178":"markdown","85c7b44d":"markdown","605cdd6e":"markdown","a8244533":"markdown","975bb8a0":"markdown","8599400f":"markdown","732760a6":"markdown","7c204762":"markdown","1b775839":"markdown","747968ac":"markdown","1fb325c9":"markdown","f36b3c76":"markdown","77b71a5c":"markdown","958326ae":"markdown","d763c43d":"markdown","03e693ca":"markdown","3a8c3527":"markdown","b02edfa7":"markdown","ec78b966":"markdown","8fa69476":"markdown","5a6ef85e":"markdown","7d879755":"markdown","b9fbe58a":"markdown","2a5425e3":"markdown","003dee6a":"markdown","3c205c27":"markdown","39fe5c87":"markdown","878e09ed":"markdown","4478b6b8":"markdown","224bc0d4":"markdown","1a2c310e":"markdown","d5b559be":"markdown","d5679981":"markdown","ca8ff7ac":"markdown"},"source":{"008dae9c":"from IPython.display import Image\nImage(url='https:\/\/raw.githubusercontent.com\/MaksimEkin\/COVID19-Literature-Clustering\/master\/cover\/bokeh_plot.png', width=800, height=800)","89241264":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","185cccb8":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","27a1f26e":"meta_df.info()","18b842ba":"all_json = ! ls $root_path\/document_parses\/pdf_json\nlen(all_json)","2ba64f9a":"all_json[:5]","35035795":"all_json = [root_path + \"document_parses\/pdf_json\/\" + s for s in all_json]\nall_json[:5]","80fa2af0":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","60bba089":"from tqdm import tqdm\nall_json_clean = list()\nfor idx, entry in tqdm(enumerate(all_json), total=len(all_json)):\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    if len(content.body_text) == 0:\n        continue\n    \n    all_json_clean.append(all_json[idx])\n    \nall_json = all_json_clean\nlen(all_json)","8b3d7463":"import random\n\nrandom.seed(42)\n\nall_json = random.sample(all_json, 15000)","69979a0e":"all_json[:5]","ae89c7c6":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","905365f1":"from tqdm import tqdm\ndict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in tqdm(enumerate(all_json), total = len(all_json)):\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    if len(content.body_text) == 0:\n        continue\n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","c020a84f":"df_covid.info()","270fc36d":"df_covid.head()","97a76788":"df = df_covid.sample(10000, random_state=42)\ndel df_covid","2e544bf1":"df.dropna(inplace=True)\ndf.info()","a13bba3c":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","3d91a1d7":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","143aab9d":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","23c81242":"df = df[df['language'] == 'en'] \ndf.info()","52ca6b6b":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz   ","53391fa8":"#NLP \nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg","c4fdd9cd":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","bb3e4e68":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","312754b6":"# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","f03355ca":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)","8c75a949":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X","d75bbfed":"text = df['processed_text'].values\nmax_features = 2**12\n\nX = vectorize(text, max_features)","5b4bfa58":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42)\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","805191f0":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans","9bc710eb":"from sklearn import metrics\nfrom scipy.spatial.distance import cdist\n\n# run kmeans with many different k\ndistortions = []\nK = range(2, 30)\nfor k in K:\n    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n    k_means.fit(X_reduced)\n    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n    #print('Found distortion for {} clusters'.format(k))","cb532d43":"X_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","a3aec879":"k = 20\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['y'] = y_pred","dc9a6715":"from sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\nX_embedded = tsne.fit_transform(X.toarray())","c37e6bb8":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\nplt.title('t-SNE with no Labels')\nplt.savefig(\"t-sne_covid19.png\")\nplt.show()","7ce93251":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(13,9)})\n\n# colors\npalette = sns.hls_palette(20, l=.4, s=.9)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"improved_cluster_tsne.png\")\nplt.show()","dcee498a":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer","de9b89a5":"Image(url='https:\/\/miro.medium.com\/max\/1276\/0*Sj65xR38wDwuxhtr.jpg', width=800, height=800)","5a665674":"vectorizers = []\n    \nfor ii in range(0, 20):\n    # Creating a vectorizer\n    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))","83bfc645":"vectorizers[0]","cd1033cd":"vectorized_data = []\n\nfor current_cluster, cvec in enumerate(vectorizers):\n    try:\n        vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'processed_text']))\n    except Exception as e:\n        print(\"Not enough instances in cluster: \" + str(current_cluster))\n        vectorized_data.append(None)","c6c67f08":"len(vectorized_data)","6d776300":"# number of topics per cluster\nNUM_TOPICS_PER_CLUSTER = 20\n\n\nlda_models = []\n\nfor ii in range(0, 20):\n    # Latent Dirichlet Allocation Model\n    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n    lda_models.append(lda)\n    \nlda_models[0]","2f890988":"clusters_lda_data = []\n\nfor current_cluster, lda in enumerate(lda_models):\n    print(\"Current Cluster: \" + str(current_cluster))\n    \n    if vectorized_data[current_cluster] != None:\n        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))","a244af3a":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=3):\n    current_words = []\n    keywords = []\n    \n    for idx, topic in enumerate(model.components_):\n        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n        for word in words:\n            if word[0] not in current_words:\n                keywords.append(word)\n                current_words.append(word[0])\n                \n    keywords.sort(key = lambda x: x[1])  \n    keywords.reverse()\n    return_values = []\n    for ii in keywords:\n        return_values.append(ii[0])\n    return return_values","e48c638b":"all_keywords = []\nfor current_vectorizer, lda in enumerate(lda_models):\n    print(\"Current Cluster: \" + str(current_vectorizer))\n\n    if vectorized_data[current_vectorizer] != None:\n        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))","7dfb93bf":"all_keywords[0][:10]","44d7809b":"len(all_keywords)","c2dcb586":"f=open('topics.txt','w')\n\ncount = 0\n\nfor ii in all_keywords:\n\n    if vectorized_data[count] != None:\n        f.write(', '.join(ii) + \"\\n\")\n    else:\n        f.write(\"Not enough instances to be determined. \\n\")\n        f.write(', '.join(ii) + \"\\n\")\n    count += 1\n\nf.close()","949dbbc0":"import pickle\n\n# save the COVID-19 DataFrame\npickle.dump(df, open(\"df_covid.p\", \"wb\" ))\n\n# save the final t-SNE\npickle.dump(X_embedded, open(\"X_embedded.p\", \"wb\" ))\n\n# save the labels generate with k-means(20)\npickle.dump(y_pred, open(\"y_pred.p\", \"wb\" ))","4d4e7274":"! mkdir lib\n! ls","7983cfec":"! wget https:\/\/raw.githubusercontent.com\/MaksimEkin\/COVID19-Literature-Clustering\/master\/lib\/plot_text.py\n! wget https:\/\/raw.githubusercontent.com\/MaksimEkin\/COVID19-Literature-Clustering\/master\/lib\/call_backs.py\n! mv plot_text.py lib\/.\n! mv call_backs.py lib\/.\n! ls lib\/","bc40a4dd":"# required libraries for plot\nfrom lib.plot_text import header, description, description2, cite, description_search, description_slider, notes, dataset_description, toolbox_header \nfrom lib.call_backs import input_callback, selected_code\nimport bokeh\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap, transform\nfrom bokeh.io import output_file, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\nfrom bokeh.layouts import column, widgetbox, row, layout\nfrom bokeh.layouts import column","fa55d51e":"import os\n\ntopic_path = os.path.join(os.getcwd(), 'topics.txt')\nwith open(topic_path) as f:\n    topics = f.readlines()","21307402":"# show on notebook\noutput_notebook()\n# target labels\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= df['title'],\n    authors = df['authors'],\n    journal = df['journal'],\n    abstract = df['abstract_summary'],\n    labels = [\"C-\" + str(x) for x in y_labels],\n    links = df['doi']\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors{safe}\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n    (\"Link\", \"@links\")\n],\npoint_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\nplot = figure(plot_width=1200, plot_height=850, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'tap'], \n           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\", \n           toolbar_location=\"above\")\n\n# plot settings\nplot.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\nplot.legend.background_fill_alpha = 0.6","dc20e867":"# Keywords\ntext_banner = Paragraph(text= 'Keywords: Slide to specific cluster to see the keywords.', height=25)\ninput_callback_1 = input_callback(plot, source, text_banner, topics)\n\n# currently selected article\ndiv_curr = Div(text=\"\"\"Click on a plot to see the link to the article.\"\"\",height=150)\ncallback_selected = CustomJS(args=dict(source=source, current_selection=div_curr), code=selected_code())\ntaptool = plot.select(type=TapTool)\ntaptool.callback = callback_selected\n\n# WIDGETS\nslider = Slider(start=0, end=20, value=20, step=1, title=\"Cluster #\", callback=input_callback_1)\nkeyword = TextInput(title=\"Search:\", callback=input_callback_1)\n\n# pass call back arguments\ninput_callback_1.args[\"text\"] = keyword\ninput_callback_1.args[\"slider\"] = slider\n# column(,,widgetbox(keyword),,widgetbox(slider),, notes, cite, cite2, cite3), plot","35c8837e":"# STYLE\nheader.sizing_mode = \"stretch_width\"\nheader.style={'color': '#2e484c', 'font-family': 'Julius Sans One, sans-serif;'}\nheader.margin=5\n\ndescription.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription.sizing_mode = \"stretch_width\"\ndescription.margin = 5\n\ndescription2.sizing_mode = \"stretch_width\"\ndescription2.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription2.margin=10\n\ndescription_slider.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription_slider.sizing_mode = \"stretch_width\"\n\ndescription_search.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription_search.sizing_mode = \"stretch_width\"\ndescription_search.margin = 5\n\nslider.sizing_mode = \"stretch_width\"\nslider.margin=15\n\nkeyword.sizing_mode = \"scale_both\"\nkeyword.margin=15\n\ndiv_curr.style={'color': '#BF0A30', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndiv_curr.sizing_mode = \"scale_both\"\ndiv_curr.margin = 20\n\ntext_banner.style={'color': '#0269A4', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ntext_banner.sizing_mode = \"stretch_width\"\ntext_banner.margin = 20\n\nplot.sizing_mode = \"scale_both\"\nplot.margin = 5\n\ndataset_description.sizing_mode = \"stretch_width\"\ndataset_description.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndataset_description.margin=10\n\nnotes.sizing_mode = \"stretch_width\"\nnotes.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\nnotes.margin=10\n\ncite.sizing_mode = \"stretch_width\"\ncite.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ncite.margin=10\n\nr = row(div_curr,text_banner)\nr.sizing_mode = \"stretch_width\"","16777ed3":"# LAYOUT OF THE PAGE\nl = layout([\n    [header],\n    [description],\n    [description_slider, description_search],\n    [slider, keyword],\n    [text_banner],\n    [div_curr],\n    [plot],\n    [description2, dataset_description, notes, cite],\n])\nl.sizing_mode = \"scale_both\"\n\n\n# show\noutput_file('t-sne_covid-19_interactive.html')\nshow(l)","7688b977":"# Conclusion\nIn this project, we have attempted to cluster published literature on COVID-19 and reduce the dimensionality of the dataset for visualization purposes. This has allowed for an interactive scatter plot of papers related to COVID-19, in which material of similar themes is grouped. Grouping the literature in this way allows for professionals to quickly find material related to a central topic. Instead of having to manually search for related work, every publication is connected to a larger topic cluster. [...]\n \nThe clustering of the data was done through k-means on a pre-processed, vectorized version of the literature\u2019s body text. As k-means simply split the data into clusters, topic modeling through LDA was performed to identify keywords. This gave the topics that were prevalent in each of the clusters.\n\nK-means and t-SNE were able to independently find clusters, showing that relationships between papers can be identified and measured. Papers written on highly similar topics are typically near each other on the plot and bear the same k-means label. However, due to the complexity of the dataset, k-means and t-SNE will sometimes arrive at different decisions. The topics of much of the given literature are continuous and will not have a concrete decision boundary. In these conditions, our approach performs quite well to organize the literature by similarity. \n\n- Look at clusters\n- Look at titles\/abstract to see that they are similar\n- Use classification to see if k-means found effective decisions\n\nAs neither of the authors is qualified to assess the meaning of the literature, the analysis by inspection was performed solely on the titles. Even so, it was apparent that articles on key topics could be easily found near each other. For example [MASK CLUSTER EXAMPLE FROM FINAL PLOT].  We believe that health professionals can use this tool to find real links in the texts. By organizing the literature, qualified people can quickly find related publications that answer the task questions. \n\n\"This project can further be improved by abstracting the underlying data analysis techniques as described in this notebook to develop a user interface\/tool that presents the related articles in a user-friendly manner.\"\n\n<br>\n<br>\n<br>","cf32fbce":"## Setup","08864de5":"Now that we have our dataset loaded, we need to clean up the text to improve any clustering or classification efforts. First, let's drop Null values:","faa3f76e":"Extracts the keywords from each cluster","41cdcdf2":"Let's take a look at the language distribution in the dataset","e8634370":"**Some areas we thought were great:** (pros)\n- The tool is saved as an HTML file. It can be downloaded and used locally\/offline.\n- It is portable\/mobile, easily deployable, and failover safe; the risk of denial of service in the case of emergencies such as the loss of network connection is mitigated\n- Dimensionality reduction allows for the whole dataset to be easily accessible. The papers are all up on the plot and can be quickly examined by hovering over them. If the abstract seems interesting, the user can click on the point to bring up a text box with more information that will contain a link to the full paper\n- Patterns in the data found through clustering\/dimensionality reduction may not be readily apparent to researchers. These unsupervised techniques can show humans hidden connections in the literature\n- If the topics\/clusters are not narrow enough for the user, a search for a key term will only bring up papers that contain the search term. Search can be performed inside of a selected cluster or the entire dataset if preferred.\n- A surface-level examination of the plot showed some very interesting organization of the data. For example, one subcluster consisted of papers that tried to determine the efficacy of masks in preventing the spread of COVID-19.\n- This work can be easily replicated and modified as needed, serving as a foundation for future projects.\n\n<br>\n\n**Future thoughts to consider:** (cons)\n- Possible false positives, difficult to draw an exact line between subjects\n- K-means and t-SNE are unsupervised approaches that will not necessarily predictably group instances. Due to their unsupervised nature, this is not an exact science \n- Not labeled data; no certainty.\n- No in between, clustering either looks ok or is very wrong\n- Loss of foreign language papers. This leads to the loss of experience from different geographic locations in dealing with COVID-19\n- The algorithms used in this notebook are stochastic so the results may vary depending on the random state. In this notebook, all of the algorithms are set to random state 42 (the meaning of life) to ensure reproducible results","afd22b6c":"# Plotting the data\nThe previous steps have given us clustering labels and a dataset of papers reduced to two dimensions. By pairing this with Bokeh, we can create an interactive plot of the literature. This should organize the papers such that related publications are nearby. To try to understand what the similarities may be, we have also performed topic modeling on each cluster of papers to pick out the key terms. ","0851a72c":"Helper function adds a break after every word when the character length reaches a certain amount. This is for the interactive plot so that the hover tool fits the screen.","8b628d80":"Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together.","bc4b2719":"# Approach:\n\n- Parse the text from the body of each document using Natural Language Processing (NLP).\n- Turn each document instance di into a feature vector Xi using Term Frequency\u2013Inverse Document Frequency (TF-IDF).\n- Apply Dimensionality Reduction to each feature vector Xi using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the two-dimensional plane X embedding Y1.\n- Use Principal Component Analysis (PCA) to project down the dimensions of X to several dimensions that will keep .95 variance while removing noise and outliers in embedding Y2.\n- Apply k-means clustering on Y2, where k is 20, to label each cluster on Y1.\n- Apply Topic Modeling on X using Latent Dirichlet Allocation (LDA) to discover keywords from each cluster.\n- Investigate the clusters visually on the plot, zooming down to specific articles as needed, and via classification using Stochastic Gradient Descent (SGD).\n\n<br>\n<br>","64172249":"### Stopwords\n\nPart of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step).","9a26149f":"Unfortunately, running the next steps of the notebook is not possible on the full dataset within Kaggle. **The full plot is available is at https:\/\/maksimekin.github.io\/COVID19-Literature-Clustering\/plots\/t-sne_covid-19_interactive.html**.\n\nIn Kaggle we will limit the dataframe to **10,000** instances","22d16955":"# Table of Contents\n1. Loading the data\n2. Pre-processing\n3. Vectorization\n4. PCA  & Clustering\n5. Dimensionality Reduction with t-SNE\n6. Topic Modeling on Each Cluster\n7. Classify\n8. Plot\n9. How to Use the Plot?\n10. Conclusion\n11. Citation\/Sources\n\n<br>\n<br>","1a8839b7":"# PCA  & Clustering\n\nLet's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. This will reduce the complexity of our dimensionality reduction step and filter noise.","80de33b8":"Download the spacy bio parser","4fa0af2d":"First, we will create 20 vectorizers, one for each of our cluster labels","2a422dfe":"For each cluster, we had created a corresponding LDA model in the previous step. We will now fit_transform all the LDA models on their respective cluster vectors","6fa4b148":"### Save current outputs to file\n\nRe-running some parts of the notebook (especially vectorization and t-SNE) are time-intensive tasks. We want to make sure that the important outputs for generating the bokeh plot are saved for future use.","5ff20522":"### Helper Functions","d3a1b852":"To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively.","c724ad5b":"## Take a Look at the Data:","80a05d6d":"# Loading the Data\nLoad the data following the notebook by Ivan Ega Pratama, from Kaggle.\n#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)","0b311e8b":"Append list of keywords for a single cluster to 2D list of length NUM_TOPICS_PER_CLUSTER","5bb2753d":"### How many clusters? \n\nTo find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters.","2a2ae793":"### Handling multiple languages\nNext, we are going to determine the language of each paper in the data frame. Not all of the sources are English and the language needs to be identified so that we know how to handle these instances","cee7fffb":"This looks pretty bland. t-SNE was able to reduce the dimensionality of the texts, but now clustering is required. <br>\nLet's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics.","93b22b13":"# Citation\/Sources\n\nKaggle Submission: [COVID-19 Literature Clustering | Kaggle](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering#Unsupervised-Learning:-Clustering-with-K-Means)\n \n ```\n@inproceedings{Raff2020,\n\tauthor = {Raff, Edward and Nicholas, Charles and McLean, Mark},\n\tbooktitle = {The Thirty-Fourth AAAI Conference on Artificial Intelligence},\n\ttitle = {{A New Burrows Wheeler Transform Markov Distance}},\n\turl = {http:\/\/arxiv.org\/abs\/1912.13046},\n\tyear = {2020},\n}\n```\n```\n@misc{Kaggle,\n\tauthor = {Kaggle},\n\ttitle = {COVID-19 Open Research Dataset Challenge (CORD-19)},\n\tyear = {2020},\n\tmonth = {March},\n\tnote = {Allen Institute for AI in partnership with the Chan Zuckerberg Initiative, Georgetown University\u2019s Center for   Security and Emerging Technology, Microsoft Research, and the National Library of Medicine - National Institutes of Health, in coordination with The White House Office of Science and Technology Policy.},\n\thowpublished = {\\url{https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge}}\n}\n```\n```\n@inproceedings{Shakespeare,\n\tauthor = {Nicholas, Charles},\n\ttitle = {Mr. Shakespeare, Meet Mr. Tucker},\n\tbooktitle = {High Performance Computing and Data Analytics Workshop},\n\tyear = {2019},\n\tmonth = {September},\n\tlocation = { Linthicum Heights, MD, USA},\n}\n```\n```\n@inproceedings{raff_lzjd_2017,\n\tauthor = {Raff, Edward and Nicholas, Charles},\n\ttitle = {An Alternative to NCD for Large Sequences, Lempel-Ziv Jaccard Distance},\n\tbooktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n\tseries = {KDD '17},\n\tyear = {2017},\n\tisbn = {978-1-4503-4887-4},\n\tlocation = {Halifax, NS, Canada},\n\tpages = {1007--1015},\n\tnumpages = {9},\n\turl = {http:\/\/doi.acm.org\/10.1145\/3097983.3098111},\n\tdoi = {10.1145\/3097983.3098111},\n\tacmid = {3098111},\n\tpublisher = {ACM},\n\taddress = {New York, NY, USA},\n\tkeywords = {cyber security, jaccard similarity, lempel-ziv, malware classification, normalized compression distance},\n}\n```\n\n<br>\n<br>\n\n**Thank you for looking at our notebook. We greatly appreciate tips, suggestions, and upvotes :)** <br>\n**If you would like to check out the interactive plot with the full dataset, please visit:**\n\nhttps:\/\/maksimekin.github.io\/COVID19-Literature-Clustering\/plots\/t-sne_covid-19_interactive.html","f628d178":"# Dimensionality Reduction with t-SNE","85c7b44d":"# Vectorization\n\nNow that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose, we will be using tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole.","605cdd6e":"### Loading Metadata","a8244533":"## Style","975bb8a0":"### Load the Data into DataFrame","8599400f":"# COVID-19 Literature Clustering","732760a6":"![kmeans.PNG](attachment:kmeans.PNG)\n\n[source](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering)","7c204762":"<h4><center>t-SNE Scatter Plot from CORD-19<\/center><\/h4>","1b775839":"### Next let's create a function that will process the text data for us. \nFor this purpose, we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text.","747968ac":"# Goal\nGiven the large number of literature and the rapid spread of COVID-19, it is difficult for health professionals to keep up with new information on the virus. Can clustering similar research articles together simplify the search for related publications? How can the content of the clusters be qualified?\n\nBy using clustering for labeling in combination with dimensionality reduction for visualization, the collection of literature can be represented by a scatter plot. On this plot, publications of highly similar topics will share a label and will be plotted near each other. In order, to find meaning in the clusters, topic modeling will be performed to find the keywords of each cluster.\n\nBy using Bokeh, the plot will be interactive. Users will have the option of seeing the plot as a whole or filtering the data by cluster. If a narrower scope is required, the plot will also have a search function that will limit the output to only papers containing the search term. Hovering over points on the plot will give basic information like title, author, journal, and abstract. Clicking on a point will bring up a menu with a URL that can be used to access the full publication.\n\nThis is a difficult time in which health care workers, sanitation staff, and much other essential personnel are out there keeping the world afloat. Sitting at home has given us time to try to help in our own way. We hope that our work will have some impact on the fight against COVID-19. It should be noted, however, that we are not epidemiologists, and it is not our place to gauge the importance of these papers. This tool was created to help make it easier for trained professionals to sift through many, many publications related to the virus, and find their own determinations.\n\n\n#### We welcome feedback so that we can continue to improve this project.\n\n<br>\n<br>\n<br>\n","1fb325c9":"For topic modeling, we will use LDA (Latent Dirichlet Allocation). In LDA, each document can be described by a distribution of topics and each topic can be described by a distribution of words[.](https:\/\/towardsdatascience.com\/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)","f36b3c76":"\n[source](https:\/\/miro.medium.com\/max\/1276\/0*Sj65xR38wDwuxhtr.jpg)","77b71a5c":"## How to Cite This Work?\n```\n@inproceedings{10.1145\/3395027.3419591, \n\tauthor = {Eren, Maksim Ekin and Solovyev, Nick and Raff, Edward and Nicholas, Charles and Johnson, Ben}, \n\ttitle = {COVID-19 Kaggle Literature Organization}, \n\tyear = {2020}, \n\tisbn = {9781450380003}, \n\tpublisher = {Association for Computing Machinery}, \n\taddress = {New York, NY, USA}, \n\turl = {https:\/\/doi.org\/10.1145\/3395027.3419591}, \n\tdoi = {10.1145\/3395027.3419591}, \n\tabstract = {The world has faced the devastating outbreak of Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2), or COVID-19, in 2020. Research in the subject matter was fast-tracked to such a point that scientists were struggling to keep up with new findings. With this increase in the scientific literature, there arose a need for organizing those documents. We describe an approach to organize and visualize the scientific literature on or related to COVID-19 using machine learning techniques so that papers on similar topics are grouped together. By doing so, the navigation of topics and related papers is simplified. We implemented this approach using the widely recognized CORD-19 dataset to present a publicly available proof of concept.}, \n\tbooktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},\n\tarticleno = {15}, \n\tnumpages = {4}, \n\tkeywords = {clustering, document visualization, COVID-19, dimensionality reduction}, \n\tlocation = {Virtual Event, CA, USA}, series = {DocEng '20} }\n```","958326ae":"So that step took a while! Let's take a look at what our data looks like when compressed into 2 dimensions. ","d763c43d":"Using [t-SNE](https:\/\/lvdmaaten.github.io\/tsne) we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the body_text can be plotted. t-SNE will attempt to preserve the relations of the higher dimensional data as closely as possible when shrunk to 2D. The similar article will thus be in closer proximity to each other.","03e693ca":"Get path to all JSON files:","3a8c3527":"Now the above stopwords are used in everyday English text. Research papers will often use words that don't contribute to the meaning and are not considered everyday stopwords.\n\nThank you Daniel Wolffram for the idea.\n#### Cite: [Custom Stop Words | Topic Modeling: Finding Related Articles](https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles)","b02edfa7":"Now we will vectorize the data from each of our clusters","ec78b966":"# Data Pre-processing","8fa69476":"### Run k-means","5a6ef85e":"First lets download our scripts from the repository that contains the plot settings and text:","7d879755":"## Load the Keywords per Cluster","b9fbe58a":"Vectorize our data. We will be clustering based on the content of the body text. The maximum number of features will be the maximum number of unique words out of all of the papers","2a5425e3":"\nWe will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n\n1. API calls were limited\n\n2. Translating the language may not carry over the true semantic meaning of the text\n","003dee6a":"### Fetch All of JSON File Path","3c205c27":"## Widgets","39fe5c87":"Applying the text-processing function on the **body_text**. ","878e09ed":" File Reader Class","4478b6b8":"<h2 style=\"color:black;\">The full version of the interactive plot:<\/h2>\n\n## https:\/\/maksimekin.github.io\/COVID19-Literature-Clustering\/plots\/t-sne_covid-19_interactive.html\n\n<h2 style=\"color:black;\">GitHub:<\/h2>\n\n## https:\/\/github.com\/MaksimEkin\/COVID19-Literature-Clustering\n\n<h2 style=\"color:black;\">Link to the paper:<\/h2>\n\n## https:\/\/www.maksimeren.com\/publication\/eren_doceng2020\/\n\n<br>\n<br>","224bc0d4":"Topic modeling will be performed through the use of Latent Dirichlet Allocation (LDA). This is a generative statistical model that allows sets of words to be explained by a shared topic","1a2c310e":"Using the helper functions, let's read the articles into a DataFrame that can be used easily:","d5b559be":"# Topic Modeling on Each Cluster\n\nNow we will attempt to find the most significant words in each cluster. K-means clustered the articles but did not label the topics. Through topic modeling, we will find out what the most important terms for each cluster are. This will add more meaning to the cluster by giving keywords to quickly identify the themes of the cluster.","d5679981":"The labeled plot gives better insight into how the papers are grouped. Interestingly, both k-means and t-SNE can find independent clusters even though they were run independently. This shows that structure within the literature can be observed and measured to some extent. \n\nNow there are other cases where the colored labels are spread out on the plot. This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it was hard to cleanly separate them. ","ca8ff7ac":"## SHOW"}}