{"cell_type":{"8c6c60b0":"code","b0025298":"code","96f4dd39":"code","dad649b3":"code","c468ee73":"code","6e153df3":"code","dd674593":"code","c13b7ca4":"code","bd994f73":"code","c8066b02":"code","da3c50d7":"code","012db559":"code","5b84952f":"code","45d19688":"code","ef9a437f":"code","a9045fe5":"code","a314e143":"code","c077cfab":"code","14d828b7":"code","2a0b913f":"markdown","0026ae2e":"markdown","ef67e179":"markdown","203da9a0":"markdown"},"source":{"8c6c60b0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))","b0025298":"import pandas as pd\nimport librosa\nimport librosa.display\nimport glob \nimport matplotlib.pyplot as plt","96f4dd39":"set_a = pd.read_csv(\"\/kaggle\/input\/heartbeat-sounds\/set_a.csv\")\nset_b = pd.read_csv(\"\/kaggle\/input\/heartbeat-sounds\/set_b.csv\")\nframes = [set_a, set_b]\ntrain_ab=pd.concat(frames)\ntrain_ab.info()","dad649b3":"nb_classes=train_ab.label.unique()\nprint (nb_classes)","c468ee73":"import IPython.display as ipd\nnormal_file=\"\/kaggle\/input\/heartbeat-sounds\/set_a\/normal__201106111136.wav\"\nipd.Audio(normal_file) ","6e153df3":"from scipy.io import wavfile\nrate, data = wavfile.read(normal_file)\nprint(\"Sampling (frame) rate = \", rate)\nprint(\"Total samples (frames) = \", data.shape)\nprint(data)","dd674593":"# plot wave by audio frames\nplt.figure(figsize=(16, 3))\nplt.plot(data, '-', );","c13b7ca4":"mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n#print (mfccs)\nplt.figure(figsize=(12, 3))\nlibrosa.display.specshow(mfccs, x_axis='time')\nplt.colorbar()\nplt.title('Mel-frequency cepstral coefficients (MFCCs)')\nplt.tight_layout()","bd994f73":"print(\"Number of training examples=\", train_ab.shape[0], \"  Number of classes=\", len(train_ab.label.unique()))","c8066b02":"SAMPLE_RATE = 16000\n# seconds\nMAX_SOUND_CLIP_DURATION=12 \n\nimport numpy as np\n\ndef audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+0.0001)\n    return data-0.5\n\n# get audio data without padding highest qualify audio\ndef load_file_data_without_change(folder,file_names, duration=3, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0) \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)\n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data\n\n\n# get audio data with a fix padding may also chop off some file\ndef load_file_data (folder,file_names, duration=12, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # pad audio file same duration\n            if (round(dur) < duration):\n                print (\"fixing audio lenght :\", file_name)\n                y = librosa.util.fix_length(X, input_length)                \n            #normalized raw audio \n            # y = audio_norm(y)            \n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)             \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)        \n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data","da3c50d7":"# simple encoding of categories, limited to 3 types\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# Map label text to integer\nCLASSES = ['artifact','murmur','normal']\n# {'artifact': 0, 'murmur': 1, 'normal': 3}\nNB_CLASSES=len(CLASSES)\n\n# Map integer value to text labels\nlabel_to_int = {k:v for v,k in enumerate(CLASSES)}\nprint (label_to_int)\nprint (\" \")\n# map integer to label text\nint_to_label = {v:k for k,v in label_to_int.items()}\nprint(int_to_label)","012db559":"# load dataset-a, keep them separate for testing purpose\nimport os, fnmatch\nINPUT_DIR = \"\/kaggle\/input\/heartbeat-sounds\"\nA_folder=INPUT_DIR+'\/set_a\/'\n# set-a\nA_artifact_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'artifact*.wav')\nA_artifact_sounds = load_file_data(folder=A_folder,file_names=A_artifact_files, duration=MAX_SOUND_CLIP_DURATION)\nA_artifact_labels = [0 for items in A_artifact_files]\n\nA_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'normal*.wav')\nA_normal_sounds = load_file_data(folder=A_folder,file_names=A_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nA_normal_labels = [2 for items in A_normal_sounds]\n\nA_extrahls_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'extrahls*.wav')\nA_extrahls_sounds = load_file_data(folder=A_folder,file_names=A_extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\nA_extrahls_labels = [1 for items in A_extrahls_sounds]\n\nA_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'murmur*.wav')\nA_murmur_sounds = load_file_data(folder=A_folder,file_names=A_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nA_murmur_labels = [1 for items in A_murmur_files]\n\n# test files\nA_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'Aunlabelledtest*.wav')\nA_unlabelledtest_sounds = load_file_data(folder=A_folder,file_names=A_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nA_unlabelledtest_labels = [-1 for items in A_unlabelledtest_sounds]\n","5b84952f":"# load dataset-b, keep them separate for testing purpose \nB_folder=INPUT_DIR+'\/set_b\/'\n# set-b\nB_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'normal*.wav')  # include noisy files\nB_normal_sounds = load_file_data(folder=B_folder,file_names=B_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nB_normal_labels = [2 for items in B_normal_sounds]\n\nB_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'murmur*.wav')  # include noisy files\nB_murmur_sounds = load_file_data(folder=B_folder,file_names=B_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nB_murmur_labels = [1 for items in B_murmur_files]\n\nB_extrastole_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'extrastole*.wav')\nB_extrastole_sounds = load_file_data(folder=B_folder,file_names=B_extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\nB_extrastole_labels = [1 for items in B_extrastole_files]\n\n#test files\nB_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'Bunlabelledtest*.wav')\nB_unlabelledtest_sounds = load_file_data(folder=B_folder,file_names=B_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nB_unlabelledtest_labels = [-1 for items in B_unlabelledtest_sounds]\n","45d19688":"x_data = np.concatenate((A_artifact_sounds, A_normal_sounds,A_extrahls_sounds,A_murmur_sounds, \n                         B_normal_sounds,B_murmur_sounds,B_extrastole_sounds))\n\ny_data = np.concatenate((A_artifact_labels, A_normal_labels,A_extrahls_labels,A_murmur_labels,\n                         B_normal_labels,B_murmur_labels,B_extrastole_labels))\n\ntest_x = np.concatenate((A_unlabelledtest_sounds,B_unlabelledtest_sounds))\ntest_y = np.concatenate((A_unlabelledtest_labels,B_unlabelledtest_labels))\n\nprint (\"combined training data record: \",len(y_data), len(test_y))","ef9a437f":"import keras\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.9, shuffle=True)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, shuffle=True)\n\n# One-Hot encoding for classes\ny_train = np.array(keras.utils.to_categorical(y_train, len(CLASSES)))\ny_test = np.array(keras.utils.to_categorical(y_test, len(CLASSES)))\ny_val = np.array(keras.utils.to_categorical(y_val, len(CLASSES)))\ntest_y=np.array(keras.utils.to_categorical(test_y, len(CLASSES)))","a9045fe5":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, LSTM\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\nfrom keras.utils import np_utils\nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport itertools","a314e143":"model = Sequential()\nmodel.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.20, return_sequences=True,input_shape = (40,1)))\nmodel.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.20, return_sequences=False))\nmodel.add(Dense(len(CLASSES), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['acc','mse', 'mae', 'mape', 'cosine'])\nmodel.summary()","c077cfab":"\nMAX_PATIENT=12\nMAX_EPOCHS=100\nMAX_BATCH=32\n\n\ncallback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=1)]\n\nhistory=model.fit(x_train, y_train, \n                  batch_size=MAX_BATCH, \n                  epochs=MAX_EPOCHS,\n                  verbose=0,\n                  validation_data=(x_val, y_val),\n                  callbacks=callback) \n","14d828b7":"#Plot Keras History\n#Plot loss and accuracy for the training and validation set.\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    plt.figure(figsize=(22,10))\n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    ## Accuracy\n    plt.figure(221, figsize=(20,10))\n    ## Accuracy\n    # plt.figure(2,figsize=(14,5))\n    plt.subplot(221, title='Accuracy')\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(222, title='Loss')\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# plot history\nplot_history(history)","2a0b913f":"# training","0026ae2e":"\n\nClassification of audio sample data. The differences between heart sounds corresponding to different heart symptoms. ","ef67e179":"* samples of audio from training","203da9a0":"### feature extraction in Audio Domain\n\n* https:\/\/www.kadenze.com\/courses\/machine-learning-for-musicians-and-artists\/info SESSION 5\n\n* Mel Frequency Cepstral Coefficient (MFCC) is by far the most successful feature used in the field of Speech Processing.It might work here as well"}}