{"cell_type":{"0dc3f8e6":"code","b3f7f919":"code","5f66c7c5":"code","c1e25018":"code","848fef09":"code","aebc172a":"code","0cd8bae3":"code","7dfae883":"code","58875a28":"code","fd0d4766":"code","a8eecea6":"code","bec192d7":"code","f0132b0c":"code","1489f883":"code","92e63b1b":"markdown","b1d4cbab":"markdown"},"source":{"0dc3f8e6":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport time\nimport gc\nimport sys\nimport torch\nimport re\nimport math\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport shutil\nimport pickle\nfrom functools import partial\nfrom joblib import Parallel, delayed\nfrom operator import itemgetter\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nfrom transformers import PreTrainedModel\nimport transformers\nimport tokenizers\n\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers.modeling_tf_utils import get_initializer\nfrom nltk import sent_tokenize\n\ntqdm.pandas()\nprint(os.listdir(\"..\/input\/\"))","b3f7f919":"def seed_everything(seed=88888):\n    \n    # Python\/TF Seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['XLA_USE_BF16'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n    # Torch Seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(88888)","5f66c7c5":"start_time = time.time()\nprint(\"Loading Model Parameters ...\")\n\n# Model Version\nmodel_version = \"1.0.0\"\n\n# Model Parameters\nn_epochs = 1\nn_splits = 3\nmax_length = 96\ntrain_batch_size = 32\nmodel_seed = 88888\n\n# Model Directories\ndirectory = \"..\/input\/\"\nroberta_directory = directory + \"tf-roberta\/\"\n\n# Model File Paths\ntrain_file = directory + 'tweet-sentiment-extraction\/train.csv'\ncomplete_file = directory + 'complete-tweet-sentiment-extraction-data\/tweet_dataset.csv'\ntest_file = directory + 'tweet-sentiment-extraction\/test.csv'\nsample_file = directory + 'tweet-sentiment-extraction\/sample_submission.csv'\n\n# Model Tokenizer\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = roberta_directory + 'vocab-roberta-base.json', \n    merges_file = roberta_directory + 'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","c1e25018":"start_time = time.time()\nprint(\"Loading Initial Data ...\")\n\ndf_train = pd.read_csv(train_file)\ndf_complete = pd.read_csv(complete_file)\n\ndf_train['text'] = df_train['text'].astype(str).str.strip()\ndf_train['selected_text'] = df_train['selected_text'].astype(str)\ndf_train.fillna('', inplace=True, axis=1)\n\ndf_complete = df_complete[[\"aux_id\", \"text\", \"sentiment\"]]\ndf_complete.columns = [\"textID\", \"text\", \"sentiment_main\"] \ndf_train = pd.merge(df_train, df_complete[[\"textID\", \"sentiment_main\"]], on=\"textID\", how=\"left\")\ndf_train = df_train.dropna(axis=0).reset_index(drop=True)\ndf_train = df_train[[\"textID\", \"text\", \"sentiment_main\", \"sentiment\"]]\n\ndf_secondary = df_complete[~df_complete[\"textID\"].isin(df_train[\"textID\"].values)]\ndf_secondary = df_secondary.dropna(axis=0).reset_index(drop=True)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","848fef09":"df_train","aebc172a":"df_secondary","0cd8bae3":"start_time = time.time()\nprint(\"Preprocess Training Data ...\")\n\nct = df_train.shape[0]\ntarget_mapping = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n\ninput_ids = np.ones((ct, max_length), dtype='int32')\nattention_mask = np.zeros((ct, max_length), dtype='int32')\ntoken_type_ids = np.zeros((ct, max_length), dtype='int32')\n\nfor k in tqdm(range(df_train.shape[0]), total=df_train.shape[0]):\n\n    # Establish Initial Preprocessing\n    text1 = \" \" + \" \".join(df_train.loc[k, 'text'].split())\n    s_tok = tokenizer.encode(df_train.loc[k,'sentiment_main']).ids[0]\n    encoded_ids = [0, s_tok] + tokenizer.encode(text1).ids[0:max_length - 3]\n    input_ids[k, :len(encoded_ids) + 3] = [0, s_tok] + encoded_ids + [2]\n    attention_mask[k, :len(encoded_ids) + 3] = 1\n    \nprint(\"--- %s seconds ---\" % (time.time() - start_time))","7dfae883":"start_time = time.time()\nprint(\"Preprocess Labels ...\")\n\nencoder = OneHotEncoder(sparse=False)\ntarget_values = np.array(df_train[\"sentiment\"].map(target_mapping).values)\ntarget_values = target_values.reshape(len(target_values), 1)\ntarget_values = encoder.fit_transform(target_values)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","58875a28":"start_time = time.time()\nprint(\"Preprocess Secondary Data ...\")\n\nct = df_secondary.shape[0]\ninput_ids_2 = np.ones((ct, max_length), dtype='int32')\nattention_mask_2 = np.zeros((ct, max_length), dtype='int32')\ntoken_type_ids_2 = np.zeros((ct, max_length), dtype='int32')\n\nfor k in tqdm(range(df_secondary.shape[0]), total=df_secondary.shape[0]):\n\n    # Establish Initial Preprocessing\n    text1 = \" \" + \" \".join(df_train.loc[k, 'text'].split())\n    s_tok = tokenizer.encode(df_train.loc[k,'sentiment_main']).ids[0]\n    encoded_ids = [0, s_tok] + tokenizer.encode(text1).ids[0:max_length - 3]\n    input_ids_2[k, :len(encoded_ids) + 3] = [0, s_tok] + encoded_ids + [2]\n    attention_mask_2[k, :len(encoded_ids) + 3] = 1\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","fd0d4766":"start_time = time.time()\nprint(\"Loading Model Pipeline ...\")\n\ndef loss_fn(y_true, y_pred):\n    loss = tf.nn.softmax_cross_entropy_with_logits(\n        y_true, y_pred, axis=-1, name=None\n    )\n    loss = tf.math.reduce_mean(loss)\n    return loss\n\ndef build_model():\n\n    # Establish Inputs\n    input_word_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n    token_type_ids = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(input_word_ids, 1), tf.int32)\n\n    # Pad Inputs by Batch Max Length\n    length = max_length - tf.reduce_sum(padding, -1)\n    max_batch_length = tf.reduce_max(length)\n    input_word_ids_ = input_word_ids[:, :max_batch_length]\n    attention_mask_ = attention_mask[:, :max_batch_length]\n    token_type_ids_ = token_type_ids[:, :max_batch_length]\n\n    # Load Model\n    config = transformers.AutoConfig.from_pretrained(roberta_directory + \"config-roberta-base.json\")\n    config.num_labels = 3\n    transformer = transformers.TFRobertaForSequenceClassification.from_pretrained(roberta_directory + \"pretrained-roberta-base.h5\", config=config)\n    outputs = transformer(input_word_ids_, attention_mask=attention_mask_, token_type_ids=token_type_ids_)[0]\n\n    # Establish Model\n    model = tf.keras.models.Model(inputs=[input_word_ids, attention_mask, token_type_ids], outputs=[outputs])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # Return Model\n    return model\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","a8eecea6":"start_time = time.time()\nprint(\"Loading Metrics Pipeline ...\")\n\ndef calculate_accuracy(oof_model, labels, indices):\n    y_pred, y_true = [], []\n    for k in indices:\n        y_pred.append(np.argmax(oof_model[k,]))\n        y_true.append(np.argmax(labels[k,]))\n    accuracy = accuracy_score(y_pred, y_true)\n    return accuracy\n\ndef generate_output(oof_model, indices):\n    outputs = []\n    for k in indices:\n        outputs.append(np.argmax(oof_model[k,]))\n    return outputs\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","bec192d7":"start_time = time.time()\nprint(\"Training Model ...\")\n\n# Establish Variables\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=model_seed)\nsplits = skf.split(input_ids, df_train.sentiment.values)\n\n# Establish Outputs\noof_model = np.zeros((input_ids.shape[0], 3))\npreds_model = np.zeros((input_ids_2.shape[0], 3))\n\n# Train Models on Individual Folds\nfor fold, (idxT, idxV) in tqdm(enumerate(splits), total=n_splits):\n    print(\"Currently Training Fold: {}\".format(fold))\n\n    # Build Model, Inputs\n    K.clear_session()\n    model = build_model()\n\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [target_values[idxT,]]\n    inpV = [input_ids[idxV,], attention_mask[idxV,], token_type_ids[idxV,]]\n    targetV = [target_values[idxV,]]\n\n    # Train Established Model\n    for epoch in tqdm(range(1, n_epochs + 1), total=n_epochs):\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, \n            batch_size=train_batch_size, verbose=True, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)\n\n        oof_model[idxV,] = model.predict([input_ids[idxV,], attention_mask[idxV,], token_type_ids[idxV,]], verbose=True)\n        current_accuracy = calculate_accuracy(oof_model, target_values, idxV)\n        print('Current Epoch (Accuracy [{}]): {}'.format(0, current_accuracy))\n\n    preds_model += model.predict([input_ids_2, attention_mask_2, token_type_ids_2], verbose=True)\n    \nindices = range(0, df_train.shape[0])\ncurrent_accuracy = calculate_accuracy(oof_model, target_values, indices)\nprint('Final (Accuracy [{}]): {}'.format(0, current_accuracy))\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","f0132b0c":"start_time = time.time()\nprint(\"Finalizing Sentiment ...\")\n\nindices = range(0, df_secondary.shape[0])\ntarget_mapping = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n\nsentiment_output = np.array(generate_output(preds_model, indices)).reshape(-1, 1)\ndf_secondary[\"sentiment\"] = sentiment_output\ndf_secondary[\"sentiment\"] = df_secondary[\"sentiment\"].map(target_mapping)\ndf_secondary.to_csv('tweet_sentiment.csv', index=False)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","1489f883":"df_secondary","92e63b1b":"# Predicting Sentiment on the Full Dataset\nWhen I first found this competition, I came across @jonathanbesomi's notebook (https:\/\/www.kaggle.com\/jonathanbesomi\/private-test-not-that-private-afterall\/) discussing and exploring the dataset that this competition's data was derived from. Looking through it, the option to use pseudolabelling was something that definitely jumped out at me, although the main problem in doing so is that we're not given the same sentiment labels as those in Kaggle's dataset. As such, this notebook highlights one method to map the sentiment labels found in the original dataset to those in the data for this competition.\n\nSadly, I wasn't able to put too many hours towards the competition, but I hope that this method helps someone. Pseudolabelling resulting from these mapped sentiments was able to improve my CV\/LB score from 0.713\/0.709 -> 0.715\/0.711. Further testing showed that it seems to reliably provide a ~0.002 boost in both CV and LB","b1d4cbab":"# Output Secondary File\nThis output file consists of all the rows in the complete dataset not present in Kaggle's training set with sentiment labelled. In order to use it for pseudolabels, simply substitute it for Kaggle's test set and predict 'selected_text' for each row."}}