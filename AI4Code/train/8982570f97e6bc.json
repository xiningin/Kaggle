{"cell_type":{"3069bbb6":"code","3aa24a27":"code","ea728043":"code","c21b0ab6":"code","04c19ab5":"code","8286397f":"code","d96e518e":"code","86dd58d4":"code","ca13734b":"code","d972c8c6":"code","45e33531":"code","2d321f28":"code","0011858e":"code","1f17037b":"code","7ef2a454":"code","4748ad34":"code","38b6f9d7":"code","5a7f0ad0":"code","f6aa62d2":"code","6b44d137":"code","a9efa4f2":"code","3c10d201":"code","16790652":"code","42e89ddf":"code","a550b048":"code","ccc08e59":"code","e4e3318d":"code","32056fea":"code","2f64f456":"code","22f8c16a":"code","0557eb65":"code","2ce670d4":"code","a4d7a898":"code","e5afc026":"code","ecc10320":"code","9b6285e7":"code","1069e3f1":"markdown","faf3409d":"markdown","5a7aff4c":"markdown","0072c7d6":"markdown","e1fcd461":"markdown","fe7757ed":"markdown","a313943c":"markdown","2f90ddc6":"markdown","20d9ae86":"markdown","492aa79d":"markdown","a151f444":"markdown","95d3cb34":"markdown","27940b7c":"markdown","3b3e47bb":"markdown","70e31fd5":"markdown","17e07755":"markdown","9e834a31":"markdown","1539ea3b":"markdown","bddfa99e":"markdown","a89d9cc2":"markdown","6db1ac05":"markdown","ab52e6bc":"markdown","4ce252b5":"markdown","2d9531d5":"markdown","f1be60f4":"markdown","67114d07":"markdown","5907ffed":"markdown","940f04e2":"markdown","dd558af7":"markdown"},"source":{"3069bbb6":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier  \nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3aa24a27":"data = pd.read_csv('..\/input\/credit-card\/application_data.csv')\ndata1 = data.copy()\ndata.head()","ea728043":"data.info()","c21b0ab6":"def reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('object')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\ndata = reduce_mem_usage(data)","04c19ab5":"print(\"Object type values:\",np.count_nonzero(data.select_dtypes('object').columns))\nprint(\"___________________________________________________________________________________________\")\nprint(data.select_dtypes('object').columns)\nprint(\"___________________________________________________________________________________________\")","8286397f":"le = LabelEncoder()\ndata['NAME_CONTRACT_TYPE'] = le.fit_transform(data['NAME_CONTRACT_TYPE'])\ndata['CODE_GENDER'] = le.fit_transform(data['CODE_GENDER'])\ndata['FLAG_OWN_CAR'] = le.fit_transform(data['FLAG_OWN_CAR'])\ndata['FLAG_OWN_REALTY'] = le.fit_transform(data['FLAG_OWN_REALTY'])\ndata['NAME_TYPE_SUITE'] = le.fit_transform(data['NAME_TYPE_SUITE'].astype(str))\ndata['NAME_INCOME_TYPE'] = le.fit_transform(data['NAME_INCOME_TYPE'])\ndata['NAME_EDUCATION_TYPE'] = le.fit_transform(data['NAME_EDUCATION_TYPE'])\ndata['NAME_FAMILY_STATUS'] = le.fit_transform(data['NAME_FAMILY_STATUS'])\ndata['NAME_HOUSING_TYPE'] = le.fit_transform(data['NAME_HOUSING_TYPE'])\ndata['OCCUPATION_TYPE'] = le.fit_transform(data['OCCUPATION_TYPE'].astype(str))\ndata['WEEKDAY_APPR_PROCESS_START'] = le.fit_transform(data['WEEKDAY_APPR_PROCESS_START'])\ndata['ORGANIZATION_TYPE'] = le.fit_transform(data['ORGANIZATION_TYPE'])\ndata['FONDKAPREMONT_MODE'] = le.fit_transform(data['FONDKAPREMONT_MODE'].astype(str))\ndata['HOUSETYPE_MODE'] = le.fit_transform(data['HOUSETYPE_MODE'].astype(str))\ndata['WALLSMATERIAL_MODE'] = le.fit_transform(data['WALLSMATERIAL_MODE'].astype(str))\ndata['EMERGENCYSTATE_MODE'] = le.fit_transform(data['EMERGENCYSTATE_MODE'].astype(str))","d96e518e":"def colors(value):\n    if value > 50 and value < 100:\n        color = 'red'\n    elif value > 154000 and value < 250000:\n        color = 'red'\n    elif value == 1 :\n        color = 'blue'\n    else:\n        color = 'green'\n    return 'color: %s' % color\n\ndef missing(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    total = total[total>0]\n    percent = df.isnull().sum().sort_values(ascending = False)\/len(df)*100\n    percent = percent[percent>0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percentage']).style.applymap(colors)\nmissing(data1.select_dtypes('object'))","86dd58d4":"def mode_impute(df,col):\n    return df[col].fillna(df[col].mode()[0])\ndata1['FONDKAPREMONT_MODE'] = mode_impute(data1,'FONDKAPREMONT_MODE')\ndata1['WALLSMATERIAL_MODE'] = mode_impute(data1,'WALLSMATERIAL_MODE')\ndata1['HOUSETYPE_MODE'] = mode_impute(data1,'HOUSETYPE_MODE')\ndata1['EMERGENCYSTATE_MODE'] = mode_impute(data1,'EMERGENCYSTATE_MODE')\ndata1['OCCUPATION_TYPE'] = mode_impute(data1,'OCCUPATION_TYPE')\ndata1['NAME_TYPE_SUITE'] = mode_impute(data1,'NAME_TYPE_SUITE')\nmissing(data1.select_dtypes('object'))","ca13734b":"data1.describe(include=['O'])","d972c8c6":"print(\"___________________________________________________________________________________________\")\nprint(\"Int type values:\",np.count_nonzero(data1.select_dtypes('int').columns))\nprint(data.select_dtypes('int').columns)\nprint(\"___________________________________________________________________________________________\")","45e33531":"missing(data1.select_dtypes('int'))","2d321f28":"data1.select_dtypes('int').agg(['count','min', 'max','mad','mean','median','quantile','kurt','skew','var','std'])","0011858e":"plt.figure(figsize=(30,5))\nsns.boxplot(data=data1.select_dtypes('int'))\nplt.show()","1f17037b":"data1.select_dtypes('int').hist(figsize=(25,25), ec='w')\nplt.show()","7ef2a454":"def color_(value):\n    if value < 0 :\n        color = 'red'\n    elif value == 1 :\n        color = 'blue'\n    else:\n        color = 'green'\n    return 'color: %s' % color\ndata1.select_dtypes('int').corr().style.applymap(color_)\n","4748ad34":"data1.select_dtypes('int').cov().style.applymap(color_)","38b6f9d7":"print(\"___________________________________________________________________________________________\")\nprint(\"float type values:\",np.count_nonzero(data1.select_dtypes('float').columns))\nprint(data1.select_dtypes('float').columns)\nprint(\"___________________________________________________________________________________________\")","5a7f0ad0":"missing(data1.select_dtypes('float'))","f6aa62d2":"data1 = data1.select_dtypes('float').interpolate(method ='linear', limit_direction ='forward')\nmissing(data1.select_dtypes('float'))","6b44d137":"data1 = data1.dropna(axis = 1)\nmissing(data1)","a9efa4f2":"plt.figure(figsize=(30,5))\nsns.boxplot(data=data1.select_dtypes('float'))\nplt.show()","3c10d201":"data1.select_dtypes('float').hist(figsize=(25,25), ec='w')\nplt.show()","16790652":"def color_(value):\n    if value < 0 :\n        color = 'red'\n    elif value == 1 :\n        color = 'blue'\n    else:\n        color = 'green'\n    return 'color: %s' % color\ndata1.select_dtypes('float').corr().style.applymap(color_)\n","42e89ddf":"data1.select_dtypes('float').cov().style.applymap(color_)","a550b048":"data = data.interpolate(method ='linear', limit_direction ='forward')\ndata = data.dropna(axis = 1)\nmissing(data)","ccc08e59":"corr = data.corrwith(data['TARGET'],method='spearman').reset_index()\n\ncorr.columns = ['Index','Correlations']\ncorr = corr.set_index('Index')\ncorr = corr.sort_values(by=['Correlations'], ascending = False).head(10)\n\nplt.figure(figsize=(10, 15))\nfig = sns.heatmap(corr, annot=True, fmt=\"g\", cmap='Set3', linewidths=0.4, linecolor='green')\n\nplt.title(\"Correlation of Variables with Class\", fontsize=20)\nplt.show()","e4e3318d":"X = data.drop(['TARGET'],axis = 1)\ntarget = data['TARGET']\nX_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size= 0.3, random_state = 0)","32056fea":"def ml_model(X_train,X_test, Y_train, Y_test):\n  MLA = [LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier(),GaussianNB()]\n  MLA_columns = []\n  MLA_compare = pd.DataFrame(columns = MLA_columns)\n  row_index = 0\n  for alg in MLA:\n    predicted = alg.fit(X_train, Y_train).predict(X_test)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, Y_train), 2)\n    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, Y_test), 2)\n    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(Y_test, predicted),2)\n    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(Y_test, predicted),2)\n    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(Y_test, predicted),2)\n    row_index+=1\n  MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \n  return MLA_compare  \nml_model(X_train,X_test, Y_train, Y_test)","2f64f456":"from sklearn.feature_selection import SelectKBest,mutual_info_classif\nbestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(X,target,)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns) \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score'] \nprint(featureScores.nlargest(10,'Score'))  ","22f8c16a":"X = data[['FLAG_CONT_MOBILE','FLAG_MOBIL','FLAG_EMP_PHONE','NAME_TYPE_SUITE','NAME_EDUCATION_TYPE','NAME_HOUSING_TYPE','REGION_RATING_CLIENT_W_CITY','REGION_RATING_CLIENT',\n         'FLAG_DOCUMENT_3','FLAG_OWN_REALTY']]\nX_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size= 0.3, random_state = 0)\nFeature_selection = ml_model(X_train,X_test, Y_train, Y_test)\nFeature_selection","0557eb65":"print('before Oversampling:',Counter(Y_train))\noversample = RandomOverSampler(sampling_strategy='minority')\nX_train1, Y_train1 = oversample.fit_resample(X_train, Y_train)\nprint('After Oversampling:',Counter(Y_train1))","2ce670d4":"print(Counter(target))\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10,4.5))\nfig.subplots_adjust(bottom=0.10, left=0.10, top = 0.900, right=1.00)\nfig.suptitle(' Target Class Before and After Over Sampling', fontsize = 20)\nsns.set_palette(\"bright\")\nsns.countplot(Y_train, ax=ax1)\nax1.margins(0.1)\nax1.set_facecolor(\"#e1ddbf\")\nfor p in ax1.patches:\n        ax1.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nsns.set_palette(\"bright\")\nsns.countplot(Y_train1, ax=ax2)\nax2.margins(0.1)\nax2.set_facecolor(\"#e1ddbf\")\nfor p in ax2.patches:\n        ax2.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nsns.set_style('dark')","a4d7a898":"oversampling = ml_model(X_train1,X_test, Y_train1, Y_test)\noversampling","e5afc026":"print('before SMOTE:',Counter(Y_train))\nsm = SMOTE(sampling_strategy='minority')\nX_train2, Y_train2 = sm.fit_resample(X_train, Y_train)\nprint('After SMOTE:',Counter(Y_train2))","ecc10320":"Smote = ml_model(X_train2,X_test, Y_train2, Y_test)\nSmote","9b6285e7":"def ensemble_model(X_train,X_test, Y_train, Y_test):\n  class_weight={0:0.087, 1:1}\n  MLA = [CatBoostClassifier(iterations=200, learning_rate=0.3,random_seed=42,task_type=\"GPU\",verbose=False),XGBClassifier(n_estimators=200,learning_rate=0.1,\n  tree_method = 'gpu_hist',random_state=42),LGBMClassifier(n_estimators=200,is_unbalance=True,learning_rate=0.1,\n     class_weight=class_weight,num_leaves=200,device='gpu',random_state=42,n_jobs=-1)]\n  MLA_columns = []\n  MLA_compare = pd.DataFrame(columns = MLA_columns)\n  row_index = 0\n  for alg in MLA:\n    predicted = alg.fit(X_train, Y_train).predict(X_test)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, Y_train), 2)\n    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, Y_test), 2)\n    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(Y_test, predicted),2)\n    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(Y_test, predicted),2)\n    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(Y_test, predicted),2)\n    row_index+=1\n  MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \n  return MLA_compare  \nensemble_model(X_train,X_test, Y_train, Y_test)","1069e3f1":"<div align='left'><font size=\"5\" color=\"#A52A2A\"> Integer Type Value<\/font><\/div>  ","faf3409d":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Missing Value detection<\/font><\/div>       ","5a7aff4c":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Missing value Imputation : Mode<\/font><\/div> ","0072c7d6":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Checking Distribution of variables using Histogram<\/font><\/div>        ","e1fcd461":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Outlier Detection<\/font><\/div>        ","fe7757ed":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Descriptive Statistics<\/font><\/div> ","a313943c":"<a id = \"Imbalance\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Data Imbalance-Over Sampling<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> One of the common issues found in datasets that are used for classification is imbalanced classes issue. Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed. It usually reflects an unequal distribution of classes within a dataset.<\/font><\/div>   \n> <img style=\"float: centre;\" src=\"https:\/\/miro.medium.com\/max\/364\/1*QoW_njAnS3D0QWve7NNB8w.png\" width=\"350px\"\/>\n<div align='left'><font size=\"3\" color=\"#000000\"> If there are two classes, then balanced data would mean 50% points for each of the class.In our case 50:1 ratio between the fraud and non-fraud classes which is highly imbalance. There are 3 major techniques are there to eliminate the imbalance problem. \n<\/font><\/div>  \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1.Random Over Sampling<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\"> 2.Random Under Sampling<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">3.SMOTE(Synthetic Minority Over-sampling Technique).<\/font><\/div>   \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Since under sampling may discard the useful information which could be important for building good classifiers, we choose random over sampling and SMOTE (Synthetic Minority Oversampling Technique). Random over sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. Implementing Random over sampling on this  dataset helps to the balance the labels (more no fraud than fraud transactions).<\/font><\/div> \n\n   ","2f90ddc6":"<a id = \"table_of_contents\"><\/a>\n\n# Table of contents\n\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">1. Importing the necessary libraries<\/font><\/div>](#Imports)\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">2. Reading the datasets<\/font><\/div>](#reading)\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">3. Basic EDA<\/font><\/div>](#eda)\n- <div align='left'><font size=\"3\" color=\"#1E90FF\">3.1 Object datatype<\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#1E90FF\">3.1.1 Label encoding<\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#1E90FF\">3.1.2 Missing value Detection and treatment<\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#1E90FF\">3.1.3 Descriptive Statistics<\/font><\/div>\n        \n- <div align='left'><font size=\"3.5\" color=\"#1E90FF\">3.2 Integer Type<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.2.1 Missing value Detection and treatment<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.2.2 Descriptive Statistics<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.2.3 Outler Detection<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.2.4 Checking Distribution<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.2.5 Relation between Variables<\/font><\/div>\n         \n- <div align='left'><font size=\"3\" color=\"#1E90FF\">3.3 Float Type<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.3.1 Missing value Detection and treatment<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.3.2 Descriptive Statistics<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.3.3 Outler Detection<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.3.4 Checking Distribution<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#1E90FF\"> 3.3.5 Relation between Variables<\/font><\/div>\n         \n[<div align='left'><font size=\"3\" color=\"#1E90FF\">4. Buiding a Classification model<\/font><\/div>](#model)\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">5. Feature Selection<\/font><\/div>](#FS)\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">6. Data Imbalance Problem<\/font><\/div>](#Imbalance)\n- <div align='left'><font size=\"3\" color=\"#1E90FF\">6.1 Random Over Sampling <\/font><\/div>\n- <div align='left'><font size=\"3\" color=\"#1E90FF\">6.2 SMOTE <\/font><\/div>\n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">7. Ensembling <\/font><\/div>](#Ensembling) \n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">8. Conclusion<\/font><\/div>](#Conclusion) \n\n[<div align='left'><font size=\"3\" color=\"#1E90FF\">9. Reference<\/font><\/div>](#References) \n","20d9ae86":"<a id = \"Imports\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Loading Libraries<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)\n\n","492aa79d":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Outlier Detection<\/font><\/div>    ","a151f444":"<div align='left'><font size=\"3\" color=\"#000000\"> After applying Random over sampling and SMOTE, there is a drastic decrease in the accuracy of the model. Accuracy is not the best metric to use when evaluating imbalanced datasets as it can be misleading instead use f1-score, precision\/recall score or confusion matrix. After applying SMOTE we observe precision\/recall of minority class increases. <\/font><\/div> \n<hr>\n\n<a id = \"Ensembling\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Ensembling<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> In order to increase the performance of the model we use ensemble techniques. Here we use 3 ensembling models.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">1.RandomForestClassifier<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\"> 2.BaggingClassifier<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">3.AdaBoostClassifier<\/font><\/div> ","95d3cb34":"<div align='left'><font size=\"3\" color=\"#000000\"> SMOTE is a technique that generates new observations by interpolating between observations in the original dataset.Implementing SMOTE on our imbalanced dataset helped us with the imbalance of our labels (more no fraud than fraud transactions).<\/font><\/div> ","27940b7c":"<div align='center'><font size=\"7\" color=\"#DAA520\">Credit Card Fraud Detection<\/font><\/div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#B8860B\"> Feature Selection-Over Sampling techniques on dataset<\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> The Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be fraud. This model is then used to identify whether a new transaction is fraudulent or not.<\/font><\/div>\n\n> <img style=\"float: centre;\" src=\"https:\/\/ai-journey.com\/wp-content\/uploads\/2019\/06\/fraud-EMV-chip-credit-card.jpg\" width=\"450px\"\/>\n\n<div align='left'><font size=\"3.5\" color=\"#00BFFF\"> About Dataset:<\/font><\/div>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> This data set is uploaded in order to get the insights of Credit card Defaultees based on the respective attributes.Dataset contains 307510 rows and 122 columns including target columns. Here our aim is to clean the dataset and find some good insight from it.<\/font><\/div>\n<hr>\n\n","3b3e47bb":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Descriptive Statistics<\/font><\/div>   ","70e31fd5":"<a id = \"reading\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Reading Dataset<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)\n","17e07755":"<div align='left'><font size=\"5\" color=\"#A52A2A\"> Float Type Value<\/font><\/div>      ","9e834a31":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Relation Between Variables<\/font><\/div>         ","1539ea3b":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Missing Value detection<\/font><\/div>  ","bddfa99e":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Relation Between Variables<\/font><\/div>     ","a89d9cc2":"<a id = \"FS\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Selecting 10 best features<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Here we use Kbest select methodto find out the top 10 best features.<\/font><\/div> ","6db1ac05":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Label Encoding<\/font><\/div>","ab52e6bc":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Object type values<\/font><\/div>","4ce252b5":"<a id = \"References\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> References<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/#:~:text=Imbalanced%20classification%20is%20the%20problem,and%20may%20require%20specialized%20techniques.\" target=\"_blank\">1. A Gentle Introduction to Imbalanced Classification<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-data-classification\" target=\"_blank\">2. Imbalanced Data : How to handle Imbalanced Classification Problems<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/feature-selection-techniques-in-machine-learning\" target=\"_blank\">3. Feature Selection Techniques in Machine Learning<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/towardsdatascience.com\/sampling-techniques-for-extremely-imbalanced-data-part-ii-over-sampling-d61b43bc4879\" target=\"_blank\">4. Using Over-Sampling Techniques for Extremely Imbalanced Data<\/a><\/div>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\" target=\"_blank\">5. A Comprehensive Guide to Ensemble Learning<\/a><\/div>\n<hr>\n<div align='center'><font size=\"7\" color=\"#808080\"> If you like my work kindly upvote \ud83d\udc4d.Thank You<\/font><\/div>\n","2d9531d5":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Checking Distribution of variables using Histogram<\/font><\/div>     ","f1be60f4":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Missing Value Detection<\/font><\/div>","67114d07":"<a id = \"eda\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Exploratory Data Analysis<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)\n\n\n<div align='left'><font size=\"3\" color=\"#000000\"> As the dataset is too large, we devide the dataset into 3 part (object,integer,float) based on its datatype and analyze it.<\/font><\/div> ","5907ffed":"<a id = \"model\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Building Classification Model<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> After the EDA, we build 3 baseline classification model and compare which model gives better accuracy.<\/font><\/div>   \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1. Logistic Regression<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">2. K-Nearest Classifier<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">3. Decision Tree<\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">4. Naive Bayes<\/font><\/div>  ","940f04e2":"<div align='left'><font size=\"4\" color=\"#A52A2A\"> Missing Value Imputation<\/font><\/div>       ","dd558af7":"<a id = \"Conclusion\"><\/a>\n<div align='left'><font size=\"5\" color=\"#A52A2A\"> Conclusion<\/font><\/div>\n<hr>\n\n[Go back to the Table of Contents](#table_of_contents)  \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\">1. EDA helpful to understand the overview of the dataset. <\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">2. Feature Selection reduce the training time and do not effect of model performance.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">3. SMOTE reduce accuracy of the model but it increase the overall performance of the dataset.<\/font>\n<div align='left'><font size=\"3\" color=\"#000000\">4. In 3 baseline classification model dicision tree perform better compare to other two models.<\/font>\n<div align='left'><font size=\"3\" color=\"#000000\">5. Ensembling learning techniques increase the overall performance<\/font>"}}