{"cell_type":{"90702541":"code","0e19fb80":"code","db4650fb":"code","0dbdcd7e":"code","2d734747":"code","d748bbbc":"code","06051344":"code","93f6e661":"code","4e2cf6a5":"code","135254e9":"code","7cbfc3ff":"code","dd57fbd8":"code","7e5d82ab":"code","3673dce3":"code","be5b05ca":"code","d9ced007":"code","725c1440":"code","d8e2d8ca":"code","f2c333fd":"code","2d44da94":"code","dc922eae":"code","3189930b":"code","685f65b5":"code","2b1bd83a":"code","78fa49fe":"code","6bba10b7":"code","3f35443a":"code","d1703535":"code","a35c9940":"code","a2533e48":"code","0f4e794b":"code","d7b58cc5":"code","9d6077b2":"code","fe730e02":"code","0d32a30f":"code","af12d4e4":"code","a695b789":"code","0f4073e9":"code","12a6cfba":"code","046beb16":"code","7894dc89":"code","5c7d4b73":"code","bfc3050f":"code","664d21da":"code","c694063f":"code","84fd9b19":"code","4b974f78":"code","d09c0f6a":"code","e4a8038a":"code","8ba2932e":"code","c8fdc2c4":"code","95e22159":"code","d30214c7":"code","9c375c47":"code","d04df865":"code","069bdab9":"code","ba70e794":"code","31700db5":"code","6d304e29":"code","394ba9d8":"code","278d5344":"code","58070a9d":"code","df949b20":"code","a7685851":"code","83b96679":"code","028fe5e2":"code","c5b9fa42":"code","e6071351":"code","fe0b692a":"code","81b7a515":"code","5ec63faf":"code","00c7f76a":"code","9bab04f3":"code","3f953828":"code","bea013fc":"code","75ac756c":"markdown","9d703581":"markdown","64e00090":"markdown","c1519dbd":"markdown","fcf268b7":"markdown","b983369b":"markdown","aca917ea":"markdown","e746e360":"markdown","19bd5697":"markdown","967f77f4":"markdown","ae79ccdf":"markdown","b457b14c":"markdown","04d86e68":"markdown","18aabf61":"markdown","c4cfacd2":"markdown","8c6df06d":"markdown","2f6efa16":"markdown","c4311f03":"markdown","aa3238e7":"markdown","667f3c43":"markdown","af77522c":"markdown","186b3446":"markdown","3fc800ab":"markdown","678a3776":"markdown","567f5fe9":"markdown","589e019c":"markdown","935d876c":"markdown","dd0eb463":"markdown","53ca3dc5":"markdown","8fc22add":"markdown","a91843de":"markdown","6dd1aa1d":"markdown","1e636e8e":"markdown","f1068533":"markdown","e0e70ca2":"markdown","bfbd84e3":"markdown","6d14255b":"markdown","fb08368a":"markdown","b00176b2":"markdown","06c79fbb":"markdown","ee8e60bd":"markdown","cba5d8fa":"markdown","168ed559":"markdown","4846d7a0":"markdown","00b53e0d":"markdown","1973dbe7":"markdown","d7d7450a":"markdown","09ba11ff":"markdown","9ddc094c":"markdown","3f337b5a":"markdown","b54a0c1e":"markdown","5c7677fe":"markdown","025a94a0":"markdown","8936d9ce":"markdown","3cf701bb":"markdown","b5f5b388":"markdown"},"source":{"90702541":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0e19fb80":"hr_data=pd.read_csv(\"..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","db4650fb":"hr_data.head(5)","0dbdcd7e":"hr_data.info()","2d734747":"front =hr_data['Attrition']\nhr_data.drop(labels=['Attrition'], axis=1,inplace = True)\nhr_data.insert(0, 'Attrition', front)\nhr_data.head()","d748bbbc":"hr_data.shape","06051344":"attrition_rate = hr_data.Attrition.value_counts() \/ 1470\nattrition_rate","93f6e661":"AttritionSummery= hr_data.groupby('Attrition')","4e2cf6a5":"AttritionSummery.mean()","135254e9":"correlation = hr_data.corr()\ncorrelation","7cbfc3ff":"f,ax =  plt.subplots(figsize=(20,20))\nsns.heatmap(hr_data.corr(),annot=True,linewidth=.4,ax=ax,fmt='.1f',cmap=\"Paired\")","dd57fbd8":"pd.set_option('display.max_rows',None)\ndef corrank(hr_data):\n        import itertools\n        df = pd.DataFrame([[(i,j),hr_data.corr().loc[i,j]] for i,j in list(itertools.combinations(hr_data.corr(), 2))],columns=['pairs','corr'])    \n        print(df.sort_values(by='corr',ascending=False))\n\nprint(corrank(hr_data))\n\n# prints a descending list of correlation pair (Max on top)","7e5d82ab":"hr_data['Attrition'] = hr_data['Attrition'].map({'No':0,'Yes':1})","3673dce3":"# Let's compare the means of our employee Attrition satisfaction against the employee population satisfaction\npopulation_satisfaction = hr_data['JobSatisfaction'].mean()\nleft_satisfaction = hr_data[hr_data['Attrition']==1]['JobSatisfaction'].mean()\n\nprint( 'The mean for the employee population is: ' + str(population_satisfaction) )\nprint( 'The mean for the employees that had a Attrition is: ' + str(left_satisfaction) )","be5b05ca":" import scipy.stats as stats\n stats.ttest_1samp(a=  hr_data[hr_data['Attrition']==1]['JobSatisfaction'], # Sample of Employee satisfaction who had a Turnover\n                      popmean = emp_population_satisfaction)  # Employee Population satisfaction mean","d9ced007":"degree_freedom = len(hr_data[hr_data['Attrition']==1])\n\nLQ = stats.t.ppf(0.025,degree_freedom)  # Left Quartile\n\nRQ = stats.t.ppf(0.975,degree_freedom)  # Right Quartile\n\nprint ('The t-distribution left quartile range is: ' + str(LQ))\nprint ('The t-distribution right quartile range is: ' + str(RQ))","725c1440":"categorical_col = []\nfor column in hr_data.columns:\n    if hr_data[column].dtype == object and len(hr_data[column].unique()) <= 50:\n        categorical_col.append(column)\n        print(f\"{column} : {hr_data[column].unique()}\")\n        print(\"====================================\")","d8e2d8ca":"hr_data.columns\n","f2c333fd":"plt.figure(figsize=(14,6))\nsns.countplot(hr_data.Age,color='hotpink')","2d44da94":"sns.factorplot(data=hr_data,y='Age',x='Attrition',size=6,aspect=1,kind='box',palette='winter')","dc922eae":"f,ax = plt.subplots(figsize = (12,10))\nsns.boxplot(x=\"Gender\",y=\"Age\",hue=\"BusinessTravel\",data=hr_data,palette=\"hls\")","3189930b":"sns.factorplot(data=hr_data,x='BusinessTravel',y='Attrition',size=6,aspect=1,kind='bar',palette='winter')","685f65b5":"sns.jointplot(hr_data.MonthlyIncome ,hr_data.Age,size=8, kind = \"scatter\")   \nplt.show()","2b1bd83a":"sns.catplot(x=\"Attrition\", y=\"MonthlyIncome\", data=hr_data,hue='Gender',size=7)","78fa49fe":"plt.figure(figsize=(13,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 0, 'MonthlyIncome'], label = 'Active Employee',color='olive')\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 1, 'MonthlyIncome'], label = 'Employee Left',color='maroon')\nplt.xlabel('Monthly Income')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Monthly Income in Percent by Attrition Status');","6bba10b7":"#Distribution of Years at company\nplt.figure(figsize=(8,8))\nsns.distplot(hr_data[\"YearsAtCompany\"].astype(int),color='lime', kde=False);","3f35443a":"plt.figure(figsize=(13,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 0, 'YearsAtCompany'], label = 'Active Employee',color='orangered')\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 1, 'YearsAtCompany'], label = 'Employees Left',color='mediumblue')\nplt.xlabel('YearsAtCompany')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years At Company in Percent by Attrition');","d1703535":"plt.figure(figsize=(13,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 0, 'TotalWorkingYears'], label = 'Active Employee',color='cyan')\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 1, 'TotalWorkingYears'], label = 'Ex-Employees',color='limegreen')\nplt.xlabel('TotalWorkingYears')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Total Working Years in Percent by Attrition Status');","a35c9940":"plt.figure(figsize=(13,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 0, 'YearsWithCurrManager'], label = 'Active Employee',color='fuchsia')\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 1, 'YearsWithCurrManager'], label = 'Ex-Employees',color='darkblue')\nplt.xlabel('YearsWithCurrManager')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years With Curr Manager in Percent by Attrition Status');","a2533e48":"plt.figure(figsize=(13,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 0, 'PercentSalaryHike'], label = 'Active Employee',color='deeppink')\nsns.kdeplot(hr_data.loc[hr_data['Attrition'] == 1, 'PercentSalaryHike'], label = 'Employees Left',color='darkgreen')\nplt.xlabel('PercentSalaryHike')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Percent Salary Hike in Percent by Attrition Status');","0f4e794b":"sns.factorplot(data=hr_data,x='Department',y='Attrition',size=7,aspect=1,kind='bar',palette='cubehelix')","d7b58cc5":"plt.figure(figsize=(13,6))\nax = sns.countplot(data=hr_data,x='Education',hue='Gender',palette='Purples')\nax.set_xticklabels([ '1-Below College' , '2-College' , '3-Bachelor' ,'4-Master',  '5-Doctor'])\nplt.show()","9d6077b2":"ax=sns.factorplot(data=hr_data,x='Education',y='Attrition',size=7,aspect=1,kind='bar',palette=\"cubehelix\")\nax.set_xticklabels([ '1-Below College' , '2-College' , '3-Bachelor' ,'4-Master',  '5-Doctor'])","fe730e02":"labels=hr_data.EducationField.value_counts().index\nsizes=hr_data.EducationField.value_counts().values\nplt.figure(figsize=(7,7))\nplt.pie(sizes,labels=labels,colors=[\"deepskyblue\",\"darkorchid\",\"hotpink\",\"cyan\",\"tomato\",\"lime\"],autopct=\"%1.1f%%\")\nplt.title(\"Education Field Counts\",fontsize=18,color='maroon')","0d32a30f":"x=sns.factorplot(data=hr_data,x='EducationField',y='Attrition',size=7,aspect=1,kind='bar')\n","af12d4e4":"plt.figure(figsize=(10,6))\nax = sns.countplot(data=hr_data,x='EnvironmentSatisfaction',hue='Gender',palette='bright')\nax.set_xticklabels([ '1-Low' , '2-Medium' , '3-High' , '4-Very High'])\nplt.show()","a695b789":"ax=sns.factorplot(data=hr_data,x='EnvironmentSatisfaction',y='Attrition',size=7,aspect=1,kind='bar',color='lightskyblue')\nax.set_xticklabels([ '1-Low' , '2-Medium' , '3-High' , '4-Very High'])","0f4073e9":"plt.figure(figsize=(12,6))\nax = sns.countplot(data=hr_data,x='JobSatisfaction',hue='Gender',palette='Accent')\nax.set_xticklabels([ '1-Low' , '2-Medium' , '3-High' , '4-Very High'])\nplt.show()","12a6cfba":"plt.figure(figsize=(10,6))\nax=sns.violinplot(data=hr_data,x='JobSatisfaction',y='Attrition');\nax.set_xticklabels([ '1-Low' , '2-Medium' , '3-High' , '4-Very High'])","046beb16":"plt.figure(figsize=(12,6))\nax = sns.countplot(data=hr_data,x='PerformanceRating',hue='Gender',palette='Set2')\n#ax.set_xticklabels( ['1-Low' , '2-Good','3-Excellent' , '4-Outstanding'])\nplt.show()","7894dc89":"g=sns.factorplot(data=hr_data,x='PerformanceRating',y='Attrition',size=6,aspect=1,kind='violin')","5c7d4b73":"plt.figure(figsize=(12,6))\nax = sns.countplot(data=hr_data,x='WorkLifeBalance',hue='Gender',palette='Set1')\nax.set_xticklabels([ '1-Bad', '2-Good', '3-Better', '4-Best'])\nplt.show()","bfc3050f":"ax=sns.factorplot(data=hr_data,x='WorkLifeBalance',y='Attrition',size=7,aspect=1,kind='bar')\nax.set_xticklabels([ '1-Bad', '2-Good', '3-Better', '4-Best'])","664d21da":"income=pd.DataFrame(hr_data.groupby(\"JobRole\").MonthlyIncome.mean().sort_values(ascending=False))","c694063f":"\n    fig =plt.figure(figsize=(13,8))\n    ax=sns.barplot(x=income.index,y=income.MonthlyIncome)\n    plt.xticks(rotation=90)\n    plt.xlabel(\"Job Roles\")\n    plt.ylabel(\"Monthly Income\")\n    plt.title(\"Job Roles with Monthly Income\")\nplt.show()","84fd9b19":"g = sns.pairplot(hr_data, vars=[\"MonthlyIncome\", \"MonthlyRate\"],hue=\"Department\",size=5)","4b974f78":"sns.factorplot(data=hr_data,y='Attrition',x='DistanceFromHome',size=7,aspect=1,kind='bar')","d09c0f6a":"plt.figure(figsize=(10,10))\nsns.jointplot(x=hr_data['TotalWorkingYears'], y=hr_data['YearsAtCompany'],kind='reg',\n              height=8,color= 'mediumvioletred')","e4a8038a":"sns.factorplot(data=hr_data,y='Attrition',x='NumCompaniesWorked',size=7,aspect=1,kind='bar')","8ba2932e":"sns.factorplot(data=hr_data,y='OverTime',x='Attrition',size=6,aspect=1,kind='bar')","c8fdc2c4":"sns.swarmplot(x=\"Department\", y=\"MonthlyIncome\", hue=\"Attrition\", data=hr_data);\nplt.show()\n\nsns.swarmplot(x=\"JobRole\", y=\"MonthlyIncome\", hue=\"Attrition\", data=hr_data);\nplt.xticks( rotation=90 )\nplt.show()\n\n\nsns.swarmplot(x=\"JobLevel\", y=\"MonthlyIncome\", hue=\"Attrition\", data=hr_data);\nplt.show()","95e22159":"age=pd.DataFrame(hr_data.groupby(\"Age\")[[\"MonthlyIncome\",\"DailyRate\",\"MonthlyRate\",'HourlyRate']].mean())\nage[\"Count\"]=hr_data.Age.value_counts(dropna=False)\nage.reset_index(level=0, inplace=True)\nage.head()","d30214c7":"age.describe().plot(kind = \"area\",fontsize=15, figsize = (25,8), table = True,colormap=\"rainbow\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Rate\")","9c375c47":"sns.relplot(y=\"YearsInCurrentRole\", x=\"MonthlyIncome\", hue='Department', size=\"JobSatisfaction\",\n            sizes=(40, 400), alpha=.5,  palette=\"cubehelix\",\n            height=8, data=hr_data)","d04df865":"sns.catplot(y=\"JobRole\",x=\"Attrition\", kind=\"bar\",size=9, data=hr_data);","069bdab9":"sns.catplot(x=\"OverTime\", y=\"Age\", kind=\"swarm\", data=hr_data);","ba70e794":"sns.catplot(x=\"MaritalStatus\", y=\"Attrition\", kind=\"bar\",size=7, data=hr_data,color='darkmagenta');","31700db5":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# Create a label encoder object\nle = LabelEncoder()","6d304e29":"hr_data.shape","394ba9d8":"le_count = 0\nfor col in hr_data.columns[1:]:\n    if hr_data[col].dtype == 'object':\n        if len(list(hr_data[col].unique())) <= 2:\n            le.fit(hr_data[col])\n            hr_data[col] = le.transform(hr_data[col])\n            le_count += 1\nprint('{} columns were label encoded.'.format(le_count))","278d5344":"# convert rest of categorical variable into dummy\nhr_data = pd.get_dummies(hr_data, drop_first=True)","58070a9d":"hr_data.shape","df949b20":"hr_data.head(3)","a7685851":"#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split\n#from sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n#from sklearn.model_selection import GridSearchCV\n\n#from imblearn.over_sampling import SMOTE\n\n#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder","83b96679":"scaler=StandardScaler()\nscaled_df=scaler.fit_transform(hr_data.drop('Attrition',axis=1))\nX=scaled_df\nY=hr_data['Attrition'].as_matrix()","028fe5e2":"# assign the target to a new dataframe and convert it to a numerical feature\n#df_target = df_HR[['Attrition']].copy()\ntarget = hr_data['Attrition'].copy()","c5b9fa42":"# let's remove the target feature and redundant features from the dataset\nhr_data.drop(['Attrition', 'EmployeeCount', 'EmployeeNumber',\n            'StandardHours', 'JobRole_Research Scientist','Over18','DailyRate','HourlyRate','MonthlyRate','PercentSalaryHike','PerformanceRating',], axis=1, inplace=True)\nprint('Size of Full dataset is: {}'.format(hr_data.shape))","e6071351":"# Since we have class imbalance (i.e. more employees with turnover=0 than turnover=1)\n# let's use stratify=y to maintain the same ratio as in the training dataset when splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(hr_data,\n                                                    target,\n                                                    test_size=0.25,\n                                                    random_state=7,\n                                                    stratify=target)  \nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","fe0b692a":"# Create base rate model\ndef base_rate_model(X) :\n    y = np.zeros(X.shape[0])\n    return y","81b7a515":"# Check accuracy of base rate model\ny_base_rate = base_rate_model(X_test)\nfrom sklearn.metrics import accuracy_score\nprint (\"Base rate accuracy is %2.2f\" % accuracy_score(y_test, y_base_rate))","5ec63faf":"# Check accuracy of Logistic Model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty='l2', C=1)\n\nmodel.fit(X_train, y_train)\nprint (\"Logistic accuracy is %2.2f\" % accuracy_score(y_test, model.predict(X_test)))","00c7f76a":"# Using 10 fold Cross-Validation to train our Logistic Regression Model\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression(class_weight = \"balanced\")\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","9bab04f3":"# Compare the Logistic Regression Model V.S. Base Rate Model V.S. Random Forest Model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nprint (\"---Base Model---\")\nbase_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))\nprint (\"Base Rate AUC = %2.2f\" % base_roc_auc)\nprint(classification_report(y_test, base_rate_model(X_test)))\n\n# NOTE: By adding in \"class_weight = balanced\", the Logistic Auc increased by about 10%! This adjusts the threshold value\nlogis = LogisticRegression(class_weight = \"balanced\")\nlogis.fit(X_train, y_train)\nprint (\"\\n\\n ---Logistic Model---\")\nlogit_roc_auc = roc_auc_score(y_test, logis.predict(X_test))\nprint (\"Logistic AUC = %2.2f\" % logit_roc_auc)\nprint(classification_report(y_test, logis.predict(X_test)))\n\n# Decision Tree Model\ndtree = tree.DecisionTreeClassifier(\n    #max_depth=3,\n    class_weight=\"balanced\",\n    min_weight_fraction_leaf=0.01\n    )\ndtree = dtree.fit(X_train,y_train)\nprint (\"\\n\\n ---Decision Tree Model---\")\ndt_roc_auc = roc_auc_score(y_test, dtree.predict(X_test))\nprint (\"Decision Tree AUC = %2.2f\" % dt_roc_auc)\nprint(classification_report(y_test, dtree.predict(X_test)))\n\n# Random Forest Model\nrf = RandomForestClassifier(\n    n_estimators=1000, \n    max_depth=None, \n    min_samples_split=10, \n    class_weight=\"balanced\"\n    #min_weight_fraction_leaf=0.02 \n    )\nrf.fit(X_train, y_train)\nprint (\"\\n\\n ---Random Forest Model---\")\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nprint (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\nprint(classification_report(y_test, rf.predict(X_test)))\n\n\n# Ada Boost\nada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nada.fit(X_train,y_train)\nprint (\"\\n\\n ---AdaBoost Model---\")\nada_roc_auc = roc_auc_score(y_test, ada.predict(X_test))\nprint (\"AdaBoost AUC = %2.2f\" % ada_roc_auc)\nprint(classification_report(y_test, ada.predict(X_test)))","3f953828":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, logis.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\nada_fpr, ada_tpr, ada_thresholds = roc_curve(y_test, ada.predict_proba(X_test)[:,1])\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % dt_roc_auc)\n\n# Plot AdaBoost ROC\nplt.plot(ada_fpr, ada_tpr, label='AdaBoost (area = %0.2f)' % ada_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","bea013fc":"feat_importances = pd.Series(rf.feature_importances_, index=hr_data.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.show()","75ac756c":"#### Age","9d703581":"### Evaluation Metric\n\nAnother important point while dealing with the imbalanced classes is the choice of right evaluation metrics.\n\nNote that accuracy is not a good choice. This is because since the data is skewed even an algorithm classifying the target as that belonging to the majority class at all times will achieve a very high accuracy. For eg if we have 20 observations of one type 980 of another ; a classifier predicting the majority class at all times will also attain a accuracy of 98 % but doesnt convey any useful information.\n\nHence in these type of cases we may use other metrics such as -->\n\n'Precision'-- (true positives)\/(true positives+false positives)\n\n'Recall'-- (true positives)\/(true positives+false negatives)\n\n'F1 Score'-- The harmonic mean of 'precision' and 'recall'\n\n'AUC ROC'-- ROC curve is a plot between 'senstivity' (Recall) and '1-specificity' (Specificity=Precision)\n\n'Confusion Matrix'-- Plot the entire confusion matrix","64e00090":"#### Hourly rate\n","c1519dbd":"Employees qualified as Below college and Bachelor Education tend to leave the company than others.","fcf268b7":"**MaritalStatus **: The workers who have Single marital status are more likely to quit the Married, and Divorced.","b983369b":"#### Worklife balance","aca917ea":"Education","e746e360":"### Preparing DataSet\n\n#### Feature Encoding","19bd5697":"**satisfaction comparison**","967f77f4":"Looks like about 84% of employees stayed and 16% of employees left. \nNOTE: When performing cross validation, its important to maintain this turnover ratio","ae79ccdf":"**Problem statement**\nUncover the factors that lead to employee attrition and explore important questions such as \u2018show me a breakdown of distance from home by job role and attrition\u2019 or \u2018compare average monthly income by education and attrition\u2019. This is a fictional data set created by IBM data scientists.\n\n The aim is what factors contribute most to employee turnover and create a model that can predict if a certain employee will leave the company or not.\n\nEducation 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n\nEnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobInvolvement\n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nPerformanceRating\n1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n\nRelationshipSatisfaction\n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nWorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'","b457b14c":"#### One-Sample T-Test Summary\n#### T-Test = 3.58 | P-Value = 0.0004125521 | Reject Null Hypothesis\n\n**Reject the null hypothesis because:**\n\nT-Test score is outside the quantiles\nP-value is lower than confidence level of 5%\nBased on the statistical analysis of a one sample t-test, there seems to be some significant difference between the mean satisfaction of employees who had a Attrition and the entire employee population. The super low P-value of 0.0004125521 at a 5% confidence level is a good indicator to reject the null hypothesis.\n\nBut this does not neccessarily mean that there is practical significance. We would have to conduct more experiments or maybe collect more data about the employees in order to come up with a more accurate finding.","04d86e68":"**Age:** Employees in relatively young age bracket 25-35 are more likely to leave. Hence, efforts should be made to clearly articulate the long-term vision of the company and young employees fit in that vision, as well as provide incentives in the form of clear paths to promotion for instance.\n","18aabf61":"**YearsAtCompany**:  Employees who hit their two-year anniversary should be identified as potentially having a higher-risk of leaving.\nA strategic \"Retention Plan\" should be drawn for each Risk Category group..","c4cfacd2":"### Calculating feature importance","8c6df06d":"BusinessTravel : The workers who travel alot are more likely to quit then other employees.\n\nDepartment : The worker in Research & Development are more likely to stay then the workers on other departement.\n\nEducationField : The workers with Human Resources and Technical Degree are more likely to quit then employees from other fields of educations.\n\nGender : The Male are more likely to quit.\n\nJobRole : The workers in Laboratory Technician, Sales Representative, and Human Resources are more likely to quit the workers in other positions.\n\nMaritalStatus : The workers who have Single marital status are more likely to quit the Married, and Divorced.\n\nOverTime : The workers who work more hours are likely to quit then others.","2f6efa16":"Shows Imbalanced data","c4311f03":"**Monthly Income**: people on higher wages are less likely to leave the company. Hence, efforts should be made to gather information on industry benchmarks in the current local market to determine if the company is providing competitive wages.","aa3238e7":"#### Correlation Matrix","667f3c43":"Move the attrition column to front","af77522c":"**TotalWorkingYears:** The more experienced employees are less likely to leave. Employees who have between 5-8 years of experience should be identified as potentially having a higher-risk of leaving.","186b3446":"### Overtime","3fc800ab":"People who live further away from their work show higher proportion of leavers compared to their counterparts.","678a3776":"**YearsWithCurrManager**: A large number of leavers leave 6 months after their Current Managers. By using Line Manager details for each employee, one can determine which Manager have experienced the largest numbers of employees resigning over the past year. Several metrics can be used here to determine whether action should be taken with a Line Manager:","567f5fe9":"### Statistical Test for Correlation\n##### One-Sample T-Test (Measuring Satisfaction Level)\nA one-sample t-test checks whether a sample mean differs from the population mean. Let's test to see whether the average satisfaction level of employees that had Attrition differs from the entire employee population.\n\n**Hypothesis Testing:** Is there significant difference in the means of satisfaction level between employees who had a Attrition and the entire employee population?\n\n**Null Hypothesis**:  The null hypothesis would be that there is no difference in satisfaction level between employees who did Attrition and the entire employee population.\n\n**Alternate Hypothesis**:  The alternative hypothesis would be that there is a difference in satisfaction level between employees who did Attrition and the entire employee population.","589e019c":"Low Environment satisfaction and Job satisfaction  people more likly to leave the company.","935d876c":"#### Years at Company","dd0eb463":"#### EnvironmentSatisfaction","53ca3dc5":"There is no missing data","8fc22add":"1. Obtaining the data is the first approach in solving the problem.\n1. Scrubbing or cleaning the data is the next step. This includes data imputation of missing or invalid data and fixing column names.\n1. Exploring the data will follow right after and allow further insight of what our dataset contains. Looking for any outliers or weird data. Understanding the relationship each explanatory variable has with the response variable resides here and we can do this with a correlation matrix.\n1. Modeling the data will give us our predictive power on whether an employee will leave.\n1. INterpreting the data is last. With all the results and analysis of the data, what conclusion is made? What factors contributed most to employee turnover? What relationship of variables were found?","a91843de":"### Create Model","6dd1aa1d":"#### Conducting the T-Test\n\nLet's conduct a t-test at 95% confidence level and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the employee population. To conduct a one sample t-test, we can use the stats.ttest_1samp() function:","1e636e8e":"Employees got salary hike of 10% to 17%  more chance to leave company","f1068533":"We use Label Encoder to encode categorical labels with numerical values.","e0e70ca2":"#### PerformanceRating","bfbd84e3":"**EducationField **: The workers with Human Resources and Technical Degree are more likely to quit then employees from other fields of educations.","6d14255b":"### MonthlyIncome and MonthlyRate","fb08368a":"#### Distance from Home","b00176b2":"### Feature Scaling","06c79fbb":"People who worked more companies are likly to leave","ee8e60bd":"**Department :** The worker in Research & Development are more likely to stay then the workers on other departement.","cba5d8fa":"### TotalWorkingYears vs YearsAtCompany","168ed559":"**JobRole** : The workers in Laboratory Technician, Sales Representative, and Human Resources are more likely to quit the workers in other positions.","4846d7a0":"**Over Time**: people who work overtime are more likelty to leave the company. Hence efforts must be taken to appropriately scope projects upfront with adequate support and manpower so as to reduce the use of overtime.","00b53e0d":"### Job Role vs Monthly Income","1973dbe7":"**BusinessTravel** : The workers who travel alot are more likely to quit then other employees.","d7d7450a":"#### Job satisfation","09ba11ff":"### Monthly Income vs Job role,Job Level,Department","9ddc094c":"### Visulization help us to uderstand our data in detail","3f337b5a":" Label Encoding will be used for columns with 2 or less unique values","b54a0c1e":"#### T-Test Result\n\nThe test result shows the test statistic \"t\" is equal to 3.58. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies outside the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with stats.t.ppf():\n\n#### T-Test Quantile\nIf the t-statistic value we calculated above (3.58) is outside the quantiles, then we can reject the null hypothesis","5c7677fe":"**Year at Company**","025a94a0":"Mean value of other variables(Stayed vs Not stayed)","8936d9ce":"#### YearsInCurrentRole vs YearsAtCompany","3cf701bb":"#### Department","b5f5b388":"Feature Scaling using MinMaxScaler essentially shrinks the range such that the range is now between 0 and n. Machine Learning algorithms perform better when input numerical variables fall within a similar scale. In this case, we are scaling between 0 and 5."}}