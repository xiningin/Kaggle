{"cell_type":{"2d665480":"code","ad70f979":"code","9b9a43fe":"code","576e8e45":"code","641e9624":"code","13d43fb8":"code","67e99b16":"code","142fb85a":"code","92d90338":"code","1173f889":"code","345ec2cb":"code","3ea492ad":"code","d89f86ce":"code","23366af7":"code","efe086e4":"code","acfcf6f9":"code","e01b1041":"code","00e83fe9":"code","b994a546":"code","8fb42fa7":"code","df85f9c6":"code","d09fbd85":"code","167f5e91":"code","d65547d0":"code","99c75521":"code","0259041c":"markdown","dad6f68e":"markdown","4b663350":"markdown","219e7490":"markdown","6f4b792f":"markdown","fac0ad70":"markdown","52816c47":"markdown","4698d869":"markdown","b3ea18ff":"markdown","4fb6fee4":"markdown","c82473f6":"markdown","e3df76a9":"markdown","2273e320":"markdown","fb641bfe":"markdown","cb91d6ba":"markdown","dd474812":"markdown","b715a08a":"markdown","5d20a35e":"markdown","86e34695":"markdown","f5aa37c9":"markdown","36397046":"markdown","1f795347":"markdown","ba24a94f":"markdown","984bd489":"markdown","d64f34e9":"markdown"},"source":{"2d665480":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","ad70f979":"train_df = pd.read_csv(\"..\/input\/quora-question-pairs\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/quora-question-pairs\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","9b9a43fe":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.0001, random_state=2019)\n\n## some config values \nembed_size = 500 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\nepochs = 40 # num of epochs\ndropout = 0.35 # dropout part\n\n## fill up the missing values\ntrain_X1 = train_df[\"question1\"].fillna(\"_na_\").values\ntrain_X2 = train_df[\"question2\"].fillna(\"_na_\").values\nval_X1 = val_df[\"question1\"].fillna(\"_na_\").values\nval_X2 = val_df[\"question2\"].fillna(\"_na_\").values\ntest_X1 = test_df[\"question1\"].fillna(\"_na_\").values\ntest_X2 = test_df[\"question2\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(np.concatenate([train_X1, train_X2])))\ntrain_X1 = tokenizer.texts_to_sequences(train_X1)\ntrain_X2 = tokenizer.texts_to_sequences(train_X2)\nval_X1 = tokenizer.texts_to_sequences(val_X1)\nval_X2 = tokenizer.texts_to_sequences(val_X2)\ntest_X1 = tokenizer.texts_to_sequences(test_X1)\ntest_X2 = tokenizer.texts_to_sequences(test_X2)\n\n## Pad the sentences \ntrain_X1 = pad_sequences(train_X1, maxlen=maxlen)\ntrain_X2 = pad_sequences(train_X2, maxlen=maxlen)\nval_X1 = pad_sequences(val_X1, maxlen=maxlen)\nval_X2 = pad_sequences(val_X2, maxlen=maxlen)\ntest_X1 = pad_sequences(test_X1, maxlen=maxlen)\ntest_X2 = pad_sequences(test_X2, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['is_duplicate'].values\nval_y = val_df['is_duplicate'].values","576e8e45":"# Each instance will consist of two inputs: a single question1, and a single question2\nquestion1_input = Input(shape=(maxlen,), name='question1')\nquestion2_input = Input(shape=(maxlen,), name='question2')\nquestion1_embedded = Embedding(max_features, embed_size, name='question1_embedded')(question1_input)\nquestion2_embedded = Embedding(max_features, embed_size, name='question2_embedded')(question2_input)\n\n# the first branch operates on the first input\nx = Bidirectional(CuDNNGRU(75, return_sequences=True))(question1_embedded)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(dropout)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nx = Model(inputs=question1_input, outputs=x)\n\n# the second branch opreates on the second input\ny = Bidirectional(CuDNNGRU(64, return_sequences=True))(question2_embedded)\ny = GlobalMaxPool1D()(y)\ny = Dense(16, activation=\"relu\")(y)\ny = Dropout(dropout)(y)\ny = Dense(1, activation=\"sigmoid\")(y)\ny = Model(inputs=question2_input, outputs=y)\n\n# combine the output of the two branches\ncombined = layers.concatenate([x.output, y.output])\n \n# apply a FC layer and then a regression prediction on the\n# combined outputs\nz = Dense(16, activation=\"relu\")(combined)\nz = Dropout(dropout)(z)\nz = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(\n    inputs = [question1_input, question2_input],\n    outputs = z,\n)\n\nmodel.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n\nmodel.summary()","641e9624":"## Train the model \nmodel.fit([train_X1, train_X2], train_y, batch_size=1024, epochs=4, validation_data=([val_X1, val_X2], val_y))","13d43fb8":"pred_noemb_val_y = model.predict([val_X1, val_X2], batch_size=1024, verbose=1)\nprint(\"log_loss score is {0}\".format(metrics.log_loss(val_y, pred_noemb_val_y)))","67e99b16":"pred_noemb_test_y = model.predict([test_X1, test_X2], batch_size=1024, verbose=1)","142fb85a":"del model, question1_input, question2_input, x, y, z\nimport gc; gc.collect()\ntime.sleep(10)","92d90338":"EMBEDDING_FILE = '..\/input\/quora-insincere-questions-classification\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n# Each instance will consist of two inputs: a single question1, and a single question2\nquestion1_input = Input(shape=(maxlen,), name='question1')\nquestion2_input = Input(shape=(maxlen,), name='question2')\nquestion1_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question1_embedded')(question1_input)\nquestion2_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question2_embedded')(question2_input)\n\n# the first branch operates on the first input\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(question1_embedded)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(dropout)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nx = Model(inputs=question1_input, outputs=x)\n\n# the second branch opreates on the second input\ny = Bidirectional(CuDNNGRU(64, return_sequences=True))(question2_embedded)\ny = GlobalMaxPool1D()(y)\ny = Dense(16, activation=\"relu\")(y)\ny = Dropout(dropout)(y)\ny = Dense(1, activation=\"sigmoid\")(y)\ny = Model(inputs=question2_input, outputs=y)\n\n# combine the output of the two branches\ncombined = layers.concatenate([x.output, y.output])\n \n# apply a FC layer and then a regression prediction on the\n# combined outputs\nz = Dense(16, activation=\"relu\")(combined)\nz = Dropout(dropout)(z)\nz = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(\n    inputs = [question1_input, question2_input],\n    outputs = z,\n)\n\nmodel.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n\nmodel.summary()","1173f889":"model.fit([train_X1, train_X2], train_y, batch_size=1024, epochs=epochs, validation_data=([val_X1, val_X2], val_y))","345ec2cb":"pred_glove_val_y = model.predict([val_X1, val_X2], batch_size=1024, verbose=1)\nprint(\"log_loss score is {0}\".format(metrics.log_loss(val_y, pred_glove_val_y)))","3ea492ad":"pred_glove_test_y = model.predict([test_X1, test_X2], batch_size=1024, verbose=1)","d89f86ce":"del word_index, embeddings_index, all_embs, embedding_matrix, model, question1_input, question2_input, x, y, z\nimport gc; gc.collect()\ntime.sleep(10)","23366af7":"EMBEDDING_FILE = '..\/input\/quora-insincere-questions-classification\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf8', errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \n# Each instance will consist of two inputs: a single question1, and a single question2\nquestion1_input = Input(shape=(maxlen,), name='question1')\nquestion2_input = Input(shape=(maxlen,), name='question2')\nquestion1_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question1_embedded')(question1_input)\nquestion2_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question2_embedded')(question2_input)\n\n# the first branch operates on the first input\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(question1_embedded)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(dropout)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nx = Model(inputs=question1_input, outputs=x)\n\n# the second branch opreates on the second input\ny = Bidirectional(CuDNNGRU(64, return_sequences=True))(question2_embedded)\ny = GlobalMaxPool1D()(y)\ny = Dense(16, activation=\"relu\")(y)\ny = Dropout(dropout)(y)\ny = Dense(1, activation=\"sigmoid\")(y)\ny = Model(inputs=question2_input, outputs=y)\n\n# combine the output of the two branches\ncombined = layers.concatenate([x.output, y.output])\n \n# apply a FC layer and then a regression prediction on the\n# combined outputs\nz = Dense(16, activation=\"relu\")(combined)\nz = Dropout(dropout)(z)\nz = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(\n    inputs = [question1_input, question2_input],\n    outputs = z,\n)\n\nmodel.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n\nmodel.summary()","efe086e4":"model.fit([train_X1, train_X2], train_y, batch_size=1024, epochs=epochs, validation_data=([val_X1, val_X2], val_y))","acfcf6f9":"pred_fasttext_val_y = model.predict([val_X1, val_X2], batch_size=1024, verbose=1)\nprint(\"log_loss score is {0}\".format(metrics.log_loss(val_y, pred_fasttext_val_y)))","e01b1041":"pred_fasttext_test_y = model.predict([test_X1, test_X2], batch_size=1024, verbose=1)","00e83fe9":"del word_index, embeddings_index, all_embs, embedding_matrix, model, question1_input, question2_input, x, y, z\nimport gc; gc.collect()\ntime.sleep(10)","b994a546":"EMBEDDING_FILE = '..\/input\/quora-insincere-questions-classification\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \n# Each instance will consist of two inputs: a single question1, and a single question2\nquestion1_input = Input(shape=(maxlen,), name='question1')\nquestion2_input = Input(shape=(maxlen,), name='question2')\nquestion1_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question1_embedded')(question1_input)\nquestion2_embedded = Embedding(max_features, embed_size, weights=[embedding_matrix], name='question2_embedded')(question2_input)\n\n# the first branch operates on the first input\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(question1_embedded)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(dropout)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nx = Model(inputs=question1_input, outputs=x)\n\n# the second branch opreates on the second input\ny = Bidirectional(CuDNNGRU(64, return_sequences=True))(question2_embedded)\ny = GlobalMaxPool1D()(y)\ny = Dense(16, activation=\"relu\")(y)\ny = Dropout(dropout)(y)\ny = Dense(1, activation=\"sigmoid\")(y)\ny = Model(inputs=question2_input, outputs=y)\n\n# combine the output of the two branches\ncombined = layers.concatenate([x.output, y.output])\n \n# apply a FC layer and then a regression prediction on the\n# combined outputs\nz = Dense(16, activation=\"relu\")(combined)\nz = Dropout(dropout)(z)\nz = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(\n    inputs = [question1_input, question2_input],\n    outputs = z,\n)\n\nmodel.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n\nmodel.summary()","8fb42fa7":"model.fit([train_X1, train_X2], train_y, batch_size=1024, epochs=epochs, validation_data=([val_X1, val_X2], val_y))","df85f9c6":"pred_paragram_val_y = model.predict([val_X1, val_X2], batch_size=1024, verbose=1)\nprint(\"log_loss score is {0}\".format(metrics.log_loss(val_y, pred_paragram_val_y)))","d09fbd85":"pred_paragram_test_y = model.predict([test_X1, test_X2], batch_size=1024, verbose=1)","167f5e91":"del word_index, embeddings_index, all_embs, embedding_matrix, model, question1_input, question2_input, x, y, z\nimport gc; gc.collect()\ntime.sleep(10)","d65547d0":"pred_val_y = 0.33*pred_glove_val_y + 0.33*pred_fasttext_val_y + 0.34*pred_paragram_val_y\nprint(\"log_loss score is {0}\".format(metrics.log_loss(val_y, pred_val_y)))","99c75521":"pred_test_y = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.34*pred_paragram_test_y\nout_df = pd.DataFrame({\"test_id\":test_df[\"test_id\"].values})\nout_df['is_duplicate'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","0259041c":"# \u0415\u0441\u0442\u044c \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0433\u043e\u0434\u043d\u044b\u0439 \u043d\u043e\u0443\u0442\u0431\u0443\u043a!\n\u0417\u0434\u0435\u0441\u044c \u043c\u043e\u0439 \u043d\u043e\u0443\u0442\u0431\u0443\u043a, \u0434\u0430\u0432\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.41974 \u0441 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433\u043e\u043c. \u041e\u043d \u043b\u0443\u0447\u0448\u0435 \u044d\u0442\u043e\u0433\u043e \u043f\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c, \u043d\u043e \u0445\u0443\u0436\u0435 \u043f\u043e \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u044e \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438! https:\/\/www.kaggle.com\/turing228\/embedding-with-the-preprocessing?scriptVersionId=14942999","dad6f68e":"# What is good to be implemented in the future?\n\n1. \u0412\u0430\u0436\u0435\u043d \u043b\u0438 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432 Keras NN model? \u0415\u0441\u043b\u0438 \u0434\u0430, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0432 \u0441\u0438\u043c\u043c\u0435\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043f\u0430\u0440\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432\n2. \u0423\u0441\u043b\u043e\u0436\u043d\u0438\u0442\u044c NN \u0438 \u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \u041f\u0440\u043e\u0441\u0442\u044b\u043c \u043f\u043e\u0434\u0431\u043e\u0440\u043e\u0434 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043d\u0435 \u043e\u0431\u043e\u0439\u0442\u0438\u0441\u044c \u0443\u0436\u0435, \u0438\u0445 \u0437\u0430\u043c\u0435\u043d\u0430 \u043d\u0435 \u0434\u0430\u0435\u0442 \u0432\u044b\u0438\u0433\u0440\u044b\u0448\u0430\n3. \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c XGBoost, LSTM, CatBoost \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0438 \u0441\u0443\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438. \u0410 \u043c\u043e\u0436\u043d\u043e \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043d\u043e\u0432\u0443\u044e \u043c\u0430\u0448\u0438\u043d\u043a\u0443 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0438\u043a\u0448\u0435\u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u044d\u0442\u0438\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438\n4. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u0442\u0430\u043a\u043e\u0439 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043c\u043e\u0436\u0435\u0442. \u0421\u043e\u0433\u043b\u0430\u0441\u043d\u043e [\u044d\u0442\u043e\u0439 \u0441\u0442\u0430\u0442\u044c\u0435](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings)","4b663350":"Now let us get the validation sample predictions and check the logloss score. ","219e7490":"# \u041e\u0442\u0431\u043e\u0440\u043e\u0447\u043d\u043e\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 \u21162 \u043d\u0430 \u0441\u0442\u0430\u0436\u0438\u0440\u043e\u0432\u043a\u0443 \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 Core ML VK \u043a \u0421\u0435\u043c\u0451\u043d\u0443 \u041f\u043e\u043b\u044f\u043a\u043e\u0432\u0443: \n\u0426\u0435\u043b\u044c \u2014 \u043d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432.\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0449\u0443\u044e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0442\u0435\u043a\u0441\u0442\u0430 (\u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b), \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u043d\u0430\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043b\u0438\u0431\u043e \u043e\u0431\u0443\u0447\u0438\u0432 \u0435\u0451 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e. \u0412\u0438\u0434 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u044b\u043c, \u043d\u043e \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u043d\u0435 \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 1000 \u0447\u0438\u0441\u0435\u043b.\n\n\u041f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 Quora Question Pairs (https:\/\/www.kaggle.com\/c\/quora-question-pairs\/) \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0443\u044e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0445\u043e\u0436\u0435\u0441\u0442\u0438 \u043f\u0430\u0440 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e \u043f\u0430\u0440\u0435 \u0438\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432. \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043e\u043b\u0436\u043d\u0430 \u0432\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432 \u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e\u043c \u043b\u0438\u0434\u0435\u0440\u0431\u043e\u0440\u0434\u0435 kaggle logloss \u043d\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 0,5. \n\n\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043a\u043e\u0434 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438, \u043a\u043e\u0434 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u0441\u0441\u044b\u043b\u043a\u0443 \u043d\u0430 \u0441\u0432\u043e\u0439 \u043f\u0440\u043e\u0444\u0438\u043b\u044c \u0432 kaggle. \u041a\u0440\u043e\u043c\u0435 logloss \u0431\u0443\u0434\u0435\u0442 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c\u0441\u044f \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0438 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f.\n\n## \u0420\u0435\u0448\u0430\u0435\u0442 \u0437\u0430\u0434\u0430\u0447\u0443:\n\u041b\u0438\u0441\u043e\u0432\u0435\u0442\u0438\u043d \u041d\u0438\u043a\u0438\u0442\u0430 \u0412\u0430\u043b\u0435\u0440\u044c\u0435\u0432\u0438\u0447 ([github](https:\/\/github.com\/turing228), [vk](https:\/\/vk.com\/nikitalisovetin))","6f4b792f":"# Notebook Objective:\n\nObjective of the notebook is to look at the different pretrained embeddings provided in the dataset and to see how they are useful in the model building process. \n\nFirst let us import the necessary modules and read the input data.","fac0ad70":"# Without Pretrained Embeddings:\n\nNow that we are done with all the necessary preprocessing steps, we can first train a Bidirectional GRU model. We will not use any pre-trained word embeddings for this model and the embeddings will be learnt from scratch. Please check out the model summary for the details of the layers used. ","52816c47":"# Task manager\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 3:\nhttps:\/\/www.kaggle.com\/turing228\/different-embeddings-10-epochs-final?scriptVersionId=14893249\n1. \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u043b \u043d\u0430 \u043d\u043e\u0447\u044c, \u0440\u0430\u0431\u043e\u0442\u0430\u043b 2 \u0447\u0430\u0441\u0430. \u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u043e\u0441\u044c \u043c\u043d\u043e\u0433\u043e \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u043b \u0432 \u0438\u0442\u043e\u0433\u0435\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 4:\n1. Yes \u0418\u0441\u043f\u0440\u0430\u0432\u0438\u043b \u043e\u0434\u043d\u0443 \u043e\u0448\u0438\u0431\u043a\u0443 (\u0438\u0437-\u0437\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u043d\u0435 \u0443\u0437\u043d\u0430\u043b\u0438 logloss \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f wiki-news embedding)\n2. Yes \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043b \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0432 dropout \u0441 0.1 \u0434\u043e 1.5 \u0438 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0447\u0438\u0441\u043b\u043e \u044d\u043f\u043e\u0445 \u0432\u043c\u0435\u0441\u0442\u043e 60 - 10\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 5:\n1. Yes \u0418\u0441\u043f\u0440\u0430\u0432\u0438\u043b \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0432 \u0432\u044b\u0432\u043e\u0434\u0435\n2. Yes \u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043b dropout \u043e\u0431\u0440\u0430\u0442\u043d\u043e\n3. Yes \u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043b \u0447\u0438\u0441\u043b\u043e \u044d\u043f\u043e\u0445 \u0434\u043e 4\n4. Yes \u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043b \u0442\u0435\u0441\u0442\u044b \u0434\u043e 0.15%\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 9:\n1. Yes dropout \u0434\u043e 0.2\n2. Yes epochs = 10\n3. Yes as_int \u0432 \u0432\u044b\u0432\u043e\u0434\u0435 \u0443\u0431\u0440\u0430\u043b. \u041e\u041d\u041e \u041f\u0420\u0415\u0412\u0420\u0410\u0429\u0410\u041b\u041e \u041c\u041e\u0418 0.5 \u0432 8.5 \u0432 \u043b\u0438\u0434\u0435\u0440\u0431\u043e\u0440\u0434\u0435!!!\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 12:\n\n1. Yes \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u043a\u0430\u043a \u0434\u043b\u044f \u0433\u043b\u043e\u0432\u0430\n2. Yes \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0442\u0442\u0443\u0434\u0430 \u0436\u0435\n3. No XGBoost \u0434\u043e\u0434\u0435\u043b\u0430\u0442\u044c \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 (\u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u0441 Keras NN)\n4. No \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u0447\u043a\u0438 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u043a\u0438 \u0441 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435\u043c x \u0438 y\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 14:\n\n1. Yes \u041e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435\n2. Yes \u041a\u0430\u0440\u0442\u0438\u043d\u043e\u0447\u043a\u0438\n\n### \u0412\u0435\u0440\u0441\u0438\u044f 15:\n\n1. \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043b dropout \u0441 0.2 \u0434\u043e 0.35\n2. \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043b epoches \u0441 10 \u0434\u043e 40","4698d869":"# Observations:\n * Overall pretrained embeddings seem to give better results comapred to non-pretrained model. \n * The performance of the different pretrained embeddings are almost similar.\n \n# Final Blend:\n\nThough the results of the models with different pre-trained embeddings are similar, there is a good chance that they might capture different type of information from the data. So let us do a blend of these three models by averaging their predictions.","b3ea18ff":"# Glove Embeddings:\n\nIn this section, let us use the Glove embeddings and rebuild the GRU model.","4fb6fee4":"Results seem to be better than the model without pretrained embeddings.","c82473f6":"We have four different types of embeddings.\n * GoogleNews-vectors-negative300 - https:\/\/code.google.com\/archive\/p\/word2vec\/\n * glove.840B.300d - https:\/\/nlp.stanford.edu\/projects\/glove\/\n * paragram_300_sl999 - https:\/\/cogcomp.org\/page\/resource_view\/106\n * wiki-news-300d-1M - https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n \nA very good explanation for different types of embeddings are given in this [kernel](https:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge). Please refer the same for more details..","e3df76a9":"Now let us get the test set predictions as well and save them","2273e320":"# How our model is planning to be?\n\nAs we need to join two parameters (question1 and question2), our Keras NN model will looks so:\n![image.png](attachment:image.png)\nA key thing to note is that this network is not simply a stack of layers from input to output. We're treating the question1 and the question2 as separate inputs, which come together only after each has gone through its own embedding layer.","fb641bfe":"# Paragram Embeddings:\n\nIn this section, we can use the paragram embeddings and build the model and make predictions.","cb91d6ba":"# Wiki News FastText Embeddings:\n\nNow let us use the FastText embeddings trained on Wiki News corpus in place of Glove embeddings and rebuild the model.","dd474812":"Next steps are as follows:\n * Split the training dataset into train and val sample. Cross validation is a time consuming process and so let us do simple train val split.\n * Fill up the missing values in the text column with '_na_'\n * Tokenize the text column and convert them to vector sequences\n * Pad the sequence as needed - if the number of words in the text is greater than 'max_len' trunacate them to 'max_len' or if the number of words in the text is lesser than 'max_len' add zeros for remaining values.","b715a08a":"## References:\n\nThanks to the below kernels which helped me with this one. \n1. https:\/\/www.kaggle.com\/jhoward\/improved-lstm-baseline-glove-dropout\n2. https:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge","5d20a35e":"# Result:\n\n\u041f\u043e\u043b\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u043e\u0442\u043b\u0438\u0447\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c, \u0434\u0430\u044e\u0449\u0430\u044f \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u044f\u044e\u0449\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0434\u043b\u044f \u0441\u0435\u0431\u044f. \u041e\u0431\u0443\u0447\u0438\u043b\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u043e\u0439 Keras NN \u0441 \u0434\u0432\u0443\u043c\u044f input'\u0430\u043c\u0438. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b 3 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0445 embedding'\u0430 \u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u044b \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \u0417\u0430\u0434\u0430\u0447\u0430 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0430 \u0438 \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0446\u0435\u043b\u0438 \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u044b! ","86e34695":"Now that our model building is done, it might be a good idea to clean up some memory before we go to the next step.","f5aa37c9":"# What have I done?\n\n1. \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0441\u043e \u0441\u0432\u043e\u0438\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u043c\n2. \u0421\u0440\u0430\u0432\u043d\u0438\u043b \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438\n3. \u041d\u0430\u0443\u0447\u0438\u043b\u0441\u044f \u043e\u0431\u0443\u0447\u0430\u0442\u044c Keras \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 input'\u0430\u043c\u0438\n4. \u0412\u044b\u0431\u0440\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0440\u0435\u0448\u0435\u043d\u0438\u044f\n5. \u0421\u0434\u0435\u043b\u0430\u043b \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u0445\u0443\u0434\u0448\u0438\u043b\u0438\u0441\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0443\u0431\u0440\u0430\u043b \u0435\u0433\u043e\n6. \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043b \u043e\u0434\u043d\u0443 \u0432\u0435\u0440\u0441\u0438\u044e \u0440\u0435\u0448\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0430 2 \u0447\u0430\u0441\u0430. \u0421 \u0443\u0447\u0435\u0442\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0432 \u0434\u0432\u0430-\u0442\u0440\u0438 \u0440\u0430\u0437\u0430 \u0432 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0438 \u0441 \u0434\u0432\u0443\u043c\u044f \u044d\u043f\u043e\u0445\u0430\u043c\u0438), \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f 0.7 logloss","36397046":"# What did I read before?\nhttps:\/\/www.nkj.ru\/open\/36052\/\n\nhttps:\/\/habr.com\/ru\/company\/ods\/blog\/329410\/\n\nhttps:\/\/habr.com\/ru\/company\/ods\/blog\/328372\/\n\nhttps:\/\/stats.stackexchange.com\/questions\/153531\/what-is-batch-size-in-neural-network\n\n\n# What do I want to implement? Usefull links\n\nhttps:\/\/keras.io\/optimizers\/\n\nhttps:\/\/www.pyimagesearch.com\/2019\/02\/04\/keras-multiple-inputs-and-mixed-data\/\n\nhttps:\/\/www.kaggle.com\/colinmorris\/embedding-layers\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\n\nhttps:\/\/www.kaggle.com\/shujian\/blend-of-lstm-and-cnn-with-4-embeddings-1200d\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings\n\nhttps:\/\/www.kaggle.com\/shujian\/different-embeddings-with-attention-fork-fork\n\nhttps:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge\n\nhttps:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer\n\nhttps:\/\/www.kaggle.com\/antmarakis\/bidirectional-gru-model\n\nhttps:\/\/www.kaggle.com\/anokas\/data-analysis-xgboost-starter-0-35460-lb\n\nhttps:\/\/www.kaggle.com\/arathee2\/random-forest-vs-xgboost-vs-deep-neural-network","1f795347":"Train the model using train sample and monitor the metric on the valid sample. This is just a sample model running for 2 epochs. Changing the epochs, batch_size and model parameters might give us a better model.","ba24a94f":"The result seems to better than individual pre-trained models and so we let us create a submission file using this model blend.","984bd489":"# What have I tried to do?\n\n\u0414\u0440\u0443\u0433\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438:\n1. XGBoost (https:\/\/www.kaggle.com\/anokas\/data-analysis-xgboost-starter-0-35460-lb). \u0423 \u043c\u0435\u043d\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c, \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435 \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c 0.32 \u0443 \u0441\u0435\u0431\u044f, \u043d\u043e \u0441\u043c\u044b\u0441\u043b? \u042d\u0442\u043e \u043d\u0435\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\n2. LSTM https:\/\/www.kaggle.com\/amoyyean\/lstm-with-glove \u0434\u0430\u044e\u0449\u0438\u0439 0.18\n\n\u041f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433:\n1. \u0420\u0430\u0437\u043d\u044b\u043c\u0438 \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438 \u043f\u044b\u0442\u0430\u043b\u0441\u044f -  \u0432\u043e\u0442 [\u0442\u0430\u043a](https:\/\/www.kaggle.com\/anokas\/data-analysis-xgboost-starter-0-35460-lb) \u0438 [\u0442\u0430\u043a](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings), \u043d\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438\u0441\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0445\u0443\u0436\u0435. \u041f\u0435\u0440\u0432\u044b\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u043b \u0432 \u0447\u0435\u0440\u043d\u043e\u0432\u0438\u043a\u0435 \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 (0.35 \u043f\u0440\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u044d\u043f\u043e\u0445\u0430\u0445), \u043d\u043e, \u043a\u043e\u0433\u0434\u0430 \u044f \u0437\u0430\u043a\u043e\u043c\u043c\u0438\u0442\u0438\u043b \u0441 \u043a\u0443\u0447\u0435\u0439 \u044d\u043f\u043e\u0445, \u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u0445\u043e\u0442\u044c \u0438 \u0432\u044b\u0434\u0430\u043b\u0430 \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b, \u043d\u0430 \u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e\u043c \u043b\u0438\u0434\u0435\u0440\u0431\u043e\u0440\u0434\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 0.55 (\u044d\u0442\u043e 12 \u0432\u0435\u0440\u0441\u0438\u044f \u0431\u044b\u043b\u0430). \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u043b \u0441 \u043d\u0438\u043c ([\u0441\u0441\u044b\u043b\u043a\u0430](https:\/\/www.kaggle.com\/turing228\/embedding-with-the-preprocessing?scriptVersionId=14942999)) \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.41 !!!! (\u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u0443 \u042d\u0422\u041e\u0413\u041e \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430)","d64f34e9":"So we got some baseline GRU model without pre-trained embeddings. Now let us use the provided embeddings and rebuild the model again to see the performance. \n\n"}}