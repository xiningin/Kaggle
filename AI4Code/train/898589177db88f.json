{"cell_type":{"419a32a0":"code","bce90713":"code","18240a2e":"code","f5cd29f3":"code","ab8b115f":"code","b7484ade":"code","f675737c":"code","de2cae2b":"code","15d73063":"code","df2cf838":"code","1b2a2ecd":"code","e4ce2be7":"code","113cc272":"code","b4ccc7be":"code","7b512662":"code","f69e121d":"code","4e5df91f":"code","f6227cc0":"code","cabd282f":"code","82e9e2a0":"code","057ed67c":"code","ad1aa6bf":"code","5f6274ec":"code","f4159f7d":"code","a4b01ef1":"code","fd99464b":"code","2680eace":"code","325ed3ea":"code","5c0abc8e":"code","6dc41e78":"code","ed332d6d":"code","ea8455ad":"code","f3fb2fa8":"code","f27ee3d2":"code","2ad40041":"code","058bbefe":"code","cc3b6e4e":"code","931f3667":"code","a9ee7249":"code","21d0ba81":"code","6060b2db":"code","013b9bb4":"code","eb3e4e96":"code","54e64e52":"code","901f05c0":"code","69842d77":"code","62cf92c8":"markdown","9c99bb7e":"markdown","acca10a9":"markdown","22e7f1e7":"markdown","250f3701":"markdown","85f9bf4d":"markdown","b42fed76":"markdown","22be2fda":"markdown","4a33624e":"markdown","b0c4dd20":"markdown","bd9367ee":"markdown","a82351ae":"markdown","1afb9265":"markdown","80821ae3":"markdown","c6cc13b9":"markdown","9a137797":"markdown","89aab689":"markdown","682c027f":"markdown","61d82899":"markdown","6ce0393a":"markdown","2cbb815e":"markdown","888d56ec":"markdown","12175db4":"markdown","543b5656":"markdown","5dc3fec4":"markdown","5cedecd2":"markdown","89dfb36f":"markdown","0d3bd7d9":"markdown","8ab23500":"markdown","e06ee606":"markdown","cf9f7ed3":"markdown","2a828ba4":"markdown","274bfa8d":"markdown","bfaa4508":"markdown","4e3ffd8b":"markdown","2ab5b900":"markdown","63a36bf7":"markdown","81b62e84":"markdown","c53126eb":"markdown","989988a2":"markdown","55eda34a":"markdown","38163535":"markdown","1c9d3298":"markdown","47bf29e4":"markdown","9bb0cf82":"markdown","c7f18aa0":"markdown","18f1f14b":"markdown","29cb54c9":"markdown","fea823a3":"markdown","ada6258c":"markdown","8d83eec2":"markdown","d64e1c4c":"markdown","aa9a126e":"markdown","6a473059":"markdown","651d253e":"markdown","19646eef":"markdown","6e0ce1f7":"markdown","a9546279":"markdown","e6cd2a4e":"markdown"},"source":{"419a32a0":"\n# Project packages.\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport numpy as np\n\n# Visualisations.\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Statistics.\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom statistics import mode\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Machine Learning.\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn import metrics\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\n# Filter out warnings when fitting.\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","bce90713":"#Dictionary containing the descriptions of all columns in the data set.\ndesc = {\n 'SalePrice': \"The property's sale price in dollars. This is the target variable to be predicted\",\n 'MSSubClass': \"The building class\",\n 'MSZoning': \"The general zoning classification\",\n 'LotFrontage': \"Linear feet of street connected to property\",\n 'LotArea': \"Lot size in square feet\",\n 'Street': \"Type of road access\",\n 'Alley': \"Type of alley access\",\n 'LotShape': \"General shape of property\",\n 'LandContour': \"Flatness of the property\",\n 'Utilities': \"Type of utilities available\",\n 'LotConfig': \"Lot configuration\",\n 'LandSlope': \"Slope of property\",\n 'Neighborhood': \"Physical locations within Ames city limits\",\n 'Condition1': \"Proximity to main road or railroad\",\n 'Condition2': \"Proximity to main road or railroad (if a second is present)\",\n 'BldgType': \"Type of dwelling\",\n 'HouseStyle': \"Style of dwelling\",\n 'OverallQual': \"Overall material and finish quality\",\n 'OverallCond': \"Overall condition rating\",\n 'YearBuilt': \"Original construction date\",\n 'YearRemodAdd': \"Remodel date\",\n 'RoofStyle': \"Type of roof\",\n 'RoofMatl': \"Roof material\",\n 'Exterior1st': \"Exterior covering on house\",\n 'Exterior2nd': \"Exterior covering on house (if more than one material)\",\n 'MasVnrType': \"Masonry veneer type\",\n 'MasVnrArea': \"Masonry veneer area in square feet\",\n 'ExterQual': \"Exterior material quality\",\n 'ExterCond': \"Present condition of the material on the exterior\",\n 'Foundation': \"Type of foundation\",\n 'BsmtQual': \"Height of the basement\",\n 'BsmtCond': \"General condition of the basement\",\n 'BsmtExposure': \"Walkout or garden level basement walls\",\n 'BsmtFinType1': \"Quality of basement finished area\",\n 'BsmtFinSF1': \"Type 1 finished square feet\",\n 'BsmtFinType2': \"Quality of second finished area (if present)\",\n 'BsmtFinSF2': \"Type 2 finished square feet\",\n 'BsmtUnfSF': \"Unfinished square feet of basement area\",\n 'TotalBsmtSF': \"Total square feet of basement area\",\n 'Heating': \"Type of heating\",\n 'HeatingQC': \"Heating quality and condition\",\n 'CentralAir': \"Central air conditioning\",\n 'Electrical':\" Electrical system\",\n '1stFlrSF': \"First Floor square feet\",\n '2ndFlrSF': \"Second floor square feet\",\n 'LowQualFinSF': \"Low quality finished square feet (all floors)\",\n 'GrLivArea': \"Above grade (ground) living area square feet\",\n 'BsmtFullBath': \"Basement full bathrooms\",\n 'BsmtHalfBath': \"Basement half bathrooms\",\n 'FullBath': \"Full bathrooms above grade\",\n 'HalfBath': \"Half baths above grade\",\n 'Bedroom': \"Number of bedrooms above basement level\",\n 'Kitchen': \"Number of kitchens\",\n 'KitchenQual': \"Kitchen quality\",\n 'TotRmsAbvGrd': \"Total rooms above grade (does not include bathrooms)\",\n 'Functional': \"Home functionality rating\",\n 'Fireplaces': \"Number of fireplaces\",\n 'FireplaceQu': \"Fireplace quality\",\n 'GarageType': \"Garage location\",\n 'GarageYrBlt': \"Year garage was built\",\n 'GarageFinish': \"Interior finish of the garage\",\n 'GarageCars': \"Size of garage in car capacity\",\n 'GarageArea': \"Size of garage in square feet\",\n 'GarageQual': \"Garage quality\",\n 'GarageCond': \"Garage condition\",\n 'PavedDrive': \"Paved driveway\",\n 'WoodDeckSF': \"Wood deck area in square feet\",\n 'OpenPorchSF': \"Open porch area in square feet\",\n 'EnclosedPorch': \"Enclosed porch area in square feet\",\n '3SsnPorch': \"Three season porch area in square feet\",\n 'ScreenPorch': \"Screen porch area in square feet\",\n 'PoolArea': \"Pool area in square feet\",\n 'PoolQC': \"Pool quality\",\n 'Fence': \"Fence quality\",\n 'MiscFeature': \"Miscellaneous feature not covered in other categories\",\n 'MiscVal': \"Value of miscellaneous feature\",\n 'MoSold': \"Month Sold\",\n 'YrSold': \"Year Sold\",\n 'SaleType': \"Type of sale\",\n 'SaleCondition': \"Condition of sale\"\n}","18240a2e":"#Reading from and Dumping to outfile for description dictionary\nimport json\njson.dump(desc, open(\"desc.json\",'w'))\n\nwith open(\"desc.json\", \"r\") as read_file:\n    d= json.load(read_file)","f5cd29f3":"\"\"\"\nCustom function that yields a dictionary of missing data in a DataFrame \n\"\"\"\ndef percent_missing(df):    \n    cols = list(df.columns)\n    outputDict = {}\n    for x in range(len(cols)):\n        key = cols[x]\n        if (df[cols[x]].isnull().sum()) > 0:\n            outputDict[key] = round(((df[cols[x]].isnull().sum()) \/ len(df)*100),2)\n    return outputDict\n\n\"\"\"\nCustom function that yields an overview of missing data in a DataFrame or a list \nof column names with missing data based on data type (Dependant on 'return_' parameter input, 'N'-numerical, 'C'-categorical)\n\"\"\"\ndef percent_missing_overview(df,return_ = 'None'):\n    \n    \n    numerical = [x for x in df.columns if df.dtypes[x] != 'object']\n    categorical = [x for x in df.columns if df.dtypes[x] == 'object']\n    missing_keys = percent_missing(df).keys()\n    \n    if return_ == 'None':\n        #numerical = [x for x in df.columns if df.dtypes[x] != 'object']\n        #categorical = [x for x in df.columns if df.dtypes[x] == 'object']\n        print(\"---\")\n        #missing_keys = list(percent_missing(train).keys())\n        print(\"Missing Data:\")\n        print(\"---\")\n        print(\"Numerical : \",\"(\",len([x for x in numerical if x in missing_keys]),\")\")\n        for x in numerical:\n            if x in missing_keys:\n                print(\" (\",percent_missing(df)[x],\"%)\",x,\":\",desc[x])\n        print(\"---\")\n        print(\"Categorical : \",\"(\",len([x for x in categorical if x in missing_keys]),\")\")\n        for x in categorical:\n            if x in missing_keys:\n                print(\" (\",percent_missing(df)[x],\"%)\",x,\":\",desc[x])\n        print(\"---\")        \n    if return_ == 'N':\n        return [x for x in numerical if x in missing_keys]\n    if return_ == 'C':\n        return [x for x in categorical if x in missing_keys]","ab8b115f":"#Prints out a bried overview of the shape and column data types of a DataFrame\ndef data_overview(df):\n    print(\"---\")\n    print(\"Data Overview\")\n    print(\"---\")\n    numerical = [x for x in df.columns if df.dtypes[x] != 'object']\n    print(\"There are\" , len(numerical) , \"numerical features\")\n    categorical = [x for x in df.columns if df.dtypes[x] == 'object']\n    print(\"There are\" , len(categorical) , \"categorical features\")\n    print(\"---\")\n    print(\"Shape : \")\n    print(df.shape)","b7484ade":"#Reading in Train and Test data from CSV files.\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","f675737c":"#The ID column is retained in temporary variables before being dropped.\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\n#Lists of column names of predictor variables whether numerical or categorical\nnumerical = [x for x in train.columns if train.dtypes[x] != 'object']\n#Dropping the target variable as it is not a predictor\nnumerical.remove('SalePrice')\ncategorical = [x for x in train.columns if train.dtypes[x] == 'object']\n\n#Retaining the predictor variable before dropping.\ny = train.SalePrice.reset_index(drop=True)\n\n#Creating variables to hold our features before EDA.\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n#Combining features from test and train set to perform uniform preprocessing later.\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\n\ndata_overview(features)","de2cae2b":"# Checking for outliers in GrLivArea as indicated in dataset documentation\nplt.figure(figsize=(10,8))\nax = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\nplt.show()","15d73063":"# Removing two very extreme outliers in the bottom right hand corner\nfeatures = features.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\ntrain.reset_index(drop=True, inplace=True)\n\n# Re-check graph\nplt.figure(figsize=(10,8))\nax = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\nplt.show()","df2cf838":"#Acquiring mu and sigma for normal distribution plot of 'SalePrice'\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Plotting distribution plot of 'SalePrice'\nplt.figure(figsize=(8,8))\nax = sns.distplot(train['SalePrice'] , fit=norm);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')","1b2a2ecd":"#Plotting Q-Q plot\nplt.figure(figsize=(8,8))\nax = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","e4ce2be7":"fig, ax = plt.subplots(figsize=(16,16), ncols=2, nrows=2)\n\nsns.distplot(y, kde=False,color = 'green', ax=ax[0][0], fit=stats.norm)\nsns.distplot(y, kde=False,color = 'green', ax=ax[0][1], fit=stats.johnsonsu)\nsns.distplot(y, kde=False,color = 'green', ax=ax[1][0], fit=stats.lognorm)\nsns.distplot(y, kde=False,color = 'green', ax=ax[1][1], fit=stats.johnsonsb)\n\nax[0][0].set_title(\"Normal\",fontsize=24)\nax[0][1].set_title(\"Johnson SU\",fontsize=24)\nax[1][0].set_title(\"Log Normal\",fontsize=24)\nax[1][1].set_title(\"Johnson SB\",fontsize=24)","113cc272":"# Applying a log(1+x) transformation to SalePrice\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.reset_index(drop=True)","b4ccc7be":"#Plotting distribution plot of 'SalePrice'\nplt.figure(figsize=(8,8))\nax = sns.distplot(train['SalePrice'] , fit=stats.johnsonsu);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Johnson SU'],\n            loc='best')","7b512662":"#Plotting Q-Q plot\nplt.figure(figsize=(8,8))\nax = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","f69e121d":"# Creating a correlation matrix to plot.\ncorr = train.corr()\n# Creating a mask to filter out unnecessary correlations\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(16, 10))\nplt.title('Correlation Matrix', fontsize=18)\n# Plotting the correlation matrix\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.4, annot_kws={'size':20})\nplt.show()","4e5df91f":"# show higher correlations between predictor variables and the response variable\ncorr_dict = train.corr()['SalePrice'][(train.corr()['SalePrice'] > 0.5) & (train.corr()['SalePrice'] < 1.0)].sort_values(ascending=False).to_dict()\nprint(\"Higher correlations between predictor variables and the response variable \")\nprint(\"-------------------------------------------------------------------------\")\nfor x in corr_dict.keys():\n    print(\"(\" , round(corr_dict[x],3), \")\",x, \":\" , \"  -\",desc[x])","f6227cc0":"data_overview(features)","cabd282f":"# Visualising missing data.\nf, ax = plt.subplots(figsize = (10, 6))\nplt.xticks(rotation = '90')\nsns.barplot(x = list(percent_missing(features).keys()), y = list(percent_missing(features).values()))\nplt.xlabel('Features', fontsize = 15)\nplt.ylabel('Percentage of missing values (%)', fontsize = 15)\nplt.title('Missing Data', fontsize = 15)\n\n# Generate missing data report using a custom function.\npercent_missing_overview(features) ","82e9e2a0":"#Converting categorical predictors that are stored as numbers to strings\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","057ed67c":"# Imputing above mentioned categorical features to 'None'.\nfor f in ('MasVnrType','Alley','PoolQC', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',):\n    features[f] = features[f].fillna('None')","ad1aa6bf":"# Imputing 'MiscFeature' to 'None'.\nfeatures['MiscFeature'] = features['MiscFeature'].fillna('None')","5f6274ec":"# Imputing remaining categorical features to mode.\nfor f in ('Exterior1st', 'Exterior2nd', 'SaleType','Utilities'):\n    features[f] = features[f].fillna(features[f].mode()[0])\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")    \n# Grouping property class features and imputing with the most frequent entry.\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Final imputation of categorical features\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))","f4159f7d":"features[percent_missing_overview(features,'N')].describe().T.round(3)","a4b01ef1":"features[features['GarageYrBlt'] == 2207][['YearBuilt','YearRemodAdd','GarageYrBlt']]","fd99464b":"# Changing entry from 2207 to 2007.\nfeatures.GarageYrBlt.iloc[2592] = 2007","2680eace":"# Imputing other Garage features to 0.\nfor f in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    features[f] = features[f].fillna(0)","325ed3ea":"# Imputing Basement features to 0.\nfor f in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    features[f] = features[f].fillna(0)","5c0abc8e":"# Imputing 'MasVnrArea' feature to 0.\nfeatures['MasVnrArea'] = features['MasVnrArea'].fillna(0)","6dc41e78":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","ed332d6d":"# Checking for missing data after imputation.\npercent_missing_overview(features) ","ea8455ad":"# Recreating list of numerical features\nnumerical = [x for x in features.columns if features.dtypes[x] != 'object']\n# Calculating skewness\nskewed = features[numerical].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed})\nskewness","f3fb2fa8":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\nskew_features = features[numerics2].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","f27ee3d2":"# Creating simplified features\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasGarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_SQR_Footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_Porch_SF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","2ad40041":"final_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)","058bbefe":"# Splitting up features for training and prediction.\nX = final_features.iloc[:len(y), :]\nX_pred = final_features.iloc[len(X):, :]\n\nX_train = X\nX_test = X_pred\ny_train = y\n\nprint('X', X.shape, 'y', y.shape, 'X_pred', X_pred.shape)","cc3b6e4e":"# RMSE scoring function with cross validation\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)","931f3667":"# Possible list of alpha values.\nalphas = [5,10,15,20,25,30]\n\n# Iterate over alpha's\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\n# Plot findings\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","a9ee7249":"# Creating Ridge Regression Model with estimated alpha\nmodel_ridge = Ridge(alpha = 10)","21d0ba81":"# Setting up list of alpha's\nalphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n# Iterate over alpha's\ncv_lasso = [rmse_cv(Lasso(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","6060b2db":"# Creating Lasso Regression Model with estimated alpha\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0004))","013b9bb4":"# Calculating RMSE score with estimated alpha values\ncv_ridge = rmse_cv(model_ridge).mean()\ncv_lasso = rmse_cv(model_lasso).mean()","eb3e4e96":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Ridge','Lasso'],\n    'Score': [cv_ridge,cv_lasso]})\n\n# Build dataframe of values\nresult_df = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nresult_df.head()","54e64e52":"# Fitting models and calculating predictions.\nmodel_lasso.fit(X_train, y_train)\nlasso_pred = np.expm1(model_lasso.predict(X_test))\n\nmodel_ridge.fit(X_train, y_train)\nridge_pred = np.expm1(model_ridge.predict(X_test))\n","901f05c0":"#Stacking\nStack = (lasso_pred + ridge_pred) \/ 2","69842d77":"submission = pd.DataFrame({'Id':test_ID, 'SalePrice':lasso_pred})\nsubmission.to_csv('Stack.csv', index=False)","62cf92c8":"It can be seen above that the 'SalePrice' does not follow a Normal Distribution.\nThis indicates that the target variable ('SalePrice') is right skewed.","9c99bb7e":"It is clear to see that the overall quality of the house and the amount of living area above ground are the highest correlated predictors to the target variable.","acca10a9":"Ridge and Lasso are regularized linear regression methods. They are both simple techniques which reduce model complexity and prevent over-fitting. The difference between the ridge and lasso regression lies in how the regularization term is calculated.\n\nLasso penalizes the absolute size of coefficients which leads to coefficients that can be exactly 0. It also offers automatic feature selection because it can completely remove some features.\n\nRidge regression penalizes the squared size of coefficients which leads to smaller coefficients, but it doesn't force them to 0.  It will not get rid of irrelevant features but will instead minimize their impact on the trained model.","22e7f1e7":"### ->Imputation (Numerical Features)","250f3701":"### Simplified features","85f9bf4d":"Whilst doing a log('SalePrice') transformation will yield satisfactory results, the more optimal choice would be a log1p('SalePrice') transformation.\n\nThe log1p(x) calculates log(1 + x) and returns the natural logarithm of one plus the input array, element-wise.\n\nSince for real-valued input, log1p is accurate also for x so small that 1 + x == 1 in floating-point accuracy.\nThis will help getting the target variable to follow an unbounded Johnson distribution better accounting for the fact that the distribution goes to infinity in both upper or lower tail.","b42fed76":"<img src=\"https:\/\/kimcranehomes.com\/wp-content\/uploads\/sites\/20\/2018\/02\/market-stats.jpg\" style=\"width: 600px\">","22be2fda":"## Predictions","4a33624e":"Applying a power transformation to these numerical features will allow the data to become more Gaussian-like.\n\nThe Box-Cox transformation will be applied on features with a skewness greater than 0.5 using a lambda value obtained from boxcox_normmax(1+x).","b0c4dd20":"From the above plot selecting an alpha value of 10 would be sufficient.","bd9367ee":"## Data Import","a82351ae":"## Exploratory Data Analysis","1afb9265":"### Model Scoring","80821ae3":"###  Target variable ('SalePrice')","c6cc13b9":"### Missing Data","9a137797":"The above Q-Q plot confirms a skewness in the target variable ('SalePrice').\nThis entails a transformation being done before an accurate regression model can be built.\n\nBy mapping the target variable to various distributions a decision can be made on what kind of transformation to select.","89aab689":"### Lasso","682c027f":"The features pertaining to the Garage will be imputed to zero as their missing values most likely indicate a lack of a Garage.\n\nThere is however a house with 'GarageYrBlt' set to 2207, this is obvisouly an error.\nLooking at that partiuclar entry it can be seen the house was built in 2006 and a remodel was done in 2007.\nThe remodel most likely included the addition of a garage.\nSo the 'GarageYrBlt' entry for this house will be changed to 2007.","61d82899":"Looking at the descriptive statistics of these numerical features with missing data will allow a better decision being made as to impute these features with a mean or median or zero.","6ce0393a":"###  Q\u2013Q (quantile-quantile) plot","2cbb815e":"# House Prices Regression","888d56ec":"It can be seen that both Ridge and Lasso score very similarly on this data.","12175db4":"# Model Building","543b5656":"### Correlation","5dc3fec4":"In order to find the optimal alpha value for this model, a number of models will be created iteratively using various alpha values with their RMSE scores being plotted against one another , allowing for the alpha value of the model with the lowest RMSE score to be selected.\n\nThe alpha parameter needs to greater than 0 as alpha = 0 will yield the same coefficients as simple linear regression.","5cedecd2":"## EDSA - Team 7","89dfb36f":"The remaining categorical features have very low percentages of missing data (<1%).\nIt would be better to impute these would the mode of their respective columns as these features don't show a lack of some attribute such as a Basement or Garage.","0d3bd7d9":"The cross_val_score function from sklearn.model_selection will be used to calculate the RMSE score for the models. The fucntion will be scoring the mean squared error with a cross validation splitting strategy of 10 folds.\n\nRoot Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). \nResiduals are a measure of how far from the regression line data points are.\n\nThis function will also aid in finding the optimal alpha value when building the models.","8ab23500":"Finding the correlation between the target variable and other predictor varibles is crucial to model development when it comes to feature selection impacting the overall quality of the model.","e06ee606":"These features are intended to show that a house has certain attributes based on its other features. \nSuch as having a Garage, Basement or Pool.","cf9f7ed3":"###  Outliers\n\nThe Ames dataset documentation reveals two outliers in the feature GrLivArea (Above grade (ground) living area square feet)","2a828ba4":"There are some numerical features that would serve better as categorical features as they represent various categories pertaining to condition and quality as well as building class.\n\nThe aforementioned features are : \n- MSSubClass  : (The building class)\n- YrSold      : (Year Sold)\n- MoSold      : (Month Sold)\nAs such these features will have their data type changed to string (object).","274bfa8d":"Based on the above distribution and Q-Q plots, the target variable now looks ready for modelling.","bfaa4508":"## Package Imports","4e3ffd8b":"For the categorical features with missing data pertaining to toward the Garage, Basement, Fence, Fireplace, Alley access and Pool. These features will be changed to 'None' as the houses most likely lack these attributes.","2ab5b900":"After dropping the 'ID' and target variable 'SalePrice' columns,\nthe data set contains 79 features and 2917 entries.","63a36bf7":"From the above Heatmap we can see that there are a decent amount of predictor variables highly correlated with the target variable.","81b62e84":"The skewness for the numerical features will be calculated using scipy.stats.skew(x), as for normally distributed data, the skewness should be about 0. Since a log(1+x) transformation was done to the target variable the same needs to be done for the predictor variables depending on their skewness.","c53126eb":"The 'LotFrontage' feature which describes the linear number feet of street connected to the property\nhas 16.65 % of data missing. This feature will be imputed with median after grouping the data by neighbourhood as each neighbourhood would have an allocated amount of street connected to properties depending on the neighbourhood.","989988a2":"### ->Imputation (Categorical Features)","55eda34a":"### Feature Engineering","38163535":"### Ridge","1c9d3298":"### Transformation of Target Variable ('SalePrice')","47bf29e4":"It can be seen above that the 'SalePrice' does not follow a Normal Distribution.\nWhilst the LogNormal and Johnson SB (Bounded) Distributions are fit quite well,\nThe Johnson SU (Unbounded) Distribution is fit more optimally.\n\nThe Johnson's SU distribution (Unbounded) is a distribution that goes to infinity in both the upper or lower tail.","9bb0cf82":"Evaluation of the target variable is important as it will allow us to gain a better understanding of how it relates to the predictor variables as well as detecting any possible skew.","c7f18aa0":"### Getting Dummies","18f1f14b":"A Q-Q plot can also be used to determine the presence of skew.\nIn statistics, a Q\u2013Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. ","29cb54c9":"### Skewness","fea823a3":"###  Distribution Plot","ada6258c":"Features with missing data most likely relay the fact that those particular house entries lack the attribute that these features describe.\n\nFor instance, the 'FireplaceQu' feature which describes Fireplace quality with 48.65 % data missing would entail that those houses most likely lack a fireplace.","8d83eec2":"The MiscFeature feature that describes a miscellaneous feature not covered in other categories has 96.4 % missing data, it would be safe to also impute this feature to 'None'.","d64e1c4c":"## Custom Functions and Variables","aa9a126e":"### Data Shape","6a473059":"The 'MasVnrArea' feature which represents the Masonry veneer area in square feet only has 0.79 % data missing which is a small amount relative to the overall size of the dataset.\nHence this feature will also be imouted to 0.","651d253e":"The majority of numerical features with missing pertain to features regarding the Basement and Garage.\n","19646eef":"## Models","6e0ce1f7":"## Preliminary Data Overview","a9546279":"## Data pre-processing","e6cd2a4e":"\nIt is clear that the 2 data points in the bottom right are outliers,\nIt's not always appropriate to delete outliers - removing too many can detriment the model's quality.\n\nHowever with backing from the data set author's documentation,\nWe shall remove these and compare the new plot.\n\n"}}