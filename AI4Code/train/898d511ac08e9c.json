{"cell_type":{"fe8a7028":"code","a1dc144a":"code","57953fd4":"code","1d67c504":"code","fcb82f09":"code","562a6968":"code","7d5707fc":"code","dc73932e":"code","76cc1e36":"code","f6adb9e9":"code","7e58db79":"code","eb7d9c72":"code","e7ff649e":"code","4873fdb3":"code","7fface9c":"code","6787d716":"code","b6d521f6":"code","dca3edcc":"code","ad9cad43":"code","6abc0dd7":"code","f603e48c":"code","2a3594e3":"code","2be16f2a":"code","82678fa3":"code","b431494e":"code","0b5395f1":"code","7f0255a6":"code","f0cda509":"code","6718ef8f":"markdown","4411f6ae":"markdown","28626979":"markdown","165c80e8":"markdown","f1826257":"markdown","d83f5bb0":"markdown","1c68c988":"markdown","8e67ad9d":"markdown"},"source":{"fe8a7028":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1dc144a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport pylab\npylab.rcParams['figure.figsize'] = (15.0, 10.0)\n\nimport warnings\nwarnings.filterwarnings('ignore')","57953fd4":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","1d67c504":"df.info()","fcb82f09":"df.describe()","562a6968":"df.hist(bins=10)","7d5707fc":"df.isnull().sum()","dc73932e":"sns.boxplot(data = df, orient= 'v', palette=\"pastel\")\nplt.title(\"Distribui\u00e7\u00e3o das caracter\u00edsticas dos registros\")","76cc1e36":"sns.pairplot(df, hue=\"Outcome\", palette=\"Set1\")","f6adb9e9":"ax = sns.boxplot(x=\"Outcome\", y=\"Age\", data=df)\n\nax.figure.set_size_inches(14, 8)\nax.set_title(\"Distribui\u00e7\u00e3o das Idades entre Diab\u00e9ticos e n\u00e3o diabeticos\", fontsize=14)\nax.set_xlabel(\"Quantidade de Diab\u00e9ticas\", fontsize= 14)\nax.set_ylabel(\"Idade\", fontsize = 14)\nax.set_xticklabels([\"Diabeticos \", \"Nao Diabeticos\"], fontsize=14)\nax","7e58db79":"df_correlacao = df.corr()\n\nmask = np.triu(np.ones_like(df_correlacao, dtype=np.bool))\nplt.figure(figsize=(12, 8))\nheatmap = sns.heatmap(df_correlacao, annot=True, cmap='RdBu', fmt='.2f', mask=mask, square=True, linecolor=\"white\")\nheatmap.set_title(\"Correla\u00e7\u00e3o entre as caracter\u00edsticas\")","eb7d9c72":"x = df.drop([\"Outcome\"], axis=1, inplace=False)\ny = df[\"Outcome\"]","e7ff649e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0, 1))\nX = scaler.fit_transform(x)\ncolunas = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Pregnancies', 'DiabetesPedigreeFunction', 'Age']\nX = pd.DataFrame(X)\nX.columns = colunas","4873fdb3":"from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \nnum_folds = 10 # numero folds da valida\u00e7\u00e3o cruzada\nnum_instances = len(X) #numero de instancias\nseed = 7 # numero do seed garante que o resultado seja sempre o mesmo","7fface9c":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nteste = SelectKBest(score_func = chi2, k=5)\nfit = teste.fit(X,y)\n\nprint(fit.scores_)\nfeatures = fit.transform(X)\n\nfeatures = pd.DataFrame(features)\nprint(features)","6787d716":"from sklearn.tree import DecisionTreeClassifier\n\narvore = DecisionTreeClassifier()\narvore.fit(x_train, y_train)\nresult = arvore.predict(x_test)","b6d521f6":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nprint(arvore.score(x_test, y_test))\nprint(confusion_matrix(y_test, result))\nprint(metrics.classification_report(y_test, result))","dca3edcc":"arvore.feature_importances_","ad9cad43":"feature_imp = pd.Series(arvore.feature_importances_, index=X.columns)","6abc0dd7":"feature_imp.sort_values(ascending=False)","f603e48c":"plt.figure(figsize=(12, 6))\nfeature_imp_sort = feature_imp.sort_values(ascending=False)\nsns.barplot(x=feature_imp_sort, y=feature_imp_sort.index)\nplt.title(\"Import\u00e2ncia de Features\")\nplt.show()","2a3594e3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n","2be16f2a":"from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split, GridSearchCV\n\n# Definindo os valores para o n\u00famero de folds\nnum_folds = 5\nnum_instances = len(X)\nseed = 7\n\n# Preparando o modelo\n\nmodelos = []\nmodelos.append((\"LogisticRegression\", LogisticRegression()))\nmodelos.append((\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=0, max_depth=30, max_features=5)))\nmodelos.append((\"KNeighborsClassifier\", KNeighborsClassifier(n_neighbors=17, p=1)))\nmodelos.append((\"MLPClassifier\", MLPClassifier(hidden_layer_sizes=(20, 10, 20), max_iter=3000)))\nmodelos.append((\"SuportVectorMachine\", SVC(kernel='linear', gamma='auto')))\nmodelos.append((\"NaiveBayes\", GaussianNB()))\nmodelos.append((\"RandomForestClassifier\", RandomForestClassifier(n_estimators=200, criterion='entropy', n_jobs=-1, max_depth=100, \n                                                                 bootstrap=True, random_state=0)))\n\n# Avaliando cada modelo\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n   kfold = KFold(n_splits = num_folds, shuffle=True, random_state = seed)\n   cv_results = cross_val_score(modelo, X, y, cv = kfold, scoring = 'f1_macro')\n   resultados.append(cv_results)\n   nomes.append(nome)\n   msg = \"%s - F1 Score: %f, Desvio Padr\u00e3o: %f\" % (nome, cv_results.mean(), cv_results.std())\n   print(msg)","82678fa3":"df_resultado = pd.DataFrame(resultados, columns=[\"Resultado1\", \"Resultado2\", \"Resultado3\", \"Resultado4\", \"Resultado5\"])\ndf_resultado","b431494e":"df_nomes = pd.DataFrame(nomes, columns=[\"Nomes\"])\ndf_nomes","0b5395f1":"nome_resultado = pd.concat([df_nomes, df_resultado], axis=1)\nnome_resultado","7f0255a6":"nome_resultado[\"Media\"] = (nome_resultado[\"Resultado1\"]+ nome_resultado[\"Resultado2\"]+ nome_resultado[\"Resultado3\"]+ \n                           nome_resultado[\"Resultado4\"]+ nome_resultado[\"Resultado5\"]) \/ 5\nnome_resultado.sort_values(by=\"Media\", ascending=False)","f0cda509":"fig = plt.figure(figsize = (18, 6))\nsplot = sns.barplot(x=\"Nomes\", y=\"Media\", data=nome_resultado.sort_values(by=\"Media\", ascending=False))\n# percorrendo cada barra e calculando sua altura para imprimir no gr\u00e1fico\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.6f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"M\u00e9dia F1 dos modelos treinados\")","6718ef8f":"**Encontrando as features mais importantes**","4411f6ae":"**Criando uma coluna com a media dos resultados obtidos em cada treinamento**","28626979":"**Agora podemos juntar o Dataframe de nomes e os resultados ap\u00f3s cada treinamento**","165c80e8":"Feature Selection","f1826257":"Decision Tree Classifier","d83f5bb0":"**Transformando esses resultados em um Dataframe**","1c68c988":"**Visualizando as features mais importantes**","8e67ad9d":"**Testando os modelos**"}}