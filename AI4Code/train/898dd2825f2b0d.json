{"cell_type":{"54260cb5":"code","afb623f5":"code","df2c339b":"code","498d0988":"code","f7721906":"code","2a050f6b":"code","20bbe62d":"code","98c9409c":"code","8e1a3361":"code","59875f46":"code","61343ebc":"code","eb267497":"code","7e417383":"code","feac1eee":"code","9cbb2f71":"markdown","0fcf0175":"markdown","170d0e88":"markdown","d18a4564":"markdown","8882f4de":"markdown"},"source":{"54260cb5":"# This Python 3 environment \n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint('CWD : ', os.getcwd())","afb623f5":"df = pd.read_csv('..\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv')\n\ndf.info() # No missing values\n\n# Attribute Information:\n# 1. variance of Wavelet Transformed image (continuous)\n# 2. skewness of Wavelet Transformed image (continuous)\n# 3. curtosis of Wavelet Transformed image (continuous)\n# 4. entropy of image (continuous)\n# 5. class (integer)","df2c339b":"df['class'].value_counts() # Fairly balanced","498d0988":"df.describe(include='all')","f7721906":"sns.pairplot(df, hue='class') # Classes appear to be distinguishable","2a050f6b":"sns.heatmap(df.corr(), annot=True, fmt='.2f', mask=np.triu(np.ones((df.corr().shape)), k=1), vmin=-1, vmax=1, cmap='viridis')","20bbe62d":"X = df.drop(columns='class')\ny = df['class'].copy()","98c9409c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=42)","8e1a3361":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix","59875f46":"name = 'john'\nprint('hello: {}'.format(name))","61343ebc":"# Logistic Regression\nlogm = LogisticRegression()\nlogm.fit(X_train, y_train)\nprint(confusion_matrix(y_test, logm.predict(X_test)), '\\n')\nprint('Intercept = {:.4f}'.format(logm.intercept_[0]))\npd.DataFrame(data=logm.coef_.reshape(-1,1), index=X.columns, columns=['Coeffs'])","eb267497":"# SVC (radial kernel by default)\nsvc =  SVC()\nsvc.fit(X_train, y_train)\nprint(confusion_matrix(y_test, svc.predict(X_test)))","7e417383":"# Random Forest \nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nprint(confusion_matrix(y_test, rf.predict(X_test)))\n\npd.DataFrame(data=rf.feature_importances_, index=X.columns, columns=['Feature Importance'])","feac1eee":"# To visualize the test data\n# sns.pairplot(pd.concat([X_test, y_test], axis=1), hue='class')","9cbb2f71":"---","0fcf0175":"## Summary:  \n* Data is super clean (well balanced b\/w classes, no missing values, no discernible outliers, no extreme correlations etc.)  - *upside of academic datasets* \n* Scale of different features also appears to be comparable - *we don't normalise the data to begin with*\n* The classes sppear to be distinguishable based on pair plots - *let's start with simple out-of-box models*\n* Voila! Out-of-box models - Logistic, SVC, RF - without any customization or parameter-tuning yield ~100% accurate results\n* **In the absence of any other specific requirement, the suggestion would be to go with the simplest\/most interpretable model, i.e Logistic Regression**","170d0e88":"## Model fitting \ud83d\udd25\ud83d\udd25\ud83d\udd25","d18a4564":"## Data Load & Review","8882f4de":"---"}}