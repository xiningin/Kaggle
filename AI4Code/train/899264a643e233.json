{"cell_type":{"4e44cbb3":"code","747f21c3":"code","24edb634":"code","8f54cae8":"code","47d09029":"code","50925ff6":"code","d058383c":"code","d8c8a7f7":"code","4d3e4dcc":"code","040eb66b":"code","28b82622":"code","68d2f800":"code","d612a3c4":"code","51eb1ac2":"code","a7a18059":"code","b9006925":"code","37747972":"code","2db3e08e":"code","4af9bd7c":"code","a60a7458":"code","150a548c":"code","69981f37":"code","aeaeed42":"code","7022a4c1":"code","63734342":"code","4d0511d0":"code","94156d89":"code","8e4caf0c":"code","4571c2c4":"code","91e34ac5":"code","81a4399a":"code","cd010c9f":"code","aeab0431":"code","2d5cd56c":"code","4d72bb81":"code","080e849d":"code","536ee3ab":"code","f94f73e0":"code","36fc3342":"code","79837954":"code","992a895e":"code","899d6212":"code","783d98ae":"code","90958cac":"code","212eeced":"code","a954165b":"markdown","8b9bcca5":"markdown","81ec9026":"markdown","11ec83d0":"markdown","415ee41e":"markdown","c66e5a18":"markdown","5e8fc6cc":"markdown","4decaf2d":"markdown","7aa478f1":"markdown","ff172115":"markdown","a75f38f5":"markdown","dc115d1d":"markdown","f46e9d2e":"markdown","ae2bd26e":"markdown","c4d80f9a":"markdown","147c151d":"markdown","891b3076":"markdown","2678622c":"markdown","077d4a73":"markdown","e6b84ae5":"markdown","fc8e7c1c":"markdown","fca9fccd":"markdown","c3e0a476":"markdown","8865705f":"markdown","736c69c6":"markdown","43e4601d":"markdown","20f2f1c6":"markdown","ec0af1f7":"markdown","c44bc61b":"markdown","e0ee45fc":"markdown","8a552ea9":"markdown","6b79c8b5":"markdown","b60e3ad1":"markdown","cf57cb3d":"markdown","37f24eff":"markdown","8eba41e0":"markdown","726c1f14":"markdown","903fc74d":"markdown","7df52696":"markdown","d62aa556":"markdown","c059cc34":"markdown","e747b817":"markdown","deaa173d":"markdown","76560dcb":"markdown","e37de1af":"markdown","67e7d615":"markdown","839fd2bc":"markdown","b691f0da":"markdown"},"source":{"4e44cbb3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC","747f21c3":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","24edb634":"df.head()","8f54cae8":"df.shape","47d09029":"df.info()","50925ff6":"df.describe().T","d058383c":"df.drop([\"id\", \"Unnamed: 32\"], axis=1, inplace=True)","d8c8a7f7":"df.isnull().sum()","4d3e4dcc":"df.duplicated().sum()","040eb66b":"sns.set()\n\ncols_to_pairplot = df.columns[:11]\nsns.pairplot(df[cols_to_pairplot], hue=\"diagnosis\")\nplt.legend()\nplt.show()","28b82622":"plt.figure(figsize=(10,8))\nsns.countplot(df[\"diagnosis\"])\nplt.title(\"diagnosis\", size=15)\nplt.show()\n\nfor col in df.drop(\"diagnosis\", axis=1).columns:\n    plt.figure(figsize=(10,8))\n    sns.distplot(df[col])\n    plt.title(f\"{col}\", size=15)\n    plt.show()","68d2f800":"for col in df.drop(\"diagnosis\", axis=1).columns:\n    plt.figure(figsize=(10,8))\n    sns.barplot(x=df[\"diagnosis\"], y=df[col])\n    plt.title(f\"{col} and diagnosis\", size=15)\n    plt.show()","d612a3c4":"plt.figure(figsize=(14,10))\nsns.heatmap(df.corr(), cmap=\"Blues\")\nplt.title(\"Correlations Between Variables\", size=16)\nplt.show()","51eb1ac2":"X = df.drop(\"diagnosis\", axis=1)\ny = df[\"diagnosis\"].replace({\"B\": 0, \"M\": 1})","a7a18059":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","b9006925":"pca = PCA()\npca.fit(X)\n\nplt.figure(figsize=(12,8))\nplt.plot(pca.explained_variance_ratio_)\nplt.title(\"N Components and Explained Variance Ratio\", size=15)\nplt.xlabel(\"N Components\")\nplt.ylabel(\"Explained Variance Ratio\")\nplt.show()","37747972":"pca = PCA(n_components = 5)\nX = pca.fit_transform(X)","2db3e08e":"pca.explained_variance_ratio_.sum()","4af9bd7c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","a60a7458":"models = pd.DataFrame(columns=[\"Model\", \"Accuracy Score\"])","150a548c":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\npredictions = log_reg.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"LogisticRegression\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","69981f37":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"RandomForestClassifier\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","aeaeed42":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npredictions = gbc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"GradientBoostingClassifier\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","7022a4c1":"svc = SVC()\nsvc.fit(X_train, y_train)\npredictions = svc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"SVC\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","63734342":"models.sort_values(by=\"Accuracy Score\", ascending=False)","4d0511d0":"plt.figure(figsize=(12,8))\nsns.barplot(x=models[\"Model\"], y=models[\"Accuracy Score\"])\nplt.title(\"Models' Accuracy Scores\", size=15)\nplt.xticks(rotation=30)\nplt.show()","94156d89":"def visualize_roc_auc_curve(model, model_name):\n    pred_prob = model.predict_proba(X_test)\n    fpr, tpr, thresh = roc_curve(y_test, pred_prob[:,1], pos_label=1)\n    \n    score = roc_auc_score(y_test, pred_prob[:, 1])\n    \n    plt.figure(figsize=(10,8))\n    plt.plot(fpr, tpr, linestyle=\"--\", color=\"orange\", label=\"ROC curve (area = %0.5f)\" % score)\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n    \n    plt.title(f\"{model_name} ROC Curve\", size=15)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc=\"lower right\", prop={'size': 15})\n    plt.show()","8e4caf0c":"tuned_models = pd.DataFrame(columns=[\"Model\", \"Accuracy Score\"])","4571c2c4":"param_grid_log_reg = {\"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), param_grid_log_reg, scoring=\"accuracy\", cv=5, verbose=0, n_jobs=-1)\n\ngrid_log_reg.fit(X_train, y_train)","91e34ac5":"log_reg_params = grid_log_reg.best_params_\nlog_reg = LogisticRegression(**log_reg_params)\nlog_reg.fit(X_train, y_train)\npredictions = log_reg.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"LogisticRegression\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","81a4399a":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt=\"d\")\nplt.title(\"Confusion Matrix of Logistic Regression\", size=15)\nplt.show()","cd010c9f":"visualize_roc_auc_curve(log_reg, \"Logistic Regression\")","aeab0431":"param_grid_rfc = {\"min_samples_split\": [2, 3, 10],\n                  \"min_samples_leaf\": [1, 3, 10],\n                  \"n_estimators\" :[100, 200, 500]}\n\ngrid_rfc = GridSearchCV(RandomForestClassifier(), param_grid_rfc, scoring=\"accuracy\", cv=5, verbose=0, n_jobs=-1)\n\ngrid_rfc.fit(X_train, y_train)","2d5cd56c":"rfc_params = grid_rfc.best_params_\nrfc = RandomForestClassifier(**rfc_params)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"RandomForestClassifier\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","4d72bb81":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Random Forest\", size=15)\nplt.show()","080e849d":"visualize_roc_auc_curve(rfc, \"Random Forest\")","536ee3ab":"param_grid_gbc = {'n_estimators' : [100, 200, 500],\n                  'learning_rate': [0.1, 0.05, 0.01],\n                  'max_depth': [2, 3, 6],\n                  'min_samples_leaf': [1, 2, 5]}\n\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid_gbc, scoring=\"accuracy\", cv=5, verbose=0, n_jobs=-1)\n\ngrid_gbc.fit(X_train, y_train)","f94f73e0":"gbc_params = grid_gbc.best_params_\ngbc = GradientBoostingClassifier(**gbc_params)\ngbc.fit(X_train, y_train)\npredictions = gbc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"GradientBoostingClassifier\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","36fc3342":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Gradient Boosting\", size=15)\nplt.show()","79837954":"visualize_roc_auc_curve(gbc, \"Gradient Boosting\")","992a895e":"param_grid_svc = {'gamma': [ 0.001, 0.01, 0.1, 1, 10],\n                  'C': [1, 10, 50, 100, 200, 300, 500, 1000]}\n\ngrid_svc = GridSearchCV(SVC(), param_grid_svc, scoring=\"accuracy\", cv=5, verbose=0, n_jobs=-1)\n\ngrid_svc.fit(X_train, y_train)\n","899d6212":"svc_params = grid_svc.best_params_\nsvc = SVC(**svc_params)\nsvc.fit(X_train, y_train)\npredictions = svc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"SVC\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","783d98ae":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Support Vector Machines\", size=15)\nplt.show()","90958cac":"tuned_models.sort_values(by=\"Accuracy Score\", ascending=False)","212eeced":"plt.figure(figsize=(12, 8))\nsns.barplot(x=tuned_models[\"Model\"], y=tuned_models[\"Accuracy Score\"])\nplt.title(\"Models' Accuracy Scores After Hyperparameter Tuning\", size=15)\nplt.xticks(rotation=30)\nplt.show()","a954165b":"***Checking the shape\u2014i.e. size\u2014of the data.***","8b9bcca5":"<h3>Model Comparison Before Hyperparameter Tuning<\/h3>","81ec9026":"***There is no duplicates either, data seems clean so far.***","11ec83d0":"***Defining a ROC AUC Curve visualization function for the convenience of evaluation.***","415ee41e":"# Model Comparison After Hyperparameter Tuning","c66e5a18":"# Data Visualization","5e8fc6cc":"<h3>Random Forest<\/h3>","4decaf2d":"Ten real-valued features are computed for each cell nucleus:\n* **radius** (mean of distances from center to points on the perimeter) --- *Input Variable*\n* **texture** (standard deviation of gray-scale values) --- *Input Variable*\n* **perimeter** --- *Input Variable*\n* **area**\n* **smoothness** (local variation in radius lengths) --- *Input Variable*\n* **compactness** (perimeter^2 \/ area - 1.0) --- *Input Variable*\n* **concavity** (severity of concave portions of the contour) --- *Input Variable*\n* **concave points** (number of concave portions of the contour) --- *Input Variable*\n* **symmetry** --- *Input Variable*\n* **fractal dimension** (\"coastline approximation\" - 1) --- *Input Variable*","7aa478f1":"# Conclusion","ff172115":"<h3>Plotting the Values of Each Variable<\/h3>","a75f38f5":"<h1 style=\"font-family: Times New Roman;\">Thank you so much for reading the notebook. Preparing notebooks is taking a great deal of time. If you liked it, please do not forget to give an upvote. Peace Out \u270c\ufe0f ...<\/h1>","dc115d1d":"* Blue ---> **Malignant**\n* Orange ---> **Benign**","f46e9d2e":"<h3>Train-Test Split<\/h3>","ae2bd26e":"<h3>X, y Split<\/h3>","c4d80f9a":"<h3>Tuning the Support Vector Machines<\/h3>","147c151d":"Principal Component Analysis is an unsupervised statistical technique used for Dimensionality Reduction. It combines the correlated features, creates the same number of features which are uncorrelated to one another and compresses most of the information (variance) into the first components. Hence, it helps us to handle the Multicollinearity which affects the ML models in a bad way.","891b3076":"<h3>Logistic Regression<\/h3>","2678622c":"# Loading the Data","077d4a73":"# Data Preprocessing","e6b84ae5":"# Machine Learning Models","fc8e7c1c":"# <center>Breast Cancer Diagnosis Prediction \ud83c\udf97\ufe0f<\/center>","fca9fccd":"***Learning the dtypes of columns' and how many non-null values there are in those columns.***","c3e0a476":"<h3>Tuning the Gradient Boosting<\/h3>","8865705f":"***Getting the statistical summary of dataset.***","736c69c6":"<h3>Data Standardization<\/h3>","43e4601d":"***It can easily be seen that first 5 components hold over 84% of the variance present in the dataset.***","20f2f1c6":"The output variable that will be predicted is \"diagnosis\" column:\n* **diagnosis** (Malignant or Benign)  - *Output Variable*","ec0af1f7":"***NOTE: If we take a look at the official technical documentation of SVC, we can observe that predict_proba() function may be inconsistent with predict() function. It sucks especially on the small datasets, that's why I don't prefer to plot ROC AUC Curve for SVC model.***","c44bc61b":"# About the Dataset","e0ee45fc":"# Hyperparameter Tuning","8a552ea9":"***Dropping some redundant columns such as \"id\" and \"Unnamed: 32\"***","6b79c8b5":"<h3>Gradient Boosting<\/h3>","b60e3ad1":"<h3>Tuning the Random Forest<\/h3>","cf57cb3d":"***Visualizing the Correlation between the ten numerical real-valued variables using pairplot visualization.***","37f24eff":"***Taking a look at the first 5 rows of the dataset.***","8eba41e0":"<h3>Relationship Between Each Variable and Target Variable (diagnosis)<\/h3>","726c1f14":"**It can easily be observed that Logistic Regression is the one which yields the best scores among the four models that we built with the accuracy score of 0.988304. Also, it is important to pay attention to the increase in the accuracy scores of SVC and Gradient Boosting Classifier.**","903fc74d":"# Exploratory Data Analysis","7df52696":"***Visualizing the linear correlations between variables using Heatmap visualization. The measure used for finding the linear correlation between each variable is Pearson Correlation Coefficient.***","d62aa556":"# Handling Missing Values and Duplicates","c059cc34":"***It looks like first 5 components hold most of the information.***","e747b817":"<h3>Tuning the Logistic Regression<\/h3>","deaa173d":"**Note:** The mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.","76560dcb":"<img src=\"https:\/\/images.unsplash.com\/photo-1598885159329-9377168ac375?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=873&q=80\">","e37de1af":"<h3>Support Vector Machines<\/h3>","67e7d615":"***There is no missing value in the dataset***","839fd2bc":"# Importing Essential Libraries, Metrics, Tools and Models","b691f0da":"# Principal Component Analysis (PCA)"}}