{"cell_type":{"d030ab56":"code","c32219af":"code","5e8f8329":"code","5b69ca1e":"code","d5b63c91":"code","6c6c0305":"code","98bd9c2b":"code","30ba6fb2":"code","c9126b8d":"code","980dafcd":"code","33c46fbd":"code","2ff56671":"code","ce268b5b":"code","2e916312":"code","f79b1e0d":"code","f62aaf18":"code","453ed41e":"code","a001afdd":"code","790dc903":"code","bac1af16":"markdown","04152e29":"markdown","9f6582cb":"markdown","fa7836b5":"markdown","6f0f5bf2":"markdown","c0e9e0b8":"markdown","d95d5f61":"markdown","88bb0051":"markdown","2edcc0a5":"markdown","03c144aa":"markdown","f36c63f1":"markdown","7654bcb7":"markdown","9ac6889a":"markdown","823b0fc8":"markdown","e568084f":"markdown","94406e36":"markdown","e9659331":"markdown"},"source":{"d030ab56":"#Bibliotecas necess\u00e1rias\n\nimport numpy as np\nimport tensorflow as tf\nfrom random import randint\nimport matplotlib.pyplot as plt","c32219af":"def generate_sequence(number_of_features, time_steps):\n    return [randint(0,number_of_features-1) for i in range(time_steps)]","5e8f8329":"def one_hot_encode(number_of_features,x):\n    max_ = number_of_features\n    tmp = np.zeros(shape=(len(x),max_))\n    \n    for i in range(len(x)):\n        tmp[i][x[i]] = 1\n    \n    return tmp\n\ndef one_hot_decode(x):\n    tmp = []\n    for encode in x:\n        tmp.append(np.argmax(encode))\n        \n    return tmp\n\nsequence = generate_sequence(10,5)\nprint(sequence)\nencoded = one_hot_encode(10,sequence)\ndecoded = one_hot_decode(encoded)\nprint(decoded)","5b69ca1e":"def generate_sample(size,time_steps, number_of_features, out_index):\n    \n    X = np.zeros(shape=(size,time_steps,number_of_features))\n    Y = np.zeros(shape=(size,number_of_features))\n    \n    for i in range(size):\n        \n        #generate the sequence\n        sequence = generate_sequence(number_of_features,time_steps)\n        #one hot encode it\n        encoded = one_hot_encode(number_of_features,sequence)\n        #reshape it to be 3D (1 sample, length timesteps, nr_features features)\n        x = encoded.reshape((1, time_steps, number_of_features))\n        y = encoded[out_index].reshape(1, number_of_features)\n        \n        X[i] = x\n        Y[i] = y\n        \n\n    return X, Y","d5b63c91":"def show_history(history):\n    print(history.history.keys())\n\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy with 16 units')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()","6c6c0305":"number_of_features = 10\ntime_steps = 5\nout_index = 2\nsize = 5000\nX,y = generate_sample(size,time_steps,number_of_features,out_index)","98bd9c2b":"def build_model(time_steps,nr_features,units):\n    \n    tf.keras.backend.clear_session()\n    \n    model = tf.keras.Sequential()\n    \n    model.add(tf.keras.layers.LSTM(units = units, input_shape= (time_steps, nr_features), return_sequences = False))\n    \n    model.add(tf.keras.layers.Dense(nr_features,activation='softmax'))\n\n    model.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","30ba6fb2":"def fit(epochs,model,X,y):\n    \n    from keras.callbacks.callbacks import EarlyStopping\n    \n    callbacks=[\n            EarlyStopping(monitor='accuracy', patience=1, verbose=2, mode='max')\n    ]\n    \n    history = model.fit(X,y,shuffle=False,verbose=2,epochs=epochs,callbacks=callbacks, validation_split = 0.2)\n    \n    return history","c9126b8d":"model  = build_model(time_steps, number_of_features,16)\nmodel.summary()\nhistory = fit(5000,model,X,y)","980dafcd":"show_history(history)","33c46fbd":"nr_of_features = 10\nsteps = 5\nout_index = 2\nsize = 5000\n\nX,y = generate_sample(size,steps,nr_of_features,out_index)\n\nmodel  = build_model(steps, nr_of_features, 128)\nmodel.summary()\n\nhistory = fit(5000,model,X,y)","2ff56671":"show_history(history)","ce268b5b":"nr_of_features = 50\nsteps = 20\nout_index = 2\nsize = 5000\n\nX,y = generate_sample(size,steps,nr_of_features,out_index)\n\nmodel  = build_model(steps, nr_of_features, 16)\n\nmodel.summary()\n\nfrom keras.callbacks.callbacks import EarlyStopping\n\n#Vou mudar o campo patience, e monotorizar o validation data como o train data\ncallbacks=[\n    EarlyStopping(monitor='accuracy', patience=4, verbose=2, mode='max')\n]\n    \nhistory = model.fit(X,y,shuffle=False,verbose=2,epochs=5000, callbacks=callbacks ,validation_split = 0.2)\n#history = fit(5000,model,X,y)","2e916312":"show_history(history)","f79b1e0d":"nr_of_features = 50\nsteps = 20\nout_index = 2\nsize = 5000\n\nX,y = generate_sample(size,steps,nr_of_features,out_index)\n\nmodel  = build_model(steps, nr_of_features, 128)\nmodel.summary()\n\nfrom keras.callbacks.callbacks import EarlyStopping\n\n#Vou mudar o campo patience, e monotorizar o validation data como o train data\ncallbacks=[\n    EarlyStopping(monitor='accuracy', patience=4, verbose=2, mode='max')\n]\n    \nhistory = model.fit(X,y,shuffle=False,verbose=2,epochs=5000, callbacks=callbacks ,validation_split = 0.2)\n#history = fit(5000,model,X,y)","f62aaf18":"show_history(history)","453ed41e":"def build_model(time_steps,nr_features,units):\n    \n    tf.keras.backend.clear_session()\n    \n    model = tf.keras.Sequential()\n    \n    \n    \n    \n    model.add(tf.keras.layers.LSTM(units = units,\n                                   input_shape= (time_steps, nr_features),\n                                   dropout = 0.25,\n                                   recurrent_dropout = 0.25,\n                                   return_sequences = True)\n             )\n    \n    model.add(tf.keras.layers.LSTM(units = units,\n                                   dropout = 0.25,\n                                   recurrent_dropout = 0.25,\n                                   return_sequences = False)\n             )\n                                 \n\n    \n    model.add(tf.keras.layers.Dense(nr_features,activation='softmax'))    \n\n\n    model.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model\n\ndef fit(epochs,model,X,y):\n    \n    from keras.callbacks.callbacks import EarlyStopping\n    \n    callbacks=[\n            EarlyStopping(monitor='val_accuracy', patience=10, verbose=2, mode='max')\n    ]\n    \n    history = model.fit(X,y,shuffle=False,verbose=2,epochs=epochs, validation_split = 0.2, callbacks=callbacks)\n    \n    return history","a001afdd":"nr_of_features = 50\nsteps = 20\nout_index = 2\nsize = 5000\n\n\nX,y = generate_sample(size,steps,nr_of_features,out_index)\n\nmodel  = build_model(steps, nr_of_features, 128)\nmodel.summary()\n\nhistory = fit(5000,model,X,y)","790dc903":"show_history(history)","bac1af16":"# LSTM\n\n> Implementar uma rede LSTM para resolver o Echo Sequence Prediction Problem. \n\n> Analisar qual o impacto, na precisa\u0303o do modelo, de treinar mais\/menos e\u0301pocas, aumentar\/diminuir o nu\u0301mero de neuro\u0301nios e sujeitar o modelo a seque\u0302ncias com mais timesteps (10) e features (25);\n\n> Acrescentar mais camadas LSTM ao modelo (verificar a API tf.keras.layers.LSTM). Analisar o impacto desta mudanc\u0327a na precisa\u0303o do modelo.","04152e29":"# Conclus\u00f5es\n\n> Podemos assumir atrav\u00e9s destes gr\u00e1ficos que tanto com 16 unidades como com 128 na LSTM, ambos os modelos conseguiram convergir e apresentar uma accuracy de 100%. A diferen\u00e7a foi na velocidade, com 128 neur\u00f3nios conseguimos convergir muito mais r\u00e1pido. Conseguimos observar este comportamento na diferen\u00e7a da curva, uma coverge muito mais r\u00e1pido que a outra.\n\n1. LSTM com 16 neur\u00f3nios\n2. LSTM com 128 neur\u00f3nios","9f6582cb":"## Podemos observar que com esta rede mais complexa e com regulariza\u00e7\u00e3o a rede consegue generalizar bem","fa7836b5":"## Cria o dataset\n\n1. 10 features diferentes, est\u00e3o entre [0,10[\n2. 5 time steps\n3. indice de previs\u00e3o \u00e9 o 2 \u00ednidice do array\n4. tamanho do dataset = 5000","6f0f5bf2":"## Vamos mudar a arquitetura da rede para algo mais complexo\n\n\n> LSTM (com dropout) -> LSTM (com dropout) -> Dense","c0e9e0b8":"> Resultado com 10 features, 5 time-steps, train_size de 4000 exemplos e validation com 1000 exemplos\n\n> Mini_batch de 1","d95d5f61":"## Agora vou tentar aumentar a complexidade do problema\n### Aumento n\u00famero de steps para 20 e as features para 50\n\n1. Resultados com 16 neur\u00f3nios\n2. Resultados com 128 neur\u00f3nios","88bb0051":"## Generate samples cria o train_set e o respetivo label\n\n> Em vez de passar 1 a 1 para a rede, porque demora muito mais a convergir, decido criar 5000 exemplos","2edcc0a5":"> One hot encoding para depois passar para a LSTM\n\n> Cada input tem que ter a mesma dimens\u00e3o logo estes valores t\u00eam um tamanha m\u00e1ximo = features","03c144aa":"> Provavelmente com mais itera\u00e7\u00f5es a rede conseguia generalizar para o train_set, mais ia levar ao overfitting da rede, se aumentarmos o n\u00famero de neur\u00f3nios para 128, a rede provavelmente ir\u00e1 fazer o que esta rede n\u00e3o fez devido ao n\u00famero reduzido de epochs, vai convergir mais r\u00e1pido mas apresentar um grande overfitting da rede","f36c63f1":"### Agora vou tentar com um modelo em que a LSTM tenha 128 unidades","7654bcb7":"## Modelo utilizado:\n\n> LSTM -> Dense","9ac6889a":"## Podemos notar que a arquitetura utilizada anteriormente com 16 neur\u00f3nios, j\u00e1 n\u00e3o consegue generalizar t\u00e3o facilmente.\n\n> Causas:\n\n1. Rede pouco complexa","823b0fc8":"> Gera a sequ\u00eancia de inteiros aleaat\u00f3rios\n1. Number de features -> valores compreendidos entre 0 e features-1\n2. Time steps -> n\u00famero de inteiros gerados","e568084f":"## Fun\u00e7\u00e3o para mostrar gr\u00e1ficos finais com loss e accuracy","94406e36":"## Podemos notar que este tipo de arquitetura converge mas apresenta um graude muito alto de overfitting\n\n> Causas:\n\n1. Arquitetura n\u00e3o \u00e9 a adequada\n2. N\u00e3o existe regulariza\u00e7\u00e3o\n\n\n> Aconteceu exatamente o previsto, esta rede converge muito mais r\u00e1pido como tinhamos testado para 10 features e 5 time_steps mas apresenta muito overfitting","e9659331":"## Este modelo tem 16 unidades na LSTM"}}