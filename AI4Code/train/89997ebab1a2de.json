{"cell_type":{"77439d0f":"code","26b2b353":"code","fff96101":"code","ac1da204":"code","04f68e59":"code","c3d9718b":"code","cf2aed17":"code","478d0657":"code","1d709b39":"code","677635e1":"code","d9080918":"code","0301733f":"code","5a779a69":"code","c79c3d89":"code","c768f0c8":"code","765a69f6":"code","b441bd70":"code","863f1d3a":"code","3eaf8eec":"code","11e55fd9":"markdown","bce99c7d":"markdown","ab63f195":"markdown","fc00299c":"markdown","6e6fce74":"markdown","aba41af3":"markdown","6f47f754":"markdown","78935362":"markdown","d2ce075e":"markdown","cf7b6394":"markdown","6f2dfa1d":"markdown","437639a9":"markdown","262a78bf":"markdown","9add942f":"markdown"},"source":{"77439d0f":"import os\nimport gc\nimport json\nimport glob\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.path import Path\nimport seaborn as sns\nimport tqdm\n\nimport torchvision\nfrom torchvision import models\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision import transforms\n\nimport PIL\nfrom PIL import Image, ImageDraw\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom IPython.display import clear_output","26b2b353":"torch.__version__, torch.cuda.is_available()","fff96101":"# \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u043e\u0438\u0441\u043a \u0434\u043b\u044f \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0439 \u043c\u0430\u0441\u043a\u0438 4-\u0445\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u043e\u043c\ndef simplify_contour(contour, n_corners=4):\n    n_iter, max_iter = 0, 1000\n    lb, ub = 0., 1.\n\n    while True:\n        n_iter += 1\n        if n_iter > max_iter:\n            print('simplify_contour didnt coverege')\n            return None\n\n        k = (lb + ub)\/2.\n        eps = k*cv2.arcLength(contour, True)\n        approx = cv2.approxPolyDP(contour, eps, True)\n\n        if len(approx) > n_corners:\n            lb = (lb + ub)\/2.\n        elif len(approx) < n_corners:\n            ub = (lb + ub)\/2.\n        else:\n            return approx\n\n# \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c 4-\u0445\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a \u0432 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a \n# \u0421\u043f\u0430\u0441\u0438\u0431\u043e ulebok \u0437\u0430 \u0438\u0434\u0435\u044e \n# \u0418 \u0432\u043e\u0442 \u044d\u0442\u0438\u043c \u0440\u0435\u0431\u044f\u0442\u0430\u043c \u0437\u0430 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e: https:\/\/www.pyimagesearch.com\/2014\/08\/25\/4-point-opencv-getperspective-transform-example\/\ndef four_point_transform(image, pts):\n    \n    rect = order_points(pts)\n    \n    tl, tr, br, bl = pts\n    \n    width_1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n    width_2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n    max_width = max(int(width_1), int(width_2))\n    \n    height_1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n    height_2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n    max_height = max(int(height_1), int(height_2))\n    \n    dst = np.array([\n        [0, 0],\n        [max_width, 0],\n        [max_width, max_height],\n        [0, max_height]], dtype = \"float32\")\n    \n    M = cv2.getPerspectiveTransform(rect, dst)\n    warped = cv2.warpPerspective(image, M, (max_width, max_height))\n    return warped\n\ndef order_points(pts):\n    rect = np.zeros((4, 2), dtype = \"float32\")\n    \n    s = pts.sum(axis = 1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    \n    diff = np.diff(pts, axis = 1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    \n    return rect\n\n\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u044e (4 \u0442\u043e\u0447\u043a\u0438, bounding box \u0438 \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u043d\u044b\u0439 \u043f\u043e \u043c\u0430\u0441\u043a\u0435 \u043a\u043e\u043d\u0442\u0443\u0440)\ndef visualize_prediction_plate(file, model, device='cuda', verbose=True, thresh=0.0, \n                               n_colors=None, id_to_name=None):\n    img = Image.open(file)\n    img_tensor = my_transforms(img)\n    model.to(device)\n    model.eval()\n    with torch.no_grad():\n        predictions = model([img_tensor.to(device)])\n    prediction = predictions[0]\n    \n    if n_colors is None:\n        n_colors = model.roi_heads.box_predictor.cls_score.out_features\n    \n    palette = sns.color_palette(None, n_colors)\n    \n    img = cv2.imread(file, cv2.COLOR_BGR2RGB)\n    h, w = img.shape[:2]\n    image = img\n    \n    blackImg = np.zeros(image.shape, image.dtype)\n    blackImg[:,:] = (0, 0, 0)\n    for i in range(len(prediction['boxes'])):\n        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n        label = int(prediction['labels'][i].cpu())\n        score = float(prediction['scores'][i].cpu())\n        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n        name = id_to_name[label]\n        color = palette[label]\n        \n        if verbose:\n            if score > thresh:\n                print ('Class: {}, Confidence: {}'.format(name, score))\n        if score > thresh:            \n            crop_img = image[y_min:y_max, x_min:x_max]\n            print('Bounding box:')\n            show_image(crop_img, figsize=(10, 2))\n            \n            # \u0412 \u0440\u0430\u0437\u043d\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u044f\u0445 opencv \u044d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0440\u0430\u0437\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n            # contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n            _,contours,_ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n            approx = simplify_contour(contours[0], n_corners=4)\n            \n            if approx is None:\n                x0, y0 = x_min, y_min\n                x1, y1 = x_max, y_min\n                x2, y2 = x_min, y_max\n                x3, y3 = x_max, y_max\n#                 points = [[x_min, y_min], [x_min, y_max], [x_max, y_min],[x_max, y_max]]\n            else:\n                x0, y0 = approx[0][0][0], approx[0][0][1]\n                x1, y1 = approx[1][0][0], approx[1][0][1]\n                x2, y2 = approx[2][0][0], approx[2][0][1]\n                x3, y3 = approx[3][0][0], approx[3][0][1]\n                \n            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n            \n            \n            points = np.array(points)\n            crop_mask_img = four_point_transform(img, points)\n            print('Rotated img:')\n            crop_mask_img = cv2.resize(crop_mask_img, (320, 64), interpolation=cv2.INTER_AREA)\n            show_image(crop_mask_img, figsize=(10, 2))\n            if approx is not None:\n                cv2.drawContours(image, [approx], 0, (255,0,255), 3)\n            image = cv2.circle(image, (x0, y0), radius=5, color=(0, 0, 255), thickness=-1)\n            image = cv2.circle(image, (x1, y1), radius=5, color=(0, 0, 255), thickness=-1)\n            image = cv2.circle(image, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)\n            image = cv2.circle(image, (x3, y3), radius=5, color=(0, 0, 255), thickness=-1)\n            \n            image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), np.array(color) * 255, 2)\n            \n    show_image(image)\n    return prediction\n\n# \u041f\u0440\u043e\u0441\u0442\u043e \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443. \u0421 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430\ndef show_image(image, figsize=(16, 9), reverse=True):\n    plt.figure(figsize=figsize)\n    if reverse:\n        plt.imshow(image[...,::-1])\n    else:\n        plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n    \n\n# \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 \u0442\u0435\u043a\u0441\u0442. \u0421 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430\ndef decode(pred, alphabet):\n    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n    outputs = []\n    for i in range(len(pred)):\n        outputs.append(pred_to_string(pred[i], alphabet))\n    return outputs\n\ndef pred_to_string(pred, alphabet):\n    seq = []\n    for i in range(len(pred)):\n        label = np.argmax(pred[i])\n        seq.append(label - 1)\n    out = []\n    for i in range(len(seq)):\n        if len(out) == 0:\n            if seq[i] != -1:\n                out.append(seq[i])\n        else:\n            if seq[i] != -1 and seq[i] != seq[i - 1]:\n                out.append(seq[i])\n    out = ''.join([alphabet[c] for c in out])\n    return out\n    \n\n        \ndef load_json(file):\n    with open(file, 'r') as f:\n        return json.load(f)\n    \n# \u0427\u0442\u043e\u0431\u044b \u0431\u0435\u0437 \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u043e\u0432\u044b\u0432\u0430\u0442\u044c json. \u0411\u0435\u0437 \u043d\u0435\u0433\u043e \u0435\u0441\u0442\u044c \u043d\u044e\u0430\u043d\u0441\u044b\nclass npEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.int32):\n            return int(obj)\n        return json.JSONEncoder.default(self, obj)","ac1da204":"DATA_PATH = '.\/data\/'\nTRAIN_SIZE = 0.9\nBATCH_SIZE = 2\nBATCH_SIZE_OCR = 16\nDETECTOR_MODEL_PATH = 'detector.pt'\nOCR_MODEL_PATH = 'ocr.pt'\n\nall_marks = load_json(os.path.join(DATA_PATH, 'train.json'))\ntest_start = int(TRAIN_SIZE * len(all_marks))\ntrain_marks = all_marks[:test_start]\nval_marks = all_marks[test_start:]\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","04f68e59":"def get_detector_model():\n    \n    model = models.detection.maskrcnn_resnet50_fpn(\n        pretrained=True, \n        pretrained_backbone=True,\n        progress=True, \n        num_classes=91, \n    )\n\n    num_classes = 2\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    \n    box_predictor = FastRCNNPredictor(in_features, num_classes)\n    model.roi_heads.box_predictor = box_predictor\n    \n    mask_predictor = MaskRCNNPredictor(256, 256, num_classes)\n    model.roi_heads.mask_predictor = mask_predictor\n\n    # \u0417\u0430\u043c\u043e\u0440\u043e\u0437\u0438\u043c \u0432\u0441\u0435 \u0441\u043b\u043e\u0438 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445\n    \n    for param in model.parameters():\n        param.requires_grad = False\n        \n    for param in model.backbone.fpn.parameters():\n        param.requires_grad = True\n\n    for param in model.rpn.parameters():\n        param.requires_grad = True\n\n    for param in model.roi_heads.parameters():\n        param.requires_grad = True\n    \n    return model","c3d9718b":"class DetectionDataset(Dataset):\n    def __init__(self, marks, img_folder, transforms=None):\n        \n        self.marks = marks\n        self.img_folder = img_folder\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        item = self.marks[idx]\n        img_path = f'{self.img_folder}{item[\"file\"]}'\n        img = Image.open(img_path).convert('RGB')\n        w, h = img.size\n        \n        box_coords = item['nums']\n        boxes = []\n        labels = []\n        masks = []\n        for box in box_coords:\n            points = np.array(box['box'])  \n            x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n            x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n            boxes.append([x0, y0, x2, y2])\n            labels.append(1)\n            \n            # \u0417\u0434\u0435\u0441\u044c \u043c\u044b \u043d\u0430\u0448\u0438 4 \u0442\u043e\u0447\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 \u043c\u0430\u0441\u043a\u0443\n            # \u042d\u0442\u043e \u043d\u0443\u0436\u043d\u043e, \u0447\u0442\u043e\u0431\u044b \u043a\u0440\u043e\u043c\u0435 bounding box \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u043c\u0430\u0441\u043a\u0443 :)\n            nx, ny = w, h\n            poly_verts = points\n            x, y = np.meshgrid(np.arange(nx), np.arange(ny))\n            x, y = x.flatten(), y.flatten()\n            points = np.vstack((x,y)).T\n            path = Path(poly_verts)\n            grid = path.contains_points(points)\n            grid = grid.reshape((ny,nx)).astype(int)\n            masks.append(grid)\n            \n        boxes = torch.as_tensor(boxes)\n        labels = torch.as_tensor(labels)\n        masks = torch.as_tensor(masks)\n        \n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'masks': masks,\n        }\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n        \n        return img, target\n    \n    \n    def __len__(self):\n        return len(self.marks)\n    \nmy_transforms = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntrain_dataset = DetectionDataset(\n    marks=train_marks, \n    img_folder='data\/', \n    transforms=my_transforms\n)\nval_dataset = DetectionDataset(\n    marks=val_marks, \n    img_folder='data\/', \n    transforms=my_transforms\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    drop_last=True,\n    num_workers=4,\n    collate_fn=collate_fn, \n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    drop_last=False,\n    num_workers=4,\n    collate_fn=collate_fn, \n)","cf2aed17":"torch.cuda.empty_cache()\ngc.collect()\nmodel = get_detector_model()\n# model.load_state_dict(torch.load(DETECTOR_MODEL_PATH))\nmodel.to(device);","478d0657":"optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5, verbose=True)\n\nmodel.train()\nfor epoch in range(1):\n\n    print_loss = []\n    for i, (images, targets) in tqdm.tqdm(enumerate(train_loader), leave=False, position=0, total=len(train_loader)):\n\n        images = [image.to(device) for image in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss_dict.values())\n\n        losses.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        print_loss.append(losses.item())\n        if (i + 1) % 20 == 0:\n            mean_loss = np.mean(print_loss)\n            print(f'Loss: {mean_loss:.7f}')\n            scheduler.step(mean_loss)\n            print_loss = [] ","1d709b39":"# torch.save(model.state_dict(), DETECTOR_MODEL_PATH)","677635e1":"test_images = glob.glob(os.path.join(DATA_PATH, 'test\/*'))","d9080918":"visualize_prediction_plate(np.random.choice(test_images), model, id_to_name={1: 'plate'}, thresh=0.95)","0301733f":"THRESHOLD_SCORE = 0.93\nTRESHOLD_MASK = 0.05\n\npreds = []\nmodel.eval()\n\n\nfor file in tqdm.tqdm(test_images, position=0, leave=False):\n\n    img = Image.open(file).convert('RGB')\n    img_tensor = my_transforms(img)\n    with torch.no_grad():\n        predictions = model([img_tensor.to(device)])\n    prediction = predictions[0]\n\n    pred = dict()\n    pred['file'] = file\n    pred['nums'] = []\n\n    for i in range(len(prediction['boxes'])):\n        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n        label = int(prediction['labels'][i].cpu())\n        score = float(prediction['scores'][i].cpu())\n        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n\n        if score > THRESHOLD_SCORE:            \n            # \u0412 \u0440\u0430\u0437\u043d\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u044f\u0445 opencv \u044d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0440\u0430\u0437\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n            # contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n            _,contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n            approx = simplify_contour(contours[0], n_corners=4)\n            \n            if approx is None:\n                x0, y0 = x_min, y_min\n                x1, y1 = x_max, y_min\n                x2, y2 = x_min, y_max\n                x3, y3 = x_max, y_max\n            else:\n                x0, y0 = approx[0][0][0], approx[0][0][1]\n                x1, y1 = approx[1][0][0], approx[1][0][1]\n                x2, y2 = approx[2][0][0], approx[2][0][1]\n                x3, y3 = approx[3][0][0], approx[3][0][1]\n                \n            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n\n            pred['nums'].append({\n                'box': points,\n                'bbox': [x_min, y_min, x_max, y_max],\n            })\n\n    preds.append(pred)   \n\n    \nwith open(os.path.join(DATA_PATH, 'test.json'), 'w') as json_file:\n    json.dump(preds, json_file, cls=npEncoder)","5a779a69":"class OCRDataset(Dataset):\n    def __init__(self, marks, img_folder, alphabet, transforms=None):\n        ocr_marks = []\n        for items in marks:\n            file_path = items['file']\n            for box in items['nums']:\n                \n                ocr_marks.append({\n                    'file': file_path,\n                    'box': np.clip(box['box'], 0, None).tolist(),\n                    'text': box['text'],\n                    'boxed': False,\n                })\n                            \n                # \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0442\u043e\u0447\u043a\u0438, \u0437\u0430\u043f\u0430\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0432 BoundingBox. \n                # \u0412\u043c\u0435\u0441\u0442\u043e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 rotate. \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0431\u0443\u0434\u0435\u0442 \u0432 2 \u0440\u0430\u0437\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\n                \n                #\u041a\u043b\u0438\u043f\u0430\u0435\u043c, \u0438\u0431\u043e \u0435\u0441\u0442\u044c \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b\n                points = np.clip(box['box'], 0, None) \n                x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n                x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n\n                ocr_marks.append({\n                    'file': file_path,\n                    'box': [x0, y0, x2, y2],\n                    'text': box['text'],\n                    'boxed': True,\n                })\n                \n        self.marks = ocr_marks\n        self.img_folder = img_folder\n        self.transforms = transforms\n        self.alphabet = alphabet\n        \n    def __getitem__(self, idx):\n        item = self.marks[idx]\n        img_path = os.path.join(self.img_folder, item[\"file\"])\n        img = cv2.imread(img_path)\n\n        if item['boxed']:\n            x_min, y_min, x_max, y_max = item['box']\n            img = img[y_min:y_max, x_min:x_max]\n        else:\n            points = np.clip(np.array(item['box']), 0, None)\n            img = four_point_transform(img, points)\n            \n        text = item['text']\n        seq = [self.alphabet.find(char) + 1 for char in text]\n        seq_len = len(seq)\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        output = {\n            'img': img,\n            'text': text,\n            'seq': seq,\n            'seq_len': seq_len\n        }\n        \n        return output\n    \n    \n    def __len__(self):\n        return len(self.marks)\n    \n    \nclass Resize(object):\n    def __init__(self, size=(320, 64)):\n        self.size = size\n\n    def __call__(self, img):\n\n        w_from, h_from = img.shape[1], img.shape[0]\n        w_to, h_to = self.size\n        \n        # \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0440\u0430\u0437\u043d\u0443\u044e \u0438\u043d\u0442\u0435\u0440\u043f\u043e\u043b\u044f\u0446\u0438\u044e \u043f\u0440\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u0438 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0438\n        # \u0415\u0441\u043b\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443, \u043c\u0435\u043d\u044f\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u043f\u043e\u043b\u044f\u0446\u0438\u044e\n        interpolation = cv2.INTER_AREA\n        if w_to > w_from:\n            interpolation = cv2.INTER_CUBIC\n        \n        img = cv2.resize(img, dsize=self.size, interpolation=interpolation)\n        return img\n    \nmy_ocr_transforms = transforms.Compose([\n    Resize(size=(320, 64)),\n    transforms.ToTensor()\n])\n\ndef get_vocab_from_marks(marks):\n    train_texts = []\n    for item in marks:\n        for num in item['nums']:\n            train_texts.append(num['text'])\n\n    counts = Counter(''.join(train_texts))\n    alphabet = ''.join(set(''.join(train_texts)))\n    corted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n    char_to_idx = {item[0]: idx + 1 for idx, item in enumerate(corted_counts)}\n    idx_to_char = {idx:char for char, idx in char_to_idx.items()}\n    return char_to_idx, idx_to_char, alphabet\n\nchar_to_idx, idx_to_char, alphabet = get_vocab_from_marks(train_marks)\n\ntrain_ocr_dataset = OCRDataset(\n    marks=train_marks, \n    img_folder=DATA_PATH, \n    alphabet=alphabet,\n    transforms=my_ocr_transforms\n)\nval_ocr_dataset = OCRDataset(\n    marks=val_marks, \n    img_folder=DATA_PATH, \n    alphabet=alphabet,\n    transforms=my_ocr_transforms\n)\n\ndef collate_fn_ocr(batch):\n    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n    Accepts list of dataset __get_item__ return values (dicts).\n    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n    \"\"\"\n    images, seqs, seq_lens, texts = [], [], [], []\n    for sample in batch:\n        images.append(sample[\"img\"])\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n    return batch\n\ntrain_ocr_loader = DataLoader(\n    train_ocr_dataset, \n    batch_size=BATCH_SIZE_OCR, \n    drop_last=True,\n    num_workers=0, # \u041f\u043e\u0447\u0435\u043c\u0443-\u0442\u043e \u0443 \u043c\u0435\u043d\u044f \u0432\u0438\u0441\u043d\u0435\u0442 DataLoader, \u0435\u0441\u043b\u0438 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0442\u043e\u043a\u043e\u0432\n    collate_fn=collate_fn_ocr,\n    timeout=0,\n    shuffle=True # \u0427\u0442\u043e\u0431\u044b \u043f\u043e\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u0435 \u0434\u0443\u0431\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0435 \u0448\u043b\u0438 \u043f\u043e\u0434\u0440\u044f\u0434\n)\n\nval_ocr_loader = DataLoader(\n    val_ocr_dataset, \n    batch_size=BATCH_SIZE_OCR, \n    drop_last=False,\n    num_workers=0,\n    collate_fn=collate_fn_ocr, \n    timeout=0,\n)\n\ngc.collect()","c79c3d89":"class FeatureExtractor(nn.Module):\n    \n    def __init__(self, input_size=(64, 320), output_len=20):\n        super(FeatureExtractor, self).__init__()\n        \n        h, w = input_size\n        resnet = getattr(models, 'resnet18')(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n        \n        self.pool = nn.AvgPool2d(kernel_size=(h \/\/ 32, 1))        \n        self.proj = nn.Conv2d(w \/\/ 32, output_len, kernel_size=1)\n  \n        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n    \n    def apply_projection(self, x):\n        \"\"\"Use convolution to increase width of a features.\n        Accepts tensor of features (shaped B x C x H x W).\n        Returns new tensor of features (shaped B x C x H x W').\n        \"\"\"\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1).contiguous()\n        return x\n   \n    def forward(self, x):\n        # Apply conv layers\n        features = self.cnn(x)\n        \n        # Pool to make height == 1\n        features = self.pool(features)\n        \n        # Apply projection to increase width\n        features = self.apply_projection(features)\n        \n        return features\n    \nclass SequencePredictor(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n        super(SequencePredictor, self).__init__()\n        \n        self.num_classes = num_classes        \n        self.rnn = nn.GRU(input_size=input_size,\n                       hidden_size=hidden_size,\n                       num_layers=num_layers,\n                       dropout=dropout,\n                       bidirectional=bidirectional)\n        \n        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n        self.fc = nn.Linear(in_features=fc_in,\n                         out_features=num_classes)\n    \n    def _init_hidden_(self, batch_size):\n        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n        Accepts batch size.\n        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n        \"\"\"\n        num_directions = 2 if self.rnn.bidirectional else 1\n        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n        \n    def _prepare_features_(self, x):\n        \"\"\"Change dimensions of x to fit RNN expected input.\n        Accepts tensor x shaped (B x (C=1) x H x W).\n        Returns new tensor shaped (W x B x H).\n        \"\"\"\n        x = x.squeeze(1)\n        x = x.permute(2, 0, 1)\n        return x\n    \n    def forward(self, x):\n        x = self._prepare_features_(x)\n        \n        batch_size = x.size(1)\n        h_0 = self._init_hidden_(batch_size)\n        h_0 = h_0.to(x.device)\n        x, h = self.rnn(x, h_0)\n        \n        x = self.fc(x)\n        return x\n    \nclass CRNN(nn.Module):\n    \n    def __init__(\n        self, \n        alphabet=alphabet,\n        cnn_input_size=(64, 320), \n        cnn_output_len=20,\n        rnn_hidden_size=128, \n        rnn_num_layers=2, \n        rnn_dropout=0.3, \n        rnn_bidirectional=False\n    ):\n        super(CRNN, self).__init__()\n        self.alphabet = alphabet\n        \n        self.features_extractor = FeatureExtractor(\n            input_size=cnn_input_size, \n            output_len=cnn_output_len\n        )\n        \n        self.sequence_predictor = SequencePredictor(\n            input_size=self.features_extractor.num_output_features,\n            hidden_size=rnn_hidden_size, \n            num_layers=rnn_num_layers,\n            num_classes=(len(alphabet) + 1), \n            dropout=rnn_dropout,\n            bidirectional=rnn_bidirectional\n        )\n    \n    def forward(self, x):\n        features = self.features_extractor(x)\n        sequence = self.sequence_predictor(features)\n        return sequence","c768f0c8":"crnn = CRNN()\n# crnn.load_state_dict(torch.load(OCR_MODEL_PATH))\ncrnn.to(device);","765a69f6":"optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=True)","b441bd70":"# \u0417\u0434\u0435\u0441\u044c \u0442\u043e\u0436\u0435 \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u043f\u043e\u043a\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0431\u0435\u0437 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438. \n# \u041c\u043e\u0436\u0435\u0442, \u0437\u0434\u0435\u0441\u044c \u0438 \u0441\u0442\u043e\u0438\u0442 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c. \n# \u041d\u043e \u043e\u043f\u044f\u0442\u044c \u0436\u0435, 1-2 \u044d\u043f\u043e\u0445 \u0445\u0432\u0430\u0442\u0430\u0435\u0442, \u0430 \u0437\u043d\u0430\u0447\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0438\u0434\u0438\u0442 \u043f\u043e\u0447\u0442\u0438 \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \n# \u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u0438 \u043b\u043e\u0441\u0441 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\n\ncrnn.train()\nfor epoch in range(2):\n    epoch_losses = []\n    print_loss = []\n    \n    for i, batch in enumerate(tqdm.tqdm(train_ocr_loader, total=len(train_ocr_loader), leave=False, position=0)):\n        images = batch[\"image\"].to(device)\n        seqs_gt = batch[\"seq\"]\n        seq_lens_gt = batch[\"seq_len\"]\n\n        seqs_pred = crnn(images).cpu()\n        log_probs = F.log_softmax(seqs_pred, dim=2)\n        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n        loss = F.ctc_loss(\n            log_probs=log_probs,  # (T, N, C)\n            targets=seqs_gt,  # N, S or sum(target_lengths)\n            input_lengths=seq_lens_pred,  # N\n            target_lengths=seq_lens_gt # N\n        )  \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print_loss.append(loss.item())\n        if (i + 1) % 20 == 0:\n            mean_loss = np.mean(print_loss)\n            print(f'Loss: {mean_loss:.7f}')\n            scheduler.step(mean_loss)\n            print_loss = [] \n    \n        epoch_losses.append(loss.item())\n\n    print(i, np.mean(epoch_losses))","863f1d3a":"torch.save(crnn.state_dict(), OCR_MODEL_PATH)","3eaf8eec":"test_marks = load_json(os.path.join(DATA_PATH, 'test.json'))\ncrnn.eval()\nresizer = Resize()\n\nfile_name_result = [] \nplates_string_result = []\n\nfor item in tqdm.tqdm(test_marks, leave=False, position=0):\n\n    img_path = item[\"file\"]\n    img = cv2.imread(img_path)\n\n    results_to_sort = []\n    for box in item['nums']:\n        x_min, y_min, x_max, y_max = box['bbox']\n        img_bbox = resizer(img[y_min:y_max, x_min:x_max])\n        img_bbox = my_transforms(img_bbox)\n        img_bbox = img_bbox.unsqueeze(0)\n\n\n        points = np.clip(np.array(box['box']), 0, None)\n        img_polygon = resizer(four_point_transform(img, points))\n        img_polygon = my_transforms(img_polygon)\n        img_polygon = img_polygon.unsqueeze(0)\n\n        preds_bbox = crnn(img_bbox.to(device)).cpu().detach()\n        preds_poly = crnn(img_polygon.to(device)).cpu().detach()\n\n        preds = preds_poly + preds_bbox\n        num_text = decode(preds, alphabet)[0]\n\n        results_to_sort.append((x_min, num_text))\n\n    results = sorted(results_to_sort, key=lambda x: x[0])\n    num_list = [x[1] for x in results]\n\n    plates_string = ' '.join(num_list)\n    file_name = img_path[img_path.find('test\/'):]\n\n    file_name_result.append(file_name)\n    plates_string_result.append(plates_string)\n    \ndf_submit = pd.DataFrame({'file_name': file_name_result, 'plates_string': plates_string_result})\ndf_submit.to_csv('submission.csv', index=False)","11e55fd9":"### c) \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438","bce99c7d":"### a) \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043d\u043e\u043c\u0435\u0440\u043e\u0432\n\n\u0418\u0437 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0435\u0439 - \u043d\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 \u043c\u044b \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c bounding box + \u0432\u044b\u0440\u0435\u0437\u0430\u0435\u043c \u043f\u043e \u0442\u043e\u0447\u043a\u0430\u043c \u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a \u043d\u0430\u0448 4-\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0442\u043e\u0447\u043a\u0430\u043c. \u0422.\u0435. 2 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u043d\u043e\u043c\u0435\u0440.","ab63f195":"# 2. \u0420\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0435\u043c \u043d\u043e\u043c\u0435\u0440\u0430","fc00299c":"### \u0421\u0435\u0439\u0447\u0430\u0441 \u0431\u0443\u0434\u0435\u0442 \u043c\u043d\u043e\u0433\u043e \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u043c\u043e\u0442\u0430\u0442\u044c\n\n\u0427\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u043e \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e \u0438 \u043d\u0435 \u043f\u0440\u0438\u0445\u043e\u0434\u0438\u043b\u043e\u0441\u044c \u043b\u0435\u0437\u0442\u044c \u0432 \u043c\u043e\u0434\u0443\u043b\u0438, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u043a\u0430\u043a\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442, \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0434\u043b\u044f \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043a\u0430 \u0447\u0442\u043e \u0432\u0441\u0435 \u0432 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435. \u041c\u043e\u0436\u043d\u043e \u0430\u043a\u043a\u0443\u0440\u0430\u0442\u043d\u043e \u043f\u0435\u0440\u0435\u043d\u0435\u0441\u0442\u0438 \u0432 \u043c\u043e\u0434\u0443\u043b\u0438 :)","6e6fce74":"# 1. \u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043d\u043e\u043c\u0435\u0440\u0430","aba41af3":"### d) \u041d\u0430\u043a\u043e\u043d\u0435\u0446, \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","6f47f754":"### b) \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u043e\u043c\u0435\u0440\u0430\n\n\u0412\u0437\u044f\u043b RCNN \u0438\u0437 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430","78935362":"### a) \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438\n\n\u0412 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u0435\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043e 4 \u0442\u043e\u0447\u043a\u0430\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u0434\u0430\u044e\u0442 \u043d\u043e\u043c\u0435\u0440. \u042d\u0442\u0438 4 \u0442\u043e\u0447\u043a\u0438 - \u043f\u043e\u0447\u0442\u0438 \u0432\u0441\u0435\u0433\u0434\u0430 \u043d\u0435 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a, \u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u044b\u0439 \u0447\u0435\u0442\u044b\u0440\u0435\u0445\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a. \u0411\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c:\n\n- bounding box, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u043a\u0440\u0443\u0436\u0430\u0435\u0442 \u0442\u043e\u0447\u043a\u0438 (\u0434\u0435\u0442\u0435\u043a\u0446\u0438\u044f)\n- \u043c\u0430\u0441\u043a\u0443, \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u0443\u044e \u0442\u0435\u043c, \u0447\u0442\u043e \u0432\u043d\u0443\u0442\u0440\u0438 4-\u0445 \u0442\u043e\u0447\u0435\u043a (\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f)\n\n\u041f\u043e\u044d\u0442\u043e\u043c\u0443, \u0432\u043e\u0437\u044c\u043c\u0435\u043c maskrcnn. \u0411\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u0441\u043e\u043b\u0435\u0432. \u042d\u0442\u043e\u0433\u043e \u0441 \u0437\u0430\u043f\u0430\u0441\u043e\u043c \u0445\u0432\u0430\u0442\u0430\u0435\u0442.","d2ce075e":"### b) \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438","cf7b6394":"### d) \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c bounding box-\u044b \u0438 \u043c\u0430\u0441\u043a\u0443. \n\n- \u041c\u0430\u0441\u043a\u0443 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 4-\u0443\u0433\u043e\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u043b\u0438\u0433\u043e\u043d. \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432 json\n- \u0415\u0441\u043b\u0438 \u043c\u0430\u0441\u043a\u0430 \u043d\u0435 \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u0435\u0442\u0441\u044f 4-\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u043e\u043c (\u0440\u0435\u0434\u043a\u043e \u0442\u0430\u043a\u043e\u0435 \u0431\u044b\u0432\u0430\u0435\u0442, \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u043e\u0438\u0441\u043a \u043f\u043e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 \u043d\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442), \u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b bounding box ","6f2dfa1d":"\u041a\u0430\u043a-\u0442\u043e \u0442\u0430\u043a :)","437639a9":"### c) \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u043e\u043c\u0435\u0440\u0430","262a78bf":"# \u0411\u0435\u0439\u0437\u043b\u0430\u0439\u043d \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u043d\u043e\u043c\u0435\u0440\u043e\u0432. \n\nMADE with love :)","9add942f":"\u0412\u0430\u043b\u0438\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0447\u0435\u043c-\u0442\u043e \u043d\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430, \u0438\u0431\u043e \u043b\u043e\u0441\u0441 \u043f\u0435\u0440\u0435\u0441\u0442\u0430\u0435\u0442 \u043f\u0430\u0434\u0430\u0442\u044c \u0435\u0449\u0435 \u0434\u043e \u0442\u043e\u0433\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u043a\u0430\u043a \u043f\u0440\u043e\u0439\u0434\u0435\u0442 1-\u044f \u044d\u043f\u043e\u0445\u0430. \u0422.\u0435. \u043b\u043e\u0441\u0441 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435 \u0432\u043f\u043e\u043b\u043d\u0435 \u0432\u0430\u043b\u0438\u0434\u043d\u044b\u0439, \u0438\u0431\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0438\u0434\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437."}}