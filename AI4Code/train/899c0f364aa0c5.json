{"cell_type":{"38b756b0":"code","e7c8e625":"code","c201fb3e":"code","7989aaa8":"code","8b377e61":"code","2fe1bb2e":"code","cb08e34a":"code","f56a4d72":"code","37cc43b8":"code","f03c0365":"code","a421c788":"code","ee37b91d":"code","80f1490e":"code","da9f8f57":"code","e12cf038":"code","5b1c88d6":"code","97487486":"code","41b27ef4":"code","fd02115c":"code","d82c804f":"code","fe7604e7":"code","5dce5255":"code","cc18ca16":"code","c7c9eccf":"code","14c60a9d":"code","348f8fbf":"code","05710457":"code","9015e161":"code","af596dbf":"code","8bf6cb38":"code","9f227412":"code","ebbd7cdb":"code","b0c3a808":"code","aa2d501e":"code","58263fe6":"code","706deb3d":"code","87c983aa":"code","e0d4985f":"code","00af59b0":"code","ac6f6824":"code","ea453e1d":"code","efee5b51":"code","6fec7dd0":"code","ae1e8832":"markdown","8b94bfbf":"markdown","0881cefc":"markdown","4d4d784c":"markdown","2b407cad":"markdown","68433e81":"markdown","a018f4aa":"markdown","e89c48f1":"markdown","8d0c7177":"markdown","448177d4":"markdown","3ce55542":"markdown","d1a3ba8a":"markdown","c6eacae9":"markdown"},"source":{"38b756b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7c8e625":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport nltk as nlp","c201fb3e":"f_data = pd.read_csv('\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv')\nf_data.head(3)","7989aaa8":"f_data.columns","8b377e61":"def get_hour(ts):\n    return ts.hour\ndef get_day(ts):\n    return ts.weekday()\ndef get_month(ts):\n    return ts.month\ndef get_year(ts):\n    return ts.year","2fe1bb2e":"eng_data = f_data.copy()\neng_data['CreationDate'] = pd.to_datetime(eng_data['CreationDate'])\neng_data['Creation_Hour'] = eng_data['CreationDate'].apply(get_hour)\neng_data['Creation_Day'] = eng_data['CreationDate'].apply(get_day)\neng_data['Creation_Month'] = eng_data['CreationDate'].apply(get_month)\neng_data['Creation_Year'] = eng_data['CreationDate'].apply(get_year)\neng_data.drop(columns=['CreationDate'],inplace=True)","cb08e34a":"eng_sw =set(nlp.corpus.stopwords.words('english'))\ndef get_char_amount(val):\n    return len(val)\n\ndef average_word_length(val):\n    splited = val.split(' ')\n    char_count = 0\n    word_count = 0\n    for word in splited:\n        if word not in eng_sw:\n            char_count =char_count + len(word)\n            word_count = word_count+1\n    return char_count\/word_count\n\n\ndef number_of_words(val):\n    splited = val.split(' ')\n    char_count = 0\n    word_count = 0\n    for word in splited:\n        if word not in eng_sw:\n            word_count = word_count+1\n    return word_count    ","f56a4d72":"eng_data['Body_Char_Length'] = eng_data['Body'].apply(get_char_amount)\neng_data['Title_Char_Length'] = eng_data['Title'].apply(get_char_amount)\neng_data['Body_Avg_Word_Length'] = eng_data['Body'].apply(average_word_length)\neng_data['Title_Avg_Word_Length'] = eng_data['Title'].apply(average_word_length)\neng_data['Body_Num_Of_Words'] = eng_data['Body'].apply(number_of_words)\neng_data['Title_Num_Of_Words'] = eng_data['Title'].apply(number_of_words)","37cc43b8":"def find_most_common_words(ser):\n    word_aux = {}\n    for sample in ser:\n        splited = sample.split(' ')\n        for word in splited:\n            if word not in eng_sw:\n                if word in word_aux:\n                    word_aux[word] += 1\n                else:\n                    word_aux[word] = 1\n    \n    return word_aux    \n    ","f03c0365":"wa = find_most_common_words(eng_data[eng_data['Y']=='HQ']['Title'])\nTop_5_words_hq_titles = sorted(wa, key=wa.get, reverse=True)[:5]\nwa = find_most_common_words(eng_data[eng_data['Y']=='HQ']['Body'])\nTop_5_words_hq_bodies = sorted(wa, key=wa.get, reverse=True)[5:10]\n\n\nwa = find_most_common_words(eng_data[eng_data['Y']=='LQ_CLOSE']['Title'])\nTop_5_words_lq_titles = sorted(wa, key=wa.get, reverse=True)[:5]\nwa = find_most_common_words(eng_data[eng_data['Y']=='LQ_CLOSE']['Body'])\nTop_5_words_lq_bodies = sorted(wa, key=wa.get, reverse=True)[6:11]","a421c788":"def contains_hq_title_words(val):\n    splited = set(val.split(' '))\n    return len(splited.intersection(Top_5_words_hq_titles))\ndef contains_hq_body_words(val):\n    splited = set(val.split(' '))\n    return len(splited.intersection(Top_5_words_hq_bodies))\n    \ndef contains_lq_title_words(val):\n    splited = set(val.split(' '))\n    return len(splited.intersection(Top_5_words_lq_titles))\ndef contains_lq_body_words(val):\n    splited = set(val.split(' '))\n    return len(splited.intersection(Top_5_words_lq_bodies))","ee37b91d":"eng_data['Title_Contains_Top_5_hq_Words'] =eng_data['Title'].apply(contains_hq_title_words)\neng_data['Body_Contains_Top_5_hq_Words'] =eng_data['Title'].apply(contains_hq_body_words)\neng_data['Title_Contains_Top_5_lq_Words'] =eng_data['Title'].apply(contains_lq_title_words)\neng_data['Body_Contains_Top_5_lq_Words'] =eng_data['Title'].apply(contains_lq_body_words)\n","80f1490e":"def tag_cleaner(val):\n    splited = val.split('><')\n    clean = []\n    for tag in splited:\n        tag =  tag.replace('<',' ')\n        tag =  tag.replace('>',' ')\n        clean.append(tag)\n    return set(clean)","da9f8f57":"# all unique tags:\nunique_tags = set()\nfor tag in eng_data['Tags']:\n    unique_tags = unique_tags|tag_cleaner(tag)","e12cf038":"google_related      = []\nprog_lang_related   = []\napp_related         = []\nml_dl_related       = []\ncs_am_related       = []\ndb_related          = []\nweb_dev_related     = []\nelectronics_related = []\n\nprog_langs = ['swift','basic','c#','f#','c++','java','python','kotlin','camel','coffee','perl',\n             'lisp','ruby','visual-studio','azure','assembly','go','haskell',',rust','.net','spyder',\n             'jcl','sap','opengl','jenkins','apache','verilog','numpy']\n\nweb_keys = ['js','.j','net','docker','server','web','webpage','chrome','firefox','rest','api',\n           'angular','react','node','facebook','twitter','amazon-ses','chromium','browser','ntp','svn',\n           'xml','explorer','kivy','php']\n\napp_android = ['android','apk','sdk','ipad','iphone','ios']\n\ndb_keys = ['db','sql','query','mongo','nosql','json','database','cloud']\nml_dl_keys = ['tensorflow','machine_learning','deep_learning','scatter-plot','opencv','lda',\n             'mlmodel','regression','principal-components','pca','pytorch','sklearn','face-recognition',\n             ]\nelectronics_keys = ['esp8266','cpu','ram','core','tsu','gpu','arduino','raspberry']\ncs_am_keys  = ['optimization','x509','class','array','sort','algorithm','code','runtime','header-files',\n             'calculus','theory','geometry','polynomial']\n\nc_unique_tags = unique_tags.copy()\nc_unique_tags = list(c_unique_tags)\nc_unique_tags = [tag.strip() for tag in c_unique_tags]\n\n#google realted\nfor tag in c_unique_tags:\n    if 'google'  in tag:\n        google_related.append(tag)\nc_unique_tags = [tag for tag in c_unique_tags if tag not in google_related]\n\n#prog_lang realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for plang in prog_langs:\n        if s_flag is True:\n            break;\n        elif tag.find(plang) != -1:\n            prog_lang_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in prog_lang_related]\n\n#web_dev realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in web_keys:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            web_dev_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in web_dev_related]\n\n#phone\/app_dev realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in app_android:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            app_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in app_related]\n\n#db realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in db_keys:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            db_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in db_related]\n\n#ml_dl realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in ml_dl_keys:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            ml_dl_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in ml_dl_related]\n\n\n#electronics realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in electronics_keys:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            electronics_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in electronics_related]\n\n#cs_am realted\nfor tag in c_unique_tags:\n    s_flag = False\n    for wk in cs_am_keys:\n        if s_flag is True:\n            break;\n        elif tag.find(wk) != -1:\n            cs_am_related.append(tag)\n            s_flag=True\n            continue\nc_unique_tags = [tag for tag in c_unique_tags if tag not in cs_am_related]\n","5b1c88d6":"google_related_col = []    \nprog_lang_related_col = []  \napp_related_col = []        \nml_dl_related_col = []      \ncs_am_related_col = []      \ndb_related_col = []         \nweb_dev_related_col = []    \nelectronics_related_col = []\nother_related_col = []","97487486":"for tag in eng_data['Tags']:\n    clean_tag = list(tag_cleaner(tag))\n    google_related_score =0   \n    prog_lang_related_score =0  \n    app_related_score =0        \n    ml_dl_related_score =0      \n    cs_am_related_score =0      \n    db_related_score =0         \n    web_dev_related_score =0    \n    electronics_related_score =0 \n    other_related_score =0 \n\n    for tg in clean_tag:\n        zero_count = 0\n        if tg in google_related:\n            google_related_score = google_related_score +1\n        else:\n            zero_count = zero_count+1\n        if tg in prog_lang_related:\n            prog_lang_related_score = prog_lang_related_score+1\n        else:\n            zero_count = zero_count+1\n        if tg in app_related:\n            app_related_score=app_related_score+1\n        else:\n            zero_count = zero_count+1\n        if tg in ml_dl_related:\n            ml_dl_related_score = ml_dl_related_score + 1\n        else:\n            zero_count = zero_count+1\n        if tg in cs_am_related:\n            cs_am_related_score=cs_am_related_score+1\n        else:\n            zero_count = zero_count+1\n        if tg in db_related:\n            db_related_score=db_related_score+1\n        else:\n            zero_count = zero_count+1\n        if tg in web_dev_related:\n            web_dev_related_score=web_dev_related_score+1\n        else:\n            zero_count = zero_count+1\n        if tg in electronics_related:\n            electronics_related_score=electronics_related_score+1\n        else:\n            zero_count = zero_count+1\n        if zero_count == 8:\n            other_related_score = other_related_score + 1\n    google_related_col.append(google_related_score)\n    prog_lang_related_col.append(prog_lang_related_score) \n    app_related_col.append(app_related_score)       \n    ml_dl_related_col.append(ml_dl_related_score)      \n    cs_am_related_col.append(cs_am_related_score)  \n    db_related_col.append(db_related_score)        \n    web_dev_related_col.append(web_dev_related_score)  \n    electronics_related_col.append(electronics_related_score) \n    other_related_col.append(other_related_score)\n","41b27ef4":"eng_data['Google_Related'] = google_related_col\neng_data['Programing_Lang_Related'] = prog_lang_related_col\neng_data['App\/Phone_Related'] = app_related_col\neng_data['ML\/DL_Related'] = ml_dl_related_col\neng_data['CS\/AM_Related'] = cs_am_related_col\neng_data['DB\/Storage_Related'] = db_related_col\neng_data['WebApp\/Dev_Related'] = web_dev_related_col\neng_data['Electronics_Related'] = electronics_related_col\neng_data['Unclassified_Related'] = other_related_col","fd02115c":"#last feature we will add is the number of tags in a question\nnum_of_tags = []\n\nfor tag in eng_data['Tags']:\n    clean_tag = list(tag_cleaner(tag))\n    num_of_tags.append(len(clean_tag))\neng_data['Number_Of_Tags'] = num_of_tags","d82c804f":"#lets get rid of the text data the we will no longer use\neng_data.drop(columns=['Body','Tags','Title','Id'],inplace=True)","fe7604e7":"#lets transform our target label from nominal to numeric where we can use a ordinal scale to show the ranking \ntarget_labels = eng_data.Y.value_counts().to_frame().reset_index()['index'].to_list()\ntl_dic = {target_labels[num-1]:num for num in np.arange(1,4)}\neng_data.Y.replace(tl_dic,inplace=True)","5dce5255":"eng_data.head(4)","cc18ca16":"plt.figure(figsize=(20,11))\ncorrelations = eng_data.corr('pearson')\nax =sns.heatmap(correlations,cmap='Greens',annot=True)","c7c9eccf":"#removal_of_outliers\nnm = eng_data['Body_Avg_Word_Length']\neng_data['Body_Avg_Word_Length'] = eng_data['Body_Avg_Word_Length'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\nnm = eng_data['Title_Avg_Word_Length']\neng_data['Title_Avg_Word_Length'] = eng_data['Title_Avg_Word_Length'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\n\nnm = eng_data['Body_Char_Length']\neng_data['Body_Char_Length'] = eng_data['Body_Char_Length'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\nnm = eng_data['Title_Char_Length']\neng_data['Title_Char_Length'] = eng_data['Title_Char_Length'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\n\n\nnm = eng_data['Body_Num_Of_Words']\neng_data['Body_Num_Of_Words'] = eng_data['Body_Num_Of_Words'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\nnm = eng_data['Title_Num_Of_Words']\neng_data['Title_Num_Of_Words'] = eng_data['Title_Num_Of_Words'][nm.between(nm.quantile(0.10),nm.quantile(0.85))]\n","14c60a9d":"plt.figure(figsize=(20,11))\n\nax = sns.barplot(x=eng_data['Creation_Year'],y=eng_data['Number_Of_Tags'],hue=eng_data['Y'])\n","348f8fbf":"plt.figure(figsize=(20,11))\n\nax = sns.boxplot(x=eng_data['Creation_Hour'],y=eng_data['Body_Num_Of_Words'])\n","05710457":"plt.figure(figsize=(20,11))\n\nax = sns.distplot(eng_data[eng_data['Y'] == 3]['Body_Num_Of_Words'],hist=True,kde_kws={'lw':3.5},label='HQ')\nax = sns.distplot(eng_data[eng_data['Y'] == 2]['Body_Num_Of_Words'],hist=True,kde_kws={'lw':3.5},label='E_LQ')\nax = sns.distplot(eng_data[eng_data['Y'] == 1]['Body_Num_Of_Words'],hist=True,kde_kws={'lw':3.5},label='LQ')\nax.legend(prop={'size':20})","9015e161":"plt.figure(figsize=(20,11))\n\nax = sns.distplot(eng_data[eng_data['Y'] == 3]['Creation_Hour'],hist=True,kde_kws={'lw':3.5},label='HQ')\nax = sns.distplot(eng_data[eng_data['Y'] == 2]['Creation_Hour'],hist=True,kde_kws={'lw':3.5},label='E_LQ')\nax = sns.distplot(eng_data[eng_data['Y'] == 1]['Creation_Hour'],hist=True,kde_kws={'lw':3.5},label='LQ')\nax.plot([12,12],[0,0.12],color='r',linestyle='--',linewidth=3,label='$\\sigma=%.2f$'%(12))\nax.legend(prop={'size':20})","af596dbf":"fig,axs = plt.subplots(2,2)\nfig.set_figwidth(19)\nfig.set_figheight(11)\nsns.countplot(eng_data[eng_data['Programing_Lang_Related']>0]['Programing_Lang_Related'],hue=eng_data['Y'],\n             ax=axs[0,0])\naxs[0,0].legend(['HQ','E_LQ','LQ'])\n\nax = sns.countplot(eng_data[eng_data['WebApp\/Dev_Related']>0]['WebApp\/Dev_Related'],hue=eng_data['Y'],ax=\n                  axs[0,1])\nax.legend(['HQ','E_LQ','LQ'])\n\nax = sns.countplot(eng_data[eng_data['App\/Phone_Related']>0]['App\/Phone_Related'],hue=eng_data['Y'],ax=\n                  axs[1,0])\nax.legend(['HQ','E_LQ','LQ'])\n\nax = sns.countplot(eng_data[eng_data['DB\/Storage_Related']>0]['DB\/Storage_Related'],hue=eng_data['Y'],ax=\n                  axs[1,1])\nax.legend(['HQ','E_LQ','LQ'])\nplt.show()","8bf6cb38":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split","9f227412":"target = eng_data.pop('Y')\nfeatures = ['Creation_Year','Title_Avg_Word_Length','App\/Phone_Related','Number_Of_Tags','Electronics_Related',\n           'DB\/Storage_Related','ML\/DL_Related','CS\/AM_Related']","ebbd7cdb":"y = target.copy()\nX = eng_data[features].copy()","b0c3a808":"X.Title_Avg_Word_Length = X.Title_Avg_Word_Length.fillna(X.Title_Avg_Word_Length.mean())","aa2d501e":"x_train,x_test,y_train,y_test = train_test_split(X,y)","58263fe6":"Rf_model = RandomForestRegressor(n_estimators=20)\nRf_model.fit(x_train,y_train)\npred = Rf_model.predict(x_test)\npred = np.round(pred)\nrf_score = (f1(pred,y_test,average='macro'))\nprint(rf_score)","706deb3d":"ada_model = AdaBoostClassifier(learning_rate=0.03)\nada_model.fit(x_train,y_train)\npred = ada_model.predict(x_test)\npred = np.round(pred)\nada_score = (f1(pred,y_test,average='macro'))\nprint(ada_score)","87c983aa":"knn_model = KNeighborsClassifier(n_neighbors=300)\nknn_model.fit(x_train,y_train)\npred = knn_model.predict(x_test)\npred = np.round(pred)\nknn_score = (f1(pred,y_test,average='macro'))\nprint(knn_score)","e0d4985f":"tree_model = DecisionTreeClassifier(max_leaf_nodes=35)\ntree_model.fit(x_train,y_train)\npred = tree_model.predict(x_test)\npred = np.round(pred)\ntree_score = (f1(pred,y_test,average='macro'))\nprint(tree_score)","00af59b0":"from keras import Sequential\nfrom keras.layers import Dense","ac6f6824":"fcnn_model = Sequential()\nfcnn_model.add(Dense(8,activation='tanh',input_dim = len(features)))\nfcnn_model.add(Dense(16,activation='tanh'))\nfcnn_model.add(Dense(16,activation='tanh'))\nfcnn_model.add(Dense(1,activation='tanh'))\n\nfcnn_model.compile(optimizer='adam',loss='categorical_crossentropy')","ea453e1d":"fcnn_model.fit(x_train,y_train,epochs=10)","efee5b51":"fcnn_pred = fcnn_model.predict(x_test)\nfcnn_pred = np.round(fcnn_pred)\nprint((f1(fcnn_pred,y_test,average='macro')))","6fec7dd0":"tree_model = DecisionTreeClassifier(max_leaf_nodes=35)\ntree_model.fit(X,y)\npred = tree_model.predict(X)\npred = np.round(pred)\ntree_score = (f1(pred,y,average='macro'))\n\ncf_matrix = confusion_matrix(pred,y)\n\nax = sns.heatmap(cf_matrix,cmap='Blues',annot=True,fmt='g')","ae1e8832":"### We can see that for every year in our data the more tags in a question the higher chance of it being a HQ question","8b94bfbf":"### We can clearly see that our data is positively skewed and a new questions body length in any of our quality categories is most like to fall in the range between 40 and 60 words. ","0881cefc":"### So apparently the number of words in a questions body is usually smaller during night time starting from midnight till 10 in the morning the next day there is a trend of shorter question bodies.","4d4d784c":"### Question of all types usually are posted mid day at around 12-13 pm","2b407cad":"Here is the data the we will be working with:","68433e81":"# Intro \nBefore diving into our eda lets firstly look at some global information about our data set and try to set some goals and find some interesting question to awnser.\n\n### Quality Feature Description:\nHQ: High-quality posts with 30+ score and without a single edit.\n\nLQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\n\nLQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n\n### Goals:\n1) Predict Which Stack Overflow questions should be edited or closed Via the Quality Feature 'Y' ?\n\n2) Predict tags according to the text and title.\n\n### Interesting Questions To Investigate :\n1) Whats The Distribution Of The Different Tags And Are They Correlated With The Questions Quality \n\n2) Does the Datetime information of the creation date influence our the tags or the quality of the question \n\n3) Does the body\/title length influence the quality of the question \n\n4) what are the 3 most common words in the title and body of HQ questions and LQ questions \n\n### Feature Engineering Goals:\nSome of the features we will try to create inorder potentially\u200b gain some extra insight on our data are :\n\n1) The day that the question was created on\n\n2) the month the question was created in\n\n3) the year the question was created in\n\n4) the hour the question was create in\n\n5) The amount of chars in the title and body\n\n6) the amount of words in the title and body\n\n7) the amount average word length in each questions body and the average word length in each questions title  \n\n8) Find the most 3 most frequent words in low quality question and high quality questions and create a feature telling us does a particular question contain does 3 words ","a018f4aa":"### As for strightforward numeric correlation we can see that the highest features correlated with our target variable are the average length of the tite the creation year, the number of tags and is it web\/app dev related. ","e89c48f1":"### So we tried a few simple classification models and our f1 score is very low, lets try using a neural network and see if it can overscore the simple models","8d0c7177":"# EDA\nNow that we have extracted some features from our text data including the question body,tilte and tags, we can start and look for trends and correlation and see what will be the best set of features to make a robust prediction. \nAlso we will take a closer look into the question we asked in the begining of our kernel and try to awnser them before moving on to the model selection and evaluation step.\n\nIts important to state that the last features we create classifing the tags to a certian type of field like 'computer science' or 'web development' ect isn't perfect because most of the values need to be more closely looked at and carefully classified to create this features, firstly i plan to see does it even helps us understand any hidden trend before dedicating the time into create a prefet version of this practicular feature.","448177d4":"# Model Selection And Evaluation","3ce55542":"### Here we can clearly see that low quality question that are closed by stack overflow are usually the ones with more tags realated to the same field, for example the question who were tagged with 3 tags with the same theme like \"programing languages\" and etc are usually lower quality the the ones taged with fewer tags.","d1a3ba8a":"# Feature Engineering","c6eacae9":"### Apparently the neural network here is useless lets try and fit our decision tree model which did the best on the training data. "}}