{"cell_type":{"a9a94c95":"code","83f0e4e6":"code","e52a5fef":"code","ce534cae":"code","e9955d75":"code","8c9ccad4":"code","6b224a22":"code","1bce5e28":"code","3aa7cd83":"code","71b502f1":"code","e6919ecc":"code","6318e789":"code","f60a699d":"code","b663ed49":"code","2304c2c2":"code","f0aa50db":"code","3b06a0a6":"code","dbef0411":"code","0a4eb7fe":"code","4a6bd446":"code","4912cfb2":"code","c9262a9f":"code","df383b97":"code","f5ebd9b2":"code","d0e90134":"code","6f8abc28":"code","1da05ba2":"code","19cb039e":"code","9bd06376":"code","6c90c96b":"code","2de5245d":"code","64b7314c":"code","c21695ba":"code","6cc6193b":"code","59b8ca8a":"code","3590b891":"code","46bc0b39":"code","aa1a1e57":"code","929143b0":"code","1651853a":"code","b983fa41":"code","e62305f6":"code","cbf82590":"code","2f1fcba8":"code","5c1a4741":"code","55699d03":"code","460ae7f9":"code","0076cb51":"code","5fa44eee":"code","130f4ff2":"code","d54e8d59":"code","ba62eb20":"code","104d3b0f":"code","ac121548":"code","1693a6e6":"markdown","609b84ab":"markdown","674d89c8":"markdown","33a47957":"markdown","edbb4308":"markdown","02965fbb":"markdown","2a7c5cd4":"markdown","26635f7f":"markdown","36a3d63c":"markdown","db6f8463":"markdown","478ebd48":"markdown","dee83091":"markdown","ebde235d":"markdown","7e47599a":"markdown","76fa1e18":"markdown","c2c6245d":"markdown","e5122ac4":"markdown","19a7f1ee":"markdown","b3ea0096":"markdown","3d34da85":"markdown","3c45a20b":"markdown","f58a33c2":"markdown","2bf094d6":"markdown","bf9d3b08":"markdown","e8276e4d":"markdown","2242c4d2":"markdown","aa78ef48":"markdown","bb2ad7a8":"markdown","f0e45205":"markdown","5879a75b":"markdown","6455d086":"markdown","691bffbb":"markdown","65a55877":"markdown","93070459":"markdown","26bf0823":"markdown","f7489f48":"markdown","74a3a60d":"markdown","787a51b8":"markdown","065b1b8f":"markdown","748322eb":"markdown","c4abb4b9":"markdown","9839c278":"markdown","ae07cf3f":"markdown"},"source":{"a9a94c95":"\n#import libraries\nfrom __future__ import division\n\nfrom datetime import datetime, timedelta,date\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n\nimport plotly as py\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nimport xgboost as xgb\n","83f0e4e6":"#Read data\ntx_data = pd.read_csv('..\/input\/customer_segmentation\/customer_segmentation.csv', encoding='cp1252')","e52a5fef":"#initate plotly\npyoff.init_notebook_mode()\n\n#read data from csv and redo the data work we done before\ntx_data.head()","ce534cae":"#converting the type of Invoice Date Field from string to datetime.\ntx_data['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])","e9955d75":"#creating YearMonth field for the ease of reporting and visualization\ntx_data['InvoiceYearMonth'] = tx_data['InvoiceDate'].map(lambda date: 100*date.year + date.month)","8c9ccad4":"tx_data.describe()","6b224a22":"tx_data['Country'].value_counts()","1bce5e28":"#we will be using only UK data\ntx_uk = tx_data.query(\"Country=='United Kingdom'\").reset_index(drop=True)\n","3aa7cd83":"#create a generic user dataframe to keep CustomerID and new segmentation scores\ntx_user = pd.DataFrame(tx_data['CustomerID'].unique())\ntx_user.columns = ['CustomerID']\ntx_user.head()","71b502f1":"tx_uk.head()","e6919ecc":"#get the max purchase date for each customer and create a dataframe with it\ntx_max_purchase = tx_uk.groupby('CustomerID').InvoiceDate.max().reset_index()\ntx_max_purchase.columns = ['CustomerID','MaxPurchaseDate']\ntx_max_purchase.head()","6318e789":"# Compare the last transaction of the dataset with last transaction dates of the individual customer IDs.\ntx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days\ntx_max_purchase.head()","f60a699d":"#merge this dataframe to our new user dataframe\ntx_user = pd.merge(tx_user, tx_max_purchase[['CustomerID','Recency']], on='CustomerID')\ntx_user.head()","b663ed49":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Recency']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","2304c2c2":"#build 4 clusters for recency and add it to dataframe\nkmeans = KMeans(n_clusters=4)\ntx_user['RecencyCluster'] = kmeans.fit_predict(tx_user[['Recency']])\n","f0aa50db":"tx_user.head()","3b06a0a6":"tx_user.groupby('RecencyCluster')['Recency'].describe()","dbef0411":"#function for ordering cluster numbers\ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final\n\ntx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)","0a4eb7fe":"tx_user.head()","4a6bd446":"tx_user.groupby('RecencyCluster')['Recency'].describe()","4912cfb2":"#get order counts for each user and create a dataframe with it\ntx_frequency = tx_uk.groupby('CustomerID').InvoiceDate.count().reset_index()\ntx_frequency.columns = ['CustomerID','Frequency']","c9262a9f":"tx_frequency.head() #how many orders does a customer have","df383b97":"#add this data to our main dataframe\ntx_user = pd.merge(tx_user, tx_frequency, on='CustomerID')\n\ntx_user.head()","f5ebd9b2":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Frequency']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","d0e90134":"# Applying k-Means\nkmeans=KMeans(n_clusters=4)\ntx_user['FrequencyCluster']=kmeans.fit_predict(tx_user[['Frequency']])\n\n#order the frequency cluster\ntx_user = order_cluster('FrequencyCluster', 'Frequency', tx_user, True )\ntx_user.groupby('FrequencyCluster')['Frequency'].describe()","6f8abc28":"#calculate revenue for each customer\ntx_uk['Revenue'] = tx_uk['UnitPrice'] * tx_uk['Quantity']\ntx_revenue = tx_uk.groupby('CustomerID').Revenue.sum().reset_index()","1da05ba2":"tx_revenue.head()","19cb039e":"#merge it with our main dataframe\ntx_user = pd.merge(tx_user, tx_revenue, on='CustomerID')\ntx_user.head()\n","9bd06376":"from sklearn.cluster import KMeans\n\nsse={} # error\ntx_recency = tx_user[['Revenue']]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n    tx_recency[\"clusters\"] = kmeans.labels_  #cluster names corresponding to recency values \n    sse[k] = kmeans.inertia_ #sse corresponding to clusters\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","6c90c96b":"#apply clustering\nkmeans = KMeans(n_clusters=4)\ntx_user['RevenueCluster'] = kmeans.fit_predict(tx_user[['Revenue']])\n\n#order the cluster numbers\ntx_user = order_cluster('RevenueCluster', 'Revenue',tx_user,True)\n\n#show details of the dataframe\ntx_user.groupby('RevenueCluster')['Revenue'].describe()","2de5245d":"#calculate overall score and use mean() to see details\ntx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']\ntx_user.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()","64b7314c":"tx_user['Segment'] = 'Low-Value'\ntx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' \ntx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value' ","c21695ba":"tx_user","6cc6193b":"tx_uk.head()","59b8ca8a":"tx_uk['InvoiceDate'].describe()","3590b891":"tx_3m = tx_uk[(tx_uk.InvoiceDate < date(2011,6,1)) & (tx_uk.InvoiceDate >= date(2011,3,1))].reset_index(drop=True) #3 months time\ntx_6m = tx_uk[(tx_uk.InvoiceDate >= date(2011,6,1)) & (tx_uk.InvoiceDate < date(2011,12,1))].reset_index(drop=True) # 6 months time","46bc0b39":"#calculate revenue and create a new dataframe for it\ntx_6m['Revenue'] = tx_6m['UnitPrice'] * tx_6m['Quantity']\ntx_user_6m = tx_6m.groupby('CustomerID')['Revenue'].sum().reset_index()\ntx_user_6m.columns = ['CustomerID','m6_Revenue']","aa1a1e57":"tx_user_6m.head()","929143b0":"#plot LTV histogram\nplot_data = [\n    go.Histogram(\n        x=tx_user_6m['m6_Revenue']\n    )\n]\n\nplot_layout = go.Layout(\n        title='6m Revenue'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","1651853a":"tx_user.head()","b983fa41":"tx_uk.head()","e62305f6":"tx_merge = pd.merge(tx_user, tx_user_6m, on='CustomerID', how='left') #Only people who are in the timeline of tx_user_6m","cbf82590":"tx_merge = tx_merge.fillna(0)\n","2f1fcba8":"tx_graph = tx_merge.query(\"m6_Revenue < 50000\") #because max values are ending at 50,000 as seen in graph above\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['OverallScore'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['m6_Revenue'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['OverallScore'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['m6_Revenue'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['OverallScore'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['m6_Revenue'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"6m LTV\"},\n        xaxis= {'title': \"RFM Score\"},\n        title='LTV'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","5c1a4741":"#remove outliers\ntx_merge = tx_merge[tx_merge['m6_Revenue']<tx_merge['m6_Revenue'].quantile(0.99)]","55699d03":"tx_merge.head()","460ae7f9":"#creating 3 clusters\nkmeans = KMeans(n_clusters=3)\ntx_merge['LTVCluster'] = kmeans.fit_predict(tx_merge[['m6_Revenue']])\n\ntx_merge.head()","0076cb51":"#order cluster number based on LTV\ntx_merge = order_cluster('LTVCluster', 'm6_Revenue',tx_merge,True)\n\n#creatinga new cluster dataframe\ntx_cluster = tx_merge.copy()\n\n#see details of the clusters\ntx_cluster.groupby('LTVCluster')['m6_Revenue'].describe()","5fa44eee":"tx_cluster.head()","130f4ff2":"#convert categorical columns to numerical\ntx_class = pd.get_dummies(tx_cluster) #There is only one categorical variable segment\ntx_class.head()","d54e8d59":"#calculate and show correlations\ncorr_matrix = tx_class.corr()\ncorr_matrix['LTVCluster'].sort_values(ascending=False)\n","ba62eb20":"#create X and y, X will be feature set and y is the label - LTV\nX = tx_class.drop(['LTVCluster','m6_Revenue'],axis=1)\ny = tx_class['LTVCluster']\n\n#split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)","104d3b0f":"#XGBoost Multiclassification Model\nltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,n_jobs=-1).fit(X_train, y_train)\n\nprint('Accuracy of XGB classifier on training set: {:.2f}'\n       .format(ltv_xgb_model.score(X_train, y_train)))\nprint('Accuracy of XGB classifier on test set: {:.2f}'\n       .format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))\n\ny_pred = ltv_xgb_model.predict(X_test)\n","ac121548":"print(classification_report(y_test, y_pred))","1693a6e6":"<a href=2.1><h3>2.1 Feature Engineering <\/h3><\/a>","609b84ab":"<a href=3.1><h3> 3.1 Assigning a recency score<\/h3><\/a> \n\nWe are going to apply K-means clustering to assign a recency score. But we should tell how many clusters we need to K-means algorithm. To find it out, we will apply Elbow Method. Elbow Method simply tells the optimal cluster number for optimal inertia. Code snippet and Inertia graph are as follows:","674d89c8":"Since we are calculating recency, we need to know when last the person bought something. Let us calculate the last date of transaction for a person.","33a47957":"We have finished LTV clustering and here are the characteristics of each clusters as shown above.\n\nCluster 2 is the best with average 8.2k LTV whereas 0 is the worst with 396.","edbb4308":"Clsuter with max frequency is cluster 3, least frequency cluster is cluster 0.","02965fbb":"Great! cluster 1 earlier is now cluster0, cluster 2 earlier is now cluster 1 and so on. The clusters are arranged according to inactiviuty. Cluster 0 now is most inactive, cluster 3 is most active. ","2a7c5cd4":"<a href=5><h3>5. Revenue<\/h3><\/a>\n\nLet\u2019s see how our customer database looks like when we cluster them based on revenue. We will calculate revenue for each customer, plot a histogram and apply the same clustering method.","26635f7f":"<a href=1><h2> 1. Identifying the features <\/h2><\/a>","36a3d63c":"<h3><a href=4>4. Frequency <\/a><\/h3>\n\nTo create frequency clusters, we need to find total number orders for each customer. First calculate this and see how frequency look like in our customer database","db6f8463":"<a href=4.1><h3>4.1 Frequency clusters<\/h3><\/a>","478ebd48":"Here it looks like 3 is the optimal one. Based on business requirements, we can go ahead with less or more clusters. We will be selecting 4 for this example","dee83091":"<a href=5.1><h3> 5.1. Revenue clusters <\/h3><\/a>","ebde235d":"Since our LTV Clusters are 3 types, high LTV, mid LTV and low LTV; we will perform multi class classification. ","7e47599a":"Since our feature set is ready, let\u2019s calculate 6 months LTV for each customer which we are going to use for training our model.\n\n**Lifetime Value: Total Gross Revenue - Total Cost**\n\nThere is no cost specified in the dataset. That\u2019s why Revenue becomes our LTV directly.\n","76fa1e18":"We see that customers are active from 1 December 2010. Let us consider customers from March onwards (so that they are not new customers). We shall divide them into 2 subgroups. One will be where timeframe of analysing is 3 months, another will be timeframe of 6 months.","c2c6245d":"We have all the crucial information we need:\nCustomer ID\nUnit Price\nQuantity\nInvoice Date\nRevenue = Active Customer Count * Order Count * Average Revenue per Order\n","e5122ac4":"<a href=7><h3> 7. Customer Lifetime Value<\/a><\/h3>","19a7f1ee":"<h3><a href=3>3. Recency<\/a><\/h3>\n\nTo calculate recency, we need to find out most recent purchase date of each customer and see how many days they are inactive for. After having no. of inactive days for each customer, we will apply K-means* clustering to assign customers a recency score.\n\nLets go ahead and calculate that.","b3ea0096":"**Segmentation Techniques**\n\nYou can do many different segmentations according to what you are trying to achieve. If you want to increase retention rate, you can do a segmentation based on churn probability and take actions. But there are very common and useful segmentation methods as well. Now we are going to implement one of them to our business: RFM.\nRFM stands for Recency - Frequency - Monetary Value. Theoretically we will have segments like below:\n\n* Low Value: Customers who are less active than others, not very frequent buyer\/visitor and generates very low - zero - maybe negative revenue.\n* Mid Value: In the middle of everything. Often using our platform (but not as much as our High Values), fairly frequent and generates moderate revenue.\n* High Value: The group we don\u2019t want to lose. High Revenue, Frequency and low Inactivity.\n\nAs the methodology, we need to calculate Recency, Frequency and Monetary Value (we will call it Revenue from now on) and apply unsupervised machine learning to identify different groups (clusters) for each. Let\u2019s jump into coding and see how to do RFM Clustering.\n\n","3d34da85":"We see that Revenue, Frequency and RFM scores will be helpful for our machine learning models from the correlation with LTVCluster.\n","3c45a20b":"<a href=9> <h2>9. Final Clusters for Customer Lifetime Value <\/h2><\/a>\n\n- **Cluster 0**: Good precision, recall, f1-score and support\n- **Cluster 1**: Needs better precision, recall and f1-score\n- **Cluster 2**: Bad precision, F1-Score needs improvement","f58a33c2":"<a href=3.2><h3>3.2 Ordering clusters<\/a><\/h3>\n\nWe have a cluster corresponding to each customerID. But each cluster is randomly assigned. Cluster 2 is not better than cluster 1 for e.g. and so on. We want to give clusters according to most recent transactions.\n\nWe will first find the mean of recency value corresponding to each cluster. Then we will sort these values. Let's say cluster 3 has the most recent transactions mean value. From the above table we see that cluster 1(mean recency 304) > cluster 2 > cluster 3 > cluster 0. That means that cluster 1 is most inactive and cluster 0 is most recent. We will give indices to these clusters as 0,1,2,3. So cluster 1 becomes cluster 0, cluster 2 becomes cluster 1, cluster 3 becomes cluster 2 and so on. Now we will drop the original cluster numbers and replace them with 0,1,2,3. Code is below.","2bf094d6":"<a href = 7.1> <h3>7.1 Feature Engineering<\/h3><\/a>","bf9d3b08":"We invest in customers (acquisition costs, offline ads, promotions, discounts & etc.) to generate revenue and be profitable. Naturally, these actions make some customers super valuable in terms of lifetime value but there are always some customers who pull down the profitability. We need to identify these behavior patterns, segment customers and act accordingly.\nCalculating Lifetime Value is the easy part. First we need to select a time window. It can be anything like 3, 6, 12, 24 months. By the equation below, we can have Lifetime Value for each customer in that specific time window:\n\n**Lifetime Value: Total Gross Revenue - Total Cost**\n\nThis equation now gives us the historical lifetime value. If we see some customers having very high negative lifetime value historically, it could be too late to take an action. \n\nWe are going to build a simple machine learning model that predicts our customers lifetime value.\n\n<h3>Lifetime Value Prediction<\/h3>\n\n* Define an appropriate time frame for Customer Lifetime Value calculation\n* Identify the features we are going to use to predict future and create them\n* Calculate lifetime value (LTV) for training the machine learning model\n* Build and run the machine learning model\n* Check if the model is useful\n\n**1. How to decide the timeframe**\n\nDeciding the time frame really depends on your industry, business model, strategy and more. For some industries, 1 year is a very long period while for the others it is very short. In our example, we will go ahead with 6 months.\n\n**2. Identifying the features for prediction**\n\nRFM scores for each customer ID (which we calculated in the previous article) are the perfect candidates for feature set. To implement it correctly, we need to split our dataset. We will take 3 months of data, calculate RFM and use it for predicting next 6 months. So we need to create two dataframes first and append RFM scores to them.\n\nAfter the first two steps, it is easy to calculate CLTV and train and test the model.\n\n- <a href='#1'>1. Identifying the features<\/a>  \n- <a href='#2'>2. Importing necessary libraries and packages and reading files<\/a>\n   - <a href='#2.1'>2.1 Feature Engineering<\/a>\n- <a href='#3'>3. Recency<\/a>\n   - <a href='#3.1'>3.1 Assigning a recency score <\/a>\n   - <a href='#3.2'>3.2 Ordering clusters<\/a>\n- <a href='#4'> 4. Frequency<\/a>\n   - <a href='#4.1'>4.1 Frequency clusters<\/a>\n- <a href='#5'>5. Revenue<\/a>\n   - <a href='#5.1'>5.1 Revenue clusters<\/a>\n- <a href='#6'>6. Overall score based on RFM Clustering<\/a>  \n- <a href='#7'>7. Customer Lifetime Value <\/a>\n   - <a href='#7.1'>7.1 Feature engineering<\/a>\n- <a href='#8'>8. Machine Learning Model for Customer Lifetime Value Prediction<\/a>  \n- <a href='#9'>9. Final Clusters for Customer Lifetime Value<\/a>  \n","e8276e4d":"Before building the machine learning model, we need to identify what is the type of this machine learning problem. LTV itself is a regression problem. A machine learning model can predict the $ value of the LTV. But here, we want LTV segments. Because it makes it more actionable and easy to communicate with other people. By applying K-means clustering, we can identify our existing LTV groups and build segments on top of it.\n\nConsidering business part of this analysis, we need to treat customers differently based on their predicted LTV. For this example, we will apply clustering and have 3 segments (number of segments really depends on your business dynamics and goals):\n* Low LTV\n* Mid LTV\n* High LTV\n\nWe are going to apply K-means clustering to decide segments and observe their characteristics\n","2242c4d2":"By Elbow method, clusters number should be 4 as after 4, the graph goes down.","aa78ef48":"Score 8 is our best customer, score 0 is our worst  customer.","bb2ad7a8":"If model tells us this customer belongs to cluster 0, 93 out of 100 will be correct (precision). And the model successfully identifies 95% of actual cluster 0 customers (recall).\n\nWe really need to improve the model for other clusters. For example, we barely detect 67% of Mid LTV customers. \n\n**Possible actions to improve performance**\n\n- Adding more features and improve feature engineering\n- Try different models other than XGBoost\n- Apply hyper parameter tuning to current model\n- Add more data to the model if possible\n","f0e45205":"<h3> 2. Importing relevant packages and libraries <\/h3>","5879a75b":"We have some customers with negative revenue as well. Let\u2019s continue and apply k-means clustering:\n","6455d086":"From elbow's method, we find that clusters can be 3 or 4. Lets take 4 as the number of clusters","691bffbb":"<a href=6><h3>6. Overall Score based on RFM Clsutering<\/h3><\/a>\n\nWe have scores (cluster numbers) for recency, frequency & revenue. Let\u2019s create an overall score out of them\n","65a55877":"Histogram clearly shows we have customers with negative LTV. We have some outliers too. Filtering out the outliers makes sense to have a proper machine learning model.","93070459":"We can visualise correlation between overall RFM score and revenue. Positive correlation is quite visible here. High RFM score means high LTV.\n","26bf0823":"<A href = 8><h3> 8. Machine Learning Model for Customer Lifetime Value Prediction<\/h3> <\/a>","f7489f48":"<H2>Customer Lifetime prediction value <\/H2>","74a3a60d":"Cluster 3 has max revenue, cluster 0 has lowest revenue","787a51b8":"Ok, next step. We will merge our 3 months and tx_uk and also merge 6 months dataframe and tx_uk to see correlations between LTV and the feature set we have.","065b1b8f":"There are few more step before training the machine learning model:\n* Feature engineering. \n* Convert categorical columns to numerical columns.\n* We will check the correlation of features against our label, LTV clusters.\n* We will split our feature set and label (LTV) as X and y. We use X to predict y.\n* Will create Training and Test dataset. Training set will be used for building the machine learning model. We will apply our model to Test set to see its real performance.\n","748322eb":"Determine the right number of clusters for K-Means by elbow method","c4abb4b9":"**Elbow method to find out the optimum number of clusters for K-Means**","9839c278":"Accuracy looks good on training and test set. Let's check the precision, recall, fscore too","ae07cf3f":"Starting from this part, we will be focusing on UK data only (which has the most records). We can get the monthly active customers by counting unique CustomerIDs. The same analysis can be carried out for customers of other countries as well."}}