{"cell_type":{"43d2458d":"code","f82cb17d":"code","5b16ca5f":"code","2c89c7c1":"code","ab70c0a6":"code","61655c64":"code","d7f2e20c":"code","4e8e5627":"code","5a9fc190":"code","437b8ff6":"code","44a3edf9":"code","a05fc6ff":"code","7779e400":"code","367a4dfe":"code","ca2dfbbf":"markdown","0c4d435e":"markdown","dd5d7d12":"markdown","0578d746":"markdown","214c230a":"markdown","5731f4cc":"markdown","2afa6017":"markdown","c52708f0":"markdown","26d9fa10":"markdown","ab3564d6":"markdown","856a4301":"markdown","c3defb4b":"markdown"},"source":{"43d2458d":"from sklearn import model_selection,linear_model,metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn import decomposition,ensemble\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import fmin\nfrom functools import partial\nimport xgboost\n\nimport numpy as np \nimport pandas as pd \nimport glob\nimport time\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f82cb17d":"## getting all the data files available\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5b16ca5f":"def create_folds(df_path,num_folds):\n    df  = pd.read_csv(df_path,sep='\\t')\n    df.loc[:,\"kfold\"] = -1\n    \n    ## for random shuffle of data, frac=1 will return all the records in shuffled manner.If you want to extract random subsample\n    ##, change the frac paramter value\n    \n    df = df.sample(frac=1).reset_index(drop=True) \n    \n    y = df.sentiment.values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds)\n    \n    ## t_ : indices for training, v_: indices for validation, f : fold number\n    for f,(t_,v_) in enumerate(skf.split(X=df,y=y)):\n        df.loc[v_,\"kfold\"] = f\n        \n    df.to_csv(f\"train_folds_{num_folds}.csv\",index=False)\n    \n","2c89c7c1":"create_folds(\"\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\",5)\npd.read_csv(\"train_folds_5.csv\").kfold.value_counts()","ab70c0a6":"model_dict = {'lr':linear_model.LogisticRegression(),'rf':ensemble.RandomForestClassifier()}\nfeat_create_dict = {'tfidf':TfidfVectorizer(max_features=1000),'cntvec':CountVectorizer(),'svd':decomposition.TruncatedSVD(n_components=120)}","61655c64":"def run_training(path,fold,feat_create_mode,model_name):\n    df = pd.read_csv(path)\n    \n    df_train = df[df.kfold!=fold].reset_index(drop=True)\n    df_valid = df[df.kfold==fold].reset_index(drop=True)\n\n    if feat_create_mode!='svd':\n        featvec = feat_create_dict[feat_create_mode]\n        featvec.fit(df_train.review.values)\n\n        xtrain = featvec.transform(df_train.review.values)\n        xvalid = featvec.transform(df_valid.review.values)\n    else :\n        featvec = feat_create_dict['tfidf']\n        featvec.fit(df_train.review.values)\n\n        xtrain = featvec.transform(df_train.review.values)\n        xvalid = featvec.transform(df_valid.review.values)\n        \n        svd = feat_create_dict[feat_create_mode]\n        svd.fit(xtrain)\n        \n        xtrain = svd.transform(xtrain)\n        xvalid = svd.transform(xvalid)\n        \n    \n    ytrain = df_train.sentiment.values\n    yvalid = df_valid.sentiment.values\n    \n    clf = model_dict[model_name]\n    clf.fit(xtrain,ytrain)\n    \n    ypred = clf.predict_proba(xvalid)[:,1]\n    auc = metrics.roc_auc_score(yvalid,ypred)\n    \n    print(f\"fold = {fold} AUC = {auc}\")\n    \n    df_valid.loc[:,f'{feat_create_mode}_{model_name}_pred'] = ypred\n    \n    return df_valid[['id','sentiment','kfold',f'{feat_create_mode}_{model_name}_pred']]","d7f2e20c":"for key_feat,val_feat in  feat_create_dict.items():\n    for key_model,val_model in model_dict.items():\n        strt = time.time()\n        print(f\" Training model feature creation:{key_feat} and model:{key_model}\")\n        dfs = []\n        for j in range(5):\n            temp_df = run_training(\"train_folds_5.csv\",j,key_feat,key_model)\n            dfs.append(temp_df)\n        fin_valid_df = pd.concat(dfs)\n        fin_valid_df.to_csv(f\"{key_feat}_{key_model}.csv\",index=False)\n        end = time.time()\n        print(f\"Time Taken: {end-strt}secs\")\n        print(fin_valid_df.shape)","4e8e5627":"files = glob.glob(\"*.csv\")\nfiles.remove(\"train_folds_5.csv\")\ndf = pd.DataFrame()\nfor f in files :\n    if len(df)<=0:\n        df = pd.read_csv(f)\n    else :\n        temp_df = pd.read_csv(f).drop(columns = ['sentiment','kfold'])\n        df = pd.merge(df,temp_df,on=['id'],how='left')\npred_cols = [col for col in df.columns if col.find(\"pred\")>=0]\ntargets = df.sentiment.values\n\npred_dict = {col:df[col].values for col in pred_cols}\npred_rank_dict = {col:df[col].rank().values for col in pred_cols}\n\n## Getting AUC for all models separately \nfor col in pred_cols:\n    auc = metrics.roc_auc_score(targets,df[col].values)\n    print(f\"pred_col = {col}; overall_auc ={auc}\")","5a9fc190":"print(\"-------------------------------------------\")\nprint(\"Blending Results\")\nprint(\"-------------------------------------------\")\n\nprint(\"average\")\navg_pred = df[pred_cols].mean(axis=1).values\nprint(metrics.roc_auc_score(targets,avg_pred))\n\nprint(\"-------------------------------------------\")\nprint(\"weighted average\")\nwt_dict = {col:1 for col in pred_cols}\nwt_dict['tfidf_lr_pred'] = 3\nprint(\"weights used\")\nprint(wt_dict)\navg_pred = np.sum(np.array([val*wt_dict[key] for key,val in pred_dict.items()]),axis=0)\/sum(list(wt_dict.values()))\nprint(metrics.roc_auc_score(targets,avg_pred))\n\nprint(\"-------------------------------------------\")\nprint(\"rank averaging\")\navg_pred  = np.mean(np.array([val for key,val in pred_dict.items()]),axis=0)\nprint(metrics.roc_auc_score(targets,avg_pred))\n\nprint(\"-------------------------------------------\")\nprint(\"weighted rank averaging\")\nwt_rank_dict = {col:1 for col in pred_cols}\nwt_rank_dict['tfidf_lr_pred'] = 3\nprint(\"weights used\")\nprint(wt_rank_dict)\navg_pred = np.sum(np.array([val*wt_rank_dict[key] for key,val in pred_rank_dict.items()]),axis=0)\/sum(list(wt_rank_dict.values()))\nprint(metrics.roc_auc_score(targets,avg_pred))\n\nprint(\"-------------------------------------------\")\nprint(\"weighted rank averaging\")\nwt_rank_dict = {col:1 for col in pred_cols}\nwt_rank_dict['cntvec_lr_pred'] = 3\nprint(\"weights used\")\nprint(wt_rank_dict)\navg_pred = np.sum(np.array([val*wt_rank_dict[key] for key,val in pred_rank_dict.items()]),axis=0)\/sum(list(wt_rank_dict.values()))\nprint(metrics.roc_auc_score(targets,avg_pred))","437b8ff6":"## defining custom class for optimzation to get optimal weights\nclass OptimizeAUC():\n    def __init__(self):\n        self.coef_ = 0\n    \n    ## function to caluculate AUC for each fold while optimizing and \n    ## multiplying it with -1 , becasue we are using fmin (minimizing the metric) which in turn led to maximization of AUC\n    def _auc(self,coef,X,y):\n        X_coef = X*coef\n        predictions = np.sum(X_coef,axis=1)\n        auc_score = metrics.roc_auc_score(y,predictions)\n        return -1.0*auc_score\n    \n    ## function for initiating optimization process.Here we are initializing the weights with dirichlet distribution\n    ##, we can take any other values also for weight initialization\n    def fit(self,X,y):\n        partial_loss = partial(self._auc,X=X,y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss,init_coef,disp=True)\n    \n    ## function to make prediction using weights obtained while training\n    def predict(self,X):\n        x_coef = X*self.coef_\n        predictions = np.sum(x_coef,axis=1)\n        return predictions","44a3edf9":"## function for model development for optimal parameters\ndef run_training_wts(pred_df,fold,pred_cols,model_name,std=False):\n\n    train_df = pred_df[pred_df.kfold!=fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold==fold].reset_index(drop=True)\n    \n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    \n    if std:\n        std_ = StandardScaler()\n        std_.fit(xtrain)\n        \n        xtrain = std_.transform(xtrain)\n        xvalid = std_.transform(xvalid)\n    \n    opt = model_dict[model_name]\n    opt.fit(xtrain,train_df.sentiment.values)\n    if model_name != 'lr':\n        preds = opt.predict(xvalid)\n    else :\n        preds = opt.predict_proba(xvalid)[:,1]\n    \n    auc = metrics.roc_auc_score(valid_df.sentiment.values,preds)\n    \n    print(f\"fold={fold} auc={auc}\")\n\n    return opt.coef_","a05fc6ff":"model_dict = {'lr':linear_model.LogisticRegression(),'custom_opt':OptimizeAUC(),'linear':linear_model.LinearRegression()}\nfor key_model,val_model in model_dict.items():\n    for std in [True,False]:\n        print(f\"model:{key_model}; Scaling:{std}\")\n        coefs = []\n        for j in range(5):\n            temp_df =  run_training_wts(df,j,pred_cols,key_model,std)\n            coefs.append(temp_df)\n        coefs  = np.mean(np.array(coefs),axis=0)\n        print(coefs)\n        if key_model!='lr':\n            wt_avg = np.sum(np.array([coefs[idx]*df[col].values for idx,col in enumerate(pred_cols)]),axis=0)\n        else :\n            wt_avg = np.sum(np.array([coefs[0,idx]*df[col].values for idx,col in enumerate(pred_cols)]),axis=0)\n            \n        auc = metrics.roc_auc_score(targets,wt_avg)\n        print(f\"optimal coefs overall auc = {auc}\")\n        print(\"==================================\")\n","7779e400":"def run_training_stack(pred_df,fold,pred_cols):\n\n    train_df = pred_df[pred_df.kfold!=fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold==fold].reset_index(drop=True)\n    \n    xtrain = train_df[pred_cols].values\n    xvalid = valid_df[pred_cols].values\n    \n    clf = xgboost.XGBClassifier()\n    clf.fit(xtrain,train_df.sentiment.values)\n    preds = clf.predict_proba(xvalid)[:,1]\n    \n    auc = metrics.roc_auc_score(valid_df.sentiment.values,preds)\n    print(f\"fold={fold} auc={auc}\")\n\n    valid_df.loc[:,\"xgb_pred\"] = preds\n    \n    return valid_df","367a4dfe":"dfs = []\nfor j in range(5):\n    temp_df = run_training_stack(df,j,pred_cols)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"xgb.csv\",index=False)\nauc = metrics.roc_auc_score(targets,fin_valid_df.xgb_pred.values)\nprint(f\"AUC using Xgboost : {auc}\")","ca2dfbbf":"## Stacking \n\nIt means stack of models that is on top of the model predictions which means using prediction values as features to make final predictions using another model","0c4d435e":"This notebook is to understand the concept of stacking, blending and ensemble as part of model building process.It contains the learning and implementation of the same from Abhishek Thakur's awesome video tutorial on You Tube.Link for same is below :\nhttps:\/\/www.youtube.com\/watch?v=TuIgtitqJho&ab_channel=AbhishekThakur\n\nFor starter , let's answer some questions that came to me while watching the tutorial and then we will move towards implementation.\n\n**1. Where can we use these metods?<br>**\nThese techniques were used initially mostly for competitions, but it is a widespread technique used for model improvemnt in various organizations and open source projects. I have used stacking for one of the project at my work place and observed significant improvemnt in accuracy of model.<br>\n\n**2. How to implement?<br>**\nIn the video, implementation is done in nice form of multiple python scripts.Here I will be implementing the same in form of functions as I am using notebook.\n<br>\n<br>\n\nP.S.: Do watch Abhishek's video on this topic and other topics as well.","dd5d7d12":"## Creating Data Folds\n1. It takes path of the input data and number of folds as arguments\n2. We are using stratified K fold here to maintain the distribution of data","0578d746":"Further Experiments :\n1. Here , we are getting AUC score less then the previous section. We can improve this by tuning xgboost or experimenting with other models\n2. We can also try using the predictions from previous models that is before stacking as features in the main training data and build model on top of that data","214c230a":"## Combining all Model Results\nFrom above, we get 6 different models with variations in feature creation.Now,we are going to see how to combine these predictions to get better overall AUC by blending,stacking or ensembling","5731f4cc":"### Optimizing the Weighing Parameters for Blending\n\nBelow we are going to obtain optimal weights by taking all predictions columns as features with objective of maximizing the AUC ","2afa6017":"# **Again thanks to Abhishek Thakur for making tutorial videos and making it easily understandable.**","c52708f0":"## Model Development\n\n<b>Classification Model-<\/b><br>\n1.We are using logistic regression and random forest<br>\n2.AUC is taken as evaluation parameter<br>\n3.We are going to predict_proba as AUC calucation requires probablity values\n\n\n<b>Feature Creation -<\/b><br>\n1.We are using TF-IDF and count vectorization for feature creation from text columns \"review\"<br>\n2.We are using SVD on top of TF-IDF for feature transformation\n\n","26d9fa10":"## Blending\nIn this section , we are going to following ways of combining all the predictions:\n1. Average \n2. Weighted Average : Manual Setting of Weights\n3. Rank Average : Learned first time from video tutorial\n4. Weighted Rank Average\n\nManual setting of weights :<br>\n\nAs visible form above overall AUC scores is best for cntvec_lr_pred and tfidf_lr_pred models.So, we will try to give more weights to these models while combining ","ab3564d6":"## Import Required Libraries","856a4301":"As we can see from previous section best AUC score is 0.9457357184 from cntvec_lr_pred models and after combining we are getting improvment of 0.01 points by doing weighted rank average that is 0.9503327391999999 from weighted rank averaging. Now, next step to get the optimal weights instead of manual setting for efficient processing.Let's see how to do that:","c3defb4b":"So, again we have seen improvment in AUC score from previous one that is from 0.9503327391999999 to 0.9535015936000001 by taking weights obtained from custom optimization function and without scaling the prediction values. This way, we are able to get optimal weights to get maximum improvement in AUC by weighing method"}}