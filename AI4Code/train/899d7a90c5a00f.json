{"cell_type":{"d96c4e67":"code","6e292a36":"code","4bf8a74e":"code","8508203f":"code","cd4d9c22":"code","05536e89":"code","0182dd62":"code","20474b5d":"code","3b4aed5d":"code","748669fd":"code","37c4a72d":"code","8809d188":"code","9407d9db":"code","285c3ca4":"code","400bd5d8":"code","2cba2d70":"code","11334b1c":"code","06bfe7d7":"code","4c9119aa":"code","71f42dec":"code","1a1a7215":"code","ba378edc":"markdown"},"source":{"d96c4e67":"import numpy as np\nimport pandas as pd","6e292a36":"import matplotlib.pyplot as plt\nimport os\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical","4bf8a74e":"directory = '\/kaggle\/input\/rockpaperscissors'\nprint(os.listdir(directory))","8508203f":"dataset=[]\nmapping={\"paper\":0,\"rock\":1,\"scissors\":2}\ncount=0\n\nfor file in os.listdir(directory):   \n    if file=='README_rpc-cv-images.txt' or file=='rps-cv-images':\n        continue\n    path=os.path.join(directory,file)                     \n    for im in os.listdir(path):\n        image=load_img(os.path.join(path,im),target_size=(150,150))\n        image=img_to_array(image)\n        image=image\/255.0\n        dataset.append([image,count])\n\n    count=count+1","cd4d9c22":"image0=load_img('..\/input\/rockpaperscissors\/paper\/0eqArS2GgsBeqgSn.png',target_size=(150,150))\nimage1=img_to_array(image0)\nimage2=image1\/255.0","05536e89":"data,labels=zip(*dataset)","0182dd62":"c0=0\nc1=0\nc2=0\nfor i in labels:\n    if i==0:\n        c0+=1\n    elif i==1:\n        c1+=1\n    elif i==2:\n        c2+=1\n        \nprint(\"Total no. of images in dataser are {}\".format(len(labels)))        \nprint(\"Number of images of rock: {}\\nNumber of images of paper: {}\\nNumber of images of scissors: {}\".format(c1,c0,c2))","20474b5d":"labels=to_categorical(labels)\ndata=np.array(data)\nlabels=np.array(labels)\n\nprint(\"Data Shape:{}\\nLabels shape: {}\".format(data.shape,labels.shape))","3b4aed5d":"data2=data.reshape(-1,150,150,3)\nprint(data2.shape)\nprint(data2[0].shape)","748669fd":"from sklearn.model_selection import train_test_split\ntrainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=44)","37c4a72d":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","8809d188":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                    width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","9407d9db":"pretrained_model0 = tf.keras.applications.MobileNetV2(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model1 = tf.keras.applications.ResNet50(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model2 = tf.keras.applications.VGG16(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3 = tf.keras.applications.DenseNet121(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model4 = tf.keras.applications.Xception(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model5 = tf.keras.applications.InceptionV3(input_shape=(150,150,3),include_top=False,weights='imagenet',pooling='avg')\n\npretrained_model0.trainable = False\npretrained_model1.trainable = False\npretrained_model2.trainable = False\npretrained_model3.trainable = False\npretrained_model4.trainable = False\npretrained_model5.trainable = False","285c3ca4":"inputs0 = pretrained_model0.input\ninputs1 = pretrained_model1.input\ninputs2 = pretrained_model2.input\ninputs3 = pretrained_model3.input\ninputs4 = pretrained_model4.input\ninputs5 = pretrained_model5.input\n\nx0 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model0.output)\noutputs0 = tf.keras.layers.Dense(3, activation='softmax')(x0)\nx1 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model1.output)\noutputs1 = tf.keras.layers.Dense(3, activation='softmax')(x1)\nx2 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model2.output)\noutputs2 = tf.keras.layers.Dense(3, activation='softmax')(x2)\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(3, activation='softmax')(x3)\nx4 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model4.output)\noutputs4 = tf.keras.layers.Dense(3, activation='softmax')(x4)\nx5 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model5.output)\noutputs5 = tf.keras.layers.Dense(3, activation='softmax')(x5)\n\nmodel0 = tf.keras.Model(inputs=inputs0, outputs=outputs0)\nmodel1 = tf.keras.Model(inputs=inputs1, outputs=outputs1)\nmodel2 = tf.keras.Model(inputs=inputs2, outputs=outputs2)\nmodel3 = tf.keras.Model(inputs=inputs3, outputs=outputs3)\nmodel4 = tf.keras.Model(inputs=inputs4, outputs=outputs4)\nmodel5 = tf.keras.Model(inputs=inputs5, outputs=outputs5)","400bd5d8":"model0.summary()\nmodel1.summary()\nmodel2.summary()\nmodel3.summary()\nmodel4.summary()\nmodel5.summary()","2cba2d70":"model0.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel1.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel2.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel3.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel4.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel5.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhis0=model0.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)\nhis1=model1.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)\nhis2=model2.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)\nhis3=model3.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)\nhis4=model4.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)\nhis5=model5.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)","11334b1c":"y_pred0=model0.predict(testx)\ny_pred1=model1.predict(testx)\ny_pred2=model2.predict(testx)\ny_pred3=model3.predict(testx)\ny_pred4=model4.predict(testx)\ny_pred5=model5.predict(testx)\n\npred0=np.argmax(y_pred0,axis=1)\npred1=np.argmax(y_pred1,axis=1)\npred2=np.argmax(y_pred2,axis=1)\npred3=np.argmax(y_pred3,axis=1)\npred4=np.argmax(y_pred4,axis=1)\npred5=np.argmax(y_pred5,axis=1)\n\nground = np.argmax(testy,axis=1)\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(ground,pred0))\nprint(classification_report(ground,pred1))\nprint(classification_report(ground,pred2))\nprint(classification_report(ground,pred3))\nprint(classification_report(ground,pred4))\nprint(classification_report(ground,pred5))","06bfe7d7":"get_acc0 = his0.history['accuracy']\nget_acc1 = his1.history['accuracy']\nget_acc2 = his2.history['accuracy']\nget_acc3 = his3.history['accuracy']\nget_acc4 = his4.history['accuracy']\nget_acc5 = his5.history['accuracy']\n\nepochs = range(len(get_acc0))\n\nplt.plot(epochs, get_acc0, 'r', label='MobileNetV2')\nplt.plot(epochs, get_acc1, 'b', label='ResNet50')\nplt.plot(epochs, get_acc2, 'g', label='VGG16')\nplt.plot(epochs, get_acc3, 'y', label='DenseNet121')\nplt.plot(epochs, get_acc4, 'm', label='Xception')\nplt.plot(epochs, get_acc5, 'c', label='InceptionV3')\n\nplt.title('Accuracy comparison')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","4c9119aa":"get_loss0 = his0.history['loss']\nget_loss1 = his1.history['loss']\nget_loss2 = his2.history['loss']\nget_loss3 = his3.history['loss']\nget_loss4 = his4.history['loss']\nget_loss5 = his5.history['loss']\n\nepochs = range(len(get_loss0))\n\nplt.plot(epochs, get_loss0, 'r', label='MobileNetV2')\nplt.plot(epochs, get_loss1, 'b', label='ResNet50')\nplt.plot(epochs, get_loss2, 'g', label='VGG16')\nplt.plot(epochs, get_loss3, 'y', label='DenseNet121')\nplt.plot(epochs, get_loss4, 'm', label='Xception')\nplt.plot(epochs, get_loss5, 'c', label='InceptionV3')\n\nplt.title('Loss comparison')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","71f42dec":"value_acc0 = his0.history['val_accuracy']\nvalue_acc1 = his1.history['val_accuracy']\nvalue_acc2 = his2.history['val_accuracy']\nvalue_acc3 = his3.history['val_accuracy']\nvalue_acc4 = his4.history['val_accuracy']\nvalue_acc5 = his5.history['val_accuracy']\n\nepochs = range(len(get_acc0))\n\nplt.plot(epochs, value_acc0, 'r', label='MobileNetV2')\nplt.plot(epochs, value_acc1, 'b', label='ResNet50')\nplt.plot(epochs, value_acc2, 'g', label='VGG16')\nplt.plot(epochs, value_acc3, 'y', label='DenseNet121')\nplt.plot(epochs, value_acc4, 'm', label='Xception')\nplt.plot(epochs, value_acc5, 'c', label='InceptionV3')\n\nplt.title('Validation accuracy comparison')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","1a1a7215":"val_loss0 = his0.history['val_loss']\nval_loss1 = his1.history['val_loss']\nval_loss2 = his2.history['val_loss']\nval_loss3 = his3.history['val_loss']\nval_loss4 = his4.history['val_loss']\nval_loss5 = his5.history['val_loss']\n\nepochs = range(len(val_loss0))\n\nplt.plot(epochs, val_loss0, 'r', label='MobileNetV2')\nplt.plot(epochs, val_loss1, 'b', label='ResNet50')\nplt.plot(epochs, val_loss2, 'g', label='VGG16')\nplt.plot(epochs, val_loss3, 'y', label='DenseNet121')\nplt.plot(epochs, val_loss4, 'm', label='Xception')\nplt.plot(epochs, val_loss5, 'c', label='InceptionV3')\n\nplt.title('Validation loss comparison')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","ba378edc":"## Keras applications ('MobileNetV2', 'ResNet50', 'VGG16', 'DenseNet121', 'Xception' and 'InceptionV3') were compared using RPS image data.\n[https:\/\/keras.io\/api\/applications\/](https:\/\/keras.io\/api\/applications\/)"}}