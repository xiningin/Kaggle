{"cell_type":{"1dd3dffc":"code","3aac0044":"code","17fbbd51":"code","96f553bc":"code","d2d9c88b":"code","3132e30f":"code","cc3c3b26":"code","dd3d51e9":"code","16b3252b":"code","eb5fb8b4":"code","bedffd54":"code","65825d0c":"code","8d062f1e":"code","60bddda3":"code","f26e717d":"code","e477dfea":"code","c43dfaf7":"code","be0c4d5c":"code","73c75749":"code","fb261f7c":"code","5b0f2728":"code","b13a5fba":"code","a7b16c57":"code","e4926ae4":"code","3d294b2d":"code","dca10a25":"code","ea16a2c1":"code","7ffb46e2":"code","f54e7cfe":"code","861df00a":"code","cfedd799":"code","ad2892fd":"code","cd457d37":"code","bad37b1d":"markdown","2ad97bab":"markdown","58c4826c":"markdown","bf4fd651":"markdown","e215b705":"markdown","b4b48226":"markdown","e5efd045":"markdown"},"source":{"1dd3dffc":"import time\nimport random\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Basic\nimport datetime\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV\nfrom sklearn import metrics\nimport statsmodels as sm\nfrom sklearn.pipeline import make_pipeline\n\n# Model considerations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Resampling \nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.svm import LinearSVC\n\n# Input\nimport os\nprint(os.listdir(\"..\/input\"))","3aac0044":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","17fbbd51":"train.head()","96f553bc":"x = train.drop(['ID_code', 'target'], axis = 1)\ny = train['target']","d2d9c88b":"x_test = test.drop(['ID_code'], axis = 1)","3132e30f":"x.head()","cc3c3b26":"# how much training data we have\nx.shape","dd3d51e9":"train.target.unique()","16b3252b":"# % of each class, as target variable is binary\ny0 = len(train[train.target == 0]) \/ len(train.target) * 100\ny1 = len(train[train.target == 1]) \/ len(train.target) * 100\nclass_percentage = [y0, y1]","eb5fb8b4":"print(\"Percentage of Class0: %s \\nPercentage of Class1: %s: \" % (y0, y1))","bedffd54":"label = ['y0', \"y1\"]\nsns.barplot(label, class_percentage)","65825d0c":"# let's check the variable distribution\nx.describe()","8d062f1e":"# random column selection\n# print('=====Randomly selected Column Distributions=====')\n# plt.figure(figsize=(26, 24))\n# columns = x.columns.values\n# random.seed(32)\n# col_random = random.SystemRandom()\n# for i in range(0, 32):\n#     col = col_random.choice(columns)\n#     plt.subplot(8, 4, i + 1)\n#     sns.distplot(train[col], color = 'seagreen')\n#     plt.axvline(train[col].mean(), 0, 1, linestyle = '--', color = 'blue')\n#     plt.title(col)","60bddda3":"# random column selection for outliers check using BOXPlot\n# plt.figure(figsize=(20, 40))\n# for i in range(0, 32):\n#     col = col_random.choice(columns)\n#     plt.subplot(16, 2, i+1)\n#     sns.boxplot(x[col], color='orange')\n#     plt.title(col)","f26e717d":"# correlation graph of randomly selected two variables\n# plt.figure(figsize = (24, 24))\n# for i in range(0, 12):\n#     col1 = col_random.choice(columns)\n#     col2 = col_random.choice(columns)\n#     plt.subplot(6, 4, i+1)\n#     sns.regplot(x=x[col1][0:2000], y=x[col2][0:2000], data=x);\n#     plt.title(col1+ ' v\/s ' +col2)","e477dfea":"# params = {\n#         'num_leaves': 6,\n#         'max_bin': 63,\n#         'min_data_in_leaf': 17,\n#         'learning_rate': 0.019,\n#         'min_sum_hessian_in_leaf': 0.000446,\n#         'bagging_fraction': 0.81, \n#         'bagging_freq': 5, \n#         'lambda_l1': 4.218,\n#         'lambda_l2': 1.734,\n#         'min_gain_to_split': 0.1501,\n#         'max_depth': 14,\n#         'save_binary': True,\n#         'seed': 42,\n#         'feature_fraction_seed': 42,\n#          'feature_fraction': 0.85,\n#         'bagging_seed': 42,\n#         'drop_seed': 42,\n#         'data_random_seed': 42,\n#         'objective': 'binary',\n#         'boosting_type': 'gbdt',\n#         'verbose': 1,\n#         'metric': 'auc',\n#         'is_unbalance': True,\n#         'boost_from_average': False,\n#     }\n\n# n_fold = 5\n# folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","c43dfaf7":"# for fold_n, (train_index, valid_index) in enumerate(folds.split(x,y)):\n#     print('Fold', fold_n, 'started at', time.ctime())\n    \n#     X_train, X_valid = x.iloc[train_index], x.iloc[valid_index]\n#     y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n#     train_data = lgb.Dataset(X_train, label=y_train)\n#     valid_data = lgb.Dataset(X_valid, label=y_valid)\n    \n#     model = lgb.train(params,train_data,\n#                       num_boost_round = 20000,\n#                       valid_sets = [train_data, valid_data],\n#                       verbose_eval = 300,\n#                       early_stopping_rounds = 200)","be0c4d5c":"# predictions = model.predict(x_test)\n# sub = pd.read_csv('..\/input\/sample_submission.csv')\n# sub['target'] = predictions\n# sub.to_csv('lgb.csv', index=False)","73c75749":"# # model - 2 Catboost\n# train_pool = Pool(X_train, y_train)\n# m = CatBoostClassifier(iterations=300, eval_metric=\"AUC\", boosting_type = 'Ordered', task_type = \"GPU\")\n# m.fit(X_train, y_train, silent=True)\n# score = m.score(X_valid, y_valid)\n# print(score)\n# predictions1 = m.predict(x_test)","fb261f7c":"# sub['target'] = predictions1\n# sub.to_csv('catboost.csv', index=False)","5b0f2728":"# model - 3 Random Foreset - LB score - 0.506 (Not Satisfactory!)\n# X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, stratify=y)\n# model = RandomForestClassifier()\n# cv_scores = cross_val_score(model,X_train,y_train,scoring='accuracy')\n# model.fit(X_train, y_train)\n# score = model.score(X_valid, y_valid)\n# print(score)\n# predictions2 = model.predict(x_test)","b13a5fba":"# sub['target'] = predictions2\n# sub.to_csv('randomforest.csv', index=False)","a7b16c57":"# Resampling the data\nsampler = SMOTE(random_state = 0)\nX_resampled, y_resampled = SMOTE().fit_resample(x, y)","e4926ae4":"# Model - 4: lgb with resampled data\n# for fold_n, (train_index, valid_index) in enumerate(folds.split(X_resampled,y_resampled)):\n#     print('Fold', fold_n, 'started at', time.ctime())\n#     X_train, X_valid = x.iloc[train_index], x.iloc[valid_index]\n#     y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n#     train_data = lgb.Dataset(X_train, label=y_train)\n#     valid_data = lgb.Dataset(X_valid, label=y_valid)\n    \n#     model = lgb.train(params,train_data,\n#                       num_boost_round = 20000,\n#                       valid_sets = [train_data, valid_data],\n#                       verbose_eval = 300,\n#                       early_stopping_rounds = 200)","3d294b2d":"# predictions3 = model.predict(x_test)\n# sub['target'] = predictions3\n# sub.to_csv('lgb1.csv', index=False)","dca10a25":"# # Model - 5 CatBoost with resampled data\n# X_train_re, X_valid_re, y_train_re, y_valid_re = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled)\n# train_pool = Pool(X_train_re, y_train_re)\n# m1 = CatBoostClassifier(iterations=300, eval_metric=\"AUC\", boosting_type = 'Ordered', task_type = \"GPU\")\n# m1.fit(X_train_re, y_train_re, silent=True)\n# score = m1.score(X_valid_re, y_valid_re)\n# print(score)","ea16a2c1":"# # One more test could be done with data by resampling the training data and validating the model on the fresh data.\n# predictions4 = m1.predict(x_test)\n# sub['target'] = predictions4\n# sub.to_csv('catboost1.csv', index=False)","7ffb46e2":"# Model - 6 RandomForest With resampled data - score improves to 0.527 (Better but still not satisfactory)\n# m2 = RandomForestClassifier()\n# cv_scores = cross_val_score(m2,X_train_re,y_train_re,scoring='accuracy')\n# m2.fit(X_train_re, y_train_re)\n# score = m2.score(X_valid_re, y_valid_re)\n# print(score)","f54e7cfe":"# predictions5 = m2.predict(x_test)\n# sub['target'] = predictions5\n# sub.to_csv('randomforest1.csv', index=False)","861df00a":"# Outliers Handling\n# Data Standardization using Z-score Normalization\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py","cfedd799":"# explore the feature importance\nxg_cls = xgb.XGBClassifier(max_depth=50, \n                          min_child_weight=1,  \n                          n_estimators=200,\n                          n_jobs=-1 , \n                          verbose=1,\n                          learning_rate=0.16)\nX_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.1, stratify=y)\nxg_cls.fit(X_train,y_train)","ad2892fd":"print(xg_cls.score(X_valid, y_valid))","cd457d37":"predictions5 = xg_cls.predict(x_test)\nsub['target'] = predictions5\nsub.to_csv('xg_cls.csv', index=False)","bad37b1d":"* Clearly a case of Class imbalance.\n* We can try resampling to increase the amount of **Class - 1**\n\nalternatively, this could also be used to get the distribution:\n\n    train['target'].value_counts(normalize=True)","2ad97bab":"Testing out different approaches\n\n1. Model with the raw imbalanced dataset without handling outliers\n2. Model with different Matrix to avoid imbalanced data effect\n3. Model with raw imbalanced data with outliers handling\n4. Model with resampled data with outliers handling\n5. Model with resampled data without handling outliers","58c4826c":"As we can see none of the variables are correlated with each other.\n\nReference:\n![Positive Correlation](https:\/\/mste.illinois.edu\/courses\/ci330ms\/youtsey\/SCATTER2.GIF)\n![Negative Correlation](https:\/\/mste.illinois.edu\/courses\/ci330ms\/youtsey\/SCATTER1.GIF)","bf4fd651":"* As one can see, some of the columns in the data has a lot of outliers even after randomly choosing the columns.\n* Although there are columns with no outliers at all!\n* There should be some explanation for these outliers as they contains a lot of values.\n\nReference: \n![Outlier from boxplot](https:\/\/www.whatissixsigma.net\/wp-content\/uploads\/2015\/07\/Box-Plot-Diagram-to-identify-Outliers-figure-1.png)","e215b705":"WORK IN PROGRESS","b4b48226":"### Approach:\n    1. EDA\n    2. Statistical analysis of the data, i.e. correlation b\/w variables, outliers, data distribution etc.\n    3. Data Processing, Such that, Data Normalization, handeling outliers, Resampling etc.\n    Repeat:\n        4. Model Definitions, trying out different models is the goal here. More you try to look for options, more options you would have to consider while improving your score\n        5. Train the model\n        6. improvement","e5efd045":"* Data Distribution seem to be normal (except of some cases)."}}