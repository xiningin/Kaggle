{"cell_type":{"b5d6dd40":"code","71a741a7":"code","ab13b069":"code","7da1fca4":"code","a7245e61":"code","3b6ad133":"code","4deb8bef":"code","7940b4ef":"code","c48e6960":"code","96adc435":"code","8979c708":"code","7117a6d1":"code","d2b5e51d":"code","f50186d5":"code","f6b58eb7":"code","18db43a0":"code","e61ac961":"code","3bcc0a93":"code","4d15369c":"code","713cd039":"code","2410e2d4":"code","98679f79":"code","fc6b1437":"code","af6eb56c":"code","374042ab":"code","a4774986":"code","eda2ce48":"code","17991057":"code","ba408385":"code","fbc19471":"code","6f5c636a":"code","ea5719ee":"markdown","2145baab":"markdown","b59176f8":"markdown","e3cd8174":"markdown","865df689":"markdown","6551819f":"markdown","b807934e":"markdown","88d198f1":"markdown","0191d5c3":"markdown"},"source":{"b5d6dd40":"import pandas\nimport numpy as np\nimport cudf as pd\nimport cupy as cp\n\nimport glob\nimport os\nimport gc\nimport time\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.optimize import minimize\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom catboost import Pool, CatBoostRegressor\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport cuml\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import LinearRegression\nfrom cuml import Ridge\nfrom cuml.ensemble import RandomForestRegressor\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\ndef convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","71a741a7":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\ntrain['is_train'] = 1\ntest['is_train'] = 0\n\ntrain = convert_to_32bit(train)\ntest = convert_to_32bit(test)\n\nprint( train.shape )\nprint( test.shape )","ab13b069":"print(train.head(20))","7da1fca4":"print(test.head(20))","a7245e61":"train_stock_ids = train['stock_id'].to_pandas().unique()\ntest_stock_ids = test['stock_id'].to_pandas().unique()\nprint( 'Sizes:', len(train_stock_ids), len(test_stock_ids) )\nprint( 'Train stocks:', train_stock_ids )\nprint( 'Test stocks:', test_stock_ids )","3b6ad133":"def transform(df, groupby='time_id', feat='price', agg='mean' ):\n    return df.merge( \n        df.groupby(groupby)[feat].agg(agg).reset_index().rename({feat:feat+'_'+agg}, axis=1),\n        on=groupby,\n        how='left' \n    )\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    # Calculate Wap\n    df['wap1'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap2'] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    df['wap3'] = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap4'] = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    \n    \n    # Calculate log returns\n    df['log_return1'] = df['wap1'].log()\n    df['log_return1'] = df['log_return1'] - df.groupby(['time_id'])['log_return1'].shift(1).reset_index(drop=True)\n\n    df['log_return2'] = df['wap2'].log()\n    df['log_return2'] = df['log_return2'] - df.groupby(['time_id'])['log_return2'].shift(1).reset_index(drop=True)\n\n    df['log_return3'] = df['wap3'].log()\n    df['log_return3'] = df['log_return3'] - df.groupby(['time_id'])['log_return3'].shift(1).reset_index(drop=True)\n\n    df['log_return4'] = df['wap4'].log()\n    df['log_return4'] = df['log_return4'] - df.groupby(['time_id'])['log_return4'].shift(1).reset_index(drop=True)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    df['log_return1_sqr'] = df['log_return1'] ** 2\n    df['log_return2_sqr'] = df['log_return2'] ** 2\n    df['log_return3_sqr'] = df['log_return3'] ** 2\n    df['log_return4_sqr'] = df['log_return4'] ** 2\n    \n#     df = transform(df, groupby='time_id', feat='wap1', agg='median' )\n#     df = transform(df, groupby='time_id', feat='wap2', agg='median' )\n#     df = transform(df, groupby='time_id', feat='wap3', agg='median' )\n#     df = transform(df, groupby='time_id', feat='wap4', agg='median' )\n#     df = transform(df, groupby='time_id', feat='log_return1', agg='median' )\n#     df = transform(df, groupby='time_id', feat='log_return2', agg='median' )\n#     df = transform(df, groupby='time_id', feat='log_return3', agg='median' )\n#     df = transform(df, groupby='time_id', feat='log_return4', agg='median' )\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': ['sum', 'std'],\n        'wap2': ['sum', 'std'],\n        'wap3': ['sum', 'std'],\n        'wap4': ['sum', 'std'],\n        'log_return1_sqr': ['sum', 'std'],\n        'log_return2_sqr': ['sum', 'std'],\n        'log_return3_sqr': ['sum', 'std'],\n        'log_return4_sqr': ['sum', 'std'],\n        'wap_balance': ['sum', 'mean'],\n        'price_spread':['sum', 'mean'],\n        'price_spread2':['sum', 'mean'],\n        'bid_spread':['sum', 'mean'],\n        'ask_spread':['sum', 'mean'],\n        'total_volume':['sum', 'mean'],\n        'volume_imbalance':['sum', 'mean'],\n        \"bid_ask_spread\":['sum',  'mean'],\n        \n#         \"wap1_median\":['sum',  'mean', 'min','max'],\n#         \"wap2_median\":['sum',  'mean', 'min','max'],\n#         \"wap3_median\":['sum',  'mean', 'min','max'],\n#         \"wap4_median\":['sum',  'mean', 'min','max'],\n#         \"log_return1_median\":['mean',  'std', 'min','max'],\n#         \"log_return2_median\":['mean',  'std', 'min','max'],\n#         \"log_return3_median\":['mean',  'std', 'min','max'],\n#         \"log_return4_median\":['mean',  'std', 'min','max'],\n    }\n    create_feature_dict_time = {\n        'log_return1_sqr': ['sum', 'std'],\n        'log_return2_sqr': ['sum', 'std'],\n        'log_return3_sqr': ['sum', 'std'],\n        'log_return4_sqr': ['sum', 'std'],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    #df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    \n    return df_feature","4deb8bef":"%%time\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df = convert_to_32bit(df)\n    df = df.sort_values('time_id').reset_index(drop=True)\n\n    df['log_return'] = df['price'].log()\n    df['log_return'] = df['log_return'] - df.groupby(['time_id'])['log_return'].shift(1).reset_index(drop=True)\n    df['log_return_sqr'] = df['log_return'] ** 2\n    \n    df['amount']=df['price']*df['size']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return_sqr': ['sum', 'std'],\n        'seconds_in_bucket':['nunique','std', 'mean'],\n        'size':['sum', 'nunique','std'],\n        'order_count':['sum','nunique','std'],\n        'amount':['sum','std'],\n    }\n    create_feature_dict_time = {\n        'log_return_sqr': ['sum', 'std'],\n        'seconds_in_bucket':['nunique'],\n        'size':['sum','mean','std'],\n        'order_count':['sum','mean','std'],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature.columns = [col + '_' + str(seconds_in_bucket) for col in df_feature.columns]\n        return df_feature\n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_250 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 250, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    df = df.sort_values(['time_id','seconds_in_bucket']).reset_index(drop=True)\n    \n    df = transform(df, groupby='time_id', feat='price', agg='mean' )\n    df = transform(df, groupby='time_id', feat='price', agg='sum' )\n    df = transform(df, groupby='time_id', feat='size', agg='mean' )\n    df['price_dif'] = ((df['price'] - df.groupby(['time_id'])['price'].shift(1).reset_index(drop=True)) \/ df['price']).fillna(0.)\n    df['tendencyV'] = df['size'] * df['price_dif']\n    df['f_max'] = 1 * (df['price'] >= df['price_mean'])\n    df['f_min'] = 1 * (df['price'] < df['price_mean'])\n    df['df_max'] = 1 * (df['price_dif'] >= 0)\n    df['df_min'] = 1 * (df['price_dif'] < 0)\n    df['abs_dif'] = (df['price'] - df['price_mean']).abs()\n    df['price_sqr'] = df['price']**2\n    df['size_dif'] = (df['size'] - df['size_mean']).abs()\n    df['size_sqr'] = df['size']**2\n    df['iqr_p25'] = df.groupby(['time_id'])['price'].quantile(0.05).reset_index(drop=True)\n    df['iqr_p75'] = df.groupby(['time_id'])['price'].quantile(0.95).reset_index(drop=True)\n    df['iqr_p_v25'] = df.groupby(['time_id'])['size'].quantile(0.05).reset_index(drop=True)\n    df['iqr_p_v75'] = df.groupby(['time_id'])['size'].quantile(0.95).reset_index(drop=True)\n\n    df = transform(df, groupby='time_id', feat='price_dif', agg='std' )\n    df = transform(df, groupby='time_id', feat='tendencyV', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='f_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_max', agg='std' )\n    df = transform(df, groupby='time_id', feat='df_min', agg='std' )\n    df = transform(df, groupby='time_id', feat='size_dif', agg='std' )\n    \n    dt = df.groupby('time_id')[['tendencyV','price','price_dif','f_max','f_min','df_max','df_min','abs_dif','price_sqr','size_dif','size_sqr','iqr_p25','iqr_p75','iqr_p_v25','iqr_p_v75','price_dif_std','tendencyV_std','f_max_std','f_min_std','df_max_std','df_min_std','size_dif_std']].agg(\n        {\n            'tendencyV':['sum','std','max', 'min'],\n            'price':['mean','std','max', 'min'],\n            'price_dif':['mean','std','max', 'min'],\n            'f_max':['mean','std','max', 'min'],\n            'f_min':['mean','std','max', 'min'],\n            'df_max':['mean','std','max', 'min'],\n            'df_min':['mean','std','max', 'min'],\n            'abs_dif':['median','std','max', 'min'],\n            'price_sqr':['sum','std','max', 'min'],\n            'size_dif':['median','std','max', 'min'],\n            'size_sqr':['sum','std','max', 'min'],\n            'iqr_p25':['mean','std','max', 'min'],\n            'iqr_p75':['mean','std','max', 'min'],\n            'iqr_p_v25':['mean','std','max', 'min'],\n            'iqr_p_v75':['mean','std','max', 'min'],\n            'price_dif_std':['mean','std','max', 'min'],\n            'tendencyV_std':['mean','std','max', 'min'],\n            'f_max_std':['mean','std','max', 'min'],\n            'f_min_std':['mean','std','max', 'min'],\n            'df_max_std':['mean','std','max', 'min'],\n            'df_min_std':['mean','std','max', 'min'],\n            'size_dif_std':['mean','std','max', 'min'],\n        }\n    )\n    dt.columns = [i+'_'+j for i, j in dt.columns] \n    df_feature = df_feature.merge(dt, left_on='time_id_', right_index=True, how='left')\n    \n    # Merge all\n    #df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_250, how = 'left', left_on = 'time_id_', right_on = 'time_id__250')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    \n    # Drop tmp columns\n    df_feature = df_feature.sort_values(['time_id_' ]).reset_index(drop=True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['stock_id'] = str(stock_id) + '-'\n    df_feature['row_id'] = df_feature['stock_id'] + df_feature['time_id_'].astype(str)\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__250', 'time_id__200', 'time_id_','time_id__100','stock_id'], axis = 1, inplace = True)\n\n    fnames = ['trade_' + f for f in df_feature.columns]\n    fnames[-1] = 'row_id'\n    df_feature.columns = fnames\n\n    return df_feature","7940b4ef":"%%time\nDF_TRAIN = []\nfor stock_id in tqdm(train_stock_ids):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'train_parquet\/'+str(stock_id)+'.parquet' )\n    DF_TRAIN.append(df_tmp)\n\n# Concatenate all stock_id in the same dataframe\nDF_TRAIN = pd.concat(DF_TRAIN, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train\/test rows\nDF_TRAIN['is_test'] = 0\nDF_TRAIN.shape","c48e6960":"%%time\nDF_TEST = []\nfor stock_id in tqdm(test_stock_ids):\n    df_tmp = pd.merge( \n        book_preprocessor(data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)),\n        trade_preprocessor(data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)),\n        on = 'row_id',\n        how = 'left'\n    )\n    df_tmp['stock_id'] = stock_id\n    df_tmp = convert_to_32bit(df_tmp) # to save memory\n    #df_tmp.to_parquet( 'test_parquet\/'+str(stock_id)+'.parquet' )\n    DF_TEST.append(df_tmp)\n    \n# Concatenate all stock_id in the same dataframe\nDF_TEST = pd.concat(DF_TEST, ignore_index=True )\n_ = gc.collect()\n\n# Flag to filter train\/test rows\nDF_TEST['is_test'] = 1\nDF_TEST.shape","96adc435":"TRAIN = pd.concat( [DF_TRAIN, DF_TEST] ).sort_values(['stock_id','time_id_']).reset_index(drop=True)\n\ndel DF_TRAIN, DF_TEST\n_ = gc.collect()\nTRAIN.shape","8979c708":"%%time\n\ndef get_time_stock(df_):\n    vol_cols = ['log_return1_sqr_sum_400', 'log_return2_sqr_sum_400', 'log_return3_sqr_sum_400', 'log_return4_sqr_sum_400', 'trade_log_return_sqr_sum', 'trade_log_return_sqr_std', 'trade_seconds_in_bucket_nunique' ]\n\n    df = df_.copy()\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_stock_id.columns = ['_'.join(col) + '_stock' for col in df_stock_id.columns]\n\n    df_time_id = df.groupby(['time_id_'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col)+ '_time' for col in df_time_id.columns]\n    \n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id_'], right_on = ['time_id___time'])\n    df.drop(['stock_id__stock', 'time_id___time'], axis = 1, inplace = True)\n    return df\n\nTRAIN_ = get_time_stock(TRAIN)\nTRAIN_.drop(['stock_id','time_id_'], axis = 1, inplace = True)\nprint(TRAIN_.shape)\nprint(TRAIN_.head())","7117a6d1":"train = train.merge(TRAIN_, on='row_id', how='left' )\ntest  = test.merge(TRAIN_, on='row_id', how='left' )\n\ndel TRAIN_, TRAIN\n_ = gc.collect()\n\ntrain.shape, test.shape","d2b5e51d":"train.head()","f50186d5":"%%time\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns=['stock_id'], values=['target']).fillna(0.)\ncorr = train_p.corr()\n\nkm = cuml.KMeans(n_clusters=6, max_iter=2000, n_init=5).fit(corr)\ndf = pd.DataFrame( {'stock_id': [ f[1] for f in corr.columns ], 'cluster': km.labels_} )\ndf = convert_to_32bit(df)\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n\ndel train_p, df, corr, km\n_ = gc.collect()\n\n# Clusters found\ntrain.groupby('cluster')['time_id'].agg('count')","f6b58eb7":"matTrain = []\nmatTest = []\n\n# 6 clusters\nfor ind in range(train.cluster.max()+1):\n    print(ind)\n    newDf = train.loc[train['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTrain.append ( newDf )\n    \n    newDf = test.loc[test['cluster']==ind].copy()\n    newDf = newDf.groupby(['time_id']).agg('mean')\n    newDf.loc[:,'stock_id'] = 127+ind\n    matTest.append ( newDf )\n    \nmatTrain = pd.concat(matTrain).reset_index()\nmatTrain.drop(columns=['target'],inplace=True)\n\nmatTest = pd.concat(matTest).reset_index()\n\nmatTrain.shape, matTest.shape","18db43a0":"matTest = pd.concat([matTest , matTrain.loc[matTrain.time_id==5]])\nmatTrain = matTrain.pivot(index='time_id', columns='stock_id')\nmatTrain.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTrain.columns]\nmatTrain.reset_index(inplace=True)\n\nmatTest = matTest.pivot(index='time_id', columns='stock_id')\nmatTest.columns = [x[0]+'_stock'+str(int(x[1])) for x in matTest.columns]\nmatTest.reset_index(inplace=True)\n\nmatTrain.shape, matTest.shape","e61ac961":"kfeatures = [\n    'time_id',\n        \n    'wap1_sum_stock127',\n    'wap1_sum_stock128',     \n    'wap1_sum_stock129',\n    'wap1_sum_stock130',     \n    'wap1_sum_stock131',\n    'wap1_sum_stock132',\n        \n    'wap2_sum_stock127',\n    'wap2_sum_stock128',     \n    'wap2_sum_stock129',\n    'wap2_sum_stock130',     \n    'wap2_sum_stock131',\n    'wap2_sum_stock132',\n        \n    'wap3_sum_stock127',\n    'wap3_sum_stock128',     \n    'wap3_sum_stock129',\n    'wap3_sum_stock130',     \n    'wap3_sum_stock131',\n    'wap3_sum_stock132',\n        \n    'wap4_sum_stock127',\n    'wap4_sum_stock128',     \n    'wap4_sum_stock129',\n    'wap4_sum_stock130',     \n    'wap4_sum_stock131',\n    'wap4_sum_stock132',\n    \n    'log_return1_sqr_sum_stock127',\n    'log_return1_sqr_sum_stock128',     \n    'log_return1_sqr_sum_stock129',\n    'log_return1_sqr_sum_stock130',     \n    'log_return1_sqr_sum_stock131',\n    'log_return1_sqr_sum_stock132',\n\n    'log_return2_sqr_sum_stock127',\n    'log_return2_sqr_sum_stock128',     \n    'log_return2_sqr_sum_stock129',\n    'log_return2_sqr_sum_stock130',     \n    'log_return2_sqr_sum_stock131',\n    'log_return2_sqr_sum_stock132',\n\n    'log_return3_sqr_sum_stock127',\n    'log_return3_sqr_sum_stock128',     \n    'log_return3_sqr_sum_stock129',\n    'log_return3_sqr_sum_stock130',     \n    'log_return3_sqr_sum_stock131',\n    'log_return3_sqr_sum_stock132',\n\n    'log_return4_sqr_sum_stock127',\n    'log_return4_sqr_sum_stock128',     \n    'log_return4_sqr_sum_stock129',\n    'log_return4_sqr_sum_stock130',     \n    'log_return4_sqr_sum_stock131',\n    'log_return4_sqr_sum_stock132',\n    \n    'total_volume_sum_stock127',\n    'total_volume_sum_stock128', \n    'total_volume_sum_stock129',\n    'total_volume_sum_stock130', \n    'total_volume_sum_stock131',\n    'total_volume_sum_stock132',\n    \n    'trade_size_sum_stock127',\n    'trade_size_sum_stock128', \n    'trade_size_sum_stock129',\n    'trade_size_sum_stock130', \n    'trade_size_sum_stock131',\n    'trade_size_sum_stock132',\n    \n    'trade_order_count_sum_stock127',\n    'trade_order_count_sum_stock128',\n    'trade_order_count_sum_stock129',\n    'trade_order_count_sum_stock130',\n    'trade_order_count_sum_stock131',      \n    'trade_order_count_sum_stock132',\n    \n    'price_spread_sum_stock127',\n    'price_spread_sum_stock128',\n    'price_spread_sum_stock129',\n    'price_spread_sum_stock130',\n    'price_spread_sum_stock131',   \n    'price_spread_sum_stock132',\n    \n    'bid_spread_sum_stock127',\n    'bid_spread_sum_stock128',\n    'bid_spread_sum_stock129',\n    'bid_spread_sum_stock130',\n    'bid_spread_sum_stock131',\n    'bid_spread_sum_stock132',\n    \n    'ask_spread_sum_stock127',\n    'ask_spread_sum_stock128',\n    'ask_spread_sum_stock129',\n    'ask_spread_sum_stock130',\n    'ask_spread_sum_stock131',   \n    'ask_spread_sum_stock132',\n    \n    'volume_imbalance_sum_stock127',\n    'volume_imbalance_sum_stock128',\n    'volume_imbalance_sum_stock129',\n    'volume_imbalance_sum_stock130',\n    'volume_imbalance_sum_stock131',       \n    'volume_imbalance_sum_stock132',\n    \n    'bid_ask_spread_sum_stock127',\n    'bid_ask_spread_sum_stock128',\n    'bid_ask_spread_sum_stock129',\n    'bid_ask_spread_sum_stock130',\n    'bid_ask_spread_sum_stock131',\n    'bid_ask_spread_sum_stock132',\n]\nmatTrain = convert_to_32bit(matTrain)\nmatTest = convert_to_32bit(matTest)\n\ntrain = pd.merge(train,matTrain[kfeatures],how='left',on='time_id')\ntest = pd.merge(test,matTest[kfeatures],how='left',on='time_id')\n_ = gc.collect()\n\nprint( train.shape, test.shape )","3bcc0a93":"# train=train[~(train[\"stock_id\"]==31)].reset_index(drop=True)\n# _= gc.collect()\n\ntrain = convert_to_32bit(train)\ntest  = convert_to_32bit(test)\n_= gc.collect()\n\ntrain.shape, test.shape","4d15369c":"y_target = train.target.to_pandas() #need to be numpy or pandas for sklearn \ntime_id = train.time_id.to_pandas()\nNFOLD = 5\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Target min and max values\nnp.min(y_target), np.max(y_target)","713cd039":"xgbtime = time.time()\n\n# Define the custom metric to optimize\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    err = rmspe(labels, preds)\n    return 'rmspe', err\n\ndef train_and_evaluate_xgb(train, test, params, colNames):\n    # Sample weight\n    train['target_sqr'] = 1. \/ (train['target'] ** 1.60 + 9e-7)    \n    train.loc[train.stock_id==31,'target_sqr'] = 0.0001\n\n    dtest = xgb.DMatrix(test[colNames])\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        dtrain = xgb.DMatrix(train.loc[train_idx, colNames], train.loc[train_idx, 'target']*10000, weight=train.loc[train_idx, 'target_sqr'])\n        dvalid = xgb.DMatrix(train.loc[valid_idx, colNames], train.loc[valid_idx, 'target']*10000)\n        model = xgb.train(\n            params,\n            dtrain,\n            3000,\n            #[(dtrain, \"train\"), (dvalid, \"valid\")],\n            [(dvalid, \"valid\")],\n            verbose_eval=250,\n            early_stopping_rounds=50,\n            feval=evalerror,\n        )\n        y_train[valid_idx] = np.clip(model.predict(dvalid)\/10000, 2e-4, 0.072)\n        y_test += np.clip((model.predict(dtest))\/10000, 2e-4, 0.072)\n        print( 'Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )\n    y_test \/= NFOLD\n    \n    print( 'XGBoost Rmspe CV:', rmspe(y_target, y_train) )\n    print( pandas.DataFrame.from_dict( model.get_score(), orient='index').sort_values(0, ascending=False).head(20) )\n    print()\n    \n    del model, dtest, dtrain, dvalid\n    _ = gc.collect()\n    \n    return y_train, y_test\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('min')<0 ]\nparams = {\n        \"subsample\": 0.60,\n        \"colsample_bytree\": 0.40,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2021,\n        'single_precision_histogram': False,\n    }\ny_train1a, y_test1a = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ncolNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\ncolNames = [col for col in colNames if col.find('max')<0 ]\nparams = {\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.25,\n        \"max_depth\": 7,\n        \"learning_rate\": 0.02,\n        \"objective\": \"reg:squarederror\",\n        'disable_default_eval_metric': 1, # <- necessary for XGBoost to earlystop by Rmspe and not the default rmse\n        \"nthread\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"max_bin\": 128, \n        'min_child_weight': 0,\n        'reg_lambda': 0.001,\n        'reg_alpha': 0.001, \n        'seed' : 2022,\n        'single_precision_histogram': False,\n    }\ny_train1b, y_test1b = train_and_evaluate_xgb(train, test, params, colNames)\n\n\ny_train1 = 0.5*y_train1a + 0.5*y_train1b\ny_test1  = 0.5*y_test1a  + 0.5*y_test1b\n\nxgbtime = time.time() - xgbtime\n\nprint( 'XGBoost Rmspe CV:', rmspe(y_target, y_train1), 'time: ', int(xgbtime), 's', y_test1[:3] )","2410e2d4":"catbtime = time.time()\n\ndef train_and_evaluate_catb(train, test, params):\n\n    # Sample weight\n    train['target_sqr'] = 1. \/ (train['target'] ** 1.75 + 1e-6)\n    train.loc[train.stock_id==31,'target_sqr'] = 1.\n\n    colNames = [col for col in list(train.columns) if col not in {'is_train', 'time_id', 'target', 'row_id', 'target_sqr', 'is_train'}]\n\n    y_train = np.zeros(len(train))\n    y_test = np.zeros(len(test))\n\n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n\n        model = CatBoostRegressor(\n            iterations=3000,\n            learning_rate=0.05,\n            depth=7,\n            loss_function='RMSE',\n            #l2_leaf_reg = 0.001,\n            #random_strength = 0.5,\n            #bagging_temperature = 1.0,\n            task_type=\"GPU\",\n            random_seed = 2021,\n        )        \n        model.fit(\n            X=train.loc[train_idx, colNames].to_pandas(), y=train.loc[train_idx, 'target'].to_pandas(),\n            sample_weight = train.loc[train_idx, 'target_sqr'].to_pandas(),\n            eval_set = (train.loc[valid_idx, colNames].to_pandas(), train.loc[valid_idx, 'target'].to_pandas(),),\n            early_stopping_rounds = 20,\n            cat_features = [0],\n            verbose=False)\n\n        y_train[valid_idx] = np.clip(model.predict(train.loc[valid_idx, colNames].to_pandas()), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[colNames].to_pandas())), 2e-4, 0.072)\n        print( 'Catboost Rmspe Fold:', rmspe(y_target[valid_idx], y_train[valid_idx]) )        \n        print()\n    y_test \/= NFOLD\n    return y_train, y_test\n\n\ny_train2, y_test2 = train_and_evaluate_catb(train, test, params)\n_= gc.collect()\ncatbtime = time.time() - catbtime\n     \nprint( 'Catboost Rmspe CV:', rmspe(y_target, y_train2), 'time: ', int(catbtime), 's', y_test2[:3]  )","98679f79":"lgbtime = time.time()\n\n# Define the custom metric to optimize\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"target_sqr\", \"row_id\", 'is_train'}]\n    y = train['target']\n    \n    y_train = np.zeros(train.shape[0])\n    y_test = np.zeros(test.shape[0])\n    \n    kf = GroupKFold(n_splits=NFOLD)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(train, y_target, time_id)):\n        print('Fold:', fold)\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_tra, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        train_dataset = lgb.Dataset(x_train[features], y_tra, weight = (1. \/ (np.square(y_tra) + 1e-6)) )\n        valid_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=3000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, valid_dataset], \n                          verbose_eval = 100,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        y_train[val_ind] = np.clip(model.predict(x_val[features]), 2e-4, 0.072)\n        y_test += np.clip((model.predict(test[features])), 2e-4, 0.072)        \n    y_test\/=NFOLD\n    \n    print('LightGBM Rmspe Fold:', rmspe(y_target, y_train))\n    lgb.plot_importance(model,max_num_features=20)\n    \n    return y_train, y_test\n\n\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}\n\ny_train3, y_test3 = train_and_evaluate_lgb(train.to_pandas(), test.to_pandas(), params)\n_= gc.collect()\n\nprint( 'LightGBM Rmspe CV:', rmspe(y_target, y_train3), 'time: ', int(time.time() - lgbtime), 's', y_test3[:3]   )","fc6b1437":"print( 'LightGBM Rmspe:', rmspe(y_target, y_train3) )\nprint( 'XGBoost Rmspe:', rmspe(y_target, y_train1) )\nprint( 'CatBoost Rmspe:', rmspe(y_target, y_train2) )","af6eb56c":"def minimize_arit(W):\n    ypred = W[0] * y_train1 + W[1] * y_train2 + W[2] * y_train3\n    return rmspe(y_target, ypred )\n\nW0 = minimize(minimize_arit, [1.\/3]*3, options={'gtol': 1e-6, 'disp': True}).x\nprint('Weights arit:',W0)","374042ab":"def signed_power(var, p=2):\n    return np.sign(var) * np.abs(var)**p\n\ndef minimize_geom(W):\n    ypred = signed_power(y_train1, W[0]) * signed_power(y_train2, W[1]) * signed_power(y_train3, W[2])\n    return rmspe(y_target, ypred)\n\nW1 = minimize(minimize_geom, [1.\/3]*3, options={'gtol': 1e-6, 'disp': True}).x\n\nprint('weights geom:',W1)","a4774986":"ypred0 = W0[0] * y_train1 + W0[1] * y_train2 + W0[2] * y_train3\nprint( np.min(ypred0), np.max(ypred0))\n\nypred1 = signed_power(y_train1, W1[0]) * signed_power(y_train2, W1[1]) * signed_power(y_train3, W1[2])\nprint( np.min(ypred1) , np.max(ypred1) )\n\nprint( 'Ensemble:', rmspe(y_target, np.clip((ypred0+ypred1)\/2 ,0.0002, 0.071) ) )","eda2ce48":"print( np.min(ypred0),np.mean(ypred0),np.max(ypred0),np.std(ypred0) )\nprint( np.min(ypred1),np.mean(ypred1),np.max(ypred1),np.std(ypred1) )","17991057":"plt.hist(ypred0, bins=100)\nplt.hist(ypred1, bins=100, alpha=0.5)","ba408385":"train['ypred'] = np.clip((ypred0+ypred1)\/2 ,0.0002, 0.071)\ntrain['error'] = (train['target'] - train['ypred']) \/ train['target']\ntrain['error'] = train['error']**2\n\ndt = train.groupby('stock_id')['error'].agg('mean').reset_index()\ndt['error'] = np.sqrt(dt['error'])\ndt = dt.sort_values('error', ascending=False)\ndt.to_csv('error-contribution.csv', index=False)\ndel train['ypred'], train['error']\ndt.head(10)","fbc19471":"dt.tail(10)","6f5c636a":"ypred0 = W0[0] * y_test1 + W0[1] * y_test2 + W0[2] * y_test3\nypred1 = signed_power(y_test1, W1[0]) * signed_power(y_test2, W1[1]) * signed_power(y_test3, W1[2])\n\nypredtest = np.clip((ypred0+ypred1)\/2,0.0002, 0.071)\nprint( ypred0[:3],  ypred1[:3], ypredtest[:3] )\n\ntest['target'] = ypredtest\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)\ntest[['row_id', 'target']].head(3)","ea5719ee":"# XGBoost GPU","2145baab":"# GPU accelerated solution using NVIDIA RAPIDS cudf and cuml\n# Data loading, preprocessing and feature engineering takes less than 3min in GPU.","b59176f8":"# Process all train .parquet files. Create features using cudf (GPU)\n# Note cudf speed to load and apply all feature engineering in all train set stocks.","e3cd8174":"# Loading train and test sets","865df689":"# Checking how many stock_id there are in train and test","6551819f":"# Process all test .parquet files. Create features using cudf (GPU)","b807934e":"# LightGBM GPU","88d198f1":"# Now time to calculate correlation between all stock. The best way is using a correlation matrix, so first pivot all target variables by stock_id, then just calculate the correlation matrix.\n# To Find correlated stocks use Kmeans algorithm on the correlation matrix. This procedure is a bit leak because it not being processed using crossvalidation, but it won't leak much since only 6 clusters are being calculated.","0191d5c3":"# Ensembling Time"}}