{"cell_type":{"ee4aafbd":"code","0d637fd8":"code","9b6323a2":"code","90b7fb8a":"code","13db0183":"code","00718361":"code","2192c08d":"code","75496843":"code","143eca36":"code","488108fa":"code","cf1100ed":"code","b8d15038":"code","5b77262c":"code","f555a498":"code","5820117b":"code","8b15df82":"code","b14cec16":"code","c7e80e7b":"code","c582a5e6":"code","461104b9":"code","7240fdf7":"code","90907cb2":"code","896aae70":"code","3fd67d69":"code","f941a269":"code","a04b87a5":"code","4f755816":"markdown","caf3efa2":"markdown","314f34a3":"markdown","1e798271":"markdown","929eb101":"markdown","60c1040e":"markdown","9db90e42":"markdown","e903a01e":"markdown","383dffb9":"markdown","7160db2d":"markdown","1b2581c3":"markdown","e968101b":"markdown","7b56eaab":"markdown","bc218bb7":"markdown","b4e21033":"markdown","d4ca0673":"markdown","4ca6891c":"markdown","1824b5d6":"markdown","333c1303":"markdown","a9b92536":"markdown","a94a3fc8":"markdown","23761e6e":"markdown","187aaba8":"markdown","e712de22":"markdown","aedb480d":"markdown","70e7bfb9":"markdown","9541f166":"markdown"},"source":{"ee4aafbd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0d637fd8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","9b6323a2":"df = pd.read_csv ('\/kaggle\/input\/dementia-prediction-dataset\/dementia_dataset.csv')\ndf.rename ( columns = { 'Group': 'Dementia'}, inplace=True ) # rename\ndf.rename ( columns = { 'M\/F': 'Sex'}, inplace=True ) # rename\ndf.drop(columns=['Subject ID', 'MRI ID'], inplace=True) # drop\ndf.head(10).style.set_properties(**{'background-color':'black',\n                                     'color': 'orchid'})","90b7fb8a":"print ( \"Let's see the values \u200b\u200bof the `Hand` column:\", df.Hand.unique(), '\\n' )\nprint ( 'Unique value in this column is R. We can drop it.' )\ndf.drop(columns=['Hand'], inplace=True)","13db0183":"# rename columns\ncol = df.columns\nnew_col = []\nfor columns in col:\n  columns_low = columns.title()\n  new_col.append (columns_low)\ndf.columns = new_col","00718361":"df.describe().T.style.background_gradient( cmap='tab10')","2192c08d":"df.info()","75496843":"df.isna().sum()","143eca36":"df.Ses.fillna ( df.Ses.mode() [0], inplace=True ) # impute mode\ndf.Mmse.fillna ( df.Mmse.mean() , inplace=True ) # impute mean\ndf.isna().sum()","488108fa":"custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", rc=custom_params, palette='pastel')\nfig = plt.figure ( figsize= (8,6) )\nax=sns.countplot(data=df, x='Dementia')\nfor i in ax.patches:\n    ax.text(x=i.get_x()+i.get_width()\/2, y=i.get_height()\/7, s=f\"{np.round(i.get_height()\/len(df)*100,0)}%\", ha='center', size=20, weight='bold', rotation=360, color='white')\nplt.title(\"Dementia Feature\", size=20, weight='bold')\nplt.ylabel ( 'Count' )\nplt.show()","cf1100ed":"plt.figure ( figsize= (8,6) )\nsns.countplot ( df.Dementia, data=df, hue='Sex' )\nplt.title(\"Dementia Feature by Sex\", size=20, weight='bold')\nplt.ylabel ( 'Count' )\nplt.xlabel ('', size = 16 )\nplt.show()","b8d15038":"plt.figure ( figsize= (8,6) )\nsns.histplot( data=df, x=\"Age\", binwidth=5, kde=True, hue=\"Dementia\" )\nplt.xlabel ('Age_bins' )\nplt.ylim(0,50)\nplt.show()","5b77262c":"plt.figure ( figsize= (8,6) )\nsns.boxplot( data=df,y=\"Age\", x='Sex', showfliers = False )\nplt.show ()","f555a498":"plt.figure ( figsize= (8,6) )\nsns.countplot ( df.Educ, data=df )\nplt.ylabel ('Count')\nplt.show ()","5820117b":"plt.figure ( figsize= (8,6) )\nsns.boxplot(x=\"Dementia\", y=\"Mmse\", data=df, showfliers = False ) # without outliers\nplt.ylim(0,35)\nplt.show ()","8b15df82":"plt.figure ( figsize= (8,6) )\nsns.scatterplot(data=df, x=\"Cdr\", y=\"Mmse\", hue=\"Dementia\", alpha=0.7)\nplt.show()","b14cec16":"corr = df.corr ()\nplt.figure ( figsize= (8,6) )\nsns.heatmap(corr, annot=True, fmt=\".2f\", linewidths=0.7, cbar = True, cmap='RdBu' )\nplt.title ( 'Matrix of correlation', size = 16)\nplt.show ()","c7e80e7b":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder ()\ndf.Sex = le.fit_transform ( df.Sex.values )\nprint ( 'Sex:\\n0 : %s \\n1 : %s\\n\\n' %(le.classes_[0], le.classes_[1]) )\ndf.Dementia = le.fit_transform ( df.Dementia.values )\nprint ( 'Dementia:\\n0 : %s \\n1 : %s \\n2 : %s' %(le.classes_[0], le.classes_[1], le.classes_[2]) )\n\ndf.Dementia = df.Dementia.astype('category')\ndf.Sex = df.Sex.astype('category')","c582a5e6":"df.info()","461104b9":"from sklearn.model_selection import train_test_split\n\nX, y = df.drop ('Dementia', axis=1).values , df.Dementia.values\nX_train, X_test, y_train, y_test = train_test_split ( X, y,\n                                                     test_size = 0.2,\n                                                     random_state = 1,\n                                                     stratify = y)","7240fdf7":"from imblearn.over_sampling import SMOTE\n\nprint ('Number of observations in the target variable before oversampling of the minority class:', np.bincount (y_train) )\n\nsmt = SMOTE ()\nX_train, y_train = smt.fit_resample (X_train, y_train)\n\nprint ('\\nNumber of observations in the target variable after oversampling of the minority class:', np.bincount (y_train) )","90907cb2":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nX_train_std = std_scaler.fit_transform ( X_train )\nX_test_std = std_scaler.transform ( X_test )","896aae70":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nclf = [ LogisticRegression(random_state=42), DecisionTreeClassifier(random_state=42), SVC (random_state=42),\n       RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42) ]\nmodels = [ 'Logistic Regression', 'Tree', 'Support vector machine', 'RFC', 'Gradient boost' ]\n\nfor clf, model in zip(clf,models):\n  clf.fit ( X_train_std, y_train )\n  y_pred = clf.predict ( X_test_std )\n  print ( f'Cross validation score of {model}: %.3f \\n' %cross_val_score (clf, X_train_std, y_train, cv=5).mean() )","3fd67d69":"from sklearn.model_selection import GridSearchCV\n\nrfc = RandomForestClassifier(n_jobs=-1, random_state=42) \n\nparam_grid = { \n    'n_estimators': [500, 700, 900],\n    'min_samples_split': [2,4,6,8,10]\n}\n\ngs = GridSearchCV ( estimator = rfc,\n                   param_grid = param_grid,\n                   scoring = 'accuracy',\n                   cv = 5,\n                   refit = True,\n                   n_jobs = -1\n                   )\n\ngs = gs.fit ( X_train_std, y_train )\n\nprint ( 'Parameter setting that gave the best results on the hold out data:', gs.best_params_ )\n\nprint ( 'Mean cross-validated score of the best_estimator: %.3f' %gs.best_score_ )\n\ngs = gs.best_estimator_","f941a269":"gs.fit ( X_train_std, y_train )\ny_pred = gs.predict ( X_test_std )\nprint ( f'Accuracy train score: %.4f' %gs.score (X_train_std, y_train) )\nprint ( f'Accuracy test score: %.4f' %accuracy_score ( y_test, y_pred ) )","a04b87a5":"from sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix (  y_test, y_pred )\n\nprint ('Number of records in the test dataset: %d\\n' %y_test.shape[0])\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=False)\n#plot 1\nsns.heatmap(conf_matrix,ax=axes[0],annot=True, cmap='Blues', cbar=False, fmt='d')\naxes[0].set_xlabel('\\nPredicted label', size = 14)\naxes[0].set_ylabel('True label\\n', size = 14)\n\n# plot 2\nsns.heatmap(conf_matrix\/np.sum(conf_matrix),ax=axes[1], annot=True, \n            fmt='.2%', cmap='Blues', cbar=False)\naxes[1].set_xlabel('\\nPredicted label', size = 14)\naxes[1].set_ylabel('True label\\n', size = 14)\naxes[1].yaxis.tick_left()\nplt.show()","4f755816":"<font face='Comic Sans MF' color='orchid' size = 4 ><b>Missing value imputation <\/font>","caf3efa2":"<font face='hacker' size=3>\n\n- `Mr Delay` and `Visit` have a strong positive correlation. You can think of eliminating one of the two variables to reduce noise in the data if you are using a parametric model.\n\n- `Asf` and `Etiv`have a strong negative correlation.","314f34a3":"<font face='hacker' size=3>\n\nThe test dataset contains 75 records. From the confusion matrix it is concluded that:\n\n- All subjects who do not suffer from dementia are correctly classified.\n\n- All bellies suffering from dementia are also correctly classified.\n\n- The model makes more mistakes only in classifying those borderline subjects, that is, those subjects who were not initially classified as demented but who became so during the data collection. In particular, 2.67% of them are labeled as having dementia from the beginning and 4% are classified as having no dementia from the beginning of the survey to the end.","1e798271":"# <font face='Comic Sans MF' color='indianred' size = 6 ><b><center>4. MODELS<\/center><\/font>","929eb101":"<font face='Comic Sans MF' color='indianred' size = 4 ><b>Tuning RFC with cross-validation<\/font>","60c1040e":"<font face='Comic Sans MF' color='gold' size = 4 ><b>Management of categorical data<\/font>","9db90e42":"<font face='Comic Sans MF' color='lightseagreen' size = 4 ><b>Analysis of the other variables <\/font>","e903a01e":"<font face='hacker' size=3>\nLet's focus on some information that provides the description of the dataset:\n\n- `Age` $\\to$ the subjects that make up the dataset have a minimum age of 60 and a maximum of 98.\n\n- `Educ` $\\to$ the average level of education is 14.6 years","383dffb9":"# <font face='Comic Sans MF' color='gold' size = 6 ><b><center>3. PRE-PROCESSING<\/center><\/font>","7160db2d":"<font face='Comic Sans MF' color='gold' size = 4 ><b>Standardization of features<\/font>","1b2581c3":"<font face='hacker' size=3>\n\nFrom the boxplot it is clear that:\n\n- subjects without dementia have `Mmse` values \u200b\u200bof 30 or nearly 30.\n\n- subjects classified as 'Converted' take values \u200b\u200bthat tend to be slightly lower than 30.\n\n- subjects suffering from dementia have generally lower `Mmse` values, on average equal to 26.","e968101b":"# <font face='Comic Sans MF' color='black' size=6><b><center>INTRODUCTION<\/center><\/b><\/font>\n<br>\n\n**PROBLEM END GOAL:**\n- The goal of the problem is to predict the target variable, called 'Dementia'.\n- It has three associated values, so let's treat this a multi-class classification problem.\n\n<br>\n\n**CONTENTS OF THE DATASET:**\n\nThis set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer\u2019s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit","7b56eaab":"# <font face='Comic Sans MF' color='lightseagreen' size = 6 ><b><center>2. EXPLORATORY DATA ANALYSIS<\/center><\/font>","bc218bb7":"<font face='Comic Sans MF' color='gold' size = 4 ><b>Re-sampling train set<\/font>\n\n<font face='hacker' size=3>\n\nBalance the distribution of classes in the target variable with an oversampling of minority classes.","b4e21033":"<font face='hacker' size=3>\n\nAfter hyperparameter tuning, the mean cross-validation score stands at 0,948, up from 0,945. \ud83d\udc4d","d4ca0673":"<br>\n<br>\n<br>\n\n<font face='hacker' size=4><U>I await suggestions, advice and criticisms. Comment below.<\/U><\/font>\n\n<font face='hacker' size = 4 ><mark>If you liked my work, please rate the code and follow my Kaggle profile.<br>Thanks \ud83d\ude0a <\/mark><\/font>\n","4ca6891c":"<font face='Comic Sans MF' color='lightseagreen' size = 4 ><b>Correlation <\/font>","1824b5d6":"# <font face='Comic Sans MF' color='orchid' size = 6 ><b><center>1. DATA MANIPULATION AND DATA CLEASING<\/center><\/font>","333c1303":"<font face='hacker' size=3>\n\nLooking at the scatter plot, it can be seen that for values $<25$ \u200b\u200bof Mmse, the subjects are almost all affected by dementia.\n\n<font face='hacker' size=3>\n\nIn addition, those without dementia all have a `Cdr` value of 0.","a9b92536":"<font face='hacker' size=3>\n\nAccuracy of the trained model on the test data is 0.933. A good result, but now let's see what kind of mistakes he makes when he fails to classify subjects.\n<br>Look at Confusion Matrix. \ud83d\udc40","a94a3fc8":"<font face='hacker' size=3>\n\nThere are missing values \u200b\u200bin the following variables: `Ses` and `Mmse`. Let's solve it now.","23761e6e":"<font face='hacker' size=3>\n\n\nMost of the subjects have an education level of 12, 16 and 18 years.","187aaba8":"<font face='hacker' size=3>\n\nThe age distribution of converted subjects has a higher average than that of non-demented and demented subjects.","e712de22":"<font face='hacker' size=3>\n\nThe classes in the target variable are **unbalanced**, this is a problem to be solved later for a good classification.","aedb480d":"<font face='Comic Sans MF' color='gold' size = 4 ><b>Split in Train and Test set<\/font>","70e7bfb9":"<font face='Comic Sans MF' color='lightseagreen' size = 4 ><b>Analysis of the target variable <\/font>","9541f166":"<font face='Comic Sans MF' color='indianred' size = 4 ><b>Confusion matrix<\/font>"}}