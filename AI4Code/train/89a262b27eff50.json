{"cell_type":{"ab7d0d56":"code","bb8907be":"code","3b2970b6":"code","2a4ada9c":"code","512136f5":"code","62d4200c":"code","bdc46818":"code","d5fb601b":"code","625bd937":"code","ce6f5e66":"code","f839f690":"code","f1936a05":"code","b871c538":"code","64e66c2a":"code","f6849505":"code","bf12f09e":"code","b4761c70":"code","672cb94c":"code","cd955fa6":"code","8a867a30":"code","eaf8347c":"code","b30609cd":"code","eee887f5":"code","e65ec7c8":"code","544cf9de":"code","4a6a641e":"code","54c4fb91":"code","984167c6":"code","1bea98bd":"markdown","59fc134a":"markdown","b53d5a25":"markdown","c4f374d9":"markdown","d6a1b66b":"markdown","b1048ff7":"markdown","c6d13397":"markdown","5ba3feb1":"markdown","cf6977d5":"markdown","28a38825":"markdown","b8717eba":"markdown","af85ef1e":"markdown","d96ad5be":"markdown","83ff479c":"markdown"},"source":{"ab7d0d56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.t","bb8907be":"DATA_ROOT = '..\/input\/'\nGAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap-coreference')\nSUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\nFAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2m')","3b2970b6":"test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\ntrain_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\ndev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\nsub_test_df_path = os.path.join(SUB_DATA_FOLDER, 'test_stage_2.tsv')\n\ntrain_df = pd.read_csv(train_df_path, sep='\\t')\ntest_df = pd.read_csv(test_df_path, sep='\\t')\ndev_df = pd.read_csv(dev_df_path, sep='\\t')\nsub_test_df = pd.read_csv(sub_test_df_path, sep='\\t')\n\n#pd.options.display.max_colwidth = 1000","2a4ada9c":"train_df.head()","512136f5":"from spacy.lang.en import English\nfrom spacy.pipeline import DependencyParser\nimport spacy\nfrom nltk import Tree","62d4200c":"nlp = spacy.load('en_core_web_lg')\n\n# build a tree\ndef to_nltk_tree(node):\n    if node.n_lefts + node.n_rights > 0:\n        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n    else:\n        return node.orth_\n# binary search for a target_\ndef bs(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) \/ 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid + 1\n    return lo\n\n# get k preceding words starting from offest\ndef _get_preceding_words(tokens, offset, k):\n    start = offset - k\n    \n    precedings = [None] * max(0, 0-start)\n    start = max(0, start)\n    precedings += tokens[start: offset]\n    \n    return precedings\n\n# get k following words starting from offest\ndef _get_following_words(tokens, offset, k):\n    end = offset + k\n    \n    followings = [None] * max(0, end - len(tokens))\n    end = min(len(tokens), end)\n    followings += tokens[offset: end]\n    \n    return followings\n        \n\ndef extrac_embed_features_tokens(text, char_offset):\n    doc = nlp(text)\n    \n    # char offset to token offset\n    lens = [token.idx for token in doc] # list if indices of the start of each token in the doc \n    mention_offset = bs(lens, char_offset) - 1\n    # mention_word\n    mention = doc[mention_offset]\n    \n    # token offset to sentence offset\n    lens = [len(sent) for sent in doc.sents]\n    acc_lens = [len_ for len_ in lens]\n    pre_len = 0\n    for i in range(0, len(acc_lens)):\n        pre_len += acc_lens[i]\n        acc_lens[i] = pre_len\n    sent_index = bs(acc_lens, mention_offset)\n    # mention sentence\n    sent = list(doc.sents)[sent_index]\n    \n    # dependency parent\n    head = mention.head\n    \n    # last word and first word\n    first_word, last_word = sent[0], sent[-2]\n    \n    assert mention_offset >= 0\n    \n    # two preceding words and two following words\n    tokens = list(doc)\n    precedings2 = _get_preceding_words(tokens, mention_offset, 2)\n    followings2 = _get_following_words(tokens, mention_offset, 2)\n    \n    # five preceding words and five following words\n    precedings5 = _get_preceding_words(tokens, mention_offset, 5)\n    followings5 = _get_following_words(tokens, mention_offset, 5)\n    \n    # sentence words\n    sent_tokens = [token for token in sent]\n    \n    return mention, head, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens","bdc46818":"print(\"Texts: \")\ntext = u\"Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\"\nprint(text)\n\nprint(\"\\nDependency parsing trees: \")\ndoc = nlp(text)\n[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n\nprint(\"\\nFeatures:\")\nmention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text, 274)\nfeatures = pd.Series([str(feature) for feature in (mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens)], index=['mention', 'parent', 'first_word', 'last_word', 'precedings2', 'followings2', 'precedings5', 'followings5', 'sent_tokens'])\nfeatures","d5fb601b":"num_embed_features = 11\nembed_dim = 300","625bd937":"def create_embedding_features(df, text_column, offset_column):\n    text_offset_list = df[[text_column, offset_column]].values.tolist()\n    num_features = num_embed_features\n    \n    embed_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features, embed_dim))\n    for text_offset_index in range(len(text_offset_list)):\n        text_offset = text_offset_list[text_offset_index]\n        mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text_offset[0], text_offset[1])\n        \n        feature_index = 0\n        embed_feature_matrix[text_offset_index, feature_index, :] = mention.vector\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index, :] = parent.vector\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index, :] = first_word.vector\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index, :] = last_word.vector\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings2])\n        feature_index += len(precedings2)\n        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings2])\n        feature_index += len(followings2)\n        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings5]), axis=0)\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings5]), axis=0)\n        feature_index += 1\n        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector for token in sent_tokens]), axis=0) if len(sent_tokens) > 0 else np.zeros(embed_dim)\n        feature_index += 1\n    \n    return embed_feature_matrix","ce6f5e66":"def bs_(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) \/ 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid\n    return lo\n\n# one hot encoding distance\ndef ohe_dist(dist, buckets):\n    idx = bs_(buckets, dist)\n    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n    oh[idx] = 1\n    \n    return oh","f839f690":"def extrac_positional_features(text, char_offset1, char_offset2):\n    doc = nlp(text)\n    max_len = 64\n    \n    # char offset to token offset\n    lens = [token.idx for token in doc]\n    mention_offset1 = bs(lens, char_offset1) - 1\n    mention_offset2 = bs(lens, char_offset2) - 1\n    \n    # token offset to sentence offset\n    lens = [len(sent) for sent in doc.sents]\n    acc_lens = [len_ for len_ in lens]\n    pre_len = 0\n    for i in range(0, len(acc_lens)):\n        pre_len += acc_lens[i]\n        acc_lens[i] = pre_len\n    sent_index1 = bs(acc_lens, mention_offset1)\n    sent_index2 = bs(acc_lens, mention_offset2)\n    \n    sent1 = list(doc.sents)[sent_index1]\n    sent2 = list(doc.sents)[sent_index2]\n    \n    # buckets\n    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n    \n    # relative distance\n    dist = mention_offset2 - mention_offset1\n    dist_oh = ohe_dist(dist, bucket_dist)\n    \n    # buckets\n    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n    \n    # absolute position in the sentence\n    # position of the first mention from both sides of the sentence\n    sent_pos1 = mention_offset1 + 1\n    if sent_index1 > 0:\n        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n    sent_pos_inv1 = len(sent1) - sent_pos1\n    assert sent_pos_inv1 >= 0\n    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n    \n    # position of the second mention from both sides of the sentence\n    sent_pos2 = mention_offset2 + 1\n    if sent_index2 > 0:\n        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n    sent_pos_inv2 = len(sent2) - sent_pos2\n    if sent_pos_inv2 < 0:\n        print(sent_pos_inv2)\n        print(len(sent2))\n        print(sent_pos2)\n        raise ValueError\n    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n    \n    sent_pos_ratio1 = sent_pos1 \/ len(sent1)\n    sent_pos_ratio2 = sent_pos2 \/ len(sent2)\n    \n    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2","f1936a05":"text = 'He admitted making four trips to China and playing golf there. He also admitted that ZTE officials, whom he says are his golf buddies, hosted and paid for the trips. Jose de Venecia III, son of House Speaker Jose de Venecia Jr, alleged that Abalos offered him US$10 million to withdraw his proposal on the NBN project.'\ndist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text, 256, 208)\nfeatures = pd.Series([str(feature) for feature in (dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2)], index=['dist_oh', 'sent_pos_oh1', 'sent_pos_oh2', 'sent_pos_inv_oh1', 'sent_pos_inv_oh2'])\nfeatures","b871c538":"num_pos_features = 45","64e66c2a":"def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n    num_features = num_pos_features\n    \n    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n    for text_offset_index in range(len(text_offset_list)):\n        text_offset = text_offset_list[text_offset_index]\n        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n        \n        feature_index = 0\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n        feature_index += len(dist_oh)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n        feature_index += len(sent_pos_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n        feature_index += len(sent_pos_oh2)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n        feature_index += len(sent_pos_inv_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n        feature_index += len(sent_pos_inv_oh2)\n    \n    return pos_feature_matrix","f6849505":"p_emb_tra = create_embedding_features(train_df, 'Text', 'Pronoun-offset')\np_emb_dev = create_embedding_features(dev_df, 'Text', 'Pronoun-offset')\np_emb_test = create_embedding_features(test_df, 'Text', 'Pronoun-offset')\np_emb_sub_test = create_embedding_features(sub_test_df, 'Text', 'Pronoun-offset')\n\na_emb_tra = create_embedding_features(train_df, 'Text', 'A-offset')\na_emb_dev = create_embedding_features(dev_df, 'Text', 'A-offset')\na_emb_test = create_embedding_features(test_df, 'Text', 'A-offset')\na_emb_sub_test = create_embedding_features(sub_test_df, 'Text', 'A-offset')\n\nb_emb_tra = create_embedding_features(train_df, 'Text', 'B-offset')\nb_emb_dev = create_embedding_features(dev_df, 'Text', 'B-offset')\nb_emb_test = create_embedding_features(test_df, 'Text', 'B-offset')\nb_emb_sub_test = create_embedding_features(sub_test_df, 'Text', 'B-offset')\n\npa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_sub_test = create_dist_features(sub_test_df, 'Text', 'Pronoun-offset', 'A-offset')\n\npb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_sub_test = create_dist_features(sub_test_df, 'Text', 'Pronoun-offset', 'B-offset')","bf12f09e":"def _row_to_y(row):\n    if row.loc['A-coref']:\n        return 0\n    if row.loc['B-coref']:\n        return 1\n    return 2\n\ny_tra = train_df.apply(_row_to_y, axis=1)\ny_dev = dev_df.apply(_row_to_y, axis=1)\ny_test = test_df.apply(_row_to_y, axis=1)","b4761c70":"X_train = [p_emb_tra, a_emb_tra, b_emb_tra, pa_pos_tra, pb_pos_tra]\nX_dev = [p_emb_dev, a_emb_dev, b_emb_dev, pa_pos_dev, pb_pos_dev]\nX_test = [p_emb_test, a_emb_test, b_emb_test, pa_pos_test, pb_pos_test]\nX_sub_test = [p_emb_sub_test, a_emb_sub_test, b_emb_sub_test, pa_pos_sub_test, pb_pos_sub_test]","672cb94c":"import numpy as np\nfrom keras import backend\nfrom keras import layers\nfrom keras import models","cd955fa6":"def build_mlp_model(\n    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n    model_dim, mlp_dim, mlp_depth=1, drop_out=0.5, return_customized_layers=False):\n    \"\"\"\n    Create A Multi-Layer Perceptron Model.\n    \n    inputs: \n        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n        \n    outputs: \n        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n        \n    :param output_dim: the output dimension size\n    :param model_dim: rrn dimension size\n    :param mlp_dim: the dimension size of fully connected layer\n    :param mlp_depth: the depth of fully connected layers\n    :param drop_out: dropout rate of fully connected layers\n    :param return_customized_layers: boolean, default=False\n        If True, return model and customized object dictionary, otherwise return model only\n    :return: keras model\n    \"\"\"\n    \n    def _mlp_channel1(feature_dropout_layer, feature_map_layer, flatten_layer, x):\n        x = feature_dropout_layer(x)\n        x = feature_map_layer(x)\n        x = flatten_layer(x)\n        return x\n    \n    def _mlp_channel2(feature_map_layer, x):\n        x = feature_map_layer(x)\n        return x\n\n    # inputs\n    inputs1 = list()\n    for fi in range(num_feature_channels1):\n        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n        \n    print('inputs1 ', inputs1)\n    inputs2 = list()\n    for fi in range(num_feature_channels2):\n        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n    \n    # define feature map layers\n    # MLP Layers\n    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n    feature_map_layer1 = layers.TimeDistributed(layers.Dense(model_dim, name=\"feature_map_layer1\", activation=\"relu\"))\n    flatten_layer1 = layers.Flatten(name=\"feature_flatten_layer1\")\n    feature_map_layer2 = layers.Dense(model_dim, name=\"feature_map_layer2\", activation=\"relu\")\n    \n    print('feature_dropout_layer1 ', feature_dropout_layer1)\n    x1 = [_mlp_channel1(feature_dropout_layer1, feature_map_layer1, flatten_layer1, input_) for input_ in inputs1]\n    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n    \n    print('x1+x2 ', x1+x2 )\n    x = layers.Concatenate(axis=1, name=\"concate_layer\")(x1+x2)\n    print('x ', x)\n    # MLP Layers\n    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n        \n    for i in range(mlp_depth - 1):\n        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n\n    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n\n    model = models.Model(inputs1 + inputs2, outputs)\n\n    if return_customized_layers:\n        return model, {}\n\n    return model","8a867a30":"num_feature_channels1 = 3\nnum_feature_channels2 = 2\n\nnum_embed_features = 11\nnum_features1 = num_embed_features\nnum_features2 = num_pos_features\nfeature_dim1 = embed_dim\noutput_dim = 3\nmodel_dim = 10 \nmlp_dim = 60\nmlp_depth=1\ndrop_out=0.5\nreturn_customized_layers=True\n\nmodel, co_mlp = build_mlp_model(\n    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n    model_dim, mlp_dim, mlp_depth, drop_out, return_customized_layers\n)","eaf8347c":"from keras import callbacks as kc\nfrom keras import optimizers as ko\nfrom keras import initializers, regularizers, constraints\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\n\n\nhistories = list()","b30609cd":"print(model.summary())\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')","eee887f5":"from PIL import Image\nimg = Image.open('model.png')\ndisplay(img)","e65ec7c8":"adam = ko.Nadam()\nmodel.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n\nfile_path = \"best_mlp_model.hdf5\"\ncheck_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\nearly_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3)\nhistory = model.fit(X_train, y_tra, batch_size=20, epochs=20, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n\nhistories.append(np.min(np.asarray(history.history['val_loss'])))\n\n# del model, history\ngc.collect()\n","544cf9de":"history.history['val_sparse_categorical_accuracy']","4a6a641e":"loss, score = model.evaluate(x=X_test, y=y_test)\nprint('model evalutation loss = {} and score = {}'.format(loss, score))","54c4fb91":"\ny_sub_preds = model.predict(X_sub_test, batch_size = 1024, verbose = 1)\n\nsub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_2.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(y_sub_preds[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(y_sub_preds[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(y_sub_preds[:, 2])\n\nsub_df.head()\nsub_df","984167c6":"sub_df.to_csv(\"submission.csv\", index=False)","1bea98bd":"Example:","59fc134a":" ##  Position Features","b53d5a25":"This kernel implements 4 DL models for coreference resolution. All the model in this kernel are Non-RNN Based DL models.\n\nFeatures extraction used in this kernel follows Clark and Mannings work: https:\/\/nlp.stanford.edu\/pubs\/clark2016improving.pdf\nIf you are interested in RNN based End2End coreference solution model, please check this kernel: https:\/\/www.kaggle.com\/keyit92\/end2end-coref-resolution-by-attention-rnn.","c4f374d9":"### Parse Text","d6a1b66b":"## Embedding Features","b1048ff7":"# Import Data","c6d13397":"### Generate Training, Validation and Testing Data","5ba3feb1":"### Train Model","cf6977d5":"# Explore Features for Building Mention-Pair Distributed Representation","28a38825":"### Generate Embedding Features","b8717eba":"Encode the absolute positions in the sentence and the relative position between the pronoun and the entities.","af85ef1e":"Follow the idea from the work by Clark and Manning, extract word embedding of head word, dependency parent, first word, last word, two preceding words and two following words of the mention.  Average word embeding of the five preceding words, five following words, all words in the mention, all words in the sentences.","d96ad5be":"# Define DL Models","83ff479c":"## Baseline Model MLP"}}