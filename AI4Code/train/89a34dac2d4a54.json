{"cell_type":{"2fe5af39":"code","45adf4dd":"code","56cae174":"code","3af468e9":"code","30e2fcc9":"code","d8b96910":"code","7fcb222b":"code","cb746d88":"code","2b28ba46":"code","56c2a450":"code","722b2b67":"code","33330288":"code","d1031a40":"code","d8a0ccd3":"code","d3efc29c":"code","e73bc75e":"code","08b5959e":"code","2a0084f0":"code","d7794f4d":"code","496c973e":"code","c7cf4684":"code","f316364b":"code","45bbe03e":"code","b6bed17d":"code","1c54034c":"code","b218b645":"code","d622ad73":"code","5a6b8a28":"code","e37aef4f":"code","535ec970":"code","b671943d":"code","d959d7e2":"code","d011702b":"code","596f7a5e":"code","20fa5618":"code","d9574a7c":"code","6ae211b5":"code","ad7f4e9e":"code","fb8dd2b5":"code","0215e58d":"code","f9447f38":"markdown","2c8b69ef":"markdown","1bcdaa95":"markdown","1cab4767":"markdown","158efe60":"markdown","3df3d535":"markdown","4932b061":"markdown","3d9bf6c3":"markdown","32420f6e":"markdown","e3a45041":"markdown","d9fe9f99":"markdown","9c866203":"markdown","1c760865":"markdown","86f24308":"markdown","f56c3abd":"markdown","3cfbca5d":"markdown","93337528":"markdown","68665881":"markdown","903b4540":"markdown","bfcc60cf":"markdown","ff2a748c":"markdown","2d361eec":"markdown","e30cedda":"markdown","1d6e4ea3":"markdown","76f84e6f":"markdown","a26ed787":"markdown","11a73aa4":"markdown","5adaa48a":"markdown","a1cdb742":"markdown","9d34ca5c":"markdown","4f189403":"markdown","45684363":"markdown"},"source":{"2fe5af39":"# import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn","45adf4dd":"train = pd.read_csv('..\/input\/train.csv')\ntrain.describe()","56cae174":"train.shape","3af468e9":"test = pd.read_csv('..\/input\/test.csv')\ntest.describe()","30e2fcc9":"test.shape","d8b96910":"train.columns[train.dtypes=='object']","7fcb222b":"train['dependency'].unique()","cb746d88":"train.loc[(train['dependency']=='yes')]['SQBdependency'].unique()","2b28ba46":"train.loc[(train['dependency']=='no')]['SQBdependency'].unique()","56c2a450":"train.loc[train['dependency']=='yes', 'dependency'] = 1\ntrain.loc[train['dependency']=='no', 'dependency'] = 0\ntrain['dependency'].unique()","722b2b67":"train['dependency'] = train['dependency'].astype(float)","33330288":"train['edjefe'].unique()","d1031a40":"train.loc[train['edjefe']=='yes', 'edjefe'] = 1\ntrain.loc[train['edjefe']=='no', 'edjefe'] = 0","d8a0ccd3":"train['edjefa'].unique()","d3efc29c":"train.loc[train['edjefa']=='yes', 'edjefa'] = 1\ntrain.loc[train['edjefa']=='no', 'edjefa'] = 0\ntrain['edjefa'].unique()","e73bc75e":"train['edjefe'] = train['edjefe'].astype(int)\ntrain['edjefa'] = train['edjefa'].astype(int)","08b5959e":"train.isnull().sum().sort_values(ascending=False)[0:10]","2a0084f0":"train['rez_esc'].unique()","d7794f4d":"train_null = train.loc[train['rez_esc'].isnull()]\ntrain_non_null = train.loc[train['rez_esc'].notnull()]","496c973e":"sns.distplot(train_null['age'], color='blue')\nsns.distplot(train_non_null['age'], color='red')","c7cf4684":"train['rez_esc'] = train['rez_esc'].fillna(0)","f316364b":"train['v18q1'].unique()","45bbe03e":"train['v18q1'] = train['v18q1'].fillna(0)","b6bed17d":"train['v2a1'] = train['v2a1'].fillna(0)","1c54034c":"meaneduc_null = train.loc[train['meaneduc'].isnull()]\nmeaneduc_null","b218b645":"meaneduc_null[['Id', 'idhogar', 'escolari']]","d622ad73":"meaneduc_null[['Id', 'idhogar', 'escolari', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']]","5a6b8a28":"# for the household with id=1b31fd159\ninstlevel4_one = train.loc[(train['instlevel4']==1) & (train['meaneduc'].notnull())]\nsns.distplot(instlevel4_one['meaneduc'])","e37aef4f":"# find mean\ninstlevel4_one['meaneduc'].mean()","535ec970":"# for the household with id=a874b7ce7\ninstlevel2_one = train.loc[(train['instlevel2']==1) & (train['meaneduc'].notnull())]\ninstlevel3_one = train.loc[(train['instlevel3']==1) & (train['meaneduc'].notnull())]","b671943d":"(instlevel2_one['meaneduc'].mean() + instlevel3_one['meaneduc'].mean())\/2","d959d7e2":"# for the household with id=faaebf71a\ninstlevel7_one = train.loc[(train['instlevel7']==1) & (train['meaneduc'].notnull())]","d011702b":"instlevel7_one['meaneduc'].mean()","596f7a5e":"# replace\ntrain.loc[train['idhogar']=='faaebf71a', 'meaneduc'] = instlevel7_one['meaneduc'].mean()\ntrain.loc[train['idhogar']=='faaebf71a', 'SQBmeaned'] = instlevel7_one['meaneduc'].mean()**2","20fa5618":"# replace\ntrain.loc[train['idhogar']=='1b31fd159', 'meaneduc'] = train.loc[train['idhogar']=='1b31fd159', 'escolari']\ntrain.loc[train['idhogar']=='1b31fd159', 'SQBmeaned'] = train.loc[train['idhogar']=='1b31fd159', 'escolari']**2\ntrain.loc[train['idhogar']=='faaebf71a', 'meaneduc'] = train.loc[train['idhogar']=='faaebf71a', 'escolari']\ntrain.loc[train['idhogar']=='faaebf71a', 'SQBmeaned'] = train.loc[train['idhogar']=='faaebf71a', 'escolari']**2\ntrain.loc[train['idhogar']=='a874b7ce7', 'meaneduc'] = train.loc[train['idhogar']=='a874b7ce7', 'escolari']\ntrain.loc[train['idhogar']=='a874b7ce7', 'SQBmeaned'] = train.loc[train['idhogar']=='a874b7ce7', 'escolari']**2","d9574a7c":"# test = test.drop(['Id', 'idhogar'], axis=1)\ntest.isnull().sum().sort_values(ascending=False)[0:10]","6ae211b5":"test['rez_esc'] = test['rez_esc'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\n\ntest_meaneduc_null = test.loc[test['meaneduc'].isnull()]\ntest_meaneduc_null_ids = test_meaneduc_null['idhogar'].tolist()\nfor idhogar in test_meaneduc_null_ids:\n    test.loc[test['idhogar']==idhogar, 'meaneduc'] = test.loc[test['idhogar']==idhogar, 'escolari']\n    test.loc[test['idhogar']==idhogar, 'SQBmeaned'] = test.loc[test['idhogar']==idhogar, 'escolari']**2\n    # print(test.loc[test['idhogar']==idhogar][['escolari', 'meaneduc', 'SQBmeaned']])","ad7f4e9e":"test.columns[test.dtypes=='object']\ntest.loc[test['dependency']=='yes', 'dependency'] = 1\ntest.loc[test['dependency']=='no', 'dependency'] = 0\ntest.loc[test['edjefe']=='yes', 'edjefe'] = 1\ntest.loc[test['edjefe']=='no', 'edjefe'] = 0\ntest.loc[test['edjefa']=='yes', 'edjefa'] = 1\ntest.loc[test['edjefa']=='no', 'edjefa'] = 0","fb8dd2b5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\n\nclassifiers = [\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n]\n# remove the ids from the dataset\ntrain = train.drop(['idhogar', 'Id'], axis=1)\n# preprocess dataset, split into training and test part\ny = train['Target']\nX = train.drop(columns=['Target'])\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=40)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    print(score)","0215e58d":"# # create the predictions\n# clf = SVC(kernel=\"linear\", C=0.025)\n# clf.fit(X_train, y_train)\n\n# test_ids = test['Id']\n# test = test.drop(['idhogar', 'Id'], axis=1)\n# y_pred = clf.predict(test)\n\n# results = pd.DataFrame(columns=['Id', 'Target'])\n# results['Id'] = test_ids\n# results['Target'] = y_pred\n\n# results.to_csv('submission.csv', index=False)\n","f9447f38":"Five columns appear to have missing values, three of which have a high number.\nFor convenience, this is the meaning of each of the variables:\n* rez_esc, Years behind in school\n* v18q1, number of tablets household owns\n* v2a1, Monthly rent payment\n* meaneduc,average years of education for adults (18+)\n* SQBmeaned, square of the mean years of education of adults (>=18) in the household","2c8b69ef":"Yes, this is very interesting. It seems that people with years behind in school field missing are mainly pupils, people less than 19 years old.\n\nIt is safe then to simply fill those values with zero.","1bcdaa95":"We cross check that mean value of meaneduc is almost equal to the escolari value (9.33303845601356 ~= 10). \n\nWe apply the same logic to the rest two households, by getting the mean of the two instlevels in case we have different levels between the persons.","1cab4767":"As a first attempt, we test different usual classifiers for this kind of data, using the default parameters.","158efe60":"Before modelling, we should apply the same cleaning and filling missing values techniques with the train data.","3df3d535":"## Cleaning test data","4932b061":"There are mainly numerical values, except for the 'yes' and 'no' String values. To deal with it, we can see what values has the 'SQBdependency' field which is the squared dependency.\nFirst, when dependency equals to 'yes' and then to 'no':","3d9bf6c3":"### rez_esc\n\nStarting from the yeard behind in schooling, let's check the values:","32420f6e":"Then comes the 'edjefe' field. Same here, we have 'yes' and 'no' among the numbers.","e3a45041":"It is consistent, when it is 'yes', it equals to 1 and when it is 'no', it equals to 0. It is safe then to assign the respecting values to dependency:","d9fe9f99":"SVC seems to have slightly better performance from the rest of the classifiers.\n\nStill lots of work to be done....","9c866203":"Again, 11.947~=12, so in general is safe to replace nan values with the escolari values. We also fill the SQBmeaned field with the squared value of escolari. ","1c760865":"## Loading and looking into the data","86f24308":"## Modelling (still processing...)\n","f56c3abd":"In this case we have either 4 and 6 on escolari values, which is not far from what we found. Let's check the last household.","3cfbca5d":"As expected we have int values on the rest of the cases. \n\nLet's separate the people that have 'rez_esc' field missing and not missing and check their age distributions.","93337528":"In order to fill those values, we can just use the 'escolari' field, which gives us the years of schooling.","68665881":"However, we are also given the a number of fields denoted as instlevel**x** that show the level of the education in a categorical way.","903b4540":"The number of tablets the household owns seems to have nan instead of zero, as there are only positive values in the data. We fill nan with zero.","bfcc60cf":"## Handling anomalous data","ff2a748c":"Converting the field to float is also useful at this stage.","2d361eec":"As we can see, there is information here about the level of education. \n\nAs an alternative, we can simply get all the records from our dataset that have the respecting instlevel and get the mean of their meaneduc values. Then we can compare this value with the 'escolari' years, in order to make sure our assumption to use the 'escolari' years is correct.\nStarting from the case for the household with id='1b31fd159', we get all the rows with instlevel=4 (this is the level of education of this person) whose meaneduc is not null and we look at the distribution and the mean value below.","e30cedda":"### v2a1\n\nWhen the rent payment is missing, we assume that people own the house. We fill with 0 the missing values.","1d6e4ea3":"Obviously, the 'Id' and 'idhogar' are identifiers of the data. So, let's move to the 'dependency' field and have a look at the unique values:","76f84e6f":"### v18q1","a26ed787":"## Missing data\n\nAt this point, we should check if there are missing values in our data.","11a73aa4":"# Costa Rican Household Poverty Level Prediction \n","5adaa48a":"This is an initial attempt to analyse the Costa Rican Household data. Please feel free to comment and give feedback.","a1cdb742":"First, let's look at the data having mixed types.","9d34ca5c":"### meaneduc\nJust five rows appear to have missing years of education.","4f189403":"We are told though in the data fields descriptions that 'yes' means 1 and 'no' 0 so we assign respectively.","45684363":"Exactly the same happens for the 'edjefa' so we apply the same assignment and we finally convert to int type."}}