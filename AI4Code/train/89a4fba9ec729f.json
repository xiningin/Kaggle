{"cell_type":{"284f713d":"code","eb55f6a7":"code","51b1c1bd":"code","620714d7":"code","40a73d74":"code","91e9df85":"code","acdc8a9b":"code","f7c9e4b7":"code","d0fa7256":"code","ce46bea5":"code","166daa17":"code","3b1c496a":"code","8284b39d":"code","92dc4654":"code","9c871c63":"code","bf0e3dd2":"code","f712d3ea":"code","8222f81e":"code","eb7a4004":"code","0e8458c5":"code","30521a49":"code","81b8d501":"code","8a87cd9d":"code","95eb341d":"code","42a8fe28":"markdown"},"source":{"284f713d":"!pip install textstat\n!pip install language_check","eb55f6a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/overall\/overall\"))\n\n# Any results you write to the current directory are saved as output.","51b1c1bd":"import pandas as pd\nimport numpy as np\nimport os\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport glob\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.model_selection import train_test_split\nimport _pickle as cPickle\nimport textstat\nimport pandas as pd\nimport numpy as np\nimport textstat\nimport glob\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.neural_network import MLPClassifier\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn                        import metrics, svm\nfrom sklearn.svm                    import SVC\nfrom sklearn.neighbors              import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport statistics\nimport string\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem.snowball import SnowballStemmer","620714d7":"df = pd.DataFrame(columns=['text', 'label'])\n\npath=\"..\/input\/overall\/overall\"\n\ni=0\nfile_list = glob.glob(path+\"\/fake\/*.txt\")\nfor file_name in file_list:\n    file=open(file_name,\"r\", encoding=\"utf8\")\n    a=file.read()\n    df.loc[i]=[a,1]\n    i=i+1\n\nfile_list = glob.glob(path+\"\/real\/*.txt\")\nfor file_name in file_list:\n    file=open(file_name,\"r\", encoding=\"utf8\")\n    a=file.read()\n    df.loc[i]=[a,0]\n    i=i+1 ","40a73d74":"df.tail()","91e9df85":"tag_dict={\n    'J':wordnet.ADJ,\n    'V':wordnet.VERB,\n    \"N\": wordnet.NOUN,\n    \"R\": wordnet.ADV\n}\ndef get_pos_tag(word):\n    tag=nltk.pos_tag([word])[0][1][0].upper()\n    return tag_dict.get(tag,wordnet.NOUN)\n\ndef clean_text(text):\n    ## Remove puncuation\n    #text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \"\", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \"\", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \"\", text)\n    text = re.sub(r\"\\.\", \"\", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \"\", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \"\", text)\n\n    ## Stemming\n#     text = text.split()\n#     stemmer = SnowballStemmer('english')\n#     stemmed_words = [stemmer.stem(word) for word in text]\n#     text = \" \".join(stemmed_words)\n    ##Lemmatization\n    sentence_clean=[]\n    for word in text.split():\n        sentence_clean.append(lemmatizer.lemmatize(word,get_pos_tag(word)))\n    return \" \".join(sentence_clean)\n\n\n# apply the above function to df['text']\nlemmatizer = WordNetLemmatizer() \ndf['text'] = df['text'].map(lambda x: clean_text(x))","acdc8a9b":"print(df.loc[1,'text'])\n#tfidf\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(2, 3))\ncounts = count_vectorizer.fit_transform(df['text'].values)\ntfidf = transformer.fit_transform(counts)\n\ntarget=df['label'].values.astype('int')\nselector = SelectKBest(chi2, k=1000)\nselector.fit(tfidf, target)\ntop_words = selector.get_support().nonzero()\n\n# Pick only the most informative columns in the data.\nchi_matrix = tfidf[:,top_words[0]]","f7c9e4b7":"\n# Our list of functions to apply.\ntransform_functions = [\n    \n    lambda x: x.count(\" \")\/len(x.split()),\n    lambda x: x.count(\".\")\/len(x.split()),\n    lambda x: x.count(\"!\")\/len(x.split()),\n    lambda x: x.count(\"?\")\/len(x.split()),\n    lambda x: x.count(\"-\")\/len(x.split()),\n    lambda x: x.count(\",\")\/len(x.split()),\n    lambda x: x.count(\"$\")\/len(x.split()),\n    lambda x: x.count(\"(\")\/len(x.split()),\n    lambda x: len(x) \/ (x.count(\" \") + 1),\n    lambda x: x.count(\" \") \/ (x.count(\".\") + 1),\n    lambda x: len(re.findall(\"\\d\", x)),\n    lambda x: len(re.findall(\"[A-Z]\", x)),\n    lambda x: textstat.flesch_reading_ease(x),\n    lambda x: textstat.smog_index(x),\n    lambda x: textstat.flesch_kincaid_grade(x),\n    lambda x: textstat.coleman_liau_index(x),\n    lambda x: textstat.automated_readability_index(x),\n    lambda x: textstat.dale_chall_readability_score(x),\n    lambda x: textstat.difficult_words(x),\n    lambda x: textstat.linsear_write_formula(x),\n    lambda x: textstat.gunning_fog(x),\n]\n\n# Apply each function and put the results into a list.\ncolumns = []\nfor func in transform_functions:\n    columns.append(df[\"text\"].apply(func))\n","d0fa7256":"    \n# Convert the meta features to a numpy array.\nmeta = np.asarray(columns).T\nfeatures = np.hstack([ meta,chi_matrix.todense()])\ntargets = df['label'].values\nprint('Features-shape: ',features.shape)\nprint('Target-shape: ',target.shape)\n","ce46bea5":"#split in samples\nfrom sklearn.model_selection import train_test_split\nfeatures, features_test, targets, targets_test = train_test_split(features, targets, random_state=0)\nfeatures_test = features_test.astype('int')\ntargets_test = targets_test.astype('int')\nprint(features_test.shape,targets_test.shape)\nprint(features.shape)","166daa17":"## MODEL 1: Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nimport statistics\n#split in samples\nskf = StratifiedKFold(n_splits=5, random_state=0,shuffle=True) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\n\nC_possible = [1e0,1e-1,1e-2,1e-3]\nfor C in C_possible:\n    clf_lr=LogisticRegression(C=C, solver='liblinear')\n    score=[]\n    for train_index, val_index in skf.split(features,targets):\n        X_train, X_test = features[train_index], features[val_index] \n        y_train, y_test = targets[train_index], targets[val_index]\n        clf_lr.fit(X_train,y_train)\n        confidence = clf_lr.score(X_test, y_test)\n        score.append(confidence) \n    print(score)\n    print('C: ',C,' and score: ',statistics.mean(score))\n    print(\"Testing accuracy\")\n    print(clf_lr.score(features_test,targets_test))","3b1c496a":"## MODEL 2: MLP classifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport statistics\n#split in samples\nskf = StratifiedKFold(n_splits=5, random_state=8,shuffle=True) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\n\nalpha_possible = [1e0]\nfor alpha in alpha_possible:\n    clf_mlp=MLPClassifier(alpha=alpha, learning_rate='adaptive',random_state=10, solver='lbfgs')\n    score=[]\n    for train_index, val_index in skf.split(features,targets):\n        X_train, X_test = features[train_index], features[val_index] \n        y_train, y_test = targets[train_index], targets[val_index]\n        clf_mlp.fit(X_train,y_train)\n        confidence = clf_mlp.score(X_test, y_test)\n        score.append(confidence) \n    print(score)\n    print('Alpha: ',alpha,' and score: ',statistics.mean(score))\n    print(\"Testing accuracy\")\n    print(clf_mlp.score(features_test,targets_test))","8284b39d":"## MODEL 3: Decision trees\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport statistics\n#split in samples\nskf = StratifiedKFold(n_splits=5, random_state=2,shuffle=True) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\n\nmin_imp = [1e-5]\nfor imp in min_imp:\n    clf_dt=DecisionTreeClassifier(criterion='gini', splitter='best', min_impurity_decrease=imp)\n    score=[]\n    for train_index, val_index in skf.split(features,targets):\n        X_train, X_test = features[train_index], features[val_index] \n        y_train, y_test = targets[train_index], targets[val_index]\n        clf_dt.fit(X_train,y_train)\n        confidence = clf_dt.score(X_test, y_test)\n        score.append(confidence) \n    print(score)\n    print('Impurity ',imp,' and score: ',statistics.mean(score))\n    print(\"Testing accuracy\")\n    print(clf_dt.score(features_test,targets_test))\n","92dc4654":"## MODEL 4: SVM\nfrom sklearn.model_selection import StratifiedKFold\nimport statistics\n#split in samples\nscore=[]\nskf = StratifiedKFold(n_splits=5, random_state=None) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\nclf_svc=SVC(gamma='auto')\nfor train_index, val_index in skf.split(features,targets):\n    X_train, X_test = features[train_index], features[val_index] \n    y_train, y_test = targets[train_index], targets[val_index]\n    clf_svc.fit(X_train,y_train)\n    confidence = clf_svc.score(X_test, y_test)\n    score.append(confidence)\nprint(score)\nprint(\"Testing accuracy\")\nprint(clf_svc.score(features_test,targets_test))","9c871c63":"## MODEL 5: Random Forest\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport statistics\n#split in samples\nscore=[]\nskf = StratifiedKFold(n_splits=5, random_state=None) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\nclf_rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\nfor train_index, val_index in skf.split(features,targets):\n    X_train, X_test = features[train_index], features[val_index] \n    y_train, y_test = targets[train_index], targets[val_index]\n    clf_rf.fit(X_train,y_train)\n    confidence = clf_rf.score(X_test, y_test)\n    score.append(confidence)\nprint(score)\nprint(\"Testing accuracy\")\nprint(clf_rf)\nprint(clf_rf.score(features_test,targets_test))","bf0e3dd2":"## MODEL 6: KNN\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nimport statistics\n#split in samples\nscore=[]\nskf = StratifiedKFold(n_splits=5, random_state=None) #5 splits; 80-20 splitting, using Stratified K fold\ntargets = targets.astype('int')\nclf_knn = KNeighborsClassifier(n_neighbors=7)\nfor train_index, val_index in skf.split(features,targets):\n    X_train, X_test = features[train_index], features[val_index] \n    y_train, y_test = targets[train_index], targets[val_index]\n    clf_knn.fit(X_train,y_train)\n    confidence = clf_knn.score(X_test, y_test)\n    score.append(confidence)\nprint(score)\nprint(\"Testing accuracy\")\nprint(clf_knn)\nprint(clf_knn.score(features_test,targets_test))","f712d3ea":"classifiers=[clf_lr,clf_mlp,clf_dt,clf_svc,clf_rf,clf_knn]\nlearner_error = dict()\ndef feature_extraction_meta_classifer(feat,tar):\n    global learner_error\n    classifier_outputs=[]\n    print(type(feat))\n    for classifier in classifiers:\n        l=classifier.predict(feat)\n        diff = l - tar\n        print(diff)\n        error_index_list = np.argwhere(diff)\n        error_index_list=np.array([i[0] for i in error_index_list])\n        learner_error[str(classifier)]=error_index_list\n        classifier_outputs.append(l)\n    classifier_outputs=np.array(classifier_outputs).T\n    targets_outputs=tar[:]\n    return (classifier_outputs,targets_outputs)","8222f81e":"#Creating a testing dataset\nclassifier_outputs,targets_outputs=feature_extraction_meta_classifer(features,targets)\nclassifier_outputs_test,targets_outputs_test=feature_extraction_meta_classifer(features_test,targets_test)","eb7a4004":"#print(learner_error)\n# error_lr=set(learner_error[str(clf_knn)])\n# error_mlp=set(learner_error[str(clf_mlp)])\n# intersect=len(error_lr.intersection(error_mlp))\n# total=len(error_lr)\n# print(intersect\/total)","0e8458c5":"# from sklearn.ensemble import AdaBoostClassifier\n# clf_final=AdaBoostClassifier()\n# clf_final.fit(classifier_outputs,targets)\n# print(clf_final.score(classifer_outputs,targets))\n# clf_final.score(classifer_outputs_test,targets_test)","30521a49":"# print(clf_final.estimator_weights_)\n\n# print(clf_final.n_classes_)","81b8d501":"df_classifiers=pd.DataFrame(classifier_outputs)\ndf_classifiers.corr()","8a87cd9d":"#Based on the correlation matrix, consider all columns except column 4\nclassifier_output_trimmed=classifier_outputs[:,[0,1,2,4,5]]\nclassifier_output_test_trimmed=classifier_outputs_test[:,[0,1,2,4,5]]","95eb341d":"from sklearn.neural_network import MLPClassifier\nclf_final=MLPClassifier(alpha=1e1)\nclf_final.fit(classifier_output_trimmed,targets_outputs)\nprint(clf_final.score(classifier_output_trimmed,targets_outputs))\nclf_final.score(classifier_output_test_trimmed,targets_test)","42a8fe28":"ENSEMBLE LEARNING"}}