{"cell_type":{"fdfd4d88":"code","826d0f68":"code","f3062d03":"code","3f980059":"code","40c27c30":"code","5abb5e34":"code","82b06181":"code","d968e2a2":"code","350c749e":"code","5629aa4d":"code","f6cee9f3":"code","eeb1922e":"code","f809bf47":"code","6ae8e08a":"code","0dd0bdb5":"code","84b804fc":"code","ee8e77a5":"code","9044c2b2":"code","e965d9d5":"code","f2f8c340":"code","247d95c5":"code","40f08952":"code","bcce2e95":"code","b1422da3":"code","a1277fa9":"code","d0051687":"code","8f46fac9":"code","1c9d1e61":"code","df00100f":"code","621b6c85":"code","17770f3c":"code","573fbcb6":"code","359a6f40":"code","a0de7844":"code","2afeb85b":"code","fb85c081":"code","d3e2a543":"code","2ee10cc3":"code","ed7cc58a":"code","076f7c30":"code","b76672ac":"code","28483d1e":"code","08803aea":"code","3320f56d":"code","4bd693d3":"code","9b1043f6":"code","bf80001d":"code","be0e0138":"code","f31ef806":"code","d4e799f8":"code","cb5ba34e":"code","f06e79d9":"code","19ba9ec3":"code","77166e0f":"code","09b4ef81":"markdown","49c873ea":"markdown","3d9d14de":"markdown","d6eb02c6":"markdown","b6fa3665":"markdown","6db0a9f6":"markdown","5ffb9e3b":"markdown","a440e064":"markdown","9eca664c":"markdown","afc19794":"markdown","a8cc609c":"markdown","e5b7b3f5":"markdown","bdeffc98":"markdown","de2fc89c":"markdown","f9c4ce73":"markdown","d4b9e290":"markdown","9b2a1bb7":"markdown","41658c10":"markdown","8e57e9ef":"markdown","920054b8":"markdown","3e721dc8":"markdown","30785f10":"markdown","3f575d03":"markdown","6858c70d":"markdown","13bfa6f4":"markdown","1c02c025":"markdown","a46f5488":"markdown","d8b7b58a":"markdown","596c048d":"markdown","f5c34d3b":"markdown","7dbd3abd":"markdown","ca08b27e":"markdown","085985a2":"markdown","ce5dece3":"markdown","306c47d2":"markdown","32e783af":"markdown","45fc0fae":"markdown"},"source":{"fdfd4d88":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")","826d0f68":"Normal_df = pd.read_csv(\"..\/input\/patients-prediction\/Part1 - Normal.csv\")\nType_h_df = pd.read_csv(\"..\/input\/patients-prediction\/Part1 - Type_H.csv\")\nType_s_df = pd.read_csv(\"..\/input\/patients-prediction\/Part1 - Type_S.csv\")","f3062d03":"Normal_df.head()","3f980059":"Type_h_df.head()","40c27c30":"Type_s_df.head()","5abb5e34":"Normal_df.info()","82b06181":"Type_h_df.info()","d968e2a2":"Type_s_df.info()","350c749e":"print(\"Size and shape of each dataframes:\")\nprint(\"Normal.csv : {}\".format(Normal_df.shape))\nprint(\"Type_H.csv : {}\".format(Type_h_df.shape))\nprint(\"Type_S.csv : {}\".format(Type_s_df.shape))","5629aa4d":"df=pd.concat([Normal_df, Type_h_df,Type_s_df], axis=0)","f6cee9f3":"df.head()","eeb1922e":"df.shape","f809bf47":"df.info()","6ae8e08a":"df['Class'].value_counts()","0dd0bdb5":"cl_type={'Nrmal':'Normal','type_h':'Type_H','tp_s':'Type_S','Normal':'Normal','Type_S':'Type_S','Type_H':'Type_H'}\ndf['Class']=df['Class'].apply(lambda x: cl_type[x])","84b804fc":"df['Class'].value_counts()","ee8e77a5":"df.isnull().sum()","9044c2b2":"# Import requried plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e965d9d5":"df.describe().T.style.bar(['mean','std'], color='Green').background_gradient(cmap='PuBu', subset=['50%'])","f2f8c340":"df.skew()","247d95c5":"df.median()","40f08952":"df['Class'].value_counts()","bcce2e95":"sns.countplot(df['Class'])","b1422da3":"num_feat=df.select_dtypes(exclude='object').columns.to_list()\n\nfig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,8))\nax=ax.flatten()\nj=0\nfor i in num_feat:\n    sns.kdeplot(df[i], ax=ax[j])\n    ax[j].axvline(df[i].mean(), ls='--',c='black')\n    ax[j].axvline(df[i].mean()+df[i].std(), ls='--',c='y')\n    ax[j].axvline(df[i].mean()-df[i].std(), ls='--',c='y')\n    ax[j].axvline(df[i].median(), ls='--',c='r')\n    j=j+1","a1277fa9":"fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,8))\nax=ax.flatten()\nj=0\nfor i in num_feat:\n    sns.boxplot(df[i], ax=ax[j])\n    ax[j].axvline(df[i].mean(), ls='--',c='black')\n    ax[j].axvline(df[i].median(), ls='--',c='r')\n    j=j+1","d0051687":"#Lets us check the values of the outliers\ndf['S_Degree'].max()","8f46fac9":"df['S_Degree'].min()","1c9d1e61":"df[df['S_Degree']>100]","df00100f":"def clean_outliers(df1, features):\n    for i in features:\n        Q1=df1[i].quantile(0.25)\n        Q2=df1[i].quantile(0.75)\n        IQR= (Q2-Q1)\n        print(\"Feature {} has min value: {} max value: {}\".format(i, Q1-IQR*1.5,Q2+IQR*1.5))\n        df_c=df1[((df1[i]>(Q1-IQR*1.5))&(df1[i]<(Q2+IQR*1.5)))]\n        df1=df_c\n    return df1","621b6c85":"#df_clean=outlier_remove_zscore(df,num_feat)\n#removing ouliers only on S_Degree feature as it has more outliers compared to other features\ndf_clean=clean_outliers(df, ['S_Degree'])\n\n\n\nplt.boxplot(df_clean[num_feat], whis=1.5);","17770f3c":"df_clean.describe().T.style.bar(['mean','std'], color='Green').background_gradient(cmap='PuBu', subset=['50%'])","573fbcb6":"df_clean[num_feat].plot.kde()","359a6f40":"sns.countplot(df_clean['Class'])","a0de7844":"fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,8))\nax=ax.flatten()\nj=0\nfor i in num_feat:\n    sns.boxplot(x=df_clean[i], ax=ax[j], y=df_clean['Class'])\n    j=j+1","2afeb85b":"from scipy.stats import norm\nfig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,8))\nax=ax.flatten()\nj=0\nfor i in num_feat:\n    sns.distplot(df_clean[i], ax=ax[j])\n    ax[j].axvline(df[i].mean(), ls='--',c='black')\n    j=j+1","fb85c081":"fig=plt.figure(figsize=(15,8))\nsns.pairplot(df_clean, hue='Class')","d3e2a543":"sns.heatmap(df_clean.corr(), annot=True)","2ee10cc3":"sns.jointplot(data=df, x='P_incidence',y='S_Degree', hue='Class')\nplt.ylim(0,200)","ed7cc58a":"cols=df_clean.columns.to_list()\ncols.remove('Class')\nfrom scipy.stats import f_oneway\n\nfor i in cols:\n    df_normal=df_clean[df_clean['Class']=='Normal'][i]\n    df_typeh = df_clean[df_clean['Class']=='Type_H'][i]\n    df_types =df_clean[df_clean['Class']=='Type_S'][i]\n    print(\"\\nH0: Class has no impact on the feature {} \".format(i))\n    print(\"H1: Class has impact on the feature {} \\n\".format(i))\n    st,p_value=f_oneway(df_normal,df_typeh,df_types)\n    \n    if p_value<0.05:\n        print(\"Reject Null Hypothesis: feature {} has impact on Class\".format(i))\n    else:\n        print(\"Accept Null Hypothesis: feature {} has no impact on Class \\n\".format(i))\n        \n    \n","076f7c30":"#Train Test Split\nX=df_clean.drop(['Class'], axis=1)\ny=df_clean['Class']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)","b76672ac":"y_test.value_counts()","28483d1e":"y_train.value_counts()","08803aea":"# Nomalization\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nscale=StandardScaler()\nX_train=scale.fit_transform(X_train)\nX_test =scale.transform(X_test)\n\nencode=LabelEncoder()\ny_train=encode.fit_transform(y_train)\ny_test=encode.transform(y_test)","3320f56d":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train,y_train)\nprint(\"Training data accuracy: {}\".format(model.score(X_train,y_train)))\npred=model.predict(X_test)\nprint(\"Test data accuracy: {}\".format(model.score(X_test,y_test)))","4bd693d3":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test,pred)))","9b1043f6":"print(classification_report(y_test,pred))","bf80001d":"encode.classes_","be0e0138":"sns.heatmap(confusion_matrix(y_test,pred), annot=True)","f31ef806":"KNeighborsClassifier().get_params()","d4e799f8":"acc_score=[]\nn_value = pd.DataFrame(columns=['K_value','acc_score','MSE','weight','p'])\nk_val=[]\nw=[]\np_val=[]\nweights=['uniform','distance']\np=[1,2]\n\nfor k in range(1,20):\n    for j in weights:\n        for i in p:\n            model = KNeighborsClassifier(n_neighbors=k, weights=j, p=i)\n            model.fit(X_train,y_train)\n            pred=model.predict(X_test)\n            acc_score.append(accuracy_score(y_test,pred))\n            k_val.append(k)\n            w.append(j)\n            p_val.append(i)\nn_value['K_value']=k_val\nn_value['acc_score']=acc_score\nn_value['MSE']=1-n_value['acc_score']  \nn_value['weight']=w\nn_value['p']=p_val\n    ","cb5ba34e":"fig, ax=plt.subplots(nrows=2, ncols=2, figsize=(15,8))\nax=ax.flatten()\nsns.lineplot(data=n_value, x='K_value',y='acc_score', ax=ax[0], hue='weight')\nsns.lineplot(data=n_value, x='K_value',y='acc_score', ax=ax[1], hue='p')\nsns.lineplot(data=n_value, x='K_value',y='MSE', ax=ax[2], hue='weight')\nsns.lineplot(data=n_value, x='K_value',y='MSE', ax=ax[3], hue='p')","f06e79d9":"k_value=n_value[n_value['acc_score']==n_value['acc_score'].max()]\nminimu_MSE=n_value[n_value['MSE']==n_value['MSE'].min()]","19ba9ec3":"print(\"Optimal K value for KNN\")\nprint(k_value)\n","77166e0f":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=5, weights='uniform', p=1)\nmodel.fit(X_train,y_train)\nprint(\"Training data accuracy: {}\".format(model.score(X_train,y_train)))\npred=model.predict(X_test)\nprint(\"Test data accuracy: {}\".format(model.score(X_test,y_test)))\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test,pred)))\nprint(classification_report(y_test,pred))\nsns.heatmap(confusion_matrix(y_test,pred), annot=True)","09b4ef81":"Both in accuracy score & MSE the Uniform weight with manhattan_distance (p=1) perfroms better. however, lets us check the details further","49c873ea":"# Model training, testing and tuning: ","3d9d14de":"**Observations**\n1. P_incidence - mean and median is around 60, so the data must be Normally distributed with clustring around less variance.\n2. P_title - values must be normally distribured with less standar deviation\n3. L_angle - Similar to P_incidence, Data is assumed to be normally distributed.\n4. S_slope - Mean & Median are more or less same, so data should be normally distributed\n5. P_radius - Similar to P_incidence\n6. S_degree - Std.Dev is above mean, so there must be larger distribution of data. and kind of right skewed as Median is lesser thant mean","d6eb02c6":"### Fine tune\/Hyper tune the model for better accuracy\nLet us try to provide different P value, n_neighbours & weights parameters to check which is performing better.","b6fa3665":"***It is clear that all feaures has relationship with all 3 classes***","6db0a9f6":"## Perform a detailed univariate, bivariate and multivariate analysis with appropriate detailed comments after each analysis.  ","5ffb9e3b":"***Few Features has linear relationship and we can see the clusters of classes. P_incidence & S_Degree are highly correlated. we can use either one for analysis. however, Hypothesis testing would explain in more details***","a440e064":"**Observation**  \n***precision:***\n1. 86% of Nomal are predicted correctly from overall positives\n2. 73% of Type_H are predicted correctly from overall positives\n3. 95% of Type_S are predicted correctly from overall positives  \n***Recall:***\n1. 86% of total values are predicted as Normal correctly.\n2. 69% of total values are predicted as Type_H\n3. 97% of total values are predicted as Type_S\n\nsince it is Medical prediction data, it is better to consider Recall than the precision to make sure to avoide False Negative.\nthe variance in Type_H predictions is because of the imablance in the data.","9eca664c":"### Hypothesis testing","afc19794":"### check the dataset type and shape","a8cc609c":"# 1. Import and warehouse data: ","e5b7b3f5":"### Univariated Data analysis","bdeffc98":"***There are outliers in the features which need to be treated***\n1. as discussed in the above plot, S_Degree is the right skewed and outliers are toward the right. \n2. P_radius feature has outlier on the both side.\n3. P_incidence feaure has less number of outliers towards right.\n4. P_tilt - sampe as P_inscidence, less outlers on both ends.","de2fc89c":"# 2. Data cleansing:","f9c4ce73":"***There is no null value in the dataframe. so droping or impute may not be required***","d4b9e290":"# 4. Data pre-processing:","9b2a1bb7":"***It is clear that the K_value 5 with Uniform Weights & manhattan_distance algorithm is giving optimal accuracy score from 20 Neighbours. let us check the perfromance metrics with above said parameters***","41658c10":"***Type_S has higher number of data and followed by Normal & Type_H counts. Lets explore further in EDA***","8e57e9ef":"## Merge all 3 dataframes into single dataframe","920054b8":"***It is clear that that there are 3 different classes in 3 csv files. all files consistes of similar features and datatypes, so we can go ahead and combine the dataframes for further analysis***","3e721dc8":"***As mentioned in above description, the data is imbalance towards the number of classes.***","30785f10":"## Explore for null values in the attributes and if required drop or impute values.","3f575d03":"***Except S_Degree feature rest other features can be assumed as normally distributed, as the mean and median are more or less the same***","6858c70d":"## Read Dataset\nThere are 3 csv files. each containg dataset for each classes. we need to combaine all three files for analysis and model creation\n1. Part1 - Normal.csv\n2. Part1 - Type_H.csv\n3. Part1 - Type_s.csv","13bfa6f4":"##  Explore and if required correct the datatypes of each attribute ","1c02c025":"***Observations***\n1. out of 35 observations, 30 entries it is predected as Normal (True Positive) and 4 & 1 times as Type_H & Type_S. which should be lowered.\n2. out of 16 observation in Type_H data, 11 times entries correctly as Type_H (True Positive), where in 4 & 1 time as Normal & Type_S. let us try to minimise this error.\n3. out of 39 observations in Type_S data, 38 entries are predicted correctly as Type_S and 1 time as Normal. Cosidereably good. let us find if we can improvise the model","a46f5488":"***after renaming the class types, we get the numbers which is similar to the number of records in each files with respect to class type***","d8b7b58a":"***All independent features are float datatype and Target variable is object.***","596c048d":"# 3. Data analysis & visualisation: ","f5c34d3b":"***Classes are mentioned in different format or type or typo in the class fields, Lets us consider Normal, Type_H & Type_S as 3 different classes and rename the errors accordingly***","7dbd3abd":"***After merging the files we get the cumulative numbers of rows and columns from all 3 files***","ca08b27e":"***Data points are mostly clustered. Feature S_Degree can provide more information about Type_S class as it can seperate from other classes.***","085985a2":"### Bivariated Analysis & Multivariated Analysis","ce5dece3":"### Observations:\nK_neighbour with 5 neighbours is optimal for the requried output  \n1. we can see some good improvements in recall score for Type_H (81%) and the Normal prediction score is slightly lowered. which is good in terms for Medical predictions. where False Positive is accepted than the False Negative.\n2. out of 35 records in Normal, we predicted 30 correcly and 4 & 1 as Type_H & Type_S. which is fine.\n3. out of 16 records in Type_H, our model has predicted 13 records as Type_h and only 3 records as Normal.(it was 4+1 in False Negative).\n4. out of 39 records in Type_S, our model can predict all 39 as Type_S. \n\n***Imbalance in the record for the each class is also the reason for lower predicted rate in Type_H. however, we can improvise the model in the best was possible***\n","306c47d2":"## Create KNN model","32e783af":"#### Plots after outlier removal","45fc0fae":"# <center> Predict Patients Condition  <\/center>\n\n**Objective:**\nDemonstrate the ability to fetch, process and leverage data to generate useful predictions by training Supervised Learning algorithms.   \nObjective of the project is to classify the dataset with Normal, Type_H & Type_S classess. since the dataset is based on Medical details, object of the model to minimise the 'False Negative Rate'."}}