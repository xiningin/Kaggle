{"cell_type":{"1c9a1d0f":"code","fe5056af":"code","c3cb09b1":"code","f20f75ab":"code","37a3044a":"code","91908e7b":"code","37497b8b":"code","990feb82":"code","84ada5d5":"code","6012939c":"code","8fee90ca":"code","f9037209":"code","1a523b00":"code","b59c6282":"code","bd2b5df1":"code","380da476":"code","9d429f17":"code","61435fc0":"code","e8c1d648":"code","8df25971":"code","d7c18306":"code","fc251521":"code","c53bc06b":"code","fc5a9fe7":"code","67dc7682":"code","3b98306a":"code","659b95de":"code","8949cfd2":"code","e0b5b542":"code","ec32c839":"code","7e291e8a":"code","72a0d417":"code","c8cf2070":"code","f6d0eedd":"code","f61a335f":"code","3d3a2fae":"code","7fab7a28":"code","dae8030f":"code","2bb3d987":"code","3a999d99":"code","0cc24cdd":"code","65fa7c14":"code","831595a5":"code","246bd88f":"code","76da7549":"code","7cf85b65":"code","01d4cae2":"code","dc89bd4a":"code","c30369fb":"code","f639fae9":"code","1ab8ce52":"code","13b725b1":"code","44c6a0cc":"code","20f540fb":"code","ce83e13e":"code","395b5ef9":"code","9cca54fd":"code","c62668ee":"code","9cc7f6f5":"code","76e98084":"code","7d91981a":"code","a0e947c1":"code","8321ac06":"code","27c2d6f8":"code","bc0b1943":"code","61706704":"code","ef1bffa0":"code","5f5c77ec":"code","4439a28a":"code","1f2a79cb":"code","4f4a443c":"code","9691f826":"code","8f9a2a3b":"code","5cf0b8a1":"code","d1a5e9ce":"code","b47c035f":"code","aae57fe1":"code","af5b0830":"code","f11c89b1":"code","315bc308":"code","163667e7":"code","75199483":"code","2a77449c":"markdown","2299a2f9":"markdown","f737a4b1":"markdown","d5dbdc71":"markdown","47a549f4":"markdown","46de33b3":"markdown","f8d90196":"markdown","277b2749":"markdown","84a5cb27":"markdown","b2ebe4c2":"markdown","bcb2d4f1":"markdown","91b3d514":"markdown","93875f5f":"markdown","ac97ca85":"markdown","4a2930f0":"markdown","2602dd7f":"markdown","6a8c5771":"markdown","133881a3":"markdown","65a81e52":"markdown","d138a806":"markdown","3ecf1745":"markdown","0aeabe34":"markdown","b3d018ac":"markdown","5b8dc806":"markdown","597fb900":"markdown","fe42afd1":"markdown","2110f5db":"markdown","667efd6d":"markdown","f29f6431":"markdown","88d9bc9f":"markdown","2312efe4":"markdown","3ee28394":"markdown","e0451f55":"markdown","07a23593":"markdown"},"source":{"1c9a1d0f":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set(\n    style=\"whitegrid\",\n    font_scale=2\n);","fe5056af":"DATA_FOLDER = Path('..\/input\/data-for-activity-recognition\/data\/data')\nFRAME_LENGTH = 30\nclasses = [f.name for f in DATA_FOLDER.iterdir()]\nclasses","c3cb09b1":"x_acc_cols = [f'acc_x_{i}' for i in range(FRAME_LENGTH)]\ny_acc_cols = [f'acc_y_{i}' for i in range(FRAME_LENGTH)]\nz_acc_cols = [f'acc_z_{i}' for i in range(FRAME_LENGTH)]\n\nframes = []\nlabels = []\n\nfor class_name in classes:\n    class_folder = DATA_FOLDER \/ class_name\n    for file in class_folder.iterdir():\n        df = pd.read_csv(file)\n        flat_frame = np.hstack([df['accelerometer_X'], df['accelerometer_Y'], df['accelerometer_Z']]).astype(float)\n        frames.append(flat_frame)\n        labels.append(class_name)\n        \nraw_data = pd.DataFrame(frames, columns = x_acc_cols+y_acc_cols+z_acc_cols)\nraw_data = pd.concat([raw_data, pd.Series(labels, name='label')], axis=1)\nraw_data.head(3)","f20f75ab":"raw_data.shape","37a3044a":"raw_data.label.value_counts()","91908e7b":"fig, axes = plt.subplots(1, 3, figsize=(16,6))\naxes = axes.flatten()\n\nfor label, cols, ax in zip(['X', 'Y', 'Z'], [x_acc_cols, y_acc_cols, z_acc_cols], axes):\n    sns.distplot(raw_data[cols].values.flatten(), ax=ax)\n    ax.set_title(f'Distr. of {label} axis readings');\n    ax.set_xlabel('m\/$s^2$');\n    ax.set_ylabel('')","37497b8b":"from sklearn.manifold import TSNE","990feb82":"tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(raw_data[x_acc_cols+y_acc_cols+z_acc_cols])","84ada5d5":"df_subset = pd.DataFrame({\n    'tsne-2d-0': tsne_results[:,0],\n    'tsne-2d-1': tsne_results[:,1],\n    'label': raw_data['label'],\n})\n\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-0\", y=\"tsne-2d-1\",\n    hue=\"label\",\n    palette=sns.color_palette(\"hls\", 4),\n    data=df_subset,\n    legend=\"full\",\n    alpha=0.3\n);","6012939c":"fig, axes = plt.subplots(2, 2, figsize=(20, 20))\naxes = axes.flatten()\n\n\nsteps = np.arange(30)\n\nfor ax, label in zip(axes, classes):\n    sample = raw_data[raw_data['label'] == label].iloc[0]\n    sns.lineplot(y=sample[x_acc_cols].astype(float), x=steps, label='X', ax=ax)\n    sns.lineplot(y=sample[y_acc_cols].astype(float), x=steps, label='Y', ax=ax)\n    sns.lineplot(y=sample[z_acc_cols].astype(float), x=steps, label='Z', ax=ax).set_title(label)\n    ax.set_ylim(-40, 40)\n    ax.set_xlabel('time step')\n    ax.set_ylabel('m\/$s^2$')","8fee90ca":"mean_x = raw_data[x_acc_cols].mean(axis=1)\nmean_y = raw_data[y_acc_cols].mean(axis=1)\nmean_z = raw_data[z_acc_cols].mean(axis=1)\n\nsimple_features = pd.DataFrame({\n    'mean_x': mean_x,\n    'mean_y': mean_y,\n    'mean_z': mean_z,\n    'label': raw_data['label']\n})","f9037209":"print('Pairplot of mean accelerometer value during frame of length 30');\nsns.pairplot(data=simple_features, hue='label', height=5);\n","1a523b00":"from sklearn.model_selection import StratifiedKFold\n\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)","b59c6282":"X = raw_data['acc_x_0']\ny = raw_data['label']\n\nfor i, (_, valid_ix) in enumerate(kfold.split(X, y)):\n    y_valid = y.loc[valid_ix]\n    value_counts = y_valid.value_counts()\n    print(f'split#{i}, we have {value_counts.stairs} \"stairs\", {value_counts.running} \"running\" samples')\n#     print(    (y_valid == 'stairs').index)","bd2b5df1":"from sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# target label encoder\ntarget_le = LabelEncoder()\ntarget_le.fit(classes)","380da476":"from sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.metrics import f1_score, confusion_matrix, multilabel_confusion_matrix, accuracy_score\nfrom IPython.display import display, HTML","9d429f17":"numerical_metrics = ['accuracy', 'f1_macro', 'f1_min']\n\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Calculates metrics and returns result as dict\"\"\"\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n        'f1_min': f1_score(y_true, y_pred, average=None).min(),\n        'CM': confusion_matrix(y_true, y_pred, normalize='true'),\n        'CM_not_normalized': confusion_matrix(y_true, y_pred),\n    }\n    \n    return metrics\n\ndef plot_confusion_matrix(df, title=None):\n    # C_ij - i is true, predicted in j\n    cm = np.stack(df['CM'].values, axis=0).mean(axis=0)\n    cm_nn = np.stack(df['CM_not_normalized'].values, axis=0).mean(axis=0)\n    cm_print = np.empty_like(cm_nn).astype(str)\n    for i in range(len(cm_print)):\n        for j in range(len(cm_print)):\n            cm_print[i,j] = (\"%.2f\" % (cm[i,j]*100))+'%\\n'+str(cm_nn[i,j])\n    plt.figure(figsize=(6,6))\n    sns.set(font_scale=1.3)\n    labels_ordered = target_le.inverse_transform(range(4))\n    sns.heatmap(cm,\n                annot=cm_print,\n                fmt='',\n                cmap='Blues',\n                cbar=False,\n                xticklabels=labels_ordered,\n                yticklabels=labels_ordered,\n               )\n    plt.xlabel('predicted')\n    plt.ylabel('actual');\n    plt.title(title);\n    \ndef print_cv_metrics(cv_metrics, title=None):\n    \"\"\"Displays results on cross validation\"\"\"\n    df = pd.DataFrame(cv_metrics)\n    plot_confusion_matrix(df, title=title);\n    plt.show()\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df = print_df.applymap(lambda x: round(x, 3))\n    print_df.columns = ['mean', 'std']\n    display(HTML(print_df.T.to_html()))    \n    ","61435fc0":"X = simple_features[['mean_x', 'mean_y', 'mean_z']]\ny = simple_features['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Baseline logistic regression')","e8c1d648":"from sklearn.ensemble import RandomForestClassifier","8df25971":"X = simple_features[['mean_x', 'mean_y', 'mean_z']]\ny = simple_features['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n#     scaler = StandardScaler()\n#     X_train = scaler.fit_transform(X_train)\n#     X_valid = scaler.transform(X_valid)\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Baseline Random Forest')","d7c18306":"def extract_features(x_part, y_part, z_part):\n    \"\"\"\n    Extract features from accelerometer readings.\n    \n    First, magnitude across 3 axes is calculed and added as 4ht timeseries 'm'\n    Then, the followng features for each axis is calculated:\n    - mean: simple average for each axis\n    - minmax: difference between max value and min value for each axis\n    - min: minumum value for each axis\n    - rms: root mean square for each axis\n    - corr_xy, corr_yz, corr_xz - Pearson correlation coefs. for corresponding axis.\n    \n    as proposed in 'A Study on Human Activity Recognition Using Accelerometer Data from Smartphones' \n    https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1877050914008643\n\n    Parameters:\n            x_part, y_part, z_part: Arrays of shape n_samples x FRAME_LENGTH\n            with accelerometer readings\n\n    Returns:\n            features (DataFrame): DataFrame with generated features (19 features in total)\n    \"\"\"\n    magnitude_part = np.sqrt(x_part**2 + y_part**2 + z_part**2)\n    ts_matrix = np.stack([x_part, y_part, z_part, magnitude_part], axis=1)\n    ts_ax_names = ['x', 'y', 'z', 'm']\n    assert len(ts_ax_names) == ts_matrix.shape[1]\n    \n    mean_features = pd.DataFrame(ts_matrix.mean(axis=2), columns=[f'mean_{ax}' for ax in ts_ax_names])\n    \n    minmax_m = ts_matrix.max(axis=2) - ts_matrix.min(axis=2)\n    minmax_features = pd.DataFrame(minmax_m, columns=[f'minmax_{ax}' for ax in ts_ax_names])\n    \n    std_m = ts_matrix.std(axis=2)\n    std_features = pd.DataFrame(std_m, columns=[f'std_{ax}' for ax in ts_ax_names])\n    \n    min_m = ts_matrix.min(axis=2)\n    min_features = pd.DataFrame(min_m, columns=[f'min_{ax}' for ax in ts_ax_names])\n    \n    rms_m = np.sqrt(np.square(ts_matrix).mean(axis=2))\n    rms_features = pd.DataFrame(rms_m, columns=[f'rms_{ax}' for ax in ts_ax_names])\n    \n    ix = np.arange(len(ts_matrix))\n    iy = ix + len(ts_matrix)\n    corr_xy = np.corrcoef(ts_matrix[:, 0, :], ts_matrix[:, 1, :])[ix,iy]\n    corr_yz = np.corrcoef(ts_matrix[:, 1, :], ts_matrix[:, 2, :])[ix,iy]\n    corr_xz = np.corrcoef(ts_matrix[:, 0, :], ts_matrix[:, 2, :])[ix,iy]\n    corr_features = pd.DataFrame({'corr_xy': corr_xy, 'corr_yz': corr_yz, 'corr_xz': corr_xz})\n    \n    features = pd.concat([mean_features, minmax_features, min_features, rms_features, corr_features], axis=1)\n    return features","fc251521":"feature_df = extract_features(\n    raw_data[x_acc_cols].values,\n    raw_data[y_acc_cols].values,\n    raw_data[z_acc_cols].values)","c53bc06b":"feature_df['label'] = target_le.transform(raw_data['label'])","fc5a9fe7":"all_features = [c for c in feature_df.columns if c != 'label']\nprint('len(all_features): ', len(all_features))\nall_features","67dc7682":"X = feature_df[all_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regression on all features')","3b98306a":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_transform = PolynomialFeatures(2, include_bias=False)","659b95de":"X = poly_transform.fit_transform(feature_df[all_features])\ny = feature_df['label'].values\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X[train_ix], X[valid_ix]\n    y_train, y_valid = y[train_ix], y[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regressions on all features + Polynomial degree 2')","8949cfd2":"X = poly_transform.fit_transform(feature_df[all_features])\ny = feature_df['label'].values\n\ndef l1_objective(C=1.0):\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X[train_ix], X[valid_ix]\n        y_train, y_valid = y[train_ix], y[valid_ix]\n\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n\n        cls = linear_model.LogisticRegression(penalty='l1', solver='liblinear', C=C)\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","e0b5b542":"from tqdm.cli import tqdm","ec32c839":"Cs = np.linspace(1e-3, 1.5, num=20)\nf1_values = []\nf1_stds   = []\n\nfor C in tqdm(Cs):\n    cv_metrics = l1_objective(C=C)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    f1_values.append(f1_macro['mean'])\n    f1_stds.append(f1_macro['std'])\n","7e291e8a":"sns.lineplot(x=Cs, y=f1_values)\nsns.scatterplot(x=Cs, y=f1_values, color='green')\nplt.errorbar(Cs, f1_values, f1_stds, linestyle='None')\n\nplt.xlabel('C')\nplt.ylabel('F1 macro')\nplt.title('Choosing inverse regularization strength C for Lasso with CV');","72a0d417":"Cs[1]","c8cf2070":"scaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\nlasso_cls = linear_model.LogisticRegression(penalty='l1', C=.2, solver='liblinear')\nlasso_cls.fit(X_scaled, y)","f6d0eedd":"importance_coefs = np.abs(lasso_cls.coef_).max(axis=0)\nfeature_importance = list(zip(poly_transform.get_feature_names(all_features), importance_coefs))\n\nimportant_features = [(name, coef) for name, coef in feature_importance]\nimportance_df = pd.DataFrame(important_features, columns = ['feature_name', 'importance']\n                            ).sort_values(by='importance', ascending=False\n                            ).reset_index(drop=True)","f61a335f":"importance_df.head()","3d3a2fae":"importance_df[importance_df['importance'] > 1e-2].shape","7fab7a28":"def l2_objective_n_features(selected_features):\n    X = poly_transform.fit_transform(feature_df[all_features])\n    X = pd.DataFrame(X, columns = poly_transform.get_feature_names(all_features))\n    X = X[selected_features]\n    y = feature_df['label']\n\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n\n        cls = linear_model.LogisticRegression(penalty='l2', max_iter=2_000)\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","dae8030f":"n_features = np.arange(1, 61)\n\nscores = {\n    'selected_features': [],\n    'f1_macro': [],\n    'f1_macro_std': [],\n    'f1_min': [],\n    'f1_min_std': [],\n}\n\nfor n in tqdm(n_features):\n    selected_features = importance_df['feature_name'].head(n).values\n    cv_metrics = l2_objective_n_features(selected_features)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    scores['selected_features'].append(selected_features)\n    scores['f1_macro'].append(print_df.loc['f1_macro']['mean'])    \n    scores['f1_macro_std'].append(print_df.loc['f1_macro']['std'])\n    \n    scores['f1_min'].append(print_df.loc['f1_min']['mean'])    \n    scores['f1_min_std'].append(print_df.loc['f1_min']['std'])    \n\n","2bb3d987":"plt.figure(figsize=(16,6))\nsns.lineplot(x=n_features, y=scores['f1_macro'])\nsns.scatterplot(x=n_features, y=scores['f1_macro'], label='f1_macro')\n\nsns.lineplot(x=n_features, y=scores['f1_min'])\nsns.scatterplot(x=n_features, y=scores['f1_min'], label='f1_min')\n\nplt.xlabel('Number of features')\nplt.ylabel('F1 score')\nplt.title('Choosing number of features');","3a999d99":"top_23_features = importance_df['feature_name'].head(23).values\ntop_23_features","0cc24cdd":"X = poly_transform.fit_transform(feature_df[all_features])\nX = pd.DataFrame(X, columns = poly_transform.get_feature_names(all_features))\nX = X[top_23_features]\ny = feature_df['label']\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regression on top 23 features')","65fa7c14":"X = feature_df[all_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on all features')","831595a5":"X = feature_df[all_features]\ny = feature_df['label']\n\ncls = RandomForestClassifier()\ncls.fit(X, y)","246bd88f":"cls.feature_importances_","76da7549":"plt.figure(figsize=(16, 6));\nplt.title('Impurity-based feature importances of the forest')\nplot = sns.barplot(x=all_features, y=cls.feature_importances_)\nfor item in plot.get_xticklabels():\n    item.set_rotation(45)","7cf85b65":"importance_df = pd.DataFrame(zip(all_features, cls.feature_importances_), columns = ['feature_name', 'importance']\n                            ).sort_values(by='importance', ascending=False\n                            ).reset_index(drop=True)\nimportance_df.head()","01d4cae2":"top_10_features = importance_df['feature_name'].head(10).values\ntop_10_features","dc89bd4a":"def RF_objective_n_features(selected_features):\n    X = feature_df[selected_features]\n    y = feature_df['label']\n\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        cls = RandomForestClassifier()\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","c30369fb":"n_features = np.arange(1, len(all_features)+1)\n\nscores = {\n    'selected_features': [],\n    'f1_macro': [],\n    'f1_macro_std': [],\n    'f1_min': [],\n    'f1_min_std': [],\n}\n\nfor n in tqdm(n_features):\n    selected_features = importance_df['feature_name'].head(n).values\n    cv_metrics = RF_objective_n_features(selected_features)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    scores['selected_features'].append(selected_features)\n    scores['f1_macro'].append(print_df.loc['f1_macro']['mean'])    \n    scores['f1_macro_std'].append(print_df.loc['f1_macro']['std'])\n    \n    scores['f1_min'].append(print_df.loc['f1_min']['mean'])    \n    scores['f1_min_std'].append(print_df.loc['f1_min']['std'])    \n\n","f639fae9":"plt.figure(figsize=(16,6))\nsns.lineplot(x=n_features, y=scores['f1_macro'])\nsns.scatterplot(x=n_features, y=scores['f1_macro'], label='f1_macro')\n\nsns.lineplot(x=n_features, y=scores['f1_min'])\nsns.scatterplot(x=n_features, y=scores['f1_min'], label='f1_min')\n\nplt.xlabel('Number of features')\nplt.ylabel('F1 score')\nplt.title('Choosing number of features for Random Forest');","1ab8ce52":"top_7_features = importance_df['feature_name'].head(7).values\ntop_7_features","13b725b1":"X = feature_df[top_7_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on top 7 features')","44c6a0cc":"X = feature_df[top_7_features]\ny = feature_df['label']\n\ncls = RandomForestClassifier(n_estimators=100, oob_score=True)\ncls.fit(X, y)","20f540fb":"oob_preds = cls.oob_decision_function_.argmax(axis=1)\nconfusion_matrix(y, oob_preds)","ce83e13e":"print_cv_metrics([evaluate_performance(y, oob_preds)], 'out-of-bag performance of RF on top 7 features')","395b5ef9":"from hyperopt import tpe, hp, fmin, STATUS_OK, Trials, space_eval\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.pyll.stochastic import sample","9cca54fd":"space = {\n    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 20, 1000, 1)),\n    \"max_depth\": hp.choice('max_depth', [None, scope.int(hp.quniform(\"max_depth_int\", 1, 20,1))] ),\n    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n}\n\nMAX_EVALS = 100","c62668ee":"top_5_features = importance_df['feature_name'].head(5).values\ntop_5_features","9cc7f6f5":"X = feature_df[top_5_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier(\n        random_state=1\n    )\n    \n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on top 5 features with default hyperparams')","76e98084":"X = feature_df[top_5_features]\ny = feature_df['label']\n\ndef RF_objective_hyperparams(params):\n#     print(params)\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        cls = RandomForestClassifier(\n            n_estimators=params['n_estimators'],\n            max_depth=params['max_depth'],\n            criterion=params['criterion'],\n            random_state=1\n        )\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n        \n    loss = 1 - pd.DataFrame(cv_metrics)['f1_min'].mean()\n    return {'loss': loss, 'params': params, 'status': STATUS_OK}","7d91981a":"%%time\nRF_objective_hyperparams({'n_estimators': 100, 'criterion': 'gini', 'max_depth': None})","a0e947c1":"bayes_trials = Trials()\n\n# Optimize\n# best = fmin(fn = RF_objective_hyperparams, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)","8321ac06":"# best_params = space_eval(space, best)\nbest_params = {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 274}\nbest_params","27c2d6f8":"X = feature_df[top_5_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier(\n        n_estimators=best_params['n_estimators'],\n        max_depth=best_params['max_depth'],\n        criterion=best_params['criterion'],\n        random_state=1\n    )\n    \n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Tuned Random Forest on top 5 features')","bc0b1943":"X = raw_data[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_data['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on raw data')","61706704":"from sklearn import svm","ef1bffa0":"X = raw_data[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_data['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = svm.SVC()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'SVM (rbf) on raw data')","5f5c77ec":"raw_stairs = raw_data[raw_data['label'].isin(['stairs', 'walking'])].reset_index(drop=True).copy()\nraw_stairs['is_stairs'] = raw_stairs['label'] == 'stairs'\nraw_stairs.shape","4439a28a":"X = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\ncv_metrics = []\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = svm.SVC()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    f_scores.append(f1_score(y_valid, y_pred))\nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","1f2a79cb":"def split_frames_to_windows(X, window_size = 20):\n    \"\"\"Create windows of smaller size from batch of timeseries\"\"\"\n    frame_length = X.shape[-1]\n    n_windows = frame_length - window_size + 1\n    i = 0\n\n    X_extended = []\n    for i in range(0, n_windows):\n        X_extended.append(X[:, :, i:window_size+i])\n\n    X_extended = np.vstack(X_extended)\n    return X_extended","4f4a443c":"np.random.seed(1)\nsample_X = np.random.randint(6, size=(1,3,4))\nsample_X","9691f826":"sample_windows = split_frames_to_windows(sample_X, window_size=3)\nassert len(sample_windows)      == 2 # we expect to get 2 windows\nassert sample_windows.shape[1:] == (3,3) # we expect each window to be 3x3\n\nsample_windows","8f9a2a3b":"def assemble_ts_parts(X):\n    x_part = X[x_acc_cols].values\n    y_part = X[y_acc_cols].values\n    z_part = X[z_acc_cols].values\n    features = np.stack([x_part, y_part, z_part], axis=1)\n    \n    return features","5cf0b8a1":"def fold_predictions(y_pred, n_windows):\n    y_pred = y_pred.reshape(n_windows, -1)\n    return y_pred.mean(axis=0)\n\nsample_preds = np.array([1, 0, 1, 1, 0, 0])\nexpected_preds = np.array([1., 0., .5])\n\nactual_preds = fold_predictions(sample_preds, n_windows=2)\n\nassert np.allclose(actual_preds, expected_preds)","d1a5e9ce":"window_size = 29\nthreshold = .5\nn_windows = FRAME_LENGTH - window_size + 1\n\nX = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\n\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    ts_frames = assemble_ts_parts(X_train)\n    X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n    X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n    y_train_extended = np.tile(y_train, n_windows)\n    \n    cls = svm.SVC()\n    cls.fit(X_train_extended, y_train_extended)\n    X_valid_extended = assemble_ts_parts(X_valid)\n    X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n    X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n    \n    y_pred_extended = cls.predict(X_valid_extended)\n    y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n    y_pred = y_pred > threshold\n    f_scores.append(f1_score(y_valid, y_pred))\n    \nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","b47c035f":"window_size = 20\nthreshold = .5\nn_windows = FRAME_LENGTH - window_size + 1\n\nX = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\n\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    ts_frames = assemble_ts_parts(X_train)\n    X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n    X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n    y_train_extended = np.tile(y_train, n_windows)\n    \n    cls = svm.SVC()\n    cls.fit(X_train_extended, y_train_extended)\n    X_valid_extended = assemble_ts_parts(X_valid)\n    X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n    X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n    \n    y_pred_extended = cls.predict(X_valid_extended)\n    y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n    y_pred = y_pred > threshold\n    f_scores.append(f1_score(y_valid, y_pred))\n    \nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","aae57fe1":"def get_SVM_F1_on_window_size(window_size):\n    threshold = .5\n    n_windows = FRAME_LENGTH - window_size + 1\n\n    X = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\n    y = raw_stairs['is_stairs']\n\n    f_scores = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        ts_frames = assemble_ts_parts(X_train)\n        X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n        X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n        y_train_extended = np.tile(y_train, n_windows)\n\n        cls = svm.SVC()\n        cls.fit(X_train_extended, y_train_extended)\n        X_valid_extended = assemble_ts_parts(X_valid)\n        X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n        X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n\n        y_pred_extended = cls.predict(X_valid_extended)\n        y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n        y_pred = y_pred > threshold\n        f_scores.append(f1_score(y_valid, y_pred))\n\n    return f_scores\n","af5b0830":"get_SVM_F1_on_window_size(30)","f11c89b1":"window_sizes = np.arange(30, 9, -1)\nf_scores_means = []\nf_scores_stds  = []\n \nfor window_size in tqdm(window_sizes):\n    f_scores_cv = get_SVM_F1_on_window_size(window_size)\n    f_scores_means.append(np.mean(f_scores_cv))\n    f_scores_stds.append(np.std(f_scores_cv))    ","315bc308":"plt.figure(figsize=(14,6))\nplt.title('Binary classification performance (stairs vs walking) on raw data')\nplt.xlabel('Window size');\nplt.ylabel('F1 score')\nsns.lineplot(x=window_sizes, y=f_scores_means)\nsns.scatterplot(x=window_sizes, y=f_scores_means, s=100);\nplt.errorbar(window_sizes, f_scores_means, f_scores_stds, linestyle='None')","163667e7":"from scipy import signal as sig","75199483":"fig, axes = plt.subplots(2, 2, figsize=(20, 20))\naxes = axes.flatten()\n\n\nsteps = np.arange(30)\n\nfor ax, label in zip(axes, classes):\n    sample = raw_data[raw_data['label'] == label].iloc[0]\n    sns.lineplot(y=sample[x_acc_cols].astype(float), color='g', x=steps, alpha=.3, label='X', ax=ax)\n    sns.lineplot(y=sample[y_acc_cols].astype(float), color='r', x=steps, alpha=.3, label='Y', ax=ax)\n    sns.lineplot(y=sample[z_acc_cols].astype(float), color='b', x=steps, alpha=.3, label='Z', ax=ax).set_title(label)\n    \n    sns.lineplot(y=sig.medfilt(sample[x_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 X', color='g', ax=ax)\n    sns.lineplot(y=sig.medfilt(sample[y_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 Y', color='r', ax=ax)\n    sns.lineplot(y=sig.medfilt(sample[z_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 Z', color='b', ax=ax)\n    \n    ax.set_ylim(-40, 40)\n    ax.set_xlabel('time step')\n    ax.set_ylabel('m\/$s^2$')","2a77449c":"## 4.1 Logistic regression <a class=\"anchor\" id=\"section_4_1\"><\/a>","2299a2f9":"### Tuning Random Forest on 5 features","f737a4b1":"#### Choosing optimal number of features","d5dbdc71":"t-SNE transformation, applied on raw accelerometer data in a frame of length 30, is visualized on 2d plane. We can see 3 clusters here:\n- running\n- idle\n- stairs + walking","47a549f4":"# 1. EDA <a class=\"anchor\" id=\"section_1\"><\/a>","46de33b3":"## t-SNE visualization","f8d90196":"We can see that there is class imbalance: only 165 samples of 'stairs' class","277b2749":"We can see, that calculating mean for every axis is enough to separate all classes for each other, except for 'walking' vs 'stairs'","84a5cb27":"# 2. Setup validation and baselines <a class=\"anchor\" id=\"section_2\"><\/a>","b2ebe4c2":"### Split to smaller window, then assemble predictions","bcb2d4f1":"## Sample activities","91b3d514":"### Final model","93875f5f":"### Adding polynomial features","ac97ca85":"## Baselines","4a2930f0":"### Switch to binary classification","2602dd7f":"# 5. Other experiments <a class=\"anchor\" id=\"section_5\"><\/a>","6a8c5771":"## 4.2 RandomForest <a class=\"anchor\" id=\"section_4_2\"><\/a>","133881a3":"Each tree in Random forest is trained on subsample of the original data. We expect that for each training sample there are trees in our forest that had not seen this sample during training. Thus, we can make a good estimate of **generalization ability** of our forest even without holdout set or crossvalidation.","65a81e52":"## 5.2 Median filter visualization <a class=\"anchor\" id=\"section_5_2\"><\/a>","d138a806":"## StratifiedKFold","3ecf1745":"# 4. Modeling <a class=\"anchor\" id=\"section_4\"><\/a>","0aeabe34":"### Feature selection with L1 regularization","b3d018ac":"Visualize one sample for each activity","5b8dc806":"# 3. Feature engineering <a class=\"anchor\" id=\"section_3\"><\/a>","597fb900":"### out-of-bag errors","fe42afd1":"### With original window","2110f5db":"We can see that all readings are centered around zero. X axis is approximately normally distributed, while Y and Z looks like bimodal distributions.","667efd6d":"## Mean for each axis","f29f6431":"Lets work with binary classification (stairs vs walking) for simplicity. And setup baseline on raw data.","88d9bc9f":"## Distibution of readings across each axis","2312efe4":"## 5.1 Using raw data + smaller window size <a class=\"anchor\" id=\"section_5_1\"><\/a>","3ee28394":"We can see, that median filter can be used for denoizing signal from accelerometer and provide great source for futher feature engineering","e0451f55":"# 0. Data loading <a class=\"anchor\" id=\"section_0\"><\/a>","07a23593":"# Table of Contents\n\n* [0. Data loading](#section_0)\n* [1. EDA](#section_1)\n* [2. Setup validation and baselines](#section_2)\n* [3. Feature engineering](#section_3)\n* [4. Modeling](#section_4)\n    * [4.1 Logistic regression](#section_4_1)\n    * [4.2 Random Forest](#section_4_2)\n* [5. Other experiments](#section_5)\n    * [5.1 Using raw data + smaller window size](#section_5_1)\n    * [5.2 Median filter visualization](#section_5_2)"}}