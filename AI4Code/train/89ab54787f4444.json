{"cell_type":{"32d406c6":"code","c8a3109c":"code","1c819f4e":"code","04baafd4":"code","d6a3a627":"code","18202bdb":"code","8c0b6554":"code","55fe1f45":"code","2c3a3230":"code","925a5e86":"code","885cb3e0":"code","2c49b80b":"code","b96fa20d":"code","3113e1fb":"code","ef2089c8":"code","37820e33":"code","9b16b582":"code","a7598ff1":"code","03550645":"code","3753701f":"code","07f6abe5":"code","1933e6b9":"code","8a4eb38b":"code","27c942d5":"code","3a32d4ed":"code","ed3a97a4":"code","3bf2ebba":"code","f0a70b5b":"code","2d23f003":"code","da47a022":"code","8a8110ba":"code","023fefef":"code","d2d975fa":"code","82613135":"code","f33d7b0f":"code","6cae5e2f":"code","c3f75c79":"code","5aebc06f":"code","ed55ae82":"code","cc29ae74":"code","b283b609":"code","24ce0b7a":"code","9e197e5e":"code","8915b16f":"code","6289a7d1":"code","eb4949dd":"code","0cc18561":"code","30f05233":"code","f5b7fb56":"code","385d90db":"code","8eaaf381":"code","5e98e2d9":"code","d6cf1c8a":"code","c9a890d8":"code","14f5defb":"code","7a3b1b57":"code","2f5753b6":"code","5e4520f8":"code","d45ab6fa":"code","5dc4f49e":"code","ed9d18e1":"code","0d2c478e":"markdown","ecc64d05":"markdown","98284022":"markdown","5d6cd8a9":"markdown","a4472811":"markdown","7adbb013":"markdown","b35f0346":"markdown","69236f2a":"markdown","557af8c4":"markdown","81af1d2a":"markdown","43decccd":"markdown","ce93f14f":"markdown","5f11b245":"markdown","c0dc5142":"markdown","5ee9ce6a":"markdown","c760db12":"markdown","fb2786e3":"markdown","ec362f3b":"markdown","a091a3c9":"markdown","b3eef341":"markdown","91138a84":"markdown","e5910fd5":"markdown","3b6814c0":"markdown","01b0491b":"markdown","fa302983":"markdown","43143587":"markdown","8b987e4b":"markdown","5a97e0cb":"markdown","45df83aa":"markdown","9be4e1ff":"markdown","39a4db32":"markdown","849a7925":"markdown"},"source":{"32d406c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8a3109c":"train = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')","1c819f4e":"train.head()","04baafd4":"train.info()","d6a3a627":"null = train.isnull().sum()\nnull_df = pd.DataFrame(null, columns=['count'])\nnull_df[null_df['count']!=0]","18202bdb":"mean = train.isnull().mean()\nmean_df = pd.DataFrame(mean, columns=['mean'])\nmean_df[mean_df['mean']!=0]","8c0b6554":"train.drop(columns=['v18q1', 'rez_esc'], inplace=True)\ntest.drop(columns=['v18q1', 'rez_esc'], inplace=True)","55fe1f45":"train['meaneduc'].plot(kind='hist', bins=30)","2c3a3230":"train['meaneduc'].fillna(np.mean(train['meaneduc']), inplace=True)\ntest['meaneduc'].fillna(np.mean(test['meaneduc']), inplace=True)","925a5e86":"np.sqrt(train['SQBmeaned']).plot(kind='hist', bins=30)","885cb3e0":"train.drop(columns=['SQBmeaned'], inplace=True)\ntest.drop(columns=['SQBmeaned'], inplace=True)","2c49b80b":"train['Target'].value_counts()","b96fa20d":" train[train['tamviv']!=train['r4t3']].head()","3113e1fb":"corr1 = train.corr()\nprint(corr1['Target']['tamviv'])\nprint(corr1['Target']['r4t3'])","ef2089c8":" train[train['hhsize']!=train['tamhog']]","37820e33":" train[train['hogar_total']!=train['tamhog']]","9b16b582":"train_select = train.loc[train['parentesco1'] == 1]\ntest_select = test.loc[test['parentesco1'] == 1]","a7598ff1":"train_select.head()","03550645":"train_select.info()","3753701f":"disabled=[]\nnum_male=[]\nnum_female=[]\nmean_edu=[]\nfor i in train_select['idhogar']:\n    k = train[train['idhogar']== i]['dis'].sum()\n    j = train[train['idhogar']== i]['male'].sum()\n    p = train[train['idhogar']== i]['female'].sum()\n    e = train[train['idhogar']== i]['escolari'].mean()\n    disabled.append(k)\n    num_male.append(j)\n    num_female.append(p)\n    mean_edu.append(e)\n\n# For test data\ndisabledt=[]\nnum_malet=[]\nnum_femalet=[]\nmean_edut=[]\nfor i in test_select['idhogar']:\n    k = test[test['idhogar']== i]['dis'].sum()\n    j = test[test['idhogar']== i]['male'].sum()\n    p = test[test['idhogar']== i]['female'].sum()\n    e = test[test['idhogar']== i]['escolari'].mean()\n    disabledt.append(k)\n    num_malet.append(j)\n    num_femalet.append(p)\n    mean_edut.append(e)\n    ","07f6abe5":"train_select.loc[:,'mean_edu'] = mean_edu\ntrain_select.loc[:,'no.of.males'] = num_male\ntrain_select.loc[:,'no.of.females'] = num_female\ntrain_select.loc[:,'diabled'] = disabled\n\ntest_select.loc[:,'mean_edu'] = mean_edut\ntest_select.loc[:,'no.of.males'] = num_malet\ntest_select.loc[:,'no.of.females'] = num_femalet\ntest_select.loc[:,'diabled'] = disabledt","1933e6b9":"train_select['dependency'] = train_select['dependency'].replace('yes', '1')\ntrain_select['dependency'] = train_select['dependency'].replace('no', '0')\n\ntest_select['dependency'] = test_select['dependency'].replace('yes', '1')\ntest_select['dependency'] = test_select['dependency'].replace('no', '0')","8a4eb38b":"train_select['dependency'] = train_select['dependency'].astype(float)\ntest_select['dependency'] = test_select['dependency'].astype(float)","27c942d5":"rr = train_select['v2a1'].isnull().sum()\nprint('Number of null rows in montly rent column - {}'.format(rr))","3a32d4ed":"train_select[(train_select['tipovivi1']==1) ]['v2a1'].isnull().sum()","ed3a97a4":"train_select[(train_select['tipovivi2']==1) ]['v2a1'].isnull().sum()","3bf2ebba":"train_select[(train_select['tipovivi3']==1) ]['v2a1'].isnull().sum()","f0a70b5b":"train_select[(train_select['tipovivi4']==1) ]['v2a1'].isnull().sum()","2d23f003":"train_select[(train_select['tipovivi5']==1) ]['v2a1'].isnull().sum()","da47a022":"index = train_select.index\nfor i in index:\n    if train_select['tipovivi1'][i]==1:\n        train_select.loc[i, 'v2a1'] = 0\n\nindex2 = test_select.index\nfor i in index2:\n    if test_select['tipovivi1'][i]==1:\n        test_select.loc[i, 'v2a1'] = 0","8a8110ba":"median = train_select[(train_select['tipovivi3']==1) | ((train_select['tipovivi2']==1)) ]['v2a1'].median()\n\nmediant = test_select[(test_select['tipovivi3']==1) | ((test_select['tipovivi2']==1)) ]['v2a1'].median()","023fefef":"for i in train_select.index:\n    if train_select['tipovivi4'][i]==1:\n        train_select.loc[i, 'v2a1'] = median\n        \nfor i in train_select.index:\n    if train_select['tipovivi5'][i]==1:\n        train_select.loc[i, 'v2a1'] = median\n        \nfor i in test_select.index:\n    if test_select['tipovivi4'][i]==1:\n        test_select.loc[i, 'v2a1'] = mediant\n        \nfor i in test_select.index:\n    if test_select['tipovivi5'][i]==1:\n        test_select.loc[i, 'v2a1'] = mediant","d2d975fa":"train_select['v2a1'].isnull().sum()","82613135":"train_select['v2a1'].plot(kind='hist', bins=30)","f33d7b0f":"corr2 = train_select.corr()\ncorr2['Target']","6cae5e2f":"remove_columns=['tamhog', 'r4t3', 'r4h1','r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', \n               'parentesco1', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5','parentesco6',\n                'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11',\n               'parentesco12', 'age', 'SQBescolari', 'SQBage', 'SQBhogar_total','SQBhogar_nin',\n                'SQBovercrowding', 'SQBdependency', 'agesq', 'edjefa','edjefe' ,'SQBedjefe', 'instlevel1', \n                'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5','instlevel6', 'instlevel7', \n                'instlevel8', 'instlevel9','meaneduc', 'hacdor', 'hacapo', 'estadocivil1', 'estadocivil2',\n               'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 'tipovivi1',\n               'tipovivi2','tipovivi3','tipovivi4','tipovivi5', 'hhsize', 'dis', 'male', 'female']","c3f75c79":"train_select_2 = train_select.drop(columns= remove_columns)\ntest_select_2 = test_select.drop(columns= remove_columns)","5aebc06f":"train_select_2.head()","ed55ae82":"X = train_select_2.drop(columns=['Target', 'idhogar', 'Id'])\ny = train_select_2['Target']\nX_given = test_select_2.drop(columns=[ 'idhogar', 'Id'])","cc29ae74":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","b283b609":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()","24ce0b7a":"\nfrom sklearn.model_selection import RandomizedSearchCV\nrandom_grid = {'n_estimators':[100, 200, 400, 600, 800],\n              'max_depth' : [5, 10, 15,20, 30, None],\n              'max_features' : ['auto', 'log2'],\n              'min_samples_leaf': [1, 2, 4],\n              'min_samples_split' : [2, 5, 10]} ","9e197e5e":"rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, n_jobs = -1)","8915b16f":"rf_random.fit(X_train,y_train)","6289a7d1":"rf_random.best_params_","eb4949dd":"pred = rf_random.predict(X_test)","0cc18561":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score","30f05233":"print(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\nprint(f1_score(y_test, pred, average='macro'))","f5b7fb56":"from sklearn.model_selection import GridSearchCV","385d90db":"grid = {'n_estimators':[ 470, 480, 490, 400, 510, 520, 530],\n              'max_depth' : [25, 30,35, None],\n              'max_features' : ['auto', 'log2'],\n              'min_samples_leaf': [1, 2],\n              'min_samples_split' : [ 2,3]}","8eaaf381":"rf = RandomForestClassifier()","5e98e2d9":"grid_search = GridSearchCV(estimator = rf, param_grid = grid, \n                          cv = 5, n_jobs = -1, verbose = 2)","d6cf1c8a":"grid_search.fit(X_train, y_train)","c9a890d8":"grid_search.best_params_","14f5defb":"pred =grid_search.predict(X_test)","7a3b1b57":"print(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\nprint(f1_score(y_test, pred, average='macro'))","2f5753b6":"real_pred = grid_search.predict(X_given)","5e4520f8":"\nc=0\nfor i in test_select['idhogar']:\n    test2 = test[test['idhogar'] == i]\n    for j in test2.index:\n        test.loc[j, 'Target'] = real_pred[c] \n    c=c+1","d45ab6fa":"submit = pd.DataFrame({'Id': test['Id'], 'Target': test['Target']})","5dc4f49e":"submit.head()","ed9d18e1":"submit.to_csv('submission.csv', index=False)","0d2c478e":"**CLEANING FAMILY MEMBERS DATA**","ecc64d05":"**EXTRA FEATURES**","98284022":"There is a difference between the target outcomes. Let us procced anyway, if the outcome is not upto mark, we may try oversampling the data.","5d6cd8a9":"Since we have calculated mean education of all members we remove meaneduc column as well. The edjefe\/edjefa column indicates the education of the household head. But since we have taken only the rows containing the head member, the 'escolari' column indicates the same\n\nHence we remove the edjefe\/edjefa column and keep the escolari column in place.\n","a4472811":"**DATA CLEANING**","7adbb013":"There are 1856 households which are owned and thus we will asign the v2a1(monthly rent) column as 0.\nSince the rent distribution is higly skewed, we will use median of the data and assign that value to remaininig 300 values(tipovivi4 and tipovivi5)","b35f0346":"As expected tamviv is correlated more than r4t3(Though sligth difference). So let us remove r4t3 as well.","69236f2a":"There are two more columns with 5 missing data. meaneduc and SBQmeaned","557af8c4":"As the plot is a fairly well guasian curve, we can use mean to fill up the remaining values","81af1d2a":"After implementation of Randomized Search CV we came up with the following best parameters-\n1. n_estimators    : 400\n2. max_depth       : 30\n3. max_features    : auto\n4. min_sample_leaf : 1\n5. min_sample_split: 2","43decccd":"**RENT FEATURE**","ce93f14f":"Clearly the square root of SQBmeaned column gives us exactly the same plot as meaneduc plot. Thus we can remove this feature as it is repetative.","5f11b245":"We look if any features are repeating or similar to each other","c0dc5142":"To know which hyperparameters are best, we implement Randomized Search CV. It randomly initializes the hyperparameters given in a dictionary and outputs the best parameters. Though it is faster than Grid Search CV, it still takes a lot of time for execution.","5ee9ce6a":"We notice that 'dependency' column has a few yes\/no in them. According to the formula, yes corresponds with 1 and no with 0","c760db12":"Since every household has one head member and not only the Target feature but most of the independent features are common to one household. From here we only take the head of the household rows(parentesco1) and analyze them.","fb2786e3":"The macro f1 score has sligtly increased. Accuracy is around 67%. We notice that class 4 has been predicted very well. But the other classes are not very accurate. This may be due to the fact that the training dataset had many many Target values belonging to class 4. Steps we can take to increase the accuracy-\n1. Oversampling technique to avoid dominance of a single class\n2. We can try to reduce the number of features furthermore \n3. We can use other algorithms such as Gradient boosting or Xg boost\n\nWe will now apply model to original test dataset and save our results to submission file.","ec362f3b":"There are 5 features to be taken into considertion before filling up the montly rent column\n1. tipovivi1 =1 own and fully paid house\n2. tipovivi2 =1 own,  paying in installments\n3. tipovivi3 =1 rented\n4. tipovivi4 =1 precarious\n5. tipovivi5 =1 other(assigned,  borrowed)\n\nFor all the households which are owned and fully paid, the montly rent column can be set to zero.\n\nWe can consider paying in installments as monthly rent.\n\nWe will decide precarious and borrowed house rents according to the data distribution","a091a3c9":"Clearly hhsize and tamhog and hogar_total are same features. We will remove hhsize and tamhog features","b3eef341":"Maybe the number of members living in the household(tamviv) should be more revelent than all persons in the household(r4t3). Let us just verify once","91138a84":"There are multiple hyperparameters when we use Random Forest. A few important ones are-\n1. n_estimators    : Number of decision trees we want to use\n2. max_depth       : Maximum depth of a decision tree. If None, then nodes are expanded till they are pure\n3. max_features    : The maximum features to consider while spliting a node. \n4. min_sample_leaf : The minimum number of samples(row wise different examples) present at the leaf(end node).\n5. min_sample_split: The minimum number of samples(row wise different examples) required to be present to split a node.","e5910fd5":"We will be using Random Forest Classifier algorithm. It is one of an ensamble technique and is also called as bootstrap aggregration. \n\n  We select random subsets of our data row and column wise. We feed these subsets to different decision trees simultaneously. The subsets which we create are always taken with replacement. Hence this technique is called **Row and Column Sampling with replacement**.\n\t\n  Once the model is trained, we input the test data. Suppose we are classifying things, then the output that the majority of models give will be taken into account. Since we are first dividing then reassembling, this is called **Bootstrap aggregation**\n  \n![rf.PNG](attachment:rf.PNG)\n","3b6814c0":"Below is the corelation array between Target and every other feature","01b0491b":"Graph below is a histogram plot of monthly rent","fa302983":"There are a few columns which are individual member specific. We do not wish to loose that data with the above code. Some of these columns are-\n* disabled\n* male\n* female\n* escolari(No of years of education)\n\nWe create 4 new features- \n1. mean_edu      : Average education of all family members\n2. no.of.males   : Number of males in household\n3. no.of.females : Number of females in household\n4. disabled      : Number of disabled members in family","43143587":"We will now look if there is any missing data","8b987e4b":"Since the ratio of missing data of rez_esc(Years behind in schooling) and v18q1(No of tablets in household) are 0.82 and 0.76 respectively, we are going to drop those columns.\nWe will deal with v2a1(montly rent) later","5a97e0cb":"To make the code more accurate, we will narrow our search down and use grid search cv which calculates accuracy for each and every combination.","45df83aa":"Along with the features we remove as mentioned above, we also remove all the SQB columns as they are just the square root of other columns. We remove instlevel(education of a member), parentesco(relation as a family member), estadocivil(marriage data) as we are considering only household head data. ","9be4e1ff":"For the next 5 code snippets we will analyze which montly rent columns are null with respect to tipovivi feature","39a4db32":"**TRAINING THE MODEL**","849a7925":"Another technique which we use is K-fold cross validation. Suppose we use 5 fold CV, it implies that the training data is divided into 5 parts. We take the first part as test and remaining 4 as train dataset. Then we take the second part as test and remaining as training sets. After complete process we take the average of all our scores. This will be the performance matrix of the model."}}