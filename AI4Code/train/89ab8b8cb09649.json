{"cell_type":{"cce09da0":"code","70ea031e":"code","633c3ffa":"code","1994c53c":"code","ca58e5ed":"code","1a3458b6":"code","b24a2163":"code","680cc31e":"code","5991755f":"code","b1570f36":"code","1f490cf7":"code","012d5b06":"code","0cf4404b":"code","cceb6d53":"code","615e3b73":"code","008cb33b":"code","7a9c63de":"code","6a2815e4":"code","6ad8a09b":"code","8514cfca":"code","14965287":"code","3e39c7bc":"code","a9ab3202":"code","f5bde243":"code","6de206ad":"code","b4d51cb3":"code","5bbc0d56":"code","fc4bd8a6":"code","59f2ae3c":"code","63b12d62":"code","ddd7e76c":"code","31885eda":"code","6de0388c":"code","1498bdc7":"code","8c9e27f5":"code","ae9b11a7":"code","eaf1f62c":"code","1a11bb4f":"code","682d4598":"code","1fe5e715":"code","4c6e7012":"code","2aa3044f":"code","a069b2ee":"code","68a7ef36":"code","ad0bda0e":"code","4773abfa":"code","b2e1ec98":"markdown","ea95e45d":"markdown","3fc9602e":"markdown","7dc666e3":"markdown","55459fcc":"markdown","d44cc8ac":"markdown","bee24dcf":"markdown","d0a53398":"markdown","d30880c9":"markdown","d8362573":"markdown","43030d76":"markdown","c0bbdc6f":"markdown","567eb381":"markdown","59ef8980":"markdown","2b6c99db":"markdown","4c7fa881":"markdown","2cc90a04":"markdown","3d6e6d5f":"markdown","378ec383":"markdown","02b18cca":"markdown","4b2a6647":"markdown","64368445":"markdown","9de751de":"markdown","d6431e67":"markdown","ad082c42":"markdown","cb4a53c1":"markdown","0a4117d3":"markdown","e9b9cd92":"markdown","04c3d1fa":"markdown","b9f5e72a":"markdown","454384cf":"markdown","da1973ca":"markdown","660a3599":"markdown","fe9219bb":"markdown"},"source":{"cce09da0":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \n%matplotlib inline","70ea031e":"cust_df=pd.read_csv('..\/input\/clustering\/Cust_Segmentation.csv')\ncust_df.head()","633c3ffa":"cust_df.shape","1994c53c":"df = cust_df.drop('Address', axis=1)\ndf.head()","ca58e5ed":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX = df.values[:,1:]   # Select all columns expect CustomerId\nX = np.nan_to_num(X)  #Convert all nan to zero\nClust_dataset=ss.fit_transform(X)\nClust_dataset","1a3458b6":"kmeans = KMeans(init='k-means++', n_clusters=3, n_init=12)\nkmeans.fit(X)","b24a2163":"print(kmeans.labels_)","680cc31e":"df['k-means']=kmeans.labels_\ndf.head()","5991755f":"df.groupby('k-means').mean()","b1570f36":"kmeans.cluster_centers_","1f490cf7":"plt.scatter(X[:, 0], X[:, 3], c=kmeans.labels_.astype(np.float), alpha=0.5)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\n\nplt.show()","012d5b06":"fig, ax = plt.subplots(figsize=(8,5))\n\nscatter = ax.scatter(X[:, 0], X[:, 3], s=(X[:, 1]**3), c=kmeans.labels_.astype(np.float), alpha=0.5)\nlegend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n#ax.add_artist(legend1)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\nplt.show()","0cf4404b":"from scipy import ndimage \nimport pylab\nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering ","cceb6d53":"pdf=pd.read_csv('..\/input\/clustering\/cars_clus.csv')\npdf.head()","615e3b73":"pdf.dtypes","008cb33b":"pdf.size #Number of rows x columns","7a9c63de":"pdf.iloc[:,2::]=pdf.iloc[:,2::].apply(pd.to_numeric, errors='coerce') #Select all columns which should be numerical, then apply conversion\npdf = pdf.dropna()  #All non-numerical field were converted to NaN, so now we have to drop them\npdf = pdf.reset_index(drop=True)  \npdf.head()  ","6a2815e4":"pdf.dtypes","6ad8a09b":"pdf.size","8514cfca":"featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]\nfeatureset.head()","14965287":"from sklearn.preprocessing import MinMaxScaler\nx = featureset.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx [0:5]","3e39c7bc":"from sklearn.metrics.pairwise import euclidean_distances\ndist_matrix = euclidean_distances(feature_mtx,feature_mtx) \nprint(dist_matrix)","a9ab3202":"dist_matrix.shape","f5bde243":"Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')\nplt.figure()\ndn = hierarchy.dendrogram(Z_using_dist_matrix)","6de206ad":"Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')\nfig = pylab.figure(figsize=(18,50))\ndef llf(id):\n    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n    \ndendro = hierarchy.dendrogram(Z_using_dist_matrix,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')","b4d51cb3":"agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'ward')\nagglom.fit(dist_matrix)\n\nagglom.labels_","5bbc0d56":"agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\nagglom.fit(dist_matrix)\n\nagglom.labels_","fc4bd8a6":"pdf['cluster']=agglom.labels_\npdf.head()","59f2ae3c":"pdf.groupby('cluster').mean() ","63b12d62":"fig, ax = plt.subplots(figsize=(8,5))\n\nscatter = ax.scatter(pdf['horsepow'], pdf['mpg'], s=(pdf['price']*3), c=pdf.cluster.astype(np.float), alpha=0.7)\nlegend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\nplt.xlabel('Horsepower', fontsize=18)\nplt.ylabel('mpg', fontsize=16)\nplt.show()","ddd7e76c":"agg_cars = pdf.groupby(['cluster','type'])['horsepow','engine_s','mpg','price'].mean()\nagg_cars","31885eda":"import matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\nplt.figure(figsize=(16,10))\nfor color, label in zip(colors, cluster_labels):\n    subset = agg_cars.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","6de0388c":"from sklearn.cluster import DBSCAN ","1498bdc7":"pdf=pd.read_csv('..\/input\/clustering\/weather-stations20140101-20141231.csv')\npdf.head()","8c9e27f5":"pdf.size","ae9b11a7":"pdf.head()","eaf1f62c":"pdf['Tm'].isnull().sum()","1a11bb4f":"pdf = pdf.iloc[pdf['Tm'].dropna().index,:]\npdf = pdf.reset_index(drop=True)\npdf.head()","682d4598":"pdf.dtypes","1fe5e715":"pdf.size","4c6e7012":"# Notice: For visualization of map, you need basemap package.\n# if you dont have basemap install on your machine, you can use the following line to install it\n!conda install -c conda-forge  basemap matplotlib==3.1 -y\n# Notice: you maight have to refresh your page and re-run the notebook after installation","2aa3044f":"from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nllon=-140\nulon=-50\nllat=40\nulat=65\n\npdf = pdf[(pdf['Long'] > llon) & (pdf['Long'] < ulon) & (pdf['Lat'] > llat) &(pdf['Lat'] < ulat)]\n\nmy_map = Basemap(projection='merc',\n            resolution = 'l', area_thresh = 1000.0,\n            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\n# my_map.drawmapboundary()\nmy_map.fillcontinents(color = 'white', alpha = 0.3)\nmy_map.shadedrelief()\n\n# To collect data based on stations        \n\nxs,ys = my_map(np.asarray(pdf.Long), np.asarray(pdf.Lat))\npdf['xm']= xs.tolist()\npdf['ym'] =ys.tolist()\n\n#Visualization1\nfor index,row in pdf.iterrows():\n#   x,y = my_map(row.Long, row.Lat)\n   my_map.plot(row.xm, row.ym,markerfacecolor =([1,0,0]),  marker='o', markersize= 5, alpha = 0.75)\n#plt.text(x,y,stn)\nplt.show()","a069b2ee":"import sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\nsklearn.utils.check_random_state(1000)\nClus_dataSet = pdf[['xm','ym']]\nClus_dataSet = np.nan_to_num(Clus_dataSet)\nClus_dataSet = StandardScaler().fit_transform(Clus_dataSet)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)\nlabels = db.labels_\npdf[\"Clus_Db\"]=labels\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels)) \n\n\n# A sample of clusters\npdf[[\"Stn_Name\",\"Tx\",\"Tm\",\"Clus_Db\"]].head(5)","68a7ef36":"pdf[[\"xm\",\"ym\",\"Tx\",\"Tm\",\"Clus_Db\"]].groupby('Clus_Db').mean()","ad0bda0e":"set(labels)","4773abfa":"from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nmy_map = Basemap(projection='merc',\n            resolution = 'l', area_thresh = 1000.0,\n            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\n#my_map.drawmapboundary()\nmy_map.fillcontinents(color = 'white', alpha = 0.3)\nmy_map.shadedrelief()\n\n# To create a color map\ncolors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))\n\n\n\n#Visualization1\nfor clust_number in set(labels):\n    c=(([0.4,0.4,0.4]) if clust_number == -1 else colors[np.int(clust_number)])\n    clust_set = pdf[pdf.Clus_Db == clust_number]                    \n    my_map.scatter(clust_set.xm, clust_set.ym, color =c,  marker='o', s= 20, alpha = 0.85)\n    if clust_number != -1:\n        cenx=np.mean(clust_set.xm) \n        ceny=np.mean(clust_set.ym) \n        plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)\n        print (\"Cluster \"+str(clust_number)+', Avg Temp: '+ str(np.mean(clust_set.Tm)))","b2e1ec98":"As we know DBSCAN detects outliers and its cluster label is -1","ea95e45d":"## Visualization\nVisualization of stations on map using basemap package. The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. Basemap does not do any plotting on it\u2019s own, but provides the facilities to transform coordinates to a map projections.","3fc9602e":"Lets remove rows that dont have any value in the Tm field.","7dc666e3":"Most of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision. However, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor. Additionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nDBSCAN is specially very good for tasks like class identification on a spatial context. The wonderful attribute of DBSCAN algorithm is that it can find out any arbitrary shape cluster without getting affected by noise. For example, this following example cluster the location of weather stations in Canada. As we will see, it not only finds different arbitrary shaped clusters, can find the denser part of data-centered samples by ignoring less-dense areas or noises.","55459fcc":"# Customer Segmentation with K-Means\nCustomer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retaining those customers. Another group might include customers from non-profit organizations. And so on.","d44cc8ac":"## Feature selection\nLets select our feature set:","bee24dcf":"Now let's normalize the dataset. But why do we need normalization in the first place? Normalization is a statistical method that helps mathematical-based algorithms to **interpret features with different magnitudes and distributions equally**. We use StandardScaler() to normalize our dataset.","d0a53398":"## Modeling\nThe KMeans class has many parameters that can be used, but we will be using these three:\n\n* init: Initialization method of the centroids. Value will be: \"k-means++\", k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n* n_clusters: The number of clusters to form as well as the number of centroids to generate. Value will be: 3 \n* n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. Value will be: 12 Initialize KMeans with these parameters.\n\nLets apply k-means on our dataset, and take look at cluster labels.","d30880c9":"Clearly we can see the type of columns does not make sense, so we have to drop all non-numerical fields and convert to float or int.","d8362573":"Now we can normalize the feature set. MinMaxScaler transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one.","43030d76":"We can check the 'centroid' of our cluster by averaging their features:","c0bbdc6f":"# Clustering weather stations with DBSCAN","567eb381":"We can group by cluster label and see the centroid based on location of each cluster:","59ef8980":"## Insights\nWe assign the labels to each row in our dataframe:","2b6c99db":"Note: In the figure above the clusters are differentiated by colour, as we know every time the model is run it will assign other number to clusters, thus it's not recommended to always relate a colour to a cluster because this could change if we execute the code again.  \nThree clusters of different colour are shown in the figure and we can infer that the top cluster corresponds to customers with the highest income who also are middle and old aged nevertheless is the small number of records who belongs to this group, for the middle cluster it best represents the records of middle income distributed between young to old aged customers, and finally the best characteristic of the bottom cluster is the low income and uniform distribution for all ages. We could add a new interesting feature to this plot, such as education, which we could think is positively correlated with income, and we could do this by increasing the size of the points proportional to the educational level.","4c7fa881":"## Distance Measurements\nWe begin the agglomerative clustering process by measuring the distance between the data points using the euclidean method.","2cc90a04":"Fortunately kmeans has an attribute which prints the centroids 'cluster_centers_':","3d6e6d5f":"We could do the same as in K-means, make a scatter plot of 2 main features and a third proportional to the size of the points, and we will differenciate the clusters and their distribution by color.","378ec383":"## Pre-processing ","02b18cca":"Pay attention to the labels printed above for both linkage methods, if we compare we will realize these only differ in the order of the clusters, they are classified as other number, but we see patterns and groups.","4b2a6647":"Let's group by cluster to find the cluster centers:","64368445":"In the figure above we can see the big circles have a little tendency to appear in middle to old ages, on the other hand we can also see tiny circles for old ages and some with high income. K-means will partition our customers into mutually exclusive groups, the customers in each cluster are similar to each other demographically. Now we can create a profile for each group, considering the common characteristics of each cluster. For example, the 3 clusters could be:  \n\n* AFFLUENT, EDUCATED AND OLD AGED\n* MIDDLE AGED AND MIDDLE INCOME\n* YOUNG AND LOW INCOME","9de751de":"Address in this dataset is a categorical variable. k-means algorithm isn't directly applicable to categorical variables because Euclidean distance function isn't really meaningful for discrete variables. So, lets drop this feature and run clustering.","d6431e67":"## Reading file ","ad082c42":"\n## Clustering of stations based on their location i.e. Lat & Lon\nDBSCAN form sklearn library can runs DBSCAN clustering from vector array or distance matrix. In our case, we pass it the Numpy array Clus_dataSet to find core samples of high density and expands clusters from them.\nIt works based on two parameters: Epsilon and Minimum Points\n* Epsilon: Determines a specified radius that if includes enough number of points within, we call it dense area. Value = 0.15.\n* MminimumSamples: Determines the minimum number of data points we want in a neighborhood to be defined as a cluster. Value = 10.","cb4a53c1":"Let's make a similar scatter plot as before, but using this new data grouped by type of automobile:","0a4117d3":"# Clustering on Vehicle dataset with Hierarchical agglomerative\nA famous automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n\nOur objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models.","e9b9cd92":"### Hi, welcome to my project!, today we will learn and study the 3 fundamental clustering algorithms of unsupervised machine learning (K-means, Hierarchical clustering and DBSCAN).","04c3d1fa":"The Agglomerative Clustering class will require two inputs:\n\n* n_clusters: The number of clusters to form as well as the number of centroids to generate. Value will be: 6\n* linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. Let's see the outcome for 'ward' and then 'complete'.","b9f5e72a":"The feature sets include price in thousands (price), engine size (engine_s), horsepower (horsepow), wheelbase (wheelbas), width (width), length (length), curb weight (curb_wgt), fuel capacity (fuel_cap) and fuel efficiency (mpg).","454384cf":"### Now, lets look at the distribution of customers based on their age and income: ","da1973ca":"## Visualization of clusters based on location:","660a3599":"In the figure above we can see that is not very clear where is placed the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, \"truck\" (value of 0 in the type column) and \"car\" (value of 1 in the type column). So, we use them to distinguish the classes, and summarize the cluster:","fe9219bb":"Now the type of columns make sense and below we see the size of the new dataframe is 1872, so we dropped 672 fields."}}