{"cell_type":{"ce4b8683":"code","5b1a539a":"code","609a30cd":"code","99a191cc":"code","af4ac6e8":"code","f028f07b":"code","973e4646":"code","76c66925":"code","37f8c246":"code","99022af7":"code","fa820945":"code","c51faabd":"code","bd4b63c0":"code","2eed1e8c":"code","07b3dfd9":"code","98a2f6ff":"code","fbfb407f":"code","30f234d8":"code","c7e98b5e":"code","258fa155":"code","e8626c63":"code","7eff0bc3":"code","ac2ec9e5":"code","0e67f0ef":"code","71754497":"code","25050601":"code","2840e9d4":"code","c5e41534":"code","b1b26314":"code","a515a675":"code","e7aac723":"code","b942f83f":"code","2e63a013":"code","557694c6":"code","ad523da8":"code","04bbb14e":"code","91fb0e13":"code","21a0aadc":"code","64f926b5":"code","9d5ed485":"code","e1deadbb":"code","bf627d8b":"code","8bbd3413":"code","6dbe5938":"code","516d2c91":"markdown","3fdf0e00":"markdown","d759b0c5":"markdown","5eccefcf":"markdown","c18524ed":"markdown","6330f83b":"markdown","540a740e":"markdown","a40f4e0c":"markdown","5fc1e105":"markdown","9c5948d4":"markdown","2ada062c":"markdown","96b86775":"markdown","17a86ef0":"markdown","792ddaf6":"markdown","2aa3230b":"markdown","9266a181":"markdown","3e8eb011":"markdown","306ac286":"markdown","33ad7c45":"markdown","c5154eec":"markdown","7ea85f01":"markdown","f9cac146":"markdown"},"source":{"ce4b8683":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","5b1a539a":"train_data = pd.read_csv('\/kaggle\/input\/hranalysis\/train.csv')\ntrain_data.head()","609a30cd":"sns.pairplot(train_data)","99a191cc":"train_data.drop('employee_id',inplace=True,axis='columns')","af4ac6e8":"train_data.head()","f028f07b":"sns.countplot(x='department',data=train_data,hue='is_promoted')","973e4646":"sns.countplot(x='gender',data=train_data,hue='is_promoted')","76c66925":"sns.heatmap(train_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","37f8c246":"def fill_edu(col):\n    if(pd.isnull(col)):\n        return \"Bachelor's\"\n    else:\n        return col","99022af7":"train_data['education'] = train_data['education'].apply(fill_edu)","fa820945":"sns.heatmap(train_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","c51faabd":"sns.countplot(x='previous_year_rating',data=train_data)","bd4b63c0":"def fill_rating(col):\n    if(pd.isnull(col)):\n        return 3.0\n    else:\n        return col","2eed1e8c":"train_data['previous_year_rating'] = train_data['previous_year_rating'].apply(fill_rating)","07b3dfd9":"sns.heatmap(train_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","98a2f6ff":"train_data.info()","fbfb407f":"dep = pd.get_dummies(train_data['department'],drop_first=True)\nedu = pd.get_dummies(train_data['education'],drop_first=True)\nreg = pd.get_dummies(train_data['region'],drop_first=True)\ngen = pd.get_dummies(train_data['gender'],drop_first=True)\nrec = pd.get_dummies(train_data['recruitment_channel'],drop_first=True)\ndep.head()","30f234d8":"train_data.drop(['department','region','education','gender','recruitment_channel'],axis=1,inplace=True)","c7e98b5e":"train_data = pd.concat([train_data,dep,reg,edu,gen,rec],axis=1)","258fa155":"train_data.head()","e8626c63":"from sklearn.model_selection import train_test_split","7eff0bc3":"X_train, X_test, y_train, y_test = train_test_split(train_data.drop('is_promoted',axis=1), \n                                                    train_data['is_promoted'], test_size=0.25, \n                                                    random_state=101)","ac2ec9e5":"from sklearn.linear_model import LogisticRegression","0e67f0ef":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","71754497":"predictions = logmodel.predict(X_test)","25050601":"from sklearn.metrics import classification_report","2840e9d4":"print(classification_report(y_test,predictions))","c5e41534":"from sklearn.metrics import confusion_matrix","b1b26314":"print(confusion_matrix(y_test,predictions))","a515a675":"from sklearn.tree import DecisionTreeClassifier","e7aac723":"dtree = DecisionTreeClassifier()","b942f83f":"dtree.fit(X_train,y_train)","2e63a013":"predictions = dtree.predict(X_test)","557694c6":"print(classification_report(y_test,predictions))","ad523da8":"print(confusion_matrix(y_test,predictions))\nprint(\"Training Accuracy for Decision tree classifier :\", dtree.score(X_train, y_train))","04bbb14e":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","91fb0e13":"logmodel2 = LogisticRegression()\nlogmodel2.fit(X_train,y_train)","21a0aadc":"predictions = logmodel2.predict(X_test)","64f926b5":"print(classification_report(y_test,predictions))","9d5ed485":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","e1deadbb":"rfc_pred = rfc.predict(X_test)","bf627d8b":"print(classification_report(y_test,rfc_pred))","8bbd3413":"print(\"Training Accuracy for Random Forest classifier :\", rfc.score(X_train, y_train))","6dbe5938":"print(\"Training Accuracy for logistic regression classifier :\", logmodel2.score(X_train, y_train))","516d2c91":"Great! Now that our data doesn't have any missing education values, let's try and do the same for previous_year_rating column also. Here we'll try to find the most popular rating and assign that to all missing values.","3fdf0e00":"Let's do some more analysis of our data by using these countlots. I have restricted myself to only 2 and rather given more time to the cleaning and improvement of our data.","d759b0c5":"Let's predict our test data and evaluate the precision of our model now.","5eccefcf":"Let's now look for the presence of any missing values in out data and try to clean it.","c18524ed":"Let's start by importing the important libraries for data analysis, data visualization, etc.","6330f83b":"We can see that the random forest classifier is slightly better that logistic regression. Let us test their training accuracy to check which is better.","540a740e":"Now let's check our data.","a40f4e0c":"Our main goal is to effectively classify our data. Feature employee_id has no contribution in this since the employee-id is not anyhow related to whether the person gets promotion or not. So we just remove that from our dataset.","5fc1e105":"We can see that the precision of is_promoted=1 is much better here. Now lets try to do the same for a random forest classifier.","9c5948d4":"Great! Now our dataset has no missing values. We'll now proceeed further to convert the categorical data formats to dummies format so that our models can use them more effectively. We will use the get_dummies() method of pandas for this purpose. (Dummies can increase the precision of a model a lot instead of normal object types)","2ada062c":"We can see that accuracy of Random forest classifier is only better. Thanks for reading upto here, if you liked my kernel please leave an upvote.","96b86775":"Let's check our dataset now.","17a86ef0":"We can see that 3.0 is the most common rating so we will insert 3.0 for all missing values in previous_year_rating.","792ddaf6":"Let's try first for logistic regression model and test.","2aa3230b":"Only 2 columns have missing data in them: education and previous_year_rating. For education, most of the entries are 'Bachelor's'. So it would make sense to actually replace all the missing values with 'Bachelor's' since they are having maximum probability. The function below will do this.","9266a181":"Now that we have all our dummies let's remove the object type columns and insert our dummies instead.","3e8eb011":"Let's try using an decision tree classifier and check its precision.","306ac286":"Let us now use a simple logistic regression model to classify our data. We will check its accuracy later on.","33ad7c45":"As we can see that the our models are able to predict is_promoted=0 quite accurately but not as much for is_promoted=1. This is probably because the dataset doesn't have as much is_promoted=1 cases as is_promoted=0. Let's try standard scaling our data and then check the precision.(training accuracy is good but not good enough for validation accuracy)","c5154eec":"Let's now do some exploratory data analysis by using the pairplot feature of seaborn to get a brief idea about our dataset and its features.(Warning: this can take some time)","7ea85f01":"Now our data is ready to be fit into our model, so let's first start by doing a train_test_split to validate our model.","f9cac146":"Let's now import our training data and store in a dataframe named train_data."}}