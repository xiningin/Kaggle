{"cell_type":{"740ef5d1":"code","c71087ae":"code","1013474c":"code","064071ab":"code","42a0b0df":"code","ff7ccc68":"code","83b38af7":"code","d9d639b5":"code","bd4317d1":"code","8a81c37a":"code","04fa127b":"code","d4fd60f0":"code","bfa24116":"code","e499e647":"code","e58f804f":"code","b6123720":"code","29fceef8":"code","08a787a6":"code","b0256c79":"code","92c3ba33":"code","ae14f1bf":"code","f9a3354c":"code","ed7c61e6":"code","fc629dbc":"code","a1a8df1a":"code","7465f6d5":"code","19e75dab":"code","a3cb15b8":"code","d8902fe0":"code","9c3bac0b":"code","3b3cc344":"code","027b20bc":"code","ee699c8e":"code","5c32dd9f":"code","f4fde7f7":"code","eb236468":"code","845ed82c":"code","8a67c17e":"code","56b00788":"code","15dd4c45":"code","6e179d7c":"code","421df4d5":"code","f7f93e18":"code","d8ff4fb1":"code","3d941167":"code","bea0d290":"code","88b3a5bc":"code","f474e4af":"code","cca8746b":"code","25da7a2f":"code","9af79ea8":"code","6b862e28":"code","988a2649":"code","212a8405":"code","7af66b05":"code","cc1071c7":"code","faa664b1":"code","66acb317":"code","7977cf75":"code","75ffc88e":"code","d5b1ff7a":"code","ae405366":"markdown","aa22909c":"markdown","8db6f0d9":"markdown","f54c7168":"markdown","4764a27a":"markdown","59e983d1":"markdown","52545f1d":"markdown","96a18dbf":"markdown","9bafafdf":"markdown","89b78e37":"markdown","7ed76c71":"markdown","521180b1":"markdown","8bce75fb":"markdown","70be2c8c":"markdown","0e85d40b":"markdown","818dd55f":"markdown","c88ae4b5":"markdown","7132eb5d":"markdown","c5bc8ccf":"markdown","06f06240":"markdown","46354fcf":"markdown","8bb9e45e":"markdown","5a99f723":"markdown","f266ade2":"markdown"},"source":{"740ef5d1":"# import library and data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, f1_score, make_scorer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv', engine='python')\ndf.head()","c71087ae":"X = df.drop(['Outcome'], axis=1)\ny = df['Outcome']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42, stratify=y)","1013474c":"print(x_train.info())\nprint('===============')\nprint(x_test.info())","064071ab":"# check if the outcome is balanced or not\nprint(np.sum(y_train) \/ len(y_train))\nprint(np.sum(y_test) \/ len(y_test))","42a0b0df":"x_train.describe()","ff7ccc68":"x_test.describe()","83b38af7":"# using IQR\ntrain_columns = list(x_train.columns)[:-1]\nfor col in train_columns:\n    q1 = x_train[col].quantile(q=0.25)\n    q3 = x_train[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (x_train[col] < (q1 - (iqr*1.5))) | (x_train[col] > (q3 + (iqr*1.5)))\n    outliers = len(x_train[condi])\n    print('train', col, outliers)","d9d639b5":"# using IQR\ntest_columns = list(x_test.columns)[:-1]\nfor col in test_columns:\n    q1 = x_test[col].quantile(q=0.25)\n    q3 = x_test[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (x_test[col] < (q1 - (iqr*1.5))) | (x_test[col] > (q3 + (iqr*1.5)))\n    outliers = len(x_test[condi])\n    print('train', col, outliers)","bd4317d1":"# help(pd.Series.replace)","8a81c37a":"# replace outliers with median\ntrain_columns = list(x_train.columns)[:-1]\nfor col in train_columns:\n    q1 = x_train[col].quantile(q=0.25)\n    q3 = x_train[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (x_train[col] < (q1 - (iqr*1.5))) | (x_train[col] > (q3 + (iqr*1.5)))\n    outliers = x_train[condi]\n    if len(outliers) > 0:\n        x_train[col].loc[outliers.index] = x_train[col].median()\n    \ntest_columns = list(x_test.columns)[:-1]\nfor col in test_columns:\n    q1 = x_test[col].quantile(q=0.25)\n    q3 = x_test[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (x_test[col] < (q1 - (iqr*1.5))) | (x_test[col] > (q3 + (iqr*1.5)))\n    outliers = x_test[condi]\n    if len(outliers) > 0:\n        x_test[col].loc[outliers.index] = x_test[col].median()","04fa127b":"x_train.describe()","d4fd60f0":"x_test.describe()","bfa24116":"# Standard Scaling for every column, in order to make all features distributed equivalently\nstd = StandardScaler()\nstd.fit(x_train)\nx_train_std = std.transform(x_train)\nx_test_std = std.transform(x_test)","e499e647":"# Make normalized sets, in order to compare the result with that from the standardized sets above\nminmax = MinMaxScaler()\nminmax.fit(x_train)\nx_train_minmax = minmax.transform(x_train)\nx_test_minmax = minmax.transform(x_test)","e58f804f":"# set models by default params\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nada = AdaBoostClassifier()\ngbc = GradientBoostingClassifier()\n\nmodels = [lr, dt, rf, ada, gbc]","b6123720":"# help(cross_val_score)","29fceef8":"# evaluation metrics\ndef roc_auc(y_true, y_pred):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    return auc(fpr, tpr)\n\ndef f1(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n\nroc_custom = make_scorer(roc_auc, greater_is_better=True, needs_proba=True)\nf1_custom = make_scorer(f1, greater_is_better=True, needs_proba=False)\n\nfor model in models:\n    name = model.__class__.__name__\n    accs = cross_val_score(model, X=x_train_std, y=y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    rocs = cross_val_score(model, X=x_train_std, y=y_train, cv=5, scoring=roc_custom, n_jobs=-1)\n    f1s = cross_val_score(model, X=x_train_std, y=y_train, cv=5, scoring=f1_custom, n_jobs=-1)\n    print('Model %s - Accuracy: %.4f, ROC AUC: %.4f, F1 score: %.4f' % (name, np.mean(accs), np.mean(rocs), np.mean(f1s)))","08a787a6":"# Take Random Forest and GBC as the final models, since each shows the highest accuracy and AUC, respectively\nrf_params = {'n_estimators': [50, 100, 200, 300], 'max_depth': [3, 5, 7, 9],\n            'min_samples_leaf': [2, 3, 5], 'max_features': ['auto'], 'n_jobs': [-1]}\n\nrf_grid = GridSearchCV(rf, param_grid=rf_params, cv=3, scoring='accuracy')\nrf_grid.fit(x_train_std, y_train)\n\nprint('RF grid Best params: {0}, best scores: {1}'.format(rf_grid.best_params_, round(rf_grid.best_score_, 4)))","b0256c79":"# Fit and Train\nrf_final = RandomForestClassifier(**rf_grid.best_params_)\nrf_final.fit(x_train_std, y_train)\nrf_pred = rf_final.predict(x_test_std)\nrf_pred_proba = rf_final.predict_proba(x_test_std)[:, 1]\n\naccuracy = np.sum(rf_pred == y_test) \/ y_test.shape[0]\nrocauc = roc_auc(y_test, rf_pred_proba)\nf1score = f1(y_test, rf_pred)\n\nprint('Accuracy: %.4f, AUC score: %.4f, F1 score: %.4f' % (accuracy, rocauc, f1score))","92c3ba33":"# evaluation metrics\ndef roc_auc(y_true, y_pred):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    return auc(fpr, tpr)\n\ndef f1(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n\nroc_custom = make_scorer(roc_auc, greater_is_better=True, needs_proba=True)\nf1_custom = make_scorer(f1, greater_is_better=True, needs_proba=False)\n\nfor model in models:\n    name = model.__class__.__name__\n    accs = cross_val_score(model, X=x_train_minmax, y=y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    rocs = cross_val_score(model, X=x_train_minmax, y=y_train, cv=5, scoring=roc_custom, n_jobs=-1)\n    f1s = cross_val_score(model, X=x_train_minmax, y=y_train, cv=5, scoring=f1_custom, n_jobs=-1)\n    print('Model %s - Accuracy: %.4f, ROC AUC: %.4f, F1 score: %.4f' % (name, np.mean(accs), np.mean(rocs), np.mean(f1s)))","ae14f1bf":"# Take Random Forest and GBC as the final models, since each shows the highest accuracy and AUC, respectively\nrf_params = {'n_estimators': [50, 100, 200, 300], 'max_depth': [3, 5, 7, 9],\n            'min_samples_leaf': [2, 3, 5], 'max_features': ['auto'], 'n_jobs': [-1]}\n\nrf_grid = GridSearchCV(rf, param_grid=rf_params, cv=3, scoring='accuracy')\nrf_grid.fit(x_train_minmax, y_train)\n\nprint('RF grid Best params: {0}, best scores: {1}'.format(rf_grid.best_params_, round(rf_grid.best_score_, 4)))","f9a3354c":"# Fit and Train\nrf_final = RandomForestClassifier(**rf_grid.best_params_)\nrf_final.fit(x_train_minmax, y_train)\nrf_pred = rf_final.predict(x_test_minmax)\nrf_pred_proba = rf_final.predict_proba(x_test_minmax)[:, 1]\n\naccuracy = np.sum(rf_pred == y_test) \/ y_test.shape[0]\nrocauc = roc_auc(y_test, rf_pred_proba)\nf1score = f1(y_test, rf_pred)\n\nprint('Accuracy: %.4f, AUC score: %.4f, F1 score: %.4f' % (accuracy, rocauc, f1score))","ed7c61e6":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv', engine='python')\ndf.head()","fc629dbc":"print(df.info())","a1a8df1a":"# check if the outcome is balanced or not\nprint(np.sum(df['Outcome']) \/ len(df))","7465f6d5":"df.describe()","19e75dab":"# using IQR\ncolumns = list(df.columns)[:-1]\nfor col in columns:\n    q1 = df[col].quantile(q=0.25)\n    q3 = df[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (df[col] < (q1 - (iqr*1.5))) | (df[col] > (q3 + (iqr*1.5)))\n    outliers = len(df[condi])\n    print(col, outliers)","a3cb15b8":"# replace outliers with median\ncolumns = list(df.columns)[:-1]\nfor col in columns:\n    q1 = df[col].quantile(q=0.25)\n    q3 = df[col].quantile(q=0.75)\n    iqr = q3 - q1\n    condi = (df[col] < (q1 - (iqr*1.5))) | (df[col] > (q3 + (iqr*1.5)))\n    outliers = df[condi]\n    if len(outliers) > 0:\n        df[col].loc[outliers.index] = df[col].median()","d8902fe0":"df.describe()","9c3bac0b":"X = df.drop(['Outcome'], axis=1)\ny = df['Outcome']","3b3cc344":"# Standard Scaling for every column, in order to make all features distributed equivalently\nstd = StandardScaler()\nX_std = std.fit_transform(X)","027b20bc":"# Make normalized sets, in order to compare the result with that from the standardized sets above\nminmax = MinMaxScaler()\nX_minmax = minmax.fit_transform(X)","ee699c8e":"# Split\nx_train_std, x_test_std, y_train_std, y_test_std = train_test_split(X_std, y, test_size=0.2, \n                                                                    shuffle=True, random_state=42, stratify=y)\nx_train_minmax, x_test_minmax, y_train_minmax, y_test_minmax = train_test_split(X_minmax, y, test_size=0.2, \n                                                                                shuffle=True, random_state=42, stratify=y)","5c32dd9f":"# set models by default params\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nada = AdaBoostClassifier()\ngbc = GradientBoostingClassifier()\n\nmodels = [lr, dt, rf, ada, gbc]","f4fde7f7":"# evaluation metrics\ndef roc_auc(y_true, y_pred):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    return auc(fpr, tpr)\n\ndef f1(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n\nroc_custom = make_scorer(roc_auc, greater_is_better=True, needs_proba=True)\nf1_custom = make_scorer(f1, greater_is_better=True, needs_proba=False)\n\nfor model in models:\n    name = model.__class__.__name__\n    accs = cross_val_score(model, X=x_train_std, y=y_train_std, cv=5, scoring='accuracy', n_jobs=-1)\n    rocs = cross_val_score(model, X=x_train_std, y=y_train_std, cv=5, scoring=roc_custom, n_jobs=-1)\n    f1s = cross_val_score(model, X=x_train_std, y=y_train_std, cv=5, scoring=f1_custom, n_jobs=-1)\n    print('Model %s - Accuracy: %.4f, ROC AUC: %.4f, F1 score: %.4f' % (name, np.mean(accs), np.mean(rocs), np.mean(f1s)))","eb236468":"# Take Random Forest and GBC as the final models, since each shows the highest accuracy and AUC, respectively\nrf_params = {'n_estimators': [50, 100, 200, 300], 'max_depth': [3, 5, 7, 9],\n            'min_samples_leaf': [2, 3, 5], 'max_features': ['auto'], 'n_jobs': [-1]}\n\nrf_grid = GridSearchCV(rf, param_grid=rf_params, cv=3, scoring='accuracy')\nrf_grid.fit(x_train_std, y_train_std)\n\nprint('RF grid Best params: {0}, best scores: {1}'.format(rf_grid.best_params_, round(rf_grid.best_score_, 4)))","845ed82c":"# Fit and Train\nrf_final = RandomForestClassifier(**rf_grid.best_params_)\nrf_final.fit(x_train_std, y_train_std)\nrf_pred = rf_final.predict(x_test_std)\nrf_pred_proba = rf_final.predict_proba(x_test_std)[:, 1]\n\naccuracy = np.sum(rf_pred == y_test_std) \/ y_test_std.shape[0]\nrocauc = roc_auc(y_test_std, rf_pred_proba)\nf1score = f1(y_test_std, rf_pred)\n\nprint('Accuracy: %.4f, AUC score: %.4f, F1 score: %.4f' % (accuracy, rocauc, f1score))","8a67c17e":"# evaluation metrics\ndef roc_auc(y_true, y_pred):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    return auc(fpr, tpr)\n\ndef f1(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n\nroc_custom = make_scorer(roc_auc, greater_is_better=True, needs_proba=True)\nf1_custom = make_scorer(f1, greater_is_better=True, needs_proba=False)\n\nfor model in models:\n    name = model.__class__.__name__\n    accs = cross_val_score(model, X=x_train_minmax, y=y_train_minmax, cv=5, scoring='accuracy', n_jobs=-1)\n    rocs = cross_val_score(model, X=x_train_minmax, y=y_train_minmax, cv=5, scoring=roc_custom, n_jobs=-1)\n    f1s = cross_val_score(model, X=x_train_minmax, y=y_train_minmax, cv=5, scoring=f1_custom, n_jobs=-1)\n    print('Model %s - Accuracy: %.4f, ROC AUC: %.4f, F1 score: %.4f' % (name, np.mean(accs), np.mean(rocs), np.mean(f1s)))","56b00788":"# Take Random Forest and GBC as the final models, since each shows the highest accuracy and AUC, respectively\nrf_params = {'n_estimators': [50, 100, 200, 300], 'max_depth': [3, 5, 7, 9],\n            'min_samples_leaf': [2, 3, 5], 'max_features': ['auto'], 'n_jobs': [-1]}\n\nrf_grid = GridSearchCV(rf, param_grid=rf_params, cv=3, scoring='accuracy')\nrf_grid.fit(x_train_minmax, y_train_minmax)\n\nprint('RF grid Best params: {0}, best scores: {1}'.format(rf_grid.best_params_, round(rf_grid.best_score_, 4)))","15dd4c45":"# Fit and Train\nrf_final = RandomForestClassifier(**rf_grid.best_params_)\nrf_final.fit(x_train_minmax, y_train_minmax)\nrf_pred = rf_final.predict(x_test_minmax)\nrf_pred_proba = rf_final.predict_proba(x_test_minmax)[:, 1]\n\naccuracy = np.sum(rf_pred == y_test_minmax) \/ y_test_minmax.shape[0]\nrocauc = roc_auc(y_test_minmax, rf_pred_proba)\nf1score = f1(y_test_minmax, rf_pred)\n\nprint('Accuracy: %.4f, AUC score: %.4f, F1 score: %.4f' % (accuracy, rocauc, f1score))","6e179d7c":"# # \uc2dc\ud5d8\ud658\uacbd \uc138\ud305 (\ucf54\ub4dc \ubcc0\uacbd X, \uc2dc\ud5d8\ud658\uacbd\uacfc \uc720\uc0ac\ud558\uac8c \ud30c\uc77c\uc77d\uae30 \uad6c\ud604)\n# import pandas as pd\n# df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n\n# from sklearn.model_selection import train_test_split\n# X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n# y_train = X_train['Outcome']\n# X_train = X_train.drop(columns=['Outcome'])\n# y_test = X_test['Outcome']\n# X_test = X_test.drop(columns=['Outcome'])\n\n# X_train.shape, y_train.shape, X_test.shape, y_test.shape","421df4d5":"# # \ub77c\uc774\ube0c\ub7ec\ub9ac\n# import pandas as pd","f7f93e18":"# # \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 (\uc0dd\ub7b5)\n# X_train.shape, y_train.shape, X_test.shape","d8ff4fb1":"# X_train.head()","3d941167":"# y_train.value_counts()","bea0d290":"# X_train.info()","88b3a5bc":"# X_train.isnull().sum()","f474e4af":"# X_test.isnull().sum()","cca8746b":"# X_train.describe()","25da7a2f":"# #\uc774\uc0c1\uce58 \ucc98\ub9ac\n# #Train\n# print('Glucose:',len(X_train[X_train['Glucose']==0]))\n# print('BloodPressure:',len(X_train[X_train['BloodPressure']==0]))\n# print('SkinThickness:',len(X_train[X_train['SkinThickness']==0]))\n# print('Insulin:',len(X_train[X_train['Insulin']==0]))\n# print('BMI:',len(X_train[X_train['BMI']==0]))","9af79ea8":"# #Test\n# print('Glucose:',len(X_test[X_test['Glucose']==0]))\n# print('BloodPressure:',len(X_test[X_test['BloodPressure']==0]))\n# print('SkinThickness:',len(X_test[X_test['SkinThickness']==0]))\n# print('Insulin:',len(X_test[X_test['Insulin']==0]))\n# print('BMI:',len(X_test[X_test['BMI']==0]))","6b862e28":"# # \ud3ec\ub3c4\ub2f9 \uc774\uc0c1\uce58 \uc0ad\uc81c\n# del_idx = X_train[(X_train['Glucose']==0)].index\n# del_idx","988a2649":"# print('Glucose \uc774\uc0c1\uce58 \uc0ad\uc81c \uc804 :', X_train.shape, y_train.shape)\n# X_train = X_train.drop(index=del_idx, axis=0)\n# y_train = y_train.drop(index=del_idx, axis=0)\n# print('Glucose \uc774\uc0c1\uce58 \uc0ad\uc81c \ud6c4 :', X_train.shape, y_train.shape)","212a8405":"# # \ud3ec\ub3c4\ub2f9\uc744 \uc81c\uc678\ud55c \uc774\uc0c1\uce58, \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4\n# cols = ['BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n# cols_mean = X_train[cols].mean()\n# X_train[cols].replace(0, cols_mean)","7af66b05":"# # \uc2a4\ucf00\uc77c\ub9c1\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.fit_transform(X_test)","cc1071c7":"# \uc2a4\ucf00\uc77c \ud6c4 \ub370\uc774\ud130 \ud655\uc778\n# pd.DataFrame(X_train_scaled, columns=X_train.columns).head()","faa664b1":"# from sklearn.svm import SVC\n# model = SVC(random_state=2022)\n# model.fit(X_train_scaled, y_train)\n# predictions = model.predict(X_test)","66acb317":"# round(model.score(X_train_scaled, y_train) * 100, 2) ","7977cf75":"# output = pd.DataFrame({'idx': y_test.index, 'Outcome': predictions})\n# output.head()","75ffc88e":"# # \uc218\ud5d8\ubc88\ud638.csv\ub85c \ucd9c\ub825\n# output.to_csv('1234567.csv', index=False)","d5b1ff7a":"# round(model.score(X_test, y_test) * 100, 2) # 60\uc810\ub300\ub85c \ub0ae\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90c","ae405366":"## \ub2f9\ub1e8\ubcd1 \uc5ec\ubd80 \ud310\ub2e8\n- \uc774\uc0c1\uce58 \ucc98\ub9ac (Glucose, BloodPressure, SkinThickness, Insulin, BMI\uac00 0\uc778 \uac12)","aa22909c":"# EDA","8db6f0d9":"## \ub370\uc774\ud130 \uc804\ucc98\ub9ac","f54c7168":"\ud2b8\ub808\uc778\uc5d0\ub9cc 0\uc774 \uc788\ub294 \ud3ec\ub3c4\ub2f9(Glucose)\ub294 \uc0ad\uc81c\n\ub098\uba38\uc9c0\ub294 \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4 \ud558\uc5ec \uc774\uc0c1\uce58\ub97c \ucc98\ub9ac\ud568","4764a27a":"## Standard Scaling","59e983d1":"### with standard scaled","52545f1d":"## Model Selection","96a18dbf":"### with standard scaled","9bafafdf":"## MinMax Scaling","89b78e37":"## outliers","7ed76c71":"### with minmax scaled","521180b1":"## MinMax Scaling","8bce75fb":"## EDA","70be2c8c":"# Scaling After Split","0e85d40b":"## outliers","818dd55f":"## \uacb0\uacfc \uccb4\uc810 (\uc218\ud5d8\uc790\ub294 \uc54c \uc218 \uc5c6\ub294 \ubd80\ubd84\uc784)","c88ae4b5":"## Standard Scaling","7132eb5d":"# EDA","c5bc8ccf":"## Model Selection","06f06240":"## [\ucc38\uace0]\uc791\uc5c5\ud6152 \ubb38\uad6c\n- \ucd9c\ub825\uc744 \uc6d0\ud558\uc2e4 \uacbd\uc6b0 print() \ud568\uc218 \ud65c\uc6a9\n- \uc608\uc2dc) print(df.head())\n- getcwd(), chdir() \ub4f1 \uc791\uc5c5 \ud3f4\ub354 \uc124\uc815 \ubd88\ud544\uc694\n- \ud30c\uc77c \uacbd\ub85c \uc0c1 \ub0b4\ubd80 \ub4dc\ub77c\uc774\ube0c \uacbd\ub85c(C: \ub4f1) \uc811\uadfc \ubd88\uac00\n\n### \ub370\uc774\ud130 \ud30c\uc77c \uc77d\uae30 \uc608\uc81c\n- import pandas as pd\n- X_test = pd.read_csv(\"data\/X_test.csv\")\n- X_train = pd.read_csv(\"data\/X_train.csv\")\n- y_train = pd.read_csv(\"data\/y_train.csv\")\n\n### \uc0ac\uc6a9\uc790 \ucf54\ub529\n\n### \ub2f5\uc548 \uc81c\ucd9c \ucc38\uace0\n- \uc544\ub798 \ucf54\ub4dc \uc608\uce21\ubcc0\uc218\uc640 \uc218\ud5d8\ubc88\ud638\ub97c \uac1c\uc778\ubcc4\ub85c \ubcc0\uacbd\ud558\uc5ec \ud65c\uc6a9\n- pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)","46354fcf":"# Scaling Before Split","8bb9e45e":"## \ub77c\uc774\ube0c\ub7ec\ub9ac \ubc0f \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30","5a99f723":"### with minmax scaled","f266ade2":"## Start"}}