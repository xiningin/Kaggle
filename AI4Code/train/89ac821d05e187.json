{"cell_type":{"5f362d46":"code","f72f076b":"code","d0c7e7e9":"code","e25b4766":"code","5115ccd7":"code","85970c6c":"code","0071184b":"code","120b75c3":"code","5e05a37e":"code","a5a1e8fc":"code","0b7491e1":"code","a7f6564b":"code","f44af19d":"code","86f2df7d":"code","d1e4b948":"code","069fae8f":"code","328f13ae":"code","9a92624f":"code","d3d75d43":"code","196a78f7":"code","f2cddf5b":"code","2b6aabfa":"code","16307346":"code","febb4e94":"code","553afec0":"code","6957fbea":"code","2a4de664":"code","d96e123b":"code","76d684a3":"code","d0f0c102":"code","f48fa290":"code","6db2261d":"code","663523ab":"code","9e26d995":"code","8a01024c":"code","7dabb93a":"code","dc1ec65c":"code","e516e1f0":"code","586e7063":"code","077da21f":"code","f9193cda":"code","3e5cc8e4":"code","240c006b":"code","4d1bf604":"code","e02f3de0":"code","f01375d4":"code","d42125c9":"code","21cb2b19":"code","be9ed84e":"code","50738ec2":"code","58d2e319":"code","ec21edf2":"code","07396e24":"code","33966cf7":"code","66acc714":"code","a54eea5a":"code","ab880341":"code","ea1e431c":"code","db036c93":"code","c1875c11":"code","8b8c7330":"code","b1d38c7e":"code","94fa3f72":"code","3acf4665":"code","7f9934c9":"code","6fa5b307":"code","db5b4942":"code","1cc204c1":"code","952b48d6":"code","3aa828b6":"code","74068fcb":"code","84e3ce63":"code","3cbde86e":"code","51043803":"code","e52ae6b6":"code","385d0f28":"code","643ee191":"code","ffdbaa68":"code","07c75ebe":"code","75ced47f":"code","7b8fd778":"code","0cb1575b":"code","e0775843":"code","e6b9be93":"code","a3f4ab9e":"code","83ee1ace":"code","c91b9969":"code","15280fa8":"code","3ed6fc79":"code","ee73f972":"code","d23097a7":"code","69aaa65f":"code","d733038f":"code","aca12641":"code","c8ff41fe":"code","99ca3faa":"code","48fad01e":"code","d9c68b8d":"code","f0396d0c":"code","e5ebd8d5":"code","b7d46daf":"code","59afe56c":"code","8d8aa085":"code","33bbd7c4":"code","7c6add12":"code","80ce516c":"code","c6a7ad08":"code","b1778a7a":"markdown","245e278c":"markdown","cb05f2cd":"markdown","7005f7bf":"markdown","d69aa4bf":"markdown","dfb4bb21":"markdown","2cd287d2":"markdown","efa56b96":"markdown","41c29be6":"markdown","8d2f8479":"markdown","c5476365":"markdown","9674cd96":"markdown","41c08327":"markdown","e097f5c4":"markdown","b514c152":"markdown","64ced9bd":"markdown","d5d5714f":"markdown","8c1d1ba9":"markdown","9fd36da6":"markdown","c10923a3":"markdown","34e114c3":"markdown","ab774173":"markdown","8467ae91":"markdown","3548581a":"markdown","7050815b":"markdown","d6afca1f":"markdown","c05684b1":"markdown","c3df9eff":"markdown","ca852d34":"markdown","8bfb7e99":"markdown","d92e5aaa":"markdown","0643fe6f":"markdown","df270644":"markdown","071b6f00":"markdown","16f56e74":"markdown","7a44b2a5":"markdown","e782d9d8":"markdown","3d1b5beb":"markdown","ff21c433":"markdown","968768b2":"markdown","59c14db3":"markdown","31ac70ad":"markdown","18367483":"markdown","762408ae":"markdown","a4f2a027":"markdown","e0e66b7f":"markdown","6cf26401":"markdown","e2794e8a":"markdown","ea8cd3d4":"markdown","e472a325":"markdown","65f298b9":"markdown","62ae585a":"markdown","25b0e199":"markdown","7a3cb51c":"markdown","68323225":"markdown","d2a2364a":"markdown","ed843243":"markdown","48594560":"markdown","47ffb1e4":"markdown","c788982a":"markdown","cfbfd7a4":"markdown","9bc30892":"markdown","8009c73b":"markdown","c85b665b":"markdown","0a29e20b":"markdown"},"source":{"5f362d46":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport string\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom itertools import tee\n\nfrom IPython.display import display\n\nimport numpy as np\nfrom scipy import stats\nfrom scipy.sparse import hstack as sparse_hstack\nimport pandas as pd\n\nimport matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns; sns.set()\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport eli5","f72f076b":"# statistic methods\ndef tconfint(sample, alpha=0.05):\n    '''Confidence interval based on Student t distribution.'''\n    mean = np.mean(sample)\n    S = np.std(sample, ddof=1)\n    n = len(sample)\n\n    t = stats.t.ppf(1 - alpha \/ 2, n - 1)\n    left_boundary = mean - t * S \/ np.sqrt(n)\n    right_boundary = mean + t * S \/ np.sqrt(n)\n\n    return left_boundary, right_boundary\n\n\ndef tconfint_diff(sample1, sample2, alpha=0.05):\n    '''Confidence interval based on Student t distribution for\n    the difference in means of two samples.'''\n    mean1 = np.mean(sample1)\n    mean2 = np.mean(sample2)\n    s1 = np.std(sample1, ddof=1)\n    s2 = np.std(sample2, ddof=1)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    sem1 = np.var(sample1) \/ (n1 - 1)\n    sem2 = np.var(sample2) \/ (n2 - 1)\n    semsum = sem1 + sem2\n    z1 = (sem1 \/ semsum) ** 2 \/ (n1 - 1)\n    z2 = (sem2 \/ semsum) ** 2 \/ (n2 - 1)\n    dof = 1 \/ (z1 + z2)\n\n    t = stats.t.ppf(1 - alpha \/ 2, dof)\n    left_boundary = (mean1 - mean2) - t * np.sqrt((s1 ** 2) \/ n1 + (s2 ** 2) \/ n2)\n    right_boundary = (mean1 - mean2) + t * np.sqrt((s1 ** 2) \/ n1 + (s2 ** 2) \/ n2)\n\n    return left_boundary, right_boundary\n\n\ndef bootstrap_statint(sample, stat=np.mean, n_samples=5000, alpha=0.05):\n    '''Statistical interval for a `stat` of a `sample` calculation\n    using bootstrap sampling mechanism. `stat` is a numpy function\n    like np.mean, np.std, np.median, np.max, np.min, etc.'''\n    indices = np.random.randint(0, len(sample), (n_samples, len(sample)))\n    samples = sample[indices]\n\n    stat_scores = stat(samples, axis=1)\n    boundaries = np.percentile(stat_scores, [100 * alpha \/ 2, 100 * (1 - alpha \/ 2)])\n    return boundaries\n\n\ndef bootstrap_statint_diff(sample1, sample2, stat=np.mean, n_samples=5000, alpha=0.05):\n    '''Statistical interval for a difference in `stat` of two samples\n    calculation using bootstrap sampling mechanism. `stat` is a numpy\n    function like np.mean, np.std, np.median, np.max, np.min, etc.'''\n    indices1 = np.random.randint(0, len(sample1), (n_samples, len(sample1)))\n    indices2 = np.random.randint(0, len(sample2), (n_samples, len(sample2)))\n    samples1 = sample1[indices1]\n    samples2 = sample2[indices2]\n\n    stat_scores1 = stat(samples1, axis=1)\n    stat_scores2 = stat(samples2, axis=1)\n    stat_scores_diff = stat_scores1 - stat_scores2\n    boundaries = np.percentile(stat_scores_diff, [100 * alpha \/ 2, 100 * (1 - alpha \/ 2)])\n    return boundaries\n\n\ndef proportion_confint(sample, alpha=0.05):\n    '''Wilson\\'s \u0441onfidence interval for a proportion.'''\n    p = np.mean(sample)\n    n = len(sample)\n\n    z = stats.norm.ppf(1 - alpha \/ 2)\n    left_boundary = 1 \/ (1 + z ** 2 \/ n) * (p + z ** 2 \/ (2 * n) \\\n                                            - z * np.sqrt(p * (1 - p) \/ n + z ** 2 \/ (4 * n ** 2)))\n    right_boundary = 1 \/ (1 + z ** 2 \/ n) * (p + z ** 2 \/ (2 * n) \\\n                                             + z * np.sqrt(p * (1 - p) \/ n + z ** 2 \/ (4 * n ** 2)))\n\n    return left_boundary, right_boundary\n\n\ndef proportions_diff_confint_ind(sample1, sample2, alpha=0.05):\n    '''Confidence interval for the difference of two independent proportions.'''\n    z = stats.norm.ppf(1 - alpha \/ 2)\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1) \/ n1 + p2 * (1 - p2) \/ n2)\n    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1) \/ n1 + p2 * (1 - p2) \/ n2)\n\n    return left_boundary, right_boundary\n\n\ndef permutation_test_ind(sample1, sample2, max_permutations=None, alternative='two-sided'):\n    '''Permutation test for two independent samples.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    t_stat = np.mean(sample1) - np.mean(sample2)\n\n    joined_sample = np.hstack((sample1, sample2))\n    n1 = len(sample1)\n    n = len(joined_sample)\n\n    if max_permutations:\n        index = list(range(n))\n        indices = set([tuple(index)])\n        for _ in range(max_permutations - 1):\n            np.random.shuffle(index)\n            indices.add(tuple(index))\n\n        indices = [(index[:n1], index[n1:]) for index in indices]\n    else:\n        indices = [(list(index), list(filter(lambda i: i not in index, range(n)))) \\\n                    for index in itertools.combinations(range(n), n1)]\n\n    zero_distr = [joined_sample[list(i[0])].mean() - joined_sample[list(i[1])].mean() \\\n                  for i in indices]\n\n    if alternative == 'two-sided':\n        p_value = sum([abs(x) >= abs(t_stat) for x in zero_distr]) \/ len(zero_distr)\n\n    if alternative == 'less':\n        p_value = sum([x <= t_stat for x in zero_distr]) \/ len(zero_distr)\n\n    if alternative == 'greater':\n        p_value = sum([x >= t_stat for x in zero_distr]) \/ len(zero_distr)\n\n    return t_stat, p_value\n\n\ndef proportions_ztest_ind(sample1, sample2, alternative='two-sided'):\n    '''Z-test for two independent proportions.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    P = (p1 * n1 + p2 * n2) \/ (n1 + n2)\n    z_stat = (p1 - p2) \/ np.sqrt(P * (1 - P) * (1 \/ n1 + 1 \/ n2))\n\n    if alternative == 'two-sided':\n        p_value = 2 * (1 - stats.norm.cdf(np.abs(z_stat)))\n\n    if alternative == 'less':\n        p_value = stats.norm.cdf(z_stat)\n\n    if alternative == 'greater':\n        p_value = 1 - stats.norm.cdf(z_stat)\n\n    return z_stat, p_value\n\n\ndef cramers_v(contingency_table):\n    '''Cramer\\'s V coefficient.'''\n    n = np.sum(contingency_table)\n    ct_nrows, ct_ncols = contingency_table.shape\n    if n < 40 or np.sum(contingency_table < 5) \/ (ct_nrows * ct_ncols) > 0.2:\n        raise ValueError('Contingency table isn\\'t suitable for Cramers\\'s V coefficient calculation.')\n\n    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n    corr = np.sqrt(chi2 \/ (n * (min(ct_nrows, ct_ncols) - 1)))\n    return corr, p_value","d0c7e7e9":"data = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv')\ndata.head(10)","e25b4766":"data.info()","5115ccd7":"bin_features = ['telecommuting', 'has_company_logo', 'has_questions']\ncat_features = ['department', 'employment_type', 'required_experience', \n                'required_education', 'industry', 'function']\n\ntext_features = ['title', 'company_profile', 'description', 'requirements', 'benefits']\ncomplex_features = ['location', 'salary_range']","85970c6c":"data.drop('job_id', axis=1, inplace=True)","0071184b":"data[text_features].head()","120b75c3":"for feature_name in text_features[1:]:\n    unspec_feature_name = f'{feature_name}_specified'\n    data[unspec_feature_name] = (~data[feature_name].isna()).astype('int')\n    bin_features += [unspec_feature_name]","5e05a37e":"data.head()[text_features + bin_features[-4:]]","a5a1e8fc":"for feature_name in text_features[1:]:\n    data[feature_name].fillna('', inplace=True)","0b7491e1":"# nltk.download('stopwords')\n# nltk.download('punkt')","a7f6564b":"nltk_supported_languages = ['hungarian', 'swedish', 'kazakh', 'norwegian',\n                            'finnish', 'arabic', 'indonesian', 'portuguese',\n                            'turkish', 'azerbaijani', 'slovene', 'spanish',\n                            'danish', 'nepali', 'romanian', 'greek', 'dutch',\n                            'tajik', 'german', 'english', 'russian',\n                            'french', 'italian']","f44af19d":"# stop words list\nstop_words = set(stopwords.words(nltk_supported_languages))","86f2df7d":"# stemmer\nporter = PorterStemmer()","d1e4b948":"def preprocess_texts(texts):\n    '''Returns a list of clean and word-stemmed strings.'''\n    preprocessed_texts = []\n    for text in tqdm(texts):\n        # punctuation marks cleaning\n        text = ''.join([sym.lower() for sym in text if sym.isalpha() or sym == ' '])\n        \n        # tokenization\n        tokenized_text = word_tokenize(text)\n        \n        # stop words cleaning\n        tokenized_text_wout_sw = [word for word in tokenized_text if word not in stop_words]\n        \n        # stemming\n        tokenized_text_wout_sw_stem = [porter.stem(word) for word in tokenized_text_wout_sw]\n        \n        # saving result\n        preprocessed_texts += [' '.join(tokenized_text_wout_sw_stem)]\n    \n    return preprocessed_texts","069fae8f":"%%time\nfor feature_name in text_features:\n    data[feature_name] = preprocess_texts(data[feature_name])\n\ndata[text_features].head()","328f13ae":"location = data['location'].copy()\nlocation.head(15)","9a92624f":"location_splitted = list(location.str.split(', ').values)\nlocation_splitted[:15]","d3d75d43":"for loc_ind, loc in enumerate(location_splitted):\n    if loc is np.nan:\n        location_splitted[loc_ind] = ['Unpecified'] * 3\n    else:\n        for el_ind, el in enumerate(loc):\n            if el == '':\n                loc[el_ind] = 'Unpecified'","196a78f7":"location_splitted[:15]","f2cddf5b":"any([len(loc) > 3 for loc in location_splitted])","2b6aabfa":"any([len(loc) < 3 for loc in location_splitted])","16307346":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) > 3:\n        print(loc_ind, loc)","febb4e94":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) < 3:\n        print(loc_ind, loc)","553afec0":"location_splitted[0] is list","6957fbea":"type(location_splitted[0])","2a4de664":"location_splitted = list(map(lambda loc: list(loc), location_splitted))","d96e123b":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) > 3:\n        location_splitted[loc_ind] = loc[:2] + [', '.join(loc[2:])]\n    if len(loc) < 3:\n        location_splitted[loc_ind] += ['Unpecified'] * 2","76d684a3":"any([len(loc) != 3 for loc in location_splitted])","d0f0c102":"data_location = pd.DataFrame(location_splitted, columns=['country', 'state', 'city'])\ndata_location.head(15)","f48fa290":"# complementing the list of categorical features\ncat_features += ['country', 'state', 'city']","6db2261d":"data = pd.concat([data, data_location], axis=1)\ndata.head()","663523ab":"data.drop('location', axis=1, inplace=True)","9e26d995":"salary_range = data.salary_range.copy()\nsalary_range.head(15)","8a01024c":"salary_range.fillna('0-0', inplace=True)","7dabb93a":"salary_range_sep = list(salary_range.str.split('-').values)\nsalary_range_sep[:5]","dc1ec65c":"for range_ind, s_range in enumerate(salary_range_sep):\n    if len(s_range) < 2 or len(s_range) > 2:\n        print(range_ind, s_range)","e516e1f0":"salary_range_sep[5538] = ['40000', '40000']","586e7063":"error_range_inds = []\nfor range_ind, s_range in enumerate(salary_range_sep):\n    min_value, max_value = s_range\n    if not min_value.isdigit() or not max_value.isdigit():\n        print(range_ind, (min_value, max_value))\n        error_range_inds += [range_ind]","077da21f":"for range_ind in error_range_inds:\n    salary_range_sep[range_ind] = ['0', '0']","f9193cda":"data_salary_range = pd.DataFrame(np.array(salary_range_sep, dtype='int64'), \n                                 columns=['min_salary', 'max_salary'])\ndata_salary_range.head(15)","3e5cc8e4":"data_salary_range['salary_specified'] = ((data_salary_range.min_salary != 0) | \n                                         (data_salary_range.max_salary != 0)).astype('int64')\ndata_salary_range.head(15)","240c006b":"# creating the list of numerical features names and complementing the list of binary ones\nnum_features = ['min_salary', 'max_salary']\nbin_features += ['salary_specified']","4d1bf604":"data = pd.concat([data, data_salary_range], axis=1)\ndata.head()","e02f3de0":"data.drop('salary_range', axis=1, inplace=True)","f01375d4":"data.info()","d42125c9":"data.fillna('Unspecified', inplace=True)\ndata.info()","21cb2b19":"plt.figure(figsize=(6, 4))\nax = sns.countplot(data.fraudulent)\nplt.title('The distribution of the target feature (fraudulent)')\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x()+0.33, p.get_height()))\n\nplt.show()","be9ed84e":"fig = plt.figure(figsize=(25, 30))\nouter = gridspec.GridSpec(4, 2, wspace=0.2, hspace=0.1)\n\nfor feature_ind, feature_name in enumerate(bin_features):\n    inner = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=outer[feature_ind], \n                                             wspace=0.5, hspace=0.7)\n    \n    ax = plt.Subplot(fig, outer[feature_ind])\n    ax.set_title(f'The distribution of fraudulent for each {feature_name}\\'s class')\n    ax.axis('off')\n    fig.add_subplot(ax)\n    \n    for feature_class in [0, 1]:\n        ax = plt.Subplot(fig, inner[feature_class])\n        feature_cl_vc = data[data[feature_name] == feature_class].fraudulent.value_counts().sort_index()\n        if len(feature_cl_vc) == 2:\n            feature_cl_vc.index = ['non-fraudulent', 'fraudulent']\n        else:\n            feature_cl_vc.index = ['fraudulent']\n        \n        ax.pie(feature_cl_vc.values, labels=feature_cl_vc.index, autopct='%1.1f%%')\n        ax.set_title(f'{feature_name} = {feature_class}')\n        fig.add_subplot(ax)\n\nfig.suptitle('Distributions of fraudulent for the binary features')\nfig.subplots_adjust(top=0.95)\nfig.show()","50738ec2":"cont_table = pd.crosstab(data.fraudulent, data.description_specified)\nprint('Contingency table (fraudulent x description_specified):')\ndisplay(cont_table)","58d2e319":"def show_feature1_x_feature2_info(feature_name1, feature_name2, figsize=(12, 4), is_binxcat=False):\n    '''Shows info about a combination of two binary\/categorical features.'''\n    cont_table = pd.crosstab(data[feature_name1], data[feature_name2]).fillna(0)\n    prop_table = pd.pivot_table(data, index=feature_name1, columns=feature_name2, \n                                values='fraudulent', aggfunc=np.mean).fillna(0)\n    \n    corr, p = cramers_v(cont_table.values)\n    \n    if is_binxcat:\n        fig, axes = plt.subplots(2, 1, figsize=figsize, sharex=True)\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n    \n    sns.heatmap(cont_table, annot=True, fmt='d', ax=axes[0])\n    axes[0].set_title(f'Contingency table:')\n    if is_binxcat:\n        axes[0].set_xlabel('')\n    \n    sns.heatmap(prop_table, annot=True, ax=axes[1])\n    axes[1].set_title(f'Proportion of fraudulent posts:')\n    \n    fig_title = f'{feature_name1} x {feature_name2} (Correlation: {round(corr, 4)}, p-value: {round(p, 4)}))'\n    if is_binxcat:\n        fig.suptitle(fig_title, y=1.05, x=0.45)\n    else:\n        fig.suptitle(fig_title, y=1.05)\n    \n    fig.show()","ec21edf2":"show_feature1_x_feature2_info('has_company_logo', 'company_profile_specified')","07396e24":"show_feature1_x_feature2_info('benefits_specified', 'has_questions')","33966cf7":"show_feature1_x_feature2_info('telecommuting', 'has_questions')","66acc714":"show_feature1_x_feature2_info('telecommuting', 'benefits_specified')","a54eea5a":"show_feature1_x_feature2_info('benefits_specified', 'salary_specified')","ab880341":"round_confint = lambda confint: list(map(lambda lim: round(lim, 4), confint))","ea1e431c":"def print_stats_for_proportions(feature_name):\n    fraudulent_0 = data[data[feature_name] == 0].fraudulent\n    fraudulent_1 = data[data[feature_name] == 1].fraudulent\n    \n    prop_0 = round(np.mean(fraudulent_0), 4)\n    prop_1 = round(np.mean(fraudulent_1), 4)\n    prop_0_confint = round_confint(proportion_confint(fraudulent_0))\n    prop_1_confint = round_confint(proportion_confint(fraudulent_1))\n    \n    bigger_prop, smaller_prop = (fraudulent_0, fraudulent_1) if prop_0 > prop_1 else (fraudulent_1, fraudulent_0)\n    props_diff = round(np.mean(bigger_prop) - np.mean(smaller_prop), 4)\n    props_diff_confint = round_confint(proportions_diff_confint_ind(bigger_prop, smaller_prop))\n    z_test_p = proportions_ztest_ind(fraudulent_0, fraudulent_1)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Proportion of fraudulent posts for 0: {prop_0}')\n    print(f'Proportion of fraudulent posts for 1: {prop_1}')\n    print(f'Confidence interval for the proportion of fraudulent posts for 0: {prop_0_confint}')\n    print(f'Confidence interval for the proportion of fraudulent posts for 1: {prop_1_confint}')\n    print(f'Difference in these proportions: {props_diff}')\n    print(f'Confidence interval for the difference in these proportions: {props_diff_confint}')\n    print(f'Z-test result: {z_test_p} (p-value)')","db036c93":"print_stats_for_proportions('has_questions')","c1875c11":"round((0.0331 \/ 0.0284) * 100, 1)","8b8c7330":"print_stats_for_proportions('salary_specified')","b1d38c7e":"round((0.0273 \/ 0.0427) * 100, 1)","94fa3f72":"for feature_name in cat_features:\n    print(f'Count of {feature_name}\\'s unique values: {data[feature_name].unique().shape[0]}')","3acf4665":"def plot_cat_feature_distribution(feature_name):\n    '''Makes a plotly chart with categorical feature\\'s distribution.'''\n    feature_0f = data[data.fraudulent == 0][feature_name].value_counts()\n    feature_1f = data[data.fraudulent == 1][feature_name].value_counts()\n    \n    fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]], \n                        subplot_titles=['non-fraudulent', 'fraudulent'])\n    fig.add_trace(go.Pie(labels=feature_0f.index, \n                         values=feature_0f.values), \n                  row=1, col=1)\n    fig.add_trace(go.Pie(labels=feature_1f.index, \n                         values=feature_1f.values), \n                  row=1, col=2)\n    \n    fig.update_layout(title_text=f'The distribution of {feature_name}')\n    fig.show()","7f9934c9":"plot_cat_feature_distribution('employment_type')","6fa5b307":"plot_cat_feature_distribution('required_experience')","db5b4942":"plot_cat_feature_distribution('required_education')","1cc204c1":"func_meanfr_pt = pd.pivot_table(data, index='function', values='fraudulent', \n                                aggfunc=np.mean).sort_values(by='fraudulent', ascending=False)\nfunc_meanfr_pt.columns = ['Proportion of fraudulent posts']\nprint('Top-15 function\\'s values with the biggest proportions of fraudulent posts:')\ndisplay(func_meanfr_pt.head(15))","952b48d6":"country_meanfr_pt = pd.pivot_table(data, index='country', values='fraudulent', \n                                   aggfunc=np.mean).sort_values(by='fraudulent', ascending=False)\ncountry_meanfr_pt.columns = ['Proportion of fraudulent posts']\nprint('Top-15 country\\'s values with the biggest proportions of fraudulent posts:')\ndisplay(country_meanfr_pt.head(15))","3aa828b6":"show_feature1_x_feature2_info('employment_type', 'required_experience', (18, 5))","74068fcb":"show_feature1_x_feature2_info('benefits_specified', 'required_education', (14, 4.5), True)","84e3ce63":"show_feature1_x_feature2_info('has_questions', 'required_education', (14, 4.5), True)","3cbde86e":"fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n\nfor ind, feature_name in enumerate(num_features):\n    sns.boxplot(y=feature_name, x='fraudulent', data=data[data.salary_specified == 1], ax=axes[ind])\n    axes[ind].set_ylim([-1e4, 2e5])\n    axes[ind].set_xticklabels(['non-fraudulent', 'fraudulent'])\n    axes[ind].set_title(f'Distributions of specified {feature_name}')\n\nfig.suptitle('Distributions of min_salary and max_salary')\nfig.show()","51043803":"diff_salary = data[data.salary_specified == 1]['max_salary'] - data[data.salary_specified == 1]['min_salary']","e52ae6b6":"plt.figure(figsize=(5, 5))\nsns.boxplot(y=diff_salary, x='fraudulent', data=data[data.salary_specified == 1])\nplt.ylim([-1e4, 1e5])\nplt.xticks([0, 1], ['non-fraudulent', 'fraudulent'])\nplt.ylabel('Difference')\nplt.title('Distribution of difference between\\n min and max salary')\nplt.show()","385d0f28":"specified_salaries = data[data.salary_specified == 1][num_features]\nspecified_salaries['difference'] = diff_salary\nspecified_salaries['fraudulent'] = data.fraudulent\nspecified_salaries.head()","643ee191":"np.sum(np.unique(specified_salaries.min_salary, return_counts=True)[1] > 10)","ffdbaa68":"np.sum(np.unique(specified_salaries.max_salary, return_counts=True)[1] > 10)","07c75ebe":"def print_stats_for_salary(feature_name):\n    '''Calculates statistics for fraudulent and non-fraudulent salary-feature.'''\n    np.random.seed(42)\n    feature_0f = specified_salaries[specified_salaries.fraudulent == 0][feature_name]\n    feature_1f = specified_salaries[specified_salaries.fraudulent == 1][feature_name]\n    \n    med_0f = np.median(feature_0f)\n    med_1f = np.median(feature_1f)\n    med_0f_confint = bootstrap_statint(feature_0f.values, stat=np.median)\n    med_1f_confint = bootstrap_statint(feature_1f.values, stat=np.median)\n    \n    bigger_med, smaller_med = (feature_0f, feature_1f) if med_0f > med_1f else (feature_1f, feature_0f)\n    med_diff = np.median(bigger_med) - np.median(smaller_med)\n    med_diff_confint = bootstrap_statint_diff(bigger_med.values, smaller_med.values, stat=np.median)\n    perm_test_p = permutation_test_ind(feature_0f, feature_1f, max_permutations=5000)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Median of {feature_name} in non-fraudulent posts: {med_0f}')\n    print(f'Median of {feature_name} in fraudulent posts:     {med_1f}')\n    print(f'Statistical interval for the median of {feature_name} in non-fraudulent posts: {med_0f_confint}')\n    print(f'Statistical interval for the median of {feature_name} in fraudulent posts:     {med_1f_confint}')\n    print(f'Difference in these medians: {med_diff}')\n    print(f'Statistical interval for the difference in these medians: {med_diff_confint}')\n    print(f'Permutation test result: {perm_test_p} (p-value)')","75ced47f":"print_stats_for_salary('min_salary')","7b8fd778":"print_stats_for_salary('max_salary')","0cb1575b":"print_stats_for_salary('difference')","e0775843":"fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n\ntext_features_gen = iter(text_features)\n\nfor row in range(3):\n    for col in range(2):\n        try:\n            feature_name = next(text_features_gen)\n        except StopIteration:\n            break\n        \n        if feature_name == 'title':\n            feature_values_0f = data[(data.fraudulent == 0)][feature_name].astype(str)\n            feature_values_1f = data[(data.fraudulent == 1)][feature_name].astype(str)\n        else:\n            feature_values_0f = data[(data.fraudulent == 0) & data[f'{feature_name}_specified']][feature_name].astype(str)\n            feature_values_1f = data[(data.fraudulent == 1) & data[f'{feature_name}_specified']][feature_name].astype(str)\n\n        fv_0f_len = feature_values_0f.str.split(' ').apply(len)\n        fv_1f_len = feature_values_1f.str.split(' ').apply(len)\n        \n        sns.distplot(fv_0f_len, label='non-fraudulent', ax=axes[row, col])\n        sns.distplot(fv_1f_len, label='fraudulent', ax=axes[row, col])\n        axes[row, col].set_title(f'The distribution of {feature_name}\\'s count of words')\n        axes[row, col].legend()\n        \nfig.suptitle('Distributions of count of words for each text feature', y=0.92)\nfig.show()","e6b9be93":"def print_stats_for_texts(feature_name):\n    '''Calculates statistics for fraudulent and non-fraudulent count of words in feature\\'s texts.'''\n    if feature_name == 'title':\n        feature_values_0f = data[(data.fraudulent == 0)][feature_name].astype(str)\n        feature_values_1f = data[(data.fraudulent == 1)][feature_name].astype(str)\n    else:\n        feature_values_0f = data[(data.fraudulent == 0) & data[f'{feature_name}_specified']][feature_name].astype(str)\n        feature_values_1f = data[(data.fraudulent == 1) & data[f'{feature_name}_specified']][feature_name].astype(str)\n    \n    lens_0f = feature_values_0f.str.split(' ').apply(len)\n    lens_1f = feature_values_1f.str.split(' ').apply(len)\n    \n    mean_lens_0f = round(np.mean(lens_0f), 4)\n    mean_lens_1f = round(np.mean(lens_1f), 4)\n    mean_lens_0f_confint = round_confint(tconfint(lens_0f.values))\n    mean_lens_1f_confint = round_confint(tconfint(lens_1f.values))\n    \n    bigger_mean, smaller_mean = (lens_0f, lens_1f) if mean_lens_0f > mean_lens_1f else (lens_1f, lens_0f)\n    mean_diff = round(np.mean(bigger_mean) - np.mean(smaller_mean), 4)\n    \n    mean_diff_confint = round_confint(tconfint_diff(bigger_mean.values, smaller_mean.values))\n    perm_test_p = permutation_test_ind(lens_0f, lens_1f, max_permutations=5000)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Mean of {feature_name}\\'s count of words in non-fraudulent posts: {mean_lens_0f}')\n    print(f'Mean of {feature_name}\\'s count of words in fraudulent posts:     {mean_lens_1f}')\n    print(f'Confidence interval for the mean of {feature_name}\\'s count of words in non-fraudulent posts: {mean_lens_0f_confint}')\n    print(f'Confidence interval for the mean of {feature_name}\\'s count of words in fraudulent posts:     {mean_lens_1f_confint}')\n    print(f'Difference in these means: {mean_diff}')\n    print(f'Confidence interval for the difference in these means: {mean_diff_confint}')\n    print(f'Permutation test result: {perm_test_p} (p-value)')","a3f4ab9e":"for feature_name in text_features:\n    print_stats_for_texts(feature_name)\n    print()","83ee1ace":"data['company_profile_count_of_words'] = data['company_profile'].astype(str).str.split(' ').apply(len)\ndata['requirements_count_of_words'] = data['requirements'].astype(str).str.split(' ').apply(len)\ndata.head()[['company_profile_count_of_words', 'requirements_count_of_words']]","c91b9969":"num_features += ['company_profile_count_of_words', 'requirements_count_of_words']","15280fa8":"data_1f = data[data.fraudulent == 1]","3ed6fc79":"original_data = data.copy()\ndata = pd.concat([data] + [data_1f] * 7, axis=0)","ee73f972":"plt.figure(figsize=(6, 4))\nax = sns.countplot(data.fraudulent)\nplt.title('The distribution of the target feature (fraudulent)')\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x()+0.33, p.get_height()))\n\nplt.show()","d23097a7":"skf = StratifiedKFold(n_splits=4, random_state=42)","69aaa65f":"X, y = data.drop('fraudulent', axis=1), data.fraudulent","d733038f":"num_transformer = Pipeline(steps=[('scaler', StandardScaler())])\ncat_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\ntext_transformer = Pipeline(steps=[('tfidf', TfidfVectorizer(ngram_range=(1, 2)))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features),\n        *[(feature_name, text_transformer, feature_name) \n          for feature_name in text_features]\n    ]\n)","aca12641":"log_reg_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('classifier', LogisticRegression())])","c8ff41fe":"%%time\ncv_scores = cross_validate(log_reg_pipe, X, y, return_train_score=True, cv=skf, \n                           scoring=['accuracy', 'roc_auc'], n_jobs=-1)\n\nprint(f'Accuracy on train part: {cv_scores[\"train_accuracy\"]}, mean: {cv_scores[\"train_accuracy\"].mean()}')\nprint(f'Accuracy on test part:  {cv_scores[\"test_accuracy\"]}, mean: {cv_scores[\"test_accuracy\"].mean()}')\nprint(f'ROC AUC on train part: {cv_scores[\"train_roc_auc\"]}, mean: {cv_scores[\"train_roc_auc\"].mean()}')\nprint(f'ROC AUC on test part:  {cv_scores[\"test_roc_auc\"]}, mean: {cv_scores[\"test_roc_auc\"].mean()}')","99ca3faa":"%%time\nfeature_names = num_features.copy()\n\nnum_features_scaled = StandardScaler().fit_transform(data[num_features])\nX = num_features_scaled\n\nfeature_names += bin_features\nX = np.hstack([X, data[bin_features]])\n\n\nfor feature_name in cat_features:\n    encoder = OneHotEncoder()\n    encoded_feature = encoder.fit_transform(data[feature_name].values.reshape(-1, 1))\n    \n    X = sparse_hstack([X, encoded_feature])\n    f_names = list(map(lambda cat: f'{feature_name}:{cat}', encoder.categories_[0]))\n    feature_names += f_names\n\nfor feature_name in text_features:\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n    vectorized_feature = vectorizer.fit_transform(data[feature_name])\n    \n    X = sparse_hstack([X, vectorized_feature])\n    sorted_phrases = [pair[0] for pair in list(sorted(vectorizer.vocabulary_.items(), \n                                                      key=lambda pair: pair[1]))]\n    f_names = list(map(lambda phrase: f'{feature_name}:{phrase}', sorted_phrases))\n    feature_names += f_names","48fad01e":"X.shape[1], len(feature_names)","d9c68b8d":"log_reg = LogisticRegression(random_state=42, n_jobs=-1).fit(X, y)","f0396d0c":"eli5.explain_weights(log_reg, feature_names=feature_names, top=(30, 30))","e5ebd8d5":"original_data[original_data.country == 'MY'].fraudulent.value_counts().sort_index()","b7d46daf":"original_data[original_data.country == 'GR'].fraudulent.value_counts().sort_index()","59afe56c":"original_data[original_data.industry == 'Accounting'].fraudulent.value_counts().sort_index()","8d8aa085":"original_data[original_data.industry == 'Internet'].fraudulent.value_counts().sort_index()","33bbd7c4":"original_data[original_data.city == 'london'].fraudulent.value_counts().sort_index()","7c6add12":"original_data[original_data.city == 'London'].fraudulent.value_counts().sort_index()","80ce516c":"original_data[original_data.city == 'chicago'].fraudulent.value_counts().sort_index()","c6a7ad08":"original_data[original_data.city == 'Chicago'].fraudulent.value_counts().sort_index()","b1778a7a":"### Complex features\n\n#### `location`\n\nThe main structure of `location`'s values is `Country, State, City`:","245e278c":"Look's like those who post fraudulent posts more often don't have company logo\/company profile and more often indicate in their posts that there will be no screening questions at the survey. Also fraudulent post writers less often specify salary and more often offer remote employment.","cb05f2cd":"Adding a column for marking specified salary ranges:","7005f7bf":"\\- so we will use Permutation test:","d69aa4bf":"Firstly let's increase the count of 1 `fraudulent` records in the dataset using oversampling:","dfb4bb21":"Let's check how some of categorical and binary features may be related:","2cd287d2":"Let's create two new numerical features for `company_profile`'s and `requirements`'s count of words (because their distributions are probably different, as well as `title`'s, but the maximum difference in means of 0.4 of a word not very noticeable in terms of logic):","efa56b96":"Filling NA-values with an empty string:","41c29be6":"Let's look at the distribution of the target feature:","8d2f8479":"## Analysis","c5476365":"The largest one of these probabilities of being fraudulent have posts with specified benefits and without announcement of any questions during interview.","9674cd96":"I think everything is pretty good though... \n\nThe `country` value with the biggest positive weight is `MY`, which is top-1 country in \"Proportion of fraudulent posts\" table (for the `country` feature, value of proportion - 0.571429). The country with the smallest count of fraudulent posts is `GR`:","41c08327":"Classes are not balanced, so we have to use oversampling\/undersampling and calculate \\[not-only-accuracy\\] metrics (ROC AUC, etc.) to estimate our models in the future.\n\nDistributions of `fraudulent` for the binary features:","e097f5c4":"Done! Now move on to complex features.","b514c152":"Adding indicators of specified values (there is no NA-values in `title`):","64ced9bd":"The largest one of these probabilities of being fraudulent have posts that offer remote work and promise an interview without questions.","d5d5714f":"Filling in the missing values with a `0-0` value (in the future we will create an indicator for the unspecified data):","8c1d1ba9":"#### `salary_range`\n\nNow we need to do something with the `salary_range` column because we can't work with it as with a categorical feature:","9fd36da6":"Now let's add new features to the dataset table and remove the old one from it:","c10923a3":"Filling in missing values (we will use `'Unspecified'` word to replace NA-values for all categorical features):","34e114c3":"There are many NA-values in the table and there are features that have to be preprocessed for further usage. Let's save names of each feature-type and work on complex ones.","ab774173":"Using info about weights we can figure out which values of a feature are associated with the biggest and the smallest proportions of fraudulent posts:","8467ae91":"There is a difference in medians, let's calculate them and some descriptive statistics (there is no sense in compairing means because there are too many outliers here):","3548581a":"## Feature preparation\n\n### Text features\n\nFeatures that describe textual components of a job post:","7050815b":"Not all gained values are numerical:","d6afca1f":"Somebody specified some kind of dates instead of salary range, let's replace these values with a `['0', '0']`:","c05684b1":"The largest one of these probabilities of being fraudulent have posts that offer remote work and don't specify any benefits.","c3df9eff":"Most of the problems arose due to the refinement of the position at the third element using a comma. Let's resolve it simply (and supplement values in which only the country is specified):","ca852d34":"And we can also conclude that fraudulent post writers often start writing `London` (and many other city names) using the lowercase letter:","8bfb7e99":"And splitting them:","d92e5aaa":"Numerical features have to be scaled, categorical features have to be transformed into sets of binary ones and text features have to be vectorized (I'm using TF-IDF method):","0643fe6f":"Alright:","df270644":"### Other features\n\nWe still have NA-values in other columns:","071b6f00":"Cross-validation splitter:","16f56e74":"Now let's look at the numerical features (salary info):","7a44b2a5":"Checking for unusual values:","e782d9d8":"Saving results into a `pandas.DataFrame` object:","3d1b5beb":"## First look\n\nThe dataset:","ff21c433":"But there are some troubles:","968768b2":"Dividing features and targets:","59c14db3":"But the rest features are categorical so we will fill the missing values using `'Unspecified'` value:","31ac70ad":"The distributions of fraudulent for `description_specified` feature looks strange because there is only one record in the table that have an unspecified description and it's fraudulent:","18367483":"The chance to meet a fraudulent post among posts that don't announce any questions is at least 116.5% higher than the chance to meet it among posts that do it.","762408ae":"When you browse posts with specified salaries it's at least 63.9% higher chance to meet a fraudulent one than when you check posts without specified salaries.","a4f2a027":"Let's divide and extract these elements. We will use them as a categorical features in the future.","e0e66b7f":"Those who write fake job posts often offer slightly lower salaries... Let's look at differencies between min and max salaries:","6cf26401":"The largest one of these probabilities of being fraudulent have posts without company's profile and logo, the smallest - posts that have them.","e2794e8a":"# Fake Job Postings\n\nSorry for my English please \/\\\n\n## Data\n\nFeatures list (`Variable`: Definition):\n\n- `job_id`: Unique Job ID<br>\n- `title`: The title of the job ad entry<br>\n- `location`: Geographical location of the job ad<br>\n- `department`: Corporate department (e.g. sales)<br>\n- `salary_range`: Indicative salary range (e.g. $50,000-60,000)<br>\n- `company_profile`: A brief company description<br>\n- `description`: The details description of the job ad<br>\n- `requirements`: Enlisted requirements for the job opening<br>\n- `benefits`: Enlisted offered benefits by the employer<br>\n- `telecommuting`: True for telecommuting positions<br>\n- `has_company_logo`: True if company logo is present<br>\n- `has_questions`: True if screening questions are present<br>\n- `employment_type`: Full-type, Part-time, Contract, etc<br>\n- `required_experience`: Executive, Entry level, Intern, etc<br>\n- `required_education`: Doctorate, Master\u2019s Degree, Bachelor, etc<br>\n- `industry`: Automotive, IT, Health care, Real estate, etc<br>\n- `function`: Consulting, Engineering, Research, Sales etc<br>\n- `fraudulent`: target - Classification attribute","ea8cd3d4":"We can't use Mann\u2013Whitney U test for distributions comparison because -","e472a325":"I think we can stop at this point. Maybe the dataset wasn't assembled correctly or there are too few records in it... Or maybe I've done something wrong, but the data is too easy to classify. Or maybe everything is alright? Let's check model's weights:","65f298b9":"And saving results into the original table:","62ae585a":"## Transforming features and fitting models","25b0e199":"To resolve these problems a strange move have to be undertaken due to this oddity:","7a3cb51c":"And fixing it:","68323225":"Now we are ready to fit models.","d2a2364a":"And drop `job_id`, it's useless.","ed843243":"We'll start with a logistic regression model (default parameters):","48594560":"Let's look at the categorical features:","47ffb1e4":"Let's check how some of the binary features may be related:","c788982a":"Not all values of `location` were described in 3 elements. Let's look at unusual values:","cfbfd7a4":"Differencies between `min_salary`'s medians of 0 and 1 `fraudulent` groups are much more significant than between `max_salary`'s and `different`'s ones. But there isn't difference in distributions for any of them. \n\nLet's compare mean count of words and distributions of lenghts of each textual feature for groups of 0 and 1 `fraudulent`:","9bc30892":"Now we have to clean our texts from punctuation marks and stop-words, and apply stemming:","8009c73b":"It will be difficult to make any kind of plot for every categorical feature due to the number of classes in most of them. Let's plot ones that have fewer amount of classes (`plotly` charts fit better for this):","c85b665b":"The largest one of these probabilities of being fraudulent have posts that include specified benefits and specified salaries.\n\nLet's compare proportions of fraudulent posts for `has_questions` and `salary_specified` classes (0 and 1):","0a29e20b":"Weights x feature names:"}}