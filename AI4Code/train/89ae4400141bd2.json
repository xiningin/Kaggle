{"cell_type":{"6df465ac":"code","806272d4":"code","694cd9ef":"code","dd26ed56":"code","913acbbf":"code","760260df":"code","4575857f":"code","a43220e4":"code","1650be8d":"code","12b3ba51":"code","6ac8c280":"code","96832f8a":"code","d865c792":"code","99c995fe":"code","de112ebe":"code","8c4ddfb6":"code","2e9eedd3":"code","a5e83979":"code","fda9079d":"code","31f79023":"code","93d10961":"code","8e689795":"code","d3ea7bb6":"code","edfbcf03":"code","4d283bf5":"code","9cecb867":"code","a8853d7f":"code","b2c3ade1":"code","35c92bca":"code","61e38ebe":"code","0066a485":"code","8d885f95":"code","30ec9730":"code","e61fb30c":"code","14389b3b":"code","20d3445f":"code","ae91af12":"code","3cd4bf27":"code","26eb413d":"code","98a3d356":"code","70dc1c68":"code","ced96561":"code","aa57fdb6":"code","c4d370f6":"code","10f6341b":"code","2f56d71a":"code","50b02b1d":"code","dd1c64c3":"code","fea9365c":"code","0c1efef1":"code","fccd5b5b":"code","f1512d99":"code","e518eaa1":"code","cc04a1c5":"code","d64eeb69":"code","34b514eb":"code","a05a9e66":"code","5375984e":"code","5bd98e04":"code","b878ad4f":"code","a3a6551c":"code","6a8cab37":"code","3b602d70":"code","aa128e7e":"code","bb89f7d2":"code","b0bef8a2":"code","7f76b0d8":"code","84f481f2":"code","b79b6a72":"code","62be369b":"code","ac0e6033":"code","479a5d33":"code","1a4c0989":"code","935aaaf9":"code","b09fd53b":"code","db22f203":"code","72012dde":"code","8dd0ab85":"code","1f3ad724":"code","2c6653aa":"code","07530aca":"code","2eeb790a":"code","92346b22":"code","700afb04":"code","b7d2c3b9":"code","24ac5621":"code","f3392866":"code","11055dd5":"markdown","26fe7b67":"markdown","083dd5fd":"markdown","c04d79f4":"markdown","d75c75a5":"markdown","128829bd":"markdown","3816d330":"markdown","9678cc96":"markdown","4f496150":"markdown","433b437a":"markdown","198135b5":"markdown","b1595154":"markdown","2d76ca45":"markdown","621f4aba":"markdown","7ef5f27a":"markdown","88552e44":"markdown","0730637d":"markdown","69c519eb":"markdown","7426e3b9":"markdown","84363eb9":"markdown","86b59a92":"markdown","fc2b4569":"markdown","fce4d1ed":"markdown","f29b3106":"markdown","c3e11b2f":"markdown","f40f70ac":"markdown","5f4c200d":"markdown","b1b8ad04":"markdown","9204a981":"markdown","2d089345":"markdown","e23a01c3":"markdown","1327e342":"markdown","78672f23":"markdown","48a56f95":"markdown","13aa225d":"markdown","f322b1e9":"markdown","00093730":"markdown","cde81c62":"markdown","b0a1c5e6":"markdown","ca30a515":"markdown","0159c842":"markdown","b910f565":"markdown","5e53f33b":"markdown","6259899b":"markdown","ca1c6eda":"markdown","fa51ae13":"markdown","e2715a1a":"markdown","bba33bbd":"markdown","cd11c4bf":"markdown","21d007d2":"markdown","c1533f68":"markdown","594696c9":"markdown","30722ccb":"markdown","23f7f207":"markdown","5ccdb7ec":"markdown","514b46a8":"markdown","b3c4878c":"markdown","7d83341c":"markdown"},"source":{"6df465ac":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","806272d4":"# list folder data\n!echo $PWD\n!ls -ahl\n!ls \/kaggle\/input\/yeast-uvb-radiation-induced-damage","694cd9ef":"data_file = \"\/kaggle\/input\/yeast-uvb-radiation-induced-damage\/ScUVB-rc.xlsx\"","dd26ed56":"!python --version","913acbbf":"!pip install numpy==1.19.5 #1.19.5\n!pip show numpy","760260df":"!pip install tensorflow==2.4.2 #2.4.1\n!pip show tensorflow","4575857f":"# system\nimport os\nimport random as rn\n\n# numpy\nimport numpy as np\n\n# tensorflow\nimport tensorflow as tf\n\nRANDOM_STATE = 1\n\n# ---\n\nos.environ['PYTHONHASHSEED'] = '0'\n\n# Setting the seed for numpy-generated random numbers\nnp.random.seed(RANDOM_STATE)\n\n# Setting the seed for python random numbers\nrn.seed(RANDOM_STATE)\n\n# Setting the graph-level random seed.\ntf.random.set_seed(RANDOM_STATE)","a43220e4":"!pip install matplotlib==3.4.3 #3.4.3\n!pip show matplotlib","1650be8d":"# matplotlib\nimport matplotlib\n\n# Customize matplotlib\nmatplotlib.rcParams.update(\n    {\n        'text.usetex': False,\n        'font.family': 'stixgeneral',\n        'mathtext.fontset': 'stix',\n    }\n)","12b3ba51":"!pip install pandas==1.3.2 #1.3.2\n!pip show pandas","6ac8c280":"!pip install openpyxl==3.0.9\n!pip show openpyxl","96832f8a":"# pandas\nimport pandas as pd\n\n# data_file = \"\/kaggle\/input\/yeast-uvb-radiation-induced-damage\/ScUVB-rc.xlsx\"\n\nall_data_columns=[\"GLCMContrast\", \"FractalLacunarity\", \"CellNucleusAssesment\",\n                  \"CellMembraneAssesment\", \"Treatment\"]\n\ncells = pd.read_excel(data_file, sheet_name = 0,\n                      names=all_data_columns,\n                      keep_default_na=False)\n\n# # The last column, \"Treatment\", indicates whether the cell is damaged (1) or not (0).\n# treated_cells = pd.read_excel(data_file, sheet_name = 1,\n#                       names=all_data_columns)\nuntreated_cells = cells[0:1000]\ntreated_cells = cells[1000:2000]","d865c792":"train_df = cells\ntarget_column = 'Treatment'\n# predictors = list(set(list(train_df.columns))-set([target_column]))\npredictors=[\"GLCMContrast\", \"FractalLacunarity\", \"CellNucleusAssesment\", \"CellMembraneAssesment\"]\nprint(predictors)\n\nprint('Shape size: ', train_df.shape)\ntrain_df.describe() # train_df.describe(include='all')","99c995fe":"!pip install seaborn==0.11.2 #0.11.2\n!pip show seaborn","de112ebe":"# seaborn\nimport seaborn as sns\nsns.pairplot(train_df, hue=target_column)","8c4ddfb6":"!pip install scipy==1.7.1 #1.7.1\n!pip show scipy","2e9eedd3":"import scipy.stats as stats\n\nttest_GLCMContrast = stats.ttest_ind(train_df['GLCMContrast'][train_df[target_column] == 0],\n                                     train_df['GLCMContrast'][train_df[target_column] == 1])\n\nttest_FractalLacunarity = stats.ttest_ind(train_df['FractalLacunarity'][train_df[target_column] == 0],\n                                          train_df['FractalLacunarity'][train_df[target_column] == 1])\n\nttest_CellNucleusAssesment = stats.ttest_ind(train_df['CellNucleusAssesment'][train_df[target_column] == 0],\n                                             train_df['CellNucleusAssesment'][train_df[target_column] == 1])\n\nttest_CellMembraneAssesment = stats.ttest_ind(train_df['CellMembraneAssesment'][train_df[target_column] == 0],\n                                              train_df['CellMembraneAssesment'][train_df[target_column] == 1])\n\nprint('GLCMContrast: ' + str(ttest_GLCMContrast))\nprint('FractalLacunarity: ' + str(ttest_FractalLacunarity))\nprint('CellNucleusAssesment: ' + str(ttest_CellNucleusAssesment))\nprint('CellMembraneAssesment: ' + str(ttest_CellMembraneAssesment))\n\nprint('\\nMax statistic: ' + str(max(ttest_GLCMContrast.statistic,\n      ttest_FractalLacunarity.statistic,\n      ttest_CellNucleusAssesment.statistic,\n      ttest_CellMembraneAssesment.statistic)))\n\nprint('Max pvalue: ' + str(max(ttest_GLCMContrast.pvalue,\n      ttest_FractalLacunarity.pvalue,\n      ttest_CellNucleusAssesment.pvalue,\n      ttest_CellMembraneAssesment.pvalue)))\n","a5e83979":"# Show Na values\n# print(data[data.isnull().any(axis=1)])\nprint(train_df[train_df.isna().any(axis=1)])\n\n# Show rows with Na value and empty string in all cells\ntrain_df[train_df.astype(bool).any(axis=1) == False]","fda9079d":"# Remove Na values and empty strings\n\n# Option 1\ntrain_df.replace(\"\", np.nan,inplace=True)\ntrain_df.dropna(inplace=True)\n\n# Replace Na values with zero\n# train_df = train_df.replace(np.nan, 0, regex=True)","31f79023":"type(train_df)\nprint(train_df.columns)\nprint(train_df.dtypes)","93d10961":"sns.boxplot(x=train_df['GLCMContrast'])","8e689795":"sns.boxplot(x=train_df['FractalLacunarity'])","d3ea7bb6":"sns.boxplot(x=train_df['CellNucleusAssesment'])","edfbcf03":"sns.boxplot(x=train_df['CellMembraneAssesment'])","4d283bf5":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n\nax[0].scatter(train_df['GLCMContrast'], train_df['Treatment'])\nax[0].set_xlabel('GLCMContrast')\nax[0].set_ylabel('Treatment')\n\nax[1].scatter(train_df['FractalLacunarity'], train_df['Treatment'])\nax[1].set_xlabel('FractalLacunarity')\nax[1].set_ylabel('Treatment')\n\nplt.show()","9cecb867":"from scipy import stats\nimport numpy as np\n\nz = np.abs(stats.zscore(train_df))\nprint(z)","a8853d7f":"z_threshold = 3\noutliers = np.where(z > z_threshold)\nprint(outliers)","b2c3ade1":"len(outliers[0])","35c92bca":"# for i in range (0, len(outliers[0])):\n#   print(z[outliers[0][i]][outliers[1][i]])","61e38ebe":"train_df_clean = train_df[(z < z_threshold).all(axis=1)]","0066a485":"z_clean = np.abs(stats.zscore(train_df_clean))\nprint(np.where(z_clean > z_threshold))","8d885f95":"sns.pairplot(train_df_clean, hue=target_column)","30ec9730":"# sklearn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n## Convert the column value of the dataframe as floats\ntrain_df.all().values.astype(float)\n\n## Create a min max processing object\nmin_max_scaler = MinMaxScaler()\n\n## Scale dataframe columns\ntrain_df_clean_scaled = pd.DataFrame(min_max_scaler.fit_transform(train_df_clean[predictors]),\n                          #  index = train_df[predictors].index,\n                           columns = predictors)\n\n## Show scaled columns\nprint(train_df_clean_scaled)","e61fb30c":"train_df_clean_scaled.describe()","14389b3b":"from sklearn.model_selection import train_test_split\n\nX = train_df_clean_scaled[predictors].values\ny = train_df_clean[target_column].values\n\nprint(X.shape)\nprint(y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=RANDOM_STATE)","20d3445f":"print(X_train.shape)\nprint(y_train.shape)\n\nprint(X_test.shape)\nprint(y_test.shape)","ae91af12":"X_train_y = np.concatenate([X_train, y_train.reshape(y_train.shape[0],1)], axis=1)\nX_test_y = np.concatenate([X_test, y_test.reshape(y_test.shape[0],1)], axis=1)\n\nprint(X_train_y.shape)\nprint(X_test_y.shape)","3cd4bf27":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(5,5))\ntrain_df[target_column].value_counts().plot.pie(ylabel=' ', autopct = '%0.1f%%')\nplt.title(f'0 - Untreated (negative)\\n 1 - Treated (positive)', size=14, c='green')\nplt.tight_layout()\nplt.show() ","26eb413d":"train_df_clean_scaled[predictors]","98a3d356":"# keras\nfrom tensorflow.keras.utils import to_categorical\n\n# One hot encode outputs\ny_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)\n\ncount_classes = y_test_cat.shape[1]\nprint(count_classes)","70dc1c68":"# keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nclassifier = Sequential()\n\n# First Hidden Layer\nclassifier.add(Dense(3, activation='relu', input_dim=4))\n\n# Output Layer\nclassifier.add(Dense(2, activation='softmax'))\n\nprint(classifier.summary())\n\n# Compile the model\nclassifier.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","ced96561":"# !pip install keras==2.6.0 #\n# !pip show keras","aa57fdb6":"# keras\n# from keras import callbacks\nfrom tensorflow.keras import callbacks \n\n# build the model\nearly_stopping = callbacks.EarlyStopping(monitor =\"accuracy\", # \"accuracy\"\n                                        min_delta=0, patience = 44, #50\n                                        verbose=1,\n                                        restore_best_weights = True)\n#  validation_split=0.25,\ntrain_history = classifier.fit(X_train, y_train_cat,\n                               batch_size=None, epochs = 214, #200\n                               validation_data=(X_test, y_test_cat),\n                               callbacks =[early_stopping])","c4d370f6":"acc = train_history.history['accuracy']\nval_acc = train_history.history['val_accuracy']\nloss = train_history.history['loss']\nval_loss = train_history.history['val_loss']\nepochs=range(len(acc))","10f6341b":"plt.plot(epochs, acc, label='accuracy', color='blue')\nplt.plot(epochs, val_acc, label='val_accuracy', color='green')\nplt.plot(epochs, loss, label='loss', color='red')\nplt.plot(epochs, val_loss, label='val_loss', color='yellow')\nplt.xlabel('Epoch')\nplt.legend()\nplt.title(\"Training and Validation Accuracy\")","2f56d71a":"pred_train= classifier.predict(X_train)\nscores = classifier.evaluate(X_train, y_train_cat, verbose=0)\nprint('Accuracy on training data: {}%\\nError on training data: {}'.format(scores[1], 1 - scores[1]))  \n\npred_test= classifier.predict(X_test)\nscores2 = classifier.evaluate(X_test, y_test_cat, verbose=0)\nprint('Accuracy on test data: {}% \\nError on test data: {}'.format(scores2[1], 1 - scores2[1]))","50b02b1d":"# sklearn\nfrom sklearn.metrics import confusion_matrix\n\n# Get the confusion matrix\ny_test_single_mlp = np.argmax(y_test_cat, axis=1)\npredictions_rounded_single_mlp = np.argmax(pred_test, axis=1)\n# print(y_test_cat)\n# print(y_test_single)\n# print(predictions_rounded_single)\n\ncf_matrix = confusion_matrix(y_true=y_test_single_mlp, y_pred=predictions_rounded_single_mlp)\ntn, fp, fn, tp = cf_matrix.ravel()\nprint(f\"True negative: {tn}\\nFalse positive: {fp}\\nFalse negative: {fn}\\nTrue positive: {tp}\\n\")\nprint(cf_matrix)","dd1c64c3":"%matplotlib inline\n\n# https:\/\/howtolearnmachinelearning.com\/code-snippets\/python-confusion-matrix\/\ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        percent=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    Percent can be applied by setting `percent=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n      if percent:\n        cm = np.round(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], 2)*100\n      \n      cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n      print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"black\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True')\n    plt.xlabel('Predicted')","fea9365c":"# itertools\nimport itertools\n\ncm_plot_labels = ['No damage', 'With damage']\nplot_confusion_matrix(cm=cf_matrix, classes=cm_plot_labels, normalize= True,\n                      percent=True, title='Confusion Matrix')","0c1efef1":"real_y_test=y_test_single_mlp[:10]\n\npred_no=[]\npred_yes=[]\n\nfor p in pred_test:\n  pred_no.append(p[0])\n  pred_yes.append(p[1])\n\npred_no_show=pred_no[:10]\npred_yes_show=pred_yes[:10]\n\nwidth = 0.3\nind = np.arange(10)\nplt.xticks(ind)\n# plt.yticks([])\nplt.bar(np.arange(len(real_y_test)), real_y_test, width=width, label='Real damaged', color='blue')\nplt.bar(np.arange(len(pred_no_show))+ width, pred_no_show, width=width, label='No', color='red')\nplt.bar(np.arange(len(pred_yes_show)) + width, pred_yes_show,\n        bottom=pred_no_show, width=width, label='Yes', color='green')\nplt.ylabel('Probability ')\nplt.xlabel('Samples')\nplt.legend()\nplt.title(\"Real and predicted values\")\nplt.show()","fccd5b5b":"from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n                            f1_score, roc_auc_score, classification_report\n\ndef get_metrics(targets, predictions):\n    \"\"\"\n    Calculate metrics.\n    \"\"\"\n    \n    # Accuracy\n    acc = accuracy_score(targets, predictions)\n    \n    # Class weighted accuracy\n    conf = confusion_matrix(y_true=targets, y_pred=predictions)\n    wacc = conf.diagonal()\/conf.sum(axis=1)\n\n    # Precision\n    precision = precision_score(targets, predictions)\n\n    # Recall\n    recall = recall_score(targets, predictions)\n\n    # F1-Score\n    f1 = f1_score(targets, predictions)\n\n    # AUC\n    roc_auc = roc_auc_score(targets, predictions)\n\n    # Report\n    report = classification_report(targets, predictions)\n    \n    # Print\n    print(\"Accuracy:\", acc)\n    print(\"WACC:\", wacc)\n    print(\"Mean WACC:\", np.mean(wacc))\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1-Score:\", f1)\n    print(\"AUC:\", roc_auc)\n    # print(\"Mean Auc:\", np.mean(roc_auc))    \n    print(\"\\n--- Clasification report ---\\n\", report)\n\nget_metrics(y_test_single_mlp, predictions_rounded_single_mlp)","f1512d99":"from sklearn.metrics import roc_curve\n\ndef plot_roc_curve(fpr, tpr):\n    \"\"\"\n    Plot the ROC Curve.\n    \"\"\"\n    \n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfpr, tpr, thresholds = roc_curve(y_test_single_mlp, predictions_rounded_single_mlp)\nplot_roc_curve(fpr, tpr)\n\nauc = roc_auc_score(y_test_single_mlp, predictions_rounded_single_mlp)\nprint('AUC: %.2f' % auc)","e518eaa1":"from sklearn.linear_model import LogisticRegression\n\n# Instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n\n# Fit the model with data\nlogreg.fit(X_train, y_train)\n\n# Prediction\ny_pred_blr = logreg.predict(X_test)","cc04a1c5":"from sklearn.metrics import confusion_matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred_blr)\ncf_matrix","d64eeb69":"plot_confusion_matrix(cm=cf_matrix, classes=cm_plot_labels, normalize= True,\n                      percent=True, title='Confusion Matrix')","34b514eb":"get_metrics(y_test, y_pred_blr)","a05a9e66":"y_pred_proba_blr = logreg.predict_proba(X_test)[::, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_blr)\nplot_roc_curve(fpr, tpr)","5375984e":"!cat \/proc\/cpuinfo","5bd98e04":"!pip install chefboost==0.0.13\n!pip show chefboost","b878ad4f":"from chefboost import Chefboost as chef","a3a6551c":"# Train -> DataFrame\nX_train_y_df = pd.DataFrame(X_train_y, columns = all_data_columns)\nX_train_y_df.rename(columns={'Treatment': 'Decision'}, inplace=True)\nX_train_y_df['Decision'].replace({0: 'no', 1: 'yes'}, inplace=True) # Regression -> Classification\n\n# Test -> DataFrame\nX_test_y_df = pd.DataFrame(X_test_y, columns = all_data_columns)\nX_test_y_df.rename(columns={'Treatment': 'Decision'}, inplace=True)\nX_test_y_df['Decision'].replace({0: 'no', 1: 'yes'}, inplace=True) # Regression -> Classification\n\ntest_instance = X_train_y_df.iloc[0]\n# del test_instance['Treatment']\nprint(test_instance)\n\nprint(X_train_y_df)\nprint(X_test_y_df)","6a8cab37":"chaid_config = {'algorithm': 'CHAID', 'enableParallelism': True, 'num_cores': 2}\nchaid_tree_model = chef.fit(X_train_y_df, config = chaid_config)","3b602d70":"%%bash\n\n# ls outputs\/rules\nmkdir -p outputs\/rules\/saved\n\\cp outputs\/rules\/rules.py outputs\/rules\/saved\/chaid_tree_model_rules.py","aa128e7e":"chef.save_model(chaid_tree_model, \"saved\/chaid_tree_model.pkl\")","bb89f7d2":"chef.predict(chaid_tree_model, test_instance)","b0bef8a2":"chef.evaluate(chaid_tree_model, X_test_y_df, task=\"test\")","7f76b0d8":"chaid_tree_model_rules = \"outputs\/rules\/rules.py\"\nchaid_tree_model_fi = chef.feature_importance(chaid_tree_model_rules).set_index(\"feature\")\nchaid_tree_model_fi.plot(kind=\"barh\", title=\"Feature Importance\")","84f481f2":"chaid_tree_model_fi = chef.feature_importance(chaid_tree_model_rules)\nprint(chaid_tree_model_fi)","b79b6a72":"cart_config = {'algorithm': 'CART', 'enableParallelism': True, 'num_cores': 2}\ncart_tree_model = chef.fit(X_train_y_df, config = cart_config)","62be369b":"%%bash\n\n# ls outputs\/rules\nmkdir -p outputs\/rules\/saved\n\\cp outputs\/rules\/rules.py outputs\/rules\/saved\/cart_tree_model_rules.py","ac0e6033":"chef.save_model(cart_tree_model, \"saved\/cart_tree_model.pkl\")","479a5d33":"chef.predict(cart_tree_model, test_instance)","1a4c0989":"chef.evaluate(cart_tree_model, X_test_y_df, task=\"test\")","935aaaf9":"cart_tree_model_rules = \"outputs\/rules\/rules.py\"\ncart_tree_model_fi = chef.feature_importance(cart_tree_model_rules).set_index(\"feature\")\ncart_tree_model_fi.plot(kind=\"barh\", title=\"Feature Importance\")","b09fd53b":"cart_tree_model_fi = chef.feature_importance(cart_tree_model_rules)\nprint(cart_tree_model_fi)","db22f203":"# Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Gaussian Classifier\nrf_clf = RandomForestClassifier(n_estimators = 100,\n                                random_state = RANDOM_STATE)","72012dde":"# Train the model using the training sets y_pred=clf.predict(X_test)\nrf_clf.fit(X_train, y_train)","8dd0ab85":"y_pred_rf=rf_clf.predict(X_test)","1f3ad724":"from sklearn import metrics\n\n# Model Accuracy\n# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_rf))\n\nprint(\"=== Metrics===\")\nget_metrics(y_test, y_pred_rf)","2c6653aa":"import pandas as pd\nfi_rf = pd.Series(rf_clf.feature_importances_,index=predictors).sort_values(ascending=False)\nfi_rf","07530aca":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x=fi_rf, y=fi_rf.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","2eeb790a":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100,\n                                            stop = 1000,\n                                            num = 10)]\n\n# Number of features at every split\nmax_features = ['auto', 'sqrt']\n\n# Max depth\nmax_depth = [int(x) for x in np.linspace(100, 500, num = 11)]\nmax_depth.append(None)\n\n# Create random grid\nrandom_grid = {\n 'n_estimators': n_estimators,\n 'max_features': max_features,\n 'max_depth': max_depth\n }\n \n# Random search of parameters\nrf_clf_random = RandomizedSearchCV(estimator = rf_clf,\n                                param_distributions = random_grid,\n                                n_iter = 100, cv = 3, verbose=2,\n                                random_state = RANDOM_STATE, n_jobs = -1)\n\n# Fit the model\nrf_clf_random.fit(X_train, y_train)\n\n# Show results\nprint(rf_clf_random.best_params_)","92346b22":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrf_clf_best = RandomForestClassifier(n_estimators=500,\n                                     max_depth=460,\n                                     max_features='sqrt',\n                                     random_state = RANDOM_STATE)\n\nrf_clf_best.fit(X_train, y_train)\n\nrf_clf_best_predict = rf_clf_best.predict(X_test)\n\n# rf_clf_best_predict_single = np.argmax(rf_clf_best_predict, axis=1)\n\nrf_clf_best_cv_score = cross_val_score(rf_clf_best, X, y, cv=10,\n                                       scoring='roc_auc')","700afb04":"print(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test_single_mlp, rf_clf_best_predict))\nprint('\\n')\nprint(\"=== Metrics===\")\nget_metrics(y_test_single_mlp, rf_clf_best_predict)\nprint('\\n')\nprint(\"=== Mean AUC Score ===\")\nprint(\"Mean AUC Score - Random Forest: \", rf_clf_best_cv_score.mean())","b7d2c3b9":"from IPython.display import Image, display\n\ndef view_pydot(pdot):\n    plt = Image(pdot.create_png())\n    display(plt)","24ac5621":"from sklearn.tree import export_graphviz\nimport pydot\n\n# Pull out one tree from the forest\ntree = rf_clf_best.estimators_[10]\n\n# Export the image to a dot file\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = predictors,\n                rounded = True, precision = 1)\n\n# Use dot file to create a graph\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\n\n# Plot the graph\nview_pydot(graph)","f3392866":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nalgorithm_labels = [\"MLP\", \"BLR\", \"CHAID\", \"CART\", \"RF\", \"RF(+HP)\"]\nrow_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n\nevaluation_results = [\n    [\"89.20%\", \"88.94%\", \"83.42%\", \"52.26%\", \"88.69%\", \"89.70%\"],\n    [\"90.64%\", \"90.59%\", \"83.18%\", \"52.26%\", \"89.76%\", \"89.95%\"],\n    [\"88.46%\", \"87.98%\", \"85.58%\", \"100.0%\", \"88.46%\", \"90.38%\"],\n    [\"89.54%\", \"89.27%\", \"84.36%\", \"68.65%\", \"89.10%\", \"90.17%\"],\n]\n\ndf_evaluation_results = pd.DataFrame(\n    evaluation_results,\n    columns=algorithm_labels\n)\n\nax.axis(\"tight\")\nax.axis(\"off\")\n\n# Evaluation table\nax.set_title(\"Evaluation results\")\nax.table(\n    cellText=df_evaluation_results.values,\n    colLabels=df_evaluation_results.columns,\n    rowLabels=row_labels,\n    rowColours=[\"yellow\"] * 4,\n    colColours=[\"yellow\"] * 6,\n    loc=\"center\",\n)\n\n# Plot the results\nplt.show()","11055dd5":"##### 2.5.3.3.1. Model training","26fe7b67":"# 4. Conclusion\n\nThe accelerated development of artificial intelligence and machine learning applications, primarily in medicine, indicates a significant shift in research and medical diagnostics in the future. The application of machine learning methods for the detection of yeast cell damage under the action of UVB radiation, based on the results obtained using the GLCM method, is a less researched area. The obtained results seem encouraging and lead us to think about the possibilities of using this research approach in a series of similar experiments. Models of machine learning based on the *Random forest* algorithm gave a surprisingly good result compared to other methods, so we can consider them particularly important for solving both the considered and related problems.","083dd5fd":"### 2.5.1. Multilayer perceptron\n\nArtificial neural networks based on the *Multilayer perceptron* algorithm give much better results compared to the other listed models with, at the same time, a relatively simple implementation. (Davidovic et al., 2021). My unpublished research has shown that a model based on the *Binary linear regression* algorithm gives 1% to 5% better results over test data than the described model.\n","c04d79f4":"##### 2.5.3.4.2. Model storing","d75c75a5":"# 6. Software libraries\n\n## 6.1. GitHub\n\n1. numpy\/numpy: v1.19.5 (1.19.5). (2021). [Python, C, Cython, C++, JavaScript, Shell, Other]. NumPy. https:\/\/github.com\/numpy\/numpy\/releases\/tag\/v1.19.5 (Original work published 2010)\n\n2. Keras: Deep Learning for humans: v2.6.0 (v2.6.0). (2021). [Python, Starlark, Other]. Keras. https:\/\/github.com\/keras-team\/keras\/releases\/tag\/v2.6.0 (Original work published 2015)\n\n3. Serengil, S. I. (2021). chefboost: v0.0.13 (0.0.13) [Python]. https:\/\/github.com\/serengil\/chefboost (Original work published 2019)\n\n## 6.2. Zenodo\n\n1. Caswell, T. A., Droettboom, M., Lee, A., Hunter, J., Firing, E., Andrade, E. S. D., Hoffmann, T., Stansby, D., Klymak, J., Varoquaux, N., Nielsen, J. H., Root, B., Elson, P., May, R., Dale, D., Jae-Joon Lee, Sepp\u00e4nen, J. K., McDougall, D., Straw, A., \u2026 Ernest, E. (2021). matplotlib\/matplotlib: REL: v3.4.3 (v3.4.3) [Computer software]. Zenodo. https:\/\/doi.org\/10.5281\/zenodo.5194481.\n\n2. Reback, J., McKinney, W., Jbrockmendel, Bossche, J. V. D., Augspurger, T., Cloud, P., Gfyoung, Sinhrks, Hawkins, S., Roeschke, M., Klein, A., Terji Petersen, Tratner, J., She, C., Ayd, W., Naveh, S., Garcia, M., Schendel, J., Hayden, A., \u2026 Gorelli, M. (2021). pandas-dev\/pandas: Pandas 1.3.2 (v1.3.2) [Computer software]. Zenodo. https:\/\/doi.org\/10.5281\/zenodo.5203279.\n\n3. TensorFlow Developers. (2021). TensorFlow (v2.4.2) [Computer software]. Zenodo. https:\/\/doi.org\/10.5281\/zenodo.4960227.\n\n4. Virtanen, P., Gommers, R., Burovski, E., Oliphant, T. E., Cournapeau, D., Weckesser, W., Alexbrc, Peterson, P., Wilson, J., Mayorov, N., Endolith, Walt, S. V. D., Haberland, M., Laxalde, D., Brett, M., Reddy, T., Nelson, A., Millman, J., Larson, E., \u2026 Vanderplas, J. (2021). scipy\/scipy: SciPy 1.7.1 (v1.7.1) [Computer software]. Zenodo. https:\/\/doi.org\/10.5281\/zenodo.5152559.\n\n5. Waskom, M., Gelbart, M., Botvinnik, O., Ostblom, J., Hobson, P., Lukauskas, S., Gemperline, D. C., Augspurger, T., Halchenko, Y., Warmenhoven, J., Cole, J. B., Ruiter, J. D., Vanderplas, J., Hoyer, S., Pye, C., Miles, A., Corban Swain, Meyer, K., Martin, M., \u2026 Brunner, T. (2021). mwaskom\/seaborn: V0.11.2 (August 2021) (v0.11.2) [Computer software]. Zenodo. https:\/\/doi.org\/10.5281\/zenodo.5205191.\n\n<!-- ## 6.3. Other\n\n1. Gazoni, E. (2021). openpyxl: v3.0.9 (3.0.9) [Python, Shell]. openpyxl. https:\/\/foss.heptapod.net\/openpyxl\/openpyxl\/-\/tags\/3.0.9 (Original work published 2010) -->","128829bd":"## 2.3. Data preparation","3816d330":"#### 2.5.4.5. Adjustment of model hyperparameters\n\nWhen using the cross-validation method, it is necessary to determine the optimal parameters for creating a random forest model (*n_estimators*, *max_features*, *max_depth*), while the model already made over the observed data set is used as a basis for testing these values.","9678cc96":"#### 2.5.2.2. Model evaluation","4f496150":"The previous graph provides a comparative view of the existence of real damage (binary categorical variable, blue) and the probability that the classifier will decide whether the damage exists (green) or not (red), individually, for the first ten samples from the test data set.","433b437a":"#### 2.5.4.1. Model creating","198135b5":"#### 2.5.2.1. Model training","b1595154":"The previous table does not show missing values in the data set, so it is complete. Otherwise, these missing values would need to be filled in to match the others of the observed attribute or to be completely removed from the set.\n\nThe following is an example of code whose function is to remove such values if it finds them in the data.","2d76ca45":"#### 2.5.1.1. Model creating\n\nThe neural network model created is the *Multilayer perceptron* with four input neurons, three hidden-layer neurons, and two output-layer neurons. The number of neurons in the input and output layer depends on the type of data or the desired output, so that here it is constant. Changing the number of neurons in the hidden layer by adding new layers increases the complexity of the network while improving the accuracy of classification is minimal.\n\nOther reasons for choosing this type of network lie in the results of previous research. They rely on guidelines from the literature in which this type of network - one-hidden-layer network - is used to solve similar problems.","621f4aba":"## 2.5. Defining, compiling, and training the classification models\n\nIn the following, the implementation and obtained accuracy results of the following classification models will be displayed:\n\n1. *Multilayer perceptron* neural network,\n2. *Binary logistic regression*,\n3. decision tree using the *Chi-square automatic interaction detection* (*CHAID*) algorithm,\n4. decision tree using *Classification and Regression Trees* (*CART*) algorithm,\n5. *Random forest*.\n","7ef5f27a":"### 2.3.4. Data normalization\n\nIt is important to note that the normalization of data has a massive influence on the accuracy of predictions of individual models, such as artificial neural networks, as opposed to others (e.g., decision trees), where it does not play a significant role.\n\nWe can normalize the data in several ways, and below is an example of using the min-max method. Before normalization, it should be tested whether the data has notably less or higher values than the average for the observed attribute (so-called outliers). If such values exist, this type of normalization is not a suitable form to use, so we should look for an alternative.\n\nSince the test and elimination of the outliers were done in the previous step, the data were normalized using the *min-max* method.","88552e44":"#### 2.5.1.2. Prediction on test data and model evaluation","0730637d":"In the data exist 12 outliers that need special attention. As previously stated, we will remove these values from this work.","69c519eb":"#### 2.5.3.3. Decision tree using *CHAID* algorithm\n\nIn his paper with several study examples and analyzes, the author of the algorithm has shown its effectiveness, comparing its capabilities with *AID* and *THAID* algorithms created up to 17 years earlier (Kass, 1980). Ever since he had shown its efficiency, the decision tree using *CHAID* algorithm has remained one of the best choices for application in medicine to this day.","7426e3b9":"Based on the test results, the *CellNucleusAssesment* attribute has the best influence on determining the cell condition (*Treatment* attribute).","84363eb9":"## 1.3. Data collection and processing\n\nSeveral different methods could be used to quantify the structural changes in chromatin associated with aging, the most effective is the textural method based on the *Gray level co-occurrence matrix* (GLCM). Haralick, the pioneer in the development of this method, in his original work (Haralick, 1979), lists a total of 14 statistical measures for better determination of the image texture. In practice, it is not necessary to determine all these measures to achieve the desired results, especially when collecting data for the training of the machine learning models. Generally, 5 to 10 measures are sufficient to obtain adequate results, if we do not consider other values for the attributes.\n\nThe GLCM method has found application in all areas where image processing is the basis for further assay or research. In some of the relatively recent works in the field of medicine, this method has given good results in:\n\n- detection of Covid-19 virus infection based on the classification of images obtained by computed tomography (CT), which based on ten attributes obtained by the GLCM method achieves a prediction accuracy of 94% (Ameer & Mohammed, 2021),\n- processing of fingerprint data to determine the similarity in texture between parents and children based on data collected from people from the island of Lombok in Indonesia (Bakti et al., 2021),\n- detection of cell damage under the action of toxin oxidopamine and iron oxide nanoparticles based on five statistical measures obtained by the GLCM method (Nikolovski et al., 2019), \n- etc.\n\nThe results of the last experiment on saccharomycetes (lat. *Saccharomyces cerevisiae*) were used as input data for the development of the model in the research when the changes were analyzed using the GLCM method. The data set contains results for a total of 2000 cells, of which 1000 cells were treated with UVB rays for 60 seconds inducing damage (*Treatment = 1*), while the other 1000 cells were not treated (*Treatment = 0*). *GLCM contrast* and *fractal lacunarity* were determined, and *scores* on the integrity of the *cell nucleus* and *cell membrane* were assigned (based on the researcher's subjective opinion).\n","86b59a92":"### 2.5.3. Decision trees\n\n*Decision trees* belong to supervised models of machine learning. Several variations of this model implement different algorithms based on varied metrics, and features they all have in common are ease of implementation, clarity, and transparency in decision making. This decision-making model is well accepted in the field of medicine.","fc2b4569":"##### 2.5.3.3.2. Model storing","fce4d1ed":"## 1.1. Assignment\n\nThe paper aims to develop models capable of detecting UVB radiation-induced changes in cells by using modern methods and techniques of image processing, along with the application of machine learning.","f29b3106":"From the previous results of cross-validation, we can conclude that the recommended parameters for the development of a random forest model over the defined data are the following:\n\n- 'n_estimators': 500\n- 'max_features': 'sqrt'\n- 'max_depth': 460","c3e11b2f":"#### 2.3.2.2. Independent T-test","f40f70ac":"# 3. Research results\n\nThe paper tests the classification possibilities for five algorithms that may give the best results for a given data type.\n\n*Multilayer perceptron*, *Binary logistic regression*, and *Random forest* gave roughly similar results. Compared with this, the *Random forest* algorithm with adjusted hyperparameters proved to be the best algorithm for model development over a given data type with 0.5% higher classification accuracy than *Multilayer perceptron* models.\n\nThe decision tree based on the *CHAID* algorithm gave about 6% worse results than the previous two models. The *CART* algorithm provided the worst model, which achieved a classification accuracy of 90.50% over training data, whilst the different test data provided accuracy in the range of 45% to 55%. Given that it has previously proved to be a poor choice for classifying this type of data, giving approximate results, we can say that this type of algorithm is still not applicable to solve the considered problem.\n\nBased on a statistical significance *t-test*, the attribute that best describes the state of a cell is *CellNucleusAssesment*. However, the decision tree models recognized *CellMembraneAssesment* as the most significant parameter, while for instance, *Random forest* as the most indispensable parameter singled out *GLCMContrast*. This could be considered one of the reasons that influenced the accuracy of classification of *Random forest* models to be the best, bearing in mind that, in addition to *fractal lacunarity*, the value of this attribute is not based on the personal opinion of researchers but is the result of statistical analysis.","5f4c200d":"By defining the deviation threshold, all higher values were eliminated.\n\n```py\nz_threshold = 3\n```","b1b8ad04":"# 5. References\n\n1. Ameer, A. Q. A., & Mohammed, R. F. (2021). Covid-19 detection using CT scan based on gray level Co-Occurrence matrix. Materials Today: Proceedings, S2214785321031448. https:\/\/doi.org\/10.1016\/j.matpr.2021.04.224\n\n2. Bakti, L. D., Imran, B., Wahyudi, E., Arwidiyarti, D., Suryadi, E., Multazam, M., & Maspaeni. (2021). Data extraction of the gray level Co-occurrence matrix (GLCM) Feature on the fingerprints of parents and children in Lombok Island, Indonesia. Data in Brief, 36, 107067. https:\/\/doi.org\/10.1016\/j.dib.2021.107067\n\n\n3. Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (Eds.). (1984). Classification and regression trees (1. CRC Press repr). Chapman & Hall\/CRC.\n\n4. Chollet, F. & others. (2015). Keras. https:\/\/keras.io\n\n5. Davidovic, L. M., Laketic, D., Cumic, J., Jordanova, E., & Pantic, I. (2021). Application of artificial intelligence for detection of chemico-biological interactions associated with oxidative stress and DNA damage. Chemico-Biological Interactions, 345, 109533. https:\/\/doi.org\/10.1016\/j.cbi.2021.109533\n\n6. Haralick, R. M. (1979). Statistical and structural approaches to texture. Proceedings of the IEEE, 67(5), 786\u2013804. https:\/\/doi.org\/10.1109\/PROC.1979.11328\n\n7. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del R\u00edo, J. F., Wiebe, M., Peterson, P., \u2026 Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357\u2013362. https:\/\/doi.org\/10.1038\/s41586-020-2649-2\n\n8. Kass, G. V. (1980). An Exploratory Technique for Investigating Large Quantities of Categorical Data. Applied Statistics, 29(2), 119. https:\/\/doi.org\/10.2307\/2986296\n\n9. Nikolovski, D., Cumic, J., & Pantic, I. (2019). Application of Gray Level co-Occurrence Matrix Algorithm for Detection of Discrete Structural Changes in Cell Nuclei After Exposure to Iron Oxide Nanoparticles and 6-Hydroxydopamine. Microscopy and Microanalysis, 25(4), 982\u2013988. https:\/\/doi.org\/10.1017\/S1431927619014594\n\n10. Papakonstantinou, D., Zanni, V., Nikitaki, Z., Vasileiou, C., Kousouris, K., & Georgakilas, A. G. (2021). Using Machine Learning Techniques for Asserting Cellular Damage Induced by High-LET Particle Radiation. Radiation, 1(1), 45\u201364. https:\/\/doi.org\/10.3390\/radiation1010005\n\n11. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., M\u00fcller, A., Nothman, J., Louppe, G., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, \u00c9. (2018). Scikit-learn: Machine Learning in Python. ArXiv:1201.0490 [Cs]. http:\/\/arxiv.org\/abs\/1201.0490","9204a981":"#### 2.5.4.3. Model evaluation","2d089345":"## 2.4. Creating a training data and test data sets\n\nFollowing the recommendations of good practice, the initial data set was divided into the part used for training (80%), while the rest was left for testing the model results (20%).","e23a01c3":"Working with outliers involves two possible strategies: removing or modifying values.\n\nThe value removal method will be applied below. To find the outliers, the mathematical function *Z-score* will be used, which has the role of finding the values of each attribute based on their relationship with standard deviation and mean value or rather to display all values above the mean value.","1327e342":"From the previous graph, we can see that the data has been removed from outliers. The data arranged in this way can be further normalized and used for model training.","78672f23":"The essential attribute of the model in the graphs is the contrast parameter obtained by the GLCM method. Others contribute less to the accuracy of the classifier. Since there is not one of them with less significance (e.g., less than 0.1), we can state that each attribute makes a significant contribution in obtaining better classification results.","48a56f95":"##### 2.5.3.3.3. Model evaluation","13aa225d":"### 2.2.1. The *matplotlib* library set up to display graphics","f322b1e9":"##### 2.5.3.4.1. Model training","00093730":"#### 2.5.3.2. Customize training data and test data","cde81c62":"Application of Artificial Intelligence Methods in the detection of changes in the Nuclear Chromatin Structure caused by UVB radiation\n---","b0a1c5e6":"#### 2.5.3.1. Installation of the required libraries","ca30a515":"From the previous graphs, we see outliers in the values of the *GLCMContrast* and *FractalLacunarity* attributes. In contrast, we do not have them in the *CellNucleusAssesment* and *CellMembraneAssesment* attributes because they are values from a pre-limited range (from 1 to 5).","0159c842":"#### 2.3.3.2. Test and elimination of outliers\n\nOutliers are attribute values that deviate significantly from others. They occur most often when entering data incorrectly in the collection phase.\n\nSuch values can significantly affect the accuracy of the machine learning model, leading it to draw wrong conclusions, so it is necessary to make corrections to the data set.","b910f565":"#### 2.5.4.2. Model training","5e53f33b":"## 1.2. Methods and tools\n\nIn the case of cell classification problems, the best results could come from the following models: *Logistic regression*, *Decision trees*, *Bayesian networks*, *Random Forests*, *Linear Support Vector Machine*, *Naive Bayes classifier*, as well as deep learning algorithms such as *Artificial Neural Networks*, *Recurrent Neural Networks*, and *Convolutional Neural Networks*. (Davidovic et al., 2021).\n\nSome of the algorithms that can be specifically applied to develop variations of the above models, and which we can expect to achieve the best results for the observed data type, are:\n\n- *Multi-layer Perceptron*\n- *Binomial (or Binary) Logistic Regression*\n- *Decision Tree Using Chi_Square Automatic Interaction Detection (CHAID))*\n- *Classification and Regression Trees (CART)*\n- *QUEST - Quick Unbiased Efficient Statistical Tree*\n- *C5.0 Decision Tree*\n- *Random Forest*","6259899b":"# 1. Research description\n\nThe paper covers the development of machine learning classification models that could distinguish healthy cells from the cells damaged through UVB radiation. The idea of developing classification models for this purpose arose from previous research on the progress of the machine learning application in detecting oxidative stress and DNA damage (Davidovic et al., 2021).\n\nIonizing radiation has a significant effect on DNA damage in particular. The ways of its action on cells are a very researched area, observed from a medical point of view. Moreover, researchers in the field of data processing see tremendous potential for developing advanced software systems for simpler and faster diagnostics of changes. Most of these changes are not easy to detect using conventional methods, while the machine learning models, such as neural networks, offering the possibility to identify even these, the so-called isolated damages (Davidovic et al., 2021).","ca1c6eda":"#### 2.5.3.4. Classification trees and regression trees\n\nDecision trees based on the *CART* algorithm use the *Gini* index as a metric for decision-making. Both can be used to solve classification problems and to solve regression problems.\n\n![gini-index-formula.png](https:\/\/miro.medium.com\/max\/358\/1*1JDsJTBtb-HdV53Bo3AUaw.png)\n\nThe pioneers in the development of this decision algorithm, in their book, allege medicine as one of the potential areas of application, focusing on solving the problems of diagnosis and prognosis of heart attacks, as well as cancer (Breiman et al., 1984). We can freely say that this model of decision-making is, to a large extent, used in this area today.","fa51ae13":"## 2.1. Setting up the development environment\n\nThe *Kaggle IDE* was used for development. The setup of the development environment is performed in line with the requirements of development using this tool. For the script to run on another platform, it is necessary to edit these settings.","e2715a1a":"# 2. Development of machine learning models\n\nThe study aims to probe the distinctive possibilities of classification models based on artificial intelligence and machine learning methods in detecting cell damage. Some of the models presented in the research are *Multilayer perceptron*, *Binary logistic regression*, *decision tree* using the *Chi-square automatic interaction detection* algorithm (CHAID), *Classification and Regression Trees*, and the *Random forest* algorithm. The implementation and success results of the classification of each of these models are listed below.","bba33bbd":"##### 2.5.3.4.3. Model evaluation","cd11c4bf":"#### 2.3.3.1. Elimination of NA values","21d007d2":"### 2.3.3. Elimination of NA values and outliers\n\nPerforming this step is not required if all the data are entered correctly into the database we are working with, as that is the case here. If some of the data are missing, it is necessary to eliminate the missing values that can significantly affect the model results, and put them below satisfactory level.","c1533f68":"## 2.2. Adjustment of reproduction of the obtained results\n\nTo get the same results every time you run the software, you need to set the initial values for random selection functions.","594696c9":"#### 2.3.2.1. Pairplot","30722ccb":"### 2.5.2. Binary logistic regression\n\n*Logistic regression* describes the relationship between predictors that can be of continuous, binary, or categorical type. If the dependent variable is dichotomous (binary), although the independent ones can be of any sort, this type is called *Binary logistic regression*. We use it to solve the classification problem when there are two possible options between which it is necessary to make the right choice.\n\nThe dependent variable in this type of regression is Bernoulli's random variable, so there are two categories that we denote, 0 - for failure and 1 - for success.","23f7f207":"#### 2.5.4.4. Determining the most important predictors","5ccdb7ec":"### 2.5.4. Random forest\n\n*Random forest* is an algorithm that combines the capabilities of multiple decision trees. Its use has given good results in detecting radiation with High-LET (high linear energy transfer) (Papakonstantinou et al., 2021), which contributed significantly to cancer research. The ability to notice cell damage by UVB radiation is one of the potentially best possibilities for practicing this method in medicine, especially in radiology.","514b46a8":"#### 2.5.4.6. Visual display of the best RF model","b3c4878c":"### 2.3.1. Loading of the data\n\nModel training data is stored as an *Excel spreadsheet*. The first sheet contains all the samples (2000). The *Treatment* column indicates which cell was treated (1) and which was not (0).","7d83341c":"### 2.3.2. Data verification\nAfter loading the data, it is necessary to define the predictor and the target column and check the predictor dependence to determine the attributes with the best impact on the classification result."}}