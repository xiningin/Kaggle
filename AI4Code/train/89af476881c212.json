{"cell_type":{"6debed13":"code","2a41a9f3":"code","21b30392":"code","5b7217fe":"code","08432309":"code","e2c80f8c":"code","695110cd":"code","0b4089bd":"code","aab4127e":"code","5a2ce2da":"code","538fffaa":"code","05de279b":"code","0df93805":"code","c16be914":"code","9961e5e2":"code","90e9e856":"code","0a6b2289":"code","b8791b40":"code","6da42db8":"code","f646e685":"code","c0430e01":"code","a91513c8":"code","2fcff913":"code","b1eb4ec8":"code","f6d74414":"code","3d1484a1":"code","3673c757":"code","87204fd1":"markdown","ae0202f1":"markdown","6ad0cd6b":"markdown","a4ac9d32":"markdown","e4a5bb8d":"markdown","34bf7edc":"markdown","8be4c354":"markdown","74576fe1":"markdown"},"source":{"6debed13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# kaggle standard imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# extra imports\nnp.random.seed(235)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport gc\nimport re\nfrom sklearn.metrics import f1_score\n\n# XGboost related\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom scipy.sparse import csr_matrix, hstack\n\n# Any results you write to the current directory are saved as output.","2a41a9f3":"print('load data') \n# load training and testing data\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\n# split training data to validation\ntrain_df, val_df = train_test_split(train_df, train_size=0.9, random_state=235)","21b30392":"print('fill missing and get the values')\n# fill missing and get the values\nX_train = train_df[\"question_text\"].fillna(\"na_\").values\nX_val = val_df[\"question_text\"].fillna(\"na_\").values\nX_test = test_df[\"question_text\"].fillna(\"na_\").values\n\ny_train = train_df['target'].values\ny_val = val_df['target'].values","5b7217fe":"print('size of training data: ', X_train.shape)","08432309":"char_vector = TfidfVectorizer(\n    ngram_range=(2,4),\n    max_features=20000,\n    stop_words='english',\n    analyzer='char_wb',\n    token_pattern=r'\\w{1,}',\n    strip_accents='unicode',\n    sublinear_tf=True, \n    max_df=0.98,\n    min_df=2\n)","e2c80f8c":"print('fit char vector')\nchar_vector.fit(X_train[:85000])","695110cd":"print('transfer data based on char vector')\nprint('transfer train')\ntrain_char_vector = char_vector.transform(X_train).tocsr()\nprint('transfer validation')\nvalid_char_vector = char_vector.transform(X_val).tocsr()\nprint('transfer test')\ntest_char_vector = char_vector.transform(X_test).tocsr()","0b4089bd":"all_text = list(X_train) + list(X_test)","aab4127e":"word_vector = TfidfVectorizer(\n    ngram_range=(1,1), \n    max_features=9000,\n    sublinear_tf=True, \n    strip_accents='unicode', \n    analyzer='word', \n    token_pattern=\"\\w{1,}\", \n    stop_words=\"english\",\n    max_df=0.95,\n    min_df=2\n)","5a2ce2da":"print('fit word vector')\nword_vector.fit(all_text)","538fffaa":"print('transfer data based on word vector')\ntrain_word_vector = word_vector.transform(X_train).tocsr()\nvalid_word_vector = word_vector.transform(X_val).tocsr()\ntest_word_vector = word_vector.transform(X_test).tocsr()","05de279b":"del all_text\ndel X_train\ndel X_val\ndel X_test\ngc.collect()","0df93805":"data = [train_df, val_df, test_df]","c16be914":"# references: https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\nmistake_list = ['colour', 'centre', 'favourite', 'travelling', 'counselling', 'theatre', 'cancelled', 'labour', 'organisation', 'wwii', 'citicise', 'youtu ', 'youtube ', 'Qoura', 'sallary', 'Whta', 'narcisist', 'howdo', 'whatare', 'howcan', 'howmuch', 'howmany', 'whydo', 'doI', 'theBest', 'howdoes', 'mastrubation', 'mastrubate', \"mastrubating\", 'pennis', 'Etherium', 'narcissit', 'bigdata', '2k17', '2k18', 'qouta', 'exboyfriend', 'airhostess', 'whst', 'watsapp', 'demonitisation', 'demonitization', 'demonetisation']","9961e5e2":"def get_features(data):\n    for dataframe in data:\n        dataframe[\"text_size\"] = dataframe[\"question_text\"].apply(len).astype('uint16')\n        dataframe[\"capital_size\"] = dataframe[\"question_text\"].apply(lambda x: sum(1 for c in x if c.isupper())).astype('uint16')\n        dataframe[\"capital_rate\"] = dataframe.apply(lambda x: float(x[\"capital_size\"]) \/ float(x[\"text_size\"]), axis=1).astype('float16')\n        dataframe[\"exc_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"!\")).astype('uint16')\n        dataframe[\"quetion_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"?\")).astype('uint16')\n        dataframe[\"unq_punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2')).astype('uint16')\n        dataframe[\"punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '.,;:^_`')).astype('uint16')\n        dataframe[\"symbol_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '*&$%')).astype('uint16')\n        dataframe[\"words_count\"] = dataframe[\"question_text\"].apply(lambda x: len(x.split())).astype('uint16')\n        dataframe[\"unique_words\"] = dataframe[\"question_text\"].apply(lambda x: (len(set(1 for w in x.split())))).astype('uint16')\n        dataframe[\"unique_rate\"] = dataframe[\"unique_words\"] \/ dataframe[\"words_count\"]\n        dataframe[\"word_max_length\"] = dataframe[\"question_text\"].apply(lambda x: max([len(word) for word in x.split()]) ).astype('uint16')\n        dataframe[\"mistake_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(w) for w in mistake_list)).astype('uint16')\n    return data","90e9e856":"print('generate the features')\ndata = get_features(data)","0a6b2289":"feature_cols = [\"text_size\", \"capital_size\", \"capital_rate\", \"exc_count\", \"quetion_count\", \"unq_punctuation_count\", \"punctuation_count\", \"symbol_count\", \"words_count\", \"unique_words\", \"unique_rate\", \"word_max_length\", \"mistake_count\"]","b8791b40":"print('final preparation for input')\nX_train = csr_matrix(train_df[feature_cols].values)\nX_val = csr_matrix(val_df[feature_cols].values)\nX_test = csr_matrix(test_df[feature_cols].values)\n\ndel val_df\ndel train_df\ndel test_df\n\ngc.collect()","6da42db8":"'''\ninput_train = hstack([X_train, train_char_vector,train_word_vector])\ninput_valid = hstack([X_val, valid_char_vector, valid_word_vector])\ninput_test = hstack([X_test, test_char_vector, test_word_vector])\n'''\ninput_train = hstack([X_train, train_word_vector, train_char_vector])\ninput_valid = hstack([X_val, valid_word_vector, valid_char_vector])\ninput_test = hstack([X_test, test_word_vector, test_char_vector])\n\n#print('input_train: ', input_train)\ntrain_word_vector = None\ntrain_char_vector = None\nvalid_word_vector = None\nvalid_char_vector = None\ntest_word_vector = None\ntest_char_vector = None\n#print('input_train: ', input_train)","f646e685":"'''reference: some settings inspired by Toxic competition kernels'''\ndef build_xgb(train_X, train_y, valid_X, valid_y=None, subsample=0.75):\n\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n    if valid_y is not None:\n        xgvalid = xgb.DMatrix(valid_X, label=valid_y)\n    else:\n        xgvalid = None\n    \n    model_params = {}\n    # binary 0 or 1\n    model_params['objective'] = 'binary:logistic'\n    # eta is the learning_rate, [default=0.3]\n    model_params['eta'] = 0.3\n    # depth of the tree, deeper more complex.\n    model_params['max_depth'] = 6\n    # 0 [default] print running messages, 1 means silent mode\n    model_params['silent'] = 1\n    model_params['eval_metric'] = 'auc'\n    # will give up further partitioning [default=1]\n    model_params['min_child_weight'] = 1\n    # subsample ratio for the training instance\n    model_params['subsample'] = subsample\n    # subsample ratio of columns when constructing each tree\n    model_params['colsample_bytree'] = subsample\n    # random seed\n    model_params['seed'] = 2018\n    # imbalance data ratio\n    #model_params['scale_pos_weight'] = \n    \n    # convert params to list\n    model_params = list(model_params.items())\n    \n    return xgtrain, xgvalid, model_params","c0430e01":"def train_xgboost(xgtrain, xgvalid, model_params, num_rounds=500, patience=20):\n    \n    if xgvalid is not None:\n        # watchlist what information should be printed. specify validation monitoring\n        watchlist = [ (xgtrain, 'train'), (xgvalid, 'test') ]\n        #early_stopping_rounds = stop if performance does not improve for k rounds\n        model = xgb.train(model_params, xgtrain, num_rounds, watchlist, early_stopping_rounds=patience)\n    else:\n        model = xgb.train(model_params, xgtrain, num_rounds)\n    \n    return model","a91513c8":"print('train the model')\nxgtrain, xgvalid, model_params = build_xgb(input_train, y_train ,input_valid, y_val)\nmodel = train_xgboost(xgtrain, xgvalid, model_params)","2fcff913":"print('predict validation')\nvalidate_hat = np.zeros(( X_val.shape[0], 1) )\nvalidate_hat[:,0] = model.predict(xgb.DMatrix(input_valid), ntree_limit=model.best_ntree_limit)","b1eb4ec8":"scores_list = []\nfor threshold in [0.2, 0.3, 0.31, 0.33, 0.4, 0.45, 0.5]:\n    score = f1_score(y_val, (validate_hat > threshold).astype(int))\n    scores_list.append([threshold, score])\n    print('F1 score: {} for threshold: {}'.format(score, threshold))\n        \n    scores_list.sort(key=lambda x:x[1] , reverse=True)\n    best_threshold = scores_list[0][0]\n    print('best threshold to generate predictions: ', best_threshold)\n    print('best score: ', scores_list[0][1])","f6d74414":"print('predict results')\npredictions = np.zeros(( X_test.shape[0], 1) )\npredictions[:,0] = model.predict(xgb.DMatrix(input_test), ntree_limit=model.best_ntree_limit)","3d1484a1":"def save_results(submit, y_hat, name, threshold=0.35):\n    print('threshold is: ', threshold)\n    results = (y_hat > threshold).astype(int)\n    print(results[:100])\n    submit['prediction'] = results\n    save_to = (name+'.csv')\n    submit.to_csv(save_to, index=False)","3673c757":"print('save results')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsave_results(submission, predictions, 'submission', threshold=best_threshold)","87204fd1":"**Predict And Export Results**","ae0202f1":"**Build The model**","6ad0cd6b":"**Prepare Vectors For XGboost input**","a4ac9d32":"**XGboost Intro**\n\nXGboost is tree based model and one of the most powerful machine learning techniques; it can be used with patterns, numbers and text problems. However RNN models more common for text problems.\n\n**Different models structure and design produce better ensemble or stacking results.**\n\nThis model can be used as an ensemble or stack item alongside with RNN models to produce better results than any of the two models.","e4a5bb8d":"**Features Engineering**","34bf7edc":"**Train The Model**","8be4c354":"**Input Final Format**","74576fe1":"**Prepare Data**\n\nreferences:\n* Data preparing and process inspired by (Shujian Liu) Kernals"}}