{"cell_type":{"02d04317":"code","5902f595":"code","494b536a":"code","58593979":"code","f5c6361f":"code","0b2da268":"code","6e8411d2":"code","9c515a41":"code","fd0fc1d1":"code","5fed5f30":"code","57bd7dc8":"code","610cced4":"code","1a9fb584":"markdown","c867dfe8":"markdown"},"source":{"02d04317":"!pip install xlrd \n!pip install openpyxl","5902f595":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nnp.random.seed(42)\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","494b536a":"df = pd.read_excel(\"\/kaggle\/input\/hr-employee-data-descriptive-analytics\/HR_Employee_Data.xlsx\")\ndf.head()","58593979":"df.drop(['Emp_Id'], axis=1, inplace=True)\n\ndf.info()","f5c6361f":"df.corr().style.background_gradient(sns.light_palette('green', as_cmap=True))","0b2da268":"list(enumerate(df.drop(['left'],1).columns))","6e8411d2":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n\nct = ColumnTransformer([\n    (\"one-hot\", OneHotEncoder(), [7]),\n    (\"ordinal\", OrdinalEncoder(), [8])\n], remainder='passthrough')","9c515a41":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\npipes = {}\nmodels = {}\n\nmodels['SVC'] = SVC(probability=True)\nmodels['LR'] = LogisticRegression()\nmodels['DT'] = DecisionTreeClassifier()\nmodels['RF'] = RandomForestClassifier()\nmodels['XGB'] = XGBClassifier(use_label_encoder=False)\n\nfor m in models:\n    pipes[m] = Pipeline([\n            (\"columns compose\", ct),\n            (\"standardize\", StandardScaler()),\n            (m, models[m])\n        ])","fd0fc1d1":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nX, y = df.drop(['left'],axis=1), df['left']\n\nle = LabelEncoder().fit(y)\ny = le.transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=112)","5fed5f30":"for p in pipes:\n    pipes[p].fit(X_train, y_train)","57bd7dc8":"from sklearn.metrics import confusion_matrix, classification_report\n\nfor p in pipes:\n    print(f'\\t\\t{p}')\n    args = dict(y_true=y_test, y_pred=pipes[p].predict(X_test))\n    print(confusion_matrix(**args))\n    print(classification_report(**args))\n    print(\"=\"*60)","610cced4":"from sklearn.metrics import roc_curve\n\nplt.figure(figsize=(13,6))\n\nfor p in pipes:\n    y_pred = pipes[p].predict_proba(X_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=p)\n\nplt.title('ROC curve')\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","1a9fb584":"# Predictive Models\n___","c867dfe8":"We can see that Random Forest and XGBoost has done the best job."}}