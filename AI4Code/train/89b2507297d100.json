{"cell_type":{"8b69882f":"code","94a7fd3c":"code","f7c6a6eb":"code","eeecdb3d":"code","0ddb55b0":"code","8a7fc3dd":"code","434a251f":"code","e9119710":"code","bcb32519":"code","b0d4f397":"code","74849dd1":"code","4a5c4bc7":"code","69f5443d":"code","0bf060a3":"code","bfe6828d":"code","6905560d":"code","96ea8608":"code","c7689add":"code","8e90a2fa":"code","fd750387":"code","0c1f8557":"code","8a7d9604":"code","3e507952":"code","cb9bafbd":"code","ef0c9963":"code","ef4071b2":"code","c65d6a2e":"code","b737a78e":"code","cda0a0bb":"code","8f99d71f":"code","bb304a1a":"code","f1e698f9":"code","ab75b602":"code","6e623fb9":"code","8066d1b7":"code","096d0e61":"code","493e7acd":"code","85ea928a":"code","7e961ac2":"code","99941dd7":"code","3353e156":"code","d896a002":"code","4dff1e29":"code","a4d5f1a8":"code","a4af8dfc":"code","f984bc27":"code","f95621ff":"markdown","a8685e8d":"markdown","e4509347":"markdown","bbd01f80":"markdown","231c60ef":"markdown","a09985a0":"markdown","89b87e18":"markdown","ffeacff2":"markdown","18b557ef":"markdown","3b5a044d":"markdown","8e92ee91":"markdown","e0434c46":"markdown","146b9d70":"markdown","04e7c92f":"markdown","16b367b4":"markdown","8ef9071d":"markdown","15338dfc":"markdown","a7494621":"markdown","6fa9b14f":"markdown","c403da7f":"markdown","93d6ae53":"markdown","5bcf1cbb":"markdown"},"source":{"8b69882f":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport collections\nimport gc\nimport json\nimport os\nimport random\nimport time\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensor\nimport cv2\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport tifffile as tiff\nimport timm\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline","94a7fd3c":"!ls ..\/input\/256-x-256-cropped-images","f7c6a6eb":"DATASET = \"..\/input\/iwildcam2021-fgvc8\"\nCROPED_DATA = \"..\/input\/256-x-256-cropped-images\/\"\n\nTRAIN_CROPED_DATA = \"croped_images_train\/\"\nTEST_CROPED_DATA = \"croped_images_test\/\"","eeecdb3d":"BATCH_SIZE = 32\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 300\nNUM_WORKERS = 4\nSEED = 2021","0ddb55b0":"def set_seed(seed=2**3):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\nset_seed(SEED)","8a7fc3dd":"df_croped_img_ids_train = pd.read_csv(CROPED_DATA + \"croped_train.csv\")\ndf_croped_img_ids_test = pd.read_csv(CROPED_DATA + \"croped_test.csv\")","434a251f":"df_croped_img_ids_train.head()","e9119710":"df_croped_img_ids_test.head()","bcb32519":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n    train_annotations =json.load(json_file)\ndf_train_annotation = pd.DataFrame(train_annotations[\"annotations\"])","b0d4f397":"train = df_croped_img_ids_train[[\"id\", \"idx\"]].merge(df_train_annotation[[\"image_id\", \"category_id\"]], \n                                      left_on='id', right_on='image_id')[[\"id\", \"idx\", \"category_id\"]]","74849dd1":"df_categories = pd.DataFrame(train_annotations[\"categories\"])","4a5c4bc7":"cat_idxs = df_categories[\"id\"]\n\ndef convert_cat_to_index(x):\n    return np.where(cat_idxs==x)[0][0]","69f5443d":"train[\"category_id\"] = train[\"category_id\"].map(lambda x: convert_cat_to_index(x))","0bf060a3":"train.head()","bfe6828d":"! unzip ..\/input\/256-x-256-cropped-images\/croped_images_train.zip ","6905560d":"! unzip ..\/input\/256-x-256-cropped-images\/croped_images_test.zip","96ea8608":"# ====================================================\n# Dataset for train\n# ====================================================\n\nmean = np.array([0.37087523, 0.370876, 0.3708759] )\nstd = np.array([0.21022698, 0.21022713, 0.21022706])\n\ndef img2tensor(img,dtype:np.dtype=np.float32):\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass IWildcamTrainDataset(Dataset):\n    def __init__(self, df, tfms=None):\n        self.ids = df[\"id\"]\n        self.idxs = df[\"idx\"]\n        self.categories = df[\"category_id\"]\n        self.tfms = tfms\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        size = (256, 256)\n        image_id = self.ids[idx]\n        image_idx = self.idxs[idx]\n        iamge_categorie = self.categories[idx]\n        \n        image_path = TRAIN_CROPED_DATA + f\"{image_id}_{image_idx}.jpg\"\n        img = cv2.resize(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB),size)\n\n        if self.tfms is not None:\n            augmented = self.tfms(image=img)\n            img = augmented['image']\n            \n        # we should normalize here\n        return img2tensor((img\/255.0  - mean)\/std), torch.tensor(iamge_categorie)","c7689add":"def get_aug(p=1.0):\n    return Compose([\n        HorizontalFlip(),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        RandomBrightnessContrast(p=0.9),\n    ], p=p)","8e90a2fa":"# ====================================================\n# EfficientNet Model\n# ====================================================\n\nclass enet_v2(nn.Module):\n\n    def __init__(self, backbone, out_dim, pretrained=False):\n        super(enet_v2, self).__init__()\n        self.enet = timm.create_model(backbone, pretrained=pretrained)\n        in_ch = self.enet.classifier.in_features\n        self.myfc = nn.Linear(in_ch, out_dim)\n        self.enet.classifier = nn.Identity()\n\n    def forward(self, x):\n        x = self.enet(x)\n        x = self.myfc(x)\n        return x","fd750387":"model = enet_v2(backbone=\"tf_efficientnet_b0\", out_dim=205)\nmodel.to(DEVICE)","0c1f8557":"# ====================================================\n# Optimizer and Loss\n# ====================================================\n\noptimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 1e-4}])\ncriterion = nn.CrossEntropyLoss()","8a7d9604":"rus = RandomUnderSampler(random_state=SEED, replacement=True)\n\ndef generate_dataloders(train):\n    \n    train_resampled, _ = rus.fit_resample(train, train[\"category_id\"])\n    test_resampled, _ = rus.fit_resample(train, train[\"category_id\"])\n\n    train_resampled = train_resampled.reset_index(drop=True)\n    test_resampled = test_resampled.reset_index(drop=True)\n    \n    ds_train = IWildcamTrainDataset(train_resampled, tfms=get_aug())\n    dl_train = DataLoader(ds_train,batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    ds_test = IWildcamTrainDataset(test_resampled)\n    dl_test = DataLoader(ds_test,batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    \n    return dl_train, dl_test","3e507952":"# ====================================================\n# Train\n# ====================================================\n\nfor epoch in tqdm(range(EPOCHS)):\n    \n    dl_train, dl_test = generate_dataloders(train)\n    \n    ###Train\n    model.train()\n    train_loss = 0\n    \n    for data in dl_train:\n        optimizer.zero_grad()\n        imgs, categories = data\n        imgs = imgs.to(DEVICE)\n        categories = categories.to(DEVICE)\n        \n        outputs = model(imgs)\n    \n        loss = criterion(outputs, categories)\n        loss.backward()\n        optimizer.step()\n            \n        train_loss += loss.item()\n    train_loss \/= len(dl_train)\n        \n    print(f\"EPOCH: {epoch + 1}, train_loss: {train_loss}\")\n        \n    ###Validation\n    model.eval()\n    valid_loss = 0\n        \n    for data in dl_test:\n        imgs, categories = data\n        imgs = imgs.to(DEVICE)\n        categories = categories.to(DEVICE)\n        \n        outputs = model(imgs)\n    \n        loss = criterion(outputs, categories)\n        \n        valid_loss += loss.item()\n    valid_loss \/= len(dl_test)\n        \n    print(f\"EPOCH: {epoch + 1}, valid_loss: {valid_loss}\")\n        \n    \n    if (epoch+1)%50 == 0 or (epoch+1)%EPOCHS == 0:\n        ###Save model\n        torch.save(model.state_dict(), f\"{epoch+1}_.pth\")","cb9bafbd":"# ====================================================\n# Dataset for test\n# ====================================================\n\nmean = np.array([0.37087523, 0.370876, 0.3708759] )\nstd = np.array([0.21022698, 0.21022713, 0.21022706])\n\nclass IWildcamTestDataset(Dataset):\n    def __init__(self, df, tfms=None):\n        self.ids = df[\"id\"]\n        self.idx = df[\"idx\"]\n        self.tfms = tfms\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        size = (256, 256)\n        image_id = self.ids[idx]\n        image_idx = self.idx[idx]\n        \n        image_path = TEST_CROPED_DATA + f\"{image_id}_{image_idx}.jpg\"\n        \n        img = cv2.resize(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB),size)\n\n        if self.tfms is not None:\n            augmented = self.tfms(image=img)\n            img = augmented['image']\n            \n        # we should normalize here\n        return img2tensor((img\/255.0 - mean)\/std), image_id","ef0c9963":"ds_test = IWildcamTestDataset(df_croped_img_ids_test)\ndl_test = DataLoader(ds_test,batch_size=32,shuffle=False,num_workers=NUM_WORKERS)","ef4071b2":"model = enet_v2(backbone=\"tf_efficientnet_b0\", out_dim=205)\nmodel.to(DEVICE)\nmodel.load_state_dict(torch.load(f\"{epoch+1}_.pth\"))\nmodel.eval()","c65d6a2e":"pred_categories = []\npred_img_ids = []","b737a78e":"with torch.no_grad():\n    for imgs, img_ids in tqdm(dl_test):\n        imgs = imgs.to(DEVICE)\n        \n        outputs = model(imgs)\n        output_labels = torch.argmax(outputs, dim=1).tolist()\n        pred_categories += output_labels\n        pred_img_ids += img_ids","cda0a0bb":"pred = collections.defaultdict(list)\nfor category, img_id in zip(pred_categories, pred_img_ids):\n    pred[img_id].append(category)","8f99d71f":"pred","bb304a1a":"sub = pd.read_csv(\"..\/input\/iwildcam2021-fgvc8\/sample_submission.csv\")\ncol_Predicted = [col for col in sub.columns if \"Predicted\" in col]","f1e698f9":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n    train_annotations =json.load(json_file)\ndf_categories = pd.DataFrame.from_records(train_annotations[\"categories\"])","ab75b602":"results = []\n\nfor key in pred.keys():\n    c = collections.Counter(pred[key])\n    \n    res = []\n    cnts = [ 0 for i in range(205)]\n    for category, cnt in c.items():\n        cnts[category] = cnt\n    res += [key] + cnts[1:]\n    results.append(res)","6e623fb9":"sub_tmp = pd.DataFrame(results, columns=sub.columns)","8066d1b7":"sub_tmp.head()","096d0e61":"sub_tmp.to_csv(\".\/sub_tmp.csv\", index=False)","493e7acd":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_test_information.json', encoding='utf-8') as json_file:\n    test_information =json.load(json_file)\n    \ndf_test_info = pd.DataFrame(test_information[\"images\"])[[\"id\", \"seq_id\"]]\ndf_test_info.head()","85ea928a":"sub_tmp = sub_tmp.merge(df_test_info, left_on=\"Id\", right_on=\"id\", how=\"right\")","7e961ac2":"sub_tmp.head()","99941dd7":"sum_counts = []\nfor i in range(len(sub_tmp)):\n    sum_counts.append(sum(sub_tmp.iloc[i][col_Predicted]))","3353e156":"sub_tmp[\"total\"] =  sum_counts\nsub_tmp = sub_tmp.sort_values('total', ascending=False)\nsub_tmp = sub_tmp[~sub_tmp.duplicated(keep='first', subset='seq_id')].fillna(\"0\")","d896a002":"sub_tmp","4dff1e29":"# Since it was difficult to join the pandas series, I intentionally created an extra column.\nsub = sub.reset_index()\nsub = sub[[\"index\", \"Id\"]].merge(sub_tmp, left_on=\"Id\", right_on=\"seq_id\")","a4d5f1a8":"sub = sub[[\"Id_x\"] + col_Predicted].rename(columns={\"Id_x\": \"Id\"})\nsub.to_csv(\"sub.csv\", index=False)","a4af8dfc":"sub.head()","f984bc27":"#If we don't delete them, csv files are buried and cannot be retrieved.\n!rm -r croped_images_train\n!rm -r croped_images_test","f95621ff":"Add seq_id information to the counted results. iwildcam2021_test_information.json contains the mapping between the id of the image and the id of the sequence.","a8685e8d":"Convert to pandas dataframe.","e4509347":"### create train dataframe","bbd01f80":"## Sample solution part of [iWildCam 2021 - Starter Notebook](https:\/\/www.kaggle.com\/nayuts\/iwildcam-2021-starter-notebook).","231c60ef":"Take right join on the image id.","a09985a0":"For each image, count the number of each animal species and store them in the corresponding column.","89b87e18":"Since there are multiple lines for the same sequence ID. We should aggregate them to single line. In this case, we will choose the image with the highest number of animals shown and submit the animal species and the number of animals shown in that image.","ffeacff2":"# Create submit file","18b557ef":"### unzip croped data","3b5a044d":"## Create dataset for training","8e92ee91":"## Create dataset for test","e0434c46":"# Train","146b9d70":"## Train\n\nSince we know that [the training data is imbalanced](https:\/\/www.kaggle.com\/nayuts\/iwildcam-2021-overviewing-for-start#EDA), I undersampled it.","04e7c92f":"I'll match the result to the sample submission format. I was told that the order of the rows is not related to the score, but we will match it just in case.","16b367b4":"## inference","8ef9071d":"### setting","15338dfc":"I haven't beaten kaggle_sample_all_zero_iwildcam_2021.csv yet, but I will publish the idea.\n\n1. First we crop the image based on the bbox detected by MegaDetector.\n2. In the training data, the correct answer labels are given as annotations, so we can use them to train the model.\n3. Classify the cropped images of the test data with the trained model.\n4. We choose the animal species and their counts of the image with the highest count among the images in the same image burst.\n\nCropping is time consuming, so I did it on [a different notebook](https:\/\/www.kaggle.com\/nayuts\/256-x-256-cropped-images). This notebook is also available to the public.","a7494621":"## Create model","6fa9b14f":"## Load trained model","c403da7f":"## train setting","93d6ae53":"<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/iwildcam2021\/model_image.png\" width=\"***300***\">","5bcf1cbb":"# Inference"}}