{"cell_type":{"9efb8b27":"code","23a0eabb":"code","43a775d6":"code","fa6a0172":"code","6bdbff9d":"code","2cdbd5eb":"code","6ec4ef5d":"code","4818c723":"code","039300fd":"code","fb39eebd":"code","4e608b7f":"code","53db4d99":"code","e677eb68":"code","acff59f9":"code","d67c2163":"code","ed0624ce":"code","0d75ceb2":"code","7efcc109":"code","6736f9c1":"code","53783ae7":"code","0e59c5cd":"code","77c29c6c":"code","e345d05b":"code","ac246340":"code","053d944b":"code","f5ad1b75":"code","3f588938":"code","ebc47292":"code","64e5b876":"code","2524e117":"code","77c89f02":"code","857c2683":"code","a9b139fd":"code","e657a821":"markdown","d89a37fe":"markdown","c5ec88d2":"markdown","b579bbe5":"markdown","6050e8a7":"markdown","add3273b":"markdown","dc0bc3e7":"markdown","0feeaf0b":"markdown","341c9b83":"markdown","18f97c5b":"markdown","32f54ec1":"markdown","ce7b0eec":"markdown","3cd5e1ea":"markdown","3dba9dfb":"markdown","aa4e473b":"markdown","4e4f3c2a":"markdown","00d9b4f6":"markdown","62fe6db1":"markdown","78c4337e":"markdown","b66515e6":"markdown","7fbf4997":"markdown"},"source":{"9efb8b27":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport datetime\nimport calendar","23a0eabb":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n","43a775d6":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","fa6a0172":"import warnings\nwarnings.filterwarnings(\"ignore\")","6bdbff9d":"path_in = '..\/input\/'\nprint(os.listdir(path_in))","2cdbd5eb":"train_data = pd.read_csv(path_in + 'train.csv', parse_dates = ['datetime'],\n                         index_col='datetime', infer_datetime_format=True)\ntest_data = pd.read_csv(path_in + 'test.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)\nsamp_subm = pd.read_csv(path_in+'sampleSubmission.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)","6ec4ef5d":"def plot_bar(data, feature):\n    \"\"\" Plot distribution \"\"\"\n    \n    fig = plt.figure(figsize=(5,3))\n    sns.barplot(x=feature, y='count', data=data, palette='Set3',orient='v')","4818c723":"def plot_timeseries(data, feature):\n    \"\"\" Plot timeseries \"\"\"\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(data.index, data[feature])\n    plt.title(feature)\n    plt.grid()","039300fd":"def plot_timeseries_train_and_predict(train, predict, year, month):\n    \"\"\" Compare train and predict data for a month \"\"\"\n    \n    start_date = datetime.datetime(year, month, 1, 0, 0, 0)\n    last_day_of_month = calendar.monthrange(year, month)[1]\n    end_date = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(train[start_date: end_date].index, train.loc[start_date:end_date, 'count'], 'b', label = 'train')\n    plt.plot(predict[start_date: end_date].index, predict.loc[start_date:end_date, 'count'], 'r', label = 'predict')\n    plt.title('Train and Predict')\n    plt.legend()\n    plt.grid()","fb39eebd":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    \n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","4e608b7f":"# Parameters\nnum_months_per_year = 12\nyear_list = [2011, 2012]","53db4d99":"train_data.head()","e677eb68":"test_data.head()","acff59f9":"month = 5\nyear = 2011\nstart_date = datetime.datetime(year, month, 1, 0, 0, 0)\nend_date = datetime.datetime(year, month, 19, 23, 0, 0)\n# train_data['count_log'] = np.log1p(train_data['count'])\n# train_data['rolling_mean'] = train_data['count'].rolling(window = 24).mean()\n# train_data['rolling_std'] = train_data['count'].rolling(window = 24).std()\n","d67c2163":"train_data_temp = pd.DataFrame(columns=train_data.columns)\n\nfor year in year_list:\n    for month in range(num_months_per_year):\n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        # Fill missing timestamps\n        temp = train_data[start_date:end_date].resample('H').asfreq()\n        # Handle missing values\n        features_fill_zero = ['casual', 'registered', 'count']\n        temp[features_fill_zero] = temp[features_fill_zero].fillna(0)\n        features_fill_bbfil = ['season', 'holiday', 'workingday', 'weather']\n        temp[features_fill_bbfil] = temp[features_fill_bbfil].fillna(method='bfill')\n        features_fill_linear = ['temp', 'atemp', 'humidity', 'windspeed']\n        temp[features_fill_linear] = temp[features_fill_linear].interpolate(method='linear')\n        \n        train_data_temp = train_data_temp.append(temp)\n        \ntrain_data = train_data_temp","ed0624ce":"train_data['weekday'] = train_data.index.weekday\ntrain_data['hour'] = train_data.index.hour\ntest_data['weekday'] = test_data.index.weekday\ntest_data['hour'] = test_data.index.hour","0d75ceb2":"train_data.head()","7efcc109":"plot_bar(train_data, 'season')\nplt.grid()","6736f9c1":"plot_bar(train_data, 'weekday')\nplt.grid()","53783ae7":"plot_bar(train_data, 'hour')\nplt.grid()","0e59c5cd":"def hour_group(s):\n    if((0<=s) & (s<=6)):\n        return 1\n    elif((s==7) | (s==9)):\n        return 2\n    elif((s==8) | (s==16) | (s==19)):\n        return 3\n    elif((10<=s) & (s<=15)):\n        return 4\n    elif((s==17) | (s==18)):\n        return 5\n    elif(20<=s):\n        return 6","77c29c6c":"train_data['hour_group'] = train_data['hour'].apply(hour_group)\ntest_data['hour_group'] = test_data['hour'].apply(hour_group)","e345d05b":"# features_cyc = ['hour', 'weekday']\n# for feature in features_cyc:\n#     train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])\/max(train_data[feature]))\n#     train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])\/max(train_data[feature]))\n#     test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])\/max(test_data[feature]))\n#     test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])\/max(test_data[feature]))\n# train_data = train_data.drop(features_cyc, axis=1)\n# test_data = test_data.drop(features_cyc, axis=1)","ac246340":"features_one_hot = ['weekday', 'hour_group', 'weather']\ntrain_data[features_one_hot] = train_data[features_one_hot].astype(int).astype(str)\ntest_data[features_one_hot] = test_data[features_one_hot].astype(int).astype(str)","053d944b":"train_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)","f5ad1b75":"scale_features = ['temp', 'atemp', 'humidity', 'hour', 'windspeed']","3f588938":"scaler = MinMaxScaler()\ntrain_data[scale_features] = scaler.fit_transform(train_data[scale_features])\ntest_data[scale_features] = scaler.transform(test_data[scale_features])","ebc47292":"# Features\nfeature_list = ['holiday', 'workingday', 'weather', 'temp', 'atemp',\n                'humidity', 'windspeed', 'hour_group',\n                'hour', 'weekday']\nno_features = ['casual', 'registered', 'count']","64e5b876":"predictions = []\nfor year in year_list:\n    for month in range(num_months_per_year):\n        # Train model\n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        X_train = train_data[start_date:end_date][train_data.columns.difference(no_features)].copy()\n        y_train = train_data[start_date:end_date]['count'].copy()\n        \n        y_train = np.log1p(y_train)\n\n        #model = XGBRegressor(n_estimators = 100, random_state=2020)\n        model_rfr = RandomForestRegressor(n_estimators=500, n_jobs=-1, max_features='auto')\n        model_gbr = GradientBoostingRegressor(n_estimators=1000)\n        model_rfr.fit(X_train, y_train)\n        model_gbr.fit(X_train, y_train)\n\n        # Predict test data\n        start_date = datetime.datetime(year, month+1, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year, month+1)[1]\n        end_date = datetime.datetime(year, month+1, last_day_of_month, 23, 0, 0)\n        X_test = test_data[start_date:end_date][train_data.columns.difference(no_features)].copy()\n        \n        y_test_rfr = model_rfr.predict(X_test)\n        y_test_gbr = model_rfr.predict(X_test)\n        \n        y_test = 0.0 * y_test_gbr + 1.0 * y_test_rfr\n        y_test = np.expm1(y_test)\n        \n        predictions.extend(y_test)","2524e117":"predictions = [0 if i < 0 else i for i in predictions]","77c89f02":"output = pd.DataFrame({'datetime': test_data.index,\n                       'count': predictions})\noutput.to_csv('submission.csv', index=False)","857c2683":"predict = pd.DataFrame(index=output['datetime'])\npredict['count'] = output['count'].values\npredict.head()\nplot_timeseries_train_and_predict(train_data, predict, 2011, 2)","a9b139fd":"fig = plt.figure(figsize=(16,9))\nplt.plot(train_data.index, train_data['count'], 'b', label = 'train')\nplt.plot(output['datetime'],output['count'], 'r', label = 'test')\nplt.title('Train and Test')\nplt.legend()\nplt.grid()","e657a821":"Set negative values to zero:","d89a37fe":"# Analyse Results","c5ec88d2":"# Input path","b579bbe5":"## Feature Season","6050e8a7":"## Trend","add3273b":"# Predict Monthly","dc0bc3e7":"The datetime and the seasons are cyclic features. So we can use a cyclic encoding for it.","0feeaf0b":"# Functions","341c9b83":"## Feature Hour\nAdd new feature hour_group by group of hours.","18f97c5b":"## Feature Weekday","32f54ec1":"# Encoding \n## Cyclic features","ce7b0eec":"# Scale Data","3cd5e1ea":"# EDA\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.","3dba9dfb":"# Missing Timestamps\nThere are losts of missing hours in the train dataset. We expect $ 2 years*12 months*19 days *24 hours = 10944 timesteps$. We count 10886 timesteps so there are 58 missing. Every month in the train data set hast 456 timestamps.  ","aa4e473b":"## One Hot Encoding for categorical variables","4e4f3c2a":"# Load Libraries","00d9b4f6":"# Intro \nWelcome to the [Bike Sharing](https:\/\/www.kaggle.com\/c\/bike-sharing-demand) Demand Competition\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3948\/media\/bikes.png)\n\nThis notebook is a starter code for all beginners and easy to understand. We will give an introduction to analysis and feature engineering.<br> \nTherefore we focus on\n* a simple analysis of the data,\n* create new features,\n* encoding and\n* scale data.\n\nWe use categorical feature encoding techniques, compare <br>\nhttps:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb <br>\n\nFor this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Thank you. <\/span>","62fe6db1":"Fill missing timestamps:","78c4337e":"# Load Data","b66515e6":"# Create new features\nBased on the datetime we create new features for the month, the weekday the hour and the year. These are also cyclic features.","7fbf4997":"# Generate Output"}}