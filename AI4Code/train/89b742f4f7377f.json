{"cell_type":{"fbbfe5f0":"code","98eabd24":"code","a4fdf6f9":"code","0cb2029e":"code","8a898425":"code","c67bfa37":"code","d5e63440":"code","b80b9b19":"code","72928c13":"code","804a7236":"code","b29f7449":"code","644d112e":"code","7e240a13":"code","4762ec6c":"code","eb86ec52":"code","e4e38354":"code","aca4910c":"code","cdd9131f":"code","9cd6ea7b":"code","c7561e06":"code","a6f3972d":"code","a87247f3":"code","5b6f10d0":"code","04d4f576":"code","7017134d":"code","b8e4e4cd":"code","cb0b51f3":"code","bb25aa61":"code","00f907a4":"code","5e4fe54c":"code","e539cb02":"code","735e0009":"code","821685ea":"code","ad82bf7b":"code","830c67f9":"code","ad2d08af":"code","86874eea":"code","70f2d6e1":"code","56ace471":"code","5a9b0c7e":"code","3f26f2f4":"code","90b6b69e":"code","b653746c":"code","1d055843":"code","eb06ba61":"code","282bb7b7":"code","380a10e2":"code","320e87b9":"code","01c74322":"code","e389269a":"code","9df80700":"code","522fc533":"code","d75872f7":"code","f195013f":"code","3fa81053":"code","dc70fcda":"code","54bbe2f0":"code","827920d6":"code","3a709888":"code","61c0fc17":"code","5ddad7c8":"code","e063c41a":"markdown","2c794e0a":"markdown","dc86b4a1":"markdown","4c94f431":"markdown","4498b10b":"markdown","ac6ffce4":"markdown","fd81a1d5":"markdown","e696ff26":"markdown","25b94ba5":"markdown","4d9ec1b1":"markdown","1c39eda0":"markdown","24f7c0ec":"markdown","6cc91e75":"markdown","13525b2f":"markdown","7f4ae649":"markdown","0427144c":"markdown","4596ae10":"markdown","98dc6060":"markdown","ffc933bf":"markdown","aadf985b":"markdown","32a9f9d4":"markdown","01b79209":"markdown","86f6bb52":"markdown","88a2d7b3":"markdown","128eddf0":"markdown","465d61f0":"markdown","430eb060":"markdown","b61ea9e2":"markdown","4e6fb2d6":"markdown","81319c5b":"markdown","e4d27703":"markdown","82ee99f7":"markdown"},"source":{"fbbfe5f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# optuna\nimport optuna\nfrom optuna.samplers import TPESampler\n\n#sklearn\nfrom sklearn.ensemble import GradientBoostingClassifier,StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures,StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_curve, auc\n\n#xgboost\nfrom xgboost import XGBClassifier\n\n#catboost\nfrom catboost import CatBoostClassifier\n\n#lightboost\nimport lightgbm as lgb\n\n\n\nfrom string import ascii_letters\nimport os\nimport pickle\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98eabd24":"plt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","a4fdf6f9":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","0cb2029e":"!apt-get install -y -qq libboost-all-dev","8a898425":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","c67bfa37":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","d5e63440":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","b80b9b19":"train=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")","72928c13":"train.select_dtypes(\"float64\").shape[1]==train.shape[1]-2","804a7236":"sns.displot(train[\"claim\"])","b29f7449":"from string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"white\")\n\n\n\n# Compute the correlation matrix\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","644d112e":"train.corr()[\"claim\"].map(lambda x:abs(x)).sort_values()","7e240a13":"train.isnull().sum()","4762ec6c":"X,y=train.drop([\"id\",\"claim\"],1),train[\"claim\"]\nimputer = SimpleImputer(strategy='mean')\nX=imputer.fit_transform(X)\nX_test=imputer.transform(test.drop('id',1))","eb86ec52":"X.shape","e4e38354":"def create_stack(**kwargs):\n    '''\n    Input:\n        **Kwargs:Dict Consist of all the Main Learners that I want for the stack\n    Output:\n        Stack:Sklearn Classifier\n    '''\n    #meta learner\n    LR=LogisticRegression()\n    estimators = [(key,value) for key,value in kwargs.items()]\n    Stack = StackingClassifier(estimators=estimators, final_estimator= LR, cv= 5, n_jobs= -1, passthrough = True,stack_method='predict_proba')\n    return Stack","aca4910c":"#stack=create_stack(adaboost=AdaBoostClassifier(),xgb=XGBClassifier(tree_method='gpu_hist'),\n            #lgbm=lgb.LGBMClassifier(objective=\"binary\",device='gpu'))\n#stack.fit(X,y)","cdd9131f":"#predictions=stack.predict(X_test)","9cd6ea7b":"pca=PCA(n_components=5)\npca.fit(X)","c7561e06":"#stack=create_stack(adaboost=AdaBoostClassifier(),xgb=XGBClassifier(tree_method='gpu_hist'),\n            #lgbm=lgb.LGBMClassifier(objective=\"binary\",device='gpu'))","a6f3972d":"#pipeline=Pipeline([('StandardScaler',StandardScaler()),\n      #   ('PCA',PCA(n_components=5)),('stack',stack)])","a87247f3":"plot_variance(pca, width=8, dpi=100)","5b6f10d0":"#pipeline.fit(X,y)","04d4f576":"#predictions=pipeline.predict_proba(X_test)","7017134d":"#sns.distplot(predictions[:,1])","b8e4e4cd":"X_pca=pca.transform(X)","cb0b51f3":"for i in range(X_pca.shape[1]):\n    X=np.concatenate((X,X_pca[:,i].reshape(-1,1)),axis=1)","bb25aa61":"#stack=create_stack(adaboost=AdaBoostClassifier(),xgb=XGBClassifier(tree_method='gpu_hist'),lgbm=lgb.LGBMClassifier(objective=\"binary\",device='gpu'))\n#stack.fit(X,y)","00f907a4":"X_pca_test=pca.transform(X_test)","5e4fe54c":"for i in range(X_pca_test.shape[1]):\n    X_test=np.concatenate((X_test,X_pca_test[:,i].reshape(-1,1)),axis=1)","e539cb02":"#predictions=stack.predict_proba(X_test)","735e0009":"del X_pca_test\ndel X_pca","821685ea":"def objective_xgboost(trial):\n\n    param_grid = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': trial.suggest_int('n_estimators', 1000, 5000),\n              'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n              'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.1),\n              'colsample_bytree': trial.suggest_uniform('colsample_bytree',0.1,1.0),\n              'max_depth': trial.suggest_int('max_depth', 3, 20),\n              'booster': 'gbtree',\n              'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n              'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n              'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n              'random_state': 42,\n                 }\n\n\n    xgb_model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor')\n\n    xgb_model.fit(x_train, y_train, verbose=False)\n    y_pred = xgb_model.predict_proba(x_val)[:, 1]\n    return roc_auc_score(y_val, y_pred)","ad82bf7b":"def xgboost_optimize():\n    train_time = 1 * 30 * 60 # h * m * s\n    study = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='XGBClassifier')\n    study.optimize(objective_xgboost, timeout=train_time)\n\n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    trial = study.best_trial\n\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print('\\t\\t{}: {}'.format(key, value))","830c67f9":"def objective_lightgbm(trial):\n    param = {\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        }\n\n\n    lgb_model = lgb.LGBMClassifier(**param,device='gpu')\n\n    lgb_model.fit(x_train, y_train, verbose=False)\n    y_pred = lgb_model.predict_proba(x_val)[:, 1]\n    return roc_auc_score(y_val, y_pred)","ad2d08af":"def light_gbm_optimize():\n    train_time = 1 * 30 * 60 # h * m * s\n    study = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='LightGBMClassifier')\n    study.optimize(objective_lightgbm, timeout=train_time)\n\n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    trial = study.best_trial\n\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print('\\t\\t{}: {}'.format(key, value))","86874eea":"#For C Strategy\nparms_xg={'n_estimators': 2293,'learning_rate': 0.04,'subsample': 0.6000000000000001,'colsample_bytree': 0.10086730985728953,\n'max_depth': 19,'gamma': 5.050049288179512,'reg_alpha': 90,'reg_lambda': 50}\n\nparms_lgbm={'lambda_l1': 0.7625376202750715,\n'lambda_l2': 2.917592113412615e-05,\n'num_leaves': 172,\n'feature_fraction': 0.4767313692164907,\n'bagging_fraction': 0.9929830736818223,\n'bagging_freq': 7,\n'min_child_samples': 74}\n\n#stack=create_stack(xgb=XGBClassifier(**parms_xg,tree_method='gpu_hist'),\n                  # lgbm=lgb.LGBMClassifier(**parms_lgbm,device='gpu'))","70f2d6e1":"#stack.fit(X,y)","56ace471":"#predictions=stack.predict_proba(X_test)","5a9b0c7e":"# feature Engineering\ndef get_stats_per_row(data):\n    data['mv_row'] = data.isna().sum(axis=1)\n    data['min_row'] = data.min(axis=1)\n    data['std_row'] = data.std(axis=1)\n    return data","3f26f2f4":"X,y=train.drop([\"id\",\"claim\"],1),train[\"claim\"]\nX_test=test.drop('id',1)","90b6b69e":"X = get_stats_per_row(X)\nX_test = get_stats_per_row(X_test)","b653746c":"pipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\nX = pd.DataFrame(columns=X.columns, data=pipeline.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=pipeline.transform(X_test))","1d055843":"del train","eb06ba61":"def k_fold(model_instance,param=None):\n    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n    pred_tmp = []\n    scores = []\n\n    for fold, (idx_train, idx_valid) in enumerate(kf.split(X)):\n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n        if param==None:\n            model=model_instance\n        else:\n            model = model_instance(**param)\n        model.fit(X_train, y_train)\n\n        # validation prediction\n        pred_valid = model.predict_proba(X_valid)[:,1]\n        fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n        score = auc(fpr, tpr)\n        scores.append(score)\n\n        print(f\"Fold: {fold + 1} Score: {score}\")\n        print('::'*15)\n\n        # test prediction\n        y_hat = model.predict_proba(X_test)[:,1]\n        pred_tmp.append(y_hat)\n    print(f\"Overall Validation Score: {np.mean(scores)}\")\n    return pred_tmp\n\n    ","282bb7b7":"best_params_catboot = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}","380a10e2":"pred_tmp=k_fold(CatBoostClassifier,best_params_catboot)","320e87b9":"# average predictions over all folds\npredictions_catboost = np.mean(np.column_stack(pred_tmp),axis=1)","01c74322":"best_parms_xg={'n_estimators': 4995,'learning_rate': 0.04,'subsample': 0.8,'colsample_bytree': 0.7934918764052317,\n'max_depth': 3,'gamma':  2.143727883794004,'reg_alpha': 74,'reg_lambda': 89,'tree_method':'gpu_hist'}","e389269a":"x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=0)","9df80700":"#xgboost_optimize()","522fc533":"pred_tmp=k_fold(XGBClassifier,best_parms_xg)","d75872f7":"# average predictions over all folds\npredictions_xgboost = np.mean(np.column_stack(pred_tmp),axis=1)","f195013f":"best_parms_lgbm={'lambda_l1':2.217260840183882,\n'lambda_l2': 0.00018113739596471642,\n'num_leaves': 60,\n'feature_fraction': 0.840321554033748,\n'bagging_fraction':0.9762295967509231,\n'bagging_freq': 4,\n'min_child_samples': 27,\n'device':'gpu'}","3fa81053":"x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=0)","dc70fcda":"#light_gbm_optimize()","54bbe2f0":"pred_tmp=k_fold(lgb.LGBMClassifier,best_parms_lgbm)","827920d6":"# average predictions over all folds\npredictions_lightgbm = np.mean(np.column_stack(pred_tmp),axis=1)","3a709888":"final_prediction=(0.20*predictions_lightgbm)+(0.5*predictions_xgboost)+(0.30*predictions_catboost)","61c0fc17":"submission=pd.DataFrame({\n    \"id\":test[\"id\"],\n    \"claim\":final_prediction\n    \n})","5ddad7c8":"submission.to_csv(\"submission.csv\",index=False)","e063c41a":"![image.png](attachment:8d34eb3f-4b3c-48d1-8967-b16aac93d94e.png)","2c794e0a":"#### 1- Xgboost","dc86b4a1":"<p> Stack is Not Working; Lets Try Individual Models and some other kind of feature engineering <\/p>\n\n","4c94f431":"#### Helper Functions","4498b10b":"## Weightage Mean Submission","ac6ffce4":"<img src=\"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png\">","fd81a1d5":"![image.png](attachment:7cefd048-77fa-404c-a973-e8d30ac78d68.png)","e696ff26":"#### LightGBM GPU Support","25b94ba5":"### K-Fold Training Stratgy","4d9ec1b1":"![image.png](attachment:6c29c750-b47d-4982-acbc-4eaa9b122170.png)","1c39eda0":"#### 2- LightGBM","24f7c0ec":"### 2-Xgboost","6cc91e75":"**<p> The dataset seems balanced which is a good thing :D <\/p>**","13525b2f":"![image.png](attachment:daf56c9b-18ee-4fde-abee-55f7e6ec2948.png)","7f4ae649":"**<p> Took the best params for Catboost and K_fold training Strategy from [tps-09-single-catboostclassifier](https:\/\/www.kaggle.com\/mlanhenke\/tps-09-single-catboostclassifier)<\/p>**","0427144c":"![image.png](attachment:f0a7f857-4bef-4ec1-990b-a898f4eb5340.png)","4596ae10":"# Using Mean Imputing for Strategies A,B,C","98dc6060":"### 1-Catboost","ffc933bf":"# Stack Net Architecture Used for A,B,C","aadf985b":"## D) Individual Boosting Models-(StandardScaling+Simple Mean Imputation+Statistical Feature Engineering)","32a9f9d4":"![image.png](attachment:0e429715-2833-4058-b486-f715737e2760.png)","01b79209":"--------------------------------------------------------------------------------------------------------------------------","86f6bb52":"## B) Featuring Engineering(PCA+Simple Mean Imputing+StandardScaler)","88a2d7b3":"> ## Hyper-Parameter Tuning(Optuna) for Xgboost and LightGBM given a Strategy\n        Please Note:\n        1- Removing Adaboost because tunning it takes alot of running time\n        2- Going to use this for Strategy C and D","128eddf0":"_________________________________________________________________________________________________","465d61f0":"## A) Base Prediction(Simple Mean Imputing)","430eb060":"<img src=\"https:\/\/miro.medium.com\/max\/700\/1*MiJ_HpTbZECYjjF1qepNNQ.png\">","b61ea9e2":"### Score on Leaderboard","4e6fb2d6":"![image.png](attachment:9cfcca5c-b683-4f3d-b785-e9dd1ea5be5b.png)","81319c5b":"### 3-LightGBM","e4d27703":"## C) Feature Engineering(Adding PCA Features+Simple Mean Imputing)","82ee99f7":"**<p> No Multi-Collinearity at all which is a good thing <\/p>**"}}