{"cell_type":{"cdc2c204":"code","f706bd7c":"code","13a0813a":"code","3014541e":"code","b86e5d8c":"code","3c331e98":"code","379a6fdb":"code","e08362f5":"code","586243bf":"code","ab1bcbd8":"code","eda4be6e":"code","873c9dad":"code","5c1836f0":"code","81a72bf0":"code","22b093ce":"code","2d6e6f40":"code","179d98ef":"code","3a02bb8b":"code","29c77573":"code","8ba01a78":"code","d0d99ffd":"code","22698cad":"code","16e36258":"code","454d4345":"code","2dc8ba76":"code","60c3be78":"code","51bb22ba":"code","9497d138":"code","bc87af9c":"code","44279448":"code","b2c7b403":"code","2e3a2e43":"code","9d744c3d":"code","49fb5260":"code","90ce9967":"code","2bb2ff32":"code","3dcfa40c":"code","def4e36a":"code","0dbaa674":"code","7a132a44":"code","663f85e8":"code","081797e7":"code","0e2794ec":"code","0d1afd90":"code","d5917bcc":"code","c39c43fc":"code","430ced80":"code","58a807c1":"code","dae16b2e":"code","f7522b68":"code","eedfc214":"code","c8402612":"code","0af160ac":"code","8f435da7":"code","29002507":"code","fa5945e4":"code","da72ccfa":"code","6ddd84ff":"code","8b7d943d":"code","861b9ecb":"code","43890c60":"code","bbb5861f":"code","20193d3d":"code","87b56cf3":"code","82a87ac4":"code","6edcff07":"code","e61f7059":"markdown","3e3345eb":"markdown","9401c23a":"markdown","49f97faa":"markdown","9439513d":"markdown","a610c1cb":"markdown","353f5685":"markdown","477658ec":"markdown","5480b86f":"markdown","b99b5635":"markdown","dc2eb41d":"markdown","f7f8d545":"markdown","184a61cc":"markdown","541428a7":"markdown","bf4dba63":"markdown","9f732c70":"markdown","70718ff3":"markdown","0e76ec81":"markdown","65b03564":"markdown","9f9c7939":"markdown","29ef1bec":"markdown","a1c02181":"markdown"},"source":{"cdc2c204":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f706bd7c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt","13a0813a":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","3014541e":"train.shape","b86e5d8c":"train.head(20)","3c331e98":"train.describe(include = 'all')","379a6fdb":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e08362f5":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived', ascending=False)","586243bf":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ab1bcbd8":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=True)","eda4be6e":"sns.countplot(x= 'Survived', data = train  )","873c9dad":"sns.countplot(x= 'Sex',hue = 'Survived', data = train  )","5c1836f0":"sns.countplot(x= 'Embarked',hue = 'Survived', data = train  )","81a72bf0":"sns.countplot(x= 'Pclass',hue = 'Survived', data = train  )","22b093ce":"sns.catplot(x= 'SibSp',y = 'Survived',kind = 'bar', data = train  )","2d6e6f40":"sns.catplot(x= 'Parch',y = 'Survived',kind = 'bar', data = train  )","179d98ef":"sns.catplot(x= 'Pclass',y = 'Embarked',hue = 'Survived',kind = 'violin', data = train  )","3a02bb8b":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=15)","29c77573":"g = sns.FacetGrid(train, col='Survived',row = 'Pclass')\ng.map(plt.hist, 'Fare', bins=15)","8ba01a78":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","d0d99ffd":"sns.catplot(y = 'Pclass',  x = 'Embarked', kind = 'bar',col = 'Survived',row = 'Sex' ,data = train)","22698cad":"\ntrain.isnull().sum()","16e36258":"sns.heatmap(train.isnull())","454d4345":"train_df = train.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df] #Combinning both the table otherwise we have do thesame things twice.","2dc8ba76":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False) #it will give the strings which is end up with '.' .\n","60c3be78":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean() \n","51bb22ba":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","9497d138":"#Let's see the relation betwwen title and survived columns.\nsns.barplot(y = 'Survived', x = 'Title', data = train_df)","bc87af9c":"#Now safely delete the Name Name from both of the dataset only PassengerId from the train Dateset.\ntrain_df = train_df.drop(['Name','PassengerId'], axis = 1)\ntest_df = test_df.drop(['Name'], axis = 1)\ncombine =[train_df,test_df]","44279448":"#Let's handle the null values.\ntrain_df.isnull().sum()","b2c7b403":"#test['Fare'].fillna(test['Fare'].mean(),inplace = True)","2e3a2e43":"# An example of visualizing the outliers.\nsns.boxplot( x = 'Age', data = train_df)","9d744c3d":"# detecting outliers\n\nQ1 = train_df.quantile(.25)\nQ3 = train_df.quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n#for the test data\nQ1 = test_df.quantile(.25)\nQ3 = test_df.quantile(.75)\nIQR = Q3 - Q1\nprint(IQR)\n","49fb5260":"print(train_df < (Q1 -1.5 * IQR)) or (train_df > (Q3  + 1.5 * IQR))","90ce9967":"#Replace the outliers of age columns with the median value\nprint(train_df['Age'].quantile(.50))\nprint(train_df['Age'].quantile(.95))\nprint(train_df['Age'].quantile(.75))\n\n#for the test data\n\ntest_df['Age'].quantile(.50)\ntest_df['Age'].quantile(.95)\ntest_df['Age'].quantile(.75)\n","2bb2ff32":"train_df['Age'] = np.where(train_df['Age']>54.0, 35.0, train_df['Age'])\n\n\n# test data\ntest_df['Age'] = np.where(test_df['Age']>54.0, 35.0, test_df['Age'])\nCombine = [train_df , test_df]","3dcfa40c":"train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())\ntrain_df['Embarked'] = train_df['Embarked'].fillna('S')\ntest_df['Age'] = test_df['Age'].fillna(test_df['Age'].mean())  \ntest_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())        ","def4e36a":"test_df.isnull().sum()","0dbaa674":"#Earlier we seen that catagory of age has a great impact on the Survival .So let's make some Age group\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","7a132a44":"train_df.head()","663f85e8":"#Let' create a new features from wxixsting SibSp and Parch.\nfor dataset in combine:\n     dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n     dataset.loc[dataset['FamilySize'] > 1, 'FamilySize'] = 1\n     dataset.loc[dataset['FamilySize'] < 0, 'FamilySize'] = 0\n    ","081797e7":"train_df.head()","0e2794ec":"train_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_df, test_df]\n\n","0d1afd90":"train_df['FareBand'] = pd.cut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean()","d5917bcc":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n   \ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head()","c39c43fc":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'male': 0, 'female': 1} ).astype(int)\ntrain_df.head()","430ced80":"\ncols_to_scale = ['Pclass','Sex','Age','Fare','Embarked','Title']\nscaler = MinMaxScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])","58a807c1":"train_df.head()","dae16b2e":"X = train_df.iloc[:,1:8]\ny = train_df.Survived\nX.head()","f7522b68":"from sklearn.model_selection import train_test_split","eedfc214":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33)\n","c8402612":"import xgboost\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error","0af160ac":"from xgboost import XGBClassifier\nXGBClassifier()","8f435da7":"params = {\n        'learning_rate': [.05, .10, .15, .20, .25, .40, .50, .80, .6, .90],\n        'n_estimators' : [100, 200, 250, 350, 400,500,600],\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,7, 8, 10, 12]\n        }","29002507":"xgb = XGBClassifier( objective='binary:logistic',\n                    silent=True, nthread=1)","fa5945e4":"folds = 10\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf, verbose=3, random_state=1001 )\n\n\nrandom_search.fit(X_train, y_train)","da72ccfa":"random_search.best_estimator_","6ddd84ff":"random_search.best_score_","8b7d943d":"random_search.best_params_","861b9ecb":"results = pd.DataFrame(random_search.cv_results_)\nresults","43890c60":"model = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=5,\n              min_child_weight=10, monotone_constraints='()',\n              n_estimators=200, n_jobs=1, nthread=1, num_parallel_tree=1,\n              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              subsample=0.8, tree_method='exact',\n              validate_parameters=1, verbosity=None)\n","bbb5861f":"model.fit(X_train, y_train)","20193d3d":"y_pred = model.predict(X_test)","87b56cf3":"mse = mean_squared_error(y_test, y_pred)\nprint(mse)","82a87ac4":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred, y_test))","6edcff07":"Pid = test_df['PassengerId']\npred = model.predict(test_df.drop(['PassengerId'],axis = 1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : Pid, 'Survived': pred })\noutput.to_csv('submission.csv', index=False)","e61f7059":"Let's have the corelation of numerical features.","3e3345eb":"\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values and only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.","9401c23a":"Submitting:","49f97faa":"From above 20 rows make some assumtions.\n\n* 1.The main thing to notice here is female passengers are more likley to survive. \n* 2. And in the Parch features out of 20 only 5 has the non zero value.this means that most of the passengers were travelling without parents and child.\n* 3.And Passengers  were get into ship mostly from 'S' location in the enbarked columns also has a fare survival rate.But who were form 'C' location survived all of them 3 out of 3. It also means that people from 'S' location were  very much exited for the trip or anything  in this journy by titanic which was the world largest ship at that time.\n* 4.Passengers were in the no. 1 class more likely to survived here Out 4 , 3 of them survived.Who were in the 3 no class more likely ot death only 3 out 14 survived.  ","9439513d":"# Titanic Survival Prediction","a610c1cb":"let's have some insights.\n* we are seeing that 75% people were below 38 years old.So hardly (5-10)% were old.\n* 75% passengers wrer travelling were having Prach = 0.Which was one of our assumption and and which comes to true.\n* And another interesting thing is out of 891 tickets only 681 are unique and  210 passengers were travelling without ticket.Which means there  were some duplicate tickets or baby less then 5-8 years were much in number.\n ","353f5685":"# What we are going to do.\n\n*  1.Collecting Data.\n*  2.Analizing data.\n*  3.Cleaning data.\n*  4.EDA\n*  5.Feature Engineering.\n*  6.Model selection.\nThere is no hard and fast rule that we have to follow this works flow sequentially.This could interchange for the Analyzing purposes.Our main goal is to get usefull insights from our data as much as possible.","477658ec":"Here we are seeing that null values in Age and Embarked coluumns.And in both the cases it's not possible that those values were not\nexist that's why we are going to replace null values in Age with mean and in Embarked wit most occuring values.\nBut before doing that first handle the outliers.","5480b86f":"# 1.Collecting Data","b99b5635":"# 6. Model Selection","dc2eb41d":"# 3. Data Cleaning and 4.Feature and engineering","f7f8d545":"Before Deletimg the Name column let's create new feature.Because we have seen particular pattern with the title and the survuval.\nSuch as certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).","184a61cc":"\n\nAS our train and test data should be in same manner that's why we are going to follow Data Science pipelines for both the data sets,but Explore only train data.\nFrom the above information we are seeing that in the age and Cabin columns we have our most of the null value.","541428a7":"Now it's time to see how many types of features we have?\nHere I am seeing three types of features.\n* 1.Numerical:PassengerId,Age,SibSp,Parch,Fare.\n* 2.Categorical:Survived,Pclass,Sex,Embarked.\n* 3.Mixed:Cabin,Ticket(Numeric and AlphaNumeric).","bf4dba63":"Setting parameter distribution","9f732c70":"* Now Question is how to do that?\n* Here is simple  mathemtical formulato do this.\n* (train < (Q1 -1.5 * IQR)) or (train > (Q3  + 1.5 * IQR)).\n* Which means if any data points is less then (Q1 - 1.5 * IQR) or greater then (Q3 + 1.5 * IQR) are considered as outliers.\n* Here Q1 and Q3 are the 1st and 3rd quantile vaalue.","70718ff3":"Now it's time to make our model by taking best params.","0e76ec81":"# Analyze by pivoting features.","65b03564":"As life is all about actions towards the goal.So before we dig into any dataset we should have the domain knowledge and set the goal and here our goal is to find the titanic survival.Let's go and play.****","9f9c7939":"\nHere we are seeing that Age range betwween 0 -5 has higher survival rate","29ef1bec":"Before dig into the practical EDA let's try to understand the data and make some earlier Asasumptions.And later we will test this.We will not be able to suspect properly by seeing only top 5 rows and that's why let's see first 20 rows and see What's going on.......","a1c02181":"# 2. Analizing  Data"}}