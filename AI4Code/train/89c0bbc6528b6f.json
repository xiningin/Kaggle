{"cell_type":{"63b2e39d":"code","3b36d84a":"code","17ebdacf":"code","5d416fdb":"code","405beb0e":"code","1be6da1e":"code","47cc1250":"code","481503a6":"code","48e3793c":"code","a6eaabe8":"code","a87bb75b":"code","d3d2c797":"code","a3c5489e":"code","fe20771c":"code","c6c64174":"code","36e0a615":"code","4c041bc9":"code","3fe1805d":"code","8e845458":"code","5bc56c42":"code","34d666cc":"code","08260fa6":"code","7c8bf9dd":"code","d6885287":"code","a206c9f4":"code","2c094bc6":"code","8a3c6c3d":"code","f470d95e":"code","88271c13":"code","efa6d24b":"code","12674dfc":"code","e648bbfd":"code","7bcac408":"code","8d0aabc9":"code","4396c60c":"code","58a49d66":"code","88783c78":"code","fbe3e0db":"code","7f63fa47":"code","29166be5":"code","c1328aba":"code","ae08b8b6":"code","88e0c007":"markdown","6c08d861":"markdown","7df689a9":"markdown","48356688":"markdown","39fdeb44":"markdown","731fd5e5":"markdown","37d75805":"markdown","475f213b":"markdown","f8771995":"markdown","3c717e4b":"markdown","592418fb":"markdown","324c077e":"markdown","c489217d":"markdown","bbb88dfc":"markdown","c1372da9":"markdown","6e611cdb":"markdown","6522d0cf":"markdown","1c6c2337":"markdown","5b9f0478":"markdown","a98751ae":"markdown","f23a2a1d":"markdown","207fcc95":"markdown","060f7fd4":"markdown","03ee71f6":"markdown","0b7d5b64":"markdown","ae0012f2":"markdown","2cb91c16":"markdown","c7b18e40":"markdown","e5d87148":"markdown","d463c155":"markdown","0633eaf3":"markdown","ac7fd1ba":"markdown","66e1c431":"markdown","f02b5f22":"markdown","a0bf1cde":"markdown"},"source":{"63b2e39d":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nfrom sklearn.metrics import mean_squared_error # MSE metric\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder # ordinal encoding categorical variables\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\nSEED = 91 # random seed\n\npd.set_option('display.max_columns', 100)","3b36d84a":"PATH = '\/kaggle\/input\/30-days-of-ml\/'\n\nprint('Files in directory:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print('  '+os.path.join(dirname, filename))\nprint()\n\n# Load the training data\ntry:\n    df_train = pd.read_csv(PATH+'train.csv', index_col=0)\n    df_test = pd.read_csv(PATH+'test.csv', index_col=0)\n    submission = pd.read_csv(PATH+'sample_submission.csv', index_col=0)\n    print('All of the data has been loaded successfully!')\nexcept Exception as err:\n    print(repr(err))\nprint()","17ebdacf":"full_lenght_data = len(df_train) + len(df_test)\nprint(f\"train: {len(df_train)}  ({100*len(df_train)\/full_lenght_data:.3f} %)\")\nprint(f\"test:  {len(df_test)} ({100*len(df_test)\/full_lenght_data:.3f} %)\")","5d416fdb":"if not sum(df_train.isna().any()) and not sum(df_test.isna().any()):\n    print('No missing values in dataset')","405beb0e":"df_train.head(5)","1be6da1e":"CAT_FEATURES = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n                'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES","47cc1250":"# join train and test dat\ndf_full = pd.concat([df_test[CAT_FEATURES], df_train[CAT_FEATURES]]).sort_index()\ndf_full['test'] = 'train'\ndf_full.loc[df_test.index,'test'] = 'test'","481503a6":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.5, wspace=0.3)\n#sns.set_palette(\"Spectral\")\nfor i, col in enumerate(CAT_FEATURES):\n    plt.subplot(2, 4, i+1)\n    (df_full[col]\n     .groupby(df_full['test'])\n     .value_counts(normalize=True)\n     .rename('proportion')\n     .reset_index()\n     .pipe((sns.barplot, \"data\"), x=col, y='proportion', hue='test'))\n    plt.title(col)\nplt.show()","48e3793c":"df_train[NUM_FEATURES].describe()","a6eaabe8":"df_train['target'].describe()","a87bb75b":"f,ax=plt.subplots(figsize=(12,4))\nsns.histplot(df_train['target'])\nplt.legend()\nplt.xticks(np.arange(0,10.8,.5))\nplt.show()","d3d2c797":"X = df_train.drop(['target'], axis=1).copy()\ny = df_train['target'].copy()\nX_test = df_test.copy()\nX.head()","a3c5489e":"X['cat1_A'] = X['cat1'].apply(lambda x: 1 if x=='A' else 0)\nX['cat1_B'] = X['cat1'].apply(lambda x: 1 if x=='B' else 0)\nX['cat8_A'] = X['cat8'].apply(lambda x: 1 if x=='A' else 0)\nX['cat8_C'] = X['cat8'].apply(lambda x: 1 if x=='C' else 0)\nX['cat8_E'] = X['cat8'].apply(lambda x: 1 if x=='E' else 0)\n\nX_test['cat1_A'] = X_test['cat1'].apply(lambda x: 1 if x=='A' else 0)\nX_test['cat1_B'] = X_test['cat1'].apply(lambda x: 1 if x=='B' else 0)\nX_test['cat8_A'] = X_test['cat8'].apply(lambda x: 1 if x=='A' else 0)\nX_test['cat8_C'] = X_test['cat8'].apply(lambda x: 1 if x=='C' else 0)\nX_test['cat8_E'] = X_test['cat8'].apply(lambda x: 1 if x=='E' else 0)","fe20771c":"peaks_l = [0.24, 0.295, 0.36, 0.421, 0.488, 0.555, 0.622, 0.84, 0.89]\npeaks_r = [0.271, 0.321, 0.38, 0.44, 0.508, 0.575, 0.638, 0.848, 0.9]","c6c64174":"X['cont1_cat'] = X['cont1'].apply(\n    lambda x: 1 if [True for i in range(len(peaks_l)) if (x>peaks_l[i] and x<peaks_r[i])] else 0\n)\nX['cont1_cat'].value_counts()","36e0a615":"X_test['cont1_cat'] = X_test['cont1'].apply(\n    lambda x: 1 if [True for i in range(len(peaks_l)) if (x>peaks_l[i] and x<peaks_r[i])] else 0\n)\nX_test['cont1_cat'].value_counts()","4c041bc9":"EXTRA_FEATURES = ['cat1_A', 'cat1_B', 'cat8_A', 'cat8_C', 'cat8_E', 'cont1_cat']\nALL_FEATURES += EXTRA_FEATURES\nCAT_FEATURES += EXTRA_FEATURES","3fe1805d":"ordinal_encoder = OrdinalEncoder()\nX[CAT_FEATURES] = ordinal_encoder.fit_transform(X[CAT_FEATURES]).astype(int)\nX_test[CAT_FEATURES] = ordinal_encoder.transform(X_test[CAT_FEATURES]).astype(int)\n\nX.head()","8e845458":"corrMatrix = X[['cat1', 'cat1_A', 'cat1_B', 'cat8', 'cat8_A', 'cat8_C', 'cat8_E', 'cont1_cat']].corr(method='pearson', min_periods=1)\nplt.figure(figsize=(8,4))\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corrMatrix, dtype=bool), k=1)\nax = sns.heatmap(corrMatrix, annot=True, mask=mask, cbar_kws={\"shrink\": .7}, cmap='coolwarm')\nplt.show()","5bc56c42":"CAT_FEATURES.remove('cat1')\nALL_FEATURES.remove('cat1')\nCAT_FEATURES.remove('cat8')\nALL_FEATURES.remove('cat8')","34d666cc":"X[ALL_FEATURES].head(5)","08260fa6":"N_FOLD = 5\n#kf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)","7c8bf9dd":"class ContinuousTargetStratifiedKFold:\n    \"\"\"\n    Obtain stratified cross validation splits from the continuous targets\n    -------\n    Parameters\n    -------\n    - n_splits(int): Number of folds\n    - nbins(int): The criteria to bin by(from pd.cut)\n    - shuffle(bool): Whether to shuffle\n    - random_state(int):\n    -------\n    Examples\n    -------\n    >>> cskf = ContinuousTargetStratifiedKFold(n_splits=n_s)\n    >>> for fold_no, (t, v) in enumerate(skf.split(target, target)):\n            pass\n    -------\n    Sources\n    -------\n    - [Continuous Target Stratification](https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification)\n    - [How to generate a custom cross-validation generator in scikit-learn?](https:\/\/stackoverflow.com\/questions\/30040597\/how-to-generate-a-custom-cross-validation-generator-in-scikit-learn\/39721727)\n    \"\"\"\n    def __init__(self, n_splits=N_FOLD, nbins=100, shuffle=False,\n                 random_state=None):\n        self.skf = StratifiedKFold(n_splits=n_splits,\n                                   shuffle=shuffle,\n                                   random_state=random_state)\n        self.nbins = nbins\n        self.n_splits = n_splits\n\n    def split(self, X, y):\n        y = pd.qcut(y, self.nbins, labels=False) # implortant to use qcut\n        for train_index, test_index in self.skf.split(y, y):\n            yield train_index, test_index\n\n    def get_n_splits(self, X=None, y=None):\n        return self.n_splits","d6885287":"kf = ContinuousTargetStratifiedKFold(n_splits=N_FOLD, nbins=30, shuffle=True, random_state=SEED)","a206c9f4":"model_results = {'model': [], 'score': [], 'training_time': []}\n\ndef add_model_result(dic, model, score, time=None):\n    dic['model'].append(model)\n    dic['score'].append(score)\n    if time:\n        dic['training_time'].append(time)","2c094bc6":"models = {\n    'XGBoost 1': {\n        'model': XGBRegressor(\n            booster = 'gbtree',\n            eval_metric = \"rmse\",\n            n_estimators = 10000,\n            learning_rate = 0.05,\n            reg_lambda = 7,\n            reg_alpha = 25.8,\n            subsample = 0.9,\n            colsample_bytree = 0.12,\n            max_depth = 3,\n            random_state = SEED \n        ),\n        'features': ALL_FEATURES,\n        'feature_importance': 0\n    },\n\n    'XGBoost 2': {\n        'model': XGBRegressor(\n            booster = 'gbtree',\n            eval_metric = \"rmse\",\n            n_estimators = 10000,\n            learning_rate = 0.035,\n            reg_lambda = 5,\n            reg_alpha = 25.6,\n            subsample = 0.9,\n            colsample_bytree = 0.12,\n            max_depth = 3,\n            random_state = SEED \n        ),\n        'features': ALL_FEATURES,\n        'feature_importance': 0\n    },\n\n    'XGBoost 3': { \n        'model': XGBRegressor(\n            booster = 'gbtree',\n            eval_metric = \"rmse\",\n            n_estimators = 10000,\n            learning_rate = 0.04,\n            reg_lambda = 0.5,\n            reg_alpha = 24.5,\n            subsample = 0.82,\n            colsample_bytree = 0.13,\n            max_depth = 3,\n            random_state = SEED \n        ),\n        'features': [f for f in ALL_FEATURES if f not in ['cat0', 'cat2', 'cat4', 'cat9', 'cont1_cat']],\n        'feature_importance': 0\n    },\n\n    'XGBoost 4': {\n        'model': XGBRegressor(\n            booster = 'gbtree',\n            eval_metric = \"rmse\",\n            n_estimators = 10000,\n            learning_rate = 0.04,\n            reg_lambda = 5,\n            reg_alpha = 25.5,\n            subsample = 0.97,\n            colsample_bytree = 0.11,\n            max_depth = 3,\n            random_state = SEED \n        ),\n        'features':  [f for f in ALL_FEATURES if f not in ['cat0', 'cat2', 'cat3', 'cat4', 'cat5', 'cat9', 'cont1_cat']],\n        'feature_importance': 0\n    },\n    \n    # https:\/\/www.kaggle.com\/sudipbishwakarma\/30-days-of-ml-stacking-lgbm-xgboost\n    'LightGBM': {\n        'model': LGBMRegressor(\n            max_depth = 10, # reduce max depth from 18\n            learning_rate = 0.03,\n            metric = 'rmse', \n            n_estimators = 20000,\n            reg_alpha = 10.924491968127692,\n            reg_lambda = 17.396730654687218,\n            colsample_bytree = 0.11807135201147481,\n            subsample = 0.7582562557431147,\n            num_leaves = 63,\n            min_child_samples = 27,\n            max_bin = 523,\n            n_jobs = -1,\n            random_seed = SEED\n        ),\n        'features': ALL_FEATURES,\n        'feature_importance': 0\n    },\n\n    'CatBoost': {\n        'model': CatBoostRegressor(\n            loss_function = 'RMSE',\n            eval_metric = 'RMSE',\n            depth = 4,\n            l2_leaf_reg = 4.2,\n            od_type = 'IncToDec',\n            bagging_temperature = 290,\n            cat_features = [f for f in CAT_FEATURES if f not in ['cat0']],\n            iterations = 10000,\n            thread_count = 4,\n            random_seed = SEED\n        ),\n        'features': [f for f in ALL_FEATURES if f not in ['cat0']],\n        'feature_importance': 0\n    },\n}","8a3c6c3d":"model_results_level0 = {'model': [], 'score': [], 'training_time': []}\n\nfor m in models:\n    print(f\"{m}:\")\n    X[m] = np.zeros((X.shape[0],))\n    predictions_valid = np.zeros((X.shape[0],))\n    predictions = 0\n    model_fi = 0\n    mean_rmse = 0\n    \n    start_time = time.time()\n    for num, (train_idx, valid_idx) in enumerate(kf.split(y,y)):\n        # split the train data into train and validation\n        X_train = X.iloc[train_idx][models[m]['features']]\n        X_valid = X.iloc[valid_idx][models[m]['features']]\n        y_train = y.iloc[train_idx]\n        y_valid = y.iloc[valid_idx]\n        \n        model = models[m]['model']\n        model.fit(X_train, y_train,\n                  eval_set = [(X_valid, y_valid)],\n                  early_stopping_rounds = 70,\n                  verbose = False\n                 )\n        \n        # Mean of the predictions\n        predictions += model.predict(X_test[models[m]['features']]) \/ N_FOLD\n        \n        # Mean of feature importance\n        models[m]['feature_importance'] += model.feature_importances_ \/ N_FOLD \n        \n        # Out of Fold predictions\n        predictions_valid[valid_idx] = model.predict(X_valid)\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, predictions_valid[valid_idx]))\n        print(f\"Fold {num} | RMSE: {fold_rmse:.3f}\")\n        \n        mean_rmse += fold_rmse \/ N_FOLD\n        \n    X[m] = predictions_valid\n    X_test[m] = predictions\n    add_model_result(model_results_level0, m, mean_rmse, time.time()-start_time)\n    print(f\"Overall RMSE: {mean_rmse:.6f}\")\n    \npd.DataFrame(model_results_level0).sort_values('score')","f470d95e":"df_fi = pd.concat([pd.DataFrame(models[m]['feature_importance'], index=models[m]['features'], columns=[m]) for m in models],\n                  axis=1)\ndf_fi.sort_values('XGBoost 1', ascending=False)","88271c13":"df_fi = df_fi.fillna(0).apply(lambda x: x\/sum(x)*100)\ndf_fi['overall'] = df_fi.apply(lambda x: sum(x), axis=1)\ndf_fi = df_fi.apply(lambda x: x\/sum(x)*100)\ndf_fi.sort_values('overall', ascending=False)","efa6d24b":"useful_features = models.keys()","12674dfc":"N_FOLD = 5\n#kf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\nkf = ContinuousTargetStratifiedKFold(n_splits=N_FOLD, nbins=50, shuffle=True, random_state=SEED)","e648bbfd":"stacking_models_1 = {\n    'ridge': Ridge(\n        alpha = 2.5\n    ),\n    \n    'xgb': XGBRegressor(\n        booster = 'gbtree',\n        n_estimators = 3000,\n        max_depth = 1,\n        reg_alpha = 27.36157337726597,\n        reg_lambda = 33.49289864031397,\n        subsample =  0.96,\n        colsample_bytree = 0.25360286709413943,\n        min_child_weight = 6.867848163290424,\n        n_jobs = 4,\n        random_state = SEED,\n    ),\n\n    'rforrest': RandomForestRegressor(\n        n_estimators=400,\n        max_depth=4,\n        n_jobs= -1, \n        random_state= SEED,\n    ),\n\n    'catboost': CatBoostRegressor(\n        loss_function = 'RMSE',\n        eval_metric = 'RMSE',\n        depth = 1,\n        iterations = 6000,\n        early_stopping_rounds = 50,\n        od_type = 'IncToDec',\n        l2_leaf_reg = 4.482142193283575,\n        bagging_temperature = 45.91463334134044,\n        thread_count = 4, \n        verbose = False,\n        random_state = SEED,\n    ),\n    \n    'lgbm': LGBMRegressor(\n        metric = 'rmse',      \n        n_estimators = 50000,    \n        reg_alpha = 10.924491968127692,\n        reg_lambda = 17.396730654687218,\n        learning_rate = 0.09985133666265425,\n        max_depth = 5,\n        num_leaves = 5,\n        min_child_samples = 10,    \n        max_bin = 523,\n        colsample_bytree = 0.11807135201147481,\n        n_jobs = 4,   \n        random_state = SEED, \n    ),\n}","7bcac408":"model_results_level1 = {'model': [], 'score': [], 'training_time': []}\n\nfor i in stacking_models_1:\n    predictions_valid = np.zeros((X.shape[0],))\n    predictions = 0\n    mean_rmse = 0\n\n    start_time = time.time()\n    for num, (train_idx, valid_idx) in enumerate(kf.split(y,y)):\n        # split the train data into train and validation\n        X_train = X.iloc[train_idx][useful_features]\n        X_valid = X.iloc[valid_idx][useful_features]\n        y_train = y.iloc[train_idx]\n        y_valid = y.iloc[valid_idx]\n\n        model = stacking_models_1[i]\n        model.fit(X_train, y_train)\n\n        # Mean of the predictions\n        predictions += model.predict(X_test[useful_features]) \/ N_FOLD\n\n        # Out of Fold predictions\n        predictions_valid[valid_idx] = model.predict(X_valid)\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, predictions_valid[valid_idx]))\n        print(f\"Fold {num} | RMSE: {fold_rmse:.3f}\")\n\n        mean_rmse += fold_rmse \/ N_FOLD\n\n    print(f\"Meta model {i} RMSE: {mean_rmse:.6f}\\n\")\n    X[f\"pred_level1_{i}\"] = predictions_valid\n    X_test[f\"pred_level1_{i}\"] = predictions\n    add_model_result(model_results_level1, i, mean_rmse, time.time()-start_time)\n    \npd.DataFrame(model_results_level1).sort_values('score')","8d0aabc9":"useful_features = [f\"pred_level1_{i}\" for i in stacking_models_1.keys()]","4396c60c":"N_FOLD = 5\n#kf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED+1)\nkf = ContinuousTargetStratifiedKFold(n_splits=N_FOLD, nbins=100, shuffle=True, random_state=SEED+1)","58a49d66":"stacking_models_2 = {\n    'ridge': Ridge(\n        alpha = 3.0\n    ),\n\n    'catboost': CatBoostRegressor(\n        loss_function = 'RMSE',\n        eval_metric = 'RMSE',\n        depth = 1,\n        iterations = 6000,\n        early_stopping_rounds = 50,\n        od_type = 'IncToDec',\n        l2_leaf_reg = 2.427672821898561,\n        bagging_temperature = 87.74424148586161,\n        thread_count = 4, \n        verbose = False,\n        random_state = SEED+1,\n    ),\n\n    'rforrest': RandomForestRegressor(\n        n_estimators=500,\n        max_depth=4,\n        n_jobs= -1, \n        random_state= SEED+1,\n    ),\n}","88783c78":"model_results_level2 = {'model': [], 'score': [], 'training_time': []}\n\nfor i in stacking_models_2:\n    predictions_valid = np.zeros((X.shape[0],))\n    predictions = 0\n    mean_rmse = 0\n\n    start_time = time.time()\n    for num, (train_idx, valid_idx) in enumerate(kf.split(y,y)):\n        # split the train data into train and validation\n        X_train = X.iloc[train_idx][useful_features]\n        X_valid = X.iloc[valid_idx][useful_features]\n        y_train = y.iloc[train_idx]\n        y_valid = y.iloc[valid_idx]\n\n        model = stacking_models_2[i]\n        model.fit(X_train, y_train)\n\n        # Mean of the predictions\n        predictions += model.predict(X_test[useful_features]) \/ N_FOLD\n\n        # Out of Fold predictions\n        predictions_valid[valid_idx] = model.predict(X_valid)\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, predictions_valid[valid_idx]))\n        print(f\"Fold {num} | RMSE: {fold_rmse:.3f}\")\n\n        mean_rmse += fold_rmse \/ N_FOLD\n\n    print(f\"Meta model {i} RMSE: {mean_rmse:.6f}\\n\")\n    X[f\"pred_level2_{i}\"] = predictions_valid\n    X_test[f\"pred_level2_{i}\"] = predictions\n    add_model_result(model_results_level2, i, mean_rmse, time.time()-start_time)\n    \npd.DataFrame(model_results_level2).sort_values('score')","fbe3e0db":"useful_features = [f\"pred_level2_{i}\" for i in stacking_models_2.keys()]","7f63fa47":"N_FOLD = 7\n#kf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED+2)\nkf = ContinuousTargetStratifiedKFold(n_splits=N_FOLD, nbins=200, shuffle=True, random_state=SEED+2)","29166be5":"predictions_valid = np.zeros((X.shape[0],))\nfinal_predictions = 0\nmean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(kf.split(y,y)):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx][useful_features]\n    X_valid = X.iloc[valid_idx][useful_features]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    model = Ridge(alpha=3.0)\n    model.fit(X_train, y_train)\n    \n    # Mean of the predictions\n    final_predictions += model.predict(X_test[useful_features]) \/ N_FOLD\n    \n    # Out of Fold predictions\n    predictions_valid[valid_idx] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, predictions_valid[valid_idx]))\n    print(f\"Fold {num} | RMSE: {fold_rmse:.3f}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse:.6f}\")","c1328aba":"final_predictions","ae08b8b6":"output = pd.DataFrame({'Id': df_test.index,\n                       'target': final_predictions})\noutput.to_csv('submission.csv', index=False)","88e0c007":"# 2. Exploratory Data Analysis (EDA)","6c08d861":"In the las step I use Ridge regression model with alpha hyperparameter = 3.0","7df689a9":"Reshuffle folds","48356688":" ## _If you find it useful please upvote_","39fdeb44":"### Target","731fd5e5":"### 0 level (base)","37d75805":"### Blending and stacking","475f213b":"I use various hyperparameters. Also, every model has his own features set.","f8771995":"# 3. Data preprocessing","3c717e4b":"For cross-validation use KFold with 7 folds","592418fb":"Apply one-hot encoding to categorical features cat1 and cat8. In cat8 I use rare categories as not A, C or E","324c077e":"X - fatures,  \ny - target","c489217d":"### Feature importance\nLet's look at feature importance for every model","bbb88dfc":"### Final meta model","c1372da9":"I made small research where I find reasons try to use anomaly peaks of cont1 as additional categorical features:  \nhttps:\/\/www.kaggle.com\/sergeyzemskov\/eda-predict-anomalies-of-target-experiment\/edit\/run\/73746908","6e611cdb":"Use only meaningful features  \n(some features are deleted as useless because of feature importance of built regression model)  \nSee here: https:\/\/www.kaggle.com\/sergeyzemskov\/catboost-feature-importance-with-shap  \nI haven't saved all notebooks comparison of feature importance","6522d0cf":"My final scheme of models  \n<img src='https:\/\/snipboard.io\/6SJToD.jpg' width='900' height=\"500\" >","1c6c2337":"### Ordinal encode of categorical columns","5b9f0478":"I tried StratifiedKFold but it doesn't improve my result on leaderbord.  \nThanks [Kaihua Zhang](https:\/\/www.kaggle.com\/zhangkaihua88\/30ml-continuous-target-stratification#Continuous-Target-StratifiedKFold) for research","a98751ae":"Remove cat1 and cat8 due strong correlation with derived features","f23a2a1d":"### Correlation matrix of generated features","207fcc95":"Sum up all feature importance to get overall feature importance.  \n100% - sum of importance values of all features for one model","060f7fd4":"Check if exist missing values","03ee71f6":"Train and predict using every model that was described above","0b7d5b64":"Use target prediction from previous step as features","ae0012f2":"We can see that LightGBM is fastest model in these conditions.","2cb91c16":"### Categorical features\nComparison of distribution of features in train and test data","c7b18e40":"# 4. Model","e5d87148":"### 1 level","d463c155":"### Continous features","0633eaf3":"## <center>30 Days of ML competition<\/center>\n### <center>Full top 5% soulution<\/center>\n<center><img src='https:\/\/storage.googleapis.com\/kaggle-media\/Images\/30_Days_ML_Hero.png' width='240' height=\"240\" ><\/center>\n\nThis notebook contains full solution from reading and analysis of data to building stacking pipeline.\n\n\n#### Dataset:\nThe dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. \n* 'cat0' - 'cat9' categorical features\n* 'cont0' - 'cont13' continuous features\n* 'target' - continous target","ac7fd1ba":"### 2 level","66e1c431":"# 1. Load data and first look","f02b5f22":"Variables of categorical features on test and train data are distributed very similar","a0bf1cde":"# 5. Submit predictions\nSave the predictions to a CSV file"}}