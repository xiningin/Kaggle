{"cell_type":{"4ec6addd":"code","5855b8ad":"code","6b8d0162":"code","e73449f0":"code","bae6bcfb":"code","7741d451":"code","5e2af986":"code","882082cf":"code","0f7e0e25":"code","73f9f728":"code","6adaacb5":"code","db905e02":"code","6ea0f8d0":"code","563e3b1a":"code","b4e92749":"code","32831ce0":"code","0175bba6":"code","9ccda7b6":"code","4feeebc2":"code","98ac1134":"code","1849954d":"code","1db8afcb":"code","07836ba5":"code","5b119d80":"code","4b9e58f9":"code","8daacac8":"code","6a94b7a3":"code","980d7f26":"code","d11de211":"code","80a50c47":"code","bd80300d":"code","380de17d":"code","a0e25439":"code","cd14ad7d":"code","ab0ba06f":"code","5b44709c":"code","704bb7cb":"code","4f5eb4f5":"code","6bc30e3b":"code","02f95e48":"code","7f23f23c":"code","24ac19da":"code","c318f4ef":"code","5cec5160":"code","a8063117":"code","cf6f82ed":"code","0796174d":"code","3e5986de":"code","4c3bdfe9":"code","bc5aa112":"code","134ad035":"code","ec4a55f1":"code","76b5b2a4":"code","83e177f5":"code","bd8756b5":"code","58ad9250":"code","0294bdfb":"code","28da5057":"code","658f4a18":"code","fa8a404e":"code","25fa429c":"code","65095c5c":"code","0c1a934e":"code","30ba9047":"code","3f7ee970":"code","552380b7":"code","57698a11":"code","484ca66c":"code","4e7a5e30":"code","4b8220ce":"code","3a45977b":"code","f5889323":"code","834c26ca":"markdown","5bbfabca":"markdown","c4e9f935":"markdown","c30c066f":"markdown","da2c6189":"markdown","0c0e9daf":"markdown","ca118e4d":"markdown"},"source":{"4ec6addd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5855b8ad":"import os #paths to file\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport warnings# warning filter\n\n\n#ploting libraries\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#relevant ML libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n#ML models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Libraries imported\")","6b8d0162":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","e73449f0":"df.shape","bae6bcfb":"df.head()","7741d451":"df.info()\n# We have missing data , we will handle them as we go","5e2af986":"# Describe the numerical data\ndf.describe()","882082cf":"# We will change the type of Credit_History to object becaues we can see that it is 1 or 0\ndf['Credit_History'] = df['Credit_History'].astype('O')\n\n# describe categorical data (\"object\")\ndf.describe(include='O')","0f7e0e25":"# We will drop ID because it's not important for our model and it will just mislead the mode\n\ndf.drop('Loan_ID', axis=1, inplace=True)","73f9f728":"df.duplicated().any()\n\n# We got no duplicated rows","6adaacb5":"# Let's look at the target percentage\n\nplt.figure(figsize=(8,6))\nsns.countplot(df['Loan_Status']);\n\nprint('The percentage of Y class : %.2f' % (df['Loan_Status'].value_counts()[0] \/ len(df)))\nprint('The percentage of N class : %.2f' % (df['Loan_Status'].value_counts()[1] \/ len(df)))\n\n# We can consider it as imbalanced data, but for now i will not","db905e02":"df.columns","6ea0f8d0":"df.head(1)","563e3b1a":"# Credit_History\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Credit_History');\n\n# We didn't give a loan for most people who got Credit History = 0\n# but we did give a loan for most of people who got Credit History = 1\n# so we can say if you got Credit History = 1 , you will have better chance to get a loan","b4e92749":"print(pd.crosstab(df['Credit_History'],df['Loan_Status']))\n\ndf_history_Y = df[df['Credit_History'] == 1]\ndf_history_N = df[df['Credit_History'] == 0]\n\nperc_df_self_Y = df_history_Y['Loan_Status'].value_counts()['Y']\/len(df_history_Y)\nperc_df_self_N = df_history_N['Loan_Status'].value_counts()['Y']\/len(df_history_N)\n\nprint('\\n')\n\nprint('Percentage loans with Credit_History Y: %.3f' %perc_df_self_Y)\nprint('Percentage loans with Credit_History N: %.3f' %perc_df_self_N)","32831ce0":"# Gender\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Gender');\n\n# Most males got loan and most females got one too so (No pattern)\n\n# I think it's not so important feature, we will see later","0175bba6":"# Married\nplt.figure(figsize=(15,5))\nsns.countplot(x='Married', hue='Loan_Status', data=df);\n\n# Most people who get married did get a loan.\n# If you'r married then you have better chance to get a loan.\n# Good feature","9ccda7b6":"# Dependents\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='Dependents', hue='Loan_Status', data=df);\n\n# First if Dependents = 0 , we got higher chance to get a loan ((very hight chance))\n# Good feature","4feeebc2":"# Education\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Education');\n\n# If you are graduated or not, you will get almost the same chance to get a loan (No pattern)\n# Here you can see that most people did graduated, and most of them got a loan.\n# On the other hand, most of people who did't graduate also got a loan, but with less \n# percentage from people who graduated.","98ac1134":"df_graduated = df[df['Education'] == 'Graduate']\ndf_not_graduated = df[df['Education'] != 'Graduate']\n\nprint('Graduate')\ndf_graduated.head()","1849954d":"print('Not graduate')\ndf_not_graduated.head()","1db8afcb":"n_loans_gr = len(df_graduated[df_graduated['Loan_Status'] == 'Y'])\n#n_loans_gr = df_graduated['Loan_Status'].value_counts()[0]\nlength_gr =len(df_graduated)\nperc_df_graduated_Y = n_loans_gr\/length_gr\n\nn_loans_not_gr = len(df_not_graduated[df_not_graduated['Loan_Status'] == 'Y'])\n#n_loans_gr = df_graduated['Loan_Status'].value_counts()[0]\nlength_not_gr =len(df_not_graduated)\nperc_df_not_graduated_Y = n_loans_not_gr\/length_not_gr\n\nprint('Percentage loans for NOT graduated: %.2f' % perc_df_not_graduated_Y)\n\nprint('Percentage loans for graduated: %.2f' % perc_df_graduated_Y)","07836ba5":"# Self_Employed\n\ngrid = sns.FacetGrid(df,col='Self_Employed', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Loan_Status');\n\n# No pattern (same as Education)","5b119d80":"df_self_employed_Y = df[df['Self_Employed'] == 'Yes']\ndf_self_employed_N = df[df['Self_Employed'] == 'No']\n\nn_loans_self_y = df_self_employed_Y['Loan_Status'].value_counts()[0]\nlength_self_y =len(df_self_employed_Y)\nperc_df_self_Y = n_loans_self_y\/length_self_y\n\nn_loans_self_n = df_self_employed_N['Loan_Status'].value_counts()[0]\nlength_self_n =len(df_self_employed_N)\nperc_df_self_N = n_loans_self_n\/length_self_n\n\nprint('Percentage loans for self employed: %.2f' % perc_df_self_Y)\n\nprint('Percentage loans for not self employed: %.2f' % perc_df_self_N)","4b9e58f9":"# Property_Area\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='Property_Area', hue='Loan_Status', data=df);\n\n# We can say, Semiurban Property_Area got more than 50% chance to get a loan\n# Good feature","8daacac8":"# ApplicantIncome\n\nplt.scatter(df['ApplicantIncome'], df['Loan_Status']);\n\n# No pattern","6a94b7a3":"# The numerical data\n\ndf.groupby('Loan_Status').median() # median because Not affected with outliers\n\n# We can see that when we got low median in CoapplicantInocme we got Loan_Status = N\n\n# CoapplicantInocme is a good feature","980d7f26":"df.isnull().sum().sort_values(ascending=False)","d11de211":"# We will separate the numerical columns from the categorical\n\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(df.dtypes):\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else :\n        num_data.append(df.iloc[:, i])\n        \n\ncat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","80a50c47":"cat_data.head()","bd80300d":"num_data.head()","380de17d":"cat_data.isnull().sum().any() #cat_data missing values?","a0e25439":"# Categorical data\n# If you want to fill every column with its own most frequent value you can use.\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\ncat_data.isnull().sum().any() # No more missing data","cd14ad7d":"num_data.isnull().sum().any() # num_data missing data?","ab0ba06f":"# Numerical data\n# Fill every missing value with their previous value in the same column.\n\nnum_data.fillna(method='bfill', inplace=True)\nnum_data.isnull().sum().any() # no more missing data","5b44709c":"from sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\ncat_data.head()","704bb7cb":"# Transform the target column\n\ntarget_values = {'Y': 0 , 'N' : 1}\n\n# Save 'Loan_Status' column in 'target'\ntarget = cat_data['Loan_Status']\n\n#Remove 'Loan_Status' column from cat_data\ncat_data.drop('Loan_Status', axis=1, inplace=True)\n\n# Map 'target' according to 'target_values' \ntarget = target.map(target_values)","4f5eb4f5":"# Transform the remaining columns of cat_data\n\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])\ntarget.head()","6bc30e3b":"cat_data.head()","02f95e48":"# Create new Pandas object \ndf = pd.concat([cat_data, num_data, target], axis=1)\ndf.head()","7f23f23c":"# Create (X, y) Pandas objects for data training \nX = pd.concat([cat_data, num_data], axis=1)\ny = target","24ac19da":"# We will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# Almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values\/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values\/ len(df))","c318f4ef":"# We will use 4 different models for training\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = {\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'SVC': SVC(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}","5cec5160":"# Loss\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' \n              % (pre, rec, f1, loss, acc))","a8063117":"# Train data\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n\ntrain_eval_train(models, X_train, y_train)\n\n# We can see that best model is LogisticRegression at least for now, SVC is just \n# memorizing the data so it is overfitting.","cf6f82ed":"# Cross validation\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n    # We will change X & y to dataframe because we will use iloc (iloc don't work on numpy array)\n    X = pd.DataFrame(X) \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  #[0] because we don't want to show the name of the column\n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)\n\n# SVC is just memorizing the data, and you can see \n# that here DecisionTreeClassifier is better than LogisticRegression","0796174d":"# Some explanation about Logistic Regression\n\nx = []\nidx = [' pre', ' rec', ' f1', ' loss', ' acc']\n\n# We will use one model\nlog = LogisticRegression()\n\nfor train, test in skf.split(X_train, y_train):\n    log.fit(X_train.iloc[train], y_train.iloc[train])\n    ls = loss(y_train.iloc[test], log.predict(X_train.iloc[test]), retu=True)\n    x.append(ls)\n    \n# Thats what we get\npd.DataFrame(x, columns=idx)\n\n# (column 0 represent the precision_score of the 10 folds)\n# (row 0 represent the (pre, rec, f1, loss, acc) for the first fold)\n# then we should find the mean of every column\n# pd.DataFrame(x, columns=idx).mean(axis=0)","3e5986de":"# We got it right for most of the features, as you can see we've say at the first of \n# the kernel, that Credit_History and Married etc, are good features, actually \n# Credit_History is the best.\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# Here we got 58% similarity between LoanAmount & ApplicantIncome \n# and that may be bad for our model so we will see what we can do","4c3bdfe9":"# I will try to make some operations on some features, here I just tried diffrent operations on different features,\n# having experience in the field, and having knowledge about the data will also help\n\nX_train['new_col'] = X_train['CoapplicantIncome'] \/ X_train['ApplicantIncome']  \nX_train['new_col_2'] = X_train['LoanAmount'] * X_train['Loan_Amount_Term']","bc5aa112":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# new_col 0.03 , new_col_2, 0.047\n# Not that much , but that will help us reduce the number of features","134ad035":"X_train.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)","ec4a55f1":"train_eval_cross(models, X_train, y_train, skf)\n\n# SVC is improving, but LogisticRegression is overfitting\n# We do not change anything so we can see what will happen as we go","76b5b2a4":"# Let's take a look at the value counts of every label\n\n#print(X_train.shape)\n#print(X_train.shape[0]) # Rows\n#print(X_train.shape[1]) # Columns\n\nprint('************************\\n')\n\nfor i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n----------\\\n--------------------------------------\\n')","83e177f5":"# new_col_2\n\n# We can see we got right_skewed\n# We can solve this problem with very simple statistical technique , by taking the logarithm of all the values\n# because when data is normally distributed that will help improving our model\n\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.distplot(X_train['new_col_2'], ax=ax[0], fit=norm)\nax[0].set_title('new_col_2 before log')\n\nX_train['new_col_2'] = np.log(X_train['new_col_2'])  # logarithm of all the values\n\nsns.distplot(X_train['new_col_2'], ax=ax[1], fit=norm)\nax[1].set_title('new_col_2 after log');","bd8756b5":"# Now we will evaluate our models, and i will do that continuously ,so i don't need to \n# mention that every time\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# Our models improved really good by just doing the previous step.","58ad9250":"# new_col\n\n# Most of our data is 0 , so we will try to change other values to 1\n\nprint('before:')\nprint(X_train['new_col'].value_counts())\n\nX_train['new_col'] = [x if x==0 else 1 for x in X_train['new_col']]\nprint('-'*50)\nprint('\\nafter:')\nprint(X_train['new_col'].value_counts())","0294bdfb":"train_eval_cross(models, X_train, y_train, skf)\n\n# We are improving our models as we go","28da5057":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n    \n# Looks better","658f4a18":"# Outliers: we will use boxplot to detect them \n\nsns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 outliers', fontsize=15);\nplt.xlabel('');","fa8a404e":"threshold = 0.1  # This number is a hyper parameter, by reducing it, more points are removed.\n                 # You can just try different values, the deafult value is (1.5) it works good for most cases.\n                 # Be careful, you don't want to try a small number because you may loss some important information from data.\n                 # That's why I was surprised when 0.1 gived me the best result\n            \nnew_col_2_out = X_train['new_col_2']\n\nq25, q75 = np.percentile(new_col_2_out, 25), np.percentile(new_col_2_out, 75) # Q25, Q75\n\nprint('Quartile 25: {} , Quartile 75: {}'.format(q25, q75))\n\niqr = q75 - q25\nprint('iqr: {}'.format(iqr))\n\ncut = iqr * threshold\nlower, upper = q25 - cut, q75 + cut\nprint('Cut Off: {}'.format(cut))\nprint('Lower: {}'.format(lower))\nprint('Upper: {}'.format(upper))\n\noutliers = [x for x in new_col_2_out if x < lower or x > upper]\nprint('Nubers of Outliers: {}'.format(len(outliers)))\nprint('outliers:{}'.format(outliers))\n\ndata_outliers = pd.concat([X_train, y_train], axis=1)\nprint('\\nlen X_train before dropping the outliers', len(data_outliers))\ndata_outliers = data_outliers.drop(data_outliers[(data_outliers['new_col_2'] > upper) | (data_outliers['new_col_2'] < lower)].index)\n\nprint('len X_train before dropping the outliers', len(data_outliers))","25fa429c":"X_train = data_outliers.drop('Loan_Status', axis=1)\ny_train = data_outliers['Loan_Status']","65095c5c":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 without outliers', fontsize=15);\nplt.xlabel('');\n\n# good :)","0c1a934e":"train_eval_cross(models, X_train, y_train, skf)\n\n# Now we get 94.1 ??? for precision & 53.5 for recall","30ba9047":"# Self_Employed got really bad corr (-0.00061) , let's try to remove it and see what will happen\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","3f7ee970":"#X_train.drop(['Self_Employed'], axis=1, inplace=True)\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# looks like Self_Employed is not important\n# KNeighborsClassifier improved\n\n# droping all the features Except for Credit_History actually improved KNeighborsClassifier and didn't change anything in other models\n# so you can try it by you self\n# but don't forget to do that on testing data too\n\n#X_train.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","552380b7":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","57698a11":"X_test.head()","484ca66c":"X_test_new = X_test.copy()","4e7a5e30":"x = []\n\nX_test_new['new_col'] = X_test_new['CoapplicantIncome'] \/ X_test_new['ApplicantIncome']  \nX_test_new['new_col_2'] = X_test_new['LoanAmount'] * X_test_new['Loan_Amount_Term']\nX_test_new.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)\n\nX_test_new['new_col_2'] = np.log(X_test_new['new_col_2'])\n\nX_test_new['new_col'] = [x if x==0 else 1 for x in X_test_new['new_col']]\n\n#X_test_new.drop(['Self_Employed'], axis=1, inplace=True)\n\n# drop all the features Except for Credit_History\n#X_test_new.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","4b8220ce":"X_test_new.head()","3a45977b":"X_train.head()","f5889323":"for name,model in models.items():\n    print(name, end=':\\n')\n    loss(y_test, model.predict(X_test_new))\n    print('-'*40)","834c26ca":"# evaluate the models on Test_data","5bbfabca":"# Loan Prediction\n\nThis is my first kernel, (completely) copied for learning purposes from:\n\nhttps:\/\/www.kaggle.com\/yaheaal\/loan-status-with-different-models\n\nOther kernels I got inspiration from:\n\nhttps:\/\/www.kaggle.com\/aarti19\/loan-predictionproblem\nhttps:\/\/www.kaggle.com\/yonatanrabinovich\/loan-prediction-dataset-ml-project\n\nThanks @Yaheaal, @aarti19, @yonatanrabinovich\n\nLibraries: **sklearn, matplotlib, numpy, pandas, seaborn, scipy**\n\n\nMissing values filled in using **backward 'bfill' method** for numerical columns , and **most frequent value** for categorical columns (simple techniques)\n\n\n* Train models\n\n    **a) Logistic regression**\n    \n    **b) KNeighborsClassifier**\n    \n    **C) SVC**\n    \n    **d) DecisionTreeClassifier**\n","c4e9f935":"# **4. Features Engineering**","c30c066f":"# **2. Data Cleaning**","da2c6189":"* # **1. Exploratory Analysis**","0c0e9daf":"# **Features selection**","ca118e4d":"# **3. Train Data**"}}