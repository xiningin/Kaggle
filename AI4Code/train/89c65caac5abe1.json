{"cell_type":{"d8993d78":"code","70432682":"code","f545b0e3":"code","899e1335":"code","66b83211":"code","e284acca":"code","4b59e1e1":"code","d21f9a72":"code","9586b095":"code","13d198ff":"code","3cd4659b":"code","7f40e6e4":"code","57b07ae6":"code","c69565ad":"code","78f1bf80":"code","a1be63fb":"code","461a751b":"code","94384b75":"code","86acee13":"code","e1cab082":"code","d27aa71f":"code","c78bfd4c":"code","b17d82b3":"code","285f789b":"code","57673ec5":"code","12139cc6":"code","3ee09277":"markdown","0a392385":"markdown","dedb0dc1":"markdown","a45cdbf1":"markdown","751a8595":"markdown","65dd289a":"markdown"},"source":{"d8993d78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70432682":"import numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nfrom collections import Counter\nimport re, spacy, string\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom pprint import pprint\nimport time\n\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","f545b0e3":"path = '..\/input\/jigsaw-toxic-comment-classification-challenge\/'\ndf = pd.read_csv(path+'train.csv.zip')\ndf_test = pd.read_csv(path+'test.csv.zip')\ndf_submission = pd.read_csv(path+'sample_submission.csv.zip')\n\ndf.head()","899e1335":"df.shape","66b83211":"df.isnull().sum()","e284acca":"comments = df.drop(['id','comment_text'],axis = 1)\ncomments.columns","4b59e1e1":"#Distribution of the target variable data in terms of proportions.\n\nfor i in list(comments.columns):\n    print(\"Percent of {0}s: \".format(i), round(100*comments[i].mean(),2), \"%\")","d21f9a72":"com_dict = {}\nfor i in list(comments.columns):\n    com_dict[i]=comments[i].sum()\n\ncom_list = sorted(com_dict,key=com_dict.get,reverse=True)","9586b095":"plt.figure(figsize=(12,8))\nsns.barplot(com_list,comments.sum().sort_values(ascending=False))\nplt.xticks(rotation=80)\nplt.show()","13d198ff":"# Function to clean the review text and remove all the unnecessary elements.\n\ndef clean_review_text(text):\n    text = text.lower()  # covert the text to lowercase\n    text = re.sub('<.*?>','',text).strip() # remove html chars\n    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() # remove text in square brackets and parenthesis\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation marks\n    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n    return text.strip()","3cd4659b":"df.comment_text = df.comment_text.astype(str)\ndf.comment_text = df.comment_text.apply(clean_review_text)\ndf.comment_text.head()","7f40e6e4":"# Snowball stemmer\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\nsnow_stemmer = SnowballStemmer(language='english')\n\nstopwords = nlp.Defaults.stop_words\ndef apply_stemmer(text):\n    words = text.split()\n    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n    return ' '.join(sent)","57b07ae6":"df.comment_text = df.comment_text.apply(apply_stemmer)\ndf.comment_text.head()","c69565ad":"#Using a word cloud find the top 50 words by frequency among all the review texts\n!pip install wordcloud\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(stopwords=stopwords,max_words=50).generate(str(df.comment_text))\n\nprint(wordcloud)\nplt.figure(figsize=(10,6))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","78f1bf80":"X = df.comment_text\ny = df.drop(['id','comment_text'],axis = 1)","a1be63fb":"# Split the dataset into test and train\nfrom sklearn.model_selection import train_test_split\nseed = 100 \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)","461a751b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nword_vectorizer = TfidfVectorizer(\n    strip_accents='unicode',     \n    analyzer='word',            \n    token_pattern=r'\\w{1,}',    \n    ngram_range=(1, 3),         \n    stop_words='english',\n    sublinear_tf=True)\n\nword_vectorizer.fit(X_train)    # Fiting it on Train\ntrain_word_features = word_vectorizer.transform(X_train)","94384b75":"## transforming the train and test datasets\nX_train_transformed = word_vectorizer.transform(X_train)\nX_test_transformed = word_vectorizer.transform(X_test)\n\n\n# # Print the shape of each dataset.\nprint('X_train_transformed', X_train_transformed.shape)\nprint('y_train', y_train.shape)\nprint('X_test_transformed', X_test_transformed.shape)\nprint('y_test', y_test.shape)","86acee13":"import xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\n\nimport time","e1cab082":"from sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance","d27aa71f":"# Logistic Regression \ntime1 = time.time()\n# logistic regression\nlog_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=seed)\n\n# fit model\nclassifier_ovr_log = OneVsRestClassifier(log_reg)\nclassifier_ovr_log.fit(X_train_transformed, y_train)\n\ntime_taken = time.time() - time1\nprint('Time Taken: {:.2f} seconds'.format(time_taken))\n\ny_train_pred_proba = classifier_ovr_log.predict_proba(X_train_transformed)\ny_test_pred_proba = classifier_ovr_log.predict_proba(X_test_transformed)\n\n\nroc_auc_score_train = roc_auc_score(y_train, y_train_pred_proba,average='weighted')\nroc_auc_score_test = roc_auc_score(y_test, y_test_pred_proba,average='weighted')\n\nprint(\"ROC AUC Score Train:\", roc_auc_score_train)\nprint(\"ROC AUC Score Test:\", roc_auc_score_test)","c78bfd4c":"y_test_pred_proba","b17d82b3":"classifier = classifier_ovr_log","285f789b":"df_test.head()","57673ec5":"def make_test_predictions(df,classifier):\n    df.comment_text = df.comment_text.astype(str)\n    df.comment_text = df.comment_text.apply(clean_review_text)\n    df.comment_text = df.comment_text.apply(apply_stemmer)\n    X_test = df.comment_text\n    X_test_transformed = word_vectorizer.transform(X_test)\n    y_test_pred = classifier.predict_proba(X_test_transformed)\n    y_test_pred_df = pd.DataFrame(y_test_pred,columns=comments.columns)\n    submission_df = pd.concat([df.id, y_test_pred_df], axis=1)\n    submission_df.to_csv('submission.csv', index = False)\n    ","12139cc6":"make_test_predictions(df_test,classifier)","3ee09277":"### Lemmatization","0a392385":"## Load data","dedb0dc1":"### visualization of the distribution of types of toxic comments","a45cdbf1":"### Checking missing values","751a8595":"### Checking shape of the data","65dd289a":"### Text preprocessing"}}