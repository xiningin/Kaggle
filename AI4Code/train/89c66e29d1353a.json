{"cell_type":{"444134a3":"code","ddb770e4":"code","dddc882a":"code","6227b6b2":"code","6a803425":"code","eba0d50a":"code","01580583":"code","b6aeaa89":"code","22bd49c1":"code","fe85dda0":"code","d546d10d":"code","2adcb633":"code","ae2765cd":"code","eee75aa7":"code","cefd9fb3":"code","b034188a":"code","e8af7caf":"code","db493b73":"code","3b14064f":"code","d1454034":"code","fe463f39":"code","07f568ce":"code","88c99486":"code","9a226720":"code","bf67db80":"code","eda76bc5":"code","13900d14":"markdown","b50cb20e":"markdown","a08b9d8a":"markdown","6d750cfd":"markdown","549175cb":"markdown","d379fc59":"markdown","7439b30a":"markdown","d083ea0e":"markdown","64e7cc41":"markdown","658061c3":"markdown","27b17917":"markdown","7244af57":"markdown","75b7b7da":"markdown","a7087fd1":"markdown"},"source":{"444134a3":"import numpy as np \nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.simplefilter(\"ignore\")","ddb770e4":"tweets_df = pd.read_csv(\"\/kaggle\/input\/spamham\/spam.csv\", encoding=\"latin1\")","dddc882a":"print(f\"data shape: {tweets_df.shape}\")","6227b6b2":"tweets_df.info()","6a803425":"tweets_df.describe()","eba0d50a":"tweets_df.head()","01580583":"tweets_df.columns = [\"class\", \"text\", \"c3\", \"c4\", \"c5\"]","b6aeaa89":"tweets_df.head()","22bd49c1":"tweets_df.c3.value_counts()","fe85dda0":"tweets_df.c4.value_counts()","d546d10d":"tweets_df.c5.value_counts()","2adcb633":"tweets_df['text'] = tweets_df.apply(lambda x: x['text'] + str(x['c3']) + str(x['c4']) + str(x['c5']), axis=1)","ae2765cd":"tweets_df = tweets_df[[\"class\", \"text\"]]","eee75aa7":"tweets_df.head()","cefd9fb3":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","b034188a":"missing_data(tweets_df)","e8af7caf":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","db493b73":"unique_values(tweets_df)","3b14064f":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","d1454034":"most_frequent_values(tweets_df)","fe463f39":"def plot_count(feature, title, df, size=1, ordered=True):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    if ordered:\n        g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    else:\n        g = sns.countplot(df[feature], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    ","07f568ce":"plot_count(\"class\", \"Class\", tweets_df,2)","88c99486":"from wordcloud import WordCloud, STOPWORDS\ndef show_wordcloud(data, title=\"\"):\n    text = \" \".join(t for t in data.dropna())\n    stopwords = set(STOPWORDS)\n    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\", \"nannannan\"])\n    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"black\").generate(text)\n    fig = plt.figure(1, figsize=(16,16))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.show()","9a226720":"show_wordcloud(tweets_df['text'], title = 'Prevalent words in tweets')","bf67db80":"ham_df = tweets_df.loc[tweets_df['class']==\"ham\"]\nshow_wordcloud(ham_df['text'], title = 'Prevalent words in tweets in class ham')","eda76bc5":"spam_df = tweets_df.loc[tweets_df['class']==\"spam\"]\nshow_wordcloud(spam_df['text'], title = 'Prevalent words in tweets in class spam')","13900d14":"### Class distribution","b50cb20e":"## Load data","a08b9d8a":"## Visualize the data distribution","6d750cfd":"### Text wordclouds\n\nWe will remove, before creating the wordclouds, the frequent or parasite terms. Besides the stopwords, we will also add internet\/tweets specific content, as well as the ubiquous \"nannannan\" word in this dataset.","549175cb":"Drop the three columns now.","d379fc59":"### Most frequent values","7439b30a":"Merging the text.","d083ea0e":"### Missing data","64e7cc41":"### Text wordcloauds","658061c3":"### Unique values","27b17917":"# Data exploration\n\n\n## Glimpse the data","7244af57":"# Data preparation\n\n## Load packages","75b7b7da":"<h1>Explore Spam Ham Tweets<\/h1>\n\n\n# Introduction\n\nWe will analyse the data distribution for this Spam\/Ham tweets dataset.\n","a7087fd1":"### Fix issue with wrong columns\n\nWrong columns (`c3`, `c4`, `c5`) will be merged with corresponding `text` column values."}}