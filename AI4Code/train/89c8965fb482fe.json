{"cell_type":{"6ad25960":"code","eeb568b3":"code","d91747cf":"code","b3ec5666":"code","a1a60836":"code","9e90ca0f":"code","6f010b72":"code","7708b31d":"code","8d174e08":"code","5a400165":"code","cd61c575":"code","0eba14b2":"code","316b269d":"code","bbe08f4f":"code","571923b2":"code","da528c81":"code","0cb988c7":"code","edfbdbf0":"code","d933213c":"code","0a87f092":"code","6c4ad103":"code","baf8821b":"code","0ca8bce2":"code","35abe7ef":"code","b741680a":"code","204ed4be":"code","bde82148":"code","0b56fc07":"code","55e9676e":"code","dea773e0":"code","3ef20e8f":"code","ca22682b":"code","0999c7ca":"code","ff44b424":"code","b98ed388":"code","57062edb":"code","db2c17e7":"code","2c491dae":"code","0a94abcc":"markdown","bf2b2233":"markdown","610c3c0c":"markdown","cf8b9e87":"markdown"},"source":{"6ad25960":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eeb568b3":"import os\nimport shutil\nimport re\nimport math\n\nimport pandas as pd\nimport numpy as np\n\nimport PIL.Image\nimport cv2\n\nfrom random import shuffle\nfrom glob import glob\n\nfrom sklearn.model_selection import train_test_split\n\n#from tensorflow.python.keras.applications import VGG16\n\nfrom keras.applications import VGG16\n\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\n#from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\nfrom keras import models\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom keras.optimizers import Adam","d91747cf":"base_image_dir = os.path.join('..', 'input\/aptos2019-blindness-detection\/')\ntrain_dir = os.path.join(base_image_dir,'train_images\/')\ndf = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\ndf['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\ndf = df.drop(columns=['id_code'])\ndf = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ndf.head(10)","b3ec5666":"df.diagnosis.value_counts()","a1a60836":"image_train, image_test, y_train, y_test = train_test_split(np.array(df.path), \n                                                            np.array(df.diagnosis), \n                                                            test_size=0.3,\n                                                            random_state=123, \n                                                            stratify=df.diagnosis)","9e90ca0f":"image_train.shape","6f010b72":"image_and_class_train = dict(zip(image_train, y_train))\nimage_and_class_test = dict(zip(image_test, y_test))","7708b31d":"IMG_SIZE = (224, 224)  # \u0440\u0430\u0437\u043c\u0435\u0440 \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438\nNUM_CLASSES = 5        # \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432","8d174e08":"def crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","5a400165":"def circle_crop(path, img_size=(224,224), sigmaX=10):   \n    \"\"\"\n    Create circular crop around image centre    \n    \"\"\"    \n    \n    img = cv2.imread(path)\n    img = crop_image_from_gray(img)    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    height, width, depth = img.shape    \n    \n    x = int(width\/2)\n    y = int(height\/2)\n    r = np.amin((x,y))\n    \n    circle_img = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n    img = cv2.bitwise_and(img, img, mask=circle_img)\n    img = crop_image_from_gray(img)\n    img = cv2.addWeighted ( img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\n    img = cv2.resize(img, img_size)\n    return preprocess_input(img) ","cd61c575":"# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c\n# \u0418\u0421\u0425\u041e\u0414\u041d\u0410\u042f \u041f\u0420\u0415\u0414\u041e\u0411\u0420\u0410\u0411\u041e\u0422\u041a\u0410 \u0414\u0410\u041d\u041d\u042b\u0425\n\ndef load_image(path, target_size=IMG_SIZE):\n    img = load_img(path, target_size=target_size)  # \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n    array = img_to_array(img)\n    return preprocess_input(array)  # \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u043b\u044f VGG16","0eba14b2":"# \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0447\u0442\u0435\u043d\u0438\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0434\u0438\u0441\u043a\u0430\ndef fit_generator(files, batch_size=32):\n    while True:\n        shuffle(files)\n        for k in range(math.ceil(len(files) \/ batch_size)):   # \u043e\u043a\u0440\u0443\u0433\u043b\u044f\u0435\u043c \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0446\u0435\u043b\u043e\u0433\u043e \u0432\u0432\u0435\u0440\u0445\n            i = k * batch_size                                # k -- \u043d\u043e\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u0432 \u043f\u0440\u043e\u0445\u043e\u0434\u0435                      \n            j = i + batch_size\n            if j > len(files):\n                j = len(files)\n                \n            # \u0435\u0441\u043b\u0438 \u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e load_image, \u0442\u043e \u0431\u0443\u0434\u0443\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n            #x = np.array([load_image(path)\/255 for path in files[i:j]])         # \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0432 \u0432\u0438\u0434\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b\n            \n            # \u0430 \u044d\u0442\u043e \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \n            x = np.array([circle_crop(path) for path in files[i:j]]) \n            \n            label = np.array([image_and_class_train[path] for path in files[i:j]])   # \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n            y = keras.utils.to_categorical(label, num_classes=NUM_CLASSES)      # one hot \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n            yield (x, y)","316b269d":"# \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0447\u0442\u0435\u043d\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0434\u0438\u0441\u043a\u0430\ndef predict_generator(files):\n    while True:\n        for path in files:\n            \n            # \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439\n            yield np.array([circle_crop(path)])\n            \n            # \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435\n            #yield np.array([load_image(path)])\n            ","bbe08f4f":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(20, 10))\nfor i, path in enumerate(image_train[:10], 1):\n    subplot = fig.add_subplot(2, 5, i)\n    \n    # \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\n    plt.imshow(plt.imread(path));\n    \n\n    subplot.set_title('{} \\n label: {}'.format(os.path.basename(path), image_and_class_train[path]))","571923b2":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(20, 10))\nfor i, path in enumerate(image_train[:10], 1):\n    subplot = fig.add_subplot(2, 5, i)\n    \n    # \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\n    #plt.imshow(plt.imread(path));\n    \n    # \u043d\u043e\u0432\u0430\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\n    image = circle_crop(path,sigmaX=10)\n    plt.imshow(image)\n\n    subplot.set_title('{} \\n label: {}'.format(os.path.basename(path), image_and_class_train[path]))","da528c81":"conv_base = VGG16(include_top=False, weights='imagenet', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))","0cb988c7":"# \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u0432\u0435\u0441\u0430 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u0431\u043b\u043e\u043a\u0430 \nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv3':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","edfbdbf0":"conv_base.summary()","d933213c":"model_2 = models.Sequential()\nmodel_2.add(conv_base)  # \u043a\u0443\u0441\u043e\u043a VGG-16 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d \u0432 \u043c\u043e\u0434\u0435\u043b\u044c\nmodel_2.add(layers.BatchNormalization())\nmodel_2.add(layers.Flatten())\n#model_1.add(layers.Dense(512, activation='relu'))\nmodel_2.add(layers.Dense(NUM_CLASSES, activation='softmax'))\nmodel_2.summary()","0a87f092":"from keras import metrics","6c4ad103":"model_2.compile(optimizer=Adam(lr=0.001), \n              loss='categorical_crossentropy',  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c 'categorical_crossentropy' (log loss\n              metrics=['accuracy', 'categorical_accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC()])","baf8821b":"shuffle(image_train)  # \u043f\u0435\u0440\u0435\u043c\u0435\u0448\u0438\u0432\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\n\ntrain_val_split = 100  # \u0447\u0438\u0441\u043b\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n\nvalidation_data = next(fit_generator(image_train[:train_val_split], train_val_split))\n\n# \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nhistory = model_2.fit_generator(fit_generator(image_train[train_val_split:]),  # \u0434\u0430\u043d\u043d\u044b\u0435 \u0447\u0438\u0442\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u043e\u043c\n        steps_per_epoch=10,  # \u0447\u0438\u0441\u043b\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0437\u0430 \u044d\u043f\u043e\u0445\u0443\n        epochs=100,  # \u0447\u0438\u0441\u043b\u043e \u044d\u043f\u043e\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n        validation_data=validation_data,\n        callbacks=[ #EarlyStopping(patience = 5),\n                   ModelCheckpoint(filepath='the_least_loss_new_preproc_gpu.h5',\n                                  verbose=1,\n                                  save_best_only=True)]\n                               )","0ca8bce2":"start = 0\nplt.plot(history.history['loss'][start:])\nplt.plot(history.history['val_loss'][start:])\nplt.legend(['Train loss', 'Validation loss'])\nplt.savefig('loss_preproc.png')","35abe7ef":"f1 = open('train_loss_preproc.txt', 'w')\nf1.writelines('%s\\n' % i for i in history.history['loss'][start:])\nf1.close()","b741680a":"f2 = open('vall_loss_preproc.txt', 'w')\nf2.writelines('%s\\n' % i for i in history.history['val_loss'][start:])\nf2.close()","204ed4be":"plt.plot(history.history['accuracy'][start:])\nplt.plot(history.history['val_accuracy'][start:])\nplt.legend(['Train acc', 'Validation acc'])\nplt.savefig('accuracy_preproc.png')\n\nf3 = open('train_acc_preproc.txt', 'w')\nf3.writelines('%s\\n' % i for i in history.history['accuracy'][start:])\nf3.close()\n\nf4 = open('vall_acc_preproc.txt', 'w')\nf4.writelines('%s\\n' % i for i in history.history['val_accuracy'][start:])\nf4.close()","bde82148":"plt.plot(history.history['precision_2'][start:])\nplt.plot(history.history['val_precision_2'][start:])\nplt.legend(['Train precision', 'Validation precision'])\nplt.savefig('precision_preproc.png')\n\nf5 = open('train_precision_preproc.txt', 'w')\nf5.writelines('%s\\n' % i for i in history.history['precision_2'][start:])\nf5.close()\n\nf6 = open('vall_precision_preproc.txt', 'w')\nf6.writelines('%s\\n' % i for i in history.history['val_precision_2'][start:])\nf6.close()","0b56fc07":"plt.plot(history.history['recall_2'][start:])\nplt.plot(history.history['val_recall_2'][start:])\nplt.legend(['Train recall', 'Validation recall'])\nplt.savefig('recall_preproc.png')\n\nf7 = open('train_recall_preproc.txt', 'w')\nf7.writelines('%s\\n' % i for i in history.history['recall_2'][start:])\nf7.close()\n\nf8 = open('vall_recall_preproc.txt', 'w')\nf8.writelines('%s\\n' % i for i in history.history['val_recall_2'][start:])\nf8.close()","55e9676e":"plt.plot(history.history['auc_2'][start:])\nplt.plot(history.history['val_auc_2'][start:])\nplt.legend(['Train auc', 'Validation auc'])\nplt.savefig('auc_preproc.png')\n\nf9 = open('train_auc_preproc.txt', 'w')\nf9.writelines('%s\\n' % i for i in history.history['auc_2'][start:])\nf9.close()\n\nf10 = open('vall_auc_preproc.txt', 'w')\nf10.writelines('%s\\n' % i for i in history.history['val_auc_2'][start:])\nf10.close()","dea773e0":"model_2.save('preproc_100_epoch.h5')","3ef20e8f":"# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0433\u043e loss \nmodel_2.load_weights('the_least_loss_new_preproc_gpu.h5')","ca22682b":"pred = model_2.predict_generator(predict_generator(image_test), len(image_test), max_queue_size=500)","0999c7ca":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(20, 20))\nfor i, (path, score) in enumerate(zip(image_test[80:][:10], pred[80:][:10]), 1):\n    subplot = fig.add_subplot(math.ceil(i \/ 5), 5, i)\n    plt.imshow(plt.imread(path))\n    subplot.set_title('label: {} \\n prediction: {} \\n model confidence: {:.3f}'\\\n                      .format(image_and_class_test[path],\n                              int(np.argmax(score)),\n                             np.max(score)))","ff44b424":"from IPython.display import HTML","b98ed388":"create_download_link('\/kaggle\/output\/project_3_model_2-vgg16.hdf5')","57062edb":"model_2.save('model_2.h5')","db2c17e7":"model_2.save_weights('my_model_weights.h5')","2c491dae":"from IPython.display import FileLink, FileLinks\nFileLinks('.')","0a94abcc":"\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0432\u0438\u0434\u0430 {\u0438\u043c\u044f_\u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 : \u043a\u043b\u0430\u0441\u0441, ...} \u0434\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0433\u043e \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a \u043c\u0435\u0442\u043a\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 \u043f\u043e \u0438\u043c\u0435\u043d\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438","bf2b2233":"**\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445:**","610c3c0c":"**\u0421\u0442\u0440\u043e\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c**\n\n\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 'ImagNet' \u043c\u043e\u0434\u0435\u043b\u044c VGG16 ","cf8b9e87":"**\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f**"}}