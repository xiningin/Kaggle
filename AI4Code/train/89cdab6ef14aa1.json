{"cell_type":{"3113bcb1":"code","8c98ecb8":"code","f4c8567e":"code","49149886":"code","22084d65":"code","088c5d4a":"code","2a1e3ab3":"code","6599276b":"code","308a1add":"code","df8fc66a":"code","7a3d1e2b":"code","d5fa4bef":"code","9464a41c":"code","564e9fd7":"code","5e574f70":"code","a3806148":"code","d3764790":"code","ce534a94":"code","5c931f3a":"markdown","4f56cd60":"markdown","c4c21602":"markdown","f38a0e79":"markdown","fe22bdb1":"markdown","2eef217a":"markdown","f1eb3c5c":"markdown","52c6473c":"markdown","8145bd5a":"markdown","7d7e0fc1":"markdown","27be69a9":"markdown","505087c5":"markdown","691fd51d":"markdown","0e900a79":"markdown","328ccbde":"markdown","57554d4d":"markdown"},"source":{"3113bcb1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', encoding='utf-8')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',encoding='utf-8')\n\ndf_train.head()\n","8c98ecb8":"df_test.head()","f4c8567e":"df_train['SalePrice'].describe()","49149886":"sns.distplot(df_train['SalePrice'], color='blue')","22084d65":"print(\"Skew is : %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis is: %f\" % df_train['SalePrice'].kurt())","088c5d4a":"sns.distplot(df_train['SalePrice'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","2a1e3ab3":"corrmat = df_train.corr()\nk = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","6599276b":"nulls = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([nulls, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","308a1add":"df_test = df_test.drop(['PoolQC'],1)\ndf_test = df_test.drop(['MiscFeature'],1)\ndf_test = df_test.drop(['Alley'],1)\ndf_test = df_test.drop(['Fence'],1)\ndf_test = df_test.drop(['FireplaceQu'],1)\n\ndf_train = df_train.drop(['PoolQC'],1)\ndf_train = df_train.drop(['MiscFeature'],1)\ndf_train = df_train.drop(['Alley'],1)\ndf_train = df_train.drop(['Fence'],1)\ndf_train = df_train.drop(['FireplaceQu'],1)\n\nnulls = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([nulls, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(15)","df8fc66a":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')\n    df_test[col] = df_test[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_train[col] = df_train[col].fillna(0)\n    df_test[col] = df_test[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_train[col] = df_train[col].fillna(0)\n    df_test[col] = df_test[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_train[col] = df_train[col].fillna('None')\n    df_test[col] = df_test[col].fillna('None')\n\ndf_test[\"MasVnrType\"] = df_test[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_test[\"MasVnrArea\"] = df_test[\"MasVnrArea\"].fillna(0)\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)\n\ndf_test['MSZoning'] = df_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0])\n\ndf_test[\"Functional\"] = df_test[\"Functional\"].fillna(\"Typ\")\n\ndf_test['LotFrontage'] = df_test['LotFrontage'].fillna(df_test['LotFrontage'].mode()[0])\ndf_train['LotFrontage'] =df_train['LotFrontage'].fillna(df_train['LotFrontage'].mode()[0])\n\ndf_test['Electrical'] = df_test['Electrical'].fillna(df_test['Electrical'].mode()[0])\ndf_train['Electrical'] =df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])\n\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\n\ndf_test['Exterior1st'] = df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\n\ndf_test['SaleType'] = df_test['SaleType'].fillna(df_test['SaleType'].mode()[0])\n\nnulls = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([nulls, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(15)","7a3d1e2b":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,3), sharey=True, dpi=120)\nfig, (ax3, ax4) = plt.subplots(1,2, figsize=(12,3), sharey=True, dpi=120)\n\n\nax1.plot(df_train['TotalBsmtSF'], df_train['SalePrice'], 'bo')\nax2.plot(df_train['GrLivArea'], df_train['SalePrice'], 'bo')\nax3.plot(df_train['OverallQual'], df_train['SalePrice'], 'bo')\nax4.plot(df_train['GarageCars'], df_train['SalePrice'], 'bo')\n\nax1.set_ylabel('SalePrice')\nax3.set_ylabel('SalePrice')\nax1.set_xlabel('TotalBsmtSF')\nax2.set_xlabel('GrLivArea')\nax3.set_xlabel('OverallQual')\nax4.set_xlabel('GarageCars')","d5fa4bef":"df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 30].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 88].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 462].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 631].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 1322].index)\n","9464a41c":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,3), sharey=True, dpi=120)\nfig, (ax3, ax4) = plt.subplots(1,2, figsize=(12,3), sharey=True, dpi=120)\n\nax1.plot(df_train['TotalBsmtSF'], df_train['SalePrice'], 'bo')\nax1.set_xlim(left = 0,right = 6000)\nax2.plot(df_train['GrLivArea'], df_train['SalePrice'], 'bo')\nax2.set_xlim(left = 0,right = 5000)\nax3.plot(df_train['OverallQual'], df_train['SalePrice'], 'bo')\nax4.plot(df_train['GarageCars'], df_train['SalePrice'], 'bo')\n\nax1.set_ylabel('SalePrice')\nax3.set_ylabel('SalePrice')\nax1.set_xlabel('TotalBsmtSF')\nax2.set_xlabel('GrLivArea')\nax3.set_xlabel('OverallQual')\nax4.set_xlabel('GarageCars')","564e9fd7":"sns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","5e574f70":"df_train['GrLivArea'] = np.log(df_train['GrLivArea'])\ndf_test['GrLivArea'] = np.log(df_test['GrLivArea'])\n\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","a3806148":"y_train = df_train.SalePrice.values\ndf_train.drop(['SalePrice'], axis=1, inplace=True)\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\n\nm = len(df_train)","d3764790":"all_data = pd.concat((df_train,df_test),sort=True)\n\nquantitative = [f for f in all_data.columns if all_data.dtypes[f] != 'object']\ncategoricals = df_train.select_dtypes(exclude=[np.number])\nprint('We have :',len(categoricals.columns),' categoricals variables')\nprint('And :',len(quantitative), 'quantitative variables')","ce534a94":"all_data = pd.get_dummies(all_data)\n\nquantitative = [f for f in all_data.columns if all_data.dtypes[f] != 'object']\ncategoricals = all_data.select_dtypes(exclude=[np.number])\nprint('We have :',len(categoricals.columns),' categoricals variables')\nprint('And :',len(quantitative), 'quantitative variables')\n\ndf_train = all_data.iloc[0:m, :]\ndf_test = all_data.iloc[m:, :]","5c931f3a":"## Small conclusion:\n1. Deviate from the normal distribution\n2. Big positive skewness.","4f56cd60":"## Perfect !\n","c4c21602":"### Log transformation for skewed data","f38a0e79":"### Data Correlation","fe22bdb1":"In our case we see that PoolQC, MiscFeature, Alley, Fence and FireplaceQU have more than 40% of missing data, so we should <strong>just delete this variables<\/strong><\/p>\n","2eef217a":"### Let's create some dummy variables !","f1eb3c5c":"# Feature Engineering (work with categorical variables)\n## Categorical and numerical data","52c6473c":"## Find this Outliers and drop them","8145bd5a":"# Data Preparation \n## Data Cleaning (missing values, outliers)\n### Let's take a look on missing data\n ####  First of all we have to understand the reason why data goes missing.\n<p style='margin: 20px'>1. <strong>Missing at Random (MAR)<\/strong>: Missing at random means that the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data<\/p>\n<p style='margin: 20px'>2. <strong>Missing Completely at Random (MCAR)<\/strong>: The fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.<\/p>\n<p style='margin: 20px'>3. <strong>Missing not at Random (MNAR)<\/strong>: Two possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable\u2019s value (e.g. Let\u2019s assume that females generally don\u2019t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)<\/p>\n\nIn the first two cases, it is safe to remove the data with missing values depending upon their occurrences, while in the third case removing observations with missing values can produce a bias in the model. So we have to be really careful before removing observations. Note that imputation does not necessarily give better results.","7d7e0fc1":"## Conlusion \nNow data are ready and it's time to create a predictive model, your turn.\nThanks !\n### References\nhttps:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4,\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python","27be69a9":"## Check","505087c5":"## Now we need to finalize the rest of missing values","691fd51d":"### SalePrice has the biggest correlations with OverallQual, GrLivArea,GarageCars and GarageArea (they have strong corraletion between each other)  and TotalBsmtSF","0e900a79":"# Data understanding - is everything\n\n<img style='width: 80%;margin:50px 0' src='https:\/\/40ujpa2tq6oi3ky5sc2d4o2f-wpengine.netdna-ssl.com\/wp-content\/uploads\/napoleon-war-information.png'>\n<strong>War is 90% information.<\/strong> \u2013 Napoleon Bonaparte, French military and political leader\n\nNowadays data in the whole world increase with incredible speed.\nYou can solve a lot of problems and do your buisness better due to using data science methods (make predictions, understand your customer better, optimize your processes and so on)\n\nAnd simple rule works perfectly:\n<strong>Who has more data - that one wins<\/strong>\n\nData understanding isn't the most difficult thing in data science, but it have to be the first and very important step for safing time and for correct further conclusions and actions.\n\n**Our pipeline :**:\n\n1. EDA (exploratory data analysis)\n2. Data preparation \n\n    <p style='margin-top:10px;'>2.1. Data Cleaning (missing values, outliers)<\/p>\n    <p>2.2. Feature Engineering(work with categorical variables)<\/p>\n\n# EDA (exploratory data analysis)\n## First of all you need to understand the problem that you going to deal with.\n\nIn this case, we need to predict houses prices - typical **regression problem**.\n\n**So, lets begin** !","328ccbde":"\n## Let's work with outliers of the most correlated with SalePrice variable","57554d4d":"## Let's explore SalePrice "}}