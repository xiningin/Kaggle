{"cell_type":{"f26505f5":"code","112dec66":"code","7fe2dc12":"code","85a80dfe":"code","8e589c50":"code","d6ffd5cf":"code","806f0556":"code","d1c59574":"code","9c1ff89a":"code","57e8fc39":"code","b9545bbf":"code","aaba23fb":"code","a238eec0":"code","29da6469":"code","2dc1494b":"code","6554016f":"code","3c36cdb1":"code","9f4922b8":"code","a90912c5":"code","923c0525":"code","b8f598c0":"code","6d7852f2":"code","7ea76757":"code","d6d07301":"code","693d6938":"code","ce5a99dc":"code","cf45283e":"code","8914cba0":"code","b765f317":"code","6fd02db4":"code","e855498d":"code","f9659a33":"code","471276b0":"code","8f6852ba":"code","f71d54aa":"code","fd9d17bd":"code","b95d4c39":"code","d14d1706":"code","e0519e31":"code","a50f33e0":"code","d56ee4b8":"code","a355a465":"code","2268f2f7":"code","bb4221b0":"code","2edf8e01":"code","380a28a4":"code","40afbb37":"code","1b702b9b":"markdown","7b9e1d9f":"markdown","8b91a9df":"markdown","af7f5654":"markdown","eb27808c":"markdown","96d83887":"markdown","a2cdb362":"markdown","5df49bb8":"markdown","8a20f334":"markdown","dc9022cd":"markdown","e42255f0":"markdown","88bcbc5f":"markdown","1b5078b0":"markdown","237111a1":"markdown","8fda5310":"markdown","daa44671":"markdown","ff50b46b":"markdown","ccd94447":"markdown","64cc5e30":"markdown"},"source":{"f26505f5":"#Install\/load required packages\n!pip install dython | grep -v 'already satisfied'\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","112dec66":"#Upload and record housing prices data \ntrain_file_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_file_path = '..\/input\/home-data-for-ml-course\/test.csv'\n\ntrain_data = pd.read_csv(train_file_path, index_col='Id')\ntest_data = pd.read_csv(test_file_path, index_col='Id')","7fe2dc12":"#Visualize the first entries of the dataset\ntrain_data.head()\n#test_data.head()","85a80dfe":"#Brief summary of numerical columns\ntrain_data.describe()\n#test_data.describe()","8e589c50":"#Visualize the house sale price distribution\nplt.figure(figsize=(12, 6))\nplt.title('Fig. 0. Distribution of sale price.')\nplt.xlabel('Sale price')\nplt.ylabel('Frequency')\nfig0 = sns.histplot(x=train_data.SalePrice, kde=True)","d6ffd5cf":"#Brief summary of categorical columns\ntrain_data.describe(include=['O'])","806f0556":"#Identify columns with 'NaN' values and count the NaNs (train set)\ntrain_data.isnull().sum()[train_data.isna().sum() != 0]","d1c59574":"#Identify columns with 'NaN' values and count the NaNs (test set)\ntest_data.isna().sum()[test_data.isna().sum() != 0]","9c1ff89a":"#Train\/test dataset columns with missing values (sorted in an ascending order of NA entries)\ntrain_data_na_cols = train_data.isna().sum().sort_values()[train_data.isna().sum() != 0].index\ntest_data_na_cols = test_data.isna().sum().sort_values()[test_data.isna().sum() != 0].index","57e8fc39":"#Visualize missing entries in the train dataset  \nplt.figure(figsize=(15,6))\n\nfig1 = sns.heatmap(train_data.isna()[train_data_na_cols], yticklabels = False, \n                   cbar = False, cmap=\"viridis\")\nplt.ylabel('');\nfig1.set(title = \"Fig. 1. Columns with missing entries from the train dataset.\");","b9545bbf":"#Visualize missing entries in the test dataset  \nplt.figure(figsize=(15,6))\n\nfig2 = sns.heatmap(test_data.isna()[test_data_na_cols], yticklabels = False, \n                   cbar = False, cmap=\"viridis\")\nplt.ylabel('');\nfig2.set(title = \"Fig. 2. Columns with missing entries from the test dataset.\");","aaba23fb":"num_cols = [col for col in train_data.columns if train_data[col].dtype in ['int64', 'float64']]\n\n#Numerical columns without 'SalePrice' and 'LotFrontage'\nnum_cols_no_SPLF = [col for col in num_cols if col not in ['SalePrice', 'LotFrontage']]\n\ncat_cols = [col for col in train_data.columns if train_data[col].dtype == 'object']\n\nall_cols = cat_cols + num_cols\n\n#Print out the total number of columns to make sure that nothing is missing\nprint(f'The total number of columns: {len(all_cols)}.')\nprint(f'Expected number of columns: {train_data.shape[1]}.')","a238eec0":"#Create lists of garage-related numerical and categorical features  \ngar_num_cols = ['GarageYrBlt','GarageCars','GarageArea']\ngar_cat_cols = ['GarageType','GarageFinish','GarageQual','GarageCond']\ngar_cols = gar_num_cols + gar_cat_cols","29da6469":"#Create lists of basement-related numerical and categorical features  \nbsmt_num_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                 'TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\nbsmt_cat_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', \n                 'BsmtFinType1', 'BsmtFinType2']\nbsmt_cols = bsmt_num_cols + bsmt_cat_cols","2dc1494b":"from dython.nominal import compute_associations\n\n#Compute numerical-numerical correlations\nnum_corr_table = compute_associations(train_data[num_cols])\n\n#Visualize correlations\nplt.subplots(figsize=(15,2))\nfig3 = sns.heatmap(num_corr_table[['SalePrice']].sort_values(by=['SalePrice']).T)\nfig3.set(title = \"Fig. 3. Corelations between 'SalePrice' and the rest of numerical features.\");","6554016f":"#Add 'SalePrice' to the list of categorical features for the data visualisation purposes\ncat_cols.append('SalePrice')\n\n#Compute numerical-categorical feature associations\nnumcat_assoc_table = compute_associations(train_data[cat_cols])\n\n#Visualize associations\nplt.subplots(figsize=(15,2))\nfig4 = sns.heatmap(numcat_assoc_table[['SalePrice']].sort_values(by=['SalePrice']).T)\nfig4.set(title = \"Fig. 4. Associations between 'SalePrice' and categorical features.\")\n\n#Remove 'SalePrice' from the list of categorical features\ncat_cols.pop();","3c36cdb1":"#Add 'SalePrice' to the list of basement-related features for the data visualisation purposes\nbsmt_cols.append('SalePrice')\n\n#Compute associations\nbsmt_assoc_table = compute_associations(train_data[bsmt_cols])\n\n#Visualize associations\nplt.subplots(figsize=(15,2))\nfig5 = sns.heatmap(bsmt_assoc_table[['SalePrice']].sort_values(by=['SalePrice']).T)\nfig5.set(title = \"Fig. 5. Associations between 'SalePrice' and basement-related features.\");\nbsmt_cols.pop();","9f4922b8":"#Before imputing the data, we create a copy of the train and test datasets\ntrain_data_imp = train_data.copy() \ntest_data_imp = test_data.copy() ","a90912c5":"#Locate the rows with a basement area = 0 (meaning there are no basements) \n#and replace the corresponding 'NaN' values of the garage categorical columns with 'NoBsmt'\ntrain_data_imp.loc[train_data_imp['TotalBsmtSF'] == 0, bsmt_cat_cols] = \\\ntrain_data_imp.loc[train_data_imp['TotalBsmtSF'] == 0, bsmt_cat_cols].fillna(value = 'NoBsmt')\n\ntest_data_imp.loc[test_data_imp['TotalBsmtSF'] == 0, bsmt_cat_cols] = \\\ntest_data_imp.loc[test_data_imp['TotalBsmtSF'] == 0, bsmt_cat_cols].fillna(value = 'NoBsmt')","923c0525":"#Count remaining 'NaN' values in the basement-related columns in the train set\ntrain_data_imp[bsmt_cols].isna().sum()[train_data_imp[bsmt_cols].isna().sum() != 0]","b8f598c0":"#Count remaining 'NaN' values in the basement-related columns in the test set\ntest_data_imp[bsmt_cols].isna().sum()[test_data_imp[bsmt_cols].isna().sum() != 0]","6d7852f2":"#Add 'SalePrice' to the list of garage-related features for the data visualisation purposes\ngar_cols.append('SalePrice')\n\n#Compute associations\ngar_assoc_table = compute_associations(train_data[gar_cols])\n\n#Visualize associations\nplt.subplots(figsize=(15,2))\nfig6 = sns.heatmap(gar_assoc_table[['SalePrice']].sort_values(by=['SalePrice']).T)\nfig6.set(title = \"Fig. 6. Associations between 'SalePrice' and garage-related features.\");\ngar_cols.pop();","7ea76757":"#Locate the rows with a garage area = 0 (meaning there are no garages in such houses) \n#and replace the corresponding 'NaN' categorical values with 'NoGarage'\ntrain_data_imp.loc[train_data_imp.GarageArea == 0, gar_cat_cols] = \\\ntrain_data_imp.loc[train_data_imp.GarageArea == 0, gar_cat_cols].fillna(value = 'NoGarage')\n\ntest_data_imp.loc[test_data_imp.GarageArea == 0, gar_cat_cols] = \\\ntest_data_imp.loc[test_data_imp.GarageArea == 0, gar_cat_cols].fillna(value = 'NoGarage')\n\n#When there is no garage, then replace the 'NaN' values in the 'GarageYrBlt' column with 0\ntrain_data_imp.loc[train_data_imp.GarageArea == 0, 'GarageYrBlt'] = \\\ntrain_data_imp.loc[train_data_imp.GarageArea == 0, 'GarageYrBlt'].fillna(value = 0)\n\ntest_data_imp.loc[test_data_imp.GarageArea == 0, 'GarageYrBlt'] = \\\ntest_data_imp.loc[test_data_imp.GarageArea == 0, 'GarageYrBlt'].fillna(value = 0)","d6d07301":"#Count remaining 'NaN' values in the garage-related columns in the train set\ntrain_data_imp[gar_cols].isna().sum()[train_data_imp[gar_cols].isna().sum() != 0]","693d6938":"#Count remaining 'NaN' values in the basement-related columns in the test set\ntest_data_imp[gar_cols].isna().sum()[test_data_imp[gar_cols].isna().sum() != 0]","ce5a99dc":"def replace_na_values(check_column, nan_column, replace_value):\n    train_data_imp.loc[(train_data_imp[nan_column].isna()) & \n                   (train_data_imp[check_column] == 0), nan_column] = replace_value\n    test_data_imp.loc[(test_data_imp[nan_column].isna()) & \n                   (test_data_imp[check_column] == 0), nan_column] = replace_value","cf45283e":"#Impute the fireplace-, pool-, and miscellaneous-related features\nreplace_na_values('Fireplaces', 'FireplaceQu', 'NoFireplace')\nreplace_na_values('PoolArea', 'PoolQC', 'NoPool')\nreplace_na_values('MiscVal', 'MiscFeature', 'NoMisc')","8914cba0":"#Replace NaN 'Fence' values\ntrain_data_imp.loc[train_data_imp['Fence'].isna(), 'Fence'] = 'NoFence'\ntest_data_imp.loc[test_data_imp['Fence'].isna(), 'Fence'] = 'NoFence'\n\n#Replace NaN 'Alley' values\ntrain_data_imp.loc[train_data_imp['Alley'].isna(), 'Alley'] = 'NoAlley'\ntest_data_imp.loc[test_data_imp['Alley'].isna(), 'Alley'] = 'NoAlley'","b765f317":"train_data_imp.drop(['LotFrontage'], axis=1, inplace=True)\ntest_data_imp.drop(['LotFrontage'], axis=1, inplace=True)","6fd02db4":"#Identify columns with 'NaN' values and count NaNs (train set)\ntrain_data_imp.isna().sum()[train_data_imp.isna().sum() != 0]","e855498d":"test_data_imp.isna().sum()[test_data_imp.isna().sum() != 0]","f9659a33":"#Train\/test dataset columns with missing values (sorted in an ascending order of NA entries)\ntrain_data_imp_na_cols = train_data_imp.isna().sum().sort_values()\\\n                         [train_data_imp.isna().sum() != 0].index\ntest_data_imp_na_cols = test_data_imp.isna().sum().sort_values()\\\n                        [test_data_imp.isna().sum() != 0].index","471276b0":"#Visualize missing entries in the train dataset after implementing the imputation  \nplt.figure(figsize=(15,7))\n\nfig7 = sns.heatmap(train_data_imp.isna()[train_data_imp_na_cols], yticklabels = False, \n                   cbar = False, cmap=\"viridis\")\nplt.ylabel('');\nfig7.set(title = \"Fig. 7. Train dataset columns with missing entries after imputation.\");","8f6852ba":"#Visualize missing entries in the test dataset after implementing the imputation  \nplt.figure(figsize=(15,7))\n\nfig8 = sns.heatmap(test_data_imp.isna()[test_data_imp_na_cols], yticklabels = False, \n                   cbar = False, cmap=\"viridis\")\nplt.ylabel('');\nfig8.set(title = \"Fig. 8. Test dataset columns with missing entries after imputation.\");","f71d54aa":"#Create a list of ordinal categorical features\ncat_cols_ordinal = ['LotShape', 'LandContour', 'Utilities', 'LandSlope', \n                    'ExterQual', 'ExterCond', 'BsmtCond', 'BsmtExposure',\n                    'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir',\n                    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', \n                    'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'BsmtQual']\n\n#Create a list of nominal categorical features\ncat_cols_onehot = [x for x in cat_cols if x not in cat_cols_ordinal]\n\nprint(f'The number of ordinal categorical columns: {len(cat_cols_ordinal)}.')\nprint(f'The number of nominal categorical columns: {len(cat_cols_onehot)}.')\nprint(f'The total expected number of categorical columns: {len(cat_cols)}.')","fd9d17bd":"#Create a dictionary with ordinal categorical columns as keys \n#and corresponding categories as values\nordinal_cols_dict = {}\nfor col in cat_cols_ordinal:\n    ordinal_cols_dict[col] = train_data_imp[col].unique().tolist()\nordinal_cols_dict","b95d4c39":"from sklearn.preprocessing import OrdinalEncoder\n\nex_data = np.asarray([['Reg'], ['IR2'], ['IR1'], ['IR3']])\nprint('Example dataset before ordinal encoding: \\n', ex_data)\n\nord_enc = OrdinalEncoder(categories=[['Reg', 'IR1', 'IR2', 'IR3']])\nex_data_transformed = ord_enc.fit_transform(ex_data)\nprint('Example dataset after ordinal encoding (without proper ordering): \\n', \n      ex_data_transformed)","d14d1706":"#Perform a proper orgering of the corresponding categories\nordinal_cols_dict['LotShape'] = ['IR3', 'IR2', 'IR1', 'Reg']\nordinal_cols_dict['LandContour'] = ['Low', 'HLS', 'Bnk', 'Lvl']\nordinal_cols_dict['Utilities'] = ['NoSeWa', 'AllPub']\nordinal_cols_dict['LandSlope'] = ['Sev', 'Mod', 'Gtl']\nordinal_cols_dict['ExterQual'] = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['ExterCond'] = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['BsmtCond'] = ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['BsmtExposure'] = ['NoBsmt', 'No', 'Mn', 'Av', 'Gd']\nordinal_cols_dict['BsmtFinType1'] = ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\nordinal_cols_dict['BsmtFinType2'] = ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\nordinal_cols_dict['HeatingQC'] = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['CentralAir'] = ['N', 'Y']\nordinal_cols_dict['KitchenQual'] = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['Functional'] = ['Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ']\nordinal_cols_dict['FireplaceQu'] = ['NoFireplace', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['GarageFinish'] = ['NoGarage', 'Unf', 'RFn', 'Fin']\nordinal_cols_dict['GarageQual'] = ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['GarageCond'] = ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['PavedDrive'] = ['N', 'P', 'Y']\nordinal_cols_dict['PoolQC'] = ['NoPool','Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict['BsmtQual'] = ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nordinal_cols_dict","e0519e31":"#Create a list of ordered ordinal categories suitable for model-training in the subsequent section\nmy_categories = list(ordinal_cols_dict.values())\nmy_categories","a50f33e0":"#Train dataset that is ready for model training\/prediction\ny_train = train_data_imp.SalePrice\ntrain_data_imp.drop(['SalePrice'], axis=1, inplace=True)\nX_train_imp = train_data_imp.copy()\n\n#Test dataset that is ready for model training\/prediction\nX_test_imp = test_data_imp.copy()","d56ee4b8":"#Load required packages\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n#Numerical and categorical columns transformation pipelines\nnum_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='median'))\n])\n\ncat_transformer_ordinal = Pipeline(steps=[\n    ('cat_ord_imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal_transf', OrdinalEncoder(categories=my_categories,\n                                      handle_unknown='use_encoded_value', \n                                      unknown_value=np.nan))\n])\n\ncat_transformer_onehot = Pipeline(steps=[\n    ('cat_nom_imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot_transf', OneHotEncoder(handle_unknown='ignore'))\n])\n\n#Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_simple', num_transformer, num_cols_no_SPLF),\n        ('cat_ordinal', cat_transformer_ordinal, cat_cols_ordinal),\n        ('cat_onehot', cat_transformer_onehot, cat_cols_onehot)\n])","a355a465":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\nmodel_LR = LinearRegression()\n\n#Bundle preprocessing and modeling code in a pipeline\npipeline_LR = Pipeline(steps=[('preprocessor', preprocessor), \n                               ('LR_model', model_LR)])\n\n#MAE score using cross validation\nscores = -1 * cross_val_score(pipeline_LR, X_train_imp, y_train,\n                              cv=5, scoring='neg_mean_absolute_error')\n\nprint('MAE score for the Linear Regression:', np.mean(scores))","2268f2f7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel_DTR = DecisionTreeRegressor()\n\n#Bundle preprocessing and modeling code in a pipeline\npipeline_DTR = Pipeline(steps=[('preprocessor', preprocessor), \n                               ('DTR_model', model_DTR)])\n\n#Hyperparameter tuning implementation\nparam_grid_DTR = {\n    'DTR_model__max_depth': [10, 20, 40], \n    'DTR_model__min_samples_leaf': [5, 10, 20], \n    'DTR_model__min_samples_split': [2, 5, 10],\n    'DTR_model__random_state': [10],\n}\n\nsearchCV_DTR = GridSearchCV(pipeline_DTR, param_grid=param_grid_DTR,\n                            cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n\nsearchCV_DTR.fit(X_train_imp, y_train)\nprint('Best parameters for the Decision Tree Regressor:')\nfor parameter in searchCV_DTR.best_params_:\n    print (f'{parameter}: {searchCV_DTR.best_params_[parameter]}')\n      \nprint('\\nBest MAE score for the Decision Tree Regressor: ',\n      searchCV_DTR.best_score_*(-1))","bb4221b0":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_RFR = RandomForestRegressor()\n\n#Bundle preprocessing and modeling code in a pipeline\npipeline_RFR = Pipeline(steps=[('preprocessor', preprocessor), \n                               ('RFR_model', model_RFR)])\n\n#Hyperparameter tuning implementation\nparam_grid_RFR = {\n    'RFR_model__n_estimators': [80, 100], \n    'RFR_model__max_depth': [15, 30],\n    'RFR_model__min_samples_split': [2, 5], \n    'RFR_model__random_state': [10],\n}\n\nsearchCV_RFR = GridSearchCV(pipeline_RFR, param_grid=param_grid_RFR,\n                            cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n\nsearchCV_RFR.fit(X_train_imp, y_train)\n\nprint('Best parameters for the Random Forest Regressor:')\nfor parameter in searchCV_RFR.best_params_:\n    print (f'{parameter}: {searchCV_RFR.best_params_[parameter]}')\n    \nprint('\\nBest MAE score for the Random Forest Regressor: ',\n      searchCV_RFR.best_score_*(-1))","2edf8e01":"from xgboost import XGBRegressor\n\nmodel_XGBR = XGBRegressor()\n\n#Bundle preprocessing and modeling code in a pipeline\npipeline_XGBR = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('XGBR_model', model_XGBR)])\n\n#Hyperparameter tuning implementation\n#To speed up this block, non-optimal parameters are commented out\nparam_grid = {\n    'XGBR_model__n_estimators': [1500], #[1000, 1250, 1500]\n    'XGBR_model__learning_rate': [0.07], #[0.07] [0.065, 0.07, 0.075],\n    'XGBR_model__max_depth': [2], #[2,5,10]\n    'XGBR_model__reg_alpha': [1], #[0, 0.5, 1, 5]\n    'XGBR_model__reg_lambda': [0.01], #[0, 0.05, 0.1, 1]\n    'XGBR_model__subsample': [0.95], #[0.5, 0.75, 0.9]\n    'XGBR_model__colsample_bytree': [0.5]\n}\n\nsearchCV_XGBR = GridSearchCV(pipeline_XGBR, param_grid=param_grid,\n                             cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n\nsearchCV_XGBR.fit(X_train_imp, y_train)\nprint('Best parameters for the XGBoosting Regressor:')\nfor parameter in searchCV_XGBR.best_params_:\n    print (f'{parameter}: {searchCV_XGBR.best_params_[parameter]}')\n    \nprint('Best MAE score for the XGBoosting Regressor:',\n      searchCV_XGBR.best_score_*(-1))","380a28a4":"pipeline_final = searchCV_XGBR.best_estimator_\npredictions = pipeline_final.predict(X_test_imp)","40afbb37":"#Generate output\noutput = pd.DataFrame({'Id': X_test_imp.index, \n                       'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)\nprint('Your submission was successfully saved!')","1b702b9b":"One can see that most of the missing values in the garage-related features have been fixed, as well.\n\n## <a id='Link-SecImp-FP'>Fireplace-, pool-, miscellaneous-, fence-, and alley-related features<\/a>\n\nIn this subsection we will impute the rest of the missing values whose values may be deduced from the information provided in columns that don't contain missing entries.\n\nFirst, we will consider the fireplace-, pool-, and miscallaneous-related features. We notice that <span style=\"color:FireBrick;\">'FireplaceQu'<\/span>, <span style=\"color:FireBrick;\">'PoolQC'<\/span>, and <span style=\"color:FireBrick;\">'MiscFeature'<\/span> contain significant amount of missing values. These values can be imputed using the following strategy: when the number of fireplaces (<span style=\"color:FireBrick;\">'Fireplaces'<\/span>)\/the pool area (<span style=\"color:FireBrick;\">'PoolArea'<\/span>)\/the number of miscellaneous features (<span style=\"color:FireBrick;\">'MiscVal'<\/span>) equals to 0, then the corresponding 'NaN' values in the <span style=\"color:FireBrick;\">'FireplaceQu'<\/span>\/<span style=\"color:FireBrick;\">'PoolQC'<\/span>\/<span style=\"color:FireBrick;\">'MiscFeature'<\/span> column indicate that the corresponding house did not contain such a feature.  \n\nSince the imputation strategy for these 3 columns is identical, it is convenient to define a general function that will be performing such an imputation.","7b9e1d9f":"### <a id='Link-SecModel-Training-XGBR'>Gradient boosting regressor<\/a>","8b91a9df":"| Model  | MAE | \n| --- | --- | \n| Linear Regression | 19929 |\n| Decision Tree Regressor | 23758 | \n| Random Forest Regressor | 17190 | \n| Gradient Boosting Regressor | 14827 | ","af7f5654":"# <a id='Link-SecSubm'>Submission<\/a> \n\nFinally, we prepare our predictions for a submition to the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course). \ud83e\udd73\ud83c\udf89","eb27808c":"Now we can see that all the remaining columns with 'NaN' values contain only a few NaNs, which can be safely imputed using one of the standard imputers.\n\n# <a id='Link-SecOrdPrep'>Preparation of ordinal categorical data for label encoding<\/a>\n\nIn this section, we will prepare our categorical columns for model training\/prediction. We note that our dataset contains 2 types of categorical columns:\n\n* Ordinal (when the column has a clear ordering of categories) \n* Nominal (when the column has no clear ordering of categories). \n\nWe will use the ordinal encoding strategy for ordinal columns and the one-hot encoding strategy for nominal columns. Check out [this](https:\/\/www.kaggle.com\/alexisbcook\/missing-values) and [this](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) Kaggle tutorials for more details.\n\nThe categories of ordinal data need to be properly ordered, so that the ordinal encoder assigns them proper numerical values. The corresponding values can be deduced from [this](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data) data description information. \nFor example, an entry in the <span style=\"color:FireBrick;\">'LotShape'<\/span> column can take the following values: `Reg`, `IR1`, `IR2`, or `IR3`. We want the ordinal encoder to assign them the values `3`, `2`, `1`, `0`, respectively (and not vice versa!!!).\n\nBelow we prepare our data for ordinal and one-hot encodings.","96d83887":"Now let's impute the missing values in <span style=\"color:FireBrick;\">'Fence'<\/span> and <span style=\"color:FireBrick;\">'Alley'<\/span> columns. The respective imputation strategy is even simpler than for the <span style=\"color:FireBrick;\">'FireplaceQu'<\/span>\/<span style=\"color:FireBrick;\">'PoolQC'<\/span>\/<span style=\"color:FireBrick;\">'MiscFeature'<\/span> columns. According to [this description](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data), the 'NaN' values in <span style=\"color:FireBrick;\">'Fence'<\/span> and <span style=\"color:FireBrick;\">'Alley'<\/span> columns are most likely due to the absence of the corresponding feature. Therefore, we will simply replace the coresponding NaNs by 'NoFence' or 'NoAlley'.","a2cdb362":"## <a id='Link-SecImp-Others'>Rest of the features<\/a>\n\nAt this stage, none of the remaining 'NaN' values can be imputed in a clever manner. Luckily, most of the remaning columns contain only a few 'NaN' entries. The only exception is the <span style=\"color:FireBrick;\">'LotFrontage'<\/span> column, which contains significant number of NaNs. By taking a look at [Fig. 3](#Fig1Link) and checking the correlation between <span style=\"color:FireBrick;\">'LotFrontage'<\/span> and <span style=\"color:FireBrick;\">'SalePrice'<\/span>, we notice that the <span style=\"color:FireBrick;\">'LotFrontage'<\/span> feature is not strongly correlated with the <span style=\"color:FireBrick;\">'SalePrice'<\/span>. In addition, by checking [here](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data) the feature description information, we notice that most likely the <span style=\"color:FireBrick;\">'LotArea'<\/span> column (by default) contains information about <span style=\"color:FireBrick;\">'LotFrontage'<\/span>. Therefore, we decide to remove the <span style=\"color:FireBrick;\">'LotFrontage'<\/span> column from our model training\/prediction.","5df49bb8":"### <a id='Link-SecModel-Training-DTR'>Decision tree regressor<\/a>","8a20f334":"We notice that most of the features that contain missing values in the train\/test dataset have a strong degree of association\/correlation with <span style=\"color:FireBrick;\">'SalePrice'<\/span>. This means that we should carefully impute the corresponding missing values, as they may have a significant impact on the predictions of our model.\n\n# <a id='Link-SecImp'>Data imputation<\/a>\n\nIn order to better understand the data imputation strategy discussed below, one may need to look up occasionaly the \"Data Description\" information for the housing-prices dataset provided [here](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data).\n\n## <a id='Link-SecImp-Bsmt'>Basement-related features<\/a>\n\nWe will start with imputing the basement-related features. First, we will visualize the corresponding data and notice that, for example, the hight of a basement (<span style=\"color:FireBrick;\">'BsmtQual'<\/span>) and its area (<span style=\"color:FireBrick;\">'TotalBsmtSF'<\/span>) have a strong association\/correlation with the price of a house.","dc9022cd":"## <a id='Link-SecDataExpl-GenAss'>General assesment of correlations\/associations <\/a>\n\nHere, we will peform a general assesment of correlation\/assosiation between the <span style=\"color:FireBrick;\">'SalePrice'<\/span> column and rest of the features. The <span style=\"color:FireBrick;\">'SalePrice'<\/span> is a numerical column, whereas the other 79 columns are either numerical or categorical. This means, we will need to analyze 2 different types of association\/correlation: 1) a numerical-numerical correlation and 2) a categorical-numerical association.\n\nTo estimate the corresponding associations\/correlations, we will use the [dython](http:\/\/shakedzy.xyz\/dython\/) library (see [this link](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9) and [this kaggle notebook](https:\/\/www.kaggle.com\/alexkoshchii\/top-7-titanic-dataset-notebook) for more examples). The dython library enables one to evaluate the following:\n\n* The `categorical-categorical` feature association (e.g. using the _Theil's U_ measure).\n* The `numerical-categorical` feature association (using the _correlation ratio_ measure called Eta). \n* The `numerical-numerical` correlation (using the _Pearson's R_ measure).\n<a id='Fig1Link'><\/a>","e42255f0":"## <a id='Link-SecModel-Training'>Model training<\/a>\n\nIn this subsection, we will fit different machine learning algorithms with our preprocessed data and choose the optimal parameters for each model. We will consider the following models:\n* Linear regression\n* Decision tree regressor\n* Random forest regressor\n* Gradient boosting regressor\n\n### <a id='Link-SecModel-Training-LR'>Linear regression<\/a>\n","88bcbc5f":"# <a id='Link-SecModel'>Model training, selection, and predictions<\/a>\n\n## <a id='Link-SecModel-DataPrepr'>Data preprocessing<\/a>\n\nBefore we train our model and make predictions, we will first preprocess our categorical and numerical columns using the ColumnTransformer class of the `sklearn.compose` module and prepare the data for fitting a machine learning model. (Check out [this turotial](https:\/\/www.kaggle.com\/alexisbcook\/pipelines) to learn more about keeping your data preprocessing and modeling code organized) ","1b5078b0":"### <a id='Link-SecModel-Training-RFR'>Random forest regressor<\/a>","237111a1":"# <a id='Link-SecDataExpl'>Data exploration<\/a>  \n\n## <a id='Link-SecDataExpl-Summary'>Data summary<\/a>","8fda5310":"<h2><center><font size='6'>Top 2% Housing Prices Dataset Notebook<\/font><\/center><\/h2>\n\nTable Of Contents  \n=\n\n1. <a href='#Link-SecIntro'>Introduction<\/a>  \n2. <a href='#Link-SecDataPrep'>Data preparation<\/a>  \n3. <a href='#Link-SecDataExpl'>Data exploration<\/a>   \n - <a href='#Link-SecDataExpl-Summary'>Data summary<\/a>   \n - <a href='#Link-SecDataExpl-Supplem'>Supplemental lists of features<\/a>\n - <a href='#Link-SecDataExpl-GenAss'>General assesment of correlations\/associations <\/a>\n4. <a href='#Link-SecImp'>Data imputation<\/a>\n - <a href='#Link-SecImp-Bsmt'>Basement-related features<\/a>\n - <a href='#Link-SecImp-Gar'>Garage-related features<\/a>\n - <a href='#Link-SecImp-FP'>Fireplace-, pool-, miscellaneous-, fence-, and alley-related features<\/a>\n - <a href='#Link-SecImp-Others'>Rest of the features<\/a>\n5. <a href='#Link-SecOrdPrep'>Preparation of ordinal categorical data for label encoding<\/a>\n6. <a href='#Link-SecModel'>Model training, selection, and predictions<\/a>\n - <a href='#Link-SecModel-DataPrepr'>Data preprocessing<\/a>\n - <a href='#Link-SecModel-Training'>Model training<\/a><br>\n  a) <a href='#Link-SecModel-Training-LR'>Linear regression<\/a><br>\n  b) <a href='#Link-SecModel-Training-DTR'>Decision tree regressor<\/a><br>\n  c) <a href='#Link-SecModel-Training-RFR'>Random forest regressor<\/a><br>\n  d) <a href='#Link-SecModel-Training-XGBR'>Gradient boosting regressor<\/a><br>\n - <a href='#Link-SecModel-Selection'>Model selection and predictions<\/a>\n7. <a href='#Link-SecSubm'>Submission<\/a>  \n\n# <a id='Link-SecIntro'>Introduction<\/a>  \n\nIn this notebook we explore a housing pricing dataset using Python libraries such as `pandas`, `numpy`, `matplotlib`, `seaborn`, and `dython`. After that, we will prepare the dataset for model training, after that we will consider four different ML models using `sklearn` and `xgboost` libraries, and then assess the performance of each model using cross validation. Ultimately, we will choose the best ML model, select the best parameters for this model using `GridSearchCV`, and apply the model on the test dataset to predict the housing prices in Boston. Finally, we will prepare the predictions for a submission to the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) competition. This notebook achieves top 2% performance on the test set.\n\n\ud83d\ude00 !!!Any feedback is appreciated!!! \ud83d\ude00\n\n# <a id='Link-SecDataPrep'>Data preparation<\/a>  ","daa44671":"Below, let us consider a small example which illustrates how the OrdinalEncoder transforms a dataset. This example emphasizes that a proper ordering of the ordinal categories is required in order to assign them meaningful numerical values. ","ff50b46b":"One can see that most of the missing values in the basement-related features have been fixed.\n\n## <a id='Link-SecImp-Gar'>Garage-related features<\/a>\n\nIn a similar fashion, we will fix most of the 'NaN' values in the garage-related columns.","ccd94447":"## <a id='Link-SecModel-Selection'>Model selection and predictions<\/a>\n\nAfter testing four different regression models on cross validation data, we provide below a table that provides a summarizy of the mean absolute error scores achieved by these models. It is clear that the XGBoosting regressor performs the best on the cross validation data, and thus it will be employed for generating predictions for the test dataset. ","64cc5e30":"We can see that there is a significant portion of missing values in both the training and testing datasets. However, as we will see later on, most of the missing values can be imputed in a clever way, meaning that the actual values can be deduced from the information provided in columns that don't contain missing entries.\n\n## <a id='Link-SecDataExpl-Supplem'>Supplemental lists of features<\/a>\n\nIn this subsection we create supplemental lists of features that we will be using for a clever imputation of missing data. "}}