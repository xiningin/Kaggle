{"cell_type":{"71d4e70f":"code","198e731e":"code","00e6b7d8":"code","bee6dd69":"code","95cf250d":"code","3ed29dc4":"code","1b615fb3":"code","96b0bcff":"code","7ddde091":"code","108e6c9b":"code","933a1c14":"code","7a3b1437":"code","b9c2cedf":"code","d9e0d805":"code","1cc1c0e8":"code","96371c93":"code","74e2ce82":"code","dd58cc78":"code","fc843762":"code","3f38e250":"code","ca130a57":"code","2060b412":"code","23dad595":"code","08efcc24":"code","b484adaf":"code","57f28a3d":"code","f71f6b6f":"code","a8bd5415":"code","c39cd0f0":"code","58ee3959":"code","6765f2f4":"code","c8a5c7b1":"code","4864cda5":"code","0bc9516d":"code","a512dd2a":"code","d9f5c2f9":"code","d3b78e21":"code","46656118":"code","154e20a6":"code","fd4edb79":"code","a8f97326":"code","59229fb0":"code","6d8e62e3":"code","854de084":"code","85d7ad4c":"code","8f2b85e5":"code","843aa8d8":"code","9ffe9b44":"code","241b7134":"code","d05f330f":"code","df059696":"code","a804633d":"code","e9726b3e":"code","32e82707":"code","4f749b1f":"code","34b74877":"code","d2e1ca37":"code","1b33e829":"code","22dea531":"code","6f213a1c":"code","b6673270":"code","691726b3":"code","9cf6229f":"code","807144d5":"code","c6e4bfb9":"code","fdba8ded":"code","bd2f3a47":"code","629d6aa2":"code","40ab5425":"code","674245f5":"code","9dcd52d6":"code","0c2388f7":"code","ee70abef":"code","b2b493c5":"code","51e3394c":"code","15ffcbf1":"code","67a4775f":"code","beefdcc0":"code","f09275cb":"code","bf5fe664":"code","01c14b54":"code","47d5313d":"code","d8957849":"code","3356daa0":"code","2572c5fa":"markdown","4c0634a5":"markdown","8f997cd5":"markdown","3ab5393f":"markdown","98eef6e6":"markdown","4424941f":"markdown","54ef48c1":"markdown","e38368af":"markdown","121d59f8":"markdown","9d786d19":"markdown","75d0640d":"markdown","0e99ffb9":"markdown","a0dc823d":"markdown","555323f9":"markdown","75e9f412":"markdown","6a945fc1":"markdown","2c3b01e6":"markdown","a34668e3":"markdown","3a982423":"markdown","f623e72a":"markdown","fdfbf978":"markdown","d2e3b168":"markdown","6b0ef89b":"markdown","4d3422b1":"markdown","9e38b41e":"markdown","27b94bde":"markdown","814b273b":"markdown","321a2db6":"markdown","5a998d80":"markdown","77559bad":"markdown","2c9cf68c":"markdown","686f8ecc":"markdown","055cee5a":"markdown","81bf29d5":"markdown","0a2bce07":"markdown","95781933":"markdown","d9946bd3":"markdown","6a0918f1":"markdown","ef16eafd":"markdown","97432b80":"markdown","ba01ffff":"markdown","4fcb2739":"markdown","05ab11b1":"markdown","efc54622":"markdown","09f083a0":"markdown","34d4404d":"markdown","52ff8455":"markdown","55c98e79":"markdown","fb4b71f6":"markdown","984dc264":"markdown","5b316145":"markdown","7bfaa8c2":"markdown","53bf8df9":"markdown","ac02ac6f":"markdown","14e31ae1":"markdown","5758116e":"markdown","2d946078":"markdown","654ba9ce":"markdown","1f1ed254":"markdown","4ad8a73e":"markdown","34529594":"markdown","77c0ab2a":"markdown"},"source":{"71d4e70f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom scipy.stats import norm, skew \nimport warnings\nwarnings.filterwarnings(\"ignore\")","198e731e":"train = pd.read_csv('\/kaggle\/input\/loan-prediction-practice-av-competition\/train_csv.csv')\ntest = pd.read_csv('\/kaggle\/input\/loan-prediction-practice-av-competition\/test.csv.csv')\ntrain.head()","00e6b7d8":"train = train.drop('Loan_ID',axis = 1)","bee6dd69":"train.shape","95cf250d":"train.describe()","3ed29dc4":"train.describe(include = ['object'])","1b615fb3":"print(train.info())","96b0bcff":"target = 'Loan_Status'\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntrain[target] = encoder.fit_transform(train[target])\ntrain.head()","7ddde091":"cat_cols = train.dtypes =='object'\ncat_cols = list(cat_cols[cat_cols].index)\nnum_cols = train.dtypes != 'object'\nnum_cols = list(num_cols[num_cols].index)\nnum_cols.remove('Loan_Status')","108e6c9b":"train[cat_cols].head()","933a1c14":"train[num_cols].head()","7a3b1437":"train[target].describe()","b9c2cedf":"sns.countplot(x=target, data=train)","d9e0d805":"def num_boxplot(col,target,train,y = 80000):\n    data =  pd.concat([train[target], train[col]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=str(target), y=col, data=train)\n    fig.axis(ymin=0, ymax=y);","1cc1c0e8":"#Maximum value in ApplicantIncome is 81000.\nnum_boxplot(num_cols[0],target,train,81000)\nnum_boxplot(num_cols[0],target,train,25000)","96371c93":"#Maximum value in CoapplicantIncome is 41667.\nnum_boxplot(num_cols[1],target,train,41667)\nnum_boxplot(num_cols[0],target,train,15000)","74e2ce82":"num_boxplot(num_cols[2],target,train,700)","dd58cc78":"#Maximum loan amount trem is 700. Notice that Q1,Q2 and Q3 are equal\nnum_boxplot(num_cols[3],target,train,1000)","fc843762":"num_boxplot(num_cols[4],target,train,3)","3f38e250":"train[train.Loan_Status == 1].mean()","ca130a57":"train[train.Loan_Status == 0].mean()","2060b412":"for col in cat_cols:\n    sns.barplot(col, target, data=train, color=\"darkturquoise\")\n    plt.show()","23dad595":"#Join the df together handling the missing data together\nall_df = pd.concat([train,test.drop('Loan_ID',axis =1)],axis = 0)\n#train = all_df.iloc[1:614]","08efcc24":"test_id = test['Loan_ID']","b484adaf":"y = train[target]","57f28a3d":"#Drop the target column, it hasn't dropped in test data set. \nall_df = all_df.drop('Loan_Status',axis = 1)\nall_df.head()","f71f6b6f":"all_cols = list(all_df.columns)\nmissing_cols = [col for col in all_cols if all_df[col].isnull().any()]\nlen(missing_cols)","a8bd5415":"#Function to create a data frame with number and percentage of missing data in a data frame\ndef missing_to_df(df):\n    #Number and percentage of missing data in training data set for each column\n    total_missing_df = df.isnull().sum().sort_values(ascending =False)\n    percent_missing_df = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending=False)\n    missing_data_df = pd.concat([total_missing_df, percent_missing_df], axis=1, keys=['Total', 'Percent'])\n    return missing_data_df","c39cd0f0":"missing_df = missing_to_df(all_df)\nmissing_df[missing_df['Total'] > 0]","58ee3959":"all_df['Credit_History'] = all_df['Credit_History'].fillna(2)","6765f2f4":"all_df['Self_Employed'] = all_df['Self_Employed'].fillna('Other')","c8a5c7b1":"from sklearn.impute import SimpleImputer\n\nnum_missing = ['LoanAmount',  'Loan_Amount_Term']\ncat_missing = ['Gender', 'Married','Dependents']","4864cda5":"median_imputer = SimpleImputer(strategy = 'median')\nfor col in num_missing:\n    all_df[col] = pd.DataFrame(median_imputer.fit_transform(pd.DataFrame(all_df[col])))","0bc9516d":"freq_imputer = SimpleImputer(strategy = 'most_frequent')\nfor col in cat_missing:\n    all_df[col] = pd.DataFrame(freq_imputer.fit_transform(pd.DataFrame(all_df[col])))","a512dd2a":"missing_df = missing_to_df(all_df)\nmissing_df[missing_df['Total'] > 0]","d9f5c2f9":"numeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","d3b78e21":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_df[feat] = boxcox1p(all_df[feat], lam)","46656118":"all_df.head()","154e20a6":"#Adding total income by combining applicant's income and coapplicant's income\nall_df['Total_Income'] = all_df['ApplicantIncome'] + all_df['CoapplicantIncome']","fd4edb79":"all_df = all_df.replace({'Dependents': r'3+'}, {'Dependents': 3}, regex=True)","a8f97326":"# process column, apply LabelEncoder to categorical features\nfrom sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\nlbl.fit(list(all_df[\"Dependents\"].values))\nall_df[\"Dependents\"] = lbl.transform(list(all_df[\"Dependents\"].values))\n# shape        \nprint('Shape all_data: {}'.format(all_df.shape))","59229fb0":"s = (all_df.dtypes == 'object')\nobject_cols = list(s[s].index)","6d8e62e3":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    all_df[col] = label_encoder.fit_transform(all_df[col])","854de084":"#all_df = pd.get_dummies(all_df)\nprint(all_df.shape)","85d7ad4c":"train = all_df.iloc[:614]\nprint(train.shape)\ntrain.tail()","8f2b85e5":"test = all_df[614:]\nprint(test.shape)\ntest.tail()","843aa8d8":"def get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test","9ffe9b44":"from sklearn.feature_selection import SelectKBest, f_classif\nsel_train, valid, _ = get_data_splits(pd.concat([train,y],axis=1))\nfeature_cols = sel_train.columns.drop(target)\n\n# Keep 5 features\nselector = SelectKBest(f_classif, k=8)\n\nX_new = selector.fit_transform(sel_train[feature_cols], sel_train[target])\nX_new","241b7134":"# Get back the features we've kept, zero out all other features\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=sel_train.index, \n                                 columns=feature_cols)\nselected_features.head()\n","d05f330f":"# Dropped columns have values of all 0s, so var is 0, drop them\nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the valid dataset with the selected features.\nvalid[selected_columns].head()","df059696":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n","a804633d":"x_train, x_test, y_train, y_test = train_test_split(train,y,random_state = 1)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","e9726b3e":"#Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(accuracy_score(y_test,y_pred)*100,2)\n","32e82707":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc  = round(accuracy_score(y_test,y_pred)*100,2)\nacc_svc\n","4f749b1f":"#kNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_test,y_pred)*100,2)","34b74877":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_test,y_pred)*100,2)","d2e1ca37":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_test,y_pred)*100,2)","1b33e829":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(accuracy_score(y_test,y_pred)*100,2)","22dea531":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state = 1)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nacc_random_forest = round(accuracy_score(y_test,y_pred)*100,2)","6f213a1c":"#LGBMClassifier\n\nlgbc = lgb.LGBMClassifier()\nlgbc.fit(x_train, y_train)\ny_pred = lgbc.predict(x_test)\nacc_lgbc = round(accuracy_score(y_test,y_pred)*100,2)","b6673270":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n             acc_decision_tree,acc_lgbc]})\nmodels.sort_values(by='Score', ascending=False)","691726b3":"x_train, x_test, y_train, y_test = train_test_split(train[selected_columns],y,random_state = 1)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","9cf6229f":"logreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(accuracy_score(y_test,y_pred)*100,2)\nacc_log","807144d5":"coeff_df = pd.DataFrame(train[selected_columns].columns.delete(-1))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","c6e4bfb9":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc  = round(accuracy_score(y_test,y_pred)*100,2)\nacc_svc","fdba8ded":"#kNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_test,y_pred)*100,2)\nacc_knn","bd2f3a47":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_test,y_pred)*100,2)\nacc_gaussian","629d6aa2":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_test,y_pred)*100,2)\nacc_perceptron","40ab5425":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(accuracy_score(y_test,y_pred)*100,2)\nacc_decision_tree","674245f5":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state = 1)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nacc_random_forest = round(accuracy_score(y_test,y_pred)*100,2)\nacc_random_forest","9dcd52d6":"#LGBMClassifier\n\nlgbc = lgb.LGBMClassifier()\nlgbc.fit(x_train, y_train)\ny_pred = lgbc.predict(x_test)\nacc_lgbc = round(accuracy_score(y_test,y_pred)*100,2)\nacc_lgbc","0c2388f7":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n             acc_decision_tree,acc_lgbc]})\nmodels.sort_values(by='Score', ascending=False)","ee70abef":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train[selected_columns].values)\n    rmse= (cross_val_score(model, train[selected_columns].values, \n                                   y.values, scoring=\"f1\", cv = kf))\n    return(rmse)","b2b493c5":"log_reg_score = rmsle_cv(logreg)\nprint(\"\\nLogistic Regression score: {:.4f} ({:.4f})\\n\".format(log_reg_score.mean(), log_reg_score.std()))","51e3394c":"svc_score = rmsle_cv(svc)\nprint(\"\\nSupport Vector Machines score: {:.4f} ({:.4f})\\n\".format(svc_score.mean(), svc_score.std()))","15ffcbf1":"knn_score = rmsle_cv(knn)\nprint(\"\\nkNN score: {:.4f} ({:.4f})\\n\".format(knn_score.mean(), knn_score.std()))","67a4775f":"naive_score = rmsle_cv(gaussian)\nprint(\"\\nGaussian Naive Bayes score: {:.4f} ({:.4f})\\n\".format(naive_score.mean(), naive_score.std()))","beefdcc0":"perceptron_score = rmsle_cv(perceptron)\nprint(\"\\nPerceptron score: {:.4f} ({:.4f})\\n\".format(perceptron_score.mean(), perceptron_score.std()))","f09275cb":"decision_tree_score = rmsle_cv(decision_tree)\nprint(\"\\nDecision_tree score: {:.4f} ({:.4f})\\n\".format(decision_tree_score.mean(), decision_tree_score.std()))","bf5fe664":"random_forest_score = rmsle_cv(random_forest)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(random_forest_score.mean(), random_forest_score.std()))","01c14b54":"lgbc_score = rmsle_cv(lgbc)\nprint(\"\\nLGB Classificier score: {:.4f} ({:.4f})\\n\".format(lgbc_score.mean(), lgbc_score.std()))","47d5313d":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Mean_Score': [svc_score.mean(), knn_score.mean(), log_reg_score.mean(), \n              random_forest_score.mean(), naive_score.mean(), perceptron_score.mean(), \n             decision_tree_score.mean(),lgbc_score.mean()]})\nmodels.sort_values(by='Mean_Score', ascending=False)","d8957849":"logreg = LogisticRegression()\nlogreg.fit(train[selected_columns], y)\nY_pred = logreg.predict(test[selected_columns])","3356daa0":"submission = pd.DataFrame({\n        \"Loan_Id\": test_id,\n        \"Loan_Status\": Y_pred\n    })\nsubmission.head(10)","2572c5fa":"In clustering, the k-Nearest Neighbors algorithm is an unsupervisied learning model. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors","4c0634a5":"In the following section, plot boxes will be made to see the relationship of target varible and numerical features.","8f997cd5":"# Cross valudation","3ab5393f":"The models built with selected features are more accurate.","98eef6e6":"Getting dummy categorical features","4424941f":"**Models with selected features**","54ef48c1":"**Data summary**","e38368af":"**LGBMClassifier**","121d59f8":"Svc , logistic regression and Naive Bayes are always the top 3 highest accurate models in this case.","9d786d19":"Now we compute the coefficient of each features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n* Credit History is highest positive coefficient, implying as the credit history value rises, the probability of Loan Status =1 (Loan approved) increases the most.\n\n* Total income isn't a good artificial feature to model as it is not included in selected features.","75d0640d":"# Visualization","0e99ffb9":"**Naive Bayes Classifiers**","a0dc823d":"KNN confidence score is the lowest.","555323f9":"The distributions of applicant income on different loan status are similar. Hence, there're lot of outliers.","75e9f412":"# Missing Data\/ Data Cleaning","6a945fc1":"Loan_Amount_Term and target","2c3b01e6":"Missing in credit history might mean the credit history of the clients are not available. Fill the missing data with 2 means the data aren't available.","a34668e3":"**Perceptron**","3a982423":"The distributions of applicant income on different loan status are similar. Hence, there're lot of outliers.","f623e72a":"There 7 columns with missing values, let's go further.","fdfbf978":"In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the include parameter.","d2e3b168":"# Modelling","6b0ef89b":"Train test split for model building.","4d3422b1":"We change the target from Yes, No into logical expression.","9e38b41e":"**kNeighborsClassifier**","27b94bde":"Random foresdt is not bad.","814b273b":"**What are average values of numerical features for each loan status?**","321a2db6":"The distribution are similar.","5a998d80":"\nThe following section is using **cross validation** strategy to compare performance of differnence models. f1 is used as our target is binary.\n[ The scoring parameter](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)","77559bad":"Convert target variable into numceric form","2c9cf68c":"Below show how the models above perform on the data by evaluating the cross-validation rmsle error.","686f8ecc":"Missing in self employed can mean a person is not in labor force or retired. So, we give a new categorical to those people.","055cee5a":"**Relationship with numerical features**","81bf29d5":"**Support Vector Machines**","0a2bce07":"Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nNaive Bayes classifiers confidence score is better than SVM but sightly worse than Logisitics Regression.\n","95781933":"# EDA","d9946bd3":"**Box Cox Transformation of (highly) skewed features**","6a0918f1":"**Random Forest**","ef16eafd":"CoapplicantIncome and target","97432b80":"Import Libraries","ba01ffff":"LoanAmount and target","4fcb2739":"# **Data overview**","05ab11b1":"**Relationship with categorical variables.**","efc54622":"The distributions are differnet here. Creidit history is more vary in loan that were disapproved.","09f083a0":"**Decision Tree**","34d4404d":"**Feature engineering**","52ff8455":"**Baseline models**","55c98e79":"**Features selection**","fb4b71f6":"Support Vector Machines is a supervised learning models with associated learning algorithms that analyse data used for classification and regression analysis. \n\nNote that the model generates a confidence score which is lower than Logistics Regression model.","984dc264":"The accuaracy of decision tree is not too high.","5b316145":"There are no more missing data in our data set.","7bfaa8c2":"Applicant Income and target","53bf8df9":"**Model Evaluation**","ac02ac6f":"There are outliers in Loan Amount (maximum value is 700 and Q3 is 162), so the missing value in this column will be filled with median. The remaining columns with missing values will be filled by median value as well. There size are relatively small, it's safe to do so.","14e31ae1":"Credit history and traget","5758116e":"**Skewed features**","2d946078":"Label Encoding dependets that contain information in their ordering set.\n\nConvert 3+ in depedents into 3, and convert the column into numeric feature.","654ba9ce":"It is a linear classifier that makes predictions based on a linear predcition function integrating a set of weights with the feature's vector.\n\nThe model generated confidence score is not high.","1f1ed254":"**Logistic Regression**","4ad8a73e":"Getting the new train and test sets","34529594":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. ","77c0ab2a":"Get summary of target variable. "}}