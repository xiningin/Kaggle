{"cell_type":{"8188c288":"code","f937a0c0":"markdown"},"source":{"8188c288":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot(w_star, w, X, y):\n\n    positive_points = X[y == 1, :]\n    negative_points = X[y == -1, :]\n\n    bias = 0.01\n\n    plt.figure(figsize=(8, 6))\n\n    plt.style.use(\"ggplot\")\n\n    plt.scatter(positive_points[:, 0], positive_points[:, 1], marker=\"o\", color=\"blue\", s=10, label=\"positive class\")\n    plt.scatter(negative_points[:, 0], negative_points[:, 1], marker=\"o\", color=\"green\", s=10, label=\"negative class\")\n\n    # f function on the hyperplane (target line)\n    line_x = np.linspace(-2, 2, 50)\n    line_y = -(bias + w[0] * line_x) \/ w[1]     # bias + w0*x + w1*y = 0\n\n    # g function on the hyperplane (which approximates f - chosen from hypothesis set H)\n    line_x1 = np.linspace(-2, 2, 50)\n    line_y1 = -(get_trained_bias() + w_star[0] * line_x1) \/ w_star[1]\n\n    plt.plot(line_x, line_y, color= \"black\", label=\"f function\")\n    plt.plot(line_x1, line_y1, color= \"red\", label=\"g function\")\n    plt.legend(loc = 1, prop= {'size': 7})\n\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.show()\n    return\n\ndef train_perceptron(X, y):\n\n    w = np.zeros(X.shape[1])\n\n    global iteration\n    iteration = 1\n\n    global b\n    b = float(0)\n\n    while True:\n        results = np.sign(np.dot(X, w) + b)\n        misclassified_indices = np.where(y != results)[0]\n        if len(misclassified_indices) == 0:\n            break\n        picked_misclassified = np.random.choice(misclassified_indices)\n        w += y[picked_misclassified] * X[picked_misclassified]\n        b += y[picked_misclassified]\n        iteration += 1\n    return w\n\n# return number of iteration from train perceptron function\ndef get_iteration():\n    return iteration\n\n# return bias from train perceptron function\ndef get_trained_bias():\n    return b\n\n#  Generate a linearly separable data set of size 20, 100, and 1000.\nif __name__ == \"__main__\":\n\n    range = np.array([20, 100, 1000])\n    for x in range:\n        weight = np.array([0.3,0.2])\n        bias = 0.01\n        input_X = np.random.randn(x,2)\n        output_Y = np.sign(np.dot(input_X,weight)+bias)\n        w_star = train_perceptron(input_X, output_Y)\n        plot(w_star, weight, input_X, output_Y)\n        iteration = get_iteration()\n        print(\"The number of iteration for\", x, \"data point is: \" , str(iteration))","f937a0c0":"**PERCEPTRON**\n\nWhen implementing perceptron learning algorithm, I have created a linearly separable dataset of size 20, 100, 1000. As the data points are linearly separable, the algorithm finally converges after certain number of iterations."}}