{"cell_type":{"632dc0b4":"code","d45ffdb1":"code","2ec53791":"code","046d0216":"code","101f0ef1":"code","dc7dcf09":"code","36d54229":"code","253b4c5b":"code","4226bed5":"code","9b9c9eba":"code","58eeb4e7":"code","084d62f8":"code","b896bd71":"code","e012f5f8":"code","1609fc18":"markdown","facff79f":"markdown","98b85ebb":"markdown","f766e263":"markdown","53342a4c":"markdown","4c76012e":"markdown","910863c2":"markdown","f8f21464":"markdown","241314e8":"markdown","1a3eb7e6":"markdown","74da2adb":"markdown","7d7696c3":"markdown","3f1b7216":"markdown"},"source":{"632dc0b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom pandas_profiling import ProfileReport\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata = pd.read_csv('\/kaggle\/input\/marketing-data\/marketing_data.csv')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d45ffdb1":"data.head(10)","2ec53791":"print(data.shape)\nprint(data.columns)\ndata.rename(columns={' Income ':'Income'},inplace=True)\nprint(data.dtypes)","046d0216":"#Converting Column to Correct\/usable datatypes\ndata['Income'] = (data['Income'].str.replace(r'[^-+\\d.]', '').astype('float').round(2))\ndata['Dt_Customer']=pd.to_datetime(data['Dt_Customer'], dayfirst = True, yearfirst = False)","101f0ef1":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    y=data.isna().sum().reset_index(name='blanks')['blanks'],\n    x=data.columns\n    ))\n#data.isna().sum()\nfig.show()","dc7dcf09":"data[\"Income\"] = data.groupby(\"Education\")[\"Income\"].transform(lambda x: x.fillna(x.median()))","36d54229":"data.describe()","253b4c5b":"print((data['Year_Birth'].value_counts().to_frame().reset_index()).sort_values(by='index').head(5))\ndata = data.loc[data['Year_Birth']>=1940,]","4226bed5":"fig = go.Figure()\nfig.add_trace(go.Box(\ny=data['Income'],\nx=data['Education'],\nname='Income',\n))\nfig.update_layout(\nyaxis_title='Income In USD',\nboxmode='group',\ntitle = 'Income By Educational Qualifications'\n)\nfig.show()","9b9c9eba":"data = data.loc[data['Income']<666666.000000,]\nprint(data.shape)","58eeb4e7":"fig = go.Figure()\nfor i in data.loc[:, data.columns.str.startswith('Mnt')].columns:\n    fig.add_trace(go.Box(\n    y=data[i],\n    name=i,\n    ))\n\nfig.update_layout(\nyaxis_title='Quantity',\nboxmode='group',\ntitle = 'Quantity By Products Bought'\n)\n    \nfig.show()","084d62f8":"import collections\n\n\nBENFORD_PERCENTAGES = [0, 0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046]\n\ndef calculate(data):\n\n    \"\"\"\n    Calculates a set of values from the numeric list\n    input data showing how closely the first digits\n    fit the Benford Distribution.\n    Results are returned as a list of dictionaries.\n    \"\"\"\n\n    results = []\n\n    first_digits = list(map(lambda n: str(n)[0], data))\n    first_digit_frequencies = collections.Counter(first_digits)\n    benford_law = pd.DataFrame(columns=['n','data_frequency','data_frequency_percent','benford_frequency','benford_frequency_percent','difference_frequency','difference_frequency_percent'])\n\n    for n in range(1, 10):\n        benford_law = benford_law.append({'n':n},ignore_index=True)\n        benford_law.loc[benford_law['n']==n,'data_frequency'] = first_digit_frequencies[str(n)]\n        benford_law.loc[benford_law['n']==n,'data_frequency_percent']= (first_digit_frequencies[str(n)]) \/ len(data)\n        benford_law.loc[benford_law['n']==n,'benford_frequency']= len(data) * BENFORD_PERCENTAGES[n]\n        benford_law.loc[benford_law['n']==n,'benford_frequency_percent']=BENFORD_PERCENTAGES[n]\n        benford_law.loc[benford_law['n']==n,'difference_frequency']= first_digit_frequencies[str(n)] - (len(data) * BENFORD_PERCENTAGES[n])\n        benford_law.loc[benford_law['n']==n,'difference_frequency_percent']= (first_digit_frequencies[str(n)]) - (first_digit_frequencies[str(n)] - (len(data) * BENFORD_PERCENTAGES[n]))\n    fig = go.Figure()\n    fig.add_trace(go.Bar(\n    x = benford_law['n'],\n    y = benford_law['data_frequency_percent']*100,\n    name = 'Actual Data'))\n\n    fig.add_trace(go.Bar(\n    x = benford_law['n'],\n    y = benford_law['benford_frequency_percent']*100,\n    name = 'Benford Frequency Percentage'))\n\n    fig.show()\n    return benford_law","b896bd71":"res = calculate(data['MntMeatProducts'])\nres","e012f5f8":"\nfig = make_subplots(rows=1,cols=2)\nfig.add_trace(go.Box(\n    y=data['Recency'],\n    name = 'Recency Boxplot'\n),\nrow=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=data['Recency'].value_counts(),\n    x=data['Recency'].unique(),\n    name= 'Recency Distribution'\n),\nrow=1,col=2)\n\nfig.show()","1609fc18":"We can see huge number of outliers in case of MntMeat Products. We can use [Benford law](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Benford%27s_law) to check if there are any anomalies in the data.","facff79f":"# 1. 2. Are there any outliers Values? How will you wrangle\/handle them?","98b85ebb":"# Year_Birth:\n\nSeems to have few outliers: It's likely impossible to have customers born in 1893, 1899. So we will exclude these customers for further analysis.","f766e263":"# Quantity Of Products Purchased:","53342a4c":"# Income:\n\nMax Income seems too far from the 75th percentile of the income distribution","4c76012e":"Based on above plot its clearly visible that *MntMeatProducts* data follows Benford law and it dosen't have any anomalies","910863c2":"\n1. Are there any null values or outliers? How will you wrangle\/handle them?\n1. Are there any variables that warrant transformations?\n1. Are there any useful variables that you can engineer with the given data?\n1. Do you notice any patterns or anomalies in the data? Can you plot them?\n","f8f21464":"# Recency","241314e8":"# Section 01: *Exploratory Data Analysis*","1a3eb7e6":"1. *Are there any null values or outliers? How will you wrangle\/handle them?*\n\nI will break this question into two sections:\n    1. 1. Are there any null Values? How will you wrangle\/handle them?\n       2. Are there any outliers Values? How will you wrangle\/handle them?\n       \n# 1.Are there any null Values? How will you wrangle\/handle them?","74da2adb":"We found Income column has 24 missing values, as this can prove to be one of the important feature for our analysis we should not ignore the feature. We will impute the missing values based on median income earned by customers based on their Educational background.","7d7696c3":"Based on the above summary of data, we do get a good picture of the features by simply looking at each feature's distribution. Most of the columns have no or very small Outliers we can conclude by comparing min values with 25th percentile and max values with 75th percentile (closer the better). We will still investigate following:\n1. Year_Birth\n1. Income\n1. Recency\n1. All the products (columns starting with Mnt)","3f1b7216":"Using the above plot we can clearly see the outliers, in two Income ranges: \n\n1. Approx. Range $ 150K: Ideally we should not consider these as outliers as people do have such high incomes with Phd, Master and even with Graduation.\n\n\n2. Approx.Range $666K: This data point is actually an outlier as we do not usually see such high income in actual world, also the figure $ 6,66,666 looks fishy. So we will exclude this data point from our further analysis. \n\n\n"}}