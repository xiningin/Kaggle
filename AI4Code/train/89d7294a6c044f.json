{"cell_type":{"171971b5":"code","b09ebfbe":"code","1d3a824f":"code","58895eaa":"code","c283efd6":"code","5aa13112":"code","fa8bd0c2":"code","240cc3ef":"code","134e22d2":"code","a87c9981":"code","655d2072":"code","e6c04faf":"code","5a485416":"code","812eeda9":"code","3d07c646":"code","eb997b5f":"code","fa488943":"code","940152a4":"code","292f71db":"code","8a3caec3":"code","6a5c67cd":"code","7de040ce":"code","e207fd49":"code","5576cc53":"code","8b69faa7":"code","d0f2c60a":"markdown","0e1a5f1f":"markdown","fe5419f0":"markdown","19305fde":"markdown","ee1e4bc8":"markdown","e63c9d89":"markdown"},"source":{"171971b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b09ebfbe":"data=pd.read_csv(\"\/kaggle\/input\/hypothyroid\/hypothyroid.csv\")\ndata.head()","1d3a824f":"data=data.rename(columns={data.columns[0]:\"target\"})","58895eaa":"data[\"target\"].value_counts()","c283efd6":"data[\"target\"]=data[\"target\"].map({\"negative\":0,\"hypothyroid\":1})","5aa13112":"for column in data.columns:\n    howmany=len(set(data[column]))\n    print(column,\": \",howmany)","fa8bd0c2":"data[\"pregnant\"].value_counts()","240cc3ef":"data=data.replace({\"t\":1,\"f\":0})","134e22d2":"data[\"T3_measured\"].value_counts()","a87c9981":"data=data.replace({\"y\":1,\"n\":0})","655d2072":"data","e6c04faf":"data[\"TBG\"].value_counts()","5a485416":"del data[\"TBG\"]","812eeda9":"data","3d07c646":"data=data.replace({\"?\":np.NAN})","eb997b5f":"data.isnull().sum()","fa488943":"data.dtypes","940152a4":"cols = data.columns[data.dtypes.eq('object')]\ndata[cols] = data[cols].apply(pd.to_numeric, errors='coerce')\ndata.dtypes","292f71db":"y=data.iloc[:,0:1]\nx=data.iloc[:,1:]","8a3caec3":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)","6a5c67cd":"from  xgboost import XGBClassifier\nimport sklearn.metrics as metrik\nxgbo = XGBClassifier(learning_rate=0.01)\n\n\nxgbo.fit(x_train,y_train)\nypred=xgbo.predict(x_test)\n\n\nprint(metrik.accuracy_score(y_pred=ypred,y_true=y_test))\nprint(metrik.confusion_matrix(y_pred=ypred,y_true=y_test))","7de040ce":"print(metrik.f1_score(y_pred=ypred,y_true=y_test))","e207fd49":"from catboost import CatBoostClassifier\ncatboost1= CatBoostClassifier(max_depth=3,verbose=0)\ncatboost1.fit(x_train,y_train)\nypred=catboost1.predict(x_test)\nimport sklearn.metrics as metrik\nprint(metrik.confusion_matrix(y_pred=ypred,y_true=y_test))\nprint(metrik.f1_score(y_pred=ypred,y_true=y_test))","5576cc53":"from sklearn.model_selection import cross_val_score\nxgbo = XGBClassifier(learning_rate=0.01)\ncross_val_score(xgbo,x, y, cv=7, scoring='f1')","8b69faa7":"from sklearn.model_selection import cross_val_score\ncatboost1= CatBoostClassifier(max_depth=3,verbose=0)\ncross_val_score(catboost1,x, y, cv=7, scoring='f1')","d0f2c60a":"Catbost and XGBoost work very well in \u0130mbalanced and NaN contains Dataset. \u0130t is suprisingly well.","0e1a5f1f":"F1 score is also very good.","fe5419f0":"## Accuracy is not a good metric for this dataset. Because dataset is imbalanced.","19305fde":"## Okay we can't handle nan values and result is very well. So many times nan value imputation make our model worse, you shouldn't forget it. ","ee1e4bc8":"## Null values is somewhat understandable but hard to handle it. Median or Mean values is option but this may change characteristic of data. So I can't impute them in first place. Because of this I use nan friendly classification tools first.","e63c9d89":"## Too many ? in this column. Drop this column"}}