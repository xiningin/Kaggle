{"cell_type":{"209aa94e":"code","14408ce0":"code","0b4316dd":"code","ac8f15c9":"code","2b3dca66":"code","454ad6b4":"code","33f8ad58":"code","44775c10":"code","17932600":"code","b3b1db0c":"code","3372edfe":"markdown","50e54b3c":"markdown","b73f4410":"markdown","46c0eb47":"markdown","f73c7e95":"markdown","f79a9097":"markdown"},"source":{"209aa94e":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical","14408ce0":"data = \"\"\" Jack and Jill went up the hill\\n\n        To fetch a pail of water\\n\n        Jack fell down and broke his crown\\n\n        And Jill came tumbling after\\n \"\"\"","0b4316dd":"#object of tokenizer class created \ntokenizer=Tokenizer()\n#data fitted by tokenizer\ntokenizer.fit_on_texts([data])\n#encoded_data stores the value of repeated score of the word 1= highly repeated 21= least repeated \nencoded_data=tokenizer.texts_to_sequences([data])[0]\nprint(encoded_data)","ac8f15c9":"vocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","2b3dca66":"word_sequence=[] #list_word_sequence created \nfor i in range(1,len(encoded_data)):\n    sequence=encoded_data[i-1:i+1]        #saving encoder current word and next word in sequence variable\n    word_sequence.append(sequence)\nword_sequence=np.array(word_sequence)   #converting list into array\nprint(word_sequence)","454ad6b4":"x_train,y_train = word_sequence[:,0],word_sequence[:,1]  #slicing coloumn of the array into x_train and y_train\nprint(f\"x_train:{x_train} \\ny_train:{y_train}\")  #print statement","33f8ad58":"#converting categorical data to numerical data using one hot encoding \n#The binary variables are often called \u201cdummy variables\"\ny_train=to_categorical(y_train,num_classes=vocab_size)","44775c10":"model=Sequential()\nmodel.add(Embedding(vocab_size,10,input_length=1))\nmodel.add(LSTM(70))\nmodel.add(Dense(vocab_size,activation=\"softmax\"))\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\nprint(model.summary())","17932600":"model.fit(x_train,y_train,epochs=500,verbose=1)","b3b1db0c":"x_test=\"jack\"\nno_next_word=3\n#in_test variable created to prevent chnages form testing element. can be ignored and changes can be done directly\n#on the x_test\nin_test,next_word=x_test,x_test\nprint(f\"entered word:{x_test}\")\nfor i in range(no_next_word):\n    print(i+1)\n    out_word=\"\"\n    encoded_test=tokenizer.texts_to_sequences([in_test])[0]\n    encoded_test=np.array(encoded_test)\n    #prediction:\n    y_predict=model.predict_classes(encoded_test,verbose=0)\n    for word, index in tokenizer.word_index.items():\n        if y_predict==index:\n            out_word=word\n            break\n    in_test=out_word\n    print(f\"predicted next words:{out_word}\")","3372edfe":"Input dataset here is Jack and Jill poem.we will convert the following poem(that is words) to integer no.\n\nfor that purpose some preprocessing will be applied in the code using Tokenizer class directly from keras.","50e54b3c":"# Neural Network ","b73f4410":"# Fitting the neural Network","46c0eb47":"# Language Model (1 word in - 1 word out)\n\nprogram is a basic language model using very basic dataset=\"jack and jill poem\"\n\n**dataset can be changed**\n\nDIFFICULTY LEVEL :**INTERMEDIATE**\n\nTHE CODE PREDICTS THE NEXT WORD ACCORDING TO ITS TRAINING VIA **JACK AND JILL POEM**\n\nNEURAL LAYERS USED:\n     \n     1.EMBEDDING LAYER\n     \n     2.LSTM LAYER (RNN LAYER)\n     \n     3.DENSE LAYER\n     \n**KERAS IS USED AS A WRAPPER FOR TENSORFLOW**     \n\n**PROBLEMS**\n\n     1.SOMETIMES OUTPUT MAY VARY AS SOME WORDS ARE REPEATED AND THERE NEXT WORD IS DIFFERENT \n     \n\n","f73c7e95":"# Splitting of Data\n\nnow we have generated data in right format and now we can use the data to in our ML model\n","f79a9097":"# Data Formation\n\ncalculate the vocab size\n\ntokenizer.word_size gives dictionary of words corresponidng to the there label no.\n\neg: \"and\" is repeated most in the data and thus it has label 1\n\nnow the list will be created containing current word and next word. this list will be fed into the neural network \n\nand training will be done on the list created\n\nlist format ----> [current word :next word]"}}