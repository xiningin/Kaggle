{"cell_type":{"eeeb1277":"code","590e9290":"code","443ec6ad":"code","597d9436":"code","b7baeabe":"code","d5e4d5a1":"code","fdfedf7a":"code","d2084948":"code","908f08af":"code","fcd03aad":"code","e5fed156":"code","6113d35f":"code","966d7113":"code","69087b57":"code","c5be0c0b":"code","1317de11":"code","14cacdf3":"markdown","b90508f0":"markdown","c166baa6":"markdown","cdfa9807":"markdown","65e50637":"markdown","f67b5543":"markdown","7cae514e":"markdown","dafa2ee5":"markdown","0f2696f7":"markdown","26c5921d":"markdown","24b4302d":"markdown"},"source":{"eeeb1277":"# import libraries\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport joblib\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n","590e9290":"# Load the training data into a DataFrame named 'train'\npd.set_option('max_columns', None)\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n# Print the shape of the resulting DataFrame. \nprint(train.shape)","443ec6ad":"# Display the head of the train DataFrame\ntrain.head()","597d9436":"# Calculate and print the number of number of missing values in each column\ntrain.isna().sum(axis = 0).to_frame().T","b7baeabe":"# Display a DataFrame showing the proportion of Survived column\n(train.Survived.value_counts() \/ len(train)).to_frame()","d5e4d5a1":"# Add a new column named 'FamSize' to the DataFrame (sum of the 'SibSp' and 'Parch') \ntrain['FamSize'] = train['SibSp'] + train['Parch']\n\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n# Use the map() method of the train DataFrame to apply the function above \n# to the 'Cabin' column. Store the results in a new column named 'Deck'. \ntrain['Deck'] = train['Cabin'].map(set_deck)","fdfedf7a":"# Create a list of numberical feature names: 'Age', 'FamSize', 'Fare'\nnum_features = ['Age', 'FamSize', 'Fare']\n# Create a list of categorical feature names: 'Sex', 'Pclass', 'Deck', 'Embarked'\ncat_features = ['Sex', 'Pclass', 'Deck', 'Embarked']\n# Combine the two previous lists into one list named 'features'\nfeatures = num_features + cat_features\n# Create a Pipeline object for processing the numerical features. \n# This pipeline should consist of a SimpleImputer and a StandardScaler\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n# Create a Pipeline object for processing the categorical features. \n# This pipeline should consist of a SimpleImputer and a OneHotEncoder\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n# Create a ColumnTransformer object that combines the two pipelines created above. \n# Name this ColumnTransformer 'preprocessor'\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","d2084948":"# Fit the preprocessor to the training data, selecting only the columns in features \npreprocessor.fit(train[features])\n\n# Apply the fitted preprocessor to the training data, again selecting only features\n# Store the array created in the previous step into a variable named 'X_train'\nX_train = preprocessor.transform(train[features])\n\n# Create a variable named 'y_train' that contains the training labels. \ny_train = train.Survived.values\n# Print the shapes of X_train and y_train.\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)","908f08af":"%%time \n# log reg with various parameters to find best \nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.001,0.01,0.1,1,10,100]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring= 'accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","fcd03aad":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","e5fed156":"%%time\n# decision tree with various parameters\n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16],\n    'min_samples_leaf': [4, 8, 16, 32]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv= 10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","6113d35f":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","966d7113":"%%time \n# random forest with different parameters\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=50)\n\nrf_parameters = {\n    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16],\n    'min_samples_leaf': [4, 8, 16, 32]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","69087b57":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","c5be0c0b":"final_model = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=14, min_samples_leaf=4)\nfinal_model.fit(X_train, y_train)\n\nprint(final_model.score(X_train, y_train))","1317de11":"# Save your pipeline to a file. \njoblib.dump(preprocessor, 'titanic_preprocessor_01.joblib')\n# Determine the best model found above and save that to a file. \njoblib.dump(final_model, 'titanic_model_01.joblib')\nprint('Model written to file.')\n# Download both files to your local device and then upload them as a Kaggle dataset. ","14cacdf3":"## Decision Trees","b90508f0":"# Titanic Dataset","c166baa6":"## Logistic Regression","cdfa9807":"# Save Pipeline and Model","65e50637":"## Random Forests","f67b5543":"# Load Training Data","7cae514e":"# Check Label Distribution","dafa2ee5":"# Preprocessing","0f2696f7":"# Import Statements","26c5921d":"# Check for Missing Values","24b4302d":"# Model Selection"}}