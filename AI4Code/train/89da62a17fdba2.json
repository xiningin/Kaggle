{"cell_type":{"a28d6532":"code","2a34bef6":"code","d31e5bce":"code","fc7bf492":"code","a9fd1976":"code","e0122716":"code","7f71e59e":"code","92381163":"markdown","49241a89":"markdown","5fe67d46":"markdown","c79d40d3":"markdown","df2c9987":"markdown"},"source":{"a28d6532":"#number of bins in the histogram\nslots = 500\n\n#try to model each feature with 8 to 9 normal distributions\n#you can adjust this to different ranges\nallNorms = range(8,10)\n\n#for keeping this kernel short, model only 5 features\/classes\nallClasses = range(0,5)\n\n#for early stopping\npatience = 10\n\nprint('started')\n\n\n\n","2a34bef6":"import numpy as np, pandas as pd, scipy as sp\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.models import Model\nfrom keras.constraints import NonNeg\nfrom keras.callbacks import EarlyStopping\n\ncompetitionFolder = '..\/input\/'\ntrainFile = competitionFolder + 'train.csv'\ntestFile = competitionFolder + 'test.csv'\n\nnClasses = 200 #max 199\nclassNames = ['var_' + str(i) for i in range(nClasses)]\n\ndef loadData(fileName, targets): \n    frame = pd.read_csv(fileName)\n    inputs = frame[classNames].values\n    if targets:\n        targets = frame['target'].values\n        return inputs, targets\n    else:\n        ids = frame['ID_code'].values\n        return inputs, ids","d31e5bce":"trainData, targets = loadData(trainFile, True)\ntestData, ids = loadData(testFile, False)\ntotalData = np.concatenate([trainData, testData], axis=0)","fc7bf492":"class NormLayer(Layer):\n    \n    #n = number of normal distributions to model\n    \n    def __init__(self, n,  **kwargs):\n        self.n = n        \n        super(NormLayer, self).__init__(**kwargs)\n        \n        \n    #this layer contains 3 groups of weights:\n        #- centers (the center of each normal distribution)\n        #- scales (will influence the width of the distribution)\n        #- amps (the amplitude of each distribution)\n    def build(self, inputShape):\n        self.centers = [self.add_weight(name='centers'+str(i), shape=(1,), trainable=True, \n                                        initializer='zeros') \n                        for i in range(self.n)]\n        \n        self.scales = [self.add_weight(name='scales'+str(i), shape=(1,), trainable=True, \n                                       constraint = NonNeg(), initializer='ones') \n                       for i in range(self.n)]\n        \n        self.amps = [self.add_weight(name='amps'+str(i), shape=(1,), trainable=True, \n                                     constraint = NonNeg(), initializer='ones') \n                     for i in range(self.n)]\n\n\n    #gets the amplitudes for remodeling the distribution - for test purposes\n    def getAmps(self):\n        return K.eval(K.concatenate(self.amps))\n\n    \n    #this layer returns two results:\n        #the actual normal distribution - sums all normal components\n        #the indivudial normal distributions without amplitudes - to be used as features\n    def call(self, x):\n        distribution = K.sum(K.stack([a * K.exp(-K.square(s*(x - c))) \n                              for c,s,a in zip(self.centers, self.scales, self.amps)], \n                             axis=0), axis=0)\n        features = K.concatenate([K.exp(-K.square(s*(x - c)))\n                       for c,s,a in zip(self.centers, self.scales, self.amps)],\n                       axis=-1)\n        return [distribution, features]\n                             \n    \n    #keras requirement: the resulting shape of the outputs\n    def compute_output_shape(self,inputShape):\n        return [inputShape, inputShape[:1] + (self.n,)]\n    \n    #here you can set the initial parameters of the distributions\n    def setParams(self, centers, scales, amps):\n        for i in range(self.n):\n            K.set_value(self.centers[i], np.array([centers[i]])) \n            K.set_value(self.scales[i], np.array([scales[i]]))\n            K.set_value(self.amps[i], np.array([amps[i]]))\n\n#builds a keras model simulating n normal components\ndef getNormModel(n):\n    inp = Input((1,))\n    \n    #gets the two layer outputs\n    outDist, outFeat = NormLayer(n, name='normLayer'+str(n))(inp)\n    \n    #builds a model to get the resulting distribution - for training\n    modelDist = Model(inp,outDist,name='modelDist'+str(n))\n    modelDist.compile(optimizer='adam', loss='mse')\n    \n    #builds a model to get the resulting features - for further use\n    modelFeat = Model(inp,outFeat,name='modelFeat'+str(n))\n\n    return modelDist,modelFeat\n    \n#one model for each number of desired normal distributions\nmodels = {n:getNormModel(n) for n in allNorms}  ","a9fd1976":"def getHistogramNeg(dataAllClasses):\n    results = list()\n    for var in range(nClasses):\n        data = dataAllClasses[:,var]\n        \n        hist, lims = np.histogram(data, bins=slots, density=True)\n        x = (lims[:-1] + lims[1:]) \/ 2. \n        results.append((x, hist))\n    return results\n\n   \nhistogramData = getHistogramNeg(totalData)\nprint('histogram data len', len(histogramData))\n","e0122716":"def fitNormals(xAndHists):\n    #params to save if desired\n    allParams = list()\n    \n    #for each class to model\n    for i in allClasses:\n        x,hist = xAndHists[i] #get respective histogram data\n            \n        #for deciding the best number of normals\n        bestLoss = 1000000\n        bestNorms = 0\n        \n        \n        #for each number of normal distributions\n        for norms in allNorms:\n            modelDist, modelFeat = models[norms]\n        \n            #initial params for the distributions\n            xMin, xMax = x.min(), x.max()\n            xMean = (xMin + xMax) \/ 2.\n            divSize = (xMax - xMin) \/ (norms+1)\n\n            histMax = hist.max() \n            scale = 5 \/ (xMax - xMin)\n\n            centers = np.array([xMin +(divSize * i) for i in range(1,norms+1)])\n            scaleFactors = np.array([(1 + (abs(xMean - c) \/ divSize)) for c in centers])\n            scales = np.array([f*scale for f in scaleFactors])\n            amps = np.array([histMax\/(f**2) for f in scaleFactors])\n\n            #set initial params in the model\n            modelDist.get_layer('normLayer'+str(norms)).setParams(centers, scales, amps)\n            \n            \n            #test code to see the distribution for the initial parameters\n            \n#             outputs = modelDist.predict_on_batch(x)\n\n#             fig,ax = plt.subplots(ncols=2, figsize=(20,5))\n#             fig.suptitle(\n#                 'initial and final curve for class '+str(i)+\" - norms = \"+str(norms)\n#             )\n#             ax[0].plot(x,hist)\n#             ax[0].plot(x,outputs)\n\n\n            #fitting the model with histogram data\n            print('fitting class', i, ' - norms:', norms)\n            callbacks = [EarlyStopping(monitor='loss', mode='min',\n                                       min_delta=0,patience=patience)]\n            history = modelDist.fit(x, hist, batch_size=slots+1,epochs = 1000000, \n                                    callbacks = callbacks, verbose=0)\n            loss = history.history['loss'][-patience-1]\n            print('loss:', history.history['loss'][-1])\n            \n            \n            #test code to see the final distribution with histogram data\n#             ax[1].plot(x,hist)\n#             ax[1].plot(x,modelDist.predict(x))\n#             plt.show()\n            \n            #updates which is the best number of distributions\n            if loss < bestLoss:\n                bestLoss = loss\n                bestN = norms\n        \n        \n        #uses the best model for getting params and seeing results\n        print('best norms = ', bestN)\n        modelDist, modelFeat = models[bestN]\n        #modelDist.save_weights('class'+str(i)+\"_norms\" + str(bestN))\n        params = modelDist.get_weights()\n        allParams.append((x,params)) \n        \n        #for test purposes, predicts with histogram data\n        testFeats = modelFeat.predict_on_batch(x)\n        amps = modelDist.get_layer('normLayer'+str(bestN)).getAmps()\n            #applies the amplitudes to the features and sum\n        testDist = (amps.reshape((1,-1))*testFeats).sum(axis=-1) \n        \n        #gets model outputs using the original data\n        realData = totalData[:,i] #data for a single feature\n        realDist = modelDist.predict_on_batch(realData)\n        realFeats = modelFeat.predict_on_batch(realData)\n        np.save('trainFeats' + str(i) + '.npy', realFeats[:200000])\n        np.save('testFeats' + str(i) + '.npy', realFeats[200000:])\n        \n        \n        #visualizing\n        print('testFeats shape', testFeats.shape)\n\n        fig,ax = plt.subplots(ncols=2,figsize=(20,5))\n        plt.suptitle('feature ' + str(i) + ' - norms: ' + str(bestN))\n        \n        #plot original histogram data for comparison\n        ax[0].plot(x, hist) \n        \n        #plot each feature (individual normal distributions - no amps)\n        for p in range(bestN):\n            ax[0].plot(x, testFeats[:,p])\n        ax[1].plot(x, hist) #plots histogram data for comparison\n        ax[1].plot(x, testDist) #plots test distribution for comparison \n        \n        #sorts the real data and outputs to generate a distribution\n        sortedArgs = np.argsort(realData)\n        ax[1].plot(realData[sortedArgs], realDist[sortedArgs])\n        plt.show()\n    \n    return allParams","7f71e59e":"xAndParams = fitNormals(histogramData)        ","92381163":"### Fitting the distributions and saving features","49241a89":"# Results\n\nUsing the generated numpy files as input features in a model can result (so far) in a regular CV score arond 0.89+   \nNot sure if selecting the right features from these can bring better results.\n\nThis method doesn't quite model the extreme bumps like the ones in var_12 and var_108, but maybe increasing the number of normal distributions?\n\nFinding the right number of distributions could be an interesting question here. Trying from 3 to 16, most features get their best results between 9 and 16 distributions. I'm afraid using too many subdistributions would simply \"overfit\" the distribution without creating any useful feature.\n\n# Open to ideas :)\n\nIf you know any easier or better optimized way of doing this, especially one that allows finding the best\/minimum number of distributions, please let me know.\n\nIf this helped you, or gave you new ideas, please comment.\nAnd remember I'm in search of a team - not necessarily the campions - to help me find a little magic.","5fe67d46":"### Histogram generating","c79d40d3":"# Introduction\n\nIn search of weirdness, magic and sorcery, this kernel tries some sort of feature engineering based on the supposition that each feature is composed of many features summed with different scales, offsets, etc. \n\nHere, we assume the data is a composition of many normal distributions and try to create features from this.\n\nThis seems a good option either for modeling bumps in features as well as modeling their tendency to form a few hills. \n\n### Config and load data:","df2c9987":"### Keras layer and model to represent a sum of normal distributions"}}