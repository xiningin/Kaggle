{"cell_type":{"3d49824b":"code","f49ca042":"code","49d557d6":"code","647767e7":"code","f4aa51aa":"code","2234d0a2":"code","69f28ca4":"code","9b75d6ae":"code","b82dc15d":"code","7a59f6bf":"code","3910f546":"code","0288dc51":"code","58284258":"code","42fd1eb5":"code","7fb3709c":"code","75e577a9":"code","2479d2f9":"code","dfbe1dc7":"code","2023596d":"code","8f55faf3":"code","24fd400e":"code","3fb8d5b6":"code","96920eea":"code","45c35344":"code","ad4592db":"code","80e19032":"code","8cb6bd3d":"code","c9381974":"code","2392b7b5":"code","fd850669":"code","aa32359c":"code","4003e2e2":"code","e44f8f4a":"code","546bff9e":"code","3f2dfdc2":"code","6e6663d9":"code","c81863c7":"markdown","c5da42d8":"markdown","8904c744":"markdown","002107c8":"markdown","e52b8777":"markdown","51f88554":"markdown","55acba62":"markdown","c9ac41f3":"markdown","0a98172d":"markdown","14be8e52":"markdown","dce06fac":"markdown","3eb94a30":"markdown","e52ac909":"markdown","377678ee":"markdown","4049f695":"markdown","5bd6f946":"markdown","54825d97":"markdown"},"source":{"3d49824b":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nimport tensorflow_datasets as tfds\nimport keras\nfrom keras import layers\nfrom keras.preprocessing import image\ntfkl = tfk.layers","f49ca042":"path = \"\/kaggle\/input\/fashion-product-images-dataset\/fashion-dataset\/fashion-dataset\/\"\nprint(os.listdir(path))","49d557d6":"df = pd.read_csv(path + \"styles.csv\", nrows=5000, error_bad_lines=False)\ndf['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\ndf = df.reset_index(drop=True)\ndf.head(10)","647767e7":"df.shape","f4aa51aa":"test_id = 15970","2234d0a2":"def retrieval(df,image_id):\n    selected_rows = df.loc[df['image'] == str(image_id)+'.jpg']\n    sub_cat = selected_rows['subCategory'].iloc[0]\n    gender = selected_rows['gender'].iloc[0]\n    retr_dt = df.loc[(df['subCategory']==sub_cat) & (df['gender']==gender)]#retrived dataset\n    return retr_dt","69f28ca4":"retr_dt = retrieval(df,test_id)\nretr_dt.head(5)","9b75d6ae":"def read_image(image_id):\n    img = str(image_id)+'.jpg'\n    img = cv2.imread(path+\"images\/\"+str(img))\n    #print(img.shape)\n    if img.shape != (2400,1800,3):\n        img = image.load_img(path+\"images\/\"+str(image_id)+'.jpg', target_size=(2400,1800,3))\n        img = image.img_to_array(img)\n    return img","b82dc15d":"def plot_image(image_id):\n    img = str(image_id)+'.jpg'\n    img = cv2.imread(path+\"images\/\"+str(img))\n    # If directly use cv2.imshow(img)m, the color is in wrong order\n    b,g,r = cv2.split(img)\n    frame_rgb = cv2.merge((r,g,b))\n    plt.imshow(frame_rgb)","7a59f6bf":"plot_image(test_id)","3910f546":"#from imageai.Detection import ObjectDetection","0288dc51":"def with_without_model(test_id):\n    execution_path = \"\/kaggle\/input\/fashion-product-images-dataset\/fashion-dataset\/images\/\"\n    detector = ObjectDetection()\n    detector.setModelTypeAsRetinaNet()\n    detector.setModelPath(\"\/kaggle\/input\/imageai\/resnet50_coco_best_v2.0.1.h5\")\n    detector.loadModel()\n    \n    detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path,str(test_id)+\".jpg\"), output_image_path=os.path.join(os.getcwd(),str(test_id)+\".jpg\"))\n    \n    for eachObject in detections:\n        if eachObject[\"name\"]=='person' and eachObject[\"percentage_probability\"]>50:\n            return 1\n        else:\n            continue\n    return 0","58284258":"#with_without_model(test_id)","42fd1eb5":"from keras.applications.resnet50 import ResNet50\n\ntfkl = tfk.layers","7fb3709c":"#remember the input_shape set for this model is in_shape, which is a tuple, so the image should be resized\ndef build_model(in_shape,high_d=True):\n    #build model for embedding\n    resnet_base = ResNet50(weights='imagenet', \n                      include_top=False, \n                      input_shape = in_shape)\n    resnet_base.trainable = False\n    \n    model = tfk.Sequential()\n    model.add(resnet_base)\n    if high_d==True:\n        model.add(tfkl.GlobalMaxPooling2D()) #add layer embedding\n    else:\n        model.add(tfkl.Dense(100, activation=tf.nn.relu))\n    \n    print(model.summary())\n    return model","75e577a9":"in_shape = [2400,1800,3]\nmodel = build_model(tuple(in_shape))","2479d2f9":"img = read_image(test_id)\nemb = model.predict(img.reshape(tuple([1]+in_shape))) #(1, 2400, 1800, 3)\n# emb\n## convert the shape (1,2048) to (2048,)\nemb = emb.reshape(-1)\n\nemb.shape\nemb","dfbe1dc7":"def get_embedding(mod, image_name, in_shape):\n    # Reshape and load image\n    img = image.load_img(path+\"images\/\"+str(image_name), target_size=in_shape)\n    img = image.img_to_array(img)\n    ## img = cv2.imread(path+\"images\/\"+str(image_name))\n    return mod.predict(img.reshape(tuple([1]+in_shape))).reshape(-1)","2023596d":"numRows = df.shape[0]\nnumCols = 2048 #representing dimensions for embedding, see the output dim of model\nemb_matrix = pd.DataFrame(index=range(numRows),columns=range(numCols))","8f55faf3":"%%time\n# Compute every image's embedding in df, and attach it as a column\nfor r in range(0,df.shape[0]):\n    im = df['image'][r]\n    emb = get_embedding(model,im,in_shape)\n    emb_matrix.iloc[r,:]=emb","24fd400e":"emb_matrix.head(5)","3fb8d5b6":"#store emb_matrix\uff0cinstead as a ram\nemb_matrix.to_csv(\"emb_matrix.csv\",index=False)","96920eea":"emb_store = pd.concat([emb_matrix, df[[\"image\",\"id\"]]],axis=1,ignore_index=False)","45c35344":"emb_store.to_csv(\"emb_store.csv\",index=True)","ad4592db":"#retr_dt.index\ndt = emb_store.loc[retr_dt.index,]\ndt.head(5)","80e19032":"def compute_similarity(dt,test_id):\n    dt.index = dt[\"id\"].apply(str)\n    dt[\"sim\"] = np.nan\n    try:\n        dt = dt.drop([\"image\",\"id\"],axis=1)\n    except:\n        dt = dt\n    target_vec = dt.loc[dt.index==str(test_id)]\n    target_vec = list(target_vec.iloc[0,0:2048])\n    #again, 2048 represents dimensions for embedding, see the output dim of model\n    \n    from scipy import spatial\n    for i in dt.index:\n        vec = dt.loc[dt.index==i, dt.columns!=\"sim\"]\n        vec = list(vec.iloc[0,:])\n        cosine_similarity = 1 - spatial.distance.cosine(target_vec, vec)\n        dt.loc[dt.index==i,\"sim\"] = round(cosine_similarity,3)\n    \n    sort_dt = dt.sort_values('sim',ascending=False)\n    \n    return sort_dt","8cb6bd3d":"sorted_dat = compute_similarity(dt, test_id)\nsorted_dat.head(5)","c9381974":"#plot the top 10 recommendations for test_id\nfig=plt.figure(figsize=(10, 10))\ncolumns = 5\nrows = 2\ni = 1\nfor img in sorted_dat.iloc[0:10,i].index:\n    im = cv2.imread(path+\"images\/\"+str(img)+\".jpg\")\n    b,g,r = cv2.split(im)\n    frame_rgb = cv2.merge((r,g,b))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(frame_rgb)\n    i+=1\nplt.show()","2392b7b5":"test_id_2 = 58183","fd850669":"retr_dt_2 = retrieval(df,test_id_2)\nretr_dt_2.head(5)","aa32359c":"plot_image(test_id_2)","4003e2e2":"in_shape_2 = [2400,1800,3]\nmodel_2 = build_model(tuple(in_shape_2))","e44f8f4a":"img_2 = read_image(test_id_2)\nemb_2 = model_2.predict(img_2.reshape(tuple([1]+in_shape_2))) #(1, 2400, 1800, 3)\n# emb\n## convert the shape (1,2048) to (2048,)\nemb_2 = emb_2.reshape(-1)\n\nemb_2.shape\nemb_2","546bff9e":"dt_2 = emb_store.loc[retr_dt_2.index,]\ndt_2.head(5)","3f2dfdc2":"sorted_dat_2 = compute_similarity(dt_2, test_id_2)\nsorted_dat_2.head(10)","6e6663d9":"#plot the top 10 recommendations for test_id\nfig=plt.figure(figsize=(10, 10))\ncolumns = 5\nrows = 2\ni = 1\nfor img in sorted_dat_2.iloc[0:10,i].index:\n    im = cv2.imread(path+\"images\/\"+str(img)+\".jpg\")\n    b,g,r = cv2.split(im)\n    frame_rgb = cv2.merge((r,g,b))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(frame_rgb)\n    i+=1\nplt.show()","c81863c7":"### read in images and make plot","c5da42d8":"Here I directly use the product of the same subcategory, but in our real case","8904c744":"**Add-up note, but not necessary:**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise_distances.html\nfrom sklearn.metrics.pairwise import pairwise_distances\n\nCalculate distance Matrix\ncosine_sim = 1-pairwise_distances(dt.loc[:,dt.columns!=\"sim\"], metric='cosine')\ncosine_sim[:4, :4]","002107c8":"## Step1. Retrieval","e52b8777":"## Step2. Ranking based on similarity of product images","51f88554":"### **We will use the product image 15970.jpg as for example**","55acba62":"Unfortunately, the pre-trained model Resnet used the COCO dataset, which is generally for custom items (e.g person, car, etc), not specifically for clothes image segmentation, and we lack the training dataset to specify the part of clothes\/products. \n\nTherefore, the rough solution here is: use ResNet to identify if there's a person. If there is a model, we would recommend similar products which are always with models, and select some product images without models for add-up.","c9ac41f3":"model.compile(\n    optimizer=tfk.optimizers.RMSprop(),\n    loss=tfk.losses.CategoricalCrossentropy(),\n    metrics=[\"acc\"]\n)\n\nresults = model.fit(ds_train, batch_size=32, steps_per_epoch=30, epochs=20,verbose=1)","0a98172d":"There's a dataset ***model_dat*** which is generated from the process of mapping the function ***with_without_model*** to every image.","14be8e52":"### compute similarity for all retrieved images","dce06fac":"## Try another test_id","3eb94a30":"we have to reshape the images, because not every image is in the same shape","e52ac909":"**image-processing difficulties & Future Work:**\n\n(1) semantic segmentation *(eliminate the background\/models, only keep products)*\n\n(2) resize picture matricies\n\n(3) cope with image distortion \/ different filming angles\n\n(3) compute similarities","377678ee":"#### instance segmentation\nSome pictures are with models while others are not, and perhaps some photos does not have a clear background. Therefore, it's important to apply instance segmentation for identifying the part of product image.","4049f695":"Attach a column *embedding* to store image embedding for every photo. ","5bd6f946":"### image embedding","54825d97":"The output embedding for every image is 2048, is there a curse of dimensionality?\n\nNo problem arises, and 2048 dimension works better than 100 dimension."}}