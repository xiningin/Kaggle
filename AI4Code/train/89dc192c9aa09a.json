{"cell_type":{"8ad2d810":"code","604a7fa8":"code","05d51cdd":"code","d0d36d80":"code","acc8887e":"code","281d2ea6":"code","f1d57ce6":"code","400c568a":"code","21b1d985":"code","1e8eb496":"code","c4da861a":"code","3b7a4e38":"code","f405f99a":"markdown","faf341c6":"markdown","3e63a972":"markdown","eb1dd443":"markdown","a90da18c":"markdown","45262e93":"markdown","bceae18c":"markdown","b1219e16":"markdown","4fef0c33":"markdown","bfd0d457":"markdown","738811c5":"markdown","279c26a8":"markdown"},"source":{"8ad2d810":"from keras.models import Sequential, Model\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, Input, GRUCell, RNN\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom matplotlib import pyplot as plt\nfrom numpy import array\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.callbacks import CSVLogger","604a7fa8":"window_size =  40\ndf = pd.read_csv(\"..\/input\/exrate2.csv\")\ndf_norm = df.drop(['Date'], 1, inplace=True)\ndf_norm = df.drop(['Buy'], 1, inplace=True)\n\ndf = df.values[:]\ndf = np.array(df).astype(np.float32)\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf = scaler.fit_transform(df)\n\ndf_train= df[:int(len(df)*0.8)]\ndf_val= df[int(len(df)*0.8):]\n#%%\n\nX = []\nX_dec = []\ny = []\n\nfor i in range(len(df_train) - window_size * 2):\n    X.append(df_train[i:i + window_size])\n    y.append(df_train[i + window_size:i + window_size * 2])\n    #for dec_input=dec_output-1 teacher force\n    temp = np.insert(df_train[i + window_size:i + (window_size * 2)-1],0,\n                        #0)\n                        df_train[i + window_size - 1:i + window_size])\n    \n    #for dec_input = zero\n    #temp = np.zeros((window_size, 1))\n\n    #for dec_input=enc_input-1\n    #temp = np.insert(df_train[i:i + window_size-1],0,0)\n    \n    X_dec.append(temp)   \n\nvalX = []\nvalX_dec = []\nvalY = []\nfor i in range(len(df_val) - window_size * 2):\n    valX.append(df_val[i:i + window_size])\n    valY.append(df_val[i + window_size:i + window_size * 2])\n    #for dec_input=dec_output-1\n    temp = np.insert(df_val[i + window_size:i + (window_size * 2)-1],0,\n                        #0)\n                        df_val[i + window_size - 1:i + window_size])\n    \n    #for force teaching\n    #temp = np.zeros((window_size, 1))\n\n    #for dec_input=enc_input-1\n    #temp = np.insert(df_val[i:i + window_size-1],0,0)\n    \n    valX_dec.append(temp)\n\n#%%\nX = np.array(X).astype(np.float32)\nX_dec = np.array(X_dec).astype(np.float32)\n#X = X.reshape(X.shape[0], 40, 1)\nvalX = np.array(valX).astype(np.float32)\nvalX_dec = np.array(valX_dec).astype(np.float32)\n#valX = valX.reshape(valX.shape[0], 40, 1)\ny = np.array(y).astype(np.float32)\nvalY = np.array(valY).astype(np.float32)\n\nX = [X.reshape((X.shape[0],X.shape[1],1)), X_dec.reshape((X_dec.shape[0],X_dec.shape[1],1))]\ny = y.reshape((y.shape[0],y.shape[1],1))\nvalX = [valX.reshape((valX.shape[0],valX.shape[1],1)), valX_dec.reshape((valX_dec.shape[0],valX_dec.shape[1],1))]\nvalY = valY.reshape((valY.shape[0],valY.shape[1]))\n","05d51cdd":"layers = [30,30]\ndropout = 0.2\nnum_input_features = 1 \nnum_output_features = 1 \n\nregulariser = None\ninput_sequence_length = 40 \ntarget_sequence_length = 40 \nnum_steps_to_predict = 40 \n\n\nencoder_inputs = Input(shape=(None, num_input_features), )\n\nencoder_cells = []\nfor hidden_neurons in layers:\n    encoder_cells.append(GRUCell(hidden_neurons, dropout=dropout,\n                                              kernel_regularizer=regulariser,\n                                              recurrent_regularizer=regulariser,\n                                              bias_regularizer=regulariser))\n\nencoder = RNN(encoder_cells, return_state=True)\n\nencoder_outputs_and_states = encoder(encoder_inputs)\n\nencoder_states = encoder_outputs_and_states[1:]\n\ndecoder_inputs = Input(shape=(None, 1))\n\ndecoder_cells = []\nfor hidden_neurons in layers:\n    decoder_cells.append(GRUCell(hidden_neurons,\n                                              kernel_regularizer=regulariser,\n                                              recurrent_regularizer=regulariser,\n                                              bias_regularizer=regulariser))\n\ndecoder = RNN(decoder_cells, return_sequences=True, return_state=True)\n\ndecoder_outputs_and_states = decoder(decoder_inputs, initial_state=encoder_states)\n\ndecoder_outputs = decoder_outputs_and_states[0]\n\ndecoder_dense = Dense(num_output_features,\n                                   activation='linear',\n                                   kernel_regularizer=regulariser,\n                                   bias_regularizer=regulariser)\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n\nmodel = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n\nmodel.summary()\nplot_model(model, to_file='encdec.png', show_shapes=True, show_layer_names=True)\n\n","d0d36d80":"batch_size = 128\nepochs = 1000\nlearning_rate = 0.0001 #0.00001 adam\nsteps_per_epoch = None\ndecay = 0 # Learning rate decay\n#optimiser = Adam(lr=learning_rate, decay=decay, amsgrad = False)\noptimiser = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n\n# compile model\nmodel.compile(loss='mse', optimizer=optimiser)\n\nimport datetime, time\nimport os\nimport sys    \nfile_name =  os.path.splitext(os.path.basename(sys.argv[0]))[0]\nawal = datetime.datetime.now()\npath = \"{}_{}\".format(file_name,awal.timestamp())\nos.makedirs(\"{}\".format(path))\nfilepath=\"{}\/weights.best.hdf5\".format(path)\ncsv_logger = CSVLogger('{}\/training.log'.format(path))\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nmonitor_earlydropping = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, verbose=1, mode='auto')\ncallbacks_list = [checkpoint, csv_logger , monitor_earlydropping]\n\n# history = model.fit(X,y , epochs=epochs, \n#  \t\t\tvalidation_data=(valX,valY ), batch_size=batch_size, shuffle=True, callbacks=callbacks_list)\nhistory = model.fit(X,y , epochs=epochs, steps_per_epoch=steps_per_epoch, batch_size=batch_size,\n\t\t\tvalidation_split=0.2, shuffle=False, callbacks=callbacks_list)","acc8887e":"# plot train and validation loss\nplt.figure(figsize=(12, 5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model train vs validation error \\nlyr\/nr={},lr={}, bs={}, opt={}\".format(\n            layers, learning_rate, batch_size, optimiser))\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\n#plt.show()\nplt.savefig('{}\/error.png'.format(path))","281d2ea6":"\n(x_encoder_test, x_decoder_test), y_test = valX, valY # x_decoder_test is composed of zeros.\ny_test_predicted = model.predict([x_encoder_test, x_decoder_test])\npredicted = scaler.inverse_transform(y_test_predicted.reshape((y_test_predicted.shape[0],\n                                        y_test_predicted.shape[1])))\nx_test = x_encoder_test.reshape((x_encoder_test.shape[0],x_encoder_test.shape[1]))\nx_test = scaler.inverse_transform(x_test)\ny_test = scaler.inverse_transform(y_test)\n\n# Select 10 random examples to plot\nindices = np.random.choice(range(x_test.shape[0]), replace=False, size=10)\n\n\nfor index in indices:\n    plt.figure(figsize=(12, 5))\n\n    past = x_test[index,:]\n    true = y_test[index,:]\n    pred = predicted[index,:]\n    \n    label1 = \"Seen (past) values\" \n    label2 = \"True future values\"\n    label3 = \"Predictions\"\n\n    plt.plot(range(len(past)), past, \"o--b\",\n                label=label1)\n    plt.plot(range(len(past),\n                len(true)+len(past)), true, \"x--b\", label=label2)\n    plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\",\n                label=label3)\n    plt.legend(loc='best')\n    plt.title(\"Predictions v.s. true values\")\n    plt.show()\n    #plt.show()\n    plt.savefig('{}\/graphpredict{}.png'.format(path,index))\n","f1d57ce6":"#INFERENCE ENCODER AND DECODER\nencoder_predict_model = Model(encoder_inputs,\n                                           encoder_states)\n\ndecoder_states_inputs = []\n\nfor hidden_neurons in layers[::-1]:\n    decoder_states_inputs.append(Input(shape=(hidden_neurons,)))\n\ndecoder_outputs_and_states = decoder(\n    decoder_inputs, initial_state=decoder_states_inputs)\n\ndecoder_outputs = decoder_outputs_and_states[0]\ndecoder_states = decoder_outputs_and_states[1:]\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_predict_model = Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n#model.summary()\n#plot_model(model, to_file='encdec.png', show_shapes=True, show_layer_names=True)","400c568a":"def predict(x, x_dec, encoder_predict_model, decoder_predict_model, num_steps_to_predict):\n    \n    y_predicted = []\n    y2_predicted = []\n\n    states = encoder_predict_model.predict(x)\n\n    if not isinstance(states, list):\n        states = [states]\n\n    #decoder_input = np.zeros((x.shape[0], 1))\n    #decoder_input = output = np.zeros((x.shape[0],1,1))\n    #decoder_input = x_dec[:,:2,0].shape(len(x_dec),1,1)\n    target_ = x_dec.shape[1]\n    x_dec =  np.concatenate((x_dec,\n            np.zeros((x_dec.shape[0],(num_steps_to_predict-x_dec.shape[1]),1))), axis=1)\n    \n    for i in range(num_steps_to_predict):\n        decoder_input = np.array(x_dec[:,i:i+1,0]).reshape(len(x_dec),1,1)  \n        outputs_and_states = decoder_predict_model.predict(\n       [decoder_input] + states , batch_size=batch_size)\n        output = outputs_and_states[0]\n        states = outputs_and_states[1:]\n        \n        if ((i>=(num_steps_to_predict-target_-1)) and (i<=num_steps_to_predict-2) \n                and (num_steps_to_predict>40) and (decoder_input != np.zeros((len(x_dec), 1, 1))).all()):\n\n            for j in range(num_steps_to_predict-1-i):\n                x_dec[:,i+1+j,0]=output[:,0,0]\n\n        y_predicted.append(output)\n\n    return np.concatenate(y_predicted, axis=1)\n\ndef plot_prediction(x, y_true, y_pred, graphnum, isTest, path):\n    \n    plt.figure(figsize=(12, 5))\n\n    past = x\n    true = y_true\n    pred = y_pred\n\n    label1 = \"Seen (past) values\" \n    label2 = \"True future values\"\n    label3 = \"Predictions\"\n\n    plt.plot(range(len(past)), past, \"o--b\",\n                label=label1)\n    plt.plot(range(len(past),\n                len(true)+len(past)), true, \"x--b\", label=label2)\n    plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\",\n                label=label3)\n    plt.legend(loc='best')\n    plt.title(\"Predictions v.s. true values\")\n    #plt.show()\n\n    if isTest:\n        plt.savefig('{}\/graphTest{}.png'.format(path,graphnum))\n    else:\n        plt.savefig('{}\/graphTrain{}.png'.format(path,graphnum))\n","21b1d985":"\n(x_test, x_dec_test), y_test = valX, valY\n\ny_test_predicted = predict(x_test, x_dec_test, encoder_predict_model, \n                                decoder_predict_model, num_steps_to_predict)\n\npredicted_val = scaler.inverse_transform(y_test_predicted.reshape((y_test_predicted.shape[0],\n                                        y_test_predicted.shape[1])))\nx_test = x_test.reshape((x_test.shape[0],x_test.shape[1]))\nx_test = scaler.inverse_transform(x_test)\ny_test = scaler.inverse_transform(y_test)\n\nindices = np.random.choice(range(x_test.shape[0]), replace=False, size=10)\n\nallpred = list()#store predictions across random model initializations\nfor index in indices:\n    plot_prediction(x_test[index, :], y_test[index, :], predicted_val[index, :], \n                    index, isTest=True, path=path)\n    allpred.append(np.ndarray.tolist(predicted_val[index, :]))","1e8eb496":"(x_train, x_dec_test), y_train = X,y\n\ny_train_predicted = predict(x_train, x_dec_test, encoder_predict_model, decoder_predict_model, num_steps_to_predict)\n\npredicted = scaler.inverse_transform(y_train_predicted.reshape((y_train_predicted.shape[0],\n                                y_train_predicted.shape[1])))\nx_train = x_train.reshape((x_train.shape[0],x_train.shape[1]))\nx_train = scaler.inverse_transform(x_train)\ny_train = y_train.reshape((y_train.shape[0],y_train.shape[1]))\ny_train = scaler.inverse_transform(y_train)\n\nindices = np.random.choice(range(x_train.shape[0]), replace=False, size=10)\n\nallpred2 = list()#store predictions across random model initializations\nfor index in indices:\n    plot_prediction(x_train[index, :], y_train[index, :], predicted[index, :], \n                        index, isTest=False,path=path)\n    allpred2.append(np.ndarray.tolist(predicted[index, :]))\n","c4da861a":"#####ARIMA START######\nfrom statsmodels.tsa.arima_model import ARIMA\n\ntrain = df_train \ntest = df_val \n\nmodel_arima = ARIMA(train, order=(0,1,2)) # ARIMA with grid searched parameters\nmodel_fit = model_arima.fit(disp=0)\noutput = model_fit.forecast(steps=len(test))\nyhat = output[0]\n    \ntest = test.reshape((test.shape[0],test.shape[1]))\ntest = scaler.inverse_transform(test)\nyhat = scaler.inverse_transform(yhat.reshape(-1, 1))\n\n# plot\nplt.figure(figsize=(12, 6))\nlabel1 = \"True values\" \nlabel2 = \"ARIMA\" \nplt.plot(test[len(test)-window_size:], label=label1)\nplt.plot(yhat[len(yhat)-window_size:], color='red', label=label2)\nplt.plot(predicted_val[len(predicted_val)-1,:], color='gold', label=label3)\n\nplt.ylabel('IDR')\nplt.xlabel('Days')\nplt.legend(loc='best')\nplt.title(\"Model Comparison of IDR prediction over %d days\" % window_size)\n#plt.show()\n\nplt.savefig('{}\/graph4.png'.format(path))","3b7a4e38":"inderror = (test[len(test)-window_size:] - yhat[len(yhat)-window_size:])**2\ncummulativesumar = np.cumsum(inderror)\ninderror = (test[len(test)-window_size:] - predicted_val[predicted_val.shape[0]-1,:].reshape(-1, 1))**2\ncummulativesumalldl = np.cumsum(inderror)\n\nplt.figure(figsize=(12, 6))\n\nlabel1 = \"Seq2Seq model \" \nlabel2 = \"ARIMA model\" \n\nplt.plot(cummulativesumalldl,color='gold', label=label1)\nplt.plot(cummulativesumar, color='red', label=label2)\nplt.legend(loc='best')\nplt.ylabel('Error')\nplt.xlabel('Days')\nplt.title(\"Model Comparison of RMSE over %d days\" % window_size)\nplt.show()\nplt.savefig('{}\/graph5.png'.format(path))\n\n\n","f405f99a":"## **Predict and plot train data**","faf341c6":"## **Training the model**","3e63a972":"## **Data preparation**\n*  Data yang dihimpun adalah data jual per hari dengan cakupan waktu dari tanggal 24 Januari 2001 hingga 17 Oktober 2018 sebanyak 4344 data. \n* Training data 80% dan Test data 20%","eb1dd443":"## **Predict and plot test data**","a90da18c":"## **Compare RMSE of both models**","45262e93":"## **Compare the model over ARIMA model**","bceae18c":"## **Plot prediction**","b1219e16":"## **Plot training and validation loss**","4fef0c33":"## **Inference the model**","bfd0d457":"## **Create seq2seq model**\n\n[<a href=\"https:\/\/imgur.com\/IOj44HZ\"><img src=\"https:\/\/i.imgur.com\/IOj44HZ.png\" title=\"source: imgur.com\" \/><\/a>](https:\/\/i.imgur.com\/08FJpK5.png)","738811c5":"## **Predict and Plot modules**","279c26a8":"# Sequence to sequence (encoder decoder) model for exchange rates prediction using Keras\n\n\nLet's start with importing some libraries"}}