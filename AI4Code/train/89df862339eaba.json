{"cell_type":{"2250aa8f":"code","115a8957":"code","3b6d9183":"code","e82ae546":"code","d15f3f81":"code","bf7322bf":"code","a76eb7bc":"markdown","031b8519":"markdown","b3b90fd2":"markdown","eb47b5fd":"markdown","40264492":"markdown","86e96e84":"markdown","b5279e2f":"markdown","8c84a297":"markdown"},"source":{"2250aa8f":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","115a8957":"#!pip install pytorch-lightning==1.4.9\n#!pip install pytorch-lightning==1.3.8\n!pip install pytorch-lightning==1.2.10","3b6d9183":"import torch\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom typing import List, Dict, Any\nfrom transformers import AdamW\n\n\nclass ImageClassifier(pl.LightningModule):\n    \n    @staticmethod\n    def get_schedulers(pm_groups: List[Dict[str, Any]]) -> List[Any]:\n        res: List[Any] = []\n        \n        return res\n\n    def __init__(self, scheduler_params, swa_start_epoch):\n        super(ImageClassifier, self).__init__()\n        # not the best model...\n        self.l1 = torch.nn.Linear(28 * 28, 10)\n        self.lr = 1e-3\n        self.scheduler_params = scheduler_params\n        self.swa_start_epoch = swa_start_epoch\n\n    def forward(self, x):\n        # called with self(x)\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n    \n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('val_loss', loss, prog_bar=True)\n\n    def test_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('test_loss', loss, prog_bar=True)\n\n    def configure_optimizers(self):\n        # REQUIRED\n        # can return multiple optimizers and learning_rate schedulers\n        # (LBFGS it is automatically supported, no need for closure function)\n        optimizers = [AdamW(self.parameters(), lr=self.lr, correct_bias=True)]    \n        schs = []\n        #schs = [torch.optim.lr_scheduler.CosineAnnealingLR(optimizers[0], T_max=2, verbose=True)]\n        print(f\"scheduler_params={self.scheduler_params}\")\n        for sp in self.scheduler_params:\n            params = dict(sp)\n            params[\"optimizer\"] = optimizers[0]\n            qn = params.pop(\"qualified_name\")\n            if qn == \"torch.optim.lr_scheduler.CosineAnnealingLR\":\n                schs.append(torch.optim.lr_scheduler.CosineAnnealingLR(**params))\n                continue\n            if qn == \"torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\":\n                schs.append(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(**params))\n                continue\n            if qn == \"torch.optim.swa_utils.SWALR\":\n                schs.append(torch.optim.swa_utils.SWALR(**params))\n        if self.swa_start_epoch >= 0:\n            if len(schs) == 0:\n                raise ValueError(\"There must be at least one scheduler\")\n            if not isinstance(schs[0], torch.optim.swa_utils.SWALR):\n                raise ValueError(\n                    \"The first scheduler must be of the type `torch.optim.swa_utils.SWALR`\"\n                )\n        return optimizers, schs","e82ae546":"import os\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import random_split, DataLoader\n\n# train\/val split\nmnist_train = MNIST('', train=True, download=True, transform=transforms.ToTensor())\nmnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\nmnist_train = DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=4)\nmnist_val = DataLoader(mnist_val, batch_size=64, shuffle=False, num_workers=4)\n\n# test split\nmnist_test = MNIST('', train=False, download=True, transform=transforms.ToTensor())\nmnist_test = DataLoader(mnist_test, batch_size=64, num_workers=4)","d15f3f81":"scheduler_params = []\nscheduler_params.append(\n    {\n        \"qualified_name\": \"torch.optim.lr_scheduler.CosineAnnealingLR\",\n        \"T_max\": 2,\n        \"verbose\": True,\n    }\n)\nimage_classifier = ImageClassifier(\n    scheduler_params=scheduler_params,\n    swa_start_epoch=-1,\n)\ntrainer = pl.Trainer(\n    tpu_cores=8,\n    max_epochs=2,\n    accelerator=None,\n    gpus=None,\n    deterministic=True,\n    callbacks=[\n        pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, verbose=True),\n        pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", verbose=True, save_top_k=1),\n        pl.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n    ],\n)    \ntrainer.fit(image_classifier, mnist_train, mnist_val)","bf7322bf":"# ckpt_path=None simply uses the weights your model currently has!\ntrainer.test(image_classifier, ckpt_path=None, test_dataloaders=mnist_test)","a76eb7bc":"---\n## 2. Data\nLightning uses standard pytorch dataloaders.\nHere we just split the data into 3 sets, val, train, test","031b8519":"# PyTorch Lightning TPU kernel\nUse this kernel to bootstrap a PyTorch project on TPUs using PyTorch Lightning\n\n## What is PyTorch Lightning?\nLightning is simply organized PyTorch code. There's NO new framework to learn.\nFor more details about Lightning visit the repo:\n\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning","b3b90fd2":"---\n## 1. Define the LightningModule\nThis is just regular PyTorch organized in a specific format.\n\nNotice the following:\n- no TPU specific code\n- no .to(device)\n- np .cuda()\n\nFor a full intro, read the following:   \nhttps:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/introduction_guide.html","eb47b5fd":"### Optional: DataModule\nFYI... \nLightning offers an optional abstraction called a DataModule which is simply a collection of dataloaders.\nFor complex datasets, we recommend you use this instead. It makes your code more reusable.\n\n[LightningDataModule docs](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/datamodules.html)","40264492":"## Run test set\nIn this example we used the test set to validation (a big no no), it's just for simplicity.\nIn real training, make sure to split the train set into train\/val and use test for testing.","86e96e84":"### Install XLA\nXLA powers the TPU support for PyTorch","b5279e2f":"## Install PyTorch Lightning\n- torch 1.9, pl 1.4.9 error `\/opt\/conda\/lib\/python3.7\/site-packages\/torch\/package\/_mock_zipreader.py:17: UserWarning: Failed to initialize NumPy: module compiled against API version 0xe but this version of numpy is 0xd (Triggered internally at  \/pytorch\/torch\/csrc\/utils\/tensor_numpy.cpp:67.)`\n- torch 1.9, pl 1.3.8 same error as above\n- torch 1.7, pl 1.3.8: EarlyStopping callback throws xmp.spawn error in `\/opt\/conda\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/tpu_spawn.py` ","8c84a297":"---\n## 3. Train on TPU\nThe Trainer automates the rest.\n\nTrains on 8 TPU cores"}}