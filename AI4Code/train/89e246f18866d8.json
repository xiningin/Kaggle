{"cell_type":{"7c9c5afe":"code","8f39d9bd":"code","278081d2":"code","77836be1":"code","bd656c4c":"code","32c875ed":"code","26a87f50":"code","e3a8d79d":"code","8e495bbb":"code","96b067b4":"code","37721e04":"code","e1a92b53":"code","a62942cf":"code","8eba4735":"code","cb4585b0":"code","3ddb607c":"code","c5f5ec90":"code","620eda7a":"code","f67be9c9":"code","a0195a98":"code","af61c166":"code","61c10b00":"code","9b630af4":"code","06cbec3b":"code","a64e4066":"code","568708dd":"code","e41dec3d":"code","0994172b":"code","17af0e11":"code","4e8437a3":"code","b4241bfa":"code","0ae69af6":"code","c7841582":"code","c986075b":"code","17342d37":"code","6b2e7109":"code","d4c30c38":"code","41478c13":"code","516f81e5":"code","248bae22":"code","3b8bc04b":"code","4691ffdb":"code","60b0363a":"markdown","7f12777d":"markdown","0b435970":"markdown","3d9a3b76":"markdown","b3eb851e":"markdown","ab6a68e5":"markdown","9d7262e6":"markdown","4600e7cf":"markdown","71d4c739":"markdown","07118653":"markdown","91377fc3":"markdown","e96863e5":"markdown","acdcf451":"markdown","08bb5c98":"markdown","11ef68f3":"markdown","32f68a04":"markdown","1ca24743":"markdown","c0bc66bc":"markdown","57eaf9c1":"markdown","9bc697cb":"markdown","085b1808":"markdown","85f06146":"markdown","94063a76":"markdown","bd35746d":"markdown","9092aade":"markdown","59dfe519":"markdown","36e5487e":"markdown","690a85e2":"markdown","8095f028":"markdown","54194139":"markdown","f35a5a42":"markdown","9c4a5d32":"markdown","3fcf4256":"markdown","53754d7b":"markdown","f94be7a1":"markdown","20fe5f5d":"markdown","d790fcbd":"markdown"},"source":{"7c9c5afe":"!pip install millify #Readable large numbers","8f39d9bd":"#Library Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Visualization\nimport seaborn as sns #Visualitzation\nfrom millify import millify #Readable large numbers","278081d2":"#Get Data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","77836be1":"train.head().T #I am using .T transpose as the table is to big to visualize horitzontaly.","bd656c4c":"y_counts = train[\"Cover_Type\"].value_counts()                # Count number of records for each distinc value (class)\ny_counts.sort_index(inplace = True)                          # Sort the returned Series, for better plotting\nbars = sns.barplot(x = y_counts.index,y = y_counts.values)   # Make barplot\n\nrecords = []                                                 #Convert the numerical values to easy readable strings \nfor x in y_counts:\n    records.append(millify(x, precision= 1))\n    \nplt.bar_label(bars.containers[0], labels = records)          # Set tha labels displaying the values of each bar\nplt.show()\nprint(\"Total number of records: {:,}\".format(sum(y_counts)))","32c875ed":"train.info()   #Checking if all features are numbers and how they are organised in the table","26a87f50":"#Splitting continuous and categorical data for easier analysis\ncontinuous = train.iloc[:, 1:11]\ncategorical = train.iloc[:, 11:-1]\ntarget = train[\"Cover_Type\"]","e3a8d79d":"continuous.describe().T    # I have selected only the columns with values, as they are ordered I just selected the fist 10 ","8e495bbb":"continuous['Cover_Type'] = target     # I am joining the target to see the relation between features and target ","96b067b4":"continuous_s =  continuous.sample(30000, random_state = 1)   # As we have many rows I am just going to take a sample of the data to work faster","37721e04":"features = continuous.columns[:-1]   # I drop the target as it is not a feature\n\ni = 1\nfig = plt.figure(figsize = (30,10))\nfor feature in features:\n    ax = fig.add_subplot(2, 5, i)\n    sns.kdeplot(data = continuous_s,x = feature, alpha = 0.2, fill = True, legend = True, palette=\"Set2\")\n    i += 1\nplt.show()","e1a92b53":"#Example of direct density plot, classes with very low samples are harder to see!\nfig = plt.figure(figsize = (12,4))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\nsns.kdeplot(data = continuous_s,x = \"Elevation\", hue=\"Cover_Type\", alpha = 0.2, fill = True, legend = True, ax = ax1, palette=\"Set2\")\nsns.violinplot(data = continuous_s,x = \"Cover_Type\", y = \"Elevation\", linewidth = 0.5, palette=\"Set2\", ax=ax2)","a62942cf":"features = continuous.columns[:-1]   # I drop the target as it is not a feature\n\ni = 1\nfig = plt.figure(figsize = (30,10))\nfor feature in features:\n    ax = fig.add_subplot(2, 5, i)\n    sns.violinplot(data = continuous_s,x = \"Cover_Type\", y = feature, linewidth = 0.5, palette=\"Set2\")\n    i += 1\nplt.show()","8eba4735":"sns.pairplot(continuous_s, hue= \"Cover_Type\", palette=\"Set2\")","cb4585b0":"corr = continuous_s.corr(method='pearson')\nsns.heatmap(corr)","3ddb607c":"categorical.columns                   #Print all categorical columns","c5f5ec90":"categories = pd.DataFrame()\ncategories[\"Wilderness_Area\"] = categorical.iloc[:,: 4].idxmax(1)\ncategories[\"Soil_Type\"] = categorical.iloc[:,4:].idxmax(1)\ncategories[\"Cover_Type\"] = target                                  #Include target to see relation with features\ncategories.head()","620eda7a":"fig = plt.figure(figsize = (12,4))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nsns.countplot(data= categories, y = \"Wilderness_Area\", ax = ax1)\nsns.countplot(data= categories, y = \"Soil_Type\", ax = ax2)\nax2.axes.get_yaxis().set_visible(False)\n","f67be9c9":"fig = plt.figure(figsize = (12,4))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nsns.countplot(data= categories, y = \"Wilderness_Area\", hue = \"Cover_Type\", ax = ax1)\nsns.countplot(data= categories, y = \"Soil_Type\", hue = \"Cover_Type\", ax = ax2)\nax2.get_yaxis().set_visible(False)","a0195a98":"features = continuous.columns[:-1]                       # Exclude target\ncomplete = pd.concat([continuous, categories], axis=1)   # Concat continuos and categorical datasets\ncomplete_s =  complete.sample(30000, random_state = 1)   # Take sample to speed up viz\n\ni = 0\nfig = plt.figure(figsize = (30,60))\n\nfor feature in features:\n    i += 1\n    ax = fig.add_subplot(10, 2, i)\n    sns.violinplot(data = complete_s, x= \"Wilderness_Area\", y = feature)\n    i += 1\n    ax = fig.add_subplot(10, 2, i)\n    sns.boxplot(data = complete_s, x= \"Soil_Type\", y = feature, width = 0.2)\n    ax.get_xaxis().set_visible(False)\n    ","af61c166":"df = complete[['Soil_Type','Wilderness_Area']]                                            # Select only cat. variables\ndf = pd.DataFrame(df.groupby([\"Soil_Type\",\"Wilderness_Area\"], as_index=False).size())     # Count how records of each\nmatrix = df.pivot(index='Soil_Type', columns='Wilderness_Area', values='size')            # Pivot into matrix\nsns.heatmap(matrix)                                                                       #Viz, easier as there are many Soil types ","61c10b00":"train.isna().sum()       #Count missing","9b630af4":"train[train.duplicated(keep=False)]","06cbec3b":"train[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360\n\ntrain.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","a64e4066":"from sklearn.model_selection import train_test_split\n\nx = train.iloc[:, :-1]        # Drop the target column\ny = train[\"Cover_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n\nx.head()                    # Check that everything is OK\n","568708dd":"from sklearn.dummy import DummyClassifier\n\nbaseline = DummyClassifier(strategy = \"most_frequent\")\nbaseline.fit(X_train,y_train )\nbaseline.score(X_test,y_test )","e41dec3d":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\n\nnb = GaussianNB()\nnb.fit(X_train,y_train )\n\ntrain_score = nb.score(X_train,y_train) *100\ntest_score = cross_val_score(nb,X_test,y_test).mean() *100\n\nprint(f\"Train accuracy: {train_score} %\")\nprint(f\"Test accuracy: {test_score} %\")\n\n#nb.score(X_test,y_test )","0994172b":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state=1, solver='liblinear')\nlr.fit(X_train,y_train )\n\ntrain_score = lr.score(X_train,y_train) *100\ntest_score = cross_val_score(lr,X_test,y_test).mean() *100\n\nprint(f\"Train accuracy: {train_score} %\")\nprint(f\"Test accuracy: {test_score} %\")","17af0e11":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=1, max_depth=10)\ndtc.fit(X_train,y_train )\ndtc.score(X_test,y_test )\n\ntrain_score = dtc.score(X_train,y_train) *100\ntest_score = cross_val_score(dtc,X_test,y_test).mean() *100\n\nprint(f\"Train accuracy: {train_score} %\")\nprint(f\"Test accuracy: {test_score} %\")","4e8437a3":"y_train_xgb = y_train-1\ny_train_xgb.value_counts()\ny_test_xgb = y_test - 1","b4241bfa":"from xgboost import XGBClassifier\n\ntree_method = \"gpu_hist\" #hist or gpu_hist\n\nxgb = XGBClassifier(objective='multi:softmax', n_estimators=100,seed=123, tree_method = tree_method,use_label_encoder=False)\nxgb.fit(X_train,y_train_xgb )\n\ntrain_score = xgb.score(X_train,y_train_xgb) *100\ntest_score = cross_val_score(xgb,X_test,y_test_xgb).mean() *100\n\nprint(f\"Train accuracy: {train_score} %\")\nprint(f\"Test accuracy: {test_score} %\")","0ae69af6":"from sklearn.metrics import confusion_matrix\n\ny_pred = xgb.predict(X_test)\n\ncm = confusion_matrix(y_test_xgb, y_pred)\nplt.figure(figsize=(12,10))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt='.2%')","c7841582":"from sklearn.model_selection import train_test_split\n\n#train_s = train.sample(50000, random_state = 1)\ntrain_s = train.groupby(\"Cover_Type\").apply(lambda x: x.sample(min(len(x), 10000)))\n\nx = train_s.iloc[:, :-1]        # Drop the target column\ny = train_s[\"Cover_Type\"]\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(x, y, test_size=0.33, random_state=1)\n\ny_train_xgb = y_train_s - 1\ny_test_xgb = y_test_s - 1","c986075b":"y_train_xgb.value_counts()","17342d37":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\ntree_method = \"gpu_hist\" #hist or gpu_hist\n\n#score = {\"Accuracy\": \"accuracy\"}\nclf_xgb = XGBClassifier(objective='multi:softmax', n_estimators=100,seed=123, tree_method = tree_method,use_label_encoder=False)\n\nparameter = {'n_estimators': [10,100,300,500, 800, 1000, 5000]}\nsearch = GridSearchCV(clf_xgb, parameter, n_jobs=-1, return_train_score = True)\nsearch.fit(X_train_s, y_train_xgb)\nsearch.score(X_test_s, y_test_xgb)\n\nprint(search.best_score_)\nprint(search.best_estimator_)","6b2e7109":"results = pd.DataFrame(search.cv_results_)\nresults.T","d4c30c38":"clf_xgb = XGBClassifier(objective='multi:softmax', n_estimators = 200, seed = 123, tree_method = tree_method, use_label_encoder=False)\n\nparameter = {'max_depth': [4,6,8,10], 'subsample':[0.5, 0.7, 0.9], 'colsample_bytree': [0.5, 0.7, 0.9]}\nsearch = RandomizedSearchCV(clf_xgb, parameter, refit = \"AUC\", cv = 5, n_jobs=-1)\nsearch.fit(X_train_s, y_train_xgb)\nsearch.score(X_test_s, y_test_xgb)\n\nprint(search.best_score_)\nprint(search.best_estimator_)","41478c13":"results = pd.DataFrame(search.cv_results_)\nresults.T","516f81e5":"y_train_xgb = y_train-1\ny_train_xgb.value_counts()\ny_test_xgb = y_test - 1","248bae22":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\ntree_method = \"gpu_hist\" #hist or gpu_hist\n\nclf_xgb = XGBClassifier(objective='multi:softmax', n_estimators = 200, seed = 123, tree_method = tree_method, use_label_encoder=False, subsample = 0.9, max_depth = 8, colsample_bytree = 0.9 )\nclf_xgb.fit(X_train,y_train_xgb)\n\nprint(clf_xgb.score(X_train,y_train_xgb))\nprint(clf_xgb.score(X_test, y_test_xgb))","3b8bc04b":"predictions = clf_xgb.predict(test) + 1       # Make predictions\nsubmission = pd.DataFrame(test[\"Id\"])     # Create submission file with Ids\nsubmission[\"Cover_Type\"] = predictions    # Append predictions\nsubmission.head()                         # Check","4691ffdb":"submission.to_csv('submission.csv', index=False)","60b0363a":"As seen before most data is in Soil type 1 and W.A. 1 and 3. I can't see any relevant relationship in this matrix.","7f12777d":"# 1. EDA (Exploratory Data Analysis)","0b435970":"Again, no duplicate rows. We can get to the final steps safely.","3d9a3b76":"I want to know how many values we have for each class. We will see if there are any class imbalances and take them into account for future steps.","b3eb851e":"## 1.3 Descriptive Statistics \nNext, we are going to look at the descriptive statistics of each feature to get an idea of the values it contains. As we have continuos and categorical data, I will first split it and work with each independently. As I am just trying to get an idea of how the data is, there is no problem on doing several steps. Later I will combine them to look for more specific characteristics.","ab6a68e5":"# 3. Modeling\nFirst, I am going to creat a baseline model, then I am going to try multiple models with some basic parameters and select the best scores. I will finally invest some time in fine tuning these best models to get the most of them and select the best one.\n##  3.1 Baseline\nI am going to start with a basic model that can serve as a baseline. As the dataset is highly immbalanced I will use a dummy classifier that just predicts the most frequent class. This will probably give me an accuracy of just over 50%.","9d7262e6":"Exactly what we expected. Now we have a score to beat!","4600e7cf":"Both distributions are quite unbalanced, I wonder if there is a class that is directly related to a Cover type (target). To check it  I am going to plot the relation.","71d4c739":"Features come in very different scales and ranges. It might be interesting to try scaling for some models. However we have to be carefull with scaling as models lose interpretability and possible relations between variables.","07118653":"## 2.2 Duplicates\nNow I am going to check for duplicate rows:","91377fc3":"## 1.1 Data Inspection \nI start by visualizing the table to get an idea of all features and its values. An explanation of all features is detailed in: https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data","e96863e5":"Library Installs and Imports:","acdcf451":"Now we can check how many times each class apears to get an idea of the distribution","08bb5c98":"## 2.3 Train Test Split\nI am going to split my data into train\/test with 1\/3 of the data as test.","11ef68f3":"We clearly see that features don't follow a normal distribution. We will have to standarize distributions for some models. ","32f68a04":"First I am going to check for nulls or missing values. If it is the case I will decide wether to remove the record or fill the gap:","1ca24743":"# December Tabular Series\nMy approach will be in 3 main steps:\n1. EDA (Exploratory Data Analysis)\n2. Preprocessing\n3. Modeling & fine tuning","c0bc66bc":"## 1.5 Relation with target \nAs we have a hughe class imbalance I will use the violinplot to understand the direct relationship between features and target. A direct density distribution is unsuitable as the imbalances make the plot harder to interpretate. ","57eaf9c1":"## 1.7 Categorical EDA\n\nNext, I am going to analise the categorical features.","9bc697cb":"As expected, we can cleary see again that there is no correlation between features.","085b1808":"## Decision Tree","85f06146":"Finally let's see the relationship between the categorical variables","94063a76":"## XGBoost","bd35746d":"## 2.1 Missing Values","9092aade":"## Logistic Regression","59dfe519":"There is a hughe class inbalance, one class has 1 record while an other has 2.3Millions! We will have to put a lot of emphasis into it to correctly train our models. We will also need to be careful when getting results as, predicting only classes 1 and 2 might have very good results despite failing to identify all other classes.","36e5487e":"## 1.4 Feature Distribution\nNow we are going to look for the features distribution and relation to the target (Cover Type). ","690a85e2":"There are many similar distributions between classes, the most distinctive is elevation. It will be interesting to check wheter a naive model based on eleveation will give good results","8095f028":"Test data will only be used for model testing to avoid any data leakage. All the EDA will be done with train set.","54194139":"Categorical features are all One-Hot Encoded, which means that each feature has multiple binary columns refearing to each possible class. That is fine for modeling but not for EDA, so first thing is grouping each feature in one column.","f35a5a42":"## 1.2 Target class distribution ","9c4a5d32":"## Naive Bayes - Gaussian","3fcf4256":"I can not see any strong relationship between them","53754d7b":"# 2. Data Preprocessing\nCheck for errors on the dataset and handle them to get a clean DB.","f94be7a1":"Luckly the dataset is clean! There are no missing values.","20fe5f5d":"## 1.6 Correlation between features\n\nI am cheching wheter there exist a correlation between the variables. This could let to poor model performance and needs to be adressed.","d790fcbd":"There is defenetly not a direct correlation between features. But again, elevation seems to be a good predictior for Cover Type (target). As an alternative check, I am going to plot the correlation matrix (by pearson coeficient) of the features. "}}