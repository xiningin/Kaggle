{"cell_type":{"2e1799fc":"code","1fd4ed11":"code","f9391a06":"code","755c4687":"code","390bc275":"code","e4de3ba1":"code","de789ce7":"code","4cb6332a":"code","9d03212f":"code","d4f7ea87":"code","e83bfb28":"code","af1facff":"code","95dfa65d":"code","a08c5b80":"code","5351ec16":"code","9d436fdf":"code","d1100a01":"code","cc8e837c":"code","1d20e701":"code","ae7ca55f":"code","5bb88e7b":"code","d2df113f":"code","eb308aee":"code","08311c0a":"code","c36c6ab4":"code","7e10cc15":"code","386e8669":"code","c1c36e31":"code","7ad460c2":"code","90a4a549":"code","b6efe249":"code","d76982d0":"code","5f532a16":"code","aacc132d":"code","002e41c1":"code","12cbd481":"code","48449cd0":"code","a841a1fd":"code","dd05682d":"code","abf6fbe5":"code","e188455d":"code","b6e336ad":"code","f3ebcee5":"code","9b858e69":"code","8e162c4b":"code","62a88eef":"code","3a5ecb93":"code","79bc345a":"code","a3872625":"code","12a4d806":"code","1bdab92e":"markdown","751adae4":"markdown","325366b2":"markdown","f85330f1":"markdown","cd994b5f":"markdown","d0c8d2b6":"markdown","8d32340e":"markdown","6e311b1b":"markdown","46e3ba66":"markdown","0ebbdc08":"markdown","83a94256":"markdown","d3251095":"markdown","f1dacd9a":"markdown","7d1e5061":"markdown","fc3e5c7d":"markdown","2e2b1adb":"markdown","1010ceb5":"markdown","7b64ab15":"markdown","2363db22":"markdown","96cb9af7":"markdown","44410d92":"markdown","5de167ce":"markdown","11d40cfa":"markdown","3edc5298":"markdown","0b8b8a7d":"markdown","d4ec625a":"markdown","03d72000":"markdown","adcf8767":"markdown","81c6335a":"markdown","cd74bf0c":"markdown","3fd40971":"markdown","db4df81d":"markdown","d78542d5":"markdown","0f8d6158":"markdown","5af10d09":"markdown","d8c3d554":"markdown","3f95c243":"markdown","42ea478d":"markdown","87964316":"markdown"},"source":{"2e1799fc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier","1fd4ed11":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","f9391a06":"print(train_df.columns.values)","755c4687":"# preview the data\ntrain_df.head(10)","390bc275":"train_df.tail()","e4de3ba1":"train_df.info()","de789ce7":"test_df.info()","4cb6332a":"print(train_df.columns)","9d03212f":"train_df.describe()","d4f7ea87":"train_df.describe(include=['O'])","e83bfb28":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","af1facff":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","95dfa65d":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a08c5b80":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5351ec16":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","9d436fdf":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","d1100a01":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","cc8e837c":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","1d20e701":"train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\nprint(train_df.shape,test_df.shape)","ae7ca55f":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","5bb88e7b":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d2df113f":"pd.crosstab(train_df['Title'], train_df['Sex'])","eb308aee":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head(10)","08311c0a":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","c36c6ab4":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","7e10cc15":"guess_ages = np.zeros((2,3))\nguess_ages","386e8669":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","c1c36e31":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","7ad460c2":"train_df","90a4a549":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_df.head(10)","b6efe249":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","d76982d0":"test_df.head()","5f532a16":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","aacc132d":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","002e41c1":"train_df.head()","12cbd481":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","48449cd0":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\ntrain_df.head()","a841a1fd":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","dd05682d":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","abf6fbe5":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","e188455d":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","b6e336ad":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","f3ebcee5":"test_df.head(10)","9b858e69":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","8e162c4b":"test_df.head()","62a88eef":" #Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)","3a5ecb93":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\ndf=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\n","79bc345a":"df","a3872625":"df.to_csv(\"submission.csv\",index=False)","12a4d806":"!jupyter nbconvert --to html your_notebook_notebook1.ipynb","1bdab92e":"**DERIVED VARIABLE**","751adae4":"We can replace many titles with a more common name or classify them as Rare.","325366b2":"in this part describe funtion gives categorical variables details","f85330f1":"Converting nominal to ordinal variable","cd994b5f":"**4. Missing Value treatment**\n\nFilling Embarked with mode","d0c8d2b6":"**5. Last Step---Create a model**\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\nKNN or k-Nearest Neighbors\nSupport Vector Machines\nNaive Bayes classifier\nDecision Tree\nRandom Forrest\nPerceptron\nArtificial neural network\nRVM or Relevance Vector Machine","8d32340e":"**3. FEATURE ENGINEERING**\n\nWHY WE DO IT HERE?\n\n  1.To eliminate the redundant  feature.\n  2.To create useful derived feature.\n  \nDropping ticket and cabin feature.","6e311b1b":"**Have a glimpse over null values(missing values)**","46e3ba66":"**insights**\n\nfemale survived in more number in comparison to male\n ","0ebbdc08":"Here Describe function  helps us determine, among other early insights, how representative is the training dataset of the actual problem domain maily it gives numerical variable details.","83a94256":"**Result**\n\ninfants survived more\n\npeople above 80 survived\n\nlarge number of people between 15-25 years of age did not survive","d3251095":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.","f1dacd9a":"**findings:**\n\n class 1 passenger survived more\n \n class 3 passenged died alot","7d1e5061":"coverting continous to ordinal","fc3e5c7d":"**Which features are categorical?**\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n**Categorical**: Survived, Sex, and Embarked. Ordinal: Pclass.\nWhich features are numerical?\n\n**Which features are numerical?** These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\n**Continous**: Age, Fare. Discrete: SibSp, Parch.","2e2b1adb":"**Analyze by describing data**","1010ceb5":"**Observations.**\n\nFemale passengers had much better survival rate than males. \nException in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\nMales had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. \nPorts of embarkation have varying survival rates for Pclass=3 and among male passengers","7b64ab15":"Dropping Redundant Variables","2363db22":"**Create new feature combining existing features**\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","96cb9af7":"We can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code\n\n","44410d92":"Another derived variable","5de167ce":"\n\n**2.Data analysis(Bivariate & Univariate)**\n\n**Univariate analysis**:- provides summary statistics for each field in the raw data set (or) summary only on one variable. Ex:- CDF,PDF,Box plot, Violin plot.\n\n**Bivariate analysis**:- is performed to find the relationship between each variable in the dataset and the target variable of interest (or) using 2 variables and finding the relationship between them.Ex:-Box plot, Violin plot.","11d40cfa":"**Insight:**\n\nmore people in the family more chances of death\n\nWe can create another feature Isalone","3edc5298":"**result**\n\nlow SibSp survived more","0b8b8a7d":"**Observations.**\n\nPclass=3 had most passengers, however most did not survive. \nInfant passengers in Pclass=2 and Pclass=3 mostly survived. \nMost passengers in Pclass=1 survived.\nPclass varies in terms of Age distribution of passengers.","d4ec625a":"**Correlating numerical features**\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\nInfants (Age <=4) had high survival rate.\nOldest passengers (Age = 80) survived.\nLarge number of 15-25 year olds did not survive.\nMost passengers are in 15-35 age range.\nDecisions.\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\nWe should consider Age (our assumption classifying #2) in our model training.\nComplete the Age feature for null values (completing #1).\nWe should band age groups (creating #3).","03d72000":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","adcf8767":"**Converting categorical feature to numeric**\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.","81c6335a":"**Acquire data**\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\n\n**Read the csv file to a dataframe.**","cd74bf0c":"**result**\n\nHigh Parch-->Died\nor here we can say it depends on god.","3fd40971":"**1.Variable identification**","db4df81d":"converting nominal to ordinal.","d78542d5":"**Correlating categorical features**\n\nNow we can correlate categorical features with our solution goal.","0f8d6158":"Convert the Fare feature to ordinal values based on the FareBand.","5af10d09":"**Correlating numerical and ordinal features**\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.Correlating numerical and ordinal features\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.","d8c3d554":"**Correlating categorical and numerical features**\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).","3f95c243":"**The confidence score generated by the model based on our training dataset**","42ea478d":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","87964316":"**Observations.**\n\nHigher fare paying passengers had better survival. \nPort of embarkation correlates with survival rates. "}}