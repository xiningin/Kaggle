{"cell_type":{"c4f8b934":"code","44b18e57":"code","aa4878c4":"code","9429c419":"code","242816a9":"code","0e6598a9":"code","ef027035":"code","06ebed2f":"code","88b85b63":"code","d352e302":"code","7891ab79":"code","4eacb84b":"code","4b6da302":"code","2da99c88":"code","821b676c":"code","0561e8db":"code","fc809a67":"code","306996f7":"code","d0720736":"code","04199712":"code","ff5683d5":"code","4be3a2fa":"code","adb36792":"code","853d78ef":"code","0249cfec":"code","952496d5":"code","fcba7f56":"code","63414ef0":"code","c42537f2":"code","3bb87c03":"code","32d54966":"code","e8bd7767":"code","0840c34f":"code","7791f159":"code","40ee6c17":"code","448c437f":"code","718ff870":"code","1a89f39a":"code","3ed96faa":"code","efd964fe":"code","43922d10":"code","619e45b5":"code","1d83f7ab":"code","2e6a798d":"code","d101d84e":"code","897b3af0":"code","2bb15b16":"code","c9ffcc96":"code","dfaf062d":"code","d0fb24cb":"code","e780c549":"code","be5caa01":"code","163f5369":"code","3a31d415":"code","ac2cbb8d":"code","5d91184a":"code","a06bea7e":"code","e552e602":"code","45dd240d":"code","a72b4047":"code","b7ff6613":"code","14f98a2e":"code","0d5f1e12":"code","b932632c":"markdown","e7d56a37":"markdown","0f365cd3":"markdown","5d943d13":"markdown","2437d874":"markdown","bd285382":"markdown","d3e89261":"markdown","27770cbd":"markdown","4573bed7":"markdown","e65113bc":"markdown","41ab9803":"markdown","65e698f5":"markdown","3d11c046":"markdown","bcd7570d":"markdown","238f2270":"markdown","dfb4b7d6":"markdown","3256e94d":"markdown","3b8e4a87":"markdown","7def7e89":"markdown","8f5f6847":"markdown","27d5b6a3":"markdown","49436cdc":"markdown","7d2409d6":"markdown","1dfcfdad":"markdown","ceaa0cca":"markdown","2952877c":"markdown","21acc3f1":"markdown","dfd4876a":"markdown","6663b880":"markdown","433fc59f":"markdown","6a114170":"markdown"},"source":{"c4f8b934":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44b18e57":"from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\n%matplotlib inline","aa4878c4":"# Importing train data\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head(6) # Mention no of rows to be displayed from the top in the argument","9429c419":"# Importing test data\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head(6) # Mention no of rows to be displayed from the top in the argument","242816a9":"train.info()","0e6598a9":"train.describe().transpose()","ef027035":"# In training set\n\nfor i in range(train.shape[1]):\n    print(train.columns[i],\"-\",train.iloc[:,i].isnull().sum())","06ebed2f":"# In test set\n\nfor i in range(test.shape[1]):\n    print(test.columns[i],\"-\",test.iloc[:,i].isnull().sum())","88b85b63":"fig = plt.figure(figsize=(15,8))\nsns.distplot(train[\"SalePrice\"],bins=26,color=\"brown\")\nsns.set_style(\"white\")\nsns.set_context(\"poster\",font_scale=2)\nplt.tight_layout()","d352e302":"#skewness \n\nprint(\"Skewness: \" + str(train['SalePrice'].skew()))","7891ab79":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n \n\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), \n            cmap=sns.diverging_palette(20, 220, n=200), \n            mask = mask, \n            annot=True, \n            center = 0, \n            cbar=\"coolwarm\",\n           );\nplt.tight_layout()","4eacb84b":"train.corr()[\"SalePrice\"].sort_values(ascending=False)[1:]","4b6da302":"fig = plt.figure(figsize=(15,8))\nsns.scatterplot(x=\"OverallQual\",y=\"SalePrice\",data=train)\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\",font_scale=2)\nplt.tight_layout()","2da99c88":"fig = plt.figure(figsize=(15,8))\nsns.scatterplot(x=\"GrLivArea\",y=\"SalePrice\",data=train)\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\",font_scale=1.5)\nplt.tight_layout()","821b676c":"fig = plt.figure(figsize=(15,8))\nsns.scatterplot(x=\"GarageArea\",y=\"SalePrice\",data=train)\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\",font_scale=2)\nplt.tight_layout()","0561e8db":"## Deleting those two values with outliers.\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop = True, inplace = True)\n\nprevious_train = train.copy()\nprint(train.shape)","fc809a67":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ntrain.drop(columns=['Id'],axis=1, inplace=True)\ntest.drop(columns=['Id'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['SalePrice'].reset_index(drop=True)\n\n\n\n# getting a copy of train\nprevious_train = train.copy()","306996f7":"## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((train, test)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['SalePrice'], axis = 1, inplace = True)\n","d0720736":"# count of missing values in each feature\nfor i in range(all_data.shape[1]):\n    print(all_data.columns[i],\"-\",all_data.iloc[:,i].isnull().sum())","04199712":"missing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor i in missing_val_col:\n    all_data[i] = all_data[i].fillna('None')","ff5683d5":"## In the following features the null values are there for a purpose, so we replace them with \"0\"\nmissing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageYrBlt',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor i in missing_val_col2:\n    all_data[i] = all_data[i].fillna(0)\n    \n## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))","4be3a2fa":"## Zoning class are given in numerical; therefore converted to categorical variables. \nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str) ","adb36792":"all_data['Functional'] = all_data['Functional'].fillna('Typ') \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\") ","853d78ef":"# count of missing values in each feature\n\nsum = 0\nfor i in range(all_data.shape[1]):\n    sum = sum + all_data.iloc[:,i].isnull().sum()\nprint(sum)    ","0249cfec":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats","952496d5":"fig = plt.figure(figsize=(15,8))\nsns.distplot(train[\"1stFlrSF\"],bins=26,color=\"brown\")\nsns.set_style(\"white\")\nsns.set_context(\"poster\",font_scale=2)\nplt.tight_layout()","fcba7f56":"## Fixing Skewed features \ndef fixing_skewness(df):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.stats import skew\n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = high_skew.index\n\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n\nfixing_skewness(all_data)","63414ef0":"sns.distplot(all_data['1stFlrSF']);","c42537f2":"all_data['TotalSF'] = (all_data['TotalBsmtSF'] \n                       + all_data['1stFlrSF'] \n                       + all_data['2ndFlrSF'])\n\nall_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\n\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] \n                                 + all_data['BsmtFinSF2'] \n                                 + all_data['1stFlrSF'] \n                                 + all_data['2ndFlrSF']\n                                )\n                                 \n\nall_data['Total_Bathrooms'] = (all_data['FullBath'] \n                               + (0.5 * all_data['HalfBath']) \n                               + all_data['BsmtFullBath'] \n                               + (0.5 * all_data['BsmtHalfBath'])\n                              )\n                               \n\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] \n                              + all_data['3SsnPorch'] \n                              + all_data['EnclosedPorch'] \n                              + all_data['ScreenPorch'] \n                              + all_data['WoodDeckSF']\n                             )","3bb87c03":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n","32d54966":"all_data.shape","e8bd7767":"all_data = all_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","0840c34f":"final_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","7791f159":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):, :]","40ee6c17":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","448c437f":"def overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99.94:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = overfit_reducer(X)\n\nX = X.drop(overfitted_features, axis=1)\nX_sub = X_sub.drop(overfitted_features, axis=1)","718ff870":"X.shape,y.shape, X_sub.shape","1a89f39a":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state = 42)","3ed96faa":"# Ridge\n\nalpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    \n    ridge = Ridge(alpha= i, normalize=True)\n    \n    ridge.fit(X_train, y_train)\n\n    y_pred = ridge.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    temp_mse[i] = mse\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))    ","efd964fe":"# Lasso\n\ntemp_mse = {}\nfor i in alpha_ridge:\n     \n    lasso_reg = Lasso(alpha= i, normalize=True)\n    \n    lasso_reg.fit(X_train, y_train)\n    \n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    \n    temp_mse[i] = mse\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","43922d10":"# Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\ntemp_mse = {}\nfor i in alpha_ridge:\n \n    lasso_reg = ElasticNet(alpha= i, normalize=True)\n    \n    lasso_reg.fit(X_train, y_train)\n    \n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    \n    temp_mse[i] = mse\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","619e45b5":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","1d83f7ab":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","2e6a798d":"# Ridge, Lasso and Elastic Net\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                                              alphas=alphas2, \n                                              random_state=42, \n                                              cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))  ","d101d84e":"score = cv_rmse(ridge)\nprint(\"Ridge:\" , score.mean(), score.std())\n\nscore = cv_rmse(lasso)\nprint(\"Lasso:\" , score.mean(), score.std())\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet:\" , score.mean() , score.std())","897b3af0":"# SVR\n\nsvr = SVR()\nparameters = {'C':[15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],'epsilon':[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009],'gamma':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009]}\nrs= RobustScaler()\nX_rs = rs.fit_transform(X)\nsvr_reg = RandomizedSearchCV(svr,parameters,scoring=\"neg_mean_squared_error\",cv = 5,n_iter = 100,verbose =3,n_jobs =-1)\nsvr_reg.fit(X_rs,y)","2bb15b16":"svr_reg.best_params_","c9ffcc96":"# SVR\n\nsvr = make_pipeline(RobustScaler(), SVR(C= 28, epsilon= 0.001, gamma=0.0002,))\nscore = cv_rmse(svr)\nprint(\"SVR:\" , score.mean() , score.std())","dfaf062d":"# LGBMRegressor\n\nlgbm = LGBMRegressor(objective='regression',random_state=42)\nparameters={'num_leaves':[1,3,4,5,6,8,10],'learning_rate':[0.001,0.01,0.02,0.03,0.04,0.05,0.06],'n_estimators':[500,1000,3000,5000,10000],'max_bin':[100,200,300,400,500],'bagging_fraction':[0.25,0.50,0.75],'bagging_freq':[3,4,5,6,7], 'bagging_seed':[5,6,7,8,9],'feature_fraction':[0.1,0.2,0.3,0.4,0.5,0.6],'feature_fraction_seed':[5,6,7,8,9]}\nlgbm_reg = RandomizedSearchCV(lgbm,parameters,scoring=\"neg_mean_squared_error\",cv = 5,n_iter = 100,verbose =3,n_jobs =-1)\nlgbm_reg.fit(X,y)","d0fb24cb":"lgbm_reg.best_params_","e780c549":"np.sqrt(-lgbm_reg.best_score_)","be5caa01":"# LGBMRegressor\n\nlgbm = LGBMRegressor(objective='regression',random_state=42,num_leaves=10,\n  n_estimators=3000,\n  max_bin= 300,\n  learning_rate= 0.01,\n  feature_fraction_seed=5,\n  feature_fraction=0.3,\n  bagging_seed=8,\n  bagging_freq=4,\n  bagging_fraction=0.75)","163f5369":"# Xgboost\n\nxgb = XGBRegressor(random_state=42,learning_rate=0.01)\nparameters={'n_estimators':[3000,3500,3250,3750,4000,5000],'max_depth':[1,2,3,4,5,6],'min_child_weight':[1,3,5,7],'gamma':[0.0,0.1,0.2,0.3],'colsample_bytree':[0.3,0.6,0.7]}\nxgb_reg = RandomizedSearchCV(xgb,parameters,scoring=\"neg_mean_squared_error\",cv = 5,n_iter = 5,verbose =3,n_jobs =-1)\nxgb_reg.fit(X,y)","3a31d415":"xgb_reg.best_params_","ac2cbb8d":"np.sqrt(-xgb_reg.best_score_)","5d91184a":"# Xgboost\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3500,\n                                     max_depth=5, min_child_weight=5,\n                                     gamma=0.0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27)","a06bea7e":"# Stacking of regression model\nstack_reg = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lgbm ),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","e552e602":"stack_model = stack_reg.fit(np.array(X), np.array(y))\nprint(1)\n\nelastic_model = elasticnet.fit(X, y)\nprint(2)\n\nlasso_model = lasso.fit(X, y)\nprint(3)\n\nridge_model = ridge.fit(X, y)\nprint(4)\n\nsvr_model = svr.fit(X, y)\nprint(5)\n\nxgb_model = xgboost.fit(X, y)\nprint(6)\n\nlgbm_model = lgbm.fit(X, y)\nprint(7)","45dd240d":"def blend_models(X):\n    return ((0.1 * elastic_model.predict(X)) + \\\n            (0.05 * lasso_model.predict(X)) + \\\n            (0.2 * ridge_model.predict(X)) + \\\n            (0.1 * svr_model.predict(X)) + \\\n            (0.15 * xgb_model.predict(X)) + \\\n            (0.1 * lgbm_model.predict(X)) + \\\n            (0.3 * stack_model.predict(np.array(X))))","a72b4047":"print(rmsle(y, blend_models(X)))","b7ff6613":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models(X_sub)))","14f98a2e":"sub_1 = pd.read_csv('..\/input\/top-submission1\/submission 1.csv')\nsub_2 = pd.read_csv('..\/input\/top-submission2\/submission 2.csv')\nsub_3 = pd.read_csv('..\/input\/top-submission3\/submission 3.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","0d5f1e12":"q1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_n.csv\", index=False)","b932632c":"# Importing Necessary Libraries","e7d56a37":"# House Prices: Advanced Regression Techniques[RMSE: 0.11220 ]\n\n* This is my third notebook. Do point out my mistakes in comment section.\n* I have achieved 86th rank in the leaderboard.\n* A brief work is done on EDA and Feature Engineering.\n* If you find my work interesting, do upvote it.","0f365cd3":"# Feature Engineering","5d943d13":"# Looking for Missing Values","2437d874":"### Deleting Features ","bd285382":"* From the above plot, it is visible that there are outliers in the scatter plot.\n* As the column \"GrLivArea\" increase, the value of SalePrice increases. But two points after 4500 do not follow this trend. ","d3e89261":"As we can see, the multicollinearity(strong correlation between independent variables) exists in various features. Rather than deleting these features from the dataset, we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible outcome.","27770cbd":"### Fixing Outliers\n","4573bed7":"# Data Loading\n\nOur first step is to extract train and test data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them.","e65113bc":"No missing values are left.","41ab9803":"### Submission","65e698f5":"### Creating Dummy Variables ","3d11c046":"### Creating New Features","bcd7570d":"### Blending with top kernels ","238f2270":"# Thank You!!","dfb4b7d6":"# Describing Dataset","3256e94d":"Sice the tail is on right side, the distribution is positively skewd. You can verify it by checking the value of mean, median and mode. In positively skewd, mean and median is greater than mode. Let's, check the magnitude of skewness.","3b8e4a87":"# EDA","7def7e89":"Look like we have lot of missing values to deal with. We will fix missing values after the EDA part.","8f5f6847":"### Blending of models\n","27d5b6a3":"This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis.","49436cdc":"Now, let's plot and visualize the relation between dependent feature and highly corelated independent feature.","7d2409d6":"### Eliminating missing values ","1dfcfdad":"Our main focus is target variable which is SalePrice. Let's find out how this column is distributed.","ceaa0cca":"# Modeling[Stacking and Blending]\n\nIn this section, we will use different regression model and will also create model using stacking them. Now, we will perform blending of all these models and also perform blending with top submission kernel. We perform blending with top submission to improve accuracy.\n\n* Ridge\n* Lasso\n* Elastic Net\n* SVR\n* LGBMRegressor\n* Xgboost\n","2952877c":"We will fix this later. Now, let's check the relation between the target variable and other features.","21acc3f1":"### Fixing Skewness","dfd4876a":"### **SalePrice vs GrLivArea** ","6663b880":"### **SalePrice vs GarageArea**","433fc59f":"### **SalePrice vs OverallQual **","6a114170":"# Introduction\n\nThe three major portion of this notebook are:-\n\n1. Exploratory Data Analysis\n2. Feature Engineering\n3. Modeling[Stacking and Blending] \n"}}