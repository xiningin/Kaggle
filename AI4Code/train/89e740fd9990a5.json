{"cell_type":{"de785ab5":"code","bb572880":"code","c6fb244c":"code","50e3882e":"code","6fcf1c03":"code","864efd7c":"code","cb090f26":"code","0c80e8a4":"code","5a490491":"code","c5f73b85":"code","4ebceedd":"code","f957beac":"code","bced9fbe":"code","8f98e5f6":"code","088f6cec":"code","d82f6824":"code","5db50692":"code","95cc7288":"code","87d054d3":"code","ea064c3f":"code","2ed5ea80":"code","2cae4440":"markdown","ad8e4433":"markdown","b4573c83":"markdown","db7f15e9":"markdown","c3e6b6f0":"markdown","a76c24b4":"markdown","31e2f186":"markdown","61044b16":"markdown","5c5e31da":"markdown","5d93b15d":"markdown","a18f346e":"markdown"},"source":{"de785ab5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bb572880":"#To remove depricated warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","c6fb244c":"#Loading train and test files:\ntrain_path=\"\/kaggle\/input\/human-activity-recognition-with-smartphones\/train.csv\"\ndf_train=pd.read_csv(train_path)\ntest_path=\"\/kaggle\/input\/human-activity-recognition-with-smartphones\/test.csv\"\ndf_test=pd.read_csv(test_path)\ndf = pd.concat([df_train,df_test])","50e3882e":"df.head(10)","6fcf1c03":"df.columns\n# Program to remove all whitespaces\nimport re\n# matches all whitespace characters\npattern = r'[()-.,]+'\n\ntempcol = []\ncol_new = []\nfor col in df_train.columns:\n  new_string = re.sub(pattern, '_', col) \n  col_new.append(new_string)\n  tempcol.append(new_string.split('_')[0])\n  \ndf.columns = col_new\n\n#The main features are:\nprint('The main columns are:')\nfor temp in list(set(tempcol)):\n    print(temp)\n","864efd7c":"# Returns a Summary dataframe for numeric columns only \n# output will be same as host_df.describe()\ndf.describe(exclude='O')\n# Returns a Summary dataframe \n#  for object type (or categorical) columns only \ndf.describe(include='O')\n#Cheking data disribtion\n#1. Value greater then -1 and 1 , data is higly skewed\n#2. Values between -1 to -0.5 and 0.5 to 1 is less skewed\n#3. Values between -0.5 to 0.5 is almost symmetrically distributed\ndf.skew()\n\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\nprint(corr_matrix)","cb090f26":"#Since its a classification problem, its important to know if data is balanced or not?\nprint(df.Activity.value_counts())\ndf.Activity.value_counts().plot.bar()","0c80e8a4":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n#le.fit([\"WALKING\", \"LAYING\", \"STANDING\", \"SITTING\",\"WALKING_UPSTAIRS\",\"WALKING_DOWNSTAIRS\"])\ndf['Activity'] = le.fit_transform(df['Activity'])\ndf_en = df.drop(columns=['subject']) #dropping unwaned columns\ndf_en_data = df_en.drop(columns = ['Activity'])\ndf_en_target = df_en['Activity']\ndf_en_target.value_counts()","5a490491":"!pip install pyod","c5f73b85":"\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\n#from pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n\nrandom_state = np.random.RandomState(42)\n#Removing 5% outliners\noutliers_fraction = 0.05\n# Define outlier detection tools to be compared\nclassifiers = {\n        'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n        'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n        #'Feature Bagging':FeatureBagging(LOF(n_neighbors=35),contamination=outliers_fraction,check_estimator=False,random_state=random_state),\n        'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state),\n        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n        'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n}\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(df_en)\n    # predict raw anomaly score\n    scores_pred = clf.decision_function(df_en) * -1\n        \n    # prediction of a datapoint category outlier or inlier\n    y_pred = clf.predict(df_en)\n    n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n    n_outliers = np.count_nonzero(y_pred == 1)\n    \n    print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n    print('-'*50)","4ebceedd":"'''\nUsing Robust based technique to scale data as it has outliners\n'''\nfrom sklearn.preprocessing import RobustScaler\nrobustscaler = RobustScaler()\ndf_robust = robustscaler.fit_transform(df_en_data)\ndf_robust","f957beac":"#from sklearn.preprocessing import StandardScaler\n#df_en_std = StandardScaler().fit_transform(df_en)\nprint('Covariance matrix \\n')\ndf_en_cov_mat= np.cov(df_robust, rowvar=False)\ndf_en_cov_mat\ndf_en_cov_mat = np.cov(df_robust.T)\neig_vals, eig_vecs = np.linalg.eig(df_en_cov_mat)\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)\ntot = sum(eig_vals)\nprint(\"\\n\",tot)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\nprint(\"\\n\\n1. Variance Explained\\n\",var_exp)\ncum_var_exp = np.cumsum(var_exp)\nprint(\"\\n\\n2. Cumulative Variance Explained\\n\",cum_var_exp)\nprint(\"\\n\\n3. Percentage of variance the first 200 principal components each contain\\n \",var_exp[0:200])\nprint(\"\\n\\n4. Percentage of variance the first 200 principal components together contain\\n\",sum(var_exp[0:200]))","bced9fbe":"# Splitting the training and test data","8f98e5f6":"from sklearn.decomposition import PCA\npca = PCA(n_components=200)\nprincipalComponents = pca.fit_transform(df_robust)\ndf_pca = pd.DataFrame(data = principalComponents)\n\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(df_pca,df_en_target,test_size=0.4, random_state=42)\n","088f6cec":"#Apply model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n","d82f6824":"Classifiers=[LogisticRegression(max_iter=1000, C=0.1,solver= 'newton-cg'),\n             DecisionTreeClassifier(class_weight =  'balanced', criterion = 'entropy'),\n             RandomForestClassifier(class_weight =  'balanced', criterion = 'entropy'),\n             GradientBoostingClassifier(),\n             AdaBoostClassifier(),\n             ExtraTreesClassifier(),\n             KNeighborsClassifier(),\n             SVC(kernel=\"linear\",degree=3,C=10,gamma=0.001),\n             GaussianNB()]\npipelines = []\nfor model in Classifiers:\n    pipeline = make_pipeline(\n              model)\n    pipelines.append(pipeline)\nfor pipeline in pipelines:\n    pipeline.fit(xtrain, ytrain)","5db50692":"Accuracy_mean = []\nAccuracy_std = []\nmodel_names = ['LR','DTC','RFC','GBC','AB','ET','KNN','SVC','GNB']\noutcome = []\n#models scores\nfor pipeline in pipelines:\n    print(pipeline)\n    print('Train Score: ',pipeline.score(xtrain, ytrain))\n    print('Test Score: ',pipeline.score(xtest, ytest))\n    pred = pipeline.predict(xtest)\n    precision_score_temp = precision_score(ytest, pred, average='micro')\n    recall_score_temp = recall_score(ytest, pred, average='micro')\n    f1_score_temp = f1_score(ytest, pred, average='micro')\n    all_accuracies = cross_val_score(estimator=pipeline, X=xtrain, y=ytrain, cv=5)\n    print(f'All Accuracies: {all_accuracies}')\n    print(f'Mean Accuracies: {all_accuracies.mean()}')\n    print(f'Std of Accuracies: {all_accuracies.std()}')\n    print(f'Accuracy: {accuracy_score(ytest, pred)}')\n    #print(f'Precision: {precision_score_temp}')\n    #print(f'Recall: {recall_score_temp}')\n    #print(f'f1: {f1_score_temp}')\n    print(classification_report(ytest, pred))\n    print('Confusion_matrix:')\n    print(f'{confusion_matrix(ytest, pred ,labels=[0,1,2,3,4,5])}')\n    Accuracy_mean.append(all_accuracies.mean())\n    Accuracy_std.append(all_accuracies.std())\n    outcome.append(all_accuracies)\n    print('*'*50)","95cc7288":"import matplotlib.pyplot as plt\nfig = plt.figure()\nfig.suptitle('Machine Learning Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(outcome)\nax.set_xticklabels(model_names)\nplt.show()","87d054d3":"import matplotlib.pyplot as plt\nfig = plt.figure()\nfig.suptitle('Machine Learning Model Comparison')\nax = fig.add_subplot(111)\nplt.bar(model_names,Accuracy_mean)\nax.set_xticklabels(model_names)\nplt.show()","ea064c3f":"#Making pipeline for Logestic regression:\n#from sklearn.linear_model import LogisticRegression\n#steps = [('LR', LogisticRegression())]\n#make_pipeline = Pipeline(steps) # define the pipeline object.    \n#parameteres = {'LR__max_iter':[1000, 5000],'LR__C':[0.1,10,100,10e5], 'LR__fit_intercept':[True, False], 'LR__class_weight':[None , 'balanced'], 'LR__solver' : ['newton-cg', 'lbfgs', 'liblinear']}\n#grid_LR = GridSearchCV(make_pipeline, param_grid=parameteres)\n#grid_LR.fit(xtrain, ytrain)\n\n#grid_preds = grid_LR.predict(xtest)\n#accuracy = accuracy_score(ytest, grid_preds)\n#precision = precision_score(ytest, grid_preds, average='micro')\n#recall = recall_score(ytest, grid_preds, average='micro')\n#f1 = f1_score(ytest, grid_preds, average='micro')\n#['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']\n#confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])\n#print('Best params: ', grid_LR.best_params_)\n#print('Best score: ', grid_LR.best_score_)\n#print('LogisticRegression accuracy: ', accuracy)\n#print('LogisticRegression precision: ', precision)\n#print('LogisticRegression recall: ', recall)\n#print('LogisticRegression f1: ', f1)\n#print(\"score = %3.2f\" %(grid_LR.score(xtest,ytest)))\n#print(f'Accuracy LogisticRegression classifier on training set {grid_LR.score(xtrain, ytrain)}')\n#print(f'Accuracy LogisticRegression classifier on testing set {grid_LR.score(xtest, ytest)}')","2ed5ea80":"#Making pipeline for SVC:\n#from sklearn.svm import SVC\n#steps = [('SVC', SVC())]\n#make_pipeline = Pipeline(steps) # define the pipeline object.    \n#parameteres = {'SVC__C':[0.001,0.1,10,100,10e5], 'SVC__kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'SVC__degree':[3,4,5], 'SVC__class_weight' : [None,'balanced']}\n#grid_SVC = GridSearchCV(make_pipeline, param_grid=parameteres)\n#grid_SVC.fit(xtrain, ytrain)\n\n#grid_preds = grid_SVC.predict(xtest)\n#accuracy = accuracy_score(ytest, grid_preds)\n#precision = precision_score(ytest, grid_preds, average='micro')\n#recall = recall_score(ytest, grid_preds, average='micro')\n#f1 = f1_score(ytest, grid_preds, average='micro')\n#['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']\n#confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])\n#print('Best params: ', grid_SVC.best_params_)\n#print('Best score: ', grid_SVC.best_score_)\n#print('SVC accuracy: ', accuracy)\n#print('SVC precision: ', precision)\n#print('SVC recall: ', recall)\n#print('SVC f1: ', f1)\n#print(\"score = %3.2f\" %(grid_SVC.score(xtest,ytest)))\n#print(f'Accuracy SVC classifier on training set {grid_SVC.score(xtrain, ytrain)}')\n#print(f'Accuracy SVC classifier on testing set {grid_SVC.score(xtest, ytest)}')\n#print(classification_report(ytest, grid_preds))\n#print(f'{confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])}')","2cae4440":"                                                        About Project dataset\nHuman activity Recognition using smartphones Data set The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings\/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nAttribute Information:\n\nFor each record in the dataset it is provided:\n\nTriaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\nTriaxial Angular velocity from the gyroscope.\nA 561-feature vector with time and frequency domain variables.\nIts activity label.\nAn identifier of the subject who carried out the experiment.\nData Analysis:\n\n1. From the give signals of x,y,z axis of accelrometer and gyroscope, we aim to provide the classification to following groups.\n\nLaying\nStanding\nSitting\nWalking\nWalking_Upstairs\nWalking_Downstairs","ad8e4433":"# Encoding catageorical values","b4573c83":"The total 563 columns basically consist of above parameter divided on x, y and z axis.","db7f15e9":"                                                      What to expect from this Notebook?\n1. PreProcessing steps\n    * Reading and understanding the data with basic panda library\n    * Encoding categorical values\n    * Finding outliners using pyod library\n    * Feature selection using PCA\n2. Model selection and comaprision\n    * Pipeline to create Classifier models\n    * Hyper tuning the models\n    * Display of classification results and comparision of models.\n    ","c3e6b6f0":"# To detect outliners using pyod lib","a76c24b4":"Conclusion: Logestic regression and SVC gave 97% accuracy with hypertuning of models and using PCA","31e2f186":"                            Code For Hyper-tuning the model using grid search and finding the best parameter","61044b16":"Scaling the dataset , because many outliners are detected","5c5e31da":"# Model Selection","5d93b15d":"# Preprocessing","a18f346e":"# Applying PCA"}}