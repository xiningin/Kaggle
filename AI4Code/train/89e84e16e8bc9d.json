{"cell_type":{"0d8fa15b":"code","a25683bd":"code","2852ea3a":"code","58374881":"code","89eb4945":"code","3595a857":"code","5b4951a9":"code","3fc44e3c":"code","26bc7c96":"code","bb730c87":"code","20fa9f5b":"code","d1f7827d":"code","0a8bafcf":"code","c3f8b628":"code","bd4a3453":"code","f9de3290":"code","467ba341":"code","562ad82e":"code","d7226d76":"code","04efa44e":"code","dc58f6cb":"code","7438454a":"code","5171389b":"code","93178e28":"code","e415d798":"code","947ef072":"code","c213c423":"code","530f4e8b":"code","20b9881e":"code","6ccb8168":"code","73bc9e24":"code","7e229bd7":"code","3990ac84":"code","9597fb0e":"code","667d78f0":"code","f9daabb5":"code","aeeec92a":"code","276eddff":"code","0ee62e6f":"code","d4b46763":"code","7739d76f":"code","eb88e5c8":"code","e1da0026":"code","06275347":"code","295ef7f8":"code","fe4cda33":"code","28423140":"code","b21ecfd2":"code","0a90c09f":"code","40227a2a":"code","9010a3df":"code","c832d437":"code","7a3ae592":"code","63219424":"code","51ab3908":"code","88038362":"code","b901c0ef":"code","f1178f90":"code","e8873ea2":"code","9ee30deb":"code","d4a44321":"code","e96205cd":"code","71969a73":"code","b6b39d68":"code","544da0b6":"code","6c6625ef":"code","0019b217":"code","4e427531":"code","b3d528ca":"code","0485bdbf":"code","e3065b03":"code","dcb00dcf":"markdown","d9f25ad5":"markdown","c88da9a9":"markdown","005631b0":"markdown","0f4b7f66":"markdown","7e0b19d1":"markdown"},"source":{"0d8fa15b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nimport scipy.stats as st\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nimport warnings\nwarnings.filterwarnings('ignore')","a25683bd":"df_train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ndf_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')","2852ea3a":"df_train.columns","58374881":"df_train['SalePrice'].describe()","89eb4945":"df_train.describe()","3595a857":"df_train.head()","5b4951a9":"# Checking distribution\ny = df_train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=st.lognorm)\nplt.show()","3fc44e3c":"# Looking for outliers on features\n#numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n#numeric_train = df_train.select_dtypes(include=numerics)\n\n#for feature in numeric_train:\n#    if feature != 'SalePrice' and feature != 'Id':\n#        plt.figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n#        plt.scatter(numeric_train[feature], numeric_train['SalePrice'])\n#        plt.xlabel(feature)\n#        plt.xticks(rotation=90)\n#        plt.ylabel('SalePrice')\n#        plt.show()\n","26bc7c96":"# Cutting off numerical outliers\ndf_train = df_train.drop(df_train[(df_train.SalePrice >= 700000)].index)\ndf_train = df_train.drop(df_train[(df_train.LotFrontage >= 200)].index)\ndf_train = df_train.drop(df_train[(df_train.LotArea >= 100000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallQual == 4) & (df_train.SalePrice > 200000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallQual == 8) & (df_train.SalePrice > 500000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallQual == 10) & (df_train.SalePrice < 300000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallCond == 2) & (df_train.SalePrice > 300000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallCond == 5) & (df_train.SalePrice > 700000)].index)\ndf_train = df_train.drop(df_train[(df_train.OverallCond == 6) & (df_train.SalePrice > 700000)].index)\ndf_train = df_train.drop(df_train[(df_train.YearBuilt <= 1900) & (df_train.SalePrice > 200000)].index)\ndf_train = df_train.drop(df_train[(df_train.MasVnrArea >= 1200)].index)\ndf_train = df_train.drop(df_train[(df_train.BsmtFinSF1 >= 3000)].index)\ndf_train = df_train.drop(df_train[(df_train.BsmtFinSF2 >= 1200)].index)\ndf_train = df_train.drop(df_train[(df_train.TotalBsmtSF >= 4000)].index)\ndf_train = df_train.drop(df_train[(df_train['1stFlrSF'] >= 4000)].index)\ndf_train = df_train.drop(df_train[(df_train.LowQualFinSF > 500) & (df_train.SalePrice > 400000)].index)\ndf_train = df_train.drop(df_train[(df_train.GrLivArea >= 4000)].index)\ndf_train = df_train.drop(df_train[(df_train.BedroomAbvGr >= 7)].index)\ndf_train = df_train.drop(df_train[(df_train.KitchenAbvGr < 1)].index)\ndf_train = df_train.drop(df_train[(df_train.TotRmsAbvGrd > 12)].index)\ndf_train = df_train.drop(df_train[(df_train.Fireplaces > 2)].index)\ndf_train = df_train.drop(df_train[(df_train.GarageCars > 3)].index)\ndf_train = df_train.drop(df_train[(df_train.GarageArea > 1200) & (df_train.SalePrice < 300000)].index)\ndf_train = df_train.drop(df_train[(df_train.WoodDeckSF > 700)].index)\ndf_train = df_train.drop(df_train[(df_train.OpenPorchSF > 450)].index)\ndf_train = df_train.drop(df_train[(df_train.EnclosedPorch > 450)].index)\ndf_train = df_train.drop(df_train[(df_train['3SsnPorch'] > 350)].index)","bb730c87":"# Joining DFs for ease of processing\nfull = pd.concat([df_train, df_test], ignore_index = True)\nfull.drop(['SalePrice'], axis=1, inplace=True)\n\n# Starting to handle categoric columns\n\n# \"MiscFeature\" column\nwords_set = set()\nfor data in full['MiscFeature']:\n    if not pd.isnull(data):\n        for word in data.split(' '):\n            words_set.add(word)\n#print (\"Raw words set: {}\".format(words_set))\n\nfor feature in words_set:\n    count = 0\n    for data in full['MiscFeature']:\n        if not pd.isnull(data):\n            if feature in data:\n                count += 1\n    print (\"Category {}: {} samples\".format(feature, count))","20fa9f5b":"def add_columns (dataframe):\n    add_categorical = ['Othr', 'TenC', 'Gar2', 'Shed']\n    for column in add_categorical:\n        data = dataframe['MiscVal'][dataframe['MiscFeature'] == column]\n        dataframe[column] = pd.Series(data, index = dataframe.index).fillna(value=0)\n    dataframe.drop(columns = ['MiscFeature'], inplace = True)\n    dataframe.drop(columns = ['MiscVal'], inplace = True)\n\nadd_columns(full)\n    \n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","d1f7827d":"plt.scatter(range(0, len(df_train['SalePrice'])), df_train['SalePrice'])\nplt.show()","0a8bafcf":"categorical_features = [\n    'Alley',            # Will have to check on plot\n    'BldgType',         # Will have to check on plot\n    'BsmtCond',         # Simple mapping w\/ NaN\n    'BsmtExposure',     # Simple mapping w\/ NaN\n    'BsmtFinType1',     # Will have to check on plot\n    'BsmtFinType2',     # Will have to check on plot\n    'BsmtQual',         # Simple mapping w\/ NaN\n    'CentralAir',       # Yes\/No\n    'Condition1',       # Will have to check on plot\n    'Condition2',       # Will have to check on plot\n    'Electrical',       # Will have to check on plot\n    'ExterCond',        # Simple mapping\n    'Exterior1st',      # Will have to check on plot\n    'Exterior2nd',      # Will have to check on plot\n    'ExterQual',        # Simple mapping\n    'Fence',            # Will have to check on plot\n    'FireplaceQu',      # Simple mapping w\/ NaN\n    'Foundation',       # Will have to check on plot\n    'Functional',       # Will have to check on plot\n    'GarageCond',       # Simple mapping w\/ NaN\n    'GarageFinish',     # Simple mapping w\/ NaN\n    'GarageQual',       # Simple mapping w\/ NaN\n    'GarageType',       # Will have to check on plot\n    'Heating',          # Will have to check on plot\n    'HeatingQC',        # Simple mapping\n    'HouseStyle',       # Will have to check on plot\n    'KitchenQual',      # Simple mapping\n    'LandContour',      # Will have to check on plot\n    'LandSlope',        # Will have to check on plot\n    'LotConfig',        # Will have to check on plot\n    'LotShape',         # Will have to check on plot\n    'MasVnrType',       # Will have to check on plot\n    'MSZoning',         # Will have to check on plot\n    'Neighborhood',     # Will use this to extract latitude\/longitude\n    'PavedDrive',       # Simple mapping\n    'PoolQC',           # Simple mapping w\/ NaN\n    'RoofMatl',         # Will have to check on plot\n    'RoofStyle',        # Will have to check on plot\n    'SaleCondition',    # Will have to check on plot\n    'SaleType',         # Will have to check on plot\n    'Street',           # Simple mapping\n    'Utilities'         # Will parse this into separate features\n]\n\nnumerical_features = [\n    '1stFlrSF',         # SqFt value, won't change anything here for now\n    '2ndFlrSF',         # SqFt value, won't change anything here for now\n    '3SsnPorch',        # SqFt value, won't change anything here for now\n    'BsmtFinSF1',       # SqFt value, won't change anything here for now\n    'BsmtFinSF2',       # SqFt value, won't change anything here for now\n    'BsmtFullBath',     # Absolute number, won't change anything here for now\n    'BsmtHalfBath',     # Absolute number, won't change anything here for now\n    'BsmtUnfSF',        # SqFt value, won't change anything here for now\n    'EnclosedPorch',    # SqFt value, won't change anything here for now\n    'Fireplaces',       # Absolute number, won't change anything here for now\n    'FullBath',         # Absolute number, won't change anything here for now\n    'GarageArea',       # SqFt value, won't change anything here for now\n    'GarageCars',       # Absolute number, won't change anything here for now\n                        #\n    'GarageYrBlt',      # Will use this for creating a \"GarageAge\" feature\n                        #\n    'GrLivArea',        # SqFt value, won't change anything here for now\n    'HalfBath',         # Absolute number, won't change anything here for now\n    'LotArea',          # SqFt value, won't change anything here for now\n    'LotFrontage',      # Absolute number, won't change anything here for now\n    'LowQualFinSF',     # SqFt value, won't change anything here for now\n    'MasVnrArea',       # SqFt value, won't change anything here for now\n    'MoSold',           # Absolute number, won't change anything here for now\n    'MSSubClass',       # Absolute number, won't change anything here for now\n    'OpenPorchSF',      # SqFt value, won't change anything here for now\n    'OverallCond',      # Absolute number, won't change anything here for now\n    'OverallQual',      # Absolute number, won't change anything here for now\n                        #\n    'PoolArea',         # Will use this for creating a \"Pool\" feature\n                        #\n    'ScreenPorch',      # SqFt value, won't change anything here for now\n    'TotalBsmtSF',      # SqFt value, won't change anything here for now\n    'TotRmsAbvGrd',     # Absolute number, won't change anything here for now\n    'WoodDeckSF',       # SqFt value, won't change anything here for now\n                        #\n    'YearBuilt',        # Will use this for creating a \"HouseAge\" feature\n                        #\n    'YearRemodAdd',     # Will use this for creating a \"RemodelAge\" feature\n                        #\n    'YrSold',           # Absolute number, won't change anything here for now\n    'BedroomAbvGr',     # Absolute number, won't change anything here for now\n    'KitchenAbvGr',     # Absolute number, won't change anything here for now\n    'Othr',             # Extracted from 'MiscFeature' column\n    'TenC',             # Extracted from 'MiscFeature' column\n    'Gar2',             # Extracted from 'MiscFeature' column\n    'Shed'              # Extracted from 'MiscFeature' column\n]","c3f8b628":"# Creating \"GarageAge\", \"RemodelAge\" and \"HouseAge\" features\ndef createAgeFeatures (dataframe):\n    dataframe['GarageAge'] = dataframe['GarageYrBlt'].apply(lambda x: 2019 - x)\n    dataframe['RemodelAge'] = dataframe['YearRemodAdd'].apply(lambda x: 2019 - x)\n    dataframe['HouseAge'] = dataframe['YearBuilt'].apply(lambda x: 2019 - x)\n\n# Creating \"Pool\" feature\ndef createPoolFeature (dataframe):\n    dataframe['Pool'] = dataframe['PoolArea'].apply(lambda x: x != 0).map({True: 1, False: 0})\n\n# Handling numerical features\ndef handleNumericalFeatures (dataframe):\n    createAgeFeatures(dataframe)\n    createPoolFeature(dataframe)\n\nhandleNumericalFeatures(full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","bd4a3453":"# Mapping the \"Simple mapping\" features\ndef mapSimpleMapping (dataframe):\n    # features = ['ExterCond', 'ExterQual', 'HeatingQC', 'KitchenQual', 'PavedDrive', 'Street']\n    dataframe['ExterCond'] = dataframe['ExterCond'].map({\n        'Ex' : 2,\n        'Gd' : 1,\n        'TA' : 0,\n        'Fa' : -1,\n        'Po' : -2\n    })\n    dataframe['ExterQual'] = dataframe['ExterQual'].map({\n        'Ex' : 2,\n        'Gd' : 1,\n        'TA' : 0,\n        'Fa' : -1,\n        'Po' : -2\n    })\n    dataframe['HeatingQC'] = dataframe['HeatingQC'].map({\n        'Ex' : 2,\n        'Gd' : 1,\n        'TA' : 0,\n        'Fa' : -1,\n        'Po' : -2\n    })\n    dataframe['KitchenQual'] = dataframe['KitchenQual'].map({\n        'Ex' : 2,\n        'Gd' : 1,\n        'TA' : 0,\n        'Fa' : -1,\n        'Po' : -2\n    })\n    dataframe['PavedDrive'] = dataframe['PavedDrive'].map({\n        'Y' : 1,\n        'P' : 0.5,\n        'N' : 0,\n    })\n    dataframe['Street'] = dataframe['Street'].map({\n        'Grvl' : -1,\n        'Pave' : 1,\n    })\n\n# Mapping the \"Simple mapping w\/ NaN\" features\ndef mapSimpleWithNan (dataframe):\n    #features = ['BsmtCond', 'BsmtExposure', 'BsmtQual', 'FireplaceQu',\n    #            'GarageCond', 'GarageFinish', 'GarageQual', 'PoolQC']\n    dataframe['BsmtCond'] = dataframe['BsmtCond'].map({\n        'Ex' : 4,\n        'Gd' : 3,\n        'TA' : 2,\n        'Fa' : 1,\n        'Po' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['BsmtExposure'] = dataframe['BsmtExposure'].map({\n        'Gd' : 3,\n        'Av' : 2,\n        'Mn' : 1,\n        'No' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['BsmtQual'] = dataframe['BsmtQual'].map({\n        'Ex' : 4,\n        'Gd' : 3,\n        'TA' : 2,\n        'Fa' : 1,\n        'Po' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['FireplaceQu'] = dataframe['FireplaceQu'].map({\n        'Ex' : 4,\n        'Gd' : 3,\n        'TA' : 2,\n        'Fa' : 1,\n        'Po' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['GarageCond'] = dataframe['GarageCond'].map({\n        'Ex' : 4,\n        'Gd' : 3,\n        'TA' : 2,\n        'Fa' : 1,\n        'Po' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['GarageFinish'] = dataframe['GarageFinish'].map({\n        'Fin' : 2,\n        'RFn' : 1,\n        'Unf' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['GarageQual'] = dataframe['GarageQual'].map({\n        'Ex' : 4,\n        'Gd' : 3,\n        'TA' : 2,\n        'Fa' : 1,\n        'Po' : 0,\n        'NA' : -1\n    }).fillna(-1)\n    dataframe['PoolQC'] = dataframe['PoolQC'].map({\n        'Ex' : 3,\n        'Gd' : 2,\n        'TA' : 1,\n        'Fa' : 0,\n        'NA' : -1\n    }).fillna(-1)\n\n# Mapping boolean features\ndef mapBoolean (dataframe):\n    dataframe['CentralAir'] = dataframe['CentralAir'].map({\n        'N' : 0,\n        'Y' : 1,\n    })\n\n# Handling mapped categorical features\ndef handleMappedCategorical (dataframe):\n    mapSimpleMapping(dataframe)\n    mapSimpleWithNan(dataframe)\n    mapBoolean(dataframe)\n\nhandleMappedCategorical(full)","f9de3290":"# Checking plots for other categorical features\ncheck_plot_features = [\n    'Alley',\n    'BldgType',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'Condition1',\n    'Condition2',\n    'Electrical',\n    'Exterior1st',\n    'Exterior2nd',\n    'Fence',\n    'Foundation',\n    'Functional',\n    'GarageType',\n    'Heating',\n    'HouseStyle',\n    'LandContour',\n    'LandSlope',\n    'LotConfig',\n    'LotShape',\n    'MasVnrType',\n    'MSZoning',\n    'RoofMatl',\n    'RoofStyle',\n    'SaleCondition',\n    'SaleType'\n]\n\n#for feature in check_plot_features:\n#    data = pd.concat([df_train['SalePrice'], df_train[feature]], axis=1)\n#    f, ax = plt.subplots(figsize=(16, 10))\n#    fig = sns.boxplot(x=feature, y=\"SalePrice\", data=data)\n#    fig.axis(ymin=0);\n#    xt = plt.xticks(rotation=45)\n#    plt.xticks(rotation=90)\n#    plt.show()\n","467ba341":"#\n#    After plots observation, I decided to proceed with the following:\n#\n#    'Alley',\n#\t'Grvl' : 0, 'Pave' : 1\n#\n#    'BldgType',\n#\tOne-hot encoding only\n#\n#    'BsmtFinType1',\n#\tOne-hot encoding only\n#\n#    'BsmtFinType2',\n#\tOne-hot encoding only\n#\n#    'Condition1',\n#\tCreate features using information about distance\n#\tOne-hot encoding\n#\n#    'Condition2',\n#\tCreate features using information about distance\n#\tOne-hot encoding\n#\n#    'Electrical',\n#\t'Mix' : 0, 'FuseP' : 1, 'FuseF' : 2, 'FuseA' : 3, 'SBrkr' : 4\n#\n#    'Exterior1st',\n#\tOne-hot encoding only\n#\n#    'Exterior2nd',\n#\tOne-hot encoding only\n#\n#    'Fence',\n#\tSeparate in two features:\n#\t'FencePrivacy'\n#\t\t'MnPrv' : 0, 'GdPrv': 1, 'NA' : -1\n#\t'FenceWood'\n#\t\t'MnWw' : 0, 'GdWo' : 1, 'NA' : -1\n#\n#    'Foundation',\n#\tOne-hot encoding only\n#\n#    'Functional',\n#\t'Typ' : 0, 'Min1' : -1, 'Min2' : -2, 'Mod' : -3, 'Maj1' : -4, 'Maj2' : -5, 'Sev' : -6, 'Sal' : -7\n#\n#    'GarageType',\n#\tOne-hot encoding only\n#\n#    'Heating',\n#\tOne-hot encoding only\n#\n#    'HouseStyle',\n#\tOne-hot encoding only\n#\n#    'LandContour',\n#\tOne-hot encoding only\n#\n#    'LandSlope',\n#\tOne-hot encoding only\n#\n#    'LotConfig',\n#\tOne-hot encoding only\n#\n#    'LotShape',\n#\t'Reg' : 0, 'IR1' : -1, 'IR2' : -2, 'IR3' : -3\n#\n#    'MasVnrType',\n#\tOne-hot encoding only\n#\n#    'MSZoning',\n#\tOne-hot encoding only\n#\n#    'RoofMatl',\n#\tOne-hot encoding only\n#\n#    'RoofStyle',\n#\tOne-hot encoding only\n#\n#    'SaleCondition',\n#\tOne-hot encoding only\n#\n#    'SaleType'\n#\tOne-hot encoding only\n#\n#    'Neighborhood'\n#\tWill use this to extract latitude\/longitude\n#\n#    'Utilities'\n#\tWill parse this into separate features\n\n# One-hot encodings\none_hots = ['BldgType', 'BsmtFinType1', 'BsmtFinType2', 'Condition1', 'Condition2', 'Exterior1st', 'Exterior2nd',\n           'Foundation', 'GarageType', 'Heating', 'HouseStyle', 'LandContour', 'LandSlope', 'LotConfig',\n           'MasVnrType', 'MSZoning', 'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType']\n\ndef doOneHot (dataframe):\n    try:\n        for category in one_hots:\n            df = pd.Categorical(dataframe[category])\n            dfDummies = pd.get_dummies(df, prefix = category)\n            dataframe = pd.concat([dataframe, dfDummies], axis=1)\n            if category not in ['Condition1', 'Condition2']:\n                dataframe.drop(columns = [category], inplace = True)\n        return dataframe\n    except KeyError:\n        print (\"Oops! Category {} not found! Probably this has already been done...\".format(category))\n        return dataframe\n        \n\n# Processing one-hots\nfull = doOneHot(full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","562ad82e":"# Still missing handling:\n# - Alley\n# - Condition1\n# - Condition2\n# - Electrical\n# - Fence\n# - Functional\n# - LotShape\n# - Neighborhood\n# - Utilities\n\ndef handleAlley (dataframe):\n    dataframe['Alley'] = dataframe['Alley'].map({'Grvl' : 0, 'Pave' : 1})\n\ndef handleConditions (dataframe):\n    handler = {\n        'Normal' : 0,  # ??\n        'RRNn'   : 1,  # Within 200'\n        'RRNe'   : 1,  #\n        'PosN'   : 2,  # Near\n        'Artery' : 3,  # Adjacent\n        'Feedr'  : 3,  #\n        'RRAn'   : 3,\n        'PosA'   : 3,\n        'RRAe'   : 3\n    }\n    dataframe['Condition1'] = dataframe['Condition1'].map(handler)\n    dataframe['Condition2'] = dataframe['Condition2'].map(handler)\n    \ndef handleElectrical (dataframe):\n    handler = {'Mix' : 0, 'FuseP' : 1, 'FuseF' : 2, 'FuseA' : 3, 'SBrkr' : 4}\n    dataframe['Electrical'] = dataframe['Electrical'].map(handler)\n\ndef handleFunctional (dataframe):\n    handler = {'Typ' : 0, 'Min1' : -1, 'Min2' : -2, 'Mod' : -3, 'Maj1' : -4, 'Maj2' : -5, 'Sev' : -6, 'Sal' : -7}\n    dataframe['Functional'] = dataframe['Functional'].map(handler)\n\ndef handleLotShape (dataframe):\n    handler = {'Reg' : 0, 'IR1' : -1, 'IR2' : -2, 'IR3' : -3}\n    dataframe['LotShape'] = dataframe['LotShape'].map(handler)\n    \ndef handlePostOneHots (dataframe):\n    handleAlley(dataframe)\n    handleConditions(dataframe)\n    handleElectrical(dataframe)\n    handleFunctional(dataframe)\n    handleLotShape(dataframe)\n    \nhandlePostOneHots(full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","d7226d76":"# Still missing handling:\n# - Fence\n# - Neighborhood\n# - Utilities\n\ndef handleFence (dataframe):\n    dataframe['FencePrivacy'] = dataframe['Fence'].map({'MnPrv' : 0, 'GdPrv': 1}).fillna(-1)\n    dataframe['FenceWood'] = dataframe['Fence'].map({'MnWw' : 0, 'GdWo': 1}).fillna(-1)\n    dataframe.drop(columns = ['Fence'], inplace = True)\n    return dataframe\n\nfull = handleFence (full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","04efa44e":"# Still missing handling:\n# - Neighborhood\n# - Utilities\n\ndef handleUtilities (dataframe):\n    dataframe['Electricity'] = dataframe['Utilities'].map({'AllPub' : 1, 'NoSewr' : 1, 'NoSeWa' : 1, 'ELO' : 1}).fillna(0)\n    dataframe['Gas'] = dataframe['Utilities'].map({'AllPub' : 1, 'NoSewr' : 1, 'NoSeWa' : 1, 'ELO' : 0}).fillna(0)\n    dataframe['Water'] = dataframe['Utilities'].map({'AllPub' : 1, 'NoSewr' : 1, 'NoSeWa' : 0, 'ELO' : 0}).fillna(0)\n    dataframe['Septic Tank'] = dataframe['Utilities'].map({'AllPub' : 1, 'NoSewr' : 0, 'NoSeWa' : 0, 'ELO' : 0}).fillna(0)\n    dataframe.drop(columns = ['Utilities'], inplace = True)\n    return dataframe\n\nfull = handleUtilities (full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","dc58f6cb":"# Still missing handling:\n# - Neighborhood\n\ngeo_heatmap = {\n    'Neighborhood' : [\n        'Blmngtn',\n        'Blueste',\n        'BrDale',\n        'BrkSide',\n        'ClearCr',\n        'CollgCr',\n        'Crawfor',\n        'Edwards',\n        'Gilbert',\n        'IDOTRR',\n        'MeadowV',\n        'Mitchel',\n        'NAmes',\n        'NoRidge',\n        'NPkVill',\n        'NridgHt',\n        'NWAmes',\n        'OldTown',\n        'SWISU',\n        'Sawyer',\n        'SawyerW',\n        'Somerst',\n        'StoneBr',\n        'Timber',\n        'Veenker'\n    ],\n    'Latitude' : [\n        42.0563761,\n        42.0218678,\n        42.052795,\n        42.024546,\n        42.0360959,\n        42.0214232,\n        42.028025,\n        42.0154024,\n        42.1068177,\n        42.0204395,\n        41.997282,\n        41.9903084,\n        42.046618,\n        42.048164,\n        42.0258352,\n        42.0597732,\n        42.0457802,\n        42.029046,\n        42.0266187,\n        42.0295218,\n        42.034611,\n        42.0508817,\n        42.0595539,\n        41.9999732,\n        42.0413042\n    ],\n    'Longitude' : [\n        -93.6466598,\n        -93.6702853,\n        -93.6310097,\n        -93.6545201,\n        -93.6575849,\n        -93.6584089,\n        -93.6093286,\n        -93.6875441,\n        -93.6553512,\n        -93.6243787,\n        -93.6138098,\n        -93.603242,\n        -93.6362807,\n        -93.6496766,\n        -93.6613958,\n        -93.65166,\n        -93.6472075,\n        -93.6165288,\n        -93.6486541,\n        -93.7102833,\n        -93.7024257,\n        -93.6485768,\n        -93.6365891,\n        -93.6518812,\n        -93.6524905\n    ]\n}\n\ntrain_neighborhood = set(df_train['Neighborhood'].tolist())\n\ngeo_dataframe = pd.DataFrame.from_dict(geo_heatmap)\n\ngeo_dataframe = geo_dataframe[geo_dataframe['Neighborhood'].isin(train_neighborhood)]\n\ngeo_dataframe[\"SalePrice\"] = pd.Series(df_train.groupby([\"Neighborhood\"]).mean()[\"SalePrice\"].values, index = geo_dataframe.index)\n\nimport folium\nfrom folium.plugins import HeatMap\nmax_amount = float(geo_dataframe['SalePrice'].max())\nhmap = folium.Map(location = [42.045042,-93.6473567], zoom_start = 12)\nhm_wide = HeatMap (list (zip (geo_dataframe.Latitude.values, geo_dataframe.Longitude.values, geo_dataframe.SalePrice.values)),\n                   min_opacity = 0.4,\n                   max_val = max_amount,\n                       radius = 17,\n                   blur = 15,\n                   max_zoom = 1\n                  )\nhmap.add_child(hm_wide)\n","7438454a":"# Building auxiliar dicts\nlat_dict = {}\nlon_dict = {}\nfor i in range(len(geo_heatmap['Neighborhood'])):\n    neighborhood = geo_heatmap['Neighborhood'][i]\n    lat = geo_heatmap['Latitude'][i]\n    lon = geo_heatmap['Longitude'][i]\n    lat_dict[neighborhood] = lat\n    lon_dict[neighborhood] = lon\n\n# Method that adds latitude and longitude columns\ndef add_lat_lon_columns (df):\n    df ['Latitude'] = df['Neighborhood'].map(lat_dict)\n    df ['Longitude'] = df['Neighborhood'].map(lon_dict)\n    df.drop(columns=['Neighborhood'], inplace=True)\n    \n# Adding columns to train and test sets\nadd_lat_lon_columns(full)\n\n# Checking data shapes\nprint (\"Full data shape is {}\".format(full.shape))","5171389b":"# Checking for missing data\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","93178e28":"# Handling KitchenQual w\/ average values\nfull['KitchenQual'] = full['KitchenQual'].fillna(0)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","e415d798":"# Checking missing Garage stuff\nfull[full['GarageArea'].isnull()]","947ef072":"# Will fill this single one with \"no-garage\" information\nnew = full.iloc[2576].copy()\nnew['GarageArea'] = 0\nnew['GarageCars'] = 0\nnew['GarageCond'] = -1\nnew['GarageFinish'] = -1\nnew['GarageQual'] = -1\n\nfull.iloc[2576] = new.copy()\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","c213c423":"# As the missing \"Electrical\" is from our train dataframe, put the average value (from 0 to 4)\nfull['Electrical'] = full['Electrical'].fillna(2)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","530f4e8b":"# Checking missing BsmtFinSF1\nfull[full['BsmtFinSF1'].isnull()]","20b9881e":"# As BsmtFinSF1, BsmtFinSF2, TotalBsmtSF and BsmtUnfSF are sqft data of houses without basement, will fill\n# with zeros\nfull['BsmtFinSF1'] = full['BsmtFinSF1'].fillna(0)\nfull['BsmtFinSF2'] = full['BsmtFinSF2'].fillna(0)\nfull['TotalBsmtSF'] = full['TotalBsmtSF'].fillna(0)\nfull['BsmtUnfSF'] = full['BsmtUnfSF'].fillna(0)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","6ccb8168":"# Checking missing BsmtFullBath\nfull[full['BsmtFullBath'].isnull()]","73bc9e24":"# As these samples refer to houses without basement, fill with zeros\nfull['BsmtFullBath'] = full['BsmtFullBath'].fillna(0)\nfull['BsmtHalfBath'] = full['BsmtHalfBath'].fillna(0)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","7e229bd7":"# Checking missing Functional\nfull[full['Functional'].isnull()]","3990ac84":"# Before I handle this feature, I will check for others trying to acquire enough data to predict it's\n# functionality\n\n# Checking missing MasVnrArea\nfull[full['MasVnrArea'].isnull()]","9597fb0e":"# As MasVnrArea refers to sqft data, will assume these houses have none\nfull['MasVnrArea'] = full['MasVnrArea'].fillna(0)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","667d78f0":"# Checking missing GarageYrBlt\nfull[full['GarageYrBlt'].isnull()]","f9daabb5":"# As it has no data on the year of construction of the garagage, I will assume it's been built the same year\n# the house was built.\nnull_df = full[full['GarageYrBlt'].isnull()]\nnull_df['GarageAge'] = null_df['YearBuilt'].apply(lambda x: 2019 - x)\nnull_df['GarageYrBlt'] = null_df['YearBuilt']\n\n# Replacing data\nfull[full['GarageYrBlt'].isnull()] = null_df\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","aeeec92a":"# Checking missing LotFrontage\nfull[full['LotFrontage'].isnull()]","276eddff":"# Tried to predict LotFrontage using LotArea and GrLivArea w\/out success\n# Will now handle Condition2 and Condition1\nfull['Condition1'] = full['Condition1'].fillna(0)\nfull['Condition2'] = full['Condition2'].fillna(0)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","0ee62e6f":"# Handling Alley missing data based on the data_description file\nfull['Alley'] = full['Alley'].fillna(-1)\n\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","d4b46763":"# Will now retry to predict log(LotFrontage) using log(LotArea)\n# Idea extracted from https:\/\/www.kaggle.com\/clustersrus\/house-prices-dealing-with-the-missing-data\nfull['LogLotArea'] = np.log(full['LotArea'])\nfull['LogLotFrontage'] = np.log(full['LotFrontage'])","7739d76f":"plt.scatter(full['LogLotArea'], full['LogLotFrontage'] )","eb88e5c8":"missing_mask = full.isnull()['LotFrontage']\nfilled_mask  = full.notnull()['LotFrontage']\npredict = full[missing_mask]\ntrain = full[filled_mask]\n\nX_train = train['LogLotArea']\ny_train = train['LogLotFrontage']\nX_predict = predict['LogLotArea']\nmodel = XGBRegressor(n_jobs = -1)\n\nmodel.fit(np.array(X_train).reshape(-1,1), np.array(y_train))\ny_predict = model.predict(np.array(X_predict).reshape(-1,1))\ny_predict.shape\n\nfull['LogLotFrontage'][missing_mask] = y_predict","e1da0026":"missing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","06275347":"# Filling LotFrontage by doing reverse log transformation\nfull['LotFrontage'][missing_mask] = np.exp(full['LogLotFrontage'][missing_mask])\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","295ef7f8":"# Filling functional with the Typical value\nfull['Functional'] = full['Functional'].fillna(0)\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","fe4cda33":"# Checking missing MasVnrArea\nfull[full['GarageCars'].isnull()]['GarageCond']","28423140":"full['GarageCars'] = full['GarageCars'].fillna(-1)\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","b21ecfd2":"full[full['GarageArea'].isnull()]['GarageCond']","0a90c09f":"full['GarageArea'] = full['GarageArea'].fillna(-1)\nmissing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","40227a2a":"# Adding transformed features (only high skewed will be transformed)\n# Idea extracted from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n# Improvements were initially proposed here https:\/\/www.kaggle.com\/gabrielmilan\/pre-os-de-im-veis-em-recife-pe\/notebook\nskewed_feats = full.apply(lambda x: st.skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nskewed_features = skewness.index\n\nwalk_through_lambdas = True\n\n# Method that gets best lambda for a given distribution\ndef getBestLambda (data):\n    from scipy.stats import boxcox, normaltest\n\n    statistic = []\n    pvalue = []\n\n    bclambda_range = np.linspace(-3, 3, 1000)\n\n    for bclambda in bclambda_range:\n        transform = boxcox1p(data, bclambda)\n        stat, pval = normaltest(transform)\n        statistic.append(stat)\n        pvalue.append(pval)\n\n    lowest_stat = 8000\n    closest = -10\n    best_stat = -1\n    best_pval = -1\n    for i in range(len(bclambda_range)):\n        if statistic[i] < lowest_stat:\n            lowest_stat = statistic[i]\n            best_stat = i\n        if (abs(pvalue[i]) - 1) > closest:\n            closest = (abs(pvalue[i]) - 1)\n            best_pval = i\n\n    return bclambda_range[best_pval], bclambda_range[best_stat]\n\nfrom tqdm import tqdm\n\nfor feat in tqdm(skewed_features):\n    if walk_through_lambdas:\n        full[\"BC_{}\".format(feat)] = boxcox1p(full[feat], getBestLambda(full[feat])[0])\n    else:\n        full[\"BC_{}\".format(feat)] = boxcox1p(full[feat], .15)\n        \n# Clearing columns with null values after transforming\nmissing_values = full.isnull().sum()\nfor column in missing_values[missing_values>0].keys():\n    print (\"Dropping feature {}\".format(column))\n    full.drop(column, axis = 1, inplace = True)","9010a3df":"# Split again into train and test\nX_train = full[full['Id'].isin(df_train['Id'])]\nX_train.drop('Id', axis = 1, inplace = True)\nX_test  = full[full['Id'].isin(df_test['Id'])]\nX_test.drop('Id', axis = 1, inplace = True)\ny_train = df_train['SalePrice']\n\nprint (\"X_train shape is {}, y_train shape is {} and X_test shape is {}\".format(\n    X_train.shape,\n    y_train.shape,\n    X_test.shape\n))","c832d437":"from sklearn.preprocessing import StandardScaler\n\n# Scaling data\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit(X_train).transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nbclambda = getBestLambda(y_train)[0]\ny_train_transformed = boxcox1p(y_train, bclambda)#np.log1p(y_train)#y_train.copy()#","7a3ae592":"#\n# Modeling\n#\n\n# Validation function extracted from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\nn_folds = 5\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Define Root Mean Square Error w\/ Cross-validation \ndef rmse_cv (model, X, y, cv=5):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = \"neg_mean_squared_error\", cv = cv))\n    return rmse\n\n# Define Pure Root Mean Square Error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef rmse (y_actual, y_predicted):\n     return sqrt(mean_squared_error(y_actual, y_predicted))\n","63219424":"#\n# Extracted from https:\/\/www.kaggle.com\/gabrielmilan\/pre-os-de-im-veis-em-recife-pe\/notebook\n#\n\nfrom xgboost import plot_importance\n\nmodel = XGBRegressor(nthread = -1)\nmodel.fit(X_train, y_train_transformed)\n\nf, ax = plt.subplots(figsize=(16, 10))\nplot_importance(model, ax = ax)\nplt.show()\n\n# Getting chart data\nfscore_dict = model.get_booster().get_fscore()\n\n# Getting threshold values\nthreshold_values = []\nfor feature in fscore_dict:\n    if fscore_dict[feature] not in threshold_values:\n        threshold_values.append(fscore_dict[feature])\n\n# Building lists for later plotting\nscores_y = []\n        \n# Setting a range for the threshold\nfor threshold in threshold_values:\n    feature_list = [feature for feature in X_train.columns if (feature in fscore_dict) and (fscore_dict[feature] >=  threshold)]\n    scores_y.append(rmse_cv(model, X_train[feature_list], y_train_transformed).mean())\n    \n# Plotting\nplt.figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\nplt.scatter(threshold_values, scores_y)\nplt.show()\n\n# Getting best threshold\nmin_value = 1000000000000000\nbest_threshold = 1000\nfor i in range(len(threshold_values)):\n    if scores_y [i] < min_value:\n        best_threshold = threshold_values[i]\n\nprint (\"Your best threshold is {}\".format(best_threshold))\nxgb_features_list = [feature for feature in fscore_dict if fscore_dict[feature] >= best_threshold]\nprint (\"With this threshold, the features you want are: {}\".format(xgb_features_list))\n","51ab3908":"X_train_xgb = X_train [xgb_features_list]\nX_train_xgb_scaled = scaler.fit(X_train_xgb).transform(X_train_xgb)\nX_test_xgb = X_test [xgb_features_list]\nX_test_xgb_scaled = scaler.transform(X_test_xgb)","88038362":"from sklearn.feature_selection import RFECV\n\n#model = lgb.LGBMRegressor()\n#selector = RFECV (model)\n#selector.fit(X_train, y_train_transformed)\n\n#RFECV_mask = selector.support_\n\n#rfecv_features_list = []\n#for i in range(len(RFECV_mask)):\n#    if RFECV_mask[i] == True:\n#        rfecv_features_list.append(X_train.columns[i])\n#    \n#print (\"For this metod, the features you want are: {}\".format(rfecv_features_list))\n\nrfecv_features_list = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtCond', 'BsmtExposure',\n                       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtQual', 'BsmtUnfSF',\n                       'CentralAir', 'Condition1', 'Electrical', 'EnclosedPorch', 'ExterCond',\n                       'ExterQual', 'FireplaceQu', 'FullBath', 'Functional', 'GarageArea',\n                       'GarageCars', 'GarageFinish', 'GarageQual', 'GarageYrBlt', 'GrLivArea',\n                       'HalfBath', 'HeatingQC', 'KitchenAbvGr', 'KitchenQual', 'LotArea',\n                       'LotFrontage', 'LotShape', 'LowQualFinSF', 'MSSubClass', 'MasVnrArea',\n                       'MoSold', 'OpenPorchSF', 'OverallCond', 'OverallQual', 'PavedDrive',\n                       'ScreenPorch', 'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF', 'YearBuilt',\n                       'YearRemodAdd', 'YrSold', 'BsmtFinType1_ALQ', 'BsmtFinType1_GLQ',\n                       'BsmtFinType2_BLQ', 'Condition1_Artery', 'Exterior1st_BrkFace',\n                       'Exterior1st_HdBoard', 'Exterior1st_MetalSd', 'Exterior1st_Plywood',\n                       'Exterior1st_VinylSd', 'Exterior1st_Wd Sdng', 'Foundation_CBlock',\n                       'Foundation_PConc', 'GarageType_Attchd', 'GarageType_Detchd',\n                       'HouseStyle_1Story', 'LandContour_Lvl', 'LotConfig_CulDSac',\n                       'LotConfig_FR2', 'MasVnrType_BrkFace', 'MasVnrType_Stone', 'MSZoning_RL',\n                       'MSZoning_RM', 'RoofStyle_Gable', 'SaleCondition_Abnorml',\n                       'SaleCondition_Normal', 'SaleType_New', 'SaleType_WD', 'FenceWood',\n                       'Latitude', 'Longitude', 'BC_LotArea']","b901c0ef":"X_train_rfecv = X_train [rfecv_features_list]\nX_train_rfecv_scaled = scaler.fit(X_train_rfecv).transform(X_train_rfecv)\nX_test_rfecv = X_test [rfecv_features_list]\nX_test_rfecv_scaled = scaler.transform(X_test_rfecv)","f1178f90":"# Heatmap of positive correlation features\nimport seaborn as sns\ncorrelation = df_train.corr()\nk = len([i for i in correlation['SalePrice'] if abs(i) >= 0.4])\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf , ax = plt.subplots(figsize = (18,16))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\nax.set_title('SalePrice correlation heatmap')\nplt.show()\nmy_cols = list(cols)\nif \"SalePrice\" in my_cols:\n    my_cols.remove(\"SalePrice\")","e8873ea2":"X_train_corr = X_train[my_cols]\nX_train_corr.shape","9ee30deb":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nvanilla_input_size = 0\n\ndef VanillaRegressor (input_size = None):\n    # Create model\n    model = Sequential()\n    model.add(Dense(5, input_dim=input_size, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\ndef BetaRegressor (input_size = None):\n    model = Sequential()\n    model.add(Dense(128, kernel_initializer='normal',input_dim = input_size, activation='relu'))\n    model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n    \ntrain_sets = [\n    X_train,\n    #X_train_scaled,\n    X_train_xgb,\n    #X_train_xgb_scaled,\n    X_train_rfecv,\n    #X_train_rfecv_scaled,\n    X_train_corr\n]\n\nother_models = [\n    ElasticNet(),                       # X_train          -> 0.1308\n    Lasso(),                            # X_train          -> 0.1393\n    BayesianRidge(),                    # X_train_rfecv    -> 0.0953\n    LassoLarsIC(),                      # X_train_rfecv    -> 0.1034\n    RandomForestRegressor(),            # X_train          -> 0.1193\n    GradientBoostingRegressor(),        # X_train_rfecv    -> 0.1002\n    KernelRidge(),                      # X_train_rfecv    -> 0.0959\n    xgb.XGBRegressor(),                 # X_train_rfecv    -> 0.0997\n    xgb.XGBRFRegressor(),               # <any>            -> 0.3244\n    lgb.LGBMRegressor(),                # X_train_rfecv    -> 0.1024\n    SVR()                               # X_train_corr     -> 0.3224\n]\n\nother_models_names = [\n    'ElasticNet',\n    'Lasso',\n    'BayesianRidge',\n    'LassoLarsIC',\n    'RandomForestRegressor',\n    'GradientBoostingRegressor',\n    'KernelRidge',\n    'XGBRegressor',\n    'XGBRFRegressor',\n    'LGBMRegressor',\n    'SVR'\n]\n\n# Evaluating models\n# for j in range(len(other_models)):\n#     for i in range(len(train_sets)):\n#         score = rmse_cv(other_models[j], train_sets[i], y_train_transformed)\n#         print (\"{} scored {:.4f}(+-{:.4f}) with the set #{}\".format(other_models_names[j], score.mean(), score.std(), i+1))","d4a44321":"# Choosing one X_train for hyper-parameter tuning\nchosen_X_train = X_train_rfecv.copy()\nchosen_X_test = X_test_rfecv.copy()","e96205cd":"# Grid search for hyper-parameters\nfrom sklearn.model_selection import GridSearchCV\nclass grid():\n    def __init__ (self, model):\n        self.model = model\n    def grid_get (self, X, y, param_grid):\n        grid_search = GridSearchCV(self.model, param_grid, scoring='neg_mean_squared_error', verbose=99)\n        grid_search.fit(X, y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params', 'mean_test_score', 'std_test_score']])\n\nmake_search = False\nmax_iter = 15000\ndataset = chosen_X_train\n\n# Will keep the following models for hyper-params tuning:\n# - BayesianRidge\n# - GradientBoostingRegressor\n# - KernelRidge\n# - XGBRegressor\n# - LGBMRegressor\n#\tBayRidge\tGBR\tKernelR\tXGB\tLGBM\t\n#1\t0.0956\t0.1018\t0.0971\t0.1008\t0.1028\t0.4981\n#3\t0.1066\t0.1049\t0.1063\t0.106\t0.1079\t0.5317\n#5\t0.0953\t0.1006\t0.0959\t0.1005\t0.1022\t0.4945\n#\t0.2975\t0.3073\t0.2993\t0.3073\t0.3129\t\n","71969a73":"# Grid search for Gradient Boost Regressor\nif (make_search):\n    grid(GradientBoostingRegressor()).grid_get(dataset, y_train_transformed, {\n        'loss' : ['ls', 'huber'],\n        'learning_rate' : np.linspace(0.01, 0.1, 11),\n        'n_estimators' : np.linspace(300, 700, 6).astype(int)\n    })\n\n# {'learning_rate': 0.064, 'loss': 'huber', 'n_estimators': 620}","b6b39d68":"# Grid search for BayesianRidge\n#if (make_search):\n#    grid(BayesianRidge()).grid_get(dataset, y_train_transformed, {\n#        'alpha_1' : np.linspace(1e-7, 1e-5, 101),\n#        'alpha_2' : np.linspace(1e-7, 1e-5, 101),\n#        'lambda_1': np.linspace(1e-7, 1e-5, 101),\n#        'lambda_2': np.linspace(1e-7, 1e-5, 101),\n#    })\n\n# Can't tune BayRidge for my PC crashes","544da0b6":"# Grid search for LGBMRegressor\nif (make_search):\n    grid(lgb.LGBMRegressor()).grid_get(dataset, y_train_transformed, {\n        'n_jobs'        : [-1],\n        'boosting_type' : ['gbdt'],\n        'num_leaves'    : np.linspace(1, 50, 11).astype(int),\n        'learning_rate' : np.linspace(0.01, 0.1, 11),\n        'n_estimators'  : np.linspace(100, 700, 6).astype(int)\n    })\n\n# {'boosting_type': 'gbdt', 'learning_rate': 0.028000000000000004, 'n_estimators': 700, 'n_jobs': -1, 'num_leaves': 5}","6c6625ef":"import xgboost as xgb\nfrom sklearn.pipeline import make_pipeline\n\ntrain_sets = [\n    X_train_rfecv,\n]\n\nbr = BayesianRidge()\nllic = LassoLarsIC()\ngb = GradientBoostingRegressor(learning_rate=0.064, loss='huber', n_estimators=620)\nkr = KernelRidge()\nlgbm = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.028000000000000004, n_estimators=700, n_jobs=-1, num_leaves=5)\nxgbr = xgb.XGBRegressor()\n\nother_models = [\n#     BayesianRidge(),                                                           # --> 0.0953\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), br),                  # 0.1010\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), br),                  # 0.1011\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), br),                # 0.1343\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), br),                  # 0.2745\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), br),                    # 0.1106\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), br),           # 0.1053\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), br),              # 0.1392\n\n#     LassoLarsIC(),                                                               # 0.1034\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), llic),                  # 0.1096\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), llic),                  # --> 0.1034\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), llic),                # 0.2979\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), llic),                  # 0.2979\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), llic),                    # 0.3098\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), llic),           # 0.1160\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), llic),              # 0.2901\n\n#     GradientBoostingRegressor(),                                               # --> 0.1002\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), gb),                  # 0.1020\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), gb),                  # 0.1044\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), gb),                # 0.1073\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), gb),                  # 0.1053\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), gb),                    # 0.1211\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), gb),           # 0.1034\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), gb),              # 0.1080\n\n#     KernelRidge(),                                                             # --> 0.0959\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), kr),                  # 0.2539\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), kr),                  # 0.1260\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), kr),                # 1.5792\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), kr),                  # 7.8296\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), kr),                    # 0.2113\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), kr),           # 0.3479\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), kr),              # 1.2862\n\n#     xgb.XGBRegressor(),                                                          # --> 0.0997\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), xgbr),                  # 0.1037\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), xgbr),                  # 0.1068\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), xgbr),                # 0.1075\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), xgbr),                  # 0.1097\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), xgbr),                    # 0.1270\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), xgbr),           # 0.1047\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), xgbr),              # 0.1069\n\n#     lgb.LGBMRegressor(),                                                         # 0.1024\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), lgbm),                  # --> 0.0999\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), lgbm),                  # 0.1031\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), lgbm),                # 0.1054\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), lgbm),                  # 0.1048\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), lgbm),                    # 0.1197\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), lgbm),           # 0.1016\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), lgbm),              # 0.1059\n    \n]\n\n# Evaluating models\nfor j in range(len(other_models)):\n    for i in range(len(train_sets)):\n        score = rmse_cv(other_models[j], train_sets[i], y_train_transformed, cv=2)\n        print (\"{} scored {:.4f}(+-{:.4f}) with the set #{}\".format(other_models[j], score.mean(), score.std(), i+1))","0019b217":"# Defining function for making models combinations\ndef make_combinations (iterable):\n    from itertools import combinations\n    my_combs = []\n    for item in iterable.copy():\n        iterable.remove(item)\n        for i in range(len(iterable)):\n            for comb in combinations(iterable, i+1):\n                my_combs.append((item, comb))\n        iterable.append(item)\n    return my_combs\n\nmodels = [\n    # 0.09534141759933776\n    BayesianRidge(),\n    # 0.10947055605680345\n    make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LassoLarsIC()),\n    # 0.09675792855029856\n    GradientBoostingRegressor(learning_rate=0.064, loss='huber', n_estimators=620),\n    # 0.09594388448467231\n    KernelRidge(),\n    # 0.09972041539723736\n    xgb.XGBRegressor(),\n    # 0.09624848214154685\n    make_pipeline(MinMaxScaler(), PolynomialFeatures(2), lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.028000000000000004, n_estimators=700, n_jobs=-1, num_leaves=5))\n]\n\nmy_combs = make_combinations(models)\n\nprint (\"I have {} combinations to test!\".format(len(my_combs)))","4e427531":"# Testing every possible combination\nprint (\"Testing raw models...\")\ni = 0\nresults = []\nbest = 10000\n# with progressbar.ProgressBar(max_value=len(models)) as bar:\n#     for model in models:\n#         X = chosen_X_train#.values\n#         score = rmse_cv(model, X, y_train_transformed).mean()\n#         results.append(score)\n#         print (score)\n#         if (score < best):\n#             best = score\n#             best_model = model\n#         i+=1\n#         bar.update(i)","b3d528ca":"class CustomEnsemble (BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models, meta_model, scaler=MaxAbsScaler(), feature_generator=None):\n        self.models = models\n        if scaler:\n            if feature_generator:\n                self.meta_model = make_pipeline(scaler, feature_generator, meta_model)\n            else:\n                self.meta_model = make_pipeline(scaler, meta_model)\n        else:\n            if feature_generator:\n                self.meta_model = make_pipeline(feature_generator, meta_model)\n            else:\n                self.meta_model = meta_model\n    def fit(self,X,y):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            model.fit (X, y)\n            predictions[:,i] = model.predict(X)\n        self.meta_model.fit(predictions, y)\n    def predict(self,X):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            predictions[:,i] = model.predict(X)\n        return self.meta_model.predict(predictions)\n    def __str__ (self):\n        return \"<CustomEnsemble (meta={}, models={})>\".format(self.meta_model, self.models)\n    def __repr__ (self):\n        return self.__str__()\n\nmodels = [\n    # 0.09534141759933776\n    BayesianRidge(),\n    # 0.09594388448467231\n    KernelRidge(),\n    # 0.09624848214154685\n    make_pipeline(MinMaxScaler(), PolynomialFeatures(2), lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.028000000000000004, n_estimators=700, n_jobs=-1, num_leaves=5))\n]\nmy_combs = make_combinations(models)\nprint (\"I chose {} combinations to test!\".format(2*len(my_combs)))\n    \n# Testing every possible combination\n# print (\"Testing combinations...\")\n# i = 0\n# results = []\n# best = 10000\n# with progressbar.ProgressBar(max_value=2*len(my_combs)) as bar:\n#     for comb in my_combs:\n#         X = chosen_X_train#.values\n#         stack_model = CustomEnsemble(list(comb[1]), comb[0])\n#         score = rmse_cv(stack_model, X, y_train_transformed).mean()\n#         results.append(score)\n#         #print (\"Score: {}\".format(score))\n#         if (score < best):\n#             print (\"Score {:.4f} is better than previous best. Saving...\".format(score))\n#             best = score\n#             best_model = stack_model\n#         i+=1\n#         bar.update(i)\n#         stack_model = CustomEnsemble(list(comb[1]), comb[0], scaler=None)\n#         score = rmse_cv(stack_model, X, y_train_transformed).mean()\n#         results.append(score)\n#         #print (\"Score: {}\".format(score))\n#         if (score < best):\n#             print (\"Score {:.4f} is better than previous best. Saving...\".format(score))\n#             best = score\n#             best_model = stack_model\n#         i+=1\n#         bar.update(i)\n\n# === Best model was:\n# <CustomEnsemble (meta=Pipeline(memory=None,\n#          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),\n#                 ('kernelridge',\n#                  KernelRidge(alpha=1, coef0=1, degree=3, gamma=None,\n#                              kernel='linear', kernel_params=None))],\n#          verbose=False), models=[Pipeline(memory=None,\n#          steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n#                 ('polynomialfeatures',\n#                  PolynomialFeatures(degree=2, include_bias=True,\n#                                     interaction_only=False, order='C')),\n#                 ('lgbmregressor',\n#                  LGBMRegressor(boosting_type='gbdt', class_weight=None,\n#                                colsample_bytree=1.0, importance_type='split',\n#                                learning_rate=0.028000000000000004, max_depth=-1,\n#                                min_child_samples=20, min_child_weight=0.001,\n#                                min_split_gain=0.0, n_estimators=700, n_jobs=-1,\n#                                num_leaves=5, objective=None, random_state=None,\n#                                reg_alpha=0.0, reg_lambda=0.0, silent=True,\n#                                subsample=1.0, subsample_for_bin=200000,\n#                                subsample_freq=0))],\n#          verbose=False), BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n#               fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n#               normalize=False, tol=0.001, verbose=False)])>\nbest_model = CustomEnsemble(\n    models = [\n        make_pipeline(MinMaxScaler(), PolynomialFeatures(2), lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.028000000000000004, n_estimators=700, n_jobs=-1, num_leaves=5)),\n        BayesianRidge()\n    ],\n    meta_model = KernelRidge(),\n    scaler = MaxAbsScaler()\n)\nbest = 0.09228172938603223","0485bdbf":"print (\"And the best model goes to...\")\nprint (best_model)\nprint (\"Its score was {}\".format (best))","e3065b03":"from scipy.special import inv_boxcox1p\n\nbest_model.fit(chosen_X_train, y_train_transformed)\n\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id']\nsub['SalePrice'] = inv_boxcox1p(best_model.predict(chosen_X_test), bclambda)\n#sub['SalePrice'] = inv_boxcox1p(best_model.predict(chosen_X_test.values), bclambda)\nsub.to_csv('submission.csv',index=False)","dcb00dcf":"# Feature selection","d9f25ad5":"# Split","c88da9a9":"# Model stacking and choosing best combo","005631b0":"# Few improvements using scalers and feature generators","0f4b7f66":"# Selecting few models","7e0b19d1":"# Hyper-params tuning"}}