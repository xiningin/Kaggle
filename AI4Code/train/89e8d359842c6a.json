{"cell_type":{"ca670107":"code","6e5d8058":"code","0302793d":"code","a8f46f5f":"code","920f4aa0":"code","ddedbefb":"code","07d464cd":"code","2906ac6b":"code","b57db27a":"code","a2969a12":"code","cdf18871":"code","b873361d":"code","2fdec38f":"code","c184eceb":"code","b0b3d7d0":"code","384b79cc":"code","76794b91":"code","8809342c":"code","22576816":"code","4ae07716":"code","51cc2873":"code","897b9fdc":"code","d0404da7":"code","d0f47c9b":"code","151f1928":"code","57a3b782":"code","53e4c32f":"code","36c2b0a7":"code","0a1a9417":"code","37d3f821":"code","5ddb9fc7":"code","599abe0f":"code","81a7b7dd":"code","329c8ef5":"code","e03297c7":"code","ffd4352c":"code","63df8d35":"code","66822736":"code","67a6156d":"code","c63a4f7d":"code","f1469163":"code","0207fd4c":"code","3fa08029":"code","02472388":"code","a0b1a708":"code","3dfb6868":"code","b0e9922b":"code","7d14aa55":"code","1469da68":"code","4b777e08":"code","a2d50c2f":"code","036a9fe3":"code","99d0e078":"code","1c849732":"code","527bce09":"code","5f15e310":"code","58e0b29e":"code","953d4fe5":"markdown","d6cdefbd":"markdown","f654ad3f":"markdown","8496a255":"markdown","91825da2":"markdown","9ebf091b":"markdown","f17d1dd1":"markdown","08b4345c":"markdown","9aa63fdb":"markdown","9bff3dbc":"markdown","fc4414e9":"markdown","c90220ca":"markdown"},"source":{"ca670107":" # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e5d8058":"import time\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.width', 500)","0302793d":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntrain.head()","a8f46f5f":"train.shape","920f4aa0":"train.isnull().any().sum()","ddedbefb":"test = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\ntest.head()","07d464cd":"test.shape","2906ac6b":"test.isnull().any().sum()","b57db27a":"sample_sub = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\nsample_sub.head()","a2969a12":"df = pd.concat([train, test], sort=False)\ndf.head()","cdf18871":"df.shape","b873361d":"df.isnull().sum()","2fdec38f":"df[\"date\"].min(), df[\"date\"].max()","c184eceb":"df['store'].nunique(), df['item'].nunique()","b0b3d7d0":"df.groupby('store').agg({'item':'count'})","384b79cc":"df.groupby('store').agg({'sales' : 'mean'}).sort_values('sales', ascending = False)","76794b91":"df.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","8809342c":"df.date","22576816":"df.date.dt.weekday","4ae07716":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek + 1\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4  \n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)  \n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)   \n    return df\n\n\ndf = create_date_features(df)\ndf.head()","51cc2873":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","897b9fdc":"df['sales']","d0404da7":"df.groupby([\"store\", \"item\"])['sales']","d0f47c9b":"pd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"lag1\": df[\"sales\"].shift(3).values[0:10],\n              \"lag2\": df[\"sales\"].shift(2).values[0:10],\n              \"lag3\": df[\"sales\"].shift(3).values[0:10],\n              \"lag4\": df[\"sales\"].shift(4).values[0:10]})","151f1928":"def lag_features(dataframe, lags):\n    dataframe = dataframe.copy()\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","57a3b782":"df.columns","53e4c32f":"df.head()","36c2b0a7":"df[\"sales\"].head(10)","0a1a9417":"pd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].shift(1).rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].shift(1).rolling(window=5).mean().values[0:10]})\n","37d3f821":"def roll_mean_features(dataframe, windows):\n    dataframe = dataframe.copy()\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(dataframe)\n    return dataframe\n\ndf = roll_mean_features(df, [365, 546]) \n\ndf.head()","5ddb9fc7":"pd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"ewm099\": df[\"sales\"].shift(1).ewm(alpha=0.99).mean().values[0:10],\n              \"ewm095\": df[\"sales\"].shift(1).ewm(alpha=0.95).mean().values[0:10],\n              \"ewm07\": df[\"sales\"].shift(1).ewm(alpha=0.7).mean().values[0:10],\n              \"ewm01\": df[\"sales\"].shift(1).ewm(alpha=0.1).mean().values[0:10]})","599abe0f":"def ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]","81a7b7dd":"df = ewm_features(df, alphas, lags)\ndf.head()","329c8ef5":"df.shape","e03297c7":"df.head() ","ffd4352c":"df.columns","63df8d35":"columns = ['store', 'item', 'day_of_week', 'month', 'is_wknd', 'is_month_start', 'is_month_end']","66822736":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, columns)","67a6156d":"df.head()","c63a4f7d":"df['sales'] = np.log1p(df[\"sales\"].values)","f1469163":"def smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num\/denom))\/n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","0207fd4c":"train[\"date\"].min(), train[\"date\"].max()","3fa08029":"test[\"date\"].min(), test[\"date\"].max()","02472388":"# train data set\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\ntrain.head()","a0b1a708":"# Validation data set\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\nval.head()","3dfb6868":"cols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]","b0e9922b":"# X_train, Y_train, x_test, y_test\nY_train = train['sales']\nX_train = train[cols]\n\nY_train.shape, X_train.shape","7d14aa55":"Y_val = val['sales']\nX_val = val[cols]\n\nY_val.shape, X_val.shape","1469da68":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 10000,\n              'early_stopping_rounds': 900,\n              'nthread': -1}","4b777e08":"lgbtrain = lgb.Dataset(data=X_train, label = Y_train, feature_name = cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)","a2d50c2f":"model = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],  \n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=2500) ","036a9fe3":"y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","99d0e078":"def plot_lgb_importances(model, plot=False, num=10):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\n\nplot_lgb_importances(model,plot=True, num=30)\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")","1c849732":"train = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}","527bce09":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nfinal_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\ntest_preds = final_model.predict(X_test, num_iteration=model.best_iteration)","5f15e310":"submission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)","58e0b29e":"submission_df.head(20)","953d4fe5":"# Final Model","d6cdefbd":"# Exponentially Weighted Mean Features","f654ad3f":"## Time-Based Validation Sets","8496a255":"# Lag\/Shifted Features","91825da2":"# Random Noise","9ebf091b":"# Custom Cost Function","f17d1dd1":"# One-Hot Encoding","08b4345c":"# LightGBM Model","9aa63fdb":"# Rolling Mean Features","9bff3dbc":"# Exploratory Data Analysis","fc4414e9":"# Converting sales to log(1+sales)","c90220ca":"# MODEL VALIDATION"}}