{"cell_type":{"e2ece66a":"code","7bd84556":"code","fae42941":"code","cebca9b6":"code","7ff54cda":"code","10fef382":"code","5dcbeee6":"code","e735df3c":"code","0909a49d":"code","be25c553":"code","57121c79":"code","a86a3e6b":"code","8fcafbff":"code","c928a44e":"code","3cb214f6":"code","89da0a62":"code","1c23d387":"code","24e541e1":"code","fca18649":"code","6d06ef2d":"code","fb9a31ed":"code","50625400":"code","b31210eb":"code","8e6fe49a":"code","77635055":"code","1c34a85a":"code","66504e11":"code","4f514a77":"code","156cd725":"code","28c6937f":"code","225f1497":"code","7416d49c":"code","00326e1c":"code","d5e8969e":"code","fad5662f":"code","3310e274":"code","df24cd39":"code","9bd41d06":"code","8e4710a7":"code","d3e181bd":"code","db6066b5":"code","85a598b9":"code","d2b43d40":"code","86d14ded":"code","ff1b92db":"code","ea84dd61":"code","56a22ba8":"code","a2009efc":"code","5d3bfc43":"code","20530c17":"code","6fb638b6":"code","af82a0df":"code","a405dd13":"code","b4b6815f":"code","50fc9aaf":"code","76972e23":"code","55c80758":"code","e7f08ec4":"code","ec6b8ca8":"code","ed4a1c1b":"code","47a2e126":"code","eeabea9e":"code","1a747feb":"code","a2790cab":"code","47f9ef28":"code","d02f4adb":"code","4159a24b":"code","359bd808":"code","f3dd0d8c":"code","c10445e4":"code","a67f0944":"code","1542e7a3":"code","dd9c44b5":"code","748dd3c7":"code","18e1a454":"code","28415db9":"code","9bb951eb":"code","52bdda6d":"code","acbf3c75":"code","4b6df487":"code","b08f01bd":"code","22fcba1c":"code","3498d2c1":"code","74f11fd7":"code","3868754a":"markdown","b3dda686":"markdown","d26caff7":"markdown","b82afb1e":"markdown","eedf4223":"markdown","b01abef8":"markdown","bf2815a9":"markdown","fc9dabda":"markdown","24c80169":"markdown","1746b926":"markdown","7570c7e5":"markdown","de7a796c":"markdown","f863b535":"markdown","dcaf38e1":"markdown","1bf3711f":"markdown","4eca9079":"markdown","a9256f2c":"markdown","1510819a":"markdown","22d4c2cb":"markdown","f0539b56":"markdown","7f97b77a":"markdown","eb8a83a3":"markdown","f568d1f6":"markdown","7fdef33b":"markdown","4d1ce6f9":"markdown","1025d8b0":"markdown","ff911b28":"markdown","528fe1c9":"markdown","0691d51b":"markdown","5d1bbd05":"markdown","f61be131":"markdown","9814f33f":"markdown","3aed5870":"markdown","7a624109":"markdown","c671cfb6":"markdown","4c7af975":"markdown","5580a2d5":"markdown","a926006c":"markdown","78a4ec3f":"markdown","f82c909a":"markdown","5541d9a3":"markdown","0d092ea0":"markdown","82c86b05":"markdown"},"source":{"e2ece66a":"project_name = 'emotional-speech-classification2d-resnet-demonstration'","7bd84556":"# Uncomment the following line to run in Google Colab\n\n# CPU:\n# !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\n# GPU:\n# !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\n# For interactive demo at the end:\n# !pip install pydub","fae42941":"# !chmod 600 kaggle.json\n# !pip install opendatasets --upgrade --quiet","cebca9b6":"# import opendatasets as od\n# import os\n\n# def download(url):\n#   #preventing multiple downloads and unziping\n#   folder_name = url.split(\"\/\")[-1]\n#   if os.path.isdir(folder_name):\n#     print(\"Already Exists !\")\n#     return\n#   od.download(url)\n\n# dataset_url = 'https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio'\n# download(dataset_url)\n# dataset_url = 'https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-song-audio'\n# download(dataset_url)\n","7ff54cda":"import os\n# data_dir = '.\/ravdess-emotional-speech-audio'\nspeech_dir = '..\/input\/ravdess-emotional-speech-audio'\nsong_dir = '..\/input\/ravdess-emotional-song-audio'\n[dir for dir in os.listdir(speech_dir) if dir.startswith('Actor_')]","10fef382":"x = [dir for dir in os.listdir(song_dir) if dir.startswith('Actor_')]\nx","5dcbeee6":"import csv\nimport sys\nimport os\nimport random\nimport zipfile\n\nfrom copy import deepcopy\n\nimport torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as tt\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\n# from torchvision.utils import make_grid\n# from torch.utils.data import random_split\n\nfrom tqdm.notebook import tqdm # Visualize the progress per epoch\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\nfrom sklearn.model_selection import train_test_split","e735df3c":"classes_list = [\n  ['full-AV', 'video-only','audio-only'],\n  ['speech', 'song'],\n  ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised'],\n  ['normal', 'strong'], # NOTE: There is no strong intensity for the 'neutral' emotion.\n  [\"Kids are talking by the door\", \"Dogs are sitting by the door\"],\n  ['1st repetition', '2nd repetition'],\n  [\"male\", \"female\"],\n]","0909a49d":"def load_RAVDESinfo(data_dir, list_classes, X=None, Y=None):\n  \"\"\"\n  this function will return audio file PATH and labels seperately when data directory(`data_dir`) \n  and the class list(list_classes) are passed to it\n  \"\"\"\n  audio_dataset = list()\n\n  if not(X or Y):\n    X = list()\n    Y = list()\n  actors = [dir for dir in os.listdir(data_dir) if dir.startswith('Actor_')]\n  for dir in actors:\n    act_dir = os.path.join(data_dir,dir)\n    for wav in os.listdir(act_dir):\n      # getting labels form the encoded file names\n      label = [(int(i)-1) for i in wav.split('.')[0].split('-')]\n      # converting gender labels to only 0 and 1\n      label[-1] = 1 if label[-1]%2 else 0\n\n      l_text = []\n\n      # converting labels back to string\n      for i in range(0, len(label)):\n        l_text.append(list_classes[i][label[i]])\n        \n      # excluding nutral, disgust and surprise Emotion\n      if(l_text[2] == \"neutral\" or l_text[2] == \"disgust\" or l_text[2] == \"surprised\"):\n        continue\n      X.append(os.path.join(act_dir, wav))\n      Y.append(l_text)\n  \n  return X,Y","be25c553":"speech_info = load_RAVDESinfo(speech_dir,classes_list)\nprint(f\"length of : files - {len(speech_info[0])}, labels - {len(speech_info[1])}\")\nsong_speech_info=load_RAVDESinfo(song_dir,classes_list, *speech_info)\nprint(f\"length of : files - {len(song_speech_info[0])}, labels - {len(song_speech_info[1])}\")","57121c79":"song_speech_info[0][14], song_speech_info[1][14]","a86a3e6b":"!pip install jovian --upgrade -q\nimport jovian","8fcafbff":"def test_trainSplit(dir_list,class_list,test_size=0.3):\n  \"\"\"\n  this function given the list of directories `dir_list` and `test_size` writes out \n  `test.csv` and `train.csv` with the help of the previous function `load_RAVDESinfo`. \n  the function will not overwrite anyfile if present in the same directory.\n  \"\"\"\n  if os.path.isfile('test.csv') or os.path.isfile('train.csv'):\n    print (\"csv files exist\")\n    return\n  \n  print (\"csv files do not exist\\n creating Test, Train Dataset(train.csv, test.csv)\")\n  X = None\n  Y = None\n  for dir in dir_list:\n    X,Y = load_RAVDESinfo(dir,class_list,X,Y)\n\n  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=test_size, random_state=42)\n\n  test_audios = [] \n  train_audios = [] \n  \n  label_headers = ['Audio_file', 'modality', 'vocal_channel', 'emotion', 'emotional_intensity', 'statement', 'repetition', 'gender']\n  \n  with open('train.csv', 'w+') as train:\n    train_write = csv.writer(train)\n    train_write.writerow(label_headers)\n    for audio, label in zip(X_train, Y_train):\n      row = [audio]\n      row.extend(label)\n      train_write.writerow(row)\n        \n  with open('test.csv', 'w+') as test:\n    test_write = csv.writer(test)\n    test_write.writerow(label_headers)\n    for audio, label in zip(X_test, Y_test):\n      row = [audio]\n      row.extend(label)\n      test_write.writerow(row)","c928a44e":"# # once csv files are created We no longer need to execute this functtion again \n# !rm *.csv\n# test_trainSplit([speech_dir,song_dir],classes_list,test_size=320)\n\n# jovian.commit(project=project_name, files=['test.csv', 'train.csv'])","3cb214f6":"# test_csv  = '.\/test.csv'\n# train_csv = '.\/train.csv'\ntest_csv = '..\/input\/ravdessemotionalspeech-songtrain-testcsv\/test.csv'\ntrain_csv = '..\/input\/ravdessemotionalspeech-songtrain-testcsv\/train.csv'","89da0a62":"test_dataframe = pd.read_csv(test_csv)\ntrain_dataframe = pd.read_csv(train_csv)\nlen(test_dataframe), len(train_dataframe)","1c23d387":"test_dataframe.sample(3)","24e541e1":"train_dataframe.sample(3)","fca18649":"fig = plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nplt.title('Count of Emotions in test DATA', size=16)\nsns.countplot(x=test_dataframe.emotion)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\n# plt.show()\n\nplt.subplot(1,2,2)\nplt.title('Count of Emotions in train DATA', size=16)\nsns.countplot(x=train_dataframe.emotion)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","6d06ef2d":"def check_channel(Dataframe):\n  \"\"\"\n  this function will take a dataframe as input and loop throught it searching for multichannel audio, \n  if found it will print the shape as a Tensor and its index in the dataframe\n  \"\"\"\n  for i,audio_file in enumerate(Dataframe[\"Audio_file\"]):\n#     fname = os.path.join(data_dir, audio_file)\n    data, rate = torchaudio.load(audio_file)\n    data_shape = list(data.shape)\n\n    if data_shape[0] != 1:\n      print(f\"index: {i}, shape : {data.shape}\")","fb9a31ed":"check_channel(test_dataframe)","50625400":"check_channel(train_dataframe)","b31210eb":"def load_mono(audio_path):\n  wave, sr = librosa.load(audio_path,sr=None,mono=True)\n  \n  # trim silent edges\n  wave, _ = librosa.effects.trim(wave)\n\n  # convert to tensor\n  wave = torch.FloatTensor(wave).unsqueeze(0)\n  return wave, sr","8e6fe49a":"def show_audio(audio_path):\n  y, sr = librosa.load(audio_path, sr=None,mono=True)\n  print(f\"Sample rate : {sr}\")\n  \n  # trim silent edges\n  audio, _ = librosa.effects.trim(y)\n  \n  fig = plt.figure(figsize=(20,15))\n  n_fft = 2048\n  hop_length = 256\n  n_mels = 128\n\n  plt.subplot(3,3,1)\n  librosa.display.waveplot(audio, sr=sr);\n  plt.title('1. raw wave form data');\n\n  plt.subplot(3,3,2)\n  D = np.abs(librosa.stft(audio[:n_fft], n_fft=n_fft, hop_length=n_fft+1))\n  plt.plot(D);\n  plt.title(f'2. fourier transform of a window(length={n_fft})');\n\n  plt.subplot(3,3,3)\n  D = np.abs(librosa.stft(audio, n_fft=n_fft,  hop_length=hop_length))\n  librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear');\n  plt.colorbar();\n  plt.title('3. applyed the Fourier Transform');\n\n  plt.subplot(3,3,4)\n  DB = librosa.amplitude_to_db(D, ref=np.max)\n  librosa.display.specshow(DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log');\n  plt.colorbar(format='%+2.0f dB');\n  plt.title('4. Spectrogram');\n\n  mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\n  \n  plt.subplot(3,3,5);\n  librosa.display.specshow(mel, sr=sr, hop_length=hop_length, x_axis='linear');\n  plt.ylabel('Mel filter');\n  plt.colorbar();\n  plt.title('5. Our filter bank for converting from Hz to mels.');\n\n  plt.subplot(3, 3, 6);\n  mel_10 = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=10)\n  librosa.display.specshow(mel_10, sr=sr, hop_length=hop_length, x_axis='linear');\n  plt.ylabel('Mel filter');\n  plt.colorbar();\n  plt.title('6. Easier to see what is happening with only 10 mels.');\n\n  plt.subplot(3, 3, 7);\n  idxs_to_plot = range(0,127,10)\n  for i in idxs_to_plot:\n      plt.plot(mel[i]);\n  plt.legend(labels=[f'{i+1}' for i in idxs_to_plot]);\n  plt.title('6. Plotting some of the triangular filters from the mels');\n\n  plt.subplot(3,3,8)\n  plt.plot(D[:, 1]);\n  plt.plot(mel.dot(D[:, 1]));\n  plt.legend(labels=['Hz', 'mel']);\n  plt.title('8. One sampled window for example, before and after converting to mel.');\n\n  plt.subplot(3,3,9)\n  S = librosa.feature.melspectrogram(audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n  S_DB = librosa.power_to_db(S, ref=np.max)\n  librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel');\n  plt.colorbar(format='%+2.0f dB');\n  plt.title('9. Mel - Spectrogram');\n\n  fig.tight_layout() \n  plt.show()","77635055":"path = test_dataframe.iloc[20][\"Audio_file\"]\nprint(test_dataframe.iloc[20][\"emotion\"])\nshow_audio(path)\n\n# Lets play the audio\ndata, rate = load_mono(path)\nipd.Audio(data=data.numpy(),rate=rate)","1c34a85a":"i = 606\npath = train_dataframe[\"Audio_file\"].iloc[i]\nprint(train_dataframe.iloc[i][\"emotion\"])\nshow_audio(path)\n\n# Lets play the audio \ndata, rate = load_mono(path)\nipd.Audio(data=data.numpy(),rate=rate)","66504e11":"emo_classes = list(set(train_dataframe[\"emotion\"]))\nlen(emo_classes), emo_classes","4f514a77":"def load_spec(audio_path, mode=0):\n  \"\"\"\n  takes audio path and mode to return various audio 2D representation with the \n  actual audio and sample rate as tensor\n\n  use mode=1 to get melspectrogram\n  and mode=2 to get mfcc\n  Default mode=0 for Spectrogram\n  \"\"\"\n  wave, sr = librosa.load(audio_path,sr=None,mono=True)\n  # trim silent edges(below 60 db by default), change the threashold by passing `top_db`\n  # The threshold (in decibels) below reference to consider as silence (default : 60 db)\n  s, _ = librosa.effects.trim(wave,top_db=60)\n  \n  # convert to tensor\n  wave = torch.FloatTensor(s).unsqueeze(0)\n    \n  # generate (mel)spectrogram \/ mfcc\n  if(mode == 1):\n    # s = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(wave)\n    s = librosa.feature.melspectrogram(y=s, sr=sr, hop_length=512)\n  elif(mode == 2):\n    # s = torchaudio.transforms.MFCC(sample_rate=sr)(wave)\n    s = librosa.feature.mfcc(y=s, sr=sr, n_mfcc=40)\n  else:\n    # s = torchaudio.transforms.Spectrogram()(wave)\n    freqs, times, s = librosa.reassigned_spectrogram(y=s, sr=sr, hop_length=512)\n    \n  s = torch.FloatTensor(s).unsqueeze(0)\n  return s, wave, sr","156cd725":"i = 20\naudio_path = test_dataframe[\"Audio_file\"].iloc[i]\n\nSPECTROGRAM, audio, rate = load_spec(audio_path,mode=0)\nprint(test_dataframe[\"emotion\"].iloc[i])\nprint(audio.shape)\nprint(SPECTROGRAM.shape)\n\nplt.imshow(SPECTROGRAM.log10()[0,:,:].numpy(), cmap=\"inferno\")\nipd.Audio(data=audio,rate=rate)","28c6937f":"audio_path = test_dataframe[\"Audio_file\"].iloc[i]\nmel_spec, audio, rate = load_spec(audio_path,mode=1)\nprint(test_dataframe[\"emotion\"].iloc[i])\nprint(audio.shape)\nprint(mel_spec.shape)\n\nplt.imshow(mel_spec.log10()[0,:,:].numpy(), cmap=\"inferno\")\nipd.Audio(data=audio,rate=rate)","225f1497":"audio_path = test_dataframe[\"Audio_file\"].iloc[i]\nMFCC, audio, rate = load_spec(audio_path,mode=2)\nprint(test_dataframe[\"emotion\"].iloc[i])\nprint(audio.shape)\nprint(MFCC.shape)\n\nplt.imshow(MFCC[0,:,:].numpy(), cmap=\"inferno\")\nipd.Audio(data=audio,rate=rate)","7416d49c":"def get_maxDim(csv_file,max_height=0,max_width=0,mode =0, verbose=False):\n  min_width = min_height = 5e5\n#   stdv = []\n#   med = []\n#   mean = [] \n    \n  dataframe = pd.read_csv(csv_file)\n  for i,path in enumerate(dataframe[\"Audio_file\"]):\n\n    spec,_,_ = load_spec(path,mode=mode)\n    \n#     # calc std\n#     stdv.append(torch.std(spec))\n#     med.append(torch.median(spec))\n#     mean.append(torch.mean(spec))\n    \n    _, height, width = list(spec.shape) \n    if(height > max_height):\n      max_height = height\n      if verbose:\n        print(f\"{i} max height - {height}\")\n    if(width > max_width):\n      max_width = width\n      if verbose:\n        print(f\"{i} max width - {width}\")\n    \n    # just printing \n    if(min_width > width):\n      min_width = width\n      if verbose:\n        print(f\"{i} min width - {min_width}\")\n    if(min_height > height):\n      min_height = height\n      if verbose:\n        print(f\"{i} min height - {min_height}\")\n\n#   stdv = torch.FloatTensor(stdv)\n#   med = torch.FloatTensor(med)\n#   mean = torch.FloatTensor(mean)\n#   print(f\"\\tstdv : {torch.mean(stdv)},\\n\\tmedian : {torch.mean(med)}, \\n\\tmean: {torch.mean(mean)}\")\n  print(f\"min-width : {min_width},\\tmin-height : {min_height}\")\n  print(f\"max-width : {max_width},\\tmax-height : {max_height}\")\n\n  return max_height,max_width","00326e1c":"# mode : 0-spectrogram, 1-mel_spectrogram, 2-mfcc\n\nmax_dim = get_maxDim(test_csv,mode=1)\n# print(max_dim)\nmax_dim = get_maxDim(train_csv, *max_dim, mode=1)\n# print(max_dim)","d5e8969e":"max_dim","fad5662f":"# Data transforms (normalization & data augmentation)\n# stats = ((range of means for each channel), (range of std deviation for each channel))\n# stats = ((-1.5), (2.51))\ntrain_aug = tt.Compose([\n                         tt.RandomCrop((max_dim[0],(max_dim[0]*125)\/\/100), padding=12 ,pad_if_needed=True ,padding_mode='reflect'), \n                         tt.RandomHorizontalFlip(0.4),\n                         tt.RandomVerticalFlip(0.4),\n                         tt.RandomRotation(22.5),\n#                          tt.Normalize(*stats,inplace=True),\n                        ])\n# valid_tfms = tt.Compose([tt.Normalize(*stats)])","3310e274":"# def denormalize(images, means, stds):\n#     means = torch.tensor(means)\n#     stds = torch.tensor(stds)\n#     return images * stds + means\n#     return images\n# del denormalize","df24cd39":"audio_path = train_dataframe[\"Audio_file\"].iloc[606]\nmel_spec, audio, rate = load_spec(audio_path,mode=1)\nprint(audio.shape)\nprint(f\"mel-spec shape before:{mel_spec.shape}\")\n\n# print(f\"\\n before : \\n\\tmax :{torch.max(mel_spec)},\\n\\tmin:{torch.min(mel_spec)},\\n\\tmean:{torch.mean(mel_spec)},\\n\\tstd:{torch.std(mel_spec)}\\n\")\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.imshow(mel_spec.log10()[0,:,:].numpy(),cmap=\"inferno\")\n\nmel_spec = train_aug(mel_spec)\nprint(f\"mel-spec shape after :{mel_spec.shape}\")\n# print(f\"\\n after : \\n\\tmax :{torch.max(mel_spec)},\\n\\tmin:{torch.min(mel_spec)},\\n\\tmean:{torch.mean(mel_spec)},\\n\\tstd:{torch.std(mel_spec)}\\n\")\n# mel_spec = denormalize(mel_spec, *stats)\n\nplt.subplot(1,2,2)\nplt.imshow(mel_spec.log10()[0,:,:].numpy(),cmap=\"inferno\")\n","9bd41d06":"class ravdessEmoDataSet(Dataset):\n  def __init__(self, csv_file, dim, loader=None, mode=0):\n    self.dataframe = pd.read_csv(csv_file)\n    self.classes = list(set(self.dataframe[\"emotion\"]))\n    # self.classes = list(set(self.dataframe[\"gender\"]))\n    self.loader = loader\n    self.dim = dim\n    self.mode = mode\n\n  def __len__(self):\n    return len(self.dataframe)\n\n  def __getitem__(self, index):\n    row = self.dataframe.loc[index]\n    audio_path,_,_, emotion, emo_intensity,_,_, gender = row\n    \n    label = self.classes.index(emotion)\n    # label = self.classes.index(gender)\n\n    s, waveform, rate = load_spec(audio_path,mode=self.mode)\n\n    if self.loader:\n      s = self.loader(s)\n    else:\n      _, height, width = list(s.shape)\n      diff = [self.dim[0] - height, self.dim[1] - width]\n      # pad on both side of the tensor by zeros\n      pd1 = (diff[1]\/\/2, diff[1]-diff[1]\/\/2, diff[0]\/\/2,diff[0]-diff[0]\/\/2)\n      s = F.pad(s, pd1, mode='constant', value=0)\n\n    return s, label","8e4710a7":"test_dataset = ravdessEmoDataSet(test_csv, dim = max_dim, mode=0)\n# train_dataset = ravdessEmoDataSet(train_csv, data_dir, dim = max_dim, mode=0)\ntrain_dataset = ravdessEmoDataSet(train_csv, dim = max_dim, mode=0, loader=train_aug)\nlen(test_dataset),len(train_dataset)","d3e181bd":"x,l = train_dataset[129]\nprint(f\"spectrogram shape {x.shape}\")\nprint(f\"label : {train_dataset.classes[l]}\")\n# x = denormalize(x, 0.28, 3.52)\nplt.figure(figsize=(15,5))\nplt.imshow(x.log10()[0,:,:].numpy(),cmap=\"inferno\")","db6066b5":"x,l = test_dataset[3]\nprint(f\"spectrogram shape {x.shape}\")\nprint(f\"label : {train_dataset.classes[l]}\")\n# x = denormalize(x, 0.28, 3.52)\nplt.figure(figsize=(15,5))\nplt.imshow(x.log10()[0,:,:].numpy(),cmap=\"inferno\")","85a598b9":"batch_size = 40\n\ntrain_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_dl = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)","d2b43d40":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass SpectrogramClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n\n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, \"last_lr: {},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n            result['train_loss'], result['val_loss'], result['val_acc']))","86d14ded":"class EmotionalResnet18(SpectrogramClassificationBase):\n    def __init__(self, in_channels,num_classes, pretrained=True):\n        super().__init__()\n        # Use a pretrained model\n        self.network = torchvision.models.resnet18(pretrained)\n        \n        # Replace the first layer\n        self.network.conv1 = nn.Conv2d( \n            in_channels,\n            self.network.conv1.out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=3\n        )\n        \n        # Replace last layer\n        self.network.fc = nn.Linear(self.network.fc.in_features, 512)\n        # new layer introduced\n        self.fc2 = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(512, num_classes),\n            nn.Softmax(dim=1),\n        )\n            \n\n    def forward(self, xb):\n        out = self.network(xb)\n        return self.fc2(out)","ff1b92db":"class_len= len(test_dataset.classes)\nclass_len","ea84dd61":"model = EmotionalResnet18(1,class_len,pretrained=True)\nmodel.parameters","56a22ba8":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n#         for batch in tqdm(train_loader):\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n\n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n                                                steps_per_epoch=len(train_loader),\n                                                pct_start=0.20)\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        lrs = []\n#         for batch in tqdm(train_loader):\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","a2009efc":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","5d3bfc43":"device = get_default_device()\ndevice","20530c17":"train_dl = DeviceDataLoader(train_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)\nmodel = to_device(model,device)","6fb638b6":"## if out of memory error hapens try uncommenting this cell\n# # Data to delete to release memory \n# del speech_info\n# del song_speech_info\n# del train_dataframe,\n# del SPECTROGRAM\n# del audio\n# del mel_spec\n# del MFCC\n# del test_dataset\n# del train_dataset\n# del train_dl\n# del test_dl\n# del model\n\n# del load_RAVDESinfo\n# del test_trainSplit\n# del check_channel\n# del load_mono\n# del show_audio","af82a0df":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","a405dd13":"# %%time\n# history += fit(epochs,max_lr,model,train_dl,test_dl,opt_func=opt_func)","b4b6815f":"epochs = 50\nmax_lr = 1e-5\ngrad_clip = 0.1\nweight_decay = 1e-5\nopt_func = torch.optim.Adam","50fc9aaf":"%%time\nM = 0 # Spectrogram\n\nspectro_max_dim = get_maxDim(test_csv,mode=M)\nspectro_max_dim = get_maxDim(train_csv, *spectro_max_dim, mode=M)\n\ntest_dataset = ravdessEmoDataSet(test_csv, dim = spectro_max_dim, mode=M)\ntrain_dataset = ravdessEmoDataSet(train_csv, dim = spectro_max_dim, mode=M)\nprint(f\" length : \\n\\ttest:{len(test_dataset)},\\n\\ttrain:{len(train_dataset)}\")\n\nbatch_size = 20\n\ntrain_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_dl = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)\n\nclass_len= len(test_dataset.classes)\nprint(f\"class lenght : {class_len}\")\n\nspectro_model = EmotionalResnet18(1,class_len,pretrained=True)\n# model.load_state_dict(torch.load(\"..\/input\/emotional-speech-classification2d\/EAC-spectro_model.pth\"))\n\ndevice = get_default_device()\nprint(f\"device : {device}\")\n\ntrain_dl = DeviceDataLoader(train_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)\nspectro_model = to_device(spectro_model,device)\n\nhistory = [evaluate(spectro_model, test_dl)]\nprint(history)","76972e23":"%%time\nhistory += fit_one_cycle(epochs, max_lr, spectro_model, train_dl, test_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","55c80758":"train_time0 = \"~1h\"","e7f08ec4":"fig = plt.figure(figsize=(15,7))\n\nplt.subplot(2,2,1)\nplot_accuracies(history)\n\nplt.subplot(2,2,2)\nplot_losses(history)\n\nplt.subplot(2,2,3)\nplot_lrs(history)\n\nfig.tight_layout()\nplt.show()","ec6b8ca8":"torch.save(spectro_model.state_dict(), 'EAC-spectro_model.pth')\n# jovian.commit(project=project_name, outputs=['EAC-spectro_model.pth'],files=[test_csv,train_csv], environment=None)","ed4a1c1b":"%%time\n\nM = 1 # mel-spectrogram\n\nmel_max_dim = get_maxDim(test_csv,mode=M)\nmel_max_dim = get_maxDim(train_csv, *mel_max_dim, mode=M)\n\ntest_dataset = ravdessEmoDataSet(test_csv, dim = mel_max_dim, mode=M)\ntrain_dataset = ravdessEmoDataSet(train_csv, dim = mel_max_dim, mode=M)\nprint(f\" length : \\n\\ttest:{len(test_dataset)},\\n\\ttrain:{len(train_dataset)}\")\n\nbatch_size = 80\n\ntrain_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_dl = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)\n\nclass_len= len(test_dataset.classes)\nprint(f\"class lenght : {class_len}\")\n\nmelSpectro_model = EmotionalResnet18(1,class_len,pretrained=True)\n# model.load_state_dict(torch.load(\".\/EAC-melSpectro_model.pth\"))\n\ndevice = get_default_device()\nprint(f\"device : {device}\")\n\ntrain_dl = DeviceDataLoader(train_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)\nmelSpectro_model = to_device(melSpectro_model,device)\n\n\nhistory = [evaluate(melSpectro_model, test_dl)]\nprint(history)","47a2e126":"%%time\nhistory += fit_one_cycle(epochs, max_lr, melSpectro_model, train_dl, test_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","eeabea9e":"train_time1 = \"~45min\"","1a747feb":"fig = plt.figure(figsize=(15,7))\n\nplt.subplot(2,2,1)\nplot_accuracies(history)\n\nplt.subplot(2,2,2)\nplot_losses(history)\n\nplt.subplot(2,2,3)\nplot_lrs(history)\n\nfig.tight_layout() \nplt.show()","a2790cab":"torch.save(melSpectro_model.state_dict(), 'EAC-melSpectro_model.pth')\n# jovian.commit(project=project_name, outputs=['EAC-melSpectro_model.pth'],files=[test_csv,train_csv], environment=None)","47f9ef28":"%%time\nM = 2 # mfcc\n\nmfcc_max_dim = get_maxDim(test_csv,mode=M)\nmfcc_max_dim = get_maxDim(train_csv, *mfcc_max_dim, mode=M)\n\ntest_dataset = ravdessEmoDataSet(test_csv, dim = mfcc_max_dim, mode=M)\ntrain_dataset = ravdessEmoDataSet(train_csv, dim = mfcc_max_dim, mode=M)\n# train_dataset = ravdessEmoDataSet(train_csv, dim = max_dim, mode=0, loader=train_aug)\nprint(f\" length : \\n\\ttest:{len(test_dataset)},\\n\\ttrain:{len(train_dataset)}\")\n\nbatch_size = 80\n\ntrain_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_dl = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)\n\nclass_len= len(test_dataset.classes)\nprint(f\"class lenght : {class_len}\")\n\nmfcc_model = EmotionalResnet18(1,class_len,pretrained=False)\n# model.load_state_dict(torch.load(\".\/EAC-mfcc_model.pth\"))\n\ndevice = get_default_device()\nprint(f\"device : {device}\")\n\ntrain_dl = DeviceDataLoader(train_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)\nmfcc_model = to_device(mfcc_model,device)\n\n\nhistory = [evaluate(mfcc_model, test_dl)]\nhistory","d02f4adb":"%%time\nhistory += fit_one_cycle(epochs, max_lr, mfcc_model, train_dl, test_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","4159a24b":"train_time2 = \"~25 mins\"","359bd808":"fig = plt.figure(figsize=(15,7))\n\nplt.subplot(2,2,1)\nplot_accuracies(history)\n\nplt.subplot(2,2,2)\nplot_losses(history)\n\nplt.subplot(2,2,3)\nplot_lrs(history)\n\nfig.tight_layout() \nplt.show()","f3dd0d8c":"torch.save(mfcc_model.state_dict(), 'EAC-mfcc_model.pth')\n# jovian.commit(project=project_name, outputs=['EAC-mfcc_model.pth'],files=[test_csv,train_csv], environment=None)","c10445e4":"device","a67f0944":"spectro_max_dim, mel_max_dim, mfcc_max_dim","1542e7a3":"test_dataset = ravdessEmoDataSet(test_csv, dim = spectro_max_dim, mode=0)\ntest_dl = DataLoader(test_dataset, 20, num_workers=4, pin_memory=True)\n\nclass_len= len(test_dataset.classes)\n# model = EmotionalResnet18(1,class_len,pretrained=False)\n# model.load_state_dict(torch.load(\".\/EAC-spectro_model.pth\"))\n# model.load_state_dict(torch.load(\"..\/input\/emotional-speech-classification2d\/EAC-spectro_model.pth\"))\n\ntest_dl = DeviceDataLoader(test_dl, device)\nmodel = to_device(spectro_model, device)\n\nevaluate(model, test_dl)","dd9c44b5":"def predict_emotion(audio_path, model, max_height=0, max_width=0, mode=0):\n    s, waveform, rate = load_spec(audio_path, mode = mode)\n\n    _, height, width = list(s.shape)\n    \n    diff = [(max_height - height), (max_width - width)]\n    # pad on both side of the tensor by zeros\n    pd1 = (diff[1]\/\/2, diff[1]-diff[1]\/\/2, diff[0]\/\/2,diff[0]-diff[0]\/\/2)\n    s = F.pad(s, pd1, mode='constant', value=0)\n    \n    # Convert to a batch of 1\n    xb = to_device(s.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    print(f\"predicted : {train_dataset.classes[preds[0].item()]}\")\n    return waveform,rate\n    # return preds[0].item()","748dd3c7":"from random import randrange","18e1a454":"len(test_dataframe)","28415db9":"# indx = 15\nindx = randrange(len(test_dataframe))\n\naudio_path = os.path.join(test_dataframe.iloc[indx][\"Audio_file\"])\n\nprint(f\"Audio Location : {audio_path}\")\nprint(f\"Label : {test_dataframe.iloc[indx]['emotion']}\")\n\nwave = predict_emotion(audio_path,model, *spectro_max_dim)\n\nipd.Audio(data=wave,rate=48000)","9bb951eb":"# indx = 150\nindx = randrange(len(test_dataframe))\naudio_path = os.path.join(test_dataframe.iloc[indx][\"Audio_file\"])\n\nprint(f\"Audio Location : {audio_path}\")\nprint(f\"Label : {test_dataframe.iloc[indx]['emotion']}\")\n\nwave = predict_emotion(audio_path,model, *spectro_max_dim)\n\nipd.Audio(data=wave,rate=48000)","52bdda6d":"# indx = 199\nindx = randrange(len(test_dataframe))\naudio_path = os.path.join(test_dataframe.iloc[indx][\"Audio_file\"])\n\nprint(f\"Audio Location : {audio_path}\")\nprint(f\"Label : {test_dataframe.iloc[indx]['emotion']}\")\n\nwave = predict_emotion(audio_path,model, *spectro_max_dim)\n\nipd.Audio(data=wave,rate=48000)","acbf3c75":"# indx = 315\nindx = randrange(len(test_dataframe))\naudio_path = os.path.join(test_dataframe.iloc[indx][\"Audio_file\"])\n\nprint(f\"Audio Location : {audio_path}\")\nprint(f\"Label : {test_dataframe.iloc[indx]['emotion']}\")\n\nwave = predict_emotion(audio_path,model, *spectro_max_dim)\n\nipd.Audio(data=wave,rate=48000)","4b6df487":"# !pip install jovian --upgrade --quiet","b08f01bd":"# import jovian","22fcba1c":"# jovian.reset()\n# jovian.log_hyperparams(arch='spectrogram wins again', \n#                        epochs=epochs, \n#                        lr=max_lr, \n#                        scheduler='one-cycle', \n#                        weight_decay=weight_decay, \n#                        grad_clip=grad_clip,\n#                        opt=opt_func.__name__)","3498d2c1":"# jovian.log_metrics(val_loss=history[-1]['val_loss'],val_acc=history[-1]['val_acc'],train_loss=history[-1]['train_loss'],time=train_time)\n\n# train_loss: 0.9483, val_loss: 1.1392, val_acc: 0.7906\n# jovian.log_metrics(val_loss = 1.1392, \n#                    val_acc = 0.7906,\n#                    train_loss = 0.9483,\n#                    time=train_time0)","74f11fd7":"# jovian.commit(project=project_name, outputs=['EAC-spectro_model.pth', 'EAC-melSpectro_model.pth', 'EAC-mfcc_model.pth'],files=[test_csv, train_csv], environment=None)","3868754a":"Lets check if all the audios have only one channel of audio","b3dda686":"Loading the audio in to pandas Dataframe\n\nSince I created the test and train set and saved it in the csv files, which I'm going to use here","d26caff7":"I will be usin the `fit_one_cycle` method to train the model, however we can do the training with constant `lr` by using the `fit` function","b82afb1e":"We can now wrap our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU (if available).","eedf4223":"But the test dataset or the valudation dataset does not have all these data-auugmentation but all the mel-spectrogram is being padded with zeros to make the shape of all the images same.","b01abef8":"## Evaluate and fit","bf2815a9":"# Emotional-speech-classification using PyTorch\n\nThe dataset is taken from kaggle : \n\nhttps:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio\n\nhttps:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-song-audio\n\n\nThe dataset don't come with seperate test dataset. \n\nSO for the sake of comparing result I created a test and train dataset and saved into files(`train.csv`, `test.csv`).\n\nIf you use the dataset please use the csv files I created so that we can **compare the models optimally**\n\nI also provided the code to create the test.csv and train.csv\nwhich you can find below","fc9dabda":"## Audio loader\n\nLoad the audio and return as a tensor. it will also turn multi-channel audio to mono.","24c80169":"## Summary : Choosing the MODE\n\nit was not clear which model should I choose to for prediction.\n\n**Spectrogram-dataset**\n\n- In accuracy spectrogarm-model was ahead of the others by more than 3-5% in case of using pretrained model\n- But it required the most RAM, TIME, computation to achieve it, \n- yet some degree of over fitting was noticeable after 20-25 epochs.\n\nHISTORY(last three): \n\n1. epochs: 50,accuracy : 0.7563, time : 1h 45min 55s, pretrained: True\n2. epochs : 50, accuracy: 0.2281 time : 1h 50min 39s,  pretrained: False\n3. epochs: 30, accuracy : 0.7094, time : 1h 3min 39s,  pretrained: True\n\n**Mel_Spectrogram-dataset**\n\n- In accuracy it was neck and neck with `mfcc-dataset` ahead by only 1%,\n- Though it had taken more time than `mfcc-dataset` but a lot less than `spetrogram-dataset`\n- On the UP side working with this dataset showed the least overfitting among the three \n\nHISTORY(last three):\n1. ecochs : 50, accuracy : 0.6687, time : 37min 10s, pretrained : True\n2. ecochs : 50, accuracy : 0.3844, time : 35min 1s, pretrained : False\n3. epochs : 30, accuracy : 0.6344, time : 21min 25s, pretrained : True\n\n**MFCC-dataset**\n\n- In time-consumption it is the lowest with a cost of bit accuracy\n- most of all it didn't need use the pretrained wights to get to over 65% accuracy\n- It has also shown the most ammount of overfitting after 25 epoch\n\nHISTORY(last three):\n1. epochs: 50, accuracy : 0.6594, time : 22min 43s, pretrained : True\n2. epochs: 50, accuracy : 0.6719, time : 22min 56s, pretrained : False\n3. epochs: 30, acccurcy : 0.5688, time : 13min 14s, pretrained : False\n\nMy expectation was the dataset with the mel-spectrogram will be the best in all categories.\n\nBut seeing all these first I thought `mfcc` is the best choice and it is the best I think, then ***chosen Spectrogram-data*** as it promised more accuracy than all the others and the the trade off for time seemed resomable. ","1746b926":"### training the model on mfcc","7570c7e5":"### Downloadin dataset from kaggle \n\nusing `opendatasets` library to do the job\n\n**NOTE :** you need to place your `kaggle.json` file into the `\/content`. To download your `kaggle.json` head over to https:\/\/www.kaggle.com ==> profile ==> Account ==> create api token. The rest is handled by the `code-cell` below Q\n\nNB: if the notebook is not running in kaggle, I'm on kaggle so I commented out this section.","de7a796c":"### Using PreTrained model Resnet18\n\nPreTrained turned out not so much useful for audio classification, as the model was not trained or intended to train on audio data.\n\nBut it is good enough to make decent prediction accuracy in Emotion Classification with audio only.","f863b535":"Hidden<!-- The colors seem out of place because of the normalization. Note that normalization is also applied during inference. If we look closely, we can see the cropping, reflection padding and rotation in some of the images. Horizontal flip and vertical flip are a bit difficult to detect from visual inspection. -->","dcaf38e1":"## Loading the data \n\nfirst the information is being loaded and saved in csv files in train test split. Then the csv files are loaded into pandas Dataframe.\n\nI noticed the emotion `nutral`, `disgust` and `surprise` has verry low occurance compared to the other emotions in the dataset. so I decided to not use them for this project as they were affecting the accuracy heavily. This is done after I saw this video on youtube [(A07) Speech Emotion Detection](https:\/\/youtu.be\/26_qiXEa8lw?t=1666) by \"Amazon re:MARS\"\n\n![ravdess-test-data.png](attachment:ravdess-test-data.png)![ravdess-train-data.png](attachment:ravdess-train-data.png)\n\nThe test train csv files will not contain thouse emotions.  ","1bf3711f":"## DataSets\n\nthe `init` functions takes `csv_file` as path of the csv file, `data_dir` is the folder path the data is located, `dim` is the max dimention present in the data(spectrogram), `loader` is the function which can be used to do data augmentation and normalization, `mode` is the mode option to be used in the `load_spec` function.(mode=1 : mel-spectrogram in log10 scale, mode=2 : mfcc ,default : mode = 0) \n\nAnother thing is loader is by default is none, in this case the spectograms will be all-side padded with Zeros to the max dim to make all the spectrogram tensor of same dimention.\n(I saw padding in both sides somehow reduced the overfitting issue I was geting before with only padding the right side when I was using mel-spectrogram) \n\nIf `loader` is passed the default padding wont take place instead the `loader` will take place of the default padding function, this loader functon need work with spectrograms(2d tensors).","4eca9079":"## Training The Model","a9256f2c":"Here I'm uploading the notebook with the outputs and csv files in jovian.\n\nI commneted out this section for the `Save and Run All(Commit)` in kaggle as the `jovian.commit` prompts for a api key which I can't pass during that time so it throws long errormessages which don't like. ","1510819a":"## The Model","22d4c2cb":"## Setting up the notebook","f0539b56":"## Data transforms (normalization & data augmentation)\n\napplying diffirent ***image*** transforms on the mel-spectrogram to randomly represent the same data to the model.\n\nI could not achive full normalization on the data, so removed it.\n\n**UPDATE : I tried to use image augmentation on the spectrogram of the images though it reduced overfitting in some cases but it turned out to be a failure as it prevented to gain more accuracy than 40%.**\nSo in the final version any data augmentation is not used.","7f97b77a":"### Training the model with Mel-Spectrogram","eb8a83a3":"We're now ready to train our model. Instead of SGD (stochastic gradient descent), we'll use the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. You can learn more about optimizers here: https:\/\/ruder.io\/optimizing-gradient-descent\/index.html","f568d1f6":"## Final Thoughts\n\nClearly the model and the methods used for data processing is not perfect. \n\nHowever I tried the image data augmentation on the Spectrograms(2d representations of audio) but it didn't went well. so I ended up with just a padding to match the size.\n\nIt has room for improvement, one thing I can think of is applying data augmentation on the audio. \n","7fdef33b":"I've made the csv files available in the [Kaggle](https:\/\/www.kaggle.com\/kuntaldas599\/ravdessemotionalspeech-songtrain-testcsv) and in [jovian](https:\/\/jovian.ai\/kuntal-das\/emotional-speech-classification2d-resnet\/v\/48\/files?filename=emotional-speech-classification2d-resnet.ipynb)","4d1ce6f9":"### Spectroram\n\nwhy Spectrogram You ask ?\n\n- Because I can use any Image classification model on it.\n\n- Because of this video -> https:\/\/youtu.be\/AqqaYs7LjlM?t=255 gave me enough boost to do so\n\nHow spectrogram is created ? \n![spec-creation](https:\/\/miro.medium.com\/max\/700\/1*tIBRdtG3EfjmSIlraWVIxw.png)","1025d8b0":"## Base class","ff911b28":"### objective of this notebook:\nuse the audio to generate a spectrogram and then use a nural network for images and pass the spectograms to the model.\n\nPreviously I tried a direct approach to the `emotional classification` which didn't turned out as I expected. link to the [notebook](https:\/\/jovian.ai\/kuntal-das\/emotional-speech-classification)","528fe1c9":"below I have plotted some graphs based on the audio.\nWe mainly need to focus on `4. Spectrogram` and `9. Mel Spectrogram`\n\nbasically it explains how mel spectrogram is generated with `librosa`, more explanation can be found in this awesome blog post : [getting-to-know-the-mel-spectrogram](https:\/\/towardsdatascience.com\/getting-to-know-the-mel-spectrogram-31bca3e2d9d0)\n\nAnd another post explains how actually it is done with nice visuals : [Understanding the Mel Spectrogram](https:\/\/medium.com\/analytics-vidhya\/understanding-the-mel-spectrogram-fca2afa2ce53)\n\nLater I came to know about mfcc, so I tried to train the model on the mfcc too.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum","0691d51b":"## Custom Audio Loader\n\nNow lets modify our previous `load_mono` function to get the `Spectrogram`, `mel-sectrogram` or `mfcc`  as tensor at the same time with the `audio tensor` and `sample rate`\n\nthe function takes just the audio file path as input, then help of `librosa` it loads the audio into `numpy array` and generates the `basic spectrogram`(mode=0) `mel_spectrogram`(mode=1) or `mfcc`(mode=2) then these two are converted to tensors and returned with the `sample rate`.\n\nThis `mode` will be used throught out the notebook so please keep in mind what mode gives what to understand my work better.","5d1bbd05":"## Save and Commit","f61be131":"### Training the model with ***Spectrogram***","9814f33f":"## Using a GPU\n\nTo seamlessly use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU as required.","3aed5870":"Lets check if the `mel-spectrogram` tensors are of same `shape`\n\nTo do that we need to find max hight and width of the spectrogram for all the audios we have in the folders. \n\nLater it will be used to pad the `mel-spectrogram` tensors in the dataset.\n\nThe shape generated by `load_spec` will not be the same for all the audios passed to it for any `mode` so it need to be padded hence  the max_height and max_width calculation. ","7a624109":"Again, below we can see the tensor after all cropping, reflection padding, random rotating being present in the train_dataset.\n\nIt is just a demonstration what would be passed to the model if image data augmentation is used on the spectrograms.","c671cfb6":"\nYou will see that the mel-spectogram here is same as the 2nd example in the visualiation part as the `audio_path` is the same.\n\n(psst : the spectrograms are flipped upside down)","4c7af975":"### imports","5580a2d5":"### classes","a926006c":"## Testing Individual Audio\n\nTring to predict with the `mode` set to \"0\" for audio as Spectrogram with the model trained for 30 epochs with pretrained set to `True`","78a4ec3f":"Based on where you're running this notebook, your default device could be a CPU (torch.device('cpu')) or a GPU (torch.device('cuda'))","f82c909a":"## Data visualzation","5541d9a3":"## Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\n\nSpeech audio-only files (16bit, 48kHz .wav) from the RAVDESS. Full dataset of speech and song, audio and video (24.8 GB) available from Zenodo. Construction and perceptual validation of the RAVDESS is described in our Open Access paper in PLoS ONE.\n\n**Files**\n\n- **for Speech data-set :** This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n\n- **for song data-set :** This portion of the RAVDESS contains 1012 files: 44 trials per actor x 23 actors = 1012. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Song emotions includes calm, happy, sad, angry, and fearful expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n\n\n**File naming convention** (same for both the data-set)\n\nEach of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n\n*Filename identifiers*\n\n0. Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n\n0. Vocal channel (01 = speech, 02 = song).\n\n0. Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n\n0. Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n\n0. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n\n0. Repetition (01 = 1st repetition, 02 = 2nd repetition).\n\n0. Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\n*Filename example: 03-01-06-01-02-01-12.wav*\n\n0. Audio-only (03)\n0. Speech (01)\n0. Fearful (06)\n0. Normal intensity (01)\n0. Statement \"dogs\" (02)\n0. 1st Repetition (01)\n0. 12th Actor (12)\n0. Female, as the actor ID number is even.\n\n**How to cite the RAVDESS**\n\n*Academic citation*\n\nIf you use the RAVDESS in an academic publication, please use the following citation: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https:\/\/doi.org\/10.1371\/journal.pone.0196391.\n\n*All other attributions*\n\nIf you use the RAVDESS in a form other than an academic publication, such as in a blog post, school project, or non-commercial product, please use the following attribution: \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\" by Livingstone & Russo is licensed under CC BY-NA-SC 4.0.","0d092ea0":"lets see what are the contents of the song_speech_info","82c86b05":"## DataLoader and Batch size"}}