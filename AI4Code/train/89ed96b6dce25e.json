{"cell_type":{"74d14cd5":"code","08c0d113":"code","a280a478":"code","7ad6edb2":"code","61a1c4d8":"code","f53de995":"code","59a4d599":"code","a2db956d":"code","b400d46d":"code","f96d5d26":"code","4a14fd17":"code","686739cc":"code","7b0240e5":"code","1c82fd8e":"markdown","5be70b81":"markdown","30f50cb2":"markdown","7ebc04ac":"markdown","4eb2248f":"markdown","83f2db72":"markdown","cc249368":"markdown","392fd3b2":"markdown","9f30ad0f":"markdown","8a735248":"markdown","889d8ad4":"markdown","f7742f1f":"markdown","6f10e123":"markdown","8c9ef408":"markdown","4434cc1d":"markdown","728149f2":"markdown","4b6536b8":"markdown","09045f12":"markdown"},"source":{"74d14cd5":"import os\nimport sys\nimport nltk\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nfrom gensim.models import word2vec\nfrom nltk.corpus import stopwords \nfrom tensorflow.contrib.tensorboard.plugins import projector\n\ntqdm.pandas()","08c0d113":"trainFile = \"..\/input\/wiki_movie_plots_deduped.csv\"\n\npwd = os.getcwd()\nos.chdir(os.path.dirname(trainFile))\ndf = pd.read_csv(os.path.basename(trainFile))\nos.chdir(pwd)","a280a478":"print(\"Nombre de lignes : {0}\".format(len(df)))","7ad6edb2":"dfAnalyze = df.copy()\ndfAnalyze.head()","61a1c4d8":"hist = dfAnalyze.plot.hist()","f53de995":"columns = ['Release Year', 'Director','Cast', 'Genre', 'Wiki Page', 'Plot']\ndfPie = dfAnalyze.drop(columns, axis=1)\n\ndfPie = dfPie.groupby(['Origin\/Ethnicity']).count().rename(columns= {'Title':'count'})\npie = dfPie.plot.pie(subplots=True, figsize=(7, 7))","59a4d599":"df = df.head(2000)\nprint(\"Nombre de lignes s\u00e9lectionn\u00e9es : {0}\".format(len(df)))","a2db956d":"def tokenize_without_stop_words(text):\n    sentence = nltk.word_tokenize(text.lower())\n    sentence = [w for w in sentence if not w in stopwords.words('english')]\n    sentence = [w for w in sentence if len(w) > 2 ]\n    return sentence\n\nsentences = df.progress_apply(lambda row: tokenize_without_stop_words(row['Plot']), axis=1)\nsentences = sentences.tolist()","b400d46d":"model = word2vec.Word2Vec(sentences, min_count=50, workers=4)\nprint(model)\n\n# model = word2vec.Word2Vec(sentences, min_count=750, workers=4)\n# Result Full Dataset : Word2Vec(vocab=1686, size=100, alpha=0.025)","f96d5d26":"#for entry in sorted(model.wv.vocab):\n#    print(entry)","4a14fd17":"print(model.wv.most_similar(['suicide'], topn=5))\n# Result Full Dataset : [('murder', 0.54288649559021), ('failed', 0.47224748134613037), ('rape', 0.4682120680809021), \n# ('murders', 0.4570953845977783), ('death', 0.3709148168563843)]","686739cc":"print(model.wv.most_similar(['war'], topn=5))\n# Result Full Dataset : [('u.s.', 0.597042441368103), ('army', 0.5714285969734192), ('china', 0.55967777967453), \n# ('union', 0.558294951915741), ('japan', 0.5436923503875732)]","7b0240e5":"def transform_word2vec_to_tensor(model, output_path):\n    meta_file = \"w2x_metadata.tsv\"\n    placeholder = np.zeros((len(model.wv.index2word), 100))\n\n    with open(os.path.join(output_path,meta_file), 'wb') as file_metadata:\n        for i, word in enumerate(model.wv.index2word):\n            placeholder[i] = model[word]\n            # temporary solution for https:\/\/github.com\/tensorflow\/tensorflow\/issues\/9094\n            if word == '':\n                print(\"Emply Line, should replaced by any thing else, or will cause a bug of tensorboard\")\n                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n            else:\n                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n\n    # Correction : purge les anciens nodes\/graphes\n    tf.reset_default_graph()\n    \n    sess = tf.InteractiveSession()\n\n    embedding = tf.Variable(placeholder, trainable = False, name = 'w2x_metadata')\n    tf.global_variables_initializer().run()\n\n    saver = tf.train.Saver()\n    writer = tf.summary.FileWriter(output_path, sess.graph)\n\n    # adding into projector\n    config = projector.ProjectorConfig()\n    embed = config.embeddings.add()\n    embed.tensor_name = 'w2x_metadata'\n    embed.metadata_path = meta_file\n\n    # Specify the width and height of a single thumbnail.\n    projector.visualize_embeddings(writer, config)\n    saver.save(sess, os.path.join(output_path,'w2x_metadata.ckpt'))\n\ndirectory = '.\/output\/'\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# Pour la visualisation Kaggle, j'ai mis en commentaire transform_word2vec_to_tensor(model, directory). Il faut donc d\u00e9-commenter la ligne ci-dessous.\n#transform_word2vec_to_tensor(model, directory)","1c82fd8e":"### Mod\u00e8le\nEn utilisant tout le jeu de donn\u00e9es, je ne garde que les mots pr\u00e9sents au moins 750 fois (soit pr\u00e9sent \u00e0 2%).  \nCela repr\u00e9sente 1686 mots.  \nPour la visualisation le jeu de donn\u00e9es est r\u00e9duit et j'ai donc adapt\u00e9 le min_count.","5be70b81":"# Chargement du fichier CSV","30f50cb2":"### Vue 3D du mot \"suicide\" \u00e0 partir de tout le jeu de donn\u00e9es\n\n![Full Dataset Suicide](https:\/\/farm5.staticflickr.com\/4841\/32304879878_7836210f2a_z.jpg)","7ebc04ac":"### Tests de la coh\u00e9rence en recherchant des mots proches\nAttention car ces tests de coh\u00e9rence diff\u00e9rent en fonction du nombre de lignes prises du jeu de donn\u00e9es.   \nJ'ai mis en commentaire les 5 mots les plus proches en utilisant tout le jeu de donn\u00e9es.","4eb2248f":"## Quelques informations sur la vectorization","83f2db72":"# Nettoyage\n- Suppression des stop words anglais\n- Suppression des cha\u00eenes de caract\u00e8res de longueur inf\u00e9rieure ou \u00e9gale \u00e0 2","cc249368":"# Vectorization Word2Vc","392fd3b2":"### Nombre de films par pays","9f30ad0f":"#### Mots proches de war\nAttention car en ne prenant par exemple que les 2000 premiers enregistrements les r\u00e9sultats sont compl\u00e8tement diff\u00e9rents : le mot war se rattache \u00e0 la guerre entre la France et l'Allemagne.    \nEn revanche avec tout le jeu de donn\u00e9es, le mot war se rattache aux Etats-Unis, Japon...  \nCela vient du fait que les enregistrements sont class\u00e9s par date et que les films traitent souvent de l'actualit\u00e9.","8a735248":"# Lecture dans tensorboard\n- Pour ma part, j'utilise anaconda. J'ai donc au pr\u00e9alable install\u00e9 tensorboard (https:\/\/anaconda.org\/conda-forge\/tensorboard) avec tous ses d\u00e9pendances.\n- Ensuite je me suis plac\u00e9 dans le r\u00e9pertoire contenant mon notebook et j'ai tap\u00e9 la commande :  \n`tensorboard --logdir output --host localhost`  \nExemple : `tensorboard --logdir . --host localhost`\n- J'ouvre mon navigateur et je charge la page dont l'url est indiqu\u00e9e suite \u00e0 l'ex\u00e9cution de la commande dans la console anaconda.","889d8ad4":"### Nombre de films par date \nLe jeu de donn\u00e9es est ordonn\u00e9 par ann\u00e9e de sortie (Release Year). De ce fait les 2000 premiers enregistrements pris pour les tests vont de 1900 \u00e0 1935 (car class\u00e9s par date).  \nCela va avoir une incidence sur la coh\u00e9rence des mots proches entre le jeu de donn\u00e9es r\u00e9duit et le jeu de donn\u00e9es complet (d\u00e9tails plus bas).","f7742f1f":"### Visualisation du vocabulaire","6f10e123":"# Transformation du mod\u00e8le word2vec en mod\u00e8le exploitable par tensorboard\n\nMerci \u00e0 BrikerMan (https:\/\/gist.github.com\/BrikerMan\/7bd4e4bd0a00ac9076986148afc06507)  \nAvec l'apport personnel d'une correction (= purge des anciens nodes\/graphes)","8c9ef408":"# D\u00e9clarations","4434cc1d":"### Vue 3D du mot \"war\" \u00e0 partir des 2000 premiers enregistrements du jeu de donn\u00e9es\n\n![Small Dataset War](https:\/\/farm5.staticflickr.com\/4805\/44359366950_c42826e5e1_b.jpg)","728149f2":"# R\u00e9duction du jeu de donn\u00e9es pour les tests\nPour la visualisation du kernel sur Kaggle, j'ai r\u00e9duit le jeu de donn\u00e9es. Cela a une incidence sur les r\u00e9sultats. J'ai donc mis en commentaires plus bas les r\u00e9sultats propres \u00e0 l'ensemble du jeu de donn\u00e9es.","4b6536b8":"### Vue 3D du mot \"war\" \u00e0 partir de tout le jeu de donn\u00e9es\n\n![Full Dataset War](https:\/\/farm5.staticflickr.com\/4850\/45263897465_c06d96d2dc_b.jpg)","09045f12":"# Visualisation du jeu de donn\u00e9es"}}