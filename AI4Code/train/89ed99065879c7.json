{"cell_type":{"81cafe2d":"code","daee6fe5":"code","1143afab":"code","64967067":"code","03801566":"code","0d4c459f":"code","cda0065e":"code","9ec934af":"code","353c6c08":"code","b71c4eae":"code","d3eb712a":"code","914acfac":"code","da4a8541":"code","8a32741d":"code","6353a64e":"code","5e81d56d":"code","e5d04289":"code","097aa1b4":"code","a2a86447":"code","f31786e0":"code","eed79702":"code","b3b97f93":"code","1005d981":"markdown","162c932e":"markdown","a5c3d861":"markdown","f21a0b17":"markdown","31252313":"markdown","4204b0bf":"markdown","866d30df":"markdown","d97cb97b":"markdown","972a00c4":"markdown","63acf15d":"markdown","a095f594":"markdown","2e936a88":"markdown","3c977f9f":"markdown","6b2ccd8e":"markdown","038d0d5c":"markdown","6d143396":"markdown","c838d4c7":"markdown","e48614b2":"markdown","76fe7932":"markdown","68537203":"markdown"},"source":{"81cafe2d":"!pip install pycaret","daee6fe5":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom pycaret.classification import *","1143afab":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","64967067":"def converter(x):\n    '''\n    convert text to 2 values(string part & numeric part)\n    '''\n    c, n = '', ''\n    x = str(x).replace('.', '').replace('\/','').replace(' ', '')\n    for i in x:\n        if i.isnumeric():\n            n += i\n        else :\n            c += i \n    if n != '':\n        return c, int(n)\n    return c, np.nan\n\n# Feature Engineering based on EDA\ndef create_extra_features(data):\n    data['Ticket_type'] = data['Ticket'].map(lambda x: converter(x)[0])\n    data['Ticket_number'] = data['Ticket'].map(lambda x: converter(x)[1])\n    data['Cabin_type'] = data['Cabin'].map(lambda x: converter(x)[0])\n    data['Cabin_number'] = data['Cabin'].map(lambda x: converter(x)[1])\n    data['Name1'] = data['Name'].map(lambda x: x.split(', ')[0])    \n    data['Name2'] = data['Name'].map(lambda x: x.split(', ')[1])\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    data['isAlone'] = data['FamilySize'].apply(lambda x : 1 if x == 1 else 0)\n    \n    # Sex\n    data['Sex'] = data['Sex'].map({'male':0, 'female':1})\n    \n    # Age\n    age_map = train[['Age', 'Pclass']].dropna().groupby('Pclass').median().to_dict()['Age']\n    data.loc[train['Age'].isnull(), 'Age'] = data.loc[train['Age'].isnull(), 'Pclass'].map(age_map)\n\n    # Embarked\n    data['Embarked'] = data['Embarked'].fillna('X')\n    return data\n\ntrain = create_extra_features(train)\ntest = create_extra_features(test)","03801566":"train.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","0d4c459f":"from category_encoders.cat_boost import CatBoostEncoder\n\nce = CatBoostEncoder()\n\ncolumn_name = ['Ticket_type', 'Embarked', 'Cabin_type', 'Name1', 'Name2']\ntrain[column_name] = ce.fit_transform(train[column_name], train['Survived'])\ntest[column_name] = ce.transform(test[column_name])","cda0065e":"train.head()","9ec934af":"setup(data = train, \n      target = 'Survived',\n      numeric_imputation = 'median',\n      fold=5,\n      silent = True,\n     )","353c6c08":"best_model = compare_models(sort = 'Accuracy', n_select = 3) # we will use it later","b71c4eae":"lightgbm = create_model('lightgbm')","d3eb712a":"lightgbm = tune_model(lightgbm\n#                      ,num_iter=100\n#                      ,optimize='AUC' \n                     )","914acfac":"plot_model(lightgbm, plot = 'confusion_matrix')","da4a8541":"plot_model(lightgbm, plot = 'learning')","8a32741d":"plot_model(lightgbm, plot = 'vc')","6353a64e":"plot_model(lightgbm, plot = 'feature_all')","5e81d56d":"plot_model(lightgbm, plot = 'threshold')","e5d04289":"plot_model(lightgbm, plot = 'auc')","097aa1b4":"plot_model(lightgbm, plot = 'boundary')","a2a86447":"blended = blend_models(estimator_list = best_model, fold = 5, method = 'soft')\n\n# lightgbm = create_model('lightgbm')\n# catboost = create_model('catboost')\n# blended = blend_models(estimator_list = [lightgbm, catboost], fold = 5, method = 'soft')","f31786e0":"calibrated_blended = calibrate_model(blended)","eed79702":"predictions = predict_model(calibrated_blended, data = test)\npredictions.head()","b3b97f93":"sample_submission['Survived'] = predictions['Label']\nsample_submission.to_csv(f'submission.csv',index=False)","1005d981":"## \ud83d\uddf3\ufe0f Blending Model\n\n**Blending model**s is a method of ensembling which uses consensus among estimators to generate final predictions. \n\n\nThe idea behind blending is to combine different machine learning algorithms and use a majority vote or the average predicted probabilities in case of classification to predict the final outcome. \n\nYou can create models individually and pass them as a list.","162c932e":"## \u2699\ufe0f Install PyCaret & Import Libraries\n\nKaggle notebooks do not provide pycaret by default. So, you can install it with the following command :\n\n> `pip install pycaret`","a5c3d861":"### Feature Importance\n\nYou can select important features to increase the model's efficiency and make a good model.\n\nThe strategy for choosing a good feature among these features is the following notebooks.\n\n- [@michau96](https:\/\/www.kaggle.com\/michau96) : [Simple trick to select variables for model \ud83d\udca1](https:\/\/www.kaggle.com\/michau96\/simple-trick-to-select-variables-for-model)","f21a0b17":"## \ud83c\udd95 Create Model\n\nIn general, individual models can be made like this:\n\nI recommend starting with the fastest, almost best performing LightGBM model on these Tabular datasets.\n\nEach model name is a code. Each code can be used as it is in the table above.\n","31252313":"## \ud83e\udd55 Setup AutoML Enviroment (PyCaret)\n\nTo use pycaret's automl, you can basically enter the input data, the desired target column name, the number of folds of the cross validation, and the rest of the settings as desired.\n\nThere are many different settings you can put in. **Normalize, remove outliers, etc**. You can add your insights.\n\nAnd here too, some missing values can be resolved, and `numeric_inmputation` was used to fill the numerical missing values with the median. (`Fare` feature)","4204b0bf":"### Remove Features\n\nYou can select features to ignore in pycaret, but I just drop them.","866d30df":"Of these, the N models that appear at the top can also be extracted. (And can be tuned)\n\nIn general, it is good to select 3 to 5 and proceed with an ensemble such as blending.","d97cb97b":"### ETC\n\nOther than that, you can look at AUC Curve, Decision Boundary, etc. and provide various visualizations.","972a00c4":"Call the basic data science library.","63acf15d":"## \ud83d\udd27 Use Cateory Encoders\n\nThe generated text data can use several encoders.\n\nI've seen in previous [Categorical Competitions & Benchmarks](https:\/\/www.kaggle.com\/subinium\/11-categorical-encoders-and-benchmark) that the performance varies a lot with this encoding method.\n\nAmong the various methods of target encoding, I tried using `CatBoostEncoder`.","a095f594":"## How to Use PyCaret\ud83e\udd55 with Feature Engineering\ud83d\udee0\n\n![image](https:\/\/ericonanalytics.com\/wp-content\/uploads\/2021\/01\/image-13.png)\n\nI think kaggle, especially this competition(tabular playground series) is a fun playground where you can focus on feature engineering and minimize the effort spent on model tuning.\n\nAnd based on that most of the top notebooks are AutoML, this kernel is introduced.\n\n[PyCaret](https:\/\/pycaret.org\/) is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within minutes in your choice of notebook environment.\n\n\nThis notebook uses [Tabular-Playground-Series-Apr](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021) dataset, but you can do the same process on [Titanic dataset](https:\/\/www.kaggle.com\/c\/titanic\/data), and it can be easily used with most structured data.\n","2e936a88":"## \ud83d\udcca Plotting\n\n\nIt provides a variety of plotting, and you can better understand the results by plotting like this:\n\n### Confusion Matrix\n\nThere are two possible errors in binary classification. Let's consider the direction of the model by looking at the False Positive and False Negative.\n\n\nAnd it is good to look at the confusion matrix of several models and consider how to apply it to the ensemble.","3c977f9f":"## \ud83d\udd04 Tune Model\n\nYou can use `tune_model` to tune the performance of your model.\n\nBut here, the number of iters is only 10, so let's increase the performance by increasing the number of times. (I recommend more than 100)\n\nAnd the target metric is accuracy, but in this case, too much fit for the train occurs.\n\nIn my case, when I used AUC, LB came out better.","6b2ccd8e":"## \ud83c\udfed Feature Engineering\n\nI recommend that you refer to the following discussion for a basic full framework of feature engineering.\n\n- [@Chris Deotte](https:\/\/www.kaggle.com\/cdeotte): [IEEE-CIS Fraud Detection | Feature Engineering Techniques](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/108575)\n\n### Complex(Text + Num) \n\nAlong with the general Titanic, this competition helps to improve performance if there is a pre-processing for the Ticket Cabin Name.\n\n`Ticket`s and `Cabin`s are divided into letters and numbers, and `Name`s are divided into family names and names.\n\n\n### Familiy Size & isAlone\n\n`FamiliySize` coude be made like this : \n\n- `data['FamilySize'] = data['SibSp'] + data['Parch']`\n\n> The assumption that the survival rate is low can also be included if the number of families to be kept is large.\n\n### Sex\n\nThe `Sex` can be labeled as `1,0` or `0,1` depending on the male\/female.\n\n### Age\n\n`Age` can usually be filled in for missing values as mean or median. (usual in numeric values)\n\nHowever, in [my previous notebook, the EDA](https:\/\/www.kaggle.com\/subinium\/tps-apr-highlighting-the-data) result showed that there was some distribution of Age according to Pclass, so I grouped it into Pclass and used the median value for each group.\n\n### Embarked\n\nFor `Embarked`, you can fill in the missing values with the most observations. \n\nI have filled this data with X, which means there is no data, assuming that this data is randomly generated","038d0d5c":"Check the dataset for proper input.","6d143396":"Although not introduced in this notebook, you can also look at interpretation of the model results using SHAP, etc.\n\n### If the content is helpful, please upvote. :)","c838d4c7":"## \ud83d\udd25 Submit your Result!!\n\nIt can be used to predict on unseen data using `predict_model` function.\n\nThe format for submission is as follows:","e48614b2":"## \ud83d\udccf Calibrate Model\n\nWhen performing Classification experiments you often want to predict not only the class labels, but also obtain a probability of the prediction. T\n\nhis probability gives you some kind of confidence. \n\nSome models can give you poor estimates of the class probabilities. \n\nWell calibrated classifiers are probabilistic classifiers for which the probability output can be directly interpreted as a confidence level. \n\nCalibrating classification models in PyCaret is as simple as writing calibrate_model. ","76fe7932":"## \u2705 Benchmark\n\nModels can be made individually, but they provide benchmarks by default.\n\nIt provides a benchmark by turning all representative models used in machine learning with a small number of iters.\n\nEach model shows scores of **Accuracy, AUC, Recall, Prec, F1, etc**.\n\nThe distribution of the dataset for this competition is severely unbalanced.\n\n- [How do I compare the leaderboard and private CV results?](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231187)\n\nLooking at what I have discussed with people here, there are many thoughts that the difference between my CV results and the public leaderboard is the result of the distribution of data.\n\nTherefore, it is recommended to look at AUC or F1 together rather than simply look at Accuracy. (In my case, doing that resulted in better performance)\n","68537203":"### Validation Metrix Check\n\nI have an imbalance in the data distribution, but I think that CV will be able to resolve the gap between LB and my results to some extent.\n\nLet's check the CV's score and check the overfitting."}}