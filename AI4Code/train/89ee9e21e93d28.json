{"cell_type":{"f07590a5":"code","78cd5482":"code","d0e85a7e":"code","66315f3b":"code","2f5a3e99":"code","376a9415":"code","849a798b":"code","8a291daa":"code","96a307c4":"code","d7885d1f":"code","3a7e05c1":"code","a32fd376":"code","b40774cf":"code","76efd36a":"code","304b0899":"code","d420e740":"code","e9bab7ad":"code","22f413ef":"code","138123a5":"code","88dfe34b":"code","90593c8a":"code","4c361c3a":"code","a33a1402":"code","5d034848":"code","59cb44ac":"code","9a74c235":"code","c020dd1f":"code","7e5010e1":"code","13bef67d":"code","87ddde68":"code","536f3ef5":"code","e0c0b6b8":"code","c6c7acec":"code","af09a7d1":"code","5df7d14f":"code","b5be7351":"code","505ce79b":"code","267f4899":"code","b98e9715":"code","4bc96e79":"code","e0400505":"code","d5710999":"code","55b8747f":"code","80d35203":"code","00d2bf4a":"code","7963d325":"code","ed3cf7ae":"code","653f5397":"markdown","08d1ab60":"markdown","d93a0b2c":"markdown","9775857d":"markdown","5a9de5e7":"markdown","70bb37bb":"markdown","53be8a50":"markdown","8cb7ed68":"markdown","305fa616":"markdown","5d419914":"markdown","0af5794f":"markdown","bdafa62f":"markdown","32ca1721":"markdown","96cbcd87":"markdown","652186e6":"markdown","f2c010ea":"markdown","5e370690":"markdown","c31dd2fa":"markdown","ca3f16fa":"markdown","05e5a6d2":"markdown","f6da07c9":"markdown","b23f8b43":"markdown","9e905b51":"markdown","f62edcdc":"markdown","7214cb0b":"markdown","739d1173":"markdown","c97275ae":"markdown","47738524":"markdown","7b899a89":"markdown","d186906a":"markdown","6b5eed17":"markdown"},"source":{"f07590a5":"import pandas as pd\nimport os\nimport librosa\nimport librosa.display\nimport struct\nimport numpy as np\n# Load various imports \n","78cd5482":"df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\n\n'''We will extract classes from this metadata.'''\n\ndf.head()","d0e85a7e":"df['class'].value_counts()","66315f3b":"def read_file_properties( filename):\n\n        wave_file = open(filename,\"rb\")\n\n        riff = wave_file.read(12)\n        fmt = wave_file.read(36)\n\n        num_channels_string = fmt[10:12]\n        num_channels = struct.unpack('<H', num_channels_string)[0]\n\n        sample_rate_string = fmt[12:16]\n        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n\n        bit_depth_string = fmt[22:24]\n        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n\n        return (num_channels, sample_rate, bit_depth)","2f5a3e99":"# Load various imports\n\naudiodata = []\nfor i in range(8732):\n    file_name = '..\/input\/urbansound8k\/fold' + str(df[\"fold\"][i]) + '\/' + df[\"slice_file_name\"][i]\n    data = read_file_properties(file_name)\n    audiodata.append(data)\n\n# Convert into a Panda dataframe\naudiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])","376a9415":"audiodf.head()","849a798b":"#Num of channels\naudiodf['num_channels'].value_counts(normalize=True)","8a291daa":"#Sample Rate \naudiodf['sample_rate'].value_counts(normalize=True)","96a307c4":"#Bit depth\naudiodf['bit_depth'].value_counts(normalize=True)","d7885d1f":"def extract_features(file_name):\n\n    try:\n        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n        mfccsscaled = np.mean(mfccs.T,axis=0)\n\n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file)\n        return None \n\n    return mfccsscaled","3a7e05c1":"\n\n\n# Set the path to the full UrbanSound dataset \nfulldatasetpath = '..\/input\/urbansound8k\/'\n\n\nfeatures = []\n\n# Iterate through each sound file and extract the features \nfor i in range(8732):\n    file_name = '..\/input\/urbansound8k\/fold' + str(df[\"fold\"][i]) + '\/' + df[\"slice_file_name\"][i]\n    class_label = df[\"class\"][i]\n    data = extract_features(file_name)\n\n    features.append([data, class_label])\n\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])\n\nprint('Finished feature extraction from ', len(featuresdf), ' files')","a32fd376":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# Convert features and corresponding classification labels into numpy arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) \n\n# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)","b40774cf":"x_test.shape[0]","76efd36a":"x_train.shape","304b0899":"x_test.shape","d420e740":"import IPython.display as ipd\n\nipd.Audio('..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav')\n","e9bab7ad":"# Load imports\n\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt","22f413ef":"# Class: Air Conditioner\n\nfilename = '..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","138123a5":"# Class: Car horn \n\nfilename = '..\/input\/urbansound8k\/fold1\/101415-3-0-3.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","88dfe34b":"# Class: Children playing \n\nfilename = '..\/input\/urbansound8k\/fold10\/101382-2-0-12.wav'\nplt.figure(figsize=(12,4))\ndata,sample_rate = librosa.load(filename)\n_ = librosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename)","90593c8a":"import pandas as pd\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\nmetadata.head()","4c361c3a":"import librosa \nfrom scipy.io import wavfile as wav\nimport numpy as np\n\nfilename = '..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav' \n\nlibrosa_audio, librosa_sample_rate = librosa.load(filename) \nscipy_sample_rate, scipy_audio = wav.read(filename) \n\nprint('Original sample rate:', scipy_sample_rate) \nprint('Librosa sample rate:', librosa_sample_rate) ","a33a1402":"print('Original audio file min~max range:', np.min(scipy_audio), 'to', np.max(scipy_audio))\nprint('Librosa audio file min~max range:', np.min(librosa_audio), 'to', np.max(librosa_audio))","5d034848":"import matplotlib.pyplot as plt\n\n# Original audio with 2 channels \nplt.figure(figsize=(12, 4))\nplt.plot(scipy_audio)","59cb44ac":"# Librosa audio with channels merged \nplt.figure(figsize=(12, 4))\nplt.plot(librosa_audio)","9a74c235":"mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc=40)\nprint(mfccs.shape)","c020dd1f":"import librosa.display\nlibrosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')","7e5010e1":"def extract_features(file_name):\n   \n    try:\n        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n        mfccsscaled = np.mean(mfccs.T,axis=0)\n        \n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file)\n        return None \n     \n    return mfccsscaled","13bef67d":"# Load various imports \nimport pandas as pd\nimport os\nimport librosa\n\n# Set the path to the full UrbanSound dataset \nfulldatasetpath = '..\/input\/urbansound8k\/'\n\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\n\nfeatures = []\n\n# Iterate through each sound file and extract the features \nfor index, row in metadata.iterrows():\n    \n    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'\/',str(row[\"slice_file_name\"]))\n    \n    class_label = row[\"class\"]\n    data = extract_features(file_name)\n    \n    features.append([data, class_label])\n\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])\n\nprint('Finished feature extraction from ', len(featuresdf), ' files') ","87ddde68":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# Convert features and corresponding classification labels into numpy arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) ","536f3ef5":"# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)","e0c0b6b8":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom sklearn import metrics \n\nnum_labels = yy.shape[1]\nfilter_size = 2\n\n# Construct model \nmodel = Sequential()\n\nmodel.add(Dense(256, input_shape=(40,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","c6c7acec":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') ","af09a7d1":"# Display model architecture summary \nmodel.summary()\n\n# Calculate pre-training accuracy \nscore = model.evaluate(x_test, y_test, verbose=0)\naccuracy = 100*score[1]\n\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy)","5df7d14f":"from keras.callbacks import ModelCheckpoint \nfrom datetime import datetime \n\nnum_epochs = 100\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath='..\/input\/mixed-model\/weights.best.mix_mlp.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","b5be7351":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])","505ce79b":"import librosa \nimport numpy as np \n\ndef extract_feature(file_name):\n   \n    try:\n        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n        mfccsscaled = np.mean(mfccs.T,axis=0)\n        \n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file)\n        return None, None\n\n    return np.array([mfccsscaled])\n","267f4899":"df.head()","b98e9715":"actual_class_list=[]\npredicted_class_list=[]\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\n\nfor index, row in metadata.iterrows():\n    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'\/',str(row[\"slice_file_name\"]))\n    class_id = row[\"classID\"]\n    #print(file_name)\n    #sleep(2)\n    prediction_feature = extract_feature(file_name) \n    \n    predicted_vector = model.predict_classes(prediction_feature)\n    predicted_class = le.inverse_transform(predicted_vector) \n   # print(\"The predicted class is:\", predicted_class[0], '\\n') \n\n    predicted_proba_vector = model.predict_proba(prediction_feature) \n    predicted_proba = predicted_proba_vector[0]\n    predicted_class=np.argmax(predicted_proba) \n    predicted_class_list.append(predicted_class)\n    actual_class_list.append(class_id)\n    #print(\"pred--\",predicted_class_list)\n    #print(\"actual--\",actual_class_list)","4bc96e79":"import pandas as pd\ny_actu = pd.Series(actual_class_list, name='Actual')\ny_pred = pd.Series(predicted_class_list, name='Predicted')\ndf_confusion = pd.crosstab(y_actu, y_pred)","e0400505":"df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\ndf_confusion","d5710999":"unique_labels=['air_conditioner','car_horn','children_playing','dog_bark','drilling','engine_idling','gun_shot','jackhammer','siren','street_music']","55b8747f":"y_pred.shape[0]","80d35203":"y_pred_class=[]\ny_actu_class=[]\nfor i in range(y_pred.shape[0]):\n    y_pred_class.append(unique_labels[y_pred[i]])\n    y_actu_class.append(unique_labels[y_actu[i]])","00d2bf4a":"import seaborn as sns","7963d325":"from sklearn.metrics import confusion_matrix","ed3cf7ae":"conf_mat = confusion_matrix(y_actu_class, y_pred_class, labels=unique_labels)\ndf_cm = pd.DataFrame(conf_mat, index = unique_labels,\n                     columns = unique_labels)\nplt.figure(figsize = (10,8))\nplt.title('Confusion Matrix')\nsns.heatmap(df_cm, annot=True, cmap='viridis')\nplt.show()","653f5397":"#### Merge audio channels \n\nLibrosa will also convert the signal to mono,it's meaning the number of channels will always be 1. ","08d1ab60":"The next step will be to build and train a Deep Neural Network with these data sets and make predictions.\n\nHere we will use a Convolutional Neural Network (CNN). CNN\u2019s typically make good classifiers and perform particular well with image classification tasks due to their feature extraction and classification parts. I believe that this will be very effective at finding patterns within the MFCC\u2019s much like they are effective at finding patterns within images.\n\nWe will use a sequential model, starting with a simple model architecture, consisting of four Conv2D convolution layers, with our final output layer being a dense layer. Our output layer will have 10 nodes (num_labels) which matches the number of possible classifications.\n\nSee the full report for an in-depth breakdown of the chosen layers, we also compare the performance of the CNN with a more traditional MLP.","d93a0b2c":"## Confusion Matrix","9775857d":"### Predictions  \n\nHere we will build a method which will allow us to test the models predictions on a specified audio .wav file. ","5a9de5e7":"#### Sample rate there is a wide range of Sample rates that have been used across all the samples which is a concern (ranging from 96kHz to 8kHz).","70bb37bb":"### Load saved mixed model kapre","53be8a50":"### Audio properties that will require normalising \n\nFollowing on from the previous notebook, we identifed the following audio properties that need preprocessing to ensure consistency across the whole dataset:  \n\n- Audio Channels \n- Sample rate \n- Bit-depth\n\nWe will continue to use Librosa which will be useful for the pre-processing and feature extraction. ","8cb7ed68":"## Pre-Processing","305fa616":"#### Load csv files","5d419914":"### Test the model \n\nHere we will review the accuracy of the model on both the training and test data sets. ","0af5794f":"### Observations\nFrom a visual inspection we can see that it is tricky to visualise the difference between some of the classes.\n\nParticularly, the waveforms for reptitive sounds for air conditioner, drilling, engine idling and jackhammer are similar in shape.\n\nLikewise the peak in the dog barking sample is simmilar in shape to the gun shot sample (albeit the samples differ in that there are two peaks for two gunshots compared to the one peak for one dog bark). Also, the car horn is similar too.\n\nWe show to similarities between the children playing and street music.\n\nThe human ear can naturally detect the difference between the harmonics, it will be interesting to see how well a deep learning model will be able to extract the necessary features to distinguish between these classes.\n\nHowever, it is easy to differentiate from the waveform shape, the difference between certain classes such as dog barking and engine idling.\n\n### Dataset Metadata\nHere we will load the UrbanSound metadata .csv file into a Panda dataframe.","bdafa62f":"### Initial model architecture - MLP\n\nWe will start with constructing a Multilayer Perceptron (MLP) Neural Network using Keras and a Tensorflow backend. \n\nStarting with a `sequential` model so we can build the model layer by layer. \n\nWe will begin with a simple model architecture, consisting of three layers, an input layer, a hidden layer and an output layer. All three layers will be of the `dense` layer type which is a standard layer type that is used in many cases for neural networks. \n\nThe first layer will receive the input shape. As each sample contains 40 MFCCs (or columns) we have a shape of (1x40) this means we will start with an input shape of 40. \n\nThe first two layers will have 256 nodes. The activation function we will be using for our first 2 layers is the `ReLU`, or `Rectified Linear Activation`. This activation function has been proven to work well in neural networks.\n\nWe will also apply a `Dropout` value of 50% on our first two layers. This will randomly exclude nodes from each update cycle which in turn results in a network that is capable of better generalisation and is less likely to overfit the training data.\n\nOur output layer will have 10 nodes (num_labels) which matches the number of possible classifications. The activation is for our output layer is `softmax`. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability.","32ca1721":"This shows librosa calculated a series of 40 MFCCs over 173 frames. ","96cbcd87":"### Extract Features \n\nAs outlined in the proposal, we will extract [Mel-Frequency Cepstral Coefficients (MFCC)](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum) from the the audio samples. \n\nThe MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification. ","652186e6":"The next step is to extract the features we will need to train our model. To do this, we are going to create a visual representation of each of the audio samples which will allow us to identify features for classification, using the same techniques used to classify images with high accuracy.\n\nSpectrograms are a useful technique for visualising the spectrum of frequencies of a sound and how they vary during a very short period of time. We will be using a similar technique known as Mel-Frequency Cepstral Coefficients (MFCC).\n\nThe main difference is that a spectrogram uses a linear spaced frequency scale (so each frequency bin is spaced an equal number of Hertz apart), whereas an MFCC uses a quasi-logarithmic spaced frequencyscale, which is more similar to how the human auditory system processes sounds","f2c010ea":"### Training \n\nHere we will train the model. \n\nWe will start with 100 epochs which is the number of times the model will cycle through the data. The model will improve on each cycle until it reaches a certain point. \n\nWe will also start with a low batch size, as having a large batch size can reduce the generalisation ability of the model. ","5e370690":"## Visual inspection\nI will load a sample from each class and visually inspect the data for any patterns. I will use librosa to load the audio file into an array then librosa.display and matplotlib to display the waveform.","c31dd2fa":"## Data Exploratory\n","ca3f16fa":"### Compiling the model \n\nFor compiling our model, we will use the following three parameters: \n\n* Loss function - we will use `categorical_crossentropy`. This is the most common choice for classification. A lower score indicates that the model is performing better.\n\n* Metrics - we will use the `accuracy` metric which will allow us to view the accuracy score on the validation data when we train the model. \n\n* Optimizer - here we will use `adam` which is a generally good optimizer for many use cases.\n","05e5a6d2":"### Convert the data and labels\n\nWe will use `sklearn.preprocessing.LabelEncoder` to encode the categorical text data into model-understandable numerical data. ","f6da07c9":" In the previous section we identified the following audio properties that need preprocessing to ensure consistency across the whole dataset:\n\n* \u2022 Audio Channels\n* \u2022 Sample rate\n* \u2022 Bit-depth","b23f8b43":"## Extract features\n","9e905b51":"## Building our model\n","f62edcdc":"### Validation \n\n#### Test with sample data \n\nInitial sainity check to verify the predictions using a subsection of the sample audio files we explored in the first notebook. We expect the bulk of these to be classified correctly. ","7214cb0b":"#### Bit-depth there is also a range of bit-depths (ranging from 4bit to 32bit).\n\n","739d1173":"#### Extracting MFCC's for every file \n\nWe will now extract an MFCC for each audio file in the dataset and store it in a Panda Dataframe along with it's classification label. ","c97275ae":"The initial Training and Testing accuracy scores are quite high. As there is not a great difference between the Training and Test scores (~5%) this suggests that the model has not suffered from overfitting. ","47738524":"#### Bit-depth \n\nLibrosa\u2019s load function will normalise the data so it's values range between -1 and 1. This removes the complication of the dataset having a wide range of bit-depths. ","7b899a89":"### Split the dataset\n\nHere we will use `sklearn.model_selection.train_test_split` to split the dataset into training and testing sets. The testing set size will be 20% and we will set a random state. \n","d186906a":"## Converting the data and labels then splitting the dataset\nWe will use sklearn.preprocessing.LabelEncoder to encode the categorical text data into model-understandable numerical data.\n\nThen we will use sklearn.model_selection.train_test_split to split the dataset into training and testing sets. The testing set size will be 20% and we will set a random state.\n\n","6b5eed17":"#### Audio channels most of the samples have two audio channels (meaning stereo) with a few with just the one channel (mono)."}}