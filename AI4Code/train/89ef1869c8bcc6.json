{"cell_type":{"078b6dd0":"code","1ed836e7":"code","1b5169fa":"code","4a852b4f":"code","9fb0b01b":"code","a49b2e5f":"code","dbc629c8":"code","6c3c74fd":"code","39c30b48":"code","9e62ef68":"code","75ea3d23":"code","3871eac8":"code","f78f5736":"code","e2f85f79":"code","47e634c1":"code","890a70b5":"code","9fbd05e3":"code","a4b39d88":"code","76a452f3":"code","5574caac":"code","4ce256bc":"code","a5ed4c44":"markdown","89b6ad81":"markdown","c09ec344":"markdown","285f0fbe":"markdown","3f5829ff":"markdown","c63a0f6a":"markdown","7308db79":"markdown","04984c58":"markdown","2230c963":"markdown","0d00c7ab":"markdown","ae1ba092":"markdown","8c8134a9":"markdown","c074d0e6":"markdown","ccc90365":"markdown","bec9e6c5":"markdown","5216f1bc":"markdown","a8125e61":"markdown","4c7f0139":"markdown","1aceb58a":"markdown","d6e75f2d":"markdown","2ef8cf9a":"markdown","697566bd":"markdown","433a87cd":"markdown","5ecc0a50":"markdown","65283791":"markdown","0114877e":"markdown","8a32ac43":"markdown","e21ef86c":"markdown","9c4e0b3d":"markdown","9a0c3a48":"markdown","cef20cbf":"markdown","d3ffc6b1":"markdown","58bd0720":"markdown","548ee5e5":"markdown","4600e51d":"markdown","ea80dc3f":"markdown"},"source":{"078b6dd0":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom geopy import distance\nfrom scipy.stats import skew\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nplt.style.use('seaborn-whitegrid')\nsns.set_style('whitegrid')\nplt.rcParams[\"axes.labelsize\"] = 15\n\nfrom bokeh.plotting import figure, show, output_notebook, output_file\nfrom bokeh.tile_providers import CARTODBPOSITRON\nfrom bokeh.transform import log_cmap\nfrom bokeh.models import ColumnDataSource, LogTicker, ColorBar #, HoverTool, CategoricalColorMapper, LogColorMapper\nfrom bokeh.models.formatters import BasicTickFormatter, NumeralTickFormatter\nimport bokeh.palettes as bp\n\noutput_notebook()\n%matplotlib inline","1ed836e7":"df = pd.read_csv('..\/input\/kc_house_data.csv')\ndf.head()","1b5169fa":"df.info()","4a852b4f":"df.drop(['id', 'date'], axis=1, inplace=True)\ndf.columns","9fb0b01b":"fig, axes = plt.subplots(3,2, figsize=(18,16))\nfor xcol, ax in zip(['floors', 'waterfront', 'view', 'condition',\n                     'grade', 'bedrooms'], axes.flatten()):\n    sns.boxplot(xcol, 'price', data=df, ax=ax)\n    \n\nfig = plt.figure(figsize=(16, 8))\nsns.boxplot('bathrooms', 'price', data=df)\n\nplt.tight_layout()","a49b2e5f":"features_cont = ['sqft_living', 'sqft_lot', 'sqft_above','sqft_basement', \n                'sqft_living15', 'sqft_lot15']\n\nfig, axes = plt.subplots(3,2, figsize=(14,14))\n\nfor xcol, ax in zip(features_cont, axes.flatten()):\n    sns.scatterplot(xcol, 'price', data=df, ax=ax)\n\nplt.tight_layout()","dbc629c8":"def lgn2x(a):\n    return a * (np.pi\/180) * 6378137\n\ndef lat2y(a):\n    return np.log(np.tan(a * (np.pi\/180)\/2 + np.pi\/4)) * 6378137\n\n\n# project coordinates\ndf['x_coor'] = df['long'].apply(lambda row: lgn2x(row))\ndf['y_coor'] = df['lat'].apply(lambda row: lat2y(row))\n\n# creating the map\noutput_file(\"tile.html\")\nxmin, xmax =  df['x_coor'].min(), df['x_coor'].max() \nymin, ymax =  df['y_coor'].min(), df['y_coor'].max() \n\n# range bounds supplied in web mercator coordinates\nmap_kc = figure(x_range=(xmin, xmax), y_range=(ymin, ymax),\n           x_axis_type=\"mercator\", y_axis_type=\"mercator\", title=\"House Price on King County, USA\",\n           plot_width=700, plot_height=500,)\n\nmap_kc.title.text_font_size = '16pt'\nmap_kc.add_tile(CARTODBPOSITRON)\n\nsource = ColumnDataSource({'x':df['x_coor'], 'y':df['y_coor'], 'z':df['price']})\ncolormapper = log_cmap('z', palette=bp.Inferno256, low=df['price'].min(), high=df['price'].max())\n\nmap_kc.circle(x ='x', y='y', source=source, color=colormapper)\n\ncolor_bar = ColorBar(color_mapper=colormapper['transform'], width=18, location=(0,0), \n                     ticker=LogTicker(), label_standoff=12)\n\ncolor_bar.formatter = NumeralTickFormatter(format='0,0')\n# color_bar.formatter = BasicTickFormatter(precision=3)\n\nmap_kc.add_layout(color_bar, 'right')\n\nshow(map_kc)","6c3c74fd":"location = tuple(map(tuple, df[['lat', 'long']].values))\n# the distance of every house from downtowm seattle\nseattle_dt = (47.6050, -122.3344)\n\ndf['distance'] = [distance.distance(seattle_dt, loc).km for loc in location]\n\n# df.drop(['lat', 'long', 'x_coor', 'y_coor'], axis=1, inplace=True)\ndf.drop(['x_coor', 'y_coor'], axis=1, inplace=True)\n\ndf.head()","39c30b48":"def get_vif(data):\n    \n    X = data.iloc[:,1:]\n    vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n\n    return vif.round(1)\n\nget_vif(df)","9e62ef68":"plt.figure(figsize=(16,12))\nsns.heatmap(df.corr(), annot=True)","75ea3d23":"df['sqft'] = df['sqft_living'] + df['sqft_above'] + df['sqft_living15']\ndf.drop(['sqft_living', 'sqft_above', 'sqft_living15'], axis=1, inplace=True)\n\ndf['sqft_lot_comb'] = df['sqft_lot'] + df['sqft_lot15']\ndf.drop(['sqft_lot', 'sqft_lot15'], axis=1, inplace=True)\n\ndf.head()","3871eac8":"get_vif(df)","f78f5736":"fig = plt.figure(figsize=(11,5))\nfig = sns.distplot(df['price'])\nfig.set(yticks=[]);","e2f85f79":"# computing skewness factor\nskewness = df.apply(lambda x: skew(x))\nskewness","47e634c1":"# converting longtitude to positive values to enable us using the log function on all data\n# this operation doesn't affect results, as all the whole 'long' column is negative\ndf['long'] = abs(df['long'])\n\nskewed = skewness[skewness > 0.75].index\n\ndf[skewed] = np.log1p(df[skewed])\n\n# plot the new target ditribution\nfig = plt.figure(figsize=(11,5))\nfig = sns.distplot(df['price'])\nfig.set(yticks=[]);","890a70b5":"fig, axes = plt.subplots(2,2, figsize=(16,10))\n\nsns.scatterplot('sqft', 'price', data=df, ax=axes[0,0])\nsns.scatterplot('sqft_lot_comb', 'price', data=df, ax=axes[0,1])\nsns.boxplot('bedrooms', 'price', data=df, ax=axes[1,0])\nsns.boxplot('grade', 'price', data=df, ax=axes[1,1])\naxes[1,0].set_xticks([])\naxes[1,1].set_xticks([])\n\nplt.tight_layout()","9fbd05e3":"get_vif(df)","a4b39d88":"# Standardizing the data\ndf = (df - df.mean()) \/ df.std()\n\nget_vif(df)","76a452f3":"def split_kfold(folds, i):    \n    train = folds.copy() \n    test = folds[i]\n    del train[i]\n    train = np.concatenate(train, axis=0)\n    d = train.shape[1]-1\n    x_train, y_train = train[:, :d], train[:, d]\n    x_test, y_test = test[:, :d], test[:, d]\n    \n    return x_train, x_test, y_train, y_test\n\n\ndef get_error(Y, Yhat):\n    N = len(Y)   \n    d1 = Y - Yhat\n    d2 = Y - Y.mean()\n    r2 = 1 - (d1.dot(d1) \/ d2.dot(d2))\n    r2_adj = 1 - (1 - r2)*((N - 1) \/ (N - D - 1))\n    mse = d1.dot(d1) \/ N\n    return r2_adj, mse\n\n\ndef fit_kfold(X, Y, X_test, Y_test):\n    w = np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n    Yhat = X.dot(w)\n    Yhat_test =  X_test.dot(w)\n    r2_test, mse_test = get_error(Y_test, Yhat_test)\n    \n    return r2_test, w\n","5574caac":"# df_array = df[features].values\nX = df.iloc[:,1:]\nY = df.iloc[:,0]\n\ndf_array = np.c_[X.values, Y.values]\nk = 7\nD = X.shape[1]\nfolds = np.array_split(df_array, k)\n\nr2_test = []\ncoef = []\n\nfor i in range(k):\n    x_train, x_test, y_train, y_test = split_kfold(folds, i)\n    # prepare the array\n    x_train = np.c_[np.ones(x_train.shape[0]), x_train]\n    x_test = np.c_[np.ones(x_test.shape[0]), x_test]\n    \n    r2_test_temp, w = fit_kfold(x_train, y_train, x_test, y_test)\n    r2_test.append(r2_test_temp)\n    coef.append(w)\n    \nr2_test_kfold = sum(r2_test) \/ len(r2_test)\ncoef = np.sum(coef, axis=0) \/ len(coef)\n\nindx = list(df.columns)\nindx[0] = 'bias'\ncoef = pd.DataFrame(coef, index=indx, columns=['coef'])\n\nprint('Using  k-fold cross-validation where k = ', k,':')\nprint('R2_adjusted of the test data, using a simple linear regression, is: ', r2_test_kfold)","4ce256bc":"coef.reindex(coef['coef'].abs().sort_values(ascending=False).index)","a5ed4c44":"We can make two observations from this plot:\n1. The northern part of King County region has a higher house prices.\n2. The closer the house is to downtown Seattle, the price of the houses increases.\n\nAs for the first observation, we can assume that the *lat* feature is important in our model. We can check our assumption by using feature engineering methods, but since we don't have many features compared to the number of data points, we see better results using the entire data set, which negates the need for feature engineering. \n\nAs for the second observation, we can create a new feature that measures the *distance* from each house to downtown Seattle. This feature is a *nonlinear combination* of the *lat* and *long* features, so it doesn't increase the multicollinearity* (see next session).\n\nLet's add the *distance* feature (in km) and have a look at the head of the table.","89b6ad81":"We can see that some of the features have a linear relation to the target (*price*), but some of them have a non-linear relation, such as the *grade*, which looks more like an exponential relation to price. This seems a bit problematic, considering we are going to use a *linear* regression model. We will learn how to deal with this problem later in this kernel.\n\nLet's have a look at the continuous variables and their relation to the target.","c09ec344":"### We fit a simple linear model and got a great \\\\(R^2\\\\) result! \n**How did we do it?**\n1. Data manipulation: we created the *distance* feature.\n2. Log-transformation of the data: created a more flexible model that allows non-linear relationships with the target.\n\n### Lowered multicollinearity and got statistically significant results! \n**How did we do it?**\n1. Created new features to replace highly correlated ones with their linear combination. \n2. Standardizing the data to reduce correlations caused by interaction terms.","285f0fbe":"# Data Visualization","3f5829ff":"Collinearity between variables can produce misleading results. While the \\\\(R^2\\\\) might not be affected by collinearity, the interpretation of the results are highly affected by it. The presence of collinearity can pose problems in\nthe regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since two correlated variables tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response (in our case, the price of the house). This phenomena can completely change the coefficient values (and therefore the interpretation of their importance), and in some cases it can even change the sign of the coefficient value. \n\nUnfortunately, it is not enough to check the correlation matrix, as multicollinearity can occur between three or more variables, even when there is no indication of collinearity between two variables. A better way to assess multicollinearity is by computing the *[Variance Inflation Factor](http:\/\/www.statisticshowto.datasciencecentral.com\/variance-inflation-factor\/)* (\\\\(VIF\\\\)). As a rule of thumb, we would like to keep the \\\\(VIF\\\\) under 5, as  \\\\(VIF > 5\\\\) suggests medium multicollinearity, while \\\\(VIF > 10\\\\) suggests high multicollinearity.\n\nLet's have a look at the \\\\(VIF\\\\) value of our variables.","c63a0f6a":"You'll see that most of the variables suggest strong multicollinearity; some of the values even go to infinity. To solve this problem, we can create another variable, which will be a linear combination of two highly correlated variables. \n\nComputing a correlation heatmap will help us choose the features we would like to combine together. ","7308db79":"In the plot below, you can see the data distribution of the target. Notice that the target has a right-skewed distribution, meaning, it has a \"tail\" on the right side. Skewing the data by log-transformation will transform the right-skewed distribution to a normal distribution.\n\nTo be clear, there is no need for the target to be normally distributed in order to fit a linear regression model. The normallity assumption in linear regression refers to the error terms between the target and the predicted values (\\\\(\\epsilon\\\\)), and not the distribution of the target itself. \n\nHowever, a log transformation can sometimes solve multiple problems simultaneously: it will linearize some of the polynomial relationships, and help us create a more flexible model that allows [non-linear relashionship with the target](http:\/\/stats.stackexchange.com\/questions\/107610\/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution) (see Bill's answer, poin 2). It will also [reduce outlier influences](https:\/\/heartbeat.fritz.ai\/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07). \n\nPlus, we get normally distributed data, which is always nice to have, and it's important for some statistical hypothesis tests. \n\nWe will use the normality assumption in the next section, when we will carry on with the high multicollinearity issue.","04984c58":"As we can see above, we have high correlation between *sqft_living* and *sqft_above*. We can also see high correlation between *sqft_living15* and both *sqft_living* and *sqft_above*. \n\nLet's create a new variable called *sqft* that will be a linear combination of the three predictors, and will replace these predictors. We can do the same with *sqft_lot* and *sqft_lot15*, creating a new variable called *sqft_lot_comb*. \n\nLet's have a look at the new table's head.","2230c963":"Now, let's look at the skewness factor of the target and the features. \n\nIn order to be normally distributed, the skewness should be zero. A positive skewness is a right-skewed data. ","0d00c7ab":"<img align=\"center\" src=\"https:\/\/imgur.com\/O1ImtR8.png\" width=\"700\" hight=\"550\" title=\"King County Region, WA, USA\" \/>\n<img align=\"center\" src=\"https:\/\/imgur.com\/culbAe4.png\" width=\"700\" hight=\"250\" title=\"Downtown Seattle\" \/>","ae1ba092":"Let's do a log transformation on the data where *skewness*  \\\\(> 0.75\\\\), and have a look on the affect on the *price* distribution.","8c8134a9":"Above we can see the change in the relationship between some of the features and the target. We also see that what once was a nonlinear relation between the feature to the *price* now have a more linear relationship to it. Moreover, we've reduced the outliers' affect. \n\nThis dramatically increases the accuracy of our model. \n\nNotice that it has no affect on the multicollinearity.","c074d0e6":"**I hope you enjoy my notebook! If you did, please UPVOTE!**\n\nIf you have a comment or ideas for improvement, please leave a comment below.\n\nSee you on my next kernel :)","ccc90365":"Are you planning on buying a house for investment, and wondering which house would be the best buy? Are you planning to renovate your house to increase its value on the market, but don't know where to invest the most to get the best results? Do you have a real-estate company that wants to give the best machine-learning based solutions to its customers? You are in the right place!\n\nIn this notebook, I will analize house sales in King County, WA, USA between 2014 to 2015.  \n\nLet's start by looking the maps below: the top image is the King County region; the bottom image is downtown Seattle, the capital of Washington.","bec9e6c5":"## The Importance of Predicting House Prices","5216f1bc":"Let's see the affect on the \\\\(VIF\\\\) table.","a8125e61":"Let's see the affect of standardizing the data on the \\\\(VIF\\\\).","4c7f0139":"Let's have a look at the different features and their relation to the price of the house, starting with the discrete variables.","1aceb58a":"Going back dealing with the high  \\\\(VIF\\\\), we can use [data standardization to reduce multicollinearity](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/). In fact, only by centering the data, we will lower the \\\\(VIF\\\\) values (the scaling part is a matter of preference). \n\nUnder the assumption that the data has a symetric distribution (like normal distribution), the correlation created by interaction terms will be zero. For the full mathematical explanation, see [this link](https:\/\/psychometroscar.com\/why-does-centering-in-linear-regression-reduces-multicollinearity\/).","d6e75f2d":"We can see a big improvement: we no longer have infinite \\\\(VIF\\\\) values!  However, we still have some more data manipulation to do for better results. For example, *lat, long, zipcode* and *yr_built* have high \\\\(VIF\\\\) values. \n\nLet's have a look at the data distribution and deal with the polynomial relationships we obsereved earlier between the variables and the target.","2ef8cf9a":"Now, we will predict house prices using a simple linear regression and a k-fold cross-validation. The fitted model will be tested with the R-squared adjusted test, so it will not be affected by the number of features I chose to use in the model (like in the R-squared test).","697566bd":"Notice that the \\\\(R^2\\\\) is higher than the everage \\\\(R^2\\\\) from other notebooks that used a linear regression model. \n\nLet's have a look at the coefficients.","433a87cd":"Above you can see the coefficients sorted by their importance. We can see that the features *sqft, distance, grade* and *lat* have the greatest affect on the *price*. The negative coefficients imply that the increase of these features lowers the price of the house. For example, as the houses' distance from downtwon Seattle increases, the price of the house decreases.\n\nBecause we used log-transformation on the data and standardized it, it cannot be interpreted using traditional or straightforward methods. More information on how to interpret the coefficients will be added soon.","5ecc0a50":"We can see that the *price* is now normally distributed.\n\nLet's have a look at some of the predictors and their relation to the target.","65283791":"# Multicollinearity and Data Manipulation","0114877e":"# Data Distributuion and Polynomial Relationships","8a32ac43":"# Summary","e21ef86c":"The dataframe contains 19 house features, plus the price and the ID columns, along with 21613 observations.\n\nThe ID column doesn't contribute any insight into the data, and neither does the date, as all of the data is from 2014 to 2015. Let's drop those columns.","9c4e0b3d":"Let's read the data, look at the head of the table and the information about the features.","9a0c3a48":"Once again, we can see some features with a more linear relation to *price*, but some of the features reveal more complex relationships, like polynomial, exponential or even a square-root relation. We can see the complex relations clearly with the features such as *sqft_lot* and *sqft_lot15*.\n\nLet's have a look at the price of houses according to their location in Seattle.","cef20cbf":"# Data Standardization ","d3ffc6b1":"# About the Notebook","58bd0720":"In this kernel, I will present a simple linear regression model combined with more advanced concepts, such as *data manipulation, data distribution, multicollinearity, polynomial relationships* etc., and present ways to deal with these problems.\n\nI chose a simple data set to allow a better understanding of these concepts. Notice how applying these tools will improve the fitting  test results and the quality of your predictions.\n\nI am sometimes using different terms that might imply the same thing. For example, features and variables, or target and response. It is important to know all of the terms that are being used by the community.\n\nIf you have any question or suggestions, please don't hesitate to comment!\n\n**If you like this kernel, please don't hesitate to UPVOTE :)**","548ee5e5":"As you can see, standardizing the data has a huge affect on the \\\\(VIF\\\\) values. \n\nNow we are ready to fit the model and get predictions!","4600e51d":"# House Price Prediction","ea80dc3f":"# Explore the Data"}}