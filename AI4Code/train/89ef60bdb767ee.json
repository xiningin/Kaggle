{"cell_type":{"b535aa92":"code","9587397f":"code","a9ba9da1":"code","617280ae":"code","edea2b90":"code","d4c4061e":"code","27f4ad15":"code","eb0c422a":"code","24404063":"code","b62e4136":"code","0d648bde":"code","1ce55040":"code","43b70b83":"code","46267fce":"code","edc5835d":"code","0460de48":"code","5c2f2cf5":"code","ff11c96a":"code","aa416cd3":"code","d152ea0e":"code","847e91ab":"code","500e687e":"code","9eb75bf7":"code","e99f515a":"code","e9522d4a":"markdown","1cace5b6":"markdown","1faacf1c":"markdown","edd077c3":"markdown","8c5b7cd7":"markdown","2fafd756":"markdown","2ec6966e":"markdown","5ddad339":"markdown","7c9795c0":"markdown","9a1de9dd":"markdown","293447f8":"markdown","1ecb280b":"markdown","eb93d209":"markdown","549ffa39":"markdown","8ebc112a":"markdown","4f36e135":"markdown","855d8ef7":"markdown","dc41c15f":"markdown","fcfee888":"markdown","33172f95":"markdown","741694fd":"markdown"},"source":{"b535aa92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\"))\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nsns.set_palette('muted')\n# Any results you write to the current directory are saved as output.","9587397f":"df = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","a9ba9da1":"df.head()","617280ae":"sns.countplot(x = 'SeniorCitizen', data = df, hue = 'gender')","edea2b90":"sns.countplot(x = 'Contract', data = df, hue = 'gender')","d4c4061e":"sns.countplot(x = 'DeviceProtection', data = df, hue = 'gender')","27f4ad15":"# Categorizing Male and Female in 1s and 0s\ndef gender_labels(element):\n    if element == 'Male':\n        return 0\n    elif element == 'Female':\n        return 1\n# Making a new column in the dataframe\ndf['GenderLabel'] = df['gender'].apply(gender_labels)\n\n#Dropping the original gender column\ndf.drop(['gender'] ,axis = 1, inplace=True)    ","eb0c422a":"# Now, to relable the columns which have just \"Yes\" and \"No\" as their entries!\nlistOfColumns = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'TechSupport', 'Churn', 'StreamingMovies', 'StreamingTV', 'DeviceProtection', 'PaperlessBilling']\n\n# The Labelling Function\ndef Labelizer(input_value):\n    '''Returns 1 for a Yes and a 0 for any other No'''\n    if input_value == 'Yes':\n        return 1\n    else:\n        return 0\n    \nfor i in listOfColumns:\n    newCol = i+'_label'\n    df[newCol] = df[i].apply(Labelizer)\n\ndf.drop(listOfColumns, axis = 1, inplace=True)\n","24404063":"list_nonBinary = ['Contract', 'PaymentMethod', 'InternetService']\nfor i in list_nonBinary:\n    df = pd.concat([df, pd.get_dummies(df[i])], axis = 1)\n    df.drop([i], axis = 1, inplace=True)\n\n#print(\"Post feature Engineering, the columns are as follows : \", df.columns.values)","b62e4136":"df['TotalChargesNew'] = df['tenure']*df['MonthlyCharges']\ndf.drop(['MonthlyCharges', 'TotalCharges'], axis = 1, inplace = True)","0d648bde":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","1ce55040":"#Dropping the Customer ID for obvious reasons\ndf.drop(['customerID'], axis = 1, inplace=True)","43b70b83":"from sklearn.model_selection import train_test_split\ny = df['TotalChargesNew']\nX = df.drop(['TotalChargesNew'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","46267fce":"from sklearn.tree import DecisionTreeRegressor\nmyRegressor = DecisionTreeRegressor(criterion='mse')\nmyRegressor.fit(X_train, y_train)\nprediction = myRegressor.predict(X_test)","edc5835d":"final_df_Decision = pd.DataFrame({'Predictions':prediction, 'True' : y_test})\nfinal_df_Decision.head()","0460de48":"Prediction_Line = go.Scatter(\n    x = [i for i in range(250)],\n    y = prediction[:250]\n)\nActual_Line = go.Scatter(\n    x = [i for i in range(250)],\n    y = y_test.values[:250]\n)\n\ndata = [Prediction_Line, Actual_Line]\niplot(data)","5c2f2cf5":"from sklearn.metrics import mean_squared_error\nprint(\"Decision Tree metrics are about accurate to %.2f dollars (+ and -)\"% (mean_squared_error(prediction, y_test)**0.5))","ff11c96a":"from sklearn.metrics import accuracy_score,mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nclf2 = LinearRegression()\nclf2.fit(X_train, y_train)\npreds2 = clf2.predict(X_test)\n\nprint(\"RMSE Score for Linear Regressor is %.2f\"%(mean_squared_error(y_test, preds2))**0.5)\n","aa416cd3":"from sklearn.ensemble import GradientBoostingRegressor\nclf3 = GradientBoostingRegressor()\nclf3.fit(X_train, y_train)\npreds3 = clf3.predict(X_test)\n\nprint(\"RMSE Score for Gradient Boost Regressor is %.2f\"%(mean_squared_error(y_test, preds3))**0.5)","d152ea0e":"from xgboost import XGBRegressor\nclf4 = XGBRegressor()\nclf4.fit(X_train, y_train)\npreds4 = clf4.predict(X_test)\n\nprint(\"RMSE Score of XGBoost Regressor is %.2f\"%(mean_squared_error(y_test, preds4))**0.5)","847e91ab":"from keras.models import Sequential\nfrom keras.layers import (Dense, Dropout, BatchNormalization)\n\nmodel = Sequential()\nmodel.add(Dense(25, input_dim = 25, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.9))\n\nmodel.add(Dense(12, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dropout(0.9))\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss= 'mse')\n","500e687e":"model.summary()\nmodel.lr = 0.05","9eb75bf7":"history = model.fit(X_train, y_train, epochs=1000, verbose=2)","e99f515a":"benchmarks = pd.DataFrame({\"Naive Deep Learning Model\" : (sum(history.history['loss'])\/len(history.history['loss']))**0.5,\n                          \"XG Boost Regressor\" : mean_squared_error(y_test, preds4)**0.5,\n                          \"Gradiant Boost Regressor\" : mean_squared_error(y_test, preds3)**0.5,\n                          \"Decision Tree Regressor\" : mean_squared_error(y_test, prediction)**0.5,\n                          \"Linear Regressor\" : mean_squared_error(y_test, preds2)**0.5\n                          }, index = range(1)).T\nbenchmarks.columns=['RMSE']\nbenchmarks['Regressor'] = benchmarks.index\nbenchmarks.index = range(5)\nbenchmarks","e9522d4a":"Looking at the scenario, not many customers prefer Device Protection If offered. But for the sake of business, the company may leverage into offering a wider internet access and there maybe a **30-40%** chance of getting a new preemium customer who may opt for Device Protection. Similar scenarios can be scene in Tech Support too!","1cace5b6":"# The Dataset!","1faacf1c":"# DeepLearning Model Design!\n\nThe following is the model design of our naive model which we will be testing our dataset upon","edd077c3":"# Model Making\n\nI will be using the following models to checkout the estimation performance!\n1. Decision Tree Regressor\n2. XGBoost Regressor\n3. Linear Regressor\n4. Gradient Boost Regressor\n5. A Naive Deep Learning Model","8c5b7cd7":"# Note : The RMSE score is almost equivalent to the actual tolerance of the prediction.\n\n## As you may have noticed by now, XG Boost and Gradient Boost comes out on top with an approximate error of 111 dollars!","2fafd756":"Checking out our predicitons along side by making a simple dataset ","2ec6966e":"# Linear Regression","5ddad339":"# Simple Train Test split","7c9795c0":"# Gradient Boost Regressor","9a1de9dd":"# XGBoost Regressor","293447f8":"With the binary operations all done, its time to fix our remaining columns with **One Hot Encoding** utility","1ecb280b":"As one might guess, 0 represents that the customer is not a senior citizen . Hence, one may infer that only about 14% of the customers are actually 60+. A pretty classic scenario in current generation.","eb93d209":"# Feature Engineering\n\nNow let's move on to the feature engineering section. The motive of this part is to make sure that we convert our features in algorithm processable quantities.\n\nWhat I will be doing is mostly applying* One Hot Encodings in the columns with >2 unique values* and explicitly change **Gender** Column to match the 0 and 1 categories.","549ffa39":"# The Curves matter\nAn excellent curve fitting result of first 250 test values and seeing how our model fares against them","8ebc112a":"# Price Estimations Using Multiple Regressors\n\nWelcome to the price estimation kernel by Uddeshya Singh\n\n![](https:\/\/aia.es\/wp-content\/uploads\/2012\/10\/price_estimation.jpg)\n\nI will be covering:\n* Basic EDA\n* Feature Engineering\n* Deep Learning Modelling\n* Benchmarking Scheme\n\nAll for the **Total Price** of a particular customer","4f36e135":"# Exploratory Data Analysis\nLet's go through various visualisations to have a deeper grasp at what our data really represents!","855d8ef7":"## The Total Charges Dilemma\nOne thing which I shoud mention here is that I couldn't really fix the total charges columns to the datatype of **float** by the convention *.astype*.\nSo I confirmed to the maneover mentioned below to get through that errors!","dc41c15f":"# Benchmarks\n\nHave a look at the benchmark performances and decide for yourselves, which is the best regressor and whom you are going to opt!","fcfee888":"Another inference that might be drawn from the Contract's count plot that **Month-to-Month** plans are best served and preferred among the consumers. To attract retentivity, one may think about offering **Free Subscriptions and premium support** for the first month as a trial!","33172f95":"# Decision Tree Regressor","741694fd":"With the step one of feature engineering done, let's move on to code our utility to put **binary labels** on the columns with unique values of **Yes, No and No Internet**!"}}