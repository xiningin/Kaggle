{"cell_type":{"7ffc33cc":"code","8c75de54":"code","a2bdf433":"code","d0866c86":"code","6f04a8e4":"code","c013bc48":"code","3a29fa80":"code","6002aafe":"code","1f14ae0d":"code","f398eacf":"code","edbe27e1":"code","5d0f557a":"code","45169540":"code","2ca07b8d":"code","c2349732":"code","501780f4":"code","f24ccc0a":"code","f65b4cee":"code","fcc336ec":"code","d84ce311":"code","551d2bf5":"code","f5767c41":"code","af80c845":"markdown","707b80d9":"markdown","b18f3c76":"markdown","8afa7292":"markdown","c3d71e28":"markdown","54606ec4":"markdown","bde10e43":"markdown","5f7e133f":"markdown","8e6634c1":"markdown","d60fa6fa":"markdown","d1cfc2bc":"markdown"},"source":{"7ffc33cc":"!pip install sentence_transformers","8c75de54":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom sentence_transformers import *\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport matplotlib.cm as cm\nimport torch\nfrom nlp_package_pv import *\nimport imagehash\nimport cv2\nimport keras\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()","a2bdf433":"train_path='..\/input\/shopee-product-matching\/train_images\/'\ntrain=pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest=pd.read_csv('..\/input\/shopee-product-matching\/test.csv')","d0866c86":"train.head()","6f04a8e4":"# Using the preprocessing function to preprocess the tweet data\npreprocess_tweet_data(train,'title')\n# Using tokenizer and removing the stopwords\nrem_stopwords_tokenize(train,'title')\n# Converting all the texts back to sentences\nmake_sentences(train,'title')","c013bc48":"train.head()","3a29fa80":"model=torch.load('..\/input\/bert-model-gpu-sentence-transformer\/Large_bert_model.h5')\n# device = torch.device(\"cuda\")\n# model.to(device)","6002aafe":"# Getting a model\n# model=SentenceTransformer('bert-large-nli-mean-tokens')","1f14ae0d":"embeddings = model.encode(train['title'])","f398eacf":"query='paper bag victoria secret'\nprint(query)","edbe27e1":"query_embedding = model.encode(query)\ntop_k=5\ncos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\ncos_scores = cos_scores.cpu()\n\n#We use torch.topk to find the highest 5 scores\ntop_results = torch.topk(cos_scores, k=top_k)\n\nprint(\"\\n\\n======================\\n\\n\")\nprint(\"Item Text:\", query)\nprint(\"\\nTop 5 most similar sentences in corpus:\")\nimage_id=[]\nfor score, idx in zip(top_results[0], top_results[1]):\n    image_id.append(train['image'].values[idx])\n    print(train['title'].values[idx], \"(Score: %.4f)\" % (score),\"Image Id :%s\" % train['image'].values[idx])","5d0f557a":"# Making a function to convert the image to fixed length for subplots\ndef show_image(main_path,image_path,dim=(256,256)):\n    im=cv2.imread(main_path+image_path)\n    im=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n    im=cv2.resize(im,dim)\n    plt.imshow(im)\n","45169540":"# Let's have a look at the similar images\nfor i in range(2):\n    plt.subplot(1,2,i+1)\n    show_image(train_path,image_id[i])\n","2ca07b8d":"def preprocess_text(dataset,name):\n    # Using the preprocessing function to preprocess the tweet data\n    preprocess_tweet_data(dataset,name)\n    # Using tokenizer and removing the stopwords\n    rem_stopwords_tokenize(dataset,name)\n    # Converting all the texts back to sentences\n    make_sentences(dataset,name)\ndef make_embeddings(dataset,name):\n    embeddings = model.encode(dataset[name])\n    return embeddings\n    ","c2349732":"# We Define a threshold for our score so we get reasonable results\ndef return_results(dataset=train,embedding=embeddings,number=50,ultimate_point=0.95):\n    result=[]\n    if number>len(dataset):\n        number=len(dataset)\n    for i in dataset['title'].values :\n        query_embedding = model.encode(i)\n        top_k=number\n        cos_scores = util.pytorch_cos_sim(query_embedding, embedding)[0]\n        cos_scores = cos_scores.cpu()\n\n        #We use torch.topk to find the highest 5 scores\n        top_results = torch.topk(cos_scores, k=top_k)\n        s=''\n        count=0\n        for score, idx in zip(top_results[0], top_results[1]):\n            if score>=ultimate_point :\n                count+=1\n                if count==1:\n                    s+=dataset['posting_id'].values[idx]\n                else:\n                    s+=' '+dataset['posting_id'].values[idx]\n        result.append(s)\n    return result\n        \n                \n        \n        ","501780f4":"# Let's try this on the test_data\n# Preprocessing text on test data\npreprocess_text(test,'title')\n\n# Creating Embeddings for the test data title columns\nembeddings_test=make_embeddings(test,'title')\n\n","f24ccc0a":"return_results(test,embeddings_test)","f65b4cee":"ss=pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')","fcc336ec":"ss.head()","d84ce311":"ss['matches']=return_results(test,embeddings_test)","551d2bf5":"ss.to_csv('submission.csv',index=False)","f5767c41":"ss.head()","af80c845":"# Converting The Text To Word Embeddings","707b80d9":"# Importing Model From Sentence Transformer","b18f3c76":"# Importing Data","8afa7292":"# Adding result to submission file","c3d71e28":"# Getting Relatable Text From The Data And It's Score","54606ec4":"## Let's Try to make a dataframe which has top 50 results using above method","bde10e43":"# Up till now we predicted similar products using just text :)","5f7e133f":"# Showing Similar Data","8e6634c1":"# Let's try to improve this method using phash","d60fa6fa":"# Importing Packages","d1cfc2bc":"# Making a function to return results"}}