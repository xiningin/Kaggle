{"cell_type":{"4b4b5837":"code","48c02229":"code","5c0af116":"code","8ac80e15":"code","3530706d":"code","754dc47f":"code","64229ade":"code","91c3cd35":"code","70b1c2d6":"code","553d1173":"code","a088ee86":"code","c8edabb4":"code","6733659b":"code","db14598e":"code","be294430":"code","348f7f5b":"code","6f13a00e":"code","a4883228":"code","ea408e42":"code","819858fe":"code","317567b3":"code","0e61af92":"code","ec7d4dd4":"code","aa517bcf":"code","0581ccfa":"code","45e9ca26":"code","093f4a71":"code","74f69c13":"code","123db579":"code","3861eaa5":"code","2221a32e":"code","f5490ea4":"code","81e238b5":"code","5c8e3494":"code","ddbeaf58":"code","586a4b17":"markdown","cc289b43":"markdown","a303788e":"markdown","fec3b14f":"markdown","f826c922":"markdown","4dcc4823":"markdown","878e792d":"markdown","7b482d23":"markdown","ff7af8c5":"markdown","b42a89af":"markdown","4444bc01":"markdown","f8c9662e":"markdown","28a54c41":"markdown","4563b33c":"markdown","c63709fd":"markdown","7bc4b9fa":"markdown"},"source":{"4b4b5837":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport glob\n\ntf.__version__","48c02229":"os.listdir(r'..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/annotations\/trimaps')[-5:]","5c0af116":"my_img = tf.io.read_file(r'..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/annotations\/trimaps\/Ragdoll_85.png')\nmy_img = tf.image.decode_png(my_img)\nmy_img.shape","8ac80e15":"my_img = tf.squeeze(my_img)\nmy_img.shape","3530706d":"plt.imshow(my_img)","754dc47f":"my_image = tf.io.read_file(r'..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/images\/Ragdoll_85.jpg')\nmy_image = tf.image.decode_jpeg(my_image)\nplt.imshow(my_image)","64229ade":"my_img.numpy().max(), my_img.numpy().min()","91c3cd35":"np.unique(my_img.numpy())","70b1c2d6":"images = glob.glob(r'..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/images\/*.jpg')\nlen(images)","553d1173":"anno = glob.glob(r'..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/annotations\/trimaps\/*.png')\nimages[:3] + anno[:3]","a088ee86":"anno_names = [path.split('\/trimaps\/')[1] for path in anno]\nimg_names = [path.split('\/images\/')[1] for path in images]\nimg_names[:3] + anno_names[:3]","c8edabb4":"anno_names.sort()\nimg_names.sort()","6733659b":"img_names[:3] + anno_names[:3]","db14598e":"anno = ['..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/annotations\/trimaps\/' + name for name in anno_names]\nimages = ['..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/images\/' + name for name in img_names]\nimages[:3] + anno[:3]","be294430":"np.random.seed(2020)\nindex = np.random.permutation(7390)\nindex","348f7f5b":"images = np.array(images)[index]\nanno = np.array(anno)[index]\n\nds = tf.data.Dataset.from_tensor_slices((images, anno))\n\ntrain_ds = ds.skip(1500)\ntest_ds = ds.take(1500)","6f13a00e":"def read_jpg(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img\n    \ndef read_png(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=1)\n    return img","a4883228":"def normalize_img(input_img, input_anno):\n    input_img = tf.cast(input_img, tf.float32)\n    input_img = input_img\/127.5-1\n    input_anno -= 1\n    return input_img, input_anno","ea408e42":"def load_img(input_img, input_anno):\n    input_img = read_jpg(input_img)\n    input_anno = read_png(input_anno)\n    input_img = tf.image.resize(input_img, (224, 224))\n    input_anno = tf.image.resize(input_anno, (224, 224))\n    return normalize_img(input_img, input_anno)","819858fe":"train_ds = train_ds.map(load_img,\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest_ds = test_ds.map(load_img,\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ntrain_ds = train_ds.repeat().shuffle(100).batch(8)\ntest_ds = test_ds.batch(8)\n\ntrain_ds","317567b3":"for img, anno in train_ds.take(1):\n    plt.subplot(1, 2, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))\n    plt.subplot(1, 2, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(anno[0]))","0e61af92":"conv_base = tf.keras.applications.VGG16(weights='imagenet',\n                                        input_shape=(224, 224, 3),\n                                        include_top=False)\nconv_base.summary()","ec7d4dd4":"layer_names = ['block5_conv3',\n               'block4_conv3',\n               'block3_conv3',\n               'block5_pool']\n\noutputs = [conv_base.get_layer(name).output for name in layer_names]\n\nmulti_output_model = tf.keras.models.Model(inputs=conv_base.input,\n                                           outputs=outputs)","aa517bcf":"multi_output_model.trainable = False\n\ninputs = tf.keras.layers.Input(shape=(224, 224, 3))\n\nout_block5_conv3, out_block4_conv3, out_block3_conv3, out = multi_output_model(inputs)\n\nout.shape","0581ccfa":"out_block5_conv3.shape","45e9ca26":"x1 = tf.keras.layers.Conv2DTranspose(512, 3,\n                                     strides=2, \n                                     padding='same',\n                                     activation='relu')(out)\n\nx1.shape","093f4a71":"x1 = tf.keras.layers.Conv2D(512, 3,\n                            padding='same',\n                            activation='relu')(x1)\n\nx1.shape","74f69c13":"x2 = tf.add(x1, out_block5_conv3)\nx2.shape","123db579":"x2 = tf.keras.layers.Conv2DTranspose(512, 3,\n                                     strides=2, \n                                     padding='same',\n                                     activation='relu')(x2)\nx2 = tf.keras.layers.Conv2D(512, 3,\n                            padding='same',\n                            activation='relu')(x2)\n\nx3 = tf.add(x2, out_block4_conv3)\n\nx3 = tf.keras.layers.Conv2DTranspose(256, 3,\n                                     strides=2, \n                                     padding='same',\n                                     activation='relu')(x3)\nx3 = tf.keras.layers.Conv2D(256, 3,\n                            padding='same',\n                            activation='relu')(x3)\n\nx4 = tf.add(x3, out_block3_conv3)\n\nx4.shape","3861eaa5":"x5 = tf.keras.layers.Conv2DTranspose(64, 3,\n                                     strides=2, \n                                     padding='same',\n                                     activation='relu')(x4)\n\nx5 = tf.keras.layers.Conv2D(64, 3,\n                            padding='same',\n                            activation='relu')(x5)\n\nprediction = tf.keras.layers.Conv2DTranspose(3, 3,\n                                     strides=2, \n                                     padding='same',\n                                     activation='softmax')(x5)","2221a32e":"model = tf.keras.models.Model(inputs=inputs,\n                              outputs=prediction)\nmodel.summary()","f5490ea4":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='semantic_seg_fcn.png', show_shapes=True, show_layer_names=True)","81e238b5":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n\nhistory = model.fit(train_ds, epochs=5, steps_per_epoch=(7390-1500)\/\/8, validation_data=test_ds, validation_steps=1500\/\/8)","5c8e3494":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(5)\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation Loss')\nplt.title('Training & Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","ddbeaf58":"n = 3\n\nfor img, mask in test_ds.take(1):\n    \n    pred_mask = model(img)\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    # ... -> take all the dimensions of the original pred_mask, tf.newaxis -> expand to (224,224, 1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    \n    for i in range(n):\n        plt.subplot(n, 3, i*n+1)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(img[i]))\n        plt.subplot(n, 3, i*n+2)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[i]))\n        plt.subplot(n, 3, i*n+3)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(pred_mask[i]))","586a4b17":"Likewise,","cc289b43":"**FCN** --> Fullly Convolutional Network\n\nConvolutional Layers (like the ones used for classification) + Upsampling (Transposed Convolution)\n\n**'Skip'** --> Integrate Holistic Features with Local Ones\n\n*Cons:* \n* The results are NOT accurate, not sensible to details;\n* Didn't take the relationship between pixels into consideration, lack of consistency in space.","a303788e":"Validation loss gradually increases --> A sign of overfitting","fec3b14f":"Add another convolutional layer to extract features (without changing the size)","f826c922":"### Should be Classified into 3 Parts, i.e. Background, Edge, Entity(?)","4dcc4823":"## Make Predictions","878e792d":"## Preprocessing ","7b482d23":"### Get a Feel of It","ff7af8c5":"## Use plot_model to Visualize the NN Built","b42a89af":"### Process the Datasets","4444bc01":"### Double-Check","f8c9662e":"### Sort the Arrays So That Images & Annos are Consistent","28a54c41":"## Use of Funtional API to Construct a Model with Multiple Outputs","4563b33c":"## Compile & Fit the Model, Evaluation","c63709fd":"## Import Convolutional Base","7bc4b9fa":"## Prep for Modeling Using 'Skip'\n\n(7, 7, 512) --> Upsampling, (14, 14, 512) add with block5_conv3\n\nconv_base.get_layer('block5_conv3').output\n\nsub_model = tf.keras.models.Model(inputs=conv_base.input,\n                                  outputs=conv_base.get_layer('block5_conv3').output)\n                                  \nThe sub_model will inherit the trained weights of conv_base."}}