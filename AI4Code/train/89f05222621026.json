{"cell_type":{"8876c7fb":"code","87200934":"code","4bd711d9":"code","03034e4b":"code","f52d70e1":"code","e81ebfcb":"code","4e6ea58b":"code","ece30e35":"code","95df2ff1":"code","517e706d":"code","3e2cb2a4":"code","b3090011":"code","a866349d":"code","740c7c09":"code","f691ed40":"code","621b475a":"code","af0d2f59":"code","d02769b7":"code","3a33e4e3":"code","3fde0774":"code","4b8c79fc":"code","4967f959":"code","f1908429":"code","7a8173d1":"code","23ad4469":"code","7bbf70e1":"code","2685794e":"code","0d6f493b":"code","36d13313":"code","ad420136":"code","db3fe3e5":"code","a4716d99":"code","60e3a56c":"code","2eae1bbf":"code","b02cf6ab":"code","221f22b4":"code","4795a3ae":"code","981d6301":"code","fd6e55ca":"code","c6d9e3cc":"code","d13a0223":"code","9cbd2c35":"code","05678f43":"code","af324b84":"code","1abdd593":"code","ab717d77":"code","381ed7ff":"code","b2d9446d":"code","9de40851":"code","4546a40d":"code","555e0506":"code","7506b399":"code","965132da":"code","feaaca1b":"code","6e687ae7":"code","cb36d46e":"code","e92ff9ea":"markdown","de338d7b":"markdown","bb494510":"markdown","89f01382":"markdown","f2cf7573":"markdown","4675befe":"markdown","2042e958":"markdown","4c40f23a":"markdown","b82b9e54":"markdown","a41d8142":"markdown","f4246015":"markdown","2354c385":"markdown"},"source":{"8876c7fb":"!pip install pandas==0.23.4\n!pip install seaborn==0.9.0\n","87200934":"#!kaggle datasets download -d edalrami\/19000-spotify-songs","4bd711d9":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns; sns.set_style(\"whitegrid\")\nfrom matplotlib import pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nsns.__version__","03034e4b":"import os\n\nprint(os.chdir(\"..\/input\"))\n","f52d70e1":"df_data=pd.read_csv(\"song_data.csv\",sep=\",\",engine=\"python\")\n\ndf_info=pd.read_csv(\"song_info.csv\",sep=\",\",engine=\"python\")","e81ebfcb":"df_data.head(5)","4e6ea58b":"df_data.info()","ece30e35":"df_data.describe().T","95df2ff1":"#Well behaved dataset - no NA\n\n\ndf_data.isna().sum()","517e706d":"#Some unnecessary repetitions observed\n\ndf_data.sort_values(\"song_popularity\",ascending=False).head(10)","3e2cb2a4":"#Original database shape\ndf_data.shape","b3090011":"#After duplicate elimination dataframe shape\ndf_data[df_data.duplicated(subset=\"song_popularity\",keep=\"first\")].shape","a866349d":"df_data=df_data[df_data.duplicated(subset=\"song_popularity\",keep=\"first\")].reset_index(drop=\"index\")\n\ndf_data.head(3)","740c7c09":"#2 different types of variables - categoricals (param_cat) and numerical(param)\n\nparam_cat=[\"key\",\"audio_mode\",\"time_signature\"]\n\nparam=[\"song_duration_ms\",\n\"acousticness\",\n\"danceability\",\n\"energy\",\n\"instrumentalness\",\n\"liveness\",\n\"loudness\",\n\"speechiness\",\n\"tempo\",\n\"audio_valence\"]","f691ed40":"#Distribui\u00e7\u00e3o geral dos dados\n\nfor feature in param:\n  g = sns.FacetGrid(df_data,col=\"audio_mode\") \n  g.map(sns.distplot, feature) \n  \nplt.plot()","621b475a":"#Mapa de correla\u00e7\u00e3o com matrix sem tratamento de outliers\n\nplt.figure(figsize=(9, 6))  # Aumenta o tamanho da figura\nax=sns.heatmap(\n    pd.concat([df_data[param],df_data[\"song_popularity\"]],axis=1).corr(),\n    vmin=-1, vmax=1, annot=True, fmt='.2f')\nplt.show()","af0d2f59":"# Scatter plot e regress\u00e3o com matrix sem tratamento de outliers para vari\u00e1veis categoricas\n\n#Nota-se que time signature possui indica\u00e7\u00e3o de correla\u00e7\u00e3o positiva com popularidade do som - \u00faltima linha\n\nplt.figure(figsize=(9, 6))\ng = sns.pairplot(\n    pd.concat([df_data[param_cat],df_data[\"song_popularity\"]],axis=1).sample(n=3000),\n    kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.05}}\n)\n","d02769b7":"# Scatter plot e regress\u00e3o com matrix sem tratamento de outliers para vari\u00e1veis num\u00e9ricas\n\n#Nota-se que dan\u00e7abilidade e barulheira possui indica\u00e7\u00e3o de correla\u00e7\u00e3o positiva com popularidade do som - \u00faltima linha\n\nplt.figure(figsize=(9, 6))\ng = sns.pairplot(\n    pd.concat([df_data[param],df_data[\"song_popularity\"]],axis=1).sample(n=3000),\n    kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.03}}\n)\n","3a33e4e3":"df_info.head(2)","3fde0774":"#Word cloud\n\n# fig, ax = plt.subplots(figsize = (16, 12))\n# plt.subplot(1, 2, 1)\n# text_cat = ' '.join(all_data.loc[all_data['Type'] == 'Cat', 'Name'].fillna('').values)\n\n# wordcloud = WordCloud(max_font_size=None, background_color='white',\n#                       width=1200, height=1000).generate(text_cat)\n# plt.imshow(wordcloud)\n# plt.title('Top cat names')\n# plt.axis(\"off\")\n\n# plt.subplot(1, 2, 2)\n# text_dog = ' '.join(all_data.loc[all_data['Type'] == 'Dog', 'Name'].fillna('').values)\n# wordcloud = WordCloud(max_font_size=None, background_color='white',\n#                       width=1200, height=1000).generate(text_dog)\n# plt.imshow(wordcloud)\n# plt.title('Top dog names')\n# plt.axis(\"off\")\n\n# plt.show()","4b8c79fc":"#Boxplot univariado com as vari\u00e1veis num\u00e9ricas\n\nfor position,feature in enumerate(param):  \n  plt.figure(figsize=(3, 3))\n  plt.figure(position)  \n  plt.title(feature)\n  sns.boxplot(data=df_data,y=feature)\n  plt.plot()\n  \n\n  \n#plt.tight_layout()\n#plt.show()\n            \n#y=\"song_duration_ms\",hue=\"song_popularity\") ","4967f959":"#Corte dos valores que est\u00e3o fora do invervalo de confian\u00e7a 66%,95% e 99% - sendo que ser\u00e1 utilizado o intervalo de 99% para este exerc\u00edcio\n\ndf_data_66=df_data[df_data[param].apply(lambda x: np.abs(x - x.mean()) \/ x.std() < 1).all(axis=1)]\ndf_data_95=df_data[df_data[param].apply(lambda x: np.abs(x - x.mean()) \/ x.std() < 2).all(axis=1)]\ndf_data_99=df_data[df_data[param].apply(lambda x: np.abs(x - x.mean()) \/ x.std() < 3).all(axis=1)]","f1908429":"#An\u00e1lise de boxplot com uso de confidence interval of 99%\n\nfor position,feature in enumerate(param):  \n  plt.figure(figsize=(3, 3))\n  plt.figure(position)  \n  plt.title(feature)\n  sns.boxplot(data=df_data_95,y=feature)\n  plt.plot()","7a8173d1":"#Mesmo empregando intervalos diferentes, nota-se que que as correla\u00e7\u00f5es positivas e negativas mais fortes permanecem\n\nplt.figure(figsize=(12, 9))\n\nplt.subplot(3,1,1)\n\nplt.title(\"Original matrix\")\n\n#plt.figure(figsize=(9, 6))  # matrix original\nax=sns.heatmap(\n    pd.concat([df_data[param],df_data[\"song_popularity\"]],axis=1).corr(),\n    vmin=-1, vmax=1, annot=True, fmt='.2f')\n\nplt.title(\"66 % matrix\")\n\nplt.subplot(3,1,2)\n#matrix 66\nax=sns.heatmap(\n    pd.concat([df_data_66[param],df_data_66[\"song_popularity\"]],axis=1).corr(),\n    vmin=-1, vmax=1, annot=True, fmt='.2f')\n\n\nplt.title(\"99% matrix\")\nplt.subplot(3,1,3)\n#matrix 99\n#plt.figure(figsize=(9, 6))  # Aumenta o tamanho da figura\nax=sns.heatmap(\n    pd.concat([df_data_99[param],df_data_99[\"song_popularity\"]],axis=1).corr(),\n    vmin=-1, vmax=1, annot=True, fmt='.2f')\n\nplt.show()","23ad4469":"\n#Histograma de corte para an\u00e1lise - top 80 at\u00e9 100 versus 0 at\u00e9 20\n\nplt.figure(figsize=(5, 3))\n\ndf_data_99[\"song_popularity\"].hist()\n\nplt.axvline(x=20,color=\"red\")\n\nplt.axvline(x=80,color=\"red\")","7bbf70e1":"#Corte da base de dados considerando apenas TOP E HIT\ndf_data_versus=df_data_99.loc[(df_data_99[\"song_popularity\"]>=80) | (df_data_99[\"song_popularity\"]<=20)].reset_index(drop=\"index\")","2685794e":"#Cria\u00e7\u00e3o de categoria TOP E HIT\ndf_data_versus[\"top\"] = df_data_versus[\"song_popularity\"].apply(lambda x : \"Hit\" if x >=80 else \"Flop\")\n\ndf_data_versus.head(5)","0d6f493b":"#Scatter plot com regress\u00e3o entre as duas categorias para diversas vari\u00e1veis \n\n#Nota-se que quase todas as vari\u00e1veis apresentaram Slope diferente de zero - seja positivo ou negativo, mostrando poss\u00edveis diferen\u00e7as de valor \n\nplt.figure(figsize=(9, 6))\nfor feature in param:\n  #sns.factorplot(data=df_data_versus,x=\"song_popularity\",y=feature,hue=\"top\",kind=\"strip\")\n  sns.lmplot(data=df_data_versus,x=\"song_popularity\",y=feature,hue=\"top\",fit_reg=True, markers=[\"o\", \"x\"])\n  \n  #sns.(data=df_data_versus,x=\"song_popularity\",y=feature,hue=\"top\")","36d13313":"#Teste estat\u00edstico de m\u00e9dia T-test para todas as vari\u00e1veis num\u00e9ricas - \n#todas elas apresentaram diferen\u00e7a de m\u00e9dia estat\u00edsticamente diferente (99%) entre estas duas categorias\n#Assim, h\u00e1 indica\u00e7\u00e3o de que essas 2 categorias possuem diferen\u00e7as quanto a todos os atributos utilizados\n\nfrom scipy import stats\n\n\nfor feature in param:\n\n  rvs1=df_data_versus.loc[(df_data_versus[\"top\"]==\"Hit\"),feature]\n\n  rvs2=df_data_versus.loc[(df_data_versus[\"top\"]==\"Flop\"),feature]\n\n  t,p=stats.ttest_ind(rvs1,rvs2, equal_var = False)\n  \n  print(\"The T-test between Hit and Flop using mean value of {} is {:03.3f} and p-value of {:03.3f}\".format(feature,t,p))","ad420136":"df_data_99.head(2)","db3fe3e5":"#dummy transformation das vari\u00e1veis categoricas\n\ndf_data_99 = pd.get_dummies(data=df_data_99,columns=[\"key\",\"audio_mode\",\"time_signature\"])","a4716d99":"# Padroniza\u00e7\u00e3o dos valores num\u00e9ricos via standard scale\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nfor column in pd.concat([df_data_99[\"song_duration_ms\"],df_data_99.select_dtypes(\"float64\")],axis=1).columns:\n  \n  scaler.fit(df_data_99[column].values.reshape(-1, 1))\n  \n  df_data_99[column]=scaler.transform(df_data_99[column].values.reshape(-1, 1))","60e3a56c":"#C\u00f3pia dos dados em 2 outras bases diferentes - uma utilizada para regressao e outra para classificacao de song_popularity\n\ndf_data_99_class=df_data_99.copy()\n\ndf_data_99_regress=df_data_99.copy()","2eae1bbf":"#Classifica\u00e7\u00e3o da popularidade em 4 etiquetas sendo A - melhor e D - pior\ndf_data_99_class[\"song_popularity\"] = df_data_99_class[\"song_popularity\"].apply(lambda x : \"A\" if x>=75 else (\"B\" if x>=50 and x<=75 else (\"C\" if x>=25 and x<=50 else \"D\")))","b02cf6ab":"#X and y selection\n\ny=df_data_99_class.song_popularity\n\nX=df_data_99_class.iloc[:,2:]","221f22b4":"#split\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y,test_size=0.2, random_state=1)","4795a3ae":"#Modelo possui precis\u00e3o m\u00e9dia de 39 mas ainda h\u00e1 bastante espa\u00e7o \n#para melhora do modelo\n\n#classifier Naives\n\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.metrics import classification_report\n\n\nM_mult=MultinomialNB()\n\n#M_mult.get_params\n\ntuned_parameters = {\n    'alpha': [1, 1e-1, 1e-2]\n}\n\nclf=GridSearchCV(M_mult,tuned_parameters,cv=10)\n\nclf.fit(abs(train_X),train_y)\n\nprint(classification_report(test_y, clf.predict(test_X), digits=4))","981d6301":"#classifier KNeighborsClassifier\n#Apresentou performance um pouco acima da modelo multinomial de Naives utilizando 2-3 clusters de an\u00e1lise\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nkmeans=KNeighborsClassifier()\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(train_X,train_y)\n    \n    train_scores.append(knn.score(train_X,train_y))\n    test_scores.append(knn.score(test_X,test_y))\n\n","fd6e55ca":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","c6d9e3cc":"df_data_99_regress.head(2)","d13a0223":"#X and y usando base de regress\u00e3o\n\ny=df_data_99_regress.song_popularity\n\nX=df_data_99_regress.iloc[:,2:]","9cbd2c35":"#Split de dados\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y,test_size=0.2, random_state=1)","05678f43":"#Funcao para erro medio ao quadrado para funcoes linear normalizados como Ridge e Lasso\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train_X, train_y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","af324b84":"#Otimiza\u00e7\u00e3o de par\u00e2metros\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75,100,200,300,400,500,600]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","1abdd593":"#O valor 300 de alpha \u00e9 o ponto \u00f3timo para diminui\u00e7\u00e3o do erro da fun\u00e7\u00e3o mas ainda assim n\u00e3o h\u00e1 grandes ganhos\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Ridge\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","ab717d77":"cv_ridge.min()","381ed7ff":"#Uso de modelo Lasso\n\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(train_X, train_y)","b2d9446d":"rmse_cv(model_lasso).mean()","9de40851":"coef = pd.Series(model_lasso.coef_, index = train_X.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","4546a40d":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])","555e0506":"#Vari\u00e1veis mais importantes e as menos importantes para explicar popularidade\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","7506b399":"#Deep learning\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.regularizers import l1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","965132da":"model = Sequential()\n\nmodel.add(Dense(256, activation=\"relu\", input_dim = train_X.shape[1]))\nmodel.add(Dense(1, input_dim = train_X.shape[1], W_regularizer=l1(0.001)))\n\nmodel.compile(loss = \"mse\", optimizer = \"adam\")","feaaca1b":"model.summary()","6e687ae7":"hist = model.fit(train_X, train_y, validation_data = (test_X, test_y),epochs=50)","cb36d46e":"#Valores de previs\u00e3o\npd.Series(model.predict(test_X)[:,0]).hist()","e92ff9ea":"Para este desafio, utilizou a mesma base de dado (Spotify) com a prepara\u00e7\u00e3o de dados utilizado no desafio anterior de explora\u00e7\u00e3o","de338d7b":"## Data general info","bb494510":"Grande parte das m\u00fasicas est\u00e3o na faixa mediana. O desafio \u00e9 como desenvolver boas m\u00fasicas ou m\u00fasicas ruins. O pr\u00f3posito desta an\u00e1lise \u00e9 comparar Hits musicais (popularidade acima ou igual a 80 - eu imagino que quanto mais alto o valor - mais popular \u00e9 este som) e FLOPs musicais (popularidade igual ou abaixo de 20). \n\nAlgumas perguntas emergem: \n\n\n* Com os atributos apresentados, consigo diferenciar entre essas duas classes? \n* Se, o que elas diferenciam entre si? \n* essas diferen\u00e7as s\u00e3o significativas? ","89f01382":"## Classification ","f2cf7573":"## Visual exploration of data","4675befe":"### D_frame sem tratamento de outliers","2042e958":"# Desafio Machine Learning","4c40f23a":"# Data exploration","b82b9e54":"## An\u00e1lise HIT versus FLOP musical (Percentil 20% versus Percentil 80%)","a41d8142":"## Data Prep","f4246015":"### D_frame com tratamento de outliers","2354c385":"## Regression"}}