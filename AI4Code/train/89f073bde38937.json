{"cell_type":{"ebaa5559":"code","64a5af19":"code","b2138338":"code","1d3f0964":"code","3626d316":"code","1ced54fd":"code","f9667686":"code","b7f6dc34":"code","6e82a4d7":"code","f976929b":"code","06f5258e":"code","8cc6262d":"code","ed1ca9fb":"code","47db4749":"code","e7955484":"code","2b0c6fde":"code","ba26d461":"code","fa692e3b":"code","d2f55dd3":"code","42070e7b":"code","d8560544":"code","88543b26":"code","c0bbb6b8":"code","c32a0473":"code","21101fbf":"code","ba091798":"code","91396320":"code","442cccc7":"code","9ca9845f":"code","882f9fe5":"code","e08175ac":"code","73a9cdbc":"code","a3b0c4cb":"code","92feb990":"code","eca12de4":"code","5e62c2f3":"code","eb620d9a":"code","3502e5f9":"markdown","6f617b60":"markdown","b073c3ae":"markdown","fb2780c1":"markdown","58224930":"markdown","f3503eb8":"markdown","b195bf11":"markdown","c71dde95":"markdown","aaab40fb":"markdown","6e27c5a6":"markdown","24766d5f":"markdown","963e4111":"markdown","1e4b0b1e":"markdown","de8b3030":"markdown","55d92544":"markdown","51da44b8":"markdown","3ef048ad":"markdown","0d44572f":"markdown","d5489823":"markdown","a4a90998":"markdown","56016974":"markdown","cc906587":"markdown","f1006591":"markdown","2c4ab8eb":"markdown","dde99a43":"markdown","6e2c09cc":"markdown","e23f8b2f":"markdown","28211d2c":"markdown","d95daea9":"markdown","a576ee19":"markdown","d91113a2":"markdown","7621c7a3":"markdown","259c2c25":"markdown","8534c8cf":"markdown","28ed226f":"markdown","2a7899bd":"markdown","189c2832":"markdown","eaf50fee":"markdown","d5ff47fb":"markdown"},"source":{"ebaa5559":"import numpy as np \nimport pandas as pd \nimport re\nimport sys, gc, os\nfrom IPython.display import display\n\nfrom scipy import stats\n\nimport shap\nshap.initjs()\nimport featuretools as ft\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler\nfrom sklearn.preprocessing import (StandardScaler, PowerTransformer, QuantileTransformer ,LabelEncoder, \n                                   OneHotEncoder, OrdinalEncoder)\n\nimport catboost as cb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor","64a5af19":"df_train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\n\nSEED = 1991\n\n\n### My and another lovely kagglers latest submissions with the best score  \n\nsub_295 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_001.csv')\nsub_202 = pd.read_csv('..\/input\/feb84202\/FEB84202.csv')\nsub_222 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_021.csv')\nsub_216 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_216.csv')\nsub_257 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_257.csv')\nsub_279 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_012.csv')\nsub_305 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_lgb_305.csv')\nsub_212 = pd.read_csv('..\/input\/tp-f-my-subs\/submission_037.csv')","b2138338":"def simple_eda(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('---')\n    print('Common information')\n    print('---')\n    print()\n    print(df.info())\n    \n    # missing values\n    print()\n    print('---')\n    if df.isna().sum().sum() == 0:\n        print('There are no missing values')\n        print('---')\n    else:\n        print('Detected')\n        display(df.isna().sum())\n    \n    \n    # applying describe() method for categorical features\n    print()\n    print('---')\n    print('Categorical columns')\n    print('Total {}'.format(len(df.select_dtypes(include='object').columns)))\n    print('---')\n    display(df.describe(include = 'object'))\n    \n    # same describe() but for continious features\n    print('---')\n    print('Continuous columns')\n    print('Total {}'.format(len(df.select_dtypes(include=['int', 'float']).columns)))\n    print('---')\n    display(df.describe())\n    \n    #checking for duplicated rows\n    if df.duplicated().sum() == 0:\n        print('---')\n        print('There are no duplicates')\n        print('---')\n    else:\n        print('---')\n        print('Duplicates found')\n        print('---')\n        display(df[df.duplicated()])\n    \n    print()\n    print('---')\n    print('End of the report')","1d3f0964":"simple_eda(df_train)","3626d316":"simple_eda(df_test)","1ced54fd":"target = df_train['target']\ndf_train.drop('target', axis=1, inplace=True)","f9667686":"CAT= df_train.select_dtypes(include='object').columns.tolist()","b7f6dc34":"idx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\n\nfor row in range(5):\n    for col in range(2):\n        data = df_train[CAT[idx]].value_counts()\n        sns.barplot(x = data.values, y = data.index, palette='deep', ax=axes[row, col])\n        axes[row,col].set_title(CAT[idx])\n        idx += 1","6e82a4d7":"NUM = df_train.select_dtypes('float64').columns.tolist()","f976929b":"plt.figure(figsize=(16,5))\nsns.violinplot(data=df_train[NUM], color='slategray')\nplt.title('Continuous features distribution');","06f5258e":"plt.figure(figsize=(10,10))\nsns.heatmap(df_train[NUM].join(target).corr(), square=True, linewidths=0.7, cmap=\"bone_r\");","8cc6262d":"plt.figure(figsize=(10,5))\nsns.histplot(target, color='slategray', stat='frequency');","ed1ca9fb":"len(target[target <= 4])","47db4749":"to_drop = target[target <= 4].index\ntarget.drop(to_drop, inplace=True)\ndf_train.drop(to_drop, inplace=True)\n\nplt.figure(figsize=(10,5))\nsns.histplot(target, color='slategray', stat='frequency');","e7955484":"CAT_01 = list(set(CAT).difference(set(['cat6'])))\nCAT_01","2b0c6fde":"#dummies_train = pd.get_dummies(df_train[CAT_01])\n#dummies_test = pd.get_dummies(df_test[CAT_01])\n\n#train = df_train[NUM].join(dummies_train)\n#test = df_test[NUM].join(dummies_test)","ba26d461":"#set(train.columns.tolist()).difference(set(test.columns.tolist()))","fa692e3b":"#train.drop('cat6_G', axis=1, inplace=True)\n#train.shape[1], test.shape[1]","d2f55dd3":"#for feature in CAT_01:\n#    le = LabelEncoder()\n#    le.fit(df_train[feature])\n#    df_train[feature] = le.transform(df_train[feature])\n#    df_test[feature] = le.transform(df_test[feature])","42070e7b":"train = df_train.copy()\ntest = df_test.copy()\n\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)\n\ntrain.shape, test.shape","d8560544":"cols = train.columns.tolist()\n\nct = ColumnTransformer([('onehot',OrdinalEncoder(), slice(len(CAT))),\n                        ('quantile',QuantileTransformer(random_state=SEED, n_quantiles=1500),\n                         slice(len(CAT),len(CAT) + len(NUM) + 5))])\n\ntrain = ct.fit_transform(train)\ntest = ct.transform(test)\n\ntrain = pd.DataFrame(train, columns = cols)\ntest = pd.DataFrame(test, columns = cols)\n\ntrain[CAT] = train[CAT] \/ 10\ntest[CAT] = test[CAT] \/ 10","88543b26":"def feat_eng(df):\n    \n    # Manually multiply and drop specific columns\n    \n    #df['cont_003'] = df['cont0'] * df['cont8']\n\n    df['cont001'] = df['cont8'] * df['cont0']\n    df['cont002'] = df['cont9'] * df['cont0']\n    df['cont003'] = df['cont9'] * df['cont5']\n    df['cont004'] = df['cont8'] * df['cont5']\n    df['cont005'] = df['cont2'] * df['cont4']\n    df['cont006'] = df['cont1'] * df['cont3']\n    df['cont007'] = df['cont13'] * df['cont1']\n    \n    #df['cat005'] = df['cat2'] * df['cat1']\n   # df['cat006'] = df['cat2'] * df['cat4']\n    \n    #df.drop('cont5', axis=1, inplace=True)\n    #df.drop('cont9', axis=1, inplace=True)\n    \n    return df","c0bbb6b8":"train = feat_eng(train)\ntest = feat_eng(test)\n\n#train.drop('id', axis=1, inplace=True)\n#test.drop('id', axis=1, inplace=True)\n\ntrain.shape, test.shape","c32a0473":"#to_transform = ['cont0', 'cont1', 'cont4', 'cont5', 'cont8', 'cont9', 'cont12'] ","21101fbf":"def feat_eng_01(df):\n    \n    es = ft.EntitySet(id = 'data')\n\n    original_cols = to_transform\n\n    es = es.entity_from_dataframe(entity_id = 'data', \n                              dataframe = df[original_cols], \n                              index = 'id', \n                              time_index = None)\n    \n    new_features, new_feature_names = ft.dfs(entityset = es, target_entity = 'data', \n                                 trans_primitives = ['multiply_numeric'])\n    \n    new_features.reset_index(drop=True, inplace=True)\n    new_features.drop(original_cols, axis=1, inplace=True)\n    \n    return new_features","ba091798":"#train_fe = feat_eng_01(train)\n#test_fe = feat_eng_01(test)\n\n#train_fe.index = train.index\n#test_fe.index = test.index\n\n#train = train.join(train_fe)\n#test = test.join(test_fe)\n\n#train.shape, test.shape","91396320":"#X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=SEED)","442cccc7":"xgb_params = {\n    'booster':'gbtree',\n    'n_estimators':20000,\n    'max_depth':5, \n    'eta':0.008,\n    'gamma':3.5,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.75,\n    'colsample_bytree':0.35,\n    'reg_lambda':0.23,\n    'reg_alpha':0.52,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    #'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","9ca9845f":"N_FOLDS = 10\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_xgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_xgb += model.predict(test)\/N_FOLDS","882f9fe5":"print(f'rmse on oof: {np.round(mean_squared_error(target, oof, squared=False),5)}')","e08175ac":"#shap_values = shap.TreeExplainer(model).shap_values(X_train)\n#shap.summary_plot(shap_values, X_train)\n#shap_interactions = model.predict(test)\n#shap_preds = model.predict(test)","73a9cdbc":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \n#plot_top_k_interactions(test.columns.tolist(), shap_interactions, 10)","a3b0c4cb":"#to_drop_shap = ['cat8_G', 'cat1_B', 'cat3_A', 'cont12', 'cont7', 'cat3_C', 'cat9_L', 'cat9_I', 'cont10']\n\n#train_shap = train.copy()\n#test_shap = test.copy()\n\n#train_shap.drop(to_drop_shap, axis=1, inplace=True)\n#test_shap.drop(to_drop_shap, axis=1, inplace=True)\n#train_shap.shape, train.shape","92feb990":"avg_pred_01 = sub_202['target'] * 0.6 + sub_212['target'] * 0.4\navg_pred_02 = sub_202['target'] * 1.4 - sub_212['target'] * 0.4\navg_pred_03 = sub_202['target'] * 1.4 - sub_305['target'] * 0.4\navg_pred_04 = sub_202['target'] * 1.3 - preds_folds_xgb * 0.3\n#avg_pred_05 = sub_216['target'] * 0.5 + preds_folds_xgb * 0.2 + sub_212['target'] * 0.3\n#mixed_pred = preds_folds_xgb * 0.6 + sub_84295['target'] * 0.4","eca12de4":"df_sub['target'] = avg_pred_01\ndf_sub.to_csv('submission_045.csv', index=False)\n\ndf_sub['target'] = avg_pred_02\ndf_sub.to_csv('submission_046.csv', index=False)\n\ndf_sub['target'] = avg_pred_03\ndf_sub.to_csv('submission_047.csv', index=False)\n\ndf_sub['target'] = avg_pred_04\ndf_sub.to_csv('submission_048.csv', index=False)\n\n#df_sub['target'] = avg_pred_05\n#df_sub.to_csv('submission_044.csv', index=False)","5e62c2f3":"#plt.figure(figsize=(8,5))\n#sns.histplot(avg_pred_01, color='slategray');","eb620d9a":"### best params  \n\nlgb_params_kfold = {\n \n    'n_estimators':3000,\n    'learning_rate': 0.004,\n    'min_data_per_group': 5,\n    'boosting_type': 'gbdt',\n    'num_leaves': 256,\n    'max_depth': -1,\n    'lambda_l1': 4.5,\n    'lambda_l2': 1.2,\n    \n    'metric': 'rmse',\n    'cat_smooth': 1.0,\n    'silent': True,\n    'importance_type': 'split',\n    'feature_pre_filter': False,\n    'bagging_fraction': 0.85,\n    'min_data_in_leaf': 100,\n    'min_sum_hessian_in_leaf': 0.001,\n    'bagging_freq': 7,\n    'feature_fraction': 0.5,\n    'min_gain_to_split': 0.0,\n    'min_child_samples': 20,\n    \n    'n_jobs': -1,\n    'random_state': SEED}\n\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':5, \n    'eta':0.006,\n    'gamma':1.2,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.8,\n    'lambda':4.5,\n    'alpha':1.2,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","3502e5f9":"N_FOLDS = 10\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_xgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train_shap)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_xgb += model.predict(test)\/N_FOLDS\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')","6f617b60":"lgb_params_kfold = {\n \n    'n_estimators':6000,\n    'learning_rate': 0.004,\n    'min_child_samples':285,\n    'boosting_type': 'gbdt',\n    'num_leaves': 256,\n    'max_depth': -1,\n    'lambda_l1': 4.5,\n    'lambda_l2': 1.2,\n    'subsample':0.8,\n    #'categorical_feature':CAT_FEAT,\n    \n    'device_type':'gpu',\n    'metric': 'rmse',\n    'cat_smooth': 39,\n    'silent': True,\n    'importance_type': 'split',\n    'feature_pre_filter': False,\n    'bagging_fraction': 0.85,\n    'min_data_in_leaf': 100,\n    #'min_data_per_group': 5,\n    #'min_sum_hessian_in_leaf': 0.01,\n    #'bagging_freq': 5,\n    #'feature_fraction': 0.7,\n    #'min_child_samples': 20,\n    \n    'n_jobs': -1,\n    'random_state': SEED}","b073c3ae":"**02\/02\/21**  *Public 0.84295*  \n\nSimple EDA and preprocessing, tuned LGBR\n\n**04\/02\/21** *Public: 0.84309 \/ 0.84322 \/ 0.84361*  \n\nBasic feature engineering doesn't improve the score  \n\n**05 and 06 02\/21** *Public 0.84279*  \n\nXgb with shap feature impotrance, xgb + lbgm predictions  \npred xgb oof == 0.84194  pub_sub == 0.84279 \n\n**10\/02\/21** *Public 0.84217*  \nMixed lightbgm and two different kfold XGB predictors  \n\n**11\/02\/21** Publick 0.84212*  \nLGBM 10 folds 0.83 public and average XGB preds from three different kernels;   \nTouch of feature engineering, quantile transformation.\n","fb2780c1":"lgb_params_kfold = {\n     'reg_alpha': 6.147694913504962,\n     'reg_lambda': 0.002457826062076097,\n     'colsample_bytree': 0.3,\n     'subsample': 0.8,\n     'learning_rate': 0.001,\n     'max_depth': 20,\n     'num_leaves': 111,\n     'min_child_samples': 285,\n     'random_state':SEED,\n     'verbose':-1,\n     'n_estimators': 30000,\n     'metric': 'rmse',\n     'cat_smooth': 39\n}","58224930":"catboost_params = { \n            'iterations':10000,\n            'learning_rate':0.004,\n            'depth':9,\n            'num_leaves':111,\n            'random_strength':3,\n            'min_data_in_leaf':10,\n            'l2_leaf_reg':5.2,\n            'loss_function':'RMSE',\n            'random_seed':SEED,\n            'eval_metric':'RMSE',\n            'grow_policy':'Depthwise',\n            'max_bin':512,\n            'task_type': 'GPU',\n            'od_type':'Iter',\n            'od_wait':50,\n            'metric_period':50\n            }","f3503eb8":"Seems like the next columns are non-informative: [cat0, cat2, cat4, cat6, cat7]. \nFor the first time we will use all default columns for prediction. Later we try to remove \"unimodal' ones.  \n\n---","b195bf11":"##  Hi again, my beautiful kagglers.\n\nLet's jump into the second playground competition of the year 2021.  \nAs from today, I decided to share my monthly work with you. \nPlease, don't hesitate to comment and upvote.  \n\n---\n\nLONG TIME AGO: basic and advanced feature engineering, catboost and lgbm models, xgboost   \n\nIN THIS VERSION:  xgb and lightgbm predictors, feature transformation. LGBM and catboost fit process are disabled to reduce a computation time. Shap is also disabled.  \n\nUP NEXT: more complex models, shap.\n\n---","c71dde95":"Much better.","aaab40fb":"### Exploring categorical features \n\nBefore we start, let's drop the target columns.","6e27c5a6":"### Feature selection","24766d5f":"### Exploring target distribution","963e4111":"Unlike the first tabular competition, here we have to deal with both categorical and continuous features.  \n\nWe have pretty similar features distribution (by the first look) on both datasets. No missing values and duplicates are spotted.  \n\nA few categorical columns have a huge class disbalance. For example, 'cat0' contains approximately 93% of values 'A'. I suppose such columns are non-informative.\n\n---","1e4b0b1e":"## Data and libs","de8b3030":"N_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds = np.zeros(len(test))\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = cb.CatBoostRegressor(**catboost_params)\n    \n    train_pool = cb.Pool(X_train, y_train)\n    val_pool = cb.Pool(X_val, y_val)\n    \n    model.fit(X_train, y=y_train, \n              eval_set = (X_val, y_val),  \n              early_stopping_rounds=100, \n              verbose_eval=500, \n              use_best_model=True, \n              cat_features=train[CAT].columns.tolist(),\n              plot=False)\n    \n    p = model.predict(X_val)\n    oof[test_ind] = p\n    preds_folds += model.predict(test)\/N_FOLDS\n    \n    #print(np.round(mean_squared_error(y_val, p, squared=False),5))\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')","55d92544":"0.84351\n\ncb best RMSE on oof 0.8427","51da44b8":"best attempt - RMSE on training data: 0.84215","3ef048ad":"#### Manual feature egineering","0d44572f":"## Machine learning\n\nAt this version we will use almost raw datasets with a touch of basic preprocessing.\n\n---\n\n### Categorical features encoding","d5489823":"Let's try to mix catboost and LGBM prediction from a previous version. You may find the code above.","a4a90998":"best rmse:  \n0.84252  \n0.84194","56016974":"### Feature engineering \n\n#### Scaling and encoding","cc906587":"#### FeatureTools","f1006591":"#### Simple one-hot encoding","2c4ab8eb":"## EDA","dde99a43":"N_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_lgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = lgb.LGBMRegressor(**lgb_params_kfold)\n    \n    model.fit(X_train, y_train, eval_set = ((X_val, y_val)), early_stopping_rounds = 200, verbose = 1000)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_lgb += model.predict(test)\/N_FOLDS\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')","6e2c09cc":"#### Label encoding","e23f8b2f":"**LGBm with kfold**","28211d2c":"**xgb**","d95daea9":"### FitPredict  \n\n**Catboost with kFold**","a576ee19":"Just 34 \"outliers\". Most likely we may drop them without a huge impact on a final score.","d91113a2":"Not so bad. There are no columns with a huge amount of outliers. Four features have a highly skewed distribution [cont3, cont5, cont7 and cont1].  \n\nNext step - check the correlations.","7621c7a3":"Using shape to get some info from XGB model and training features.","259c2c25":"LGBM prediction `sub_305` is made by my other [kernel](https:\/\/www.kaggle.com\/kirillklyukvin\/tps-feb-lgbm) based on beautiful [work](https:\/\/www.kaggle.com\/maunish\/lgbm-goes-brrr) by Mister [Maunish](https:\/\/www.kaggle.com\/maunish).  \n\nSome ideas I took from [this notebook](https:\/\/www.kaggle.com\/tunguz\/ensembling-starter-tps-feb-2021) by Mister [Bojan](https:\/\/www.kaggle.com\/tunguz).   \n\nFriends, let's show our appreciation and upvote their notebooks too!\n\n---","8534c8cf":"We have almost the same target bimodal distribution as in January playground competition.  \nAlso let's see how many values bellow mark 4 do we have here.","28ed226f":"### Exploring continuous features ","2a7899bd":"## Scoreboard and notes","189c2832":"## Submission","eaf50fee":"Feature 'cat6' has a label 'G' in training set, which is missed in test set. So after simply dummy transformation we have to drop 'cat6_G' feature from the updated train set.","d5ff47fb":"We have a few multicollinear columns. As I said previously, in this version I try to use these features by default. Next, we will try to use some more complicated algorithms.  \n\n---"}}