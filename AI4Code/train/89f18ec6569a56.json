{"cell_type":{"b21445fc":"code","b1572082":"code","4508efe1":"code","f35115fe":"code","a1c06724":"code","d5d331c5":"code","660dd54e":"code","ff94e459":"code","a939b445":"code","85f3b4e3":"code","8aa5278e":"code","ddc9d241":"code","602c4d23":"code","d31d0b32":"code","a634187c":"code","c7a99a8c":"code","5888a5f3":"code","90188f28":"code","1306173b":"code","83f92ee4":"code","0ead5a77":"code","f54322d4":"code","33858d2f":"code","10b39380":"code","a8652e10":"code","d16cebe9":"code","9da32dbc":"code","e25f9fac":"code","82d6f5c1":"code","01e3fa08":"code","94144acc":"code","4d75733a":"code","9905914c":"code","bbd71d1f":"markdown","37ad7683":"markdown","bd4a3def":"markdown","62c13e5a":"markdown","b32abac9":"markdown","a2317a09":"markdown","269cc834":"markdown","d28126a2":"markdown","738e8cac":"markdown"},"source":{"b21445fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nimport time\nimport re\n\n","b1572082":"## Load data\ntrain = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","4508efe1":"## Building vocubulary from our Quest Data\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n##Apply the vocab function to get the words and the corresponding counts\nsentences = train[\"question_body\"].progress_apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:20]})","f35115fe":"%%time \nfrom gensim.models import KeyedVectors\n\nnews_path = '\/kaggle\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=False)","a1c06724":"%%time\nimport operator \n## This is a common function to check coverage between our quest data and the word embedding\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","d5d331c5":"%%time\noov = check_coverage(vocab,embeddings_index)","660dd54e":"## List 10 out of vocabulary word\noov[:10]","ff94e459":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n    return text\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: decontract(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","a939b445":"def clean_apostrophes(x):\n    apostrophes = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in apostrophes:\n        x = re.sub(s, \"'\", x)\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_apostrophes(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","85f3b4e3":"# clean weird \/ special characters\n\nletter_mapping = {'\\u200b':' ', '\u0169': \"u\", '\u1ebd': 'e', '\u00e9': \"e\", '\u00e1': \"a\", '\u0137': 'k', \n                  '\u00ef': 'i', '\u0179': 'Z', '\u017b': 'Z', '\u0160': 'S', '\u03a0': ' pi ', '\u00d6': 'O', \n                  '\u00c9': 'E', '\u00d1': 'N', '\u017d': 'Z', '\u1ec7': 'e', '\u00b2': '2', '\u00c5': 'A', '\u0100': 'A',\n                  '\u1ebf': 'e', '\u1ec5': 'e', '\u1ed9': 'o', '\u29fc': '<', '\u29fd': '>', '\u00dc': 'U', '\u0394': 'delta',\n                  '\u1ee3': 'o', '\u0130': 'I', '\u042f': 'R', '\u041e': 'O', '\u010c': 'C', '\u041f': 'pi', '\u0412': 'B', '\u03a6': \n                  'phi', '\u1ef5': 'y', '\u0585': 'o', '\u013d': 'L', '\u1ea3': 'a', '\u0393': 'theta', '\u00d3': 'O', '\u00cd': 'I',\n                  '\u1ea5': 'a', '\u1ee5': 'u', '\u014c': 'O', '\u039f': 'O', '\u03a3': 'sigma', '\u00c2': 'A', '\u00c3': 'A', '\u15ef': 'w', \n                  '\u157c': \"h\", \"\u15e9\": \"a\", \"\u1587\": \"r\", \"\u15ef\": \"w\", \"O\": \"o\", \"\u15f0\": \"m\", \"\u144e\": \"n\", \"\u142f\": \"v\", \"\u043d\": \n                  \"h\", \"\u043c\": \"m\", \"o\": \"o\", \"\u0442\": \"t\", \"\u0432\": \"b\", \"\u03c5\": \"u\",  \"\u03b9\": \"i\",\"\u043d\": \"h\", \"\u010d\": \"c\", \"\u0161\":\n                  \"s\", \"\u1e25\": \"h\", \"\u0101\": \"a\", \"\u012b\": \"i\", \"\u00e0\": \"a\", \"\u00fd\": \"y\", \"\u00f2\": \"o\", \"\u00e8\": \"e\", \"\u00f9\": \"u\", \"\u00e2\": \n                  \"a\", \"\u011f\": \"g\", \"\u00f3\": \"o\", \"\u00ea\": \"e\", \"\u1ea1\": \"a\", \"\u00fc\": \"u\", \"\u00e4\": \"a\", \"\u00ed\": \"i\", \"\u014d\": \"o\", \"\u00f1\": \"n\",\n                  \"\u00e7\": \"c\", \"\u00e3\": \"a\", \"\u0107\": \"c\", \"\u00f4\": \"o\", \"\u0441\": \"c\", \"\u011b\": \"e\", \"\u00e6\": \"ae\", \"\u00ee\": \"i\", \"\u0151\": \"o\", \"\u00e5\": \n                  \"a\", \"\u00c4\": \"A\",\"&gt\":\" greater than\",\"&lt\" :\"lesser than\", \"(not\" : \"not\" , \"});\":\"\",\">\" :\"greater\",\"<\":\"lesser\" ,\"$\":\"dollar\",\"\\\\\\\\\":\" \",\"\\\\\": \" \"} \n\ndef clean_special_chars(text):\n    new_text = ''\n    for i in range(len(text)):\n        if i in letter_mapping:\n            c = letter_mapping[i]\n        else:\n            c = text[i]\n        new_text += c\n    return new_text\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_special_chars(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)\n","8aa5278e":"# remove useless punctuations\n\nuseless_punct = ['\u091a', '\u4e0d', '\u09dd', '\u5e73', '\u1820', '\u932f', '\u5224', '\u2219',\n                 '\u8a00', '\u03c2', '\u0644', '\u17d2', '\u30b8', '\u3042', '\u5f97', '\u6c34', '\u044c', '\u25e6', '\u521b', \n                 '\u5eb7', '\u83ef', '\u1e35', '\u263a', '\u652f', '\u5c31', '\u201e', '\u300d', '\uc5b4', '\u8c08', '\u9648', '\u56e2', '\u817b', '\u6743', \n                 '\u5e74', '\u4e1a', '\u30de', '\u092f', '\u0627', '\u58f2', '\u7532', '\u62fc', '\u02c2', '\u1f64', '\u8d2f', '\u4e9a', '\u093f', '\u653e', '\u02bb', '\u1791', '\u0296', \n                 '\u9ede', '\u0acd', '\u767a', '\u9752', '\u80fd', '\u6728', '\u0434', '\u5fae', '\u85e4', '\u0303', '\u50d5', '\u5992', '\u035c', '\u1793', '\u0927', '\uc774', '\u5e0c', '\u7279',\n                 '\u0921', '\u00a2', '\u6ee2', '\u0e2a', '\ub098', '\u5973', '\u0c15', '\u6ca1', '\u4ec0', '\u0437', '\u5929', '\u5357', '\u02bf', '\u0e04', '\u3082', '\u51f0', '\u6b65', '\u7c4d', '\u897f',\n                 '\u0e33', '\u2212', '\u043b', '\u06a4', '\u17c3', '\u865f', '\u0635', '\u0938', '\u00ae', '\u028b', '\u6279', '\u179a', '\uce58', '\u8c22', '\u751f', '\u9053', '\u2550', '\u4e0b', '\u4fc4', '\u0256',\n                 '\u89c0', '\u0bb5', '\u2014', '\u06cc', '\u60a8', '\u2665', '\u4e00', '\u3084', '\u2286', '\u028c', '\u8a9e', '\u0e35', '\u5174', '\u60f6', '\u701b', '\u72d0', '\u2074', '\u092a', '\u81e3', '\u0c26',\n                 '\u2015', '\u00ec', '\u090c', '\u0c40', '\u81ea', '\u4fe1', '\u5065', '\u53d7', '\u0268', '\uc2dc', '\u05d9', '\u099b', '\u5b1b', '\u6e7e', '\u5403', '\u3061', '\u095c', '\u53cd', '\u7ea2', '\u6709',\n                 '\u914d', '\u09c7', '\u17af', '\u5bae', '\u3064', '\u03bc', '\u8a18', '\u53e3', '\u2105\u03b9', '\u094b', '\u72f8', '\u5947', '\u043e', '\u091f', '\u8056', '\u862d', '\u8aad', '\u016b', '\u6a19', '\u8981', \n                 '\u178f', '\u8bc6', '\u3067', '\u6c64', '\u307e', '\u0280', '\u5c40', '\u30ea', '\u094d', '\u0e44', '\u5462', '\u5de5', '\u0932', '\u6c92', '\u03c4', '\u17b7', '\u00f6', '\u305b', '\u4f60', '\u3093', '\u30e5', \n                 '\u679a', '\u90e8', '\u5927', '\u7f57', '\u09b9', '\u3066', '\u8868', '\u62a5', '\u653b', '\u013a', '\u0e09', '\u2229', '\u5b9d', '\u5bf9', '\u5b57', '\u6587', '\u8fd9', '\u2211', '\u9aea', '\u308a', '\u0e48', '\ub2a5',\n                 '\u7f62', '\ub0b4', '\u963b', '\u4e3a', '\u83f2', '\u064a', '\u0928', '\u03af', '\u0266', '\u958b', '\u2020', '\u8339', '\u505a', '\u6771', '\u09a4', '\u306b', '\u062a', '\u6653', '\ud0a4', '\u60b2', '\u0ab8', \n                 '\u597d', '\u203a', '\u4e0a', '\u5b58', '\uc5c6', '\ud558', '\u77e5', '\u1792', '\u65af', ' ', '\u6388', '\u0142', '\u50b3', '\u5170', '\u5c01', '\u0bcb', '\u0648', '\u0445', '\u3060', '\u4eba', '\u592a', \n                 '\u54c1', '\u6bd2', '\u1873', '\u8840', '\u5e2d', '\u5254', '\u043f', '\u86cb', '\u738b', '\u90a3', '\u68a6', '\u17b8', '\u5f69', '\u7504', '\u0438', '\u67cf', '\u0a28', '\u548c', '\u574a', '\u231a', '\u5e7f', \n                 '\u4f9d', '\u222b', '\u012f', '\u6545', '\u015b', '\u090a', '\u51e0', '\u65e5', '\u06a9', '\u97f3', '\u00d7', '\u201d', '\u25be', '\u028a', '\u091c', '\u0e14', '\u0920', '\u0909', '\u308b', '\u6e05', '\u0917', '\u0637',\n                 '\u03b4', '\u028f', '\u5b98', '\u221b', '\u09bc', '\u0e49', '\u7537', '\u9a82', '\u590d', '\u2202', '\u30fc', '\u8fc7', '\u09af', '\u4ee5', '\u77ed', '\u7ffb', '\u09b0', '\u6559', '\u5100', '\u025b', '\u2039', '\u3078', \n                 '\u00be', '\u5408', '\u5b66', '\u064c', '\ud559', '\u6311', '\u0937', '\u6bd4', '\u4f53', '\u0645', '\u0633', '\u17a2', '\u05ea', '\u8a13', '\u2200', '\u8fce', '\u179c', '\u0254', '\u0668', '\u2592', '\u5316', '\u0c1a', '\u201b', \n                 '\u09aa', '\u00ba', '\u0e19', '\uc5c5', '\u8bf4', '\u3054', '\u00b8', '\u20b9', '\u513f', '\ufe20', '\uac8c', '\u9aa8', '\u0e17', '\u090b', '\u30db', '\u8336', '\ub294', '\u0a9c', '\u0e38', '\u7fa1', '\u7bc0', '\u0a2e', \n                 '\u0989', '\u756a', '\u09dc', '\u8bb2', '\u315c', '\ub4f1', '\u4f1f', '\u0e08', '\u6211', '\u0e25', '\u3059', '\u3044', '\u1789', '\u770b', '\u010b', '\u2227', '\u092d', '\u0a98', '\u0e31', '\u1798', '\u8857', '\u0aaf', \n                 '\u8fd8', '\u9c39', '\u1781', '\u0c41', '\u8a0a', '\u092e', '\u044e', '\u5fa9', '\u6768', '\u0642', '\u0924', '\u91d1', '\u5473', '\u09ac', '\u98ce', '\u610f', '\uba87', '\u4f6c', '\u723e', '\u7cbe', '\u00b6', \n                 '\u0c02', '\u4e71', '\u03c7', '\uad50', '\u05d4', '\u59cb', '\u1830', '\u4e86', '\u4e2a', '\u514b', '\u09cd', '\u0e2b', '\u5df2', '\u0283', '\u308f', '\u65b0', '\u8bd1', '\ufe21', '\u672c', '\u0e07', '\u0431', '\u3051', \n                 '\u0c3f', '\u660e', '\u00af', '\u904e', '\u0643', '\u1fe5', '\u0641', '\u00df', '\uc11c', '\u8fdb', '\u178a', '\u6837', '\u4e50', '\u5be7', '\u20ac', '\u0e13', '\u30eb', '\u4e61', '\u5b50', '\ufb01', '\u062c', '\u6155',\n                 '\u2013', '\u1875', '\u00d8', '\u0361', '\uc81c', '\u03a9', '\u1794', '\u7d55', '\ub208', '\u092b', '\u09ae', '\u0c17', '\u4ed6', '\u03b1', '\u03be', '\u00a7', '\u0b9c', '\u9ece', '\u306d', '\ubcf5', '\u03c0', '\u00fa', '\u9e21',\n                 '\u8bdd', '\u4f1a', '\u0995', '\u516b', '\u4e4b', '\ubd81', '\u0646', '\u00a6', '\uac00', '\u05d5', '\u604b', '\u5730', '\u1fc6', '\u8a31', '\u4ea7', '\u0961', '\u0634', '\u093c', '\u91ce', '\u1f75', '\u0252', '\u5567',\n                 '\u1799', '\u180c', '\u1828', '\u0628', '\u768e', '\u8001', '\u516c', '\u2606', '\u0935', '\u09bf', '\u179b', '\u0631', '\u1782', '\ud589', '\u1784', '\u03bf', '\u8ba9', '\u17c6', '\u03bb', '\u062e', '\u1f30', '\u5bb6',\n                 '\u099f', '\u092c', '\u7406', '\u662f', '\u3081', '\u0930', '\u221a', '\uae30', '\u03bd', '\u7389', '\ud55c', '\u5165', '\u05d3', '\u522b', '\u062f', '\u0e30', '\u7535', '\u0abe', '\u266b', '\u0639', '\u0a82', '\u5835',\n                 '\u5ac9', '\u4f0a', '\u3046', '\u5343', '\uad00', '\u7bc7', '\u0915', '\u975e', '\u8363', '\u7cb5', '\u745c', '\u82f1', '\ub97c', '\u7f8e', '\u6761', '`', '\u5b8b', '\u2190', '\uc218', '\u5f8c', '\u2022',\n                 '\u00b3', '\u0940', '\uace0', '\u8089', '\u2103', '\u3057', '\u6f22', '\uc2f1', '\u03f5', '\u9001', '\u0647', '\u843d', '\u0c28', '\u1780', '\u0b95', '\u2107', '\u305f', '\u17c7', '\u4e2d', '\u5c04', '\u266a', '\u7b26',\n                 '\u1783', '\u8c37', '\u5206', '\u9171', '\u3073', '\u09a5', '\u0629', '\u0433', '\u03c3', '\u3068', '\u695a', '\u80e1', '\u996d', '\u307f', '\u79ae', '\u4e3b', '\u76f4', '\u00f7', '\u5922', '\u027e', '\u099a', '\u20d7',\n                 '\u7d71', '\u9ad8', '\u987a', '\u636e', '\u3089', '\u982d', '\u3088', '\u6700', '\u0c3e', '\u0a41', '\u4eb2', '\u179f', '\u82b1', '\u2261', '\u773c', '\u75c5', '\u2026', '\u306e', '\u767c', '\u0bbe', '\u6c5d',\n                 '\u2605', '\u6c0f', '\u0e23', '\u666f', '\u1860', '\u8bfb', '\u4ef6', '\u4ef2', '\u09b6', '\u304a', '\u3063', '\u067e', '\u1864', '\u0447', '\u266d', '\u60a0', '\u0902', '\u516d', '\u4e5f', '\u057c', '\u09df', '\u6050', \n                 '\u0939', '\u53ef', '\u554a', '\u83ab', '\u4e66', '\u603b', '\u09b7', '\u0584', '\u0302', '\uac04', '\u306a', '\u6b64', '\u611b', '\u0c30', '\u0e43', '\u9673', '\u1f08', '\u0923', '\u671b', '\u0926', '\u8bf7', '\u6cb9',\n                 '\u9732', '\ub2c8', '\u015f', '\u5b97', '\u028d', '\u9cf3', '\u0905', '\u908b', '\u7684', '\u1796', '\u706b', '\u093e', '\u0e01', '\u7d04', '\u0b9f', '\u7ae0', '\u9577', '\u5546', '\u53f0', '\u52e2', '\u3055',\n                 '\uad6d', '\u00ce', '\u7c21', '\u0908', '\u2208', '\u1e6d', '\u7d93', '\u65cf', '\u0941', '\u5b6b', '\u8eab', '\u5751', '\u09b8', '\u4e48', '\u03b5', '\u5931', '\u6bba', '\u017e', '\u0ab0', '\u304c', '\u624b',\n                 '\u17b6', '\u5fc3', '\u0a3e', '\ub85c', '\u671d', '\u4eec', '\u9ed2', '\u6b22', '\u65e9', '\ufe0f', '\u09be', '\u0906', '\u0278', '\u5e38', '\u5feb', '\u6c11', '\ufdfa', '\u17bc', '\u9062', '\u03b7', '\u56fd', \n                 '\u65e0', '\u6c5f', '\u0960', '\u300c', '\u09a8', '\u2122', '\u17be', '\u03b6', '\u7d2b', '\u0c46', '\u044f', '\u201c', '\u2668', '\u570b', '\u0947', '\u0e2d', '\u221e', \n                  '\\n', \"{\\n', '}\\n\", \"=&gt;\", '}\\n\\n', '-&gt;', '\\n\\ni', '&lt;','\/&gt;\\n','{\\n\\n','\\\\','|','&','\\\\n\\\\n',\"\\\\appendix\"]\nuseless_punct.remove(' ')\n\ndef remove_useless_punct(text):\n    return re.sub(f'{\"|\".join(useless_punct)}', '', text)\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: remove_useless_punct(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","ddc9d241":"## ReLoad data\ntrain = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","602c4d23":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\n\noov = check_coverage(vocab,embeddings_index)","d31d0b32":"oov[:500]","a634187c":"## Checking how the numbers are present in the crawl embedding .\n'1234567'in embeddings_index","c7a99a8c":"'14528' in embeddings_index","5888a5f3":"import re\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_numbers(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","90188f28":"oov[:20]","1306173b":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: replace_typical_misspell(x))\nsentences = train[\"question_body\"].progress_apply(lambda x: x.split())\nto_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","83f92ee4":"import pickle","0ead5a77":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","f54322d4":"## Reload the data \ntrain = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","33858d2f":"GLOVE_EMBEDDING_PATH = '\/kaggle\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl' ","10b39380":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","a8652e10":"tic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","d16cebe9":"vocab = build_vocab(list(train['question_body'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","9da32dbc":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n    return text\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\ndef preprocess(x):\n    x= decontract(x)\n    x=clean_text(x)\n    x=clean_number(x)\n    return x","e25f9fac":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: decontract(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","82d6f5c1":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","01e3fa08":"oov[:500]","94144acc":"'1234567' in glove_embeddings","4d75733a":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_numbers(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","9905914c":"oov[:20]","bbd71d1f":"## We can clearly see that the correct preprocessing has increased the coverage","37ad7683":"### Let us try GloVe preprocessing from pickle","bd4a3def":"#### Here we are checking base out of vocabulary word %","62c13e5a":"### Now apply the Public Kernel preprocessing functions one by one and try to see the impact of OOV %","b32abac9":"### We are going to replace the public kernel preprocessing with this new preprocessing methods. Remember the order is important . If we remove the special characters and then try to do decontraction , you will not find a word to decontract, because I'll has already has become very very Ill.","a2317a09":"### Let us now track the changes properly and see if we can get better coverage by systematic preprocessing","269cc834":"This is just for my reference and trial for this competition . I have learnt this from sensei @christofhenkel \n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings","d28126a2":"### Here I am Loading the fasttext crawl embedding using .vec file and gensim library . It takes around 10 minutes +","738e8cac":"## Let us first see what I have done in my public Kernel .\n#### We will firt load our data and fasttext crawl embedding and apply the same preprocessing as the public kernel ."}}