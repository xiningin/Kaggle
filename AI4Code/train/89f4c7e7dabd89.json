{"cell_type":{"5693561f":"code","d0232c64":"code","a350b1c8":"code","30476ee1":"code","155b953c":"code","33cc995c":"code","e35572bf":"code","76bec5b1":"code","b29a75a6":"code","08beebaf":"code","8c07f0a5":"code","3de201a8":"code","f122b37d":"code","2ab08cd2":"code","8f387f98":"code","985ba630":"code","348c489e":"code","355cc7fe":"code","fb43afed":"code","b74d6f2d":"code","6952eac2":"code","805ac336":"code","5689cdb8":"code","4e32cfde":"code","50a3daa7":"code","26730d7c":"code","b8255cc3":"code","23cfb5a4":"code","12c62dad":"code","ef42c04d":"code","54391185":"code","1beee31d":"code","a928df69":"code","f70effd2":"code","be2a434b":"code","25b98de5":"code","42b2b680":"code","c643e667":"code","5d02642d":"code","c0ff8c1a":"code","b6adc61d":"code","f22592bf":"code","465c0cac":"code","b4be2ab4":"code","5dd8cf97":"code","e9f70508":"code","b7cb0d22":"code","6f0615d2":"code","ced2b310":"code","42be3b6c":"code","7e1605d5":"code","05cb8d03":"code","2c08628f":"code","aeb9666a":"code","cfc11232":"code","e1722e49":"code","a4b2c6aa":"code","3a934ce9":"code","0325e5fb":"code","2165c520":"code","9855ca7b":"code","188c0910":"code","61b3e62c":"code","6fa79a1b":"code","343054b0":"code","8e47acb0":"code","d200b744":"code","e1f0fa70":"code","d8a0e96d":"code","7f6abef0":"code","7dc98117":"code","b4cd0d5a":"code","98da00ac":"code","7d4e8677":"code","caf3b955":"code","8018d663":"code","fa7ce745":"code","168fc22d":"code","d935befd":"code","3df3195a":"code","e024e003":"code","f56d086b":"code","a7c81db5":"code","60e1a1cf":"code","a93998bf":"code","e08a3d86":"markdown","653b6717":"markdown","d544ce17":"markdown","18a00fc9":"markdown","60f185fb":"markdown","837c3b10":"markdown","93137858":"markdown","a2fd41da":"markdown","b34fdbe0":"markdown","e2707eb2":"markdown","713053b0":"markdown","2a779781":"markdown","65a0eb91":"markdown","6ec5d3a4":"markdown","09e024e1":"markdown","70f5453f":"markdown","5720c0f3":"markdown","5ea7cf85":"markdown","c8078ba8":"markdown","856309d5":"markdown","7d4a8540":"markdown","46ad92d7":"markdown","28b6fd6f":"markdown","86a346b2":"markdown","313b3cc4":"markdown","cf9a3a5c":"markdown","04d287a2":"markdown","d8baaf9a":"markdown","1d0adece":"markdown","3ae28863":"markdown","e5fc21bf":"markdown","e2ef1c0f":"markdown","31887d0b":"markdown","e2572120":"markdown","9cf24f17":"markdown","ef446b5b":"markdown","b4a5631c":"markdown","6f22f3da":"markdown","a1c093f0":"markdown","8fd6c2d9":"markdown","12841268":"markdown","1cabc1a7":"markdown","b62c66fc":"markdown","c0e6021d":"markdown","82893954":"markdown","9d3f05b0":"markdown","ec5ac322":"markdown","b381781e":"markdown","03c8ff26":"markdown","d5b19d4a":"markdown","6b6f92b1":"markdown","7fa9d1cf":"markdown","4858ccba":"markdown","9f9ab1ac":"markdown","63a01ed7":"markdown","4c91642d":"markdown","d4249835":"markdown","96d1e91b":"markdown","d96549a0":"markdown","6191582a":"markdown","1e414076":"markdown","e5ba9971":"markdown","cf4a2508":"markdown","3cf934e7":"markdown","4f38de3f":"markdown","a789caa5":"markdown","0a43ffff":"markdown","b0e93173":"markdown","170dc9e5":"markdown"},"source":{"5693561f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nimport xgboost as xgb\nimport warnings\nfrom xgboost import plot_tree\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, KFold, cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom IPython.display import display, HTML\nimport tensorflow as tf \nimport math\nwarnings.filterwarnings(action='ignore', category=UserWarning) ","d0232c64":"# loading data \ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv');\ncombined_df= pd.concat([train_df, test_df]);","a350b1c8":"column_informations = {}\nnum_values = len(train_df)\nfor col in train_df.columns:\n    num_unique = train_df[col].nunique()\n    num_nulls = round(train_df[col].isna().sum()\/num_values,2)\n    d_type = train_df.dtypes[col]\n    \n    if (num_unique < 30):\n        # discrete column\n        info_str = \"[\"\n        value_counts = train_df[col].value_counts()\n        single_value_weight = round(value_counts.iloc[0] \/ num_values, 2)\n        for index, value in value_counts.items():\n            info_str += f\"{value} X {index}, \"\n        column_informations[col] = {\"d_type\":d_type, \"discret\": True, \"percentage_of_missing_values\": num_nulls, \"single_value_weight\": single_value_weight,\n                                    \"min\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"info_str\": info_str[:-2] + \"]\"} \n    else:\n        # continuous column\n        if d_type == \"int64\" or d_type == \"float64\":\n            column_informations[col] = {\"d_type\":d_type, \"discret\": False, \"percentage_of_missing_values\": num_nulls, \"single_value_weight\": 0.0,\n                                        \"min\": train_df[col].min(), \"max\": train_df[col].max(), \"mean\": round(train_df[col].mean(), 2),\n                                        \"median\": round(train_df[col].median(), 2), \"info_str\": \"\"}\n        else:\n            column_informations[col] = {\"d_type\":d_type, \"discret\": False, \"percentage_of_missing_values\": num_nulls, \"min\": \"-\", \"max\": \"-\",\n                                        \"mean\": \"-\", \"median\": \"-\", \"info_str\": \"\"}\n\n# build DataFrame from dictionary\ninfo_df = pd.DataFrame.from_dict(column_informations, orient='index')","30476ee1":"display(HTML(info_df[info_df[\"discret\"]==True][[\"d_type\", \"percentage_of_missing_values\", \"single_value_weight\", \"info_str\"]].to_html()))\nprint(len(info_df[info_df[\"discret\"]==True]))","155b953c":"display(HTML(info_df[info_df[\"discret\"]==False][[\"d_type\", \"percentage_of_missing_values\", \"min\", \"max\", \"mean\", \"median\"]].to_html()))\nprint(len(info_df[info_df[\"discret\"]==False]))","33cc995c":"\"\"\"\nLet's start with a general colour setting for seaborn. (We only have to do this once and all the plots in this notebook will have the same look!) \n\"\"\"\nsns.set(rc={'axes.facecolor':\"#F1FFC9\",\n            \"figure.facecolor\":\"#B2C679\",\n            \"grid.color\":\"#E0D3AF\",\n            \"axes.edgecolor\":\"#505541\",\n            \"axes.labelcolor\":\"#505541\",\n            \"text.color\":\"#505541\"\n           }) \n# change axes lebelsize. (We change the tick size for each plot separately depending on its size.)\nsns.set_context(rc={\"axes.labelsize\" : 20})","e35572bf":"f, ax = plt.subplots(figsize=(25, 7))\nax.tick_params(labelsize=15)\nsns.histplot(data = train_df,x=\"SalePrice\", kde=True, bins=200).set_title('SalePrice Distribution', fontdict= {'fontsize': 24});","76bec5b1":"\"\"\"\nUsing quantile tranformer for transforming \n\"\"\"\nqt = QuantileTransformer(random_state=0, output_distribution=\"normal\")\ntarget = train_df[\"SalePrice\"].copy()\ntarget = target.values.reshape(-1, 1)\ntarget = qt.fit_transform(target)\ntarget_df=pd.DataFrame(target).rename(columns={0: \"Transformed_SalePrice\"})","b29a75a6":"\"\"\"\nThat was relatively straightforward. Let's take a look at our transformed target.\n\"\"\"\nf, ax = plt.subplots(figsize=(25, 7))\nax.tick_params(labelsize=15)\nsns.histplot(data = target_df,x=\"Transformed_SalePrice\", kde=True, bins=200).set_title('Transformed SalePrice Distribution', fontdict= {'fontsize': 24});","08beebaf":"number_of_price_groups = 5\nnumber_of_values_per_group = len(train_df)\/number_of_price_groups\n\nvisual_df = train_df.copy()\nvisual_df.sort_values(by=['SalePrice'], inplace=True, ignore_index=True)\n\n\"\"\"\nGet group ranges (we want the same amount of members in each group)\n\"\"\"\nlast_boundary = 0\nbounder_dict={}\nfor i in range(number_of_price_groups):\n    boundary = visual_df.iloc[int((i+1)*number_of_values_per_group) - 1][\"SalePrice\"]\n    bounder_dict[f\"{last_boundary}-{boundary}\"] = [last_boundary, boundary]\n    last_boundary = boundary\n\ndef get_price_group(price, bounder_dict):\n    group_lable = \"-\"\n    for group in bounder_dict.keys():\n        if bounder_dict[group][0] < price <= bounder_dict[group][1]:\n            group_lable=group\n    return group_lable\n    \nvisual_df[\"PriceGroup\"]=visual_df[\"SalePrice\"].map(lambda x: get_price_group(x, bounder_dict));","8c07f0a5":"\"\"\"\nLet's check:\n\"\"\"\nindexs =[]\nfor group in visual_df[\"PriceGroup\"].unique():\n    indexs.append(visual_df[[\"SalePrice\", \"PriceGroup\"]][visual_df[\"PriceGroup\"]==group].head(1).index[0])\nvisual_df.loc[indexs][[\"SalePrice\", \"PriceGroup\"]]","3de201a8":"data_features = [\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"MoSold\", \"YrSold\"]\ninfo_df.loc[data_features]","f122b37d":"\"\"\"\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nNote: The code for these subplots will be fairly repetitive, so I will exclude them in most cases. \nIf you have any questions about the plots, you can feel free to ask me in the comments section.\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\"\"\"\n\n# build figure\nfig = plt.figure(figsize=(25,7))\n\n# add grid to figure \ngs = fig.add_gridspec(1,5)\n\n# fill grid with subplots\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\nax03 = fig.add_subplot(gs[0,3])\nax04 = fig.add_subplot(gs[0,4])\n\n# adjust subheadline fontsize\nax00.set_title('Year Built', fontsize=20)\nax01.set_title('Year Remod Added', fontsize=20)\nax02.set_title('Garage Year Built', fontsize=20)\nax03.set_title('Month Sold', fontsize=20)\nax04.set_title('Year Sold', fontsize=20)\n\n# adjust lable fontsize\nax00.tick_params(labelsize=12)\nax01.tick_params(labelsize=12)\nax02.tick_params(labelsize=12)\nax03.tick_params(labelsize=12)\nax04.tick_params(labelsize=12)\n\n# plot (ax=axxx is important)\nsns.histplot(data = visual_df,x=\"YearBuilt\", kde=False, ax=ax00, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"YearRemodAdd\", kde=False, ax=ax01, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"GarageYrBlt\", kde=False, ax=ax02, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"MoSold\", kde=False, ax=ax03, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"YrSold\", kde=True, ax=ax04, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\");\n\n# add headline: (subplots_adjust must be adjusted if you change the gridspec height. So if you have more than one row of plots).\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Date Features vs PriceGroup', fontsize=\"28\");","2ab08cd2":"fig = plt.figure(figsize=(25,5))\ngs = fig.add_gridspec(1,2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(axis='both', which='major', labelsize=15)\nax01.tick_params(axis='both', which='major', labelsize=15)\nax00.set_title('Buildings with RemodAdd in 1950', fontsize=20)\nax01.set_title('Buildings build bevore 1950 and RemoveAdd not in 1950', fontsize=20)\nsns.histplot(data=visual_df[visual_df[\"YearRemodAdd\"]==1950.0], x=\"YearBuilt\", ax=ax00, bins=55, color=\"g\")\nsns.histplot(data=visual_df[(visual_df[\"YearBuilt\"]<=1950.0) & (visual_df[\"YearRemodAdd\"]!=1950.0)], x=\"YearBuilt\", ax=ax01, bins=55);","8f387f98":"visual_df[\"YearsSinceRemode\"] = visual_df[\"YrSold\"] - visual_df[\"YearRemodAdd\"]\nvisual_df[\"YearsUntilRemode\"] = visual_df[\"YearRemodAdd\"] - visual_df[\"YearBuilt\"]","985ba630":"fig = plt.figure(figsize=(25,5))\ngs = fig.add_gridspec(1,2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(axis='both', which='major', labelsize=15)\nax01.tick_params(axis='both', which='major', labelsize=15)\nax00.set_title('YearsSinceRemode', fontsize=20)\nax01.set_title('YearsUntilRemode', fontsize=20)\nsns.histplot(data=visual_df, x=\"YearsSinceRemode\", kde=False, ax=ax00, bins=55, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data=visual_df, x=\"YearsUntilRemode\", kde=False, ax=ax01, bins=55, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\");","348c489e":"corr = visual_df[data_features + [\"SalePrice\", \"YearsSinceRemode\", \"YearsUntilRemode\"]].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(15, 15))\nax.tick_params(axis='both', which='major', labelsize=15)\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"black\", annot=True, \n                cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlations of continuous columns', fontsize=\"28\");","355cc7fe":"size_features = [\"LotArea\",\"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\", \"GarageArea\", \"MasVnrArea\", \"OpenPorchSF\"]\ninfo_df.loc[size_features]","fb43afed":"\"\"\"\nExclude outliers (top and bottom 10%)\n\"\"\"\nfor feature in size_features:\n    upper_quantile = visual_df[feature].quantile(0.9)\n    lower_quantile = visual_df[feature].quantile(0.1)\n    visual_df[feature] = np.where(visual_df[feature]>upper_quantile, upper_quantile,visual_df[feature])\n    visual_df[feature] = np.where(visual_df[feature]<lower_quantile, lower_quantile,visual_df[feature])","b74d6f2d":"fig = plt.figure(figsize=(25,15))\ngs = fig.add_gridspec(2,2)\nax0 = fig.add_subplot(gs[0,:])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax0.set_title('Area Features ex LotArea', fontsize=20)\nax10.set_title('LotArea vs PriceGroup', fontsize=20)\nax11.set_title('GarageCars vs PriceGroup', fontsize=20)\nax0.tick_params(labelsize=15)\nax10.tick_params(labelsize=12)\nax11.tick_params(labelsize=15)\nsns.violinplot(data=visual_df[size_features[1:]], inner=\"quart\", linewidth=1, ax=ax0, scale=\"width\")\nsns.violinplot(data=visual_df, y=size_features[0], x=\"PriceGroup\", inner=\"quart\", linewidth=1, ax=ax10, scale=\"width\", palette=\"pastel\");\nsns.histplot(visual_df, x=\"GarageCars\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax11, palette=\"pastel\", kde=False, linewidth=2);","6952eac2":"f, ax = plt.subplots(figsize=(25, 5))\nax.tick_params(labelsize=15)\nsns.scatterplot(data=visual_df, x=\"GarageArea\", y=\"GarageCars\", sizes=(50, 500), palette=\"Paired\")\nf.subplots_adjust(top=0.9)\nf.suptitle('GarageCars vs GarageArea', fontsize=\"28\");","805ac336":"room_features= [\"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\"]\ninfo_df.loc[room_features]","5689cdb8":"\"\"\"\nBuild total number of rooms feature:\n\"\"\"\nvisual_df[\"TotRms\"] = visual_df[\"TotRmsAbvGrd\"] + visual_df[\"FullBath\"] + visual_df[\"HalfBath\"] + visual_df[\"BsmtFullBath\"] + visual_df[\"BsmtHalfBath\"] + visual_df[\"BedroomAbvGr\"] + visual_df[\"KitchenAbvGr\"]\n\nfig = plt.figure(figsize=(25,5))\ngs = fig.add_gridspec(1,2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(labelsize=15)\nax01.tick_params(labelsize=15)\nsns.histplot(visual_df, x=\"TotRms\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax00, palette=\"pastel\", linewidth=2).set_title('Total number of Rooms vs PriceGroup', fontdict= {'fontsize': 24})\nsns.histplot(visual_df, x=\"TotRmsAbvGrd\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax01, palette=\"pastel\", linewidth=2).set_title('TotRmsAbvGrd vs PriceGroup', fontdict= {'fontsize': 24});","4e32cfde":"room_features= [\"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\"]\nfig = plt.figure(figsize=(25,12))\ngs = fig.add_gridspec(2,3)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax12 = fig.add_subplot(gs[1,2])\nax00.set_title('BsmtFullBath', fontsize=20)\nax01.set_title('BsmtHalfBath', fontsize=20)\nax02.set_title('FullBath', fontsize=20)\nax10.set_title('HalfBath', fontsize=20)\nax11.set_title('BedroomAbvGr', fontsize=20)\nax12.set_title('KitchenAbvGr', fontsize=20)\nax00.tick_params(labelsize=15)\nax01.tick_params(labelsize=15)\nax02.tick_params(labelsize=15)\nax10.tick_params(labelsize=15)\nax11.tick_params(labelsize=15)\nax12.tick_params(labelsize=15)\n\"\"\"\nWe remove the xlabel sinze they are clear from the subplot headline.\n\"\"\"\nsns.histplot(visual_df, x=\"BsmtFullBath\", hue=\"PriceGroup\", bins=15, multiple=\"stack\", ax=ax00, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)\nsns.histplot(visual_df, x=\"BsmtHalfBath\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax01, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)\nsns.histplot(visual_df, x=\"FullBath\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax02, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)\nsns.histplot(visual_df, x=\"HalfBath\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax10, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)\nsns.histplot(visual_df, x=\"BedroomAbvGr\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax11, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)\nsns.histplot(visual_df, x=\"KitchenAbvGr\", hue=\"PriceGroup\", bins=15, multiple=\"stack\",ax=ax12, palette=\"pastel\", kde=False, linewidth=2).set(xlabel=None)     \nfig.subplots_adjust(top=0.9)\nfig.suptitle('Number of Rooms vs PriceGroup', fontsize=\"28\");","50a3daa7":"qual_and_con_features = [\"OverallQual\", \"OverallCond\", \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtFinType1\", \"BsmtFinType2\", \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"HeatingQC\"]\ninfo_df.loc[qual_and_con_features][[\"discret\", \"percentage_of_missing_values\", \"single_value_weight\", \"info_str\"]]","26730d7c":"clean_qual_and_con_features = [f for f in qual_and_con_features if info_df.loc[f][\"percentage_of_missing_values\"] < 0.6 \n                               and info_df.loc[f][\"single_value_weight\"] < 0.8]\nprint(\"Remaining quality and condition features:\")\nclean_qual_and_con_features","b8255cc3":"# build figure\nfig = plt.figure(figsize=(25,18))\n\n# add grid to figure\ngs = fig.add_gridspec(3,3)\n\n# fill grid with subplots\nax0001 = fig.add_subplot(gs[0,0:2])\nax02 = fig.add_subplot(gs[0, 2])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax12 = fig.add_subplot(gs[1,2])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\nax22 = fig.add_subplot(gs[2,2])\n\n# adjust subheadline fontsize\nax0001.set_title('OverallQual', fontsize=20)\nax02.set_title('OverallCond', fontsize=20)\nax10.set_title('HeatingQC', fontsize=20)\nax11.set_title('ExterQual', fontsize=20)\nax12.set_title('BsmtQual', fontsize=20)\nax20.set_title('BsmtFinType1', fontsize=20)\nax21.set_title('KitchenQual', fontsize=20)\nax22.set_title('FireplaceQu', fontsize=20)\n\n# adjust lable fontsize\nax0001.tick_params(labelsize=12)\nax02.tick_params(labelsize=12)\nax10.tick_params(labelsize=12)\nax11.tick_params(labelsize=12)\nax12.tick_params(labelsize=12)\nax20.tick_params(labelsize=12)\nax21.tick_params(labelsize=12)\nax22.tick_params(labelsize=12)\n\nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax0001, bins=50, hue=\"OverallQual\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax02, bins=25, hue=\"OverallCond\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax10, bins=25, hue=\"HeatingQC\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax11, bins=25, hue=\"ExterQual\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax12, bins=25, hue=\"BsmtQual\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax20, bins=25, hue=\"BsmtFinType1\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax21, bins=25, hue=\"KitchenQual\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax22, bins=25, hue=\"FireplaceQu\", palette=\"Set2\", multiple=\"stack\").set(xlabel=None)  \n\n# add headline\nfig.subplots_adjust(top=0.92)\nfig.suptitle('Quality and condition features vs SalePrice', fontsize=\"28\");","23cfb5a4":"material_features = [\"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"PavedDrive\", \"BsmtFinType1\", \"GarageFinish\"]\ninfo_df.loc[material_features][[\"discret\", \"percentage_of_missing_values\", \"single_value_weight\", \"info_str\"]]","12c62dad":"\"\"\"\nWe exclude RoofMatl and PavedDrive due to there single value weight.\n\"\"\"\nclean_material_features = [\"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"BsmtFinType1\", \"GarageFinish\"]\nprint(\"Remaining material features:\")\nclean_material_features","ef42c04d":"# build figure\nfig = plt.figure(figsize=(25,18))\n\n# add grid to figure\ngs = fig.add_gridspec(3,2)\n\n# fill grid with subplots\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\n\n# adjust subheadline fontsize\nax00.set_title('MasVnrType', fontsize=20)\nax01.set_title('GarageFinish', fontsize=20)\nax10.set_title('Exterior1st', fontsize=20)\nax11.set_title('Exterior2nd', fontsize=20)\nax20.set_title('Foundation', fontsize=20)\nax21.set_title('BsmtFinType1', fontsize=20)\n\n# adjust lable fontsize\nax00.tick_params(labelsize=15)\nax01.tick_params(labelsize=15)\nax10.tick_params(labelsize=15)\nax11.tick_params(labelsize=15)\nax12.tick_params(labelsize=15)\nax20.tick_params(labelsize=15)\nax21.tick_params(labelsize=15)\nax22.tick_params(labelsize=15)\n\nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax00, bins=75, hue=\"MasVnrType\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax01, bins=75, hue=\"GarageFinish\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax10, bins=25, hue=\"Exterior1st\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax11, bins=25, hue=\"Exterior2nd\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax20, bins=25, hue=\"Foundation\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax =ax21, bins=25, hue=\"BsmtFinType1\", palette=\"Paired\", multiple=\"stack\").set(xlabel=None)  \n\n# add headline\nfig.subplots_adjust(top=0.92)\nfig.suptitle('Quality and condition feature vs PriceGroup', fontsize=\"28\");","54391185":"other_categorical_features = [\"MSSubClass\", \"MSZoning\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\", \"LandSlope\", \"Neighborhood\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"Heating\", \"Electrical\", \"Functional\", \"GarageType\", \"MiscFeature\", \"SaleCondition\", \"PavedDrive\", \"CentralAir\"]\ninfo_df.loc[other_categorical_features][[\"discret\", \"percentage_of_missing_values\", \"single_value_weight\", \"info_str\"]]","1beee31d":"\"\"\"\nAgain we will exclude all features with more then 80% single value weight und more then 60% missing values.\n\"\"\"\nclean_other_categorical_features = [f for f in other_categorical_features if info_df.loc[f][\"percentage_of_missing_values\"] < 0.6 \n                               and info_df.loc[f][\"single_value_weight\"] < 0.8]\nprint(\"Remaining other categorical features:\")\nclean_other_categorical_features","a928df69":"# build figure\nfig = plt.figure(figsize=(25,25))\n\n# add grid to figure\ngs = fig.add_gridspec(3,3)\n\n# fill grid with subplots\nax0001 = fig.add_subplot(gs[0,0:2])\nax02 = fig.add_subplot(gs[0,2])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax12 = fig.add_subplot(gs[1,2])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\nax22 = fig.add_subplot(gs[2,2])\n\n# adjust subheadline fontsize\nax0001.set_title('Neighborhood', fontsize=20)\nax02.set_title('LotShape', fontsize=20)\nax10.set_title('LotConfig', fontsize=20)\nax11.set_title('MSSubClass', fontsize=20)\nax12.set_title('HouseStyle', fontsize=20)\nax20.set_title('RoofStyle', fontsize=20)\nax21.set_title('GarageType', fontsize=20)\nax22.set_title('MSZoning', fontsize=20)\n\n# adjust lable fontsize\nax0001.tick_params(labelsize=15)\nax02.tick_params(labelsize=15)\nax10.tick_params(labelsize=15)\nax11.tick_params(labelsize=15)\nax12.tick_params(labelsize=15)\nax20.tick_params(labelsize=15)\nax21.tick_params(labelsize=10)\nax22.tick_params(labelsize=15)\n\nsns.histplot(data = visual_df,y=\"Neighborhood\", kde=False, ax=ax0001, bins=50, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"LotShape\", kde=False, ax=ax02, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"LotConfig\", kde=False, ax=ax10, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"MSSubClass\", kde=False, ax=ax11, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"HouseStyle\", kde=False, ax=ax12, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"RoofStyle\", kde=False, ax=ax20, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,x=\"SalePrice\", kde=False, ax=ax21, bins=25, hue=\"GarageType\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\nsns.histplot(data = visual_df,y=\"MSZoning\", kde=False, ax=ax22, bins=25, hue=\"PriceGroup\", palette=\"Set2\", multiple=\"stack\", linewidth=2)\n# add headline\nfig.subplots_adjust(top=0.94)\nfig.suptitle('Categorical Features vs PriceGroup', fontsize=\"28\");","f70effd2":"other_general_features = [\"LotFrontage\", \"BsmtExposure\", \"Fireplaces\", \"EnclosedPorch\"]\ninfo_df.loc[other_general_features]","be2a434b":"fig = plt.figure(figsize=(25,7))\ngs = fig.add_gridspec(1,4)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\nax03 = fig.add_subplot(gs[0,3])\nax00.set_title('LotFrontage', fontsize=20)\nax01.set_title('BsmtExposure', fontsize=20)\nax02.set_title('Fireplaces', fontsize=20)\nax03.set_title('EnclosedPorch', fontsize=20)\nax00.tick_params(labelsize=12)\nax01.tick_params(labelsize=12)\nax02.tick_params(labelsize=12)\nax03.tick_params(labelsize=12)\nsns.histplot(data = visual_df,x=\"LotFrontage\", kde=False, ax=ax00, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df[visual_df['BsmtExposure'].notna()],x=\"BsmtExposure\", kde=False, ax=ax01, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"Fireplaces\", kde=False, ax=ax02, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\")\nsns.histplot(data = visual_df,x=\"EnclosedPorch\", kde=False, ax=ax03, bins=25, palette=\"Set2\", multiple=\"stack\", hue=\"PriceGroup\");","25b98de5":"# build figure\nf, ax = plt.subplots(figsize=(25, 10))\nax.tick_params(labelsize=15)\nsns.scatterplot(data=visual_df, x=\"SalePrice\", y=\"YearBuilt\", hue=\"OverallQual\", sizes=(50, 500), size=\"GrLivArea\", palette=\"Paired\")\nf.subplots_adjust(top=0.9)\nf.suptitle('SalePrice compared to YearBuilt, OverallQual and GrLivArea', fontsize=\"28\");","42b2b680":"# claculate correlations:\ncorr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(35, 24))\nax.tick_params(axis='both', which='major', labelsize=15)\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"#424949\", annot=False, \n                cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlations of numeric columns', fontsize=\"28\");","c643e667":"def copy_df(df):\n    copy_of_df = df.copy()\n    return copy_of_df\n\n# drop all columns with more then x % of single value weigth \ndef drop_single_value_weight_cols(df, weight, info_df=info_df):\n    cols = info_df[(info_df[\"discret\"]==True)\n                   & (info_df[\"single_value_weight\"]>weight)].index\n    cols = [c for c in cols if c in df.columns]\n    print(f\"dropped {', '.join(cols)} due to to much single value weight\")\n    df.drop(columns=cols, inplace=True)\n    return df\n\n# drop all columns with more then x% missing values\ndef drop_missing_value_cols(df, missing_ratio, info_df=info_df):\n    cols = info_df[info_df[\"percentage_of_missing_values\"]>missing_ratio].index\n    cols = [c for c in cols if c in df.columns]\n    print(f\"dropped {', '.join(cols)} due to to much missing values\")\n    df.drop(columns=cols, inplace=True)\n    return df\n\n# fill missing values\ndef fill_missing_values(df,info_df=info_df):\n    for col in df.columns:\n        if info_df.loc[col][\"discret\"]:\n            # discret columne\n            # get the most frequent value (we should think of a better logic later on)\n            most_frequent_value = train_df[col].value_counts().index[0]\n            num_missing = df[col].isnull().sum()\n            if num_missing > 0:\n                print(f\"filling {num_missing} missing values in {col} with {most_frequent_value}...\")\n                df[col].fillna(most_frequent_value, inplace=True)\n        else:\n            # continuous columne\n            num_missing = df[col].isnull().sum()\n            if num_missing > 0:\n                print(f\"filling {num_missing} missing values in {col} with mean...\")\n                df[col].fillna(df[col].mean(), inplace=True)\n    return df\n\n# first pipeline\ndef cleaning(df, info_df=info_df, single_value_weigt=0.8, missing_value_weight=0.5):\n    print(50*\"*\")\n    print(\"running cleaning...\")\n    piped_df =(df\n              .pipe(copy_df)\n              .pipe(drop_single_value_weight_cols, single_value_weigt)\n              .pipe(drop_missing_value_cols, missing_value_weight)\n              .pipe(fill_missing_values))\n    print(\"done.\")\n    print(50*\"*\")\n    return piped_df","5d02642d":"cleaned_df = cleaning(train_df)\ncleaned_df.head()","c0ff8c1a":"print(f\"We are down to {len(cleaned_df.columns)} columns!\")","b6adc61d":"\"\"\"\nWe omit columns with low correlations to SalePrice for the first runs:\n\"\"\"\ndef get_list_of_low_corr_coluns(df):\n    corr_df = df.corr()\n    columns = corr_df[abs(corr_df[\"SalePrice\"])<0.1][\"SalePrice\"].index\n    columns = [c for c in columns if c in df.columns]\n    return list(columns)\n\ndef drop_columns(df, columns):\n    columns = [c for c in columns if c in df.columns]\n    print(f\"dropping {', '.join(columns)}...\")\n    df.drop(columns=columns, inplace=True)\n    return df\n\ndef get_list_of_numeric_columns(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    return [c for c in df.select_dtypes(include=numerics).columns if c in df.columns]\n\n# norming might not be appropriate for all numeric features. But its fine for now.\ndef norm_columns(df, columns):\n    columns = [c for c in columns if c in df.columns]\n    for col in columns:\n        print(f\"normalising {col}...\")\n        df[col] = (df[col]-df[col].mean())\/df[col].std() \n    return df\n\ndef get_list_of_non_numeric_cat_cols(df, info_df=info_df):\n    list_of_categorical_cols = list(info_df.loc[info_df[\"discret\"]].index)\n    list_of_categorical_cols = [c for c in list_of_categorical_cols if c in df.columns]\n    list_of_numerical_cols = get_list_of_numeric_columns(df)\n    return [c for c in list_of_categorical_cols if c not in list_of_numerical_cols]\n\ndef lable_encoding(df, columns):\n    columns = [c for c in columns if c in df.columns]\n    for col in columns:\n        df[col]=df[col].astype('category').cat.codes\n    return df\n\ndef drop_lable(df):\n    if \"SalePrice\" in df.columns:\n        print(f\"dropping lable...\")\n        df.drop(columns=\"SalePrice\", inplace=True)\n    return df\n\n\"\"\"\nWe start with lable coding because one hot encoding leads to a lot of columns and therefore training takes a long time.\n\"\"\"\ndef pipeline(df, info_df=info_df):\n    print(50*\"#\")\n    print(\"running pipeline...\")\n    df = (df\n          .pipe(cleaning)\n          .pipe(drop_columns, get_list_of_low_corr_coluns(train_df))\n          .pipe(norm_columns, get_list_of_numeric_columns(df))\n          .pipe(lable_encoding, get_list_of_non_numeric_cat_cols(df))\n          .pipe(drop_lable)\n         )\n    print(50*\"#\")\n    return df","f22592bf":"X = pipeline(train_df)\nX.head()","465c0cac":"\"\"\"\ntrain split:\n\"\"\"\ny =  train_df[\"SalePrice\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y)","b4be2ab4":"xgb_model = xgb.XGBRegressor(subsample = 0.2, \n                             learning_rate=0.01,\n                             max_depth=3, \n                             n_estimators=500).fit(X_train, y_train)\nprint(\"Performance on train data:\", mean_squared_error(y_true=np.log(y_train), y_pred=np.log(xgb_model.predict(X_train)))**(0.5))\nprint(\"Performance on test data:\", mean_squared_error(y_true=np.log(y_test), y_pred=np.log(xgb_model.predict(X_test)))**(0.5))","5dd8cf97":"submission_data_df = pipeline(test_df)","e9f70508":"predictions = xgb_model.predict(submission_data_df)\n\nsubmission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = test_df['Id']\nsubmission['SalePrice'] = predictions\n#submission.to_csv('submission.csv', index=False)\n#print(\"submission worked\")","b7cb0d22":"info_df.loc[X.columns]","6f0615d2":"pipeline_test_df = train_df.copy()\n\n# combine date feature from champter 1.3.1\ndef combine_date_features(df):\n    # adds YearSinceRemod Features and drops \"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\", \"YrSold\" and \"MoSold\"\n    print(\"combine date features...\")\n    df[\"YearsSinceRemode\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    df.drop(columns=[\"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\", \"YrSold\", \"MoSold\"], inplace=True)\n    print(\"done.\")\n    return df","ced2b310":"# Cases where 1stFlrSF + 2ndFlrSF != \"GrLivArea\"\npipeline_test_df[\"1FlrSF_plus_2FlrSF\"] = pipeline_test_df[\"1stFlrSF\"] + pipeline_test_df[\"2ndFlrSF\"]\nlen(pipeline_test_df[pipeline_test_df[\"1FlrSF_plus_2FlrSF\"] != pipeline_test_df[\"GrLivArea\"]])","42be3b6c":"# combine size feature from champter 1.3.2\ndef combine_size_features(df):\n    # drop \"BsmtFinSF1\", \"BsmtUnfSF\", \"1stFlrSF\", \"2ndFlrSF\", \"MasVnrArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"GarageArea\" and ScreenPorch\n    # add 2ndFloor feature\n    print(\"combine size features...\")\n    df[\"2ndFloor\"] = df[\"2ndFlrSF\"].map(lambda x : \"2ndFloor_True\" if (x > 0) else \"2ndFloor_False\")\n    df.drop(columns=[\"BsmtFinSF1\", \"BsmtUnfSF\", \"1stFlrSF\", \"2ndFlrSF\", \"MasVnrArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"GarageArea\", \"ScreenPorch\"], inplace=True)\n    print(\"done.\")\n    return df\n\n# combine room_features from 1.3.3\ndef combine_room_features(df):\n    # add feature IncludesBsmtBathroom\n    # combine FullBath and HalfBath to new feature Bath\n    # drom BsmtFullBath, BsmtHalfBath, FullBath, HalfBath and KitchenAbvGr\n    print(\"combine room features...\")\n    df[\"Bath\"] = df[\"FullBath\"] + 1\/2 * df[\"HalfBath\"]\n    df[\"IncludesBsmtFullBath\"] = df[\"BsmtFullBath\"].map(lambda x : \"BsmtFullBath_True\" if (x > 0) else \"BsmtFullBath_False\")\n    df.drop(columns=[\"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\", \"KitchenAbvGr\"], inplace=True)\n    print(\"done.\")\n    return df\n\n# combine condition ad quality features from 1.3.4\ndef clean_qual_and_con_features(df):\n    # drop OverallCond (looked a little skewed)\n    # drop ExterQual, BsmtQual, KitchenQual, HeatingQC and FireplaceQu (little information and covered in OverallQuality)\n    print(\"clear qual and con features...\")\n    df.drop(columns=[\"OverallCond\", \"ExterQual\", \"BsmtQual\", \"KitchenQual\", \"FireplaceQu\", \"HeatingQC\"], inplace=True)\n    print(\"done.\")\n    return df","7e1605d5":"# materiel features:\n# drop  MasVnrType and Exterior2nd (mostly included in Exterior1st)\n# cast Foundation down to 3 different types\n# cast BsmtFinType1 and Exterior1st down to 4 different types\n\ndisplay(HTML(info_df.loc[[\"Foundation\", \"BsmtFinType1\", \"Exterior1st\"]][[\"discret\", \"info_str\"]].to_html()))\n#info_df.loc[[\"Foundation\", \"BsmtFinType1\", \"Exterior1st\"]][\"info_str\"]","05cb8d03":"# combine material features from 1.3.5\ndef transform_material_features(df):\n    print(\"transformin material features...\")\n    foundation_lables = [\"PConc\", \"CBlock\"]\n    bsmt_fin_lables = [\"Unf\", \"GLQ\", \"ALQ\"]\n    exterior1st_lables = [\"VinylSB\", \"HdBoard\", \"MetalSd\", \"Wd Sdng\"]\n    def get_lable(val, list_of_lables, remaining_class):\n        if val in list_of_lables:\n            return val\n        else:\n            return remaining_class\n    # cast down \n    df[\"Foundation\"] = df[\"Foundation\"].map(lambda x: get_lable(x, foundation_lables, \"FBlock\"))\n    df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].map(lambda x: get_lable(x, bsmt_fin_lables, \"FLQ\"))\n    df[\"Exterior1st\"] = df[\"Exterior1st\"].map(lambda x: get_lable(x, exterior1st_lables, \"FBoard\"))\n    # drop \n    df.drop(columns=[\"MasVnrType\", \"Exterior2nd\"], inplace=True)\n    print(\"done.\")\n    return df\n\n# handle other categorical features from 1.3.6\ndef transform_other_cat_features(df):\n    # exclude LotConfig, RoofStyle, MSZoning and MSSubClass\n    # transform GarageType\n    # transform LotShape\n    # transform HouseStyle\n    print(\"transforming other material features...\")\n    def get_house_lable(val):\n        if val in [\"1Story\", \"2Story\"]:\n            return val\n        else:\n            return \"otherStory\"\n    df[\"AttachedGarage\"] = df[\"GarageType\"].map(lambda x : \"AttachedGarage_True\" if (x==\"Attchd\" or x==\"BuiltIn\") else \"AttachedGarage_False\")\n    df[\"RegLotShape\"] = df[\"LotShape\"].map(lambda x : \"RegLotShape_True\" if (x==\"Reg\") else \"RegLotShape_False\")\n    df[\"HouseStyle\"] = df[\"HouseStyle\"].map(lambda x: get_house_lable(x))\n    df.drop(columns=[\"LotConfig\", \"RoofStyle\", \"MSZoning\", \"MSSubClass\", \"GarageType\", \"LotShape\"], inplace=True)\n    print(\"done.\")\n    return df\n\n# handle other general features from 1.3.7\ndef handle_other_general_features(df):\n    # drop BsmtExposure and EnclosedPorch\n    # transform Fireplaces to has fireplace\n    print(\"handle other general features...\")\n    df[\"HasFireplace\"] = df[\"Fireplaces\"].map(lambda x : \"Fireplaces_True\" if (x>0) else \"Fireplaces_False\")\n    df.drop(columns=[\"BsmtExposure\", \"EnclosedPorch\", \"Fireplaces\"], inplace=True)\n    print(\"done.\")\n    return df","2c08628f":" pipeline_test_df = (train_df\n          .pipe(copy_df)\n          .pipe(combine_size_features)\n          .pipe(combine_date_features)\n          .pipe(combine_room_features)\n          .pipe(clean_qual_and_con_features)\n          .pipe(transform_material_features)\n          .pipe(transform_other_cat_features)\n          .pipe(handle_other_general_features)\n         )\npipeline_test_df[[\"HouseStyle\", \"RegLotShape\", \"AttachedGarage\", \"Exterior1st\", \"BsmtFinType1\", \"Foundation\", \"IncludesBsmtFullBath\", \"Bath\", \"2ndFloor\", \"YearsSinceRemode\"]].head()","aeb9666a":"pipeline_test_df = (test_df\n          .pipe(copy_df)\n          .pipe(combine_size_features)\n          .pipe(combine_date_features)\n          .pipe(combine_room_features)\n          .pipe(clean_qual_and_con_features)\n          .pipe(transform_material_features)\n          .pipe(transform_other_cat_features)\n          .pipe(handle_other_general_features)\n         )\npipeline_test_df[[\"HouseStyle\", \"RegLotShape\", \"AttachedGarage\", \"Exterior1st\", \"BsmtFinType1\", \"Foundation\", \"IncludesBsmtFullBath\", \"Bath\", \"2ndFloor\", \"YearsSinceRemode\"]].head()","cfc11232":"def advanced_pipeline(df, info_df=info_df, single_value_weigt=0.8, missing_value_weight=0.6):\n    print(50*\"#\")\n    print(\"running advanced pipeline...\")\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_missing_values)\n          .pipe(combine_size_features)\n          .pipe(combine_date_features)\n          .pipe(combine_room_features)\n          .pipe(clean_qual_and_con_features)\n          .pipe(transform_material_features)\n          .pipe(transform_other_cat_features)\n          .pipe(handle_other_general_features)\n          .pipe(drop_single_value_weight_cols, single_value_weigt)\n          .pipe(drop_missing_value_cols, missing_value_weight)\n          .pipe(drop_columns, get_list_of_low_corr_coluns(train_df))\n          .pipe(drop_lable)\n         )\n    print(50*\"#\")\n    return df\npiped_df = advanced_pipeline(train_df)","e1722e49":"print(f\"We are done to {len(piped_df.columns)} features!\")","a4b2c6aa":"display(HTML(piped_df.head().to_html()))","3a934ce9":"# one hot encoding implementation\ndef one_hot_encoding(df, columns):\n    for col in columns:\n        print(f\"one hot encoding for {col}...\")\n        # Get one hot encoding of columns B\n        one_hot_df = pd.get_dummies(df[col])\n        # Drop input column as it is now encoded\n        df = df.drop(col, axis=1)\n        # add prefix to prevent overlapping values\n        one_hot_df = one_hot_df.add_prefix(col + \"_\")\n        # Join the encoded df\n        df = df.join(one_hot_df)\n    return df\n\n# truncate function:\ndef truncate_upper_and_lower_quantile(df, columns, lower_quantile=0.1, upper_quantile=0.9):\n    for col in columns:\n        print(f\"truncate {col}\")\n        u_quant = df[col].quantile(upper_quantile)\n        l_quant = df[col].quantile(lower_quantile)\n        df[col] = np.where(df[col]>u_quant, u_quant,df[col])\n        df[col] = np.where(df[col]<l_quant, l_quant,df[col])\n    return df","0325e5fb":"\"\"\"\nLet's override our pipeline function:\n\"\"\"\ndef advanced_pipeline(df, info_df=info_df, single_value_weigt=0.8, missing_value_weight=0.6):\n    print(50*\"#\")\n    print(\"running advanced pipeline...\")\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_missing_values)\n          .pipe(combine_size_features)\n          .pipe(combine_date_features)\n          .pipe(combine_room_features)\n          .pipe(clean_qual_and_con_features)\n          .pipe(transform_material_features)\n          .pipe(transform_other_cat_features)\n          .pipe(handle_other_general_features)\n          .pipe(drop_single_value_weight_cols, single_value_weigt)\n          .pipe(drop_missing_value_cols, missing_value_weight)\n          .pipe(drop_columns, get_list_of_low_corr_coluns(train_df))\n          .pipe(one_hot_encoding, [\"Neighborhood\", \"HouseStyle\", \"Exterior1st\", \"Foundation\", \"BsmtFinType1\", \"GarageFinish\", \"2ndFloor\", \"IncludesBsmtFullBath\", \"AttachedGarage\", \"RegLotShape\", \"HasFireplace\"])\n          .pipe(truncate_upper_and_lower_quantile, [\"LotFrontage\", \"LotArea\", \"TotalBsmtSF\", \"GrLivArea\"])\n          .pipe(norm_columns, [\"LotFrontage\", \"LotArea\", \"OverallQual\", \"TotalBsmtSF\", \"GrLivArea\", \"BedroomAbvGr\", \"TotRmsAbvGrd\", \"GarageCars\", \"YearsSinceRemode\", \"Bath\"])\n          .pipe(drop_lable)\n         )\n    print(50*\"#\")\n    return df\npiped_df = advanced_pipeline(train_df)","2165c520":"display(HTML(piped_df.head().to_html()))","9855ca7b":"print(f\"Number of columns: {len(piped_df.columns)}\")","188c0910":"from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom functools import partial\n#from sklearn.metrics import mean_squared_error\n\ny = train_df[\"SalePrice\"]\nX = advanced_pipeline(train_df)\nqt = QuantileTransformer(random_state=0, output_distribution=\"normal\")\ny = y.values.reshape(-1, 1)\ny = qt.fit_transform(y)\ny = pd.Series(y[:, 0])","61b3e62c":"\"\"\"\nTrain test split\n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y)","6fa79a1b":"def get_loss(model, X, y, transformer=qt):\n    \"\"\"\n    Note: It is very important that we back-transform our data first,\n    otherwise we have a problem with negative values as input for the logarithm.\n    \"\"\"\n    prediction = model.predict(X)\n    prediction = prediction.reshape(-1, 1)\n    prediction = qt.inverse_transform(prediction)\n    \n    y = y.values.reshape(-1, 1)\n    y = qt.inverse_transform(y)\n    \n    return mean_squared_error(y_true=np.log(y), y_pred=np.log(prediction))**(0.5)","343054b0":"xgb_model = xgb.XGBRegressor(subsample=0.7, \n                             learning_rate=0.02,\n                             max_depth=3, \n                             random_state=np.random.randint(1000),\n                             n_estimators=500).fit(X_train, y_train)\nprint(\"Performance on train data:\", get_loss(xgb_model, X_train, y_train))\nprint(\"Performance on test data:\", get_loss(xgb_model, X_test, y_test))","8e47acb0":"gb_model = GradientBoostingRegressor(subsample=0.7, \n                             learning_rate=0.02,\n                             max_depth=3, \n                             random_state=np.random.randint(1000),\n                             n_estimators=500).fit(X_train, y_train)\nprint(\"Performance on train data:\",  get_loss(gb_model, X_train, y_train))\nprint(\"Performance on test data:\",  get_loss(gb_model, X_test, y_test))","d200b744":"\"\"\"\nLet's add a linear model:\n\"\"\"\nclf = linear_model.Lasso(alpha=0.5).fit(X_train, y_train)\nprint(\"Performance on train data:\", get_loss(clf, X_train, y_train))\nprint(\"Performance on test data:\", get_loss(clf, X_test, y_test))","e1f0fa70":"# We use GridSearchCV for our lasso regression model since there is only one parameter to tune.\nparam_grid_lasso = {'alpha': 0.0001 * np.arange(1, 100)}\ngrid_lasso = GridSearchCV(linear_model.Lasso(), param_grid_lasso, \n                          cv=RepeatedKFold(n_splits=10, n_repeats=2),\n                          verbose=2, n_jobs=-1).fit(X, y)\nbest_lasso_params = grid_lasso.best_params_","d8a0e96d":"def objective(trial, X, y, model_name):\n    \n    if model_name==\"gbr\":\n        params = {'n_estimators':500,\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.005, 0.01),\n              'subsample':trial.suggest_categorical('subsample', [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              'max_depth':trial.suggest_int('max_depth', 3, 11)\n             }\n        model = GradientBoostingRegressor(**params)   \n    elif model_name==\"xgbr\":\n        params = {'n_estimators':500,\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.005, 0.01),\n              'subsample':trial.suggest_categorical('subsample', [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              'max_depth':trial.suggest_int('max_depth', 3, 11),\n              'colsample_bylevel':trial.suggest_categorical('colsample_bylevel', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              'gamma':trial.suggest_uniform('gamma', 0.05, 0.1)\n             }\n        model = xgb.XGBRegressor(**params)\n    \n    k_fold = KFold(n_splits=5)\n    train_scores = []\n    test_scores = []\n    print(\"Running study for \", model_name)\n    for train_idx, val_idx in k_fold.split(X):\n        X_tr = X.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_tr = y.iloc[train_idx]\n        y_val = y.iloc[val_idx]\n         \n        model.fit(X_tr, y_tr)\n        \n        train_scores.append(get_loss(model, X_tr, y_tr))\n        test_scores.append(get_loss(model, X_val, y_val))\n        \n    train_score = np.round(np.mean(train_scores), 4)\n    test_score = np.round(np.mean(test_scores), 4)\n    print(f'train score : {train_scores}')\n    print(f'test score : {test_scores}')\n    print(f'TRAIN loss : {train_score} || TEST loss : {test_score}')\n    return test_score","7f6abef0":"# optimize XGBRegressor\nxgbr_optimize = partial(objective, X=X, y=y, model_name=\"xgbr\")\n\nxgbr_study = optuna.create_study(direction='minimize')\nxgbr_study.optimize(xgbr_optimize, n_trials=25)","7dc98117":"# optimize XGBRegressor\ngbr_optimize = partial(objective, X=X, y=y, model_name=\"gbr\")\n\ngbr_study = optuna.create_study(direction='minimize')\ngbr_study.optimize(gbr_optimize, n_trials=25)","b4cd0d5a":"fig = optuna.visualization.plot_optimization_history(xgbr_study)\nfig.update_layout(title=\"XGBRegressor L2_Score vs Studies\", autosize=False, width=1500, height=500, margin=dict(l=20, r=20, t=50, b=20),\n    paper_bgcolor=\"LightSteelBlue\", xaxis_title=\"Studies\", yaxis_title=\"L2_score\", legend_title=\"Legend\",\n    font=dict(family=\"Courier New, monospace\", size=24,color=\"RebeccaPurple\"))\nfig.show()","98da00ac":"fig = optuna.visualization.plot_slice(xgbr_study)\nfig.update_layout(title=\"XGBRegressor Hyperparameter Selection\", autosize=False, width=1500, height=500, margin=dict(l=20, r=20, t=50, b=20),\n    paper_bgcolor=\"LightSteelBlue\", xaxis_title=\"Studies\", yaxis_title=\"L2_score\", legend_title=\"Legend\",\n    font=dict(family=\"Courier New, monospace\", size=18,color=\"RebeccaPurple\"))\nfig.show()","7d4e8677":"fig = optuna.visualization.plot_optimization_history(gbr_study)\nfig.update_layout(title=\"GradientBoostingRegressor L2_Score vs Studies\", autosize=False, width=1500, height=500, margin=dict(l=20, r=20, t=50, b=20),\n    paper_bgcolor=\"LightSteelBlue\", xaxis_title=\"Studies\", yaxis_title=\"L2_score\", legend_title=\"Legend\",\n    font=dict(family=\"Courier New, monospace\", size=24,color=\"RebeccaPurple\"))\nfig.show()","caf3b955":"fig = optuna.visualization.plot_slice(gbr_study)\nfig.update_layout(title=\"GradientBoostingRegressor Hyperparameter Selection\", autosize=False, width=1500, height=500, margin=dict(l=20, r=20, t=50, b=20),\n    paper_bgcolor=\"LightSteelBlue\", xaxis_title=\"Studies\", yaxis_title=\"L2_score\", legend_title=\"Legend\",\n    font=dict(family=\"Courier New, monospace\", size=18,color=\"RebeccaPurple\"))\nfig.show()","8018d663":"pest_params_for_xgbr=xgbr_study.best_params\npest_params_for_xgbr[\"n_estimators\"] = 500\npest_params_for_gbr=gbr_study.best_params\npest_params_for_gbr[\"n_estimators\"] = 500\n\nprint(50*\"-\")\nprint(\"Tuned Hyperparameters:\")\nprint(\"Best hyperparameters for lasso regression: \", best_lasso_params)\nprint(\"Best hyperparameters for XGBRegressor: \", pest_params_for_xgbr)\nprint(\"Best hyperparameters for GradientBoostingRegressor: \", pest_params_for_gbr)\nprint(50*\"-\")","fa7ce745":"def model_evaluation(model):\n    cv = RepeatedKFold(n_splits=8, n_repeats=2)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return np.abs(scores)\n\nlasso_model = linear_model.Lasso(**best_lasso_params)\ngradient_model = GradientBoostingRegressor(**pest_params_for_gbr)\nxgb_model = xgb.XGBRegressor(**pest_params_for_xgbr)\n\n\nmodel_list = [('lasso', lasso_model),   \n               ('gradient', gradient_model),\n               ('xgb', xgb_model)]\n\nstack_model = StackingRegressor(estimators=model_list,\n                                cv=8, n_jobs=-1)\n\nmodels = {'lasso': lasso_model, \n           'gradient': gradient_model, \n           'xgb': xgb_model, \n           'stack_reg':stack_model}\ntrain_scores = []\ntest_scores = []\nnames = []\n\nk_fold = KFold(n_splits=15)\nfor name in models.keys():\n    print(\"Running study for \", name, \"...\")\n    tr_scores = []\n    te_scores = []\n    for train_idx, val_idx in k_fold.split(X):\n        X_tr = X.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_tr = y.iloc[train_idx]\n        y_val = y.iloc[val_idx]\n        model = models[name]\n        model.fit(X_tr, y_tr)\n        tr_scores.append(get_loss(model, X_tr, y_tr))\n        te_scores.append(get_loss(model, X_val, y_val))\n    train_scores.append(tr_scores)\n    test_scores.append(te_scores)\n    names.append(name)\n    print(\"done.\")\nresult_train_df = pd.DataFrame().from_dict({names[i]: train_scores[i] for i in range(len(train_scores))})\nresult_test_df = pd.DataFrame().from_dict({names[i]: test_scores[i] for i in range(len(test_scores))} )","168fc22d":"#plot results:\nfig = plt.figure(figsize=(25,7))\ngs = fig.add_gridspec(1,2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(labelsize=15)\nax01.tick_params(labelsize=15)\nax00.set_title('Train Loss Comparison', fontsize=20)\nax01.set_title('Test Loss Comparison', fontsize=20)\nsns.boxplot(data=result_train_df, orient=\"h\", palette=\"Set2\", ax=ax00)\nsns.swarmplot(data=result_train_df, orient=\"h\", color=\".25\", ax=ax00)\nsns.boxplot(data=result_test_df, orient=\"h\", palette=\"Set2\", ax=ax01)\nsns.swarmplot(data=result_test_df, orient=\"h\", color=\".25\", ax=ax01)\nf.subplots_adjust(top=0.85)\nf.suptitle('Model Performance Comparison', fontsize=\"28\");","d935befd":"submission_model = stack_model.fit(X, y);","3df3195a":"submission_test_data = advanced_pipeline(test_df)","e024e003":"predictions = submission_model.predict(submission_test_data)\npredictions = predictions.reshape(-1, 1)\npredictions = qt.inverse_transform(predictions)\n\nsubmission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = test_df['Id']\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('stack_submission5.csv', index=False)\nprint(\"submission worked\")","f56d086b":"\"\"\"\nWe'll adjust our pipeline a little bit. We don't want use one hot encoding when working with batch normalization in our neuronal network, so we just use our lable_endcoding function.\n\"\"\"\ndef nn_pipeline(df, info_df=info_df, single_value_weigt=0.8, missing_value_weight=0.6):\n    print(50*\"#\")\n    print(\"running advanced pipeline...\")\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_missing_values)\n          .pipe(combine_size_features)\n          .pipe(combine_date_features)\n          .pipe(combine_room_features)\n          .pipe(clean_qual_and_con_features)\n          .pipe(transform_material_features)\n          .pipe(transform_other_cat_features)\n          .pipe(handle_other_general_features)\n          .pipe(drop_single_value_weight_cols, single_value_weigt)\n          .pipe(drop_missing_value_cols, missing_value_weight)\n          .pipe(drop_columns, get_list_of_low_corr_coluns(train_df))\n          .pipe(lable_encoding,[\"Neighborhood\", \"HouseStyle\", \"Exterior1st\", \"Foundation\", \"BsmtFinType1\", \"GarageFinish\", \"2ndFloor\", \"IncludesBsmtFullBath\", \"AttachedGarage\", \"RegLotShape\", \"HasFireplace\"])\n          .pipe(truncate_upper_and_lower_quantile, [\"LotFrontage\", \"LotArea\", \"TotalBsmtSF\", \"GrLivArea\"])\n          .pipe(norm_columns, [\"LotFrontage\", \"LotArea\", \"OverallQual\", \"TotalBsmtSF\", \"GrLivArea\", \"BedroomAbvGr\", \"TotRmsAbvGrd\", \"GarageCars\", \"YearsSinceRemode\", \"Bath\"] +\n                [\"Neighborhood\", \"HouseStyle\", \"Exterior1st\", \"Foundation\", \"BsmtFinType1\", \"GarageFinish\", \"2ndFloor\", \"IncludesBsmtFullBath\", \"AttachedGarage\", \"RegLotShape\", \"HasFireplace\"])\n          .pipe(drop_lable)\n         )\n    print(50*\"#\")\n    return df\n\n\"\"\"\nLet's add a small function to evaluate the training of our model.\n\"\"\"\n\ndef plot_model_result(history, ymax=0.4):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    lr = history.history['lr']\n    min_index = val_loss.index(min(val_loss))\n    fig = plt.figure(figsize=(25,7))\n    gs = fig.add_gridspec(1,3)\n    ax00 = fig.add_subplot(gs[0,0:2])\n    ax01 = fig.add_subplot(gs[0,2])\n    ax00.set_title('Training and validation Loss', fontsize=20)\n    ax01.set_title('Learning rate', fontsize=20)\n    ax00.tick_params(labelsize=12)\n    ax01.tick_params(labelsize=12)\n    ax00.set(ylim=(0, ymax))\n    ax00.hlines(y=min(val_loss), color='b', linewidth=1, alpha=.7, ls='--', xmin=0 , xmax = epochs, label=\"min_validation_loss\") \n    ax00.vlines(x=min_index, ymin=0, ymax=ymax, color=\"b\", linewidth=1, alpha=.7, ls='--' )\n    sns.lineplot(x = range(0,len(loss)), linewidth=2, y=loss, ax=ax00, label=\"loss\")\n    sns.lineplot(x = range(0,len(loss)), linewidth=2, y=val_loss, ax=ax00, label=\"validation_loss\")\n    sns.lineplot(x = range(0,len(loss)), linewidth=2, y=lr, ax=ax01)\n    fig.subplots_adjust(top=0.85)\n    fig.suptitle(f'Min validation loss: {round(min(val_loss),2)} at epoch {min_index}', fontsize=\"28\");","a7c81db5":"# pipelining:\ny = train_df[\"SalePrice\"]\nX = nn_pipeline(train_df)\nqt = QuantileTransformer(random_state=0, output_distribution=\"normal\")\ny = y.values.reshape(-1, 1)\ny = qt.fit_transform(y)\ny = pd.Series(y[:, 0])\n# split:\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX.head()","60e1a1cf":"nn_model = tf.keras.models.Sequential([\n  tf.keras.layers.Input(shape=(X.shape[1],)),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(1, activation=\"linear\")\n])\n\ninitial_lr = 0.01\nepochs = 500\nbatch_size = X_train.shape[0]\nverbose = 0\nvalidation_split=0.2\n\ndecay = initial_lr \/ epochs\n\n# some other learning rate decay function that you can play with. \n# You can simply replace it in the model.fit section under callbacks\ndef linear_decay_lr(epoch, lr):\n    return lr * 1 \/ (1 + decay * epoch)\n\ndef constatt_lr(epoch, lr):\n    return lr\n\ndef step_decay_lr(epoch, lr):\n    drop_rate = 0.5\n    epochs_drop = epochs\/10\n    return initial_lr * math.pow(drop_rate, math.floor(epoch\/epochs_drop))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\nnn_model.compile(optimizer=optimizer, loss=\"mean_squared_error\")\n\n# Early stopping can also be added under model.fit, but that didn't work so well for this problem.\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n\nhistory = nn_model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=validation_split,\n    shuffle=False,\n    verbose=verbose,\n    callbacks=[tf.keras.callbacks.LearningRateScheduler(step_decay_lr, verbose=verbose)]\n)\n\nplot_model_result(history)\n\nprint(50*\"*\")\nprint(f\"RMSE test: {get_loss(nn_model, X_test, y_test)}\")\nprint(f\"RMSE train: {get_loss(nn_model, X_train, y_train)}\")\nprint(50*\"*\")","a93998bf":"submission_test_data = nn_pipeline(test_df)\npredictions = nn_model.predict(submission_test_data)\npredictions = predictions.reshape(-1, 1)\npredictions = qt.inverse_transform(predictions)\n\nsubmission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = test_df['Id']\nsubmission['SalePrice'] = predictions\n#submission.to_csv('nn_submission_private.csv', index=False)\nprint(\"submission worked\")","e08a3d86":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h4> Whats next \ud83e\udd14? <\/h4>\n<hr>\n    \n* We need to scale LotFront, LotArea, TotalBsmtSF, GrLiveArea, YearsSinceRemod, OverallQual, BedroomAbvGr, Bath and TotRmsAbvGrd.\n  - MinMax scaling may not be the best method for all of them. We should truncate the outlires of LotFront, LotArea, TotalBsmtSF and GrLiveArea\n* The rest of the features can be processed with one hot encoding\n\nSo let's write some functions for these last pipeline steps.\n<\/div>","653b6717":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <h4> Score: around 0.15 \ud83e\udd14<\/h4>\n\nAfter playing a bit with different parameters, network structure, early stopping, different learning rates and batch sizes, I couldn't really improve.\nI'll keep trying, but if you have any tips or a notbook that uses Tensorflow to get a better score in this challenge, please let me know in the comments section. \nAnd as always, have a great and productive day \ud83d\ude0e\ud83d\udc4d\n<\/div>","d544ce17":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n\nAll in all, we got a lot of information in this chapter that we will use later for our data pipeline. So stay tuned! But first we start with a general pre-processing of the data for a first training.\n<\/div>","18a00fc9":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h4> Score: Around 0.15 \ud83e\udd73<\/h4>\n\nNot too bad for a start. But we will improve!\n<\/div>","60f185fb":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nAs expected. Buildings with more rooms are more expensive and vice versa. We could take a closer look at basements and living spaces. We can see that the single value weight of so some of these features are quite hight. So they may not be so interesting alone.\nLet's take a quick look at the distribution.\n<\/div>","837c3b10":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nOk, most of the buildings built before 1950 were built after 1900 and given the history between 1914 and 1945 it seems understandable that none of them were remodeled before 1950. It still seems a little odd that so many were renovated in that year and not in 1949 for example. Perhaps there was some sort of government assistance? Bevore we move on let's look at the distribution of time between YearBuild and RemoveAdd and the time between RemovedAdd and YrSold.\n<\/div>","93137858":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nThat should do it.\n<\/div>","a2fd41da":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nThis is a very simple pipeline. We have not looked at the distribution of each numerical value to decide whether MinMax scaling is appropriate. Also, we have simply used table coding and no further feature engineering. But let's give it a try and create our first submission for this problem. \n<\/div>","b34fdbe0":"<a id=\"4.\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h2> 4. Feature Engineering And Pipeline Extension \ud83e\udd13\ud83d\udd27\ud83c\udfed<\/h2> <\/center>\n<hr>\n    <h4> So what do we have after the data is run through our pipeline? <\/h4>\n\n<\/div>","e2707eb2":"<a id=\"5.2\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <h3> 5.2 New Submission \ud83d\ude28<\/h3>\n<\/div>","713053b0":"<a id=\"4.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3> 4.1 Test Additional Features And New Pipeline Functions \ud83d\udc40\ud83d\udd0e\ud83c\udfed<\/h3> <\/center>\n<\/div>","2a779781":"<a id=\"1.3.6\"><\/a>\n\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h4> 1.3.6 Other Categorical Features: <\/h4> <\/center>\n<hr>\n<h4> Features Explanation: <\/h4>   \n<ul>\n<li> MSSubClass: Identifies the type of dwelling involved in the sale. <\/li>\n<li> MSZoning: Identifies the general zoning classification of the sale. <\/li> \n<li> LotShape: General shape of property <\/li>\n<li> LandContour: Flatness of the property <\/li>\n<li> Utilities: Type of utilities available <\/li>\n<li> LotConfig: Lot configuration <\/li>\n<li> LandSlope: Slope of property <\/li>\n<li> Neighborhood: Physical locations within Ames city limits <\/li>\n<li> BldgType: Type of dwelling <\/li>\n<li> HouseStyle: Style of dwelling <\/li>\n<li> RoofStyle: Type of roof <\/li>\n<li> Heating: Type of heating <\/li>\n<li> Electrical: Electrical system <\/li>\n<li> Functional: Home functionality (Assume typical unless deductions are warranted) <\/li>\n<li> GarageType: Garage location <\/li>\n<li> MiscFeature: Miscellaneous feature not covered in other categories <\/li>\n<li> SaleCondition: Condition of sale <\/li>\n<li> PavedDrive: Paved driveway <\/li>\n<li> CentralAir: Central air conditioning <\/li>\n<\/ul>\n<\/div>","65a0eb91":"<a id=\"1.3.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> <h3> 1.3.1 Date Features: \ud83d\udd26\ud83d\udcc6<\/h3> <\/center>\n<hr>\n<h4>Features Explanation:<\/h4>\n<ul>\n<li> YrSold: Year Sold (YYYY) <\/li> \n<li> MoSold: Month Sold (MM) <\/li>\n<li> GarageYrBlt: Year garage was built <\/li>\n<li> YearBuilt: Original construction date <\/li>\n<li> YearRemodAdd: Remodel date (same as construction date if no remodeling or additions) <\/li>\n<\/ul>\n<\/div>","6ec5d3a4":"<a id=\"1.3.8\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h4> 1.3.8 General Plots: <\/h4> <\/center>\n    \n<hr>\n\nWe could expand on this inspection, but for now we'll wrap up this section with some general plotting and move on to some preprossesing.\n<\/div>","09e024e1":"<a id=\"1.1\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center>\n<h3 id=\"1.1\">\n1.1 Discret Columns:\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/stefanschulmeister87\/visual-data-inspection-and-model-stacking-html-markdown\/notebook#1.1\">\u00b6<\/a>\n<\/h3>\n<center>\n<\/div>","70f5453f":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nLet us conclude this section with a look at the correlation of all numerical features.\n<\/div>","5720c0f3":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nGarageCars looks promesing but do we need this feature when we have GarageArea?","5ea7cf85":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \nLooks good. No \"-\" and all groups seem to work fine. So let's continue with some plots of the date features with our new price groups and see if we find something.\n<\/div>","c8078ba8":"<a id=\"1.3.7\"><\/a>\n\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h4> 1.3.7 Other General Features: <\/h4> <\/center>\n<hr>\n    \n<h4> Features Explanation: <\/h4>\n<ul>   \n<li> LotFrontage: Linear feet of street connected to property <\/li>\n<li> BsmtExposure: Walkout or garden level basement walls <\/li>\n<li> Fireplaces: Number of fireplaces <\/li>\n<li> EnclosedPorch: Enclosed porch area in square feet <\/li>\n<\/ul>","856309d5":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n    <h4> Conclusion: \ud83d\udcdd <\/h4>\n<hr>\n<ul>\n<li> BsmtFinSF1, 2ndFlrSF and MasVnrArea and all excluded size features are mostly 0, because those areas are not included in all buildings.<\/li>\n    <ul>\n    <li> We may exclude them and create some features that indicate whether certain areas are included or not. <\/li>\n    <\/ul>\n<li> We should keep either GaraeArea or GarageCase, not both. Maybe GarageCars is the better choice, since it is not so prone to overfitting. <\/li>\n<li> All these outliers will disturb our algorithms, so we should exclude them for training. <\/li>\n<\/ul>\n\nLet's move on to room features.\n<\/div>","7d4a8540":"<a id=\"2.4\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3> 2.4 Pipeline Functions Test <\/h3> <\/center>\n<\/div>","46ad92d7":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nLet's take a closer look at these features.\n<\/div>","28b6fd6f":"<a id=\"2.3\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3> 2.3 Pipeline Functions \ud83d\udcc3\u27a1\ud83c\udfed<\/h3> <\/center>\n<hr>\nLet's continue with some pipeline functions.\n<\/div>","86a346b2":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nLet's start by creating a feature for the total number of rooms (TotRms) and see how it correlates with our PriceGroups compared to the existing feature TotRmsAbvGrd.\n<\/div>","313b3cc4":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nThis time we will use multiple models and then combine them with StackingRegressor to hopefully reduce the overall variance and perhaps improve the specified loss function.\nSpeaking of the loss function, for this contest we need to calculate the mean square error between the logarithm of the target and the logarithm of the prediction. \nFor this we will write a small function in advance.\n<\/div>","cf9a3a5c":"## Table of contents\n* [1. Data Screening](#1.)\n - [1.1 Discret Columns](#1.1)\n - [1.2 Continuous Columns](#1.2)\n - [1.3 Visualizations](#1.3)\n    - [1.3.1 Date Features](#1.3.1)\n    - [1.3.2 Size Features](#1.3.2)\n    - [1.3.3 Room Features](#1.3.3)\n    - [1.3.4 Quality And Condition Features](#1.3.4)\n    - [1.3.5 Material Features](#1.3.5)\n    - [1.3.6 Other Categorical Features](#1.3.6)\n    - [1.3.7 Other General Features](#1.3.7)\n    - [1.3.8 General Plots](#1.3.8)\n* [2. Data Preparation](#2.)\n - [2.1 Cleaning Functions](#2.1)\n - [2.2 Cleaning Functions Test](#2.2)\n - [2.3 Pipeline Functions](#2.3)\n* [3. Simple Model](#3.)\n - [3.1 Fist Submission](#3.1)\n* [4. Feature Engineering And Pipeline Extension](#4.)\n - [4.1 Test Additional Features And New Pipeline Functions](#4.1)\n - [4.2 New Pipeline](#4.2)\n* [5. Modeling Selection And Parameter Tuning](#5.)\n - [5.1 Parameter Tuning With Optuna](#5.1)\n     - [5.1.1 Optuna Stundy Visualisation For XGBRegressor](#5.1.1)\n     - [5.1.2 Optuna Stundy Visualisation For GradientBoostingRegressor](#5.1.2)\n     - [5.1.3 Model Performance Comparison](#5.1.3)\n - [5.2 New Submission](#5.2)\n* [6. Tensorflow](#6.)\n - [6.1 Tensorflow Submission](#6.1)","04d287a2":"<a id=\"2.\"><\/a>\n<a id=\"2.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h2>\ud83d\udcdc\ud83e\uddfe\ud83d\udcca --> 2. Data Preparation --> \ud83d\udcc4 <\/h2> <\/center>\n\n<hr>\n    \n<center> <h3> 2.1 Cleaning Functions \ud83e\uddf9\ud83d\udcc3<\/h3> <\/center>  \n    \n<hr>    \n    \nWe will start with some simple cleaning functions.","d8baaf9a":"<a id=\"5.\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h2> 5. Modeling Selection And Parameter Tuning \ud83d\udc40\ud83d\udd0e\ud83d\udcc4<\/h2> <\/center>\n<hr>\nThis time we will train 3 different models and steck them together.\n<\/div>","1d0adece":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> <h1>\ud83c\udfe1\ud83c\udfd8 House Price Prediction \ud83c\udfe6\ud83c\udfe0<\/h1> <\/center> \n<\/div>","3ae28863":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \nContrary to my expectation, we see no correlation between YrSold and PriceGroup. YearBuilt, YearRemodAdd and GarageYrBlt have the expected impact on PriceGroup. There are no surprises here.\n\n<h4> But what about the significant number of building with YearRemodAdd in 1950 \u2754 <\/h4> \n<\/div>","e5fc21bf":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h4> Conclusion: \ud83d\udcdd<\/h4>\n<hr>\n<ul>  \n<li> As expected, the neighborhood appears to be significant. <\/li>\n<li> LotConfig, RoofStyle and MSZoning have a very high single value weight and the other properties contain little information. We could exclude them or convert them to binary features like HasHipRoof or RMZonig. <\/li>\n<li> GarageType looks interesting. We should convert this feature to a binary feature. Maybe AttachedGarage. <\/li>\n<li> The same goes for Lot Shape and HouseStyle. Here we could add something like RegularShape (Yes\/No) and HouseStyle -> (1Story, 2Story, other). <\/li>\n<li> I think we can exclude MSSubClass. We can see some strong price correlations, e.g. in group 30, but I think the size of the single group is too small and could easily lead to overfitting.<\/li> \n<\/ul> ","e2ef1c0f":"<a id=\"1.2\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n<center> <h3> 1.2 Continuous Columns<\/h3> <\/center>\n<\/div>","31887d0b":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n  \n<h4> What about the correlation to our PriceGroup feature \ud83e\udd14?<\/h4>\n<\/div>","e2572120":"<a id=\"5.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3>5.1 Parameter Tuning With Optuna <\/h3> <\/center>\n<\/div>","9cf24f17":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nFrom the distance between min, max, medina, and mean, we can tell that most of these features contain outliers. We will exclude them for the visualizations.\n<\/div>","ef446b5b":"<a id=\"4.2\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3> 4.2 New Pipeline \ud83c\udf87\ud83c\udfed\u2728<\/h3> <\/center>\n<\/div>","b4a5631c":"<a id=\"1.3.3\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n<center> <h3> 1.3.3 Room Features: \ud83d\udecf\ud83d\udebd\ud83c\udf73<\/h3> <\/center>\n<hr>\n<h4>Features Explanation: <\/h4>\n<ul>\n<li> BsmtFullBath: Basement full bathrooms <\/li>\n<li> BsmtHalfBath: Basement half bathrooms <\/li>\n<li> FullBath: Full bathrooms above grade <\/li>\n<li> HalfBath: Half baths above grade <\/li>\n<li> BedroomAbvGr: Bedrooms above grade (does NOT include basement bedrooms) <\/li>\n<li> KitchenAbvGr: Kitchens above grade <\/li> \n<li> TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) <\/li>\n<\/ul>","6f22f3da":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \nAs we can see, the distribution of our table is clearly left skewed. Most of the time, regression models will perform better if the distribution of the target is normally distributed. So let's use the quantile transformer for Sklearn to achieve this for our target.\n<\/div>   ","a1c093f0":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n\n<h4> Conclusion: \ud83d\udcdd<\/h4>\n<hr>\n<ul>  \n<li> We have 5 different date features and we could craft some more, but the included information is nearly the same. <\/li>\n  <ul>\n  <li> The resaon for that is, that YearRemodAdd and YearBuild are the some most of the time. <\/li>\n  <li> The same holds for YearBuilt and GarageYrBlt <\/li>\n  <\/ul>\n<li> We will decide later which ones to use. <\/li>\n<\/ul> \nLet us now turn to the characteristics describing the size of the different parts of the building.\n<\/div>","8fd6c2d9":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nStill a lot. Let's apply our knowledge from chapter 1.3. We'll start with the date functions. We have \"GarageYrBlt\", \"YearBuilt\" and \"YearRemodAdd\". Let's combine these three features into one and leave out the old ones.\n<\/div>","12841268":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nLooks good. So let's build a new pipeline and see what we got.\n<\/div>","1cabc1a7":"<a id=\"6.\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <h2> 6. Tensorflow<\/h2>\n<hr>\nAt this point I would like to try some NN with Tensorflow to see if we can improve or score. I have seen in other notebooks that the combination of Lasso Regression and XgBoostRegression can score much better, but since this has already been done, I will try to get a similar result with neural networks. Let's see what we can do ;).\n<\/div>","b62c66fc":"<a id=\"1.3.2\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n<center> <h3> \ud83d\udef5\ud83c\udfd8 1.3.2 Size Features: \ud83d\ude99\ud83c\udfe6 <\/h3> <\/center>\n    <hr>\n    <h4> Features Explanation: <\/h4>\n<ul>\n<li> General areas: <\/li>\n    <ul>\n    <li> WoodDeckSF: Wood deck area in square feet <\/li>\n    <li> OpenPorchSF: Open porch area in square feet <\/li>\n    <li> EnclosedPorch: Enclosed porch area in square feet <\/li>\n    <li> 3SsnPorch: Three season porch area in square feet <\/li>\n    <li> ScreenPorch: Screen porch area in square feet <\/li>\n    <li> LotArea:      Lot size in square feet <\/li>\n    <li> MasVnrArea:   Masonry veneer area in square feet <\/li>\n    <li> GarageArea:   Size of garage in square feet <\/li>\n    <li> GarageCars: Size of garage in car capacity <\/li>\n    <\/ul>\n<li> Basement areas: <\/li>\n    <ul>\n    <li> BsmtFinSF1:   Type 1 finished basement in square feet <\/li>\n    <li> BsmtFinSF2:   Type 2 finished basement in square feet <\/li>\n    <li> BsmtUnfSF:    Unfinished square feet of basement area <\/li>\n    <li> TotalBsmtSF:  Total square feet of basement area <\/li>\n    <\/ul>\n<li> Living areas: <\/li>\n    <ul>\n    <li> 1stFlrSF:     First Floor square feet <\/li>\n    <li> 2ndFlrSF:     Second floor square feet <\/li>\n    <li> LowQualFinSF: Low quality finished square feet (all floors) <\/li>\n    <li> GrLivArea:    Above grade (ground) living area square feet <\/li>\n    <\/ul>\n<\/ul>\nWe will remove \"WoodDeckSF\", \"OpenPorchSF\", \"LowQualFinSF\", \"PoolArea\", \"BsmtFinSF2\", \"EnclosedPorch\", \"3SsnPorch\" and \"ScreenPorch\" for now. (most values are 0)","c0e6021d":"<a id=\"1.3.5\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h4>Conclusion: \ud83d\udcdd<\/h4>\n<hr>\n    \n* This looks interesting. Especially Exterior1st, Exterior2nd, BsmtFinType1 and GarageFinish. \n* MasVnrType could be excluded for training.","82893954":"<a id=\"1.3.5\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nLet's see how they are distributed and what is their correlation with the selling price.\n<\/div>","9d3f05b0":"<a id=\"1.3.5\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n    \n<center> <h4> 1.3.5 Material Features: <\/h4> <\/center>\n<hr>\n    \n<h4> Features Explanation: <\/h4>\n<ul>\n<li>  RoofMatl: Roof material <\/li> \n<li>  Exterior1st: Exterior covering on house <\/li>\n<li>  Exterior2nd: Exterior covering on house (if more than one material) <\/li>\n<li>  MasVnrType: Masonry veneer type <\/li>\n<li>  Foundation: Type of foundation <\/li>\n<li>  PavedDrive: Paved driveway <\/li>\n<li>  GarageFinish: Interior finish of the garage <\/li>\n<\/ul>\n<\/div>","ec5ac322":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n<h4>Conclusion: \ud83d\udcdd<\/h4>\n<hr>\n<ul>\n<li> OverallQual seems to be the most important of these features. <\/li>\n<li> ExterQual, BsmtQual and KitchenQual look very similar. <\/li>\n<li> OverallCond looks counter intuitive. Maybe the way this feature is determined is not so standardized. <\/li>\n<ul>\n<\/div>","b381781e":"<a id=\"3.\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h2> 3. Simple Model <\/h2> <\/center>\n<hr>\nWe will start with a simple Xgboost algorithm without further pipelining and feature engineering. Just to have a score that we can improve.\n<\/div>","03c8ff26":"<a id=\"6.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <h3> 6.1 Tensorflow Submission<\/h3>\n<\/div>","d5b19d4a":"<a id=\"5.1.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <center> <h4> 5.1.1 Optuna Stundy Visualisation For XGBRegressor <\/h4> <\/center>\n<\/div>","6b6f92b1":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n   \nAs we can see, the stack_model is not really superior to the gradand boosting model. But let's make a submission with it anyway.\n<\/div>","7fa9d1cf":"<a id=\"3.1\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h3> 3.1 First Submission \ud83d\ude27<\/h3>\n<\/div>","4858ccba":"<a id=\"5.1.2\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <center> <h4> 5.1.2 Optuna Stundy Visualisation For GradientBoostingRegressor <\/h4> <\/center>\n<\/div>","9f9ab1ac":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nThese 26 cases are ignored and only the total information is used. MasVnrArea is also ignored because the distribution is not really promising. I'm not entirely sure if it makes sense to drop BsmtFinSF1 and BsmtUnfSF, but I think that the needed information is contained in TotalBsmtSF and dropping these two features could prevent some overfitting.\n<\/div>","63a01ed7":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n<h4> Conclusion: \ud83d\udcdd <\/h4>\n<hr>\n<ul>    \n<li> As with the date characteristics, we have a wide variety of expressions for these characteristics, but the information content of the individual expressions seems to be quite low. <\/li>\n<li> We will also create one or two characteristics from combinations of these in the further course and not use all of them. <\/li>\n<\/ul>\nLet's continue with quality and condition features.\n<\/div>","4c91642d":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nAs expectet we see the significant correlation between GarageYrBlt, YearBuit and YearRemodeAdd. The correlation between GarageYrBlt and YearRemodAdd is also not surprising. It is interesting to note that all three characteristics have more or less the same correlation with SalePrice. \nThe correlation between YearsSinceRemode and YearRemodAdd is also obvious, but including this feature could make the algorithm's job easier.\n<\/div>","d4249835":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nWe will exclude all features with more then 60% single value weight und more then 80% missing values.\n<\/div>","96d1e91b":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<h4> Conclusion: \ud83d\udcdd<\/h4>\n<hr>\n<ul>\n<li> Lot Frontage looks good. <\/li>\n<li> BsmtExposure and EnclosedPorch can be dropped. (Single value weight is too high) <\/li>\n<li> Fireplaces should be transformt in HasFireplace (True,False) <\/li>\n<\/ul>\n<\/div>","d96549a0":"<a id=\"1.3.4\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \n    \n<center> <h4> 1.3.4 Quality And Conditon Features: <\/h4> <\/center>\n<hr>\n<h4> Features Explanation: <\/h4>\n<ul>\n<li> OverallQual: Rates the overall material and finish of the house <\/li>\n<li> OverallCond: Rates the overall condition of the house <\/li> \n<li> ExterQual: Evaluates the quality of the material on the exterior <\/li>\n<li> ExterCond: Evaluates the present condition of the material on the exterior <\/li>\n<li> BsmtQual: Evaluates the height of the basement <\/li>\n<li> BsmtCond: Evaluates the general condition of the basement <\/li>\n<li> BsmtFinType1: Rating of basement finished area <\/li>\n<li> BsmtFinType2: Rating of basement finished area (if multiple types) <\/li>\n<li> KitchenQual: Kitchen quality <\/li>\n<li> FireplaceQu: Fireplace quality <\/li>\n<li> GarageQual: Garage quality <\/li>\n<li> GarageCond: Garage condition <\/li>\n<li> PoolQC: Pool quality <\/li>\n<li> Fence: Fence quality <\/li>\n<\/ul>\nLet's take a look at these features.\n<\/div>","6191582a":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    <h4> Score: 0.145 \ud83e\udd17<\/h4>\n\nOk, that's better than before. However, we have to keep in mind that performance can change with new submissions, as we can see in the boxplot above.\nLet's see what we can do next to improve even more. (To be continued...)\n    \n<\/div>","1e414076":"![house_price_pc_kaggle.PNG](attachment:7540f43c-cd17-4b64-b00a-704508bf1eed.PNG)","e5ba9971":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<h4> So what do we have \ud83e\udd14?<\/h4>\n<hr>  \n<ul>\n  <li>We have 61 categorical features (i.e. they have a maximum of 30 different values).  <\/li>\n  <li>We have 20 other (continues) features (with more than 30 different values). <\/li>\n  <li>Some of these columns could also be interpreted as categorical. For example, GarageYrBlt, YearRemodAdd, or YearBuilt.<\/li>\n  <li>A considerable number of categorical features have a single value weight of more than 80%, which means that more than 80% of all values are the same. <\/li>\n  <li>We need to be careful with these. We may exclude them completely. <\/li>\n  <li>Also, we have some features with many missing values. We will exclude those as well. <\/li>\n  <li>Some continuous features have a median of 0 and the distance between min-max and mean may indicate a screwed distribution for some features. <\/li>\n<\/ul>    \n<hr>  \nSo, all in all, we have a lot of different features that may or may not contain valuable information about the selling price. So let's divide these features into different groups for some visualizations to get a better understanding of the whole data structure.\nBut first let's look at the distribution of the label itself.\n<\/div>","cf4a2508":"<a id=\"1.3\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">   \n<center> <h3> 1.3 Visualizations: \ud83d\udcc9\ud83d\udcca\ud83d\udcc8<\/h3> <\/center>\n<\/div>","3cf934e7":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> <h2> 1. Data Screening \ud83d\udc40\ud83d\udd0e\ud83d\udcca<\/h2> <\/center>\n    \n<hr>\n    \nThere are a lot of features, so let's create a DataFrame that contains the needed information about each column for later use.\n<\/div>","4f38de3f":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\nLet's continue by defining an objective function for our optuna studies. This function is used to tune the hyperparameters for GradientBoostingRegressor and XGBRegressor.\n<\/div>","a789caa5":"<a id=\"5.1.3\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n    \n<center> <h4>\ud83d\udcc8\ud83d\udcc9 5.1.3 Model Performance Comparison \ud83d\udcc9\ud83d\udcc8<\/h4> <\/center>\n<hr>\n    \nAfter all this work, let's take a look at the performance of each of our models on our transformed dataset.\n<\/div>","0a43ffff":"<a id=\"2.2\"><\/a>\n<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">\n<center> <h3>  2.2 Cleaning Functions Test <\/h3> <\/center>","b0e93173":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\"> \nIt looks like very few buildings have been reworked. Of course, this makes the feature much less interesting. Too bad. It looks like very few buildings have been reworked. Of course, this makes the feature much less interesting. Too bad. Moreover, our new YearsSinceRemode feature is almost the same as the past time since construction. We will use one of the features later. Let's close this subsection by looking at the correlation between all these date features. We should see a strong correlation between YearBuilt, YearRemodAdd and GarageYrBlt.\n<\/div>   ","170dc9e5":"<div style=\"color:#505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F1FFC9;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \nThis looks very mouch normally distributed. We have to bear in mind that we will have to retransform our predictions later, if we transform our target befor training.\nFor further visualisations it will be useful to group these prices into manageable units of equal number. We will use the \"old\" SalePrices for our visual groups.\n<\/div>   "}}