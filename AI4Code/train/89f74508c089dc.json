{"cell_type":{"fcc98914":"code","66e4e179":"code","5bcd0f1e":"code","6079e867":"code","b1086abd":"code","93529cdd":"code","f51662af":"code","54120c65":"code","ba830cb7":"code","734a2745":"code","5eb21e0c":"code","9a96ab8c":"code","da5b262c":"code","f3d8a34c":"code","5a959f1e":"code","82d3b022":"code","37f8a053":"code","04032a59":"code","44cb4bdb":"code","54f79724":"code","a5342ae8":"code","30eef3ca":"code","0b4c4fb1":"code","4f7d2be5":"code","9e14909b":"code","7b3c7fda":"code","4d541de2":"code","ba2f14b2":"code","2e5111a9":"code","3af28c02":"code","81618344":"code","58811e33":"markdown","48c936bc":"markdown","fee4e515":"markdown","ce95654d":"markdown","c4153d0d":"markdown","79c5109b":"markdown","847efe3b":"markdown","000cfc42":"markdown","ec69d7e2":"markdown","482ba3bf":"markdown","7a4c3bb5":"markdown","f1a5e07e":"markdown"},"source":{"fcc98914":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom tensorflow.python.framework import ops\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport math\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","66e4e179":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","5bcd0f1e":"train_data.head()","6079e867":"test_data.head(5)","b1086abd":"X_train = (train_data.iloc[:,1:].values).astype('float32')\nY_train = train_data.iloc[:,0].values\nX_test = test_data.values.astype('float32')","93529cdd":"X_train = X_train.reshape(X_train.shape[0],28,28)\nfor i in range(6,9):\n    plt.subplot(330+(i+1))\n    plt.imshow(X_train[i+1], cmap='ocean_r')","f51662af":"X_train.shape","54120c65":"#shape of gray images = n*n*1\nX_train = X_train.reshape(X_train.shape[0],28,28,1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28,1)","ba830cb7":"#X_train -= np.sum(X_train,axis=1)\/X_train.shape[0]\nmean_train  = X_train.mean().astype(np.float32)\nvariance_train = X_train.std().astype(np.float32)\nX_train = (X_train-mean_train)\/variance_train\n\nmean_test = X_test.mean().astype(np.float32)\nvariance_test = X_test.std().astype(np.float32)\nX_test = (X_test-mean_test)\/variance_test\n\n","734a2745":"def one_hot_matrix(labels, C):\n\n    C = tf.constant(C, name='C')\n    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n    sess = tf.Session()\n    one_hot = sess.run(one_hot_matrix)\n    sess.close()\n    return one_hot","5eb21e0c":"X = X_train.reshape(X_train.shape[0],-1).T\nY = Y_train.reshape(X_train.shape[0],-1).T\nprint(\"Shape of training data = \", X.shape)\nprint(\"shape of labels \", Y.shape)\nprint(\"No of features = \", X.shape[0])\nprint(\"No of  training Examples(train + validation) = \", X.shape[1])\nX_test_flatten = X_test.reshape(X_test.shape[0],-1).T\nprint(\"Shape of testing data = \", X_test.shape)\nprint(\"NO of testing data \", X_test_flatten.shape[1])","9a96ab8c":"Y_train = Y.reshape((1,Y.shape[1]))","da5b262c":"Y = one_hot_matrix(Y_train,10)","f3d8a34c":"Y = Y.reshape(Y.shape[0],Y.shape[2])","5a959f1e":"X_train = X[:,0:37800]\nX_val = X[:,37800:]\nY_train = Y[:,0:37800]\nY_val = Y[:,37800:]","82d3b022":"Y_val.shape\n","37f8a053":"def create_placeholders(n_x, n_y):\n    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\") # middle argument  provide the shape of holder\n    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\") \n    return X, Y","04032a59":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n  \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","44cb4bdb":"def initialize_parameters():\n\n    regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n\n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 6 lines of code)\n    W1 = tf.get_variable(name=\"W1\", shape=[25, 784], initializer = tf.contrib.layers.xavier_initializer(seed=1),regularizer=regularizer)\n    b1 = tf.get_variable(\"b1\", [25, 1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1),regularizer=regularizer)\n    b2 = tf.get_variable(\"b2\", [12, 1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [10, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1),regularizer=regularizer)\n    b3 = tf.get_variable(\"b3\", [10, 1], initializer = tf.zeros_initializer())\n    ### END CODE HERE ###\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters","54f79724":"def forward_propagation(X, parameters):\n\n    \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n          # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n\n    \n    return Z3","a5342ae8":"def compute_cost(Z3, Y):\n    regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n\n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n\n    cost_1 = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n    cost = tf.reduce_mean(cost_1+reg_term)\n\n    \n    return cost","30eef3ca":"def model(X_train, Y_train, X_val, Y_val, learning_rate = 0.001,\n          num_epochs = 300, minibatch_size = 64, print_cost = True):\n   \n\n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n  \n    X, Y = create_placeholders(n_x, n_y)\n    # Initialize parameters\n   \n\n    parameters = initialize_parameters()\n\n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    \n    Z3 = forward_propagation(X, parameters)\n   \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    cost = compute_cost(Z3, Y)\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n       \n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0. # Defines a cost related to an epoch\n            num_minibatches = int(m \/ minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                epoch_cost += minibatch_cost \/ num_minibatches # total epoch cost for all minibatches combined\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) #Returns the index with the largest value across axes of a tensor.\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=\"float\"))\n\n        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print(\"Test Accuracy:\", accuracy.eval({X: X_val, Y: Y_val}))\n        \n        return parameters\n","0b4c4fb1":"def forward_propagation_for_predict(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3","4f7d2be5":"def predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3}\n    \n    x = tf.placeholder(\"float\", [784, 28000])\n    \n    z3 = forward_propagation_for_predict(x, params)\n    p = tf.argmax(z3)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction\n\ndef forward_propagation_for_predict(X, parameters):\n\n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3\n    ","9e14909b":"Y_train.shape","7b3c7fda":"parameters = model(X_train, Y_train, X_val, Y_val)","4d541de2":"X_test.shape","ba2f14b2":"result_predict = predict(X_test_flatten,parameters)","2e5111a9":"Check_result = X_test.reshape(X_test.shape[0],28,28)","3af28c02":"plt.imshow(Check_result[0])\nprint(\"Actual number\",result_predict[0]) ","81618344":"import pandas as pd \ndf = pd.DataFrame(result_predict)\ndf.to_csv(\"Kaggle_test_results.csv\")","58811e33":"# **One Hot Encoding **","48c936bc":"# **Splitting data into training and validation**","fee4e515":"# **Backward propagation & parameter updates**","ce95654d":"# **Submission result - result_predict**","c4153d0d":"# **Checking result with actual data**","79c5109b":"# **Submitting Results**","847efe3b":"# **Forward Propogation**","000cfc42":"# **Compute Cost**","ec69d7e2":"# **Flattening images**","482ba3bf":"# **Normalizing the data **","7a4c3bb5":"# **Initialize parameters**","f1a5e07e":"# **Create placeholders**"}}