{"cell_type":{"54c37644":"code","1e8ff90c":"code","9771eea3":"code","d1a9c7ee":"code","d54d4e58":"code","ab5f18a7":"code","e767179d":"code","58a0e240":"code","b2088bc8":"code","db2b28ee":"code","b1458f91":"code","d0c43738":"code","0843e97a":"code","99d55908":"code","06658886":"code","142056e5":"code","269b7f14":"code","5aadefb0":"code","da37eebf":"code","d1d95ab1":"code","f38f70fd":"code","887cb107":"code","9dc9f9b0":"code","abdd7fe1":"code","94d9e6e1":"code","c97e041d":"code","cdb9632b":"code","4fbd3c1f":"code","94bdae12":"code","4cbeeb26":"code","3a32412e":"code","0e5ad51e":"code","00000851":"code","0cd4e2af":"code","3fb5037f":"code","2609a758":"code","d5217c94":"code","b33fc7a6":"code","6322aa8d":"code","07e505ae":"code","b435eff6":"code","bf4f0482":"code","f4818c7d":"code","c71a9d5d":"code","5a0737db":"code","414f2824":"code","60b61d5c":"code","f7e312d5":"code","ef54f1d1":"code","a6fd86c9":"code","e2b40045":"code","3eb9e030":"code","40539c7a":"code","84c08293":"code","5b2e896f":"code","14e1c33b":"code","904241ad":"code","84e3a5c8":"code","6741a48c":"code","aac2ac55":"code","4711b1da":"markdown","9736bf87":"markdown","b1de0025":"markdown","6f74bc4c":"markdown","11eb5592":"markdown","b7096051":"markdown","f56101b6":"markdown","c19392d3":"markdown","4896ec1c":"markdown","3fa65f91":"markdown","31ef16f2":"markdown","9f3ea39e":"markdown","b3a42beb":"markdown","efd779c5":"markdown","6a9793cf":"markdown","5cb0196f":"markdown","67c21703":"markdown","fe2056ce":"markdown","0c94450d":"markdown","c6db420e":"markdown","f61ac1e5":"markdown","cec4a05b":"markdown","427959b2":"markdown","3aa540c5":"markdown","0325e975":"markdown","6c4bcb30":"markdown","c11f5e7b":"markdown","058ebae3":"markdown","e4b0aeea":"markdown","3958e4e6":"markdown","921a551a":"markdown","8f46b0fe":"markdown","3abfab0f":"markdown","8c86e441":"markdown","d701c4aa":"markdown","12476d86":"markdown","bcb02c14":"markdown","8208e75a":"markdown","5c744020":"markdown","9d2a663d":"markdown"},"source":{"54c37644":"import nltk\nimport pandas as pd \nimport  numpy as np\nfrom collections import Counter\nfrom ds_exam import *\nfrom update_time import *\nfrom bag_words import *","1e8ff90c":"france = pd.read_csv(\"\/kaggle\/input\/coronavirus-france-dataset\/patient.csv\")\ntunisia = pd.read_csv(\"..\/input\/coronavirus-tunisia\/Coronavirus_Tunisia.csv\")\njapan = pd.read_csv(\"\/kaggle\/input\/close-contact-status-of-corona-in-japan\/COVID-19_Japan_Mar_07th_2020.csv\")\nindonesia = pd.read_csv(\"\/kaggle\/input\/indonesia-coronavirus-cases\/patient.csv\")\nkorea = pd.read_csv(\"\/kaggle\/input\/coronavirusdataset\/PatientInfo.csv\")\nHubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/Hubei.csv\")\noutside_Hubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/outside_Hubei.csv\")\n\n","9771eea3":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name = [\"france\", \"tunisia\", \"japan\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\ngarbge = [print(\"\\n\"+datasets_name[i], [i for i in datasets[i].columns]) for i in range(len(datasets_name))]","d1a9c7ee":"o = []\nfor i in range(len(datasets)):\n    print(datasets_name[i],datasets[i].shape)\n    o.append(datasets[i].shape[0])\nprint(\"\\nnum of i \" + str(sum(o)))","d54d4e58":"france.rename(columns={\"health\":\"severity_illness\",\"status\":\"treatment\",\"infection_reason\":\"infection_place\"}\n              , inplace = True)\n\ntunisia.rename(columns={\"date\":\"confirmed_date\", \"gender\":\"sex\", \"situation\":\"severity_illness\", \n                        \"return_from\":\"infection_place\", \"health\":\"background_diseases\", \"hospital_name\":\"treatment\"}, inplace = True)\n\njapan.rename(columns={\"No.\":\"id\", \"Fixed date\":\"confirmed_date\",\"Age\":\"age\", \"residence\":\"region\",\n                      \"Surrounding patients *\":\"infected_by\"}, inplace = True)\n\nindonesia.rename(columns={\"patient_id\":\"id\",\"gender\": \"sex\", \"province\":\"region\", \"hospital\":\"treatment\",\n                          \"contacted_with\":\"infected_by\", \"current_state\":\"severity_illness\"}, inplace = True)\n\nkorea.rename(columns={\"patient_id\":\"id\", \"disease\":\"background_diseases_binary\", \"state\":\"severity_illness\",\n                      \"province\":\"region\", \"infection_case\" :\"infection_place\",\n                      \"symptom_onset_date\":\"date_onset_symptoms\"}, inplace = True)\n\nHubei = Hubei.rename(columns={ \"province\":\"region\",\"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\",\n                               \"travel_history_location\":\"infection_place\"})\n\noutside_Hubei = outside_Hubei.rename(columns={ \"province\":\"region\", \"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\", \n                              \"travel_history_location\":\"infection_place\",'travel_history_dates': \"return_date\", \n                              \"travel_history_location\":\"infection_place\" })\n","ab5f18a7":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)","e767179d":"def full_common(exam_df):\n    \"\"\"\n    Returns columns that all DATASETS have\n    \"\"\"\n    full_common = []\n    for j in exam_df.columns:\n        boolyan = exam_df[j].all()\n        if boolyan == True:\n            full_common.append(j)\n    return full_common","58a0e240":"common = []\nunique = []\nblank = []\nfor i in exam_df.columns:\n    if exam_df[i].value_counts()[True]>1:\n        common.append(i)\n    elif exam_df[i].value_counts()[True]==1:\n        unique.append(i)\n    else:\n        blank.append(i)\n        \n        \nprint(common)\nprint(unique)\nprint(blank)   ","b2088bc8":"for x in [outside_Hubei, Hubei]:\n    l = x.index[x.country_new.notnull() == True]\n    p = []\n    for i in l:\n        if x.country_new[i] == x.country[i]:\n            p.append(i)\n\n    print(\"country_new == country\",len(p))\n    print(\"country_new.notnull\",len(l),\"\\n\")","db2b28ee":"for x in [outside_Hubei, Hubei]:\n    m =[]\n    for i in range(len(x)):\n        if x.ID[i] != str(i+1):\n            m.append(i)\n    print(m[0],x.ID[m[0]])","b1458f91":"france = france.drop(['id', \"departement\",\"source\",\"comments\",\"contact_number\"],axis=1)\n\n\nindonesia = indonesia.drop([\"id\", 'nationality'],axis=1)\njapan = japan.drop([\"id\"],axis=1)\n\nkorea = korea.drop([\"id\", \"age\",\"contact_number\"],axis=1)\n\nHubei = Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                    'geo_resolution','admin_id', \"country_new\",\"source\",\"additional_information\",\"geo_resolution\"\n                    ,\"notes_for_discussion\"],axis=1)\n\noutside_Hubei = outside_Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                                    'geo_resolution', 'admin_id', \"country_new\", \"data_moderator_initials\",\n                                    \"source\",\"additional_information\",\"geo_resolution\",\n                                    \"notes_for_discussion\"],axis=1)","d0c43738":"def examining_values_by_col (datasets, datasets_name, col):\n    \"\"\"\n    Prints values of each DF per column\n    \"\"\"\n    counter = 0\n    \n    for i in datasets:\n        if col in i.columns:\n            print(\"\\n\" + datasets_name[counter])\n            print(i[col].value_counts())\n        counter =counter + 1","0843e97a":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)\n\nfor j in exam_df.columns[1:len(exam_df.columns)]:\n    print(j)\n    examining_values_by_col (datasets , datasets_name , j) ","99d55908":"l1= tunisia.index[tunisia[\"return_date\"] == \"Local\"]\nl2 = tunisia.index[ tunisia[\"return_date\"].notnull()]\n\nindex = l2.drop(l1)\n\nfor indx in index:\n    tunisia.loc[indx,\"return_date\"] = pd.to_datetime(tunisia.loc[indx,\"return_date\"])","06658886":"### CPU\ncols = [\"confirmed_date\",\"released_date\", \"deceased_date\"]\n\nfrance[cols] = france[cols].apply(pd.to_datetime)\nindonesia[cols] = france[cols].apply(pd.to_datetime)\njapan[cols] = france[cols].apply(pd.to_datetime)\nkorea[cols] = korea[cols].apply(pd.to_datetime)\n\n#### different#####\n\n# korea\nkorea_col = [\"date_onset_symptoms\"]\nkorea[korea_col] = korea[korea_col].apply(pd.to_datetime)\n\n#  tunisia\ntunisia_col = [\"confirmed_date\"]\ntunisia[tunisia_col] = tunisia[tunisia_col].apply(pd.to_datetime)\n\n# Hubei\nHubei_col = [\"confirmed_date\", \"date_death_or_discharge\", \"date_onset_symptoms\"]\nHubei[Hubei_col] = Hubei[Hubei_col].apply(pd.to_datetime)\n\n# outside_Hubei\noutside_Hubei_col = [\"date_death_or_discharge\"]\noutside_Hubei[outside_Hubei_col] = outside_Hubei[outside_Hubei_col].apply(pd.to_datetime)\n\n# 'travel_history_dates'\nfor j in [\"confirmed_date\", \"date_onset_symptoms\"]:\n    indexs = outside_Hubei.index[outside_Hubei[j].notnull()]\n    indexs_ , error = UpdateTime.updte_time(outside_Hubei, j, j, indexs,\".\",[ \"-\", ','])\n    print(j , error)","142056e5":"indexs =  outside_Hubei.index[outside_Hubei[\"return_date\"].notnull()]\n\nfor indx in indexs:\n    i = outside_Hubei.loc[indx, \"return_date\"]\n    i = i.split(\"-\")\n\n    for x in range(len(i)):\n        i[x] = pd.to_datetime(i[x], errors='ignore')\n\n\n        if len(i) == 1:\n            pass\n            outside_Hubei.loc[indx , \"return_date\"] = i[0]\n\n        elif len(i)>1 and type(i[0]) == type(i[1]):\n            outside_Hubei.loc[indx , \"return_date\"] =  pd.DataFrame({\"t\":i}).max()[0]\n        \n        \noutside_Hubei[\"return_date\"].value_counts()","269b7f14":"# error\noutside_Hubei.index[outside_Hubei[\"date_onset_symptoms\"] == \"- 25.02.2020\"]","5aadefb0":"examining_values_by_col (datasets , datasets_name , \"date_onset_symptoms\") ","da37eebf":"tunisia_sex = {\"F\":\"female\", \"M\":\"male\",np.nan:np.nan}\ntunisia.sex = [tunisia_sex[item] for item in  tunisia.sex] \n\njapan_sex = {\"Woman\":\"female\", \"Man\":\"male\",np.nan:np.nan, \"Checking\":np.nan, \"investigating\":np.nan}\njapan.sex = [japan_sex[item] for item in  japan.sex] \n\nfrance_sex = {\"female\":\"female\", \"male\":\"male\",\"Female\":\"female\", \"Male\":\"male\", \"male\\xa0?\":\"male\", \n              np.nan:np.nan }\nfrance.sex = [france_sex[item] for item in  france.sex] \n","d1d95ab1":"examining_values_by_col(datasets, datasets_name, \"sex\")","f38f70fd":"def update_index(dataset, col, indexs, data):\n    \"\"\"\n    Value change according index\n    \n    dataset: df\n    \n    col : str\n        name of col you want to change\n        \n    indexs: pd.index\n    \n    data: int\/ str\/ float\n        data you want to into\n    \n    \"\"\"\n    for indx in indexs:\n        dataset.loc[indx,col] = data","887cb107":"indexs = korea.index[korea.background_diseases_binary == True]\nupdate_index(korea, \"background_diseases_binary\", indexs, 1.0)","9dc9f9b0":"tunisia[\"background_diseases_binary\"] = np.nan\n\nfor dataset in [tunisia, Hubei, outside_Hubei]:\n    indexs = dataset.index[dataset.background_diseases.notnull()]\n    update_index(dataset,\"background_diseases_binary\",indexs,1.0) ","abdd7fe1":"examining_values_by_col (datasets, datasets_name, \"background_diseases_binary\")","94d9e6e1":"def getKeysByValue(dictOfElements, valueToFind):\n    listOfKeys = list()\n    listOfItems = dictOfElements.items()\n    for item  in listOfItems:\n        if item[1] == valueToFind:\n            listOfKeys.append(item[0])\n    return  listOfKeys\n\ndef remove(dict_a, keys_remove ):\n    for key in keys_remove:\n        if key in dict_a.keys():\n            dict_a.pop(key)","c97e041d":"ps = nltk.stem.SnowballStemmer('english')\nr = []\no = []\nfor ind in outside_Hubei.index[outside_Hubei.background_diseases.notnull()]:\n    i = outside_Hubei.loc[ind, \"background_diseases\"]\n\n    i = BagWords.clean_str(i)\n    l = [ps.stem(x) for x in i]\n\n    for x in l:\n        if x.isalpha():\n            r.append(x)\n            \nkeys_remove = ['to', 'a','like',  'no', 'and',  'yes', 'then','complaint',\"great\", \"even\", \n         \"for\", \"the\", \"non\",  'of' , \"this\",  'on' ,'with', \"was\", 'c',\n         \"cannot\", \"recommend\", \"as\", \"a\", \"i\", \"did\", \"not\", \"want\", \"to\", \"have\", \"to\", \"do\", \"this\"]\n\n\n            \ntest_dict = dict(Counter(r))\nremove(test_dict, keys_remove )\nprint(test_dict)\n","cdb9632b":"bag_words= {\"good\":[\"good\",\"stabl\", \"follow\"],\n            \"critical\":[\"critic\", \"intens\", \"sever\"], \n            \"deceased\": [\"death\",\"dead\", \"die\", \"deceas\" ],\n            \"cured\":[\"discharg\", \"releas\", \"cure\", \"recov\", 'health'],\n            np.nan: [\"isol\"]}\n\nsentences_bag = {\"good\":[['not', 'hospit'],['in', 'progress']],\n                \"critical\":[], \n                \"deceased\": [ ],\n                \"cured\":[]}\n\n\n\ndatasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\nfor ind in range(len(datasets_name1)):\n    dataset = datasets2[ind]\n    indexs = dataset.index[dataset.severity_illness.notnull()]\n    no_guess,multi_guess = BagWords.guess_category(dataset, \"severity_illness\", \"severity_illness\",indexs, ps, bag_words, sentences_bag)\n    \n    print(datasets_name1[ind])\n    print(no_guess)\n    print(multi_guess)\n\nfor x in [indonesia, france]:  \n    indexs = x.index[x.deceased_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"deceased\") \n\n    indexs = x.index[x.released_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"cured\") ","4fbd3c1f":"datasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\nexamining_values_by_col(datasets2, datasets_name1,  \"severity_illness\")","94bdae12":"categories = [\"deceased\", \"cured\"]\ncols = [\"deceased_date\",\"released_date\"]\nfor indx_j in range(len(cols)) :\n    j  = cols[indx_j]\n    category = categories[indx_j]\n    \n    for x in [outside_Hubei, Hubei]:\n        x[j] = np.nan \n        indexs = x.index[x[\"severity_illness\"] == category]\n\n        for i in indexs:\n            x.loc[i, j]= pd.to_datetime(x.loc[ i, \"date_death_or_discharge\"])\n","4cbeeb26":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"released_date\")","3a32412e":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"deceased_date\")","0e5ad51e":"for x in [outside_Hubei, Hubei]:\n    l = x.date_death_or_discharge.notnull().sum()\n    y = x.severity_illness.notnull().sum()\n    p = x.released_date.notnull().sum() +x.deceased_date.notnull().sum()\n    print(l,y,p)","00000851":"for x in [outside_Hubei]:\n    complete_features =list(x.index[x.released_date.notnull()]) + list(x.index[x.deceased_date.notnull()])\n    date_death_or_discharge = list(x.index[x.date_death_or_discharge.notnull()])\n    severity_illness = list(x.index[x.severity_illness.notnull()])\n\n    if complete_features == date_death_or_discharge:\n        print(\"==\")\n    else:\n        print(\"not ==\")\n    \n    for i in severity_illness:\n        if i not in complete_features:\n            print(i)","0cd4e2af":"indexs =  outside_Hubei.index[outside_Hubei.age.notnull()]\n\ndef int_num(dataset, col, indexs):\n    to_float = []\n\n    for i in indexs:\n        if dataset.loc[i, col].isdigit() == True:\n            to_float.append(i)\n\n    for indx in  to_float:\n        dataset.loc[indx, col] = float(dataset.loc[indx, col])\n    \n    return to_float\n        \n\nto_float= int_num(outside_Hubei, \"age\",indexs)\nindexs = indexs.drop(to_float)\n\nprint(indexs)","3fb5037f":"### NEED AGE CPU \n\ndef birth_year_to_age(data):\n    age_ls = []\n\n    for i in range(len(data)):\n        age_ls.append(data.confirmed_date[i].year - data.birth_year[i])\n    return age_ls\n\nkorea[\"age\"] = birth_year_to_age(korea)\nfrance[\"age\"] = birth_year_to_age(france)","2609a758":"tunisia[\"country\"] = [\"tunisia\" for i in range(len(tunisia))]\njapan[\"country\"] = [\"japan\" for i in range(len(japan))]\nindonesia[\"country\"] = [\"indonesia\" for i in range(len(indonesia))]","d5217c94":"for dataset in [outside_Hubei,Hubei]:\n\n    indexs = dataset.index[dataset[\"country\"].notnull()]\n    for indx in indexs:\n\n        dataset.loc[indx, \"country\"] = dataset.loc[indx, \"country\"].lower()","b33fc7a6":"outside_Hubei[\"country\"].value_counts()","6322aa8d":"print(len(korea))\nprint(outside_Hubei.country.value_counts()[\"south korea\"])\nprint()\n\nprint(len(france))\nprint(outside_Hubei.country.value_counts()[\"france\"])\nprint()\n\nprint(len(Hubei))\nprint(outside_Hubei.country.value_counts()[\"china\"])","07e505ae":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"infection_place\")","b435eff6":"print(Hubei[\"wuhan(0)_not_wuhan(1)\"].value_counts())\nprint(outside_Hubei[\"infection_place\"].value_counts())","bf4f0482":"index_chack =  outside_Hubei.index[outside_Hubei[\"country\"] == \"France\" ]","f4818c7d":"l = ['sex',  'city', 'confirmed_date',  'age']\ndf1 = france.loc[:, l]\n \ndf2 = outside_Hubei.loc[index_chack, l]\n\nx = pd.concat([df2,df1])\n\nprint(x.shape)\n\ndf_diff = x.index[x.duplicated(keep=\"last\") == True]\n\nlen(df_diff)\n","c71a9d5d":"p = pd.DataFrame({\"r\": [0,9,0,88,7,6,0,0,0],\n                 \"o\": [0,9,0,88,7,6,6,7,4]})\n\nn = pd.DataFrame({\"r\": [0,9,4,886,77,6,607,0,0],\n                 \"o\": [0,9,0,88,7,65,6,7,4]})\n\nt = pd.concat([n,p])\nk = t.index[t.duplicated(keep=\"last\") == False]\nk","5a0737db":"index = tunisia.index[tunisia[\"hospital_place\"].notnull()]\n\ny = tunisia.index[tunisia[\"treatment\"] == \"Self-insulation\"]\nindex = index.drop(y)\nprint(index)\n\nupdate_index(tunisia, \"treatment\", y,\"home isolation\")\nupdate_index(tunisia, \"treatment\", index ,\"hospital\")\n\n","414f2824":"index = indonesia.index[indonesia[\"treatment\"].notnull()]\nupdate_index(tunisia, \"treatment\", index ,\"hospital\")\n\nindonesia[\"treatment\"].value_counts()","60b61d5c":"y = france.index[france[\"treatment\"] == \"deceased\"] \nx = france.index[france[\"treatment\"] == \"released\"]\n\nupdate_index(france, \"treatment\", y ,np.nan)\nupdate_index(france, \"treatment\", x ,np.nan)","f7e312d5":"outside_Hubei[\"treatment\"] = np.nan\nindex = outside_Hubei.index[outside_Hubei['date_admission_hospital'].notnull()]\nupdate_index(outside_Hubei, \"treatment\",  index,\"hospital\")\n\nHubei[\"treatment\"] = np.nan\nindex = Hubei.index[Hubei['date_admission_hospital'].notnull()]\nupdate_index(Hubei, \"treatment\", index ,\"hospital\")\n","ef54f1d1":"for i in ['china',\"japan\",\"france\",\"south korea\"]:\n    ind = outside_Hubei.index[outside_Hubei[\"country\"] == i]\n    outside_Hubei = outside_Hubei.drop(ind, axis=0)\n\noutside_Hubei[\"country\"].value_counts()","a6fd86c9":"france = france.drop([\"birth_year\", \"group\"],axis=1)\ntunisia = tunisia.drop([\"hospital_place\"],axis=1)\njapan = japan.drop([\"Close contact situation\"],axis=1)\nkorea = korea.drop([\"birth_year\",\"global_num\"],axis=1)\nHubei = Hubei.drop([\"date_death_or_discharge\"],axis=1)","e2b40045":"Hubei.columns","3eb9e030":"Hubei = Hubei.drop(['date_admission_hospital',\"wuhan(0)_not_wuhan(1)\",'travel_history_dates',\n                    'lives_in_Wuhan', \"reported_market_exposure\",\"sequence_available\"],axis=1)\noutside_Hubei = outside_Hubei.drop( ['wuhan(0)_not_wuhan(1)', 'date_admission_hospital', \"date_death_or_discharge\", 'lives_in_Wuhan',\n 'reported_market_exposure','sequence_available'],axis=1)\n\n","40539c7a":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets2)\nexam_df = Exam.df_exam_columns_dfs(datasets2, datasets_name, columns_name)\nprint(columns_name)\nexam_df.infection_place","84c08293":"datasets3 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets3)\nexam_df2 = Exam.df_exam_columns_dfs(datasets3,datasets_name,columns_name)","5b2e896f":"for i in exam_df2.columns:\n    print(\"\\n\"+i)\n    examining_values_by_col (datasets, datasets_name, i)","14e1c33b":"exam_df.sex","904241ad":"datasets_final = [france, tunisia, japan, indonesia, korea,outside_Hubei, Hubei ]\nfinal_DS = pd.concat(datasets_final, axis=0)","84e3a5c8":"final_DS.index = range(len(final_DS))","6741a48c":"final_DS.to_csv(r'\/kaggle\/working\/Characteristics_Corona_patients2.csv', index = False)","aac2ac55":"final_DS.to_csv()","4711b1da":"unipue","9736bf87":"# orgnaze DS","b1de0025":"del country","6f74bc4c":"index","11eb5592":"# Examining values - v1 ","b7096051":"#  Garbage drop \n- Features that have only one dataset or  built with Engineered another feature with them","f56101b6":"#                                                     Complete features","c19392d3":"background_diseases","4896ec1c":"# datasets.shape","3fa65f91":"As you can see the ID is not arranged in any numerical order.\nand because there is no column that needs another row identifier\nI drop ID","31ef16f2":"**> Feature sum**","9f3ea39e":"test","b3a42beb":"infection_case\n\n= Community \\abroad \\ Nan","efd779c5":"**infection_place","6a9793cf":"country","5cb0196f":"sex","67c21703":">datetime","fe2056ce":"# drop Non-baked features","0c94450d":"# exam_df = columns vs dfs","c6db420e":"country_new","f61ac1e5":"ID","cec4a05b":"# drop","427959b2":"duplicate","3aa540c5":"released_date \/ deceased_date","0325e975":"# change name of col","6c4bcb30":"treatment","c11f5e7b":"severity_illness","058ebae3":"deceased_date exam","e4b0aeea":"# build final DS","3958e4e6":"exam ","921a551a":"Tests for columns' usefulness before drop","8f46b0fe":"# drop feature","3abfab0f":"age","8c86e441":"common feature","d701c4aa":"**The purpose of this notebook is to create a DATASET that includes**\n\n** Characteristics of patients like - **\n\n*age\n\n*sex\n\n*Country\n\nand so\n\n\n\n\n**The condition of the patients and their characteristics - **\n\n* Disease time (from diagnosis date)\n\n* Have been cured\n \n* Deaths\n\n\nThe database is designed to allow easy exploration of the data\n\nAnyone interested can use and donate","12476d86":"The column has no new information to give","bcb02c14":"# format col","8208e75a":"outside_Hubei data VS country data","5c744020":"background_diseases_binary","9d2a663d":"released_date exam"}}