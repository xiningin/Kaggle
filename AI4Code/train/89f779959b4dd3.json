{"cell_type":{"b766149d":"code","bc89893c":"code","e21c3f51":"code","b0a7576c":"code","9eadcf0c":"code","568c840b":"code","4fd0aa51":"code","079727d2":"code","60710dba":"code","b27a1b00":"code","45ad3e26":"code","f40a72a4":"code","a121d849":"code","851cbb15":"code","62948df2":"code","c54836f3":"code","860ccfa3":"code","b6664d92":"code","34614ca4":"code","2db752fc":"code","cb53bf5d":"code","28bceb46":"code","6675f1b0":"code","a781fa04":"code","150b49c3":"code","7df801d9":"code","e99f9eef":"code","fd76eee3":"code","4d5716ec":"code","b4e87db1":"code","888fa3bd":"code","984746cb":"code","bdb706a4":"code","e2830ad6":"code","187b633f":"code","0581c189":"code","fd409063":"code","f8a50f0d":"code","61a75471":"markdown","d09a19ed":"markdown","8ace76f5":"markdown","8b6379b1":"markdown","96ba3122":"markdown","c3720d07":"markdown","64d4a132":"markdown","1b0f74e4":"markdown","ebe56966":"markdown","097b18b1":"markdown","66d49429":"markdown","fe0b0ecf":"markdown","d3f08698":"markdown","fb87f99f":"markdown","6718381d":"markdown","dd47a92d":"markdown"},"source":{"b766149d":"pip install pyicu","bc89893c":"pip install pycld2","e21c3f51":"import numpy as np \nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk import *\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\n\nimport sys\nfrom termcolor import colored\nfrom polyglot.detect import Detector\nfrom polyglot.utils import pretty_list","b0a7576c":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","9eadcf0c":"train1.head()","568c840b":"train2.head()","4fd0aa51":"train2.toxic = train2.toxic.round().astype(int)\ntrain = pd.concat([train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000)\n    ])\n#rate=10\n#train = train[::rate]\ntrain.head()","079727d2":"train.info()","60710dba":"train.describe()","b27a1b00":"print(f\"Train data shape: {colored(train.shape, 'red', attrs=['bold'])}\")","45ad3e26":"train.isnull().sum()","f40a72a4":"def slashn(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \"\")\n    else:\n        return \"\"","a121d849":"nltk.download('punkt') #tokenizer\nnltk.download('stopwords') #handle stopwords\nnltk.download('wordnet') #Lemmatization\n\nstop_words = stopwords.words('english')","851cbb15":"def word_count_review(docs):\n    text = ' '.join([slashn(abstract) for abstract in docs])\n    corpus = str(text.lower())\n    txt = re.sub(r'[^a-z0-9]+',' ',str(corpus)).strip()\n    tokens = word_tokenize(txt)\n    words = [t for t in tokens if t not in stop_words]\n    lemma = WordNetLemmatizer()\n    l = [lemma.lemmatize(w) for w in words]\n    fdq = FreqDist(l)\n    return fdq","62948df2":"fd = word_count_review(train.comment_text)\nfd","c54836f3":"plt.figure(figsize=(10,10),dpi=150)\nwc = WordCloud(scale=10).generate_from_frequencies(fd)\n\nplt.imshow(wc)\nplt.axis('off')","860ccfa3":"print(pretty_list(Detector.supported_languages()))","b6664d92":"def get_language(text):\n    return Detector(\"\".join(x for x in text if x.isprintable()),quiet=True).languages[0].name\nh = get_language(\"Hell\u00f3, hogy vagy\")\ni = get_language(\"Dia duit, conas at\u00e1 t\u00fa\")\ne = get_language(\"hello, how are you\")\np = get_language(\"\u0a39\u0a48\u0a32\u0a4b \u0a24\u0a41\u0a38\u0a40 \u0a15\u0a3f\u0a35\u0a47\u0a02 \u0a39\u0a4b\")\nt = get_language(\"\u0441\u04d9\u043b\u0430\u043c, \u0445\u04d9\u043b\u043b\u04d9\u0440\u0435\u04a3 \u043d\u0438\u0447\u0435\u043a\")\nk = get_language(\"\uc548\ub155\ud558\uc138\uc694. \uc5b4\ub5bb\uac8c \uc9c0\ub0b4\uc138\uc694\")\nm = get_language(\"\u0d39\u0d32\u0d47\u0d3e, \u0d28\u0d3f\u0d19\u0d4d\u0d19\u0d7e\u0d15\u0d4d\u0d15\u0d4d \u0d38\u0d41\u0d16\u0d2e\u0d3e\u0d23\u0d47\u0d3e\")","34614ca4":"print(f\"Hell\u00f3, hogy vagy: {colored(h, 'blue', attrs=['bold','underline'])}\")\nprint(f\"Dia duit, conas at\u00e1 t\u00fa: {colored(i, 'red', attrs=['bold','underline'])}\")\nprint(f\"hello, how are you: {colored(e, 'yellow', attrs=['bold','underline'])}\")\nprint(f\"\u0a39\u0a48\u0a32\u0a4b \u0a24\u0a41\u0a38\u0a40 \u0a15\u0a3f\u0a35\u0a47\u0a02 \u0a39\u0a4b: {colored(p, 'cyan', attrs=['bold','underline'])}\")\nprint(f\"\u0441\u04d9\u043b\u0430\u043c, \u0445\u04d9\u043b\u043b\u04d9\u0440\u0435\u04a3 \u043d\u0438\u0447\u0435\u043a: {colored(t, 'white', attrs=['bold','underline'])}\")\nprint(f\"\uc548\ub155\ud558\uc138\uc694. \uc5b4\ub5bb\uac8c \uc9c0\ub0b4\uc138\uc694: {colored(k, 'magenta', attrs=['bold','underline'])}\")\nprint(f\"\u0d39\u0d32\u0d47\u0d3e, \u0d28\u0d3f\u0d19\u0d4d\u0d19\u0d7e\u0d15\u0d4d\u0d15\u0d4d \u0d38\u0d41\u0d16\u0d2e\u0d3e\u0d23: {colored(m, 'green', attrs=['bold','underline'])}\")","2db752fc":"train['language'] = train[\"comment_text\"].apply(get_language)","cb53bf5d":"train.head()","28bceb46":"train.language.unique()","6675f1b0":"print(\"Number of Unique Languages:\",train.language.nunique())","a781fa04":"import plotly.express as px\n\nlang_list = sorted(list(set(train[\"language\"])))\ncounts = [list(train[\"language\"]).count(cont) for cont in lang_list]\ndf = pd.DataFrame(np.transpose([lang_list, counts]))\ndf.columns = [\"Language\", \"Count\"]\ndf[\"Count\"] = df[\"Count\"].apply(int)\n\ndf_en = pd.DataFrame(np.transpose([[\"English\", \"Non-English\"], [max(counts), sum(counts) - max(counts)]]))\ndf_en.columns = [\"Language\", \"Count\"]\ndf_en.head()","150b49c3":"df_en.Count = df_en.Count.astype(int)","7df801d9":"df_en.plot.bar(x=\"Language\", y=\"Count\", rot=0)","e99f9eef":"dfq = df.query(\"Language != 'English' and Language != 'un'\").query(\"Count >= 20 and Count <= 30\")\nfig1 = px.bar(dfq, y=\"Language\", x=\"Count\", title=\"Language of non-English comments\", text=\"Count\", orientation=\"h\",\n             pattern_shape=\"Language\", pattern_shape_sequence=[\"|\", \"\/\", \"+\"], height=500)\nfig1.update_traces(texttemplate='%{text:.2s}',  textposition=\"outside\",marker_color='teal')\nfig1.update_layout(showlegend=False)\nfig1","fd76eee3":"dfq1 = df.query(\"Language != 'English' and Language != 'un'\").query(\"Count >= 50\")\nfig1 =px.scatter(dfq1, y=\"Language\", x=\"Count\", title=\"Count of non-English Language\", size=\"Count\", color=\"Language\", log_x=True, size_max=60)\nfig1.update_traces(mode=\"markers\")\nfig1.update_layout(showlegend=True)\nfig1","4d5716ec":"import plotly.figure_factory as ff\n\ndef new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ntrain[\"comment_words\"] = train[\"comment_text\"].apply(new_len)\nnums = train.query(\"comment_words != 0 and comment_words < 200\")[\"comment_words\"]\nfig = ff.create_distplot(hist_data=[nums],group_labels=[\"All comments\"],colors=[\"indigo\"])\n\nfig.update_layout(title_text=\"Word distribution per Comment\", xaxis_title=\"Comment words\", showlegend=False)\nfig.show()","b4e87db1":"import plotly.graph_objects as go\n\ndfaa = pd.DataFrame(np.transpose([lang_list, train.groupby(\"language\").mean()[\"comment_words\"]]))\ndfaa.columns = [\"Language\", \"avg_comment_words\"]\ndfaa[\"avg_comment_words\"] = dfaa[\"avg_comment_words\"].apply(float)\ndfaa = dfaa.query(\"avg_comment_words < 200\")\nfig = go.Figure()\nfig.add_trace(go.Bar(y=dfaa[\"avg_comment_words\"], x=dfaa[\"Language\"]))\nfig.update_layout(xaxis_title=\"Average Count of words\", yaxis_title=\"Language\", title_text=\"Language Versus Average Number of Words in comments\")\nfig.show()","888fa3bd":"import plotly.graph_objects as go\n\ndfab = pd.DataFrame(np.transpose([lang_list, train.groupby(\"language\").mean()[\"comment_words\"]]))\ndfab.columns = [\"Language\", \"avg_comment_words\"]\ndfab[\"avg_comment_words\"] = dfab[\"avg_comment_words\"].apply(float)\ndfab = dfab.query(\"avg_comment_words > 200\")\nfig = go.Figure()\nfig.add_trace(go.Bar(y=dfab[\"avg_comment_words\"], x=dfab[\"Language\"]))\nfig.update_layout(xaxis_title=\"Average Count of words\", yaxis_title=\"Language\", title_text=\"Language Versus Average Number of Words in comments\")\nfig.show()","984746cb":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef polarity_score(x):\n    if type(x) == str:\n        return SIA.polarity_scores(x)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\ntrain[\"polarity\"] = train[\"comment_text\"].progress_apply(polarity_score)","bdb706a4":"fig = go.Figure(go.Histogram(x=[pols[\"neg\"] for pols in train[\"polarity\"] if pols[\"neg\"] != 0], marker=dict(color='teal')))\n\nfig.update_layout(title_text=\"Negative sentiment\", template=\"simple_white\")\nfig.show()","e2830ad6":"fig = go.Figure(go.Histogram(x=[p[\"pos\"] for p in train[\"polarity\"] if p[\"pos\"] != 0], marker=dict(color='darkblue')))\nfig.update_layout(title_text=\"Positive sentiment\", template=\"simple_white\")\nfig.show()","187b633f":"#train[\"negativity\"] = train[\"polarity\"].apply(lambda x: x[\"neg\"])\n#one = train.query(\"toxic == 1\")[\"negativity\"]\n#zero = train.query(\"toxic == 0\")[\"negativity\"]\n\n#fig = ff.create_distplot(hist_data=[one, zero],group_labels=[\"Toxic\", \"Non-toxic\"],colors=[\"slategrey\", \"dodgerblue\"], show_hist=False)\n\n#fig.update_layout(title_text=\"Negativity vs. Toxicity\", xaxis_title=\"Negativity\", template=\"simple_white\")\n#fig.show()","0581c189":"#train[\"positivity\"] = train[\"polarity\"].apply(lambda x: x[\"pos\"])\n#nums_1 = train.sample(frac=0.1).query(\"toxic == 1\")[\"positivity\"]\n#nums_2 = train.sample(frac=0.1).query(\"toxic == 0\")[\"positivity\"]\n\n#fig = ff.create_distplot(hist_data=[nums_1, nums_2],group_labels=[\"Toxic\", \"Non-toxic\"], colors=[\"dodgerblue\", \"purple\"], show_hist=False)\n\n#fig.update_layout(title_text=\"Positivity vs. Toxicity\", xaxis_title=\"Positivity\", template=\"simple_white\")\n#fig.show()","fd409063":"pip install textstat","f8a50f0d":"import textstat\ntrain[\"flesch_reading_ease\"] = train[\"comment_text\"].progress_apply(textstat.flesch_reading_ease)\nfig = go.Figure(go.Histogram(x=train.query(\"flesch_reading_ease > 0\")[\"flesch_reading_ease\"], marker=dict(color='dodgerblue')))\n\nfig.update_layout(title_text=\"Flesch reading ease\", template=\"simple_white\")\nfig.show()","61a75471":"# Checking for Null Values","d09a19ed":"## Getting acquainted with POLYGLOT library\n#### Languages Supported","8ace76f5":"# Descriptive Analysis","8b6379b1":"#### Languages in the training data set and their counts","96ba3122":"# Exploratory Data Analysis\n### Word Cloud Creation","c3720d07":"Languages where the average number of words in comments is more than 200","64d4a132":"### Comparison between English and Non English Languages","1b0f74e4":"### Readability","ebe56966":"# Reading the training data file","097b18b1":"### Comparison of Non English languages with comments appearing more than 50 times","66d49429":"### Comment Word Distribution","fe0b0ecf":"#### Assigning a column of languages corresponding to the comment_text column","d3f08698":"Toxicity in Comparison with Negativity and Positivity","fb87f99f":"# Importing Libraries","6718381d":"### Average Comment Words vs Language\nLanguages where the average number of words in comments is less than 200","dd47a92d":"### Comparison of Non English languages with comments appearing between 20 and 30 times"}}