{"cell_type":{"c1b0d4e2":"code","30244329":"code","197c2b43":"code","d2a6b3b2":"code","34f16c85":"code","2e7c9269":"code","d5720bd1":"code","bd5948ec":"code","21e3408a":"code","26d27da7":"code","dc0d4812":"code","c63db962":"code","4541d87c":"code","9d3ff8f5":"code","0bd1eae6":"code","6802f602":"code","dc03d3e6":"code","26a840d9":"code","70cef908":"code","ec410b11":"code","fffe7cf7":"code","08f152dc":"code","c2af8f9f":"code","ebbb3601":"markdown","b1223e20":"markdown","09ee4f8f":"markdown","3359ff6e":"markdown","55b5406e":"markdown","bffac9a0":"markdown","6aeab348":"markdown"},"source":{"c1b0d4e2":"# Other modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score,classification_report\nimport tqdm\nfrom sklearn.metrics import accuracy_score,classification_report\nimport tqdm\nimport os\nimport time\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport random","30244329":"# Torch modules\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\nfrom collections import OrderedDict\nfrom torchvision import transforms, utils , models\nfrom torch.optim.lr_scheduler import StepLR\n# Setting device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","197c2b43":"# Directories\nimage_dir = '..\/input\/vehicle-images\/images'","d2a6b3b2":"# Reading the csv file\n\ndf_train = pd.read_csv('..\/input\/vehicle-images\/train.csv')\n\ndf_test = pd.read_csv('..\/input\/test-data\/submission.csv')\nclass_names = ['not_emergency','emergency']","34f16c85":"# Checking the number of training examples and test examples\nnum_train,num_test = df_train.shape[0],df_test.shape[0]\n\nprint(f'The number of training examples are {num_train} and testing are {num_test}')","2e7c9269":"# Declaration of few varaibles\nnum_epochs = 50\nbatch_size = 128\nlearning_rate = 0.0001","d5720bd1":"class VehicleDataset(Dataset):\n    def __init__(self,data,root_dir,transform,train=True):\n        self.root_dir = root_dir\n        self.transform = transform['train'] if train==True else transform['validation']\n        self.X = data['image_names']\n        self.y = data['emergency_or_not']\n        \n    \n    def __len__(self):\n        return self.X.shape[0]\n    def __getitem__(self,idx):\n        name= self.X.loc[idx]\n        img_name = os.path.join(self.root_dir,name)\n        image = Image.open(img_name)\n        \n        \n        \n        image = self.transform(image)\n        \n        label = self.y.loc[idx]\n        return image,label\n \n\n","bd5948ec":"def imshow(inp, title):\n    \"\"\"Imshow for Tensor.\"\"\"\n    plt.subplots(figsize=(35,35))\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    plt.title(title)\n    plt.show()\n","21e3408a":"import random\n\nclass RandomNoise(object):\n    def __init__(self, p=1):\n         self.probability = p\n    def __call__(self, tensor):\n        if random.random() <= self.probability:\n            return tensor + torch.randn(tensor.size()) * .25 + 0.5\n            \n        return tensor","26d27da7":"# Transforms\nmean = np.array([0.5, 0.5, 0.5])\nstd = np.array([0.25, 0.25, 0.25])\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        \n            transforms.Resize(256),\n            transforms.ColorJitter(),\n            transforms.RandomAffine(degrees=(-45,+45),scale=(0.9,1.2),translate=(0.2,0.2),shear = (0.2,0.2)),\n            transforms.RandomVerticalFlip(p=0.1),\n            transforms.RandomCrop(224),\n            transforms.RandomHorizontalFlip(),\n            \n            transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0),\n            torchvision.transforms.RandomGrayscale(p=0.15),\n            \n            transforms.ToTensor(),\n            transforms.RandomErasing(0.05),\n            transforms.Normalize(mean, std),\n            RandomNoise(p=0.4)\n            \n        \n        \n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n","dc0d4812":"# counts = df_train.emergency_or_not.value_counts().values\n# weights = 1\/counts\n# df_train['weights'] = df_train.emergency_or_not.apply(lambda x:weights[x])\n\n# sampler = WeightedRandomSampler(list(df_train['weights']),num_samples=batch_size)","c63db962":"train = VehicleDataset(df_train,image_dir,transform=data_transforms,train=True)\n\ndataloader = {'train':DataLoader(train,shuffle=True,batch_size=batch_size)}","4541d87c":"class_names = ['not_emergency','emergency']\nimages,classes=next(iter(dataloader['train']))\n# Make a grid from batch\nout = torchvision.utils.make_grid(images)\nimshow(out, title=[class_names[x] for x in classes])","9d3ff8f5":"#TRAINING THE NETWORK\ndef train(model, device, train_loader, optimizer):\n    model.train()\n    y_true = []\n    y_pred = []\n    losses = []\n    for i in train_loader:\n      \n    #LOADING THE DATA IN A BATCH\n        data, target = i\n        \n#         check\n\n        \n        target = target.type(torch.float)\n    #MOVING THE TENSORS TO THE CONFIGURED DEVICE\n        data, target = data.to(device), target.to(device)\n       \n        #FORWARD PASS\n        output = model(data.float())\n        \n        loss = criterion(output, target.unsqueeze(1)) \n                \n#         #BACKWARD AND OPTIMIZE\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n#         # PREDICTIONS\n        output = torch.sigmoid(output)\n#         out_values = 1\/(1+torch.exp(output.detach()))\n        pred = torch.round(output.detach())\n        target = torch.round(target.detach())  \n        \n        y_pred.extend(pred.tolist())\n        y_true.extend(target.tolist())\n        losses.append(loss.detach())\n        \n        \n        \n#     Getting accuracy and loss for the eopch\n    accuracy = round((accuracy_score(y_true,y_pred)).item(),4)\n    avg_loss = round((sum(losses)\/len(losses)).item(),4)\n    print(f\"Epoch Accuracy on training set is {accuracy} and loss is  {avg_loss}\")\n    \n#     Checking confusion matrix\n    print(classification_report(y_true,y_pred,target_names=class_names))\n\n    return accuracy,avg_loss","0bd1eae6":"def start_traininng(model):\n    training_accuracy = []\n    training_loss = []\n    testing_accuracy = []\n    testing_loss = []\n    for epoch in range(num_epochs):\n                # Decay Learning Rate\n\n            t0 = time.time()\n            # Print Learning Rate\n            print('Epoch:', epoch)\n            print('Learning rate ',scheduler.get_last_lr())\n            acc_train,loss_train = train(model,device,dataloader['train'],optimizer)\n    #         acc_test,loss_test = test(model,device,dataloader['validation'])\n\n            training_accuracy.append(acc_train)\n            training_loss.append(loss_train)\n    #         testing_accuracy.append(acc_test)\n    #         testing_loss.append(loss_test)\n\n    #         Learning rate schedular\n            scheduler.step()\n        \n#             time taken\n            t1 = time.time()\n            print(t1-t0)\n\n    return training_accuracy,training_loss","6802f602":"# This function predict the outputs from the trained model\ndef predict_result(model):\n    model.eval()\n    predictions= []\n    for file in tqdm.tqdm_notebook(df_test.image_names):\n        file_path = image_dir + '\/' + file\n        img = Image.open(file_path)\n        \n        img = data_transforms['val'](img)\n        data = img.to(device)\n        data=data.unsqueeze(0)\n        output = model(data)\n        output = torch.sigmoid(output)\n        pred = int(torch.round(output).item())\n        predictions.append(pred)\n    return predictions","dc03d3e6":"# Finetuning resent 18\nmodel = models.resnet50(pretrained=True)\n# Creating model and setting loss and optimizer.\nnum_ftrs = model.fc.in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nfc = nn.Sequential(OrderedDict([\n    \n    ('fc1', nn.Linear(num_ftrs,1)),\n    \n]))\nmodel.fc = fc\n","26a840d9":"# # Freezing layers\n# child_counter = 0\n# for child in model.children():\n#     if child_counter < 5:\n#         print(\"child \",child_counter,\" was frozen\")\n#         for param in child.parameters():\n#             param.requires_grad = False\n#         child_counter += 1\n#     else:\n#         print(\"child \",child_counter,\" was not frozen\")\n#         child_counter += 1","70cef908":"# Creating model, criterion, optimizer and schedular \nmodel = model.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate , weight_decay=5e-2)\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n\n# train resnet 18\ntraining_accuracy,training_loss = start_traininng(model)","ec410b11":"# Plot accuracy\nplt.plot(training_accuracy)\n\n    ","fffe7cf7":"# Plot accuracy\nplt.plot(training_loss)","08f152dc":"df_test = pd.read_csv('..\/input\/test-data\/submission.csv')\ndf_test.drop('emergency_or_not',axis=1,inplace=True)","c2af8f9f":"# prediction\npredictions=predict_result(model)\ndf_test['emergency_or_not'] = predictions\ndf_test.to_csv('submit.csv',index=False)","ebbb3601":"The best accuracy for the Test Data was 0.976076555023923.","b1223e20":"This Notebook is for Image Recognition <a href = \"https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-computer-vision-hackathon\/\">Hackathon<\/a>  to classify vehicles are emergency or not\n\n![emergency](attachment:0.jpg)![1855.jpg](attachment:1855.jpg)","09ee4f8f":"Pytorch has been used for the Process.","3359ff6e":"Vehicle Class created. This takes the id and will return image and class from that id by referring train_df.","55b5406e":"All the transformations to be done.","bffac9a0":"The training data is less in number so splitting the data into train and validation set is skipped and accuracy of the test data will be checked regularly to gauge the model.\n\nData Augmentation is used to expand the training dataset.","6aeab348":"Class Random Noise created which will add noise to the images choosen randomly."}}