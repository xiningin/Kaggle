{"cell_type":{"2f59da7c":"code","8bce98ed":"code","c27bacde":"code","7ad3c610":"code","c68d6826":"code","bc3abd08":"code","8e2e38c2":"code","9e80fe62":"code","ac17f5f9":"code","50de7216":"code","0e11fed1":"markdown","aa0fbfd5":"markdown","5336ab70":"markdown","0cf8ed03":"markdown","76216db1":"markdown","a0666f8a":"markdown","8815a6ec":"markdown"},"source":{"2f59da7c":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import timedelta, datetime\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom pylab import rcParams\nfrom tqdm import tqdm\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom contextlib import contextmanager\nimport gc\n\n\n##################\n# Files\n##################\n# raw csv data\nDIR_INPUT = '..\/input\/the-datascience-cup-beta'\nCSV_TRAIN = os.path.join(DIR_INPUT, 'train.csv')\nCSV_TEST = os.path.join(DIR_INPUT, 'test.csv')\nCSV_SAMPLE_SUBMISSION = os.path.join(DIR_INPUT, 'sample.csv')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n##################\n# Parameters\n##################\n\n# basic params\nRANDOM_SEED = 1213\nNUM_FOLDS = 10\nTARGET = 'price'\n\n# logger params\nCOMPETITION_NAME = 'dsc_beta'\nLOG_FILENAME = 'experiment'\nLOG_FORMAT = '%(asctime)s %(levelname)s %(message)s'\n\n# model params\n\nMODEL_LGB = True\nMODEL_XGB = True\nMODEL_CAT = True\nUSE_GPU = False\n\nWEIGHTS = {'LGB':1, 'XGB':1, 'CAT': 1}\n\nMODEL_PARAMS_LGB = {'objective': 'regression',\n                    'seed': RANDOM_SEED,\n                    'verbose': 0,\n                    'metrics': 'rmse',\n                    'subsample': 0.95,\n                    'min_child_weight': 1.0,\n                    'min_data_in_leaf': 10,\n                    'eta': 0.03,\n                    'alpha': 0.1\n                    }\n                    \nMODEL_PARAMS_XGB = {'objective': 'reg:squarederror',\n                    'eval_metric': 'rmse',\n                    'eta': 0.1,\n                    'seed': RANDOM_SEED\n                   }\n\nMODEL_PARAMS_CAT = {'loss_function': 'RMSE',\n                    'eval_metric': 'RMSE',\n                    'learning_rate': 0.03,\n                    'subsample': 0.95,\n                    'random_state': RANDOM_SEED,\n                    'num_boost_round': 10000,\n                   }\n\nFIT_PARAMS_LGB = {'num_boost_round': 10000, 'early_stopping_rounds': 100, 'verbose_eval': 500}\nFIT_PARAMS_XGB = {'num_boost_round': 10000, 'early_stopping_rounds': 100, 'verbose_eval': 500}   \nFIT_PARAMS_CAT = {'early_stopping_rounds': 100, 'verbose_eval': 500}   \n\n# GPU\nif USE_GPU:\n    MODEL_PARAMS_LGB.update({'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0})\n    MODEL_PARAMS_XGB.update({'tree_method': 'gpu_hist', 'n_gpus': 1})\n    MODEL_PARAMS_CAT.update({'task_type': 'GPU', 'bootstrap_type': 'Poisson'})\n\n    \n##################\n# Utility Functions\n#   - init_logger()\n#   - timer()\n##################\n\n# logger\nlogger = getLogger(COMPETITION_NAME)\n\ndef init_logger():\n    # Add handlers\n    if not logger.handlers:\n        # StreamHandler (console output)\n        handler = StreamHandler()\n        handler.setLevel(INFO)\n        handler.setFormatter(Formatter(LOG_FORMAT))\n        logger.addHandler(handler)\n        # FileHandler (file output)\n        fh_handler = FileHandler('{}.log'.format(LOG_FILENAME) + '-debug.log', mode='w')\n        fh_handler.setLevel(INFO)\n        fh_handler.setFormatter(Formatter(LOG_FORMAT))\n        logger.addHandler(fh_handler)\n    logger.setLevel(INFO)\n\n    \n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    logger.info(f'[{name}] start')\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.02f} s')\n    gc.collect()\n    \n\n##################\n# Feature Engineering\n#   - frequency_encoding()\n#   - label_encoding()\n#   - target_encoding()\n#   - TargetEncoder\n#   - calc_groupby_feat()\n#   - GroupbyTransformer\n##################\n\n\ndef frequency_encoding(X, cols, add_column=True):\n    for col in cols:\n        # this process is to keep nan\/na as is.\n        X_ = X[[col]].fillna('missing')\n        encoded = X_.groupby(col).size().reset_index(name=col + '_freqenc') \n        encoded = X_.merge(encoded, how='left', on=col)\n        X = pd.concat([X, encoded[[col + '_freqenc']]], axis=1)\n\n    if add_column is False:\n        X.drop(cols, axis=1, inplace=True)\n    return X\n\n\ndef label_encoding(df, cols):\n    for c in cols:\n        if df[c].dtype == 'object':\n            lbl = LabelEncoder()\n            df[c] = lbl.fit_transform(df[c].fillna('NA').astype(str))\n    return df\n\n\ndef target_encoding(df_train, df_test, cols, kf):\n    # Target Encoding\n    # based on https:\/\/kaggler.readthedocs.io\/en\/latest\/source\/kaggler.html#kaggler.preprocessing.TargetEncoder\n    len_train = len(df_train)\n    df_train = df[:len_train].copy()\n    df_test = df[len_train:].copy()\n    \n    encoder = TargetEncoder(cv=kf)\n    encoder.fit(df_train, df_train[TARGET], cols = cols)\n    \n    agg_tr = encoder.transform(df_train[cols].copy())\n    agg_ts = encoder.transform(df_test[cols].copy())\n    agg_tr.columns = [x + \"_target_enc\" for x in agg_tr.columns.values]\n    agg_ts.columns = [x + \"_target_enc\" for x in agg_ts.columns.values]\n    \n    df_train = pd.concat([df_train, agg_tr], axis=1)\n    df_test = pd.concat([df_test, agg_ts], axis=1)\n    \n    return df_train, df_test\n\n\nfrom sklearn import base\nclass TargetEncoder(base.BaseEstimator):\n\n    def __init__(self, smoothing=1, min_samples=10, cv=None):\n        assert (min_samples >= 0) and (smoothing >= 0), 'min_samples and smoothing should be positive'\n        self.smoothing = smoothing\n        self.min_samples = min_samples\n        self.cv = cv\n\n    def __repr__(self):\n        return('TargetEncoder(smoothing={}, min_samples={}, cv={})'.format(self.smoothing, self.min_samples, self.cv))\n\n    def _get_target_encoder(self, x, y):\n\n        assert len(x) == len(y)\n\n        # NaN cannot be used as a key for dict. So replace it with a random\n        # integer\n        mean_count = pd.DataFrame({y.name: y, x.name: x.fillna(np.nan)}).groupby(x.name)[y.name].agg(['mean', 'count'])\n        smoothing = 1 \/ (1 + np.exp(-(mean_count['count'] - self.min_samples) \/ self.smoothing))\n\n        mean_count[y.name] = self.target_mean * (1 - smoothing) + mean_count['mean'] * smoothing\n        return mean_count[y.name].to_dict()\n\n    def fit(self, X, y, cols):\n        self.target_encoders = [None] * len(cols)\n        self.target_mean = y.mean()\n        self.cols = cols\n\n        for i, col in enumerate(self.cols):\n            if self.cv is None:\n                self.target_encoders[i] = self._get_target_encoder(X[col], y)\n            else:\n                self.target_encoders[i] = []\n                for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X[col], y), 1):\n                    self.target_encoders[i].append(self._get_target_encoder(X.loc[i_trn, col], y[i_trn]))\n\n        return self\n    \n    def transform(self, X):\n        \n        for i, col in enumerate(self.cols):\n            if self.cv is None:\n                X.loc[:, col] = (X[col].fillna(np.nan)\n                                       .map(self.target_encoders[i])\n                                       .fillna(self.target_mean))\n            else:\n                for i_enc, target_encoder in enumerate(self.target_encoders[i], 1):\n                    if i_enc == 1:\n                        x = X[col].fillna(np.nan).map(target_encoder).fillna(self.target_mean)\n                    else:\n                        x += X[col].fillna(np.nan).map(target_encoder).fillna(self.target_mean)\n\n                X.loc[:, col] = x \/ i_enc\n\n        return X\n    \n\n# group-by feature extractor\ndef calc_groupby_feat(df):\n    groupby = GroupbyTransformer(param_dict=DIC_GROUPBY)\n    df = groupby.transform(df)\n    diff = DiffGroupbyTransformer(param_dict=DIC_GROUPBY)\n    df = diff.transform(df)\n    return df\n\n\nclass GroupbyTransformer():\n    def __init__(self, param_dict=None):\n        self.param_dict = param_dict\n\n    def _get_params(self, p_dict):\n        key = p_dict['key']\n        if 'var' in p_dict.keys():\n            var = p_dict['var']\n        else:\n            var = self.var\n        if 'agg' in p_dict.keys():\n            agg = p_dict['agg']\n        else:\n            agg = self.agg\n        if 'on' in p_dict.keys():\n            on = p_dict['on']\n        else:\n            on = key\n        return key, var, agg, on\n\n    def _aggregate(self, dataframe):\n        self.features = []\n        for param_dict in self.param_dict:\n            key, var, agg, on = self._get_params(param_dict)\n            all_features = list(set(key + var))\n            new_features = self._get_feature_names(key, var, agg)\n            features = dataframe[all_features].groupby(key)[\n                var].agg(agg).reset_index()\n            features.columns = key + new_features\n            self.features.append(features)\n        return self\n\n    def _merge(self, dataframe, merge=True):\n        for param_dict, features in zip(self.param_dict, self.features):\n            key, var, agg, on = self._get_params(param_dict)\n            if merge:\n                dataframe = dataframe.merge(features, how='left', on=on)\n            else:\n                new_features = self._get_feature_names(key, var, agg)\n                dataframe = pd.concat([dataframe, features[new_features]], axis=1)\n        return dataframe\n\n    def transform(self, dataframe):\n        self._aggregate(dataframe)\n        return self._merge(dataframe, merge=True)\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return ['_'.join([a, v, 'groupby'] + key) for v in var for a in _agg]\n\n    def get_feature_names(self):\n        self.feature_names = []\n        for param_dict in self.param_dict:\n            key, var, agg, on = self._get_params(param_dict)\n            self.feature_names += self._get_feature_names(key, var, agg)\n        return self.feature_names\n\n    def get_numerical_features(self):\n        return self.get_feature_names()\n    \n\nclass DiffGroupbyTransformer(GroupbyTransformer):\n    def _aggregate(self):\n        raise NotImplementedError\n        \n    def _merge(self):\n        raise NotImplementedError\n    \n    def transform(self, dataframe):\n        for param_dict in self.param_dict:\n            key, var, agg, on = self._get_params(param_dict)\n            for a in agg:\n                for v in var:\n                    new_feature = '_'.join(['diff', a, v, 'groupby'] + key)\n                    base_feature = '_'.join([a, v, 'groupby'] + key)\n                    dataframe[new_feature] = dataframe[base_feature] - dataframe[v]\n        return dataframe\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return ['_'.join(['diff', a, v, 'groupby'] + key) for v in var for a in _agg]\n\n    \n# from IPython.core.debugger import Pdb; Pdb().set_trace()\nrcParams['figure.figsize'] = 10,10\npd.set_option('display.max_columns', 70)\npd.set_option('display.max_rows', 100)\ninit_logger()","8bce98ed":"df_train = pd.read_csv(CSV_TRAIN, index_col=0)\ndf_test = pd.read_csv(CSV_TEST, index_col=0)\ndf_sample_submission = pd.read_csv(CSV_SAMPLE_SUBMISSION)\n\nprint('train: {} rows, {} columns'.format(df_train.shape[0], df_train.shape[1]))\nprint('test : {} rows, {} columns'.format(df_test.shape[0], df_test.shape[1]))\nprint('sample submission: {} rows, {} columns'.format(df_sample_submission.shape[0], df_sample_submission.shape[1]))\n\ndf_train.head()","c27bacde":"df = pd.concat([df_train, df_test], axis=0)\n# df['num_na'] = df.drop('price', axis=1).isna().sum(axis=1)\n\n\n##################\n# \u64ec\u4f3c\u7684\u306aID\n##################\n\ndf['host_id'] = df['host_since'].fillna('NA') + '_' + df['host_neighbourhood'].fillna('NA')\ndf['room_id'] = df['host_since'].fillna('NA') + \\\n    df['host_is_superhost'] + \\\n    df['host_has_profile_pic'] + \\\n    df['host_identity_verified'] + \\\n    df['neighbourhood_cleansed'] + \\\n    df['property_type'].astype('str') + \\\n    df['room_type'].astype('str') + \\\n    df['accommodates'].astype('str') + \\\n    df['bathrooms_text'].astype('str') + \\\n    df['bedrooms'].astype('str') +  \\\n    df['beds'].astype('str') + \\\n    df['minimum_nights'].astype('str') + \\\n    df['maximum_nights'].astype('str')\n\n    \n# %\u304c\u3064\u3044\u3066\u3044\u308b\u6587\u5b57\u5217\u306f\u6570\u5024\u306b\u76f4\u3059\ndf['host_response_rate'] = df['host_response_rate'].replace('%', '', regex=True).astype('float', errors='ignore')\ndf['host_acceptance_rate'] = df['host_acceptance_rate'].replace('%', '', regex=True).astype('float', errors='ignore')\n\n\n##################\n# \u65e5\u4ed8\u60c5\u5831\n##################\n\n# \u6642\u9593\u306e\u52a0\u5de5\ncols_dttm = ['host_since', 'first_review', 'last_review']\nfor c in cols_dttm:\n    df[c] = pd.to_datetime(df[c])\n\n# first_review \u304b\u3089 last_review\u307e\u3067\u306e\u6642\u9593\u306a\u3069\ndf['month'] = df['host_since'].dt.month\ndf['year'] = df['host_since'].dt.year\ndf['num_days_to_first_review'] = (df['first_review'] - df['host_since']) \/ timedelta(days=1)\ndf['num_days_to_last_review'] = (df['last_review'] - df['host_since']) \/ timedelta(days=1)\ndf['num_days_from_first_to_last_review'] = (df['last_review'] - df['first_review']) \/ timedelta(days=1)\ndf['first_review_month'] = df['first_review'].dt.month\ndf['last_review_month'] = df['last_review'].dt.month\ndf['first_review_year'] = df['first_review'].dt.year\ndf['last_review_year'] = df['last_review'].dt.year\n\n\n##################\n# \u6d17\u9762\u6240\u3084\u30d9\u30c3\u30c9\u306e\u6570\n##################\n\n# \u6d74\u5ba4\/\u6d17\u9762\u6240\u306e\u6570\u3001private\u304b\u5426\u304b\n# df['num_baths'] = df['bathrooms_text'].replace(r'Half|half', '0.5', regex=True)\ndf['num_baths'] = df['bathrooms_text'].str.extract(r'([0-9]+\\.?[0-9]*)').astype('float', errors='ignore')\ndf['private_bath'] = df['bathrooms_text'].str.contains(r'private|Private') * 1\ndf['shared_bath'] = df['bathrooms_text'].str.contains(r'shared|Shared') * 1\n\n# \u4e00\u4eba\u3042\u305f\u308a\u306e\u6d74\u5ba4\/\u6d17\u9762\u6240\u306e\u6570\u3001\u30d9\u30c3\u30c9\u306e\u6570 (\u6700\u5927\u4eba\u6570\u6642)\ndf['beds_per_person'] = df['beds'].values \/ df['accommodates'].values\ndf['bedrooms_per_person'] = df['bedrooms'].values \/ df['accommodates'].values\ndf['baths_per_person'] = df['num_baths'].values \/ df['accommodates'].values\n\n\n#################\n# \u6570\u5024\u95a2\u4fc2\n#################\ndf['room_size'] = df['accommodates'] + df['bedrooms'] + df['beds'] + df['num_baths']\ndf['available_days_ratio'] = df['accommodates'] + df['bedrooms'] + df['beds'] + df['num_baths']\ndf['reviewer_times_review'] = df['number_of_reviews'] * df['review_scores_rating']\ndf['host_rate'] = df['host_response_rate'].values * df['host_acceptance_rate'].values\n\n\n##################\n# \u671f\u9593\u3054\u3068\u306e\u4e88\u7d04\u53ef\u80fd\u6570\n##################\ndf['availability_30_60'] = df['availability_60'].values - df['availability_30'].values\ndf['availability_60_90'] = df['availability_90'].values - df['availability_60'].values\ndf['availability_90_365'] = df['availability_365'].values - df['availability_90'].values\n\n\n##################\n# \u8fd1\u96a3\u7269\u4ef6\u307e\u3067\u306e\u8ddd\u96e2\u3068\u4fa1\u683c\n##################\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(df_train[['latitude', 'longitude']], df_train[TARGET].values)\nkneighbors = knn.kneighbors(df[['latitude', 'longitude']])\ndf['dist_closest_point_5'] = [x[1] for x in kneighbors[0]]\ndf['price_closest_point_5'] = [np.mean(df_train[TARGET].values[x[1:]]) for x in kneighbors[1]]\n\n# \u6570\u5024\u30ab\u30e9\u30e0\u306e NaN \u306f -999 \u306b\u3059\u308b (\u9762\u5012\u306a\u306e\u3067\u5168\u90e8 float \u306b\u3059\u308b)\ncols_num = ['host_response_rate', 'host_acceptance_rate', 'latitude',\n            'longitude', 'accommodates', 'bedrooms', 'beds',\n            'minimum_nights', 'maximum_nights', 'availability_30', 'availability_60',\n            'availability_90', 'availability_365', 'number_of_reviews',\n            'review_scores_rating', 'reviews_per_month', 'room_size', 'num_baths']\nfor c in cols_num:\n    df[c] = df[c].fillna(-999).astype('float')\n\nprint('df: {} rows, {} columns'.format(df.shape[0], df.shape[1]))\ndf.head()","7ad3c610":"# Frequency Encoding\nif True:\n    cols = ['host_id', 'neighbourhood_cleansed', 'property_type']\n    df = frequency_encoding(df, cols)\n\n# Label Encoding\nif True:\n    cols = ['host_response_time', 'host_is_superhost', 'host_neighbourhood',\n            'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_cleansed', 'property_type',\n            'room_type', 'bathrooms_text', 'review_scores_rating', 'instant_bookable']\n    df = label_encoding(df, cols)\n\n# Target Encoding\nif True:\n    cols = ['host_id', 'host_since', 'accommodates', 'host_neighbourhood', 'neighbourhood_cleansed', 'host_response_time', 'property_type', 'room_type', 'bathrooms_text']\n    kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n    df_train, df_test = target_encoding(df_train, df_test, cols, kf)\n\n# Fill NA\ndf = pd.concat([df_train, df_test], axis=0)\ndf.fillna(-999, inplace=True)\n    \n# groupby features\nDIC_GROUPBY = [\n        {\n        'key': ['neighbourhood_cleansed'], \n        'var': cols_num, \n        'agg': ['mean', 'sum', 'median', 'min', 'max', 'var']\n        },\n        {\n        'key': ['property_type'], \n        'var': cols_num, \n        'agg': ['mean', 'sum', 'median', 'min', 'max', 'var']\n        },\n        {\n        'key': ['room_type'], \n        'var': cols_num, \n        'agg': ['mean', 'sum', 'median', 'min', 'max', 'var']\n        }\n    ]\n\ndf = calc_groupby_feat(df)\n\nprint('df: {} rows, {} columns'.format(df.shape[0], df.shape[1]))\ndf.head()","c68d6826":"##################\n# Modeling Functions\n#   - calc_score()\n#   - run_lightgbm()\n#   - run_xgboost()\n#   - run_catboost()\n#   - plot_importance()\n#   - direct_match()\n##################\n\n\ndef calc_score(y_true, y_pred):\n    score = mean_absolute_error(y_true, y_pred)\n    return score\n\n\nimport lightgbm as lgb\ndef run_lightgbm(X_train, y_train, X_valid, y_valid, model_params, fit_params, cat_cols, X_test, idx=None):\n    # prepare\n    if idx is None:\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid)\n    else:\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid.iloc[idx], y_valid[idx])\n        \n\n    # train\n    model = lgb.train(model_params, lgb_train,\n                      **fit_params,\n                      valid_names=['train', 'valid'],\n                      valid_sets=[lgb_train, lgb_valid],\n                     )\n\n    # predict\n    y_pred_valid = model.predict(X_valid)\n    y_pred_test = None\n    if X_test is not None:\n        y_pred_test = model.predict(X_test)\n\n    return y_pred_valid, y_pred_test, model\n\n\nimport xgboost as xgb\ndef run_xgboost(X_train, y_train, X_valid, y_valid, model_params, fit_params, cat_cols, X_test, idx=None):\n    # prepare\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n    if idx is None:\n        deval = dvalid\n    else:\n        deval = xgb.DMatrix(X_valid.iloc[idx], label=y_valid[idx])\n    \n    # train\n    watchlist = [(dtrain, 'train'), (deval, 'eval')]\n    model = xgb.train(model_params, dtrain, evals=watchlist, **fit_params)\n    \n    # predict\n    y_pred_valid = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n    y_pred_test = None\n    if X_test is not None:\n        dtest = xgb.DMatrix(X_test)\n        y_pred_test = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n\n    return y_pred_valid, y_pred_test, model\n\n\nfrom catboost import CatBoost, Pool\ndef run_catboost(X_train, y_train, X_valid, y_valid, model_params, fit_params, cat_cols, X_test, idx=None):\n    # prepare\n    if idx is None:\n        dtrain = Pool(X_train, label=y_train, cat_features=cat_cols)\n        dvalid = Pool(X_valid, label=y_valid, cat_features=cat_cols)\n    else:\n        dtrain = Pool(X_train, label=y_train, cat_features=cat_cols)\n        dvalid = Pool(X_valid.iloc[idx], label=y_valid[idx], cat_features=cat_cols)\n        \n    # train\n    model = CatBoost(model_params)\n    model.fit(dtrain, eval_set=[dvalid], use_best_model=True, **fit_params)\n\n    # predict\n    y_pred_valid = model.predict(X_valid)\n    y_pred_test = None\n    if X_test is not None:\n        y_pred_test = model.predict(X_test)\n\n    return y_pred_valid, y_pred_test, model\n\n\ndef plot_importance(model, cols, topn=None, fname=None):\n    \n    if 'lightgbm' in str(type(model)):\n        importance = model.feature_importance(importance_type='gain')\n    \n    # calculate importance\n    cols = [str(x) for x in cols]\n    df_importance = pd.DataFrame(sorted(zip(importance, cols)), columns=['Value','Feature'])\n    df_importance.sort_values(by=\"Value\", ascending=False, inplace=True)\n    if topn is not None:\n        df_importance = df_importance.head(topn)\n    \n    # plot\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=df_importance)\n    plt.title('Feature importance')\n    plt.tight_layout()\n    \n    # save\n    if fname is not None:\n        plt.savefig(fname)\n    else:\n        plt.show()\n\n        \n##################\n# Competition specific\n##################\n    \ndef direct_match(id_train, id_test, y_train):\n    num_match = 1\n    \n    # predict validation data\n    y_pred = np.zeros_like(id_test)\n    for i, h in tqdm(enumerate(id_test)):\n        idx = np.where(id_train == h)[0]\n        # from IPython.core.debugger import Pdb; Pdb().set_trace()\n        if idx.shape[0] >= num_match:\n            y_pred[i] = np.median(y_train[idx])\n    \n    return y_pred","bc3abd08":"# competition specific params\nDROP_COLS = ['host_id', 'room_id', 'host_since', 'first_review', 'last_review']\nCAT_COLS = []\nUSE_LOW_WHEN_UNDER = 30000\nUSE_DIRECT_WHEN_OVER = 50000\nPSEUDO_LABEL = True\n\n\n##########################\n# Prepare\n##########################\n\nlen_train = len(df_train)\nlen_test = len(df_test)\n\nX_tr = df[:len_train].copy()\nX_ts = df[len_train:].copy()\ny_tr = X_tr[TARGET].values\ny_tr_log = np.log(y_tr + 1)  # log transformation\n\nX_tr.drop([TARGET] + DROP_COLS, axis=1, inplace=True)\nX_ts.drop([TARGET] + DROP_COLS, axis=1, inplace=True)\nprint('X_tr: {} rows, {} columns'.format(X_tr.shape[0], X_tr.shape[1]))\nprint('X_ts : {} rows, {} columns'.format(X_ts.shape[0], X_ts.shape[1]))\n\n\n##########################\n# Direct Fit\n##########################\n\nwith timer('Direct fit with room_id'):    \n    kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n    \n    room_id= df['room_id']\n    room_id_tr = room_id[:len_train]\n    room_id_ts = room_id[len_train:]\n    y_pred_direct = np.zeros(len_train)\n    y_test_direct = np.zeros(len_test)\n    for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n        logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n        y_train = y_tr[train_index]\n        y_valid = y_tr[valid_index]\n        room_id_train = room_id_tr[train_index]\n        room_id_valid = room_id_tr[valid_index]\n\n        y_pred_direct[valid_index] = direct_match(room_id_train, room_id_valid, y_train)\n        score_fold = calc_score(y_tr[valid_index], y_pred_direct[valid_index])\n        logger.info('score={:.4}'.format(score_fold))\n\n    y_test_direct = direct_match(room_id_train, room_id_ts, y_train)\n    \n\n##########################\n# Pseudo Label\n##########################\n\nif PSEUDO_LABEL:\n    with timer('Pseudo Label with direct fit'):\n        idx_pseudo = np.where(y_test_direct > 0)[0]\n        X_tr = pd.concat([X_tr, X_ts.iloc[idx_pseudo]], axis=0)\n        y_tr = np.hstack([y_tr, y_test_direct[idx_pseudo].astype('float')])\n        y_tr_log = np.log(y_tr + 1)\n        len_train = len(y_tr)\n        print('len(train): {}'.format(len_train))\n        \n\n##########################\n# Boosting\n##########################\n\nif MODEL_LGB:\n    with timer('LightGBM (low price data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_lgb_low = np.empty(len_train)\n        y_test_lgb_low = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train with low price\n            idx_low = np.where(y_train <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            idx_low_valid = np.where(y_valid <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            X_train = X_train.iloc[idx_low]\n            y_train = y_train[idx_low]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_lightgbm(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_LGB, FIT_PARAMS_LGB, CAT_COLS, X_test=X_ts, idx=idx_low_valid)\n            \n            # store results\n            y_pred_lgb_low[valid_index] = y_pred_valid\n            y_test_lgb_low.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_lgb_low = np.median(y_test_lgb_low, axis=0)\n\n        # convert to original scale\n        y_pred_lgb_low = np.exp(y_pred_lgb_low) - 1\n        y_test_lgb_low = np.exp(y_test_lgb_low) - 1\n        \n        # score\n        score_lgb_low = calc_score(y_tr[:5000], y_pred_lgb_low[:5000])\n        logger.info('LightGBM (low): oof score={:.4}'.format(score_lgb_low))\n        plot_importance(model, X_train.columns.values, topn=20, fname='importance_low_price.png')\n\n\n    with timer('LightGBM (all data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_lgb_all = np.empty(len_train)\n        y_test_lgb_all = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_lightgbm(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_LGB, FIT_PARAMS_LGB, CAT_COLS, X_test = X_ts)\n\n            # store results\n            y_pred_lgb_all[valid_index] = y_pred_valid\n            y_test_lgb_all.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_lgb_all = np.median(y_test_lgb_all, axis=0)\n\n        # convert to original scale\n        y_pred_lgb_all = np.exp(y_pred_lgb_all) - 1\n        y_test_lgb_all = np.exp(y_test_lgb_all) - 1\n\n        # score\n        score_lgb_all = calc_score(y_tr[:5000], y_pred_lgb_all[:5000])\n        logger.info('LightGBM (all): oof score={:.4}'.format(score_lgb_all))\n        plot_importance(model, X_train.columns.values, topn=20, fname='importance_all_data.png')\n\n        \nif MODEL_XGB:\n    with timer('XGBoost (low price data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_xgb_low = np.empty(len_train)\n        y_test_xgb_low = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train with low price\n            idx_low = np.where(y_train <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            idx_low_valid = np.where(y_valid <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            X_train = X_train.iloc[idx_low]\n            y_train = y_train[idx_low]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_xgboost(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_XGB, FIT_PARAMS_XGB, CAT_COLS, X_test = X_ts, idx=idx_low_valid)\n\n            # store results\n            y_pred_xgb_low[valid_index] = y_pred_valid\n            y_test_xgb_low.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_xgb_low = np.median(y_test_xgb_low, axis=0)\n\n        # convert to original scale\n        y_pred_xgb_low = np.exp(y_pred_xgb_low) - 1\n        y_test_xgb_low = np.exp(y_test_xgb_low) - 1\n\n        # score\n        score_xgb_low = calc_score(y_tr[:5000], y_pred_xgb_low[:5000])\n        logger.info('XGBoost (low): oof score={:.4}'.format(score_xgb_low))\n        \n        \n    with timer('XGBoost (all data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_xgb_all = np.empty(len_train)\n        y_test_xgb_all = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_xgboost(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_XGB, FIT_PARAMS_XGB, CAT_COLS, X_test = X_ts)\n\n            # store results\n            y_pred_xgb_all[valid_index] = y_pred_valid\n            y_test_xgb_all.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_xgb_all = np.median(y_test_xgb_all, axis=0)\n\n        # convert to original scale\n        y_pred_xgb_all = np.exp(y_pred_xgb_all) - 1\n        y_test_xgb_all = np.exp(y_test_xgb_all) - 1\n\n        # score\n        score_xgb_all = calc_score(y_tr[:5000], y_pred_xgb_all[:5000])\n        logger.info('XGBoost (all): oof score={:.4}'.format(score_xgb_all))\n\n\nif MODEL_CAT:\n    with timer('CatBoost (low price data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_cat_low = np.empty(len_train)\n        y_test_cat_low = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train with low price\n            idx_low = np.where(y_train <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            idx_low_valid = np.where(y_valid <= np.log(USE_LOW_WHEN_UNDER + 1))[0]\n            X_train = X_train.iloc[idx_low]\n            y_train = y_train[idx_low]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_catboost(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_CAT, FIT_PARAMS_CAT, CAT_COLS, X_test = X_ts, idx=idx_low_valid)\n\n            # store results\n            y_pred_cat_low[valid_index] = y_pred_valid\n            y_test_cat_low.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_cat_low = np.median(y_test_cat_low, axis=0)\n\n        # convert to original scale\n        y_pred_cat_low = np.exp(y_pred_cat_low) - 1\n        y_test_cat_low = np.exp(y_test_cat_low) - 1\n\n        # score\n        score_cat_low = calc_score(y_tr[:5000], y_pred_cat_low[:5000])\n        logger.info('CatBoost (low): oof score={:.4}'.format(score_cat_low))\n        \n        \n    with timer('CatBoost (all data)'):\n        kf = KFold(n_splits=NUM_FOLDS, random_state=RANDOM_SEED, shuffle=True)\n\n        y_pred_cat_all = np.empty(len_train)\n        y_test_cat_all = []\n        for fold_id, (train_index, valid_index) in enumerate(kf.split(range(len_train))):\n            logger.info('Fold: {} \/ {}'.format(fold_id + 1, NUM_FOLDS))\n            # split data\n            X_train = X_tr.iloc[train_index]\n            X_valid = X_tr.iloc[valid_index]\n            y_train = y_tr_log[train_index]\n            y_valid = y_tr_log[valid_index]\n\n            # train\n            y_pred_valid, y_pred_test, model = run_catboost(X_train, y_train, X_valid, y_valid, MODEL_PARAMS_CAT, FIT_PARAMS_CAT, CAT_COLS, X_test = X_ts)\n\n            # store results\n            y_pred_cat_all[valid_index] = y_pred_valid\n            y_test_cat_all.append(y_pred_test)\n            score_fold = calc_score(y_tr[valid_index], np.exp(y_pred_valid) - 1)\n            logger.info('score={:.4}'.format(score_fold))\n\n        # kfold averaging\n        y_test_cat_all = np.median(y_test_cat_all, axis=0)\n\n        # convert to original scale\n        y_pred_cat_all = np.exp(y_pred_cat_all) - 1\n        y_test_cat_all = np.exp(y_test_cat_all) - 1\n\n        # score\n        score_cat_all = calc_score(y_tr[:5000], y_pred_cat_all[:5000])\n        logger.info('CatBoost (all): oof score={:.4}'.format(score_cat_all))","8e2e38c2":"# combine low, all, direct models\ndef combine_prediction(pred_low, pred_all, pred_direct):\n    pred = pred_low.copy()\n    idx = np.where(pred_all > USE_LOW_WHEN_UNDER)[0]\n    pred[idx] = pred_all[idx]\n    idx = np.where(pred_direct >= USE_DIRECT_WHEN_OVER)[0]\n    pred[idx] = pred_direct[idx]\n    return pred\n\nif MODEL_LGB:\n    y_pred_lgb_ens = combine_prediction(y_pred_lgb_low, y_pred_lgb_all, y_pred_direct)\n    y_test_lgb_ens = combine_prediction(y_test_lgb_low, y_test_lgb_all, y_test_direct)\n    score_lgb = calc_score(y_tr[:5000], y_pred_lgb_ens[:5000])\n    logger.info('LightGBM (low): oof score={:.4}'.format(score_lgb_low))\n    logger.info('LightGBM (all): oof score={:.4}'.format(score_lgb_all))\n    logger.info('LightGBM (low + all + direct): oof score={:.4}'.format(score_lgb))\n    y_pred_lgb = y_pred_lgb_ens\n    y_test_lgb = y_test_lgb_ens\n    np.save(\"y_pred_lgb.npy\", y_pred_lgb)\n\nif MODEL_XGB:\n    y_pred_xgb_ens = combine_prediction(y_pred_xgb_low, y_pred_xgb_all, y_pred_direct)\n    y_test_xgb_ens = combine_prediction(y_test_xgb_low, y_test_xgb_all, y_test_direct)\n    score_xgb = calc_score(y_tr[:5000], y_pred_xgb_ens[:5000])\n    logger.info('XGBoost (low): oof score={:.4}'.format(score_xgb_low))\n    logger.info('XGBoost (all): oof score={:.4}'.format(score_xgb_all))\n    logger.info('XGBoost (low + all + direct): oof score={:.4}'.format(score_xgb))\n    y_pred_xgb = y_pred_xgb_ens\n    y_test_xgb = y_test_xgb_ens\n    np.save(\"y_pred_xgb.npy\", y_pred_xgb)\n    \nif MODEL_CAT:\n    y_pred_cat_ens = combine_prediction(y_pred_cat_low, y_pred_cat_all, y_pred_direct)\n    y_test_cat_ens = combine_prediction(y_test_cat_low, y_test_cat_all, y_test_direct)\n    score_cat = calc_score(y_tr[:5000], y_pred_cat_ens[:5000])\n    logger.info('CatBoost (low): oof score={:.4}'.format(score_cat_low))\n    logger.info('CatBoost (all): oof score={:.4}'.format(score_cat_all))\n    logger.info('CatBoost (low + all + direct): oof score={:.4}'.format(score_cat))\n    y_pred_cat = y_pred_cat_ens\n    y_test_cat = y_test_cat_ens\n    np.save(\"y_pred_cat.npy\", y_pred_cat)","9e80fe62":"# median\ny_pred = []\ny_test = []\n\nif MODEL_LGB:\n    print('LightGBM: oof score={:.4}'.format(score_lgb))\n    y_pred.append(y_pred_lgb)\n    y_test.append(y_test_lgb)\nif MODEL_XGB:\n    print('XGBoost: oof score={:.4}'.format(score_xgb))\n    y_pred.append(y_pred_xgb)\n    y_test.append(y_test_xgb)\nif MODEL_CAT:\n    print('CatBoost: oof score={:.4}'.format(score_cat))\n    y_pred.append(y_pred_cat)\n    y_test.append(y_test_cat)\n\ny_pred = np.median(y_pred, axis=0)\ny_test = np.median(y_test, axis=0)    \n\n# weightied average\nif False:\n    y_pred = np.zeros(len_train, )\n    y_test = np.zeros(len_test, )\n\n    sum_weights = 0\n    if MODEL_LGB:\n        print('LightGBM: oof score={:.4}'.format(score_lgb))\n        y_pred += WEIGHTS['LGB'] * y_pred_lgb\n        y_test += WEIGHTS['LGB'] * y_test_lgb\n        sum_weights += WEIGHTS['LGB']\n    if MODEL_XGB:\n        print('XGBoost: oof score={:.4}'.format(score_xgb))\n        y_pred += WEIGHTS['XGB'] * y_pred_xgb\n        y_test += WEIGHTS['XGB'] * y_test_xgb\n        sum_weights += WEIGHTS['XGB']\n    if MODEL_CAT:\n        print('CatBoost: oof score={:.4}'.format(score_cat))\n        y_pred += WEIGHTS['CAT'] * y_pred_cat\n        y_test += WEIGHTS['CAT'] * y_test_cat\n        sum_weights += WEIGHTS['CAT']    \n\n    # save result\n    y_pred \/= sum_weights\n    y_test \/= sum_weights\n\n# clipping\nidx = np.where(y_pred > 1_000_000)[0]\ny_pred[idx] = 1_000_000\nidx = np.where(y_test > 1_000_000)[0]\ny_test[idx] = 1_000_000\n\nscore = calc_score(y_tr[:5000], y_pred[:5000])\nprint('oof score={:.4}'.format(score))\nnp.save('y_pred.npy', y_pred)\nnp.save('y_test.npy', y_test)","ac17f5f9":"df_sample_submission[TARGET] = y_test\ndf_sample_submission.to_csv('submission_oof{:.4}.csv'.format(score), index=False)\ndf_sample_submission.head()","50de7216":"plt.hist(y_tr, bins=128, alpha=0.5, label='train')\nplt.hist(y_test, bins=128, alpha=0.5, label='y_test')\nplt.legend(loc='upper left')\nplt.show()\n\nplt.scatter(y_tr, y_pred)\nplt.plot(np.arange(1_000_000), np.arange(1_000_000))\nplt.xlim(0, 50_000)\nplt.ylim(0, 50_000)\nplt.show()\n\nplt.scatter(y_tr, y_pred)\nplt.plot(np.arange(1_000_000), np.arange(1_000_000))\nplt.xlim(0, 1_100_000)\nplt.ylim(0, 1_100_000)\nplt.show()","0e11fed1":"## \u7279\u5fb4\u91cf: encoding \u3068 groupby","aa0fbfd5":"## \u7279\u5fb4\u91cf: \u57fa\u672c","5336ab70":"## Submission File","0cf8ed03":"## Ensemble","76216db1":"## \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","a0666f8a":"## \u30e2\u30c7\u30eb","8815a6ec":"(\u3053\u308c\u3092\u3055\u3089\u306b Seed Averaging \u3057\u3066\u3044\u307e\u3059)"}}