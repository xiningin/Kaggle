{"cell_type":{"ea077dc4":"code","98b56165":"code","e315da83":"code","243c214b":"code","e5d9a32f":"code","62beec83":"code","7908515b":"code","7250800d":"code","34cbad7f":"code","52823323":"code","71071ad1":"code","00ed2e98":"code","dd83a7be":"markdown","df85b931":"markdown","375706bb":"markdown","6cf8182b":"markdown","54ef5535":"markdown","bd8ece5a":"markdown","c26eb2be":"markdown","9a530b8e":"markdown","ed06b0a8":"markdown"},"source":{"ea077dc4":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torch import autograd\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt","98b56165":"fashion_df = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\nfashion_df.head()","e315da83":"x = fashion_df.iloc[:,1:].values.astype('uint8').reshape(-1, 28,28)\nImage.fromarray(x[0])","243c214b":"class FashionMNIST(Dataset):\n    def __init__(self, transform=None):\n        self.transform = transform\n        fashion_df = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\n        self.labels = fashion_df.label.values\n        self.images = fashion_df.iloc[:,1:].values.astype('uint8').reshape(-1, 28, 28)\n        \n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.fromarray(self.images[idx])\n        #img = self.images[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img","e5d9a32f":"transform = transforms.Compose([\n        transforms.ToTensor(),\n        #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n        transforms.Normalize(mean=(0, 0, 0), std=(1, 1, 1))\n])\ndataset = FashionMNIST(transform=transform)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)","62beec83":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.z_dim = 100\n        self.model = nn.Sequential(\n            nn.Linear(self.z_dim, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, 784),\n            nn.Tanh()\n        )\n    \n    def forward(self, z):\n        # z = batch_size*100(z_dim) dim tensor\n        out = self.model(z)\n        return out\n        ","7908515b":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n                nn.Linear(784, 1024),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Dropout(0.3),\n                nn.Linear(1024, 512),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Dropout(0.3),\n                nn.Linear(512, 256),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Dropout(0.3),\n                nn.Linear(256, 128),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Dropout(0.3),\n                nn.Linear(128, 1),\n                nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        # x is batch_size*28*28 dim tensor if sampled from the Fashion-MNIST\n        # if its from the generator, a 784 tensor\n        x = x.view(x.size(0), 784)\n        out = self.model(x)\n        return out\n    ","7250800d":"generator = Generator().cuda()\ndiscriminator = Discriminator().cuda()","34cbad7f":"criterion = nn.BCELoss()\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)","52823323":"def generator_step(batch_size):\n    g_optimizer.zero_grad()\n    z = Variable(torch.randn(batch_size, 100)).cuda()\n    \n    # Generator needs to generate images that can fool the discriminator, hence the discriminator should classify\n    # its generated image as a real one(label: 1)\n    \n    # Generator's loss is basically binary cross entropy loss between the discriminator's prediction and 1\n    fake_images = generator(z)\n    validity = discriminator(fake_images)\n    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n    g_loss.backward()\n    g_optimizer.step()\n    return g_loss.data\n    ","71071ad1":"def discriminator_step(batch_size, real_images):\n    d_optimizer.zero_grad()\n    \n    # Discriminator's loss is basically comprised of two things:\n    # 1) the real images should be classified as 1, hence a BCE between it's prediction for real images and 1\n    # 2) fake images should be classified as 0, hence a BCE between it's prediction for fake images and 0\n    \n    # real loss\n    real_validity = discriminator(real_images)\n    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n    \n    # fake loss\n    z = Variable(torch.randn(batch_size, 100)).cuda()\n    fake_images = generator(z)\n    validity = discriminator(fake_images)\n    fake_loss = criterion(validity, Variable(torch.zeros(batch_size)).cuda())\n    d_loss = real_loss + fake_loss\n    d_loss.backward()\n    d_optimizer.step()\n    return d_loss.data","00ed2e98":"NUM_EPOCHS = 50\nfor epoch in range(NUM_EPOCHS):\n    print(\"Epoch: \", epoch+1)\n    for i, images in enumerate(data_loader):\n        real_images = Variable(images).cuda()\n        batch_size = real_images.size(0)\n        generator.train()\n        d_loss = discriminator_step(batch_size, real_images)\n        g_loss = generator_step(batch_size)\n        if i == 100:\n            break\n    print(\"g_loss: {} d_loss: {}\".format(g_loss, d_loss))\n    generator.eval()\n    z = Variable(torch.randn(9, 100)).cuda()\n    #sample_images = generator(z).data\n    sample_images = generator(z)\n    sample_images = sample_images.view(sample_images.size(0), 28, 28).unsqueeze(1).data.cpu()\n    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n    plt.imshow(grid)\n    plt.show()\n    \n        ","dd83a7be":"### Data normalization and loader","df85b931":"## Data loading","375706bb":"## Gradient and losses","6cf8182b":"## Running through the data","54ef5535":"Experiments","bd8ece5a":"# Training ","c26eb2be":"### Load the Fashion-MNIST Dataset","9a530b8e":"## Models","ed06b0a8":"Implementation of Vanilla GAN on Fashion-MNIST dataset\n\nInspired from an excellent tutorial here https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan"}}