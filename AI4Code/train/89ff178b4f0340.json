{"cell_type":{"d0c29b93":"code","3961e575":"code","b13b246c":"code","8cfd7ad3":"code","0e00df67":"code","83eee6e2":"code","451f6ab4":"code","c7600acf":"code","1ab21162":"code","eb679262":"code","df47dfe7":"code","24860f27":"code","2ae72f07":"code","911a06e3":"code","edbc09a9":"code","a609a36f":"code","e36e774d":"code","bc2d9528":"code","ad858607":"code","eb4d4096":"code","3b24eef9":"code","2d01d967":"code","a2efb8ec":"code","d5e11c3d":"code","85e20a9f":"code","8f0e6e6c":"code","40c30feb":"code","e4074346":"code","7e4df0d0":"code","14b0367d":"code","c60f9893":"code","bb0bdd2a":"code","feb5a75b":"code","bd235215":"code","f2232d23":"code","9711982d":"code","597bc291":"code","260820ff":"code","840e9996":"markdown","e7b2df75":"markdown","930ea4ad":"markdown","efe99f07":"markdown","294f1bf3":"markdown","0067cdf4":"markdown","01e5b2f1":"markdown","229b3eae":"markdown","a22aa149":"markdown","b95b76eb":"markdown","c8f60914":"markdown","12c1c12c":"markdown","7d2a597c":"markdown","69f5c2a5":"markdown","edefb350":"markdown"},"source":{"d0c29b93":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nplotly.offline.init_notebook_mode (connected = True)","3961e575":"pd. set_option('display.max_columns', None)\ndf=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","b13b246c":"df.isna().any()","8cfd7ad3":"df=df.iloc[:,1:-1]","0e00df67":"df['Count']=1\ndf_diag=df.groupby('diagnosis')['Count'].sum().reset_index()","83eee6e2":"fig1=px.pie(df_diag,values='Count',names='diagnosis',hole=0.4)\nfig1.update_layout(title='Diagnosis distribution',title_x=.5,\n                   annotations=[dict(text='Diagnosis',font_size=20, \n                   showarrow=False,height=800,width=700)])\n\n\n\nfig1.show()","451f6ab4":"from sklearn.preprocessing import StandardScaler","c7600acf":"scaler=StandardScaler()\ndf_scaled=pd.DataFrame(scaler.fit_transform(df.iloc[:,1:-1]),columns=df.iloc[:,1:-1].columns)","1ab21162":"df_scaled=pd.merge(df['diagnosis'],df_scaled,on=df_scaled.index)","eb679262":"df_scaled.drop('key_0',axis=1,inplace=True)","df47dfe7":"data = pd.melt(df_scaled.iloc[:,:-1],id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","24860f27":"fig2=px.violin(data,x='features',y='value',box=True,color='diagnosis',violinmode='overlay')\nfig2.update_layout(violingap=0)\nfig2.show()","2ae72f07":"df_mean=df_scaled.iloc[:,:11]\ndata = pd.melt(df_mean,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","911a06e3":"fig3=px.violin(data,x='features',y='value',box=True,color='diagnosis',violinmode='overlay',labels={'value':'Scaled values'},color_discrete_sequence =['red','blue'])\nfig3.update_layout(violingap=0,template='plotly_dark',title='Mean parameter distribution',title_x=0.5)\nfig3.show()","edbc09a9":"df_se=df_scaled.iloc[:,11:21]\n\ndf_se=pd.merge(df['diagnosis'],df_se,on=df_se.index)\ndf_se.drop('key_0',axis=1,inplace=True)\ndata = pd.melt(df_se,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","a609a36f":"fig4=px.violin(data,x='features',y='value',box=True,color='diagnosis',violinmode='overlay',color_discrete_sequence =['red','blue'],labels={'value':'Scaled values'})\nfig4.update_layout(violingap=0,template='plotly_dark',title='SE parameter distribution',title_x=0.5)\nfig4.show()","e36e774d":"df_worst=df_scaled.iloc[:,21:]\ndf_worst=pd.merge(df['diagnosis'],df_worst,on=df_worst.index)\ndf_worst.drop('key_0',axis=1,inplace=True)","bc2d9528":"data = pd.melt(df_worst,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","ad858607":"fig5=px.violin(data,x='features',y='value',box=True,color='diagnosis',violinmode='overlay',color_discrete_sequence =['red','blue'],labels={'value':'Scaled values'})\nfig5.update_layout(violingap=0,template='plotly_dark',title='Worst parameter distribution',title_x=0.5)\nfig5.show()","eb4d4096":"corrs=df_scaled.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corrs,annot=True)","3b24eef9":"df_sim=df_scaled.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df_sim, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"viridis\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","2d01d967":"imp_feat=['diagnosis','radius_mean','compactness_mean','concavity_mean',\n          'concave points_mean','radius_worst','texture_worst','concave points_worst']\n\ndf_scaled[imp_feat].head()","a2efb8ec":"df_scaled['diagnosis']=df_scaled['diagnosis'].replace({'M':1,'B':0})","d5e11c3d":"target=df_scaled['diagnosis']\ndf_scaled.drop('diagnosis',axis=1,inplace=True)","85e20a9f":"X=df_scaled.values\nY=target","8f0e6e6c":"from sklearn.model_selection import train_test_split\n","40c30feb":"X_train,X_test,y_train,y_test=train_test_split(X,Y,shuffle=True,random_state=0,test_size=0.25)","e4074346":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","7e4df0d0":"reg_log=LogisticRegression()\nreg_log.fit(X_train,y_train)","14b0367d":"reg_log.score(X_train,y_train)","c60f9893":"y_preds_log=reg_log.predict(X_test)\nreg_log.score(X_test,y_test)","bb0bdd2a":"conf_mat_log=confusion_matrix(y_preds_log,y_test)\n\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\ncax = sns.heatmap(conf_mat_log,ax=ax,annot=True,cmap='summer')\nax.xaxis.set_ticklabels(['Benign', 'Malignant'])\nax.yaxis.set_ticklabels(['Benign','Malignant'],rotation=0)\nax.set_xlabel('Predicted',size=15)\nax.set_ylabel('Actual',size=15)\nplt.title('Confusion matrix Logsitic Regression',size=15)\nplt.figure(figsize=(10,8))","feb5a75b":"from sklearn.ensemble import RandomForestClassifier","bd235215":"rfc=RandomForestClassifier(max_depth=10,random_state=5)","f2232d23":"rfc.fit(X_train,y_train)","9711982d":"rfc.score(X_train,y_train)","597bc291":"y_preds_rfc=rfc.predict(X_test)","260820ff":"conf_mat_rfc=confusion_matrix(y_preds_rfc,y_test)\n\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\ncax = sns.heatmap(conf_mat_rfc,ax=ax,annot=True,cmap='gnuplot')\nax.xaxis.set_ticklabels(['Benign', 'Malignant'])\nax.yaxis.set_ticklabels(['Benign','Malignant'],rotation=0)\nax.set_xlabel('Predicted',size=15)\nax.set_ylabel('Actual',size=15)\nplt.title('Confusion matrix RFC',size=15)\nplt.figure(figsize=(10,8))","840e9996":"Upon checking the worst parameter features, we see that there are a few features that could help in classification quite well. Some of these parameters are:\n\n* radius_worst\n* texture_worst\n* perimeter_worst\n* area_worst\n* concave_points_worst\n\n\n## f) Heatmap correlation\n\nSince we have so many features available to us, it is important to check their correlation. Highly correlated values generally cause data leakage which is undesirable for us. Hence, we need to check for these featuers and try to remove them from our analysis.","e7b2df75":"## b) Random Forest Classifier","930ea4ad":"As can be seen from the KDE plots and scatter plots above, the features radius, perimeter and area are all correlated with a high degree. Hence, we shall drop all the permeter and area from our analysis.\n\n\n# Feature selection\n\nIn this process, we shall choose only the important features for our further analysis. The features to be selected have been decided by visualising the various data.","efe99f07":"# Conclusion\n\nBoth RFC and Logistic Regression have given us a very high classification accuracy. Hence, it can be said that the feature selection and feature scaling helped to provide a better classification power for both RFC and logistic regression.\n\n\n# If you found this notebook useful, an upvote would be amazing ! :)","294f1bf3":"As we can see from the above correlation  heatmap, some of the parameters are highly correlated and will contribute to data leakage issues. One of the evident such features are the:\n\n* radius\n* perimeter\n* area\n\nBoth area and perimeter are completely correlated to each other. The only independent parameter here is the radius. Let us check this using a pairgrid.","0067cdf4":"As we can see, about 37.3 % of the women have developed malignant breast cancer which requires immediate attention.\n\n## b) Key parameters\n\nOver here, we shall check how the key parameters change for both benign and malignant patients. Since the values of each feature vary by a large margin, we shall standardise the entries first.\n","01e5b2f1":"As we can see, apart from the final column, every other column has no null values. Infact the last column shows no information addition. Hence, we shall simply drop it from our analysis. Patient ID provides no additional info aswell. Hence we can drop this as well. Apart from that, we can head over to data visualisation since the data provided doesn't require any further pre-processing.","229b3eae":"# Introduction\n\nOne of the biggest diseases that has plagued the human kind in the modern era is breast cancer. Infact, in the United States, after skin cancer, breast cancer is considered to be the most common cancer disease.\n\nAlthough cancer generally has a lot of serious implications to the body, it is to be noted that an early diagnosis coupled with good healthcare has led to many lives saved. It is extremely important for common people to be generally aware of some of the symptoms which have been mentioned below. Breast cancer has been seen in both men and women although it is much more common for women to develop symptoms than men. In fact, in USA, every 1 in 8 women may have the chance of being diagnosed with breast cancer.\n\nIn order to be more aware of the symptoms, we would work on the dataset provided by UCI ML to get a better understanding of the parameters that define a positive diagnosis for breast cancer.\n\n![BC.jpg](attachment:BC.jpg)\n\n\n# Importing relevant libraries and datasets\n","a22aa149":"# Data Visualisation\n\n## a) Final diagnosis\n\nLet us first check what percentage of diagnosis is malignant and benign. A malignant diagnosis generally requires immediate healthcare attention.","b95b76eb":"From the given SE parameters and their distribution, it is seen that the SE parameteres are very similar for both malign and benign breast cancer patients and maynot be good enough for classification.\n\n\n## e) Worst parameters\n\nLet us check the distribution of the worst parameters available to us.","c8f60914":"As we can see,some of the above features vary enough for classification while the rest are nearly same for both malignant and benign patients. The ones that show clear difference in feature values amongst the benign and malignant patients will provide better classification power. From the above violin\/boxplots, we see that the following show clear difference:\n\n* radius_mean\n* perimeter_mean\n* area_mean\n* compactness_mean\n* concavity_mean\n* concave_points_mean\n\n## d) se parameters\n\nLet us check the se parameters as given in the data.","12c1c12c":"# Data Cleaning\n\nLet us check if there is any requirement of data cleaning for this particular dataset. We shall first check for any null values.","7d2a597c":"As we can see, the above visulisation is extremely cluttered due to too many features. We see that the features are divided into mean, se and worst. We shall visualise the 3 categories separately.\n\n## c) Mean parameters","69f5c2a5":"The above dataframe has all the features that we shall use for our classification purpose.\n\n\n# Machine Learning\n\nIn this section, we shall be using a few ML algorithms that will help us to classify the data correctly as Malginant and Benign.\n\nFirst, let us separate the target feature which is diagnosis from rest of the dataframe and encode it as:\n\n1: Malignant\n\n0: Benign","edefb350":"## a) Logistic Regression"}}