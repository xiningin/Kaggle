{"cell_type":{"1188fe5c":"code","e54d8eda":"code","09b15988":"code","63c4513a":"code","e21cab88":"code","68f84164":"code","cc0a7392":"code","791c9629":"code","320cff77":"code","0ef10181":"code","4ec712c7":"code","af1f676e":"code","6dec306d":"code","1f80d951":"code","9e3b40b3":"code","5e8c5ba9":"code","603dca5a":"code","720f0212":"code","415dd410":"code","c4a002be":"code","00413477":"code","27da871f":"code","a4f20653":"code","ef6b7aeb":"code","47bf1027":"code","ee8fca3a":"code","56e90c59":"markdown","891f1ef6":"markdown","3d12a019":"markdown","8b436107":"markdown","0858f75d":"markdown","bca38540":"markdown","c28b2ef5":"markdown","9c8a81c8":"markdown","00f5fdb2":"markdown","990a1f75":"markdown","dbf287cb":"markdown","fe0c2d98":"markdown","60b9ea2d":"markdown","6ebaacf0":"markdown","edfb5d1c":"markdown","fc1c9fe3":"markdown","5f3d9a06":"markdown","ed46eb4a":"markdown","43eba54c":"markdown","d2335bed":"markdown","4cf31004":"markdown","6b62878b":"markdown","f1540d0f":"markdown","0259834a":"markdown","df82f377":"markdown","fef9f1a2":"markdown","a0e85460":"markdown","f8078dcc":"markdown"},"source":{"1188fe5c":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt \nfrom skimage import io, transform\nimport cv2\nimport tensorflow as tf","e54d8eda":"import os\nprint(os.listdir(\"..\/input\"))","09b15988":"FAST_RUN=False\nFAST_PREDICT=True\nIMAGE_WIDTH=128\nIMAGE_HEIGHT=128\nCHANNEL=3\ninput_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL)\nmask_shape=(IMAGE_WIDTH, IMAGE_HEIGHT)\nimage_size=(IMAGE_WIDTH, IMAGE_HEIGHT)\nbatch_size=10\nepochs=3","63c4513a":"metadata = pd.read_csv(\"..\/input\/metadata.csv\")\ntrain_masks = pd.read_csv(\"..\/input\/train_masks.csv\")\ntestdata = pd.read_csv(\"..\/input\/sample_submission.csv\")","e21cab88":"print(train_masks.shape)\nprint(testdata.shape)","68f84164":"if FAST_RUN:\n    epochs=1\n    train_masks = train_masks.sample(1000).reset_index()\n\nif FAST_PREDICT: \n    testdata = testdata.sample(batch_size).reset_index()","cc0a7392":"testdata.head()","791c9629":"metadata.head()","320cff77":"train_masks.head()","0ef10181":"filenames = train_masks.img.str.split(\".\")\nmaskfilenames = filenames.str[0] + \"_mask.gif\"\ntrain_masks['img_mask'] = maskfilenames\ntrain_masks['angle'] = filenames.str[0].str.split(\"_\").str[1].astype(int)\ntrain_masks.head()","4ec712c7":"sample = train_masks.sample()\nfig=plt.figure(figsize=(16, 8))\nfor index, s in sample.iterrows():\n    original_image = io.imread('..\/input\/train\/'+s.img)\n    masked_image = io.imread('..\/input\/train_masks\/'+s.img_mask)\n    plt.subplot(2, 2, 1)\n    plt.imshow(original_image)\n    plt.subplot(2, 2, 2)\n    plt.imshow(masked_image)","af1f676e":"from skimage.transform import AffineTransform, warp\ndef shift(image, translation_matrix):\n    transformer = AffineTransform(translation=translation_matrix)\n    return warp(image, transformer, mode='wrap', preserve_range=True)","6dec306d":"def tranform_image(original_image, mask_image):\n    image = original_image\n    mask = mask_image\n    \n    isHorizontalFlip = np.random.random() < 0.5\n    isShift = np.random.random() < 0.5\n\n    if isShift:\n        translation_matrix = np.random.random_integers(-10, 10), np.random.random_integers(-10, 10)\n        image = shift(image, translation_matrix)\n        mask = shift(mask, translation_matrix)\n\n    if isHorizontalFlip:\n        image = image[:, ::-1]\n        mask = mask[:, ::-1]\n        \n    image = image \/ 255.0\n    mask = mask \/ 255.0\n    \n    return image, mask","1f80d951":"def data_gen_small(data_dir, mask_dir, df_data, precess_batch_size, original_image_shape, mask_image_shape):\n    while True:\n        for k, ix in df_data.groupby(np.arange(len(df_data))\/\/precess_batch_size):\n            imgs = []\n            labels = []\n            for index, row in ix.iterrows():\n                # images\n                original_img = io.imread(data_dir + row.img)\n                resized_img = transform.resize(original_img, image_size, mode='constant')\n                # masks\n                original_mask = io.imread(mask_dir + row.img_mask, as_gray=True)\n                resized_mask = transform.resize(original_mask, image_size, mode='constant')\n                \n                image, mask = tranform_image(resized_img, resized_mask)\n                \n                imgs.append(image)\n                labels.append(np.expand_dims(mask, axis=2))\n                \n            imgs = np.array(imgs)\n            labels = np.array(labels)\n            yield imgs, labels","9e3b40b3":"train_gen = data_gen_small(\"..\/input\/train\/\", \"..\/input\/train_masks\/\", train_masks, batch_size, input_shape, mask_shape)","5e8c5ba9":"fig=plt.figure(figsize=(16, 16))\nfor i in [1, 2, 3, 4]:\n    img, msk = next(train_gen)\n    plt.subplot(4, 2, i*2-1)\n    plt.imshow(img[0]*255.0)\n    plt.subplot(4, 2, i*2)\n    plt.imshow(msk[0].reshape(128, 128))\n    ","603dca5a":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization\n\ninputs = Input(shape=input_shape)\n# 128\n\ndown1 = Conv2D(64, (3, 3), padding='same')(inputs)\ndown1 = BatchNormalization()(down1)\ndown1 = Activation('relu')(down1)\ndown1 = Conv2D(64, (3, 3), padding='same')(down1)\ndown1 = BatchNormalization()(down1)\ndown1 = Activation('relu')(down1)\ndown1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n# 64\n\ndown2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\ndown2 = BatchNormalization()(down2)\ndown2 = Activation('relu')(down2)\ndown2 = Conv2D(128, (3, 3), padding='same')(down2)\ndown2 = BatchNormalization()(down2)\ndown2 = Activation('relu')(down2)\ndown2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n# 32\n\ndown3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\ndown3 = BatchNormalization()(down3)\ndown3 = Activation('relu')(down3)\ndown3 = Conv2D(256, (3, 3), padding='same')(down3)\ndown3 = BatchNormalization()(down3)\ndown3 = Activation('relu')(down3)\ndown3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n# 16\n\ndown4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\ndown4 = BatchNormalization()(down4)\ndown4 = Activation('relu')(down4)\ndown4 = Conv2D(512, (3, 3), padding='same')(down4)\ndown4 = BatchNormalization()(down4)\ndown4 = Activation('relu')(down4)\ndown4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n# 8\n\ncenter = Conv2D(1024, (3, 3), padding='same')(down4_pool)\ncenter = BatchNormalization()(center)\ncenter = Activation('relu')(center)\ncenter = Conv2D(1024, (3, 3), padding='same')(center)\ncenter = BatchNormalization()(center)\ncenter = Activation('relu')(center)\n# center\n\nup4 = UpSampling2D((2, 2))(center)\nup4 = concatenate([down4, up4], axis=3)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\n# 16\n\nup3 = UpSampling2D((2, 2))(up4)\nup3 = concatenate([down3, up3], axis=3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\n# 32\n\nup2 = UpSampling2D((2, 2))(up3)\nup2 = concatenate([down2, up2], axis=3)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\n# 64\n\nup1 = UpSampling2D((2, 2))(up2)\nup1 = concatenate([down1, up1], axis=3)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\n# 128\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid')(up1)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n","720f0212":"optimizer = tf.train.RMSPropOptimizer(0.0001)","415dd410":"model.compile(\n    optimizer=optimizer, \n    loss=\"binary_crossentropy\", \n    metrics=[\"accuracy\"]\n)","c4a002be":"steps_per_epoch=np.ceil(float(len(train_masks)) \/ float(batch_size)).astype(int)\nhistory = model.fit_generator(\n    train_gen, \n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs\n)","00413477":"model.save(\"model.h5\")","27da871f":"def test_gen_small(data_dir, df_data, precess_batch_size, original_image_shape):\n    while True:\n        for k, ix in df_data.groupby(np.arange(len(df_data))\/\/precess_batch_size):\n            imgs = []\n            labels = []\n            for index, row in ix.iterrows():\n                # images\n                original_img = io.imread(data_dir + row.img)\n                resized_img = transform.resize(original_img, original_image_shape) \/ 255.0\n                imgs.append(resized_img)\n\n            imgs = np.array(imgs)\n            yield imgs\n","a4f20653":"test_gen = test_gen_small(\"..\/input\/test\/\", testdata, batch_size, input_shape)","ef6b7aeb":"img = next(test_gen)\nfig=plt.figure(figsize=(16, 8))\nfor i in [1, 2, 3, 4]:\n    plt.subplot(1, 4, i)\n    plt.imshow(img[i-1]*255.0)","47bf1027":"steps = np.ceil(float(len(testdata)) \/ float(batch_size)).astype(int)\ny_predicted = model.predict_generator(\n    test_gen, \n    steps=steps\n)","ee8fca3a":"fig=plt.figure(figsize=(16, 8))\nfor i in [1, 2, 3, 4]:\n    y_predict = y_predicted[i-1]\n    plt.subplot(1, 4, i)\n    plt.imshow(y_predict.reshape(128, 128))","56e90c59":"# Working In progress","891f1ef6":"See how our generator work","3d12a019":"Compile model","8b436107":"# Test Generator","0858f75d":"# Import Global","bca38540":"Create test generator ","c28b2ef5":"# Model\n\nwe use U-Net model to do image masking","9c8a81c8":"View test data frame","00f5fdb2":"Define Image Generator","990a1f75":"See the result","dbf287cb":"Predict","fe0c2d98":"View masks data frame","60b9ea2d":"Here is the Kernel for get start with image masking. We mask Carvana Image data set with U-net model and implement with Keras and Tensorslow","6ebaacf0":"# Train Model","edfb5d1c":"See how test generator work","fc1c9fe3":"View Images","5f3d9a06":"Let see the files we have","ed46eb4a":"View meta data frame","43eba54c":"**Transform Image**\n\nShift, Flip","d2335bed":"# Save Model","4cf31004":"Define test generator","6b62878b":"Fast run for development","f1540d0f":"# Image Generator","0259834a":"Create Generator","df82f377":"Genarate Mask filename","fef9f1a2":"# Global Varaible ","a0e85460":"Shift Image","f8078dcc":"# Read Data"}}