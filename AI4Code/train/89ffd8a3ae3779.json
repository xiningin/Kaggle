{"cell_type":{"e7f463fd":"code","130170e5":"code","3a13cc8a":"code","ef97c768":"code","223b4878":"code","7055983c":"code","ea3fdf2b":"code","0387b30b":"code","c0f714e8":"code","a06a33be":"code","0b1961d0":"code","eb4c8c4d":"code","171af369":"code","8723af28":"code","1ef8df0a":"code","4e08c06b":"code","f78bf524":"code","b1a2f7db":"code","0bad2877":"markdown","ec6fa729":"markdown","2a7c36c0":"markdown","e42b7883":"markdown","45999fc4":"markdown","c103c8b8":"markdown","96a5613b":"markdown","b6d402a3":"markdown","4153a32b":"markdown","3cb91524":"markdown","dedb9737":"markdown","f8760a81":"markdown","5da9986f":"markdown","64b0404a":"markdown","0eb3682d":"markdown","fd73fe71":"markdown","369a381a":"markdown","69cc3d8e":"markdown","bb53ac9b":"markdown","c8e78a5f":"markdown","c47b0d17":"markdown","6b9c210d":"markdown","8bcaa7a4":"markdown","2de64e80":"markdown","2518d6f9":"markdown","28fc4daa":"markdown"},"source":{"e7f463fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\nfrom sklearn import metrics\n\n# include fasti.ai libraries\nfrom fastai.tabular import *\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom IPython.display import display\npd.set_option('display.max_columns', None) # display all columns\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","130170e5":"df = pd.read_csv(\"..\/input\/MiningProcess_Flotation_Plant_Database.csv\", parse_dates = True, index_col = 'date',decimal=',')","3a13cc8a":"shape1 = df.shape\ndf = df.dropna()\nshape2 = df.shape\nif shape1 == shape2:\n    print('Data contains no nan values.')\nelse:\n    print('Data contains nan values.')","ef97c768":"df.head()","223b4878":"plt.figure(figsize=(25,8))\nplt.subplot(1, 2, 1)\nplt.plot(df['% Iron Concentrate']);\nplt.xlabel('Date')\nplt.title('Iron Concentrate in %')\nplt.subplot(1, 2, 2)\nplt.plot(df['% Silica Concentrate']);\nplt.xlabel('Date')\nplt.title('Silica Concentrate in %')","7055983c":"sep_date = \"2017-03-29 12:00:00\"\nind_date = df.index<sep_date #boolean of earlier dates\ndf.drop(df.index[ind_date],inplace=True)\ndf.head(1)","ea3fdf2b":"plt.figure(figsize=(30, 25))\np = sns.heatmap(df.corr(), annot=True)","0387b30b":"train, test = train_test_split(df, test_size=0.2)\nx = train.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny = train['% Silica Concentrate']","c0f714e8":"model = RandomForestRegressor(n_estimators=50, min_samples_leaf=1, max_features=None, n_jobs=-1)\nmodel.fit(x,y)","a06a33be":"y_hat = model.predict(x)\nmse = metrics.mean_squared_error(y,y_hat)\nprint('Train Set')\nprint('RMSE:',math.sqrt(mse),'   R2:',model.score(x,y))","0b1961d0":"x_test = test.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\ny_test = test['% Silica Concentrate']\ny_hat_test = model.predict(x_test)\nmse_test = metrics.mean_squared_error(y_test,y_hat_test)\nprint('TEST Set')\nprint('RMSE:',math.sqrt(mse_test),'   R2:',model.score(x_test,y_test))","eb4c8c4d":"feat_importances = pd.Series(model.feature_importances_, index=df.columns[:-2])\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","171af369":"from scipy.cluster import hierarchy as hc\ncorr = np.round(scipy.stats.spearmanr(df).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=16)\nplt.show()","8723af28":"df_mean = df.copy()\nmean_grpby = df_mean.groupby(['date']).mean() # calculate mean\nstd_grpby = df_mean.groupby(['date']).std() # calculate std\nstd_grpby = std_grpby.loc[:, (std_grpby != 0).any(axis=0)] # delete null columns (columns with zero variance)\nstd_grpby = std_grpby.add_prefix('STD_') # add prefix to column names\ndf_merge = pd.merge(mean_grpby, std_grpby, on='date') # merge both dataframes\ndf_merge.describe()","1ef8df0a":"train, test = train_test_split(df_merge, test_size=0.2)\nx_aver = train.drop(['% Silica Concentrate','% Iron Concentrate','STD_% Silica Concentrate','STD_% Iron Concentrate'], axis=1)\ny_aver = train['% Silica Concentrate']","4e08c06b":"model = RandomForestRegressor(n_estimators=50, min_samples_leaf=1, max_features=None, n_jobs=-1)\nmodel.fit(x_aver,y_aver)","f78bf524":"y_aver_hat = model.predict(x_aver)\nmse = metrics.mean_squared_error(y_aver,y_aver_hat)\nprint('Train Set')\nprint('RMSE:',math.sqrt(mse),'   R2:',model.score(x_aver,y_aver))","b1a2f7db":"x_aver_test = test.drop(['% Silica Concentrate','% Iron Concentrate','STD_% Silica Concentrate','STD_% Iron Concentrate'], axis=1)\ny_test = test['% Silica Concentrate']\ny_hat_test = model.predict(x_aver_test)\nmse_test = metrics.mean_squared_error(y_test,y_hat_test)\nprint('TEST Set')\nprint('RMSE:',math.sqrt(mse_test),'   R2:',model.score(x_aver_test,y_test))","0bad2877":"## The Froth flotation process\n\nThe froth floatation is used to seperate the iron contents in the ore from other contaminations. The whole process usually contains for steps:\n\n1. Contioning of the ore feed pulp (mixture of ore and water) and other reagents\n2. Separation of hydrophobic and hydrophilic materials: binding particles attach to the bubbles\n3. The bubbles transport the particles upwards until they float on the surface (froth)\n4. Collection of the froth by mechanical separation (e.g. by an impeller)","ec6fa729":"## Modeling\n\nNow let's apply a model to check if we can predict the dependent variable '% Concentrate Silica'. First, we split our dataframe into train and validation set (train: first 80% of dataframe, test: last 20% of dataframe). Then we need to remove our '% Silica Concentrate' and '% Iron Concentrate' columns, since the first one is the dependent variable and the later is not available for the online implementation, as these values come from a lab measurement and takes roughly 1h 40 minutes.","2a7c36c0":"------","e42b7883":"**Train Random Forest**","45999fc4":"Wow, that revealed a high (negative) correlations between the 'Iron Feed' and 'Silica Feed' (both Inputs of the process) as well as 'Iron Concentrate' and 'Silica Concentrate' (both Outputs of the process from the lab measurement). The later basically says, the higher the quality of the Iron, the smaller the less Silica it contains.","c103c8b8":"Check if we have missing (nan) values:","96a5613b":"# Predicting the effectiveness of a Froth floating process\n\n\n**June 2019**\n\n\n**Project Description:**\n\nWe will use this dataset to analyse and predict the Froth floating process having the two aims:\n* What is the best predictor for the iron concentration of the product?\n* Can the data set be used to predict the impurity of the product (by silicate concentration)?\n\n**Data Description:** \n\nThis notebook deals with the analysis of a reverse cationic flotation process of a real production environment. The data (including its documentation) is accessible through kaggle: https:\/\/www.kaggle.com\/edumagalhaes\/quality-prediction-in-a-mining-process\n\n---","b6d402a3":"The dataframe contains data about:\n* Quality measures of the iron ore pulp before feeding it into the process (inputs)\n* Features that can effect the quality of the product (process parameters)\n* Quality measures of the iron ore pulp as product of the process (outputs)","4153a32b":"Repeat training model with new dataframe 'df_merge':","3cb91524":"Now, we quickly look at pearson correlations between our features (independent variables) to get a better understanding of our dataset:","dedb9737":"We can see the ten most important features, '% Iron Feed' and '% Silica Feed' as well as the pH level of the ore pulp seem to be substantial parameters to control. ","f8760a81":"**Identifiying redundant features**","5da9986f":"We can see that our data misses data packages of a couple of days. Based on the documentation at Kaggle, this was caused by a production shutdown. In order to rule out any influences from potentially corrupted data, we will remove the data earlier of the restart of production (\"2017-03-29 12:00:00\").\n\nWe can also see that the quality of the products does not seem to follow a clear temporal dependency.","64b0404a":"**Check Averaged Test Set**","0eb3682d":"## Model interpretation","fd73fe71":"The plot above shows how each feature correlates with the others. The features which merge on the right hand side are closer to each other, suggesting that potentially only one of both would be sufficient for training the model. This could be useful to reduce the colinearity of different features.","369a381a":"**Check Test Set**","69cc3d8e":"---\n\n## Data Analysis\n\nWe start our analysis by importing required libraries:","bb53ac9b":"OK, this looks good so far. Lets start with visualizing the data to see flaws in the data. We start by plotting our most improtant variables '% Iron Concentrate' and '% Silica Concentrate':","c8e78a5f":"**Check Averaged Train Set**","c47b0d17":"These settings can be tuned a little bit to improve performance...","6b9c210d":"## Averaging dataset to account for differently sampled features","8bcaa7a4":"Good. Let's look at the first couple of rows in our dataframe:","2de64e80":"**Check Train Set**","2518d6f9":"**Feature importance**\n\nLet's look at the importance of each feature and plot the 10 most important features:[](http:\/\/)","28fc4daa":"Based on the documentation of the dataset, some columns are sampled every 20 seconds, some every hour. For instance, the feature 'Ore Pulp Flow' changes continously during the process while the features '% Iron Feed' and '% Silica Feed' are sample only every hour. Thus, I think it is not really helpful to use every row (sampled every 20s) including the less sampled features (held constant over the duration of one hour), since this assumes that every row is an individual observation - which it isnt. Using all samples to train our model does not really represent the reality. What we can try to do is to mean the rows (observations) for every hour and create a new dataframe which uses the average of the 20s samples. This however, will strongly reduce our data size (by factor 180). What we can do to not lose all information of the 20s sampled features, is to also include their variations during one hour (e.g. by calculating also the standard deviation of the meaned columns)."}}