{"cell_type":{"0e5218f0":"code","b89eae4b":"code","a1aed52c":"code","968326d2":"code","1a1274dd":"code","ed549260":"code","27e817fc":"code","65d3cd69":"code","a0d9e33a":"code","214f033a":"markdown","20c2f075":"markdown"},"source":{"0e5218f0":"# Pandas for csv processing\nimport pandas as pd\n\n# Natural Language Toolkit (NLTK) is a state-of-the-art solution to handle text data\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# We'll vectorize our input (turn textual data into numerical form) with\n# TF-IDF model implemented in Sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Importing Numpy just to find average CV score\nimport numpy as np\n\n# We'll use Random Forest as classifier and F1 along with ROC AUC as model quality score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import f1_score, make_scorer, roc_auc_score","b89eae4b":"# Reading and viewing our data\n# Most of the features are textual, but Telecomunication and Comnpany_Logo are boolean\ndata = pd.read_csv('..\/input\/job-fraud-detection\/Job_Frauds.csv', encoding = \"ISO-8859-1\")\nlen_data = len(data)\ndata","a1aed52c":"# Our dataset is highly imbalanced as we have the ratio\n# of legal and fraudulent jobs ~17:1\ndata.Fraudulent.value_counts()","968326d2":"# Data description says that we have many missing values among the data,\n# so we'll drop the features that have 50% or more values missing,\n# others' NAs will be filled with mode\nna_cols = []\n\nfor col in data.columns:\n    na_rate = data[col].isna().sum() \/ len_data\n    print(f\"Column {col} has {round(na_rate, 2) * 100}% of missing values\")\n    if na_rate > 0.5:\n        na_cols.append(col)\n        \nprint(f\"Columns {na_cols} have more than 50% of missing values and will be dropped\")\ndata = data.drop(columns=na_cols)\n\nfor col in data.drop(columns=['Fraudulent']).columns:\n    data[col] = data[col].fillna(data[col].mode().iloc[0])","1a1274dd":"# Final look of our data; two features are dropped as they have too many NAs,\n# others are filled with their modes\ndata","ed549260":"# Let's perform the most important task - the text preprocessing\ndata_inline = pd.DataFrame()\n\nstop_words = stopwords.words('english') # We define a set of stopwords...\nsbs = SnowballStemmer(language='english') # ...and a stemmer to delete word endings\n\ndescripts = [] # List of final lines of job descriptions\n\n# Each entry in dataset will be represented as a single line of concatenated\n# textual features and Telecomunication and Comnpany_Logo flags\nfor i in range(len(data)):\n    line = ''\n    for col in data.drop(columns=['Telecomunication', 'Comnpany_Logo', 'Fraudulent']).columns:\n        clear_text = ''\n        cell_text = str(data.iloc[i][col])\n        \n        # Tokenizing, stopwords filtering and stemming\n        cell_tokens = word_tokenize(cell_text)\n        for token in cell_tokens:\n            if token.lower() not in stop_words:                    \n                clear_text += sbs.stem(token) + \" \"\n        \n        line += clear_text + \" \"\n        \n    descripts.append(line) \n    \ndata_inline['Description'] = descripts\ndata_inline['Telecomunication'] = data.Telecomunication\ndata_inline['Conmpany_Logo'] = data.Comnpany_Logo\ndata_inline['Fraudulent'] = data.Fraudulent\n\ndata_inline","27e817fc":"# Performing TF-IDF vectoring to represent text as a matrix of coefficients\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(data_inline.Description.values)\nY = data_inline.Fraudulent","65d3cd69":"# We have quite big number of columns as there are many unique words in the dataset;\n# but hopefully it won't stop our classifier from being efficient\nX.shape","a0d9e33a":"# Training and cross-validating of random forest with balanced weights that'll help us cope with class imbalance\nrfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n\ncv_rfc = cross_validate(rfc, X, Y, cv=StratifiedKFold(random_state=42, shuffle=True), scoring=['f1_weighted', 'roc_auc'],\n                       n_jobs=-1)\nprint(f\"Average Random Forest F1 on CV is {round(np.mean(cv_rfc['test_f1_weighted']), 4)}\")\nprint(f\"Average Random Forest AUC on CV is {round(np.mean(cv_rfc['test_roc_auc']), 4)}\")","214f033a":"**Greetings!**\n\nIn this notebook, I'll construct a simple text data preprocessor and classify job offers to fraudulent and legal using Sklearn's Random Forest. Let's go!","20c2f075":"We have reached almost 1.0 of F1 and AUC, but I bet this solution can be further enhanced. We can play around with vectoring and classification methods and even use recurrent neural networks that are proven to be outstanding in NLP tasks.\n\nThanks for your attention. Feel free to discuss or enhance my solution :)"}}