{"cell_type":{"41f0f842":"code","1ce9ae77":"code","cbc09cd4":"code","6e88ddd0":"code","51cab452":"code","8a031ea1":"code","8d08d4f0":"code","4e811d11":"code","9e7ba206":"code","8135b4dd":"code","a261010e":"code","a7077038":"code","b8cbf9eb":"code","65b06838":"code","6475a5c4":"code","6a83bb46":"code","ca5d5645":"code","bba792ff":"code","13cf382c":"code","18037a72":"code","d0a1b413":"code","6fea7004":"code","7bbfa6b8":"code","888ab984":"code","6e2b4f50":"code","392fec6b":"code","f506e331":"code","be63938e":"code","cb6f5cf4":"code","1c8ddce6":"code","f60d14a8":"code","c69d1e0a":"code","cfd100d1":"code","dffc415a":"code","a5a9e6a0":"code","12182731":"code","7b1b615a":"code","0428fb86":"code","3da30a16":"code","306df69b":"code","7becf211":"code","3cd5a42a":"code","cbdaf950":"markdown","77588c90":"markdown","c4e7fe62":"markdown","35a1f479":"markdown","4b1ada2b":"markdown","6dc7782b":"markdown","b82ee60e":"markdown","3f2eff6f":"markdown"},"source":{"41f0f842":"# Data Manipulation\nimport numpy as np\nimport pandas as pd \n# EDA\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n# Data Preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder \nfrom sklearn.model_selection import train_test_split,GridSearchCV\n# Model Building \n# Simple Regression models\nfrom sklearn.linear_model import LinearRegression \n# Decision tree based models\nfrom sklearn.tree import DecisionTreeRegressor \n# Ensemble based models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n# XGBoost models\nfrom xgboost import XGBRegressor\n# To evaluate the models \nfrom sklearn.metrics import r2_score, mean_absolute_error\n# Other modules \nimport os\nfrom IPython.display import display\n# Ignoring all warnings \nimport warnings\nwarnings.simplefilter(action='ignore')\n# Using the seaborn style of matplotlib\nplt.style.use('seaborn')","1ce9ae77":"BASE_PATH = '\/kaggle\/input\/house-rent-prices-of-metropolitan-cities-in-india'\nTEST_SIZE = 0.3 \nRANDOM_STATE = 42\nFIGSIZE = (16,6)\nCITIES = ['Ahmedabad','Bangalore','Chennai','Delhi','Hyderabad','Kolkata','Mumbai','Pune']\nNUMERICAL_NROWS = 2\nNUMERICAL_NCOLS = 2\nPIE_FIGSIZE = (20,15)\nMULTICOL_FIGSIZE=(16,8)\nEVALUATION_NROWS = 1\nEVALUATION_NCOLS = 2 \nCITY_NROWS = 4 \nCITY_NCOLS = 2\nMODEL_EVALUATE_FIGSIZE=(16,16)\nHYPERPARAMETER_TUNING_FIGSIZE = (20,20)","cbc09cd4":"all_data = pd.read_csv(os.path.join(BASE_PATH, '_All_Cities_Cleaned.csv'))","6e88ddd0":"df_dict = dict(zip(CITIES, [all_data[all_data['city'] == city].copy() for city in CITIES]))","51cab452":"# Checking for columns having null values\nfor city, df in df_dict.items():\n    print(f\"{city} -> {df_dict[city].isnull().sum()[df_dict[city].isnull().sum()>0]}\")","8a031ea1":"clean_dict = {} #for EDA\npreprocessed_dict = {} #for model building\n# for storing encoders while preprocessing \nlocality_encoder_dict = {}\nfurnish_type_encoder_dict = {}\nseller_type_encoder_dict = {}\nlayout_type_encoder_dict = {}\nproperty_type_encoder_dict = {}\ndef preprocess(df_dict):\n    \"\"\"\n    Cleans and preprocesses the data for future use \n    Cleaning:\n        - Removes duplicate rows (if they exist)\n        - Removes the city column as the dataframes are already separated \n        - Removes outliers on the basis of price column using lower limit as (Q1-1.5*IQR) and \n        upper limit as (Q3+1.5*IQR)\n    Preprocessing:\n        - Uses label encoding for locality column as order does not matter in locality\n        - Uses ordianal encoding for furnish_type, layout_type, seller_type and property_type as the \n        order matters in these columns for predicting the rent prices\n    Args:\n        df_dict - the dictionary containing cities as keys and the dataframe corresponding to them as values \n    Returns:\n        None \n    \"\"\"\n    for city, df in df_dict.items():\n        # Cleaning the data \n        # Dropping the city column\n        df.drop(['city'], axis=1, inplace=True)\n        # Dropping duplicate rows (if they exist)\n        df.drop_duplicates(inplace=True)\n        # Removing outliers \n        desc = df['price'].describe()\n        q1 = desc.loc['25%']\n        q3 = desc.loc['75%']\n        iqr = q3-q1 \n        lower_lim = q1-(1.5*iqr)\n        upper_lim = q3+(1.5*iqr)\n        df = df[(df['price']>=lower_lim)&(df['price']<=upper_lim)]\n        # since the data is cleaned now, we can store it in clean_dict \n        clean_df = df.copy()\n        clean_dict[city] = clean_df\n        # Preprocessing the data \n        locality_encoder = LabelEncoder()\n        df['locality'] = locality_encoder.fit_transform(df['locality'])\n        locality_encoder_dict[city] = locality_encoder\n        \n        ordinal_encoder_cols = ['seller_type','layout_type','property_type','furnish_type']        \n        ord_enc_dict = {\n            'seller_type':seller_type_encoder_dict,\n            'layout_type':layout_type_encoder_dict,\n            'property_type':property_type_encoder_dict,\n            'furnish_type':furnish_type_encoder_dict\n        }\n        for col in ordinal_encoder_cols:\n            cat = [df.groupby(by=[col])['price'].mean().sort_values(ascending=True).index]\n            col_encoder = OrdinalEncoder(categories=cat)\n            df[col] = col_encoder.fit_transform(df[[col]])\n            ord_enc_dict[col][city] = col_encoder\n        preprocessed_dict[city] = df\n        ","8d08d4f0":"preprocess(df_dict)","4e811d11":"print(f\"Seller Type Encoder -> {seller_type_encoder_dict['Mumbai'].categories_}\")\nprint(f\"Furnish Type Encoder -> {furnish_type_encoder_dict['Mumbai'].categories_}\")\nprint(f\"Property Type Encoder -> {property_type_encoder_dict['Mumbai'].categories_}\")\nprint(f\"Layout Type Encoder -> {layout_type_encoder_dict['Mumbai'].categories_}\")","9e7ba206":"for city, df in clean_dict.items():\n    df['city'] = city\n    df['affordability'] = df['area']\/df['price']","8135b4dd":"combined = pd.concat([df for df in clean_dict.values()])","a261010e":"# Overall Analysis\nfig, ax = plt.subplots(figsize=FIGSIZE)\nsns.countplot(x=combined['city'],ax=ax,order=combined['city'].value_counts().index)\nax.set_xlabel('CITY')\nax.set_ylabel('NUMBER OF HOUSES')\nax.set_title('NUMBER OF HOUSES RENTED IN EACH CITY')\nplt.show()","a7077038":"overall_analysis_cols = ['price','area','affordability']\nfor col in overall_analysis_cols:\n    overall_sort_df = combined.groupby(by=['city'])[col].mean().sort_values(ascending=False)\n    fig, ax = plt.subplots(figsize=(16,6))\n    sns.barplot(x=overall_sort_df.index, y=overall_sort_df.values,ax=ax)\n    ax.set_xlabel(city.upper())\n    ax.set_ylabel(col.upper())\n    ax.set_title(f'AVERAGE {col.upper()} OF HOUSES')\n    plt.show()","b8cbf9eb":"# create list of columns to be analyzed as pie chart for each dataframe\npie_cols = np.array(['seller_type','layout_type','property_type','furnish_type']).reshape(NUMERICAL_NROWS,NUMERICAL_NCOLS)\nfor city, df in clean_dict.items():\n    if not os.path.exists(os.path.join('outputs',city)): #check if the path exists \n        os.makedirs(os.path.join('outputs',city)) #if not, make the path \n    fig, ax = plt.subplots(figsize=PIE_FIGSIZE,nrows=NUMERICAL_NROWS, ncols=NUMERICAL_NCOLS) #create a fig with 2 rows and 2 cols \n    for i in range(NUMERICAL_NROWS): #loop through the rows \n        for j in range(NUMERICAL_NCOLS): #loop through columns   \n            ax[i,j].pie(x=df[pie_cols[i,j]].value_counts()) #plot the pie chart \n            text = pd.DataFrame(df[pie_cols[i,j]].value_counts().apply(lambda x: f'{np.round((x\/df.shape[0])*100,2)}%')) \n            # create the text to display on pie chart \n            text.index = text.index.str.upper() #convert text to upper case \n            text = text.to_string() #convert text to string \n            ax[i,j].text(1,0,text) #display text on pie chart\n            ax[i,j].set_title(f'{pie_cols[i,j]} IN {city}') #set the title \n    plt.show() #show the figure ","65b06838":"# create an array for analyzing numerical columns \nnumerical_cols = np.array([\n    ['price','area'],\n    ['bedroom','bathroom']\n]).reshape(NUMERICAL_NROWS,NUMERICAL_NCOLS)\n\nfor city, df in clean_dict.items():\n    fig, ax = plt.subplots(figsize=MULTICOL_FIGSIZE,nrows=2,ncols=2) #create a matplotlib figure \n    for i in range(NUMERICAL_NROWS): #loop through the rows \n        for j in range(NUMERICAL_NCOLS): #loop through the columns \n            if i==0: # if it is the 1st row \n                sns.histplot(df[numerical_cols[i,j]],ax=ax[i,j],kde=True) #plot the distribution of column of ith row and jth column\n                ax[i,j].set_title(f'DISTRIBUTION OF {numerical_cols[i,j]} IN {city}') # set the title \n                ax[i,j].set_xlabel(numerical_cols[i,j]) #set the xlabel \n                ax[i,j].set_ylabel('NUMBER OF HOUSES') #set the ylabel\n            if i==1: #if it is the second row \n                sns.countplot(x=df[numerical_cols[i,j]],ax=ax[i,j]) #plot the countplot of column of ith row and jth column \n                ax[i,j].set_title(f'NUMBER OF {numerical_cols[i,j]} IN THE HOUSES IN {city}') #set the title \n                ax[i,j].set_xlabel(numerical_cols[i,j]) #set the xlabel\n                ax[i,j].set_ylabel('NUMBER OF HOUSES') #set the ylabel\n    plt.tight_layout() #apply tight layout to prevent overlap of columns  \n    plt.show() #show the figure ","6475a5c4":"for city, df in clean_dict.items(): \n    affordable = df.groupby(by=['locality'])['affordability'].mean() #calculate mean area for each locality \n    most_affordable = affordable.sort_values(ascending=False)[:10] #sort in ascending order for most spacious \n    least_affordable = affordable.sort_values(ascending=True)[:10] #sort in descending order for least spacious \n    fig, ax = plt.subplots(figsize=MULTICOL_FIGSIZE,nrows=1,ncols=2) #create figure with 1 row and 2 cols \n    sns.barplot(x=least_affordable.index, y=least_affordable, ax=ax[0], order=least_affordable.index[::-1]) #plot least spacious on 1st col \n    ax[0].set_title(f'LEAST AFFORDABLE LOCALITIES IN {city.upper()}') #set title \n    ax[0].set_xlabel('LOCALITY') #set xlabel \n    ax[0].set_ylabel('AVERAGE AFFORDABLITY') #set ylabel\n    ax[0].tick_params(axis='x',labelrotation=90) #rotate the labels on x axis by 90 degrees for readibility\n    sns.barplot(x=most_affordable.index, y=most_affordable,ax=ax[1],order=most_affordable.index[::-1])#plot least affordable localities in 2nd column \n    ax[1].set_title(f'MOST AFFORDABLE LOCALITIES IN {city.upper()}') \n    ax[1].set_xlabel('LOCALITY') #set xlabel \n    ax[1].set_ylabel('AVERAGE AFFORDABLITY') #set ylabel\n    ax[1].tick_params(axis='x',labelrotation=90) #rotate the labels on x axis by 90 degrees for readibility\n    plt.tight_layout() #apply tight layout for no overlap  \n    plt.show() #show the figure ","6a83bb46":"for city, df in clean_dict.items():\n    spacious = df.groupby(by=['locality'])['area'].mean() #calculate mean area for each locality \n    most_spacious = spacious.sort_values(ascending=False)[:10] #sort in ascending order for most spacious \n    least_spacious = spacious.sort_values(ascending=True)[:10] #sort in descending order for least spacious \n    fig, ax = plt.subplots(figsize=MULTICOL_FIGSIZE,nrows=1,ncols=2) #create figure with 1 row and 2 cols \n    sns.barplot(x=least_spacious.index, y=least_spacious, ax=ax[0]) #plot least spacious on 1st col \n    ax[0].set_title(f'LEAST SPACIOUS LOCALITIES IN {city.upper()}') #set title \n    ax[0].set_xlabel('LOCALITY') #set xlabel \n    ax[0].set_ylabel('AVERAGE AREA IN SQUARE FEET') #set ylabel\n    ax[0].tick_params(axis='x',labelrotation=90) #rotate the labels on x axis by 90 degrees for readibility\n    sns.barplot(x=most_spacious.index, y=most_spacious,ax=ax[1])#plot least affordable localities in 2nd column \n    ax[1].set_title(f'MOST SPACIOUS LOCALITIES IN {city.upper()}') \n    ax[1].set_xlabel('LOCALITY') #set xlabel \n    ax[1].set_ylabel('AVERAGE AREA IN SQUARE FEET') #set ylabel\n    ax[1].tick_params(axis='x',labelrotation=90) #rotate the labels on x axis by 90 degrees for readibility\n    plt.tight_layout() #apply tight layout for no overlap \n    plt.show() #show the plot ","ca5d5645":"# Create dictionaries to store the train and test sets \nX_train_dict = {}\ny_train_dict = {}\nX_test_dict = {}\ny_test_dict = {}\n# Create dictionaries to store metrics\nr2_dict = {}\nmae_dict = {}\n\nfor city, df in preprocessed_dict.items():\n    # Separate X and y\n    X = df.drop(['price'],axis=1)\n    y = df['price']\n    # Do train_test_split and store the results in dictionaries\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=TEST_SIZE, random_state=RANDOM_STATE)\n    X_train_dict[city] = X_train\n    y_train_dict[city] = y_train\n    X_test_dict[city] = X_test\n    y_test_dict[city] = y_test","bba792ff":"def summarize_model(model):\n    \"\"\"Evaluates the models on the basis of r2_score and mean absolute error\n    \n    Keyword arguments:\n    model -- A sklearn model which you would like to evaluate\n    Return: a dictionary contraining the metrics\n    \"\"\"\n    model_r2_train_dict = {} #create a dictionary for r2 score of train set\n    model_r2_test_dict = {} #create a dictionary for r2 score of test set\n    model_mae_train_dict = {} #create a dictionary for mae of train set \n    model_mae_test_dict = {} #create a dictionary for mae of test set\n    model_dict = {} #create a dictionary of models \n    for city in CITIES: #loop through the cities \n        model.fit(X_train_dict[city],y_train_dict[city]) #fit the model \n        model_dict[city] = model #store the model for that city in model dict\n        model_train_preds = model.predict(X_train_dict[city]) #predict for train set \n        model_test_preds = model.predict(X_test_dict[city]) #predict for test set \n        model_r2_train_dict[city] = r2_score(y_true=y_train_dict[city], y_pred=model_train_preds) #calculate r2 for train set \n        model_r2_test_dict[city] = r2_score(y_true=y_test_dict[city], y_pred=model_test_preds) #calculate r2 for test set \n        model_mae_train_dict[city] = mean_absolute_error(y_true=y_train_dict[city],y_pred=model_train_preds) #calcaulte mae for train set\n        model_mae_test_dict[city] = mean_absolute_error(y_true=y_test_dict[city], y_pred=model_test_preds) #calculate mae for test set \n    # store the metrics in a dictionary \n    metrics_dict = {\n        'train r2':model_r2_train_dict,\n        'test r2':model_r2_test_dict,\n        'train mae':model_mae_train_dict,\n        'test mae':model_mae_test_dict\n    }\n    return model_dict, metrics_dict #return model and metrics ","13cf382c":"# Linear Regression \nlr_dict, lr_metrics = summarize_model(LinearRegression())\ndisplay(pd.DataFrame(lr_metrics))","18037a72":"# Decision tree regression \ndt_dict, dt_metrics = summarize_model(DecisionTreeRegressor())\ndisplay(pd.DataFrame(dt_metrics))","d0a1b413":"# Random forest regression  \nrf_dict, rf_metrics = summarize_model(RandomForestRegressor())\ndisplay(pd.DataFrame(rf_metrics))","6fea7004":"# Adaboost Regression\nadaboost_dict, adaboost_metrics = summarize_model(AdaBoostRegressor())\ndisplay(pd.DataFrame(adaboost_metrics))","7bbfa6b8":"# Gradient boosting regression \ngb_dict, gb_metrics = summarize_model(GradientBoostingRegressor())\ndisplay(pd.DataFrame(gb_metrics))","888ab984":"# XGBoost Regression \nxgb_dict, xgb_metrics = summarize_model(XGBRegressor(objective='reg:squarederror'))\ndisplay(pd.DataFrame(xgb_metrics))","6e2b4f50":"# create dictionaries for visualization \nmodel_dict = {\n    'LR':lr_dict,\n    'DT':dt_dict,\n    'RF':rf_dict,\n    'ADA':adaboost_dict,\n    'GB':gb_dict,\n    'XGB':xgb_dict\n}\nmodel_metrics = {\n    'LR':lr_metrics,\n    'DT':dt_metrics,\n    'RF':rf_metrics,\n    'ADA':adaboost_metrics,\n    'GB':gb_metrics,\n    'XGB':xgb_metrics\n}","392fec6b":"# Create dictionaries for metrics for all models \ntrain_r2 = {}\ntest_r2 = {}\ntrain_mae = {}\ntest_mae = {}\n\nfor model, metrics_dict in model_metrics.items():\n    train_r2[model] = metrics_dict['train r2']\n    test_r2[model] = metrics_dict['test r2']\n    train_mae[model] = metrics_dict['train mae']\n    test_mae[model] = metrics_dict['test mae']","f506e331":"# Convert them to dataframes \ntrain_r2 = pd.DataFrame(train_r2)\ntest_r2 = pd.DataFrame(test_r2)\ntrain_mae = pd.DataFrame(train_mae)\ntest_mae = pd.DataFrame(test_mae)","be63938e":"# Change the column names for identification \ntrain_r2.columns = [col+'_train_r2' for col in train_r2.columns]\ntest_r2.columns = [col+'_test_r2' for col in test_r2.columns] \ntrain_mae.columns = [col+'_train_mae' for col in train_mae.columns]\ntest_mae.columns = [col+'_test_mae' for col in test_mae.columns]","cb6f5cf4":"# Create dataframe for plotting r2 score \nplot_r2 = pd.DataFrame()\nfor c1, c2 in zip(train_r2.columns, test_r2.columns):\n    plot_r2[c1] = train_r2[c1]\n    plot_r2[c2] = test_r2[c2]","1c8ddce6":"# create dataframe for plotting mae \nplot_mae = pd.DataFrame()\nfor c1,c2 in zip(train_mae.columns, test_mae.columns):\n    plot_mae[c1] = train_mae[c1]\n    plot_mae[c2] = test_mae[c2]","f60d14a8":"# transpose the dataframes for easy plotting \nplot_r2 = plot_r2.T\nplot_mae = plot_mae.T","c69d1e0a":"# Reshape cities to plot better \ncities_plot = np.array(CITIES).reshape(CITY_NROWS,CITY_NCOLS)","cfd100d1":"fig, ax = plt.subplots(figsize=MODEL_EVALUATE_FIGSIZE,nrows=CITY_NROWS,ncols=CITY_NCOLS) #create a 4*2 figure \nfor i in range(CITY_NROWS): #loop through rows \n    for j in range(CITY_NCOLS): #loop through columns \n        plot_r2[cities_plot[i,j]].plot(kind='bar',ax=ax[i,j]) #plot the column  \n        ax[i,j].set_title(cities_plot[i,j]) #set title \nplt.tight_layout() #tight layout for avoiding overlap  \nplt.show() #show the plot ","dffc415a":"fig, ax = plt.subplots(figsize=MODEL_EVALUATE_FIGSIZE,nrows=CITY_NROWS,ncols=CITY_NCOLS) #create 4*2 figure \nfor i in range(CITY_NROWS): #loop through rows \n    for j in range(CITY_NCOLS): #loop through columns \n        plot_mae[cities_plot[i,j]].plot(kind='bar',ax=ax[i,j]) #plot the column \n        ax[i,j].set_title(cities_plot[i,j]) #set the title \nplt.tight_layout() #tight layout to avoid overlap \nplt.show() #show the figure ","a5a9e6a0":"# hyperparameter tuning \nparam_tuning = {\n        'learning_rate': [0.001,0.01,0.1],\n        'max_depth': [3, 5, 7],\n        'n_estimators' : [100, 200,300],\n    }","12182731":"# perform cross validation\nxgb_validated_dict = {} #create a dictionary for validated xgboost model \nfor city, model in xgb_dict.items(): #loop though  existing xgboost model \n    xgb_validated = GridSearchCV(model, param_tuning, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error') #perform cross validation\n    xgb_validated.fit(X_train_dict[city], y_train_dict[city]) #fit the validated model \n    xgb_validated_dict[city] = xgb_validated #save the model in the dictionary ","7b1b615a":"# create a dictionary for metrics \nxgb_validated_metrics = {}\ntrain_r2_dict = {}\ntest_r2_dict = {}\ntrain_mae_dict = {}\ntest_mae_dict = {}\nfor city, model in xgb_validated_dict.items(): #loop through validated xgboost models \n    train_preds = model.predict(X_train_dict[city]) #predict on train set \n    test_preds = model.predict(X_test_dict[city]) #predict on test set \n    train_r2 = r2_score(y_true=y_train_dict[city],y_pred=train_preds) #calculate r2 on train set \n    test_r2 = r2_score(y_true=y_test_dict[city],y_pred=test_preds) #calculate r2 on test set \n    train_mae = mean_absolute_error(y_true=y_train_dict[city], y_pred=train_preds) #calculate mae on train set \n    test_mae = mean_absolute_error(y_true=y_test_dict[city],y_pred=test_preds) #calculate mae on test set \n    # store metrics in respective dictionaries \n    train_r2_dict[city] = train_r2 \n    test_r2_dict[city] = test_r2\n    train_mae_dict[city] = train_mae\n    test_mae_dict[city] = test_mae\n# create dictionary for storing metrics \nxgb_validated_metrics = {\n    'train r2':train_r2_dict,\n    'test r2':test_r2_dict,\n    'train mae': train_mae_dict,\n    'test mae':test_mae_dict\n}","0428fb86":"# convert metrics of xgboost and validated model to dataframe \nxgb_metrics_df = pd.DataFrame(xgb_metrics)\nxgb_validated_metrics_df = pd.DataFrame(xgb_validated_metrics)","3da30a16":"# rename columns for easy interpretation \nxgb_metrics_df.columns = [col + '_xgb' for col in xgb_metrics_df.columns]\nxgb_validated_metrics_df.columns = [col + '_xgb_val' for col in xgb_validated_metrics_df.columns]","306df69b":"# create a final metrics dataframe for plotting \nfinal_metrics_df = pd.DataFrame()\nfor c1, c2 in zip(xgb_metrics_df.columns, xgb_validated_metrics_df.columns):\n    final_metrics_df[c1] = xgb_metrics_df[c1]\n    final_metrics_df[c2] = xgb_validated_metrics_df[c2]","7becf211":"fig, ax = plt.subplots(figsize=HYPERPARAMETER_TUNING_FIGSIZE,nrows=NUMERICAL_NROWS, ncols=NUMERICAL_NCOLS) #create a 2*2 figure \nfinal_metrics_df[['train r2_xgb', 'train r2_xgb_val']].plot(kind='bar',ax=ax[0,0],title='TRAIN R2 SCORE') #plot train r2 for both models \nfinal_metrics_df[['test r2_xgb', 'test r2_xgb_val']].plot(kind='bar',ax=ax[0,1], title='TEST R2 SCORE') #plot test r2 for both models \nfinal_metrics_df[['train mae_xgb', 'train mae_xgb_val']].plot(kind='bar', ax=ax[1,0], title='TRAIN MAE') #plot train mae for both models \nfinal_metrics_df[['test mae_xgb', 'test mae_xgb_val']].plot(kind='bar', ax=ax[1,1],title='TEST MAE') #plot test mae for both models \nplt.tight_layout() #tight layout to avoid overlap \nplt.show() #show the figure ","3cd5a42a":"# Results for the best model\ndisplay(xgb_metrics_df)","cbdaf950":"# Preprocessing","77588c90":"# Model Building","c4e7fe62":"# Exploratory Data Analysis","35a1f479":"From the above graphs, we can say that XGBRegressor() has the highest r2 score for all the cities and the lowest MAE, hence we will validate the XGBRegressor() model for GridSearchCV()","4b1ada2b":"# Constants ","6dc7782b":"Here I have defined 2 new columns \n\n* city column - To analyze the data as a whole \n* affordability column - I have defined affordability as the area per unit price, more affordable houses offer more area per unit price\n\n$\naffordability = \\frac{area}{price}\n$","b82ee60e":"Since there is not much improvement after hyperparameter tuning, hence we can conclude that XGBRegressor is the best model for almost all the cities","3f2eff6f":"# Imports"}}