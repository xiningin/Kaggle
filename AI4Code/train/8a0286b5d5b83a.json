{"cell_type":{"38cfcdce":"code","3aa9c3c7":"code","1bc5d27f":"code","cb9e57ee":"code","819dc1b4":"code","0590e1f7":"code","3292294e":"code","29904082":"code","7b11d959":"code","3870bfaa":"code","9b83288f":"code","722a2fe5":"code","b90d970c":"code","84ac0650":"code","8f40482e":"code","1ee0f639":"code","cc0ee4be":"code","4b0a26da":"code","0eb1e363":"code","085105f7":"code","86828240":"code","ebc29e8f":"markdown","256bc380":"markdown","0d610c0a":"markdown","d749be8c":"markdown","24c1bffd":"markdown","1d08b924":"markdown","59ba845b":"markdown","ffee2b1b":"markdown","87d9d44b":"markdown","35109623":"markdown","4cbb1a10":"markdown","357e6b61":"markdown","0381430c":"markdown","e3677ef4":"markdown","9b18eed7":"markdown","f5057ef7":"markdown","22810ccb":"markdown","03afffde":"markdown","3bec7818":"markdown","ee65fc80":"markdown","44d243f1":"markdown","e2f5d9e9":"markdown","9516d670":"markdown","4ba549f8":"markdown","df28c8ea":"markdown","edccd6a8":"markdown","78651ba7":"markdown","68e95e35":"markdown"},"source":{"38cfcdce":"## for data\nimport json\nimport pandas as pd\nimport numpy as np\nimport gzip\n\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## for processing\nimport re\nimport nltk as nlp\n\n## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing,metrics\n\n## for explainer\nfrom lime import lime_text\n\n## for word embedding\nimport gensim\nimport gensim.downloader as gensim_api\n\n## for deep learning\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom tensorflow.keras import backend as K\n\n## for bert language model\nimport transformers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.display import clear_output\nclear_output(wait=True)","3aa9c3c7":"# downloading the dataset\n\n!wget \"http:\/\/deepyeti.ucsd.edu\/jianmo\/amazon\/categoryFilesSmall\/Video_Games_5.json.gz\"\n\ndef parse(path):\n  g = gzip.open(path, 'rb')\n  for l in g:\n    yield json.loads(l)\n\ndef getDF(path):\n  i = 0\n  df = {}\n  for d in parse(path):\n    df[i] = d\n    i += 1\n  return pd.DataFrame.from_dict(df, orient='index')\n\n# transforming the dataset into a dataframe\n\nDS = getDF(\".\/Video_Games_5.json.gz\")","1bc5d27f":"# display the first five rows of the dataset\n\nDS.head()","cb9e57ee":"df = DS.copy()","819dc1b4":"# delete all columns except overall and review_Text\ndf.drop(['image','style','vote','reviewerName','verified','reviewTime','reviewerID','asin','summary','unixReviewTime'],inplace=True,axis=1)\n\n# rename these 2 columns\ndf.rename(columns={\"overall\": \"score\", \"reviewText\": \"review\" },inplace=True)\n\n# remove duplicates\ndf.drop_duplicates(inplace=True)\n\n# remove NAN values and duplicates\ndf.dropna(inplace = True)\n\n# cast score from float to string\ndf[\"score\"] = df[\"score\"].apply(lambda x: str(int(x)))","0590e1f7":"from wordcloud import WordCloud, STOPWORDS\n\ntext = DS['reviewText'].values \nwordcloud = WordCloud().generate(str(text))\n\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","3292294e":"categ = list(range(1,6))\nfrequence = df[\"score\"].value_counts().sort_index().tolist()","29904082":"plt.figure(figsize=(15,10))\nplt.ylabel('Fr\u00e9quence', fontsize=12)\nplt.xlabel('Cat\u00e9gorie', fontsize=12)\nsns.barplot(categ, frequence, color='cadetblue')\nplt.title('Distribution des score')","7b11d959":"# due to classes unbalance that tend models to predict a dominant category  \n# over the other we decided to take the same amount of data from each category \n# to regularize the generalization of the model\n\ndf_1 = df[df['score'] == \"1\"][:1000]\ndf_2 = df[df['score'] == \"2\"][:1000]\ndf_3 = df[df['score'] == \"3\"][:1000]\ndf_4 = df[df['score'] == \"4\"][:1000]\ndf_5 = df[df['score'] == \"5\"][:1000]\nx = [df_1, df_2, df_3,df_4,df_5]\ndf = pd.concat(x)","3870bfaa":"# preprocessing texts using a function which contains the porter stemmer for stemming \n# and WordNetLemmatizer for lemmatizing\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ni = 0\ndef preprocess_text(text):\n    chars=\"\\n,.\"\n    for char in chars:\n        text = text.replace(char,\" \")\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n    lst_text = text.split()\n    lst_stopwords = stopwords.words(\"english\")\n    lst_text = [word for word in lst_text if word not in lst_stopwords]\n    ps = PorterStemmer()\n    lst_text = [ps.stem(word) for word in lst_text]\n    lem = WordNetLemmatizer()\n    lst_text = [lem.lemmatize(word) for word in lst_text]\n    ############\n    clear_output()\n    global i\n    i+=1\n    print(f'{i} \/ {df.shape[0]}')\n    ###############\n    text = \" \".join(lst_text)\n    return text\n\n\ndf[\"review\"] = df[\"review\"].apply(preprocess_text)  ","9b83288f":"categorie = list(range(1,6))\nfreq = df[\"score\"].value_counts().sort_index().tolist()","722a2fe5":"# Visualisation of the categories frequency\n\nplt.figure(figsize=(15,10))\nplt.ylabel('Fr\u00e9quence', fontsize=12)\nplt.xlabel('Cat\u00e9gorie', fontsize=12)\nsns.barplot(categorie, freq, color='cadetblue')\nplt.title('Distribution des score')","b90d970c":"## split dataset : 30 % for test and 70% for training\n\ndf_train, df_test = model_selection.train_test_split(df, test_size=0.3)\n\n## get target\ny_train = df_train[\"score\"].values\ny_test = df_test[\"score\"].values","84ac0650":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n\nvectorizer.fit(df[\"review\"])\n\nX_train = vectorizer.transform(df_train[\"review\"])\n\nX_test = vectorizer.transform(df_test[\"review\"])","8f40482e":"corpus = df_train[\"review\"]\n\n## create a list of lists of unigrams\nlst_corpus = []\nfor string in corpus:\n   lst_words = string.split()\n   lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n   lst_corpus.append(lst_grams)\n\n## detect bigrams and trigrams\nbigrams_detector = gensim.models.phrases.Phrases(lst_corpus,delimiter=\" \".encode(), min_count=5, threshold=10)\nbigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\ntrigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus],delimiter=\" \".encode(), min_count=5, threshold=10)\ntrigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)","1ee0f639":"## fit w2v\n## window = 8 : get 8 words of context\n## sg : using skip-gram method\n\nnlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n            window=8, min_count=1, sg=1, iter=30)","cc0ee4be":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\npred = 0\n\ndef evaluate_model(classifieur, xtrain, ytrain, xtest, ytest):\n    score = []\n    for clf in classifieur:\n        clf.fit(xtrain, ytrain)\n        predicted = clf.predict(xtest)\n        acc = accuracy_score(ytest, predicted)*100\n        score.append(acc)\n        global pred\n        pred = predicted\n        print('Mod\u00e8le : %s\\n' % clf)\n        print('Testing accuracy of %s' % acc,'%')\n        print('Testing F1 score: {}'.format(f1_score(ytest, predicted, average='weighted')))   \n        print('---------------------------------------------------------------------------\\n')\n        \n    return score\n\n\nmodels = [SGDClassifier(), KNeighborsClassifier(), DecisionTreeClassifier(), LinearSVC(),BernoulliNB(), MultinomialNB(),LogisticRegression()]\nscore = evaluate_model(models, X_train, y_train , X_test, y_test)","4b0a26da":"## Plot confusion matrix\ncm = metrics.confusion_matrix(y_test, pred)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=categorie, \n       yticklabels=categorie, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","0eb1e363":"labels = ['SGD', 'KNN', 'LogisticRegression', 'DecisionTree', 'LinearSVC','BernoulliNB', 'MultinomialNB']\n\nfig=plt.figure(figsize=(10,6))\n\"\"\"\nplt.xlabel('classifiers')\nplt.ylabel('accuracy')\nplt.plot(labels, score)\nplt.title('classifiers')\nplt.show()\n\"\"\"\nax = fig.add_axes([0,0,1,1])\nax.bar(labels,score)\nplt.show()","085105f7":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntext_clf = Pipeline([('vect', vectorizer),\n                     ('clf', MultinomialNB()),\n                    ])","86828240":"from sklearn.model_selection import GridSearchCV\nparameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n                'clf__alpha': (1e-2, 1e-3),}\n\ngs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\ngs_clf = gs_clf.fit(X_train, y_train)","ebc29e8f":"- En comparent les differentes classifiers, on remarque qu'aucun modele a depas\u00e9 60% de precision,peu etre la raison  est l'utilisation d'une partie du jeux de donn\u00e9es (pour eviter le probleme du temps d'excution longue) donc les modeles ont pas bien appris a cause de manque de donn\u00e9e, la 2 eme raison peut etre les parametres par default des classifiers ou les caracteristques des vecteurs (td-idf) qui sont pas pertients , les modeles qui ont donn\u00e9 la meilleure valeur de precision sont Logistic Regression,MultinomialNB,Stochastic Gradient Descent respectivement,le KNN Aa donn\u00e9 la mauvaise precision par rapport aux autre classifieur.","256bc380":"# Conclusion and perspectives","0d610c0a":"# Spliting dataset to train and test","d749be8c":"# Transforming the dataset\nL'ensemble de donn\u00e9es se trouve dans un fichier json, donc on va d'abord le lire dans une liste de dictionnaires avec json, puis le transformer en un pandas Dataframe.","24c1bffd":"# Stemming + Lemmarization + removing Stopwords\n","1d08b924":"On remarque une confusion entre la cat\u00e9gorie 1 et 2 ","59ba845b":"## Visualisation of Data","ffee2b1b":"we will test few machine learning classifiers ","87d9d44b":"Durant ce projet nous avons pu d\u00e9couvrir le domaine du Naturel Language Processing et le Data mining, on a utilis\u00e9 toutes les phases d'un projet Machine learning de la pr\u00e9paration des donn\u00e9es jusqu'a la pr\u00e9diction avec le mod\u00e9le final, On aurait pu utiliser Bert comme m\u00e9thode d'extraction de donn\u00e9es, un mod\u00e9le puissant se basant sur les transformers, qui repr\u00e9sente l'etat de l'art en terme de traitement du langage naturel, faute de temps mais on va quand meme developper ce projet juste pour satisfaire notre curiosit\u00e9.","35109623":"On souhaiterait utiliser un GridSearch afin de trouver les bon hyperparametre  pour chaque modele et pour quoi pas un cross-validation pour la selection du mod\u00e9le","4cbb1a10":"Ce projet consiste \u00e0 analyser les donn\u00e9es de Amazon_Video_Games_Review , extraire les caract\u00e9ristiques textuelles selon plusieurs m\u00e9thodes et appliquer des mod\u00e8les de classification afin de retrouver les notes donn\u00e9es pour chaque commentaire.\n\nSommaire g\u00e9n\u00e9ral du projet : \n\n            1  -  Import et analyse exploratoire des donn\u00e9es\n            2  -  Pr\u00e9traitement des donn\u00e9es :\n                    \n                    - Stemming\n                    - Lemmatisation\n                    - Removing stop words\n\n            3 -  Extraction de caract\u00e9ristiques du texte avec:\n                    \n                    - TF-IDF               \t\n                    - Word2Vec\n                \n\n\n            4 -  Mod\u00e8les de classification\n                        \n                        - Stochastic Gradient Descent Classifier\n                        - KNeighborsClassifier\n                        - LogisticRegression\n                        - DecisionTree Classifier\n                        - LinearSVC\n                        - BernoulliNB\n                        - MultinomialNB\n\n            5  -  Visualisation\n                   \n            6  -  Discussion des r\u00e9sultats\n            \n            7  -  Conclusion et perspectives","357e6b61":"## Classe unbalancing\n\nwe noticed a classe unbalance where note 5 is in 70% of the dataset so to avoid that we took same amount of each classe","0381430c":"## Word Cloud","e3677ef4":"# Projet Data Mining : Text classification","9b18eed7":"# Importing libraires","f5057ef7":"# Visualisation","22810ccb":"# Word2Vec","03afffde":"# Feature Extraction Applying TFIDF","3bec7818":"# Preprocessing","ee65fc80":"# Subset of Dataset","44d243f1":"# Model selection \/ GridSearch Hyperparameters","e2f5d9e9":"# Results discussion","9516d670":"Le TF-IDF (en anglais term frequency-inverse document frequency) est une m\u00e9thode de pond\u00e9ration souvent utilis\u00e9e en recherche d'information et en particulier dans la fouille de textes. Cette mesure statistique permet d'\u00e9valuer l'importance d'un terme contenu dans un document, relativement \u00e0 une collection ou un corpus. Le poids augmente proportionnellement au nombre d'occurrences du mot dans le document. Il varie \u00e9galement en fonction de la fr\u00e9quence du mot dans le corpus. \n                                   \n                                    W(d, t) = TF(d, t) \u2217 log( N d f(t) )","4ba549f8":"* Stemming est le processus de r\u00e9duction de l'inflexion des mots \u00e0 leur forme racine, comme le mappage d'un groupe de mots sur la m\u00eame racine, m\u00eame si la racine elle-m\u00eame n'est pas un mot valide dans la langue.\n\n* Lemmatisation : contrairement au Stemming, elle r\u00e9duit correctement les mots fl\u00e9chis en s'assurant que le mot racine appartient \u00e0 la langue. Dans la lemmatisation, le mot racine est appel\u00e9 Lemme. Un lemme est la forme canonique, la forme du mot dans le dictionnaire ou la forme de citation d'un ensemble de mots.\n\n* Stop words: les mots vides sont des mots qui ne contiennent pas de signification importante \u00e0 utiliser dans les requ\u00eates de recherche. Habituellement, ces mots sont exclus des requ\u00eates de recherche car ils renvoient une grande quantit\u00e9 d'informations inutiles. Chaque langue donne sa propre liste de mots vides \u00e0 utiliser. voici un exemple des mots vides en la langue anglaise : as, the, be, are, etc.","df28c8ea":"## Histogram","edccd6a8":"Les mod\u00e8les en sac de mots a comme Tf-Idf  se concentre juste sur la repr\u00e9sentation syntaxique de mots (e.g. \u201cThis is good\u201d and \u201cIs this good\u201dont exactement le m\u00eame repr\u00e9sentation vectorielle).En revanche word embedding comme le word2vec apporte la signification s\u00e9mantique du mot par exemple  les mots \u201cairplane\u201d, \u201caeroplane\u201d, \u201cplane\u201d, and \u201caircraft\u201d sont souvent utilis\u00e9s dans le m\u00eame contexte.","78651ba7":"# Machine Learning","68e95e35":"## Visualisation after removing classe unbalancing"}}