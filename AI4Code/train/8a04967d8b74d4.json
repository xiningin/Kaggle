{"cell_type":{"43429cc2":"code","3410d079":"code","25c5af3f":"code","5811be80":"code","6aa0a8a6":"code","098659a4":"code","6a02dcfa":"code","675f6429":"code","dc965042":"code","0749921d":"code","cdd81222":"code","a0805612":"code","b1c6125c":"code","dc1a699e":"code","f6045618":"code","2aaf74d9":"code","edd582a8":"code","28c8dd45":"code","2c658240":"code","f2d8b342":"code","aa7fba2c":"code","37109164":"code","adfdfe82":"code","ae7ad208":"code","d2c23409":"code","258a1d17":"code","d7eab834":"code","a1743fbb":"code","3bc42b46":"code","547f1c85":"code","48ee67df":"code","6576a420":"code","c15db808":"code","fcde55ae":"code","a3952b12":"code","82290158":"code","81bbc90b":"code","f7baf092":"code","bc0e9431":"code","ca83a443":"code","c44e5a57":"code","ed86d415":"code","c8e58d87":"markdown","5a28935d":"markdown","97128941":"markdown","ac0a3adc":"markdown","c96c5f4d":"markdown","6c85c3cd":"markdown","b6cb3408":"markdown","1afbb7ff":"markdown","f3e7fc13":"markdown","c2ab96ce":"markdown","7c344a90":"markdown","df9690f5":"markdown","20dd60a7":"markdown","68ec93d4":"markdown","8d48ade9":"markdown","6fd38b61":"markdown","f087660b":"markdown","b73c5592":"markdown","4f78f43c":"markdown","0ff86134":"markdown","1317feda":"markdown","f25380e6":"markdown","c54c0432":"markdown","c921c907":"markdown","cae02c49":"markdown","522df2f7":"markdown","cb36ec99":"markdown","0b9d9c81":"markdown"},"source":{"43429cc2":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport datatable\nimport missingno as msno\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED = 11\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","3410d079":"example_submission_path = '..\/input\/jane-street-market-prediction\/example_sample_submission.csv'\nexample_test_path = '..\/input\/jane-street-market-prediction\/example_test.csv'\nfeatures_path = '..\/input\/jane-street-market-prediction\/features.csv'\ntrain_path = '..\/input\/jane-street-market-prediction\/train.csv'\n\n# use pandas to load small data files\nexample_submission_file = pd.read_csv(example_submission_path)\nexample_test_file = pd.read_csv(example_test_path)\nfeatures_file = pd.read_csv(features_path)\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","25c5af3f":"# It is found from info() that there are only two datatypes - float64 and int32\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","5811be80":"# Let's have a look at train data\nprint(train_file.columns)\ntrain_file.sample(10)","6aa0a8a6":"print('There are %s NAN values in the train data'%train_file.isnull().sum().sum())","098659a4":"train_file['action'] = (train_file['resp']>0)*1\nprint('There are %s transactions doing trade and %s transactions not doing trade'%((train_file.action==1).sum(), (train_file.action==0).sum()))","6a02dcfa":"# Let's have a look at number of missing values in each of the features\nplt.figure(figsize=(16,5))\nnull = train_file.isnull().sum()\nnull = null[null>0]\nnull.plot()\nplt.xticks(np.arange(len(null)), null.index, rotation=90)\nplt.xlabel('Features with NAN values', size=14)\nplt.title('Number of NAN values in the data', size=16, color='orange')\nplt.show()","675f6429":"null_100 = null[null>=100000].index.to_list()\nnull_50 =  null[null>=50000].index.to_list()\nnull_10 =  null[null>=10000].index.to_list()\nnull_5 =  null[null>=5000].index.to_list()\nnull_0 =  null[null>0].index.to_list()\nnull_0 = [c for c in null_0 if c not in null_5]\nnull_5 = [c for c in null_5 if c not in null_10]\nnull_10 = [c for c in null_10 if c not in null_50]\nnull_50 = [c for c in null_50 if c not in null_100]","dc965042":"fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(16,10))\nax1.plot(null[null_100])\nax2.plot(null[null_50])\nax3.plot(null[null_10])\nax4.plot(null[null_5])\nax1.set_title('Features with more than 100,000 missing values',size=14,color='r')\nax2.set_title('Features with more than 50,000 missing values',size=14,color='r')\nax3.set_title('Features with more than 10,000 missing values',size=14,color='r')\nax4.set_title('Features with more than 5,000 missing values',size=14,color='r')\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","0749921d":"plt.figure(figsize=(16,5))\nnull[null_0].plot()\nplt.title('Features with less than 5,000 missing values', size=14, c='r')\nplt.xticks(range(len(null_0)), null_0, rotation=90)\nplt.show()","cdd81222":"features = train_file.columns[train_file.columns.str.contains('feature')]\nprint(\"Features without any missing values are: \", [c for c in features if c not in null.index])","a0805612":"# Let's find number of days the data has\npd.Series(train_file.date.value_counts())","b1c6125c":"# Let's have a distribution plot of transactions occur per day to gain better insights\nplt.figure(figsize=(16,5))\nax = sns.distplot(train_file.date.value_counts(), bins=250, kde=False)\nheights = np.array([rec.get_height() for rec in ax.patches])\nnormalizer = plt.Normalize(heights.min(), heights.max())\ncmap = plt.cm.jet(normalizer(heights))\nfor rec, color in zip(ax.patches, cmap):\n    rec.set_color(color)\nplt.xlabel(\"Distribution of per-day-transactions\", size=14)\nplt.ylim([-2,22])\nplt.show()","dc1a699e":"dict(train_file.date.value_counts())","f6045618":"plt.figure(figsize=(15,6))\n(train_file.groupby('date').apply(lambda x: x.isnull().sum()).sum(axis=1)).plot(color='red')\nplt.title('Number of Missing values as per date', size=16, color='g')\nplt.ylabel('Number of missing values', size=14)\nplt.xlabel('Date',size=14)\nplt.show()","2aaf74d9":"day_96 = train_file.query('date==96')\nday_242 = train_file.query('date==242')\nday_454 = train_file.query('date==454')","edd582a8":"msno.matrix(day_96,color=(0.2,0.2,0.7))\nmsno.matrix(day_242,color=(0.5,0.2,0.5))\nmsno.matrix(day_454,color=(0.7,0.5,0.2))\nplt.show()","28c8dd45":"# Have a look at data and missing values\npd.options.display.max_columns = None\ntrain_file[null_0][train_file[null_0].isnull().any(axis=1)]","2c658240":"train_file[null_0].describe().T","f2d8b342":"f, ax = plt.subplots(4,7, figsize=(18,30))\nfor k,feature in enumerate(null_0):\n    i = k\/\/7\n    j = k%7\n    sns.boxplot(y=feature, x='action', data=train_file, ax = ax[i][j])\n    ax[i][j].set_title(str(feature), size=14, color='r')\n    ax[i][j].set_ylabel('')\n    ax[i][j].set_xlabel('')\nplt.suptitle('Features having less than 5,000 missing values', size=16, color='blue')\n#plt.tight_layout()\nplt.show()","aa7fba2c":"f, ax = plt.subplots(2,8, figsize=(18,20))\nfor k,feature in enumerate(null_5):\n    i = k\/\/8\n    j = k%8\n    sns.boxplot(y=feature, x='action', data=train_file, ax = ax[i][j])\n    ax[i][j].set_title(str(feature), size=14, color='g')\n    ax[i][j].set_ylabel('')\n    ax[i][j].set_xlabel('')\nplt.suptitle('Features having more than 5,000 missing values', size=16, color='blue')\n#plt.tight_layout()\nplt.show()","37109164":"f, ax = plt.subplots(2,8, figsize=(18,20))\nfor k,feature in enumerate(null_10):\n    i = k\/\/8\n    j = k%8\n    sns.boxplot(y=feature, x='action', data=train_file, ax = ax[i][j])\n    ax[i][j].set_title(str(feature), size=14, color='m')\n    ax[i][j].set_ylabel('')\n    ax[i][j].set_xlabel('')\nplt.suptitle('Features having more than 10,000 missing values', size=16, color='blue')\n#plt.tight_layout()\nplt.show()","adfdfe82":"f, ax = plt.subplots(2,9, figsize=(18,20))\nplt.suptitle('Features having more than 50,000 missing values', size=16, color='blue')\nfor k,feature in enumerate(null_50):\n    i = k\/\/9\n    j = k%9\n    sns.boxplot(y=feature, x='action', data=train_file, ax = ax[i][j])\n    ax[i][j].set_title(str(feature), size=14, color='tab:brown')\n    ax[i][j].set_ylabel('')\n    ax[i][j].set_xlabel('')\n#plt.tight_layout()\nplt.show()","ae7ad208":"f, ax = plt.subplots(2,7, figsize=(16,20))\nfor k,feature in enumerate(null_100):\n    i = k\/\/7\n    j = k%7\n    sns.boxplot(y=feature, x='action', data=train_file, ax = ax[i][j])\n    ax[i][j].set_title(str(feature), size=14, color='orange')\n    ax[i][j].set_ylabel('')\n    ax[i][j].set_xlabel('')\nplt.suptitle('Features having more than 100,000 missing values', size=16, color='blue')\n#plt.tight_layout()\nplt.show()","d2c23409":"val_range = train_file[features].max()-train_file[features].min()\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n# A function to maintain data type consistency of dataframe\ndtype_dict = dict(train_file[features].dtypes)\ndef consistent_dtype(df):\n    return df.astype(dtype_dict)\n","258a1d17":"def fill_missing(df):\n    df[features] = np.nan_to_num(df[features]) + filler*np.isnan(df[features])\n    return df        ","d7eab834":"train = fill_missing(train_file)\ntrain = consistent_dtype(train)","a1743fbb":"train.info()","3bc42b46":"train.isnull().sum().sum()","547f1c85":"train = train.loc[train.weight > 0]\nX = train[features]\ny = train[[c for c in train.columns if 'resp' in c]]\ny = (y>0)*1\ny = (y.mean(axis=1)>0.5).astype(np.int8)","48ee67df":"X_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\ndef Normalize(df):\n    return (df - X_mean)\/X_std\nX = Normalize(X)","6576a420":"X_train = X[train.date<=400]\nX_val = X[train.date>400]\ny_train = y[train.date<=400]\ny_val = y[train.date>400]","c15db808":"# LSTM expects 3D input (examples, timestep, features)\nprint(X_train.shape, X_val.shape)\nX_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\nprint(X_train.shape, X_val.shape)","fcde55ae":"# A Sequential model\nbatch_size = 256\nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(64, batch_size=batch_size, input_shape=(1,130), return_sequences=True ),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='mse')","a3952b12":"train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nval = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\ntrain = train.cache().batch(batch_size).repeat()","82290158":"del train_file\ndel null_0\ndel null_5\ndel null_10\ndel null_50\ndel null_100\ngc.collect()","81bbc90b":"del X_train\ndel X_val\ndel y_train\ndel y_val\ngc.collect()","f7baf092":"model.summary()","bc0e9431":"hist = model.fit(train, epochs=20, steps_per_epoch=200, validation_data=val, validation_steps=50)","ca83a443":"# Let's clear some memory\ndel train\ndel val\ndel X\ndel y\ngc.collect()","c44e5a57":"example_submission_file.info()","ed86d415":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env()\nfor test_file,pred in tqdm(env.iter_test()):\n    if test_file.weight.item()==0:\n        pred.action = np.int64(0)\n    else:\n        test = test_file[features]\n        test = fill_missing(test)\n        test = consistent_dtype(test)\n        test = Normalize(test)\n        test = test.values\n        test = np.repeat(test, batch_size).reshape(-1, batch_size).T.reshape(batch_size,1,130)\n        # LSTM requires data in the right shape only\n        action = model(test)\n        a = 1 if action[0][0].item()>0.5 else 0\n        pred.action = np.int64(a)\n        test = test[0]\n        test = np.squeeze(test)\n    env.predict(pred)","c8e58d87":"### We can visualize missing values in these representative dates","5a28935d":"### We have some basic views on missing values. Let's try to explore other insights of features ","97128941":"# 6. LSTM MODELING\n","ac0a3adc":"### Hi kagglers,\n### I wish to learn more since this is my first competition on kaggle.\n\n### I decide to develop an LSTM model using Tensorflow for this time series data.","c96c5f4d":"### Outliers in each feature compelled me to make taller plots to have a better view. \n\n### Features with different missing value counts do have some similarity among them. Mean of almost every feature lies around zero. Every feature necessarily has great positive outliers. But around half of the features do not have remarkable negative outliers.\n\n### So I decide to communicate about MISSING VALUES to my model by assigning them with newer and relatively bigger NEGATIVE OUTLIERS for all those features which have missing values. (And I strongly believe it will work)","6c85c3cd":"## Training the model","b6cb3408":"# 2. LOAD DATA","1afbb7ff":"## Normalize data for LSTM model","f3e7fc13":"# 7. PREDICTION AND SUBMISSION","c2ab96ce":"### Mostly the number of transactions per day ranges from 2500 to 7500. It will be good to have sample days one from lesser-transactions-day and another from more-transaction-day to have better understanding on handling missing values","7c344a90":"### Awesome. Irrespective of number of transactions, missing values do have the same pattern based on time. (It is true to even highest number of transactions day; I tested with date=44). \n\n### So missing values should not be \n* ### filled with mean values or nanmean values\n* ### filled with any kind of ffill method or something\n* ### dropped\n\n### Rather, they should be filled with special distinguishable values so that our model can recognize them! We can make our data to communicate our model regarding start\/interval timings via those special values.","df9690f5":"### A very interesting dataset! Even missing values do follow a distributed pattern among features. As said, features are anonymized but not shuffled. They may tell great stories if explored properly. So more care is needed on handling those missing values","20dd60a7":"### We can start by learning about the values in the missing value features. We first consider the features which have missing values lesser than 5,000 in their columns.","68ec93d4":"# 1. IMPORT PACKAGES AND LIBRARIES","8d48ade9":"### Let's split data into features (X) and target (y)","6fd38b61":"### Thank you all for your patience! I am here to learn; Kindly express your suggestions, if any.","f087660b":"### That's a great reduction in memory usage (around 73% reduction)! It will help us go further efficiently!","b73c5592":"### Is there any NAN values in the data?","4f78f43c":"## Let's split data into train and validation set in 80-20 ratio","0ff86134":"## Reduce memory usage by adopting optimal datatype","1317feda":"# 3. DEFINE PROBLEM\n\n### The data provided is a time-series one without explicit target. The problem being binary classification, to decide whether to do trade or not. A specific target \"***action***\" (named as required in final output) can be introduced based on the value of variable \"***resp***\" as of now for handling missing values. Values of resp above 0 can be considered a positive signal to trade (action = 1) otherwise say no to trade (action = 0) \n\n### While modeling neural network, we can consider all of the resp, resp_1, resp_2, resp_3 and resp_4 as contributors of our target ***action***.","f25380e6":"### We have successfully filled all of the missing values with distinguishable values","c54c0432":"### It is observed that 2390491 transactions belong to just 500 days; Transactions-per-day varies from day to day. Day 294 has the minimum of only 29 transactions whereas Day 44 has the maximum of 18884 transactions in total.","c921c907":"# 4. EXPLORE DATA\n\n### It is time to find correlations among features. Since we are using neural network, feature engineering is not mandatory. However for handling missing values data exploration is necessary. ","cae02c49":"### We randomly select day 96, day 242 and day 454 as three representatives of lower number of transactions (2785), most probable number of transactions (4025) and higher number of transactions (7300). Let's have a look at those days","522df2f7":"# 5. FILLING MISSING VALUES","cb36ec99":"## A function to fill missing values in train data, test data and future data","0b9d9c81":"### Now the target column has been generated. Trade transactions and No-trade transactions are almost balanced."}}