{"cell_type":{"962c8e91":"code","03f87223":"code","d42227f0":"code","178b6440":"code","21bd51e9":"code","cb4e8a08":"code","8c3ea18a":"code","d44b81f1":"code","3a719265":"code","75608d78":"code","80848da6":"code","72d18abc":"code","d44cdef4":"code","98acd412":"code","f549fbaa":"code","e70a9aee":"code","b362ecad":"code","c1838583":"code","cef44bb4":"code","2028780e":"code","ed18f1ba":"code","40cc8e65":"code","12714173":"code","27ee4dcf":"code","010a8d82":"code","86d83502":"code","ba95db37":"code","e6ee309f":"code","f2a46d2a":"code","0584aa33":"code","f92e0956":"code","951ed883":"code","d3adb044":"code","9317dc94":"code","ac0fc902":"code","1e9f846b":"code","0956c7ca":"code","b3085021":"code","0fa70108":"code","f1a467e7":"code","8b393320":"code","caa182ad":"code","bbb20d95":"code","d7a630c5":"code","92727cf1":"code","25cc9626":"code","79cf43d9":"code","c0b9e8a2":"code","f074fd51":"code","c9afe854":"code","4b72136c":"code","729df044":"code","b2b585c5":"code","182a3208":"code","ab8b945b":"code","b8ebd438":"code","7a2fa8da":"code","273bed39":"code","9650b5a8":"code","bb67d3ec":"code","2371ff8f":"markdown","0a4f5cb0":"markdown","55538271":"markdown","0999947c":"markdown","9b0adf07":"markdown","6caa729f":"markdown","61d65aea":"markdown","31bd9dd4":"markdown","be97c112":"markdown","2b3197d2":"markdown","c01cdbd4":"markdown","67f517e5":"markdown","4e1349f3":"markdown","3d8ea3e3":"markdown","c21beb7a":"markdown","e02ea166":"markdown","d49e6756":"markdown","4ecba06e":"markdown","c730a8c6":"markdown","ab6442e3":"markdown","e25c1186":"markdown","f1244b03":"markdown","723e84b1":"markdown","b323e67f":"markdown","85b6af3c":"markdown","1780ebc3":"markdown","ac9a1be9":"markdown","14f89a49":"markdown","df74cff3":"markdown","b90ac971":"markdown","77a78641":"markdown","638b4664":"markdown","adddc756":"markdown","85a2c137":"markdown","6d00fe7d":"markdown","6eed1ad1":"markdown","d03e71c2":"markdown","5afc49f8":"markdown","aeebdf0b":"markdown","8aea4ab6":"markdown","83e91ff4":"markdown","9b907c40":"markdown","616793fa":"markdown","7eab91ed":"markdown","5db882b7":"markdown","fee94878":"markdown","050299b8":"markdown","523a0006":"markdown","d68f4fce":"markdown","2fe0a35a":"markdown","2a54f6e5":"markdown","59cf3356":"markdown","7000df67":"markdown","a9755294":"markdown","15a24250":"markdown","c67439fc":"markdown","39182e89":"markdown","64d6c696":"markdown","cbe29103":"markdown","96d907aa":"markdown","86eacdf5":"markdown","43a0c218":"markdown"},"source":{"962c8e91":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","03f87223":"training = pd.read_csv(\"..\/input\/train.csv\")\ntesting = pd.read_csv(\"..\/input\/test.csv\")","d42227f0":"training.head()","178b6440":"testing.head()","21bd51e9":"print(\"Here are columns in training dataset:\")\nprint(training.keys())\nprint('Here are columns in testing dataset:')\nprint(testing.keys())","cb4e8a08":"types_train = training.dtypes\nnum_values = types_train[(types_train == \"float\")]","8c3ea18a":"print(\"These are numerical features:\")\nprint(num_values)","d44b81f1":"#describe() is the method to get the useful summary\ntraining.describe()","3a719265":"print(\"Nulls information in training datasets:\")\npd.isnull(training).sum()","75608d78":"print(\"Nulls information in testing datasets:\")\npd.isnull(testing).sum()","80848da6":"#droping the columns by using drop() method\ntraining.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntesting.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)","72d18abc":"#Again printing NULL information\nprint(\"Nulls information in training datasets:\")\npd.isnull(training).sum()","d44cdef4":"#This is seaborn distplot to understand the data distribution\ncopy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy['Age']);","98acd412":"training['Age'].fillna(training['Age'].median(), inplace = True)\ntesting['Age'].fillna(testing['Age'].median(), inplace = True)\ntraining['Embarked'].fillna('S', inplace = True)\ntesting['Fare'].fillna(testing['Fare'].median(), inplace = True)","f549fbaa":"print(\"Nulls information in training datasets:\")\npd.isnull(training).sum()","e70a9aee":"print(\"Nulls information in testing datasets:\")\npd.isnull(testing).sum()","b362ecad":"training.head()","c1838583":"testing.head()","cef44bb4":"#SNS barplot for AGE\nsns.barplot(x='Sex', y='Survived', data=training)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = training[training.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = training[training.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))","2028780e":"#seaborn plot for Pclass to understand how the seating class of the passenger affected its survival\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = training[training.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = training[training.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = training[training.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)\n","ed18f1ba":"#survival rate is each class based on the gender - it further concludes \n#that even in each class there are more women survived than the man\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","40cc8e65":"#interchnaging the hue parameter from the above to give more insights \nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","12714173":"survived_ages = training[training.Survived == 1][\"Age\"]\nnot_survived_ages = training[training.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.subplots_adjust(right=1.7)\nplt.show()","27ee4dcf":"#looking into the AGE feature in a different way\nsns.stripplot(x=\"Survived\", y=\"Age\", data=training, jitter=True)","010a8d82":"#Plotting every feature\nsns.pairplot(training)","86d83502":"set(training[\"Embarked\"])","ba95db37":"# For Male - 0\n# For Female  - 1\ntraining.loc[training[\"Sex\"] == \"male\", \"Sex\"] = 0\ntraining.loc[training[\"Sex\"] == \"female\", \"Sex\"] = 1\n\n# For S , C , Q - 0, 1 , 2 respectivly \n\ntraining.loc[training[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntraining.loc[training[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntraining.loc[training[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\n#Applies the same for Test data set\n\ntesting.loc[testing[\"Sex\"] == \"male\", \"Sex\"] = 0\ntesting.loc[testing[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntesting.loc[testing[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntesting.loc[testing[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntesting.loc[testing[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","e6ee309f":"training.sample(5)","f2a46d2a":"testing.sample(5)","0584aa33":"#Creating one additinol feature \n\ntraining[\"FamSize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"FamSize\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1\n","f92e0956":"#Creating one more feature named - IsAlone\n\ntraining[\"IsAlone\"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntesting[\"IsAlone\"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)\n","951ed883":"#Creating a feature - Title by extrctacting fron Name column\n\nfor name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)","d3adb044":"#Printing the titles:\n\ntitles = set(training[\"Title\"]) #making it a set gets rid of all duplicates\nprint(titles)","9317dc94":"#Checking how many times each title appeared \n\ntitle_list = list(training[\"Title\"])\nfrequency_titles = []\n\nfor i in titles:\n    frequency_titles.append(title_list.count(i))\n    \nprint(frequency_titles)","ac0fc902":"#Printing in a more meaningful way\n\ntitles = list(titles)\n\ntitle_dataframe = pd.DataFrame({\n    \"Titles\" : titles,\n    \"Frequency\" : frequency_titles\n})\n\nprint(title_dataframe)","1e9f846b":"#Replacing less frequent titles into - others\ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\"}\n\ntraining.replace({\"Title\": title_replacements}, inplace=True)\ntesting.replace({\"Title\": title_replacements}, inplace=True)\n\n#encoding the categorical variables into the numerical values\n\ntraining.loc[training[\"Title\"] == \"Miss\", \"Title\"] = 0\ntraining.loc[training[\"Title\"] == \"Mr\", \"Title\"] = 1\ntraining.loc[training[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntraining.loc[training[\"Title\"] == \"Master\", \"Title\"] = 3\ntraining.loc[training[\"Title\"] == \"Dr\", \"Title\"] = 4\ntraining.loc[training[\"Title\"] == \"Rev\", \"Title\"] = 5\ntraining.loc[training[\"Title\"] == \"Other\", \"Title\"] = 6\n\n#repeating for test data set\ntesting.loc[testing[\"Title\"] == \"Miss\", \"Title\"] = 0\ntesting.loc[testing[\"Title\"] == \"Mr\", \"Title\"] = 1\ntesting.loc[testing[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntesting.loc[testing[\"Title\"] == \"Master\", \"Title\"] = 3\ntesting.loc[testing[\"Title\"] == \"Dr\", \"Title\"] = 4\ntesting.loc[testing[\"Title\"] == \"Rev\", \"Title\"] = 5\ntesting.loc[testing[\"Title\"] == \"Other\", \"Title\"] = 6","0956c7ca":"#Droping the name feature - as we have already extracted useful insights from it\ntraining.drop(\"Name\", axis = 1, inplace = True)\ntesting.drop(\"Name\", axis = 1, inplace = True)","b3085021":"training.sample(5)","0fa70108":"testing.sample(5)","f1a467e7":"#scaling \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n#scaler requires arguments to be in a specific format shown below\n#convert columns into numpy arrays and reshape them \nages_train = np.array(training[\"Age\"]).reshape(-1, 1)\nfares_train = np.array(training[\"Fare\"]).reshape(-1, 1)\nages_test = np.array(testing[\"Age\"]).reshape(-1, 1)\nfares_test = np.array(testing[\"Fare\"]).reshape(-1, 1)\n\n#we replace the original column with the transformed\/scaled values\ntraining[\"Age\"] = scaler.fit_transform(ages_train)\ntraining[\"Fare\"] = scaler.fit_transform(fares_train)\ntesting[\"Age\"] = scaler.fit_transform(ages_test)\ntesting[\"Fare\"] = scaler.fit_transform(fares_test)","8b393320":"#Chceking the training data\n\ntraining.head()","caa182ad":"#Chceking the testing data\n\ntesting.head()","bbb20d95":"warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","d7a630c5":"from sklearn.metrics import make_scorer, accuracy_score ","92727cf1":"from sklearn.model_selection import GridSearchCV","25cc9626":"X_train = training.drop(labels=[\"PassengerId\", \"Survived\"], axis=1) #define training features set\ny_train = training[\"Survived\"] #define training label set\nX_test = testing.drop(\"PassengerId\", axis=1) #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","79cf43d9":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets\n","c0b9e8a2":"svc_clf = SVC() \n\nparameters_svc = {\"kernel\": [\"rbf\", \"linear\"], \"probability\": [True, False], \"verbose\": [True, False]}\n\ngrid_svc = GridSearchCV(svc_clf, parameters_svc, scoring=make_scorer(accuracy_score))\ngrid_svc.fit(X_training, y_training)\n\nsvc_clf = grid_svc.best_estimator_\n\nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)","f074fd51":"print(\"The Score for SVC is: \" + str(acc_svc))","c9afe854":"linsvc_clf = LinearSVC()\n\nparameters_linsvc = {\"multi_class\": [\"ovr\", \"crammer_singer\"], \"fit_intercept\": [True, False], \"max_iter\": [100, 500, 1000, 1500]}\n\ngrid_linsvc = GridSearchCV(linsvc_clf, parameters_linsvc, scoring=make_scorer(accuracy_score))\ngrid_linsvc.fit(X_training, y_training)\n\nlinsvc_clf = grid_linsvc.best_estimator_\n\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(\"The Score for LinearSVC is: \" + str(acc_linsvc))","4b72136c":"rf_clf = RandomForestClassifier()\n\nparameters_rf = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]}\n\ngrid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))\ngrid_rf.fit(X_training, y_training)\n\nrf_clf = grid_rf.best_estimator_\n\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(\"The Score for Random Forest is: \" + str(acc_rf))","729df044":"logreg_clf = LogisticRegression()\n\nparameters_logreg = {\"penalty\": [\"l2\"], \"fit_intercept\": [True, False], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                     \"max_iter\": [50, 100, 200], \"warm_start\": [True, False]}\n\ngrid_logreg = GridSearchCV(logreg_clf, parameters_logreg, scoring=make_scorer(accuracy_score))\ngrid_logreg.fit(X_training, y_training)\n\nlogreg_clf = grid_logreg.best_estimator_\n\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(\"The Score for Logistic Regression is: \" + str(acc_logreg))\n","b2b585c5":"knn_clf = KNeighborsClassifier()\n\nparameters_knn = {\"n_neighbors\": [3, 5, 10, 15], \"weights\": [\"uniform\", \"distance\"], \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n                  \"leaf_size\": [20, 30, 50]}\n\ngrid_knn = GridSearchCV(knn_clf, parameters_knn, scoring=make_scorer(accuracy_score))\ngrid_knn.fit(X_training, y_training)\n\nknn_clf = grid_knn.best_estimator_\n\nknn_clf.fit(X_training, y_training)\npred_knn = knn_clf.predict(X_valid)\nacc_knn = accuracy_score(y_valid, pred_knn)\n\nprint(\"The Score for KNeighbors is: \" + str(acc_knn))","182a3208":"gnb_clf = GaussianNB()\n\nparameters_gnb = {}\n\ngrid_gnb = GridSearchCV(gnb_clf, parameters_gnb, scoring=make_scorer(accuracy_score))\ngrid_gnb.fit(X_training, y_training)\n\ngnb_clf = grid_gnb.best_estimator_\n\ngnb_clf.fit(X_training, y_training)\npred_gnb = gnb_clf.predict(X_valid)\nacc_gnb = accuracy_score(y_valid, pred_gnb)\n\nprint(\"The Score for Gaussian NB is: \" + str(acc_gnb))","ab8b945b":"dt_clf = DecisionTreeClassifier()\n\nparameters_dt = {\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\", \"random\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n\ngrid_dt = GridSearchCV(dt_clf, parameters_dt, scoring=make_scorer(accuracy_score))\ngrid_dt.fit(X_training, y_training)\n\ndt_clf = grid_dt.best_estimator_\n\ndt_clf.fit(X_training, y_training)\npred_dt = dt_clf.predict(X_valid)\nacc_dt = accuracy_score(y_valid, pred_dt)\n\nprint(\"The Score for Decision Tree is: \" + str(acc_dt))","b8ebd438":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg, acc_knn, acc_gnb, acc_dt]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","7a2fa8da":"rf_clf.fit(X_train, y_train)","273bed39":"submission_predictions = rf_clf.predict(X_test)","9650b5a8":"submission = pd.DataFrame({\n        \"PassengerId\": testing[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)\nprint(submission.shape)","bb67d3ec":"submission_predictions","2371ff8f":"Let's create a dataframe to submit to the competition with our predictions of our model.","0a4f5cb0":"NULLs mean missing values are the most important thing to handle in your datasets. we need to consider either one of the ways the handle the NULLs carefully.\n1.\tRemove the rows which having NULL values\n2.\tReplace the NULL values for the particular column (feature) with some meaningful value i.e. mean, median etc.\n","55538271":"# DecisionTree Model","0999947c":"Age","9b0adf07":"Gender","6caa729f":"You can see from the above that we finally able to deal the categorical variables and converted them into numerical.","61d65aea":"# 7. Model Fitting, Optimizing, and Predicting","31bd9dd4":"Although we already have a test set, it is generally easy to overfit the data with these classifiers. It is therefore useful to have a third data set called the validation data set to ensure that our model doesn't overfit with the data. We can make this third data set with sklearn's train_test_split function. We can also use the validation data set to test the general accuracy of our model.","be97c112":"Now we are loading our dataset in the pandas dataframe. Remember we have 2 datasets.\n1.\tTrain dataset to train and validate our model\n2.\tTest dataset to make the prediction \n","2b3197d2":"To evaluate our model performance, we can use the make_scorer and accuracy_score function from sklearn metrics.","c01cdbd4":"If you take a look at the Age and Fare features above, you can see that the values deviate heavily from the other features. This may potentially cause some problems when we are modelling, and it would be beneficial to scale them so they are more representative. We will do this with sklearn's MinMaxScaler function. This function also requires us to reshape our data so that it accepts the input. The steps are shown below.","67f517e5":"# SVC Model","4e1349f3":"Now I believe that the cabin and ticket feature won\u2019t impact much of the outcome, so I am dropping them to reduce the volume (remember the old ETL trick to reduce the volume as easy as possible. Also, cabin is having lots of NULL values, so it will add unnecessary noise in our model.","3d8ea3e3":"Although it may not seem like it, we can also extract some useful information from the name column. Not the actual names themselves, but the title of their names like Ms. or Mr. This may also provide a hint as to whether the passenger survived or not. Therefore we can extract this title and then encode it like we did for Sex and Embarked.","c21beb7a":"Further digging into the AGE of the survived or not survived passenger to understand if the survival anything to do with passenger\u2019s age","e02ea166":"# LogisiticRegression Model","d49e6756":"This feature scaling may allow for higher accuracy for our models because of the reduced weight of their magnitudes!","4ecba06e":"Finding the numerical columns via - dtypes attribute ","c730a8c6":"Handling the NULLs by using the fillna() method \u2013 \n1.\tFilling the NULLs in AGE by its median value\n2.\tFilling Embarked by S (because it occurred most frequently)\n3.\tFilling Fare by its median value\n","ab6442e3":"# Defining Features in Training\/Test Set","e25c1186":"Now that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. We will try different types of models to see which one gives the best accuracy for its predictions.","f1244b03":"Congratulations! We have completed our analysis. At the start we have the question if you would have been on the Titanic then under which criteria you would have survived ?\nPlease look into the csv file for the answer and let me know in case of any questions. I would be happy to answer all.\n","723e84b1":"It appears that the Random Forest model works the best with our data so we will use it on the test set.","b323e67f":"WOW! All the features are in numerical form now. It is ready to be fed into our model. Before we do that however, there's something else that we should notice when looking at the preprocessed data. Particularly, the Age and Fare feature values.","85b6af3c":"# 8. Evaluating Model Performances","1780ebc3":"By plotting and visualizing each feature (predictor) against the response (survived), we try to find how a particular feature impacting the predictions to understand which the most significant feature is, and which is the least significant feature. Remember in machine learning its very important to understand the features and how they impact the prediction.","ac9a1be9":"Conclusion3: So, we can conclude there is no clear-cut evidence which says the AGE is having significant impact on survival. We can se both survival -age and not-survived age data is more and normally distributed.","14f89a49":"We can see the AGE and FARE features are now on the same scale \ud83d\ude0a","df74cff3":"Lets check if we have made a successful replacement or not?","b90ac971":"We change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. We do this same process on both the training and testing set to prepare our data for Machine Learning.","77a78641":"Conclusion1: With the above Bar-plot and statistics, we can safely and easily conclude that there are more Female survived than the Males","638b4664":"# 2. Loading and Viewing Data Set","adddc756":"Conclusion2: by looking into the above bar-plot, we can easily conclude that the passenger which were in class 1 and 2 are more likely to survived than who were present in class3","85a2c137":"# Validation Data Set","6d00fe7d":"After making so many models and predictions, we should evaluate and see which model performed the best and which model to use on our testing set.","6eed1ad1":"Sometimes it is useful to create synthetic features that we think may help us predict the target value.\n\nWe can combine SibSp and Parch into one synthetic feature called family size, which indicates the total number of family members on board for each member.","d03e71c2":"Hi All, My Name is Deepak Rajak, I am currently working as a Technical Lead in Cotiviti Inc, Hyderabad. You can check my LinkedIn profile to know more about me and you can reach out to me - https:\/\/www.linkedin.com\/in\/deepak-rajak-935b411b\/ \n This is my first attempt to build a machine learning model. As per machine learning community suggestion, I am also starting from the Titanic Survival Problem in which we are going to predict the survival of the passenger. As you can see, here we are predicting only two possible outcomes - survived or not survived so this becomes a binary classification problem. I will be doing this in python because I find python very easy to learn and it has variety of useful machine learning packages.\nSo, let\u2019s start with step by step process to find out if you would have been on the great titanic ship then in which criteria you would have survived? we will find it out this by end of our process.\n","5afc49f8":"Class","aeebdf0b":"Getting the statistical summary of the training set to understand the variables (features or predictors). Remember some of the points about the statistical data analysis:\n1.\tStatistical summary will give good insights only for the numerical columns not for the columns whose data is in text format. \n2.\tIt won\u2019t give much of the information for categorical variables. We will use plotting methods to analyze them.\n","8aea4ab6":"Remember we need to deal the categorical variables in both Train and Test set.\nThere are 2 kinds of categorical variables.\n1.\tOrdinal: which has certain order \n2.\tNominal: which doesn\u2019t have any order but has fixed sets of categories.\n","83e91ff4":"We try to find out \u2013 what columns we have via the \u2013 keys() method","9b907c40":"# 4. Plotting and Visualizing Data","616793fa":"# Creating Synthetic Features","7eab91ed":"Looking into the data density for AGE to find out the replacement of NULLs in the data","5db882b7":"# SKLEARN Models:","fee94878":"# 3. Dealing with NaN Values (Imputation)","050299b8":"Because values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. \nWe can also do this process through One-Hot-Encoding.","523a0006":"There are 3 values for Embarked: S, C, and Q. We will represent these with numbers as well.","d68f4fce":"# 9. Submission","2fe0a35a":"# 6. Feature Rescaling","2a54f6e5":"We can also use a GridSearch cross validation to find the optimal parameters for the model we choose to work with and use to predict on our testing set.","59cf3356":"# 5. Feature Engineering","7000df67":"# RandomForest Model","a9755294":"First step - is to import all the necessary packages in python. This will enable us to start exploring the titanic dataset. In machine learning terms it\u2019s called \u2013 EDA (exploratory Data Analysis)","15a24250":"# LinearSVC Model","c67439fc":"This IsAlone feature also may work well with the data we're dealing with, telling us whether the passenger was along or not on the ship.","39182e89":"# KNeighbors Model","64d6c696":"Now we can see that the datasets look nice. We can start analyzing it further.","cbe29103":"# 1. Importing Libraries and Packages","96d907aa":"So, we found out the Age and Cabin columns are having NULL values. Remember here \u2013 we need to handle NULL values in both train and test sets. ","86eacdf5":"# GaussianNB Model","43a0c218":"Now we should check whether our datasets are loaded correctly? we can just check the first few rows of both the train and test dataset. head() will give us the first 5 rows of the pandas dataframe. "}}