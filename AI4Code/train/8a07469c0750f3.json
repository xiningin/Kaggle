{"cell_type":{"e60492f6":"code","8b294c3c":"code","59a2b0fd":"code","e66f1367":"code","496b9e9d":"code","a4f959cb":"code","e0a003a3":"code","ee9cf72f":"code","abf7105d":"code","3282e385":"code","dc887222":"code","7239339d":"code","fc49b381":"code","3c03b865":"code","f26cebcf":"code","6073020e":"code","b2d991fb":"code","d8c1907b":"code","40a30641":"code","842cd614":"code","269afb63":"code","c819c0dd":"code","b75c2c99":"code","d33fb88f":"code","e3e63b41":"code","0eff74f7":"code","eb966188":"code","86501252":"code","44d96608":"code","e2aa2d8d":"code","a618a865":"code","3332762b":"markdown","b0bb28cf":"markdown","7a50df1b":"markdown","d141faa6":"markdown","aaec0a18":"markdown","61c261ac":"markdown","07643bd2":"markdown","d021c628":"markdown","4a779ac6":"markdown","a3232bb2":"markdown","80742c8a":"markdown","0c2a1656":"markdown","73b6613b":"markdown","c2a4881b":"markdown","c652e1c2":"markdown","ce88921b":"markdown"},"source":{"e60492f6":"from gensim import utils\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\nimport gensim.models\nimport re\nimport string\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport argparse\nimport gensim.downloader as api\nimport os\nimport shutil\nimport tensorflow as tf\n","8b294c3c":"train_data=pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding='ISO-8859-1',parse_dates=['TweetAt'])","59a2b0fd":"def clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","e66f1367":"train_data['OriginalTweet']=train_data['OriginalTweet'].apply(lambda x : clean(x))","496b9e9d":"class CoronaTweetCorpus:\n    def __iter__(self):\n        for line in train_data['OriginalTweet']:  \n            yield utils.simple_preprocess(line)","a4f959cb":"sentences = CoronaTweetCorpus()\nmodel = gensim.models.Word2Vec(sentences=sentences)","e0a003a3":"vector=model.wv['corona']","ee9cf72f":"vector","abf7105d":"model.wv.most_similar('corona')","3282e385":"model.wv.most_similar('people')","dc887222":"model.wv.most_similar('die')","7239339d":"vector","fc49b381":"print(model.wv.most_similar(positive=['corona'], topn=5))","3c03b865":"word_vectors = model.wv\nword_vectors.save(\"corona_tweets_word2vec\")","f26cebcf":"wv = KeyedVectors.load(\"corona_tweets_word2vec\", mmap='r')","6073020e":"x=model[model.wv.vocab]\nwords = list(model.wv.vocab)","b2d991fb":"pca = PCA(n_components=2)\nresult = pca.fit_transform(x)\nplt.figure(figsize=(12, 6))\nplt.scatter(result[:30, 0], result[:30, 1])\nfor i, word in enumerate(words[:30]):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\nplt.show()","d8c1907b":"train_data","40a30641":"# Function to get data\ndef get_dataset(data):\n    labels=data['Sentiment'].map({'Positive':0,'Negative':1,'Neutral':2,'Extremely Positive':3,'Extremely Negative':4})\n    return data['OriginalTweet'],labels","842cd614":"tweets,labels=get_dataset(train_data)","269afb63":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(tweets)\ntext_sequences = tokenizer.texts_to_sequences(tweets)\ntext_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\nnum_records = len(text_sequences)\nmax_seqlen = len(text_sequences[0])\nprint(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))","c819c0dd":"# creating labels\nNUM_CLASSES = 5\ncat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)","b75c2c99":"# vocabulary\nword2idx = tokenizer.word_index\nidx2word = {v:k for k, v in word2idx.items()}\nword2idx[\"PAD\"] = 0\nidx2word[0] = \"PAD\"\nvocab_size = len(word2idx)\nprint(\"vocab size: {:d}\".format(vocab_size))","d33fb88f":"#split into train and validation set\n\ndataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\ndataset = dataset.shuffle(10000)\nval_size = num_records \/\/ 4\nval_dataset = dataset.take(val_size)\ntrain_dataset = dataset.skip(val_size)","e3e63b41":"BATCH_SIZE = 128\nval_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)","0eff74f7":" def build_embedding_matrix(sequences, word2idx, embedding_dim,embedding_file):\n    if os.path.exists(embedding_file):\n        E = np.load(embedding_file)\n    else:\n        vocab_size = len(word2idx)\n        E = np.zeros((vocab_size, embedding_dim))\n        word_vectors = api.load(EMBEDDING_MODEL)\n        for word, idx in word2idx.items():\n            try:\n                E[idx] = word_vectors.word_vec(word)\n            except KeyError:   # word not in embedding\n                 pass\n        np.save(embedding_file, E)\n    return E","eb966188":"EMBEDDING_DIM = 300\nDATA_DIR = \".\/\"\nEMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"word2vec.npy\")\nEMBEDDING_MODEL = \"word2vec-google-news-300\"\nE_word2vec = build_embedding_matrix(text_sequences, word2idx,EMBEDDING_DIM,EMBEDDING_NUMPY_FILE)\nprint(\"Embedding matrix:\", E_word2vec.shape)","86501252":"print(E)","44d96608":"api.info()","e2aa2d8d":"EMBEDDING_DIM = 300\nDATA_DIR = \".\/\"\nEMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"glove.npy\")\nEMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\nE_glove = build_embedding_matrix(text_sequences, word2idx,EMBEDDING_DIM,EMBEDDING_NUMPY_FILE)\nprint(\"Embedding matrix:\", E_glove.shape)","a618a865":"E_glove","3332762b":"# Fundamentals \nModels we create using deep learning or machine learning do not process text directly , we need to convert text into numbers .This process of converting text  into numbers is called **vectorization**.\n\none of the most easiest way to do that is **One-Hot Encoding**.\n\nFor example ,\n\nsentence = \"Do not read the next sentence.\"\n\n**one-hot encoding** will be -\n\n```\nDo        [1 0 0 0 0 0 0]\n\nnot       [0 1 0 0 0 0 0]\n\nread      [0 0 1 0 0 0 0]\n\nthe       [0 0 0 1 0 0 0]\n\nnext      [0 0 0 0 1 0 0]\n\nsentence  [0 0 0 0 0 1 0]\n\n.         [0 0 0 0 0 0 1]\n```\n**Drawbacks** -\n\n* This will result a sparse matrix. let's say you have a corpus with 20,000 unique words and a single short text of 20 words would be represented by a matrix with 20,000 rows with a maximum of 20 non-zero matrix elements. This leaves a lot of zeroes, and can end up taking a large amount of memory for these spare representations.\n* another drawback of this representation is lack of meaning representation as it treats each word as completely independent from all the others, since similarity between any two words can not be captured.\n\nAnother approach is to use **Tf-idf** i.e **Term Frequency-Inverse Document Frequency**\n\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. \n\nThe **term frequency** of a word in a document : raw count of instances a word appears in a document. \n\n**Inverse document frequency** of the word across a set of documents. This means, how common or rare a word is in the entire document set.This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\n![image.png](attachment:image.png)\n\n**Drawback**  of tf-idf is , it does not capture position in text and semantics,\n\n**one-hot** and **TF-IDF** are relatively sparse embeddings, since vocabularies are usually quite large, and a word is unlikely to occur in more than a few documents in the corpus.\n","b0bb28cf":"## Visualise word embeddings ","7a50df1b":"## saving your model","d141faa6":"### Import necessary librariess","aaec0a18":"**min_count** -  In min count we want to ignore the words that appear less number of times , let's say if we put min count = 2 then words that appear less than 2 times will be ignored.\n\ndefault value of min_count=5\n\n**vector_size** - number of dimensions (N) of the N-dimensional space that gensim **Word2Vec** maps the words onto.\n\nThe default value of vector_size is 100\n\n**workers** - for training parallelization, to speed up training\n\ndefault value of workers=3\n","61c261ac":"## Building Embedding matrix","07643bd2":"## Training parameters \n","d021c628":"# Training your own Model using Gensim","4a779ac6":"# Word2Vec\n\nWord2Vec Embeds text into low dimentional vectors.\n\nThere are two architectures of Word2Vec models-\n\n1.  CBOW (Continuous bag of words model)\n\n2.  Skip-gram\n\n\n**Continuous Bag of Word model** tries to predict center word based on surrounding words.\n\nFor example , \"John loves to eat apples\" can be paired as (context_window,target_word) as \n\n```[['john','to'],'loves'],[['loves','eat],'to'],[['to','apples'],'eat']``` if size of window is 2\n\n\n![](attachment:image.png)\n\n*Source: https:\/\/arxiv.org\/pdf\/1301.3781.pdf*\n\n","a3232bb2":"Why not word2vec?\n\nWord2vec relies only on local information of language. That is, the semantics learnt for a given word, is only affected by the surrounding words.\n\n**GloVe** as an unsupervised learning algorithm for obtaining vector representations for words.The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors.","80742c8a":"In Skip-Gram predicts the surrounding words if center word is known.\n\n![image.png](attachment:image.png)\n\n*Source: https:\/\/arxiv.org\/pdf\/1301.3781.pdf*\n\n","0c2a1656":"### Building embedding matrix","73b6613b":"# Glove ","c2a4881b":"Another extension of one-hot encoding is BAG of WORDS model ,transforms each document to a fixed-length vector of integers.\n\nFor example - \n\nsentence1 = \"kaggle is the world's largest data science community.Kaggle is the best platform to learn data science.\"\nsentence2 = I love kaggle for data science . \n\ndictionary is \n\n```\n[\"Kaggle\",\"is\",\"the\",\"world's\",\"largest\",\"data\",\"science\",\"community\",\"best\",\"platform\",\"to\",\"learn\",\"I\",love\",\"for,\".\"]\n\n```\n\nsentence1 vector form ``` [1,1,2,1,1,2,2,1,1,1,1,1,0,0,0,2] ```\n\nsentence2 vector form ``` [1,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1] ```\n\ndrawbacks - \n\n* no word order maintained .\n* does not capture semantics \n","c652e1c2":"### Import dataset","ce88921b":"**Co-occurance matrix**\n\nGiven a corpus having N words, the co-occurrence matrix X will be a N x N matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. \n\nFor example,\n\nI like deep learning.\nI like NLP.\nI enjoy learning.\n\n```\n         I  like enjoy deep learning NLP \nI        [0  2    1     0     0]\nlike     [2  0    0     1     0]\nenjoy    [1  0    0     0     1]\ndeep     [0  1    0     0     1]\nlearning [0  0    1     1     0]\nNLP      [0  1    0     0     0]\n\n```\n\n\nThe GloVe process factorizes this co-occurrence matrix into a pair of (word, feature) and (feature, context) matrices. The process is known as Matrix Factorization and is done using Stochastic Gradient Descent (SGD)\n\n\nR = P * Q\n\nThe SGD process will start with P and Q composed of random values and attempt to reconstruct the matrix R' by multiplying them. The difference between the matrices R and R' represents the loss, and is usually computed as the mean-squared error between the two matrices. The loss dictates how much the values of P and Q need\nto change for R' to move closer to R to minimize the reconstruction loss. This process is repeated multiple times until the loss is within some acceptable threshold. At that point the (word, feature) matrix P is the GloVe embedding.\nThe GloVe process is much more resource intensive than Word2Vec. This is because Word2Vec learns the embedding by training over batches of word vectors, while GloVe factorizes the entire co-occurrence matrix in one shot.\n"}}