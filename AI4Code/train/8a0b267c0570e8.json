{"cell_type":{"cb34af39":"code","61af4493":"code","8b3a79d7":"code","3e84d410":"code","cb730d67":"code","9223e9ec":"code","89924ab5":"code","bfc20a9c":"code","57b179a6":"code","fc0bf094":"code","0163efa3":"code","db3b2841":"code","cd433269":"code","bdd85729":"code","6ead543e":"code","0cadcfee":"code","460e890d":"code","73b20666":"code","09e285e5":"code","7f8924e2":"code","fd634b1c":"code","0f628ca9":"code","a1c9b33a":"code","95f13fdc":"code","b02430bd":"code","14cd9f0f":"code","0cefa8b3":"code","62f18b28":"code","4c0572d9":"code","3e1cde24":"code","e7dbb7b5":"markdown","b8311999":"markdown"},"source":{"cb34af39":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nprint(tf.__version__)","61af4493":"# load data\ntrain_data = pd.read_csv(r\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(r\"..\/input\/titanic\/test.csv\")","8b3a79d7":"train_data","3e84d410":"test_data","cb730d67":"# Feature Engineering\n\nfrom sklearn.impute import SimpleImputer\n\ndef nan_padding(data, columns):\n    for column in columns:\n        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n        data[column]=imputer.fit_transform(data[column].values.reshape(-1,1))\n    return data\n\n\nnan_columns = [\"Age\", \"SibSp\", \"Parch\"]\n\ntrain_data = nan_padding(train_data, nan_columns)\ntest_data = nan_padding(test_data, nan_columns)","9223e9ec":"train_data","89924ab5":"#save PassengerId for evaluation\ntest_passenger_id=test_data[\"PassengerId\"]","bfc20a9c":"def drop_not_concerned(data, columns):\n    return data.drop(columns, axis=1)\n\nnot_concerned_columns = [\"PassengerId\",\"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\ntrain_data = drop_not_concerned(train_data, not_concerned_columns)\ntest_data = drop_not_concerned(test_data, not_concerned_columns)","57b179a6":"train_data.head()","fc0bf094":"test_data.head()","0163efa3":"def dummy_data(data, columns):\n    for column in columns:\n        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n        data = data.drop(column, axis=1)\n    return data\n\n\ndummy_columns = [\"Pclass\"]\ntrain_data=dummy_data(train_data, dummy_columns)\ntest_data=dummy_data(test_data, dummy_columns)","db3b2841":"test_data.head()","cd433269":"from sklearn.preprocessing import LabelEncoder\ndef sex_to_int(data):\n    le = LabelEncoder()\n    le.fit([\"male\",\"female\"])\n    data[\"Sex\"]=le.transform(data[\"Sex\"]) \n    return data\n\ntrain_data = sex_to_int(train_data)\ntest_data = sex_to_int(test_data)\ntrain_data.head()","bdd85729":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_age(data):\n    scaler = MinMaxScaler()\n    data[\"Age\"] = scaler.fit_transform(data[\"Age\"].values.reshape(-1,1))\n    return data\ntrain_data = normalize_age(train_data)\ntest_data = normalize_age(test_data)\ntrain_data.head()","6ead543e":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\ndef split_valid_test_data(data, fraction=(1 - 0.8)):\n    data_y = data[\"Survived\"]\n    lb = LabelBinarizer()\n    data_y = lb.fit_transform(data_y)\n\n    data_x = data.drop([\"Survived\"], axis=1)\n\n    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n\n    return train_x.values, train_y, valid_x, valid_y\n\ntrain_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\nprint(\"train_x:{}\".format(train_x.shape))\nprint(\"train_y:{}\".format(train_y.shape))\nprint(\"train_y content:{}\".format(train_y[:3]))\n\nprint(\"valid_x:{}\".format(valid_x.shape))\nprint(\"valid_y:{}\".format(valid_y.shape))","0cadcfee":"train_x.shape[1]","460e890d":"!pip install -U keras-tuner","73b20666":"# Build Neural Network\nimport kerastuner\n\ndef build_neural_network(hp):    \n    initializer = tf.keras.initializers.GlorotUniform()\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(units = hp.Int('units', min_value = 5, max_value = 500, step = 25),\n                                    activation=None,kernel_initializer=initializer,\n                                    input_shape = ( train_x.shape[1],)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n    \n    model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4])), metrics = ['accuracy'])\n\n    return model\n\ntuner = kerastuner.tuners.RandomSearch(\n    build_neural_network,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3)\n\n#model = build_neural_network(learning_rate_value)","09e285e5":"epochs = 50\ntuner.search(train_x, train_y, epochs = 200,validation_data = (valid_x, valid_y))","7f8924e2":"tuner.results_summary()","fd634b1c":"op_model = tuner.get_best_models()[0]\nop_model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.001), metrics = ['accuracy'])","0f628ca9":"\nhistory = op_model.fit(train_x, train_y, epochs = 200, validation_data = (valid_x, valid_y))","a1c9b33a":"## this code was stolen from udacity colabs\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","95f13fdc":"len(test_data)","b02430bd":"test_data","14cd9f0f":"passenger_id=test_passenger_id.copy()\nevaluation=passenger_id.to_frame()\nevaluation[\"Survived\"]=[int(bool(i)) for i in op_model.predict(test_data)]\nevaluation[:10]","0cefa8b3":"evaluation.to_csv(\"evaluation_submission.csv\",index=False)","62f18b28":"answers = pd.read_csv(r\"..\/input\/titanic\/gender_submission.csv\")","4c0572d9":"from sklearn.metrics import confusion_matrix\nc = confusion_matrix(answers['Survived'], evaluation['Survived'], normalize = 'true')","3e1cde24":"c","e7dbb7b5":"Okay, so the things that we need for the model are:\n\ninputs, labels, learning_rate, is_training, logits, cost, optimizer, predicted, and accuracy","b8311999":"I'd like to thank the author of the notebook \"Tensorflow- Deep Learning to Solve Titanic\", from which this code is adapted."}}