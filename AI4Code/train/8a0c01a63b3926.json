{"cell_type":{"6fa98830":"code","78247c83":"code","d4ba4b66":"code","a90df143":"code","615ce06d":"code","ba823183":"code","b38bc04a":"code","13ba239b":"code","389b6da2":"code","3c0c22ca":"code","d82d5910":"code","8100c629":"code","8d574a73":"code","222c846d":"code","5c85a3a8":"code","2e377780":"code","434fa3fb":"code","99fe9ef2":"code","39068839":"code","5f7dbc89":"code","fe762438":"code","5454f5c7":"code","4162a7dc":"code","bd4b2c45":"code","254f4c60":"code","36a833f0":"markdown","d3759f70":"markdown","9c64f01d":"markdown","d0970b14":"markdown","2d7772ae":"markdown","c6c094d2":"markdown","972be125":"markdown","1031ac0f":"markdown","6cb1aa5c":"markdown"},"source":{"6fa98830":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","78247c83":"import sys\nsys.path.append(\"..\/input\/tez-lib\/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","d4ba4b66":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out[0]\n        logits = self.output(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","a90df143":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }","615ce06d":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","ba823183":"def postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions","b38bc04a":"tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/xlmrob\")","13ba239b":"pad_on_right = tokenizer.padding_side == \"right\"\nmax_length = 400\ndoc_stride = 135\n\ntest_data = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\ntest_dataset = Dataset.from_pandas(test_data)\ntest_features = test_dataset.map(\n    partial(\n        prepare_validation_features, \n        tokenizer=tokenizer,\n        pad_on_right=pad_on_right, \n        max_length=max_length,\n        doc_stride=doc_stride\n    ),\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(\n    lambda example: example, remove_columns=['example_id', 'offset_mapping']\n)\n\nfin_start_logits = None\nfin_end_logits = None\n\nfor fold_ in tqdm(range(10)):\n    model = ChaiiModel(model_name=\"..\/input\/xlmrob\", num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n    model.load(f\"..\/input\/deepsetsquad2-v2\/pytorch_model_f{fold_}.bin\", weights_only=True)\n    model.to(\"cuda\")\n    model.eval()\n    data_loader = torch.utils.data.DataLoader(\n        ChaiiDataset(test_feats_small), \n        batch_size=32,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n    start_logits = []\n    end_logits = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output, _, _ = model(**data)\n            start = output[0].detach().cpu().numpy()\n            end = output[1].detach().cpu().numpy()\n            start_logits.append(start)\n            end_logits.append(end)\n\n    start_logits = np.vstack(start_logits)\n    end_logits = np.vstack(end_logits)\n    \n    if fin_start_logits is None:\n        fin_start_logits = start_logits\n        fin_end_logits = end_logits\n    else:\n        fin_start_logits += start_logits\n        fin_end_logits += end_logits\n        \n    del model\n    torch.cuda.empty_cache()","389b6da2":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nfrom string import punctuation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n# Now Create Function\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","3c0c22ca":"class Configration:\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n    config_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\"\n    max_seq_length = 400\n    doc_stride = 135\n\n    # train\n    epochs = 1\n    train_batch_size = 4\n    eval_batch_size = 128\n\n    # optimzer\n    optimizer_type = 'AdamW'\n    learning_rate = 1e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","d82d5910":"# Dataset_Retriever class\nclass Dataset_Retriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(Dataset_Retriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","8100c629":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","8d574a73":"def Make_Model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model","222c846d":"def Prepare_Test_Features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","5c85a3a8":"import collections","2e377780":"def Postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","434fa3fb":"test_df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')","99fe9ef2":"test_df['context'] = test_df['context'].apply(lambda x: ' '.join(x.split()))\ntest_df['question'] = test_df['question'].apply(lambda x: ' '.join(x.split()))","39068839":"\ntokenizer = AutoTokenizer.from_pretrained(Configration().tokenizer_name)\n\ntest_features = []\nfor i, row in test_df.iterrows():\n    \n    # Now Calling Function and Calling Class\n    \n    test_features += Prepare_Test_Features(Configration(), row, tokenizer)\n\n# Now Calling Classes\n\nargs = Configration()\ntest_dataset = Dataset_Retriever(test_features, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.eval_batch_size, \n    sampler=SequentialSampler(test_dataset),\n    num_workers=optimal_num_of_loader_workers(),\n    pin_memory=True, \n    drop_last=False\n)","5f7dbc89":"base_model = '..\/input\/5foldsroberta\/output\/'","fe762438":"def Get_Predictions(checkpoint_path):\n    \n    # Calling Function Make_Model and Class Configration\n    \n    config, tokenizer, model = Make_Model(Configration())\n    model.cuda();\n    model.load_state_dict(\n        torch.load(base_model + checkpoint_path)\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","5454f5c7":"start_logits1, end_logits1 = Get_Predictions('checkpoint-fold-0\/pytorch_model.bin')\nstart_logits2, end_logits2 = Get_Predictions('checkpoint-fold-1\/pytorch_model.bin')\nstart_logits3, end_logits3 = Get_Predictions('checkpoint-fold-2\/pytorch_model.bin')\nstart_logits4, end_logits4 = Get_Predictions('checkpoint-fold-3\/pytorch_model.bin')\nstart_logits5, end_logits5 = Get_Predictions('checkpoint-fold-4\/pytorch_model.bin')\n","4162a7dc":"fin_start_logits \/= 10\nfin_end_logits \/= 10","bd4b2c45":"start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) \/ 5)\nend_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 + end_logits5) \/ 5)\n\n\n# Now Calling Function \n\nfin_preds = Postprocess_qa_predictions(test_df, test_features, (start_logits, end_logits))\n\nsubmission = []\nfor p1, p2 in fin_preds.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n    \nsample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\ntest_data =pd.merge(left=test_df,right=sample,on='id')","254f4c60":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\ntamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\ntamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\ntamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\nhindi_ad = \"\u0908\"\nhindi_bc = \"\u0908.\u092a\u0942\"\n\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n\ntest_data[\"PredictionString\"] = cleaned_preds\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)","36a833f0":"# Now read test dataset","d3759f70":"Upvote these notebooks. Upvote my one if it helps :)\n\nhttps:\/\/www.kaggle.com\/mihtw1\/chaii-ensemble\n\nhttps:\/\/www.kaggle.com\/abhishek\/hello-friends-chaii-pi-lo\n\nhttps:\/\/www.kaggle.com\/jillanisofttech\/chaii-pe-lo-g-with-acc-0-792\n\nhttps:\/\/www.kaggle.com\/nbroad\/chaii-qa-torch-5-fold-with-post-processing-765\n\nhttps:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n\nhttps:\/\/www.kaggle.com\/adldotori\/how-to-qa-with-xlmr5\n\nhttps:\/\/www.kaggle.com\/kishalmandal\/5foldsroberta\n\n","9c64f01d":"# Now Create Get Prediction Function for helping the model ","d0970b14":"# Now Calling Get Predictions Function","2d7772ae":"# This making model function helping AutoConfig and AutoTokenizer the data","c6c094d2":"### My ChangeLog:\n- V4: Ensemble Hello Friend Chaii pi lo & Chaii Pe Lo G\n- V5:[Error] Change fin_start_logits \/= 6 -> fin_start_logits \/= 10 (Forgot to change iteration 6 -> 10 for fin_start_logits )\n- V6:[Error] Change fin_start_logits \/= 6 -> fin_start_logits \/= 10 (Forgot to change iteration 6 -> 10 for fin_start_logits )\n- V7: Change fin_start_logits \/= 6 -> fin_start_logits \/= 10 (agian)\n- V8: [Error] `start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) \/ 5)` \n      `end_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 +  end_logits5) \/ 5)` [Forgor to Turn of GPU] \n<br>\n- V9: `start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) \/ 5)` \n      `end_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 +  end_logits5) \/ 5)` (Again)","972be125":"# Now Create Custom CLass","1031ac0f":"# Now import collections lib and create function","6cb1aa5c":"# Now Creating Model building class"}}