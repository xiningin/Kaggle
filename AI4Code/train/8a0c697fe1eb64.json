{"cell_type":{"24d9e62a":"code","94ee6025":"code","6ebef710":"code","0f8f35ef":"code","cf93fcc3":"code","e288406c":"code","9642044f":"code","649939be":"code","42fdfb07":"code","a2bf66e9":"code","7bd2f242":"code","3a7e92c2":"code","c13b4e9b":"code","bd5ad718":"code","b9bb4c8c":"code","fcffe68c":"code","ae3d8a4e":"code","b015314f":"code","cb0702fd":"code","a7763a12":"code","ceac662a":"code","46f20edb":"code","80db3e84":"code","37e7ab28":"code","ced78586":"code","4e9c5de5":"code","622bee4d":"code","17f14991":"code","b9170204":"code","01a32cfa":"code","cde23859":"code","ae574c53":"code","d5c59a1c":"code","c620b01f":"code","e577ee52":"code","7c84945e":"code","4a8ef9b3":"code","51b9f3c8":"code","f74ca953":"code","9acaed29":"code","9b82efee":"code","5404cc75":"code","f396d9e7":"code","eb154083":"code","23577628":"code","f3468f7d":"code","0b46bb09":"code","ae001a8e":"code","faf40129":"code","74507480":"code","055a3925":"code","8476fc6f":"code","67689bcf":"code","bf1f85cc":"code","6122875b":"code","df515064":"code","29b52ee0":"code","9eb18f2e":"code","b5008fd0":"code","8ff20a83":"code","6203861e":"code","db46432f":"code","dc5977db":"code","2f2f7520":"code","b90d61b0":"code","9e76185a":"code","f02cc546":"code","f035362f":"code","96affe26":"code","8260fe52":"code","7a5f6670":"code","b6f8d2ba":"code","1c21055c":"code","1da6cd94":"code","6288c707":"markdown","e342bf32":"markdown","42cac202":"markdown","2e80d256":"markdown","b5d3ce02":"markdown","895723c3":"markdown","840dc33f":"markdown","52abf737":"markdown","dc8c601b":"markdown","22361efd":"markdown","8e92de4b":"markdown","a61be639":"markdown","8abf5b6c":"markdown","90f44e48":"markdown","a2056845":"markdown","c7c0bcd8":"markdown","d92b210d":"markdown","32b6422c":"markdown","13ed4557":"markdown","835b857d":"markdown"},"source":{"24d9e62a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","94ee6025":"train = pd.read_csv('..\/input\/iba-ml1-mid-project\/train.csv')\ntest = pd.read_csv('..\/input\/iba-ml1-mid-project\/test.csv')\n","6ebef710":"train['credit_line_utilization'] = train['credit_line_utilization'].str.replace(',' , '.').astype(float)","0f8f35ef":"test['credit_line_utilization'] = test['credit_line_utilization'].str.replace(',' , '.').astype(float)","cf93fcc3":"print('We have {} training rows and {} test rows.'.format(train.shape[0], test.shape[0]))\nprint('We have {} training columns and {} test columns.'.format(train.shape[1], test.shape[1]))\ntrain.head(2)","e288406c":"from sklearn.impute import SimpleImputer\nimpute_median = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\nfor var in train.columns.values:\n    if train[var].isnull().sum() != 0:\n        train[var] = impute_median.fit_transform(train[[var]]).ravel()\ndisplay(train.isnull().sum())","9642044f":"for var in test.columns.values:\n    if test[var].isnull().sum() != 0:\n        test[var] = impute_median.fit_transform(test[[var]]).ravel()\n        \ndisplay(test.isnull().sum())","649939be":"train.rename(columns={'number_of_credit_lines': 'no_cdtline', 'number_dependent_family_members': 'no_dep_fmemb', 'ratio_debt_payment_to_income': 'debtpay_inc_ratio',\n                      'credit_line_utilization': 'cdtline_util', 'number_of_previous_late_payments_up_to_59_days': 'no_latepay_up_59d', \n                      'number_of_previous_late_payments_up_to_89_days': 'no_latepay_up_89d', 'number_of_previous_late_payments_90_days_or_more': 'no_latepay_90d_more'}, inplace=True)","42fdfb07":"test.rename(columns={'number_of_credit_lines': 'no_cdtline', 'number_dependent_family_members': 'no_dep_fmemb', 'ratio_debt_payment_to_income': 'debtpay_inc_ratio',\n                      'credit_line_utilization': 'cdtline_util', 'number_of_previous_late_payments_up_to_59_days': 'no_latepay_up_59d', \n                      'number_of_previous_late_payments_up_to_89_days': 'no_latepay_up_89d', 'number_of_previous_late_payments_90_days_or_more': 'no_latepay_90d_more'}, inplace=True)","a2bf66e9":"train = train.drop('Id', axis = 1)\ntest = test.drop('Id', axis = 1)","7bd2f242":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler","3a7e92c2":"X = train.drop(['defaulted_on_loan'], axis=1)\ny = train['defaulted_on_loan']","c13b4e9b":"#iso = IsolationForest(contamination=0.1)\n#yhat = iso.fit_predict(X_combined_sampling)\n#mask = yhat != -1\n#X_combined_sampling, y_combined_sampling = X_combined_sampling[mask,:], y_combined_sampling[mask]","bd5ad718":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split","b9bb4c8c":"X_train,X_cv,y_train,y_cv = train_test_split(X, y,test_size=0.18, random_state = 42, stratify = y)","fcffe68c":"from sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier","ae3d8a4e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score","b015314f":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","cb0702fd":"model =  XGBClassifier(max_depth = 13,min_child_weight=6, subsample=0.7, random_state=23)\nmodel.fit(X_train, y_train)","a7763a12":"plt.figure(figsize=(20,15)) \nxgb.plot_importance(model)","ceac662a":"plt.figure(figsize=(40,25)) \nxgb.plot_tree(model, ax=plt.gca())","46f20edb":"y_predXGB = model.predict(X_cv)\nprint(classification_report(y_cv, y_predXGB))\nprint(accuracy_score(y_train, model.predict(X_train)))\nprint(accuracy_score(y_cv, model.predict(X_cv)))\n","80db3e84":"roc_auc_score(y_cv, model.predict_proba(X_cv)[:,1])","37e7ab28":"from eli5.sklearn import PermutationImportance\nfrom eli5 import show_weights\n\nperm = PermutationImportance(model).fit(X_cv,y_cv)\n\nshow_weights(perm, feature_names = list(X_cv.columns))","ced78586":"from sklearn.metrics import classification_report, roc_curve, precision_recall_curve, roc_auc_score","4e9c5de5":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(criterion = 'entropy', n_estimators = 300, max_depth = 8, min_samples_leaf = 4, max_features = 'auto')","622bee4d":"rf.fit(X_train,y_train)","17f14991":"y_pred2 = rf.predict(X_cv)\nprint(classification_report(y_cv, y_pred2))\nprint('Train performance', accuracy_score(y_train, rf.predict(X_train)))\nprint('Test performance', accuracy_score(y_cv, rf.predict(X_cv)))","b9170204":"pred_test2 = rf.predict_proba(test)[:,1]\npred_test2","01a32cfa":"from sklearn.metrics import confusion_matrix","cde23859":"print(roc_auc_score(y_cv, rf.predict_proba(X_cv)[:,1]))\nprint(roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))","ae574c53":"confusion_matrix(y_cv, y_pred2)","d5c59a1c":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rf,  X_cv, y_cv)","c620b01f":"from sklearn.inspection import permutation_importance","e577ee52":"perm = PermutationImportance(rf).fit(X_cv,y_cv)\n\nshow_weights(perm, feature_names = list(X_cv.columns))","7c84945e":"features= train.drop('defaulted_on_loan', axis = 1)\nfeature_list = list(features.columns)","4a8ef9b3":"importances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","51b9f3c8":"plt.style.use('fivethirtyeight')\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance'); \nplt.xlabel('Variable'); \nplt.title('Variable Importances');","f74ca953":"# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n","9acaed29":"\n# Find number of features for cumulative importance of 95%\n# Add 1 because Python is zero-indexed\nprint('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)","9b82efee":"feature_importances","5404cc75":"# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:8]]\n# Find the columns of the most important features\nimportant_indices = [feature_list.index(feature) for feature in important_feature_names]\n# Create training and testing sets with only the important features\nimportant_train_features = X_train.iloc[:, important_indices]\nimportant_test_features = X_cv.iloc[:, important_indices]\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_test_features.shape)","f396d9e7":"important_feature_names","eb154083":"rf.fit(important_train_features, y_train)","23577628":"y_pred2 = rf.predict(important_test_features)\nprint(classification_report(y_cv, y_pred2))\nprint('Train performance', accuracy_score(y_train, rf.predict(important_train_features)))\nprint('Test performance', accuracy_score(y_cv, rf.predict(important_test_features)))   \nprint(roc_auc_score(y_cv, rf.predict_proba(important_test_features)[:,1]))\nprint(roc_auc_score(y_train, rf.predict_proba(important_train_features)[:,1]))","f3468f7d":"pred_test2 = rf.predict_proba(test[important_feature_names])[:,1]","0b46bb09":"Id = np.array(range(1,48109))","ae001a8e":"sample_submissionRF =pd.DataFrame.from_dict({'Id':Id,'Predicted':pred_test2})\nsample_submissionRF.to_csv('sample_submissionRF.csv', index = False)","faf40129":"sample_submissionRF.shape","74507480":"dt=DecisionTreeClassifier(criterion='gini', min_samples_leaf = 9, max_depth= 14)\ndt.fit(X_train,y_train)","055a3925":"y_pred3 = dt.predict(X_cv)","8476fc6f":"print(classification_report(y_cv, y_pred3))\nprint('Train performance', accuracy_score(y_train, dt.predict(X_train)))\nprint('Test performance', accuracy_score(y_cv, dt.predict(X_cv)))\nprint(roc_auc_score(y_cv, dt.predict_proba(X_cv)[:,1]))\nprint(roc_auc_score(y_train, dt.predict_proba(X_train)[:,1]))","67689bcf":"pred_test3 = dt.predict_proba(test)[:,1]","bf1f85cc":"perm = PermutationImportance(dt).fit(X_cv,y_cv)\n\nshow_weights(perm, feature_names = list(X_cv.columns))","6122875b":"from sklearn.ensemble import VotingClassifier\nvot_clf = VotingClassifier(estimators = [('XGB', model), ('DesTree', dt), ('RandomFor', rf)], voting = 'soft')","df515064":"vot_clf.fit(X_train, y_train)","29b52ee0":"pred = vot_clf.predict(X_cv)\naccuracy_score(y_cv, pred)","9eb18f2e":"print(classification_report(y_cv, pred))\nprint('Train performance', accuracy_score(y_train, vot_clf.predict(X_train)))\nprint('Test performance', accuracy_score(y_cv, vot_clf.predict(X_cv)))\nprint(roc_auc_score(y_cv, vot_clf.predict_proba(X_cv)[:,1]))\nprint(roc_auc_score(y_train, vot_clf.predict_proba(X_train)[:,1]))","b5008fd0":"for clf in (model, rf, dt):\n  clf.fit(X_train, y_train)\n  prediction = clf.predict(X_cv)\n  print(clf.__class__.__name__, accuracy_score(y_cv, prediction))","8ff20a83":"perm = PermutationImportance(vot_clf).fit(X_cv,y_cv)\n\nshow_weights(perm, feature_names = list(X_cv.columns))","6203861e":"from sklearn.ensemble import GradientBoostingClassifier","db46432f":"clf = GradientBoostingClassifier(n_estimators=300, learning_rate=.1, max_depth= 5) ","dc5977db":"clf.fit(X_train, y_train)","2f2f7520":"y_pred4 = clf.predict(X_cv)\nprint(classification_report(y_cv, y_pred4))\nprint('Train performance', accuracy_score(y_train, clf.predict(X_train)))\nprint('Test performance', accuracy_score(y_cv, clf.predict(X_cv)))\nprint(roc_auc_score(y_cv, clf.predict_proba(X_cv)[:,1]))\nprint(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))","b90d61b0":"perm = PermutationImportance(clf).fit(X_cv,y_cv)\n\nshow_weights(perm, feature_names = list(X_cv.columns))","9e76185a":"from sklearn.metrics import roc_curve, roc_auc_score","f02cc546":"r_probs = [0 for _ in range(len(y_cv))]","f035362f":"RF_probs = rf.predict_proba(X_cv)[:,1]\nDT_probs = dt.predict_proba(X_cv)[:,1]\nVot_probs = vot_clf.predict_proba(X_cv)[:,1]\nXGB_probs = model.predict_proba(X_cv)[:,1]\nclf_probs = clf.predict_proba(X_cv)[:,1]\n","96affe26":"#ROC is the receiver operating characteristic AUROC is the area under the ROC curve\nr_auc = roc_auc_score(y_cv, r_probs)\nRF_auc = roc_auc_score(y_cv, rf.predict_proba(X_cv)[:,1])\nDT_auc = roc_auc_score(y_cv, dt.predict_proba(X_cv)[:,1])\nVot_auc = roc_auc_score(y_cv, vot_clf.predict_proba(X_cv)[:,1])\nXGB_auc = roc_auc_score(y_cv, model.predict_proba(X_cv)[:,1])\nGradB_auc = roc_auc_score(y_cv, clf.predict_proba(X_cv)[:,1])","8260fe52":"print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))\nprint('Random Forest: AUROC = %.3f' % (RF_auc))\nprint('Decision Tree: AUROC = %.3f' % (DT_auc))\nprint('Voting: AUROC = %.3f' % (Vot_auc))\nprint('XGB: AUROC = %.3f' % (XGB_auc))\nprint('GradB: AUROC = %.3f' % (GradB_auc))","7a5f6670":"#fpr - FP rate \n#tpr - TP rate\nr_fpr, r_tpr, _ = roc_curve(y_cv, r_probs)\nRF_fpr, RF_tpr, _ = roc_curve(y_cv, rf.predict_proba(X_cv)[:,1])\nDT_fpr, DT_tpr, _ = roc_curve(y_cv, dt.predict_proba(X_cv)[:,1])\nVot_fpr, Vot_tpr, _ = roc_curve(y_cv, vot_clf.predict_proba(X_cv)[:,1])\nXGB_fpr, XGB_tpr, _ = roc_curve(y_cv, model.predict_proba(X_cv)[:,1])\nclf_fpr, clf_tpr, _ = roc_curve(y_cv, clf.predict_proba(X_cv)[:,1])","b6f8d2ba":"plt.figure(figsize=(15, 10))\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %.3f)' % r_auc)\nplt.plot(RF_fpr, RF_tpr, marker='.', label='Random Forest: (AUROC = %.3f)' % RF_auc)\nplt.plot(DT_fpr, DT_tpr, marker='v', label='Decision Tree: (AUROC = %.3f)' % DT_auc)\nplt.plot(Vot_fpr, Vot_tpr, marker='v', label='Votin: (AUROC = %.3f)' % Vot_auc)\nplt.plot(XGB_fpr, XGB_tpr, marker='o', label='XGBoost: (AUROC = %.3f)' % XGB_auc)\nplt.plot(clf_fpr, clf_tpr, marker='D', label='BradBoost: (AUROC = %.3f)' % GradB_auc)\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()","1c21055c":"pred_test = rf.predict_proba(test)[:,1]\nsample_submissionRF =pd.DataFrame.from_dict({'Id':Id,'Predicted':pred_test2})\nsample_submissionRF.to_csv('sample_submissionRF.csv', index = False)","1da6cd94":"sample_submissionRF","6288c707":"# Starting","e342bf32":"param_space={\n    'criterion':['gini', 'entropy'],\n    'n_estimators': range(200, 600, 100),\n    'max_depth': range(1,10, 4),\n    'min_samples_leaf': range(2,10, 2) \n}","42cac202":"Importing datasets","2e80d256":"Doing the same step for test data ","b5d3ce02":"# Class Imbalance \n###### There is an imbalance of approximately 13 to 1 ratio. If a classification model is trained on an imbalanced dataset, it would be highly biased towards the dominant class. Thus, the model would reflect the underlying class distribution. In order to have an accurate model, we need to solve the imbalance issue.\n###### In addition, when implementing Smote or other techniques, along with scaling algorithms, model can overfit and give us a higher roc_auc_score. Based on that, I decided know using Scaling or Class Imbalance","895723c3":"Score for the random forest model was higher than others","840dc33f":"# GradientBoostingClassifier","52abf737":"over = RandomOverSampler(sampling_strategy=0.5)\nunder = RandomUnderSampler(sampling_strategy=0.8)\nX_over, y_over = over.fit_resample(X, y)\nprint(f\"Oversampled: {Counter(y_over)}\")\nX_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over)\nprint(f\"Combined Random Sampling: {Counter(y_combined_sampling)}\")\n##### once used this one, but didnt get good result","dc8c601b":"#### Outlier Detection using Isolation Forest\n\n###### with the help of Isolation Forest we can find multivariate outliers in our data. But again as mentioned earlier when using Iso Forest, model easily overfits. Of course there can be some other techniques for dealing with the problem. But at the moment it is not that much easy.","22361efd":"### Submission File","8e92de4b":"# Voting Classifier","a61be639":"# Decision Tree ","8abf5b6c":"# Random Forest Classifier","90f44e48":"oversample = SMOTE(random_state=1234)\nX, y = oversample.fit_resample(X, y)\ncounter = Counter(y)\nprint(counter)","a2056845":"# Visualization of AUROC","c7c0bcd8":"So from our visual as well, Random Forest algorithm performs better than the others. ","d92b210d":"# Feature importance\n##### so here we will select some features according to the model. Then look at the result of them. \n","32b6422c":"Drop unnecessary Id column","13ed4557":"Missing value imputation","835b857d":"# XGBoost Classifier\n"}}