{"cell_type":{"6e7d7e42":"code","7b34b0bf":"code","dd6d851e":"code","dbe68e8a":"code","df48fac4":"code","731c8452":"code","92a8cb91":"code","1eaf5e34":"code","83bcd928":"code","c45ee6c3":"code","6b9bfb9a":"code","375f142b":"code","f8fadfe5":"code","3d982b26":"code","56a52b4d":"code","ee168799":"code","471fc324":"code","22853086":"code","429fb55e":"code","0fbbc66d":"markdown","1af968a0":"markdown","4a284292":"markdown","8b6d2993":"markdown","e7b75109":"markdown","ac2653a0":"markdown","cbc03618":"markdown","fa7765f1":"markdown"},"source":{"6e7d7e42":"import os, shutil","7b34b0bf":"original_dataset_dir = \"\/Users\/aryamanbabber\/Desktop\/Dogs_vs_Cats_Classification\/kaggle_original_data\"\n\n # Creates smaller data set\nbase_dir = \"..\/input\/cats-and-dogs-small\/cats_and_dogs_small\"\n# os.mkdir(base_dir)\n\n# Creates directories for training, validation, and testing\ntrain_dir = os.path.join(base_dir, 'train') \n# os.mkdir(train_dir)\n\nvalidation_dir = os.path.join(base_dir, 'validation') \n# os.mkdir(validation_dir)\n\ntest_dir = os.path.join(base_dir, 'test') \n# os.mkdir(test_dir)","dd6d851e":"# Creates directories for cats\ntrain_cats_dir = os.path.join(train_dir, 'cats') \n# os.mkdir(train_cats_dir)\n\nvalidation_cats_dir = os.path.join(validation_dir, 'cats') \n# os.mkdir(validation_cats_dir)\n\ntest_cats_dir = os.path.join(test_dir, 'cats') \n# os.mkdir(test_cats_dir)\n\n# Creates directories for dogs\ntrain_dogs_dir = os.path.join(train_dir, 'dogs') \n# os.mkdir(train_dogs_dir)\n\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs') \n# os.mkdir(validation_dogs_dir)\n\ntest_dogs_dir = os.path.join(test_dir, 'dogs') \n# os.mkdir(test_dogs_dir)","dbe68e8a":"# PREPARES CATS DATA\n# Copies 1,000 images for training set\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname) \n    dst = os.path.join(train_cats_dir, fname) \n    shutil.copyfile(src, dst)\n\n# Copies 500 images for validation set    \nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname) \n    shutil.copyfile(src, dst)\n\n# Copies 500 images for test set    \nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname) \n    dst = os.path.join(test_cats_dir, fname) \n    shutil.copyfile(src, dst)","df48fac4":"# PREPARES DOGS DATA\n# Copies 1,000 images for training set\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname) \n    dst = os.path.join(train_dogs_dir, fname) \n    shutil.copyfile(src, dst)\n\n# Copies 500 images for validation set    \nfnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname) \n    dst = os.path.join(validation_dogs_dir, fname) \n    shutil.copyfile(src, dst)\n\n# Copies 500 images for test set    \nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)] \nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname) \n    dst = os.path.join(test_dogs_dir, fname) \n    shutil.copyfile(src, dst)","731c8452":"# Validate # of images in each set\n# Cat images\nprint('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total test cat images:', len(os.listdir(test_cats_dir)))\n\n# Dog images\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))\nprint('total test dog images:', len(os.listdir(test_dogs_dir)))","92a8cb91":"from tensorflow.keras import layers\nfrom tensorflow.keras import models","1eaf5e34":"from tensorflow.keras import optimizers\n\n\nmodel = models.Sequential() \n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=['acc'])\n\nmodel.summary()","83bcd928":"# Converts JPEG files to floating point tensors, which represent RGB grids of pixels\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255) \ntest_datagen = ImageDataGenerator(rescale=1.\/255)","c45ee6c3":"train_generator = train_datagen.flow_from_directory(train_dir, \n                                                    target_size=(150,150), \n                                                    batch_size=20, \n                                                    class_mode=\"binary\")\n\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,\n                                                    target_size=(150,150), \n                                                    batch_size=20, \n                                                    class_mode=\"binary\")","6b9bfb9a":"# Fits the model\nhistory = model.fit(train_generator,                    \n                    steps_per_epoch=100,                    \n                    epochs=15,                    \n                    validation_data=validation_generator,                    \n                    validation_steps=50)\n\n# epoch 15 validation: val_loss: 0.5937 - val_acc: 0.7240","375f142b":"# Saves the trained model\nmodel.save('cats_and_dogs_small_1.h5')","f8fadfe5":"# Displays loss\/accuracy graphs of training\nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc') \nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","3d982b26":"from tensorflow.keras import layers\nfrom tensorflow.keras import models","56a52b4d":"# Creating a new network that implements dropout\n\nmodel = models.Sequential() \nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) \nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2))) \nmodel.add(layers.Flatten()) \n\nmodel.add(layers.Dropout(0.5)) \nmodel.add(layers.Dense(512, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nfrom tensorflow.keras import optimizers\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1e-4),\n              metrics=['acc'])","ee168799":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Defines generator to augment data\ntrain_datagen = ImageDataGenerator(rotation_range=40,\n                                   width_shift_range=0.2, \n                                   height_shift_range=0.2, \n                                   shear_range=0.2, \n                                   zoom_range=0.2, \n                                   horizontal_flip=True, \n                                   fill_mode='nearest')\n\n# Augments training data\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=(150, 150),\n                                                    batch_size=32,\n                                                    class_mode='binary')\n\n# Generator to rescale data (DO NOT AUGMENT TESTING DATA)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# Rescales validation data\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,\n                                                        target_size=(150, 150),\n                                                        batch_size=32,\n                                                        class_mode='binary')","471fc324":"# Trains model using new augmented training data and rescaled validation data\nhistory = model.fit(train_generator,                   \n                    steps_per_epoch=len(train_generator),                    \n                    epochs=100,                    \n                    validation_data=validation_generator,                    \n                    validation_steps=50)\n\n# epoch 100: loss: 0.4910 - acc: 0.7675","22853086":"# Saves the trained model\nmodel.save('cats_and_dogs_small_2.h5')","429fb55e":"# Displays loss\/accuracy graphs of training\nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss'] \nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc') \n#plt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training accuracy') \nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\n#plt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training loss')\nplt.legend()\nplt.show()","0fbbc66d":"### Data Preparation","1af968a0":"### Training & Graphing the Data","4a284292":"### Building the Network","8b6d2993":"# Basic Convolutional Network","e7b75109":"### Correcting the Data","ac2653a0":"### Implementing Data Augmentation","cbc03618":"### Implementing Dropout to Convolutional Network","fa7765f1":"# Convolutional Network Improved"}}