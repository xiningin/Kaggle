{"cell_type":{"55d32ea5":"code","9ed80f92":"code","25468ce4":"code","2f237336":"code","ba728c17":"code","06e7f71f":"code","d0549a34":"code","2339d63b":"code","5c028a57":"code","5237c2a9":"code","7da17f8e":"markdown","d9ed5d55":"markdown","9a46e598":"markdown","be2b3b20":"markdown","4b614aa3":"markdown","501b0176":"markdown","b5782ca2":"markdown","aa15dad2":"markdown","be93288e":"markdown","096be9e8":"markdown","431d5156":"markdown","b276f4d1":"markdown","437ae175":"markdown","febd7b0f":"markdown","f81beff9":"markdown","7ab97c86":"markdown"},"source":{"55d32ea5":"import numpy as np\nimport pandas as pd\n\n\ndata = \"..\/input\/fe-course-data\/housing.csv\"\ntarget_feature = \"HouseAge\"\n\n\n# data = \"..\/input\/fe-course-data\/autos.csv\"\n# target_feature = \"price\"\n\n# Classification\n# target_feature = \"aspiration\"\n# target_feature = \"drive_wheels\"\n\n\n# data = \"..\/input\/fe-course-data\/concrete.csv\"\n# target_feature = \"CompressiveStrength\"\n\n\n# data = \"..\/input\/fe-course-data\/ames.csv\"\n# target_feature = \"SalePrice\"\n\n\n# data = \"..\/input\/fe-course-data\/accidents.csv\"\n# target_feature = \"Distance\"\n\n# Classification\n# target_feature = \"Severity\"\n\n\n\ndf = pd.read_csv(data)\npredictors = list(df.columns)\npredictors.remove(target_feature)\ndf","9ed80f92":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encoding(df):\n    for j in df.columns:\n        el_type = df[j].dtype\n        if el_type == 'object':\n            df[j].replace(np.nan, 'NoNoNo', inplace=True)\n            labelencoder = LabelEncoder()\n            df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        elif el_type == 'bool':\n            df.loc[:, j] = df[j].replace({False: 0, True: 1})\n    return df\n\nencoding(df)","25468ce4":"def normalization(df, predictors):\n    for col in predictors:\n        min_x = df[[col]].min()\n        max_x = df[[col]].max()\n        df[[col]] = (df[[col]] - min_x) \/ (max_x - min_x)\n    return df\n\nnormalization(df, predictors)","2f237336":"# train_test_split_ordered works correctly with test_size = 0.1, 0.2, 0.25, 0.33, 0.5 \n\ndef train_test_split_ordered(df, target_feature, test_size=0.33, verbose=0, research_iter=0):\n    \n    import matplotlib.pyplot as plt\n    from lightgbm import LGBMRegressor\n    from lightgbm import LGBMClassifier \n    from sklearn.inspection import permutation_importance\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import f1_score\n\n    \n    global df_work\n    \n    \n    CLASSIFIER_FOR_UNIQUE_VALUES_LESS_THAN = 50\n    \n\n    df_work = pd.DataFrame()\n    important_functions = []\n    predictors = []\n    ordered_columns = []\n    \n    \n    def get_predictors(df, target_feature):\n        predictors = list(df.columns)\n        predictors.remove(target_feature)\n        return predictors\n    \n    \n    def get_X(df, predictors):\n        X = df[predictors]\n        return X\n    \n    \n    def get_y(df, target_feature):\n        y = df[[target_feature]]\n        return y\n    \n    \n    def regression_score(train_X, test_X, train_y, test_y):\n        model = LGBMRegressor(random_state=0).fit(train_X, train_y)\n        predict = model.predict(test_X)\n        return mean_absolute_error(predict, test_y)\n    \n    \n    def classification_accuracy(train_X, test_X, train_y, test_y):\n        model = LGBMClassifier(random_state=0).fit(train_X, train_y.values.ravel())\n        predict = model.predict(test_X)\n        return f1_score(predict, test_y.values.ravel(), average='weighted')\n    \n    \n    def get_research(X, y, target_feature, test_size, research_iter):\n        RESULTS = pd.DataFrame()\n        print('\\n-----------------------------------')\n        if len(y.value_counts()) > CLASSIFIER_FOR_UNIQUE_VALUES_LESS_THAN:\n            print('Regression research by sklearn.model_selection.train_test_split:\\n')\n            for random_state in range(0,research_iter):\n                train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=random_state)\n                RESULTS.loc[random_state, 'score'] = regression_score(train_X, test_X, train_y, test_y)\n            print(f'Regression MAE with random_state from 0 to {research_iter - 1}:')\n        else:\n            print('Classification research by sklearn.model_selection.train_test_split:\\n')\n            print(f'Target function has {len(y.value_counts())} unique values.')\n            for random_state in range(0,research_iter):\n                train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n                RESULTS.loc[random_state, 'score'] = classification_accuracy(train_X, test_X, train_y, test_y)\n            print(f'classification F1_SCORE(average=\"weighted\") with random_state from 0 to {research_iter - 1}:')\n\n        print(f'max:  {RESULTS.score.max()}')\n        print(f'mean: {RESULTS.score.mean()}')\n        print(f'min:  {RESULTS.score.min()}\\n')\n        del RESULTS\n        return 0\n    \n    \n    def keep_primary_index(df):\n        df[['__primary_idx']] = list(df.index)\n        return df\n    \n    \n    def order_and_sort_table(df, important_functions):\n        df = df[important_functions]\n        df = df.sort_values(by=important_functions, ascending=True)\n        df = df.reset_index(drop=True)\n        if verbose:\n            print('\\n-----------------------------------')\n            print(f'The Table has been sorted by columns:\\n{important_functions}')\n        return df\n\n    \n    def get_important_functions(X, y):\n        if verbose:\n            print('\\n-----------------------------------')\n            print('Get functions ordered by importance\\n')\n        train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=0)\n\n        if len(y.value_counts()) > CLASSIFIER_FOR_UNIQUE_VALUES_LESS_THAN:\n            alg = LGBMRegressor(n_jobs=-1, random_state=0)\n        else:\n            alg = LGBMClassifier(n_jobs=-1, random_state=0)\n            \n        feature_names = [X.columns]\n        model = alg.fit(train_X, train_y.values.ravel())        \n        result = permutation_importance(model, test_X, test_y.values.ravel(), n_repeats=5)\n        importances = pd.Series(result.importances_mean, index=feature_names)\n        importances = importances.sort_values(ascending=False)\n        if verbose:\n            print(importances)\n            fig, ax = plt.subplots()\n            importances.plot.bar(yerr=result.importances_std, ax=ax)\n            ax.set_title(\"Feature importances using permutation on full model\")\n            ax.set_ylabel(\"Mean accuracy decrease\")\n            fig.tight_layout()\n            plt.show()\n            \n        for el in importances.index:\n            important_functions.append(el[0])\n        return important_functions\n    \n    \n    def order_columns(important_functions, target_feature):\n        important_functions.insert(0, target_feature)\n        if verbose:\n            print('\\n-----------------------------------')\n            print(f'The columns has been ordered as follows:\\n {important_functions}')\n        return important_functions\n\n    \n    # split works correctly with test_size = 0.1, 0.2, 0.25, 0.33, 0.5 \n    def train_test_split_ordered(X, y, test_size=0.33):\n        train_indexes = []\n        test_indexes = []\n        indexes = list(X.index)\n\n        for el in indexes:\n            if el % int(1\/test_size):\n                train_indexes.append(el)\n            else:\n                test_indexes.append(el)\n\n        train_X = X.iloc[train_indexes]\n        test_X = X.iloc[test_indexes]\n        train_y = y.iloc[train_indexes]\n        test_y = y.iloc[test_indexes]\n        print('\\n-----------------------------------')\n        print('The split has been made.')\n        return train_X, test_X, train_y, test_y\n    \n    \n\n    \n    df = keep_primary_index(df)\n    predictors = get_predictors(df, target_feature)\n    if research_iter:\n        get_research(get_X(df, predictors), get_y(df, target_feature), target_feature, test_size, research_iter)\n    \n    df_work = order_and_sort_table(df, [target_feature]+predictors)\n    \n    predictors = get_predictors(df_work, target_feature)\n    important_functions = get_important_functions(get_X(df_work, predictors), get_y(df_work, target_feature))\n    \n    ordered_columns = order_columns(important_functions, target_feature)\n    df_work = order_and_sort_table(df_work, ordered_columns)\n    predictors = get_predictors(df_work, target_feature)\n    \n    if research_iter:\n        get_research(get_X(df_work, predictors), get_y(df_work, target_feature), target_feature, test_size, research_iter)\n\n    train_X, test_X, train_y, test_y = \\\n    train_test_split_ordered(get_X(df_work, predictors), get_y(df_work, target_feature), test_size=test_size)\n\n    print('\\n-----------------------------------')\n    if  len(train_y.value_counts()) > CLASSIFIER_FOR_UNIQUE_VALUES_LESS_THAN:\n        print(f'The final result MAE of the custom split:\\n{regression_score(train_X, test_X, train_y, test_y)}')\n    else:\n        print(f'The final result F1_SCORE(average=\"weighted\") of the custom split:\\n{classification_accuracy(train_X, test_X, train_y, test_y)}')\n    \n    return train_X, test_X, train_y, test_y","ba728c17":"train_X, test_X, train_y, test_y = train_test_split_ordered(df, target_feature, test_size=0.33, verbose=1, research_iter=100)","06e7f71f":"df_work","d0549a34":"train_X","2339d63b":"test_X","5c028a57":"train_y","5237c2a9":"test_y","7da17f8e":"![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-08-15 \u0432 22.17.54.png](attachment:80201da4-8ab8-45db-9962-4cb114053433.png)","d9ed5d55":"![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-08-15 \u0432 22.16.47.png](attachment:cc5b616d-9912-4d1a-a32f-092a52ec7ad3.png)","9a46e598":"## All Data https:\/\/www.kaggle.com\/ryanholbrook\/fe-course-data","be2b3b20":"# How it works?","4b614aa3":"![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-08-15 \u0432 22.17.06.png](attachment:2b39eeef-6f95-46a9-8799-3f1057977c8f.png)","501b0176":"![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-08-15 \u0432 22.15.55.png](attachment:dc1fdcfa-dd60-4a77-9b2e-b88923b95769.png)","b5782ca2":"# Result \n# train_X, test_X, train_y, test_y","aa15dad2":"![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-08-15 \u0432 22.16.21.png](attachment:5737c7dd-4e3a-4d00-9d63-82f42f3ddf80.png)","be93288e":"# Thank you for your time and curiosity ;)","096be9e8":"# Load Data","431d5156":"# A simple encoding","b276f4d1":"# Sorted table looks like","437ae175":"## The nearest to the root decision nodes will get more <b>important<\/b> and <b>balanced<\/b> features first.","febd7b0f":"# train_test_split_ordered() custom","f81beff9":"# Normalization","7ab97c86":"# train_test_split_ordered()"}}