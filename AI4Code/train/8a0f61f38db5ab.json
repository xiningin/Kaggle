{"cell_type":{"f69693ba":"code","2054e019":"code","072952de":"code","5ba464d4":"code","46610bd8":"code","28366256":"code","81730e10":"code","4e693544":"code","90a86094":"code","445b7a1b":"code","d80958d1":"code","a2605c8e":"code","bc9de059":"code","51fc31ac":"code","8c0b613e":"code","49a23db4":"code","74c4e17e":"code","c4d26517":"markdown"},"source":{"f69693ba":"#Import Library\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport lightgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","2054e019":"# Since there are many columns, you need to configure it for EDA to be convenient.\n# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\n\n# pandas setting\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","072952de":"train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","5ba464d4":"print(train.shape)\nprint(test.shape)\n\ntrain.head()","46610bd8":"train.info()","28366256":"#find int coulmns\nfor col in train.columns:\n    if 'int' in str(train[col].dtype):\n        print(col, end=' ')","81730e10":"train.loc[:, 'f0':'f284'].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","4e693544":"train['target'].value_counts()","90a86094":"# Feature Distribution\n# If you have too much data, it's a good idea to sample and visualize the approximate distribution first.\nnp.random.seed(2110)\ntrain = train.sample(10000)\ntest = test.sample(10000)","445b7a1b":"fig, axes = plt.subplots(11,11,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature f0-f120)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","d80958d1":"fig, axes = plt.subplots(11,11,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes, 121):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature f121-f241)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","a2605c8e":"# Binary Feature\nbinary_mean = train.loc[:,'f242':'f284'].mean()\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 6))\n\nax.bar(binary_mean.index, binary_mean, linewidth=0.2, edgecolor='black', alpha=1, color='#244747')\n\nax.set_ylim(0, 1)\nax.set_xticks(range(0, 44, 4))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', linewidth=0.2, zorder=5)\nax.set_title('Mean of binary features', loc='center', fontweight='bold')\nax.legend()\nplt.show()","bc9de059":"# get the labels\ny = train.target.values\ntrain.drop(['id', 'target'], inplace=True, axis=1)\nx = train.values","51fc31ac":"#Create training and validation sets\n\nx, x_test, y, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)","8c0b613e":"# Create the LightGBM data containers\ncategorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]\ntrain_data = lightgbm.Dataset(x, label=y, categorical_feature=categorical_features)\ntest_data = lightgbm.Dataset(x_test, label=y_test)","49a23db4":"# Train the model\n\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)","74c4e17e":"# Create a submission\n\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\nids = submission['id'].values\nsubmission.drop('id', inplace=True, axis=1)\n\n\nx = submission.values\ny = model.predict(x)\n\noutput = pd.DataFrame({'id': ids, 'target': y})\noutput.to_csv(\"submission.csv\", index=False)","c4d26517":"EDA-https:\/\/www.kaggle.com\/subinium\/tps-oct-simple-eda\nLGBM-https:\/\/www.kaggle.com\/ezietsman\/simple-python-lightgbm-example"}}