{"cell_type":{"331a08bf":"code","52435f28":"code","0939d91b":"code","502632b7":"code","e87c43af":"markdown","cdbc44c7":"markdown","c4ca2345":"markdown","f8b1863f":"markdown","5e38b886":"markdown","de17fcc1":"markdown","d63d90c5":"markdown","3c107a2f":"markdown","0a8b7af0":"markdown","6cce8467":"markdown"},"source":{"331a08bf":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","52435f28":"def get_moments(Xs, decay_rate=0.9):\n    \"\"\"\n    REMEMBER AND FORGET\n    ===================\n    input: list, decay_rate\n    output: list\n    \n    Recursive equation using loop\n    \"\"\"\n    xs_w_moms = []\n\n    gamma    = decay_rate\n    prev_mom = 0 \n\n    for x in Xs:\n        # note: addition\n        # ( (1-gamma)*x ) forgets\n        # ( gamma * prev_mom ) remebers\n        w_mom = ( (1-gamma)*x ) + ( gamma * prev_mom )\n        xs_w_moms.append(w_mom)\n\n        # update:\n        prev_mom = w_mom\n        \n    return xs_w_moms\n\n\n\ndef get_moments_rem(Xs, decay_rate=0.9):\n    \"\"\"\n    REMEMBER ONLY\n    ===================\n    input: list, decay_rate\n    output: list\n    \n    Recursive equation using loop\n    \"\"\"\n    xs_w_moms = []\n\n    gamma    = decay_rate\n    prev_mom = 0 \n\n    for x in Xs:\n        # note: addition\n        w_mom = x + ( gamma * prev_mom )\n        xs_w_moms.append(w_mom)\n\n        # update:\n        prev_mom = w_mom\n        \n    return xs_w_moms","0939d91b":"# gen data\ner = np.random.normal(0, 0.1, 500)\nxs = np.arange(0, 5, 0.01)\nys = np.sin(xs) + er\n\nfig, axarr = plt.subplots(2,2)\nfig.set_size_inches(20,10)\n\n\n# plot 1\naxarr[0,0].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[0,0].set_title(\"Orignal Data\")\n\n# plot 2\naxarr[0,1].scatter(xs, get_moments_rem(ys, 0.3), color='red', alpha=0.8)\naxarr[0,1].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[0,1].set_title(\"Decay coeff: 0.3\")\n\n# plot 3\naxarr[1,0].scatter(xs, get_moments_rem(ys, 0.6), color='red', alpha=0.8)\naxarr[1,0].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[1,0].set_title(\"Decay coeff: 0.6\")\n\n# plot 4\naxarr[1,1].scatter(xs, get_moments_rem(ys, 0.9), color='red', alpha=0.8)\naxarr[1,1].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[1,1].set_title(\"Decay coeff: 0.9\")\n\nplt.show()","502632b7":"# gen data\ner = np.random.normal(0, 0.1, 500)\nxs = np.arange(0, 5, 0.01)\nys = np.sin(xs) + er\n\nfig, axarr = plt.subplots(2,2)\nfig.set_size_inches(20,10)\n\n\n# plot 1\naxarr[0,0].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[0,0].set_title(\"Orignal Data\")\n\n# plot 2\naxarr[0,1].scatter(xs, get_moments(ys, 0.3), color='red', alpha=0.8)\naxarr[0,1].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[0,1].set_title(\"Decay coeff: 0.3\")\n\n# plot 3\naxarr[1,0].scatter(xs, get_moments(ys, 0.6), color='red', alpha=0.8)\naxarr[1,0].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[1,0].set_title(\"Decay coeff: 0.6\")\n\n# plot 4\naxarr[1,1].scatter(xs, get_moments(ys, 0.9), color='red', alpha=0.8)\naxarr[1,1].scatter(xs, ys, color='blue', alpha=0.2)\naxarr[1,1].set_title(\"Decay coeff: 0.9\")\n\nplt.show()","e87c43af":"### Remember parts-of Previous and Forget Current\n\n$$\\text{$\\large{(1- \\gamma)}$} X_{t} + \\text{$\\large{\\gamma}$}  X_{t-1}$$\n\nsuch that total information is retained (sum of ratios = 1)","cdbc44c7":"- If $\\gamma \\rightarrow 1$ Previous data is remembered more \n- Momentum curve will overlap original data if decay is 0\n- Note how momentum curve remebers the flow of original data (from prev data)\n\n> *We need to forget current-state 1 minus the amount we remeber prev data (To fill the knowledge gap)*","c4ca2345":"### Remember part-of previous data points\n\n$$ X_{t} + \\text{$\\large{\\gamma}$}  X_{t-1}$$","f8b1863f":"# Further Reading:\n\n[distill.pub: Why momentum actually works and other uses other than dampening oscillations](https:\/\/distill.pub\/2017\/momentum\/)","5e38b886":"# SGD + Momentum","de17fcc1":"Regular SGD: $W_{t} = W_{t-1} - \\alpha G$ where $G = \\bigg[ \\frac{\\partial L}{dW} \\bigg]_{W_{t-1}}$\n\nWith momentum,\n\n$$W_{t} = W_{t-1} - \\alpha V_t$$\n\nwhere $V_t$ is **vector sum** of *gradient* term and *momentum* term\n\n$$V_t = \\,\\,\\, \\text{$\\large{(1- \\gamma)}$}G \\,\\,\\, + \\,\\,\\, \\text{$\\large{\\gamma}$} V_{t-1} $$\n\n\n$\\text{$\\large{\\gamma}$ is typically 0.9}$\n- **FORGET:** (part-of) current loss-gradient\n- **REMEBER:** (part-of) previous loss-gradients\n- $\\alpha V_t$ is **step**. In negative direction (to reach minima)\n\n<br>\n<br>\n\n![image.png](attachment:image.png) \n<br>\nImage: [source](https:\/\/towardsdatascience.com\/stochastic-gradient-descent-with-momentum-a84097641a5d) (Blue vector in left figure is $V_t$)","d63d90c5":"> ***Note: Momentum is as important as SGD itself***\n\n![image.png](attachment:image.png)\nImage: [source](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fai%25C2%25B3-theory-practice-business%2Fhyper-parameter-momentum-dc7a7336166e&psig=AOvVaw242Vc3RfMI2LaAeM5wOoez&ust=1599028479194000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCKCGtberx-sCFQAAAAAdAAAAABAJ)\n\n> Note the colored arrows below","3c107a2f":"> Note how seamlessly finds the average flow!!","0a8b7af0":"# Momentum \/ Exponentially Decaying Averages\n\n> *'Average' is first order moment in statistics. Even though 'momentum' in physics is quite applicable here*","6cce8467":"Average the data in **real-time** using **momentum of previous data**.\n\n$$\n X^{new}_t =\n  \\begin{cases}\n                                   X_{0} & \\text{if $t=0$} \\\\\n                                   \\text{$\\large{(1- \\gamma)}$} X_{t} + \\text{$\\large{\\gamma}$}  X_{t-1} & \\text{if $n>0$} \\\\\n  \\end{cases}\n$$ where $0 \\le \\gamma \\le 1$\n\n> - Just remember what prev-of-prev-of-prev was w\/ a exponentially decaying significance to each\n> - On expansion: $X^{new}_{t} = X_{t} + \\text{$\\large{\\gamma}$}  X_{t-1} + \\text{$\\large{\\gamma^2}$}  X_{t-2} + \\text{$\\large{\\gamma^3}$}  X_{t-3} \\dots$"}}