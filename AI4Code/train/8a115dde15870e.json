{"cell_type":{"13af164a":"code","10e84913":"code","ba17778f":"code","9ece815d":"code","108f82d1":"code","33ce09ba":"code","9d9533b6":"code","7fba82a9":"code","a46e66d8":"code","8de5c0d8":"code","05e6d38c":"code","4b057a27":"code","c2b6213f":"code","faa0e2ee":"code","3bdd2458":"code","614f3eb8":"code","f4e68bae":"code","c8b643b2":"code","ca79ad3f":"code","fec8e473":"code","c7518f16":"code","4aafd310":"code","03ab0282":"code","f93bc122":"code","1e35f406":"code","4a718cc9":"code","2ff29990":"code","77f4c67b":"code","29eba8fc":"code","5f553f14":"code","d2c9e5b5":"code","8d3a5657":"code","e039937e":"code","ec07b703":"code","dcfa9b73":"code","501aeb3a":"code","5622432b":"code","1fb562aa":"code","97088071":"code","6676aea9":"code","a5fc3b42":"code","fddee8a8":"code","363791bc":"code","7c54f241":"markdown","abbdb6bf":"markdown","dbe7c509":"markdown","1c9eaa91":"markdown","2088ed34":"markdown","bc57bda1":"markdown","b6957a72":"markdown","97e8f246":"markdown","b596675e":"markdown","27583c0f":"markdown","e2e4cfa4":"markdown","93696273":"markdown","ce33e735":"markdown","db742c19":"markdown","c456851d":"markdown","5cebaaca":"markdown","cfd9c1aa":"markdown","0054294f":"markdown","55f30a8e":"markdown","b4399020":"markdown","2429b388":"markdown","32c3d6c9":"markdown"},"source":{"13af164a":"import os\nprint(os.listdir('..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev'))","10e84913":"print(os.listdir('..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev\/C1-P1_Train'))","ba17778f":"trainPath = '..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev\/C1-P1_Train\/'\ndevPath   = '..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev\/C1-P1_Dev\/'\n\ntrainCSV = '..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev\/train.csv'\ndevCSV   = '..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Train Dev\/dev.csv'","9ece815d":"import pandas as pd\ntrainDF = pd.read_csv(trainCSV, header=None)\nprint(trainDF)","108f82d1":"trainFiles = trainDF[0].tolist()\ntrainClasses = trainDF[1].tolist()","33ce09ba":"devDF = pd.read_csv(devCSV, header=None)\nprint(devDF)","9d9533b6":"devFiles = devDF[0].tolist()\ndevClasses = devDF[1].tolist()","7fba82a9":"labels = ['A', 'B', 'C']","a46e66d8":"# plot the circle of value counts in dataset\nimport matplotlib.pyplot as plt\n\ndef plot_equilibre(equilibre, labels, title):\n    plt.figure(figsize=(5,5))\n    my_circle=plt.Circle( (0,0), 0.5, color='white')\n    plt.pie(equilibre, labels=labels, colors=['red','green','blue'],autopct='%1.1f%%')\n    p=plt.gcf()\n    p.gca().add_artist(my_circle)\n    plt.title(title)\n    plt.show()","8de5c0d8":"equilibreTrain = []\n[equilibreTrain.append(trainClasses.count(label)) for label in labels]\nprint(equilibreTrain)\nplot_equilibre(equilibreTrain, labels, 'Train Data')\ndel equilibreTrain","05e6d38c":"equilibreDev = []\n[equilibreDev.append(devClasses.count(label)) for label in labels]\nprint(equilibreDev)\nplot_equilibre(equilibreDev, labels, 'Development Data')\ndel equilibreDev","4b057a27":"import numpy as np\nimport cv2\nfrom tensorflow.keras.utils import to_categorical\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","c2b6213f":"TargetSize = (192, 144) # image ratio = 4:3\ndef prepare_image(filepath):\n    img = cv2.imread(filepath)\n    # get image height, width\n    (h, w) = img.shape[:2]\n    if (w<h): # rotate270\n        # calculate the center of the image\n        center = (w \/ 2, h \/ 2)\n        M = cv2.getRotationMatrix2D(center, 270, 1.0)\n        img = cv2.warpAffine(img, M, (h, w))\n    img_resized = cv2.resize(img, TargetSize, interpolation=cv2.INTER_CUBIC)\n    img_result  = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    return img_result","faa0e2ee":"plt.imshow(prepare_image(trainPath+trainFiles[1]))","3bdd2458":"plt.imshow(prepare_image(devPath+devFiles[1]))","614f3eb8":"trainX = []\n[trainX.append(prepare_image(trainPath+file)) for file in trainFiles]\ntrainX = np.asarray(trainX) \nprint(trainX.shape)\n\n# data normalisation\ntrainX = trainX \/ 255.0","f4e68bae":"# Convert Y_data from {'A','B','C'} to {0,1,2}\ntrainY = []\n[trainY.append(ord(trainClass) - 65) for trainClass in trainClasses]\n#print(trainY)\n\n# one-hot encoding\ntrainY = to_categorical(trainY)","c8b643b2":"validX = []\n[validX.append(prepare_image(devPath+file)) for file in devFiles]\nvalidX = np.asarray(validX)    \nprint(validX.shape)\n\n# data normalisation\nvalidX = validX \/ 255.0","ca79ad3f":"# Convert Y_data from char to integer\nvalidY = []\n[validY.append(ord(devClass) - 65) for devClass in devClasses]\n#print(validY)\n\n# One-hot encoding\nvalidY = to_categorical(validY)","fec8e473":"from sklearn.utils import shuffle\ntrainX,trainY = shuffle(trainX,trainY, random_state=42)","c7518f16":"num_classes = 3","4aafd310":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, LeakyReLU, Concatenate\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix","03ab0282":"input_shape = trainX.shape[1:]\nprint(trainX.shape[1:])","f93bc122":"# Build Model\n\ninput_image = Input(shape=input_shape)\n# 1st Conv layer\nmodel = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape)(input_image)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 2nd Conv layer\nmodel = Conv2D(32, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 3rd Conv layer\nmodel = Conv2D(64, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 4th Conv layer\nmodel = Conv2D(128, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 5th Conv layer\nmodel = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# FC layers\nmodel = Flatten()(model)\n\n#model = Dense(1024, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(model)\nmodel = Dense(1024)(model)\n#model = Dropout(0.2)(model)\n\n#model = Dense(64, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(model)\nmodel = Dense(64)(model)\n#model = Dropout(0.2)(model)\n\noutput= Dense(num_classes, activation='softmax')(model)\n\nmodel = Model(inputs=[input_image], outputs=[output])\n\nmodel.summary()","1e35f406":"# Compile Model\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","4a718cc9":"## set Checkpoint : save best only, verbose on\n#checkpoint = ModelCheckpoint(\"mango_classification.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1)","2ff29990":"batch_size = 16\nnum_epochs = 80","77f4c67b":"# Train Model\nhistory = model.fit(trainX,trainY,batch_size=batch_size,epochs=num_epochs, validation_data=(validX,validY)) #, callbacks=[checkpoint])","29eba8fc":"## Save Model\nmodel.save('mango_cnn.h5')","5f553f14":"## load best model weights if using callback (save-best-only)\n#model.load_weights(\"mango_classification.hdf5\")","d2c9e5b5":"predY = model.predict(validX)\ny_pred = np.argmax(predY,axis=1)\ny_actual = np.argmax(validY,axis=1)\n#y_label= [labels[k] for k in y_pred]\ncm = confusion_matrix(y_actual, y_pred)\nprint(cm)","8d3a5657":"import itertools\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n        \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n        \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()        ","e039937e":"plot_confusion_matrix(cm, \n                      normalize=False,\n                      target_names = labels,\n                      title=\"Confusion Matrix, not Normalized\")","ec07b703":"print(classification_report(y_actual, y_pred, target_names=labels))","dcfa9b73":"testPath  = '..\/input\/aicup-2020-mango-c1-p1\/C1-P1_Test\/C1-P1_Test\/'\ntestFiles = os.listdir(testPath)\ntestFiles.sort()\nprint(testFiles)","501aeb3a":"testFile = '07497.jpg'\nprint(testPath+testFile)","5622432b":"plt.imshow(prepare_image(testPath+testFile))","1fb562aa":"testData = prepare_image(testPath+testFile).reshape(1,144,192,3)\ntestData = testData \/ 255.0\nprint(testData.shape)\npredictions = model.predict(testData)\nprint(predictions[0])","97088071":"maxindex = int(np.argmax(predictions[0]))\nprint('Predicted: %s, Probability = %f' %(labels[maxindex], predictions[0][maxindex]) )","6676aea9":"testX = []\n[testX.append(prepare_image(testPath+file)) for file in testFiles]\ntestX = np.asarray(testX)    \nprint(testX.shape)\ntestX = testX\/255.0","a5fc3b42":"testY = model.predict(testX)\nprint(testY[0])","fddee8a8":"# create a list of the predicted label\npred_y = []\nfor y in testY:\n    maxindex = int(np.argmax(y))\n    pred_y.append(labels[maxindex])\nprint(pred_y)","363791bc":"# create dataframe with each image id & its predicted label\ndfTest = pd.DataFrame(columns = ['image_id','label'])\ndfTest['image_id'] = testFiles\ndfTest['label']=pred_y\n\n# output a .csv file\ndfTest.to_csv('test_submission.csv',index=False)","7c54f241":"## Prepare Data","abbdb6bf":"### plot confusion matrix","dbe7c509":"## Test Model","1c9eaa91":"### predict Validation set for a Confusion Matrix","2088ed34":"## Evaluate Model","bc57bda1":"### try a test image","b6957a72":"## Generate Submission","97e8f246":"### Shuffle Training Data","b596675e":"### Development Data (for Validation)","27583c0f":"## Train Model","e2e4cfa4":"## Import Libraries","93696273":"* Website: https:\/\/aidea-web.tw\/topic\/72f6ea6a-9300-445a-bedc-9e9f27d91b1c\n* Forum : https:\/\/www.facebook.com\/groups\/184056976135023\/","ce33e735":"### Display Image file","db742c19":"# Mango Classification","c456851d":"## Dataset : AICUP 2020 Mango sample data\n![Taiwan%20AICUP%202020%20Mango.JPG](attachment:Taiwan%20AICUP%202020%20Mango.JPG)","5cebaaca":"## Dataset Equilibre ","cfd9c1aa":"# Versions\n> V1. VGG16 transfer learning, accuracy : train = 100.00%, valid = 67.00% <br \/>\n> V2. VGG19 transfer learning, accuracy : train = 100.00%, valid = 65.38% <br \/>\n> V3. ResNet50V2             , accuracy : train =  99.43%, valid = 63.88% <br \/>\n> V4. ResNet101V2            , accuracy : train =  99.39%, valid = 70.00% <br \/>\n> V5. ResNet152V2            , accuracy : train =  99.41%, valid = 72.50% <br \/>\n> V6. DenseNet121            , accuracy : train =  99.30%, valid = 72.62% <br \/>\n> V7. EfficientNet B7        , accuracy : train =  99.61%, valid = 75.50% (epochs=150) <br \/>\n> V8. CNNx5+FCx2             , accuracy : train =  98.75%, valid = 73.37% (epochs=100) <br \/>\n> V9. CNNx5+FCx2 train+dev          , accuracy : train =  99.28%, valid = 99.12% (epochs=80) <br \/>\n> V10.CNNx5+FCx2 dropout=0.2@FCs    , accuracy : train =  98.20%, valid = 73.00% (epochs=80) <br \/>\n> V11.CNNx5+FCx2 dropout=0.1@Conv2Ds, accuracy : train =  95.98%, valid = 71.38% (epochs=100) <br \/>\n> V12.CNNx4+FCx2             , accuracy : train = 100.00%, valid = 71.75% (epochs=50) <br \/>\n> V12.CNNx4+FCx2             , accuracy : train = 100.00%, valid = 71.75% (epochs=50) <br \/>\n> V13.CNNx5+FCx2 (1024+64)   , accuracy : train = 100.00%, valid = 73.25% (epochs=60) <br \/>\n> V14.CNNx5+FCx2 (l2_regularizer, dropout), accuracy : train =  96.54%, valid = 70.75% (epochs=60) <br \/>\n> V15.CNNx5+FCx2 (1024+64)   , accuracy : train = 98.29%, valid = 72.75% (epochs=80) <br \/>","0054294f":"## Build Model","55f30a8e":"### model prediction","b4399020":"## Save Model","2429b388":"### Training Data","32c3d6c9":"### prepare Test data"}}