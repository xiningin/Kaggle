{"cell_type":{"326e5ba8":"code","7a740eeb":"code","13f84264":"code","4e1969aa":"code","4ea53704":"code","dc22ae1f":"code","a3b15b00":"code","512caa8c":"code","38c8f607":"code","65cef79e":"code","bcb3a168":"code","61032a5b":"code","01f06059":"code","60835d15":"code","f942c8ec":"code","66a4399c":"code","218e305a":"code","b7a3ebb0":"code","849858d9":"code","8aa1e82f":"code","09d6cfad":"code","3298332f":"code","b9b48a2b":"code","d9f697f6":"code","4021248f":"code","a242f591":"code","b00ead2f":"code","ff563caa":"code","2c29bb44":"code","2bfec5b2":"markdown","9bb44453":"markdown","58ba8e7b":"markdown","180b692b":"markdown","5055eaf3":"markdown","dd0adb93":"markdown","32c6f5c9":"markdown","94267591":"markdown","4700f0af":"markdown","27fd268e":"markdown","2c6dcfc9":"markdown","5a72cce4":"markdown","770c4f79":"markdown","4e92ac12":"markdown","24ff4a5c":"markdown","3065566a":"markdown","9500dd8c":"markdown","2927cf8e":"markdown","dc928ffb":"markdown","d50ae8cd":"markdown","609dfe24":"markdown","b8779616":"markdown"},"source":{"326e5ba8":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization,GlobalMaxPool1D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report ","7a740eeb":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","13f84264":"df_train.head()","4e1969aa":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4ea53704":"df_test.head()","dc22ae1f":"df_train.shape","a3b15b00":"df_test.shape","512caa8c":"df_train['target'].value_counts()","38c8f607":"x = df_train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('values')","65cef79e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,6))\nlength=df_train[df_train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(length)\nax1.set_title('disaster tweets')\nlength=df_train[df_train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(length)\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Number of words')\nplt.show()","bcb3a168":"vocab_length = 15000\nsent_length = 100\nembedding_vector_features = 70","61032a5b":" ps = PorterStemmer()","01f06059":"corpus_train = []\nfor i in range(df_train['text'].shape[0]):\n    tweet = re.sub(r'^RT[\\s]+', '', df_train['text'][i])\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    tweet = re.sub(r'#', '', tweet)\n    text = re.sub('\\w*\\d\\w*', '', tweet)\n    text = text.split()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    corpus_train.append(text)\nprint(\"Corpus Created\")","60835d15":"print(pd.DataFrame(corpus_train)[0].head(10))","f942c8ec":"corpus_test = []\nfor i in range(df_test['text'].shape[0]):\n    tweet = re.sub(r'^RT[\\s]+', '', df_test['text'][i])\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    tweet = re.sub(r'#', '', tweet)\n    text = re.sub('\\w*\\d\\w*', '', tweet)\n    text = text.split()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    corpus_test.append(text)\nprint(\"Corpus Created\")","66a4399c":"print(pd.DataFrame(corpus_test)[0].head(10))","218e305a":"onehot_train = [one_hot(words,vocab_length)for words in corpus_train] \nprint(\"Created\")","b7a3ebb0":"onehot_test = [one_hot(words,vocab_length)for words in corpus_test] \nprint(\"Created\")","849858d9":"embedded_docs_train = pad_sequences(onehot_train, padding='pre', maxlen=sent_length)\nprint(embedded_docs_train)","8aa1e82f":"embedded_docs_test = pad_sequences(onehot_test, padding='pre', maxlen=sent_length)\nprint(embedded_docs_test)","09d6cfad":"model = Sequential()\nmodel.add(Embedding(vocab_length, embedding_vector_features, input_length=sent_length))\nmodel.add(Bidirectional(LSTM(128, dropout=0.3)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(sent_length, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(sent_length, activation = \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nprint(model.summary())","3298332f":"X = np.array(embedded_docs_train)\n\ny = np.array(df_train['target'])\n\nX_test = np.array(embedded_docs_test)","b9b48a2b":"X.shape","d9f697f6":"y.shape","4021248f":"X_test.shape","a242f591":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25,\n                                                    random_state=45, shuffle=True)","b00ead2f":"model.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=20,batch_size=64)","ff563caa":"y_pred = model.predict_classes(X_valid)\n\nprint('Confusion Matrix :')\nprint(confusion_matrix(y_valid, y_pred)) \nprint('Accuracy Score :',accuracy_score(y_valid, y_pred))\nprint('Report : ')\nprint(classification_report(y_valid, y_pred))","2c29bb44":"y_pred_test = model.predict_classes(X_test)\ny_pred_test = y_pred_test[:, 0]\noutput = pd.DataFrame({'id': df_test.id, 'target': y_pred_test})\nprint(output)\noutput.to_csv('predictions.csv', index=False)\nprint(\"submission successfully saved!\")","2bfec5b2":"## Getting Words in Tweet","9bb44453":"# Crearing Embedding Representation","58ba8e7b":"## For Train Tweets","180b692b":"## Getting Test, Train shape","5055eaf3":"# Predicting the results for valid data","dd0adb93":"# Creating Input for model","32c6f5c9":"## For Training Tweets","94267591":"# Spliting into train and Valid","4700f0af":"# Doing One-Hot Encoding","27fd268e":"## Getting value count of target feature","2c6dcfc9":"# Importing Training and Test dataset","5a72cce4":"## For Test Tweets","770c4f79":"# Training the model","4e92ac12":"# Doing EDA and Preprocessing","24ff4a5c":"# Predicting the test dataset Results","3065566a":"## For Test Corpus","9500dd8c":"## For Test Tweets","2927cf8e":"# Cleaning the Tweets","dc928ffb":"# Creating Bidirectional LSTM Model","d50ae8cd":"## For Train Corpus","609dfe24":"# Creating Variables","b8779616":"# Importing Libraries"}}