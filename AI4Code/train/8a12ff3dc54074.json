{"cell_type":{"765eff3c":"code","783acb26":"code","088ac3ae":"code","1d223a5a":"code","1cd87f20":"code","a7f22f96":"code","177f32f6":"code","599f5bdd":"code","b6d1d2cc":"code","f07c69b9":"code","cbbaa7d5":"code","aa77994f":"code","f7fdda3e":"code","86553c6d":"code","1d9c3635":"code","1d34ed04":"code","8b5b6d4a":"code","5818c6bc":"code","e418d2bb":"code","4635f135":"code","072944c1":"code","a62f6168":"code","6d8ebecd":"code","a716cb9a":"code","70beb723":"code","f4369dea":"code","5878b7a8":"code","47922075":"code","9161c107":"code","4b399ea5":"markdown","df83170f":"markdown","dd384c2c":"markdown","d452d194":"markdown","86ac32e4":"markdown","2cf04339":"markdown","a19af1b9":"markdown","a1459592":"markdown","5b69b455":"markdown","de5f78d1":"markdown","e7ff174d":"markdown","ab0b59e9":"markdown","0d592587":"markdown","71e4d4a7":"markdown","4887714c":"markdown","c8cd42e7":"markdown","026a988c":"markdown"},"source":{"765eff3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","783acb26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","088ac3ae":"# Import the data\ndata = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","1d223a5a":"data.head()","1cd87f20":"# describe() method is one of the best way to understand the data\ndata.describe()","a7f22f96":"bins = (0, 6.5, 10)\nlabels = [0,1]\ndata['scaled_quality'] = pd.cut(x=data['quality'], bins=bins, labels=labels)\ndata['scaled_quality'].value_counts()","177f32f6":"#quick look at our data types \ndata.info()","599f5bdd":"# distribution of wine quality\nsns.barplot(x=['bad','good'], y=data['scaled_quality'].value_counts()).set_title('Wine Quality (Good\/Bad)')","b6d1d2cc":"data.columns","f07c69b9":"# Pivot table comparing survival rate across numeric variables\npd.pivot_table(data, index=['scaled_quality'], \n               values=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n                  'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n                  'pH', 'sulphates', 'alcohol'])","cbbaa7d5":"# Make histograms to understand distributions\ninput_features = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n                  'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n                  'pH', 'sulphates', 'alcohol']]\nfor i in input_features.columns:\n    sns.distplot(data[i],bins=40)\n    plt.show()","aa77994f":"X = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\nY = data['scaled_quality']","f7fdda3e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","86553c6d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)","1d9c3635":"# Machine Learning Models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Evaluation Metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score","1d34ed04":"gnb = GaussianNB()\n# Use 6 fold cross validation and take the average value to test the ability of a machine learning model to predict new data\ncross_val = cross_val_score(gnb, X_train, y_train, cv=6) * 100\ngnb_cv = cross_val.mean()\nprint(cross_val)\nprint(gnb_cv)","8b5b6d4a":"gnb.fit(X_train ,y_train)\ngnb_pred = gnb.predict(X_test)\n# Performance of the model on test set\ngnb_acc = accuracy_score(y_test, gnb_pred) * 100\nprint('Accuracy:', gnb_acc)","5818c6bc":"lr = LogisticRegression()\ncross_val = cross_val_score(lr, X_train, y_train, cv=6) * 100\nlr_cv = cross_val.mean()\nprint(cross_val)\nprint(lr_cv)","e418d2bb":"lr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nlr_acc = accuracy_score(y_test, lr_pred) * 100\nprint('Accuracy:', lr_acc)","4635f135":"dt =  tree.DecisionTreeClassifier()\ncross_val = cross_val_score(dt, X_train, y_train, cv=6) * 100\ndt_cv = cross_val.mean()\nprint(cross_val)\nprint(dt_cv)","072944c1":"dt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\ndt_acc = accuracy_score(y_test, dt_pred) * 100\nprint('Accuracy:', dt_acc)","a62f6168":"rf = RandomForestClassifier()\ncross_val = cross_val_score(rf ,X_train, y_train, cv=6) * 100\nrf_cv = cross_val.mean()\nprint(cross_val)\nprint(rf_cv)","6d8ebecd":"rf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred) * 100\nprint('Accuracy:', rf_acc)","a716cb9a":"svc = SVC()\ncross_val = cross_val_score(svc, X_train, y_train, cv=6) * 100\nsvc_cv = cross_val.mean()\nprint(cross_val)\nprint(svc_cv)","70beb723":"svc.fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\nsvc_acc = accuracy_score(y_test, svc_pred) * 100\nprint('Accuracy:', svc_acc)","f4369dea":"gb =  GradientBoostingClassifier()\ncross_val = cross_val_score(gb, X_train, y_train, cv=6) * 100\ngb_cv = cross_val.mean()\nprint(cross_val)\nprint(gb_cv)","5878b7a8":"gb.fit(X_train, y_train)\ngb_pred = gb.predict(X_test)\ngb_acc = accuracy_score(y_test, gb_pred) * 100\nprint('Accuracy:', gb_acc)","47922075":"models = [['Naive Bayes', gnb_cv, gnb_acc],\n          ['Logistic Regression', lr_cv, lr_acc],\n          ['Decision Tree', dt_cv, dt_acc],\n          ['Random Forest', rf_cv, rf_acc],\n          ['Support Vector Classification', svc_cv, svc_acc],\n          ['Gradient Boosting Classifier', gb_cv, gb_acc]]\n\nmodels_df = pd.DataFrame(models, columns=['Model','Cross Validation','Accuracy'])\nmodels_df","9161c107":"sns.barplot(x=models_df['Accuracy'], y=models_df['Model'])","4b399ea5":"**Model Comparision**","df83170f":"#### Model Building\n<br>Let's try out some models and see how they perform on the training and test set\n- Naive Bayes\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- Support Vector Classification\n- Gradient Boosting Classifier","dd384c2c":"The dataset is splited into the training and test set. The training set is used  to create the model while the test set is used to the qualify performance.","d452d194":"**Overview**\n<br>\n<br>**1\/ Data Exploration**\n<br>\n<br>**2\/ Data Preprocessing for Model**\n<br>\n<br>**3\/ Model Training**\n<br>\n<br>**4\/ Model Comparision and Selection**","86ac32e4":"**Gradient Boosting**","2cf04339":"**Model Training**","a19af1b9":"**Decision Tree**","a1459592":"Instead of predicting the quality of wine based on the scale of 10, I convert the problem to classify whether it is good or bad wine\n<br>Quality > 6.5 => \"Good\" else \"Bad\"","5b69b455":"**Logistic Regression**","de5f78d1":" **Naive Bayes**","e7ff174d":"**Import libraries**","ab0b59e9":"**Thank you for reading this notebook. Please upvote if you find it helpful. Feel free to leave feedback or suggestions in the comments.**","0d592587":"**Random Forest**","71e4d4a7":"**Data Preprocessing for Model**","4887714c":"In this notebook, I hope to show some basics of Machine Learning such as data exploration, training models, etc. The goal is to correctly predict the quality of wine (bad or good) based on the given input variables. ","c8cd42e7":"Clearly, Random Forest is the winner among the chosen models.","026a988c":"**Support Vector Classification**"}}