{"cell_type":{"d2890297":"code","c84c7bd3":"code","87553ace":"code","c8a4ab40":"code","7bc8b7d7":"code","af39f114":"code","6c974800":"code","f2d3c305":"code","fa264d1b":"code","a8b3dfb2":"code","360eda71":"code","336aafc3":"code","122cf96f":"code","c64f4013":"code","3086e2ba":"code","74d9538e":"code","c19b595c":"code","83e8f027":"code","b61406ba":"code","9dc429fb":"code","98382f95":"code","3b7b765f":"code","ada911a3":"code","fef5331a":"code","51d4f111":"code","15fae51d":"code","1ad0dce9":"markdown","2f4cb99c":"markdown","21511d1b":"markdown","ba0426d8":"markdown","c6c37d1a":"markdown","f34860ed":"markdown","2dc6965a":"markdown","82882a76":"markdown","a8070150":"markdown"},"source":{"d2890297":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#ignore warning messages\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom wordcloud import WordCloud,STOPWORDS\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c84c7bd3":"df = pd.read_csv(\"..\/input\/trumps-legacy\/Trumps Legcy.csv\")\ndf.head()","87553ace":"df.info()","c8a4ab40":"df.isnull().sum()","7bc8b7d7":"df=df.drop_duplicates(subset=['id','text'],keep='first')\ndf.shape","af39f114":"df['text_length']=df.text.apply(lambda x:len(x.split()))","6c974800":"df['text_length'].describe()","f2d3c305":"df['device'].value_counts()","fa264d1b":"px.histogram(df, x = 'device', width = 800, height = 500, title = 'Frequency of Tweets device')","a8b3dfb2":"from nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora, models\nimport pandas as pd\nimport gensim\nimport pyLDAvis.gensim\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n\nnltk.download('wordnet')\n\npattern = r'\\b[^\\d\\W]+\\b'\ntokenizer = RegexpTokenizer(pattern)\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","360eda71":"stop  = {\"https\",\"t\",\"co\",\"u\",\"s\",\"rt\"}\n\ndef converter(x):\n    try: \n        return ' '.join([x.lower() for x in str(x).split() if x not in en_stop])\n    except AttributeError:\n        return None  # or some other value\n    \n    \ndef lematize(x):\n    try:\n        return ' '.join([lemmatizer.lemmatize(x)])\n    except AttributeError:\n        return None  # or some other value\n\ndef converterCustom(x):\n    try: \n        return ' '.join([x.lower() for x in str(x).split() if x not in stop])\n    except AttributeError:\n        return None  # or some other value","336aafc3":"df['text_without_stopwords'] = df['text'].apply(converter)\ndf['text_without_stopwords'] = df['text_without_stopwords'].apply(converterCustom)","122cf96f":"texts = []\n# loop through document list\nfor i in df['text_without_stopwords'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # remove stop words from tokens\n    #stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n    \n    # remove word containing only single char\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n    \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\n# print(texts[0])\n\ndf.head()","c64f4013":"#Create term dictionary and document-term matrix\n# turn our tokenized documents into a id <-> term dictionary\ndictionary = corpora.Dictionary(texts)\n# convert tokenized documents into a document-term matrix\ncorpus = [dictionary.doc2bow(text) for text in texts]","3086e2ba":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\nimport pprint\npprint.pprint(ldamodel.top_topics(corpus,topn=5))","74d9538e":"%matplotlib inline\nvis = pyLDAvis.gensim.prepare(topic_model=ldamodel, corpus=corpus, dictionary=dictionary)\npyLDAvis.enable_notebook()\npyLDAvis.display(vis)","c19b595c":" def showWordCloud(data):\n    words = ' '.join(data)\n    STOPWORDS.update([\"https\",\"t\",\"co\",\"u\",\"s\",\"rt\"])    \n    cleaned_word = \" \".join([word for word in words.split()])\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                         background_color = 'black',\n                         width = 2500,\n                         height = 2500\n                         ).generate(cleaned_word)\n    plt.figure(1,figsize = (13,13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nshowWordCloud(df['text_without_stopwords'])","83e8f027":"from textblob import TextBlob\nfrom textblob.classifiers import NaiveBayesClassifier\ndf['sentiment'] = df.text_without_stopwords.map(lambda text: TextBlob(text).sentiment.polarity)\ndf.head(5)","b61406ba":"# sns.distplot(df.sentiment);\n# plt.title(\"Distribution of sentiment polarity of comments\");","9dc429fb":"fig = px.histogram(df, x=\"sentiment\",width = 800, height = 500,)\nfig.update_layout(title_text='Distribution of sentiment polarity of comments',\n                   xaxis_title_text='sentiment', \n    yaxis_title_text='Density')\nfig.show()","98382f95":" def label(x):\n    try: \n        if x > 0.0:\n            return 'POSITIVE'\n        if x < 0.0:\n            return 'NEGITIVE'        \n        if x == 0.0:\n            return 'NEUTRAL'\n        \n    except AttributeError:\n        return None  # or some other value\n\ndf['sentimentLabel'] = df['sentiment'].apply(label)\ndf.head(5)","3b7b765f":"fig = px.histogram(df, x=\"sentimentLabel\",width = 800, height = 500,)\nfig.update_layout(title_text='Distribution of sentiment of comments',\n                   xaxis_title_text='sentiment', \n    yaxis_title_text='Density')\nfig.show()","ada911a3":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, \n                                    num_topics=8, passes=5, minimum_probability=0)\nldamodel.print_topics()","fef5331a":"hm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])\nhm","51d4f111":"from sklearn.manifold import TSNE\ntsne = TSNE(perplexity=30, early_exaggeration=120)\nembedding = tsne.fit_transform(hm)\nembedding = pd.DataFrame(embedding, columns=['x','y'])\nembedding['hue'] = hm.argmax(axis=1)","15fae51d":"from bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()\n\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = [all_palettes['Set1'][8][i] for i in embedding.hue],\n            title = df.text,\n            year = df.date,\n            alpha = [0.9] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Text:<\/span>\n            <span style=\"font-size: 12px\">@title<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Date:<\/span>\n            <span style=\"font-size: 12px\">@year<\/span>\n        <\/div>\n    <\/div>\n    \"\"\")\ntools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\nplot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Tweets')\nplot_tsne.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\nlayout = column(plot_tsne)\nshow(layout)\n\n","1ad0dce9":"United States 45th President Donald Trump has used Twitter as no one else. He primarily ran his government from a twitter firehose. Twitter has officially banned his account on January 8th 2021 after a deadly riot at Capitol on January 6th 2021. Twitter cites its World Leaders on Twitter: Principles and Approach as a guide to adhere to for public leaders.\n\nTrump tweets and policies have far reaching effects that one can realize or he would accept to realize himself. Since, twitter is suspended there is no public way to read his past tweets and analyze it for public policy outcome or link it with global issues.\n\nHere we are presenting the complete treasure trove of President Trump's tweet, all 56,572 for the public, data scientists and researchers.\n\nThe dataset contains 56,572 tweets, tweet IDs, Tweet Date, How many liked and retweeted it. ","2f4cb99c":"# Word Cloud","21511d1b":"# Data Description\n","ba0426d8":"# t-SNE\n\nRebuild LDA model with some extra imputs and reduce number of topics ","c6c37d1a":"# sentiment Analysis","f34860ed":"**Please upvote if you find this notebook helpful! \ud83d\ude0a Thank you! I would also be very happy to receive feedback on my work.**","2dc6965a":"## Work in progress tring to add intresting stuff. if you like my work do \"up vote\"","82882a76":"Refactoring results of LDA into numpy matrix (number_of_papers x number_of_topics).","a8070150":"# Topic Modeling\n## Latent Dirichlet allocation (LDA)"}}