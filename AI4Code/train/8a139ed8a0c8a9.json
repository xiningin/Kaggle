{"cell_type":{"43b915b3":"code","868593e9":"code","00699a6a":"code","aac8c4c5":"code","a4830576":"code","d5a65b51":"code","c64362b1":"code","4c6f3693":"code","3469631d":"code","3d63f78e":"code","6b867a22":"code","bc6f69ee":"code","a529496d":"markdown","95d2b28f":"markdown","7b1ea239":"markdown","b646cde4":"markdown","d83122ae":"markdown","2efd57ab":"markdown","136876ec":"markdown","9857e72e":"markdown","2ea43d14":"markdown","ad94abcf":"markdown","4828627f":"markdown"},"source":{"43b915b3":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport time","868593e9":"class ToTensor:\n    def __init__(self, excluded_keys=(\"id\")):\n        if not isinstance(excluded_keys, set):\n            excluded_keys = set(excluded_keys)\n        self.excluded = excluded_keys\n\n    def __call__(self, x):\n        result = {k: torch.from_numpy(v) for k, v in x.items() if k not in self.excluded}\n        for k in self.excluded:\n            if k in x.keys():\n                result[k] = x[k]\n        return result","00699a6a":"class IGDataset(Dataset):\n    def __init__(self, df, transform=None, test=False):\n        self.transform = transform\n        data = df\n        self.test = test\n        if not test:\n            self.y = data[\"target\"].astype(int)\n        else:\n            self.y = pd.Series(np.array([-1] * len(data)))\n            \n        data = data.drop(['target'], axis=1)\n        \n        if 'id' in data.columns:\n            data.drop('id', axis=1, inplace=True)\n        # moving wheezy-copper-turtle-magic to the last position for convenience\n        cols = [col for col in data.columns if col != 'wheezy-copper-turtle-magic']\n        data = data[cols + ['wheezy-copper-turtle-magic']]\n        self.x = data.values\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.x[idx]\n        item = {'inputs': x}\n        y = np.array([self.y.iloc[idx]]) \n        item['targets'] = y\n\n        if self.transform:\n            item = self.transform(item)\n        \n        return x, y        ","aac8c4c5":"class IGClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.inference = False\n        self.activation = nn.ELU(inplace=True)\n        # self.sigmoid = nn.Sigmoid()\n        # self.relu = nn.ReLU()\n        self.emb = nn.Embedding(513, 512)\n\n        self.fc1 = nn.Linear(767, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 16)\n        self.bn2 = nn.BatchNorm1d(16)\n        self.fc3 = nn.Linear(16, 1)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # notice cast to long tensor here\n        # print(x)\n        emb = self.emb(x[:, -1].long())\n        num = x[:, :-1].float()\n        data = torch.cat([emb, num], 1)\n        out = self.drop(self.bn1(self.activation(self.fc1(data))))\n        out = self.drop(self.bn2(self.activation(self.fc2(out))))\n        out = self.fc3(out)\n\n        return out","a4830576":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, factor=0.6, min_lr=1e-4, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n        self.last_loss = np.inf\n        self.min_lr = min_lr\n        self.factor = factor\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def step(self, loss):\n        if loss > self.last_loss:\n            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma ** (x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","d5a65b51":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef train_model(model, train_loader,valid_loader, test_loader, loss_fn, lr=0.001,\n                batch_size=512, n_epochs=4, validate=False):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003, min_lr=0.000001,\n                         step_size=300, mode='exp_range', gamma=0.99994)\n\n    valid_loss_min = np.Inf\n    patience = 3\n    stop = False\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        for step, (seq_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(seq_batch.cuda()).float()\n            scheduler.batch_step()\n            loss = loss_fn(y_pred.cpu(), y_batch.float().cpu())\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n\n        model.eval()\n        test_preds = np.zeros((len(test_loader.dataset)))\n\n        val_loss = 0\n        if validate:\n\n            valid_preds = np.zeros((len(valid_loader.dataset)))\n            y_true = []\n            y_prediction = []\n            for i, (seq_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(seq_batch.cuda()).float()\n                val_loss += loss_fn(y_pred.cpu(), y_batch.float()).item() \/ len(valid_loader)\n                valid_preds[i * batch_size:(i+1) * batch_size] = y_pred.detach().cpu().numpy()[:, 0]\n                # print(y_batch.detach().cpu().numpy())\n                # print(valid_preds[i * batch_size:(i+1) * batch_size])\n                y_true.extend(list(y_batch.detach().cpu().numpy()))\n                y_prediction.extend(list(y_pred.detach().cpu().numpy()))\n                # print('local score', print(metrics.roc_auc_score(y_batch.detach().cpu().numpy(), valid_preds[i * batch_size:(i+1) * batch_size])))\n\n        # all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        if epoch > 0 and epoch % 10 == 0:\n            print(f'Epoch {epoch + 1}\/{n_epochs} \\t loss={avg_loss:.4f} val_loss={val_loss:.4f} \\t val_auc={metrics.roc_auc_score(y_true, y_prediction):.4f} time={elapsed_time:.2f}s')\n        \n        valid_loss = avg_loss\n        # print('epoch', epoch, 'valid_loss_min', np.round(valid_loss_min, 4), 'valid_loss', np.round(valid_loss, 4))\n        \n        if valid_loss <= valid_loss_min:\n#             print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n#             valid_loss_min,\n#             valid_loss))\n#             torch.save(model_conv.state_dict(), 'model.pt')\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            # print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n        \n\n    for i, (seq_batch, _) in enumerate(test_loader):\n        y_pred = model(seq_batch.long().cuda()).float().detach()\n\n        test_preds[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n        \n        \n    results_dict = {}\n    results_dict['test_preds'] = test_preds\n    if validate:\n        results_dict['oof'] = y_prediction\n\n    return results_dict","c64362b1":"def train_on_folds(train, test, splits, n_epochs=50, validate=False):\n    if validate:\n        scores = []\n    \n    transf = ToTensor()\n\n    test_preds = np.zeros((len(test), len(splits)))\n    train_oof = np.zeros((len(train), 1))\n    \n    test_dataset = IGDataset(test, transform=transf, test=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    for i, (train_idx, valid_idx) in enumerate(splits):\n\n        train_dataset = IGDataset(train.iloc[train_idx, :], transform=transf)\n        valid_dataset = IGDataset(train.iloc[valid_idx, :], transform=transf)\n        train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n        valid_dataloader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n        print(f'Fold {i + 1}')\n\n        set_seed(42 + i)\n        model = IGClassifier()\n        loss_fn = nn.BCEWithLogitsLoss()\n        model.cuda()\n\n        results_dict = train_model(model, train_dataloader, valid_dataloader, test_dataloader, loss_fn=loss_fn, n_epochs=n_epochs, validate=validate, batch_size=batchsize)\n\n        if validate:\n            train_oof[valid_idx] = results_dict['oof']#.reshape(-1, 1)\n            score = metrics.roc_auc_score(train['target'].iloc[valid_idx], train_oof[valid_idx])\n            print('score', score)\n            scores.append(score)\n            \n        test_preds[:, i] = results_dict['test_preds']\n    print(f'CV mean score: {np.mean(scores)}. Std: {np.std(scores)}')\n    \n    return test_preds","4c6f3693":"import random\ndef set_seed(seed: int = 0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)","3469631d":"train = pd.read_csv(\"..\/input\/train.csv\") \ntest = pd.read_csv(\"..\/input\/test.csv\")\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nlen_train = train.shape[0]\nall_data = pd.concat([train, test], axis=0, sort=False)\n\nscaler = StandardScaler()\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nall_data[cols] = scaler.fit_transform(all_data[cols])\ntrain = all_data[:len_train].reset_index(drop=True)\ntest = all_data[len_train:].reset_index(drop=True)","3d63f78e":"batchsize = 1024\nn_fold = 5\nsplits = list(StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42).split(train, train['target']))","6b867a22":"test_preds = train_on_folds(train, test, splits, n_epochs=200, validate=True)","bc6f69ee":"sub['target'] = sigmoid(test_preds.mean(1))\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","a529496d":"## Preparing data","95d2b28f":"## Code for modelling","7b1ea239":"CyclicLR","b646cde4":"Fixing random state","d83122ae":"Neural net class","2efd57ab":"## Model training","136876ec":"Training on folds","9857e72e":"## General information\n\nIn this kernel I'll train a simple Pytorch model.","2ea43d14":"Converting to tensor","ad94abcf":"Model training","4828627f":"Custom dataset class"}}