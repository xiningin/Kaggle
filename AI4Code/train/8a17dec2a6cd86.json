{"cell_type":{"dee45b03":"code","125e6369":"code","0b4c7eaf":"code","3c5ad5aa":"code","4895ea95":"code","4003e9c0":"code","6ae50a32":"code","524ade1a":"code","ab75dc36":"code","8491c7c6":"code","f1cb7f3f":"code","f95a1b7c":"code","4172bd6b":"code","fd815d72":"code","69510bc5":"code","3a1c6af6":"code","34ca2f8e":"code","db47cdff":"code","4dff98a2":"code","263edf01":"code","766eeed3":"code","e7a7f107":"code","228159e0":"code","29c34971":"code","8e96559a":"code","92eaccc5":"code","151d3c12":"code","20b5b54b":"code","c35a71ec":"code","497cd8e9":"code","98bc6eb6":"code","12f6b759":"code","a504ca8c":"code","b2cfc299":"code","ad6cbcd3":"code","0654e512":"code","2c11f36c":"code","0e0dc573":"code","6a8052de":"code","3a955641":"code","23880f9d":"code","2f69a36d":"code","90b85539":"code","7f266128":"code","0b9b12e7":"code","1e1c79d0":"code","59a5af1b":"code","94299c46":"code","5fd48ccf":"code","0a39931d":"code","ff424bcd":"code","446c7829":"code","12b45168":"code","4820114a":"code","ffc4bff9":"markdown","d21f4096":"markdown","1cad0130":"markdown","89f24b5f":"markdown","0e09e9a7":"markdown","27ef96e5":"markdown","c400b55c":"markdown","ea09f63e":"markdown","a9f67fbb":"markdown","81934b4d":"markdown","28815b7a":"markdown","13337c37":"markdown","b3f5fcda":"markdown","4c67439c":"markdown","3cd6cddb":"markdown","03314636":"markdown","f30bbbb8":"markdown","095402ac":"markdown","b9ae2965":"markdown","da9803cb":"markdown","54414245":"markdown","129eba7f":"markdown","6302e186":"markdown","331ccdcf":"markdown","16d0b327":"markdown","5d9edf0d":"markdown","ecc0071d":"markdown"},"source":{"dee45b03":"import numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom keras import regularizers\n\nimport matplotlib.pyplot as plt\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","125e6369":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","0b4c7eaf":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('ocular-disease-recognition-odir5k')","3c5ad5aa":"train = pd.read_csv('..\/input\/new-df-csv-oc\/new_df_oc.csv')\ntrain_paths = train.filename.apply(lambda x: GCS_DS_PATH+ '\/ODIR-5K\/ODIR-5K\/Training Images\/' + x).values\ntrain_labels = train.target.values","4895ea95":"train.head(10)","4003e9c0":"train=train.drop(columns=['D','C','A','M','G','O'],axis=1)","6ae50a32":"train=train[((train['N']== 1) | (train['H'] == 1))]","524ade1a":"train","ab75dc36":"train,valid = train_test_split(train,test_size = 0.2,random_state = 42)","8491c7c6":"BATCH_SIZE = 8* strategy.num_replicas_in_sync\nimg_size = 512\nEPOCHS = 10\nSEED = 42","f1cb7f3f":"def decode_image(filename, label=None, image_size=(img_size,img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.per_image_standardization(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef preprocess(df,test=False):\n    paths = df.filename.apply(lambda x: GCS_DS_PATH + '\/ODIR-5K\/ODIR-5K\/Training Images\/' + x).values\n    labels = df.loc[:, ['N', 'H']].values\n    if test==False:\n        return paths,labels\n    else:\n        return paths\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n           \n    if label is None:\n        return image\n    else:\n        return image, label","f95a1b7c":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","4172bd6b":"def transform(image,label=None):\n    DIM = img_size\n    XDIM = DIM%2 \n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 8. * tf.random.normal([1],dtype='float32') \n    w_shift = 8. * tf.random.normal([1],dtype='float32') \n  \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n              \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    \n    if label is None:\n        return tf.reshape(d,[DIM,DIM,3])\n    else:\n        return tf.reshape(d,[DIM,DIM,3]),label","fd815d72":"train_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    #.map(data_augment, num_parallel_calls=AUTO)\n    .map(transform,num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","69510bc5":"test_dataset= (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))","3a1c6af6":"LR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","34ca2f8e":"def categorical_focal_loss(gamma=2., alpha=.25):\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * K.log(y_pred)\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n        return K.sum(loss, axis=1)\n    return categorical_focal_loss_fixed","db47cdff":"with strategy.scope():\n    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)","4dff98a2":"with strategy.scope():\n    enet.trainable = True","263edf01":"with strategy.scope():\n    ef7 =tf.keras.Sequential()\n    ef7.add(enet)\n    ef7.add(tf.keras.layers.MaxPooling2D())\n    ef7.add(tf.keras.layers.Conv2D(2048,3,padding='same'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.ReLU())\n    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\n    ef7.add(tf.keras.layers.Flatten())\n\n    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.25))\n\n    ef7.add(tf.keras.layers.Dense(512,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.15))\n    ef7.add(tf.keras.layers.Dense(2,activation='softmax'))\n    ef7.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\n                       ])\n","766eeed3":"h7=ef7.fit(\n    train_dataset,\n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","e7a7f107":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(h7.epoch,h7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(h7.epoch,h7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","228159e0":"ef7.evaluate(test_dataset)","29c34971":"from sklearn.metrics import confusion_matrix\nY_pred = ef7.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N', 'H']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","8e96559a":"import sklearn.metrics\nprint(sklearn.metrics.classification_report(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)))","92eaccc5":"import seaborn as sns\ngroup_names = ['True Negative','False Positive','False Negative','True Positive']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot=labels, fmt='', cmap=\"YlGnBu\")","151d3c12":"tn, fp, fn, tp = confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)).ravel()\nspecificity = tn \/(tn+fp)\nsensitivity=  tp\/ (tp+fn)\nprint('specificity: {:.6f} | sensitivity: {:.6f} '.format(specificity, sensitivity))","20b5b54b":"with strategy.scope():\n    DenseNet201 = tf.keras.applications.DenseNet201(input_shape=(512, 512, 3), weights='imagenet', include_top=False)","c35a71ec":"with strategy.scope():\n    DenseNet201.trainable = True","497cd8e9":"with strategy.scope():\n    model_D201=tf.keras.Sequential()\n    model_D201.add(DenseNet201)\n    model_D201.add(tf.keras.layers.MaxPooling2D())\n    model_D201.add(tf.keras.layers.Conv2D(2048,3,padding='same'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.ReLU())\n    model_D201.add(tf.keras.layers.GlobalAveragePooling2D())\n    model_D201.add(tf.keras.layers.Flatten())\n\n    model_D201.add(tf.keras.layers.Dense(1024,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.25))\n    \n    model_D201.add(tf.keras.layers.Dense(512,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.15))\n    model_D201.add(tf.keras.layers.Dense(2,activation='softmax'))\n    model_D201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\n                       ])\n","98bc6eb6":"D_201=model_D201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)\n    #validation_data=test_dataset)","12f6b759":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(D_201.epoch,D_201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(D_201.epoch,D_201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","a504ca8c":"model_D201.evaluate(test_dataset)","b2cfc299":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_D201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N', 'H']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","ad6cbcd3":"import sklearn.metrics\nprint(sklearn.metrics.classification_report(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)))","0654e512":"import seaborn as sns\ngroup_names = ['True Negative','False Positive','False Negative','True Positive']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot=labels, fmt='', cmap=\"YlGnBu\")","2c11f36c":"tn, fp, fn, tp = confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)).ravel()\nspecificity = tn \/(tn+fp)\nsensitivity=  tp\/ (tp+fn)\nprint('specificity: {:.6f} | sensitivity: {:.6f} '.format(specificity, sensitivity))","0e0dc573":"from keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nimport keras\nfrom keras.models import Model\nfrom keras.activations import hard_sigmoid\nCFG = dict(\n    inp_size          = 512,\n    read_size         = 512, \n    crop_size         = 512,\n    net_size          = 512)\n","6a8052de":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = efn.EfficientNetB7(weights='noisy-student',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"..\/input\/efficientnet-keras-weights-b0b5\/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un m\u00e9canisme d'attention pour activer et d\u00e9sactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http:\/\/akosiorek.github.io\/ml\/2017\/10\/14\/visual-attention.html\n    #2-https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.25)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les cha\u00eenes\n    # kernel_size  d\u00e9termine les dimensions du noyau. Les dimensions courantes comprennent 1\u00d71, 3\u00d73, 5\u00d75 et 7\u00d77, qui peuvent \u00eatre pass\u00e9es en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple\/liste de 2 nombres entiers, sp\u00e9cifiant la hauteur et la largeur de la fen\u00eatre de convolution 2D.\n    #  Ce param\u00e8tre doit \u00eatre un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https:\/\/www.geeksforgeeks.org\/keras-conv2d-class\/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du mod\u00e8le d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https:\/\/codefellows.github.io\/sea-python-401d5\/lectures\/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(2, activation = 'softmax')(dr_steps)\n    model = Model(inputs = [in_lay], outputs = [out_layer])  \n    model.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\n                       ])","3a955641":"ATT_EF7= model.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","23880f9d":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","2f69a36d":"model.evaluate(test_dataset)","90b85539":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N', 'H']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","7f266128":"import sklearn.metrics\nprint(sklearn.metrics.classification_report(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)))","0b9b12e7":"import seaborn as sns\ngroup_names = ['True Negative','False Positive','False Negative','True Positive']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot=labels, fmt='', cmap=\"YlGnBu\")","1e1c79d0":"tn, fp, fn, tp = confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)).ravel()\nspecificity = tn \/(tn+fp)\nsensitivity=  tp\/ (tp+fn)\nprint('specificity: {:.6f} | sensitivity: {:.6f} '.format(specificity, sensitivity))","59a5af1b":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = tf.keras.applications.DenseNet201(weights='imagenet',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"..\/input\/efficientnet-keras-weights-b0b5\/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un m\u00e9canisme d'attention pour activer et d\u00e9sactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http:\/\/akosiorek.github.io\/ml\/2017\/10\/14\/visual-attention.html\n    #2-https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.25)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les cha\u00eenes\n    # kernel_size  d\u00e9termine les dimensions du noyau. Les dimensions courantes comprennent 1\u00d71, 3\u00d73, 5\u00d75 et 7\u00d77, qui peuvent \u00eatre pass\u00e9es en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple\/liste de 2 nombres entiers, sp\u00e9cifiant la hauteur et la largeur de la fen\u00eatre de convolution 2D.\n    #  Ce param\u00e8tre doit \u00eatre un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https:\/\/www.geeksforgeeks.org\/keras-conv2d-class\/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du mod\u00e8le d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https:\/\/codefellows.github.io\/sea-python-401d5\/lectures\/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(2, activation = 'softmax')(dr_steps)\n    model_d201 = Model(inputs = [in_lay], outputs = [out_layer])  \n    model_d201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=2, average=\"macro\")\n                       ])","94299c46":"d201= model_d201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","5fd48ccf":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(d201.epoch,d201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(d201.epoch,d201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","0a39931d":"model_d201.evaluate(test_dataset)","ff424bcd":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_d201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N', 'H']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","446c7829":"import sklearn.metrics\nprint(sklearn.metrics.classification_report(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)))","12b45168":"import seaborn as sns\ngroup_names = ['True Negative','False Positive','False Negative','True Positive']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot=labels, fmt='', cmap=\"YlGnBu\")","4820114a":"tn, fp, fn, tp = confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1)).ravel()\nspecificity = tn \/(tn+fp)\nsensitivity=  tp\/ (tp+fn)\nprint('specificity: {:.6f} | sensitivity: {:.6f} '.format(specificity, sensitivity))","ffc4bff9":"## 2.Affichage des courbes (acc,loss)","d21f4096":"## 2.4 Diviser notre dataset en 80% l'entra\u00eenement et 20% pour le test","1cad0130":"| Methodes | Loss | Accuracy  | Precision | Recall  |F1_score  |specificity |sensitivity  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| VGG16 |  0.9876 | 0.9884  | 0.9840|0.9874 |0.9991 |0.9991 |0.9991 |\n| VGG19 | 0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|DenseNet201|0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|EfficientNetB7 |0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |","89f24b5f":"## 2. Pr\u00e9paration de la base de donn\u00e9es\n\n### 2.1 importer les biblioth\u00e8ques n\u00e9cessaires","0e09e9a7":"## 4. Pr\u00e9traitement des donn\u00e9es","27ef96e5":"## 3. Test et \u00e9valuation","c400b55c":"## 3 Hyperparam\u00e8tre","ea09f63e":"## 11. Affichage des courbes (acc,loss)","a9f67fbb":"## 8. Fonction du taux d'apprentissage","81934b4d":"<center><img src='https:\/\/www.2020mag.com\/CMSImagesThumbnails\/2014\/9\/eyedisease-thumb-large.jpg' height=350><\/center>\n<p>\n<h1><center> Normal Vs hypertension <\/center><\/h1>","28815b7a":"# 5. \u00c9tude comparative","13337c37":"<p>\n<h1><center> DenseNet201 <\/center><\/h1>\n<center><img src='https:\/\/oi.readthedocs.io\/en\/latest\/_images\/cnn_vs_resnet_vs_densenet.png' height=20><\/center>\n<p>\n","b3f5fcda":"## 9. Fonction de perte","4c67439c":"## 6.Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es d'entra\u00eenement ","3cd6cddb":"<p>\n<h1><center> EfficientNetB7 <\/center><\/h1>\n<center><img src='https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s1600\/image2.png' height=350><\/center>\n<p>","03314636":"## 7. Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es de test","f30bbbb8":"# 1. Entra\u00eenement","095402ac":"## 4. Matrice de confusion","b9ae2965":"# 10 Entra\u00eenement","da9803cb":"## 11. Test et \u00e9valuation","54414245":"## 5. Augmentation","129eba7f":"## 2.2 Configuration de tpu","6302e186":"## 13. specificity et sensitivity","331ccdcf":"## 12. Matrice de confusion","16d0b327":"## 1.Objectifs\n\n* L\u2019objectif ici est de d\u00e9velopper un r\u00e9seau CNN en se basant sur des architecture diff\u00e9rentes (vgg16,vgg19,EfficientNetB7,DenseNet201) pour la classification de deux maladie (Normal Vs hypertension)","5d9edf0d":"### 2.3 Normal Vs hypertension","ecc0071d":"**l'\u00e9valuation des 4 mod\u00e8les (vgg16,vgg19,DenseNet-201,EfficientNetB7) est bas\u00e9 sur Accuracy,Pr\u00e9cision,Recall,F1_Score,AUC,Specificity,Sensitivity**\n\n**Positifs vrais (TP)** - Ce sont les valeurs positives correctement pr\u00e9dites, ce qui signifie que la valeur de la classe r\u00e9elle est oui et que la valeur de la classe pr\u00e9dite est \u00e9galement oui.\n\n**Vrais n\u00e9gatifs (TN)** - Il s'agit des valeurs n\u00e9gatives correctement pr\u00e9dites, ce qui signifie que la valeur de la classe r\u00e9elle est non et que la valeur de la classe pr\u00e9dite est \u00e9galement non.\n\n**Faux positifs (FP) et faux n\u00e9gatifs (FN)** , ces valeurs se produisent lorsque votre classe r\u00e9elle est en contradiction avec la classe pr\u00e9dite.\n\n**Accuracy** - La pr\u00e9cision est la mesure de performance la plus intuitive et il s'agit simplement d'un rapport entre l'observation correctement pr\u00e9dite et le total des observations.\n\n**Accuracy  = TP+TN\/TP+FP+FN+TN**\n\n**Pr\u00e9cision** - La pr\u00e9cision est le rapport entre les observations positives correctement pr\u00e9dites et le total des observations positives pr\u00e9dites. \n\n**Pr\u00e9cision = TP\/TP+FP**\n\n**Recall (sensibilit\u00e9)**- Le Recall est le rapport entre les observations positives correctement pr\u00e9dites et toutes les observations de la classe r\u00e9elle \n\n**Recall = TP\/TP+FN**\n\n**F1_Score** - Le score F1 est la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et du rappel. Par cons\u00e9quent, ce score tient compte \u00e0 la fois des faux positifs et des faux n\u00e9gatifs. \n\n**F1_Score = 2*(Rappel * Pr\u00e9cision) \/ (Rappel + Pr\u00e9cision)**\n\n**Sensitivity mesure la proportion de vrais positifs qui sont correctement identifi\u00e9s**\n**Specificity mesure la proportion de vrais n\u00e9gatifs qui sont correctement identifi\u00e9s comme non**\n"}}