{"cell_type":{"55a9588d":"code","401772b5":"code","6c631413":"code","3548daf0":"code","b015e902":"code","21859074":"code","b69bf0d3":"code","a265e90e":"code","f22041d6":"code","01269276":"code","d6a69e22":"code","31152d2a":"code","2b4c1b07":"markdown","04f6da0e":"markdown","cbd19cbb":"markdown","3748d0d5":"markdown","cbd63891":"markdown","7989e333":"markdown","5cfdb773":"markdown","f8a6c0ea":"markdown"},"source":{"55a9588d":"# Sync Notebook with VS Code\n!git clone https:\/\/github.com\/sarthak-314\/fast-nlp\nimport sys; sys.path.append('fast-nlp')\n\nfrom src import *\nfrom src.tflow import *","401772b5":"%%hyperparameters \n\n## Model Architecture ## \nbackbone: unitary\/unbiased-toxic-roberta\nattention_dropout: 0.10\nfrom_pytorch: True\nmax_seq_len: 128\n\nhidden_layers: [256, 64, 16, 4]\nhidden_dropout: 0.10\n\n## LR Scheduler ## \nlr_cosine_decay: \n    max_lr: 16e-6\n    lr_gamma: 0.90\n    decay_epochs: 1.0\n    min_lr: 0\n    step_gamma: 2\nwarmup_epochs: 1\nwarmup_power: 2\n\nbatch_size: 16\nvalid_batch_size: 32\nfreeze_batch_size: 512\n\n## Optimizer ##\noptimizer: \n    _target_: AdamW\n    weight_decay: 1e-4\n    max_grad_norm: 1.0\n    use_swa: True\n    epsilon: 1e-6\n    betas: [0.9, 0.999]\n    # average_decay: 0.999\n    # dynamic_decay: True\n\n## Loss Function ##\nlow_agree_weight: 1.0\n\n## Model Training ## \nmax_epochs: 50\ncheckpoints_per_epoch: 2\n\n## Data Factory ## \nfold: 3\nrandom_state: 69420\nadd_special_tokens: False","6c631413":"!wandb login '00dfbb85e215726cccc6f6f9ed40e618a7cf6539'\nSTRATEGY = tf_accelerator(bfloat16=False, jit_compile=False)\nset_seed(HP.random_state)\n\nwith STRATEGY.scope(): \n    backbone = TFAutoModel.from_pretrained(\n        HP.backbone, \n        attention_probs_dropout_prob=HP.attention_dropout, \n        from_pt=HP.from_pytorch, \n    )","3548daf0":"TOXIC_FEATURES = [\n    'severe_toxic', 'identity_hate', 'threat', \n    'toxic', 'insult', 'obscene', \n]\nSPECIAL_TOKENS = [\n    '[SEVERE_TOXIC]', '[IDENTITY_HATE]', '[THREAT]', '[TOXIC]', '[INSULT]', '[OBSCENE]', \n    '[SOFT]', '[HARD]', \n]\nFEAT_TO_TOKEN = {feat: token for feat, token in zip(TOXIC_FEATURES, SPECIAL_TOKENS)}\n\ntokenizer = AutoTokenizer.from_pretrained(\n    HP.backbone, additional_special_tokens=SPECIAL_TOKENS, \n)","b015e902":"def add_special_tokens_LT(row):\n    text = row.less_toxic \n    for feat in reversed(TOXIC_FEATURES): \n        if row[f'LT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.LT_toxic == -1) or (1 < row.LT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_special_tokens_MT(row):\n    text = row.more_toxic \n    for feat in reversed(TOXIC_FEATURES): \n        if row[f'MT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.MT_toxic == -1) or (1 < row.MT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_feature_values(df, old): \n    old_dict = old.set_index('comment_text').to_dict()\n    for feat in TOXIC_FEATURES: \n        df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n        df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    return df\n\n\ndef add_special_tokens(df): \n    if not HP.add_special_tokens: \n        return df\n    df.more_toxic = df.apply(add_special_tokens_MT, axis=1)\n    df.less_toxic = df.apply(add_special_tokens_LT, axis=1)\n    return df\n\ndef add_special_tokens_test(test): \n    def func(row): \n        text = row.comment_text\n        for feat in reversed(TOXIC_FEATURES): \n            if row[feat] > 0.5: \n                text = f'{FEAT_TO_TOKEN[feat]} ' + text\n        return '[HARD] ' + text\n    test['text'] = test.apply(func, axis=1)\n    return test","21859074":"df = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/valid.csv')\nold = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\ndf = df[df.more_toxic.isin(old.comment_text) & df.less_toxic.isin(old.comment_text)]\n\ndf = add_feature_values(df, old)\ndf = add_special_tokens(df)\ndf['margin'] = df.agree.apply(lambda agree: HP.low_agree_weight if agree<1 else 1)\n\ntrain, valid = df[df.fold!=HP.fold], df[df.fold==HP.fold]\ntrain_freeze = train","b69bf0d3":"%%time \ndef tokenize_text(text): \n    return tokenizer(\n        text, \n        max_length=HP.max_seq_len, \n        padding='max_length', \n        truncation=True, \n    )\n\ndef convert_to_features_train(example_batch): \n    M_tokenized_examples = tokenize_text(example_batch['more_toxic'])\n    L_tokenized_examples = tokenize_text(example_batch['less_toxic'])\n    return {\n        'MT_ids': M_tokenized_examples['input_ids'], \n        'MT_mask': M_tokenized_examples['attention_mask'], \n        'LT_ids': L_tokenized_examples['input_ids'], \n        'LT_mask': L_tokenized_examples['attention_mask'], \n    }\n\ndef convert_to_features_valid(example_batch): \n    return convert_to_features_train(example_batch)\n\ndef convert_to_features_test(example_batch): \n    tokenized_examples = tokenize_text(example_batch['text'])\n    return {\n        'input_ids': tokenized_examples['input_ids'], \n        'attention_mask': tokenized_examples['attention_mask']\n    }\n\ndef df_to_dataset(df, df_type='train'): \n    convert_to_features_fn = {\n        'train': convert_to_features_train, \n        'valid': convert_to_features_valid, \n    }[df_type]\n    raw_dataset = datasets.Dataset.from_pandas(df)\n    processed_dataset = raw_dataset.map(\n        convert_to_features_fn, \n        batched=True, \n        batch_size=1024, \n        num_proc=4, \n        desc='Running tokenizer on the dataset', \n    )\n    processed_dataset.set_format(type='numpy')\n    print(f'{blue(len(df))} examples and {blue(len(processed_dataset))} features in {df_type}')\n    return processed_dataset\n\n# Build Processed Datasets\ntrain_dataset = df_to_dataset(df=train, df_type='train')\ntrain_freeze_dataset = df_to_dataset(df=train_freeze, df_type='train')\nvalid_dataset = df_to_dataset(df=valid, df_type='valid')","a265e90e":"def dataset_to_train_ds(dataset): \n    model_inputs = {\n        'MT_ids': dataset['MT_ids'].astype(np.int32), \n        'MT_mask': dataset['MT_mask'].astype(np.int32), \n        'LT_ids': dataset['LT_ids'].astype(np.int32), \n        'LT_mask': dataset['LT_mask'].astype(np.int32), \n        'margin': dataset['margin'].astype(np.float32),\n    }\n    input_ds = tf.data.Dataset.from_tensor_slices(model_inputs)\n    \n    labels = {}\n    for feat in TOXIC_FEATURES: \n        labels[f'MT_{feat}'] = dataset[f'MT_{feat}'].astype(np.int32)\n        labels[f'LT_{feat}'] = dataset[f'LT_{feat}'].astype(np.int32)\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    \n    ds = tf.data.Dataset.zip((input_ds, label_ds))\n    return ds\n\ndef dataset_to_valid_ds(dataset): \n    return dataset_to_train_ds(dataset)\n\ndef dataset_to_test_ds(dataset): \n    id_ds = tf.data.Dataset.from_tensor_slices(dataset['input_ids'].astype(np.int32))\n    mask_ds = tf.data.Dataset.from_tensor_slices(dataset['attention_mask'].astype(np.int32))\n    ds = tf.data.Dataset.zip((id_ds, mask_ds))\n    ds = tf.data.Dataset.zip((ds, ds))\n    return ds\n    \ndef build_train_ds(dataset, batch_size): \n    train_ds = dataset_to_train_ds(dataset)\n    train_ds = train_ds.shuffle(len(dataset), reshuffle_each_iteration=True).repeat()\n    train_ds = train_ds.batch(batch_size)\n    train_steps = len(dataset)\/\/batch_size+1\n    return train_ds.prefetch(tf.data.AUTOTUNE), train_steps\n\ndef build_valid_ds(dataset, batch_size): \n    valid_ds = dataset_to_valid_ds(dataset)\n    valid_ds = valid_ds.batch(batch_size).cache()\n    valid_steps = len(dataset)\/\/batch_size\n    return valid_ds.prefetch(tf.data.AUTOTUNE), valid_steps\n\n# Build Tensorflow Datasets\ntrain_ds, train_steps = build_train_ds(train_dataset, HP.batch_size)\ntrain_freeze_ds, train_freeze_steps = build_train_ds(train_freeze_dataset, HP.freeze_batch_size)\nvalid_ds, valid_steps = build_valid_ds(valid_dataset, HP.valid_batch_size)","f22041d6":"def bce_loss(y_true, y_pred):\n    loss_fn = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n    losses = loss_fn(y_true, y_pred)\n    return tf.math.reduce_mean(losses)\n\nclass NLPModel(tf.keras.Model): \n    def train_step(self, data): \n        x, y = data\n        with tf.GradientTape() as tape:\n            batch_size = tf.shape(x['MT_ids'])[0]\n            MT_preds = self((x['MT_ids'], x['MT_mask']), training=True)\n            LT_preds = self((x['LT_ids'], x['LT_mask']), training=True)\n            loss = self.compiled_loss(\n                MT_preds, \n                LT_preds, \n                regularization_losses=self.losses, \n                sample_weight=x['margin'], \n            )\n            MT_bce_loss, LT_bce_loss = 0, 0\n            for feat in TOXIC_FEATURES: \n                MT_bce_loss += bce_loss(y[f'MT_{feat}'], MT_preds[feat])\n                LT_bce_loss += bce_loss(y[f'LT_{feat}'], LT_preds[feat])\n            loss += MT_bce_loss + LT_bce_loss\n            \n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.compiled_metrics.update_state(MT_preds, LT_preds)\n        return {m.name: m.result() for m in self.metrics}\n    \n    def test_step(self, data):\n        x, y = data\n        MT_preds = self((x['MT_ids'], x['MT_mask']), training=False)\n        LT_preds = self((x['LT_ids'], x['LT_mask']), training=False)\n        self.compiled_loss(MT_preds, LT_preds, regularization_losses=self.losses)\n        self.compiled_metrics.update_state(MT_preds, LT_preds)\n        return {m.name: m.result() for m in self.metrics}\n            \ndef bert_init(self, initializer_range=0.02): \n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)    \n            \ndef build_hidden_layer(hidden_layer_units=[], hidden_dropout=0.10, name='hidden_layer'): \n    if not hidden_layer_units: \n        return lambda x: x\n    hidden_layers = []\n    for units in hidden_layer_units: \n        hidden_layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        hidden_layers.append(tf.keras.layers.Dense(\n            units=units, \n            activation=tfa.activations.mish, \n            kernel_initializer=bert_init(0.02),\n        ))\n    return tf.keras.Sequential(hidden_layers, name=name)\n    \ndef build_model(backbone): \n    input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    if 'pooler_output' in backbone_outputs: \n        x = backbone_outputs.pooler_output\n    else: \n        x = tf.reduce_mean(backbone_outputs.last_hidden_state, axis=1)\n    \n    hidden_layer = build_hidden_layer(HP.hidden_layers, HP.hidden_dropout, name='hidden_layer')\n    x = hidden_layer(x)\n    \n    model_outputs = {}\n    for feat in TOXIC_FEATURES: \n        layer = tf.keras.layers.Dense(1, activation='sigmoid', name=feat)\n        model_outputs[feat] = layer(x)\n    model_outputs['y'] = tf.keras.layers.Dense(1, name='y')(x)\n    \n    return NLPModel(inputs=[input_ids, attention_mask], outputs=model_outputs)\n\nwith STRATEGY.scope(): \n    model = build_model(backbone)\ntf.keras.utils.plot_model(model)","01269276":"def margin_ranking_loss(MT_pred, LT_pred): \n    return tf.math.maximum(1.0 + (LT_pred-MT_pred), 0)\n    \ndef MT(MT_pred, _): \n    return tf.reduce_mean(MT_pred)\ndef LT(_, LT_pred): \n    return tf.reduce_mean(LT_pred)\n\ndef accuracy(MT_pred, LT_pred): \n    correct = tf.math.reduce_sum(tf.where(MT_pred>LT_pred, 1, 0))\n    wrong = tf.math.reduce_sum(tf.where(MT_pred>LT_pred, 0, 1))\n    return correct \/ (correct+wrong)\n\ndef get_compiled_model(): \n    with STRATEGY.scope(): \n        model = build_model(backbone)\n        lr_scheduler = lr_scheduler_factory(HP.warmup_epochs, HP.warmup_power, HP.lr_cosine_decay, train_steps)\n        optimizer = optimizer_factory(HP.optimizer, lr_scheduler)\n        model.compile(\n            optimizer=optimizer, \n            loss={'y': margin_ranking_loss}, \n            metrics={'y': [accuracy, MT, LT], 'toxic': LT},  \n            run_eagerly=HARDWARE == 'CPU',\n            steps_per_execution=1024,\n        )    \n    return model\n        \ndef quick_compile_model(): \n    with STRATEGY.scope(): \n        model = build_model(backbone)\n        optimizer = optimizer_factory(HP.optimizer, 1e-2)\n        model.compile(\n            optimizer=optimizer, \n            loss={'y': margin_ranking_loss}, \n            metrics={'y': [accuracy, MT, LT], 'toxic': LT},  \n        )\n    return model\n\n# Model Training Callbacks \naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_acc.h5', monitor='val_y_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1\n)\nloss_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_loss.h5', monitor='val_loss', mode='min', save_weights_only=True, save_best_only=True, verbose=1\n)\nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.50, patience=1, verbose=True, mode='min'\n)","d6a69e22":"with STRATEGY.scope(): \n    backbone.trainable = False\n    model = quick_compile_model()\nhistory = model.fit(\n    train_freeze_ds, steps_per_epoch=train_steps, epochs=10, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau],\n)\nmodel.save_weights('freeze_tuned_model.h5')","31152d2a":"with STRATEGY.scope(): \n    backbone.trainable = True\n    model = get_compiled_model()\n    model.load_weights('freeze_tuned_model.h5')\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps\/\/HP.checkpoints_per_epoch+1, epochs=HP.max_epochs*HP.checkpoints_per_epoch, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[accuracy_checkpoint, loss_checkpoint],\n)","2b4c1b07":"# __Toxic Valid Old Only Bi Encoder Train__\n\nFinal model fine tuning on validation_data with old only samples\n\n---\n### <a href='#hyperparameters'> \u2699\ufe0f Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#training'> \u26a1 Training <\/a> \n\n","04f6da0e":"## Model Training \n---\n#### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a>\n\n<a name='training'>","cbd19cbb":"## Build Dataframes\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#training'> \u26a1 Training <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a>\n\n<a name='huggingface-data-module'\/>","3748d0d5":"# \u2692 Data Factory \u2692\n\n#### <a href='#dataframes'> Dataframes <\/a> | <a href='#huggingface-data-module'> Huggingface Data Module <\/a> | <a href='#tensorflow-data-module'> Tensorflow Data Module <\/a>\n\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#training'> \u26a1 Training <\/a>\n\n\n\n\n<a name='data-factory'>\n   ","cbd63891":"# \u2699\ufe0f Hyperparameters \u2699\ufe0f\n---\n### <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#training'> \u26a1 Training <\/a> \n\n<a name='hyperparameters'>","7989e333":"# \ud83e\udde0 Model Factory \ud83e\udde0\n---\n#### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#training'> \u26a1 Training <\/a>\n\n<a name='model-factory'\/>","5cfdb773":"## Tensorflow Data Module\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#training'> \u26a1 Training <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a>\n\n<a name='tensorflow-data-module'\/>","f8a6c0ea":"## \ud83e\udd17 Huggingface Data Module\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#training'> \u26a1 Training <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a>\n\n<a name='huggingface-data-module'\/>"}}