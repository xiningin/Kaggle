{"cell_type":{"13e74ab2":"code","42eb3ce9":"code","f601aee9":"code","f448b08c":"code","4ac0dbc8":"code","18e66029":"code","3148cf83":"code","657b5db5":"code","7c255829":"code","f68c2613":"code","60d5cbdd":"code","3322a9f1":"code","b28f77d9":"code","a194f9f2":"code","01f05052":"code","cef46ebf":"code","a168b63e":"code","8ddad00f":"code","58c0e631":"code","2d5c4ac7":"code","5a076647":"code","248ce22c":"code","f43ace84":"code","5d1d4a77":"code","47b1f71f":"code","78549db6":"code","506e0e7f":"code","7c0826aa":"code","6851a634":"code","cb5e3fc9":"code","c160d263":"code","fbe8d1c0":"markdown"},"source":{"13e74ab2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directoryka\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42eb3ce9":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","f601aee9":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","f448b08c":"train.shape, test.shape","4ac0dbc8":"# Normalize the data\ntrain = train \/ 255.0\ntest = test \/ 255.0","18e66029":"y = train[\"label\"]\ny = tf.keras.utils.to_categorical(y, num_classes=10)\nimage_id = list(test.index)\nimage_id = [i+1 for i in image_id]\n\ntrain = train.drop(\"label\", axis=1)\ntrain = train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","3148cf83":"train.shape","657b5db5":"def plot_image(index):\n    plt.figure(0, figsize=(4,4))\n    plt.imshow(np.reshape(train[index],(28,28)))","7c255829":"plot_image(300)","f68c2613":"def plot_mulitple_images(columns, lines, index):\n    plt.figure(0, figsize=(10,10))\n    for i in range(columns*lines):\n        plt.subplot(lines, columns, i+1)\n        plt.imshow(np.reshape(train[index+i], (28,28)))\n        plt.axis('off')","60d5cbdd":"plot_mulitple_images(5, 6, 34)","3322a9f1":"import keras, os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization, MaxPool2D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","b28f77d9":"xtrain, xtest, ytrain, ytest = train_test_split(train, y, test_size=0.2)\nxtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.2)","a194f9f2":"xtrain.shape, xval.shape, xtest.shape","01f05052":"from matplotlib import pyplot as plt\nimport math\nfrom keras.callbacks import LambdaCallback\nimport keras.backend as K\n\n\nclass LRFinder:\n    \"\"\"\n    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n    See for details:\n    https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs['loss']\n        self.losses.append(loss)\n\n        # Check whether the loss got too large or NaN\n        if math.isnan(loss) or loss > self.best_loss * 4:\n            self.model.stop_training = True\n            return\n\n        if loss < self.best_loss:\n            self.best_loss = loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n        num_batches = epochs * x_train.shape[0] \/ batch_size\n        self.lr_mult = (end_lr \/ start_lr) ** (1 \/ num_batches)\n\n        # Save weights into a file\n        self.model.save_weights('tmp.h5')\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n        self.model.fit(x_train, y_train,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=[callback])\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights('tmp.h5')\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n        \"\"\"\n        Plots the loss.\n        Parameters:\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n        \"\"\"\n        plt.ylabel(\"loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n\n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        \"\"\"\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) \/ sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"rate of loss change\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n        plt.ylim(y_lim)","cef46ebf":"def model():\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n\n    adam = tf.keras.optimizers.Adam()\n\n    model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\n    checkpoint_path = 'training_1\/cp.ckpt'\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # create checkpoint callback\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\n    return model","a168b63e":"\ndef determineLearningRate(xtrain,ytrain,xtest,ytest):    \n    input_shape = (28,28,1)\n    num_classes = 10\n    epochs = 15\n    \n    lr_finder = LRFinder(model())\n    lr_finder.find(xtrain,ytrain, start_lr=0.000001, end_lr=100, batch_size=64, epochs=epochs)\n    lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n    plt.show()\n    return model\ndetermineLearningRate(xtrain, ytrain, xtest, ytest)","8ddad00f":"def model():\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n\n    adam = tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False)\n\n    model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\n    checkpoint_path = 'training_1\/cp.ckpt'\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # create checkpoint callback\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\n    print(model.summary())\n    history = model.fit(xtrain, ytrain, epochs=100, callbacks=[cp_callback, early], validation_data=(xval, yval))\n    prediction = model.predict(test)\n    prediction = np.argmax(prediction, axis=1)\n\n    return history, prediction","58c0e631":"history, prediction = model()","2d5c4ac7":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5a076647":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","248ce22c":"data = {\"ImageId\": image_id, \"Label\":prediction}\nresults = pd.DataFrame(data)\nresults.to_csv(\"result.csv\",index=False)","f43ace84":"def plot_mulitple_images_with_label(columns, lines, index):\n    plt.figure(0, figsize=(12,12))\n    for i in range(columns*lines):\n        plt.subplot(lines, columns, i+1)\n        plt.imshow(np.reshape(test[index+i], (28,28)))\n        plt.title(\"Predicted : \"+str(prediction[index+i]))\n        plt.axis('off')","5d1d4a77":"plot_mulitple_images_with_label(5, 3, 27)","47b1f71f":"from keras.preprocessing.image import ImageDataGenerator","78549db6":"datagen_train = datagen_valid = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\ndatagen_train.fit(xtrain)\ntrain_gen = datagen_train.flow(xtrain, ytrain, batch_size=64)\ndatagen_valid.fit(xval)\nvalid_gen = datagen_valid.flow(xval, yval, batch_size=64)","506e0e7f":"model = Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = (28,28,1)),\n        Conv2D(32, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        \n        \n        Flatten(),\n        \n        Dense(512, activation='relu'),\n        Dropout(0.5),\n        \n        Dense(10, activation = \"softmax\")\n        \n    ])\nadam = tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nearly = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n\ncheckpoint_path = 'training_1\/cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=True,\n                                                verbose=1)\nprint(model.summary())\nhistory = model.fit((train_gen), epochs=100, callbacks=[cp_callback, early], validation_data=(valid_gen))\nprediction = model.predict(test)\nprediction = np.argmax(prediction, axis=1)","7c0826aa":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6851a634":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","cb5e3fc9":"data = {\"ImageId\": image_id, \"Label\":prediction}\nresults = pd.DataFrame(data)\nresults.to_csv(\"result_data_generator.csv\",index=False)","c160d263":"plot_mulitple_images_with_label(5, 3, 27)","fbe8d1c0":"# Using Image data Generator"}}