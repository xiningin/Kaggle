{"cell_type":{"53439925":"code","cf90d714":"code","eac0bc58":"code","7d059301":"code","abc787dd":"code","02b5d7bf":"code","d46c932b":"code","f3e63cb5":"code","b93285c6":"code","040d4f15":"code","efdb1853":"code","5181dc6b":"code","bf4a9e35":"code","47ab8dc9":"code","41ba658f":"code","281e36a6":"code","7fc3272b":"code","5bb8fe38":"code","e22fc146":"code","d8a3f3b7":"code","49bad938":"code","4ca286b3":"code","fb8568c7":"code","39c47d25":"code","f391cce1":"code","352e7046":"code","33ddb470":"code","ec4a8cb5":"code","db43d511":"code","6d5897dd":"code","c03cd545":"code","6783bfa8":"code","ad89c225":"code","9d8fd951":"markdown","d3c9ff16":"markdown","f53c9f34":"markdown","31972f02":"markdown","abe5d50a":"markdown"},"source":{"53439925":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cf90d714":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom datetime import datetime, date\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import LabelBinarizer, OneHotEncoder\nfrom itertools import chain","eac0bc58":"from sklearn.base import BaseEstimator, TransformerMixin\n#--------------------------------------------------------------\nclass OpenCloseTransformer(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  open_col_name = 'open',\n                  close_col_name = 'close',\n                  new_col = 'fprise',\n                  remove_old_col = False\n                  ):\n        self.openc = open_col_name\n        self.closec = close_col_name\n        self.newc = new_col\n        self.isRemove = remove_old_col\n        \n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X[self.newc] = (X[self.openc]-X[self.closec])\/((X[self.openc]+X[self.closec])\/2)\n        if self.isRemove == True:\n            X = X.drop([self.openc, self.closec], axis=1)\n        return X\n#--------------------------------------------------------------\nclass toTypeTransformer(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  cat_list = [],\n                  tranformType = 'U'\n                  ):\n        self.ttype = tranformType\n        self.catlist = cat_list\n        \n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X.loc[:self.catlist] = X[self.catlist].astype(self.ttype)\n        return X\n#--------------------------------------------------------------    \nclass dataSelector(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  cat_list = [],\n                  isRetDataFrame = False):\n        self.catlist = cat_list\n        self.isDf = isRetDataFrame\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        if(self.isDf == True):\n            return X[self.catlist]\n        else:\n            return X[self.catlist].values\n    \n#--------------------------------------------------------------\nclass toLabeEncoderTransformer(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  cat = '',\n                  create_new_col = True,\n                  pref_col = 'le_'):\n        \n        self.cnc = create_new_col\n        self.cat = cat\n        self.pref = pref_col\n        pass\n\n    def fit(self, X, y=None):\n        self.enc = LabelEncoder()\n        self.enc.fit(X[self.cat])\n        return self\n\n    def transform(self, X, y=None):\n        if(True == self.cnc):\n            #X[self.pref + self.cat] = self.enc.transform(X[self.cat])\n            X.loc[:,self.pref + self.cat] = self.enc.transform(X[self.cat])\n        else:\n            #X[self.cat] = self.enc.transform(X[self.cat])\n            X.loc[:,self.cat] = self.enc.transform(X[self.cat])\n        return X\n    \n    \n #--------------------------------------------------------------\nclass fillNATransformer(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  _list = [],\n                  _value = 'undefine'):\n        \n        self.lst = _list\n        self.val = _value\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X.loc[:,self.lst] = X[self.lst].fillna(self.val)\n        return X   \n#--------------------------------------------------------------\nclass fillNASrcDscTransformer(BaseEstimator, TransformerMixin):\n    def __init__ (self,\n                  _list_src = [],\n                  _list_dst = []):\n        \n        self.src = _list_src\n        self.dst = _list_dst\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        #X[self.src] = X[self.src].fillna(0.0).astype(np.float64)\n        X.loc[:,self.src] = X[self.src].fillna(0.0).astype(np.float64)\n        for i in range(len(self.src)):\n            #X[self.dst[i]] = X[self.dst[i]].fillna(X[self.src[i]])\n            X.loc[:,self.dst[i]] = X[self.dst[i]].fillna(X[self.src[i]])\n        return X\n#--------------------------------------------------------------------------------\ndef adjust_time(market_train_df,news_train_df,time_threshold=22):\n\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -&gt; 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -&gt; 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(time_threshold,'h')).dt.ceil('1D')\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n    return (market_train_df,news_train_df)\n#--------------------------------------------------------------------------------\nfrom sklearn.model_selection import train_test_split\ndef trainTestSplit(x_all, y_all, _train, rs = 42):\n    \n    t_i, _i = train_test_split(_train.index.values,test_size=0.10, random_state=rs)\n    te_i, v_i = train_test_split(_i,test_size=0.40, random_state=rs)\n    \n    tr_x = x_all[t_i]\n    tr_r = y_all[t_i]\n    tr_y = np.int32(y_all[t_i] > 0.0)\n\n    v_x = x_all[v_i]\n    v_r = y_all[v_i]\n    v_y = np.int32(y_all[v_i] > 0.0)\n    v_u = _train.loc[v_i, 'universe']\n    v_d = _train.loc[v_i, 'time']\n\n    te_x = x_all[te_i]\n    te_r = y_all[te_i]\n    te_y = np.int32(y_all[te_i] > 0.0)\n    te_u = _train.loc[te_i, 'universe']\n    te_d = _train.loc[te_i, 'time']\n    \n    return (tr_x, tr_y, v_x, v_y, v_u, v_r, v_d, te_x, te_y, te_u, te_d, te_r)\n    #tr_x, r_x, tr_y, r_y = train_test_split(x_all, y_all, test_size=0.10, random_state=rs)\n    #te_x, v_x, te_y, v_y = train_test_split(r_x, r_y, test_size=0.40, random_state=rs+1)\n    #return (tr_x, tr_y, te_x, te_y, v_x, v_y)\n\n\n\n#--------------------------------------------------------------------------------\ndef getFeatIndexes(df, cat_feat, num_feat, multp = 0.5, return_lookup_df = False, verbose = 0):\n    if cat_feat != []:\n        multp = 0.5\n        cat_feats_idx = np.array([df.columns.get_loc(x) for x in cat_feat])\n        cat_feats_dm = np.array([np.int16(len(df[x].unique()) **multp) for x in cat_feat])\n        cat_feat_lookup = pd.DataFrame({'feature': cat_feat, 'column_index': cat_feats_idx, 'column_dm': cat_feats_dm})\n    else:\n        cat_feats_idx = np.array([])\n        cat_feats_dm = np.array([])\n        cat_feat_lookup = pd.DataFrame({'feature': cat_feat, 'column_index': cat_feats_idx, 'column_dm': cat_feats_dm})\n        \n    num_feats_idx = np.array([df.columns.get_loc(x) for x in num_feat])\n    num_feat_lookup = pd.DataFrame({'feature': num_feat, 'column_index': num_feats_idx})\n\n    if(verbose > 0):\n        display(cat_feat_lookup.head())\n        display(num_feat_lookup.head())\n    \n    if return_lookup_df == True:\n        return cat_feats_idx, cat_feats_dm, num_feats_idx, cat_feat_lookup, num_feat_lookup\n    \n    return cat_feats_idx, cat_feats_dm, num_feats_idx\n\n#----------------------------------------------------------------------------\ndef change_date_to_datetime(x):\n    str_time = str(x)\n    date = '{}-{}-{}'.format(str_time[:4], str_time[5:7], str_time[8:10])\n    return date\n\n#-----------------------------------------------------------------------------\ndef join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","7d059301":"#create enviroment\nenv = twosigmanews.make_env()","abc787dd":"(market_train, news_train) = env.get_training_data()","02b5d7bf":"display(market_train.head(2))\ndisplay(news_train.head(2))","d46c932b":"_date = date(2010, 1, 1)\nadjust_time(market_train, news_train)\nnews_train['time'] = news_train['time'].dt.date\nmarket_train['time'] = market_train['time'].dt.date\nmarket_train = market_train.loc[market_train['time']>=_date]\nnews_train = news_train.loc[news_train['time']>=_date]","f3e63cb5":"market_train = market_train.reset_index()","b93285c6":"news_cols_agg = {\n#    'urgency': ['min', 'count'],\n#    'takeSequence': ['max'],\n#    'bodySize': ['min', 'max', 'mean', 'std'],\n#    'wordCount': ['min', 'max', 'mean', 'std'],\n#    'sentenceCount': ['min', 'max', 'mean', 'std'],\n#    'companyCount': ['min', 'max', 'mean', 'std'],\n#    'marketCommentary': ['min', 'max', 'mean', 'std'],\n#    'relevance': ['min', 'max', 'mean', 'std'],\n#    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n#    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n#    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n#    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n#    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n#    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['mean'],\n    'wordCount': ['mean'],\n    'sentenceCount': ['mean'],\n    'companyCount': ['mean'],\n    'marketCommentary': ['mean'],\n    'relevance': ['mean'],\n    'sentimentNegative': ['mean'],\n    'sentimentNeutral': ['mean'],\n    'sentimentPositive': ['mean'],\n    'sentimentWordCount': ['mean'],\n    'noveltyCount12H': ['mean'],\n    'noveltyCount24H': ['mean'],\n    'noveltyCount3D':  ['mean'],\n    'noveltyCount5D':  ['mean'],\n    'noveltyCount7D':  ['mean'],\n    'volumeCounts12H': ['mean'],\n    'volumeCounts24H': ['mean'],\n    'volumeCounts3D':  ['mean'],\n    'volumeCounts5D':  ['mean'],\n    'volumeCounts7D':  ['mean']    \n}\nmarket_train = join_market_news(market_train, news_train)","040d4f15":"cat_cols = ['assetCode']\ncat_cols_sel = ['le_assetCode'] #assset column name after label encoding\nnum_cols = ['volume', \n            'op',\n            #'close', 'open', \n            'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10',\n      'urgency_min',\n       'urgency_count', 'takeSequence_max', 'bodySize_mean',\n       'wordCount_mean', 'sentenceCount_mean', 'companyCount_mean',\n       'marketCommentary_mean', 'relevance_mean',\n       'sentimentNegative_mean', 'sentimentNeutral_mean',\n       'sentimentPositive_mean', 'sentimentWordCount_mean',\n       'noveltyCount12H_mean', 'noveltyCount24H_mean',\n       'noveltyCount3D_mean', 'noveltyCount5D_mean',\n       'noveltyCount7D_mean', 'volumeCounts12H_mean',\n       'volumeCounts24H_mean', 'volumeCounts3D_mean',\n       'volumeCounts5D_mean', 'volumeCounts7D_mean']\n\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n\nnum_cols_fna = ['volume', \n           \n        'close', 'open', \n        'urgency_min',\n       'urgency_count', 'takeSequence_max', 'bodySize_mean',\n       'wordCount_mean', 'sentenceCount_mean', 'companyCount_mean',\n       'marketCommentary_mean', 'relevance_mean',\n       'sentimentNegative_mean', 'sentimentNeutral_mean',\n       'sentimentPositive_mean', 'sentimentWordCount_mean',\n       'noveltyCount12H_mean', 'noveltyCount24H_mean',\n       'noveltyCount3D_mean', 'noveltyCount5D_mean',\n       'noveltyCount7D_mean', 'volumeCounts12H_mean',\n       'volumeCounts24H_mean', 'volumeCounts3D_mean',\n       'volumeCounts5D_mean', 'volumeCounts7D_mean']\n","efdb1853":"\ndata_prep_pipeline = Pipeline([\n    ('label_enc', toLabeEncoderTransformer(cat='assetCode')),\n    ('fillSrcNa', fillNASrcDscTransformer(_list_src=column_raw, _list_dst=column_market)),\n    ('fillNa', fillNATransformer(_list=num_cols_fna, _value=0.0)),\n    ('openclose', OpenCloseTransformer(open_col_name='open', close_col_name='close', new_col='op'))\n])\n\nnum_pipeline = Pipeline([    \n    ('selector', dataSelector(cat_list=num_cols, isRetDataFrame = True)),\n    ('std_scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([    \n    ('selector', dataSelector(cat_list=cat_cols_sel, isRetDataFrame = True)),\n    #('label_bin', OneHotEncoder(sparse=False))\n])\n\ncomb_pipeline = FeatureUnion(transformer_list=[\n    ('cat_pipeline', cat_pipeline),\n    ('num_pipeline', num_pipeline)\n])\n\nfull_pipeline = Pipeline([    \n    ('prep', data_prep_pipeline),\n    ('comb', comb_pipeline)\n])","5181dc6b":"market_train.head()","bf4a9e35":"unic_assetsCode = market_train.assetCode.unique()\nunic_assetsCode","47ab8dc9":"all_x = full_pipeline.fit_transform(market_train)\nall_y = market_train['returnsOpenNextMktres10'].clip(-1, 1)\nall_y = np.array(all_y).reshape(-1,1)\n#scaler = MinMaxScaler()\n#all_y = scaler.fit_transform(np.array(all_y).reshape(-1,1))","41ba658f":"val_cnt = 200000\nif val_cnt != 0:\n    all_x = all_x[:val_cnt]\n    all_y = all_y[:val_cnt]","281e36a6":"display(all_x.shape)\ndisplay(all_y.shape)","7fc3272b":"#we need indexes of categorical and numerical features\n#cat_cols = []\nall_used_col = cat_cols + num_cols\n#all_used_col = num_cols\n(cat_feats_idx, cat_feats_dm, num_feats_idx, cat_feat_lookup, \n num_feat_lookup) = getFeatIndexes(market_train[all_used_col], cat_cols, num_cols, multp = 0.5, \n                                   return_lookup_df= True, verbose=1)","5bb8fe38":"import tensorflow as tf\nfrom functools import partial\nfrom datetime import datetime","e22fc146":"# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    \ndef random_batch(X_train, y_train, batch_size):\n    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n    X_batch = X_train[rnd_indices]\n    y_batch = np.array(y_train)[rnd_indices]\n    return X_batch, y_batch,   \n\ndef log_dir(prefix=\"\"):\n    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    root_logdir = \"tf_logs\"\n    if prefix:\n        prefix += \"-\"\n    name = prefix + \"run-\" + now\n    return \"{}\/{}\/\".format(root_logdir, name)\n\ndef getValidationError(_p, _r, _u, _d):\n    # calculation of actual metric that is used to calculate final score\n    x_t_i = _p.ravel() * _r.ravel() * np.array(_u)\n    data = {'day' : _d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score_valid = mean \/ std\n    return score_valid    ","d8a3f3b7":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.exceptions import NotFittedError\nimport shutil\n#initializer\nhe_init = tf.contrib.layers.variance_scaling_initializer()\n\n#classifier\nclass DNNClassifier(BaseEstimator, ClassifierMixin):\n    \n    def __init__(self, n_hidden_layers=3, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n                 learning_rate=0.01, batch_size=500, activation=tf.nn.elu, initializer=he_init,\n                 batch_norm_momentum=None, dropout_rate=None, random_state=None, is_sofmax_loss = False, \n                 logdir = log_dir(\"sigma_log\")):\n        \n        #\"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n        self.n_hidden_layers = n_hidden_layers\n        self.n_neurons = n_neurons\n        self.optimizer_class = optimizer_class\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.activation = activation\n        self.initializer = initializer\n        self.batch_norm_momentum = batch_norm_momentum\n        self.dropout_rate = dropout_rate\n        self.random_state = random_state\n        self.is_sofmax_loss = is_sofmax_loss\n        self._session = None  \n        self.logdir = logdir\n        \n        # parameters for storing model\n        self._checkpoint_path = \".\/tmp\/my_deep_model.ckpt\"\n        self._checkpoint_epoch_path = self._checkpoint_path + \".epoch\"\n        self._final_model_path = \".\/final\/my_deep_final_model\"      \n        \n        pass\n        \n    #create normal dnn with dense layers\n    def _dnn(self, inputs, dnn_name, _n_hidden_layers, _n_neurons, _dropout_rate, _batch_norm_momentum, _activation ):\n        #\"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n        for layer in range(_n_hidden_layers):\n            if _dropout_rate:\n                inputs = tf.layers.dropout(inputs, _dropout_rate, training=self._training)\n            inputs = tf.layers.dense(inputs, _n_neurons,\n                                     kernel_initializer=self.initializer,\n                                     name=dnn_name+\"_hidden%d\" % (layer + 1))\n            if _batch_norm_momentum:\n                inputs = tf.layers.batch_normalization(inputs, momentum=_batch_norm_momentum,\n                                                       training=self._training)\n            inputs = _activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n        return inputs    \n    \n    #create graph\n    def _build_graph(self, n_inputs, n_outputs):\n        #\"\"\"Build the same model as earlier\"\"\"\n        if self.random_state is not None:\n            tf.set_random_seed(self.random_state)\n            np.random.seed(self.random_state)\n\n        X_i = tf.placeholder(tf.float32, [None, n_inputs], name= 'X')\n        y = tf.placeholder(tf.int32, shape=[None, n_outputs], name='y')\n\n        if self.batch_norm_momentum or self.dropout_rate:\n            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n        else:\n            self._training = None\n\n        # Define placeholders for the categorical varaibles\n        with tf.name_scope(\"embedding_var\"): \n            self.cat_placeholders, self.cat_cardinalities, self.cat_dm = [], [], []\n            for idx,dm in zip(cat_feats_idx,cat_feats_dm):\n                exec('self.X_cat_{} = tf.placeholder(tf.int32, shape=[None], name=\\'X_cat_{}\\')'.format(idx, idx))\n                exec('self.cat_placeholders.append(self.X_cat_{})'.format(idx))\n                self.cat_cardinalities.append(len(np.unique(market_train[all_used_col].iloc[:,idx])))\n                self.cat_dm.append(dm)\n\n        # Add embeddings to input\n        X = tf.identity(X_i)\n        X2 = tf.identity(X_i)[:,:1]\n        for feat, card, dm in zip(self.cat_placeholders, self.cat_cardinalities, self.cat_dm):\n            X2 = self.embed_and_attach(X2, feat, card, dm, tf.get_default_graph())\n        \n        dnn_outputs_c = self._dnn(X2, dnn_name = 'X2_input', _n_neurons = 18,\n                                  _dropout_rate = self.dropout_rate,\n                                  _n_hidden_layers = 2,\n                                 _batch_norm_momentum = self.batch_norm_momentum,\n                                 _activation = self.activation)\n        \n            \n        #_dnn(self, inputs, dnn_name, _n_hidden_layers, _n_neurons, _dropout_rate, _batch_norm_momentum, _activation )    \n        dnn_outputs_1 = self._dnn(X, dnn_name = 'X_input', _n_neurons = self.n_neurons,\n                                  _dropout_rate = self.dropout_rate,\n                                  _n_hidden_layers = self.n_hidden_layers,\n                                 _batch_norm_momentum = self.batch_norm_momentum,\n                                 _activation = self.activation)\n        dnn_outputs = self._dnn(dnn_outputs_1, dnn_name = 'X_input_2', _n_neurons = 4,\n                                _dropout_rate = self.dropout_rate,\n                                _n_hidden_layers = 2,\n                                _batch_norm_momentum = self.batch_norm_momentum,\n                                _activation = self.activation)\n        dnn_conc = tf.concat([dnn_outputs, dnn_outputs_c], 1)\n        logits = tf.layers.dense(dnn_conc, n_outputs, kernel_initializer=he_init, name=\"logits\")\n\n        \n        with tf.name_scope(\"loss\"):    \n            if self.is_sofmax_loss == True:\n                xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n            else:\n                y_as_float = tf.cast(y, tf.float32)\n                xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n\n            loss = tf.reduce_mean(xentropy, name='loss')\n\n            loss_v_summary = tf.summary.scalar('validation_loss', loss)\n            loss_t_summary = tf.summary.scalar('train_loss', loss)\n\n\n        with tf.name_scope(\"train\"):\n            optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n            training_op = optimizer.minimize(loss)\n\n        with tf.name_scope(\"eval\"):    \n            if self.is_sofmax_loss == True:\n                y_pred = tf.argmax(logits, axis=1, name='class_predictions')\n                y_proba = tf.nn.softmax(logits, name='probability_predictions')    \n                y_pred_correct = tf.equal(y_pred, tf.argmax(y, axis=1))\n            else:\n                y_proba = tf.nn.sigmoid(logits)\n                y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n                #y_pred = tf.cast(tf.greater_equal(y_proba, 0.5), tf.int32)\n                y_pred_correct = tf.equal(y_pred, y)\n            \n            _,roc_score = tf.metrics.auc(y, y_proba)    \n            accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32), name='accuracy')\n \n        init_global = tf.global_variables_initializer()\n        init_local = tf.local_variables_initializer()\n        saver = tf.train.Saver()\n\n        # Make the important operations available easily through instance variables\n        \n        self._X_i, self._y = X_i, y\n        self._y_proba, self._loss = y_proba, loss\n        self._training_op = training_op\n        self._accuracy, self._roc_score =  accuracy, roc_score\n        self._init_global, self._init_local = init_global, init_local\n        self._saver = saver\n        self._loss_v_summary = loss_v_summary\n        self._loss_t_summary = loss_t_summary\n\n    def close_session(self):\n        if self._session:\n            self._session.close()\n        pass\n\n    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n        \n        reset_graph()\n        dnn_clf.clearModelFiles()\n        \n        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n        self.close_session()\n        \n        n_inputs = len(num_feats_idx)\n        n_outputs = 1\n        \n        \n        self._graph = tf.Graph()\n        with self._graph.as_default():\n            self._build_graph(n_inputs, n_outputs)\n            # extra ops for batch normalization\n            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)        \n            \n            \n        best_loss = np.infty\n        epochs_without_progress = 0\n        max_epochs_without_progress = 10\n        eval_step = 5\n        batches = (len(X) \/\/ self.batch_size)\n        \n        # Now train the model!\n        \n        self._session = tf.Session(graph=self._graph, config=tf.ConfigProto(log_device_placement=True))\n        \n        with self._session.as_default() as sess:\n        \n            if os.path.isfile(self._checkpoint_epoch_path):\n                # if the checkpoint file exists, restore the model and load the epoch number\n                with open(self._checkpoint_epoch_path, \"rb\") as f:\n                    start_epoch = int(f.read())\n                print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n                self._saver.restore(sess, self._checkpoint_path)\n            else:\n                print(\"Training start\")\n                start_epoch = 0\n                init = [self._init_global, self._init_local]\n                sess.run(init)      \n                \n                \n            for epoch in range(start_epoch, n_epochs):\n                for iteration in range(batches):\n\n                    X_batch, y_batch = random_batch(X, y, self.batch_size)\n\n                    \n                    if self._training is not None:\n                        _d = self.get_feed_dict(True, cat_feats_idx, self.cat_placeholders, num_feats_idx, X_batch, y_batch)            \n                    else:\n                        _d = self.get_feed_dict(None, cat_feats_idx, self.cat_placeholders, num_feats_idx, X_batch, y_batch)            \n                        \n                    sess.run(self._training_op, feed_dict=_d)\n                    if extra_update_ops:\n                        sess.run(extra_update_ops, feed_dict=_d)                    \n                    \n\n                loss_train_summary_str = sess.run([self._loss_t_summary], feed_dict=_d)\n\n                if X_valid is not None and y_valid is not None:\n                    if self._training is not None:\n                        _d = self.get_feed_dict(False, cat_feats_idx, self.cat_placeholders, num_feats_idx, X_valid, y_valid) \n                    else:\n                        _d = self.get_feed_dict(None, cat_feats_idx, self.cat_placeholders, num_feats_idx, X_valid, y_valid) \n\n                    loss_val, y_proba_val, loss_summary_str = sess.run([self._loss, \n                                                                                   self._y_proba, \n                                                                                   self._loss_v_summary], \n                                                                              feed_dict=_d)\n                    roc_val = self._roc_score.eval(feed_dict=_d)\n                    v_conf_val = y_proba_val * 2 - 1\n                    error_lb = getValidationError(v_conf_val,v_r,v_u,v_d)\n\n                if epoch % eval_step == 0:\n                    print(\"Epoch:\", epoch,\n                          \"\\tLoss: {:.5f}\".format(loss_val),\n                          \"\\tError:{:.5f}\".format(error_lb),\n                          \"\\tROC: {:.5f}\".format(roc_val))            \n\n                    self._saver.save(sess, self._checkpoint_path)\n\n                    with open(self._checkpoint_epoch_path, \"wb\") as f:\n                        f.write(b\"%d\" % (epoch + 1))\n\n                    if loss_val < best_loss:\n                        self._saver.save(sess, self._final_model_path)\n                        best_loss = loss_val\n                        epochs_without_progress = 0;\n                    else:\n                        epochs_without_progress += eval_step\n                        if epochs_without_progress > max_epochs_without_progress:\n                            print(\"Early stopping\")\n                            break\n\n                    print('epochs_without_progress', epochs_without_progress, 'best_loss', best_loss)  \n                \n            #self._file_writer.flush()\n            #self._file_writer.close()  \n\n            self._saver.restore(sess, self._final_model_path)\n        \n        return self\n                \n    def predict_proba(self, X):\n        if not self._session:\n            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n        with self._session.as_default() as sess:\n        \n            if self._training is not None:\n                _d = self.get_feed_dict(False, cat_feats_idx, self.cat_placeholders, num_feats_idx, X)        \n            else:\n                _d = self.get_feed_dict(None, cat_feats_idx, self.cat_placeholders, num_feats_idx, X)        \n        \n            return self._y_proba.eval(feed_dict=_d)\n        pass\n        \n    def predict(self, X):        \n        if not self._session:\n            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n        with self._session.as_default() as sess:\n        \n            if self._training is not None:\n                _d = self.get_feed_dict(False, cat_feats_idx, self.cat_placeholders, num_feats_idx, X)        \n            else:\n                _d = self.get_feed_dict(None, cat_feats_idx, self.cat_placeholders, num_feats_idx, X)        \n        \n            return self._y_pred.eval(feed_dict=_d) \n        pass\n    \n    def clearModelFiles(self):\n        if os.path.exists('.\/tmp'):\n            shutil.rmtree('.\/tmp')\n        if os.path.exists('.\/final'):\n            shutil.rmtree('.\/final') \n        pass\n    \n    def get_feed_dict(self, _training, cat_feats_idx, cat_placeholders, cont_feats_idx, batch_X, batch_y=None ):\n        \"\"\" Return a feed dict for the graph including all the categorical features\n        to embed \"\"\"\n\n        feed_dict = { }\n        # Continuous X features and the labels if training run\n        if _training is not None:\n            feed_dict[self._training] = _training\n\n        feed_dict[self._X_i] =  batch_X[:, cont_feats_idx]\n\n        # Loop through the categorical features to provide values for the placeholders\n        for idx, tensor in zip(cat_feats_idx, cat_placeholders):\n            feed_dict[tensor] = batch_X[:, idx]        \n        \n        if batch_y is not None:\n           # print('batch_y', batch_y.shape)\n            feed_dict[self._y] = batch_y\n\n        return feed_dict   \n    \n    def embed_and_attach(self,X, X_cat, cardinality, embed_size, g): \n        with self._graph.as_default():\n            embedding = tf.Variable(tf.random_uniform([cardinality, embed_size], -1.0, 1.0))\n            embedded_x = tf.nn.embedding_lookup(embedding, X_cat) \n            return tf.concat([embedded_x, X], axis=1)\n        \n    def score(self, X, y=None):\n        if not self._session:\n            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n        with self._session.as_default() as sess:\n        \n            if self._training is not None:\n                _d = self.get_feed_dict(False, cat_feats_idx, self.cat_placeholders, num_feats_idx, X,y)        \n            else:\n                _d = self.get_feed_dict(None, cat_feats_idx, self.cat_placeholders, num_feats_idx, X,y)        \n        \n            #return self._roc_score.eval(feed_dict=_d)       \n            return (1 - self._loss.eval(feed_dict=_d))\n        \n        ","49bad938":"if val_cnt != 0:\n    (tr_x, tr_y, v_x, v_y, v_u, v_r, v_d, te_x, te_y, te_u, te_d, te_r) =  trainTestSplit(all_x, all_y, market_train[:val_cnt], rs = 42)\nelse:\n    (tr_x, tr_y, v_x, v_y, v_u, v_r, v_d, te_x, te_y, te_u, te_d, te_r) =  trainTestSplit(all_x, all_y, market_train, rs = 42)\n\nprint('tr_x.shape', tr_x.shape)\nprint('tr_y.shape', tr_y.shape)\nprint('te_x.shape', te_x.shape)\nprint('te_y.shape', te_y.shape)\nprint('v_x.shape', v_x.shape)\nprint('v_y.shape', v_y.shape)","4ca286b3":"reset_graph()\ndnn_clf = DNNClassifier(random_state=42, dropout_rate=0.5, batch_norm_momentum=0.94, n_hidden_layers = 3, \n                        n_neurons=34, batch_size = 1000)\ndnn_clf.clearModelFiles()\ndnn_clf.fit(tr_x, tr_y, n_epochs=1000, X_valid=v_x, y_valid=v_y)","fb8568c7":"# calculation of actual metric that is used to calculate final score\npred_te = dnn_clf.predict_proba(te_x)\nte_conf_val =  pred_te * 2 - 1\nprint('error:', getValidationError(te_conf_val,te_r,te_u,te_d))","39c47d25":"display(te_r[:2])\ndisplay(te_conf_val[:2])\ndisplay(pred_te[:2])\ndisplay(te_u[:2])","f391cce1":"def predictValues(Xp):\n   # market_obs_df['assetCode'] = market_obs_df['assetCode'].apply(lambda x: x if x in unic_assetsCode else unic_assetsCode[0])\n    Xp['assetCode'] = Xp['assetCode'].apply(lambda x: x if x in unic_assetsCode else unic_assetsCode[0])\n    _Xp = full_pipeline.transform(Xp)\n    pred_v = dnn_clf.predict_proba(_Xp)\n    pred_inv = pred_v * 2 - 1\n    return pred_inv\n\ndef make_random_predictions(predictions_df, Xp):\n    predictions_df.confidenceValue = predictValues(Xp)","352e7046":"#days = env.get_prediction_days()\n#(market_obs_df, news_obs_df, predictions_template_df) = next(days)","33ddb470":"#make_random_predictions(predictions_template_df, market_obs_df)\n","ec4a8cb5":"#predictions_template_df.head(10)","db43d511":"#scaler.inverse_transform(predictions_template_df.confidenceValue.values.reshape(-1,1))[:5]","6d5897dd":"#scaler.inverse_transform(v_y)[:5]","c03cd545":"#v_y[:5]","6783bfa8":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    market_train = join_market_news(market_obs_df, news_obs_df)\n    make_random_predictions(predictions_template_df, market_train)\n    env.predict(predictions_template_df)\nprint('Done!')","ad89c225":"env.write_submission_file()","9d8fd951":"### TENSORFLOW","d3c9ff16":"### DNNClassifaier ","f53c9f34":"### Loading market data and news","31972f02":"**HELPER FUNCTIONS:**\nHere i implement some simple transformers to use with pipeline and helper functions","abe5d50a":"### Create piplines"}}