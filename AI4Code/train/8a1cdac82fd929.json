{"cell_type":{"1884868f":"code","14ab9b65":"code","3f958a22":"code","57fb6053":"code","cb94590f":"code","3d5b2450":"code","32137377":"code","65f7f284":"code","7df10231":"code","0a2bc664":"code","409803c9":"code","c48f1f6a":"code","836f2c26":"code","ef96f4a9":"code","6fc155b6":"code","73db4837":"code","049acaf1":"code","80847a1e":"code","c03be530":"code","17fa010c":"code","2c0e0530":"code","5107a283":"code","b53719c3":"code","2a45f5c0":"code","f9f184b3":"code","975663b9":"code","b9fbfc24":"code","98bb88b6":"code","b99133ae":"code","84feee7f":"code","eb871292":"code","63f729bc":"code","86e78d9e":"code","8502a341":"code","5874b2fe":"code","4554af3e":"code","6e468e3c":"markdown","483e25d6":"markdown","b97b6517":"markdown","6ca241d2":"markdown","89118d4e":"markdown","cda7aaaa":"markdown","1102d14e":"markdown","c1394cac":"markdown","909ac1ae":"markdown","45a6227f":"markdown","e88f87b6":"markdown","64d7a18a":"markdown","a92bb701":"markdown","7486c7a0":"markdown","fb8c93f6":"markdown","3a2f6ff6":"markdown","ed7dcd85":"markdown","d40345f6":"markdown","c20b525b":"markdown","93cd502b":"markdown","9c2ae787":"markdown","978dae02":"markdown","113ba12e":"markdown","12db4e1b":"markdown","4ccecf60":"markdown","bc778a25":"markdown","ae9e1f27":"markdown","37c9d996":"markdown","b0d66c0d":"markdown"},"source":{"1884868f":"!pip install ethnicolr\n!pip install pandas","14ab9b65":"#%%capture\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom scipy.stats import norm\nimport scipy.stats as st\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    from ordereddict import OrderedDict\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom ethnicolr import census_ln, pred_census_ln\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3f958a22":"df_train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ndisplay(df_train.head())\ndf_train.describe()","57fb6053":"print(df_train.columns)","cb94590f":"cont_FEATURES = ['Age', 'Fare']\n\ncat_FEATURES = ['Pclass', 'Sex']","3d5b2450":"df_train.info()","32137377":"df_train.fillna(0)","65f7f284":"def plot_outliers(df, feature, threshold=5):\n    mean, std = np.mean(df), np.std(df)\n    z_score = np.abs((df-mean) \/ std)\n    good = z_score < threshold\n\n    print(f\"Rejection {(~good).sum()} points\")\n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good\n\ndef plot_lof_outliers(df, feature):\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.001, p=1)\n    good = lof.fit_predict(df) > 0.5 # change this value to set the threshold for outliers\n    print(f\"Rejection {(~good).sum()} points\")\n    \n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good","7df10231":"for feature in cont_FEATURES:\n    print(feature)\n    plot_outliers(df_train[feature], feature)","0a2bc664":"for feature in cont_FEATURES:\n    # There some reshaping done here for syntax sake\n    data = df_train[~df_train[feature].isna()][feature]\n    plot_lof_outliers(data.values.reshape(data.shape[0], -1), feature)","409803c9":"for feature in cont_FEATURES:\n    sns.violinplot(x='Survived', y=feature, data=df_train, inner='quartile');\n    plt.title(feature)\n    plt.show()","c48f1f6a":"for feature in cat_FEATURES:\n    print(feature)\n    sns.histplot(df_train[feature].values)\n    plt.show()","836f2c26":"def plot_cdf(df, feature):\n    ps = 100 * st.norm.cdf(np.linspace(-4, 4, 10)) # The last number in this tuple is the number of percentiles\n    x_p = np.percentile(df, ps)\n\n    xs = np.sort(df)\n    ys = np.linspace(0, 1, len(df))\n\n    plt.plot(xs, ys * 100, label=\"ECDF\")\n    plt.plot(x_p, ps, label=\"Percentiles\", marker=\".\", ms=10)\n    plt.legend()\n    plt.ylabel(\"Percentile\")\n    plt.title(feature)\n    plt.show();\n\nfor feature in cont_FEATURES:\n    plot_cdf(df_train[feature], feature)","ef96f4a9":"# This plots a 16x16 matrix of correlations between all the features and the target\n# Note: I sometimes comment this out because it takes a few minutes to run and doesn't show any useful information.\n\n#pd.plotting.scatter_matrix(df_train, figsize=(10, 10));","6fc155b6":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(df_train.drop(columns=['PassengerId']).corr(), annot=True, cmap='viridis', fmt='0.2f', ax=ax)","73db4837":"def get_class_letter(text):\n    if str(text) != 'nan':\n        return text[0]\n\ndef get_class_number(text):\n    if str(text) != 'nan':\n        return int(text[1:])\n\ndf_train['class_letter'] = df_train['Cabin'].apply(get_class_letter)\ndf_train['class_number'] = df_train['Cabin'].apply(get_class_number)\n","049acaf1":"df_train['class_letter'].value_counts()","80847a1e":"df_train['class_number'].value_counts()","c03be530":"dummies = pd.get_dummies(df_train['Embarked'])\nfor col in dummies.columns:\n    df_train['embarked_' + col] = dummies[col] \n    \ndummies = pd.get_dummies(df_train['class_letter'])\nfor col in dummies.columns:\n    df_train['class_letter_' + col] = dummies[col] ","17fa010c":"dummies = pd.get_dummies(df_train['Sex'])\nfor col in dummies.columns:\n    df_train['sex_' + col] = dummies[col] ","2c0e0530":"ethnicity_features = ['pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'pcthispanic']\n\ndf_train['last_name'] = df_train['Name'].apply(lambda x: x.split(',')[0])\ndf_train = census_ln(df_train, 'last_name')","5107a283":"# Replace the (S) values in the ethnicity columns with a 0\ndf_train[ethnicity_features] = df_train[ethnicity_features].replace('(S)', 0).astype(float)","b53719c3":"for feature in ethnicity_features:\n    sns.violinplot(x='Survived', y=feature, data=df_train, inner='quartile');\n    plt.title(feature)\n    plt.show()","2a45f5c0":"for feature in ['Fare', 'Age', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'pcthispanic']:\n    x = df_train[feature].values.reshape(-1, 1) #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df_train[feature] = pd.DataFrame(x_scaled)","f9f184b3":"print(df_train.columns)\ndf_train.head()\n","975663b9":"model_FEATURES = ['Pclass', 'sex_female', 'sex_male', 'Age', 'SibSp', 'Parch', 'Fare', \n                  'embarked_C', 'embarked_Q', 'embarked_S',\n                  'class_number', 'class_letter_A', 'class_letter_B', 'class_letter_C',\n                  'class_letter_D', 'class_letter_E', 'class_letter_F', 'class_letter_G',\n                  'class_letter_T', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'pcthispanic']\n\nX = df_train[model_FEATURES].fillna(0)\ntarget = df_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.33, random_state=42)","b9fbfc24":"# Logistic Regression\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)","98bb88b6":"print(f'Logistic Regression Score: {clf.score(X_test, y_test)}')","b99133ae":"rf = RandomForestClassifier(max_depth=4, random_state=123)\nrf.fit(X_train, y_train)","84feee7f":"print(f'Random Forest Score: {rf.score(X_test, y_test)}')","eb871292":"df_test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\n\ndf_test['class_letter'] = df_test['Cabin'].apply(get_class_letter)\ndf_test['class_number'] = df_test['Cabin'].apply(get_class_number)\n\ndummies = pd.get_dummies(df_test['Embarked'])\nfor col in dummies.columns:\n    df_test['embarked_' + col] = dummies[col] \n    \ndummies = pd.get_dummies(df_test['class_letter'])\nfor col in dummies.columns:\n    df_test['class_letter_' + col] = dummies[col] \n\ndummies = pd.get_dummies(df_test['Sex'])\nfor col in dummies.columns:\n    df_test['sex_' + col] = dummies[col] \n    \ndf_test['last_name'] = df_test['Name'].apply(lambda x: x.split(',')[0])\ndf_test = census_ln(df_test, 'last_name')    \ndf_test[ethnicity_features] = df_test[ethnicity_features].replace('(S)', 0).astype(float)\n\nfor feature in ['Fare', 'Age', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'pcthispanic']:\n    x = df_test[feature].values.reshape(-1, 1) #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df_test[feature] = pd.DataFrame(x_scaled)\n\nX = df_test[model_FEATURES].fillna(0)","63f729bc":"predictions = rf.predict(X)","86e78d9e":"df_test.head()","8502a341":"df_predictions = pd.DataFrame(data={'PassengerId': df_test['PassengerId'], 'Survived': predictions})","5874b2fe":"df_predictions.head()","4554af3e":"df_predictions.to_csv('predictions.csv', index=False)","6e468e3c":"We can see in the above that there aren't any reasonable outliers picked out. It has marked the high Fare's as outliers but we can see from the graph that this isn't a reasonabe thing to do.","483e25d6":"This shows us that the classes are balanced enough that they won't cause any issues for our models.","b97b6517":"# Submission","6ca241d2":"### Passenger Ethnicity and Nationality","89118d4e":"# Analysing Distributions","cda7aaaa":"From the above we can see that there are null values appearing. #TOOD: Tidy these up","1102d14e":"# Random Forrest\n\nRandom forrests are known for being the some of the best models for performing classification and so it is always good to experiment with these models.","c1394cac":"# Correlation","909ac1ae":"# Outliers","45a6227f":"The above shows us that there was an equal distribution for Survived and Died but the comparison between these features will be interesting to observe in the model training.","e88f87b6":"### Target Outliers","64d7a18a":"### Continuous Variables","a92bb701":"### Preprocess Test Set","7486c7a0":"### Create out Train and Test Sets","fb8c93f6":"# Baseline","3a2f6ff6":"### Invalid Values","ed7dcd85":"# <p style=\"background-color:#F8C1EE; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\"><b>Tabular Playground Series April<\/b> <br><br> EDA \ud83d\udd0d, Outliers, Correlations and Baseline \ud83d\udcc8<\/p>\n\n# <p style=\"background-color:#F8C1EE; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">Please <u>upvote<\/u> if you find this notebook useful or interesting, I really appreciate the encouragement. Thanks!<\/p>","d40345f6":"Here we can see that there is a significant skew in the `Fare` variable where 80% of the fares are below 100.","c20b525b":"### Categorical Variables","93cd502b":"# Empirical CDFs","9c2ae787":"### Continuous Features\n\nNow we want to normalise our continuous variables, since the values of `Fare` (e.g. 400) are obviously much larger than `Age` (e.g. 21).","978dae02":"There are a few outliers here but because they seem to be mixed in with the group I am going to leave them in the dataset. I imagine the `Age` outliers are just due to some ages being `X.5`.","113ba12e":"### Categorical Features\n\nHere we are extracting the letter and number from their class since this likely gives us a good idea of where there cabin was on the boat.","12db4e1b":"# Cleaning the Dataset\n\n","4ccecf60":"Features to create:\n- Passenger nationality (from their name)\n- How good a deal did they get on their cabin?","bc778a25":"# Feature Engineering","ae9e1f27":"### Feature Outliers","37c9d996":"This heatmap shows us that there are some weak correlations between `Survived` and other variables. This could be useful to us, but also shows that there is now *silver bullet* feature that will completely solve our problems.","b0d66c0d":"Here we can see that there are subtle differences in the `Fare` paid and whether someone survived. This could therefore be a very useful feature."}}