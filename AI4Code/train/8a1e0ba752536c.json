{"cell_type":{"6c06cefe":"code","d8b1ba50":"code","7b554315":"code","1b200a19":"code","98ff42cc":"code","0033632c":"code","79272e9f":"code","4c42ab57":"code","18e444e5":"code","17c4a619":"code","6d229a44":"code","93c585da":"code","076973e7":"code","d643b41d":"code","7411ea24":"code","901a774b":"code","dee1d107":"code","4f04c90e":"code","d674f20f":"code","a36aaa7f":"code","6d26b0d7":"code","d1292674":"code","a89676bd":"code","dd4af5e6":"code","84fcbc5a":"code","e428a33d":"code","4b91d8f1":"code","4967fa4c":"code","007bfce6":"code","5e1f97e4":"code","0b70b6d7":"code","73d4ca94":"code","8789bb3d":"code","5909c724":"code","2f563eac":"code","7dc3f91c":"code","eab7fad4":"code","7478e559":"code","17a4ea8a":"code","a6b5adaa":"code","21793def":"code","0ed17e87":"code","7b38955d":"code","259e5157":"code","72468907":"code","aa590edc":"code","7416addb":"code","ffb1baf6":"code","b2087b26":"code","82a6bf9a":"code","506e4d00":"code","0fc917c1":"code","7fff4a36":"code","ba29bce0":"code","78b4887a":"code","bc5ae956":"code","ab49479d":"code","94b98467":"code","41d5ec20":"code","309ae442":"code","83b7c39a":"code","5bf460b5":"code","d34c6945":"code","55c19a33":"code","e3fa37af":"code","81efeefd":"code","63cec5a7":"code","b64fed5c":"code","ef3a0d1d":"code","f46910c3":"code","aec161d1":"code","65e3a037":"code","1e8e2505":"markdown","de403d31":"markdown","37781549":"markdown","62fd280b":"markdown","cba21cce":"markdown","e35c9893":"markdown","f0fa0b2d":"markdown","cca3f831":"markdown","3f39f324":"markdown","ecc11445":"markdown","30e8c44f":"markdown","8738515b":"markdown","db563af9":"markdown"},"source":{"6c06cefe":"from fastai.tabular import *","d8b1ba50":"path = Path('\/kaggle\/input\/covid19-global-forecasting-week-1\/')\npath.ls()","7b554315":"import pandas as pd\nsample_submission = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/submission.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv\")","1b200a19":"len(train)","98ff42cc":"sample_submission.head()","0033632c":"test.head()","79272e9f":"train.head()","4c42ab57":"#merge test set and training set and rename, som columns\nFull_data = pd.merge(test, train, on=['Lat','Long','Date','Country\/Region','Province\/State'])\nFull_data.rename(columns={'Province\/State':'Province'}, inplace=True)\nFull_data.rename(columns={'Country\/Region':'Country'}, inplace=True)\nFull_data.rename(columns={'ConfirmedCases':'Confirmed'}, inplace=True)\nFull_data.head()","18e444e5":"len(Full_data)","17c4a619":"#rename therefor the data columns\ntrain.rename(columns={'Province\/State':'Province'}, inplace=True)\ntrain.rename(columns={'Country\/Region':'Country'}, inplace=True)\ntrain.rename(columns={'ConfirmedCases':'Confirmed'}, inplace=True)","6d229a44":"#and we do the same for test set\ntest.rename(columns={'Province\/State':'Province'}, inplace=True)\ntest.rename(columns={'Country\/Region':'Country'}, inplace=True)","93c585da":"from sklearn.preprocessing import LabelEncoder\n# creating initial dataframe\nbridge_types = ('Lat', 'Date', 'Province', 'Country', 'Long', 'Confirmed',\n       'ForecastId', 'Id')\ncountries = pd.DataFrame(train, columns=['Country'])\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\ntrain['Countries'] = labelencoder.fit_transform(train['Country'])\ntrain['Countries'].head()\n","076973e7":"# train[\"Date\"] = train[\"Date\"].apply(lambda x: x.replace(\"-\",\"\"))\n# train[\"Date\"]  = train[\"Date\"].astype(int)","d643b41d":"train['Date']= pd.to_datetime(train['Date']) \ntest['Date']= pd.to_datetime(test['Date']) ","7411ea24":"train = train.set_index(['Date'])\ntest = test.set_index(['Date'])","901a774b":"def create_time_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    return X","dee1d107":"create_time_features(train).head()\ncreate_time_features(test).head()","4f04c90e":"train.head()","d674f20f":"train.drop(\"date\", axis=1, inplace=True)\ntest.drop(\"date\", axis=1, inplace=True)","a36aaa7f":"#do the same for test set\ntest['Countries'] = labelencoder.fit_transform(test['Country'])\n\n# test[\"Date\"] = test[\"Date\"].apply(lambda x: x.replace(\"-\",\"\"))\n# test[\"Date\"]  = test[\"Date\"].astype(int)","6d26b0d7":"train.head()","d1292674":"train.isnull().sum()","a89676bd":"#drop useless columns for train and test set\ntrain.drop(['Country'], axis=1, inplace=True)\ntrain.drop(['Province'], axis=1, inplace=True)","dd4af5e6":"test.drop(['Country'], axis=1, inplace=True)\ntest.drop(['Province'], axis=1, inplace=True)","84fcbc5a":"#slpit the data set in to from the merge dataframe called Full_data\ntrain_procent=int(((len(Full_data))\/100)*50)\ntest_procent=int(((len(Full_data))\/100)*50)\n\ntrain_df=Full_data.loc[train_procent:]\ntest_df=Full_data.loc[:test_procent]","e428a33d":"len(test_df)","4b91d8f1":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = train_df.copy()\nX_test_full = test_df.copy()\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['Fatalities'], inplace=True)\ny = X.Fatalities              \nX.drop(['Fatalities'], axis=1, inplace=True)\n   \n    \n    # Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n### for cname (every value, one at the time) in dataframe for columns return a value to 'numeric_cols' if the \n### dtype= int64 or float64. \n\n\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","4967fa4c":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBRegressor\n\n\n\nmodel2 = RandomForestClassifier(n_estimators=150, max_depth=4, random_state=1)\nmodel = GradientBoostingClassifier(random_state=1)\nmodel3 = DecisionTreeClassifier(random_state=1)\n#model=SGDClassifier(random_state=1)\n#model=ExtraTreesClassifier(random_state=1)\nmodel = XGBRegressor()\n# Define the models\nmodel_1 = RandomForestClassifier(n_estimators=50, random_state=0)\nmodel_2 = RandomForestClassifier(n_estimators=100, random_state=0)\nmodel_3 = RandomForestClassifier(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_4 = RandomForestClassifier(n_estimators=300, max_depth=6, random_state=1)\n\n\n\nmodel.fit(X_train, y_train)\ny_predictions = model.predict(X_valid)\n\nprint('model accuracy score',model.score(X_valid,y_valid))","007bfce6":"y_test=y_valid\nX_test=X_valid","5e1f97e4":"model2.fit(X_train,y_train)\nprint(f'Model test accuracy: {model2.score(X_test, y_test)*100:.3f}%')\nmodel3.fit(X_train,y_train)\nprint(f'Model test accuracy: {model3.score(X_test, y_test)*100:.3f}%')","0b70b6d7":"model_1.fit(X_train,y_train)\nprint(f'Model test accuracy: {model_1.score(X_test, y_test)*100:.3f}%')\nmodel_2.fit(X_train,y_train)\nprint(f'Model test accuracy: {model_2.score(X_test, y_test)*100:.3f}%')\nmodel_3.fit(X_train,y_train)\nprint(f'Model test accuracy: {model_3.score(X_test, y_test)*100:.3f}%')\nmodel_4.fit(X_train,y_train)\nprint(f'Model test accuracy: {model_4.score(X_test, y_test)*100:.3f}%')","73d4ca94":"from sklearn.tree import DecisionTreeRegressor  \nregressor = DecisionTreeRegressor(random_state = 0) ","8789bb3d":"train.head()","5909c724":"#train part 2, start over for having enought rows for the submussion\nx = train[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]\ny1 = train[['Confirmed']]\ny2 = train[['Fatalities']]\nx_test = test[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]","2f563eac":"x.head()","7dc3f91c":"# import numpy as np\n# y1=np.ravel(y1)\n# y1","eab7fad4":"regressor.fit(x,y1)\npredict_1 = regressor.predict(x_test)\npredict_1 = pd.DataFrame(predict_1)\npredict_1.columns = [\"Confirmed_predict\"]","7478e559":"predict_1.head()","17a4ea8a":"# y2=np.ravel(y2)","a6b5adaa":"regressor.fit(x,y2)\npredict_2 = regressor.predict(x_test)\npredict_2 = pd.DataFrame(predict_2)\npredict_2.columns = [\"Death_prediction\"]\npredict_2.head()","21793def":"Samle_submission = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/submission.csv\")\nSamle_submission.columns\nsubmission = Samle_submission[[\"ForecastId\"]]","0ed17e87":"Final_submission = pd.concat([predict_1,predict_2,submission],axis=1)\nFinal_submission.head()","7b38955d":"Final_submission.columns = ['ConfirmedCases', 'Fatalities', 'ForecastId']\nFinal_submission = Final_submission[['ForecastId','ConfirmedCases', 'Fatalities']]\n\nFinal_submission[\"ConfirmedCases\"] = Final_submission[\"ConfirmedCases\"].astype(int)\nFinal_submission[\"Fatalities\"] = Final_submission[\"Fatalities\"].astype(int)","259e5157":"Final_submission.head()","72468907":"Final_submission.to_csv(\"submission.csv\",index=False)\nprint('Model ready for submission!')","aa590edc":"# procs = [FillMissing, Categorify, Normalize]\n\n# dep_var = 'Fatalities'\n# cat_names = ['Country', 'Province']\n# cont_names = ['Long','Lat', 'ForecastId']\n","7416addb":"# data = (TabularList.from_df(train_df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n#         .random_split_by_pct(0.2, seed=42)\n#         .label_from_df(cols=dep_var)\n#         .add_test(test_df)\n#         .databunch()\n# )","ffb1baf6":"# data.show_batch(rows=10)","b2087b26":"\n# data = (TabularList.from_df(train_df, procs=procs, cont_names=cont_names, cat_names=cat_names)\n#         .split_by_idx(valid_idx=range(int(len(train_df)*0.9),len(train_df)))\n#         .label_from_df(cols=dep_var)\n#         .add_test(TabularList.from_df(test_df, cat_names=cat_names, cont_names=cont_names, procs=procs))\n#         .databunch())\n# print(data.train_ds.cont_names)\n# print(data.train_ds.cat_names)","82a6bf9a":"# WE HAVE TO CHANGE ACC. se below reason and code\n\n# [quote=\"stephenjohnson, post:11, topic:33778\"]\n# targs stands for **t** arget **arg** ument **s** \n# It\u2019s the values that are the truth values (the Y values) that are being compared to your model\u2019s predicted values. \n# The accuracy metric above takes two arguments the input (predicted values) and targs (target values) and calculates the accuracy. \n# The error encountered above was due to the fact that the input had Long values but targs had Float values.\n# [\/quote]\n\n\n# def accuracy_1(input:Tensor, targs:Tensor)->Rank0Tensor:\n# #     \u201cCompute accuracy with targs when input is bs * n_classes.\u201d\n#     targs = targs.view(-1).long()\n#     n = targs.shape[0]\n#     input = input.argmax(dim=-1).view(n,-1)\n#     targs = targs.view(n,-1)\n#     return (input==targs).float().mean()\n\n# # So use metrics=accuracy_1","506e4d00":"# learn = tabular_learner(data, layers=[1000,500],metrics=accuracy,model_dir=\"\/tmp\/model\/\")","0fc917c1":"#test = TabularList.from_df(train.iloc[800:1000].copy(), cat_names=cat_names, cont_names=cont_names)","7fff4a36":"#data = (TabularList.from_df(train, cat_names=cat_names, cont_names=cont_names, procs=procs)\n#                           .split_by_idx(list(range(800,1000)))\n#                           .label_from_df(cols=dep_var)\n#                           .add_test(X_test)\n#                           .databunch())","ba29bce0":"#data.show_batch(rows=2)","78b4887a":"#learn = tabular_learner(data, layers=[200,100], metrics=accuracy)","bc5ae956":"# learn.fit(5, 1e-2)","ab49479d":"# learn.lr_find()\n# learn.recorder.plot()","94b98467":"# learn.unfreeze()","41d5ec20":"# stop- learn.fit_one_cycle(20, slice(1e-3))","309ae442":"#output = pd.DataFrame({'id': sample_submission.id, 'target': y_predictions})","83b7c39a":"# preds, _ = learn.get_preds(ds_type=DatasetType.Test)\n# pred_prob, pred_class = preds.max(1)","5bf460b5":"# submission = pd.DataFrame({'id':sample_submission['id'],'target':pred_class})","d34c6945":"# submission.to_csv('submission-fastai.csv', index=False)","55c19a33":"# submission.id = submission.id.astype(int)","e3fa37af":"# submission.head()","81efeefd":"# submission.to_csv('my_submission.csv', index=False)","63cec5a7":"# sample_submission = pd.read_csv('my_submission.csv')","b64fed5c":"#row = train.iloc[0]","ef3a0d1d":"#X_test.isnull().sum()","f46910c3":"#y_predictions=learn.predict(X_test)","aec161d1":"# X_test['bin_0'].fillna(X_test['bin_0'].median(), inplace = True)\n# X_test['bin_1'].fillna(X_test['bin_1'].median(), inplace = True)\n# # X_test['bin_2'].fillna(X_test['bin_2'].median(), inplace = True)\n# X_test['ord_0'].fillna(X_test['ord_0'].median(), inplace = True)\n# X_test['day'].fillna(X_test['day'].median(), inplace = True)\n# X_test['month'].fillna(X_test['month'].median(), inplace = True)","65e3a037":"\n#output.to_csv('my_submission.csv', index=False)\n#print(\"Your submission was successfully saved!\")","1e8e2505":"# Check data","de403d31":"the above merge an not be used since the data set went from about 17600 to 3400 and the submission samle is about 12000","37781549":"### test set","62fd280b":"# Train part 1\nSome of the above code was to try to merge data from to csv files. but this cant be used for submission, so we just to a few features in the submission section. but it gives a good idea of what model will predict well.","cba21cce":"# Tabular models","e35c9893":"# Submission","f0fa0b2d":"# Import data","cca3f831":"# training models and predictions","3f39f324":"### Handling dates ","ecc11445":"## Below code is for later use, when I get the API block to work!","30e8c44f":"# Label encoding\nlabel encode Date and country","8738515b":"## Inference","db563af9":"# Data prep"}}