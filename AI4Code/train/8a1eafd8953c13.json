{"cell_type":{"3a41732c":"code","0cb606f6":"code","73a084ec":"code","15c5ae74":"code","591f7406":"code","fe4e9559":"code","cfccd1c1":"code","b6f49f6b":"code","8afe72b9":"code","ba9b62ed":"code","107a46b3":"code","87c55538":"code","6bb4bd82":"code","ff6f22fd":"code","21993f84":"code","c6e725d1":"code","27d45710":"code","a7583068":"code","f0063645":"code","2b795bea":"code","13ac1470":"code","153413cc":"code","a6fc111e":"code","3b5bf3ea":"code","b67b1e1e":"code","6452c994":"code","4abbaf80":"code","5a621bf3":"markdown","dd254fc7":"markdown","a778d559":"markdown","bdfaaea8":"markdown","965060a1":"markdown","07b87c8c":"markdown","a923e823":"markdown","447015da":"markdown","fae61f28":"markdown","c5cc506a":"markdown","e0f57488":"markdown","e97dd3b4":"markdown","86fb0c5c":"markdown","9b29885e":"markdown","6fd4b4fa":"markdown","164e1167":"markdown"},"source":{"3a41732c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0cb606f6":"#make appropriate imports \nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import f_regression\nimport math\nimport seaborn as sns\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n","73a084ec":"#read in train and test data and split \ntrain_data = pd.read_csv(\"..\/input\/bank-train.csv\")\ntest_data = pd.read_csv(\"..\/input\/bank-test.csv\")\ntrain_data\nX = train_data.iloc[:,:21]\ny = train_data.iloc[:,21:]\n#only x values, no predicted values so no need to split \nX_test = test_data\n","15c5ae74":"sns.heatmap(X.corr(), cmap=\"coolwarm\")\nprint(\"Checking for correlations among variables\")","591f7406":"train_data[\"Client Subscription\"] = train_data[\"y\"].replace({1: \"subscribed\", 0: \"did not subscribe\"})\nsns.countplot(train_data[\"Client Subscription\"], palette = \"BrBG\")\nplt.title(\"Did the client subscribe?\")\nprint(\"percentage of clients subscribed: {}%\".format(np.round(train_data.y.mean()*100, 2)))","fe4e9559":"sns.countplot(train_data[\"month\"], hue = train_data[\"Client Subscription\"], palette = \"BrBG\")","cfccd1c1":"from sklearn import preprocessing\n#get just the categorical data of train_data \ncat_data=  X.select_dtypes(include=[object])\ncat_data.head(3)\n","b6f49f6b":"#Converting all categorical var to numbers by making each option a column\n#can increase dimensionality a lotttt if lots of variety in categorical options \n#month added 12 more columns for example for each month \n#can also do this with one hot encoding \nprint('Size Before changing to dummy: ' + str(cat_data.shape))\ncat_data = pd.get_dummies(cat_data)\nprint('Size After changing to dummy: ' + str(cat_data.shape))\ncat_data.head(3)","8afe72b9":"#numerical aspects of the data for training dset\n#merge with categorical to make final X train data set\nnum_data= X.select_dtypes(include=[int, float])\nmerged = pd.concat([cat_data,num_data], axis=1)\nX = merged\nprint(X.shape)","ba9b62ed":"#change test data the same way \nnum2 = X_test.select_dtypes(include=[int, float])\ncat_for_test=  X_test.select_dtypes(include=[object])\nX_2_numerical = pd.get_dummies(cat_for_test)\nmerged2 = pd.concat([X_2_numerical,num2], axis=1)\n","107a46b3":"from sklearn.linear_model import LogisticRegression\n#first we do a logistic regression using all variables \nall_features = list(merged2.columns.values)\nx_train = X[all_features]\ny_train = y\n# create and fit model\nLogReg = LogisticRegression(solver='newton-cg')\nLogReg.fit(x_train, y_train)","87c55538":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#make predictions about what our dependent values would be for training set(yes or no)\ny_train_pred = LogReg.predict(x_train)\n#make a confusion matrix to see results of how our model performs \nconfusion_matrix = confusion_matrix(y_train, y_train_pred)\nprint(classification_report(y_train, y_train_pred))","6bb4bd82":"#finally make predictiosn about our unknown test data\nx_test = merged2[all_features]\npredictions = LogReg.predict(x_test)\npredictions","ff6f22fd":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint(\"Precision: \" + str(precision_score(y_train_pred, y_train, average='micro')))\nprint(\"Recall: \" + str(recall_score(y_train_pred, y_train, average='micro')))\nprint(\"F-1 Training Score: \" + str(f1_score(y_train_pred, y_train, average='micro')))\n\nprint(\"F-1 Test Score based on results of submission: \" + str(0.90651) )","21993f84":"from sklearn.datasets import load_digits\nfrom sklearn.feature_selection import SelectKBest,f_classif\n#we take the top 10 best features (using k = 11 because we disregard the first feature duration)\n#we are doing f-test statistic to find each score to determine most relevent feature  \nbestfeatures = SelectKBest(score_func=f_classif, k=11)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(11,'Score'))  #print 11 best features (take 10)\n\n#our top 10 best features we will model with \nbest_features = [\"nr.employed\",\"pdays\",\"poutcome_success\",\"euribor3m\",\"emp.var.rate\",\"id\",\"previous\",\"poutcome_nonexistent\",\"month_mar\",\"contact_cellular\"]\n","c6e725d1":"#Ten Best \nx_train_best10 = X[best_features]\ny_train_best10 = y\n# create and fit model\nLogReg_best10= LogisticRegression(solver='newton-cg')\nLogReg_best10.fit(x_train_best10, y_train_best10)\n","27d45710":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#make predictions about what our dependent values would be for training set(yes or no)\ny_train_pred10 = LogReg_best10.predict(x_train_best10)\n#make a confusion matrix to see results of how our model performs \nconfusion_matrix = confusion_matrix(y_train, y_train_pred10)\nprint(classification_report(y_train, y_train_pred10))","a7583068":"x_test_best = merged2[best_features]\npredictions_best10 = LogReg_best10.predict(x_test_best)\npredictions_best10","f0063645":"print(\"Precision: \" + str(precision_score(y_train_pred10, y_train, average='micro')))\nprint(\"Recall: \" + str(recall_score(y_train_pred10, y_train, average='micro')))\nprint(\"F-1 Training Score: \" + str(f1_score(y_train_pred10, y_train, average='micro')))\n\nprint(\"F-1 Test Score based on results of submission: \" + str(0.88547) )\n","2b795bea":"#using decison tree\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets","13ac1470":"# model\ntree = DecisionTreeClassifier()\n# train\ntree.fit(x_train, y_train)\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#make predictions about what our dependent values would be for training set(yes or no)\ny_train_pred_dtree = tree.predict(x_train)\n#make a confusion matrix to see results of how our model performs \nconfusion_matrix = confusion_matrix(y_train, y_train_pred_dtree)\nprint(classification_report(y_train, y_train_pred_dtree))\n\n","153413cc":"predictions_tree = tree.predict(x_test)\n#Score returns the mean accuracy on the given test data and labels\nprint(tree.score(x_train, y_train))\npredictions_dtree = tree.predict(x_test)\npredictions_dtree","a6fc111e":"print(\"Precision: \" + str(precision_score(y_train_pred_dtree, y_train, average='micro')))\nprint(\"Recall: \" + str(recall_score(y_train_pred_dtree, y_train, average='micro')))\nprint(\"F-1 Training Score: \" + str(f1_score(y_train_pred_dtree, y_train, average='micro')))\n\nprint(\"F-1 Test Score based on results of submission: \" + str(0.88668) )\n","3b5bf3ea":"from sklearn.svm import SVC\nfrom sklearn.linear_model import  LogisticRegression\n#classifier = SVC(kernel=\"linear\")\n#classifier.fit(x_train_best10,np.ravel(y_train_best10))","b67b1e1e":"# model\nforest = RandomForestClassifier(criterion = 'entropy', random_state = 42)\n\n# train\nforest.fit(x_train, y_train)\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#make predictions about what our dependent values would be for training set(yes or no)\ny_train_pred_RFtree = forest.predict(x_train)\n#make a confusion matrix to see results of how our model performs \nconfusion_matrix = confusion_matrix(y_train, y_train_pred_RFtree)\nprint(classification_report(y_train, y_train_pred_RFtree))","6452c994":"print(\"Precision: \" + str(precision_score(y_train_pred_RFtree, y_train, average='micro')))\nprint(\"Recall: \" + str(recall_score(y_train_pred_RFtree, y_train, average='micro')))\nprint(\"F-1 Training Score: \" + str(f1_score(y_train_pred_RFtree, y_train, average='micro')))\n\nprint(\"F-1 Test Score based on results of submission: \" + str(0.8712) )\n","4abbaf80":"#can use predictions_best10, predictions_dtree, ect.\nsubmission = pd.concat([merged2.id, pd.Series(predictions)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv(\"submission.csv\")\n#files.download(\"submission.csv\")","5a621bf3":"## Decison Tree","dd254fc7":"## Data Exploration##","a778d559":"## Class imbalance","bdfaaea8":"Now we want to try to do some feature selection and see if there are certain features which may be more useful for making our model as sometimes adding too many features can cause overfitting. \n","965060a1":"This is probably due to overfitting","07b87c8c":"## Random Forest","a923e823":"## Predicting Bank Telemarketing outcomes ##","447015da":"## Preprocessing data ##","fae61f28":"## Logistic Regression with all features ##\n\nWe hypothesize that this will result in a low F score as the model is probably overft\n","c5cc506a":"## Using Feature Selection##\n\nWe Expect to get a better metric as we are only using the best features to avoid bias ","e0f57488":"## Using Support Vector Machines (Unfininshed, wont run in reasonable time)","e97dd3b4":"## Submission ##","86fb0c5c":"We do the same preprocessing to the Test data ","9b29885e":"We use a heatmap to check for correlation among variables, and remove variables that are highly correlated when performing logistic regression.\n<br> We would theoretically want to drop euribo3m & emp.var.rate due to high correlation (violates assumption for log), but we dont.","6fd4b4fa":"### Reading in Data ##\n\nWe are using data collected from a Portuguese retail bank, from May 2008 to June 2013, in a total of 52,944 phone contacts.This data is collected from the agents executing phone calls to a list of clients to sell the deposit. Thus,the result is a binary unsuccessful or successful contact of selling deposits.We have many variables which are taken into account (parameters)","164e1167":"Having so many categorical variables, we dont want to just ignore them. ML algorithms cant typically take in these values, so we convert to numerical values using pandas dummy varaible method or one hot encoding from sklearn"}}