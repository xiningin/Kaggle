{"cell_type":{"a9d6f074":"code","3bffca3d":"code","ba1e6431":"code","81f0cf25":"code","d9b128c8":"code","990ce6a4":"code","050b0914":"code","d03dc152":"code","93d06c3f":"code","d4bc57db":"code","7172a85e":"code","e16eaca9":"code","75c7591d":"code","dd2d05b9":"code","534c2197":"code","55391560":"code","4a8713e2":"code","8310b083":"code","5caa9110":"code","c868705f":"code","aa48fdff":"code","24eeee4c":"code","3fc75c9b":"code","35f119c5":"code","a3e69da9":"code","1224c504":"code","4d900e15":"code","5fcfae78":"code","6fd25271":"code","e2dc51f0":"code","aa751403":"code","b03d6d72":"code","d8702f83":"code","54f27b7d":"code","8f3d6ce0":"code","ef8fdc38":"code","b90768f6":"code","fe0a5705":"code","1a11d538":"code","7ee44bc5":"code","5f879f74":"code","9ffb56b0":"code","e6a3bf9f":"code","0e56dfa4":"code","287ecdf3":"code","f817985c":"code","d1832b2b":"code","523c9b4d":"code","f9c68bbf":"code","ae1ab2f6":"code","5d4a56b0":"code","4223a369":"code","0cb22e99":"code","ebf46736":"code","11843f7b":"code","e33423ce":"code","1b14e96b":"code","29c7a950":"code","75760b23":"code","a796f486":"code","64281650":"code","b927791b":"code","981850b9":"code","4e504662":"code","6664ceea":"code","7ffcd2a5":"code","29481abb":"code","39945228":"code","1e7171fe":"code","ea697b7c":"code","ac97db63":"code","6453cc68":"code","c71c1822":"code","73456773":"code","43fc18b3":"code","a335fc0c":"code","2ea305f7":"code","1ee7ed0a":"code","7f3c1794":"code","2cb840e9":"code","937c79a3":"code","e4d57a9e":"code","695aaf57":"code","50546a8f":"code","5555de53":"code","9de72bbe":"code","d03f0d38":"code","a7ac1e85":"code","d2e32dd6":"code","b9e7a392":"code","0a97d52a":"code","6b273297":"code","44e35336":"code","44b1a7e4":"code","5d02694b":"markdown","bf79e4fb":"markdown","9d54bab4":"markdown","e870e7de":"markdown","63b1109f":"markdown","d0dd943e":"markdown","e34a6329":"markdown","4699fe78":"markdown","fc5595b1":"markdown","ea9b6ebb":"markdown","dd66151c":"markdown","fc953ebe":"markdown","fe02609b":"markdown","7d107dbd":"markdown","ce9b3f53":"markdown","d0713f45":"markdown","778e46cb":"markdown","08e1f0bf":"markdown","f05a03ff":"markdown","c15b6071":"markdown","29544acc":"markdown","229a1735":"markdown","007ac59b":"markdown","cffddd19":"markdown","d6a7649d":"markdown","35416ad7":"markdown","7b5b85ef":"markdown","87ea76a4":"markdown","a52ee207":"markdown","4f6be765":"markdown","d29c5d53":"markdown","665541b9":"markdown","d3aded35":"markdown","09f8f527":"markdown","6cc0e9b3":"markdown","17093dfd":"markdown","b911f0d6":"markdown","f8ac0b33":"markdown"},"source":{"a9d6f074":"from IPython.display import Image\nimport os\nImage(\"..\/input\/approach1\/approach1.png\")","3bffca3d":"# Packages installed\n\n!pip install xmltodict #Used for transforming xml to dictionary","ba1e6431":"import xmltodict\nimport json \nimport pandas as pd\nimport numpy as np \nimport re\nimport string\nimport random\nimport time\nfrom datetime import date ## date fields transformstions\nfrom dateutil.relativedelta import relativedelta\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n\n#data visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cv2\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM\n","81f0cf25":"# Input paths\nglove_50d_pretrained_dic_path = '\/kaggle\/input\/glove6b50d\/glove.6B.50d.txt'\ninput_dir_path = '\/kaggle\/input\/stackoverflowmeta'\nusers_data_file_path = os.path.join(input_dir_path, 'Users.xml')","d9b128c8":"## Function to extract data from xml and save it to a dataframe\ndef xml_to_df(xml_path,root_tag_name, row_tag_name):\n    \n    with open(xml_path) as xml_file:\n        data_dict = xmltodict.parse(xml_file.read())\n    json_data = json.dumps(data_dict)\n    json_row_data = json.loads(json_data)\n\n    return pd.DataFrame(json_row_data[root_tag_name][row_tag_name])\n    \ndata_df = xml_to_df(users_data_file_path, 'users', 'row')\ndata_df.head(5)","990ce6a4":"data_df.info()","050b0914":"## Function to convert datatypes for the list of columns in a given dataframe\ndef convert_dtype(df,col_list,data_type):\n    for col in col_list:\n        df[col] = df[col].astype(data_type)\n    return None\n\n# Rename the columns by replacing the special charater @ with empty string\ncolumn_names = [col.replace('@','') for col in data_df.columns]\ndata_df.columns = column_names\n\n# Identifying list of different data types columns list\nnumeric_cols = ['Reputation', 'Views', 'UpVotes', 'DownVotes' ]\n\n# Apply Date datatype conversion\nconvert_dtype(data_df, ['CreationDate','LastAccessDate'], 'datetime64[ns]')\n\n# Apply Numeric datatypes conversion\nconvert_dtype(data_df, numeric_cols, 'int')","d03dc152":"# Identify & Drop the Duplicated rows if any\nduplicate = data_df[data_df.duplicated()] \nprint(f'Duplicated data frame length : {len(duplicate)} (Zero means there are no duplicated records)')","93d06c3f":"## Function to create bar plot and display the values on the bars  \ndef plot_bar(labels, values, fig_title, x_axis_name, fig_size, rotation, title):\n    plt.figure(figsize=fig_size)\n    plt.xticks(rotation=rotation)\n    #plt.axis('off')\n    ax = sns.barplot(x=labels, y=values)\n    ax.tick_params(labelsize=12)\n    ax.set_yticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.set_title(fig_title, fontsize=20, color=\"purple\")\n    ax.set_xlabel(x_axis_name, fontsize=12)\n    for p in ax.patches:\n      ax.annotate(\"%.f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n          ha='center', va='center', fontsize=14, color='purple', xytext=(0, 8),\n          textcoords='offset points')\n    plt.tight_layout()\n    plt.savefig(f'{title}.png')","d4bc57db":"# Total number of unique observations for each featue\nuniq_cnt = []\nfor col in data_df.columns:\n    uniq_cnt.append(len(data_df[col].unique()))\n\nlabels = data_df.columns\nplot_bar(labels, uniq_cnt, '#Uniqueness with respect to feature ', '', [15,6], 90, 'features_uniqueness')","7172a85e":"# Lets visually see how much percentage of missing values exists\n\n## Function to plot bar chart for missing fields with respect to total records\ndef plot_missing_perc(df, fig_title, fig_size):\n    missing_data_sum = df.isnull().sum()\n    missing_data = pd.DataFrame({'total_missing_values': missing_data_sum,'(%)': (missing_data_sum\/df.shape[0])*100})\n    plt.figure(figsize=fig_size)\n    plt.title(fig_title, fontsize=20, color=\"purple\")\n    chart = sns.barplot(x =missing_data['(%)'] , y = df.columns, orientation='horizontal')\n    plt.savefig(f'missing_data_%.png')\n    return missing_data\nmissing_data = plot_missing_perc(data_df, \"Missing Data interms of percentages with respect to total number of observations\", [12,6])","e16eaca9":"# Lets look into another way with respect to missing fields instead of total number of observations\nmissing_data = missing_data[missing_data['(%)'] > 0]\nfig = go.Figure([go.Pie(labels=missing_data.index, values=missing_data['(%)'])])\nfig.update_traces(hole=.6, hoverinfo=\"label+percent\")\n\nfig.update_layout(title_text=\"Donut Pie Chart for Missing Data with respect to missing fields percentages(%)\",\n                  annotations=[dict(text='Missing field data %', x=0.5, y=0.5, \n                                    font_size=18, showarrow=False)])\nfig.show()","75c7591d":"# Take a look into url content\ndata_df.WebsiteUrl.unique()[0:30]","dd2d05b9":"# Check the unique url counts\ndata_df.WebsiteUrl.value_counts()","534c2197":"data_df.Location.unique()[0:30]","55391560":"## Function to extract the Country name from Location content\ndef get_country(location):\n    if ',' in location:\n        loc_name = location.split(',')[1].strip()\n        if loc_name == 'Karnataka India':\n            loc_name = 'India'\n            \n        if len(loc_name) == 2: \n            loc_name = location.split(',')[0].strip()\n            \n        return loc_name\n    \n    return location","4a8713e2":"# copying the original data frame into new dataframe to apply EDA on location field\ndata_loc_df = data_df.copy()\n\n# Adding Country column to location data frame\ndata_loc_df['Country'] = data_loc_df['Location'].apply(lambda x : get_country(str(x)))\n\ndata_loc_df = data_loc_df[data_loc_df['Country'] != 'nan']\nlen(data_loc_df)","8310b083":"temp = data_loc_df.Country.value_counts(ascending = False).head(3)\ndata = go.Bar(\n    y = temp.values,\n    x = temp.index,\n    marker_color=px.colors.qualitative.D3[2],\n    marker = dict(\n        colorscale = 'Greens',\n    )\n    \n)\nlayout = dict(\n    title = 'Top 3 Locations ',\n\n    height = 300,\n    margin = dict(\n    )\n)\nfig = go.Figure(data = data, layout = layout)\nfig.show();","5caa9110":"## Cleansing text field \ndef text_cleansing(text):\n\n    # Convert text content to lower case \n    text = text.lower()\n    text = text.replace('\\n','').replace('[','').replace(']','')\n\n    # Removal of URLs\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n\n    # Removal of Hash Tags\n    html_pattern = re.compile('<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    # Removal of Punctuations\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Removal of stop words\n    cached_stop_words = stopwords.words(\"english\") \n    text = ' '.join([word for word in text.split() if word not in cached_stop_words])\n\n    # Replacing unwanted text\n    text = text.replace('yeah yeah yeah','')\n\n    return text\n                          \n## Function to apply basic cleansing on the features\ndef apply_data_cleansing(df):\n    \n    # Drop the first row since it is created by bot\n    df = df.drop(df[df[\"Id\"] == '-1'].index)\n\n    # Drop the unused features \n    df = df.drop(['AccountId', 'DisplayName', 'WebsiteUrl', 'Location', 'ProfileImageUrl' ], axis=1)\n\n    # Drop the null values (this will get rid of Memory errors for the feature 'AboutMe')\n    df = df.drop(df[df['AboutMe'].isna() == True].index)\n\n    # Appy cleansing on text feature\n    df['AboutMe'] = df['AboutMe'].apply(lambda x: text_cleansing(x))\n                           \n    # New feature to get the total number of words in text feature content\n    df[\"NumWords\"] = df[\"AboutMe\"].apply(lambda x: len(str(x).split()))\n    \n    # Drop the rows whose total words are Zero \n    df = df.drop(df[df[\"NumWords\"] == 0].index)\n\n    # Also drop the words whose length is 1 \n    df = df.drop(df[df[\"NumWords\"] == 1].index)\n    \n    return df","c868705f":"# Apply data cleansing\ntext_df = data_df.copy()\ntext_df = apply_data_cleansing(text_df)\nprint(f'after cleaning the text (AboutMe feature) - total records will be  {text_df.AboutMe.shape} out of {data_df.AboutMe.shape}');\n\ntext_df = text_df.reset_index()\ntext_df.tail(2);","aa48fdff":"text_df.isna().sum()","24eeee4c":"# Adding new column to understand the text length frequency\ntext_df[\"TextLength\"]= text_df.AboutMe.astype('str').apply(lambda x : len(x))\n\n## Function to plot histogram on text data\ndef hist_plotly(df):\n\n    fig_title = 'Frequency of AboutMe feature text length'\n    xaxis_name = 'AboutMe Text Length'\n\n    plt2 = go.Histogram(x =df['TextLength'] , marker_color=px.colors.qualitative.D3[2])\n    lyt2 = go.Layout(title=fig_title, xaxis=dict(title=xaxis_name, range=[0,100]), yaxis=dict(title='Frequency'))\n    fig2 = go.Figure(data=[plt2], layout=lyt2, layout_yaxis_range=[0, 30000])\n    iplot(fig2)\n    return None\nhist_plotly(text_df)","3fc75c9b":"## Function to create a black image for the given name\ndef stack_image_creation(s):\n    cnt = len(s)\n    img = np.zeros((612,cnt*420,3), np.uint8)\n    \n    # Write some Text\n    font = cv2.FONT_HERSHEY_TRIPLEX\n    cv2.putText(img,s,(0,500), font, 18,(255,255,255),85)\n    \n    img = 255-img\n    #Save image\n    cv2.imwrite(\"stack.jpg\", img)\n    return img\n","35f119c5":"# Lets apply word cloud on first 5k content of AboutMe feature\nword_cloud_text_5k = ' '.join([str(item) for item in text_df.AboutMe[0:5000] ])\nstack_image_creation('STACK')\nimg = Image.open(\"stack.jpg\")\nhcmask = np.array(img)\nwordcloud = WordCloud(background_color=\"white\",\n                        mask=hcmask, random_state=75,  max_font_size=80).generate(word_cloud_text_5k)\n\n# plot the diagram on wordcloud text\nplt.figure(figsize=(30.0,60))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig(f'word+cloud_on_first_5k_rows.png')\nplt.show()","a3e69da9":"df = data_df.copy()\ndf.describe()","1224c504":"def add_features(df):\n    # Months duration between createdate and lastaccesssdate\n    df['MonthsDuration'] = ((df.LastAccessDate - df.CreationDate)\/np.timedelta64(1, 'M')).astype(int)\n\n    # Derive years & months from date features\n    df['CreationDateYear'] = df['CreationDate'].dt.year\n    df['LastAccessDateYear'] = df['LastAccessDate'].dt.year\n\n    df['CreationDateMonth'] = df['CreationDate'].dt.month\n    df['LastAccessDateMonth'] = df['LastAccessDate'].dt.month\n\n    df['LastAccessYearMonth'] = df['LastAccessDate'].dt.date.astype('str').apply(lambda x : x[0:7])\n    df['CreationYearMonth'] = df['CreationDate'].dt.date.astype('str').apply(lambda x : x[0:7])\n    return df\n","4d900e15":"df = add_features(df)\ndf.tail(2)","5fcfae78":"## Function to create yearly map\ndef year_map(year_feature, user_ids, title):\n    year_df = df.groupby([year_feature])[user_ids].count().to_frame()\n\n    trace = go.Scatter(\n        y= year_df[user_ids], x= year_df.index,\n        mode= 'lines+markers', line=dict(width=6), line_color='Green'\n    )\n\n    layout = go.Layout(autosize=True, title= title,xaxis_title=\"Year\", yaxis_title=\"Number of Users \",showlegend=False)\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    return None","6fd25271":"# Check how many user profiles were being created by year\nyear_map('CreationDateYear', 'Id', 'Number Of User Profiles Created Per Year')","e2dc51f0":"# Check how many user profiles were being accessed by yearly\nyear_map('LastAccessDateYear', 'Id', 'Number Of User Profiles Accessed Per Year')","aa751403":"## Function to get previous n months of data into a dataframe for given date field\ndef get_previous_months_df(_date, num_months):\n    previous_n_months = date.today() - relativedelta(months=+num_months)\n    previous_months_df = df[df[_date].dt.date >= previous_n_months]\n    return previous_months_df","b03d6d72":"# How many user profiles were accessed from the previous n number of months?\nnum_months = 24\nprevious_months_last_acess_df = get_previous_months_df('LastAccessDate', num_months)\n\nprevious_months_last_acess_data = previous_months_last_acess_df['LastAccessYearMonth'].value_counts()\nplot_bar(previous_months_last_acess_data.index, previous_months_last_acess_data.values,\n         f'Number of users accessing their profiles from previous {num_months} months ', '', [20,6], 60,\n        'profiles_access_from_last_24months')","d8702f83":"# How many user profiles were created recentely from the previous n number of months? \nnum_months = 24\nprevious_months_creation_df = get_previous_months_df('CreationDate', num_months)\n\nprevious_months_creation_data = previous_months_creation_df['LastAccessYearMonth'].value_counts()\nplot_bar(previous_months_creation_data.index, previous_months_creation_data.values, \n         f'Number of user profiles were created from previous {num_months} months ', '', [20,6], 60,\n        'profiles_creation_from_last_24months')","54f27b7d":"# Identifying list of different data types columns list\nadditional_numeric_cols = ['CreationDateYear', 'CreationDateMonth', 'LastAccessDateYear', 'LastAccessDateMonth']\nadditional_string_cols = ['LastAccessYearMonth', 'CreationYearMonth']\n\n# convert data types to numeric, string and date types\n\nconvert_dtype(df, additional_numeric_cols, 'int')\nconvert_dtype(df, additional_string_cols, 'str')","8f3d6ce0":"# from pandas.api.types import is_numeric_dtype\n\nnumeric_dtype_list = []\nfor col in df.columns:\n    if pd.api.types.is_numeric_dtype(df[col]):\n        numeric_dtype_list.append(col)\nnumeric_dtype_list","ef8fdc38":"# Function to plot regression graphs\ndef plot_reg(df, indipendent_features):\n  plt.figure(figsize=(25, 70))\n  for loc, feature in enumerate(indipendent_features):\n    ax = plt.subplot(5, 2, loc+1)\n    ax.set_title(f'Target Feature vs {feature}')\n    sns.regplot(x=df[feature], y=df['MonthsDuration'], color='green');\n    plt.savefig('reg_plots.png')\n  return None","b90768f6":"plot_reg(df, numeric_dtype_list)","fe0a5705":"# Look into the Highest observations for each feature\nbase_lst = [[df['Views'].max(), df['UpVotes'].max(), \n      df['DownVotes'].max(), df['Reputation'].max(),\n      df['CreationDate'].max(), df['LastAccessDate'].max()]] \n    \nbase_stats = pd.DataFrame(base_lst, columns =['Views', 'UpVotes','DownVotes', 'Reputation', 'CreationDate', 'LastAccessDate']) \nbase_stats","1a11d538":"fig = px.scatter(df, x=\"Reputation\", y=\"Views\", animation_frame=\"CreationYearMonth\", \n           size=\"UpVotes\", color=\"LastAccessDateYear\",\n           log_x=True, size_max=200, range_x=[1,base_stats.Reputation[0]], range_y=[0,2000], \n                 title='Range Slider Graph to see Views, UpVotes, Reputation by user profile creation date ')\n\nfig[\"layout\"].pop(\"updatemenus\")\nfig.show()","7ee44bc5":"plt.figure(figsize=[15,12])\nplt.title('Correlation after generation of Date features', fontsize=20, color=\"purple\")\nsns.heatmap(df.corr(), annot=True, cmap='Greens');","5f879f74":"text_df = add_features(text_df)\ntext_df.tail(2)","9ffb56b0":"## Function to apply pretrained GloVe word embedding dictionary\ndef apply_glove_embedding(df):\n\n    def get_unkwn_embedding_from_sentence(sentence):\n        words=[word.lower() for word in sentence.split()]\n        unknown_embedding = []\n        for word in words:\n            emb_vec=embedding_dict.get(word)\n            if emb_vec is  None:\n                unknown_embedding.append(word)\n\n        return unknown_embedding\n    \n    def get_mean_vector_from_sentence(sentence):\n        words=[word.lower() for word in sentence.split()]\n        sent_embedding = []\n        for word in words:\n            emb_vec=embedding_dict.get(word)\n            if emb_vec is not None:\n                sent_embedding.append(emb_vec)\n\n        vec = np.array(sent_embedding)\n        result_vector = np.mean(vec, axis=0)\n        return result_vector    \n    \n    embedding_dict={}\n    with open(glove_50d_pretrained_dic_path,'r') as f:\n        for line in f:\n            values=line.split()\n            word=values[0]\n            vectors=np.asarray(values[1:],'float32')\n            embedding_dict[word]=vectors\n    f.close()\n    \n    df['Text2MeanVec'] = df.AboutMe.apply(lambda x : get_mean_vector_from_sentence(x));\n    df['UnkwnEmbedding'] = df.AboutMe.apply(lambda x : get_unkwn_embedding_from_sentence(x))\n    df['NumUnkwnEmb'] = df[\"UnkwnEmbedding\"].apply(lambda x: len(x))\n    df['AppliedEmbedding'] = df['NumWords'] - df['NumUnkwnEmb']\n    \n    \n    plt.figure(figsize=[25,10]);\n    plt.plot(df.NumWords, \"-r\", label=\"TotalNumWords\")\n    plt.plot(df.AppliedEmbedding, \"-w\", label=\"AppliedEmbedding\")\n    plt.legend(loc=\"upper left\")\n    plt.title('Below plot shows how far the applied embeddings w.r.to total words',fontsize=14)\n    plt.show();\n    \n    return df\n    \n    ","e6a3bf9f":"glove_text_df = text_df.copy()\nglove_text_df =  apply_glove_embedding(glove_text_df);","0e56dfa4":"glove_text_df.tail(4)","287ecdf3":"glove_text_df.info()","f817985c":"## Function to split mean vectors into features\ndef get_text_vec_as_features(df, n_dim):\n\n    # filter with non na vectors\n    df = df[df['Text2MeanVec'].notna()]\n\n    t_vec_nd = np.array(df['Text2MeanVec'])\n    v_stack_vec = np.vstack(t_vec_nd)\n    v_stack_vec_df = pd.DataFrame(v_stack_vec )\n    v_stack_vec_df.columns = [f'v_{i}' for i in range(0,n_dim)]\n\n    m_df = pd.concat([v_stack_vec_df, df], axis=1, join=\"inner\")\n\n    # dropped unwanted cols \n    m_df = m_df.drop(['CreationDateMonth', 'LastAccessDateMonth','LastAccessYearMonth', 'CreationYearMonth',  'TextLength', 'NumWords', 'Text2MeanVec','UnkwnEmbedding', 'AppliedEmbedding','AboutMe','index', 'Id', 'NumUnkwnEmb'], axis = 1) \n    \n    return m_df\nm_df = get_text_vec_as_features(glove_text_df, 50)","d1832b2b":"m_df.head()","523c9b4d":"m_df.shape","f9c68bbf":"# function to calculate & apply z-score Normalization\ndef apply_zscore(df , feature):\n    _mean = df[feature].mean()\n    _std = df[feature].std()\n    df[feature] = (df[feature] - _mean) \/ _std\n    return df[feature]\n\nstd_md_df = m_df.copy()\nstd_md_df[['CreationDateYear']] = apply_zscore(std_md_df, 'CreationDateYear')\nstd_md_df[['Reputation']] = apply_zscore(std_md_df, 'Reputation')\nstd_md_df[['Views']] = apply_zscore(std_md_df, 'Views')\nstd_md_df[['UpVotes']] = apply_zscore(std_md_df, 'UpVotes')\nstd_md_df[['DownVotes']] = apply_zscore(std_md_df, 'DownVotes')\nstd_md_df[['LastAccessDateYear']] = apply_zscore(std_md_df, 'LastAccessDateYear')\nstd_md_df.tail()","ae1ab2f6":"# Applying  MinMax scalling technique to independent features\nmms = MinMaxScaler()\nmin_max_md_df = m_df.copy()\nmin_max_md_df[['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDateYear', 'LastAccessDateYear' ]] = mms.fit_transform(min_max_md_df[['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDateYear', 'LastAccessDateYear']])\nmin_max_md_df.tail()","5d4a56b0":"## Function to split the data into train & test datasets\ndef split_features(df):\n    # independent features\n    X = df.drop(['MonthsDuration','CreationDate', 'LastAccessDate'], axis = 1) \n\n    # dependent feature\n    y = df['MonthsDuration']\n    \n    \n    # random state (seed value - it could be any integer) used to train the same order of the data points every time\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n    print(\"X Train Shape: \",X_train.shape)\n    print(\"X Test Shape: \",X_test.shape)\n    print(\"y Train Shape: \",y_train.shape)\n    print(\"y Test Shape: \",y_test.shape)\n\n    \n    # reshaping X\n    X = np.array(X_train).reshape(X_train.shape[0],X_train.shape[1],1)\n    print(f'after reshape of X : {X.shape}')\n    \n    y = y_train\n    print(f'y shape : {y.shape}')\n          \n    return X, y, X_test, y_test","4223a369":"# split the data\nprint('z-score normalization applied train & test split')\nX_z_score, y_z_score, X_test_z_score, y_test_z_score = split_features(std_md_df)\nprint('\\nmin max normalization applied train & test split')\nX_mms, y_mms, X_test_mms, y_test_mms = split_features(min_max_md_df)","0cb22e99":"## Function to define model with tunable parameters\ndef stack_model(X, y, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM):\n       \n    # Defined the model layers\n    model_lstm = Sequential()\n    \n    # First LSTM layer \n    model_lstm.add(LSTM(units=50, input_shape=( X.shape[1],1 ), return_sequences=True, **LSTM_PARAM_L1))\n    \n    # Second LSTM layer\n    model_lstm.add(LSTM(units=50, return_sequences=True))\n    \n    # Third LSTM layer\n    model_lstm.add(LSTM(units=50))\n \n    # The output layer\n    model_lstm.add(Dense(units=1))\n    \n    # Compiling the RNN\n    model_lstm.compile(**LSTM_COMPILE_PARAM)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=30, shuffle=False)\n    # del X,y; gc.collect()\n    print(\"X Train Shape: \",X_train.shape)\n    print(\"X Valid Shape: \",X_valid.shape)\n    print(\"y Train Shape: \",y_train.shape)\n    print(\"y Valid Shape: \",y_valid.shape)\n\n    callbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=1,mode='auto')]\n    hist = model_lstm.fit(X_train, y_train,\n                          validation_data=(X_valid, y_valid),\n                          callbacks=callbacks_list,\n                          **LSTM_FIT_PARAM)\n\n    # Model Evaluation Parameters\n    best = np.argmin(hist.history[\"val_loss\"])\n    print(\"Optimal Epoch: {}\",best)\n    print(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))\n    \n    return model_lstm, best, hist.history[\"loss\"][best], hist.history[\"val_loss\"][best]","ebf46736":"## Function to merge all the given dictionaries into one dictionary\ndef merge_lstm_dict(dict1, dict2, dict3):\n    return {**dict1, **dict2, **dict3}","11843f7b":"print('Model Results when applying below parameters on z-score normalised data with Adam Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp1, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)","e33423ce":"# Appending tunnable parameters results into a dataframe\nLSTM_TUNING_PARAMS['test_loss'] = test_loss\nLSTM_TUNING_PARAMS['val_loss'] = val_loss\nLSTM_TUNING_PARAMS['optimal_epoch'] = optimal_epoch+1\nLSTM_TUNING_PARAMS['normalization'] = 'z-score'\nhp_results_df = pd.DataFrame([LSTM_TUNING_PARAMS], index=['Exp1'])","1b14e96b":"hp_results_df","29c7a950":"## function to update the hyper parameter values\ndef update_hyper_params_in_df(hp_results_df, normalization,optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS):\n    LSTM_TUNING_PARAMS['test_loss'] = test_loss\n    LSTM_TUNING_PARAMS['val_loss'] = val_loss\n    LSTM_TUNING_PARAMS['optimal_epoch'] = optimal_epoch+1\n    LSTM_TUNING_PARAMS['normalization'] = normalization\n    hp_results_df = hp_results_df.append(LSTM_TUNING_PARAMS, ignore_index=True)\n    return hp_results_df\n","75760b23":"print('Model Results when applying below parameters on min-max normalised data with Adam Optimizer');\n\ntrained_model_exp1_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS)","a796f486":"hp_results_df\n","64281650":"print('Model Results when applying below parameters on z-score normalised data with RMSprop Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'RMSprop',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp2, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on Min-Max normalised data with RMSprop Optimizer');\ntrained_model_exp2_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS) ","b927791b":"hp_results_df","981850b9":"print('Model Results when applying below parameters on z-score normalised data with SGD Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'SGD',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp3, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with SGD Optimizer');\ntrained_model_exp3_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","4e504662":"hp_results_df\n","6664ceea":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with Adam optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp4, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with Adam optimizer');\ntrained_model_exp4_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","7ffcd2a5":"hp_results_df","29481abb":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with RMSProp optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'RMSprop',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp5, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with RMSProp optimizer');\ntrained_model_exp5_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","39945228":"hp_results_df","1e7171fe":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with SGD optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'SGD',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\n\ntrained_model_exp6, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with SGD optimizer');\ntrained_model_exp6_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","ea697b7c":"hp_results_df","ac97db63":"print('Model Results when applying below parameters on z-score normalised data with uniform weights initialization with MAE metric');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'uniform'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp7, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with uniform weights initialization with MAE metric');\ntrained_model_exp7_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","6453cc68":"hp_results_df","c71c1822":"print('Model Results when applying below parameters on z-score normalised data with uniform weights initialization with MSE metric');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'uniform'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp8, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS)\n\nprint('Model Results when applying below parameters on min-max normalised data with uniform weights initialization with MSE metric');\ntrained_model_exp8_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","73456773":"hp_results_df","43fc18b3":"# Get the name of the metric \nhp_results_df['metrics'] = hp_results_df['metrics'].apply(lambda x: str(x).split('.')[4].split()[0])\nhp_results_df\n","a335fc0c":"## Function to plot the final predicted results  \ndef plot_pred_results(experimented_model_id, df, X_test, title):\n    mpl.style.use('fivethirtyeight')\n    \n    # predicting target values by passing test data\n    y_pred = experimented_model_id.predict(np.array(X_test).reshape(X_test.shape[0],X_test.shape[1],1))    \n    \n    # Ploting bar graph btw groudtruth and predicted alues\n    pred_df = df.loc[X_test.index][['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDate', 'LastAccessDate','MonthsDuration']]\n\n    pred_df['CreationDateYear'] = pred_df['CreationDate'].dt.year\n    pred_df['LastAccessDateYear'] = pred_df['LastAccessDate'].dt.year\n\n    pred_df['PredMonthsDuration'] = y_pred.astype('int')\n\n    pred_df['Diff'] = pred_df['PredMonthsDuration'] - pred_df['MonthsDuration']\n    result_df = pred_df.groupby('LastAccessDateYear').sum()\n    result_df['Year'] = result_df.index\n    r_df = result_df[['MonthsDuration','PredMonthsDuration']]\n    r_df.columns = ['GroundTruth', 'Predicted']\n\n    r_df.plot(kind=\"bar\")\n    plt.title(title, loc ='center', fontsize=16)\n    #plt.yticks([])\n    plt.xticks(fontsize=10, rotation=0) \n    plt.xlabel('Users Last Access Date grouped by year', fontsize=10)\n    plt.savefig(f'{title}.png')\n    plt.show();\n    \n    return None","2ea305f7":"plot_pred_results(trained_model_exp1, std_md_df, X_test_z_score, 'Model-Experiment1 Results')\nhp_results_df[hp_results_df.index == 0]","1ee7ed0a":"plot_pred_results(trained_model_exp1_, min_max_md_df, X_test_mms, 'Model-Experiment2 Results')\nhp_results_df[hp_results_df.index == 1]","7f3c1794":"plot_pred_results(trained_model_exp2, std_md_df, X_test_z_score, 'Model-Experiment3 Results')\nhp_results_df[hp_results_df.index == 2]","2cb840e9":"plot_pred_results(trained_model_exp2_, min_max_md_df, X_test_mms, 'Model-Experiment4 Results')\nhp_results_df[hp_results_df.index == 3]","937c79a3":"plot_pred_results(trained_model_exp3, std_md_df, X_test_z_score, 'Model-Experiment5 Results')\nhp_results_df[hp_results_df.index == 4]","e4d57a9e":"plot_pred_results(trained_model_exp3_, min_max_md_df, X_test_mms, 'Model-Experiment6 Results')\nhp_results_df[hp_results_df.index == 5]","695aaf57":"plot_pred_results(trained_model_exp4, std_md_df, X_test_z_score, 'Model-Experiment7 Results')\nhp_results_df[hp_results_df.index == 6]","50546a8f":"plot_pred_results(trained_model_exp4_, min_max_md_df, X_test_mms, 'Model-Experiment8 Results')\nhp_results_df[hp_results_df.index == 7]","5555de53":"plot_pred_results(trained_model_exp5, std_md_df, X_test_z_score, 'Model-Experiment9 Results')\nhp_results_df[hp_results_df.index == 8]","9de72bbe":"plot_pred_results(trained_model_exp5_, min_max_md_df, X_test_mms, 'Model-Experiment10 Results')\nhp_results_df[hp_results_df.index == 9]","d03f0d38":"plot_pred_results(trained_model_exp6, std_md_df, X_test_z_score, 'Model-Experiment11 Results')\nhp_results_df[hp_results_df.index == 10]","a7ac1e85":"plot_pred_results(trained_model_exp6_, min_max_md_df, X_test_mms, 'Model-Experiment12 Results')\nhp_results_df[hp_results_df.index == 11]","d2e32dd6":"plot_pred_results(trained_model_exp7, std_md_df, X_test_z_score, 'Model-Experiment13 Results')\nhp_results_df[hp_results_df.index == 12]","b9e7a392":"plot_pred_results(trained_model_exp7_, min_max_md_df, X_test_mms, 'Model-Experiment14 Results')\nhp_results_df[hp_results_df.index == 13]","0a97d52a":"plot_pred_results(trained_model_exp8, std_md_df, X_test_z_score, 'Model-Experiment15 Results')\nhp_results_df[hp_results_df.index == 14]","6b273297":"plot_pred_results(trained_model_exp8_, min_max_md_df, X_test_mms, 'Model-Experiment16 Results')\nhp_results_df[hp_results_df.index == 15]","44e35336":"## Function to get date for the given months duration\ndef get_predicted_last_access_date(creation_date,mnts_duration):\n    relative_months = relativedelta(months=int(mnts_duration))\n    return creation_date + relative_months","44b1a7e4":"# Predicted Date on test data\ny_pred = trained_model_exp7.predict(np.array(X_test_z_score).reshape(X_test_z_score.shape[0],X_test_z_score.shape[1],1)) \npred_df = std_md_df.loc[X_test_z_score.index][['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDate', 'LastAccessDate','MonthsDuration']]\npred_df['PredMonthsDuration'] = y_pred.astype('int')\n\npred_df['Predicted_LastAccessDate'] =  pred_df.apply(lambda x: get_predicted_last_access_date(x.CreationDate, x.MonthsDuration), axis=1)\n\npred_df[['LastAccessDate','Predicted_LastAccessDate']].tail(40)","5d02694b":"### Apply Word EMbedding on text feature","bf79e4fb":"### Normalization","9d54bab4":"## Concise Summary","e870e7de":"### AboutMe Feature\n","63b1109f":"> Observatio(s):\n\n* Like we discussed earlier, there is lot of sparse data because of this around 40k records are having location as NaN\/unKnown.\n* as per the rest of the locations we can understand that most of the user profiles are created from UK, India and Germany countries","d0dd943e":"# Model Prediction ","e34a6329":"> Observation(s):\n\n* Higheist Users created in the year 2016\n* From 2008 to 2016 , there is a positive upward trend in terms of users creation\n* After 2016 onwards the trend is moving downwards","4699fe78":"## Missing Data","fc5595b1":"> there is a linear relation we can see between the traget varaible Vs views, Upvotes, Reputation and LastAccessDateYearfeature","ea9b6ebb":"##  **Thanks for your time!!** ","dd66151c":"> Observation(s): User profiles are highly accessed in the month of April 2019","fc953ebe":"## Duplicated Accounts","fe02609b":"## Tunnable params results\n> Due to memory issues, finally i was able to ran the model with below hyper parameters","7d107dbd":"# EDA\n## XML to DataFrame ","ce9b3f53":"> Observation(s): User profiles are highly Created in the month of October & November 2020","d0713f45":"# Approach","778e46cb":"## Uniqueness ","08e1f0bf":"### base stats\n> Highest Views\/UpVotes\/DownVotes & Recent Creation\/LastAccess Dates","f05a03ff":"> Observation(s):\n\n* Accessing the Profiles are use to increasing in trend by year by year","c15b6071":"# **Input** \n#### **Problem Statement** : Goal is to predict \"LastAccessDate\" from the domain specific user profile data\n#### **Data Source** :  https:\/\/archive.org\/download\/stackexchange\n#### **Domain** : money, astronomy, chemistry, law, wordpress , stackoverflow and so on \nNote: I am playing with stackoverflow users meta data domain  in which file name starts with \"meta.stackoverflow.com\". I've manually downloaded data from above mentioned url and added into \"Kaggle\" notebook instance\/session\n\n> Below are the available Users.xml Schema taken from internet ([link](https:\/\/meta.stackexchange.com\/questions\/2677\/database-schema-documentation-for-the-public-data-dump-and-sede)) \n\n                            * Id (It is the Id of the user with respect to specific site\/domain)\n                            * Reputation (It is a way to measure user expertise)\n                            * CreationDate (when the user is created)\n                            * DisplayName \n                            * LastAccessDate (Datetime user last loaded a page; updated every 30 min at most)\n                            * WebsiteUrl\n                            * Location\n                            * AboutMe\n                            * Views (Number of times the profile is viewed by X number of profilers)\n                            * UpVotes (How many upvotes the user has cast)\n                            * DownVotes (How many downvotes the user has cast)\n                            * ProfileImageUrl\n                            * AccountId (User's Stack Exchange Network profile ID)\n\n\n","29544acc":"![sof_logo.png](attachment:sof_logo.png)\n###### stackoverflow logo taken from google images","229a1735":"## Model","007ac59b":"> Observation(s):\n\n* WebsiteUrl, Location and AboutMe fields are having more than 50% of missing values","cffddd19":"### Location Feature","d6a7649d":"### WebsiteUrl Feature","35416ad7":">Observation(s) :\n\n* There are 1659569 user profiles exists. By default all the fields are representing \"object\" data type. Hence we need to reassign to correct data type such as Date\/String\/Numeric\n* Total 13 fields\/columns are exists. In which, \"WebsiteUrl\", \"Location\", \"AboutMe\" and \"ProfileImageUrl\" are having missing values. Hence we need to dig into the data to understand more on this.\n* For convenient, rename the column names by removing symbol '@' ","7b5b85ef":"> Observations(s):\n\n* Looks like this field is not a good feature for further data processing because some of the data is not at all related to the user. Example:\n            * \"http:\/\/none\"\n            * \"http:\/\/None\"\n            * \"http:\/\/example.com\"\n            * \"http:\/\/somethingelsetodo\"\n            * \"http:\/\/perfectD0TpsychoATgmailD0Tc0m\"\n            * \"http:\/\/localhost\"\n            * http:\/\/----------------------------------\n* Also there are some genuine website urls exists in the data but its a tedious job to extract the data from those sites. Because there are unique web urls with differemt html hierarchy. However if the client requests then we need to do\n* Hence not applying any missing data handling technique at this point of time for this field, we can ignore this feature for further modelling","87ea76a4":"# Conclusion Notes:\n* We are hoping that we reduced the validation loss \n* Few points about our journey to reach to this level\n    * Initailly we understand this problem statement is of supervised learning comes under regression type.\n    * So started with basic regression models on the nuemric features and realised this model can be improvised by considering the \"AboutMe\" text feature as well\n    * Hence we started applying Word embedding on text feature. We can use any one of the text extractions like a Bag-Of-Words which is suitable for text classification, TF-IDF is for document classification and in this problem statement we are trying to transform a word into a vector using word embeddings. But we dont have large corpus to train the word embedding model. Hence used the pre-trained Word Embedding Technique \"GloVe\"to get the global statistics along with local.\n    * We passed text feature output of word embedding to the LSTM model and tuned the model as partof generalisation.\n    * And finally shown the bar graphs between GroundTruth and Predictes values \n    * Choosing best model is (due to environment constraints I've considered Early stoppinng at 1 epoch, but we can improvise it atleast by 3 and check the least validation error to get the best parameters for this model)\n","a52ee207":"### ProfileImageUrl Feature\n> Since this is an image url, we are not extracting any information from this images. Hence we can ignore this feature for further","4f6be765":"> Lets go into deep in each of these missing fields data","d29c5d53":"## Hyper Parameter Tuning\n> Finding best parameters","665541b9":"> Views , upvotes and reputation are positively correlated.","d3aded35":"### Regression plots againt target feature","09f8f527":"# Libraries","6cc0e9b3":"### EDA on Date & Numeric Features","17093dfd":"# Modelling","b911f0d6":"> Since our target feature is \"LastAccessDate', instead of exact date prediction, lets try to find out the months duration between the users profile creation and lastaccess dates","f8ac0b33":"### Statistics"}}