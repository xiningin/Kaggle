{"cell_type":{"12958ecb":"code","d74db852":"code","85f476ae":"code","32e5e0cf":"code","ee62c01e":"code","ac1c2369":"code","94906066":"code","26bed870":"code","e1754674":"code","e3ed70c9":"code","aebbfa27":"code","3bd75eb6":"code","04dabea3":"code","0edf3584":"code","a2a7e6f8":"code","2800dfc8":"code","f403676d":"code","b36b2c73":"code","87c83546":"code","c1ad340f":"code","74a89624":"code","c69928ea":"code","e47f2aa9":"code","caa66f8f":"code","a3cfc565":"code","0a97e2ab":"code","6ba98109":"code","52d6d2c2":"code","2dac5e5f":"code","f138fabe":"code","fab59225":"code","134f7123":"code","44ebf7fb":"code","06c1d777":"code","9d8632b6":"code","3d81aeea":"code","051c29ef":"code","f50bd2b0":"markdown","63038bc3":"markdown","11489eba":"markdown","8c1a1d2d":"markdown","8568b03a":"markdown","59b2ed8c":"markdown","01ac3b01":"markdown","98452bf6":"markdown","e66c55f9":"markdown","ee697af2":"markdown","963216b5":"markdown","f7a7ad0d":"markdown","27f79bda":"markdown","512e77af":"markdown","35cb98da":"markdown","f240b0c7":"markdown","10fe6ee9":"markdown","ef70a8be":"markdown","40c9d48f":"markdown","eee8343d":"markdown","e7a15948":"markdown","f4c8a017":"markdown","112ae65c":"markdown"},"source":{"12958ecb":"from __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport pandas as pd","d74db852":"train_raw = json.load(open('..\/input\/train.json','r'))\ntest_raw = json.load(open('..\/input\/test.json','r'))","85f476ae":"train_raw[0].keys()","32e5e0cf":"X_train_np = np.array([ np.mean(sample['audio_embedding'],axis=0) for sample in train_raw]).astype(np.float32)\nX_train_id = [sample['vid_id'] for sample in train_raw]\n\nY_train_np = np.array([ sample['is_turkey'] for sample in train_raw]).astype(np.float32)\n\nX_test_np = np.array([ np.mean(sample['audio_embedding'],axis=0) for sample in test_raw]).astype(np.float32)\nX_test_id = [sample['vid_id'] for sample in test_raw]","ee62c01e":"def znorm_fn(input_data):\n    mean = np.mean(input_data,axis=0)\n    std = np.std(input_data,axis=0)\n    \n    def _znorm_fn(col):        \n        return (col - mean)\/std\n    \n    return _znorm_fn","ac1c2369":"norm_fn = znorm_fn(X_train_np)","94906066":"feature_columns = [\n    tf.feature_column.numeric_column('average_embedding',shape=[128],normalizer_fn=norm_fn)]","26bed870":"batch_size = 256\nepochs = 100","e1754674":"def input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'average_embedding':X_train_np,'vid_id':X_train_id},Y_train_np))    \n    dataset = dataset.shuffle(100*batch_size).repeat(epochs).batch(batch_size)\n    return dataset","e3ed70c9":"def predict_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'average_embedding':X_test_np,'vid_id':X_test_id}))    \n    dataset = dataset.batch(batch_size)\n    return dataset","aebbfa27":"lr = tf.estimator.LinearClassifier(feature_columns=feature_columns,model_dir='.\/logistic_regression_trained_models')\nlr = tf.contrib.estimator.forward_features(lr,'vid_id')","3bd75eb6":"lr.train(input_fn=input_fn)","04dabea3":"with open('submission_logistic_regression.csv','w') as f:\n    f.write('vid_id,is_turkey\\n')\n    for prediction in lr.predict(input_fn=predict_input_fn):    \n        f.write(\"{},{}\\n\".format(prediction['vid_id'],prediction['class_ids'][0]))","0edf3584":"dnn = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n                                 model_dir='.\/dnn_trained_models',\n                                 n_classes=2,\n                                 dropout=0.2,                                \n                                 hidden_units = [128,32,8,2])\ndnn = tf.contrib.estimator.forward_features(dnn,'vid_id')","a2a7e6f8":"dnn.train(input_fn=input_fn)","2800dfc8":"with open('submission_dnn.csv','w') as f:\n    f.write('vid_id,is_turkey\\n')\n    for prediction in lr.predict(input_fn=predict_input_fn):    \n        f.write(\"{},{}\\n\".format(prediction['vid_id'],prediction['class_ids'][0]))","f403676d":"train_raw = json.load(open('..\/input\/train.json','r'))\ntest_raw = json.load(open('..\/input\/test.json','r'))","b36b2c73":"batch_size = 256\nepochs = 20","87c83546":"lstm_X_train = []\nlstm_X_train_id = []\nlstm_Y_train = []\n\nlstm_X_test = []\nlstm_X_test_id = []\n\nfor train_sample in train_raw:        \n    # Perform padding\n    if len(train_sample['audio_embedding']) < 10:\n        while len(train_sample['audio_embedding']) <10:\n            train_sample['audio_embedding'].append(np.mean(train_sample['audio_embedding'],axis=0))\n    \n    \n    lstm_X_train.append(train_sample['audio_embedding'])\n    lstm_Y_train.append(train_sample['is_turkey'])\n    lstm_X_train_id.append(train_sample['vid_id'])    \n\nfor test_sample in test_raw:       \n        \n    # Perform padding\n    if len(test_sample['audio_embedding']) < 10:\n        while len(test_sample['audio_embedding']) <10:\n            test_sample['audio_embedding'].append(np.mean(test_sample['audio_embedding'],axis=0))\n                \n    lstm_X_test.append(test_sample['audio_embedding'])    \n    lstm_X_test_id.append(test_sample['vid_id'])","c1ad340f":"lstm_X_train_np = np.array(lstm_X_train).astype(np.float32)\nlstm_Y_train_np = np.array(lstm_Y_train).astype(np.float32)\n\nlstm_X_test_np = np.array(lstm_X_test).astype(np.float32)","74a89624":"from sklearn.model_selection import StratifiedShuffleSplit","c69928ea":"sss = StratifiedShuffleSplit(1,train_size=0.9)\nlstm_train_index, lstm_eval_index = next(sss.split(lstm_X_train_np, lstm_Y_train_np))\n","e47f2aa9":"lstm_X_train_train_np = lstm_X_train_np[lstm_train_index]\nlstm_X_train_eval_np = lstm_X_train_np[lstm_eval_index]\n\nlstm_Y_train_train_np = lstm_Y_train_np[lstm_train_index]\nlstm_Y_train_eval_np = lstm_Y_train_np[lstm_eval_index]\n\nlstm_X_train_train_id = list(np.array(lstm_X_train_id)[lstm_train_index])\nlstm_X_train_eval_id = list(np.array(lstm_X_train_id)[lstm_eval_index])","caa66f8f":"print(lstm_X_train_train_np.shape)\nprint(lstm_X_train_eval_np.shape)\n\nprint(lstm_Y_train_train_np.shape)\nprint(lstm_Y_train_eval_np.shape)\n\nprint(lstm_X_test_np.shape)\n\nprint(len(lstm_X_train_train_id))\nprint(len(lstm_X_train_eval_id))","a3cfc565":"def znorm_fn(input_data):\n    mean = np.mean(input_data,axis=0)\n    std = np.std(input_data,axis=0)\n    \n    def _znorm_fn(mini_batch):        \n        norm_mini_batch = tf.map_fn(lambda c: (c-mean)\/std,mini_batch)                \n        return norm_mini_batch\n    \n    return _znorm_fn","0a97e2ab":"norm_fn = znorm_fn(lstm_X_train_train_np)","6ba98109":"feature_columns = [\n    tf.feature_column.numeric_column('audio_embedding',shape=[128],normalizer_fn=norm_fn)]","52d6d2c2":"def lstm_train_eval_input_fn(X,Y,vid_id,batch_size):\n    \n    def raw_input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices(({'audio_embedding':X,'vid_id':vid_id},Y))    \n        dataset = dataset.batch(batch_size)\n        return dataset\n    \n    return raw_input_fn\n\nlstm_train_input_fn = lstm_train_eval_input_fn(lstm_X_train_train_np,\n                                               lstm_Y_train_train_np,\n                                               lstm_X_train_train_id,\n                                              batch_size)\n\nlstm_eval_input_fn = lstm_train_eval_input_fn(lstm_X_train_eval_np,\n                                              lstm_Y_train_eval_np,\n                                              lstm_X_train_eval_id,\n                                             batch_size)","2dac5e5f":"def lstm_predict_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'audio_embedding':lstm_X_test_np,'vid_id':lstm_X_test_id}))    \n    dataset = dataset.batch(batch_size)\n    return dataset","f138fabe":"def metric_ops(target,predictions):    \n    return {\n        'Accuracy': tf.metrics.accuracy(\n        labels=target,\n        predictions=predictions,\n        name='accuracy')\n    }","fab59225":"params = tf.contrib.training.HParams(\n    learning_rate = 5e-3,\n    dropout=0.3,\n    reg_val=1e-3,\n    feature_columns = feature_columns # here you pass the feature columns to the model_fn (see below)\n)\n\nrun_config = tf.estimator.RunConfig(\n    model_dir='.\/lstm_model',\n    save_summary_steps=100,    \n    save_checkpoints_steps=100,\n    keep_checkpoint_max=3,    \n)\n","134f7123":"def model_fn(features, labels, mode, params):\n            \n    is_training = mode == tf.estimator.ModeKeys.TRAIN               \n\n    # Apply transformations on input the features given the feature_columns\n    input_features = tf.feature_column.input_layer(features, params.feature_columns)\n    \n    # Unfortunately, one needs to reshape the input tensor\n    # because the transformation above cancels the second dimension (window size)\n    input_features = tf.reshape(input_features,[-1,10,128])\n\n    # Unstack the second dimension (window size) to return a list of tensors\n    feature_sequence = tf.unstack(input_features,axis=1)    \n    \n    num_units = [128,64,8]\n    \n    # Create Bidirectional LSTM network\n    lstm_forward_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in num_units]\n    lstm_backward_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in num_units]\n    \n    stacked_lstm_forward_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_forward_cells)\n    stacked_lstm_backward_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_backward_cells)\n    \n    lstm_output, _,_ = tf.nn.static_bidirectional_rnn(stacked_lstm_forward_cell,\n                                                   stacked_lstm_backward_cell,\n                                                   feature_sequence,\n                                                   dtype=tf.float32)\n    last_lstm_output = lstm_output[-1]\n    \n\n    net = tf.layers.dropout(last_lstm_output,params.dropout,training=is_training)\n    \n    net = tf.layers.dense(inputs=net,units=8,activation=tf.nn.relu,\n                          kernel_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val),\n                          kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                          bias_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                          bias_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val))    \n        \n    net = tf.layers.dense(inputs=net,units=2,activation=tf.nn.relu,\n                          kernel_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val),\n                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                          bias_initializer=tf.contrib.layers.xavier_initializer(),\n                          bias_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val))\n    net = tf.nn.softmax(net)\n\n    predictions = {\n        'class_id':tf.argmax(net, axis=1, name=\"prediction\"),\n        'class_proba': net}    \n\n\n    total_loss = None\n    loss = None\n    train_op = None\n    eval_metric_ops = {} \n        \n    if mode != tf.estimator.ModeKeys.PREDICT:\n        \n        # As target is just 0 or 1, it is necessary to transform to one-hot encoding\n        target = tf.one_hot(tf.cast(labels,dtype=tf.uint8),depth=2)        \n        \n         # IT IS VERY IMPORTANT TO RETRIEVE THE REGULARIZATION LOSSES BY HAND\n        reg_loss = tf.losses.get_regularization_loss()        \n        loss = tf.losses.softmax_cross_entropy(target, net)                    \n        total_loss = loss + reg_loss\n\n        learning_rate = tf.constant(params.learning_rate, name='fixed_learning_rate')            \n        optimizer = tf.train.AdamOptimizer(learning_rate)\n#         optimizer = tf.train.RMSPropOptimizer(learning_rate)\n\n        if is_training:\n            # If you plan to use batch_norm layers, you DO must get this collection in order to perform updates on batch_norm variables\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                train_op = optimizer.minimize(                    \n                    loss=total_loss, global_step=tf.train.get_global_step())\n\n        eval_metric_ops = metric_ops(labels, predictions['class_id'])\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=total_loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)","44ebf7fb":"lstm = tf.estimator.Estimator(\n    model_fn=model_fn,\n    params=params,\n    config=run_config\n)\n\nlstm = tf.contrib.estimator.forward_features(lstm,'vid_id')","06c1d777":"# If you want to train without any evaluation, uncomment the line below\n# lstm.train(input_fn=lstm_train_input_fn)\n\n\nlstm_train_spec = tf.estimator.TrainSpec(input_fn=lstm_train_input_fn,\n                                        max_steps=int(epochs*(len(lstm_X_train_train_id)\/batch_size)))\nlstm_eval_spec = tf.estimator.EvalSpec(input_fn=lstm_eval_input_fn,\n                                       start_delay_secs=30,\n                                       throttle_secs=15,\n                                      steps=None)\n\ntf.estimator.train_and_evaluate(estimator=lstm,\n                                train_spec=lstm_train_spec,\n                                eval_spec=lstm_eval_spec)\n","9d8632b6":"raw_predictions = {'vid_id':[],'is_turkey':[]}\n\nfor prediction in lstm.predict(input_fn=lstm_predict_input_fn):    \n    raw_predictions['vid_id'].append(prediction['vid_id'])\n    raw_predictions['is_turkey'].append(prediction['class_proba'][1])    \n\nraw_predictions_df = pd.DataFrame(raw_predictions)","3d81aeea":"raw_predictions_df.head(10)","051c29ef":"raw_predictions_df.to_csv('.\/submission_lstm.csv',index=None,columns=['vid_id','is_turkey'])","f50bd2b0":"### model_fn\n\nHere it is defined the network architecture","63038bc3":"# Don't Call me Turkey!\n\nThis kernel will show three approaches to tackle the challenge [Don't Call me Turkey!](https:\/\/www.kaggle.com\/c\/dont-call-me-turkey) using TensorFlow Estimator API\n\n* Logistic Regression \n* Multilayer Perceptron\n* LSTM\n","11489eba":"# Tensorflow model using canned estimators","8c1a1d2d":"## Read dataset","8568b03a":"### Feature Columns\n\nAlthough not mandatory, dealing with [feature columns makes](https:\/\/www.tensorflow.org\/guide\/feature_columns) life easier when building models with [Estimator API](https:\/\/www.tensorflow.org\/guide\/estimators). \n\nThink of a feature column as an intermediary entity between the raw data and the model itself. \n\nIt helps the model to interpret\/transform the data that comes from the input function.\n\nThere are available the following feature column types: \n\n* [numeric_column](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/numeric_column): Treat raw data as a numeric scalar or matrix\n* [bucketized_column](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/bucketized_column): Map a numeric scalar into buckets, as one-hot representation, given a boundary list\n* [categorical_column_with_identity](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/categorical_column_with_identity): Map an integer input into a one-hot representation\n* [categorical_column_with_vocabulary_list](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/categorical_column_with_vocabulary_list): Map a string input into a one-hot representation given a Python list.\n* [categorical_column_with_vocabulary_file](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/categorical_column_with_vocabulary_file): Map a string input into a one-hot representation given a vocabulary list as a text file\n* [categorical column with hash_bucket](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/categorical_column_with_hash_bucket): Map a string or integer input into a hash ID as a sparse tensor.\n* [embedding_column](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/embedding_column): Map a categorical feature into a low dimensional representation\n* [indicator_column](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/indicator_column): Map a categorical feature into multi-hot encoding representation\n* [crossed_column](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/crossed_column): Map a tuple of categorical features into a hash ID as a sparse tensor\n\n\nIn our case we have a feature embedding of size 128 for each 10 frames (1sec each) per audio.\n\nThe first approach will build a simple Logistic Regression taking the average embedding as input.\nSo we'll end up with just one feature of size 128","59b2ed8c":"#### Model Parameters","01ac3b01":"#### Train model","98452bf6":"### Define normalizer_fn\n\nZNorm","e66c55f9":"### Define metrics to be measured during evalution when training","ee697af2":"### Logistic Regression\n\n**OBS**: **vid_id** is being forwarded to be output on predictions. \n\nRecall that although **vid_id** is a feature, it is not being used to train the model, once who defines which features will be used is **feature_columns**\n","963216b5":"#### Instantiate estimator","f7a7ad0d":"## Canned Estimators","27f79bda":"### Multilayer Perceptron","512e77af":"### Define Estimator input function\n\nHere it is defined how and which data will be consumed by the model during training, evaluation and prediction","35cb98da":"### Split a portion of trainset for evaluation","f240b0c7":"## LSTM \n\nAs there isn't a canned estimator for LSTM, we need to code a custom one by ourselves","10fe6ee9":"#### Input function for prediction","ef70a8be":"#### Make predictions\n\nAs predictions will be made on a sliding window, it is necessary to aggretate all windows' predictions","40c9d48f":"### Input Function","eee8343d":"#### Predictions","e7a15948":"#### Input function for training and evaluation","f4c8a017":"### Read dataset","112ae65c":"#### Predictions"}}