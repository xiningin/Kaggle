{"cell_type":{"584d3500":"code","bf0928b8":"code","290374ac":"code","af836d6f":"code","49817946":"code","73afe696":"code","c7a41986":"code","74acd8fa":"code","5e081430":"code","36097100":"code","ce8f68b0":"code","e83954cf":"code","6e7378fd":"markdown","f5c4d9be":"markdown","202eb205":"markdown","784762e5":"markdown","78d3a01b":"markdown","8b87c7ac":"markdown","7d2e56fe":"markdown","75192bca":"markdown","60561b76":"markdown","8e735b55":"markdown"},"source":{"584d3500":"!pip install transformers","bf0928b8":"!git clone https:\/\/github.com\/huggingface\/transformers.git\n!pip install -U .\/transformers\n!pip install git+https:\/\/github.com\/huggingface\/nlp.git","290374ac":"#Activate GPU\n!nvidia-smi\n","af836d6f":"!pip install nlp","49817946":"import torch\nimport transformers\nfrom transformers import AlbertTokenizerFast\ntokenizer=AlbertTokenizerFast.from_pretrained('albert-large-v1')\n#train_set=nlp.load_dataset('squad_v2',split='train')","73afe696":"!mkdir squad\n!wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/train-v2.0.json -O squad\/train-v2.0.json\n!wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/dev-v2.0.json -O squad\/dev-v2.0.json!","c7a41986":"import json\nfrom pathlib import Path\n\ndef prepare_qa(path):\n  path = Path(path)\n  with open(path, 'rb') as f:\n      squad_dict = json.load(f)\n\n  contexts = []\n  questions = []\n  answers = []\n  for group in squad_dict['data']:\n      for passage in group['paragraphs']:\n          context = passage['context']\n          for qa in passage['qas']:\n              question = qa['question']\n              for answer in qa['answers']:\n                  contexts.append(context)\n                  questions.append(question)\n                  answers.append(answer)\n\n  return contexts, questions, answers\n\ndef modify_answer_context(answers,contexts):\n  for ans,ctx in zip(answers,contexts):\n    \n    result_text=ans['text']\n    start_idx=ans['answer_start']\n    \n    end_idx=start_idx+len(result_text)\n    if (ctx[start_idx:end_idx]==result_text):\n      ans['answer_end']=end_idx\n    elif (ctx[start_idx-1:end_idx-1]==result_text):\n      ans['answer_start'][0]=start_idx-1\n      ans['answer_end']=end_idx-1\n    elif (ctx[start_idx-2:end_idx-2]==result_text):\n      ans['answer_start'][0]=start_idx-2\n      ans['answer_end']=end_idx-2\n    else:\n      raise ValueError()\ntrain_contexts,train_questions,train_answers=prepare_qa('squad\/train-v2.0.json')\nval_contexts,val_questions,val_answers=prepare_qa('squad\/dev-v2.0.json!')\nmodify_answer_context(train_answers,train_contexts)\nmodify_answer_context(val_answers,val_contexts)\n","74acd8fa":"train_encode=tokenizer(train_contexts,train_questions,truncation=True,padding=True)\nval_encode=tokenizer(val_contexts,val_questions,truncation=True,padding=True)","5e081430":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encode, train_answers)\nadd_token_positions(val_encode, val_answers)","36097100":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AlbertForQuestionAnswering\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n\nclass SquadDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\nmodel=AlbertForQuestionAnswering.from_pretrained(\"albert-large-v1\")\n\ntrain_dataset = SquadDataset(train_encode)\nval_dataset = SquadDataset(val_encode)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel.to(device)\nmodel.train()\n\ntrain_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n\noptim = AdamW(model.parameters(), lr=5e-5)\nepochs=2\nfor epoch in range(epochs):\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n\nmodel.eval()","ce8f68b0":"from transformers import  Trainer, TrainingArguments\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AlbertForQuestionAnswering\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\n\nclass SquadDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\nmodel=AlbertForQuestionAnswering.from_pretrained(\"albert-large-v1\")\n\ntrain_dataset = SquadDataset(train_encode)\nval_dataset = SquadDataset(val_encode)\ntraining_args = TrainingArguments(\n    output_dir='.\/results',          # output directory\n    num_train_epochs=0.1,              # total number of training epochs\n    per_device_train_batch_size=5,  # batch size per device during training\n    per_device_eval_batch_size=10,   # batch size for evaluation\n    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs',            # directory for storing logs\n    logging_steps=5,\n)\n\n#model=AlbertForQuestionAnswering.from_pretrained(\"albert-large-v1\")\ntrainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()","e83954cf":"from transformers import AutoTokenizer,AutoModelForQuestionAnswering\nfrom transformers import pipeline\nmodel=AutoModelForQuestionAnswering.from_pretrained('abhilash1910\/albert-squad-v2')\ntokenizer=AutoTokenizer.from_pretrained('abhilash1910\/albert-squad-v2')\nnlp_QA=pipeline('question-answering',model=model,tokenizer=tokenizer)\nQA_inp={\n    'question': 'How many parameters does Bert large have?',\n    'context': 'Bert large is really big... it has 24 layers, for a total of 340M parameters.Altogether it is 1.34 GB so expect it to take a couple minutes to download to your Colab instance.'\n}\nresult=nlp_QA(QA_inp)\nresult","6e7378fd":"## Load the Transformer to be trained \n\nWe are loading the [AlbertTokenizerFast](https:\/\/huggingface.co\/transformers\/model_doc\/albert.html#alberttokenizerfast) backed by Huggingface for Unigram based tokenization and it uses the PretrainedTokenizerFast class.[Unigram](https:\/\/huggingface.co\/docs\/tokenizers\/python\/latest\/components.html?highlight=unigram#models) is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one.","f5c4d9be":"## Training Albert Transformer for QA on SQuAD - 2.0\n\n<img src=\"https:\/\/camo.githubusercontent.com\/b9d050a07e52c7930206d37d72a229ab484cae1ace09bf0fe1c6cf9c7f5d4bc0\/68747470733a2f2f68756767696e67666163652e636f2f66726f6e742f6173736574732f68756767696e67666163655f6c6f676f2e737667\">\n\nThis notebook contains the details to train [albert transformer (large variant)](https:\/\/huggingface.co\/transformers\/model_doc\/albert.html) on closed domain SQuAD dataset. The dataset is present [here](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/).  The notebook shows the steps to train any BERT based transformer on SQuAD-2 dataset:\n\n- Clone the transformers repository\n- For training on SQuAD it is recommended to activate GPU runtime since it takes time\n- Download the SQuAD dataset from the associated repository in a local directory\n- Modify the dataset as in some cases, SQuAD-2 contains null answers to some questions\n- The file format is in json and needs to be split for separating the questions answers and contexts\n- Modify the answer,context pairs to ensure that the indices are properly marked (in some cases, the are off by 1\/2)\n- Tokenize the question and context pairs using a Huggingface backed Fast Tokenizer\n- Initialize a Torch Dataset class, so as to convert the tokenized input into a torch dataset mapping\n- Load the model from Huggingface\n- Use Torch.DataLoader for loading the dataset and also transfer the model weights to cuda runtime\n- Execute the training by using AdamW optimizer and initializing the gradients with 0\n- For each epoch the input_ids, attention_masks ,start_positions and end_positions are placesd on the input to the model.\n- Run for considerable epochs for best results\n\nWe are using the Pytorch framework for running the model training.","202eb205":"## Encode the Question and Context pairs\n\nIn this case, we will be encoding both the question and contexts and will be setting the truncation to True to ensure uniformity in the size. Padding is also left to True.Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs.","784762e5":"## Converting tokens to position ids\n\nNext we need to convert our character start\/end positions to token start\/end positions. When using \ud83e\udd17 Fast Tokenizers, we can use the built in char_to_token() method. This is done in 'add_tokens_positions' method.","78d3a01b":"<img src=\"http:\/\/ai.stanford.edu\/blog\/assets\/img\/posts\/2019-10-21-answering-complex-questions\/drqa.png\">","8b87c7ac":"## Testing the model after training\n\nOnce the training is completed, we can test it. For this we also have to make sure the tokenizer is saved in the same directory. The trained model can be found [here](https:\/\/huggingface.co\/abhilash1910\/albert-squad-v2) and can be used for inference.","7d2e56fe":"## Training through Trainer\n\n[Trainer class](https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html#transformers.Trainer) can also be used for training.","75192bca":"## Training in Pytorch and HF\n\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT0chcDI56FF_5P9DFF2KP_xxiSC9mFwwHN8_Pk-tSCntFmcGjjEtQhd3cnjLBFnFkduB8&usqp=CAU\">\nFirst we have to convert the dataset to a [Torch.dataset format.](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html). The SquadDataset class is built for this to create  a mapping of the key value pairs in the encoded question context pairs. Then the model is loaded and the DataLoader is used to load the dataset entries for training. We are also using Cuda for GPU based training. For each epoch of training, we initialize the gradient with 0 value and then pass the input_ids,attention_masks,start_positions and end_positions to the model.\nAlternately we can also use the [HF trainer](https:\/\/huggingface.co\/transformers\/custom_datasets.html#ft-trainer) module for training the dataset.\n","60561b76":"## Creating the Split and Dataset\n\nEach split is in a structured json file with a number of questions and answers for each passage (or context). We\u2019ll take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since there are multiple questions per context). This is done in the 'prepare_qa' method.\n\nThe contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context\/question pairs, and (2) integers indicating at which token positions the answer begins and ends.We habe to get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that.This is done in the 'modify_answer_context' method.\n","8e735b55":"## Conclusion\n\nThis script can be used for training any BERT based transformer for QA and has been abstracted from [Huggingface tutorials](https:\/\/huggingface.co\/transformers\/custom_datasets.html#resources)"}}