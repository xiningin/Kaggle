{"cell_type":{"118cd5ab":"code","0a1a73d7":"code","153d6023":"code","e1839371":"code","6058b38d":"code","95ff1341":"code","6595e1d6":"code","3a458ca8":"code","5a5ad13f":"code","9da36b10":"code","76d4173a":"code","0b05d340":"code","b77f8b46":"code","74d58e1e":"code","24d39791":"code","a5e847c3":"code","115fb237":"code","e0cde4f6":"code","a8a7f785":"code","1406ae8e":"code","bb16534d":"code","4a46976f":"code","c1e91436":"code","ae84ade8":"code","b9a46740":"code","0255e64c":"code","226c11a1":"markdown","f6df6a08":"markdown","6427d8b6":"markdown","ce585d62":"markdown","e732e3da":"markdown","926ff29f":"markdown","ccf650c3":"markdown","21f829a5":"markdown","3bcf01fd":"markdown","1b67d9fe":"markdown"},"source":{"118cd5ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\n\nnp.random.seed(42)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a1a73d7":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","153d6023":"test.info(memory_usage=\"deep\")","e1839371":"train.head()","6058b38d":"train[\"target\"].describe()","95ff1341":"fig, ax = plt.subplots(figsize=(7, 4))\nax.hist(train[\"target\"], bins=20, edgecolor=\"black\", color=\"steelblue\")\nax.set_title(\"Target distribution\", fontsize=15)\nax.set_xlabel(\"Target\", fontsize=12)\nax.set_ylabel(\"Amount of observations\", fontsize=12)\nplt.show();","6595e1d6":"train[\"standard_error\"].describe()","3a458ca8":"fig, ax = plt.subplots(figsize=(7, 4))\nax.hist(train[\"standard_error\"], bins=20, edgecolor=\"black\", color=\"palevioletred\")\nax.set_title(\"Standard error distribution\", fontsize=15)\nax.set_xlabel(\"Standard error\", fontsize=12)\nax.set_ylabel(\"Amount of observations\", fontsize=12)\nplt.show();","5a5ad13f":"def get_text_data_parameters(data, stop_words):\n    \"\"\"\n    Calculates some numeric parameters of paragraphs of the given series object.\n    \"\"\"\n    \n    text_shortage = []\n    quotes = []\n    sentences = []\n    sent_length = []\n    word_length = []\n    lemma_length = []\n    \n#     new_data = []\n    for row in data:\n        # Amount of quotes devided by 2 to determine if there is any dialogue\n        quotes.append(row.count('\"')\/2)\n        # The original, raw text paragraph lenght\n        initial_length = len(row)\n        # Using nltk tokenizer to split a text into sentences to determine their amount\n        num_sent = len(sent_tokenize(row))\n        sentences.append(num_sent)\n        # Getting rid of all noncharacter symbols and splitting a text into \n        # words using nltk tokenizer and getting amount of words\n        row = re.sub(\"[^a-zA-Z]\", \" \", row)\n        row = row.lower()\n        row = word_tokenize(row)\n        num_words = len(row)\n        \n        # Calculating mean amount of words per sentence and mean word length \n        sent_length.append(num_words\/num_sent)\n        word_length.append(initial_length\/num_words)\n        # Splitting text data into words and dropping stop words\n        row = [word for word in row if not word in stop_words]\n        # Words lemmatisation\n        lemma = nltk.WordNetLemmatizer()\n        row = [lemma.lemmatize(word) for word in row]\n        num_lemmas = len(row)\n        row = \" \".join(row)\n        # Text length after cleaning and lemmatisation\n        processed_length = len(row)\n        # Calculating mean lemma length and amount of text shrinkage after the processing\n        lemma_length.append(processed_length\/num_lemmas)\n        text_shortage.append(processed_length\/initial_length)\n    \n    # Creating a dataframe containing all calculated parameters\n    result_df = pd.concat([pd.Series(text_shortage), pd.Series(quotes),\n                          pd.Series(sentences), pd.Series(sent_length),\n                          pd.Series(word_length), pd.Series(lemma_length)], axis=1)\n    result_df.columns = [\"text_shortage\", \"num_quotes\",\n                        \"num_sentences\", \"sent_length\",\n                        \"mean_word_length\", \"mean_lemma_length\"]\n    \n    return result_df","9da36b10":"# Stopwords import from nltk \nstop_words = set(stopwords.words(\"english\"))","76d4173a":"text_params = get_text_data_parameters(train[\"excerpt\"].copy(), stop_words)\ntext_params.head()","0b05d340":"df = pd.concat([text_params, train[\"target\"]], axis=1)\n\nax = sns.pairplot(data=df,\n                  diag_kws=dict(bins=15, color=\"lightcoral\"),\n                  plot_kws=dict(color=\"seagreen\"))\nax.fig.suptitle(\"Target and text parameters pairplots\", fontsize=15, y=1.03)\nplt.show();","b77f8b46":"# Plot dataframe\ncorr = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(8,8))\nax = sns.heatmap(corr, annot=True, mask=mask, cmap=\"RdBu\", linewidths=1,\n                 annot_kws={\"weight\": \"bold\", \"fontsize\": 13})\n\nax.set_title(\"Target and text parameters heatmap\", fontsize=15, y=1.03)\nplt.show();","74d58e1e":"def scale_train_text_params(df):\n    scalers = []\n#     scaled_columns = []\n    \n    for column in df.columns:\n        scaler = MinMaxScaler()\n        df[column] = scaler.fit_transform(np.array(df[column]).reshape(-1, 1))\n        scalers.append(scaler)\n#         scaled_columns.append(scaled_column)\n\n    return df, scalers\n\ndef scale_test_text_params(df, scalers):\n#     scaled_columns = []\n    \n    for i, column in enumerate(df.columns):\n        scaler = scalers[i]\n        df[column] = scaler.transform(np.array(df[column]).reshape(-1, 1))\n#         scaled_columns.append(scaled_column)\n\n    return df\n\ndef vectorize_text_data(X_train, text_params, vectorizer):\n    vectorized_text = vectorizer(X_train)\n    print(f\"Vectorized text shape: {vectorized_text.shape}\")\n    \n    X_train = np.concatenate((text_params.to_numpy(), vectorized_text), axis=1)\n    print(f\"Concatenated data shape: {X_train.shape}\")\n    \n    return X_train","24d39791":"# Loading Universal Sentence Encoder\nvectorizer = hub.load(\"\/kaggle\/input\/universalsentenceencoderv4tf20\/\")","a5e847c3":"# Scaling text parameters\ntext_params, scalers = scale_train_text_params(text_params)\n# Vectorizing text data and concatenating it with text parameters\nX = vectorize_text_data(train[\"excerpt\"].str.lower().copy(), text_params, vectorizer)\ny = train[\"target\"].copy()","115fb237":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"target\"], bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"target\"], target_bin_edges, labels=np.arange(10))\ntarget_bins","e0cde4f6":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    RMSE of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n       \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [4000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.8),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"auto\"]),\n        \n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 0.00001, 0.9),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 0.00001, 0.9),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n                    }\n\n    # Model loading and training\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    # Out of fold predictions\n    oof = model.predict(X_valid)\n    # Number of actually grown trees before overfitting is detected \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","a8a7f785":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n# for train_idx, valid_idx in split.split(X, target_bins):\n#     X_train, X_valid = X[train_idx], X[valid_idx]\n#     y_train, y_valid = y[train_idx], y[valid_idx]\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n#                n_trials = 500)\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","1406ae8e":"# The function splits given train data into 10 folds and trains each model on each fold.\n# Each model makes test predictions. Mean predictions returned. \ndef train_with_folds(X, y, X_test, target_bins, params):\n    splits = 10\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros((X.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n        X_train, X_valid = X[train_idx], X[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n        model = XGBRegressor(**params)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=100,\n                  verbose=False)\n        model_preds += model.predict(X_test) \/ splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = model.predict(X_valid)\n        print(f\"Fold {num} RMSE: {np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))}\")\n    model_rmsle = np.sqrt(mean_squared_error(y, oof_preds))\n    print(f\"Overall RMSE: {model_rmsle}\")\n    \n    return model_preds, model_fi","bb16534d":"# Hyperparameters values optimized by Optuna\nxgb_params = {'n_estimators': 988,\n              'learning_rate': 0.026709466947908544,\n              'max_depth': 3,\n              'booster': 'gbtree',\n              'tree_method': 'auto',\n              'reg_lambda': 0.128405467661076,\n              'reg_alpha': 0.43102397027400285,\n              'random_state': 42,\n              'n_jobs': 4}","4a46976f":"# Preprocessing test data\ntext_params = get_text_data_parameters(test[\"excerpt\"], stop_words)\ntext_params = scale_test_text_params(text_params, scalers)\nX_test = vectorize_text_data(test[\"excerpt\"], text_params, vectorizer)","c1e91436":"%%time\npreds, feature_importances = train_with_folds(X, y, X_test, target_bins, xgb_params)","ae84ade8":"fig, ax = plt.subplots(figsize=(16, 5))\nax = sns.lineplot(data=pd.Series(feature_importances))\nax.set_title(\"Feature importance\")\nax.set_xlabel(\"Feature number\")\nax.set_ylabel(\"Importance\")\nplt.show();","b9a46740":"fig, ax = plt.subplots(figsize=(5, 5))\nax = sns.lineplot(x=text_params.columns, y=pd.Series(feature_importances[:6]))\nax.set_title(\"Feature importance\")\nax.set_xlabel(\"Feature\")\nax.set_ylabel(\"Importance\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nplt.show();","0255e64c":"submission = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\", low_memory=False)\nsubmission[\"target\"] = preds\nsubmission.to_csv('submission.csv', index=False)\nsubmission","226c11a1":"To make further analysis we need to exctract more data from text exceprts. So the function below calculates some numeric parameters I think may be important for target predicting like text length decrease after deleting stop words, mean sentence length, amount of quotes (i.e. dialogs), word and lemma mean length etc. The function returns only these parameters. The transformed and preprocessed text object is not returned.","f6df6a08":"Let's check feature importances of all features and then engineered feature only.","6427d8b6":"The code below runs hyperparameters optimization. It is commeted to save runtime.","ce585d62":"# **Data preparation**","e732e3da":"Let's take a look at these parameters.","926ff29f":"# **XGBRegressor model training**","ccf650c3":"This notebook demonstrates usage examples the following modules\/packages:\n* Matplotlib and seaborn for plots\n* Some NLTK module functions for text cleaning, splitting and lemmatisation\n* Universal Sentence Encoder for text vectorization\n* XGBRegressor model training on data folds\n* Optuna for XGBRegressor hyperparameters optimization","21f829a5":"# **Submission**","3bcf01fd":"# **Optuna**","1b67d9fe":"# **Data analysis**"}}