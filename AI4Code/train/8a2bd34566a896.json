{"cell_type":{"e7211408":"code","19edd708":"code","33593407":"code","c92ee968":"code","c4f42753":"code","6d15d1da":"code","ac154834":"code","0825e142":"code","566c1ea5":"code","bcdde450":"code","31a09b46":"code","0e9c0247":"code","f9327531":"code","8a5ec68a":"code","eca24dc6":"code","180a48c3":"code","8a30aacb":"code","4d1282c3":"code","399d0894":"code","81135bfc":"code","40fda9af":"code","743a94c5":"code","43b0d27e":"code","1fa11627":"code","8b115954":"code","810cd1fa":"code","f9d101af":"code","64075d42":"code","d97a10ec":"code","e8529783":"code","70284483":"code","b848d060":"code","902969e0":"code","d96b18b6":"code","70cf1053":"code","b36b0fbc":"code","e46163ca":"markdown","02a765dd":"markdown","54e07f86":"markdown","316c5d4f":"markdown","8fac3959":"markdown","bfc270ac":"markdown","603468e2":"markdown","ce586a6f":"markdown","c9022501":"markdown","3e65dfa2":"markdown","60e861e3":"markdown","b03bee0a":"markdown","ded62b9e":"markdown","a1a3b51a":"markdown","ecec4e36":"markdown","5e4159c2":"markdown","f77ef216":"markdown","b4249310":"markdown","b7f1c99c":"markdown"},"source":{"e7211408":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","19edd708":"# Internet needs to be on\n!pip install tensorflow-gpu==2.0a0","33593407":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.utils import shuffle\nimport os\nprint(os.listdir(\"..\/input\"))\nimport pickle \nimport matplotlib.pyplot as plt\n%matplotlib inline","c92ee968":"# Make sure tf 2.0 alpha has been installed\nprint(tf.__version__)","c4f42753":"#is it using the gpu?\ntf.test.is_gpu_available(\n    cuda_only=False,\n    min_cuda_compute_capability=None\n)\n","6d15d1da":"tf.random.set_seed(42)\ndatadir = \"..\/input\/\"","ac154834":"class Message_Passer_1(tf.keras.layers.Layer):\n    def __init__(self, intermediate_dim, state_dim):\n        super(Message_Passer_1, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate()\n        self.hidden_layer_1 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.output_layer = tf.keras.layers.Dense(units=state_dim, activation=tf.nn.relu)\n\n\n        \n    def call(self, node_i, node_j, edge_ij):\n        concat = self.concat_layer([node_i, node_j, edge_ij])\n        activation = self.hidden_layer_1(concat)\n        return self.output_layer(activation)","0825e142":"class Message_Agg(tf.keras.layers.Layer):\n    def __init__(self):\n        super(Message_Agg, self).__init__()\n    \n    def call(self, messages):\n        return tf.math.reduce_sum(messages, 2)","566c1ea5":"class Update_Func_1(tf.keras.layers.Layer):\n    def __init__(self, intermediate_dim, state_dim):\n        super(Update_Func_1, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate()\n        self.hidden_layer_1  = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.output_layer = tf.keras.layers.Dense(units=state_dim, activation =  tf.nn.relu)\n\n        \n    def call(self, old_state, agg_messages):\n        concat = self.concat_layer([old_state, agg_messages])\n        activation = self.hidden_layer_1(concat)\n        return self.output_layer(activation)\n\n","bcdde450":"class Adj_Updater_1(tf.keras.layers.Layer):\n    def __init__(self, intermediate_dim, state_dim):\n        super(Adj_Updater_1, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate()\n        self.hidden_layer_1 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.output_layer = tf.keras.layers.Dense(units=state_dim, activation = tf.nn.relu)\n\n    def call(self, node_i, node_j, edge_ij):\n        concat = self.concat_layer([node_i, node_j, edge_ij])\n        activation = self.hidden_layer_1(concat)\n        return self.output_layer(activation)","31a09b46":"class Edge_Regressor(tf.keras.layers.Layer):\n    def __init__(self, intermediate_dim):\n        super(Edge_Regressor, self).__init__()\n        self.concat_layer = tf.keras.layers.Concatenate()\n        self.hidden_layer_1 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.hidden_layer_2 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.hidden_layer_3 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n        self.output_layer = tf.keras.layers.Dense(units=1)#, activation=tf.nn.tanh)\n\n        \n    def call(self, edges):\n            \n        activation_1 = self.hidden_layer_1(edges)\n        activation_2 = self.hidden_layer_2(activation_1)\n        activation_3 = self.hidden_layer_3(activation_2)\n\n        return self.output_layer(activation_3)\n","0e9c0247":"class MP_Layer(tf.keras.layers.Layer):\n    def __init__(self, mp_int_dim, up_int_dim, out_int_dim, state_dim):\n        super(MP_Layer, self).__init__(self)\n        self.state_dim = state_dim  \n        self.message_passers_1  = Message_Passer_1(intermediate_dim = mp_int_dim, state_dim = state_dim) \n        self.update_functions_1 = Update_Func_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        self.adj_updaters_1     = Adj_Updater_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        self.message_passers_2  = Message_Passer_1(intermediate_dim = mp_int_dim, state_dim = state_dim) \n        self.update_functions_2 = Update_Func_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        self.adj_updaters_2     = Adj_Updater_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        self.message_aggs    = Message_Agg()   \n        self.batch_norm_n_1 = tf.keras.layers.BatchNormalization() \n        self.batch_norm_e_1 = tf.keras.layers.BatchNormalization() \n        self.batch_norm_e_2 = tf.keras.layers.BatchNormalization()\n        self.batch_norm_n_2 = tf.keras.layers.BatchNormalization()\n\n        \n    def call(self, nodes_branch_1, edges_branch_1, nodes_branch_2, edges_branch_2, mask):\n        \n        nodes_1          = nodes_branch_1\n        edges_1          = edges_branch_1\n        \n        n_nodes  = tf.shape(nodes_1)[1]\n        node_dim = tf.shape(nodes_1)[2]\n        \n        state_i_1 = tf.tile(nodes_1, [1, n_nodes, 1])\n        state_j_1 = tf.reshape(tf.tile(nodes_1, [1, 1, n_nodes]),[-1,n_nodes*n_nodes, node_dim ])\n\n        new_edges_1 = self.adj_updaters_1(state_i_1, state_j_1, edges_1)\n        new_edges_1 = tf.math.multiply(new_edges_1, mask)\n        \n        nodes_2          = nodes_branch_2\n        edges_2          = edges_branch_2\n        \n        n_nodes  = tf.shape(nodes_2)[1]\n        node_dim = tf.shape(nodes_2)[2]\n        \n        state_i_2 = tf.tile(nodes_2, [1, n_nodes, 1])\n        state_j_2 = tf.reshape(tf.tile(nodes_2, [1, 1, n_nodes]),[-1,n_nodes*n_nodes, node_dim ])\n\n        new_edges_2 = self.adj_updaters_2(state_i_2, state_j_2, edges_2)\n        new_edges_2 = tf.math.multiply(new_edges_2, mask)\n        \n        new_edges_1 = tf.math.multiply(new_edges_1, tf.math.sigmoid(new_edges_2))\n\n        \n        messages_1  = self.message_passers_1(state_i_1, state_j_1, new_edges_1)\n        #Do this to ignore messages from non-existant nodes\n        masked_1 =  tf.math.multiply(messages_1, mask)\n        masked_1 = tf.reshape(masked_1, [tf.shape(messages_1)[0], tf.shape(nodes_1)[1], tf.shape(nodes_1)[1], tf.shape(messages_1)[2]])\n        agg_m_1 = self.message_aggs(masked_1)\n        \n        # Update states\n        state_1 = self.update_functions_1(nodes_1, agg_m_1)\n        \n       \n\n        \n        messages_2  = self.message_passers_2(state_i_2, state_j_2, new_edges_2)\n        #Do this to ignore messages from non-existant nodes\n        masked_2 =  tf.math.multiply(messages_2, mask)\n        masked_2 = tf.reshape(masked_2, [tf.shape(messages_2)[0], tf.shape(nodes_2)[1], tf.shape(nodes_2)[1], tf.shape(messages_2)[2]])\n        agg_m_2 = self.message_aggs(masked_2)\n        \n        # Update states\n        state_2 = self.update_functions_2(nodes_2, agg_m_2)\n        \n#         # Update state 1 with state 2 value\n#         state_1 = tf.math.multiply(state_1, tf.math.sigmoid(state_2))\n      \n        # Batch norm and output\n        nodes_out_1 = self.batch_norm_n_1(state_1)\n        edges_out_1 = self.batch_norm_e_1(new_edges_1)    \n        nodes_out_2 = self.batch_norm_n_2(state_2)\n        edges_out_2 = self.batch_norm_e_2(new_edges_2)    \n\n        return nodes_out_1, edges_out_1, nodes_out_2, edges_out_2","f9327531":"class MP_Layer_edge_only(tf.keras.layers.Layer):\n    def __init__(self, mp_int_dim, up_int_dim, out_int_dim, state_dim):\n        super(MP_Layer_edge_only, self).__init__(self)\n        self.adj_updaters_1     = Adj_Updater_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        self.adj_updaters_2     = Adj_Updater_1(intermediate_dim = up_int_dim, state_dim = state_dim)\n        \n        self.message_aggs    = Message_Agg()\n        self.state_dim = state_dim         \n\n        \n    def call(self, nodes_branch_1, edges_branch_1, nodes_branch_2, edges_branch_2, mask):\n        \n        nodes_1          = nodes_branch_1\n        edges_1          = edges_branch_1\n        \n        n_nodes  = tf.shape(nodes_1)[1]\n        node_dim = tf.shape(nodes_1)[2]\n        \n        state_i_1 = tf.tile(nodes_1, [1, n_nodes, 1])\n        state_j_1 = tf.reshape(tf.tile(nodes_1, [1, 1, n_nodes]),[-1,n_nodes*n_nodes, node_dim ])\n\n        new_edges_1 = self.adj_updaters_1(state_i_1, state_j_1, edges_1)\n        new_edges_1 = tf.math.multiply(new_edges_1, mask)\n        \n        nodes_2          = nodes_branch_2\n        edges_2          = edges_branch_2\n        \n        n_nodes  = tf.shape(nodes_2)[1]\n        node_dim = tf.shape(nodes_2)[2]\n        \n        state_i_2 = tf.tile(nodes_2, [1, n_nodes, 1])\n        state_j_2 = tf.reshape(tf.tile(nodes_2, [1, 1, n_nodes]),[-1,n_nodes*n_nodes, node_dim ])\n\n        new_edges_2 = self.adj_updaters_2(state_i_2, state_j_2, edges_2)\n        new_edges_2 = tf.math.multiply(new_edges_2, mask)\n        \n        new_edges_1 = tf.math.multiply(new_edges_1, tf.math.sigmoid(new_edges_2))\n\n        return new_edges_1, new_edges_2","8a5ec68a":"# Define the MPNN here using the parts defined earlier\nadj_input = tf.keras.Input(shape=(None,), name='adj_input')\nnod_input = tf.keras.Input(shape=(None,), name='nod_input')\nclass MPNN(tf.keras.Model):\n    def __init__(self, mp_int_dim, up_int_dim, out_int_dim, state_dim, T):\n        super(MPNN, self).__init__(self)        \n        self.MP = [MP_Layer(mp_int_dim, up_int_dim, out_int_dim, state_dim) for _ in range(T)]        \n        self.MP_edge = MP_Layer_edge_only(mp_int_dim, up_int_dim, out_int_dim, state_dim) \n        self.embed_node = tf.keras.layers.Dense(units=state_dim, activation=tf.nn.relu)\n        self.embed_edge = tf.keras.layers.Dense(units=state_dim, activation=tf.nn.relu)        \n        self.edge_regressor  = Edge_Regressor(mp_int_dim)\n        \n    def call(self, inputs =  [adj_input, nod_input]):\n      \n      \n        nodes            = inputs['nod_input']\n        edges            = inputs['adj_input']\n        \n#         print(nodes)\n        \n        edges_0    = edges\n\n        len_edges = tf.shape(edges)[-1]\n        \n        _, x = tf.split(edges, [len_edges -1, 1], 2)\n\n        mask =  tf.where(tf.equal(x, 0), x, tf.ones_like(x))\n\n        nodes = tf.cast(nodes, dtype=tf.float32)\n        edges = tf.cast(edges, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        \n        nodes_1 = self.embed_node(nodes) \n        edges_1 = self.embed_edge(edges)\n        nodes_2 = nodes_1\n        edges_2 = edges_1\n\n        nodes_1_ = nodes_1\n        edges_1_ = edges_1\n        nodes_2_ = nodes_2\n        edges_2_ = edges_2\n              \n        for i, mp in enumerate(self.MP):\n            index = i + 1\n            if index%2 == 0:\n                nodes_1, edges_1, nodes_2, edges_2 =  mp(nodes_1, edges_1, nodes_2, edges_2, mask)\n                nodes_1 = nodes_1 - nodes_1_\n                edges_1 = edges_1 - edges_1_\n                nodes_2 = nodes_2 - nodes_2_\n                edges_2 = edges_2 - edges_2_\n                nodes_1_ = nodes_1\n                edges_1_ = edges_1\n                nodes_2_ = nodes_2\n                edges_2_ = edges_2             \n            else:\n                nodes_1, edges_1, nodes_2, edges_2 =  mp(nodes_1, edges_1, nodes_2, edges_2, mask)\n            \n        \n        edges_1, edges_2 = self.MP_edge(nodes_1, edges_1, nodes_2, edges_2, mask)\n        \n        con_edges = self.edge_regressor(edges_1)\n    \n        return con_edges","eca24dc6":"def log_mae(orig , preds):\n \n    # Mask values for which no scalar coupling exists\n    mask  = tf.where(tf.equal(orig, 0), orig, tf.ones_like(orig))\n\n    nums  = tf.boolean_mask(orig,  mask)\n    preds = tf.boolean_mask(preds,  mask)\n\n    reconstruction_error = tf.math.log(tf.reduce_mean(tf.abs(tf.subtract(nums, preds))))\n\n    return reconstruction_error","180a48c3":"learning_rate = 0.001\ndef warmup(epoch):\n    initial_lrate = learning_rate   \n    if epoch == 0:\n        lrate = 0.00001\n    if epoch == 1:\n        lrate = 0.0001\n    if epoch > 1:\n        lrate = 0.001   \n    if epoch > 20:\n        lrate = 0.0001\n    if epoch > 25:\n        lrate = 0.00001\n        \n    tf.print(\"Learning rate: \", lrate)\n    return lrate\n\nlrate = tf.keras.callbacks.LearningRateScheduler(warmup)\n\n\nopt = tf.optimizers.Adam(learning_rate=learning_rate)\n","8a30aacb":"mpnn = MPNN(mp_int_dim = 512, up_int_dim = 1024, out_int_dim = 512, state_dim = 256, T = 3)\n#mpnn = MPNN(mp_int_dim = 128, up_int_dim = 128, out_int_dim = 256, state_dim = 64, T = 5)\n\nmpnn.compile(opt, log_mae)\n# mpnn.summary()","4d1282c3":"batch_size = 64\nepochs = 30\n","399d0894":"# Wrap in a function so that memory is freed after calling\ndef train():\n    nodes_train     = np.load(datadir + \"internalgraphdata\/nodes_train.npz\" )['arr_0']\n    in_edges_train  = np.load(datadir + \"internalgraphdata\/in_edges_train.npz\")['arr_0']\n    out_edges_train = np.load(datadir + \"internalgraphdata\/out_edges_train.npz\" )['arr_0']\n\n    out_labels = out_edges_train.reshape(-1,out_edges_train.shape[1]*out_edges_train.shape[2],1)\n    in_edges_train = in_edges_train.reshape(-1,in_edges_train.shape[1]*in_edges_train.shape[2],in_edges_train.shape[3])\n\n\n    train_size = int(len(out_labels)*0.8)\n\n    mpnn.call({'adj_input' : in_edges_train[:10], 'nod_input': nodes_train[:10]})\n    \n#     mpnn.load_weights(datadir + \"\/basicmodelweights\/mymodel.h5\")\n\n    history = mpnn.fit({'adj_input' : in_edges_train[:train_size], 'nod_input': nodes_train[:train_size]}, y = out_labels[:train_size], batch_size = batch_size, epochs = epochs, \n             callbacks = [lrate], use_multiprocessing = True, initial_epoch = 0, verbose = 2, \n             validation_data = ({'adj_input' : in_edges_train[train_size:], 'nod_input': nodes_train[train_size:]},out_labels[train_size:]) )\n    \n    preds = mpnn.predict({'adj_input' : in_edges_train[train_size:], 'nod_input': nodes_train[train_size:]}, verbose = 0)\n    \n    return preds, train_size, history","81135bfc":"%%time\npreds, train_size, history = train()","40fda9af":"\nmpnn.save_weights(\"model.h5\")","743a94c5":"# list all data in history\nprint(history.history.keys())\n\n# plt.plot(history.history['lr'])\n# plt.title('learning rate')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","43b0d27e":"with open('\/trainHistoryDict.pkl', 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)","1fa11627":"train = pd.read_csv(datadir + \"champs-scalar-coupling\/train.csv\")\ntest = pd.read_csv(datadir + \"champs-scalar-coupling\/test.csv\")\n\ntrain_mol_names = train['molecule_name'].unique()\n\nval = train[train.molecule_name.isin(train_mol_names[train_size:])]\nval_group = val.groupby('molecule_name')","8b115954":"def make_outs(test_group, preds):\n    i = 0\n    x = np.array([])\n    for test_gp, preds in zip(test_group, preds):\n        if (not i%1000):\n            print(i)\n\n        gp = test_gp[1]\n        \n        x = np.append(x, (preds[gp['atom_index_0'].values, gp['atom_index_1'].values] + preds[gp['atom_index_1'].values, gp['atom_index_0'].values])\/2.0)\n        \n        i = i+1\n    return x\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\n    Code is from this kernel: https:\/\/www.kaggle.com\/uberkinder\/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()","810cd1fa":"max_size = 29\npreds = preds.reshape((-1,max_size, max_size))\nout_unscaled = make_outs(val_group, preds)","f9d101af":"val['pred_scalar_coupling_constant'] = out_unscaled\n\n\ncoups_to_isolate = ['1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN']\nfor i, coup in enumerate(coups_to_isolate):\n    \n    \n    scale_min = train['scalar_coupling_constant'].loc[train.type == coup].min()\n    scale_max = train['scalar_coupling_constant'].loc[train.type == coup].max()\n    scale_mid = (scale_max + scale_min)\/2\n    scale_norm = scale_max - scale_mid\n\n    val.loc[val.type == coup, 'pred_scalar_coupling_constant'] = val['pred_scalar_coupling_constant'].loc[val.type == coup]*scale_norm + scale_mid\n\n    \n    val.loc[val.type == coup, 'pred_scalar_coupling_constant'] = val['pred_scalar_coupling_constant'].loc[val.type == coup]","64075d42":"for coup in coups_to_isolate:\n    log_mae = group_mean_log_mae(val['scalar_coupling_constant'], val['pred_scalar_coupling_constant'], val['type'][val.type == coup])\n    print(coup,\"\\t\", log_mae)\n    \ntotal = group_mean_log_mae(val['scalar_coupling_constant'], val['pred_scalar_coupling_constant'], val['type'])\nprint(\"\")\nprint(\"Total:\",\"\\t\", total)","d97a10ec":"nodes_test     = np.load(datadir + \"internalgraphdata\/nodes_test.npz\" )['arr_0']\nin_edges_test  = np.load(datadir + \"internalgraphdata\/in_edges_test.npz\")['arr_0']\nin_edges_test  = in_edges_test.reshape(-1,in_edges_test.shape[1]*in_edges_test.shape[2],in_edges_test.shape[3])","e8529783":"preds = mpnn.predict({'adj_input' : in_edges_test, 'nod_input': nodes_test}, verbose=1)","70284483":"np.save(\"preds_kernel.npy\" , preds)","b848d060":"test_group = test.groupby('molecule_name')","902969e0":"preds = preds.reshape((-1,max_size, max_size))\nout_unscaled = make_outs(test_group, preds)","d96b18b6":"test['scalar_coupling_constant'] = out_unscaled\n\ncoups_to_isolate = ['1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN']\nfor i, coup in enumerate(coups_to_isolate):\n    \n    \n    scale_min = train['scalar_coupling_constant'].loc[train.type == coup].min()\n    scale_max = train['scalar_coupling_constant'].loc[train.type == coup].max()\n    scale_mid = (scale_max + scale_min)\/2\n    scale_norm = scale_max - scale_mid\n\n    test.loc[test.type == coup, 'scalar_coupling_constant'] = test['scalar_coupling_constant'].loc[test.type == coup]*scale_norm + scale_mid\n\n    \n    test.loc[test.type == coup, 'pred_scalar_coupling_constant'] = test['scalar_coupling_constant'].loc[test.type == coup]\n\n\n\n","70cf1053":"test[['id','scalar_coupling_constant']].to_csv('submission.csv', index=False)","b36b0fbc":"test[['id','scalar_coupling_constant']].head()","e46163ca":"## Define an edge only version of the MPL to do a final edge update. ","02a765dd":"## Predict on val set","54e07f86":"## Aggregator\n\nDefine the message aggregator (just sum)  \nProbably overkill to have it as its own layer, but good if you want to replace it with something more complex\n","316c5d4f":"## Node Update function\n\nThe node update function is an MLP that takes $[old\\_node, agg\\_messages]$ as input and return the new node value","8fac3959":"## Message Passing Neural Network\n\nSo, as many of you might have surmised by now the dataset for this challenge is essentially the QM9 dataset with some new values calculated for it. \n\nThe first thing I though of when seeing this challenge was the [Gilmer paper](https:\/\/arxiv.org\/abs\/1704.01212), as it uses the QM9 dataset. ([see this talk](https:\/\/vimeo.com\/238221016))\n\nThe major difference in this challenge is that we are asked to calulate bond properties (thus edges in a graph) as opposed to bulk properties in the paper. \n\nHere the model is laid out in a modular way so the parts can easily be replaced\n","bfc270ac":"## Show history of training","603468e2":"## Put it all together to form a MPNN\n\nDefines the full mpnn that does T message passing steps, where T is a hyperparameter.   \nHere each layer has it's own weights, but weights can be shared across layers. ","ce586a6f":"## Finally create the model, and compile","c9022501":"## Let the learning begin!","3e65dfa2":"## Save history of training","60e861e3":"## Define the loss functions. \n\n(**note**: that for LMAE, as the output values have been scaled down values will be much smaller than for unscaled values)","b03bee0a":"# Prediction done!\n\nNow rescale outputs and create submission.csv","ded62b9e":"## Predict on the test set","a1a3b51a":"## Message passing layer\n\nPut all of the above together to make a message passing layer which does one round of message passing and node updating","ecec4e36":"## Message passer\n\nThe message passer here is a MLP that takes $concat([node_i, edge_{ij}, node_j])$ as input and returns a message of the same dimension of the node","5e4159c2":"## Edge update \n\nThe edge update function is a MLP that takes $concat([node_i, edge_{ij}, node_j])$ as input and produces a new edge value","f77ef216":"## Output layer\n\nThis is where the model diverges with the paper.   \nAs the paper predicts bulk properties, but we are interested in edges, we need something different.   \n\nHere the each edge is passed through a MLP which is used to regress the scalar coupling for each edge","b4249310":"## Define some callbacks, the initial learning rate and the optimizer","b7f1c99c":"Define some hyperparameters"}}