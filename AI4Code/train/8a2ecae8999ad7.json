{"cell_type":{"194bd312":"code","a1fe722e":"code","2125da12":"code","95acd3fe":"code","9401dfcd":"code","c108845e":"code","9627e0c2":"code","624f9ff1":"code","605b78fe":"code","4a509c96":"code","9b276380":"code","0feccd32":"code","f7e476d6":"code","d3d81ffd":"code","70f057d3":"code","e80e0b0c":"code","9a999d6f":"code","aff780a1":"code","5d0fd361":"code","03395584":"code","da533bda":"code","f526f856":"code","7022d454":"code","98f5d439":"code","f70ae76b":"code","491c625a":"code","c0b8710a":"code","33f54a1a":"code","09de263e":"code","4a8bfc50":"code","b06baf6f":"code","ea73e201":"code","58a2b08d":"markdown","7faaf69b":"markdown","9a8a3942":"markdown","d745fea6":"markdown","7f2857d7":"markdown","858230f6":"markdown","a956c8e0":"markdown","299d0f63":"markdown","6fe2ca96":"markdown","37e13067":"markdown","ac608ef0":"markdown","5f57be44":"markdown","e6fa4b64":"markdown","d0a7f663":"markdown","27031ab9":"markdown","e5cf42e5":"markdown","3c39d24e":"markdown","5c62249c":"markdown","c43b352c":"markdown","575ea1ec":"markdown","a25cc955":"markdown","4edb74e2":"markdown","36d2f60f":"markdown","900f4da9":"markdown","bc3b243c":"markdown","434e3834":"markdown","d54c6732":"markdown","a24a27b6":"markdown","3eb912de":"markdown","abd031e2":"markdown","981d0baa":"markdown","4e767f50":"markdown","cb405035":"markdown","3e67a11e":"markdown","3cbf9d44":"markdown","3e35d06f":"markdown","560e92d4":"markdown","3c273569":"markdown","51e73d83":"markdown","a5cb7123":"markdown","ef924ba9":"markdown","44fccee9":"markdown","4a26446d":"markdown","73e80948":"markdown","17982b4e":"markdown","500e0877":"markdown","6e00d754":"markdown"},"source":{"194bd312":"import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # built in dataloader\nfrom tqdm import tqdm\nimport os\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint # callbacks\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc\nimport ipywidgets as widgets\nimport io\nfrom PIL import Image\nfrom IPython.display import display,clear_output\nfrom warnings import filterwarnings\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1fe722e":"colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\n\n# seaborn useful for statistical graphs, is based on matplotlib\nsns.palplot(colors_dark)\nsns.palplot(colors_green)\nsns.palplot(colors_red)","2125da12":"labels_first_dataset = ['glioma_tumor','no_tumor','meningioma_tumor','pituitary_tumor']\nlabels = ['no_tumor', 'tumor']","95acd3fe":"X_train1 = []\ny_train1 = []\nimage_size = 150\ntraining_folder = os.path.join('..\/input\/brain-tumor-classification-mri','Training')\ntesting_folder = os.path.join('..\/input\/brain-tumor-classification-mri','Testing')\nfor i in labels_first_dataset:\n    folderPath = os.path.join(training_folder,i)\n    # tqdm shows a progress meter in loops\n    for j in tqdm(os.listdir(folderPath)): \n        img = cv2.imread(os.path.join(folderPath,j)) # read the image, if there isn't an image return an empty matrix\n        # all images resized between 150 and 150\n        img = cv2.resize(img,(image_size, image_size))\n        X_train1.append(img) # training data\n        \n        ## only tumor or no tumor\n        if i == labels[0]: # labels[0] == no_tumor\n            y_train1.append(i) # no_tumor\n        else:\n            y_train1.append(labels[1]) # tumor\n                \n\nfor i in labels_first_dataset:\n    folderPath = os.path.join(testing_folder,i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j)) \n        img = cv2.resize(img,(image_size,image_size))\n        X_train1.append(img)\n        \n        ## only tumor or no tumor\n        if i == labels[0]: # labels[0] == no_tumor\n            y_train1.append(i) # no_tumor\n        else:\n            y_train1.append(labels[1]) # tumor","9401dfcd":"X_test1 = []\ny_test1 = []\nimage_size = 150\nlabels_idx = ['no','yes'] # for the new dataset\nfor i in labels_idx:\n    folderPath = os.path.join('..\/input\/new-dataset-yes-no',i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        # all images resized between 150 and 150\n        img = cv2.resize(img,(image_size, image_size))\n        X_test1.append(img) # training data\n        \n        ## only tumor or no tumor\n        if i == labels_idx[0]: # labels_new[0] == no_tumor\n            y_test1.append(labels[0]) # no_tumor\n        else:\n            y_test1.append(labels[1]) # tumor\n                ","c108845e":"len_labels = np.zeros(shape=2)\nfor label in y_train1:\n    for label_ty in range(len(labels)):\n        if label==labels[label_ty]:\n            len_labels[label_ty]=len_labels[label_ty]+1\n\nprint(\"Number of images in the training set with \\nNo_tumor: \",int(len_labels[0]),'\\nTumor: ', int(len_labels[1]))","9627e0c2":"var_test=np.zeros(shape=2)\nfor i in y_test1:\n    if i==labels[0]:\n        var_test[0]+=1\n    elif i==labels[1]:\n        var_test[1]+=1\n\nprint(\"Number of images in the test set with \\nNo_tumor: \",int(var_test[0]),'\\nTumor: ', int(var_test[1]))","624f9ff1":"# create arrays, numpy objects\nX_train1 = np.array(X_train1)\ny_train1 = np.array(y_train1)\nX_test1 = np.array(X_test1)\ny_test1 = np.array(y_test1)","605b78fe":"print(\"Training set for task 1 dimensionality: \", X_train1.shape)\nprint(\"Test set for task 1 dimensionality: \",X_test1.shape)","4a509c96":"k=0\nfig, ax = plt.subplots(1,2,figsize=(10,10)) \nfig.text(s='Sample Image From Each Label',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.72,x=0.3,alpha=0.8)\nfor i in labels:\n    j=0\n    while True :\n        if y_train1[j]==i:\n            ax[k].imshow(X_train1[j])\n            ax[k].set_title(y_train1[j])\n            ax[k].axis('off')\n            k+=1 \n            break\n        j+=1","9b276380":"X_train1, y_train1 = shuffle(X_train1,y_train1, random_state=101)\n\nX_test1, y_test1 = shuffle(X_test1,y_test1, random_state=101)\n# suffle randomly training and test set","0feccd32":"X_train2,X_test2,y_train2,y_test2 = train_test_split(X_train1,y_train1, test_size=0.1,random_state=101)","f7e476d6":"print(\"Dimensionality of the first training set: \",X_train1.shape)\nprint(\"Dimensionality of the second training set: \",X_train2.shape)","d3d81ffd":"# first task\ny_train_new1 = []\nfor i in y_train1: # labels\n    y_train_new1.append(labels.index(i))\ny_train1 = y_train_new1\ny_train1 = tf.keras.utils.to_categorical(y_train1)\n\ny_test_new1 = []\nfor i in y_test1:\n    y_test_new1.append(labels.index(i))\ny_test1 = y_test_new1\ny_test1 = tf.keras.utils.to_categorical(y_test1)\n\n# second task\ny_train_new2 = []\nfor i in y_train2: # labels\n    y_train_new2.append(labels.index(i))\ny_train2 = y_train_new2\ny_train2 = tf.keras.utils.to_categorical(y_train2)\n\n\ny_test_new2 = []\nfor i in y_test2:\n    y_test_new2.append(labels.index(i))\ny_test2 = y_test_new2\ny_test2 = tf.keras.utils.to_categorical(y_test2)\n","70f057d3":"effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))","e80e0b0c":"model = effnet.output\nmodel = tf.keras.layers.GlobalAveragePooling2D()(model)\nmodel = tf.keras.layers.Dropout(rate=0.5)(model)\nmodel = tf.keras.layers.Dense(2,activation='sigmoid')(model)\nmodel = tf.keras.models.Model(inputs=effnet.input, outputs = model)\n# the last layers, dense layers, are aliminated and substituted by these, just 3 layers are added\nlen(model.layers)","9a999d6f":"model.summary()","aff780a1":"model.compile(loss='binary_crossentropy',optimizer = 'Adam', metrics= ['accuracy'])","5d0fd361":"tensorboard = TensorBoard(log_dir = 'logs')\n# TensorBoard is a visualization tool provided with TensorFlow.\ncheckpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\n# Checpoint saves only the model that reaches the best accuracy\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n                              mode='auto',verbose=1)\n# Reduce learning rate when a metric has stopped improving: new_lr = lr * factor.\n# patience--> number of epochs with no improvement after which learning rate will be reduced.\n# min_delta--> threshold for measuring the new optimum","03395584":"history1 = model.fit(X_train1,y_train1,validation_split=0.1, epochs =12, verbose=1, batch_size=32,\n                   callbacks=[tensorboard,checkpoint,reduce_lr])","da533bda":"history2 = model.fit(X_train2,y_train2,validation_split=0.1, epochs =12, verbose=1, batch_size=32,\n                   callbacks=[tensorboard,checkpoint,reduce_lr])","f526f856":"filterwarnings('ignore')\n\nepochs = [i for i in range(12)]\nfig, ax = plt.subplots(2,2,figsize=(14,7))\ntrain_acc1 = history1.history['accuracy']\ntrain_loss1 = history1.history['loss']\nval_acc1 = history1.history['val_accuracy']\nval_loss1 = history1.history['val_loss']\ntrain_acc2 = history2.history['accuracy']\ntrain_loss2 = history2.history['loss']\nval_acc2 = history2.history['val_accuracy']\nval_loss2 = history2.history['val_loss']\n\nfig.text(s='Epochs vs. Training and Validation Accuracy\/Loss',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0, 0].plot(epochs, train_acc1, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Accuracy first task')\nax[0, 0].plot(epochs, val_acc1, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Accuracy first task')\nax[0, 0].legend(frameon=False)\nax[0, 0].set_xlabel('Epochs')\nax[0, 0].set_ylabel('Accuracy')\n\nsns.despine()\nax[0, 1].plot(epochs, train_loss1, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss first task')\nax[0, 1].plot(epochs, val_loss1, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Loss first task')\nax[0, 1].legend(frameon=False)\nax[0, 1].set_xlabel('Epochs')\nax[0, 1].set_ylabel('Training & Validation Loss')\n\nsns.despine()\nax[1, 0].plot(epochs, train_acc2, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Accuracy second task')\nax[1, 0].plot(epochs, val_acc2, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Accuracy second task')\nax[1, 0].legend(frameon=False)\nax[1, 0].set_xlabel('Epochs')\nax[1, 0].set_ylabel('Accuracy')\n\nsns.despine()\nax[1, 1].plot(epochs, train_loss2, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss second task')\nax[1, 1].plot(epochs, val_loss2, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Loss second task')\nax[1, 1].legend(frameon=False)\nax[1, 1].set_xlabel('Epochs')\nax[1, 1].set_ylabel('Training & Validation Loss')\n\nfig.show()","7022d454":"# last convolutional layer output of the network\nlast_conv_layer_name = \"top_activation\"","98f5d439":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    img_array = np.expand_dims(img_array, axis=0)\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef get_gradcam(img, heatmap, alpha=0.99):\n    \n    \n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    \n    return superimposed_img","f70ae76b":"# first task, 2 datasets\npred1 = model.predict(X_test1)\npred1 = np.argmax(pred1,axis=1)\ny_test_new1 = np.argmax(y_test1,axis=1)\n\n# second task, 1 dataset\npred2 = model.predict(X_test2)\npred2 = np.argmax(pred2,axis=1)\ny_test_new2 = np.argmax(y_test2,axis=1)","491c625a":"indices1 = [i for i in range(len(y_test_new1)) if y_test_new1[i] != pred1[i]]\nwrong_pred1 = X_test1[indices1,:,:,:]\n\n# plot of wrong predictions\nn = wrong_pred1.shape[0]\nncol1 = 4 # chosen by us\nif n%ncol1 == 0:\n    nrow1 = n\/ncol1\nelse:\n    nrow1 = round(n\/ncol1)+1 \n\nfig = plt.figure(figsize=(20,20))\nax= []\nfor i in range(n):\n    ax.append(fig.add_subplot(nrow1, ncol1, i+1) )\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred1[indices1[i]]], true=labels[y_test_new1[indices1[i]]]))\n    ax[i].axis('off')\n    plt.imshow(wrong_pred1[i])\nfig.tight_layout(pad=1.0)","c0b8710a":"n1 = wrong_pred1.shape[0]\nfig = plt.figure(figsize=(20,20))\nax=[]\nfor i in range(n1):\n    heatmap = make_gradcam_heatmap(wrong_pred1[i], model, last_conv_layer_name)\n    superimposed_img = get_gradcam(wrong_pred1[i], heatmap)\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    \n    # now that we have obtained the GradCam image, plot it\n    ax.append(fig.add_subplot(nrow1, ncol1, i+1) )\n    sns.despine()\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred1[indices1[i]]], true=labels[y_test_new1[indices1[i]]]))\n    ax[i].axis('off')\n    plt.imshow(superimposed_img)\n    fig.tight_layout(pad=1.0)","33f54a1a":"indices2 = [i for i in range(len(y_test_new2)) if y_test_new2[i] != pred2[i]]\nwrong_pred2 = X_test2[indices2,:,:,:]\n\n# plot of wrong predictions\nn = wrong_pred2.shape[0]\nncol2 = 4 # chosen by us\nif n%ncol2 == 0:\n    nrow2 = n\/ncol2\nelse:\n    nrow2 = round(n\/ncol2)+1 \n\nfig = plt.figure(figsize=(15,15))\nax= []\nfor i in range(n):\n    ax.append(fig.add_subplot(nrow2, ncol2, i+1) )\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred2[indices2[i]]], true=labels[y_test_new2[indices2[i]]]))\n    ax[i].axis('off')\n    plt.imshow(wrong_pred2[i])\nfig.tight_layout(pad=1.0)","09de263e":"n2 = wrong_pred2.shape[0]\nfig = plt.figure(figsize=(20,20))\nax=[]\nfor i in range(n2):\n    heatmap = make_gradcam_heatmap(wrong_pred2[i], model, last_conv_layer_name)\n    superimposed_img = get_gradcam(wrong_pred2[i], heatmap)\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    \n    # now that we have obtained the GradCam image, plot it\n    ax.append(fig.add_subplot(nrow2, ncol2, i+1) )\n    sns.despine()\n    ax[i].set_title('Pred={pred}, True={true}'.format(pred=labels[pred2[indices2[i]]], true=labels[y_test_new2[indices2[i]]]))\n    ax[i].axis('off')\n    plt.imshow(superimposed_img)\n    fig.tight_layout(pad=1.0)","4a8bfc50":"# plot ROC curve\n# task 1\n[fpr1, tpr1, _ ]= roc_curve(y_test_new1, pred1)\nroc_auc1 = auc(fpr1, tpr1)\n# task 2\n[fpr2, tpr2, _ ]= roc_curve(y_test_new2, pred2)\nroc_auc2 = auc(fpr2, tpr2)\nfig, ax = plt.subplots(1,2, figsize=(10,7))\nlw = 2\nax[0].plot(fpr1, tpr1, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc1)\nax[0].plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nax[0].set_xlim([0.0, 1.0])\nax[0].set_ylim([0.0, 1.05])\nax[0].set_xlabel('False Positive Rate')\nax[0].set_ylabel('True Positive Rate')\nax[0].set_title('ROC curve task 1')\nax[0].legend(loc=\"lower right\")\n\nax[1].plot(fpr2, tpr2, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc2)\nax[1].plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nax[1].set_xlim([0.0, 1.0])\nax[1].set_ylim([0.0, 1.05])\nax[1].set_xlabel('False Positive Rate')\nax[1].set_ylabel('True Positive Rate')\nax[1].set_title('ROC curve task 2')\nax[1].legend(loc=\"lower right\")\n\nfig.show()","b06baf6f":"print(classification_report(y_test_new1,pred1))\nprint(classification_report(y_test_new2,pred2))","ea73e201":"fig,(ax1,ax2) =plt.subplots(1,2,figsize=(14,7))\ng1 = sns.heatmap(confusion_matrix(y_test_new1,pred1),ax=ax1,xticklabels=labels,yticklabels=labels,annot=True,\n           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\nax1.set_title('Heatmap of the Confusion Matrix, task 1')\n\ng2 = sns.heatmap(confusion_matrix(y_test_new2,pred2),ax=ax2,xticklabels=labels,yticklabels=labels,annot=True,\n           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\nax2.set_title('Heatmap of the Confusion Matrix, task 2')\n\nplt.show()","58a2b08d":"# Evaluation","7faaf69b":"*Training of the model for the second task*","9a8a3942":"Apply **Grad-CAM algorithm** to the **wrong classified images** for task 2","d745fea6":"---","7f2857d7":"We finally compile our model using binary_crossentropy loss, different from the other case because here we have a *binary classification problem*.\n\nThe optimizer used is *Adam* and in this case only *accuracy* is visualized.","858230f6":"## Grad-CAM functions","a956c8e0":"**GlobalAveragePooling2D** -> This layer acts similar to the Max Pooling layer in CNNs, the only difference being is that it uses the Average values instead of the Max value while pooling. This really helps in decreasing the computational load on the machine while training.\n\n**Dropout** -> This layer omits some of the neurons at each step from the layer making the neurons more independent from the neibouring neurons. It helps in avoiding overfitting. Neurons to be ommitted are selected at random. The rate parameter is the liklihood of a neuron activation being set to 0, thus dropping out the neuron\n\n**Dense** -> This is the output layer which classifies the image into 1 of the 2 possible classes. It uses the *sigmoid* function because we have just two classes","299d0f63":"# Data Preperation","6fe2ca96":"---","37e13067":"**Metrics visualization** for the two tasks","ac608ef0":"---","5f57be44":"Number of tumor\/no tumor images in the test set","e6fa4b64":"As before, we wanted to add the gradcam algorithm in order to understand better where the network focuses in order to classify images. In order to apply it, is necessary to know the architecture of the model, so that is possible to know the name of the last convolutional layer.","d0a7f663":"**Plot of wrong predicted images**, task 1","27031ab9":"In this case, <br>\n0 - No Tumor<br>\n1 - Tumor<br>","e5cf42e5":"# Transfer Learning","3c39d24e":"Performing **One Hot Encoding** on the labels after converting it into numerical values:","5c62249c":"# Grad-CAM","c43b352c":"___","575ea1ec":"First of all labels are created.\n\nThe first labels are referred to the dataset with different types of tumor, the second are the true labels that will be used in this study.","a25cc955":"# Importing Libraries","4edb74e2":"# Introduction\nIn this third and last notebook we used the same network and the same procedure for a simpler task: classification of tumor vs. no tumor images. The network has been slightly modified to adapt it to a binary classification problem: a binary cross-entropy loss is used and the last dense layer has a sigmoidal activation function.\nWe used this network for two tasks:\n\n1. **Task 1:** use original dataset (*brain-tumor-classification-mri*) as training set, and additional dataset (*new-dataset-yes-no*) as test set, to identify possible dipendence from data and overfitting\n\n2. **Task 2:** use only original dataset, split it into training and test set and evaluate performances","36d2f60f":"Let's see the dimensionality of the training set","900f4da9":"**Confusion matrixes**","bc3b243c":"Apply **Grad-CAM algorithm** to the **wrong classified images** for task 1","434e3834":"# Color","d54c6732":"Visualization of one image for each label","a24a27b6":"# Brain Tumor MRI Classification - THIRD PART\nDeep Learning applied to Neuroscience and Rehabilitation\n\nAA 2021\/22\n\n\nFranceschin Sarah, Grisi Caterina, Pozza Giacomo, Tonello Alessio, Viberti Andrea","3eb912de":"# Prediction","abd031e2":"Use some callbacks, in particular: *TensorBoard, ModelCheckpoint and ReduceLROnPlateau*","981d0baa":"---","4e767f50":"*argmax function* has been used because the output of the sigmoidal function is a value between 0 and 1 for each class. The maximum value is taken so that the class with highest probability is selected.<br>\nSo with *argmax*, we're able to find out the index associated with the predicted outcome.","cb405035":"Dimensionality of data used for classifying tumor vs. no tumor using the original dataset as training set and the new dataset as test set","3e67a11e":"---","3cbf9d44":"*Training of the model for the first task*","3e35d06f":"**Second dataset**\n\nUse another dataset as test set for the first task","560e92d4":"Plot of ROC curve using sklearn.metrics.auc and sklearn.metrics.roc_curve","3c273569":"Images are uploaded, resized and the training set is created","51e73d83":"Dimensionality is always the same.\n\nDividing the dataset into **Training** and **Testing** sets. Used for the second classification, using only the **original dataset**","a5cb7123":"---","ef924ba9":"---","44fccee9":"As already said, we've implemented Transfer Learning using the **EfficientNetB0** model which will use the weights from the **ImageNet** dataset.\n\nThe include_top parameter is set to *False* so that the network doesn't include the last layer from the pre-built model which allows us to add our own output layers","4a26446d":"**Plot of wrong predicted images**, task 2","73e80948":"Dimensionality of the two **training sets** used for the two different tasks","17982b4e":"---","500e0877":"# Training The Model","6e00d754":"---"}}