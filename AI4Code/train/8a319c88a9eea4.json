{"cell_type":{"5140de58":"code","28568797":"code","0367b792":"code","243fa589":"code","2462e8c0":"code","404f6c31":"code","bcb388c7":"code","bd242def":"code","3549de71":"code","b801fa1c":"code","61e707da":"code","d902aec9":"code","8c7c063a":"code","be1e5850":"markdown","4eccefb0":"markdown","5ab39f3b":"markdown","63ea77a6":"markdown","97a64f88":"markdown","0c401fae":"markdown","0bffb830":"markdown","4b42038d":"markdown","c2272aae":"markdown","639ca48d":"markdown","8fa2360d":"markdown","36f45de0":"markdown"},"source":{"5140de58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28568797":"# Import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0367b792":"# We want to see whole content (non-truncated)\npd.set_option('display.max_colwidth', None)\n\n# Load the train and the test set\ntrain = pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/train.csv\", encoding= 'unicode_escape')\ntest = pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/test.csv\", encoding= 'unicode_escape')\n\n# Display first five rows\ndisplay(train.head())\n\n# Descriptive Statistics\nprint(train.describe())\n\n# Info\nprint(train.info())","243fa589":"# We do not need the Unnamed: 0 column\ntrain.drop(\"Unnamed: 0\", axis=1, inplace=True)\n\n# Info\nprint(train.info())","2462e8c0":"# Get lengths\ntrain[\"length\"] = train[\"comment\"].str.len()\n\n# Get word counts\ntrain[\"word_count\"] = train[\"comment\"].str.split().apply(len)\n\n# Display the two columns\ndisplay(train[[\"length\",\"word_count\"]])\n\n# Look at the distribution of length and word_count\nsns.distplot(train[\"length\"], bins=10)\nplt.title(\"Distribution of comment lengths\")\nplt.show()\n\nsns.distplot(train[\"word_count\"], bins=10)\nplt.title(\"Distribution of word counts\")\nplt.show()\n\n\n# Print 10 bins of length column\nprint(pd.cut(train['length'], 10).value_counts())\n\n# Print 10 bins of word_count column\nprint(pd.cut(train['word_count'], 10).value_counts())\n","404f6c31":"# Print binned lengths by label\nprint(train['Label'].groupby(pd.cut(train['length'], 10)).mean())\n\n\n# Print binned lengths by label\nprint(train['Label'].groupby(pd.cut(train['length'], 10)).mean())","bcb388c7":"# Import necessary tools from nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Import Snowballstemmer for stemming Turkish words\nfrom snowballstemmer import TurkishStemmer\n\n# Initialize the stemmer\ntr_stemmer =TurkishStemmer()\n\ndef tokenize(comment):\n    \n    # Tokenize the comment\n    tokenized = word_tokenize(comment)\n    \n    # Remove the stopwords (in Turkish)\n    tokenized = [token for token in tokenized if token not in stopwords.words(\"turkish\")]\n    \n    # Stemming the tokens\n    tokenized = [tr_stemmer.stemWord(token) for token in tokenized]\n    \n    # Remove non-alphabetic characters\n    tokenized = [token for token in tokenized if token.isalpha()]\n    \n    return tokenized\n\n# Apply the function\ntrain[\"Tokenized\"] = train[\"comment\"].str.lower().apply(tokenize)\ntest[\"Tokenized\"] = test[\"comment\"].str.lower().apply(tokenize)\n\n# See the result\ndisplay(train[\"Tokenized\"].head())\n","bd242def":"# Import TfidfVectorizer from sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Convert tokenized words from list to string\ntrain['tokenized_str']=[\" \".join(token) for token in train['Tokenized'].values]\n\n# Initialize a Tf-idf Vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the vectorizer\ntfidf_matrix = vectorizer.fit_transform(train[\"tokenized_str\"])\n\n# Let's see what we have\ndisplay(tfidf_matrix)\n\n# Create a DataFrame for tf-idf vectors and display the first five rows\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns= vectorizer.get_feature_names())\n\n# Display the first five rows of the tfidf DataFrame\ndisplay(tfidf_df.head())","3549de71":"# Import wordcloud\nfrom wordcloud import WordCloud\n\n# Create a new DataFrame called frequencies\nfrequencies = pd.DataFrame(tfidf_matrix.sum(axis=0).T,index=vectorizer.get_feature_names(),columns=['total frequency'])\n\n# Sort the words by frequency\nfrequencies.sort_values(by='total frequency',ascending=False, inplace=True)\n\n# Display the most 20 frequent words\ndisplay(frequencies.head(20))\n\n# Join the indexes\nfrequent_words = \" \".join(frequencies.index)+\" \"\n\n# Initialize the word cloud\nwc = WordCloud(width = 500, height = 500, min_font_size = 10, max_words=2000, background_color ='white')\n\n# Generate a world cloud\nwc_general = wc.generate(frequent_words)\n\n# Plot the world cloud                     \nplt.figure(figsize = (10, 10), facecolor = None) \nplt.imshow(wc_general, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.title(\"Common words in the comments\")\nplt.tight_layout(pad = 0) \nplt.show()","b801fa1c":"# Import necessary tools from sklearn\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Select the features and the target\nX = train['tokenized_str']\ny = train[\"Label\"]\n\n# Split the train set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)","61e707da":"# Create the tf-idf vectorizer\nmodel_vectorizer = TfidfVectorizer()\n\n# Fit and transform the vectorizer with X_train\ntfidf_train = model_vectorizer.fit_transform(X_train)\n\n# Tranform the vectorizer with X_test\ntfidf_test = model_vectorizer.transform(X_test)\n\n# Initialize the Bernoulli Naive Bayes classifier\nnb = BernoulliNB()\n\n# Fit the model\nnb.fit(tfidf_train, y_train)\n\n# Print the accuracy score\nbest_accuracy = cross_val_score(nb, tfidf_test, y_test, cv=10, scoring='accuracy').max()\nprint(\"Accuracy:\",best_accuracy)\n\n# Predict the labels\ny_pred = nb.predict(tfidf_test)\n\n# Print the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\\n\")\nprint(cm)\n\n# Print the Classification Report\ncr = classification_report(y_test, y_pred)\nprint(\"\\n\\nClassification Report\\n\")\nprint(cr)\n","d902aec9":"# Convert tokenized words from list to string\ntest['tokenized_str']=[\" \".join(token) for token in test['Tokenized'].values]\n\n# Look at the test data\ndisplay(test.head())\nprint(test.info())","8c7c063a":"# Get the tfidf of test data\ntfidf_final = model_vectorizer.transform(test[\"tokenized_str\"])\n\n# Predict the labels\ny_pred_final = nb.predict(tfidf_final)\n\n# Print the Confusion Matrix\ncm = confusion_matrix(test[\"Label\"], y_pred_final)\nprint(\"Confusion Matrix\\n\")\nprint(cm)\n\n# Print the Classification Report\ncr = classification_report(test[\"Label\"], y_pred_final)\nprint(\"\\n\\nClassification Report\\n\")\nprint(cr)\n","be1e5850":"# 1. Exploratory Data Analysis\nFirst of all, load the train set and see its features.","4eccefb0":"Model's performance quite well! Let's use the model with our test data.","5ab39f3b":"Let's make a word cloud for all comments","63ea77a6":"Apparently, there is no relationship!\nNow, it's time to tokenize the comments for better understanding.","97a64f88":"Vectorize the words by using Tf-idf Vectorizer","0c401fae":"# 3. Machine Learning\n\nNow we are going to train a machine learning model to classify comments as positive or negative. I am going to use **Bernouilli Naive Bayes classifier** . After select the feature and the target, we are going to split the data into traning and the test sets.","0bffb830":"Is there any relationship between the word_counts, lengths and the sentiment?","4b42038d":"Although we did not do any hyperparameter tuning, the result are not too bad. I hope this notebook will be helpful for you. I am looking forward to your improvements in machine learning model.","c2272aae":"Let's build the model","639ca48d":"In this notebook, I am going to do exploratory data analysis, feature angineering and build a machine learning model to classify labels. Let's start with importing necessary libraries","8fa2360d":"# 2. Feature Engineering\nLet's start with get word counts and the comment lengths (including spaces) in each comment","36f45de0":"Notes\n * There is no missing value. \n * There are 7996 columns\n * Dataset is balanced (50% positive and 50% negative comments)\n * Unnamed: 0 should be dropped"}}