{"cell_type":{"2eb8c7a7":"code","bf5557f1":"code","fafba04d":"code","585b319b":"code","7290b19b":"code","25b5f97c":"code","bfdd5919":"code","8063a68b":"code","9efd2dd6":"code","136fc9d6":"code","81012c5b":"markdown","e0ca4a0e":"markdown","b3756cd3":"markdown","4d1ca114":"markdown","8b57347f":"markdown","52b10888":"markdown","e071c7a7":"markdown","b82a9420":"markdown","fb813159":"markdown","d9319662":"markdown","87ff4b51":"markdown","c1de6b4b":"markdown","a0abd96c":"markdown","20fdc5a1":"markdown","2f2c1afe":"markdown","fc0bfbf9":"markdown","e715241d":"markdown","4ca2f9f1":"markdown","1864d21a":"markdown","69ff8649":"markdown","6c77c429":"markdown","9d444b10":"markdown","a6ef297d":"markdown"},"source":{"2eb8c7a7":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('..\/input\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","bf5557f1":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","fafba04d":"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndata = pd.read_csv('..\/input\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\n\n","585b319b":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","7290b19b":"\n\n# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()\n\n","25b5f97c":"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('..\/input\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\n","bfdd5919":"row_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nmy_model.predict_proba(data_for_prediction_array)","8063a68b":"\n\nimport shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\n","9efd2dd6":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","136fc9d6":"# use Kernel SHAP to explain test set predictions\nk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)","81012c5b":"**Code to Calculate SHAP Values**\n\nWe calculate SHAP values using the wonderful Shap library.\n\nFor this example, we'll reuse the model you've already seen with the Soccer data","e0ca4a0e":"**1. Permutation Importance**","b3756cd3":"\nInterpreting Permutation Importances\n\nThe values towards the top are the most important features, and those towards the bottom matter least.\n\nThe first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n\nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck\/chance.\n\nIn our example, the most important feature was Goals scored. That seems sensible. Soccer fans may have some intuition about whether the orderings of other variables are surprising or not","4d1ca114":"**PROS**\n\nThe Shapley value is the only explanation method with a solid theory.The axioms-efficiency,symmetry,dummy,additivity- give the explanation a reasonable foundation.Methods like LIME assume linear behaviour of the machine learning model locally,but there is no theory as to why this should work\n\n**CONS**\n\nThe shapley value requires a lot of computing time","8b57347f":"\n\nWe will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). For context, we'll look at the raw predictions before looking at the SHAP values\n","52b10888":"Some see machine learning models as \"black boxes\" that are difficult to interpret and explain ---Well this tutorial we are going to try to extract information from a machine learning \"black box\" model  using techniques like feature importance, partial dependence plots,SHAP values.","e071c7a7":"**2D Partial Dependence Plots**\n\nIf  you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify what this.\n\nWe will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself","b82a9420":"**2.  PARTIAL PLOTS**\n\nPartial Dependence Plots shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J.H Friedman 2001).It also shows how a feature affects predictions","fb813159":"**pros**\n\n1 Nice Interpretation:It provides a highly compressed,global insight into the model's behaviour\n\n2 It does not require retraining the model\n\n**cons**\n\n-You need access to the true outcome(i.e labelled data)\n\n-The permutation feature importance depends on shuffling the feature,which adds randomness to the measurement\n\n-If features are correlated,the permutation feature importance can be biased by unrealistic data instances.","d9319662":"**Skater is also use for model interpretation**\n\nSkater is a unified framework to enable Model Interpretation for all forms of models to help one build an Interpretable machine learning system often needed for real world use-cases using a model-agnostic approach. It is an open source python library designed to demystify the learned structures of a black box model both globally(inference on the basis of a complete data set) and locally(inference about an individual prediction).\n\nread more on: https:\/\/towardsdatascience.com\/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608**","87ff4b51":"\n\nIf you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in shap.TreeExplainer(my_model). But the SHAP package has explainers for every type of model.\n\n    shap.DeepExplainer works with Deep Learning models.\n    shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\nHere is an example using KernelExplainer to get similar results. The results aren't identical because kernelExplainer gives an approximate result. But the results tell the same story.\n","c1de6b4b":"**********","a0abd96c":"\n\nThe shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don't win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we'll pull out Shap values for positive outcomes (pulling out shap_values[1].)\n\nIt's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results","20fdc5a1":"\n\nThe team is 70% likely to have a player win the award.\n\nNow, we'll move onto the code to get SHAP values for that single prediction","2f2c1afe":"\n\nThis graph shows predictions for any combination of Goals Scored and Distance covered.\n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn't matter","fc0bfbf9":"\n\nA few items are worth pointing out as you interpret this plot\n\n    The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n    A blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \"Player of The Game.\" But extra goals beyond that appear to have little impact on predictions.","e715241d":"**3. SHAP VALUES**\n\nIt takes into account individual prediction value and assigns each feature an importance value.SHAP values tries to explain the impact of each feature by breaking down a prediction.","4ca2f9f1":"Code Example\nOur example will use a model that predicts whether a soccer\/football team will have the \"Man of the Game\" winner based on the team's statistics. The \"Man of the Game\" award is given to the best player in the game. Model-building isn't our current focus, so the cell below loads the data and builds a rudimentary model.","1864d21a":"**PROS**\n\n- If the feature for which you computed the PDP is not correlated with the other features, then the PDPs perfectly represent how the feature influences the prediction on average.\n\n-Partial dependence plots are easy to implement\n\n**CONS**\n\n-The assumption of Independence is the biggest issue with PDplots.It is assumed that the features for which the partial dependence is computed are not correlated with other features.","69ff8649":"Code Example\n\nModel building isn't our focus, so we won't focus on the data exploration or model building code.","6c77c429":"**MODEL EXPLAINABILITY**","9d444b10":"Thanks for reading\n\nFor more reading-  check out(References)\n\nhttps:\/\/www.kaggle.com\/learn\/machine-learning-explainability\n\nhttps:\/\/christophm.github.io\/interpretable-ml-book\/\n\nhttps:\/\/towardsdatascience.com\/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608","a6ef297d":"Permutation Importance is usually calculated after a model has been fitted.It works by getting a trained model and then shuffling the values of a single column and making predictions using the dataset,then calculate how much the loss function suffers from shuffling a column by using the predictions and the true target values.How the performance differ shows the importance of the variable you shuffle.You repeat this process for all columns.A feature is \"unimportant\" if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction."}}