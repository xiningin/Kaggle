{"cell_type":{"c2f7885e":"code","7d92aa21":"code","5a581094":"code","0b928b8a":"code","933e0567":"code","732f880b":"code","c1e9cbea":"code","c5469876":"code","948346d5":"code","b18bf103":"code","e6185699":"code","24c5644f":"code","f47b3550":"code","9f71b12c":"code","c65d538c":"code","f80efa12":"code","94ba36f0":"code","6be8760b":"code","cccbf4b7":"code","544db6b1":"code","d0841565":"code","9bac0b5f":"code","446d995f":"code","0a82f6b1":"code","f1616baf":"code","4745c6e5":"code","b0ae1794":"code","4c1edfb5":"code","cab8723f":"code","f21dc651":"code","d3ae21ef":"code","590c7aa7":"code","1461e4e4":"code","4554fde6":"code","3cd0e2fa":"code","0e418524":"code","414f0f1d":"code","d4463e87":"code","27c5d297":"code","b0d670f0":"code","b342111a":"code","23b435b2":"code","e53e0d68":"code","c32f0f73":"code","6a60bea3":"code","5323de18":"code","27036a0a":"code","7f6b04f5":"code","4ef0ed13":"code","bc52979e":"code","2914d86c":"code","2748510c":"code","639e2e78":"code","fddb339c":"code","eef1a161":"code","4cf342ef":"code","cbe3dbf5":"code","e6e43934":"code","379cc22b":"code","3531a7f9":"code","9d890b82":"code","c3074f22":"code","c76c1fc8":"code","3a3b4949":"code","c187b103":"code","e53c0156":"code","545bbbb7":"markdown","9e8f656b":"markdown","58e3c9c7":"markdown","98f38915":"markdown","db5f3db8":"markdown","cff0eff0":"markdown","c8705a7c":"markdown","41e22b0b":"markdown","12901a71":"markdown","e623ac03":"markdown","3ee95c27":"markdown","96e7dedc":"markdown","400efa62":"markdown","adabdff7":"markdown","113a8fc9":"markdown"},"source":{"c2f7885e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d92aa21":"import os\nimport glob\nimport wandb\nimport json\nimport warnings\nimport imageio\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML, Javascript\nimport IPython.display as py_display\nwarnings.filterwarnings(\"ignore\")","5a581094":"# A function for calculating the missing values in dataset in percent\ndef percent_missing(df: pd.DataFrame):\n    # Calculate total  number of cells in dataframe\n    totalCells = np.product(df.shape)\n\n    # Count number of missing values per column\n    missingCount = df.isnull().sum()\n\n    # Calculate total number of missing values\n    totalMissing = missingCount.sum()\n\n    # Calculate percentage of missing values\n    return print(\"The dataset contains\", round(((totalMissing \/ totalCells) * 100), 2), \"%\", \"missing values.\")\n\n\n# A function to see the percentage of missing values of every columns\ndef every_column_percent_missing(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_db = pd.DataFrame({'column_name': df.columns,\n                                     'percent_missing': percent_missing})\n\n    missing_value_db.sort_values('percent_missing', inplace=True)\n\n    print(missing_value_db)\n    \n#PLOTING FUNCTIONS\n\ndef plot_hist(df: pd.DataFrame, column: str, color: str) -> None:\n    plt.figure(figsize=(9, 7))\n    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)\n    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n    plt.show()\n\n\ndef plot_dist(df: pd.DataFrame, column: str):\n    plt.figure(figsize=(9, 7))\n    sns.distplot(df).set_title(f'Distribution of {column}')\n    plt.show()\n\n\ndef plot_count(df: pd.DataFrame, column: str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.countplot(data=df, x=column)\n    plt.title(f'Plot count of {column}', size=20, fontweight='bold')\n    plt.show()\n\n\ndef plot_bar(df: pd.DataFrame, x_col: str, y_col: str, title: str, xlabel: str, ylabel: str) -> None:\n    plt.figure(figsize=(9, 7))\n    sns.barplot(data=df, x=x_col, y=y_col)\n    plt.title(title, size=20)\n    plt.xticks(rotation=75, fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.xlabel(xlabel, fontsize=16)\n    plt.ylabel(ylabel, fontsize=16)\n    plt.show()\n\n\ndef plot_heatmap(df: pd.DataFrame, title: str, cbar=False) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.heatmap(df, annot=True, cmap='viridis', vmin=0,\n                vmax=1, fmt='.2f', linewidths=.7, cbar=cbar)\n    plt.title(title, size=18, fontweight='bold')\n    plt.show()\n\n\ndef plot_box(df: pd.DataFrame, x_col: str, title: str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.boxplot(data=df, x=x_col)\n    plt.title(title, size=20)\n    plt.xticks(rotation=75, fontsize=14)\n    plt.show()\n\n\ndef plot_box_multi(df: pd.DataFrame, x_col: str, y_col: str, title: str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.boxplot(data=df, x=x_col, y=y_col)\n    plt.title(title, size=20)\n    plt.xticks(rotation=75, fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.show()\n\n\ndef plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:\n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(data=df, x=x_col, y=y_col, hue=hue, style=style)\n    plt.title(title, size=20)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.show()\n    \n    ","0b928b8a":"def bar_plot(x, y, title, palette_len, xlim = None, ylim = None, \n             xticklabels = None, yticklabels = None, \n             top_visible = False, right_visible = False, \n             bottom_visible = True, left_visible = False,\n             xlabel = None, ylabel = None, figsize = (10, 4),\n             axis_grid = 'y'):\n    fig, ax = plt.subplots(figsize = figsize)\n    plt.title(title, size = 15, fontweight = 'bold', fontfamily = 'serif')\n\n    for i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_color('black')\n    \n    ax.spines['top'].set_visible(top_visible)\n    ax.spines['right'].set_visible(right_visible)\n    ax.spines['bottom'].set_visible(bottom_visible)\n    ax.spines['left'].set_visible(left_visible)\n\n    sns.barplot(x = x, y = y, edgecolor = 'black', ax = ax,\n                palette = reversed(sns.color_palette(\"viridis\", len(palette_len))))\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)    \n    ax.set_xticklabels(xticklabels, fontfamily = 'serif')\n    ax.set_yticklabels(yticklabels, fontfamily = 'serif')\n    plt.xlabel(xlabel, fontfamily = 'serif')\n    plt.ylabel(ylabel, fontfamily = 'serif')\n    ax.grid(axis = axis_grid, linestyle = '--', alpha = 0.9)\n    plt.show()\n    \n    \ndef line_plot(data, y, title, color,\n              top_visible = False, right_visible = False, \n              bottom_visible = True, left_visible = False,\n              ylabel = None, figsize = (10, 4), axis_grid = 'y'):\n    fig, ax = plt.subplots(figsize = figsize)\n    plt.title(title, size = 15, fontweight = 'bold', fontfamily = 'serif')\n\n    for i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_color('black')\n    \n    ax.spines['top'].set_visible(top_visible)\n    ax.spines['right'].set_visible(right_visible)\n    ax.spines['bottom'].set_visible(bottom_visible)\n    ax.spines['left'].set_visible(left_visible)\n    \n    sns.lineplot(x = range(len(data[y])), y = data[y], dashes = False, \n                 color = color, linewidth = .5)\n    ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n    \n    ax.set_xticks([])\n    plt.xticks(rotation = 90)\n    plt.xlabel('')\n    plt.ylabel(ylabel, fontfamily = 'serif')\n    ax.grid(axis = axis_grid, linestyle = '--', alpha = 0.9)\n    plt.show()\n    \n    \ndef corr_plot(data,\n              top_visible = False, right_visible = False, \n              bottom_visible = True, left_visible = False,\n              ylabel = None, figsize = (15, 11), axis_grid = 'y'):\n    fig, ax = plt.subplots(figsize = figsize)\n    plt.title('Correlations (Pearson)', size = 15, fontweight = 'bold', fontfamily = 'serif')\n    \n    mask = np.triu(np.ones_like(data.corr(), dtype = bool))\n    sns.heatmap(round(data.corr(), 2), mask = mask, cmap = 'viridis', annot = True)\n    plt.show()\n    \ndef columns_viz(data, color):\n    for i in range(len(data.columns)):\n        line_plot(data = data, y = data.columns[i],\n                  color = color,\n                  title = '{} dynamics'.format(data.columns[i]),\n                  bottom_visible = False, figsize = (10, 2))\ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\nproducts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")\neng_path = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\n","933e0567":"# Imporrting the data and loading itto a panda daraframe \n\ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\nproducts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")\n","732f880b":"districts_info.head()","c1e9cbea":"districts_info.describe()","c5469876":"# Checking for null values\ndistricts_info.isnull().sum()","948346d5":"percent_missing(districts_info)","b18bf103":"every_column_percent_missing(districts_info)","e6185699":"districts_cleaned = districts_info.dropna()","24c5644f":"print(f\" There are {districts_cleaned.shape[0]} rows and {districts_cleaned.shape[1]} columns\")","f47b3550":"every_column_percent_missing(districts_cleaned)","9f71b12c":"percent_missing(districts_cleaned)","c65d538c":"districts_cleaned.duplicated().sum()","f80efa12":"list(districts_cleaned.columns.values)","94ba36f0":"plot_hist(districts_cleaned,'state','black')","6be8760b":"plot_hist(districts_cleaned,'locale','red')","cccbf4b7":"plot_hist(districts_cleaned,'pct_black\/hispanic','green')","544db6b1":"plot_hist(districts_cleaned,'pct_free\/reduced','orange')","d0841565":"plot_hist(districts_cleaned,'pp_total_raw','green')","9bac0b5f":"def count_plot(data,colr,title):\n    plt.figure(figsize=(10,8))\n    ax=sns.countplot(x=data,palette=colr,order=data.value_counts().index)\n    plt.xticks(rotation=90)\n    plt.title(title)\n    for p in ax.patches:\n        ax.text (p.get_x() + p.get_width()  \/ 2,p.get_height()+ 0.75,p.get_height(), fontsize = 11)\n#         ax.text('%{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","446d995f":"count_plot(districts_cleaned['locale'],'RdYlGn','locale representation') ","0a82f6b1":"count_plot(districts_cleaned['state'],'RdYlGn',\"State representation\")","f1616baf":"count_plot(districts_cleaned['state'],'Reds',\"State representation\")","4745c6e5":"fig = px.pie(districts_cleaned['locale'].value_counts().reset_index().rename(columns = {'locale': 'count'}), values = 'count', names = 'index', width = 700, height = 700)\n\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent + label', \n                  hole = 0.7, \n                  marker = dict(colors = ['#90afc5','#336b87','#2a3132','#763626'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(annotations = [dict(text = ' The count of districts <br>in each type <br>of areas', \n                                      x = 0.5, y = 0.5, font_size = 26, showarrow = False, \n                                      font_family = 'monospace',\n                                      font_color = '#283655')],\n                  showlegend = False)\n                  \nfig.show()","b0ae1794":"products_cleaned = districts_info.dropna()","4c1edfb5":"list(products_cleaned.columns.values)","cab8723f":"products_info.describe()","f21dc651":"print(f\" There are {products_info.shape[0]} rows and {products_info.shape[1]} columns\")","d3ae21ef":"products_info.isnull().sum()","590c7aa7":"percent_missing(products_info)","1461e4e4":"every_column_percent_missing(products_info)","4554fde6":"products_cleaned = products_info.dropna()","3cd0e2fa":"percent_missing(products_cleaned)","0e418524":"print(f\" There are {products_cleaned.shape[0]} rows and {products_cleaned.shape[1]} columns\")","414f0f1d":"plot_hist(products_cleaned,'Sector(s)','purple')","d4463e87":"import glob\nimport pandas\nengagement_path =(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\")\nengagement_merged = glob.glob(engagement_path + \"\/*.csv\")\n\nfiles = []\n\nfor file in engagement_merged:\n    df = pd.read_csv(file, index_col = None, header = 0)\n    district_id = file.split('\/')[4].split('.')[0]\n    df['district_id'] = district_id\n    files.append(df)\n    \nmerged_engagement = pd.concat(files)\nmerged_engagement = merged_engagement.reset_index(drop = True)\nmerged_engagement['time'] = pd.to_datetime(merged_engagement['time'])","27c5d297":"merged_engagement.head(10)","b0d670f0":"print(f\" There are {merged_engagement.shape[0]} rows and {merged_engagement.shape[1]} columns\")","b342111a":"every_column_percent_missing(merged_engagement)","23b435b2":"percent_missing(merged_engagement)","e53e0d68":"merged_eng_cleaned = merged_engagement.dropna()","c32f0f73":"percent_missing(merged_eng_cleaned)","6a60bea3":"datasets = [districts_cleaned, products_cleaned, merged_eng_cleaned]\ndatasets_names = ['districts_cleaned','products_cleaned','merged_eng_cleaned']\n","5323de18":"columns_viz(datasets[0], color = 'purple')","27036a0a":"for i in ['pct_black\/hispanic', 'pct_free\/reduced']:\n    districts_cleaned[i] = districts_cleaned[i].apply(lambda x: float(x.split(',')[0][1:]) + 0.1)\n\ndistricts_cleaned['pp_total_raw'] = districts_cleaned['pp_total_raw'].apply(lambda x: int(x.split(',')[0][1:]) + 1000)\n\ndistricts_cleaned.drop('county_connections_ratio', axis = 1, inplace = True)\n\ndistricts_cleaned.head(3)","7f6b04f5":"plt.figure(figsize = (15, 8))\na = sns.barplot(data = districts_cleaned['state'].value_counts().reset_index(), x = 'state', y = 'index', color = '#90afc5')\nplt.xticks([])\nplt.yticks(fontname = 'monospace', fontsize = 14, color = '#900C3F')\nplt.title('Distribution over states',fontweight = 'bold', fontfamily = 'serif', fontsize = 20, color = '#000000')\nplt.ylabel('States')\nplt.xlabel('Distribution')\na.spines['left'].set_linewidth(1.5)\nfor w in ['right', 'top', 'bottom']:\n    a.spines[w].set_visible(False)\n    \nfor p in a.patches:\n    width = p.get_width()\n    plt.text(0.5 + width, p.get_y() + 0.55 * p.get_height(), f'{int(width)}',\n             ha = 'center', va = 'center', fontname = 'monospace', fontsize = 15, color = '#283655')\nplt.show()","4ef0ed13":"dist_area_group = districts_cleaned.groupby('locale').agg({'pct_black\/hispanic': 'mean', 'pct_free\/reduced': 'mean', 'pp_total_raw': 'mean'}).reset_index()\n\ncolors = ['#00FFFF', '#0000FF', '#808000']\n\nfig = plt.figure(figsize = (13, 12))\nfor i in range(len(dist_area_group.columns.tolist()[1:])):\n    plt.subplot(2, 2, i+1)\n    sns.set_style(\"white\")\n    plt.title(dist_area_group.columns.tolist()[1:][i], fontweight = 'bold', fontfamily = 'serif', fontsize = 20, y = 1.09, color = colors[i])\n    plt.grid(color = 'gray', linestyle = ':', axis = 'y', zorder = 0,  dashes = (1,7))\n    a = sns.barplot(data = dist_area_group, x = 'locale', y = dist_area_group.columns.tolist()[1:][i], color = colors[i])\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontweight = 'bold', fontfamily = 'serif', fontsize = 20)\n    plt.yticks([])\n    \n    for j in ['right', 'top', 'left']:\n        a.spines[j].set_visible(False)\n    for j in ['bottom']:\n        a.spines[j].set_linewidth(1.4)\n      \n    if i < 2:\n        for p in a.patches:\n            height = p.get_height()\n            a.annotate(f'{int(height*100)} %', (p.get_x() + p.get_width() \/ 2, p.get_height()-0.03), \n                   ha = 'center', va = 'center', \n                   size = 18,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   color = 'white',\n                   fontname = 'monospace')\n    else:\n        for p in a.patches:\n            height = p.get_height()\n            a.annotate(f'{int(height)} $', (p.get_x() + p.get_width() \/ 2, p.get_height()-1000), \n                   ha = 'center', va = 'center', \n                   size = 18,\n                   xytext = (0, 5), \n                   textcoords = 'offset points',\n                   color = 'white',\n                   fontname = 'monospace')\n            \nplt.figtext(0.07, 1.05, 'Characteristics of school districts by locale', fontsize = 30, fontname = 'monospace', color = '#283655')\n\n\nfig.tight_layout(pad = 3)\n\nplt.show()","bc52979e":"plt.figure(figsize = (15, 8))\nsns.set_style(\"white\")\nplt.title('TOP-15 providers', size = 35, x = 0.48, y = 1.06, fontweight = 'bold', fontfamily = 'serif', fontsize = 20, color = '#283655')\na = sns.barplot(data = products_cleaned['Provider\/Company Name'].value_counts().reset_index().head(15), x = 'Provider\/Company Name', y = 'index', color = '#90afc5')\nplt.xticks([])\nplt.yticks(fontweight = 'bold', fontfamily = 'serif', fontsize = 20, color = '#283655')\nplt.ylabel('')\nplt.xlabel('')\n\na.spines['left'].set_linewidth(1.5)\nfor w in ['right', 'top', 'bottom']:\n    a.spines[w].set_visible(False)\n#counting the numbers\nfor p in a.patches:\n    width = p.get_width()\n    plt.text(0.5 + width, p.get_y() + 0.55 * p.get_height(), f'{int(width)}',\n             ha = 'center', va = 'center', fontname = 'monospace', fontsize = 15, color = '#000000')\n\nplt.show()","2914d86c":"#Code credit Dimtry Uraov\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nstate_abb = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_cleaned['state_abb'] = districts_cleaned['state'].map(state_abb)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Count of districts in the available States\",\n    title_font = dict(\n            family = \"monospace\",\n            size = 25,\n            color = \"black\"\n            ),\n    geo_scope = 'usa'\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations = districts_cleaned['state_abb'].value_counts().to_frame().reset_index()['index'],\n        zmax = 1,\n        z = districts_cleaned['state_abb'].value_counts().to_frame().reset_index()['state_abb'],\n        locationmode = 'USA-states',\n        marker_line_color = 'white',\n        geo = 'geo',\n        colorscale = \"cividis\", \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","2748510c":"hisp_blck_districts = districts_cleaned.groupby('state').agg({'pct_black\/hispanic':np.mean,'pct_free\/reduced':np.mean,'pp_total_raw':np.mean})\n","639e2e78":"hisp_blck_districts=hisp_blck_districts.reset_index()","fddb339c":"hisp_blck_districts['state_abb']=hisp_blck_districts['state'].map(state_abb)","eef1a161":"sns.heatmap(hisp_blck_districts.corr(),annot=True)","4cf342ef":"fig = px.bar_polar(hisp_blck_districts, r=\"pct_black\/hispanic\", theta=\"state\", \n            color_discrete_sequence= px.colors.sequential.thermal)\nfig.show()","cbe3dbf5":"fig = px.bar_polar(hisp_blck_districts, r=\"pct_free\/reduced\", theta=\"state\", template=\"plotly_white\",\n            color_discrete_sequence= px.colors.sequential.Plasma_r)\nfig.show()","e6e43934":"#create a locale \nlocale_df=districts_cleaned.groupby('locale').agg({'pct_black\/hispanic':np.mean,'pct_free\/reduced':np.mean})","379cc22b":"locale_df=locale_df.reset_index()","3531a7f9":"state_locale_df=districts_cleaned.groupby(['state_abb','locale']).agg({'pct_black\/hispanic':np.mean,'pct_free\/reduced':np.mean,'pp_total_raw':np.mean})\nstate_locale_df=state_locale_df.reset_index()\nstate_locale_df","9d890b82":"plt.figure(figsize=(15,9))\nax=sns.barplot(x=\"locale\", y=\"pct_black\/hispanic\", data=state_locale_df,palette=\"magma\")\n\nplt.xticks(rotation=90)\nplt.title(\"Percentage of blacks and hispanic in the locales\")","c3074f22":"merged_data = pd.merge(products_cleaned, merged_eng_cleaned, left_on = 'LP ID', right_on = 'lp_id')\nmerged_data['district_id'] = merged_data['district_id'].astype('int64')\nmerged_data = pd.merge(merged_data, districts_cleaned, on = 'district_id')\nmerged_data.drop(['URL', 'lp_id', 'state_abb'], axis = 1, inplace = True)\nmerged_data.head(3)","c76c1fc8":"num_data = merged_data[['engagement_index', 'pct_black\/hispanic', 'pct_free\/reduced',\n                                 'pct_access',  'pp_total_raw']]\ndata_correlation = num_data.corr()\nplot_heatmap(data_correlation, 'Numerical Data Correlation')\ndata_correlation","3a3b4949":"pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip","c187b103":"import pandas_profiling as pp\nfrom pandas_profiling import ProfileReport\nprofile = ProfileReport(merged_data,title=\"Pandas Profiling Report 2015\", explorative=True)\nprofile.to_notebook_iframe()","e53c0156":"we ","545bbbb7":"**Utility methods** from week 1 scripts","9e8f656b":"\n\n    Covid had become disasterous since its genesis in China. There are around 250 million people infected with it . It castes shadows over social and economic spheres of the world. education is one of the secotors that have been hit very hard .There were global lockdowns i schools, campuses , offices , business areas etc.  Some Nations adopted online learning but countries with poor infrastructure for internet and telecom suffers the most from this crisis\n\n    IT has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow\n\n    This analysis therefore aims at analysing this common factors that bring about inequality in the education system, and through that come up with positive policies and solutions to how better not only governments but also schools can play their part in bridging the educational divide especially during this covid peri\n\n    The following data analysis's objective is to analyse the situation the education sector is in to take the necessary shift of policies to counter the effects the pandemic in the united states. \n\nDatasets\n\n\nthe datas were collected from a competition with the following description to it \n\nWe have provided a set of daily edtech engagement data from over 200 school districts in 2020, and we encourage you to leverage other publicly available data sources in your analysis. We include three basic sets of files to help you get started:\n\nThe engagement_ data folder is based on LearnPlatform\u2019s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district.\nThe products_info.csv file includes information about the characteristics of the top 372 products with most users in 2020.\nThe districts_info.csv file includes information about the characteristics of school districts, including data from NCES and FCC.\nThe definitions of each column in the three data sets are detailed in the README file.","58e3c9c7":"From the above plots we can see texas having the higheest black and hispanic representations by far while Newyork, New Jersey , DC and the likes. Also, there happens to be no correlation between he reduced percentsge colomns and hipanic\/black represenations percentages","98f38915":"    As expected , We can also observe students that are eligible for lunch or reduced-price lunches are mostly from areas where there are more hispanic and black represenataions ","db5f3db8":"Engagement data\u00b6\n\nThis particular data are based on LearnPlatform\u2019s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions.\n\nThe engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents district_id which can be used to link to district information in district_info.csv. The lp_id can be used to link to product information in product_info.csv.","cff0eff0":"The above plot shows the percent resuced data corresponding to the states. Minosota to be \n","c8705a7c":"    The  above correlation maps shows us there seems to be a significant positive correlation between the total page-load events per one thousand students of a given product (or on a given day) and the  percentage of students in the district which  have at least one page-load event of a given product and on a given day","41e22b0b":"checking the proportions of hispanic and black people in the districts","12901a71":"**Looking at the products_cleaned**","e623ac03":"District information data\nThe district file districts_info.csv includes information about the characteristics of school districts, including data from, NCES (2018-19),FCC (Dec 2018), and Edunomics Lab.\n\n* distrist_id\n* state\n* locale\n* pct_black\/hispanic - percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data.\n* pct_free\/reduced - percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data.\n* county_connections_ratio - ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version).\n* pp_total_raw - per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project.","3ee95c27":"Product information data\n\nThe product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy.\n\n\n* LP ID - the unique identifier of the product. URL\n* Product Name\n* Provider\/Company Name\n* Sector(s) - sector of education where the product is used.\n* Primary Essential Function - the basic function of the product.\n\n* There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled","96e7dedc":"**Exploratory Data Analuysis****","400efa62":"looking at the distribition datas","adabdff7":"Reference\n\nRuchie Bhatia\n\nEuel Fantaye\n\nBlaise papa\n\nhttps:\/\/www.youtube.com\/watch?v=AYalukmWroY\n\n\nhttps:\/\/gist.github.com\/rogerallen\/1583593\n\n","113a8fc9":"We can see that more hispanic and black representations are witnessed in cities and very less are in towns. "}}