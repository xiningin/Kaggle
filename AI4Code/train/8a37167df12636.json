{"cell_type":{"6599b8ed":"code","6d107d09":"code","79f2afd0":"code","70ba1b4e":"code","4301f2bd":"code","33705238":"code","f8b19ff8":"code","93ae5705":"code","8cde9427":"code","c4c30b87":"code","c4a4569c":"code","391be95f":"code","8a7a7e8d":"code","836248b6":"code","2a65bfc8":"code","39e513eb":"code","377f8736":"code","daaa689a":"code","858fc22a":"code","98c9bc5b":"code","5a875143":"code","d781cc7d":"code","f293057d":"code","dcc9bf03":"code","843c5491":"code","502c3c9b":"code","84d6ff44":"code","6cac9df6":"code","cf9d5da9":"code","53e77853":"code","8509ce56":"code","d037f3bb":"code","b0e1e3db":"code","e2e81d1a":"code","8dc2cb0f":"code","6915c1f2":"code","49bdeb33":"code","c6352bbc":"code","33a32033":"code","dfa079a2":"code","0bf4207b":"code","d3a371f8":"code","5b1eac7e":"code","c6cb9678":"code","08b0b068":"code","f56dbe3d":"code","7a76c351":"code","fda6e663":"code","2e98c6d8":"code","9a986c95":"code","8a8765c6":"code","b7918563":"code","d176a951":"code","13057b69":"code","993d3ff8":"code","2c3944d3":"code","84e5b157":"code","3ca9d844":"code","731e4ff0":"code","15335948":"code","d11e5cc1":"code","defed1b3":"code","0eb11b96":"code","66d668d8":"code","1c903b7f":"code","c8672588":"code","eb98af81":"code","a3531077":"code","1f867df4":"code","08d0eeab":"code","e9a6f19e":"code","4eb4a3c5":"code","76056f1c":"code","6f33b5c7":"code","609034ae":"code","c5e64b08":"code","78026ff3":"code","2516385b":"code","af88a4f9":"code","99a8bd76":"code","3029aa90":"code","873acb23":"code","e09007f4":"code","c453e325":"code","fc400e38":"code","e2c3b43f":"code","3993e908":"code","19d06a24":"code","14a76f55":"code","cb585893":"code","af3e050c":"code","5efc7a0d":"markdown","a7f661d3":"markdown","4c398ee9":"markdown","d06e3bb3":"markdown","59961d5b":"markdown","4132d954":"markdown","398c8c54":"markdown","0a36193d":"markdown","db6b7713":"markdown","b3f8d0ae":"markdown","6795dac8":"markdown","41517a05":"markdown","54dfb42e":"markdown","a70a3abc":"markdown","f0c35831":"markdown","bd6f8c5e":"markdown","a2bc12bf":"markdown","90ba7589":"markdown","02e1d337":"markdown","22d7deb6":"markdown","f3a15d33":"markdown","7ec8c89b":"markdown","052d10c3":"markdown","49ee96f0":"markdown","55d2d2a8":"markdown","5841da64":"markdown","470b55ec":"markdown","bdd08b93":"markdown","1199c96d":"markdown","abec4a20":"markdown","a23a7682":"markdown","c44c62c7":"markdown"},"source":{"6599b8ed":"# general libraries\nimport os\nimport gc\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\nfrom pathlib import Path\n\n# plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\n# sklearn - metric, train test split\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score, RepeatedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor","6d107d09":"def load_data():\n    train      = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\",index_col='id')\n    test       = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\",index_col='id')\n    submission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\n    return train,test,submission\n\ntrain,_,_ = load_data()\ndisplay(train)","79f2afd0":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nprint(tf.__version__)","70ba1b4e":"train.values.shape","4301f2bd":"# Label encoding\n\ntarget = train.pop('target')\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n        \n# Normalisation of the data\nnormalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(train))\n\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)\n\n# define the model\ndef ann_model(norm):\n    model = keras.Sequential([\n        norm,\n        layers.Dense(24, input_dim=24, kernel_initializer='normal', activation='relu'),\n        layers.Dense(10,activation='relu'),\n        layers.Dense(5),\n        layers.Dense(1)\n    ])\n    model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01))\n    return model\n\ndnn_model = ann_model(normalizer)\ndnn_model.summary()","33705238":"history = dnn_model.fit(X_train,y_train, validation_split =0.2,batch_size=128, epochs=50,verbose=0)","f8b19ff8":"# generic function to visualise the fit of the model and MSE\ndef plot_results(name, y, yhat, num_to_plot=25000, lims=(0,15), figsize=(10,7)):\n    MSE = math.sqrt(((yhat-y)**2).mean())\n    RMSE = np.sqrt(MSE)\n    plt.figure(figsize=figsize)\n    a = plt.axes(aspect='equal')\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.ylim(lims)\n    plt.xlim(lims)\n    _ = plt.plot(lims, lims)\n    plt.title(f'{name}: {RMSE:0.6f}', fontsize=16)\n    plt.show()","93ae5705":"predictions= dnn_model.predict(X_test).flatten()\ntest_labels = y_test\n\na = plt.axes(aspect='equal')\nplt.scatter(test_labels, predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 14]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","8cde9427":"# error distributiion\n\nerror = predictions - y_test\nplt.hist(error, bins=2000)\nplt.xlabel('Prediction Error')\n_ = plt.ylabel('Count')","c4c30b87":"# summarizing the fit of the model\ndnn_MSE      = round(mean_squared_error(y_test, predictions, squared=False),6)\ndnn_RMSE     = round(np.sqrt(mean_squared_error(y_test, predictions, squared=False)),6)\ndnn_R2       = round(metrics.r2_score(y_test, predictions),6)","c4a4569c":"models =['Neural Network','Decision Tree','Random Forest','LightGBM','XGBoost','CatBoost','Decision Tree-fastai','Random Forest-fastai',\n         'CatBoost-fastai','XGBoost-Optuna','CatBoost-Optuna-gpu','CatBoost-Optuna-cpu']\nresults = pd.DataFrame(index=models,columns=['MSE','RMSE','R2'])","391be95f":"results.iloc[0:1,0:1] = dnn_MSE\nresults.iloc[0:1,1:2] = dnn_RMSE\nresults.iloc[0:1,2:3] = dnn_R2","8a7a7e8d":"del train, target, history\ngc.collect()","836248b6":"train,test,_ = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n              \nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)","2a65bfc8":"# split the data       \ntarget = train.pop('target')\n\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","39e513eb":"## Decision Tree\n\nfrom sklearn.tree import DecisionTreeRegressor\nimport math\n\n\nmodel_tree = DecisionTreeRegressor(\n    criterion='mse',splitter='best',min_samples_split=1.0,min_samples_leaf=5, max_features=1.0,random_state=42,max_leaf_nodes=20\n)\n\nmodel_tree.fit(X_train, y_train)\ny_tree = model_tree.predict(X_test)\n\n# summarizing the fit of the model\ntree_MSE      = round(mean_squared_error(y_test, y_tree, squared=False),6)\ntree_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_tree, squared=False)),6)\ntree_R2       = round(metrics.r2_score(y_test, y_tree),6)","377f8736":"results.iloc[1:2,0:1] = tree_MSE\nresults.iloc[1:2,1:2] = tree_RMSE\nresults.iloc[1:2,2:3] = tree_R2","daaa689a":"from sklearn.ensemble import RandomForestRegressor\n\ndef rf(xs,y,n_estimators=100, n_jobs=-1,max_samples = 240000,max_features = 0.5,min_samples_leaf=0.5,**kwargs):\n    return RandomForestRegressor(n_jobs=-1,n_estimators=100,max_samples=max_samples,\n                                 max_features=max_features,min_samples_leaf=min_samples_leaf,oob_score=True).fit(xs,y)\n\n                              \nmodelRF = rf(X_train, y_train,n_estimators=100)\ny_pred_rf = modelRF.predict(X_test)\n\n# summarizing the fit of the model\n\nrf_MSE      = round(mean_squared_error(y_test, y_pred_rf, squared=False),6)\nrf_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_rf, squared=False)),6)\nrf_R2       = round(metrics.r2_score(y_test, y_pred_rf),6)\n\nresults.iloc[2:3,0:1] = rf_MSE\nresults.iloc[2:3,1:2] = rf_RMSE\nresults.iloc[2:4,2:3] = rf_R2","858fc22a":"import lightgbm as ltb\n\nmodel_ltb= ltb.LGBMRegressor(boosting_type='gbdt',num_leaves=50,min_data_in_leaf=1000,max_depth=7,learning_rate=0.009,n_estimators=500)\nmodel_ltb.fit(X_train, y_train)\ny_pred_lg = model_ltb.predict(X_test)\n\n# summarizing the fit of the model\n\nlg_MSE      = round(mean_squared_error(y_test, y_pred_lg, squared=False),6)\nlg_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_lg, squared=False)),6)\nlg_R2       = round(metrics.r2_score(y_test, y_pred_lg),6)\n\nresults.iloc[3:4,0:1] = lg_MSE\nresults.iloc[3:4,1:2] = lg_RMSE\nresults.iloc[3:4,2:3] = lg_R2","98c9bc5b":"import xgboost as xgb\n\nxgbmodel = xgb.XGBRegressor(objective='reg:squarederror') \n\n# Fitting the model \nxgbmodel.fit(X_train, y_train)\n\n# Predict the model \ny_pred_xgb = xgbmodel.predict(X_test)\n\n# summarizing the fit of the model\nxg_MSE      = round(mean_squared_error(y_test, y_pred_xgb, squared=False),6)\nxg_RMSE     = round(np.sqrt(mean_squared_error(y_test, y_pred_xgb, squared=False)),6)\nxg_R2       = round(metrics.r2_score(y_test, y_pred_xgb),6)\n\nresults.iloc[4:5,0:1] = xg_MSE\nresults.iloc[4:5,1:2] = xg_RMSE\nresults.iloc[4:5,2:3] = xg_R2","5a875143":"# fit the model on the whole dataset\ncatmodel = CatBoostRegressor(verbose=0, n_estimators=1000)\n# Fitting the model \ncatmodel.fit(X_train, y_train)\n\n# Predict the model \ny_pred_cat = catmodel.predict(X_test)\n# summarizing the fit of the model\n\ncat_MSE      = round(math.sqrt(((y_pred_cat-y_test)**2).mean()),6)\ncat_RMSE     = round(np.sqrt(cat_MSE),6)\ncat_R2       = metrics.r2_score(y_pred_cat,y_test)\n\nresults.iloc[5:6,0:1] = cat_MSE\nresults.iloc[5:6,1:2] = cat_RMSE\nresults.iloc[5:6,3:4] = cat_R2","d781cc7d":"plot_results(\"Cat Boost Regressor\", y_test, y_pred_cat)","f293057d":"del train,target,X_train,X_test,y_train,y_test\ngc.collect()","dcc9bf03":"!pip install pycaret\nfrom pycaret.regression import *","843c5491":"train,test,_ = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n              \nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)","502c3c9b":"data = train.sample(frac=0.9, random_state=42)\ndata_unseen = train.drop(data.index)\n\ntrain.reset_index(drop=True, inplace=True)\ndata_unseen.reset_index(drop=True, inplace=True)\n\nprint('Data for Modeling          : ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","84d6ff44":"target = train['target']","6cac9df6":"clf = setup(train,target='target',session_id=42)","cf9d5da9":"best = compare_models(exclude = ['rf','xgboost','lightgbm','br','ransac','lr','dt','lar','huber','par','omp','knn','ridge','et','ada','en'])","53e77853":"%%time\n\ncat = create_model('catboost',verbose=False)","8509ce56":"# untuned model is efficient tha untuned\n#tuned_model = tune_model(cat)","d037f3bb":"plot_model(cat, plot = 'error')","b0e1e3db":"predict_model(cat)","e2e81d1a":"final_model = finalize_model(cat)\npredict_model(final_model)","8dc2cb0f":"unseen_predictions = predict_model(final_model,data=data_unseen)\nunseen_predictions.head()","6915c1f2":"from pycaret.utils import check_metric\ncheck_metric(unseen_predictions.target, unseen_predictions.Label, 'RMSE')","49bdeb33":"save_model(final_model,'CatBoost_Model')","c6352bbc":"gc.collect()","33a32033":"best_params = final_model.get_all_params()\nprint(best_params)","dfa079a2":"sample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")","0bf4207b":"pred_catboost = predict_model(final_model,data=test)\n\nid = sample_submission['id'].values\nlabel = pred_catboost.Label.values\n\n\nout_df=pd.DataFrame({'id':id,'target':label})\n\n# round the predictions to 6 decimal values\nfor c in out_df.columns:\n    if out_df[c].dtype=='float64':\n        out_df[c]= round(out_df[c],6)\n        \ndisplay(out_df.head())\nout_df.to_csv('submission_catboost_pycaret.csv',index=False)","d3a371f8":"gc.collect()","5b1eac7e":"# This file contains all the main external libs we'll use\nimport fastai\nfrom fastai.imports  import *\nfrom fastai.tabular.all import *","c6cb9678":"input_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')\ntrain = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ntrain.columns","08b0b068":"print(\"Number of Samples              :\",len(train))\nprint(\"Number of Categorical variables:\",10)\nprint(\"Number of Continuos variables  :\",14)\nprint(\"Max Features                   :\",24)","f56dbe3d":"train['cat0'] = train['cat0'].astype('category')\ntrain['cat1'] = train['cat1'].astype('category')\ntrain['cat2'] = train['cat2'].astype('category')\ntrain['cat3'] = train['cat3'].astype('category')\ntrain['cat4'] = train['cat4'].astype('category')\ntrain['cat5'] = train['cat5'].astype('category')\ntrain['cat6'] = train['cat6'].astype('category')\ntrain['cat7'] = train['cat7'].astype('category')\ntrain['cat8'] = train['cat8'].astype('category')\ntrain['cat9'] = train['cat9'].astype('category')","7a76c351":"cat_names = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\ncont_names = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']","fda6e663":"sizes ='A','B','C','D','E','F','G','H','I','J','K','L','M','N','O'\n\ntrain['cat0'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat1'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat2'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat3'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat4'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat5'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat6'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat7'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat8'].cat.set_categories(sizes, ordered=False, inplace =True)\ntrain['cat9'].cat.set_categories(sizes, ordered=False, inplace =True)","2e98c6d8":"splits = RandomSplitter(valid_pct=0.2)(range_of(train))\n\ntp = TabularPandas(train,cat_names=cat_names,cont_names=cont_names,procs=[Categorify,FillMissing, Normalize],y_names='target',splits=splits)\nlen(tp.train),len(tp.valid)","9a986c95":"tp.show(3)","8a8765c6":"tp.items.head(3)","b7918563":"test= pd.read_csv(input_path \/ 'test.csv', index_col='id')\n\nto = TabularPandas(test,cat_names=cat_names,cont_names=cont_names,procs=[Categorify,FillMissing, Normalize])","d176a951":"# defining independent and dependent variables\nxs,y = tp.train.xs,tp.train.y","13057b69":"from fastai.imports import *\nfrom sklearn.tree import DecisionTreeRegressor\n\nm = DecisionTreeRegressor(max_features=24,max_leaf_nodes=25,max_depth=10)\nm.fit(xs,y)","993d3ff8":"from sklearn import tree\nimport graphviz\nfeature_names = xs.columns.values\ndot_data = tree.export_graphviz(m, out_file=None,feature_names=feature_names,class_names=y,filled=True, rounded=True) \ngraph = graphviz.Source(dot_data) \ngraph","2c3944d3":"# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\ny_pred_dt   = m.predict(valid_xs)","84e5b157":"def cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\n\ndef model_rmse(m,xs,y):return cal_rmse(m.predict(xs),y)\n\ndt_MSE      = round(mean_squared_error(valid_y, y_pred_dt, squared=False),6)\ndt_RMSE     = np.sqrt(dt_MSE)\ndt_R2       = round(metrics.r2_score(valid_y, y_pred_dt),6)\n\nresults.iloc[6:7,0:1] = dt_MSE\nresults.iloc[6:7,1:2] = dt_RMSE\nresults.iloc[6:7,2:3] = dt_R2","3ca9d844":"gc.collect()","731e4ff0":"from sklearn.ensemble import RandomForestRegressor\n\ndef rf(xs,y,n_estimators=250, n_jobs=-1,max_samples = 240000,max_features =1.0,min_samples_leaf=0.5,**kwargs):\n    return RandomForestRegressor(n_jobs=-1,n_estimators=100,max_samples=max_samples,max_features=max_features,min_samples_leaf=min_samples_leaf,oob_score=True).fit(xs,y)\n\nm_rf = rf(xs,y,min_samples_leaf=4,max_leaf_nodes=250,max_depth=10)","15335948":"def cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\ndef model_rmse(m,xs,y): return cal_rmse(m.predict(xs),y)\n\n# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\n\ny_pred_rf = m_rf.predict(valid_xs)\n\nrfa_MSE      = round(model_rmse(m_rf,valid_xs,valid_y),6)\nrfa_RMSE     = np.sqrt(rfa_MSE)\nrfa_R2       = round(metrics.r2_score(valid_y,y_pred_rf),6)\n\nresults.iloc[7:8,0:1] = rfa_MSE\nresults.iloc[7:8,1:2] = rfa_RMSE\nresults.iloc[7:8,2:3] = rfa_R2","d11e5cc1":"# fit the model on the whole dataset\n\ndef cat(xs,y,n_estimators=1000, n_jobs=-1,**kwargs):\n    return CatBoostRegressor(verbose=0, n_estimators=1000).fit(xs,y)\n\nparams = {'nan_mode': 'Min','eval_metric': 'RMSE','iterations': 2500,'sampling_frequency': 'PerTree',\n          'leaf_estimation_method': 'Newton','grow_policy': 'SymmetricTree','penalties_coefficient': 1,'boosting_type': 'Plain',\n          'model_shrink_mode': 'Constant','feature_border_type': 'GreedyLogSum','bayesian_matrix_reg': 0.10000000149011612,'l2_leaf_reg': 3,\n          'random_strength': 1,'rsm':1,'boost_from_average': True,'model_size_reg': 0.5,'subsample': 0.800000011920929,'use_best_model': False,\n          'random_seed':14,'depth': 10,'posterior_sampling': False,'border_count': 254,'classes_count': 0,'auto_class_weights': 'None',\n          'sparse_features_conflict_fraction': 0,'leaf_estimation_backtracking': 'AnyImprovement','best_model_min_trees': 1,'model_shrink_rate': 0,\n          'min_data_in_leaf': 300,'loss_function': 'RMSE','learning_rate': 0.010290546311954876,'score_function': 'Cosine','task_type': 'CPU',\n          'leaf_estimation_iterations': 1,'bootstrap_type': 'MVS','max_leaves': 64}\n\n# Fitting the model \nm_cat = cat(xs,y,**params)\n\ndef cal_rmse(pred,y):return round(math.sqrt(((pred-y)**2).mean()),6)\ndef model_rmse(m,xs,y): return cal_rmse(m.predict(xs),y)\n\n# defining independent and dependent variables\nvalid_xs,valid_y = tp.valid.xs,tp.valid.y\n\ny_pred_cat = m_cat.predict(valid_xs)\n\ncata_MSE      = round(model_rmse(m_cat,valid_xs,valid_y),6)\ncata_RMSE     = np.sqrt(cata_MSE)\ncata_R2       = round(metrics.r2_score(valid_y,y_pred_cat),6)\n\nresults.iloc[8:9,0:1] = cata_MSE\nresults.iloc[8:9,1:2] = cata_RMSE\nresults.iloc[8:9,2:3] = cata_R2","defed1b3":"def rf_imp_features(m,df):\n    return pd.DataFrame({'cols':df.columns,'imp_features':m.feature_importances_}).sort_values('imp_features',ascending=False)","0eb11b96":"ffig = rf_imp_features(m,xs)\nffig[:15]","66d668d8":"def plot_fig(ffig):\n    return ffig.plot('cols','imp_features','barh',figsize=(12,8),legend=False)\n\nplot_fig(ffig)\nplt.show()","1c903b7f":"gc.collect()","c8672588":"!pip install optuna \nimport optuna","eb98af81":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\n\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n        \ntarget = train.pop('target')\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","a3531077":"import xgboost as xgb\n\ndef objective(trial):\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.001,0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight',1,500)\n    }\n    model = xgb.XGBRegressor(**param)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(X_test)\n    rmse = round(np.sqrt(mean_squared_error(y_test,preds)),6)\n    return rmse","1f867df4":"%%time\n\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=25)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)","08d0eeab":"# plot_optimization_history: shows the scores from all trials as well as the best score so far at each point\n\n#optuna.visualization.plot_optimization_history(study)","e9a6f19e":"# fit the model on the whole dataset\n\nbest_trial =  {'lambda': 0.07768755871021779, 'alpha': 9.52276768372669,\n             'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.02, 'max_depth': 7, 'random_state': 24, 'min_child_weight': 117}\n\nmodel = xgb.XGBRegressor(**best_trial)\nmodel.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n\n# Predict the model \npreds = model.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\nxgopt_MSE      = mse\nxgopt_RMSE     = np.sqrt(mse)\nxgopt_R2       = round(metrics.r2_score(y_test, preds),6)\n\nresults.iloc[9:10,0:1] = xgopt_MSE\nresults.iloc[9:10,1:2] = xgopt_RMSE\nresults.iloc[9:10,2:3] = xgopt_R2","4eb4a3c5":"gc.collect()","76056f1c":"def objective(trial):\n    param = {\n        'loss_function': 'RMSE',\n        'task_type': 'GPU',\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        #'rsm': trial.suggest_uniform('rsm', 0.3, 1.0),\n        'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n        'n_estimators': 25000,\n        'max_depth': trial.suggest_categorical('max_depth', [7,10,14,16]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300)\n    }\n    model = CatBoostRegressor(**param)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\n    preds = model.predict(X_test)\n    rmse = mean_squared_error(y_test, preds,squared=False)\n    return rmse","6f33b5c7":"import optuna\n\n\nclass StopWhenTrialKeepBeingPrunedCallback:\n    def __init__(self, threshold: int):\n        self.threshold = threshold\n        self._consequtive_pruned_count = 0\n\n    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n        if trial.state == optuna.trial.TrialState.PRUNED:\n            self._consequtive_pruned_count += 1\n        else:\n            self._consequtive_pruned_count = 0\n\n        if self._consequtive_pruned_count >= self.threshold:\n            study.stop()","609034ae":"%%time\n\nimport logging\nimport sys\n\n# Add stream handler of stdout to show the messages\noptuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\nstudy = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner())\n\nstudy_stop_cb = StopWhenTrialKeepBeingPrunedCallback(2)\nstudy.optimize(objective, n_trials=20,callbacks=[study_stop_cb])\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","c5e64b08":"# plot_optimization_history: shows the scores from all trials as well as the best score so far at each point\n\noptuna.visualization.plot_optimization_history(study)","78026ff3":"best_trial = {'l2_leaf_reg': 0.02247766515106271, 'max_bin': 364, 'subsample': 0.6708650091202213,\n             'learning_rate': 0.010290546311954876, 'max_depth': 10, 'random_state': 24, 'min_data_in_leaf': 300,\n            'loss_function': 'RMSE','n_estimators':  25000,'rsm':0.5}\n\nmodel_catopt_cpu = CatBoostRegressor(**best_trial)  \nmodel_catopt_cpu.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\npreds = model_catopt_cpu.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\ncatboost_MSE      = mse\ncatboost_RMSE     = np.sqrt(mse)\ncatboost_R2       = round(metrics.r2_score(y_test, preds),6)\n\n\nresults.iloc[11:12,0:1] = catboost_MSE\nresults.iloc[11:12,1:2] = catboost_RMSE\nresults.iloc[11:12,2:3] = catboost_R2","2516385b":"train,test,_ = load_data()\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n        \ntest_features = test.values\n\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\n\npred3 = model_catopt_cpu.predict(test_features).flatten()\n\nsubmission['target'] = pred3\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(submission[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_cpu.csv',index=False)","af88a4f9":"best_trial = study.best_trial.params\n\nmodel = CatBoostRegressor(**best_trial)  \nmodel.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,verbose=False)\npreds = model.predict(X_test)\nmse = round(mean_squared_error(y_test, preds,squared=False),6)\n\ncatboost_MSE_gpu     = mse\ncatboost_RMSE_gpu    = np.sqrt(mse)\ncatboost_R2_gpu      = round(metrics.r2_score(y_test, preds),6)\n\n\nresults.iloc[10:11,0:1] = catboost_MSE_gpu\nresults.iloc[10:11,1:2] = catboost_RMSE_gpu\nresults.iloc[10:11,2:3] = catboost_R2_gpu","99a8bd76":"gc.collect()","3029aa90":"results = results.sort_values(by=['MSE'], ascending=True)\ndisplay(results)","873acb23":"y_pred_cat_fastai = m_cat.predict(to.items)\n# model: CatBoost-fastai\n\nsubmission['target'] = y_pred_cat_fastai\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(submission[c],6)\n    \nsubmission.to_csv('submission_catboost_fastai.csv',index=False)","e09007f4":"train,test,_ = load_data()\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n        \ntest_features = test.values\n\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\"\n\n# model: CatBoost-Optuna-GPU\n\npred2 = model.predict(test_features).flatten()\n\nsubmission['target'] = pred2\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(out_df[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_gpu.csv',index=False)\n\n# model: CatBoost-Optuna-CPU\n\npred3 = model_catopt_cpu(test_features).flatten()\n\nsubmission['target'] = pred3\n\n# round the predictions to 6 decimal values\nfor c in submission.columns:\n    if submission[c].dtype=='float64':\n        submission[c]= round(out_df[c],6)\n    \nsubmission.to_csv('submission_catboost_optuna_cpu.csv',index=False)\n\n# model: - CatBoost-pycaret\n\npred_catboost = predict_model(final_model,data=test)\n\nid = sample_submission['id'].values\nlabel = pred_catboost.Label.values\nout_df=pd.DataFrame({'id':id,'target':label})\n\n# round the predictions to 6 decimal values\nfor c in out_df.columns:\n    if out_df[c].dtype=='float64':\n        out_df[c]= round(out_df[c],6)\n        \nout_df.to_csv('submission_catboost_pycaret.csv',index=False)","c453e325":"!pip install autoviml\nfrom autoviml.Auto_ViML import Auto_ViML\n!pip install autoviml --no-cache-dir --ignore-installed","fc400e38":"!pip install --upgrade pip\n!pip install SHAP\n!pip3 install --upgrade Pillow\nimport PIL\ngc.collect()","e2c3b43f":"train,test,submission = load_data()","3993e908":"# load the data sets\n\nfrom catboost import CatBoostRegressor\n\nmodel, features, trainm, testm = Auto_ViML(\n    train=train,\n    target=\"target\",\n    test=test,\n    sample_submission=\"\",\n    hyper_param=\"RS\",\n    feature_reduction=True,\n    scoring_parameter=\"mse\",\n    KMeans_Featurizer=False,\n    Boosting_Flag=\"CatBoost\",\n    Binning_Flag=True,\n    Add_Poly=False,\n    Stacking_Flag=False,\n    Imbalanced_Flag=True,\n    verbose=0\n)","19d06a24":"print(model)","14a76f55":"train,test,submission = load_data()\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(train[c].values)\n\nfor c in test.columns:\n    if test[c].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(test[c].values)\n\n\ntarget = train.pop('target')\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","cb585893":"automodel = model.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n\npreds = automodel.predict(X_test)\n\nmse = round(mean_squared_error(y_test, preds),6)\n\n\nprint(mse)","af3e050c":"display(results)","5efc7a0d":"### Pycart - CatBoost Model - Submission","a7f661d3":"# Models with Optuna Hyperparameter Tuning\nOptuna is a black-box optimizer that needs an objective function. It returns a numerical value to evaluate the performance of the hyperparameters.","4c398ee9":"## Light GBM Regressor\n\n> ### The parameters need to tune to get good results on a leaf-wise tree algorithm:\n\n1. num_leaves      : the number of leaves should be smaller than 2^(max_depth)\n2. min_data_in_leaf: For a large dataset, it can be set to hundreds or thousands\n3. max_depth       : limit the depth of the tree\n\n> ### Faster speeds on the algorithm can be obtained by using:\n\n1. small max_bin\n2. save_binary to speed up data loading in future learning\n3. optimal bagging_freq and bagging_fraction\n4. feature_fraction for feature sub-sampling\n5. Use a small learning rate with large num_iterations\n\n> ### Avoid Overfitting\n\n1. Trying lambda_l1, lambda_l2, and min_gain_to_split for regularization\n2. Avoid growing a very deep tree","d06e3bb3":"### with GPU\n\nbest_params= {'l2_leaf_reg': 0.013856725926090555,'max_bin': 257,'bagging_fraction': 0.6788425346135741,'learning_rate': 0.010983813229740517,\n'max_depth': 10,'random_state': 24,'min_data_in_leaf': 300}","59961d5b":"### Predict on Test \/ Hold-out Sample","4132d954":"# Tabular Playground Series Competition - Feb 2021 \n\n\n### It's multivariate regression problem!\n\nIn this kernel, I am exploring the statistical regression models to predict the target with the given features (with categorical and continous variables) with the following libraries\/modules:\n\n> ### Benchmark models:\n- Artificial Neural Network\n- Scikit-Learn (Python ML Library)\n- Tabular Fastai\n\n> ### With Hyperparameter Tuning\n\n- CatBoost and XGBoost algorithms with Optuna Hyperparameter Tuning\n- PyCaret's Regression module\n\n\n__Bonus: AutoViML library__\n\nOur evaluation metric is \"Root Mean Squared Error' (RMSE). The lower is the RMSE, the better fit is the model.\n\n### Let's Start!! DO CARE TO UPVOTE\ud83d\ude01","398c8c54":"### Hyper parameters","0a36193d":"## Pull out the target, and make a validation split","db6b7713":"# Results and Submissions\n","b3f8d0ae":"## CatBoost","6795dac8":"### Unseen Data for Predictions","41517a05":"## Random Forest","54dfb42e":"## Encode the categoricals\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms.","a70a3abc":"# PyCaret - Statistical Models\n\n#### Read [**Tutorial on PyCaret Library**](https:\/\/github.com\/pycaret\/)","f0c35831":"### With CPU\n\nbest_trial = {'l2_leaf_reg': 0.02247766515106271, 'max_bin': 364, 'subsample': 0.6708650091202213,\n             'learning_rate': 0.010290546311954876, 'max_depth': 10, 'random_state': 24, 'min_data_in_leaf': 300,\n            'loss_function': 'RMSE','n_estimators':  25000,'rsm':0.5}","bd6f8c5e":"## CatBoost with fastai","a2bc12bf":"## Feature Importance","90ba7589":"## Random Forest","02e1d337":"### Predict on Unseen Data","22d7deb6":"# Read in the data files","f3a15d33":"## XGBoost with Optuna","7ec8c89b":"### Finalize Mode","052d10c3":"### Test data","49ee96f0":"# Regression on Deep Artificial Neural Networks\n\n\n[READ: Linear Regression Deep Neural Network](https:\/\/machinelearningmastery.com\/regression-tutorial-keras-deep-learning-library-python\/)","55d2d2a8":"# Using AutoViML \n\nAutoVIML is an open-source python package that makes machine learning easy.","5841da64":"## XGBoost","470b55ec":"## Decision Tree","bdd08b93":"### Saving the Model","1199c96d":"# Statistical Models with Fastai","abec4a20":"## Decision Tree","a23a7682":"# Scikit-Learn Library of Python","c44c62c7":"## CatBoost with Optuna"}}