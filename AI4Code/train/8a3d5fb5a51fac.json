{"cell_type":{"628b4b9e":"code","4f5255f6":"code","56dbb0cc":"code","e8439db2":"code","bad7add0":"code","67a333b9":"code","9a34f4e7":"code","ea5fb494":"code","0b25abdd":"code","1f45e5ee":"code","83c41bb6":"code","0f4cff0d":"code","109e964c":"code","dfe0c30d":"markdown","3348d8e3":"markdown","76bb4095":"markdown","0e5f4d46":"markdown","0991a773":"markdown","4facf1fc":"markdown","3aab9055":"markdown","588bd517":"markdown","cb25d9f2":"markdown"},"source":{"628b4b9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom scipy import stats\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4f5255f6":"sales_data_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\ncalendar_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')","56dbb0cc":"#picking out random products and analysing their sales per day\n\nd_cols = [x for x in sales_data_df.columns if 'd_' in x]\nids = sorted(list(set(sales_data_df['id'])))\n\n\nx_10 = sales_data_df.loc[sales_data_df['id'] == ids[10]].set_index('id')[d_cols].values[0]\nx_36 = sales_data_df.loc[sales_data_df['id'] == ids[36]].set_index('id')[d_cols].values[0]\nx_150 = sales_data_df.loc[sales_data_df['id'] == ids[150]].set_index('id')[d_cols].values[0]\n\n## Plotting the products\n\nfig = make_subplots(\n    rows=3, cols=1, shared_xaxes=False, vertical_spacing=0.08\n)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_10)), y=x_10,name='Sample_1'),\n              row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_36)), y=x_36,name='Sample_2'),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_150)), y=x_150,name='Sample_3'),\n              row=3, col=1)\n\nfig.update_layout(showlegend=False,height=1200, width=800,\n                  title_text=\"Randomly Picked Product Sales\")\nfig.show()","e8439db2":"product_sales = [(ids[x],sum(sales_data_df.loc[sales_data_df['id'] == ids[x]].set_index('id')[d_cols].values[0])) for x in range(0,len(ids)) ]","bad7add0":"maximum_sold_products = max([int(i[1]) for i in product_sales])\nminimum_sold_products = min([int(i[1]) for i in product_sales])\nmost_sold_product_id = [i[0] for i in product_sales if i[1] == maximum_sold_products]\nleast_sold_product_id = [i[0] for i in product_sales if i[1] == minimum_sold_products]\n\nmost_sold_product_sales = sales_data_df[sales_data_df['id']==most_sold_product_id[-1]].set_index('id')[d_cols].values[0]\nleast_sold_product_sales = sales_data_df[sales_data_df['id']==least_sold_product_id[-1]].set_index('id')[d_cols].values[0]\n\n## Plotting the products\n\nfig = make_subplots(\n    rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.08\n)\n\nfig.add_trace(go.Scatter(x=np.arange(len(most_sold_product_sales)), y=most_sold_product_sales,name='Most selling Product'),\n              row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(least_sold_product_sales)), y=least_sold_product_sales,name='Least selling Product'),\n              row=2, col=1)\n\nfig.update_layout(showlegend=False,height=1200, width=800,\n                  title_text=\"Most\/Least selling Product Sales\")\nfig.show()","67a333b9":"product_daily_sales = [ (i,sum(sales_data_df[i])) for i in d_cols]\nproduct_sales_df = pd.DataFrame(product_daily_sales,columns=['d','Total sales that day'])\n\nassert(list(product_sales_df['d']) == list(calendar_df['d'][:1913]))\ncalendar_df_merged = calendar_df.merge(product_sales_df,on='d',how='inner')\ncalendar_df_merged.shape","9a34f4e7":"calendar_df_merged.columns","ea5fb494":"# analysing sales according to calendar related features\nsales_by_weekday= calendar_df_merged.groupby('weekday').agg({'Total sales that day': 'sum'})\nsales_by_month= calendar_df_merged.groupby('month').agg({'Total sales that day': 'sum'})\nsales_by_year = calendar_df_merged.groupby('year').agg({'Total sales that day': 'sum'})\n\n\nsales_by_weekday = sales_by_weekday.reindex(index = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n\nfig = make_subplots(rows=4,cols=1)\n\nfig.add_trace(go.Bar(x=list(sales_by_weekday.index),y=list(sales_by_weekday['Total sales that day'])),row=1,col=1)\nfig.add_trace(go.Bar(x=list(sales_by_month.index),y=list(sales_by_month['Total sales that day'])),row=2,col=1)\nfig.add_trace(go.Bar(x=list(sales_by_year.index),y=list(sales_by_year['Total sales that day'])),row=3,col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(calendar_df['date'])),y=list(calendar_df_merged['Total sales that day'].rolling(90).mean())),row=4,col=1)\n\nfig.update_layout(showlegend=False,height=1000, width=1200,\n                  title_text=\"Sales versus Day\/Month\/Year\")\nfig.show()","0b25abdd":"calendar_df.head()","1f45e5ee":"## analysing patterns of sales across the states CA, TX and WC\n\nsales_by_state_per_day = [(sales_data_df.groupby('state_id').agg({ i : 'sum'})).to_dict('index') for i in d_cols]\n\n\nsales_CA = [sales_by_state_per_day[i]['CA'] for i in range(0,len(sales_by_state_per_day)) if 'CA' in sales_by_state_per_day[i].keys()]\nsales_TX = [sales_by_state_per_day[i]['TX'] for i in range(0,len(sales_by_state_per_day)) if 'TX' in sales_by_state_per_day[i].keys()]\nsales_WI = [sales_by_state_per_day[i]['WI'] for i in range(0,len(sales_by_state_per_day)) if 'WI' in sales_by_state_per_day[i].keys()]\n\n## Plotting the products\n\nfig = make_subplots(\n    rows=3, cols=1, shared_xaxes=False, vertical_spacing=0.08\n)\n\nfig.add_trace(go.Scatter(x=np.arange(len(sales_CA)), y=[sales_CA[i][\"d_{}\".format(i+1)] for i in range(0,len(sales_CA))],name='Sales in California'),\n              row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(sales_TX)), y=[sales_TX[i][\"d_{}\".format(i+1)] for i in range(0,len(sales_TX))],name='Sales in Texas'),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(sales_WI)), y=[sales_WI[i][\"d_{}\".format(i+1)] for i in range(0,len(sales_WI))],name='Sales in Wisconsin'),\n              row=3, col=1)\n\nfig.update_layout(showlegend=False,height=1200, width=800,\n                  title_text=\"Sales Analysis across CA,TX and WI\")\nfig.show()\n","83c41bb6":"### To analyse the drop in sales across all states on Day number 331,697,1062,1427 and 1792\ndrop_sales_dates = ['d_331','d_697','d_1062','d_1427','d_1792']\n[calendar_df[calendar_df['d']==i][['event_name_1','event_type_1']] for i in drop_sales_dates]","0f4cff0d":"### NOW WE CHECK SALES ACROSS ALL THE STORES ###\ndef store_sales(state,row,fig):\n    stores  = list(sales_data_df[sales_data_df['state_id']==state]['store_id'].unique())\n    df = pd.DataFrame(columns=[i for i in stores])\n    for i in range(0,len(stores)):\n    \n        store_daily_sales = [sum(sales_data_df[sales_data_df[\"store_id\"] == stores[i] ][k] ) for k in d_cols]\n        df[stores[i]] = store_daily_sales \n        fig.append_trace(go.Scatter(x=np.arange(90,len(store_daily_sales)), y=df[stores[i]].rolling(90).mean(),name = stores[i]),row=row,col=1)\n        fig.update_layout(showlegend=False,height=800, width=1100,\n            title_text=\"Sales Analysis across all stores\")\n    \nfig = make_subplots(rows=3,cols=1,vertical_spacing = 0.08)        \nstore_sales('CA',1,fig)\nstore_sales('TX',2,fig)\nstore_sales('WI',3,fig)\nfig.show()","109e964c":"fig = go.Figure()\ndef sell_price_per_store(store_id):\n    np.random.seed(0)\n    random_product_id = sell_prices_df['item_id'].sample(n=1).to_string().split('    ')[1]\n    random_product_price = sell_prices_df[(sell_prices_df['item_id']==random_product_id) & (sell_prices_df['store_id']==store_id)]\n    fig.add_trace(go.Scatter(x=sorted(random_product_price['wm_yr_wk']),y = random_product_price['sell_price'],name = store_id))\nfor i in list(sell_prices_df['store_id'].unique()):\n    sell_price_per_store(i)\nfig.show()    ","dfe0c30d":"As expected behaviour, the sudden drop in sales is due to Christmas Day.","3348d8e3":"Now analysing the trends and seasonality in the sales data by randomly picking products from the dataset","76bb4095":"There seems to be no particular seasonality or trend for the products plotted above. Let's analyse further and merge calendar dataframe to the sales dataframe for better insights","0e5f4d46":"* Alright, we have a general increasing trend in the sales numbers for all the states.\n* Another thing to notice is the drop in sales(seasonality) in all states on a particular day each year which are interestingly seperated by 365 days. Let's check if there are any events on the day this drop takes place.","0991a773":"Observations:\n\n    - Price of the product show a similar pattern in price increase across all stores\n    - But in the initial phases, the price deviation is high, like for store TX_3, the price is 2.78 USD, and another store in Texas has price 3.48 USD (TX_2)\n    - Maybe the stores must have discussed policies to keep the prices same across all states and stores, hence the similar pattern in pricing","4facf1fc":"1. Sales across all the stores GENERALLY show a increasing trend in sales across time.\n2. There's almost a sine pattern in the sales in the form of waves(with varying amplitude) for **ALMOST** all the stores\n3. Few stores like WI_3 and CA_2 have sales going in the downward direction then improve at the end\n\nNow let's look further and try to understand whether selling price has insights too or not!\n\nLet's pick up a random product and check it's selling price affected over time across all stores","3aab9055":"The above products analysed were picked randomly and don't seem to have any trend or seasonality. \nLet's pick a product with the highest and the lowest sales, and see if they have a trend.","588bd517":"**To do:\n\nForecasting Model\n\n\n# ***Any kind of Feedback is highly appreciated *** ","cb25d9f2":"**OBSERVATIONS:**\n1. Weekly sales increase around the weekends, which is expected behaviour\n2. Highest Number of Products are sold in March, ****(which needs to be further analysed)****\n3. There is a sharp downward dip in product sales from 2015 to 2016. Probably because the data from the whole year(2016) is not captured\n4. The final graph is reminiscent of the business cycle, where economies have short-term oscillatory fluctuations but grow linearly in the long run. Maybe, such small-scale trends at the level of stores add up to decide trends we see at the macroeconomic level. Below is an illustration of the macroeconomic business cycle:\n\n![image.png](attachment:image.png)"}}