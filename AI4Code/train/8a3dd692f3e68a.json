{"cell_type":{"e34a0c93":"code","710980a5":"code","299c64cb":"code","b27b8ad7":"code","6c26dbd2":"code","c0caad6c":"code","0a2cc906":"code","f50fcf1a":"code","307c28f6":"code","fcc98cd1":"code","eb8c82d2":"code","c961fbf7":"markdown","c87317e7":"markdown","8b200292":"markdown"},"source":{"e34a0c93":"# installing necessary packages here\n!pip install natsort","710980a5":"import numpy as np \nimport pandas as pd \nfrom glob import glob\nfrom skimage.io import imread\nimport skimage.io as sio\nimport os\nfrom natsort import natsorted\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom skimage.transform import resize, rotate\nimport warnings; warnings.filterwarnings(\"ignore\")","299c64cb":"# train test directories\nroot_dir = \"..\/input\"\ntrain_dir = root_dir + \"\/train\/train\/\"\ntest_dir  = root_dir + \"\/test\/test\/\"\ncsv_path  = root_dir + \"\/train.csv\"\nsub_path  = root_dir + \"sample_submission.csv\"\n\n# loading images\ndf   = pd.read_csv(csv_path)\nx    = np.array([ imread(train_dir+p) for p in df.id.values])\ny    = df.has_cactus.values","b27b8ad7":"# splitting training dataset into train\/validation\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20,stratify=y)","6c26dbd2":"# helper functions here\ndef display_images(imgs,y=None, y_pred=None):\n    n_images = imgs.shape[0]\n    n_gridx  = 5\n    n_gridy  = n_images\/\/n_gridx\n#     n_grid   = int(np.sqrt(n_images))\n    k = 1\n    plt.figure(figsize=(10,6),frameon=False)\n    for i in range(n_gridy):\n        for j in range(n_gridx):\n            plt.subplot(n_gridy, n_gridx, k)\n            plt.imshow(imgs[k-1])\n            plt.axis(\"off\")\n            if (y is not None) and (y_pred is not None):\n                plt.title(\"y=%d | pred=%0.1f\"%(y[k-1],y_pred[k-1]))\n            elif y is not None:\n                plt.title(\"y=%d\"%y[k-1])\n            k+=1\n    plt.tight_layout()\n    plt.show()\n    \n# conv-net block\ndef conv(x, filters, kernel_size, strides, padding, batchnorm=True, activation=\"relu\"):\n    x = tf.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding )(x)\n    x = tf.layers.BatchNormalization()(x)\n    if activation == \"relu\":\n        x = tf.nn.relu(x)\n    if activation == \"leakyrelu\":\n        x = tf.nn.leaky_relu(x, alpha=0.01)\n    return x\n\n\ndef augmenter(images, fliplr=True,flipud=True,rot=5,rot_mode='edge',rot90=True):\n    aug_images = []\n    for image in images:\n        aug_img = image\n        if fliplr: # randomly fliplr\n            if np.random.uniform(low=0, high=1) > 0.5:\n                aug_img = np.fliplr(aug_img)\n        if flipud: # randomly flipud\n            if np.random.uniform(low=0, high=1) > 0.5:\n                aug_img = np.flipud(aug_img)\n        if rot90: # randomly rotate 90 degrees\n            if np.random.uniform(low=0, high=1) > 0.5:\n                k = np.random.randint(low=0, high=4)\n                aug_img = np.rot90(aug_img, k = k, axes=(0,1))\n        if rot != None: # randomly rotate if True\n            if np.random.uniform(low=0, high=1) > 0.5:\n                angle = np.random.uniform(low=0, high = rot)\n                aug_img = rotate(aug_img, angle, mode = rot_mode, preserve_range=True)\n        aug_images.append(aug_img)\n    return np.array(aug_images)","c0caad6c":"n_samples  = 20\nidx_sample = np.random.randint(0,len(x_train),n_samples)\ndisplay_images(x_train[idx_sample], y_train[idx_sample])","0a2cc906":"x_input = tf.placeholder(\"float32\",shape=(None,32,32,3))\ny_label = tf.placeholder(\"uint8\",shape=(None))\ny_onehot= tf.one_hot(y_label, depth=2)\n\n# conv-net here\nh1 = conv(x_input, filters=16, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh1 = conv(h1,      filters=16, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh1 = tf.layers.MaxPooling2D(pool_size=2,strides=2)(h1)\n\nh2 = conv(h1, filters=32, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh2 = conv(h2, filters=32, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh2 = tf.layers.MaxPooling2D(pool_size=2,strides=2)(h2)\n\nh3 = conv(h2, filters=64, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh3 = conv(h3, filters=64, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh3 = tf.layers.MaxPooling2D(pool_size=2,strides=2)(h3)\n\nh4 = conv(h3, filters=128, kernel_size=3, strides=1, padding=\"same\", batchnorm=True, activation=\"leakyrelu\")\nh4 = conv(h4, filters=128, kernel_size=3, strides=1, padding=\"valid\", batchnorm=True, activation=\"leakyrelu\")\nh4 = tf.layers.MaxPooling2D(pool_size=2,strides=2)(h4)\n\nh5       = tf.layers.Flatten()(h4)\ny_logits = tf.layers.Dense(2)(h5)\ny_prob   = tf.nn.softmax(y_logits)\ny_pred   = tf.arg_max(y_logits, -1)","f50fcf1a":"# defining losses\nhinge_loss = tf.losses.hinge_loss(labels=y_onehot, logits=y_logits, )\nce_loss   = tf.losses.softmax_cross_entropy(y_onehot, y_logits)\ntotal_loss = 0.0*hinge_loss + ce_loss\n\n# defining model optimizer\nopt     = tf.train.AdamOptimizer(learning_rate=0.001,).minimize(total_loss)\n\n# starting session and init ops\nsess = tf.Session()\nglobal_init = tf.global_variables_initializer()\nlocal_init  = tf.local_variables_initializer()\n_ = sess.run([global_init, local_init])","307c28f6":"# training here\nepochs    = 100\nbatchsize = 32\n\nn_train   = len(x_train)\nn_batches = len(x_train)\/\/batchsize\nfor e in range(epochs):\n    shuffled_idx0 = np.random.permutation(np.where(y_train == 0)[0])\n    shuffled_idx1 = np.random.permutation(np.where(y_train == 1)[0])\n    run_metrics = {\"loss\":[],\"accuracy\":[], \"recall\":[], \"precision\":[],\"f1\":[], \"roc\":[]}\n    print(\"+-\"*15+\"\\t epoch = %0.3d \\t\"%e+\"-+\"*15)\n    for i in range(n_batches):\n        batch0_idx = np.random.randint(0, len(shuffled_idx0), batchsize\/\/2)\n        batch0_idx = shuffled_idx0[batch0_idx]\n        batch1_idx = np.random.randint(0, len(shuffled_idx1), batchsize\/\/2)\n        batch1_idx = shuffled_idx1[batch1_idx]\n        x0_batch   = x_train[batch0_idx]\n        x1_batch   = x_train[batch1_idx]\n        y0_batch   = y_train[batch0_idx]\n        y1_batch   = y_train[batch1_idx]\n        x_batch    = np.concatenate([x0_batch, x1_batch])\n        x_batch    = augmenter(x_batch)\n        y_batch    = np.concatenate([y0_batch, y1_batch])\n        _, y_pred_batch, y_prob_batch, loss_batch = sess.run([opt, y_pred, y_prob, total_loss], \n                                                             feed_dict={x_input: x_batch, \n                                                                        y_label: y_batch.reshape(-1,1)})\n        acc       = accuracy_score(y_batch, y_pred_batch,)\n        prec      = precision_score(y_batch, y_pred_batch)\n        rec       = recall_score(y_batch, y_pred_batch)\n        f1        = f1_score(y_batch, y_pred_batch)\n        roc       = roc_auc_score(y_batch, y_prob_batch[:,1])\n        run_metrics[\"loss\"].append(loss_batch)\n        run_metrics[\"accuracy\"].append(acc)\n        run_metrics[\"recall\"].append(rec)\n        run_metrics[\"precision\"].append(prec)\n        run_metrics[\"f1\"].append(f1)\n        run_metrics[\"roc\"].append(roc)\n\n        print(\"loss = %0.3f \\t acc = %0.3f \\t roc = %0.3f \\t precision = %0.3f \\t recall = %0.3f \\t f1 = %0.3f\"%\n              (np.mean(run_metrics[\"loss\"]),\n               np.mean(run_metrics[\"accuracy\"]), \n               np.mean(run_metrics[\"roc\"]), \n               np.mean(run_metrics[\"precision\"]), \n               np.mean(run_metrics[\"recall\"]),\n               np.mean(run_metrics[\"f1\"])),end=\"\\r\")\n    print(\"train :: loss = %0.3f :: acc = %0.3f :: roc = %0.3f :: precision = %0.3f :: recall = %0.3f :: f1 = %0.3f\"%\n      (np.mean(run_metrics[\"loss\"]),\n       np.mean(run_metrics[\"accuracy\"]), \n       np.mean(run_metrics[\"roc\"]), \n       np.mean(run_metrics[\"precision\"]), \n       np.mean(run_metrics[\"recall\"]),\n       np.mean(run_metrics[\"f1\"])))\n    # computing validation score\n    y_pred_batch, y_prob_batch, loss_batch = sess.run([y_pred,y_prob, total_loss], \n                                                      feed_dict={x_input: x_val, \n                                                                 y_label: y_val.reshape(-1,1)})\n    acc       = accuracy_score(y_val, y_pred_batch,)\n    prec      = precision_score(y_val, y_pred_batch)\n    rec       = recall_score(y_val, y_pred_batch)\n    f1        = f1_score(y_val, y_pred_batch)\n    roc       = roc_auc_score(y_val, y_prob_batch[:,1])\n    print(\"valid :: loss = %0.3f :: acc = %0.3f :: roc = %0.3f :: precision = %0.3f :: recall = %0.3f :: f1 = %0.3f\\n\\n\"%\n          (loss_batch,acc,roc, prec, rec,f1))","fcc98cd1":"# saving session\nsaver = tf.train.Saver()\nsaver.save(sess, \"cactus_net.ckpt\")","eb8c82d2":"df_test = pd.read_csv('..\/input\/sample_submission.csv')\nx_test  = np.array([ imread(test_dir+p) for p in df_test.id.values])\nx_test  = np.array(x_test)\n\n# test prediction\nfrom tqdm import tqdm\nbatchsize = 32\nn_batches = (len(x_test) \/\/ batchsize) + batchsize\ny_pred_test = []\nlast_iter = False\nfor i in tqdm(range(n_batches)):\n    start     = i*batchsize\n    end       = (i+1)*batchsize\n    if end >= len(x_test):\n        if last_iter:\n            break\n        end  = len(x_test)\n        last_iter = True    \n    y_prob_batch = sess.run([y_prob], feed_dict={x_input: x_test[start:end]})[0]\n    y_prob_batch = y_prob_batch[:,1]\n    y_pred_test.extend(y_prob_batch)\ny_pred_test = np.array(y_pred_test)\n\ndf_test['has_cactus'] = y_pred_test\ndf_test.to_csv('cactus_net_submission.csv', index=False)","c961fbf7":"# Test Prediction","c87317e7":"# ConvNet","8b200292":"## VISUALIZING CACTUS IMAGES"}}