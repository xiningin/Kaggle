{"cell_type":{"c7eac86d":"code","b35f4c17":"code","e671a5c4":"code","a82cc534":"code","74587924":"code","5323c396":"code","57248415":"code","a08f897f":"code","d596596c":"code","19bde7b1":"code","14145f6d":"code","4663ba91":"code","d8459746":"code","1d303c4b":"code","88c65aca":"code","1b10e108":"code","7aeda8fd":"code","e74b57a9":"code","41468776":"code","a15f1e2e":"code","50bfb79c":"code","46cd9906":"code","f7e05a6c":"code","9a5a01da":"code","9b63830c":"code","1a9e0183":"code","58422c93":"code","952c50fb":"code","80ecda55":"code","4a9820ce":"code","5a50f52d":"code","fce03178":"code","048d3600":"code","506484b6":"code","a93cafd7":"code","3ec236ba":"code","5e2eb92e":"code","d3908c8f":"code","949d6aaa":"code","f041f515":"code","292f3707":"markdown","c52c3deb":"markdown","21b5c6fc":"markdown","315ce9e1":"markdown","085a6d4d":"markdown","c7b7b52e":"markdown","2e4b2358":"markdown","7ed631a5":"markdown","6c1471aa":"markdown","8b933074":"markdown","dce5f1c8":"markdown","4d8ef6db":"markdown","7178868b":"markdown","651596ed":"markdown","28abe0f2":"markdown","5775be7d":"markdown","6ae7e934":"markdown"},"source":{"c7eac86d":"\n\n# imports \n\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom functools import partial\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, RBF, RationalQuadratic, ExpSineSquared\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom functools import partial\nfrom sklearn.model_selection import GridSearchCV\n\nfrom IPython.display import display\nimport time\n\nimport warnings\npd.options.display.max_colwidth = 200\nwarnings.filterwarnings(action='ignore')\n\n","b35f4c17":"data = pd.read_csv(r'..\/input\/banknote-authentication-uci\/BankNoteAuthentication.csv')\n# Split it to train and test\ntrain_data, test_data = train_test_split(data) \ntrain_data","e671a5c4":"# data information\ntrain_data.info()","a82cc534":"# Statistical description\ntrain_data.describe()","74587924":"numerical_features = ['variance','skewness','curtosis','entropy']","5323c396":"def removeNonAlphanumeric(df) :\n    \"\"\" \n    Remove non-alphanumeric characters from data values\n    Input :\n        df -- dataframe \n    Output :\n        df -- cleaned dataframe\n    \"\"\"\n    for c in df.columns :\n        if df[c].dtype == \"O\" :\n            df[c] = df[c].str.replace('\\t', '')\n            df[c] = df[c].str.replace('[^a-zA-Z0-9]', '')\n    df = df.replace('',np.nan)\n    return df\n\ndef toNumeric(df):\n    \"\"\"\" \n    Convert string column corresponding to numerical values to numerical columns\n    Input : \n        df -- dataframe \n    Output :\n        df -- dataframe with converted columns\n    \"\"\"\n    for c in df.columns :\n        if df[c].dtype == \"O\" and all(df[c].str.isnumeric()):\n            df[c] = pd.to_numeric(df[c])\n    return df\n            \n\n\nclass HandleMissingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Customized transformer to handles missing data\"\"\"\n    \n    def __init__(self, method,constant = ''):\n        '''' \n        Initialise The transformer\n        Inputs :\n            method -- method used to replace or impute missing data (drop\/constant\/most_frequent\/median\/mean)\n            constant -- if constant method is selected, the value of the constant must be specified\n        '''\n        self.method = method\n        self.constant = constant\n        self.imputerDict = {}\n        \n\n    def fit(self, df ):\n        '''\n        If impute method is selected i.e self.method not in ['drop', 'constant'], we must fit an imputer\n        Input : \n            df -- data with missing\n        '''\n        if self.method not in ['drop', 'constant'] :\n            if self.method != \"most_frequent\":\n                print(\"For non numerical columns, most frequent strategy is used\")\n            for c in df.columns :\n                imp = SimpleImputer(missing_values=np.nan, strategy=self.method if df[c].dtype!=\"O\" else \"most_frequent\")\n                imp = imp.fit(df[[c]])\n                self.imputerDict[c] = imp \n        return self\n            \n                \n        \n    def transform(self, df):\n        \"\"\"\n        If impute method is selected, impute missing values using imput_dict created in fit function\n        Input : \n            df -- data with missing values\n        \"\"\"\n        if self.method == \"drop\" :\n            df = df.dropna(inplace= True)\n        elif self.method == 'constant' :\n            df.fillna(self.constant, inplace= True)\n        else :\n            for c in df.columns : \n                df[c] = self.imputerDict[c].transform(df[[c]])\n        return df  \n    \ndef getCategFeat(df, n, target):\n    \"\"\"\n    get dataframe's categorical features \n    Inputs :\n        df     -- dataframe  \n        n      -- min modalities for numerical features\n        target -- target column name\n    \"\"\"\n    return [c for c in df.columns if (df[c].dtype == 'O' or df[c].nunique()<n) and c!=target]","57248415":"# Remove non alphanumeric \ntransf_alphaN = FunctionTransformer(removeNonAlphanumeric, validate= False)\ntransf_num = FunctionTransformer(toNumeric, validate= False)\ntrain_data = transf_alphaN.transform(train_data)\ntrain_data = transf_num.transform(train_data)","a08f897f":"# Get columns with null values\nprint(\"Columns with null values before imputing\")\nprint(train_data.columns[train_data.isna().any()].tolist())\n# Handle missing values\n#df,imput_dict = handleMissing(train_data, \"most_frequent\")\ntransf_Missing = HandleMissingTransformer(method=\"median\")\ntrain_data = transf_Missing.fit(train_data).transform(train_data)\nprint(\"Columns with null values after imputing\")\nprint(train_data.columns[train_data.isna().any()].tolist())","d596596c":"def target_variable_exploration(df, target, xlabel, ylabel, title, positive=1) :\n    \"\"\" \n    plots the distribution of the classes\n    Input :\n        df -- dataframe containing classes\n        target -- class column\n        xlabel\n        ylabel \n        title\n        positive -- modality corresponding to positive class\n    \"\"\"\n    negative =  [c for c in df[target].unique() if c !=positive][0]\n    positive_class = df[target].value_counts()[positive]\n    negative_class = df[target].shape[0] - positive_class\n    positive_per = positive_class \/ df.shape[0] * 100\n    negative_per = negative_class \/ df.shape[0] * 100\n    plt.figure(figsize=(8, 8))\n    sns.countplot(df[target], order=[positive, negative]);\n    plt.xlabel(xlabel, size=15, labelpad=15)\n    plt.ylabel(ylabel, size=15, labelpad=15)\n    plt.xticks((0, 1), [ 'Positive class ({0:.2f}%)'.format(positive_per), 'Negative class ({0:.2f}%)'.format(negative_per)])\n    plt.tick_params(axis='x', labelsize=13)\n    plt.tick_params(axis='y', labelsize=13)\n    plt.title(title, size=15, y=1.05)\n    plt.show()","19bde7b1":"### Target variable exploration\ntarget_variable_exploration(train_data, \"class\", 'Class?', ' Count', 'Training Set class Distribution')","14145f6d":"sns.pairplot(train_data, diag_kind ='hist' , hue=\"class\")\nplt.show()","4663ba91":"def plot_numeric(data, numeric_features, target) :\n    \"\"\" \n    plots analysing numerical features\n    Inputs : \n        data -- dataframe containing features to plot\n        numeric_features -- list of numerical features\n        target -- target column name\n     \"\"\"\n    # Looping through and Plotting Numeric features\n    for column in numeric_features:    \n        # Figure initiation\n        fig = plt.figure(figsize=(18,12))\n\n        ### Distribution plot\n        sns.distplot(data[column], ax=plt.subplot(221));\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Density', fontsize=14);\n        # Adding Super Title (One for a whole figure)\n        plt.suptitle('Plots for '+column, fontsize=18);\n\n        ### Distribution per Positive \/ Negative class Value\n        # Not Survived hist\n        classes = data[target].unique()\n        sns.distplot(data.loc[data[target]==classes[0], column].dropna(),\n                     color='red', label=str(classes[0]), ax=plt.subplot(222));\n        # Survived hist\n        sns.distplot(data.loc[data[target]==classes[1], column].dropna(),\n                     color='blue', label=str(classes[1]), ax=plt.subplot(222));\n        # Adding Legend\n        plt.legend(loc='best')\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Density per '+ str(classes[0])+' \/ '+str(classes[1]), fontsize=14);\n\n        ### Average Column value per positive \/ Negative Value\n        sns.barplot(x=target, y=column, data=data, ax=plt.subplot(223));\n        # X-axis Label\n        plt.xlabel('Positive or Negative?', fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Average ' + column, fontsize=14);\n\n        ### Boxplot of Column per Positive \/ Negative class Value\n        sns.boxplot(x=target, y=column, data=data, ax=plt.subplot(224));\n        # X-axis Label\n        plt.xlabel('Positive or Negative ?', fontsize=14);\n        # Y-axis Label\n        plt.ylabel(column, fontsize=14);\n        # Printing Chart\n        plt.show()\n        \n","d8459746":"### Plotting Numeric Features\nplot_numeric(train_data, numerical_features, 'class')","1d303c4b":"def correlationMap(df, target) :\n    \"\"\" \n    Correlation Heatmap\n    Inputs : \n        df -- dataframe containing features to plot\n        target -- target column name\n     \"\"\"\n    classes = df[target].unique()\n    if data[target].dtype == 'O' :\n        df[target+'_id'] = (df[target]== classes[0]).astype(int) #encode string target \n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(12, 9))\n    sns.heatmap(corr, vmax=.8,annot=True, square=True)\n    if data[target].dtype == 'O' :\n        df.drop([target+'_id'], axis=1, inplace=True)\n    # fix for matplotlib bug that cuts off top\/bottom of seaborn viz\n    b, t = plt.ylim() # Gets the values for bottom and top\n    b += 0.5 # Add 0.5 to the bottom\n    t -= 0.5 # Subtract 0.5 from the top\n    plt.ylim(b, t) # update the ylim(bottom, top) values\n    plt.show()","88c65aca":"# Correlation Analysis\n\ncorrelationMap(train_data,'class')\n","1b10e108":"def featureEng(numerical_features, categorical_features):\n    \"\"\" \n    create pipeline for feature preprocessing \n    Inputs : \n        numerical_features -- list of numerical features\n        categorical_features -- list of categorical features\n    Outputs :\n        preproc -- pipeline with feature preprocessing steps\n     \"\"\"\n    numeric_transformer = StandardScaler()\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n    t =  ColumnTransformer([('Scaler', numeric_transformer, numerical_features),('OneHotEncod', categorical_transformer, categorical_features)])\n    preproc = Pipeline(steps=[('preprocessor', t)])\n    return preproc","7aeda8fd":"# Encoding categorical features\n\n\ntransf_train = featureEng(numerical_features, categorical_features=[]).fit(train_data)\nX_train = transf_train.transform(train_data)\ny_train = train_data['class'].values","e74b57a9":"#  get columns names after transformations\nX_train.shape","41468776":"# PCA on numerical features\n\npca = PCA(n_components=X_train.shape[1])\nprincipalComponents = pca.fit_transform(X_train[:,:len(numerical_features)])\nprincipalDf = pd.DataFrame(data = principalComponents[:,:2]\n             , columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, pd.DataFrame(y_train)], axis = 1)","a15f1e2e":"pca.explained_variance_","50bfb79c":"pca.explained_variance_ratio_","46cd9906":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0, 1]\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf[0] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","f7e05a6c":"# First Component\nn_axes = len(numerical_features)\n_, axes = plt.subplots(ncols=2,nrows=2, figsize=(20,10))\ncol_id = 0\n\nfor i in range(axes.shape[0]):\n    for j in range(axes.shape[1]):\n        try :\n            axes[i][j].scatter(principalComponents[:,0], X_train[:,col_id])\n            axes[i][j].set_title(f'1st component vs {numerical_features[col_id]}')\n            col_id = col_id+1\n        except exception as e:\n            print(e)\n            break","9a5a01da":"# Second component\nn_axes = len(numerical_features)\n_, axes = plt.subplots(ncols=2,nrows=2, figsize=(20,10))\ncol_id = 0\n\nfor i in range(axes.shape[0]):\n    for j in range(axes.shape[1]):\n        try :\n            axes[i][j].scatter(principalComponents[:,1], X_train[:,col_id])\n            axes[i][j].set_title(f'1st component vs {numerical_features[col_id]}')\n            col_id = col_id+1\n        except exception as e:\n            print(e)\n            break","9b63830c":"components = pca.components_\nplt.figure(figsize=(10,10))\nfor i, (x, y) in enumerate(zip(components[0,:], components[1,:])):\n    plt.plot([0, x], [0, y], color='k')\n    plt.text(x, y, numerical_features[i])\n\nplt.plot([-0.7, 0.7], [0, 0], color='grey', ls='--')\nplt.plot([0, 0], [-0.7, 0.7], color='grey', ls='--')\n\nplt.xlim(-0.7, 0.7)\nplt.ylim(-0.7, 0.7);","1a9e0183":"class selectFeaturesTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Custom scaling transformer\"\"\"\n    def __init__(self, k=10,method='RF',discreteCol=[]):\n        \"\"\" \n        initialize transformer\n        Inputs : \n            k -- number of features to keep\n            method -- method to use, either 'Mutual Information or RF\n            discreteCol -- if Mutual Information is used, specify indexes of discrete columns\n        \"\"\"\n        self.k = k\n        self.method = method\n        self.order = []\n        self.discreteCol = discreteCol\n        \n        \n        \n\n    def fit(self, X_train,y_train):\n        \"\"\"\n        Fit the transformer on data\n        Input :\n            X_train -- features array\n            Y_train -- labels array\n        Output :\n            fitted transformer\n        \"\"\"\n        if self.method == \"Mutual Information\" :\n            discrete_mutual_info_classif = partial(mutual_info_classif, \n                                                   discrete_features=self.discreteCol)\n            featS = SelectKBest(k=self.k, score_func=discrete_mutual_info_classif).fit(X_train,y_train )\n            self.order = np.flip(featS.scores_.argsort())\n            #self.selectedColumns = [columns_eng[i]  for i in self.order[:self.k]]\n            #return X_train[:,order_mi[:self.k]]\n        \n        else :\n            rfModel = RandomForestClassifier(random_state =0).fit(X_train, y_train)\n            order = np.flip(rfModel.feature_importances_.argsort())\n            self.order = np.flip(rfModel.feature_importances_.argsort())\n            #self.selectedColumns = [columns_eng[i]  for i in order_rf[:self.k]]\n            #return X_train[:,order_[:self.k]]\n        return self\n            \n                \n        \n    def transform(self, X_train):\n        \"\"\"\n        apply fitted transformer to select features\n        Input :\n            X_train -- features array\n        Output :\n            array containing only selected features\n        \"\"\"\n        return X_train[:,self.order[:self.k]]","58422c93":"\n\ndiscreteCol = []\n\nFSelector_mi = selectFeaturesTransformer(k=4,method=\"Mutual Information\", discreteCol=False)\nFSelector_rf = selectFeaturesTransformer(k=4,method=\"Random Forest\")\nFSelector_mi.fit(X_train,y_train)\nFSelector_rf.fit(X_train,y_train)","952c50fb":"print(\"Features ordered by importance selected by Mutual information\")\nprint([numerical_features[i]  for i in FSelector_mi.order[:10]])\nprint(\"Features ordered by importance selected by Random Forest\")\nprint([numerical_features[i]  for i in FSelector_rf.order[:10]])","80ecda55":"\nclassifiers = [\n    SGDClassifier(loss='log'), # for logistic regression\n   KNeighborsClassifier(),\n    SVC(),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    MLPClassifier(),\n    GaussianNB()]\n\nker_rbf = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n\nker_rq = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RationalQuadratic(alpha=0.1, length_scale=1)\n\n#ker_expsine = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1))\n\nkernel_list = [ker_rbf, ker_rq]\n\nnames = [\"Logistic Regression with SGD\", \"Nearest Neighbors\", \"SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\",\"Gradient Boosting\", \"Neural Net\",\n         \"Naive Bayes\"]\n\nparameters = {\"Logistic Regression with SGD\" : {'Classifier__penalty':['l1','l2',None],\n                                               'Classifier__learning_rate' : ['constant','optimal','adaptive'],\n                                               'Classifier__eta0' : [0.1]},\n    \"Nearest Neighbors\" : {'Classifier__n_neighbors':[5,8,10]},\n        'SVM':{'Classifier__kernel':['linear','rbf'],'Classifier__C':[0.1,0.5,1.,1.5]},\n        \"Gaussian Process\":{\"Classifier__kernel\": kernel_list,\n                            \"Classifier__n_restarts_optimizer\": [1, 2, 3]},\n        \"Decision Tree\" : {\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[10,30,50,None]}\n        ,\"Random Forest\":{\"Classifier__n_estimators\":[8,10,20,50],\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[10,30,50,None]},\n       'Gradient Boosting':{\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[2,3,10],\n                        \"Classifier__learning_rate\":[1e-1,1e-2,1e-3]},\n         \"Neural Net\" : {'Classifier__hidden_layer_sizes': [(20,20,20), (25,50,25), (50,50)],\n                      'Classifier__activation': ['tanh', 'relu'],\"Classifier__learning_rate_init\":[1e-1,1e-2,1e-3]},\n        \"Naive Bayes\" : {\"Classifier__var_smoothing\" : [1e-8, 1e-9]}\n         }\nparameters_featuresSelection = {'FeatureSelection__method':['RF'],'FeatureSelection__k':[2,3,4]}","4a9820ce":"\n\ndef train(X_train, y_train, classifiers, names,parameters, parameters_featuresSelection, crossVal = True):\n    \"\"\" \n    training process\n    Inputs : \n        X_train -- features array\n        Y_train -- labels array\n        classifiers -- list of classifiers to test\n        names -- list of classifiers names\n        parameters -- tuning parameters corresponding for classifiers\n        parameters_featuresSelection -- parameters for fearures selection\n        crossVal -- whether to use cross validation or not\n     \"\"\"\n    results = pd.DataFrame()\n    for name, clf in zip(names, classifiers):\n        print('############# ', name, ' #############')\n        start = time.time()\n        #print(params[name])\n        FSelector = selectFeaturesTransformer()\n        pipeline = Pipeline([('FeatureSelection',FSelector),('Classifier',clf)])\n        parameters[name]['FeatureSelection__method'] = parameters_featuresSelection['FeatureSelection__method']\n        parameters[name]['FeatureSelection__k']= parameters_featuresSelection['FeatureSelection__k']\n        if crossVal:\n            classifier = GridSearchCV(pipeline, parameters[name], cv=3)\n        else:\n            classifier = pipeline\n        #print(classifier)\n        classifier.fit(X_train, y_train)\n        # All results\n        means = classifier.cv_results_['mean_test_score']\n        stds = classifier.cv_results_['std_test_score']\n        r = pd.DataFrame(means,columns = ['mean_test_score'])\n        r['std_test_score'] = stds\n        r['params'] = classifier.cv_results_['params']\n        r['classifier'] = name\n        \n        print('Training time (Cross Validation = ',crossVal,') :',(time.time()-start)\/len(means))\n        display(r.sort_values(by=['mean_test_score','std_test_score'],ascending =False))\n        results = pd.concat([results, r], ignore_index=True)\n        #for mean, std, params in zip(means, stds, classifier.cv_results_['params']):\n        #print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    results_sorted = results.sort_values(by=['mean_test_score','std_test_score'],ascending =False)\n    return results_sorted\n","5a50f52d":"np.random.seed(44)","fce03178":"results = train(X_train, y_train, classifiers, names,parameters, parameters_featuresSelection)    ","048d3600":"results_sorted = results.sort_values(by=['mean_test_score','std_test_score'],ascending =False)\nresults_sorted.iloc[:10]","506484b6":"results.groupby('classifier').head(3)","a93cafd7":"# apply transformations on test data\ntest_data = transf_alphaN.transform(test_data)\ntest_data = transf_num.transform(test_data)\ntest_data = transf_Missing.transform(test_data)","3ec236ba":"y_test = test_data['class'].values\nX_test = transf_train.transform(test_data)","5e2eb92e":"X_test","d3908c8f":"model_selected = results.iloc[0]\nmodel = classifiers[names.index(model_selected['classifier'])]\nparam = {key.split('__')[1]:val for key,val in model_selected['params'].items() if 'FeatureSelection' not in key } \nmodel.set_params(**param)","949d6aaa":"model.fit(X_train,y_train)\nmodel.score(X_test, y_test)","f041f515":"print(classification_report(y_test, model.predict(X_test), target_names=['0','1']))","292f3707":"As mentionned before, variables \"skewness\" and \"curtoisis\" are negatively correlated.\nVariance is stongly correlated with classification in comparison with other variables. This confirm our previous guess that it might be the most influential variable.","c52c3deb":"### Features Selection","21b5c6fc":"- Entropy variable present some outliers.","315ce9e1":"    - Curtoisis and skewness are negatively correlated ;\n    - From Variance distribution plot, we can see that it is very discriminative. It might be might be the most influential variable.","085a6d4d":"## <center> EDA + Binary Classi\ufb01cation on Banknote Authentication dataset  <center>","c7b7b52e":"The two PCs verifies Kaiser rule.","2e4b2358":"SVM and Neural Network give the best performances. We apply SVM model the test data (Occham's razor)","7ed631a5":"Both method find the same order, with variance at the top. This confirms our previous remarks.","6c1471aa":"The classes are  balanced.","8b933074":"Skewness and Curtoisis are correlated with PC1 while variance and entropy are correlated with the second.","dce5f1c8":"### Training :","4d8ef6db":"PCs represents relatively well initial variables.","7178868b":"### Exploratory Data Analysis","651596ed":"### Evaluation :","28abe0f2":"### Scaling","5775be7d":"### PCA","6ae7e934":"### Read Data"}}