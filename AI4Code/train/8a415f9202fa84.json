{"cell_type":{"a4335306":"code","5091e019":"code","783bd9b5":"code","53951ba0":"code","b4dc4271":"code","c7cb235c":"code","a69dff67":"code","2f693cd6":"code","58ec1e49":"code","566d1b7e":"code","98f6de68":"code","bf40a053":"code","0867d7a9":"code","6da743c4":"code","c1b83a8e":"code","84be860e":"code","528055d8":"code","140dc6f8":"code","9369323f":"code","26fdbb29":"code","95e5ebf3":"code","bad000e6":"code","d8b68c4a":"code","bcc24870":"code","f3c7e357":"code","c96a246a":"code","079dadd4":"code","f275a69a":"code","119ccc25":"code","99bd5d63":"code","b998fe52":"code","cfd367b9":"code","df45e213":"code","b1135248":"code","458085dc":"code","1f52bab9":"code","1fa0e258":"code","9a555f5f":"code","96dcabf3":"code","fad060e2":"code","ed186f74":"code","bbcdb145":"code","68b8ae45":"code","12511a66":"code","29ceb10c":"code","f639c6b0":"markdown","741bed42":"markdown","3eeae3db":"markdown","cbbff218":"markdown","ebfa997e":"markdown","8892ea41":"markdown","5ff529b7":"markdown","777d4116":"markdown","8265dea8":"markdown","4d930077":"markdown","e5038ce9":"markdown","ddfe3742":"markdown","44212a5b":"markdown","dd6936b5":"markdown","54df1cc5":"markdown","097d3a3d":"markdown","860a6ec9":"markdown","2d93f674":"markdown","59796b55":"markdown","720e2cd7":"markdown","c3b9c9f5":"markdown","76ceb9f0":"markdown","02e7d1cc":"markdown","fae48d9f":"markdown","279cd6b0":"markdown","05bac2af":"markdown","0299c883":"markdown","dade2625":"markdown","08e880a9":"markdown","f6ce1239":"markdown","cdc10a85":"markdown","4af29e26":"markdown","e0b8e7cc":"markdown","2be2d26f":"markdown","cbd30330":"markdown","2bc20613":"markdown","df10d2d8":"markdown","906fe17b":"markdown","273d6c58":"markdown","5b119051":"markdown","266f2b58":"markdown"},"source":{"a4335306":"#Import Libraries\nimport numpy as np \nimport pandas as pd \n\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\n","5091e019":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nall_data = [train,test]","783bd9b5":"train.head()\n\n","53951ba0":"train.info()","b4dc4271":"train.describe()\n","c7cb235c":"print (train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())\n","a69dff67":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\n","2f693cd6":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n","58ec1e49":"print(pd.crosstab(train.Survived, train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()\n","566d1b7e":"for dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","98f6de68":"print(pd.crosstab(train.FamilySize, train.Survived))\nsns.factorplot(x=\"FamilySize\",y=\"Survived\", data=train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()\n","bf40a053":"train = train.drop(['Parch', 'SibSp'], axis=1)\ntest = test.drop(['Parch', 'SibSp'], axis=1)\nall_data = [train, test]\n\ntrain.head()\n","0867d7a9":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=25)\n","6da743c4":"print(pd.crosstab(train.Survived,train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","c1b83a8e":"print(pd.crosstab(train.Survived, train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","84be860e":"print('Shape Befor drop: ', train.shape)\ntrain = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\nall_data = [train, test]\nprint('Shape After drop: ',train.shape)\n","528055d8":"for dataset in all_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntrain.head()","140dc6f8":"guess_ages = np.zeros((2,3))\nguess_ages\n","9369323f":"for dataset in all_data:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()\n","26fdbb29":"train['Age_group'] = pd.cut(train['Age'], 5)\ntrain[['Age_group', 'Survived']].groupby(['Age_group'], as_index=False).mean().sort_values(by='Age_group', ascending=True)\n","95e5ebf3":"for dataset in all_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain.head()\n","bad000e6":"train = train.drop(['Age_group'], axis=1)\nall_data = [train, test]\ntrain.head()\n","d8b68c4a":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n\n","bcc24870":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain.head()\n","f3c7e357":"train = train.drop(['Fare'], axis=1)\ntest = test.drop(['Fare'], axis=1)\nall_data = [train,test]\ntrain.head()","c96a246a":"# retain the new Title feature for model training.\nfor dataset in all_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])\n","079dadd4":"for dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","f275a69a":"plt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()\n","119ccc25":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()\n","99bd5d63":"train = train.drop(['Name', 'PassengerId'], axis=1)\ntest = test.drop(['Name'], axis=1)\nall_data = [train, test]\n","b998fe52":"train.head()\n","cfd367b9":"test.head()","df45e213":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\n","b1135248":"#Apply RandomForestClassifier\nrandom_forest= RandomForestClassifier(n_estimators=100,\n                             max_features='auto',\n                             criterion='entropy',\n                             max_depth=10)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n\n\n","458085dc":"#Apply GradientBoostingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(X_train, Y_train)\ny_prediction= clf.predict(X_test)\nclf.score(X_train, Y_train)\nacc_clf = round(clf.score(X_train, Y_train) * 100, 2)\nprint(round(acc_clf,2,), \"%\")\n","1f52bab9":"#Apply LGBMClassifier\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier().fit(X_train, Y_train)\ny_predict= model.predict(X_test)\nmodel.score(X_train, Y_train)\nacc_model = round(model.score(X_train, Y_train) * 100, 2)\nprint(round(acc_model,2,), \"%\")\n\n","1fa0e258":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")\n","9a555f5f":"# Apply Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")\n","96dcabf3":"from xgboost import XGBClassifier\n\nparams_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint(round(acc_xgb,2,), \"%\")\n\n","fad060e2":"results = pd.DataFrame({\n    'Model': ['LGBMClassifier', 'Logistic Regression', \n              'Random Forest', 'Boosting', \n              'Decision Tree','xgb'],\n    'Score': [ acc_model,acc_log,\n              acc_random_forest, acc_clf,\n              acc_decision_tree,acc_xgb]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)\n","ed186f74":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n","bbcdb145":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)\n\n","68b8ae45":"importances.plot.bar()\n","12511a66":"params_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\n\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nprint(\"Score: \",xgb.score, 4*100, \"%\")\n\n\n\n","29ceb10c":"submission = pd.DataFrame({\n        \"PassengerId\": test['PassengerId'],\n        \"Survived\":  y_preds\n    })\n\nsubmission.to_csv('submission.csv', index=False)\n","f639c6b0":"### Prop1: Cabine and Ticket Data[](http:\/\/)\n* I think that the Cabin and Ticket features not impact in survive so I decided to drob these features","741bed42":"We can convert the categorical titles to ordinal.\n\n","3eeae3db":"Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive\n\n","cbbff218":"## 4. Applying ML Models:","ebfa997e":" ### Prop3: Missing value in Age\n \nWe have plenty of missing values in this feature. \nWe can generate random numbers between (mean - std) and (mean + std). then we part age into 5 range.\n\n","8892ea41":"### Prop4: Missing value in Embarked:\nOur training dataset has two missing values. We simply fill these with the most common occurance.\n\n","5ff529b7":"It's meaning that the females were rescued at the expense of the males\n","777d4116":"#### Let us replace Age with ordinals based on these groups.\n\n","8265dea8":"### Show Titles names in a graph","4d930077":"### Prop2: categorical feature (Sex)\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.\n\n","e5038ce9":"## Feature Importance","ddfe3742":"Here we can create a new feature who contain SibSp(siblings\/spouse ) and Parch (children\/parents) family_size","44212a5b":"### Prop6: Name \n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).","dd6936b5":"We note that we have: \n1. mix of numeric and alphanumeric data types in Ticket column\n2. Categorical data in Embarked and Sex columns\n","54df1cc5":"We can replace many titles with a more common name","097d3a3d":"Let us drop Parch, SibSpeatures in favor of FamilySize.\n\n","860a6ec9":"## Gathering Data\n\n> We downloaded two files (train.csv) & (test.csv) and We have to read them","2d93f674":"It's meaning that dies to mens are much higher than female\n","59796b55":"Most family suvived who are consisting of Four people","720e2cd7":"### 4. Age","c3b9c9f5":"### Prop5: categorical feature (Embarked)\n\nWe can now convert the EmbarkedFill","76ceb9f0":"### Test","02e7d1cc":"We have a new probleme here, there is missing value in columns('Age' - 'Cabine' - 'Embarked')","fae48d9f":"## Exploration\n\n","279cd6b0":"#### remove the Age_group column.","05bac2af":"- we can drop the Name feature now from our data.\n- We also don't need the PassengerId column in the training dataset.","0299c883":"## 2. Assessment Data","dade2625":"## 5. Sex","08e880a9":"The title was the most impact in survive! ","f6ce1239":"### The Best Model?\n","cdc10a85":"### 3. SibSp and Parch","4af29e26":"## 6. Embarked","e0b8e7cc":"### Prop6: Fare\nI think Fare column it's not important so i decided to drop this","2be2d26f":"## Problems:\n - Missing Value in (Age, Cabin, Embarked)\n \n - Categorical data in columns (Sex, Embarked)\n \n - mix of numeric and alphanumeric data types in Ticket column\n \n - Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n \n ","cbd30330":"### Submission\n","2bc20613":"## 1. Sex","df10d2d8":"## 2. Pclass","906fe17b":"## 1. Introduction\n\n> This is my first work of machine learning in kaggle. the notebook is written in python. \nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world.\n\n> feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel! I will be glad to answer any questions you may have in the comments. Thank You!\n","273d6c58":"## 3. Cleaning","5b119051":"### Above we can see that: \n1. Infants (Age <=4) had high survival rate.\n2. Oldest passengers (Age = 80) survived.\n3. Large number of 15-25 year olds did not survive.\n4. Most passengers are in 15-35 age range.","266f2b58":"### Another graph for representation"}}