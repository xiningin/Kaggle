{"cell_type":{"797dd4fe":"code","1d79b5c1":"code","ead8cdd5":"code","6e6ae743":"code","e6b07d28":"code","6e5702df":"code","ac2dbaca":"code","006e5105":"markdown","2fba997d":"markdown","45be1992":"markdown","3ee74df4":"markdown","d58f2140":"markdown","ea80bf36":"markdown","a0eb8c33":"markdown"},"source":{"797dd4fe":"%%time\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, model_selection\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, Dropout, BatchNormalization, Conv1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","1d79b5c1":"%%time\nDATA_FILE = '..\/input\/songlyrics\/songdata.csv'\nEMBEDDING_FILE = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\nTEXT_COLUMNS = 'text'\nTARGET_COLUMNS = 'artist'\n\nEPOCHS = 10\nBATCH_SIZE = 256\nLSTM_UNITS = 128\nDENSE_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 1000\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'","ead8cdd5":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n\n    for word, i in word_index.items():\n        \n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n            \n        embedding_vector = embedding_index.get(word.lower())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n            \n        embedding_vector = embedding_index.get(word.upper())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n            \n        embedding_vector = embedding_index.get(word.title())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n            \n        embedding_matrix[i] = np.random.normal(loc=0, scale=1, size=(1,300))\n        \n    return embedding_matrix\n\ndef build_model(embedding_matrix, out_size):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    hidden = GlobalMaxPooling1D()(x)\n    hidden = Dense(DENSE_UNITS, activation='relu')(hidden)\n    hidden = Dropout(0.2)(hidden)\n    hidden = BatchNormalization()(hidden)\n    result = Dense(out_size, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model","6e6ae743":"%%time\ndf = pd.read_csv(DATA_FILE, usecols=[TARGET_COLUMNS, TEXT_COLUMNS], dtype=str)\ndf = pd.merge(df, df.groupby(TARGET_COLUMNS).size().to_frame(name='size') > 180, how='left', on=TARGET_COLUMNS)\ndf = df[df['size']][[TARGET_COLUMNS, TEXT_COLUMNS]]\nprint('data size:', df.shape)\n\nn_class = df[TARGET_COLUMNS].unique().shape[0]\nprint('n_class: ', n_class)\n\ndf[TARGET_COLUMNS] = preprocessing.LabelEncoder().fit_transform(df[TARGET_COLUMNS])\ndf = df.sample(frac=1) # shuffle\n\nX = df[TEXT_COLUMNS].astype(str)\ny = df[TARGET_COLUMNS].values\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(X)\n\nX = tokenizer.texts_to_sequences(X)\nX = sequence.pad_sequences(X, maxlen=MAX_LEN)\n\ny = preprocessing.LabelBinarizer(neg_label=0, pos_label=1).fit_transform(y)\n\nX_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3)\nprint(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n\nembedding_matrix = build_matrix(tokenizer.word_index, EMBEDDING_FILE)","e6b07d28":"%%time\nmodel = build_model(embedding_matrix, n_class)\nfor global_epoch in range(EPOCHS):\n    model.fit(\n        X_train, y_train, batch_size=BATCH_SIZE, epochs=1, verbose=2, validation_data=(X_valid, y_valid),\n        callbacks=[LearningRateScheduler(lambda _: 1e-3 * (0.9 ** global_epoch))]\n    )","6e5702df":"def batch_iter(X, y, batch_size, shuffle=True):\n    num_batches_per_epoch = int((len(X) - 1) \/ batch_size) + 1\n\n    def data_generator(X, y, batch_size, shuffle):\n        data_size = len(X)\n        while True:\n            # Shuffle the data at each epoch\n            if shuffle:\n                shuffle_indices = np.random.permutation(np.arange(data_size))\n                shuffled_X = X[shuffle_indices]\n                shuffled_y = y[shuffle_indices]\n            else:\n                shuffled_X = X\n                shuffled_y = y\n\n            for batch_num in range(num_batches_per_epoch):\n                start_index = batch_num * batch_size\n                end_index = min((batch_num + 1) * batch_size, data_size)\n                X = shuffled_X[start_index: end_index]\n                X = X[:, -int(np.percentile(np.sum(X != 0, axis=1), 95)):]\n                y = shuffled_y[start_index: end_index]\n                yield X, y\n\n    return num_batches_per_epoch, data_generator(X, y, batch_size, shuffle)","ac2dbaca":"%%time\nmodel = build_model(embedding_matrix, n_class)\nfor global_epoch in range(EPOCHS):\n    train_steps, train_batches = batch_iter(X_train, y_train, BATCH_SIZE)\n    valid_steps, valid_batches = batch_iter(X_valid, y_valid, BATCH_SIZE)\n    model.fit_generator(\n        train_batches, train_steps, epochs=1, verbose=2,\n        validation_data=valid_batches, validation_steps=valid_steps,\n        callbacks=[LearningRateScheduler(lambda _: 1e-3 * (0.9 ** global_epoch))]\n    )","006e5105":"* It took the above Wall time to train model (epoch 10).","2fba997d":"* The model predict the artist from texts of their songs.","45be1992":"* It took the Wall time to train model (epoch 10).\n* It could cut the runtime in half.","3ee74df4":"* Set sequence length per batch, so create batch iterator below,","d58f2140":"* A runtime to train model is important on kernel competition.\n* This kernel is the experiment of comparing runtimes using set 95% sequence length or not on Bi-LSTM classifier training.","ea80bf36":"* Normal training.","a0eb8c33":"*  Training the model with setting 95% sequence length per batch."}}