{"cell_type":{"4768d794":"code","a7ab0855":"code","56f91ef9":"code","aa503adc":"code","2481bee6":"code","422cd761":"code","5b3485f5":"code","a5914eb9":"code","fd3f0782":"code","bfaed946":"code","51b74158":"code","a2482a20":"code","c226d7c0":"code","fb227902":"code","d8169529":"code","0e40e350":"code","150af38a":"code","8a92b4e5":"code","eb82fb41":"code","c4ea623d":"code","10c29b24":"code","c1e24684":"code","c39377f1":"code","955ebeac":"code","aa385931":"code","8330b08b":"code","df032150":"code","ad380942":"markdown","5588d270":"markdown","b3d1f3cf":"markdown","7ce8a06b":"markdown","9415a795":"markdown","76b87c7f":"markdown","a0b68c44":"markdown","6afc7d16":"markdown","25e4d97c":"markdown","1f5b1172":"markdown","0f30ad7f":"markdown","49b98e84":"markdown","a04a1f1c":"markdown","00b8494b":"markdown","2a9a1b62":"markdown","fe3211dd":"markdown","164a4882":"markdown","109a7312":"markdown","e7d7b2d8":"markdown","884a4605":"markdown","e6d2a9f9":"markdown","5bc8dcf3":"markdown","e23d2184":"markdown","985109a4":"markdown","5ec1beef":"markdown","d9430932":"markdown","8db71100":"markdown","6bf6d9fc":"markdown","21826372":"markdown","f10d615e":"markdown","c11db250":"markdown","f7f4733f":"markdown"},"source":{"4768d794":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a7ab0855":"%pip install yfinance","56f91ef9":"import yfinance as yf\nimport datetime\nimport time\nimport requests\nimport io","aa503adc":"data = yf.download(\"^GSPC\", start=\"2000-01-01\", end=\"2021-08-01\")","2481bee6":"data.head()","422cd761":"data.shape","5b3485f5":"train_size = int(0.8*data.shape[0])\nprint(train_size)","a5914eb9":"train_data = data.iloc[0:train_size]\ntest_data = data.iloc[train_size+1:]","fd3f0782":"def augment_features(dataframe):\n    fracocp = (dataframe['Close']-dataframe['Open'])\/dataframe['Open']\n    frachp = (dataframe['High']-dataframe['Open'])\/dataframe['Open']\n    fraclp = (dataframe['Open']-dataframe['Low'])\/dataframe['Open']\n    new_dataframe = pd.DataFrame({'delOpenClose': fracocp,\n                                 'delHighOpen': frachp,\n                                 'delLowOpen': fraclp})\n    new_dataframe.set_index(dataframe.index)\n    \n    return new_dataframe","bfaed946":"def extract_features(dataframe):\n    return np.column_stack((dataframe['delOpenClose'], dataframe['delHighOpen'], dataframe['delLowOpen']))","51b74158":"features = extract_features(augment_features(train_data))","a2482a20":"features.shape","c226d7c0":"!pip install hmmlearn","fb227902":"from hmmlearn.hmm import GaussianHMM","d8169529":"model = GaussianHMM(n_components=10)","0e40e350":"feature_train_data = augment_features(train_data)\nfeatures_train = extract_features(feature_train_data)\nmodel.fit(features_train)","150af38a":"import itertools\n\ntest_augmented = augment_features(test_data)\nfracocp = test_augmented['delOpenClose']\nfrachp = test_augmented['delHighOpen']\nfraclp = test_augmented['delLowOpen']\n\nsample_space_fracocp = np.linspace(fracocp.min(), fracocp.max(), 50)\nsample_space_fraclp = np.linspace(fraclp.min(), frachp.max(), 10)\nsample_space_frachp = np.linspace(frachp.min(), frachp.max(), 10)\n\npossible_outcomes = np.array(list(itertools.product(sample_space_fracocp, sample_space_frachp, sample_space_fraclp)))","8a92b4e5":"num_latent_days = 50\nnum_days_to_predict = 300","eb82fb41":"from tqdm import tqdm\n\npredicted_close_prices = []\nfor i in tqdm(range(num_days_to_predict)):\n    # Calculate start and end indices\n    previous_data_start_index = max(0, i - num_latent_days)\n    previous_data_end_index = max(0, i)\n    # Acquire test data features for these days\n    previous_data = extract_features(augment_features(test_data.iloc[previous_data_start_index:previous_data_end_index]))\n    \n    outcome_scores = []\n    for outcome in possible_outcomes:\n        # Append each outcome one by one with replacement to see which sequence generates the highest score\n        total_data = np.row_stack((previous_data, outcome))\n        outcome_scores.append(model.score(total_data))\n        \n    # Take the most probable outcome as the one with the highest score\n    most_probable_outcome = possible_outcomes[np.argmax(outcome_scores)]\n    predicted_close_prices.append(test_data.iloc[i]['Open'] * (1 + most_probable_outcome[0]))","c4ea623d":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(30,10), dpi=80)\n\n\nx_axis = np.array(test_data.index[0:num_days_to_predict], dtype='datetime64[ms]')\nplt.plot(x_axis, test_data.iloc[0:num_days_to_predict]['Close'], 'b+-', label=\"Actual close prices\")\nplt.plot(x_axis, predicted_close_prices, 'ro-', label=\"Predicted close prices\")\nplt.legend()\nplt.show()","10c29b24":"ae = abs(test_data.iloc[0:num_days_to_predict]['Close'] - predicted_close_prices)\n\nplt.figure(figsize=(30,10), dpi=80)\n\nplt.plot(x_axis, ae, 'go-', label=\"Error\")\nplt.legend()\nplt.show()","c1e24684":"print(\"Max error observed = \" + str(ae.max()))\nprint(\"Min error observed = \" + str(ae.min()))\nprint(\"Mean error observed = \" + str(ae.mean()))","c39377f1":"num_latent_days_values = [10, 20, 30, 40, 50, 60]\nbaseline_num_latent_days = 50\nn_components_values = [4, 6, 8, 10, 12, 14]\nbaseline_n_componets = 10\nnum_steps_values = [10, 20, 40, 50]\nbaseline_num_steps = 50\nnum_days_to_predict = 100 # We don't need to predict as many days as before","955ebeac":"mae_num_components = []\nfor num_component in n_components_values:\n    model = GaussianHMM(n_components=num_component)\n    model.fit(features_train)\n    predicted_close_prices = []\n    for i in tqdm(range(num_days_to_predict)):\n        # Calculate start and end indices\n        previous_data_start_index = max(0, i - baseline_num_latent_days)\n        previous_data_end_index = max(0, i)\n        # Acquire test data features for these days\n        previous_data = extract_features(augment_features(test_data.iloc[previous_data_start_index:previous_data_end_index]))\n\n        outcome_scores = []\n        for outcome in possible_outcomes:\n            # Append each outcome one by one with replacement to see which sequence generates the highest score\n            total_data = np.row_stack((previous_data, outcome))\n            outcome_scores.append(model.score(total_data))\n\n        # Take the most probable outcome as the one with the highest score\n        most_probable_outcome = possible_outcomes[np.argmax(outcome_scores)]\n        predicted_close_prices.append(test_data.iloc[i]['Open'] * (1 + most_probable_outcome[0]))\n    mae_num_components.append((abs(test_data.iloc[0:num_days_to_predict]['Close'] - predicted_close_prices)).mean())\n","aa385931":"plt.figure(figsize=(30,10), dpi=80)\n\nplt.plot(n_components_values, mae_num_components, 'go-', label=\"Error\")\nplt.xlabel(\"Number of hidden states\")\nplt.ylabel(\"MAE\")\nplt.legend()\nplt.show()","8330b08b":"mae_num_steps = []\nmodel = GaussianHMM(n_components=baseline_n_componets)\nmodel.fit(features_train)\nfor num_step in num_steps_values:\n    sample_space_fracocp = np.linspace(fracocp.min(), fracocp.max(), num_step)\n    sample_space_fraclp = np.linspace(fraclp.min(), frachp.max(), int(num_step\/5))\n    sample_space_frachp = np.linspace(frachp.min(), frachp.max(), int(num_step\/5))\n    possible_outcomes = np.array(list(itertools.product(sample_space_fracocp, sample_space_frachp, sample_space_fraclp)))\n    predicted_close_prices = []\n    for i in tqdm(range(num_days_to_predict)):\n        # Calculate start and end indices\n        previous_data_start_index = max(0, i - baseline_num_latent_days)\n        previous_data_end_index = max(0, i)\n        # Acquire test data features for these days\n        previous_data = extract_features(augment_features(test_data.iloc[previous_data_start_index:previous_data_end_index]))\n\n        outcome_scores = []\n        for outcome in possible_outcomes:\n            # Append each outcome one by one with replacement to see which sequence generates the highest score\n            total_data = np.row_stack((previous_data, outcome))\n            outcome_scores.append(model.score(total_data))\n\n        # Take the most probable outcome as the one with the highest score\n        most_probable_outcome = possible_outcomes[np.argmax(outcome_scores)]\n        predicted_close_prices.append(test_data.iloc[i]['Open'] * (1 + most_probable_outcome[0]))\n    mae_num_steps.append((abs(test_data.iloc[0:num_days_to_predict]['Close'] - predicted_close_prices)).mean())","df032150":"plt.figure(figsize=(30,10), dpi=80)\n\nplt.plot(num_steps_values, mae_num_steps, 'go-', label=\"Error\")\nplt.xlabel(\"Number of intervals for features\")\nplt.ylabel(\"MAE\")\nplt.legend()\nplt.show()","ad380942":"### Train-test split","5588d270":"## Preprocessing","b3d1f3cf":"For each of the days that we are going to predict closing prices for, we are going to take the test data for the previous num_latent_days and try each of the outcomes in possible_outcomes to see which sequence generates the highest score. The outcome that generates the highest score is then used to make the predictions for that day's closing price.","7ce8a06b":"## Hidden Markov Model","9415a795":"We will be doing some graphs across a range of values for num_latent_days, n_components, and varying the steps in the interval that the features take","76b87c7f":"We use the data of the last 50 (latent) days to predict the closing price of the current day, and we repeat those for 300 days (this value does not matter at all)","a0b68c44":"## Goal","6afc7d16":"### Generating possible sequences","25e4d97c":"### Comparing across different number of intervals for the feature variables","1f5b1172":"Here we tried to vary the number of steps in the interval range for each of the features. Intuitively, the finer the interval (a higher number of steps), the closer the distribution gets to becoming continuous and the more accurate a candidate observation may possibly be. However in making the distribution close to continuous, we have to compute the score (or the likelihood of a candidate observation given previous observations) of a much larger number of possible candidate observations and that would require a large amount of computational resources.\n\nIt is seen above that with a higher number of intervals, the mean absolute error decreases.","0f30ad7f":"Here we notice that we are able to improve our prediction (as indicated by the lower MAE) going from 4 hidden states to 8, but beyond that there is no significant improvement in the prediction. This leads us to believe that the observations (combination of 3 features) are controlled by 10 hidden states and the emissions of these hidden states.\n\nThere is also a tradeoff between time to make a prediction and the number of hidden states or components in the model, and so it would make the most sense, to stick to just 8 components (our baseline with which we achieved the overall results).","49b98e84":"For all data, we are going to do a 80-20 train-test split.","a04a1f1c":"### Comparing across different values for num_components","00b8494b":"We are going to take the features of opening price, low price, high price and use these to derive some fractional changes. With these fractional changes, we will observe sequences (observations) from which we will derive latent factors in a Markov process. These latent factors will often vary from company to company, which is why it's often hard to fit one linear model of a certain subset of variables for all companies. Once the latent factors and their transitions and starting probabilities (the hidden sequence) are found, we will try to generate some possible values for each of the features and then check how they score with a sequence of test data. The set of possible values that leads to the highest score is then used to predict the closing price for that day.","2a9a1b62":"Plotting the predicted closing prices and the actual closing prices, we see the following","fe3211dd":"We are going to download data from 2000 onwards. If we start from 2010, we get very little data to fit our model on and that may lead to inaccurate transition probabilities in the hidden Markov process.","164a4882":"We are first going to import the GaussianHMM from hmmlearn.hmm and then fit it with 10 hidden components (or states) to our training data. We start off with 10 hidden states, but it may be possible to do a grid search among a possible set of values for the number of hidden states to see which works the best.","109a7312":"### Model","e7d7b2d8":"## Get ^GSPC data","884a4605":"For the GSPC data from 1st January, 2000 to 1st August, 2021, there are 5429 entries.","e6d2a9f9":"To generate possible possible permutations of values for the features we take the Cartesian product across a range of values for each feature as seen below. We assume a few things here to reduce model complexity.\n1. We assume that the distribution of each features is across an evenely spaced interval instead of being fully continuous\n2. We assume possible values for the start and end of the intervals","5bc8dcf3":"## Hidden Markov Models with hmmlearn","e23d2184":"The YFinance module neatly downloads company stock data in the form of a Pandas DataFrame and saves us the trouble of uploading custom CSVs. The same effect can be observed fro custom CSVs although a different subset of column names will then have to be assigned.","985109a4":"# Predicting stock closing price from stock data using the Hidden Markov Model to identify latent states","5ec1beef":"The S&P 500 index, or Standard & Poor\u2019s 500, is a very important index that tracks the performance of the stocks of 500 large-cap companies in the U.S. The ticker symbol for the S&P 500 index is ^GSPC.","d9430932":"A Hidden Markov Model, or HMM for short, may be thought of as a double stochastic process:\n1. A hidden or latent Markov stochastic process\n2. An observable stochastic process that produces sequences of observations\n\nSince HMMs are often used to capture long-term sequences and hence time-based phenomena, they may prove to be useful in analysis of financial markets.","8db71100":"## Tweaking some hyperparameters","6bf6d9fc":"## Install YFinance to directly download company stock info instead of uploading and using a CSV","21826372":"### Extracting features\n\n","f10d615e":"We are going to be working with 3 features:\n1. The fractional change in opening and closing prices (fracocp)\n2. The fractional change in high prices (frachp)\n3. The fractional change in low prices (fraclp)\n\nThese will be obtained individually in the train and test datasets.","c11db250":"### Checking predictions","f7f4733f":"The graphs above show that our model is fairly accurate in predicting the close prices. There is a maximum absolute error of 104.09, a minimum absolute error of 0.05 and a mean absolute error of 10.6812.76. The first graph also shows that thereare significant gaps between the predictions plot and the actual plot.\n\nWe also observe that the highest errors occur when the actual closing prices change very sharply (steep rise or fall).\n\nHowever, we can say that our model is able to predict the general trends in the actual closing price movement fairly well, although there are lags observed from time to time."}}