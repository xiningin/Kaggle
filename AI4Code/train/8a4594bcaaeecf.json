{"cell_type":{"c23d6f6a":"code","1b762e06":"code","1335b9e7":"code","ffd1cdb2":"code","bd819d83":"code","fe078901":"code","8271b8b2":"code","3aa16a2b":"code","d4721624":"code","577dfdd1":"code","d102fb3e":"code","ca0c2899":"code","cb119780":"code","89983cdc":"code","e498f39c":"code","51de51a2":"code","40105ea0":"code","158e6bde":"code","3dbbb045":"code","20c6bc80":"code","f4da6e55":"code","0ab6565f":"code","23989151":"code","0a3675a0":"code","4fbd1de0":"code","bdba5a76":"code","5d801e6b":"code","216a360c":"code","099cbe7a":"code","1ca08e0f":"code","aa40c55c":"code","9eeaa4cd":"code","df14d3cd":"code","e2e5f19d":"code","5034b5b7":"code","c0f77d20":"code","900d5924":"code","93dfc5a8":"code","639e7218":"code","ac32f001":"code","9d22fda2":"code","c9a98721":"code","2a7d8ae4":"code","491a76fd":"code","0ff0fedb":"code","f252eb79":"code","486860f8":"code","a7850d78":"code","e8c45965":"code","f23a784f":"code","f9eb4499":"code","e49a6bfa":"code","a5bdb6ff":"code","1cf579e6":"code","d44dcb9d":"code","6bd01a69":"code","2df49405":"code","87163610":"code","41c23db0":"code","b29d0da0":"code","894aef0c":"code","fce625a4":"code","843034d0":"code","1dc4ef9a":"code","3480e50e":"code","33653ba7":"code","32418e3e":"code","a71a5be9":"code","04fa70f6":"code","50891212":"code","2415db52":"code","d1628184":"code","6fb9c50e":"code","b2c91236":"code","f123e0b2":"markdown","3c119d98":"markdown","fa667b25":"markdown","9014a203":"markdown","f7d94ebe":"markdown","cba91ebb":"markdown","3eda6d74":"markdown","7c7e9075":"markdown","a6fad421":"markdown","b717e141":"markdown","7faf6ff0":"markdown","e4da1032":"markdown","2e59a133":"markdown","4a356368":"markdown","7e05aa2f":"markdown","c008730e":"markdown","adc4689e":"markdown","cb2d194d":"markdown","0c3df1ec":"markdown","eee7ddb5":"markdown","566bff02":"markdown","458e9951":"markdown","412004e0":"markdown","b5a18277":"markdown","10d6dae3":"markdown","128486ef":"markdown","6ccd0a1b":"markdown","2e0d087a":"markdown","0084c65d":"markdown","a4a630e3":"markdown","e68905da":"markdown","ec57a363":"markdown","73d44024":"markdown","04a56e99":"markdown","7eda8804":"markdown","270042fd":"markdown","e8e06422":"markdown","cc5f81f4":"markdown","4787819d":"markdown","00fafbfa":"markdown","09ea4bc7":"markdown","d6145e43":"markdown"},"source":{"c23d6f6a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\n\n%matplotlib inline\n\n#pd.set_option('display.max_rows', None)","1b762e06":"train = pd.read_csv('..\/input\/titanic\/train.csv')\n#train = pd.read_csv('data\/train.csv')\ntrain.head()","1335b9e7":"test = pd.read_csv('..\/input\/titanic\/test.csv')\n#test = pd.read_csv('data\/test.csv')\ntest.head()","ffd1cdb2":"combined_data = [train, test]","bd819d83":"### creating dataframe to hold our pre-processed data\ndf_train = pd.DataFrame()\ndf_train['Survived'] = train['Survived']\ndf_test = pd.DataFrame()\ncombined_df = [df_train, df_test]","fe078901":"for dataset in combined_data:\n    print(dataset.info())\n    print(dataset.isnull().sum())","8271b8b2":"for dataset in combined_data:\n    title = dataset.Name.str.extract(r' ([A-Za-z]+)\\. ', expand=False)\n    dataset['title'] = title\n    dataset.drop('Name', axis=1, inplace=True)\n    print(pd.crosstab(dataset['title'], dataset['Sex']))","3aa16a2b":"for dataset in combined_data:\n    dataset['title'] = dataset['title'].replace(['Dr', 'Rev', 'Col', 'Major',  \n                                         'Capt', 'Sir', 'Don', 'Lady', \n                                         'Jonkheer', 'Countess', 'Dona'], \n                                        \"Rare\")\n    dataset['title'] = dataset['title'].replace({\n                        'Mlle': 'Miss',\n                        'Ms': 'Miss',\n                        'Mme': 'Mrs'})\n    print(dataset['title'].value_counts())","d4721624":"plt.figure(figsize = (30, 10))\nsns.set(font_scale=2)\nsns.regplot(x=train['title'], y=train['Age'], fit_reg=False)","577dfdd1":"for dataset in combined_data:\n    for title in train['title'].unique():\n        median_age = dataset.Age[train['title'] == title].median()\n        print('{0} median age: {1}'.format(title, median_age))\n\n        dataset.Age[(dataset['title'] == title) & dataset['Age'].isnull()] = median_age\n    print('\\n')  \n","d102fb3e":"train.Embarked.value_counts()","ca0c2899":"train['Embarked'] = train['Embarked'].fillna('S')","cb119780":"# extracting deck from Cabin\nfor dataset in combined_data:\n    dataset['cabin_deck'] = dataset['Cabin'].str.get(0)\n    #print(dataset.head())","89983cdc":"for dataset in combined_data:\n    print(dataset['cabin_deck'].value_counts())","e498f39c":"# Passenger in the T deck is changed to A\nidx = train[train['cabin_deck'] == 'T'].index\ntrain.loc[idx, 'cabin_deck'] = 'A'","51de51a2":"for dataset in combined_data:\n    dataset['cabin_deck'] = dataset['cabin_deck'].fillna('Z')\n    dataset.drop('Cabin', axis=1, inplace=True)","40105ea0":"# TODO: get distribution of passengers in each deck\nprint(pd.crosstab(train['cabin_deck'], train['Pclass']))","158e6bde":"#train[['Survived', 'title']].groupby(['title'], as_index = False).mean()\ntrain[['Fare', 'Pclass', 'Embarked']].groupby(['Pclass', 'Embarked'], as_index = False).mean()","3dbbb045":"test[test['Fare'].isnull()]","20c6bc80":"test['Fare'] = test['Fare'].fillna(14.64)","f4da6e55":"for dataset in combined_data:\n    print(dataset.info())","0ab6565f":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean()","23989151":"df_train['Pclass'] = train['Pclass']\ndf_test['Pclass'] = test['Pclass']","0a3675a0":"train[['Survived', 'title']].groupby(['title'], as_index = False).mean()","4fbd1de0":"df_train['title'] = train['title']\ndf_test['title'] = test['title']","bdba5a76":"train[['Survived', 'Sex']].groupby(['Sex'], as_index = False).mean()","5d801e6b":"df_train['Sex'] = train['Sex']\ndf_test['Sex'] = test['Sex']","216a360c":"sns.distplot(train.Age)","099cbe7a":"plt.figure(figsize = (30, 10))\nsns.set(font_scale=2)\nsns.violinplot(x=train['Survived'], y=train['Age'])","1ca08e0f":"for i in range (3, 8):\n    train['Categorical_Age'] = pd.cut(train['Age'], i)\n    print(train[['Categorical_Age', 'Survived']].groupby(['Categorical_Age'], as_index=False).mean())","aa40c55c":"train.drop('Categorical_Age', axis=1, inplace=True)","9eeaa4cd":"for dataset in combined_data:\n    dataset['Categorical_Age'] = 0\n    dataset.loc[(dataset['Age'] > 16.0) & (dataset['Age'] <= 32.0), 'Categorical_Age'] = 1\n    dataset.loc[(dataset['Age'] > 32.0) & (dataset['Age'] <= 48.0), 'Categorical_Age'] = 2\n    dataset.loc[(dataset['Age'] > 48.0) & (dataset['Age'] <= 64.0), 'Categorical_Age'] = 3\n    dataset.loc[(dataset['Age'] > 64.0), 'Categorical_Age'] = 4","df14d3cd":"df_train['Categorical_Age'] = train['Categorical_Age']\ndf_test['Categorical_Age'] = test['Categorical_Age']\nprint(df_train.info())\nprint(df_test.info())","e2e5f19d":"sns.distplot(train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean())","5034b5b7":"sns.distplot(train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean())","c0f77d20":"# we add 1 to account for the passenger himself\nfor dataset in combined_data:\n    dataset['Family_size'] = dataset['SibSp'] + dataset['Parch'] + 1\n    print(dataset['Family_size'].value_counts())","900d5924":"sns.distplot(train[['Family_size', 'Survived']].groupby(['Family_size'], as_index=False).mean())","93dfc5a8":"temp_family_size = train[['Family_size', 'Survived']]\nfor i in range (2, 6):\n    temp_family_size['Categorical_Fam_size'] = pd.cut(train['Family_size'], i)\n    print(temp_family_size[['Categorical_Fam_size', 'Survived']].groupby(['Categorical_Fam_size'], as_index=False).mean())","639e7218":"for dataset in combined_data:\n    dataset['Categorical_Fam_size'] = 0\n    dataset.loc[(dataset['Family_size'] > 3.0) & (dataset['Family_size'] <= 5.0), 'Categorical_Fam_size'] = 1\n    dataset.loc[(dataset['Family_size'] > 5.0), 'Categorical_Fam_size'] = 2\n    print(dataset['Categorical_Fam_size'].value_counts())","ac32f001":"df_train['Categorical_Fam_size'] = train['Categorical_Fam_size']\ndf_test['Categorical_Fam_size'] = test['Categorical_Fam_size']\nprint(df_train.info())\nprint(df_test.info())","9d22fda2":"sns.distplot(train.Fare)","c9a98721":"sns.distplot(train[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean())","2a7d8ae4":"for i in range (2, 8):\n    train['Categorical_Fare'] = pd.qcut(train['Fare'], i)\n    print(train[['Categorical_Fare', 'Survived']].groupby(['Categorical_Fare'], as_index=False).mean())\n    sns.catplot(x='Categorical_Fare', y='Survived', kind=\"bar\", data=train)","491a76fd":"sns.catplot(x='Categorical_Fare', y='Survived', kind=\"bar\", data=train)","0ff0fedb":"for dataset in combined_data:\n    dataset['Categorical_Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Categorical_Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31.0), 'Categorical_Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 31.0), 'Categorical_Fare'] = 3\n    print(dataset['Categorical_Fare'].value_counts())","f252eb79":"df_train['Categorical_Fare'] = train['Categorical_Fare']\ndf_test['Categorical_Fare'] = test['Categorical_Fare']","486860f8":"train[['Survived', 'cabin_deck']].groupby(['cabin_deck'], as_index = False).mean()","a7850d78":"sns.catplot(x='cabin_deck', y='Survived', kind=\"bar\", data=train)","e8c45965":"cabin_deck_category ={\n    'A': 1,\n    'B': 1,\n    'C': 1,\n    'D': 1,\n    'E': 2,\n    'F': 2,\n    'G': 3,\n    'Z': 4\n}","f23a784f":"for dataset in combined_data:\n    dataset['Categorical_cabin_deck'] = dataset['cabin_deck'].map(cabin_deck_category)\n    \ntrain[['Survived', 'Categorical_cabin_deck']].groupby(['Categorical_cabin_deck'], as_index = False).mean()","f9eb4499":"df_train['Categorical_cabin_deck'] = train['Categorical_cabin_deck']\ndf_test['Categorical_cabin_deck'] = test['Categorical_cabin_deck']","e49a6bfa":"g = sns.FacetGrid(train, col='Pclass', height=4, aspect=.5)\ng.map(sns.barplot, \"Embarked\", \"Survived\")","a5bdb6ff":"g = sns.FacetGrid(train, col='Embarked', height=4, aspect=0.6)\ng.map(sns.barplot, \"Pclass\", \"Survived\")","1cf579e6":"train.groupby('Embarked')['Survived'].mean()","d44dcb9d":"df_train['Embarked'] = train['Embarked']\ndf_test['Embarked'] = test['Embarked']","6bd01a69":"for dataset in combined_df:\n    print(dataset.info())","2df49405":"from sklearn.preprocessing import LabelEncoder","87163610":"one_hot_title = pd.get_dummies(df_train['title'], prefix = 'title')\none_hot_sex = pd.get_dummies(df_train['Sex'], prefix = 'Sex')\none_hot_embarked = pd.get_dummies(df_train['Embarked'], prefix = 'Embarked')\n\ndf_train_enc = pd.concat([df_train,\n                     one_hot_title,\n                     one_hot_sex,\n                     one_hot_embarked], axis = 1)\n\none_hot_title = pd.get_dummies(df_test['title'], prefix = 'title')\none_hot_sex = pd.get_dummies(df_test['Sex'], prefix = 'Sex')\none_hot_embarked = pd.get_dummies(df_test['Embarked'], prefix = 'Embarked')\n\ndf_test_enc = pd.concat([df_test,\n                     one_hot_title,\n                     one_hot_sex,\n                     one_hot_embarked], axis = 1)","41c23db0":"combined_df_enc = [df_train_enc, df_test_enc]","b29d0da0":"# we will drop the original features that were hot-encoded and \n# one of the newly encoded categories for each feature to avoid\n# dummy variable trap\nfeatures_to_drop = ['title', 'Sex', \n                    #'Combined_pclass_embarked',\n                    'Embarked',\n                   'title_Rare', 'Sex_male', 'Embarked_C']\n\nfor dataset in combined_df_enc:\n    dataset.drop(features_to_drop, axis = 1, inplace=True)","894aef0c":"print(df_train_enc.info(), df_test_enc.info())","fce625a4":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis","843034d0":"X_train = df_train_enc.drop('Survived', axis=1)\ny_train = df_train_enc.Survived","1dc4ef9a":"X_train.shape","3480e50e":"y_train.shape","33653ba7":"df_test_enc.shape","32418e3e":"classifiers = [\n    KNeighborsClassifier(),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns = log_cols)","a71a5be9":"acc_dict = {}\n\nfor classifier in classifiers:\n    scores = cross_val_score(classifier, X_train, y_train, cv = 10, scoring = 'accuracy')\n    name = classifier.__class__.__name__\n    acc_dict[name] = scores.mean()\n    \nacc_dict","04fa70f6":"n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] \nmax_features = ['auto', 'sqrt'] \nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)] \nmax_depth.append(None) \nmin_samples_split = [2, 5, 10] \nmin_samples_leaf = [1, 2, 4] \nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features, \n               'max_depth': max_depth, \n               'min_samples_split': min_samples_split, \n               'min_samples_leaf': min_samples_leaf, \n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions=random_grid, \n                               n_iter = 100, \n                               cv = 10, \n                               verbose =2, \n                               random_state = 42, \n                               n_jobs = -1) \n\nrf_random.fit(X_train, y_train) \nprint(rf_random.best_score_) \nprint(rf_random.best_params_)","50891212":"rf = RandomForestClassifier(n_estimators=800, \n                            min_samples_split=5, \n                            min_samples_leaf=1, \n                            max_features='sqrt', \n                            max_depth=90, \n                            bootstrap=False)\nrf.fit(X_train, y_train)","2415db52":"y_pred = rf.predict(df_test_enc)","d1628184":"y_pred.shape","6fb9c50e":"submission = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_pred\n})\nsubmission","b2c91236":"submission.to_csv('submission.csv', index=False)","f123e0b2":"### Feature: Title\nWe have previously extracted the 'title' of each passenger and removed the 'Name' feature. The survival rates are pretty distinct between titles so we shall use it in our training and prediction.","3c119d98":"### Feature: SibSp & Parch\n\nFamily size is a good predictor of survivability. We will use it for training and prediction.","fa667b25":"According to https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic#Dimensions_and_layout, Deck A  was the highest deck, while Deck G  was the lowest for passenger use. \n\n- Decks A, B and C were used exclusively for First Class cabins\n- Decks D and E  were for all 3 classes\n- Decks F  is mainly for Second and Third class passenger\n- It is unclear from the page what passenger class reside in Deck G \n\nFrom https:\/\/www.encyclopedia-titanica.org\/cabins.html, Cabin T is also the Boat Deck. We shall group Cabin T under Cabin A. Also, we shall assign label 'Z' to the missing cabin_deck values. ","9014a203":"### Filling in Cabin values","f7d94ebe":"There is an distinct 'minimum age' to the titles 'Mr', 'Mrs' and 'Rare'. There is a upper limit to 'Master' which kind of agrees with our research. We shall use title group to approximate the missing ages. ","cba91ebb":"From the above, we know that:\n- 100% of Decks A, B and C are 1st class passengers\n- 100% of Deck G are 3rd class passengers\n- Deck D has a mixture of 1st and 2nd class passengers\n- Deck E has all passengers from all 3 Pclass\n- Deck F has a mixture of 2nd and 3rd class passengers\nThis matches our research. \n\nOur missing Deck Z has a mixture of all 3 passenger class.","3eda6d74":"## Choosing the best machine learning model","7c7e9075":"SibSp and Parch has really similar distribution so we are going to join them together into a new 'Family_size' feature.","a6fad421":"https:\/\/en.wikipedia.org\/wiki\/Master_(form_of_address)\n\n>The use of Master as a prefixed title is, according to Leslie Dunkling, \"a way of addressing politely a boy ... too young to be called 'Mister'.\" It can be used as a title and form of address for any boy. \n\nWe will try to see if there is any relationship between title and age, and possibly use title to fill in the missing age values. ","b717e141":"The training data set is missing 177 Age, 687 Cabin and 2 Embarked values. On the other hand, the test data set is missing 86 Age, 1 Fare and 327 Cabin values.","7faf6ff0":"### One hot encoding\nWe need to one hot encode 'title', 'Sex' and 'Embarked'.","e4da1032":"## Understanding & dealing with missing data","2e59a133":"We shall replace 'Mlle', 'Ms', and 'Mme' with their equivalent titles and replace remaining titles with less than 10 entries into a common group.","4a356368":"### Feature: Sex\n\nFemales have a significantly higher chance of survival than males. Sex is definitely a strong predictor of survivability. We will use it for our training and prediction. ","7e05aa2f":"The objective is to predict which passengers survived the Titanic shipwreck. Given that there are only 2 possibilities for surivivability, this is a classification problem.","c008730e":"We will cut fares into 4 categories:\n \n*           Categorical_Fare  Survived\n* 0         (-0.001, 7.91]  0.197309\n* 1         (7.91, 14.454]  0.303571\n* 2         (14.454, 31.0]  0.454955\n* 3         (31.0, 512.329]  0.581081","adc4689e":"According to https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_RMS_Titanic:\n> \"first class cost between \u00a330 (equivalent to \u00a33,000 in 2019) and \u00a3870 (equivalent to \u00a387,000 in 2019)\"\n\n> \"average ticket price for an adult second-class passenger was \u00a313\"\n\n> \"Third-class passengers paid \u00a37 (\u00a3698 today) for their ticket, depending on their place of origin; ticket prices often included the price of rail travel to the three departure ports.\"\n\n> \"Tickets for children cost \u00a33 (\u00a3299 today).\"\n\nWe will fill in the missing fare according to the passenger's Pclass and Embarked location.","cb2d194d":"### Label encoding\nWe already did label encoding for 'Categorical_Age', 'Categorical_Fam_size', 'Categorical_Fare' and 'Categorical_cabin_deck' previously so there is nothing for us to do here. There is also nothing to do for Pclass. ","0c3df1ec":"RandomForestClassifier gives us the best accuracy so we shall use it to make our predictions. ","eee7ddb5":"Since the mode for Embarked is 'S', we shall fill in the 2 missing Embarked values with 'S'.","566bff02":"### Filling in Age values","458e9951":"### Feature: Ticket\n\nIt is unlikely that ticket number is a good predictor for survivability so we shall leave it out of our training and prediction.","412004e0":"Passenger age seems to follow a normal distribution. Most of the passengers are around age 25-35.","b5a18277":"We observe that:\n- children below the age of 10 has higher survival rates\n- adults at the age of 30 has the highest death rate\n- adults between age 30 and 50 have higher survival rates\n- above age 50, survival rates decreases as age increases\n\nCutting the age into 5 bins most accurately capture these observations.","10d6dae3":"### Feature: PassengerId\nIt is unlikely that PassengerId is a very strong predictor of survivability so we shall ignore it.","128486ef":"## Feature engineering","6ccd0a1b":"### Filling in Embarked values","2e0d087a":"Because there is a large difference between the max and min fare, and most of the fares are concentrated on the lower end, we will use qcut instead of cut.","0084c65d":"### Feature: Age\n\nAge is a good predictor for passenger survivability. We will use it for our training and prediction.","a4a630e3":"## Parameter tuning using RandomizedSearchCV","e68905da":"### Feature: Embarked\n\nIntuitively, Embarked location shouldn't affect survival rates. However, we observe that there are very distinct survival rates between Embarked locations. We shall use them for our training and prediction. ","ec57a363":"### Filling in the missing fare","73d44024":"## Running our prediction","04a56e99":"### Feature: Pclass\n\nThe higher the social status, the higher the likelihood of the passenger's survivability. Pclass seems to be a strong predictor of survivability. We will use it for our training and prediction. ","7eda8804":"### Heavily influenced by the following notebooks:\n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n- https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\n- https:\/\/www.kaggle.com\/girijeshcse\/titanic-advanced-feature-engineering-tutorial","270042fd":"## Lessons learnt from previous versions:\n- It doesn't seem to matter whether you decide to merge categorical features together or keep them separate. This makes sense because when you one-hot encode the new combined feature, they are represented by as many columns as there are combinations so there is no loss of information. For example, when using Pclass and Embarked separately, there are 9 different combinations. Combining them and one-hot encoding this new feature will yield 9 columns as well. ","e8e06422":"### Feature encoding\nNow we have our dataframe ready, with no missing values. We shall encode the features so they are ready for our ML model.","cc5f81f4":"Rather than using the actual cabin number, we are going to extract the deck from the cabins number and fill the missing decks. For simplicity where the passenger booked multiple cabins, we are just going to take the first cabin. ","4787819d":"### Feature: Fare \n**Observations**:\n- Generally speaking, survivability increases as the fare increases\n\nFare seems like a good predictor for survivability. We will use it for our training and testing.","00fafbfa":"### Feature: Cabin\n\nLogically, passengers on a higher deck has higher chance of survival. Reducing the number of deck categories shows this relationship. We shall use cabin_deck for our training and prediction.","09ea4bc7":"Reducing cabin decks to categories show a more intuitive decrease in survival rate as we go down the deck levels, except for the increase in category 3.","d6145e43":"It can be seen that the new feature retains the distribution of its components. Also, we observe that generally surival rates drop as family size increases. There seems to be inflection points at where family size is 5 and 10.\n\nFrom below, we observe that as family size increases, it improves survival rates, up to a size of 5. Beyond that, survival rates drastically drops. We shall bin family into the following:\n- x <= 3: small family\n- 3 < x <= 5: medium family\n- x > 5: large family"}}