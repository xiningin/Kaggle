{"cell_type":{"77ab17fa":"code","96952c1d":"code","ec63ae9f":"code","08296eb2":"code","514941f0":"code","1a135c85":"code","11f57d48":"code","5d16c5ca":"code","0c4eb2c4":"code","d519be57":"code","082721e5":"code","54cf123c":"code","b6f5c95d":"code","68cbeb4d":"code","842b8b60":"code","f1735dd7":"code","5799c462":"code","fba71cdb":"code","73bdf30a":"code","f5573c32":"code","3417b636":"code","89c09d97":"code","b910602c":"code","a1cc1261":"code","2fefc603":"code","3892ed21":"code","4dcf2df6":"code","9b163e05":"code","36df5c6b":"code","5d208af6":"code","a0a4623b":"code","3aeab237":"code","1e226203":"code","3afba2f5":"code","8f6c1569":"code","c4127e75":"code","e09a694e":"code","585e33de":"code","c24737d4":"code","fb70d38d":"code","1501c5a4":"code","be3eeb5f":"code","3b6f6cec":"code","5176a944":"code","445815dd":"code","54875205":"code","001e1c1b":"code","6ac9bac8":"code","074585fc":"code","1546457d":"code","c050d883":"code","cab1f7d6":"code","9dbbc127":"code","73b160df":"code","4b9521fd":"code","e2d1c113":"code","17dc3bad":"code","5e012041":"code","df2cb316":"code","c757c290":"code","a6d9ccb6":"code","a71fa2c0":"code","902766aa":"code","1cf92581":"code","f5f23045":"code","7826e1c4":"code","91d04916":"code","4f110050":"code","3e4680c0":"code","20743aca":"code","46bacd9e":"code","8de0786b":"code","8e7ea216":"code","3e88b041":"code","4a9e018f":"code","fd33f084":"code","dad0e250":"code","1b43e3d7":"code","d2fa5aba":"code","37bd681d":"code","4f67c2f8":"code","15223584":"code","de29b2f1":"code","6a2ad900":"code","037a238d":"code","4a8794ae":"code","845a1813":"code","f7a0d21e":"code","6f1fd0b6":"code","39b2a77e":"code","02355ced":"code","26af4e70":"code","6d5f040c":"code","02f11a79":"code","01501f92":"code","76d3dfc5":"code","276d5c36":"code","ffbd850c":"code","1e1aac86":"code","fd97eecc":"code","a14f0b78":"code","1005e958":"code","2d2fb68f":"code","f5d4312e":"code","98fd5e8a":"code","091cb90a":"code","bed74c6d":"code","66926c35":"code","7452c570":"code","0b3bd64a":"code","89042e33":"code","bdcd1161":"code","cda00299":"code","5684f6ea":"code","eb00e591":"code","80484baa":"code","1fd1e020":"code","aabb9776":"code","2b710e3f":"code","3979b1ec":"code","d8f070d0":"code","2933484c":"code","2762c453":"code","b3c01bae":"code","ccb46d64":"code","2e78a38a":"code","1347943a":"code","e7833fd0":"markdown","682b0cbb":"markdown","25365ff0":"markdown","4d70b825":"markdown","e2e135b9":"markdown","0d932e1b":"markdown","a3608a3e":"markdown","7dec9948":"markdown","13b0520e":"markdown","994dc7cb":"markdown"},"source":{"77ab17fa":"import pandas as pd\nfrom tqdm import tqdm","96952c1d":"import unicodedata, re, itertools, sys\n\ndef remove_control_chars(the_string):\n    all_chars = (chr(i) for i in range(sys.maxunicode))\n    categories = {'Cc'}\n    control_chars = ''.join(c for c in all_chars if unicodedata.category(c) in categories)\n    # or equivalently and much more efficiently\n    control_chars = ''.join(map(chr, itertools.chain(range(0x00,0x20), range(0x7f,0xa0))))\n    \n    control_char_re = re.compile('[%s]' % re.escape(control_chars))\n    return control_char_re.sub('', the_string)","ec63ae9f":"def reforge_aiat_dataset(file_target, output_csv_name):\n    import pandas\n    df = pandas.read_csv(file_target)\n    row_n = len(df.index)\n    df['text'] = \"\"\n    for x in tqdm(range(0,len(df))):\n        folder_name = df['Filename'][x][:5]\n        file_name = df['Filename'][x]\n        target_location = \"train-data\/\" + folder_name + \"\/\" + file_name\n        f = open(target_location, \"r\", encoding=\"utf8\")\n        text_data = f.read()\n        df['text'][x] = remove_control_chars(text_data.encode('ascii', 'ignore').decode('utf-8').replace('\\n',' '))\n        f.close()\n    df.to_csv(output_csv_name, index=False)","08296eb2":"reforge_aiat_dataset(\"train.csv\",\"reforge_train_set.csv\")","514941f0":"df = pd.read_csv(\"reforge_train_set.csv\")","1a135c85":"def reforge_aiat_test_dataset(file_target, output_csv_name):\n    import pandas\n    df = pandas.read_csv(file_target)\n    row_n = len(df.index)\n    df['text'] = \"\"\n    for x in tqdm(range(0,len(df))):\n        folder_name = df['Id'][x][:5]\n        file_name = df['Id'][x]\n        target_location = \"test-data\/test-data\/\" + folder_name + \"\/\" + file_name\n        f = open(target_location, \"r\", encoding=\"utf8\", errors='ignore')\n        text_data = f.read()\n        df['text'][x] = remove_control_chars(text_data.encode('ascii', 'ignore').decode('utf-8').replace('\\n',' '))\n        f.close()\n    df.to_csv(output_csv_name, index=False)","11f57d48":"reforge_aiat_test_dataset('test.csv','reforge_test_set.csv')","5d16c5ca":"df.head()","0c4eb2c4":"import pandas as pd\nfrom tqdm import tqdm\n\nimport unicodedata, re, itertools, sys\n\ndef remove_control_chars(the_string):\n    all_chars = (chr(i) for i in range(sys.maxunicode))\n    categories = {'Cc'}\n    control_chars = ''.join(c for c in all_chars if unicodedata.category(c) in categories)\n    # or equivalently and much more efficiently\n    control_chars = ''.join(map(chr, itertools.chain(range(0x00,0x20), range(0x7f,0xa0))))\n    \n    control_char_re = re.compile('[%s]' % re.escape(control_chars))\n    return control_char_re.sub('', the_string)\n\ndef reforge_aiat_dataset(file_target, output_csv_name):\n    import pandas\n    df = pandas.read_csv(file_target)\n    row_n = len(df.index)\n    df['text'] = \"\"\n    for x in tqdm(range(0,len(df))):\n        folder_name = df['Filename'][x][:5]\n        file_name = df['Filename'][x]\n        target_location = \"train-data\/\" + folder_name + \"\/\" + file_name\n        f = open(target_location, \"r\", encoding=\"utf8\")\n        text_data = f.read()\n        df['text'][x] = remove_control_chars(text_data.encode('ascii', 'ignore').decode('utf-8').replace('\\n',' '))\n        f.close()\n    df.to_csv(output_csv_name, index=False)\n\nreforge_aiat_dataset(\"train.csv\",\"reforge_train_set.csv\")\n\ndf = pd.read_csv(\"reforge_train_set.csv\")\n\ndef reforge_aiat_test_dataset(file_target, output_csv_name):\n    import pandas\n    df = pandas.read_csv(file_target)\n    row_n = len(df.index)\n    df['text'] = \"\"\n    for x in tqdm(range(0,len(df))):\n        folder_name = df['Id'][x][:5]\n        file_name = df['Id'][x]\n        target_location = \"test-data\/test-data\/\" + folder_name + \"\/\" + file_name\n        f = open(target_location, \"r\", encoding=\"utf8\", errors='ignore')\n        text_data = f.read()\n        df['text'][x] = remove_control_chars(text_data.encode('ascii', 'ignore').decode('utf-8').replace('\\n',' '))\n        f.close()\n    df.to_csv(output_csv_name, index=False)\n\nreforge_aiat_test_dataset('test.csv','reforge_test_set.csv')\n\ndf.head()","d519be57":"import pandas as pd","082721e5":"data_df = pd.read_csv(\"reforge_train_set.csv\")\ndata_df.head()","54cf123c":"data_df.count()","b6f5c95d":"#Check for duplication\ndata_df['text'].count() - data_df['text'].nunique()","68cbeb4d":"data_df.drop_duplicates(['text'], inplace=True)","842b8b60":"#Check for duplication\ndata_df['text'].count() - data_df['text'].nunique()","f1735dd7":"#Check more correctness: N\/A Data Tuple\ndata_df.isna().sum()","5799c462":"data_df.count()","fba71cdb":"def sentence_langdetect(dataframe, sentencecolumn):\n    # set seed\n    DetectorFactory.seed = 0\n\n    # hold label - language\n    languages = []\n\n    # go through each text\n    for ii in tqdm(range(0,len(dataframe))):\n        # split by space into list, take the first x in text, join with space\n        text = dataframe.iloc[ii][sentencecolumn].split(\" \")\n    \n        lang = \"en\"\n        try:\n            if len(text) > 50:\n                lang = detect(\" \".join(text[:50]))\n            elif len(text) > 0:\n                lang = detect(\" \".join(text[:len(text)]))\n        # ught... beginning of the document was not in a good format\n        except Exception as e:\n            all_words = set(text)\n            try:\n                lang = detect(\" \".join(all_words))\n            # what!! :( let's see if we can find any text in abstract...\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n        # get the language    \n        languages.append(lang)\n        \n    languages_dict = {}\n    for lang in set(languages):\n        languages_dict[lang] = languages.count(lang)\n    print(\"Report of Detected Language:\")\n    print(languages_dict)\n    return languages","73bdf30a":"from tqdm import tqdm\nimport nltk\nfrom langdetect import detect\nfrom langdetect import DetectorFactory","f5573c32":"data_languages = sentence_langdetect(data_df, 'text')","3417b636":"data_df['language'] = data_languages","89c09d97":"data_df = data_df[data_df['language'] == 'en']\ndel data_df['language']","b910602c":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg\nimport string\nfrom nltk.corpus import stopwords","a1cc1261":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)\ncustom_stop_words_academicpaper = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words_academicpaper:\n    if w not in stopwords:\n        stopwords.append(w)","2fefc603":"parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef clean_spacy_tokenizer(dirty):\n    mytokens = parser(dirty)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","3892ed21":"tqdm.pandas()\ndata_df['text'] = data_df['text'].progress_apply(clean_spacy_tokenizer)","4dcf2df6":"data_df.head()","9b163e05":"data_df['word_count'] = data_df['text'].apply(lambda x: len(x.strip().split()))\ndata_df['unique_words'] = data_df['text'].apply(lambda x:len(set(str(x).split())))","36df5c6b":"import seaborn as sns","5d208af6":"sns.distplot(data_df['word_count'])\ndata_df['word_count'].describe()","a0a4623b":"sns.distplot(data_df['unique_words'])\ndata_df['unique_words'].describe()","3aeab237":"data_df.groupby('Blinding of intervention').size()","1e226203":"def blinding_label(row, targetcolumn):\n    if (row[targetcolumn] == 'N'):\n        return 0\n    if (row[targetcolumn] == 'P'):\n        return 1\n    if (row[targetcolumn] == 'Q'):\n        return 2\n    return -1 #In case of invalid input detection","3afba2f5":"def blinding_positive(row, targetcolumn):\n    if (row[targetcolumn] == 'N'):\n        return 0\n    if (row[targetcolumn] == 'P'):\n        return 1\n    if (row[targetcolumn] == 'Q'):\n        return 0\n    return -1 #In case of invalid input detection","8f6c1569":"def blinding_negative(row, targetcolumn):\n    if (row[targetcolumn] == 'N'):\n        return 1\n    if (row[targetcolumn] == 'P'):\n        return 0\n    if (row[targetcolumn] == 'Q'):\n        return 0\n    return -1 #In case of invalid input detection","c4127e75":"def blinding_question(row, targetcolumn):\n    if (row[targetcolumn] == 'N'):\n        return 0\n    if (row[targetcolumn] == 'P'):\n        return 0\n    if (row[targetcolumn] == 'Q'):\n        return 1\n    return -1 #In case of invalid input detection","e09a694e":"data_df.groupby('Classes').size()","585e33de":"def pair_of_blinding_label(row, targetcolumn):\n    if (row[targetcolumn] == 'NN'):\n        return 0\n    if (row[targetcolumn] == 'NP'):\n        return 1\n    if (row[targetcolumn] == 'NQ'):\n        return 2\n    if (row[targetcolumn] == 'PP'):\n        return 3\n    if (row[targetcolumn] == 'PQ'):\n        return 4\n    if (row[targetcolumn] == 'QN'):\n        return 5\n    if (row[targetcolumn] == 'QP'):\n        return 6\n    if (row[targetcolumn] == 'QQ'):\n        return 7\n    if (row[targetcolumn] == 'PN'):\n        return 8\n    return -1 #In case of invalid input detection","c24737d4":"def extract_feature(extraction, dataframe, targetcolumn):\n    if 'extracted_feature' in dataframe:\n        ovewriting = 1\n    else:\n        ovewriting = 0\n    dataframe['extracted_feature'] = dataframe.apply (lambda row: extraction(row, targetcolumn), axis=1)\n    if ovewriting == 1:\n        result = \"Extraction complete and did overwrite on latest 'extracted_feature'\"\n    elif ovewriting == 0:\n        result = \"Extraction complete\"\n    if -1 in dataframe['extracted_feature'].unique():\n        result = \"ERR: Some or all record of this dataframe of feature cannot be extract with this extraction, or user may make an incorrect call\"\n        del dataframe['extracted_feature']\n        if ovewriting == 1:\n            result = result + \"\\nNOTE: Your 'extracted_feature' is corrupted and has been removed\"\n    return print(result)","fb70d38d":"extract_feature(blinding_label, data_df, \"Blinding of intervention\")\ndata_df['BoI_Class'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_label, data_df, \"Blinding of Outcome assessment\")\ndata_df['BoA_Class'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(pair_of_blinding_label, data_df, \"Classes\")\ndata_df['Pair_Class'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_positive, data_df, \"Blinding of intervention\")\ndata_df['BoI_P'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_positive, data_df, \"Blinding of Outcome assessment\")\ndata_df['BoA_P'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_negative, data_df, \"Blinding of intervention\")\ndata_df['BoI_N'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_negative, data_df, \"Blinding of Outcome assessment\")\ndata_df['BoA_N'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_question, data_df, \"Blinding of intervention\")\ndata_df['BoI_Q'] = data_df['extracted_feature']\ndel data_df['extracted_feature']\n\nextract_feature(blinding_question, data_df, \"Blinding of Outcome assessment\")\ndata_df['BoA_Q'] = data_df['extracted_feature']\ndel data_df['extracted_feature']","1501c5a4":"data_df.sample(n=8, random_state=4)","be3eeb5f":"data_df.to_csv('data\/FCNP\/Train_Heavy.csv', index = False)","3b6f6cec":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","5176a944":"#Enter spliting percentage here (Sum of them must equal 100).\nTrainSize = 70\nValSize = 15\nTestSize = 15","445815dd":"def spliter(dataframe, labelcolumn, TrainSize, ValSize, TestSize):\n    X_train=[]\n    y_train=[]\n    X_val=[]\n    y_val=[]\n    X_test=[]\n    y_test=[]\n    SumSize = TrainSize + ValSize + TestSize\n    if (SumSize != 100):\n        err = 1\n    else:\n        err = 0\n    if err != 1:\n        X = dataframe\n        y = LabelEncoder().fit_transform(dataframe[labelcolumn])\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(TestSize\/100), random_state=1)\n        subsplit = (1\/((100-TestSize)\/ValSize))\n        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=subsplit, random_state=1)\n        X_train = X_train.reset_index(drop=True)\n        X_test = X_test.reset_index(drop=True)\n        X_val = X_val.reset_index(drop=True)\n    elif err == 1:\n        print(\"ERR: Dataframe spliting scale incorrect, make sure sum of them must equal to 100\")\n    return X_train, y_train, X_val, y_val, X_test, y_test","54875205":"# Y is not neccesary at this step, but make sure to declare it correct\nX_train, y_train, X_val, y_val, X_test, y_test = spliter(data_df, 'Blinding of intervention', TrainSize, ValSize, TestSize)","001e1c1b":"X_train.to_csv('data\/FCNP\/Train.csv', index = False)\nX_val.to_csv('data\/FCNP\/Val.csv', index = False)\nX_test.to_csv('data\/FCNP\/Test.csv', index = False)","6ac9bac8":"del data_df, X_train, y_train, X_val, y_val, X_test, y_test\n#Optional: For restore used memory space.","074585fc":"df_train = pd.read_csv(\"data\/FCNP\/Train.csv\")\ndf_val = pd.read_csv(\"data\/FCNP\/Val.csv\")\n#Keep Test set for later! make it like real hackaton senario.","1546457d":"from datetime import datetime\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","c050d883":"start_time = datetime.now()\n\n# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word')\ncount_vect.fit(df_train['text'])\n\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(df_train['text'])\nxvalid_count =  count_vect.transform(df_val['text'])\n\n# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', max_features=4096)\ntfidf_vect.fit(df_train['text'])\nxtrain_tfidf =  tfidf_vect.transform(df_train['text'])\nxvalid_tfidf =  tfidf_vect.transform(df_val['text'])\n\ntime_elapsed = datetime.now() - start_time\nprint('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))","cab1f7d6":"from sklearn import model_selection, preprocessing, linear_model, metrics","9dbbc127":"def train_model(classification, feature_vector_train, label):\n    # fit the training dataset on the classifier\n    classification.fit(feature_vector_train, label)\n    return classification","73b160df":"def predict_model(classifier, feature_vector_valid, label):\n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    return metrics.accuracy_score(predictions, label)","4b9521fd":"def do_classify(dataframe, Name_of_Pred, x_train, x_val, y_train, y_val):\n    LR_WordTFIDF_classifier = train_model(linear_model.LogisticRegression(), x_train, y_train)\n    LR_WordTFIDF_accuracy = predict_model(LR_WordTFIDF_classifier, x_val, y_val)\n    LR_WordTFIDF_predictions = LR_WordTFIDF_classifier.predict(x_val)\n    temp_df = dataframe\n    temp_df[Name_of_Pred] = LR_WordTFIDF_predictions\n    #temp_df = temp_df[temp_df[Name_of_Pred] != 1]\n    return temp_df, LR_WordTFIDF_accuracy","e2d1c113":"BoI_P_df_val, BoI_P_Acc = do_classify(df_val, 'Pred_BoI_P', xtrain_tfidf, xvalid_tfidf, df_train['BoI_P'], df_val['BoI_P'])\nBoI_Q_df_val, BoI_Q_Acc = do_classify(df_val, 'Pred_BoI_Q', xtrain_tfidf, xvalid_tfidf, df_train['BoI_Q'], df_val['BoI_Q'])\nBoI_N_df_val, BoI_N_Acc = do_classify(df_val, 'Pred_BoI_N', xtrain_tfidf, xvalid_tfidf, df_train['BoI_N'], df_val['BoI_N'])","17dc3bad":"BoI_P_Acc","5e012041":"BoI_Q_Acc","df2cb316":"BoI_N_Acc","c757c290":"BoA_P_df_val, BoA_P_Acc = do_classify(df_val, 'Pred_BoA_P', xtrain_tfidf, xvalid_tfidf, df_train['BoA_P'], df_val['BoA_P'])\nBoA_Q_df_val, BoA_Q_Acc = do_classify(df_val, 'Pred_BoA_Q', xtrain_tfidf, xvalid_tfidf, df_train['BoA_Q'], df_val['BoA_Q'])\nBoA_N_df_val, BoA_N_Acc = do_classify(df_val, 'Pred_BoA_N', xtrain_tfidf, xvalid_tfidf, df_train['BoA_N'], df_val['BoA_N'])","a6d9ccb6":"BoA_P_Acc","a71fa2c0":"BoA_Q_Acc","902766aa":"BoA_N_Acc","1cf92581":"def blinding_predict(row, x_p_column, y_p_column, x_q_column, y_q_column, x_n_column, y_n_column):\n    if (((row[x_p_column] == 0) & (row[y_p_column] == 0)) & ((row[x_q_column] == 0) & (row[y_q_column] == 0)) & ((row[x_n_column] == 1) & (row[y_n_column] == 1))):\n        return 0\n    if (((row[x_p_column] == 1) & (row[y_p_column] == 1)) & ((row[x_q_column] == 0) & (row[y_q_column] == 0)) & ((row[x_n_column] == 0) & (row[y_n_column] == 0))):\n        return 1\n    if (((row[x_p_column] == 0) & (row[y_p_column] == 0)) & ((row[x_q_column] == 1) & (row[y_q_column] == 1)) & ((row[x_n_column] == 0) & (row[y_n_column] == 0))):\n        return 2\n    return -1 #In case of invalid input detection","f5f23045":"def do_validation(validation, dataframe, x_p_column, y_p_column, x_q_column, y_q_column, x_n_column, y_n_column):\n    if 'validate_result' in dataframe:\n        ovewriting = 1\n    else:\n        ovewriting = 0\n    dataframe['validate_result'] = dataframe.apply (lambda row: validation(row, x_p_column, y_p_column, x_q_column, y_q_column, x_n_column, y_n_column), axis=1)\n    if ovewriting == 1:\n        result = \"Validation complete and did overwrite on latest 'validate_result'\"\n    elif ovewriting == 0:\n        result = \"Validation complete\"\n    #if -1 in dataframe['validate_result'].unique():\n    #    result = \"ERR: Some or all record of this dataframe of feature cannot be validate with this validation, or user may make an incorrect call\"\n    #    del dataframe['validate_result']\n    #    if ovewriting == 1:\n    #        result = result + \"\\nNOTE: Your 'validate_result' is corrupted and has been removed\"\n    #return print(result)","7826e1c4":"def score_validate_result(dataframe, y_column):\n    sum_score = dataframe[dataframe[y_column] == 0][y_column].count() + dataframe[dataframe[y_column] == 1][y_column].count() + dataframe[dataframe[y_column] == 2][y_column].count()\n    accuracy = (sum_score \/ dataframe[y_column].count()) * 100\n    return accuracy","91d04916":"def order_evaluate(dataframe, BlindType, N_df, P_df, Q_df):\n    temp_df = dataframe\n    #df_xX <-- x = positive or negative, X = is N or P or Q\n    df_pN = temp_df[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nN = temp_df[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    \n    df_nN_pQ = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nN_nQ = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    df_nN_nQ_pP = df_nN_nQ[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] != 1])].dropna()\n    df_nN_nQ_pP['Pred_'+BlindType+'_P'] = 1.0\n    df_NQP = pd.concat([df_pN, df_nN_pQ, df_nN_nQ_pP], ignore_index=False).sort_index()\n    \n    df_nN_pP = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nN_nP = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    df_nN_nP_pQ = df_nN_nQ[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] != 1])].dropna()\n    df_nN_nP_pQ['Pred_'+BlindType+'_Q'] = 1.0\n    df_NPQ = pd.concat([df_pN, df_nN_pP, df_nN_nP_pQ], ignore_index=False).sort_index()\n    \n    \n    df_pP = temp_df[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nP = temp_df[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    \n    df_nP_pQ = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nP_nQ = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    df_nP_nQ_pN = df_nP_nQ[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] != 1])].dropna()\n    df_nP_nQ_pN['Pred_'+BlindType+'_N'] = 1.0\n    df_PQN = pd.concat([df_pP, df_nP_pQ, df_nP_nQ_pN], ignore_index=False).sort_index()\n    \n    df_nP_pN = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nP_nN = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    df_nP_nN_pQ = df_nP_nN[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] != 1])].dropna()\n    df_nP_nN_pQ['Pred_'+BlindType+'_Q'] = 1.0\n    df_PNQ = pd.concat([df_pP, df_nP_pN, df_nP_nN_pQ], ignore_index=False).sort_index()\n    \n    \n    df_pQ = temp_df[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nQ = temp_df[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    \n    df_nQ_pP = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nQ_nP = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    df_nQ_nP_pN = df_nQ_nP[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] != 1])].dropna()\n    df_nQ_nP_pN['Pred_'+BlindType+'_N'] = 1.0\n    df_QPN = pd.concat([df_pQ, df_nQ_pP, df_nQ_nP_pN], ignore_index=False).sort_index()\n    \n    df_nQ_pN = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nQ_nN = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    df_nQ_nN_pP = df_nQ_nN[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] != 1])].dropna()\n    df_nQ_nN_pP['Pred_'+BlindType+'_P'] = 1.0\n    df_QNP = pd.concat([df_pQ, df_nQ_pN, df_nQ_nN_pP], ignore_index=False).sort_index()\n    \n    \n    \n    do_validation(blinding_predict, df_NQP, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_NQP[''+BlindType+'_Validate'] = df_NQP['validate_result']\n    del df_NQP['validate_result']\n    vars()[BlindType+'_NQP'] = score_validate_result(df_NQP, ''+BlindType+'_Validate')\n    print(''+BlindType+'_NQP:', vars()[BlindType+'_NQP'])\n    \n    do_validation(blinding_predict, df_NPQ, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_NPQ[''+BlindType+'_Validate'] = df_NPQ['validate_result']\n    del df_NPQ['validate_result']\n    vars()[BlindType+'_NPQ'] = score_validate_result(df_NPQ, ''+BlindType+'_Validate')\n    print(''+BlindType+'_NPQ:', vars()[BlindType+'_NPQ'])\n    \n    do_validation(blinding_predict, df_PQN, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_PQN[''+BlindType+'_Validate'] = df_PQN['validate_result']\n    del df_PQN['validate_result']\n    vars()[BlindType+'_PQN'] = score_validate_result(df_PQN, ''+BlindType+'_Validate')\n    print(''+BlindType+'_PQN:', vars()[BlindType+'_PQN'])\n    \n    do_validation(blinding_predict, df_PNQ, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_PNQ[''+BlindType+'_Validate'] = df_PNQ['validate_result']\n    del df_PNQ['validate_result']\n    vars()[BlindType+'_PNQ'] = score_validate_result(df_PNQ, ''+BlindType+'_Validate')\n    print(''+BlindType+'_PNQ:', vars()[BlindType+'_PNQ'])\n    \n    do_validation(blinding_predict, df_QPN, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_QPN[''+BlindType+'_Validate'] = df_QPN['validate_result']\n    del df_QPN['validate_result']\n    vars()[BlindType+'_QPN'] = score_validate_result(df_QPN, ''+BlindType+'_Validate')\n    print(''+BlindType+'_QPN:', vars()[BlindType+'_QPN'])\n    \n    do_validation(blinding_predict, df_QNP, 'Pred_'+BlindType+'_P', ''+BlindType+'_P', 'Pred_'+BlindType+'_Q', ''+BlindType+'_Q', 'Pred_'+BlindType+'_N', ''+BlindType+'_N')\n    df_QNP[''+BlindType+'_Validate'] = df_QNP['validate_result']\n    del df_QNP['validate_result']\n    vars()[BlindType+'_QNP'] = score_validate_result(df_QNP, ''+BlindType+'_Validate')\n    print(''+BlindType+'_QNP:', vars()[BlindType+'_QNP'])\n    \n    return df_NQP, df_NPQ, df_PQN, df_PNQ, df_QPN, df_QNP, vars()[BlindType+'_NQP'], vars()[BlindType+'_NPQ'], vars()[BlindType+'_PQN'], vars()[BlindType+'_PNQ'], vars()[BlindType+'_QPN'], vars()[BlindType+'_QNP']","4f110050":"BoI_NQP_df, BoI_NPQ_df, BoI_PQN_df, BoI_PNQ_df, BoI_QPN_df, BoI_QNP_df, BoI_NQP, BoI_NPQ, BoI_PQN, BoI_PNQ, BoI_QPN, BoI_QNP = \\\norder_evaluate(df_val,\"BoI\", BoI_N_df_val, BoI_P_df_val, BoI_Q_df_val)","3e4680c0":"BoA_NQP_df, BoA_NPQ_df, BoA_PQN_df, BoA_PNQ_df, BoA_QPN_df, BoA_QNP_df, BoA_NQP, BoA_NPQ, BoA_PQN, BoA_PNQ, BoA_QPN, BoA_QNP = \\\norder_evaluate(df_val,\"BoA\", BoA_N_df_val, BoA_P_df_val, BoA_Q_df_val)","20743aca":"def pair_blinding_unlabel(row, BoI_df, BoA_df):\n    if ((row[BoI_df] == 0) & (row[BoA_df] == 0)):\n        return 'NN'\n    elif ((row[BoI_df] == 0) & (row[BoA_df] == 1)):\n        return 'NP'\n    elif ((row[BoI_df] == 0) & (row[BoA_df] == 2)):\n        return 'NQ'\n    elif ((row[BoI_df] == 1) & (row[BoA_df] == 1)):\n        return 'PP'\n    elif ((row[BoI_df] == 1) & (row[BoA_df] == 2)):\n        return 'PQ'\n    elif ((row[BoI_df] == 2) & (row[BoA_df] == 0)):\n        return 'QN'\n    elif ((row[BoI_df] == 2) & (row[BoA_df] == 1)):\n        return 'QP'\n    elif ((row[BoI_df] == 2) & (row[BoA_df] == 2)):\n        return 'QQ'\n    elif ((row[BoI_df] == 1) & (row[BoA_df] == 0)):\n        return 'PN'\n    #Real next generation of Intuition (\/\/omg, burn me. lol):\n    elif (((row[BoI_df] == 0) & (row[BoA_df] == -1)) | ((row[BoI_df] == -1) & (row[BoA_df] == 0))):\n        return 'NN'\n    elif (((row[BoI_df] == 1) & (row[BoA_df] == -1)) | ((row[BoI_df] == -1) & (row[BoA_df] == 1))):\n        return 'PP'\n    elif (((row[BoI_df] == 2) & (row[BoA_df] == -1)) | ((row[BoI_df] == -1) & (row[BoA_df] == 2))):\n        return 'QQ'\n    #Most unbeliveable\n    elif ((row[BoI_df] == -1) & (row[BoA_df] == -1)):\n        return 'QQ'\n    return -1 #In case of invalid input detection","46bacd9e":"def do_pairing(pairing, dataframe, BoI_df, BoA_df):\n    dataframe['pairing_result'] = dataframe.apply (lambda row: pairing(row, BoI_df, BoA_df), axis=1)\n    result = \"Pairing complete\"\n    return print(result)","8de0786b":"Val_result_df = pd.DataFrame()\nVal_result_df['Filename'] = BoI_PNQ_df['Filename']\nVal_result_df['BoI_Validate'] = BoI_PNQ_df['BoI_Validate']\nVal_result_df['BoA_Validate'] = BoA_PNQ_df['BoA_Validate']\ndo_pairing(pair_blinding_unlabel, Val_result_df, 'BoI_Validate', 'BoA_Validate')","8e7ea216":"#Val_result_df[Val_result_df['BoA_Validate']==0]","3e88b041":"Val_result_df['Id'] = Val_result_df['Filename']\nVal_result_df['Predication'] = Val_result_df['pairing_result']\ndel Val_result_df['BoI_Validate'], Val_result_df['BoA_Validate'], Val_result_df['pairing_result'], Val_result_df['Filename']\n\nVal_result_df.head()","4a9e018f":"Val_result_df.to_csv('data\/FCNP\/Val_Submit.csv', index = False)","fd33f084":"def score_pairing(row, BoI_df, BoA_df):\n    if ((row[BoI_df] == 'NN') & (row[BoA_df] == 'NN')):\n        return 1\n    elif ((row[BoI_df] == 'NP') & (row[BoA_df] == 'NP')):\n        return 1\n    elif ((row[BoI_df] == 'NQ') & (row[BoA_df] == 'NQ')):\n        return 1\n    elif ((row[BoI_df] == 'PP') & (row[BoA_df] == 'PP')):\n        return 1\n    elif ((row[BoI_df] == 'PQ') & (row[BoA_df] == 'PQ')):\n        return 1\n    elif ((row[BoI_df] == 'QN') & (row[BoA_df] == 'QN')):\n        return 1\n    elif ((row[BoI_df] == 'QP') & (row[BoA_df] == 'QP')):\n        return 1\n    elif ((row[BoI_df] == 'QQ') & (row[BoA_df] == 'QQ')):\n        return 1\n    elif ((row[BoI_df] == 'PN') & (row[BoA_df] == 'PN')):\n        return 1\n    return 0 #In case of invalid input detection","dad0e250":"Val_result_df['y'] = df_val['Classes']\ndo_pairing(score_pairing, Val_result_df, 'Predication', 'y')","1b43e3d7":"Val_result_df['Score'] = Val_result_df['pairing_result']\ndel Val_result_df['y'], Val_result_df['pairing_result']\n\nVal_result_df.head()","d2fa5aba":"def accurary_pair_result(dataframe, y_column):\n    sum_score = dataframe[dataframe[y_column] == 1][y_column].count()\n    accuracy = (sum_score \/ dataframe[y_column].count()) * 100\n    return accuracy","37bd681d":"accurary_pair_result(Val_result_df, 'Score')","4f67c2f8":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport pickle\nfrom tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nimport tensorflow as tf\n\nimport pandas, xgboost, numpy, textblob, string, nltk\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers","15223584":"test_data_df = pd.read_csv(\"reforge_test_set.csv\")\ntest_data_df.head()","de29b2f1":"del test_data_df['Prediction']","6a2ad900":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)\ncustom_stop_words_academicpaper = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words_academicpaper:\n    if w not in stopwords:\n        stopwords.append(w)","037a238d":"parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef clean_spacy_tokenizer(dirty):\n    mytokens = parser(dirty)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","4a8794ae":"tqdm.pandas()\ntest_data_df['text'] = test_data_df['text'].progress_apply(clean_spacy_tokenizer)","845a1813":"test_data_df.to_csv('test.csv', index = False)","f7a0d21e":"df_test = pd.read_csv(\"test.csv\")","6f1fd0b6":"start_time = datetime.now()\n\n# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word')\ncount_vect.fit(df_train['text'])\n\n# transform the training and validation data using count vectorizer object\nxtest_count =  count_vect.transform(df_test['text'])\n\n# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', max_features=4096)\ntfidf_vect.fit(df_train['text'])\nxtrain_tfidf =  tfidf_vect.transform(df_train['text'])\nxtest_tfidf =  tfidf_vect.transform(df_test['text'])\n\ntime_elapsed = datetime.now() - start_time\nprint('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))","39b2a77e":"def do_classify_testset(dataframe, Name_of_Pred, x_train, x_test, y_train):\n    LR_WordTFIDF_classifier = train_model(linear_model.LogisticRegression(), x_train, y_train)\n    LR_WordTFIDF_predictions = LR_WordTFIDF_classifier.predict(x_test)\n    temp_df = dataframe\n    temp_df[Name_of_Pred] = LR_WordTFIDF_predictions\n    return temp_df","02355ced":"BoI_P_df_test = do_classify_testset(df_test, 'Pred_BoI_P', xtrain_tfidf, xtest_tfidf, df_train['BoI_P'])\nBoI_Q_df_test = do_classify_testset(df_test, 'Pred_BoI_Q', xtrain_tfidf, xtest_tfidf, df_train['BoI_Q'])\nBoI_N_df_test = do_classify_testset(df_test, 'Pred_BoI_N', xtrain_tfidf, xtest_tfidf, df_train['BoI_N'])\nBoA_P_df_test = do_classify_testset(df_test, 'Pred_BoA_P', xtrain_tfidf, xtest_tfidf, df_train['BoA_P'])\nBoA_Q_df_test = do_classify_testset(df_test, 'Pred_BoA_Q', xtrain_tfidf, xtest_tfidf, df_train['BoA_Q'])\nBoA_N_df_test = do_classify_testset(df_test, 'Pred_BoA_N', xtrain_tfidf, xtest_tfidf, df_train['BoA_N'])","26af4e70":"def testset_blinding_predict(row, p_column, q_column, n_column):\n    if ((row[p_column] == 0) & (row[q_column] == 0) & (row[n_column] == 1)):\n        return 0\n    if ((row[p_column] == 1) & (row[q_column] == 0) & (row[n_column] == 0)):\n        return 1\n    if ((row[p_column] == 0) & (row[q_column] == 1) & (row[n_column] == 0)):\n        return 2\n    return -1 #In case of invalid input detection","6d5f040c":"def testset_do_validation(validation, dataframe, p_column, q_column, n_column):\n    if 'validate_result' in dataframe:\n        ovewriting = 1\n    else:\n        ovewriting = 0\n    dataframe['validate_result'] = dataframe.apply (lambda row: validation(row, p_column, q_column, n_column), axis=1)\n    if ovewriting == 1:\n        result = \"Validation complete and did overwrite on latest 'validate_result'\"\n    elif ovewriting == 0:\n        result = \"Validation complete\"","02f11a79":"def evaluate_testset(dataframe, BlindType, N_df, P_df, Q_df):\n    temp_df = dataframe\n    #df_xX <-- x = positive or negative, X = is N or P or Q\n    df_pN = temp_df[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nN = temp_df[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    \n    df_nN_pQ = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nN_nQ = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    df_nN_nQ_pP = df_nN_nQ[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_Q'] != 1])].dropna()\n    df_nN_nQ_pP['Pred_'+BlindType+'_P'] = 1.0\n    df_NQP = pd.concat([df_pN, df_nN_pQ, df_nN_nQ_pP], ignore_index=False).sort_index()\n    \n    df_nN_pP = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nN_nP = df_nN[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    df_nN_nP_pQ = df_nN_nQ[temp_df.isin(N_df[N_df['Pred_'+BlindType+'_P'] != 1])].dropna()\n    df_nN_nP_pQ['Pred_'+BlindType+'_Q'] = 1.0\n    df_NPQ = pd.concat([df_pN, df_nN_pP, df_nN_nP_pQ], ignore_index=False).sort_index()\n    \n    \n    df_pP = temp_df[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nP = temp_df[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    \n    df_nP_pQ = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nP_nQ = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    df_nP_nQ_pN = df_nP_nQ[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_Q'] != 1])].dropna()\n    df_nP_nQ_pN['Pred_'+BlindType+'_N'] = 1.0\n    df_PQN = pd.concat([df_pP, df_nP_pQ, df_nP_nQ_pN], ignore_index=False).sort_index()\n    \n    df_nP_pN = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nP_nN = df_nP[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    df_nP_nN_pQ = df_nP_nN[temp_df.isin(P_df[P_df['Pred_'+BlindType+'_N'] != 1])].dropna()\n    df_nP_nN_pQ['Pred_'+BlindType+'_Q'] = 1.0\n    df_PNQ = pd.concat([df_pP, df_nP_pN, df_nP_nN_pQ], ignore_index=False).sort_index()\n    \n    \n    df_pQ = temp_df[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_Q'] == 1])].dropna()\n    df_nQ = temp_df[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_Q'] == 0])].dropna()\n    \n    df_nQ_pP = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] == 1])].dropna()\n    df_nQ_nP = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] == 0])].dropna()\n    df_nQ_nP_pN = df_nQ_nP[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_P'] != 1])].dropna()\n    df_nQ_nP_pN['Pred_'+BlindType+'_N'] = 1.0\n    df_QPN = pd.concat([df_pQ, df_nQ_pP, df_nQ_nP_pN], ignore_index=False).sort_index()\n    \n    df_nQ_pN = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] == 1])].dropna()\n    df_nQ_nN = df_nQ[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] == 0])].dropna()\n    df_nQ_nN_pP = df_nQ_nN[temp_df.isin(Q_df[Q_df['Pred_'+BlindType+'_N'] != 1])].dropna()\n    df_nQ_nN_pP['Pred_'+BlindType+'_P'] = 1.0\n    df_QNP = pd.concat([df_pQ, df_nQ_pN, df_nQ_nN_pP], ignore_index=False).sort_index()\n    \n    \n    \n    testset_do_validation(testset_blinding_predict, df_NQP, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_NQP[''+BlindType+'_Validate'] = df_NQP['validate_result']\n    del df_NQP['validate_result']\n    print(''+BlindType+'_NQP: Done')\n    \n    testset_do_validation(testset_blinding_predict, df_NPQ, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_NPQ[''+BlindType+'_Validate'] = df_NPQ['validate_result']\n    del df_NPQ['validate_result']\n    print(''+BlindType+'_NPQ: Done')\n    \n    testset_do_validation(testset_blinding_predict, df_PQN, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_PQN[''+BlindType+'_Validate'] = df_PQN['validate_result']\n    del df_PQN['validate_result']\n    print(''+BlindType+'_PQN: Done')\n    \n    testset_do_validation(testset_blinding_predict, df_PNQ, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_PNQ[''+BlindType+'_Validate'] = df_PNQ['validate_result']\n    del df_PNQ['validate_result']\n    print(''+BlindType+'_PNQ: Done')\n    \n    testset_do_validation(testset_blinding_predict, df_QPN, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_QPN[''+BlindType+'_Validate'] = df_QPN['validate_result']\n    del df_QPN['validate_result']\n    print(''+BlindType+'_QPN: Done')\n    \n    testset_do_validation(testset_blinding_predict, df_QNP, 'Pred_'+BlindType+'_P', 'Pred_'+BlindType+'_Q', 'Pred_'+BlindType+'_N')\n    df_QNP[''+BlindType+'_Validate'] = df_QNP['validate_result']\n    del df_QNP['validate_result']\n    print(''+BlindType+'_QNP: Done')\n    \n    return df_NQP, df_NPQ, df_PQN, df_PNQ, df_QPN, df_QNP","01501f92":"BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test = \\\nevaluate_testset(df_test,\"BoI\", BoI_N_df_test, BoI_P_df_test, BoI_Q_df_test)","76d3dfc5":"BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test = \\\nevaluate_testset(df_test,\"BoA\", BoA_N_df_test, BoA_P_df_test, BoA_Q_df_test)","276d5c36":"def instant_testing(CombineCode, BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test):\n    Test_result_df = pd.DataFrame()\n    Test_result_df['Id'] = vars()['BoI_'+CombineCode+'_df_test']['Id']\n    Test_result_df['BoI_Validate'] = vars()['BoI_'+CombineCode+'_df_test']['BoI_Validate']\n    Test_result_df['BoA_Validate'] = vars()['BoA_'+CombineCode+'_df_test']['BoA_Validate']\n    do_pairing(pair_blinding_unlabel, Test_result_df, 'BoI_Validate', 'BoA_Validate')\n    #Test_result_df['Id'] = Test_result_df['Id']\n    Test_result_df['Prediction'] = Test_result_df['pairing_result']\n    del Test_result_df['BoI_Validate'], Test_result_df['BoA_Validate']\n    del Test_result_df['pairing_result']\n    Test_result_df.to_csv('data\/FCNP\/Submit2\/Test_Submit_'+CombineCode+'.csv', index = False)\n    print(\"Export Complete\")\n    del Test_result_df","ffbd850c":"instant_testing('NQP', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","1e1aac86":"instant_testing('NPQ', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","fd97eecc":"instant_testing('PQN', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","a14f0b78":"instant_testing('PNQ', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","1005e958":"instant_testing('QPN', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","2d2fb68f":"instant_testing('QNP', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","f5d4312e":"df_train_heavy = pd.read_csv(\"data\/FCNP\/Train_Heavy.csv\")\ndf_test = pd.read_csv(\"test.csv\")","98fd5e8a":"start_time = datetime.now()\n\n# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word')\ncount_vect.fit(df_train_heavy['text'])\n\n# transform the training and validation data using count vectorizer object\nxtest_count =  count_vect.transform(df_test['text'])\n\n# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', max_features=4096)\ntfidf_vect.fit(df_train_heavy['text'])\nxtrainheavy_tfidf =  tfidf_vect.transform(df_train_heavy['text'])\nxtest_tfidf =  tfidf_vect.transform(df_test['text'])\n\ntime_elapsed = datetime.now() - start_time\nprint('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))","091cb90a":"BoI_P_df_test = do_classify_testset(df_test, 'Pred_BoI_P', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoI_P'])\nBoI_Q_df_test = do_classify_testset(df_test, 'Pred_BoI_Q', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoI_Q'])\nBoI_N_df_test = do_classify_testset(df_test, 'Pred_BoI_N', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoI_N'])\nBoA_P_df_test = do_classify_testset(df_test, 'Pred_BoA_P', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoA_P'])\nBoA_Q_df_test = do_classify_testset(df_test, 'Pred_BoA_Q', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoA_Q'])\nBoA_N_df_test = do_classify_testset(df_test, 'Pred_BoA_N', xtrainheavy_tfidf, xtest_tfidf, df_train_heavy['BoA_N'])","bed74c6d":"BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test = \\\nevaluate_testset(df_test,\"BoI\", BoI_N_df_test, BoI_P_df_test, BoI_Q_df_test)","66926c35":"BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test = \\\nevaluate_testset(df_test,\"BoA\", BoA_N_df_test, BoA_P_df_test, BoA_Q_df_test)","7452c570":"extract_feature(blinding_label, df_train_heavy, \"Blinding of intervention\")\ndf_train_heavy['BoI_Class'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_label, df_train_heavy, \"Blinding of Outcome assessment\")\ndf_train_heavy['BoA_Class'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(pair_of_blinding_label, df_train_heavy, \"Classes\")\ndf_train_heavy['Pair_Class'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_positive, df_train_heavy, \"Blinding of intervention\")\ndf_train_heavy['BoI_P'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_positive, df_train_heavy, \"Blinding of Outcome assessment\")\ndf_train_heavy['BoA_P'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_negative, df_train_heavy, \"Blinding of intervention\")\ndf_train_heavy['BoI_N'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_negative, df_train_heavy, \"Blinding of Outcome assessment\")\ndf_train_heavy['BoA_N'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_question, df_train_heavy, \"Blinding of intervention\")\ndf_train_heavy['BoI_Q'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']\n\nextract_feature(blinding_question, df_train_heavy, \"Blinding of Outcome assessment\")\ndf_train_heavy['BoA_Q'] = df_train_heavy['extracted_feature']\ndel df_train_heavy['extracted_feature']","0b3bd64a":"BoI_NQP_df_test","89042e33":"def instant_testing(CombineCode, BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test):\n    Test_result_df = pd.DataFrame()\n    Test_result_df['Id'] = vars()['BoI_'+CombineCode+'_df_test']['Id']\n    Test_result_df['BoI_Validate'] = vars()['BoI_'+CombineCode+'_df_test']['BoI_Validate']\n    Test_result_df['BoA_Validate'] = vars()['BoA_'+CombineCode+'_df_test']['BoA_Validate']\n    do_pairing(pair_blinding_unlabel, Test_result_df, 'BoI_Validate', 'BoA_Validate')\n    #Test_result_df['Id'] = Test_result_df['Id']\n    Test_result_df['Prediction'] = Test_result_df['pairing_result']\n    del Test_result_df['BoI_Validate'], Test_result_df['BoA_Validate']\n    del Test_result_df['pairing_result']\n    Test_result_df.to_csv('data\/FCNP\/Submit2\/Test_Submit_'+CombineCode+'_heavy.csv', index = False)\n    print(\"Export Complete\")\n    del Test_result_df","bdcd1161":"instant_testing('NQP', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","cda00299":"instant_testing('NPQ', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","5684f6ea":"instant_testing('PQN', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","eb00e591":"instant_testing('PNQ', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","80484baa":"instant_testing('QPN', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","1fd1e020":"instant_testing('QNP', BoI_NQP_df_test, BoI_NPQ_df_test, BoI_PQN_df_test, BoI_PNQ_df_test, BoI_QPN_df_test, BoI_QNP_df_test,BoA_NQP_df_test, BoA_NPQ_df_test, BoA_PQN_df_test, BoA_PNQ_df_test, BoA_QPN_df_test, BoA_QNP_df_test)","aabb9776":"BoI_P_df_test, BoI_P_Acc = do_classify(df_test, 'Pred_BoI_P', xtrain_tfidf, xtest_tfidf, df_train['BoI_P'], df_test['BoI_P'])\nBoI_Q_df_test, BoI_Q_Acc = do_classify(df_test, 'Pred_BoI_Q', xtrain_tfidf, xtest_tfidf, df_train['BoI_Q'], df_test['BoI_Q'])\nBoI_N_df_test, BoI_N_Acc = do_classify(df_test, 'Pred_BoI_N', xtrain_tfidf, xtest_tfidf, df_train['BoI_N'], df_test['BoI_N'])\nBoA_P_df_test, BoA_P_Acc = do_classify(df_test, 'Pred_BoA_P', xtrain_tfidf, xtest_tfidf, df_train['BoA_P'], df_test['BoA_P'])\nBoA_Q_df_test, BoA_Q_Acc = do_classify(df_test, 'Pred_BoA_Q', xtrain_tfidf, xtest_tfidf, df_train['BoA_Q'], df_test['BoA_Q'])\nBoA_N_df_test, BoA_N_Acc = do_classify(df_test, 'Pred_BoA_N', xtrain_tfidf, xtest_tfidf, df_train['BoA_N'], df_test['BoA_N'])","2b710e3f":"BoI_NQP_df, BoI_NPQ_df, BoI_PQN_df, BoI_PNQ_df, BoI_QPN_df, BoI_QNP_df, BoI_NQP, BoI_NPQ, BoI_PQN, BoI_PNQ, BoI_QPN, BoI_QNP = \\\norder_evaluate(df_test,\"BoI\", BoI_N_df_test, BoI_P_df_test, BoI_Q_df_test)","3979b1ec":"BoA_NQP_df, BoA_NPQ_df, BoA_PQN_df, BoA_PNQ_df, BoA_QPN_df, BoA_QNP_df, BoA_NQP, BoA_NPQ, BoA_PQN, BoA_PNQ, BoA_QPN, BoA_QNP = \\\norder_evaluate(df_test,\"BoA\", BoA_N_df_test, BoA_P_df_test, BoA_Q_df_test)","d8f070d0":"Test_result_df = pd.DataFrame()\nTest_result_df['Filename'] = BoI_PNQ_df['Filename']\nTest_result_df['BoI_Validate'] = BoI_PNQ_df['BoI_Validate']\nTest_result_df['BoA_Validate'] = BoA_PNQ_df['BoA_Validate']\ndo_pairing(pair_blinding_unlabel, Test_result_df, 'BoI_Validate', 'BoA_Validate')","2933484c":"Test_result_df['Id'] = Test_result_df['Filename']\nTest_result_df['Predication'] = Test_result_df['pairing_result']\ndel Test_result_df['BoI_Validate'], Test_result_df['BoA_Validate'], Test_result_df['pairing_result'], Test_result_df['Filename']\n\nTest_result_df.head()","2762c453":"Test_result_df.to_csv('data\/FCNP\/Test_Submit.csv', index = False)","b3c01bae":"Test_result_df['y'] = df_test['Classes']\ndo_pairing(score_pairing, Test_result_df, 'Predication', 'y')","ccb46d64":"Test_result_df['Score'] = Test_result_df['pairing_result']\ndel Test_result_df['y'], Test_result_df['pairing_result']\n\nTest_result_df.head()","2e78a38a":"accurary_pair_result(Test_result_df, 'Score')","1347943a":"LR_WordTFIDF_classifier.decision_function(xvalid_tfidf)","e7833fd0":"<h1>Now operate with Test Set<\/h1>","682b0cbb":"Final Score: (Late-Submission)\n![image.png](attachment:image.png)","25365ff0":"Preprocess","4d70b825":"Data Loading","e2e135b9":"Feature Engineering","0d932e1b":"Loading Preprocessed Data","a3608a3e":"Reveal Score on Test:","7dec9948":"For Pre-Executed Please use [this link](http:\/\/601059.surachet-r.net\/_FTP\/Kaggle\/Complete_FCNP.html)<br>\n\u0e14\u0e39\u0e44\u0e1f\u0e25\u0e4c\u0e41\u0e1a\u0e1a\u0e23\u0e31\u0e19\u0e44\u0e27\u0e49\u0e41\u0e25\u0e49\u0e27 [\u0e04\u0e25\u0e34\u0e01\u0e17\u0e35\u0e48\u0e19\u0e35\u0e48](http:\/\/601059.surachet-r.net\/_FTP\/Kaggle\/Complete_FCNP.html)<br>\n<br>\nDownload .ipynb [here](http:\/\/601059.surachet-r.net\/_FTP\/Kaggle\/Complete_FCNP.ipynb)","13b0520e":"Heavy Training Version","994dc7cb":"> **<h3>Scientific Journal Cleaning and Logistic Regression with Decision Tree Text Classification:<\/h3>**\n>\n> Dedicate to people who want to learn experiment from boosting Logistic Regression's accuracy with Decision Tree.<br>\n> No citation require, I did this with love."}}