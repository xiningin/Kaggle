{"cell_type":{"0d7314f3":"code","0803adc4":"code","5e8bfdaa":"code","a8e3b634":"code","1d574ffb":"code","5893defb":"code","ccc48ee0":"code","3b658950":"code","a5538363":"code","6842b44d":"code","d0956863":"code","0e8616e7":"code","7680d063":"code","e538bb2f":"code","9537cbde":"code","726ab530":"code","f15a9121":"code","6ecca446":"code","d6d5999c":"code","ab7a8532":"code","abdbf545":"code","897069bf":"code","5b766e54":"code","e93699f5":"code","19ec7d1c":"code","9800b215":"code","34a9bfd9":"code","a7121446":"code","7a199fc7":"code","c6d09641":"code","2c6e7499":"code","c0aa776f":"markdown","1b41bf35":"markdown","bb75d0de":"markdown","6021a61a":"markdown","58b8ed67":"markdown","446d5ce7":"markdown","17da65d7":"markdown","79a21f73":"markdown","0719506f":"markdown","174732c1":"markdown","a04ecd41":"markdown","06ac9976":"markdown","2a3539ae":"markdown","665e72b1":"markdown","423836a1":"markdown","03cdae0e":"markdown"},"source":{"0d7314f3":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn import ensemble\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999","0803adc4":"import os\nos.listdir(\"..\/input\/mercedes-benz-greener-manufacturing\")","5e8bfdaa":"train_df = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip\")\ntest_df = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip\")","a8e3b634":"print(f'Train Shape: {train_df.shape}')\nprint(f'Test Shape: {test_df.shape}')","1d574ffb":"train_df.head()","5893defb":"plt.figure(figsize=(8, 6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","ccc48ee0":"upper_limit = 180\ntrain_df.loc[train_df['y'] > upper_limit, 'y'] = upper_limit\n\nplt.figure(figsize=(8, 6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","3b658950":"plt.figure(figsize=(12,8))\nsns.distplot(train_df.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","a5538363":"train_df.dtypes.reset_index()","6842b44d":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Column\", \"Column Type\"]\ndtype_df['Column Type'].value_counts()","d0956863":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\")[\"Count\"].count().reset_index()","0e8616e7":"dtype_df.loc[:10, :]","7680d063":"train_df.isnull().sum(axis=0).reset_index()","e538bb2f":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count'] > 0]\nmissing_df = missing_df.sort_values(by=\"missing_count\")\nmissing_df","9537cbde":"unique_values_dict = {}\n\nfor col in train_df.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_df[col].unique()).tolist())\n        if unique_value not in unique_values_dict:\n            unique_values_dict[unique_value] = [col]\n        else:\n            unique_values_dict[unique_value].append(col)\n\nfor unique_val in unique_values_dict:\n    print(\"Columns containing the unique values : \",unique_val)\n    print(unique_values_dict[unique_val])\n    print(\"--------------------------------------------------\")","726ab530":"train_df['X1'].value_counts()","f15a9121":"var_name = \"X1\"\n\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","6ecca446":"var_name = \"X2\"\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","d6d5999c":"var_name = \"X3\"\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","ab7a8532":"one_count_list = []\nzero_count_list = []\n\ncols_list = unique_values_dict['[0, 1]']\n\n# Now to store total no. of 0's & 1's in each col\nfor col in cols_list:\n    zero_count_list.append((train_df[col] == 0).sum())\n    one_count_list.append((train_df[col] == 1).sum())\n\nN = len(cols_list)\nind = np.arange(N)\nwidth = 0.35\n\nplt.figure(figsize=(6,100))\np1 = plt.barh(ind, zero_count_list, width, color='red')\np2 = plt.barh(ind, one_count_list, width, left=zero_count_list, color=\"green\")\nplt.yticks(ind, cols_list)\nplt.legend((p1, p2), ('Zero count', 'One Count'))\nplt.title(\"Count Distribution\", fontsize=15)\nplt.show()","abdbf545":"zero_mean_list = []\none_mean_list = []\n\ncols_list = unique_values_dict['[0, 1]']\n\nfor col in cols_list:\n    zero_mean_list.append(train_df.loc[train_df[col] == 0, 'y'].mean())\n    one_mean_list.append(train_df.loc[train_df[col] == 1, 'y'].mean())\n\ntemp_df = pd.DataFrame({\"column_name\": cols_list + cols_list, \"value\": [0]*len(cols_list) + [1]*len(cols_list), \"y_mean\": zero_mean_list + one_mean_list})\ntemp_df = temp_df.pivot(index = 'column_name', columns = 'value', values = 'y_mean')\ntemp_df.head()","897069bf":"plt.figure(figsize=(8, 80))\nsns.heatmap(temp_df, cmap=\"YlGnBu\")\nplt.title(\"Mean of y val across binary variables\", fontsize=15)\nplt.show()","5b766e54":"var_name = \"ID\"\n\nplt.figure(figsize=(12,6))\nsns.regplot(x = var_name, y = 'y', data = train_df, scatter_kws = {'alpha':0.5, 's':30})\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","e93699f5":"train_df['eval_set'] = \"train\"\ntest_df['eval_set'] = \"test\"\ntrain_df['eval_set'].head()","19ec7d1c":"full_df = pd.concat([train_df[[\"ID\", \"eval_set\"]], test_df[[\"ID\", \"eval_set\"]]])\nfull_df.head()","9800b215":"plt.figure(figsize=(12,6))\nsns.stripplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('ID', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","34a9bfd9":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('ID', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","a7121446":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df['y'].values\ntrain_X = train_df.drop([\"ID\", \"y\", \"eval_set\"], axis=1)\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.getLabel()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100, feval=xgb_r2_score, maximize=True)","7a199fc7":"fig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","c6d09641":"model = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)","2c6e7499":"feat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","c0aa776f":"Now to check if the 'y' mean values of 1s and 0's are almost same or diff wrt each col","1b41bf35":"Categorical occupy the top spots followed by binary variables.\n\nLet us also build a Random Forest model and check the important variables.","bb75d0de":"After Integer Columns Analysis we can see that all the integer columns are binary with some columns have only one unique value 0","6021a61a":" Let's see how the IDs are distributed across train and test.","58b8ed67":"Binary is now on the top spot. \nSo based on diff model there is a significant difference in important features","446d5ce7":"Checking mean 'y' value for 0 & 1, for each binary cols","17da65d7":"Now let us run and xgboost model to get the important variables.","79a21f73":"Now exploring Binary Variables","0719506f":"Binary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes","174732c1":"**Objective:**\n\nThis dataset contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing.","a04ecd41":"\"y\" is the variable we need to predict","06ac9976":"Now we will look into the 'ID' col which will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)","2a3539ae":"The regplot performs a simple linear regression and there seems to be a slight decreasing trend with respect to ID variable.","665e72b1":"Now we have to visualize categorical cols against 'y' col","423836a1":"So majority of the columns are integers with 8 categorical columns and 1 float column (target variable)","03cdae0e":"X0 to X8 are the categorical columns."}}