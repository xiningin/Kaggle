{"cell_type":{"747c4411":"code","e83c479b":"code","340f5811":"code","075454a1":"code","f95406a9":"code","04f90bb4":"code","3f8f2e1e":"code","6e34123c":"code","a5aac31a":"code","6cd7be30":"code","2b1da331":"code","8bbc9bf5":"code","f3bc43d1":"code","0a17d5a3":"code","c22948bc":"code","8a6ecb90":"code","a287ffef":"code","cae869d9":"code","3835f0de":"code","7e03ccda":"code","2e6d483f":"code","e49fc736":"code","d1cb117a":"code","bc193642":"code","f97e2adf":"code","c2b6b1d4":"code","c225082e":"code","4c617aeb":"code","fbbbdac3":"code","208a6519":"code","8a9d7af7":"code","4ddd3565":"code","b336b6f3":"code","4e1ee28a":"code","18f3b751":"code","2cfa5cba":"code","ba8172f2":"code","e1c5016f":"code","41078e97":"code","ce2acd55":"code","d0916af4":"code","35f2e947":"code","4f546ddc":"code","15189d86":"code","bd5667ef":"code","53f8967c":"code","032478bc":"markdown","86b5076e":"markdown","64b2fc4f":"markdown","797aded3":"markdown","e9793bd7":"markdown","1ef45d89":"markdown","867e83d8":"markdown","d714f495":"markdown","6354faba":"markdown","c3a7cb55":"markdown","e4eb6537":"markdown","df9bdc7f":"markdown","82da036f":"markdown"},"source":{"747c4411":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline","e83c479b":"df = pd.read_csv('..\/input\/bike-buyers\/bike_buyers_clean.csv', sep=',')","340f5811":"df.head()","075454a1":"df.shape","f95406a9":"df.columns","04f90bb4":"df.info()","3f8f2e1e":"corrMatrix = df.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","6e34123c":"numerical = [var for var in df.columns if df[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\nprint('The numerical variables are :', numerical)","a5aac31a":"df[numerical].head()","6cd7be30":"# check missing values in numerical variables\ndf[numerical].isnull().sum()","2b1da331":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))\nprint('The categorical variables are :', categorical)","8bbc9bf5":"df[categorical].head()","f3bc43d1":"df[categorical].isnull().sum()","0a17d5a3":"# view frequency counts of values in categorical variables\nfor var in categorical: \n    print(df[var].value_counts())\n    print(df[var].value_counts()\/np.float(len(df)))\n    print()","c22948bc":"# check for cardinality in categorical variables\nfor var in categorical:\n    print(var, ' contains ', len(df[var].unique()), ' labels')","8a6ecb90":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\ndf['Marital Status'] = label_encoder.fit_transform(df['Marital Status'])\ndf['Gender'] = label_encoder.fit_transform(df['Gender'])\ndf['Education'] = label_encoder.fit_transform(df['Education'])\ndf['Occupation'] = label_encoder.fit_transform(df['Occupation'])\ndf['Home Owner'] = label_encoder.fit_transform(df['Home Owner'])\ndf['Commute Distance'] = label_encoder.fit_transform(df['Commute Distance'])\ndf['Region'] = label_encoder.fit_transform(df['Region'])\ndf['Purchased Bike'] = label_encoder.fit_transform(df['Purchased Bike'])\ndf.head()","a287ffef":"df['Age'].describe()","cae869d9":"df['Age'] = pd.cut(x = df['Age'], bins = [0,30,40,50,60,100,150], labels = [0, 1, 2, 3, 4, 5])\ndf['Age'] = df['Age'].astype('int64') \ndf['Age'].isnull().sum()","3835f0de":"df['Income'].describe()","7e03ccda":"df['Income'] = pd.cut(x = df['Income'], bins = [0, 30000, 50000, 75000, 100000, 150000, 200000], labels = [1, 2, 3, 4, 5, 6])\ndf['Income'] = df['Income'].astype('int64') \ndf['Income'].isnull().sum()","2e6d483f":"df.dtypes","e49fc736":"X = df.drop(['Purchased Bike'], axis=1)\ny = df['Purchased Bike']","d1cb117a":"# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 999)\nX_train.shape, X_test.shape","bc193642":"X_train.head()","f97e2adf":"X_train.shape","c2b6b1d4":"X_test.head()","c225082e":"X_test.shape","4c617aeb":"# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n# instantiate the model\ngnb = GaussianNB()\n\n# fit the model\ngnb.fit(X_train, y_train)","fbbbdac3":"y_pred = gnb.predict(X_test)\n\ny_pred[:10]\nlen(y_pred)","208a6519":"from sklearn.metrics import accuracy_score\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","8a9d7af7":"y_pred_train = gnb.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","4ddd3565":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n', cm)","b336b6f3":"cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                                 index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","4e1ee28a":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","18f3b751":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=999)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n# Predict the response for test dataset\ny_pred2 = clf.predict(X_test)","2cfa5cba":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred2)))","ba8172f2":"y_pred_train2 = clf.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train2)))","e1c5016f":"from sklearn import tree\n\nfn=['ID', 'Marital Status', 'Gender', 'Income', 'Children', 'Education',\n       'Occupation', 'Home Owner', 'Cars', 'Commute Distance', 'Region', 'Age']\ncn=['Bought', 'Not Bought']\n\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, feature_names = fn, \n               class_names=cn, filled=True)","41078e97":"from sklearn.model_selection import GridSearchCV\nclf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\ngrid_values = {'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'sqrt', 'log2'], \n               'max_depth':[4,5,6,7,8,9,10], 'min_samples_split': [2,3,4]}\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\ngrid_clf_acc.fit(X_train, y_train)","ce2acd55":"y_pred_acc = grid_clf_acc.predict(X_test)\n\n# New Model Evaluation metrics \nprint('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n\n#Confusion matrix\ncm = confusion_matrix(y_test,y_pred_acc)\nprint(cm)","d0916af4":"cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                                 index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","35f2e947":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred2))","4f546ddc":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train) \nresult = model.score(X_test, y_test)\n\nprint('Model accuracy score: {0:0.4f}'. format(result))","15189d86":"from sklearn.metrics import make_scorer, accuracy_score\n\nrfc = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(rfc, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrfc = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data\nrfc.fit(X_train, y_train)","bd5667ef":"y_pred4 = grid_clf_acc.predict(X_test)\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred4)))","53f8967c":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","032478bc":"## Random Forest Classifier","86b5076e":"## Label Encoding","64b2fc4f":"## Final Note\n\nIn this kernel, I have analysed the Bike Buyers dataset and performed Binary Classification using various Supervised Learning Classification algorithms. The accuracy is fairly less due the the limitations of the dataset. Let me know how you found this kernel, Happy Kaggling :)","797aded3":"## Binary Classification for Bike Sales\n\nAlgorithms implemented -\n1. Naive Bayes Classifier\n2. Decision Tree Classifier\n3. Random Forest Classifier\n4. XGBoost Classifier","e9793bd7":"## XGBoost Classifier","1ef45d89":"## Train-Test Split","867e83d8":"## Analyzing Numerical Variables","d714f495":"## Decision Tree Classifier","6354faba":"### Improving Accuracy using Grid Search","c3a7cb55":"## Gausian Naive Bayes","e4eb6537":"## Analyzing Categorical Variables","df9bdc7f":"### Visualize Decision Tree","82da036f":"## Categorize Continuous Data"}}