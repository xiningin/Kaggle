{"cell_type":{"f3163872":"code","b156ad54":"code","ad9aa6cb":"code","b2bc8921":"code","3e9c9337":"code","4c1cf6b2":"code","4bd5a7b5":"code","4f840e3e":"code","90d65704":"code","5543a3f2":"code","aa015a58":"code","4317fe45":"code","dc2f00d1":"code","1e629088":"code","942a9c84":"code","d514b59b":"code","473a5b81":"code","cb9e9533":"code","87d297e0":"code","e9f3511c":"code","5f6c850a":"code","7035a150":"code","b45b0fcb":"code","93b045d0":"code","d398db82":"code","7590d2f7":"code","d4474f11":"code","65c25ea9":"code","cf2244b0":"code","68179ac3":"code","7b578618":"code","e34fcd74":"code","617a8715":"code","452c09f9":"code","c4ae7d2e":"code","f165a77b":"code","fbfa3ace":"code","127660bb":"code","4d6efe4e":"code","fce0bd69":"code","144a9381":"code","7d69bed5":"code","14691154":"markdown","a6f24eef":"markdown","e3155a85":"markdown","af69305b":"markdown","4d616156":"markdown","7ef61eda":"markdown","48f8f1b8":"markdown","59b79c0c":"markdown","07edfb69":"markdown","9eec91da":"markdown","4e6d852e":"markdown","26959471":"markdown","62fb51a2":"markdown","9c40e738":"markdown","c187d6a0":"markdown","d2466f91":"markdown","fa48a580":"markdown","f6e57926":"markdown","2d9feb1a":"markdown","ae04db97":"markdown","a71a5cfb":"markdown"},"source":{"f3163872":"# Download the data\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()","b156ad54":"fetch_housing_data()","ad9aa6cb":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","b2bc8921":"# take a quick look at the data and it's stats.\nhousing= load_housing_data()\nhousing.head()","3e9c9337":"# to get quick description of data.\nhousing.info()","4c1cf6b2":"# number of categories that exists in ocean_proximity\nhousing['ocean_proximity'].value_counts()","4bd5a7b5":"# summary of numerical attributes.\nhousing.describe()","4f840e3e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","90d65704":"# Creation of training and test set.\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","5543a3f2":"test_set.head()","aa015a58":"housing['median_income'].hist()\nplt.show()","4317fe45":"import numpy as np\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","dc2f00d1":"housing[\"income_cat\"].value_counts()","1e629088":"housing['income_cat'].hist()\nplt.show()","942a9c84":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","d514b59b":"# lets see if it worked or not\nstrat_test_set['income_cat'].value_counts()\/ len(strat_test_set)","473a5b81":"# Now you should remove the income_cat attribute so the data is back to its original state.\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","cb9e9533":"# let's create copy of the dataset to play with it\nhousing= strat_train_set.copy()","87d297e0":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nplt.show()","e9f3511c":"# it's hard to see any pattern here let's reduce alpha\nhousing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)\nplt.show()","5f6c850a":"# let's make it clearer\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend()\nplt.show()\n# The radius of each circle represents the district\u2019s population (option s), and the color represents the price (option c).\n# We will use a predefined color map (option cmap) called jet, which ranges from blue(low values) to red (high prices).","7035a150":"# let's look for correlations\ncorr_matrix= housing.corr()","b45b0fcb":"#lets see the correlation with median_house_value\ncorr_matrix['median_house_value'].sort_values(ascending=False)","93b045d0":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.show()","d398db82":"# EXPERIMENTING WITH ATTRIBUTE COMBINATIONS\n# the total number of rooms in a district is not very useful if you don\u2019t know how many households there are.\n# What you really want is the number of rooms per household.\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","7590d2f7":"#now lets look at the correlation matrix\ncorr_matrix= housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","d4474f11":"housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","65c25ea9":"housing.describe()","cf2244b0":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","68179ac3":"# DATA Cleaning\n# we will fill the the numerical missing values with their medians.\n# Scikit-Learn provides a handy class to take care of missing values: SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimputer= SimpleImputer(strategy='median')","7b578618":"# DATA Cleaning\n# we will fill the the numerical missing values with their medians.\n# Scikit-Learn provides a handy class to take care of missing values: SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimputer= SimpleImputer(strategy='median')","e34fcd74":"#since median can only be computed on numerical attributes.\nhousing_num= housing.drop('ocean_proximity', axis=1)","617a8715":"imputer.fit(housing_num)","452c09f9":"imputer.statistics_","c4ae7d2e":"#checking if it is same as the median\nhousing_num.median().values","f165a77b":"X= imputer.transform(housing_num)","fbfa3ace":"# HANDLING CATEGORICAL ATTRIBUTES\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","127660bb":"# By default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method \n# or by setting 'sparse' attribute to False\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","4d6efe4e":"#CUSTOM TRANSFORMATIONS\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","fce0bd69":"# TRANSFORMATION PIPELINES\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n# The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler,\n# which is a transformer, so the pipeline has a transform() method that applies all the transforms to the data in sequence \n#(and of course also a fit_transform() method, which is the one we used).","144a9381":"# we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to \n# handle all columns, applying the appropriate transformations to each column.\nfrom sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","7d69bed5":"housing_prepared","14691154":"> we notice a few things in this:\n* First, the median income attribute does not look like it is expressed in US dollars(USD). The data is scaled.The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about 30,000)\n* The housing median age and the median house value were also capped.Your Machine Learning algorithms may learn that prices never go beyond that limit.\n* Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.","a6f24eef":"you should compute the median value on the training set, and\nuse it to fill the missing values in the training set, but also don\u2019t forget to save the\nmedian value that you have computed. You will need it later to replace missing values\nin the test set when you want to evaluate your system, and also once the system goes\nlive to replace missing values in new data.","e3155a85":"Steps in ML project.\n1. Get the data.\n2. Discover and visualize the data to gain insights.\n3. Prepare the data for Machine Learning algorithms.\n4. Select a model and train it.\n5. Fine-tune your model.\n6. Present your solution.","af69305b":"# Welcome!","4d616156":"> whatever you study, you will never learn until you see how it works practically. It's very important to work on real-life example on whatever you are studying.","7ef61eda":"1. total_bedrooms attribute have missing values.\n2. ocen_proximity if a categorical attribute.","48f8f1b8":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that\nthere is a strong positive correlation; for example, the median house value tends to go\nup when the median income goes up. When the coefficient is close to \u20131, it means\nthat there is a strong negative correlation; you can see a small negative correlation\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\ngo down when you go north).","59b79c0c":"**If you have any questions, kindly put it into comments, and please upvote if you find this imformatiove.**","07edfb69":"In this notebook we are using California's housing data to predict housing prices.\nAs you can see it's a regression task.\n\n# Get the Data","9eec91da":"The new bedrooms_per_room attribute is much more correlated with\nthe median house value than the total number of rooms or bedrooms. Apparently\nhouses with a lower bedroom\/room ratio tend to be more expensive. The number of\nrooms per household is also more informative than the total number of rooms in a\ndistrict\u2014obviously the larger the houses, the more expensive they are.","4e6d852e":"> Now, you have seen correlations between different features. But, sometimes what happens is that a attribute may not have corrletion with the target but a combination of two or more attributes could have a impact on the target so now look for such combinations:","26959471":"# Prepare the data for Machine Learning Algorithm","62fb51a2":"credits= hands on machine learning(book)","9c40e738":"let's take a quick look at the data distribution.","c187d6a0":"This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density.\n","d2466f91":"# This was the first part of the notebook. STAY TUNED FOR THE NEXT ONE.","fa48a580":"* In this notebook, i will try to walk you through a real machine learning project.","f6e57926":"# Discover and visualize the data to gain insights","2d9feb1a":"Although Scikit-Learn provides many useful transformers, you will need to write\nyour own.\n\nLet's create a custom transformer to add extra attributes:","ae04db97":"Let\u2019s look at the median income histogram more closely most median income values are clustered around 1.5 to 6 (i.e.15,000\u201360,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum\u2019s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than 15,000), category 2 from\n1.5 to 3, and so on.","a71a5cfb":"# https:\/\/www.kaggle.com\/dhirajnirne\/stratified-sampling\n* visit here to know the importance and use of stratified sampling."}}