{"cell_type":{"1a4b336c":"code","6c35dd49":"code","10c77c33":"code","e003848a":"code","9a5eb7ae":"code","9314505f":"code","bb57effa":"code","427ff64f":"code","8e8caa6a":"code","510e3a4e":"code","1dcdfbbb":"code","4c63af2e":"code","f25c93e9":"code","964e73c0":"code","4f1e46c0":"code","fb299f3b":"code","48d0e171":"code","e849376a":"code","0a18fac8":"code","620ac9d9":"code","72efa21b":"code","be8c4595":"code","36e762c6":"code","b8954823":"code","36595964":"code","11575a39":"code","de03afd7":"code","3fe1b78f":"code","4d93b94b":"code","6770c5b6":"code","c02e0435":"code","e6855fd8":"markdown"},"source":{"1a4b336c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6c35dd49":"import numpy.ma as ma\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn import preprocessing\nimport os #modulos de gestion de directorios\nimport glob #modulo de visualizaci\u00f3n de directorios\n#import xgboost as xgb\ncolor = sns.color_palette()\nimport sys\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999","10c77c33":"#cambiamos al directorio de trabajo donde tenemos los datos\n#os.chdir(\"..\/input\")","e003848a":"os.getcwd() ","9a5eb7ae":"print(glob.glob(\"..\/input\/*.*\"))","9314505f":"train_df = pd.read_csv(\"..\/input\/train.csv\", \\\n                      parse_dates=True, index_col=0)\ntest_df = pd.read_csv(\"..\/input\/test.csv\", \\\n                     parse_dates=True, index_col=0 )\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","bb57effa":"train_df=train_df.reset_index()","427ff64f":"#we create some new fields to easy manipulate\n#forecasting probably should be at item-store because demand pattens could vary much dep. items and store \ntrain_df['weekday']=pd.DatetimeIndex(train_df['date']).weekday\ntrain_df['month']=pd.DatetimeIndex(train_df['date']).month \ntrain_df['year']=pd.DatetimeIndex(train_df['date']).year\ntrain_df['itemstore']=train_df.item.astype(str)+\"-\"+train_df.store.astype(str)","8e8caa6a":"#overview of data\nprint(\"number of different items: %i\" %(len(np.unique(train_df.item))))\nprint(\"number of different stores: %i\" %(len(np.unique(train_df.store))))\nprint(\"number of different dates: %i\" %(len(np.unique(train_df.date))))\nprint(\"maximun date in data: %s\" %(max(train_df.date)))\nprint(\"minimum date in data: %s\" %(min(train_df.date)))\nprint(\"number of different itemstore: %i\" %(len(np.unique(train_df.itemstore))))","510e3a4e":"#create some lists to see range of unique values\nstores = list(set(train_df.store))\nitem = list(set(train_df.item))\nitemstore = list(set(train_df.itemstore))","1dcdfbbb":"#we check anual sales profile comparing stores\nc=train_df.groupby(['year','store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","4c63af2e":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['month', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","f25c93e9":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['weekday', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","964e73c0":"#we evaluate increase in anual sales at itemstore level\nb =train_df.drop(columns=['store', 'item','weekday','date','month'])\nc=b.groupby(['year', 'itemstore']).sum()\nd=c.unstack()\nsales_itemstore_year=d.T\nsales_itemstore_year['delta_2014\/2013']=((sales_itemstore_year[2014]-sales_itemstore_year[2013])\/sales_itemstore_year[2013])*100\nsales_itemstore_year['delta_2015\/2014']=((sales_itemstore_year[2015]-sales_itemstore_year[2014])\/sales_itemstore_year[2014])*100\nsales_itemstore_year['delta_2016\/2015']=((sales_itemstore_year[2016]-sales_itemstore_year[2015])\/sales_itemstore_year[2015])*100\nsales_itemstore_year['delta_2017\/2016']=((sales_itemstore_year[2017]-sales_itemstore_year[2016])\/sales_itemstore_year[2016])*100\nsales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","4f1e46c0":"sales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","fb299f3b":"#heat-maps to compare deltas anual and bet. itemstore each year\nsales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2014\/2013')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2014\/2013\", fontsize=15)\nplt.show()","48d0e171":"sales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2017\/2016')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2017\/2016\", fontsize=15)\nplt.show()","e849376a":"#we pivot, group to weeks\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntrain_df_train=train_df.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_train=train_df_train.resample('W').sum()\ntrain_df_train = train_df_train[:-1]","0a18fac8":"train_df_train_V1 = train_df_train\n","620ac9d9":"# we search ARIMA parameters for item 1 store 1 with 52 weeks differentation for stationary hipotesis\nimport warnings\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom pandas import tseries\n\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return np.array(diff)\n\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(X, arima_order):\n# prepare training dataset\n    X = X.astype('float32')\n    train_size = int(len(X) * 0.7)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n    # difference data\n        weeks_in_year = 52\n        diff = difference(history, weeks_in_year)\n        model = ARIMA(diff, order=arima_order)\n        model_fit = model.fit(trend='nc', disp=0)\n        yhat = model_fit.forecast()[0]\n        yhat = inverse_difference(history, yhat, weeks_in_year)\n        predictions.append(yhat)\n        history.append(test[t])\n        # calculate out of sample error\n    rmse = sqrt(mean_squared_error(test, predictions))\n    return rmse\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))","72efa21b":"#evaluate models\np_values = range(0, 6)\nd_values = range(0, 2)\nq_values = range(0, 6)\nt = '1-1'\n\nwarnings.filterwarnings(\"ignore\")\n\nevaluate_models(train_df_train_V1[t].values, p_values, d_values, q_values)","be8c4595":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n#Procedure to predict values\ndef do_predictions_join_unpack(t, database1, database2, dictionary, database3):\n    X = database1[itemstore[t]].values\n    X = X.astype('float32')\n    weeks_in_year = 52\n    diff = difference(X, weeks_in_year)\n    model = ARIMA(diff, order=(1,0,1))\n    model_fit = model.fit(trend='nc', disp=0)\n    # bias constant, could be calculated from in-sample mean residual\n    bias = 0\n    # save model\n    #model_fit.save('model.pkl')\n    #np.save('model_bias.npy', [bias])    \n    # load and prepare datasets\n    X = database1[itemstore[t]].values.astype('float32')\n    history = [x for x in X]\n    weeks_in_year = 52\n    y = database2[itemstore[t]].values.astype('float32')\n    # load model\n    #model_fit = ARIMAResults.load('model.pkl')\n    #bias = np.load('model_bias.npy')\n    # forecast 13 periods\n    predictions = list()\n    forecast = model_fit.forecast(steps=13)[0]\n    for yhat in forecast:\n        yhat = bias + inverse_difference(history, yhat, weeks_in_year)\n        history.append(yhat)\n        predictions.append(yhat)\n    #turn to daily with weekly pattern and copy in summary\n    database2 = database2.reset_index()\n    predictions = pd.DataFrame(predictions)\n    train_df_test_V1_pred = pd.concat([database2['date'], database2[itemstore[t]], predictions], axis=1)\n    train_df_test_V1_pred['date'] = pd.to_datetime(train_df_test_V1_pred['date'])\n    train_df_test_V1_pred=train_df_test_V1_pred.set_index('date')\n    new_dates = pd.date_range('2018-01-01', '2018-04-01', name='date')\n    train_df_test_V1_pred_daily = train_df_test_V1_pred.reindex(new_dates, method='ffill')\n    for k in range (13):\n        for j in range (7):\n            train_df_test_V1_pred_daily[0][(k*7)+j] = round(train_df_test_V1_pred_daily[0][(k*7)+j]*dictionary[itemstore[t]][j])\n    database3[[itemstore[t]]] = train_df_test_V1_pred_daily[0]\n    return database3, train_df_test_V1_pred_daily, predictions, train_df_test_V1_pred","36e762c6":"train_df = train_df.set_index('date')","b8954823":"#we asign in a dictionary for each item-store the de-composition of sales for SUN-MON-TUE.......-SAT-SUMA. we use 2017 weekly pattern\ndictionary_week_sales_itemstore={}\ndictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    dictionary_week_sales_itemstore.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n    dictionary_week_sales_itemstore_reparto.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n\n#Now we group sales at item-store level and week-day    \n#train_df=train_df.set_index('date')\ntrain_sales_weekday=train_df['01-01-2013':'31-12-2017'].groupby(['weekday', 'itemstore']).sum()","36595964":"#def update_dictionary_week_sales_itemstore(itemstore, train_sales_weekday)\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore[itemstore[i]][j]= train_sales_weekday.loc[(j, itemstore[i]),['sales']][0]\n    dictionary_week_sales_itemstore[itemstore[i]][7]= sum(dictionary_week_sales_itemstore[itemstore[i]][0:7])   \n    \n#Now we update second dictionary dictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore_reparto[itemstore[i]][j]= (dictionary_week_sales_itemstore[itemstore[i]][j]\/   \\\n            dictionary_week_sales_itemstore[itemstore[i]][7])","11575a39":"#we prepare dataframe for integrate all results\ntest_df['itemstore']=test_df.item.astype(str)+\"-\"+test_df.store.astype(str)\ntest_df['sales'] = 0\ntrain_df_test_V2  = test_df.drop(columns=['store', 'item'])\ntrain_df_test_V2['date'] = pd.to_datetime(train_df_test_V2['date'])\ntrain_df_test_V2 = train_df_test_V2.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_test_V1 = train_df_test_V2.resample('W').sum()","de03afd7":"#calculation of all itemstore predictions\npredictions = list()\nfor t in range (len(itemstore)):\n        do_predictions_join_unpack(t, train_df_train_V1, train_df_test_V1, dictionary_week_sales_itemstore_reparto, train_df_test_V2)\n    ","3fe1b78f":"#copy same pattern for first week\ntrain_df_test_V2.ix['2018-01-01']=train_df_test_V2.ix['2018-01-08']\ntrain_df_test_V2.ix['2018-01-02']=train_df_test_V2.ix['2018-01-09']\ntrain_df_test_V2.ix['2018-01-03']=train_df_test_V2.ix['2018-01-10']\ntrain_df_test_V2.ix['2018-01-04']=train_df_test_V2.ix['2018-01-11']\ntrain_df_test_V2.ix['2018-01-05']=train_df_test_V2.ix['2018-01-12']\ntrain_df_test_V2.ix['2018-01-06']=train_df_test_V2.ix['2018-01-13']\n","4d93b94b":"for i in range (len(test_df)):\n    test_df['sales'][(i)] = train_df_test_V2.loc[test_df['date'][(i)], test_df['itemstore'][(i)]]","6770c5b6":"submission = test_df.drop(columns=['date', 'store', 'item', 'itemstore'])\nsubmission_1= submission.reset_index()","c02e0435":"submission_1.to_csv('submissionFCTl.csv', index=False)","e6855fd8":"Thanks !"}}