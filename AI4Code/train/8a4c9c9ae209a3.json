{"cell_type":{"773fe6fe":"code","f2c32d58":"code","b3478288":"code","38cd8b15":"code","15fcdd73":"code","cd2cb609":"code","a10581e7":"code","03d1caec":"code","c2bc3b0d":"code","8f09f131":"code","51fe17b3":"code","1a29baa4":"code","88cc9d09":"code","d24451e2":"code","fda20095":"code","81cb737a":"code","fd2acd78":"code","e53b9c19":"code","d343f77b":"code","ebad514b":"code","bc6a6e5a":"code","18c0e01e":"code","63372d55":"code","db3a2a96":"code","def3d970":"code","cb52fe0d":"code","460ab1d0":"code","908548be":"code","fc90e73e":"code","64bcdecd":"code","1a9b604b":"code","7c79997f":"code","6334c79d":"code","f6d16317":"code","228ae439":"code","deddfe2d":"code","7a2b5d1e":"code","2ab93023":"code","102b4e5c":"markdown","3e882e02":"markdown","1140febe":"markdown","76388036":"markdown","bc57d2de":"markdown","f5ad9b1d":"markdown","ff32eb64":"markdown","1a9523a1":"markdown","08f1099b":"markdown","5d8f2343":"markdown","e62d448e":"markdown","289d7e30":"markdown","559cf7b8":"markdown","4f493bc5":"markdown","1997aa48":"markdown","c759ce86":"markdown","7190b03d":"markdown","237c2d55":"markdown","1be7a546":"markdown","6a13141d":"markdown","200f9f8b":"markdown","521059f1":"markdown","5bf19a03":"markdown","0e8e5f4b":"markdown","3c8607a0":"markdown"},"source":{"773fe6fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2c32d58":"# Import useful libraries\n\nimport time\nimport re\nimport string\nfrom numpy import mean\nfrom numpy import set_printoptions\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-darkgrid')\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom catboost import CatBoostClassifier\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')","b3478288":"# Read dataset\n\ntrain_data = pd.read_csv('\/kaggle\/input\/hackerearth-ml-challenge-adopt-a-buddy\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/hackerearth-ml-challenge-adopt-a-buddy\/test.csv')\ntrain_data.columns = train_data.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ntest_data.columns = test_data.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')","38cd8b15":"print('Train Data Shape: ', train_data.shape)\nprint('Test Data Shape: ', test_data.shape)\ntrain_data.head()","15fcdd73":"train_data.isnull().sum()","cd2cb609":"# See the distribution of outcome 1: breed_category\n\nsns.countplot(x = 'breed_category',data = train_data)\nsns.despine()","a10581e7":"print(train_data.breed_category.value_counts())\nprint(train_data.pet_category.value_counts())","03d1caec":"# See the distribution of outcome 2: pet_category\n\nsns.countplot(x = 'pet_category',data = train_data)\nsns.despine()","c2bc3b0d":"train_data['type'] = 'train'\ntest_data['type'] = 'test'\n\nmaster_data = pd.concat([train_data, test_data])\nmaster_data['issue_date'] = pd.to_datetime(master_data['issue_date'], dayfirst = True)\nmaster_data['listing_date'] = pd.to_datetime(master_data['listing_date'].apply(lambda x: x.split(' ')[0]), dayfirst = True)","8f09f131":"# Relation between length and breed category\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(x = 'breed_category',y = 'lengthm',data = master_data)\nplt.show()","51fe17b3":"# Relation between length and pet category\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(x = 'pet_category',y = 'lengthm',data = master_data)\nplt.show()","1a29baa4":"# See the distribution of outcome 2: pet_category\n\nplt.figure(figsize = (8, 6))\nsns.countplot(x = 'condition',data = master_data)\nsns.despine()","88cc9d09":"# See the distribution of outcome 1: breed_category\n\nplt.figure(figsize = (22, 5))\nsns.countplot(x = 'color_type',data = master_data)\nplt.xticks(rotation = 90)\nplt.show()","d24451e2":"plt.figure(figsize = (8, 6))\nsns.scatterplot(x = master_data['lengthm'], y = master_data['heightcm']\/100)\nplt.show()","fda20095":"plt.figure(figsize = (8, 5))\nsns.distplot(master_data['lengthm'])\nplt.show()","81cb737a":"plt.figure(figsize = (8, 5))\ndf = master_data[['lengthm','heightcm']]\ndf['lengthcm'] = df['lengthm']*100\ndf[['lengthcm','heightcm']].boxplot()\nplt.show()","fd2acd78":"# Correlation matrix\n\nplt.figure(figsize = (11, 10))\n#plt.subplots(figsize=(10,8))\nsns.heatmap(master_data.corr(), annot = True)","e53b9c19":"master_data['days_to_reach'] = master_data['listing_date'] - master_data['issue_date']\nmaster_data['days_to_reach'] = master_data['days_to_reach'].apply(lambda x: int(str(x).split(' ')[0]))\n\nmaster_data['age'] = master_data['days_to_reach'] \/ 365\nmaster_data['age'] = master_data['age'].abs()\n\n# Mapping for condition of pets\n\ncondition = {0.0: 'A', 1.0: 'B', 2.0: 'C'}\nmaster_data['condition'] = master_data['condition'].map(condition)\nmaster_data['condition'] = master_data['condition'].astype(str)\n\n# Convert height to cms\n\nmaster_data['heightm'] = master_data['heightcm'] \/ 100\nmaster_data = master_data.drop(['heightcm'], axis = 1)\n\n#length_mean = master_data['lengthm'].mean()\n\nmaster_data.loc[(master_data['lengthm'] == 0), 'lengthm'] = 0.005\n\n#master_data['len_to_height'] = master_data['lengthm']\/master_data['heightm']\n\nmaster_data['color_type'] = master_data['color_type'].apply(lambda x: x.lower())\n\n\n\nmaster_data.head()","d343f77b":"# 2 records exist where the listing date is less than the issue date, convert them to positive\n\nmaster_data.loc[(master_data['days_to_reach'] <= 0), 'days_to_reach'] = master_data.loc[(master_data['days_to_reach'] < 0), 'days_to_reach'] * -1","ebad514b":"# Generate master color feature from the available color_type\n\nmaster_data['master_color'] = master_data['color_type'].apply(lambda x: x.split(' ')[0])\nmaster_data['species'] = master_data['color_type'].apply(lambda x: x.split(' ')[1] if len(x.split(' ')) == 2 else x.split(' ')[0])","bc6a6e5a":"# Generate time features - e.g. Quarter\n\nmaster_data['issue_qtr'] = master_data['issue_date'].dt.quarter\nmaster_data['list_qtr'] = master_data['listing_date'].dt.quarter\n\nmaster_data['issue_yr'] = master_data['issue_date'].dt.year\nmaster_data['list_yr'] = master_data['listing_date'].dt.year\n\nmaster_data['issue_mth'] = master_data['issue_date'].dt.month\nmaster_data['list_mth'] = master_data['listing_date'].dt.month\n\nmaster_data['issue_weekend'] = master_data['issue_date'].apply(lambda x: 1 if x.dayofweek in [5, 6] else 0)\nmaster_data['list_weekend'] = master_data['listing_date'].apply(lambda x: 1 if x.dayofweek in [5, 6] else 0)\n\nmaster_data.head()","18c0e01e":"# Get numerical columns\n\ncat_cols = ['condition', 'color_type', 'master_color', 'species']\nnumerical_cols = master_data.columns[~master_data.columns.isin(cat_cols + ['pet_id', 'issue_date', 'listing_date', 'type', 'breed_category', 'pet_category'])].tolist()\nnumerical_cols","63372d55":"ss = StandardScaler()\nmaster_data[numerical_cols] = ss.fit_transform(master_data[numerical_cols])","db3a2a96":"#le = LabelEncoder()\n#for col in cat_cols:\n#    master_data[col] = le.fit_transform(master_data[col])\n    \n#master_data[numerical_cols + cat_cols] = ss.fit_transform(master_data[numerical_cols + cat_cols])","def3d970":"train_data.columns","cb52fe0d":"# Separate train and test data\n\ntrain_data = master_data.loc[master_data['type'] == 'train']\ntest_data = master_data.loc[master_data['type'] == 'test']\n\ntrain_data['breed_category'] =train_data['breed_category'].astype(str)\ntrain_data['pet_category'] =train_data['pet_category'].astype(str)\n\ntestIDs = test_data['pet_id']\n\ntrain_data = train_data.drop(['pet_id', 'issue_date', 'listing_date', 'type'], axis = 1)\n\nfor col in ['breed_category', 'pet_category']:\n    train_data[col] = train_data[col].apply(lambda x: np.float16(x))\n    train_data[col] = train_data[col].apply(lambda x: np.int8(x))\n\ntestData = test_data.drop(['issue_date', 'listing_date', 'type', 'x2', 'breed_category', 'pet_category'], axis = 1)\ntest_data = test_data.drop(['pet_id', 'issue_date', 'listing_date', 'type', 'x2', 'breed_category', 'pet_category'], axis = 1)\ntrain_data = train_data[['condition', 'color_type', 'lengthm', 'x1', 'days_to_reach', 'age', 'heightm', 'master_color', 'species', 'issue_qtr', 'list_qtr',\n                         'breed_category', 'issue_yr', 'list_yr', 'issue_mth', 'list_mth', 'issue_weekend', 'list_weekend', 'pet_category']]\ntrain_data.head()","460ab1d0":"test_data.head()","908548be":"train_data_1 = train_data.copy()\n\"\"\"\nX1 = train_data_1.drop(['breed_category', 'pet_category'],axis = 1).values\ny1 = train_data_1['pet_category'].values\n\nfor num_feats in range(1, 9):\n    print('Using {} features:'.format(num_feats))\n    test = SelectKBest(score_func = f_classif, k = num_feats)\n    fit = test.fit(X1, y1)\n    # summarize scores\n    set_printoptions(precision = 0)\n    for i in fit.scores_:\n        print(i)\n    print(fit.scores_)\n    features = fit.transform(X1)\n    # summarize selected features\n    print(features[0:num_feats + 1, :])\n\"\"\"","fc90e73e":"\"\"\"\nX2 = train_data_1.drop(['breed_category'],axis = 1).values\ny2 = train_data_1['breed_category'].values\n\nfor num_feats in range(1, 10):\n    print('Using {} features:'.format(num_feats))\n    test = SelectKBest(score_func = f_classif, k = num_feats)\n    fit = test.fit(X2, y2)\n    # summarize scores\n    set_printoptions(precision = 0)\n    for i in fit.scores_:\n        print(i)\n    print(fit.scores_)\n    features = fit.transform(X2)\n    # summarize selected features\n    print(features[0:num_feats + 1, :])\n\"\"\"","64bcdecd":"train_data_1 = train_data.drop(['lengthm', 'heightm'], axis = 1)\n\ntest_data_1 = test_data.copy()\ntest_data_1 = test_data.drop(['lengthm', 'heightm'], axis = 1)","1a9b604b":"X = train_data_1.drop(['breed_category', 'pet_category'],axis = 1).values\ny_1 = train_data_1['pet_category'].values\n\n_cat_indices_ = [0, 1, 5, 6]\n#_cat_indices_ = [0, 4, 5]\n\n\ntype_of_target(y_1)","7c79997f":"# Catboost for pet_category\n\nkfold, scores = KFold(n_splits = 6, shuffle = True, random_state = 22), list()\nfor train, test in kfold.split(X):\n    X_train, X_test = X[train], X[test]\n    y_train, y_test = y_1[train], y_1[test]\n\n    model = CatBoostClassifier(random_state = 22, max_depth = 6, n_estimators = 1000, verbose = 1000, l2_leaf_reg = 1)\n    model.fit(X_train, y_train, cat_features = _cat_indices_)\n    preds = model.predict(X_test)\n    score = f1_score(y_test, preds, average = 'weighted')\n    scores.append(score)\n    print('Validation f1_score:', score)\nprint(\"Average Validation f1_score: \", sum(scores)\/len(scores))","6334c79d":"y_Preds_1 = model.predict(test_data_1.values)","f6d16317":"pet_cat = pd.DataFrame(data = {'pet_id': testIDs, 'pet_category': y_Preds_1.ravel()})\npet_cat.head()","228ae439":"test_data_1 = testData.merge(pet_cat, on = 'pet_id', how = 'left')\ntest_data_1 = test_data_1.drop(['pet_id', 'heightm', 'lengthm'], axis = 1)","deddfe2d":"X = train_data_1.drop(['breed_category', 'issue_yr', 'list_yr', 'issue_mth', 'list_mth', 'issue_weekend', 'list_weekend'],axis = 1).values\ny_2 = train_data_1['breed_category'].values\n\n_cat_indices_ = [0, 1, 5, 6, 9]\n#_cat_indices_ = [0, 4, 5, 8]\n\ntype_of_target(y_2)","7a2b5d1e":"# Catboost for breed_category\n\nkfold, scores = KFold(n_splits = 6, shuffle = True, random_state = 22), list()\nfor train, test in kfold.split(X):\n    X_train, X_test = X[train], X[test]\n    y_train, y_test = y_2[train], y_2[test]\n\n    model_1 = CatBoostClassifier(random_state = 22, max_depth = 8, n_estimators = 300, verbose = 1000, l2_leaf_reg = 3.5)\n    model_1.fit(X_train, y_train, cat_features = _cat_indices_)\n    preds = model_1.predict(X_test)\n    score = f1_score(y_test, preds, average = 'weighted')\n    scores.append(score)\n    print('Validation f1_score:', score)\nprint(\"Average Validation f1_score: \", sum(scores)\/len(scores))","2ab93023":"y_Preds_2 = model_1.predict(test_data_1.drop(['issue_yr', 'list_yr', 'issue_mth', 'list_mth', 'issue_weekend', 'list_weekend'], axis = 1))\n\nsubmission = pd.DataFrame(data = {'pet_id': testIDs, 'breed_category': y_Preds_2.ravel(), 'pet_category': y_Preds_1.ravel()})\nsubmission.to_csv('HE_adopt_a_buddy_final_v1.csv', index = False)\nsubmission.head()","102b4e5c":"We can treat the missing class - nan as a separate category itself","3e882e02":"## Part 2: Breed Category","1140febe":"### Part 2: Predict the breed_category target class\n\nPredict the target variable **breed_category** which is dependent of the **pet_category** class\n\n--> Model: CatBoost\n--> KFold splits: 6","76388036":"## Part 1: Pet Category","bc57d2de":"## Insights\n\n* **Drop the pet dimension columns and x2 if required as they are weakly correlated and don't have enough predicting power for the 2 target variables**","f5ad9b1d":"**More than half of the features are among miority classes, we can extract first word to create new color feature to reduce the classes here**","ff32eb64":"### Feature Engineering - New features","1a9523a1":"### Separating the data back to train and test","08f1099b":"### Part 1: Predict the pet_category target class\n\nPredict the target variable **pet_category** which is independent of the **breed_category** class\n\n--> Model: CatBoost\n--> KFold splits: 6","5d8f2343":"## Standard Scaling the data","e62d448e":"![](https:\/\/media-fastly.hackerearth.com\/media\/hackathon\/hackerearth-machine-learning-challenge-pet-adoption\/images\/b96edbc6d2-PetAdoption_FBImage.jpg)","289d7e30":"# Exploratory Data Analysis","559cf7b8":"### L2 regularization improved the overall model's predictions for both the pet and breed category","4f493bc5":"### Analysing other features in combined train and test data","1997aa48":"[**HackerEarth: Machine Learning Challenge - Adopt a buddy**](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-pet-adoption\/)\n\n**Problem Statement**\n\nA leading pet adoption agency is planning to create a virtual tour experience for their customers showcasing all animals that are available in \ntheir shelter. To enable this tour experience, you are required to build a Machine Learning model that determines type and breed of the \nanimal based on its physical attributes and other factors.\n\nData Description:\n\nThe data folder consists of 2 CSV files\n   \u2022 **train.csv** - 18834 x 11\n   \u2022 **test.csv** - 8072 x 9\n","c759ce86":"## Selecting KBest features using Anova F test\n\n* Uncomment below 2 cells only for kBest features","7190b03d":"**It appears that pet category 0 and 4 are among minority classes. We can use Smote from imblearn to oversample these classes**","237c2d55":"#### It appears we can drop length and height features as they carry lowest Anova F score","1be7a546":"1. **Both the features length and height can be dropped which led to improvement of score by 1**\n2. **Note: If using dimension features - convert to same unit (m\/cm)**","6a13141d":"## Getting Predictions","200f9f8b":"# Model Building","521059f1":"**It appears that breed category 2 is a minority class. We can use Smote from imblearn to oversample this class**","5bf19a03":"## CatBoost Model","0e8e5f4b":"## Label Encoding only done to find out kBest parameters using ANOVA score\n\n* **Uncomment and run below cell only to find out kBest columns & then run entire script without label encoding the feature columns**","3c8607a0":"# F1Score of 91.4 using CatBoostClassifier"}}