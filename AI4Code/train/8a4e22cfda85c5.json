{"cell_type":{"7a40201c":"code","e0d48b03":"code","bc4e0373":"code","669aa56c":"code","68caad7e":"code","d371f314":"code","f4dd1e73":"code","8cc6701d":"code","91d842c9":"code","f70dcf2e":"code","ae3dbf77":"code","1c0aed01":"code","4fdf5157":"code","8d38e7c0":"code","1540bb5d":"code","65a2cf6d":"markdown","d4d59ad9":"markdown","469e94cf":"markdown","7b0235ef":"markdown","29137e3b":"markdown","60477303":"markdown","cd639879":"markdown","9d9663a7":"markdown","a4b5e5b8":"markdown","c69f2053":"markdown","71db99ec":"markdown"},"source":{"7a40201c":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nsns.set_style('white')\n\nfiles = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'train' in filename:\n            files['train'] = os.path.join(dirname, filename)\n        if 'valid' in filename:\n            files['valid'] = os.path.join(dirname, filename)\n        if 'test' in filename:\n            files['test'] = os.path.join(dirname, filename)","e0d48b03":"class2id = {\n    'None': 0,\n    'D0': 1,\n    'D1': 2,\n    'D2': 3,\n    'D3': 4,\n    'D4': 5,\n}\nid2class = {v: k for k, v in class2id.items()}","bc4e0373":"valid_dict = json.load(open(files['valid'], 'r'))\n_, first = next(iter(valid_dict['root'].items()))\nattributes = sorted(first['values'].keys())\n\nid2attr = {i: k for i, k in enumerate(attributes)}\nattr2id = {v: k for k, v in id2attr.items()}\n\ndel valid_dict\nattr2id","669aa56c":"# load one of 'train', 'valid' or 'test'\ndef loadXY(dataset, shuffle=True, random_state=None):\n    data_dict = json.load(open(files[dataset], 'r'))\n    keys = sorted(list(data_dict['root'].keys()))\n    \n    if shuffle:\n        if random_state is not None:\n            np.random.seed(random_state)\n        np.random.shuffle(keys)\n        \n    # float16 should be enough and saves some memory\n    X = np.zeros([len(keys), 90, 18], dtype=np.float16)\n    y = np.zeros([len(keys)], dtype=np.float16)\n    # track how many samples are skipped\n    skip_count = 0\n    for i, key in tqdm(enumerate(keys), total=len(keys), desc=f'loading {dataset} dataset'):\n        sample = data_dict['root'][key]\n        input_arr = np.zeros([90, 18])\n        try:\n            for a, j in attr2id.items():\n                input_arr[:,j] = sample['values'][a]\n            X[i-skip_count] = input_arr\n            y[i-skip_count] = float(class2id[sample['class']])\n        except:\n            skip_count += 1\n    print(f'[{dataset}]: skipped {skip_count} samples ({round(skip_count\/len(keys)*100, 3)}%), loaded {len(keys)-skip_count} samples')\n    del data_dict\n    return X, y","68caad7e":"X_train, y_train = loadXY('train', random_state=42)\nX_valid, y_valid = loadXY('valid', random_state=42)","d371f314":"batch_size_factor = 3\nbatch_size = 256 * batch_size_factor","f4dd1e73":"from sklearn.preprocessing import RobustScaler\n\nscaler_dict = {}\n\nfor attr_id in id2attr.keys():\n    scaler_dict[attr_id] = RobustScaler().fit(\n        X_train[:,:,attr_id].reshape(-1, 1)\n    )\n    X_train[:,:,attr_id] = scaler_dict[attr_id].transform(X_train[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)","8cc6701d":"for attr_id in id2attr.keys():\n    X_valid[:,:,attr_id] = scaler_dict[attr_id].transform(X_valid[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)","91d842c9":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\ntrain_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)","f70dcf2e":"valid_data = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\nvalid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)","ae3dbf77":"# hyper parameters\nlr = 7e-5 * batch_size_factor\noutput_size = 1\nhidden_dim = 512\ndropout = 0.1\nn_layers = 4\nepochs = 10\nclip = 5","1c0aed01":"import torch\nfrom torch import nn\n\nclass DroughtNetLSTM(nn.Module):\n    def __init__(self, output_size, num_input_features, hidden_dim, n_layers, drop_prob=0.2):\n        super(DroughtNetLSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(num_input_features, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.cuda().to(dtype=torch.float32)\n        lstm_out, hidden = self.lstm(x, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n        )\n        return hidden","4fdf5157":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print('using GPU')\nelse:\n    device = torch.device(\"cpu\")\n    print('using CPU')\n\n\nmodel = DroughtNetLSTM(output_size, len(id2attr), hidden_dim, n_layers, dropout)\nmodel.to(device)","8d38e7c0":"loss_function = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)","1540bb5d":"from sklearn.metrics import f1_score\n\ncounter = 0\nvalid_loss_min = np.Inf\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nmodel.train()\n\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for k, (inputs, labels) in tqdm(enumerate(train_loader), desc=f'epoch {i+1}\/{epochs}', total=len(train_loader)):\n        counter += 1\n        h = tuple([e.data for e in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = loss_function(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        scheduler.step()\n        \n        if k == len(train_loader) - 1 or k == (len(train_loader) - 1) \/\/ 2:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            labels = []\n            preds = []\n            for inp, lab in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = loss_function(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                for l in lab:\n                    labels.append(int(l))\n                for p in out.round():\n                    if p > 5:\n                        p = 5\n                    if p < 0:\n                        p = 0\n                    preds.append(int(p))\n            \n            # log data\n            log_dict = {\n                'loss': float(loss),\n                'epoch': counter\/len(train_loader),\n                'step': counter,\n                'lr': scheduler.get_last_lr()[0]\n            }\n            log_dict['validation_loss'] = np.mean(val_losses)\n            log_dict[f'macro_f1'] = f1_score(labels, preds, average='macro')\n            log_dict[f'micro_f1'] = f1_score(labels, preds, average='micro')\n            for j, f1 in enumerate(f1_score(labels, preds, average=None)):\n                log_dict[f'{id2class[j]}_f1'] = f1\n            print(log_dict)\n            \n            model.train()\n            \n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), '.\/state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","65a2cf6d":"## LSTM\nLet's train a simple LSTM on the data, treating this as a regression problem.","d4d59ad9":"We load the json files for training, validation and testing into the ``files`` dictionary.","469e94cf":"### Preprocessing and Loading","7b0235ef":"Best Macro F1 - **0.304**","29137e3b":"We now load the datasets, this will take a few minutes and use ~8GB of RAM.","60477303":"Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted.","cd639879":"## Loading the Data\nIn this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes.","9d9663a7":"The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\nThis could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes.","a4b5e5b8":"Let's also create a dictionary for the meteorological attributes.","c69f2053":"### Model","71db99ec":"> ## US Drought & Meteorological Data Starter Notebook\nThis notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors."}}