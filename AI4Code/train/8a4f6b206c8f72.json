{"cell_type":{"f0279568":"code","28883d6f":"code","9e827c7d":"code","d388cd57":"code","33e6c607":"code","ed46af17":"code","fab6a639":"code","45c6adc8":"code","6a32be06":"code","2c7a94d9":"code","7c945668":"code","30378fdf":"code","859315d2":"code","a85aa2ff":"code","5704ef23":"code","0486db92":"code","83a66966":"code","63a61099":"code","a0c4a8c3":"code","cb7ae6c7":"code","22234359":"code","33930883":"code","a35aa7b5":"code","e1c2fd36":"code","5dce0f1f":"code","0a0d1b2e":"code","7f476ef5":"code","bb665a62":"code","549245ed":"code","a3bbb119":"code","d8c03fdb":"code","e987dcac":"code","9d7fa66c":"code","1f7bc26d":"code","d0b63801":"code","31a3a54c":"code","80de895e":"code","61a69cd9":"code","79e18b15":"code","4eb70d3d":"code","de3f1298":"code","107606ef":"code","31dd1a8d":"code","a7746673":"code","0b7f4842":"code","c6bebcb7":"code","328ff430":"code","030c25f0":"code","a2d0a120":"code","1d390668":"code","19f79f43":"markdown"},"source":{"f0279568":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28883d6f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","9e827c7d":"df=pd.read_csv('\/kaggle\/input\/training-data\/Training Data.csv')","d388cd57":"df.drop('Id',axis=1,inplace=True)\ndf.drop('CITY',axis=1,inplace=True)\ndf.head()","33e6c607":"df.isnull().sum()","ed46af17":"df.dtypes","fab6a639":"df.shape","45c6adc8":"df['Profession'].value_counts()","6a32be06":"categorical_features=[feature for feature in df.columns if df[feature].dtype=='O']\ndf[categorical_features].head()","2c7a94d9":"df['STATE'].replace('Uttar_Pradesh[5]','Uttar_Pradesh',inplace=True)","7c945668":"df['STATE'].replace('Uttar_Pradesh','UP',inplace=True)\ndf['STATE'].replace('Madhya_Pradesh','MP',inplace=True)\ndf['STATE'].replace('West_Bengal','WBengal',inplace=True)\ndf['STATE'].replace('Jammu_and_Kashmir','J&K',inplace=True)\ndf['STATE'].replace('Himachal_Pradesh','HP',inplace=True)\ndf['STATE'].replace('Andhra_Pradesh','AP',inplace=True)","30378fdf":"df['STATE'].unique()","859315d2":"plt.figure(figsize=(10,7))\nsns.countplot(df['House_Ownership'])","a85aa2ff":"plt.figure(figsize=(10,7))\nsns.countplot(df['Car_Ownership'])","5704ef23":"plt.figure(figsize=(10,7))\nsns.countplot(df['Married\/Single'])","0486db92":"plt.figure(figsize=(25,7))\nsns.countplot(df['STATE'])","83a66966":"plt.figure(figsize=(10,7))\nsns.countplot(df['Risk_Flag'])\n## so we need to resample the data for value=0 since the difference is huge.","63a61099":"\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor feature in categorical_features:\n    df[feature]=le.fit_transform(df[feature])\n    \ndf.head()","a0c4a8c3":"x=df.drop('Risk_Flag',axis=1)\ny=df['Risk_Flag']\nimport statsmodels.api as sm\nx=sm.add_constant(x)\nmodel=sm.OLS(y,x).fit()\nmodel.summary()\n# since the p values are less than 0.05 we can assume no multicollinearity, we can confirm that by corr data too.","cb7ae6c7":"x.corr()","22234359":"from sklearn.model_selection import train_test_split\nxt,xtest,yt,ytest=train_test_split(x,y,test_size=0.33,random_state=0)","33930883":"df_u=pd.concat([xt,yt],axis=1)","a35aa7b5":"from sklearn.utils import resample\nval_0=df_u[df_u.Risk_Flag==0]\nval_1=df_u[df_u.Risk_Flag==1]\ndownsample_0=resample(val_0,replace=True,n_samples=len(val_1),random_state=27)\nds=pd.concat([downsample_0,val_1])\nds.Risk_Flag.value_counts()","e1c2fd36":"xtrain=ds.drop('Risk_Flag',axis=1)\nytrain=ds['Risk_Flag']","5dce0f1f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\nxtrain = scaler.fit_transform(xtrain)\nxtest = scaler.transform(xtest)","0a0d1b2e":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nrf.fit(xtrain,ytrain)\nypred=rf.predict(xtest)","7f476ef5":"from sklearn.model_selection import cross_val_score\ncv=cross_val_score(estimator=rf,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))","bb665a62":"from sklearn.metrics import accuracy_score\nacc=accuracy_score(ytest,ypred)\nacc","549245ed":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(ytest,ypred)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for RandomForest')\ndf_cm=pd.DataFrame(cm,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","a3bbb119":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(leaf_size= 2, n_neighbors= 2, p= 1)\nknn.fit(xtrain,ytrain)\nypred1=knn.predict(xtest)","d8c03fdb":"from sklearn.model_selection import cross_val_score\ncv=cross_val_score(estimator=knn,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))","e987dcac":"from sklearn.metrics import accuracy_score\nacc1=accuracy_score(ytest,ypred1)\nacc1","9d7fa66c":"from sklearn.metrics import confusion_matrix\ncm1=confusion_matrix(ytest,ypred1)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for KNeighbors')\ndf_cm=pd.DataFrame(cm1,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","1f7bc26d":"from sklearn.naive_bayes import GaussianNB\ngb=GaussianNB()\ngb.fit(xtrain,ytrain)\nypred2=gb.predict(xtest)","d0b63801":"from sklearn.model_selection import cross_val_score\ncv=cross_val_score(estimator=gb,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))","31a3a54c":"from sklearn.metrics import accuracy_score\nacc2=accuracy_score(ytest,ypred2)\nacc2","80de895e":"cm2=confusion_matrix(ytest,ypred2)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for Naive_Bayes')\ndf_cm=pd.DataFrame(cm2,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","61a69cd9":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier(criterion='entropy',random_state=0)\ndt.fit(xtrain,ytrain)\nypred3=dt.predict(xtest)","79e18b15":"cv=cross_val_score(estimator=dt,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))","4eb70d3d":"acc3=accuracy_score(ytest,ypred3)\nacc3","de3f1298":"cm3=confusion_matrix(ytest,ypred3)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for DecisionTree')\ndf_cm=pd.DataFrame(cm3,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","107606ef":"from sklearn.linear_model import LogisticRegression\nlrc=LogisticRegression(random_state=0)\nlrc.fit(xtrain,ytrain)\nypred4=lrc.predict(xtest)","31dd1a8d":"cv=cross_val_score(estimator=lrc,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))","a7746673":"acc4=accuracy_score(ytest,ypred4)\nacc4","0b7f4842":"cm4=confusion_matrix(ytest,ypred4)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for Logistic')\ndf_cm=pd.DataFrame(cm4,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","c6bebcb7":"from xgboost import XGBClassifier\nxg=XGBClassifier(learning_rate=0.1, \n                    n_estimators=1000, \n                    use_label_encoder=False,\n                    random_state=42)\nxg.fit(xtrain,ytrain)\nypred5=xg.predict(xtest)","328ff430":"cv=cross_val_score(estimator=xg,X=xtrain,y=ytrain,cv=10)\nprint(abs(cv.mean()))\n","030c25f0":"acc5=accuracy_score(ytest,ypred5)\nacc5","a2d0a120":"cm5=confusion_matrix(ytest,ypred5)\nplt.figure(figsize=(10,7))\nplt.title('Confusion matrix for XGBoost')\ndf_cm=pd.DataFrame(cm5,index=[i for i in [0,1]],columns=[i for i in ['predicted 0','predicted 1']])\nsns.heatmap(df_cm,annot=True,fmt='g');","1d390668":"algo=['RandomForest','Kneighbors','Naive_Bayes','DecisionTree','Logistic','XGBoost']\naccuracy=[acc*100,acc1*100,acc2*100,acc3*100,acc4*100,acc5*100]\nresults=pd.DataFrame({'algorithm':algo,'accuracy(%)':accuracy})\nresults=results[['algorithm','accuracy(%)']]\nresults","19f79f43":"so, the RandomForest & XGBoost classifiers have performed well on the given dataset."}}