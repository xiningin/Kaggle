{"cell_type":{"401deacb":"code","147b01bb":"code","c17d4c34":"code","385ce574":"code","60dcbc5e":"code","0b55cad4":"code","8a0e1a95":"code","100959d8":"code","3c262856":"code","e568e1b0":"code","15742f00":"code","73c46129":"code","3e519932":"code","e0d42cab":"code","20a27bfc":"code","b139e6ee":"code","e9eee296":"code","5e1fa558":"code","9d4b285b":"code","919607c7":"code","af088777":"code","3cab4251":"code","e0521925":"code","fb107e63":"code","968f0d63":"code","b28e1d9e":"code","f4b110de":"code","ba311bcc":"code","4f3d2283":"code","6ec9f297":"code","5cd217cf":"code","743c0d38":"code","f4ee44b9":"code","6229dd96":"code","d205452e":"code","e5c5ce7e":"code","27bbf0c4":"markdown","95e5967f":"markdown","fbd427b6":"markdown","42a11b6c":"markdown","f226cf4d":"markdown","16467aeb":"markdown","27d662ab":"markdown","0d9c8c74":"markdown"},"source":{"401deacb":"\nimport numpy as np \nimport cv2\nimport matplotlib.pyplot as plt\nimport keras\nimport re\nimport nltk\nimport string\nfrom time import time\nimport pickle\nfrom keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM\nfrom keras.layers.merge import add","147b01bb":"def readText(path):\n    with open(path) as f:\n        captions = f.read()\n    return captions","c17d4c34":"captions = readText('..\/input\/flickr8k\/Flickr8k_text\/Flickr8k.token.txt')","385ce574":"captions = captions.split('\\n')[:-1] # The last image caption is just \"''\".","60dcbc5e":"# Sample view of Captions\nprint(captions[:10])","0b55cad4":"# Preprocessing Captions to convert them into a dictionary\ncaption_1 = captions[0]\nprint(caption_1)\n\ncaption_1 = caption_1.split('\\t')\nprint(caption_1)\n\nprint(caption_1[0].split('.'))\n","8a0e1a95":"# Dictionary to map each image with all its captions\ndescriptions = {}\n\nfor caption in captions:\n    first, second = caption.split('\\t')\n    img_name = first.split('.')[0]\n    \n    #if image is already present or not\n    if descriptions.get(img_name) is None:\n        descriptions[img_name] = []\n        \n    descriptions[img_name].append(second)","100959d8":"# Sample Captions for 1st image\ndescriptions['1000268201_693b08cb0e']","3c262856":"# Visualizing the first Image\nimg_path = '..\/input\/flickr8k\/Flickr8k_Dataset\/'\n\nimg = cv2.imread(img_path +'1000268201_693b08cb0e.jpg')\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(6,6))\nplt.imshow(img)\nplt.title(descriptions['1000268201_693b08cb0e'][0])\nplt.axis('off')\nplt.show()","e568e1b0":"def clean_text(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(\"[^a-z']+\",' ',sentence)\n    sentence = sentence.split()\n    sentence = [ word for word in sentence if len(word)>1]\n    sentence = \" \".join(sentence)\n    return sentence","15742f00":"sent = \"We can't live without each a other\"\nclean_text(sent)","73c46129":"# Clean all Captions\nfor key,caption_list in descriptions.items():\n    for i in range(len(caption_list)):\n        caption_list[i] = clean_text(caption_list[i])","3e519932":"# Sample Cleaned Text\ndescriptions['1000268201_693b08cb0e']\n","e0d42cab":"# Saving the Descriptions\nwith open(\"descriptions.txt\",\"w\") as f:\n    f.write(str(descriptions))","20a27bfc":"\"\"\"\ndescriptions = None\nwith open(\"descriptions_1.txt\",'r') as f:\n    descriptions= f.read()\n    \njson_acceptable_string = descriptions.replace(\"'\",\"\\\"\")\ndescriptions = json.loads(json_acceptable_string)\n\"\"\"","b139e6ee":"# the total unique words in our vovcabulary\nunique_words = set()\nfor key in descriptions.keys():\n    for caption in descriptions[key]:\n        unique_words.update(caption.split())\n    \nlen(unique_words)","e9eee296":"# total words\ntotal_words = []\nfor key in descriptions.keys():\n    for caption in descriptions[key]:\n        [total_words.append(word) for word in caption.split()]\n\nprint('Total Words are %d' %len(total_words))","5e1fa558":"# Frequency count of each word\nimport collections\n\nword_freq = collections.Counter(total_words)\nword_freq = sorted(word_freq.items(),reverse=True,key= lambda x:x[1])","9d4b285b":"# Setting the final Vocabulary\nthreshold = 5\nfinal_vocab = [x[0] for x in word_freq if x[1]>threshold]\nprint(len(final_vocab))","919607c7":"# Loading Train Annotations \ntrain_data = readText('..\/input\/flickr8k\/Flickr8k_text\/Flickr_8k.trainImages.txt')\ntest_data =  readText('..\/input\/flickr8k\/Flickr8k_text\/Flickr_8k.testImages.txt')","af088777":"train_names = [lines.split('.')[0] for lines in train_data.split('\\n')[:-1]]\ntest_names = [lines.split('.')[0] for lines in test_data.split('\\n')[:-1]]","3cab4251":"print('Length of Training Data',len(train_names))\nprint('Length of Testing Data',len(test_names))","e0521925":"# Prepare Description for the Training Data\n# Tweak - Add <s> and <e> token to our training data\n\ntrain_descriptions = {}\nfor img_id in train_names:\n    train_descriptions[img_id]=[]\n    for captions in descriptions[img_id]:\n        train_descriptions[img_id].append('<s> '+captions+' <e>')\n    ","fb107e63":"train_descriptions['1000268201_693b08cb0e']","968f0d63":"model = ResNet50(weights='imagenet',include_top=False,pooling='avg')\nmodel.summary()","b28e1d9e":"# model_new = Model(model.input,model.layers[-2].output)","f4b110de":"def preprocess_img(img):\n    img = image.load_img(img,target_size=(224,224))\n    img =  image.img_to_array(img)\n    img = np.expand_dims(img,axis=0)\n    \n    ## Preprocessing Image as done in the original Imagenet Challenge\n    \n    img = preprocess_input(img)\n    return img","ba311bcc":"# Sample image after preprocessing\nimg = preprocess_img('..\/input\/flickr8k\/Flickr8k_Dataset\/1000268201_693b08cb0e.jpg')\nplt.imshow(img[0])\nplt.axis('off')\nplt.show()","4f3d2283":"def encode_img(img):\n    img = preprocess_img(img)\n    feature_vector = model.predict(img)\n    \n    feature_vector = feature_vector.reshape((-1,))\n    return feature_vector","6ec9f297":"encode_img('..\/input\/flickr8k\/Flickr8k_Dataset\/1000268201_693b08cb0e.jpg').shape","5cd217cf":"encoding_train = {}\nstart = time()\nfor ix,img_name in enumerate(train_names):\n    image_path = img_path+img_name+'.jpg'\n    encoding_train[img_name] = encode_img(image_path)\n    \n    if ix%100==0:\n        print('No. of images encoded', ix)\nend = time()\nprint('Total time taken:',end-start)\n    ","743c0d38":"with open('encoded_train_features.pkl','wb') as f:\n    pickle.dump(encoding_train,f)","f4ee44b9":"encoding_test = {}\nstart = time()\nfor ix,img_name in enumerate(test_names):\n    image_path = img_path+img_name+'.jpg'\n    encoding_test[img_name] = encode_img(image_path)\n    \n    if (ix+1)%100==0:\n        print('No. of images encoded', ix+1)\nend = time()\nprint('Total time taken:',end-start)","6229dd96":"with open('encoded_test_features.pkl','wb') as f:\n    pickle.dump(encoding_train,f)","d205452e":"print(len(final_vocab))\n\nword_to_idx = {}\nidx_to_word = {}\n\nfor i,word in enumerate(final_vocab):\n    word_to_idx[word] = i+1\n    idx_to_word[i+1] = word","e5c5ce7e":"# Two special words\nidx_to_word[1846] = '<s>'\nword_to_idx['<s>'] = 1846\n\nidx_to_word[1847] = '<e>'\nword_to_idx['<e>'] = 1847\n\nvocab_size = len(word_to_idx) + 1\nprint(\"Vocab Size\",vocab_size)","27bbf0c4":"### Importing Libraries","95e5967f":"## Steps\n1. Data collection\n1. Understanding the data\n1. Data Cleaning\n1. Loading the training set\n1. Data Preprocessing \u2014 Images\n1. Data Preprocessing \u2014 Captions\n1. Data Preparation using Generator Function\n1. Word Embeddings\n1. Model Architecture\n1. Inference","fbd427b6":"### Caption pre-processing","42a11b6c":"### Understanding the Data","f226cf4d":"### Data Collection","16467aeb":"### Data Cleaning\n","27d662ab":"### Creating a Vocabulary","0d9c8c74":"### Preparing Training Testing data\n"}}