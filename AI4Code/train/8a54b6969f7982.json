{"cell_type":{"f8243a87":"code","38d8944d":"code","0d1296a2":"code","ff94ba8e":"code","7ca7fd02":"code","084b2878":"code","e3bdc4ca":"code","5b1c24f6":"code","1f5d9170":"code","1983aea4":"code","ec133df6":"code","c15fc7f8":"code","d37f4714":"code","904ab3d4":"code","c599517d":"code","18eb7065":"code","6ec00021":"code","0422523a":"code","211aa714":"code","92a9b4a0":"code","6bfa7723":"code","09f24161":"code","a636fff7":"code","123bde63":"code","b491b19f":"code","e0f88ae0":"code","f1427602":"code","dc50f775":"code","6d2b1a96":"code","76ca1097":"code","abba7ce2":"code","d52a5541":"code","a99ffee7":"markdown","13691f5d":"markdown","f6cecc9e":"markdown","47d0771c":"markdown","9ec79f4f":"markdown","2f98fd16":"markdown","cd548c3d":"markdown","a4387a9c":"markdown","0d659ced":"markdown","1e732be5":"markdown","4851f52e":"markdown","88b25a61":"markdown","0b5c5b2d":"markdown","fd4ab59c":"markdown","6f27b7e8":"markdown","4fe7e00b":"markdown","670722c4":"markdown","45c0623c":"markdown","be81f23c":"markdown","61c3c554":"markdown","9c604677":"markdown","1900d879":"markdown","4b2efa00":"markdown","634123fa":"markdown"},"source":{"f8243a87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install --upgrade scikit-learn\nfrom catboost import CatBoostClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier,StackingClassifier\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.core.display import display, HTML\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom matplotlib_venn import venn2\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom bs4 import BeautifulSoup\nimport string\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport torch\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score,train_test_split,StratifiedKFold\nimport sys\nimport torch\nimport gc\nimport tensorflow as tf\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score,plot_roc_curve\nimport folium \nfrom folium import plugins \nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport tensorflow_hub as hub\nfrom gensim.models import word2vec\nimport itertools\nfrom sklearn.manifold import TSNE\nimport itertools\nSTOPWORDS = set(stopwords.words('english'))\nfrom IPython.display import Markdown\n\ndef bold(string):\n    display(Markdown(string))\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","38d8944d":"# !pip install simpletransformers\n\n# from simpletransformers.classification import ClassificationModel ","0d1296a2":"inputpath='..\/input\/nlp-getting-started'\n\nprint(\"Reading the data\")\ntraindata=pd.read_csv(inputpath+'\/train.csv')\ntestdata=pd.read_csv(inputpath+'\/test.csv')\nsubmission=pd.read_csv(inputpath+'\/sample_submission.csv')","ff94ba8e":"#Reference: https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\n\ndisplay(HTML(f\"\"\"\n   \n        <ul class=\"list-group\">\n          <li class=\"list-group-item disabled\" aria-disabled=\"true\"><h4>Shape of Train and Test Dataset<\/h4><\/li>\n          <li class=\"list-group-item\"><h4>Number of rows in Train dataset is: <span class=\"label label-primary\">{ traindata.shape[0]:,}<\/span><\/h4><\/li>\n          <li class=\"list-group-item\"> <h4>Number of columns Train dataset is <span class=\"label label-primary\">{traindata.shape[1]}<\/span><\/h4><\/li>\n          <li class=\"list-group-item\"><h4>Number of rows in Test dataset is: <span class=\"label label-success\">{ testdata.shape[0]:,}<\/span><\/h4><\/li>\n          <li class=\"list-group-item\"><h4>Number of columns Test dataset is <span class=\"label label-success\">{testdata.shape[1]}<\/span><\/h4><\/li>\n        <\/ul>\n  \n    \"\"\"))","7ca7fd02":"traindata.head()","084b2878":"'''A Function To Plot Pie Plot using Plotly'''\n\ndef pie_plot(cnt_srs, colors, title):\n    labels=cnt_srs.index\n    values=cnt_srs.values\n    trace = go.Pie(labels=labels, \n                   values=values, \n                   title=title, \n                   hoverinfo='percent+value', \n                   textinfo='percent',\n                   textposition='inside',\n                   hole=0.7,\n                   showlegend=True,\n                   marker=dict(colors=colors,\n                               line=dict(color='#000000',\n                                         width=2),\n                              )\n                  )\n    return trace\n\nbold(\"**Non disaster tweets vs Disaster tweets**\")\npy.iplot([pie_plot(traindata['target'].value_counts(), ['cyan', 'green'], 'Tweets')])\n","e3bdc4ca":"compare_cols = ['keyword', 'location']\n\ndef get_trace(col, df, color):\n    temp = df[col].value_counts().nlargest(5)\n    x = list(reversed(list(temp.index)))\n    y = list(reversed(list(temp.values)))\n    trace = go.Bar(x = y, y = x, width = [0.9, 0.9, 0.9], orientation='h', marker=dict(color=color))\n    return trace\n\n\ntraintraces = []\ntraintitles = []\nfor i,col in enumerate(compare_cols):\n    traintitles.append(col)\n    traintraces.append(get_trace(col, traindata, '#a3dd56'))\n\n    \ntesttraces = []\ntesttitles = []\nfor i,col in enumerate(compare_cols):\n    testtitles.append(col)\n    testtraces.append(get_trace(col, testdata, '#ef7067'))\n\ntitles = []\nfor each in compare_cols:\n    titles.append(each)\n    titles.append(each)\nfig = tools.make_subplots(rows=len(compare_cols), cols=2, print_grid=False, horizontal_spacing = 0.15, subplot_titles=titles)\n\ni = 0\n\nfor g,b in zip(traintraces, testtraces):\n    i += 1\n    fig.append_trace(g, i, 1);\n    fig.append_trace(b, i, 2);\n\nfig['layout'].update(height=1000, margin=dict(l=100), showlegend=False, title=\"Comparing Features of train and test data\",template= \"plotly_dark\");\niplot(fig, filename='simple-subplot');   ","5b1c24f6":"plt.figure(figsize=(23,13))\n\nplt.subplot(321)\nvenn2([set(traindata.keyword.unique()), set(testdata.keyword.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common keyword in training and test data\", fontsize=15)\n\nplt.subplot(322)\nvenn2([set(traindata.location.unique()), set(testdata.location.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common location in training and test data\", fontsize=15)","1f5d9170":"traindata[\"text\"] = traindata[\"text\"].astype(str)\ntestdata[\"text\"] = testdata[\"text\"].astype(str)","1983aea4":"add_stopwords = [\n    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\",\n    \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n    \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n    \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\",\n    \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\",\n    \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\",\n    \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n    \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\",\n    \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\",\n    \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\",\n    \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\",\n    \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n    \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\",\n    \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\",\n    \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\n    \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\",\n    \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\",\n    \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\",\n    \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\",\n    \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n    \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\",\n    \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\",\n    \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n    \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\",\n    \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n    \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",\n    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n    \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",\n    \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\",\n    \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\",\n    \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\",\n    \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\",\n    \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\",\n    \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\",\n    \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\",\n    \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\",\n    \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\",\n    \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\",\n    \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n    \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\",\n    \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\",\n    \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\",\n    \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\",\n    \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\",\n    \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",\n    \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\",\n    \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\",\n    \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\",\n    \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\",\n    \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\",\n    \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\",\n    \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\",\n    \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\",\n    \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\",\n    \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\",\n    \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n    \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\",\n    \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\",\n    \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\",\n    \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\",\n    \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\",\n    \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\",\n    \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\",\n    \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\",\n    \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\",\n    \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\",\n    \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\",\n    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\",\n    \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\",\n    \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\",\n    \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\",\n    \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\",\n    \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\",\n    \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n    \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\",\n    \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\",\n    \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\",\n    \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\",\n    \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\",\n    \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\",\n    \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\",\n    \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\",\n    \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\",\n    \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\",\n    \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\",\n    \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\",\n    \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\",\n    \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\",\n    \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\",\n    \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\",\n    \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\",\n    \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\",\n    \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\",\n    \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\",\n    \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\",\n    \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\",\n    \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\",\n    \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\",\n    \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\",\n    \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\",\n    \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\",\n    \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\",\n    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n    \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\n    \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",\n    \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\",\n    \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\n    \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\",\n    \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\",\n    \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\",\n    \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n    \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\n    \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\",\n    \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\",\n    \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\n    \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n    \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\",\n    \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n    \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\n    \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\",\n    \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n    \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n    \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n    \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\",\n    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n    \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\",\n    \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\",\n    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\",\n    \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n    \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\",\n    \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n    \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\",\n    \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\",\n    \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\",\n    \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\",\n    \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\",\n    \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\",\n    \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\",\n    \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\",\n    \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\",\n    \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\",\n    \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\",\n    \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\",\n    \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\",\n    \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n    \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\",\n    \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\",\n    \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\",\n    \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\",\n    \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\",\n    \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\",\n    \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\",\n    \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\",\n    \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\",\n    \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\",\n    \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\",\n    \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\",\n    \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\",\n    \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\",\n    \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\",\n    \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\",\n    \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n    \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"\n]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n#Reference: https:\/\/www.kaggle.com\/jdparsons\/tweet-cleaner\nslang_abbrev_dict = {\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'B4N': 'Bye For Now',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': 'For What It\\'s Worth',\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you',\n    'ILU': 'I Love You',\n    'IMHO': 'In My Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My Ass Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'OMG': 'Oh My God',\n    'PITA': 'Pain In The Ass',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The Fuck',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait',\n    '7K': 'Sick:-D Laugher'\n}\n","ec133df6":"traindata['Hashtags']=traindata['text'].apply(lambda x: re.findall(r'#(\\w+)',x))\ntestdata['Hashtags']=testdata['text'].apply(lambda x: re.findall(r'#(\\w+)',x))","c15fc7f8":"traindata.head()","d37f4714":"hashes=pd.DataFrame(data=list(itertools.chain(*traindata['Hashtags'].values)),columns=['Hashes'])\n\ntemp_df =  hashes['Hashes'].value_counts()[:15]\ndata = go.Bar(x = temp_df.index,y = temp_df.values,text = temp_df.values,  textposition='auto')\nfig = go.Figure(data = data)\nfig.update_traces(marker_color='#C5197D', marker_line_color='#8E0052',marker_line_width=1.5, opacity=0.6)\nfig.update_layout(barmode='stack', title={'text': \"Trending hashtags\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},template= \"plotly_dark\")\nfig.show()","904ab3d4":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    \n    stops  = set(list(STOPWORDS)+add_stopwords)\n    text = [w for w in word_tokenize(x) if w not in stops]    \n    text = \" \".join(text)\n    \n    return text\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef unslang(text):\n    \"\"\"Converts text like \"OMG\" into \"Oh my God\"\n    \"\"\"\n    text = [slang_abbrev_dict[w.upper()] if w.upper() in slang_abbrev_dict.keys() else w for w in word_tokenize(text)]    \n    return \" \".join(text)\n    \n\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    return BeautifulSoup(text, \"lxml\").text\n\n\ndef clean_data(df, col):\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: remove_urls(x))\n        df[col] = df[col].apply(lambda x: remove_html(x))\n        df[col] = df[col].apply(lambda text: remove_punctuation(text))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: unslang(x))\n        df[col] = df[col].apply(lambda x: remove_emoji(x))\n        return df","c599517d":"traindata = clean_data(traindata, 'text')\ntestdata = clean_data(testdata, 'text')","18eb7065":"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    \n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/masks\/masks-wordclouds\/'\n","6ec00021":"comments_text = str(traindata.text)\ncomments_mask = np.array(Image.open(d + 'upvote.png'))\nplot_wordcloud(comments_text, comments_mask, max_words=2000, max_font_size=300, \n               title = 'Most common words in all of the tweets', title_size=30)","0422523a":"%%time\nmodule_url = 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/4'\nembed = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')","211aa714":"USE_train_embeddings = embed(traindata.text.values)\nUSE_test_embeddings = embed(testdata.text.values)\n\ndel embed","92a9b4a0":"lr_cross = LogisticRegression(solver='lbfgs')\ndtc_cross = DecisionTreeClassifier()\nrfc_cross = RandomForestClassifier(n_estimators=50)\nknn_cross = KNeighborsClassifier(n_neighbors=1)","6bfa7723":"lr_scores = cross_val_score(lr_cross,USE_train_embeddings['outputs'].numpy(),traindata.target.values,cv=10,scoring='f1')\ndtc_scores = cross_val_score(dtc_cross,USE_train_embeddings['outputs'].numpy(),traindata.target.values,cv=10,scoring='f1')\nrfc_scores = cross_val_score(rfc_cross,USE_train_embeddings['outputs'].numpy(),traindata.target.values,cv=10,scoring='f1')\nknn_scores = cross_val_score(knn_cross,USE_train_embeddings['outputs'].numpy(),traindata.target.values,cv=10,scoring='f1')\n","09f24161":"def run_lgb(reduce_train, reduce_test):\n    \n    kf = StratifiedKFold(n_splits=10) \n    avg_f1_score=[]\n    \n    oof_pred = np.zeros((len(reduce_train)))\n    \n    y_pred = np.zeros((len(reduce_test)))\n    \n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train,traindata['target'].values)):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[tr_ind,:], reduce_train[val_ind,:]\n        y_train, y_val = traindata['target'][tr_ind], traindata['target'][val_ind]\n        \n        train_set = lgb.Dataset(x_train, y_train)#, categorical_feature=cat_features)\n        val_set = lgb.Dataset(x_val, y_val)#, categorical_feature=cat_features)\n\n        params = {\n            'learning_rate': 0.04,\n            'n_estimators': 1500,\n            'metric':'auc',\n            'colsample_bytree': 0.4,\n        }\n       \n        model = lgb.train(params, train_set, num_boost_round = 1000, early_stopping_rounds = 5, #1000\n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        \n        oof_pred[val_ind] = [1 if i>=0.5 else 0 for i in model.predict(x_val)]\n        train_folds=[1 if i>=0.5 else 0 for i in model.predict(reduce_train)]\n        \n        y_pred += model.predict(reduce_test) \/ kf.n_splits\n        \n        avg_f1_score.append(f1_score(traindata['target'].values,train_folds))\n        print('OOF F1:', f1_score(traindata['target'].values,oof_pred))\n        \n    return y_pred,model,avg_f1_score","a636fff7":"y_pred,modelobj,avg_f1_score = run_lgb(USE_train_embeddings['outputs'].numpy(),USE_test_embeddings['outputs'].numpy())","123bde63":"summary_table = pd.DataFrame([lr_scores.mean(),dtc_scores.mean(),rfc_scores.mean(),knn_scores.mean(),np.mean(avg_f1_score)],index=['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN','LGBM'], columns=['F1 Score'])\nsummary_table=summary_table.reset_index()","b491b19f":"trace1 = go.Bar(\n                x = summary_table['index'],\n                y = summary_table['F1 Score'],\n                marker = dict(color = 'rgb(153,255,153)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)))\nlayout = go.Layout(template= \"plotly_dark\",title = 'BASE_LINE_MODELS' , xaxis = dict(title = 'Models'), yaxis = dict(title = 'Mean CV'))\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()","e0f88ae0":"def make_classifier():\n    clf = CatBoostClassifier(\n                               loss_function='CrossEntropy',\n                               eval_metric=\"F1\",\n                               task_type=\"CPU\",\n                               learning_rate=0.05,\n                               n_estimators =100,   #5000\n                               early_stopping_rounds=10,\n                               random_seed=2019,\n                               silent=True\n                              )\n        \n    return clf\n\n\n","f1427602":"scoring = \"f1\"\n\n\nHistGBM_param = {\n    'l2_regularization': 0.0,\n    'loss': 'auto',\n    'max_bins': 255,\n    'max_depth': 15,\n    'max_leaf_nodes': 31,\n    'min_samples_leaf': 20,\n    'n_iter_no_change': 50,\n    'scoring': scoring,\n    'tol': 1e-07,\n    'validation_fraction': 0.15,\n    'verbose': 0,\n    'warm_start': False   \n}\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\nfold_preds = np.zeros([USE_test_embeddings['outputs'].numpy().shape[0],3])\noof_preds = np.zeros([USE_train_embeddings['outputs'].numpy().shape[0],3])\nresults = {}\n\nestimators = [\n        ('histgbm', HistGradientBoostingClassifier(**HistGBM_param)),\n        ('catboost', make_classifier())\n    ]\n\n# Fit Folds\nf, ax = plt.subplots(1,3,figsize = [14,5])\nfor i, (trn_idx, val_idx) in enumerate(folds.split(USE_train_embeddings['outputs'].numpy(),traindata['target'].values)):\n    print(f\"Fold {i} stacking....\")\n    clf = StackingClassifier(\n            estimators=estimators,\n            final_estimator=LogisticRegression(),\n            )\n    clf.fit(USE_train_embeddings['outputs'].numpy()[trn_idx,:], traindata['target'].loc[trn_idx])\n    tmp_pred = clf.predict_proba(USE_train_embeddings['outputs'].numpy()[val_idx,:])[:,1]\n    \n    oof_preds[val_idx,0] = tmp_pred\n    fold_preds[:,0] += clf.predict_proba(USE_test_embeddings['outputs'].numpy())[:,1] \/ folds.n_splits\n        \n    estimator_performance = {}\n    estimator_performance['stack_score'] = metrics.roc_auc_score(traindata['target'].loc[val_idx], tmp_pred)\n    \n    for ii, est in enumerate(estimators):\n            model = clf.named_estimators_[est[0]]\n            pred = model.predict_proba(USE_train_embeddings['outputs'].numpy()[val_idx,:])[:,1]\n            oof_preds[val_idx, ii+1] = pred\n            fold_preds[:,ii+1] += model.predict_proba(USE_test_embeddings['outputs'].numpy())[:,1] \/ folds.n_splits\n            estimator_performance[est[0]+\"_score\"] = metrics.roc_auc_score(traindata['target'].loc[val_idx], pred)\n            \n    stack_coefficients = {x+\"_coefficient\":y for (x,y) in zip([x[0] for x in estimators], clf.final_estimator_.coef_[0])}\n    stack_coefficients['intercept'] = clf.final_estimator_.intercept_[0]\n        \n    results[\"Fold {}\".format(str(i+1))] = [\n            estimator_performance,\n            stack_coefficients\n        ]\n\n    plot_roc_curve(clf, USE_train_embeddings['outputs'].numpy()[val_idx,:], traindata['target'].loc[val_idx], ax=ax[i])\n    ax[i].plot([0.0, 1.0])\n    ax[i].set_title(\"Fold {} - ROC AUC\".format(str(i)))\n\nplt.tight_layout(pad=2)\nplt.show()\n\nf, ax = plt.subplots(1,2,figsize = [11,5])\nsns.heatmap(pd.DataFrame(oof_preds, columns = ['stack','histgbm','catboost']).corr(),\n            annot=True, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'},cmap=\"magma\",ax=ax[0])\nax[0].set_title(\"OOF PRED - Correlation Plot\")\nsns.heatmap(pd.DataFrame(fold_preds, columns = ['stack','histgbm','catboost']).corr(),\n            annot=True, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'},cmap=\"inferno\",ax=ax[1])\nax[1].set_title(\"TEST PRED - Correlation Plot\")\nplt.tight_layout(pad=3)\nplt.show()","dc50f775":"del USE_train_embeddings,USE_test_embeddings\ngc.collect()\n\ncustom_args={'num_train_epochs':2,'max_seq_length':512,'fp16':False,'overwrite_output_dir': True}","6d2b1a96":"# n=5\n# kf = StratifiedKFold(n_splits=n, random_state=2019, shuffle=True)\n# results = []\n\n# for train_index, val_index in kf.split(traindata.text,traindata.target.values):\n#     train_df = traindata[['text','target']].iloc[train_index]\n#     val_df = traindata[['text','target']].iloc[val_index]\n#     model = ClassificationModel('bert', 'bert-base-uncased', args=custom_args,use_cuda=False) \n#     model.train_model(train_df)\n#     result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=f1_score)\n#     print(result['acc'])\n#     results.append(result['acc'])\n    \n# del model\n# gc.collect()","76ca1097":"with open('..\/input\/nlp-predictions\/fold_results.txt') as f:\n    results=f.readlines()\n\nfor i, result in enumerate(results, 1):\n    print(f\"Fold-{i}: {result}\")\n","abba7ce2":"# model = ClassificationModel('bert', 'bert-base-uncased', args=custom_args) \n# model.train_model(traindata[['text','target']])\n\n# gc.collect()","d52a5541":"#predictions, raw_outputs = model.predict(testdata['text'])\n\n# with open('..\/input\/nlp-predictions\/predictions.txt') as f:\n#     predictions=f.readlines()\n    \nsubmission['target'] = [1  if stack>=0.5 else 0 for stack in fold_preds[:,0] ] #predictions\nsubmission.to_csv('submission.csv', index=False)","a99ffee7":"Let's use the stacking classifiers predictions as submission.","13691f5d":"<p>This is a typical <b>Sentiment analysis<\/b> use case where-in we need to analyse the tweets and predict the binary response(Disaster or non-disaster). Cleaning plays the major role here. Let's get started","f6cecc9e":"<font color='#088a5a' size=4>Baseline models<\/font><br>","47d0771c":"<font color='#088a5a' size=4>Light boost<\/font><br>","9ec79f4f":"<font color='#088a5a' size=3>Kindly upvote the kernel if you like it!<\/font><br>","2f98fd16":"<font color='#b967ff' size=5>Model building:<\/font><br>\n    *     Baseline models -USE\n    *     Light boost -USE\n    *     Stacking classifiers\n    *     Simple transformers\n    *     Inference","cd548c3d":"This is from my previous [kernel](https:\/\/www.kaggle.com\/nandhuelan\/let-s-tickle-the-cat-meow). Please look into this for further details","a4387a9c":"* <p>There is no imbalance as such. We have <b>57%<\/b> non disaster and <b>43%<\/b> disaster tweets <\/p>","0d659ced":"<font color='#088a5a' size=4>Inference<\/font><br>","1e732be5":"<p>Creating columns for hashtags<\/p>","4851f52e":"The below code snippet is taken from my previous kernel [here](https:\/\/www.kaggle.com\/nandhuelan\/google-quest-for-nlp-v7\/edit\/run\/25329469) and also from this wonderful kernel by SRK https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing","88b25a61":"Change status:\n* Update_V3: Stacking classifiers\n* Update_V2: Multiple models, additional visualization\n* Update: Using simple transformers","0b5c5b2d":"<font color='#088a5a' size=4>Work in progress..!<\/font><br>","fd4ab59c":"<img src='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcRYylW47NjX-VyjCf1Geo5xOnq4gWKXMjSdoV6ZRVF5NC8kR96z' width=800>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","6f27b7e8":"Thanks alot to this [kernel](https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv) for bringing up this under utilized library.\nI have already ran the model offline. I have commented the code below for reference","4fe7e00b":"**Competition objective**:\n<p>Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).<\/p>","670722c4":"<font color='#088a5a' size=4>USE Features<\/font><br>","45c0623c":"<font color='#088a5a' size=4>Simple transformers - BERT<\/font><br>","be81f23c":"* <p>There are no common keywords in train and test data<\/p>\n* <p>There are 423 common locations between train and test data<\/p>","61c3c554":"<font color='#088a5a' size=4>Data glimpse<\/font><br>","9c604677":"<font color='#088a5a' size=4>Data cleaning<\/font><br>","1900d879":"<font color='#088a5a' size=4>Data walk through<\/font><br>","4b2efa00":"* <p> Keywords like fatalities,harm,damage,demolished,deluge seems to be indicating disaster related keywords in both train and test data<\/p>\n* <p> Tweet location is mostly from USA in both train and test data","634123fa":"<font color='#088a5a' size=4>Stacking models<\/font><br>\n\n* Catboost, Histgbm and logistic regression        "}}