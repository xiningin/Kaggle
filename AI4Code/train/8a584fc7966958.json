{"cell_type":{"b8e5b371":"code","bf8d3958":"code","f17e780e":"code","bbfce790":"code","1fa6b582":"code","fb7a9bd8":"code","2e76d207":"code","096083ee":"code","8c9733ef":"code","ab76fbf3":"code","e1756b0b":"code","0fe4fc40":"code","841587b9":"code","e73f6176":"code","e5244608":"code","d7ff524a":"code","1eedb5eb":"code","ce7a9d0d":"code","422501e0":"code","f6b08496":"code","d99fb4d3":"code","a6d9bf29":"code","010e8552":"code","5c02f940":"code","c9bd51fa":"code","ee50c1bc":"code","9ed90d4d":"code","faa27b3e":"code","1c13fdc1":"code","79bdba28":"code","275995e4":"code","6ed69e63":"code","5fef68fe":"code","8b332b34":"code","a7e33ee1":"code","c853d632":"code","a34bf68f":"code","27ea9e70":"code","989f6c6d":"code","497d8074":"code","51aa8fa0":"code","8ee9bcc5":"code","e93ddafb":"code","f0c3c562":"code","542f0057":"code","f2676fb0":"code","a1d27f21":"code","0e2ead3c":"code","b88ce1b9":"code","4b1a6d86":"markdown"},"source":{"b8e5b371":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf8d3958":"import pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport scipy.io\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nimport datetime as dt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier   #Neural Network\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n","f17e780e":"data = pd.read_excel('..\/input\/data-cleaned\/data_cleaned.xlsx', index_col=None, header=0) \n#data = pd.read_excel('data_cleaned.xlsx', index_col=None, header=0) ","bbfce790":"data.head()","1fa6b582":"data['Gender'].value_counts().plot(kind='bar')","fb7a9bd8":"plt.scatter(data['Times Solicitated Lifetime'], data['Dollars Contribution Lifetime'], marker='o')\nplt.title(\"Times Solicited vs Dollars Contributed (Lifetime)\")\nplt.xlabel(\"Times Solicited\")\nplt.ylabel(\"Dollars Contributed\")\nplt.show()","2e76d207":"plt.scatter(data['Times Solicitated Lifetime'], data['Times Contributed Lifetime'], marker='o')\nplt.title(\"Times Solicited vs Contributed\")\nplt.xlabel(\"Times Solicited\")\nplt.ylabel(\"Times Contributed\")\nplt.show()","096083ee":"#average amount donated\nave_donation_amt1 = data['Dollars Contribution Lifetime'].sum()\/data['Times Contributed Lifetime'].sum()\nprint(\"Average Donation Amount per Contribution = \", round(ave_donation_amt1, 2))","8c9733ef":"ave_donation_amt2 = data['Dollars Contribution Lifetime'].sum()\/data['Times Solicitated Lifetime'].sum()\nprint(\"Average Donation Amount for all Solicitations = \", round(ave_donation_amt2, 2))","ab76fbf3":"#average rate of response\/donation\nave_donation_rate = data['Times Contributed Lifetime'].sum()\/data['Times Solicitated Lifetime'].sum()\nprint(\"Average Donation Rate = \", round(ave_donation_rate, 2))","e1756b0b":"plt.scatter(data['Largest Contribution Date'].tolist(), data['Largest Contribution'])\nplt.title(\"Date and Amount of Largest Donation\")\nplt.xlabel(\"Largest Contribtion Date\")\nplt.ylabel(\"Larget Contribution ($)\")\nplt.show()","0fe4fc40":"# average contribution by gender\n# create a column with total contribution\ndata[\"Total Contribution\"] = data.loc[:,'Latest Contribution':'10th Latest Contribution'].sum(axis = 1)\ncontri_gender = data[[\"Gender\", \"Total Contribution\"]].groupby(\"Gender\").mean()","841587b9":"def split_data(xdata, y):\n    #split the data, 80% training, 20% test, random selection\n    X_train, X_test, y_train, y_test = train_test_split(xdata, y, test_size=0.2)\n    return X_train, X_test, y_train, y_test","e73f6176":"# create a column with total contribution\ndata[\"Total Contribution\"] = data.loc[:,'Latest Contribution':'10th Latest Contribution'].sum(axis = 1)\n\n# average contribution by gender\ncontri_gender = data[[\"Gender\", \"Total Contribution\"]].groupby(\"Gender\").mean()\n\n# creata a binary column that indicates whether the donor donated in Fall 1995\ndata[\"Target\"]= np.where(data['Number of Fall 1995 Donations']>0, 1, 0)\n# feature selection\n#X = data.loc[:, data.columns != 'Target']\nX = data.loc[:,'Latest Contribution':'10th Latest Contribution']\nX = X.fillna(0)\ny = data[\"Target\"]\n\nclf = LassoCV().fit(X, y)\nimportance = np.abs(clf.coef_)\nprint(importance)\n\nfeature_names = X.columns\nidx_third = importance.argsort()[-3]\nthreshold = importance[idx_third] + 0.01\n\nidx_features = (-importance).argsort()[:2]\nname_features = np.array(feature_names)[idx_features]\nprint('Selected features: {}'.format(name_features))\n","e5244608":"# logistic regression\nX = X.loc[:,['Latest Contribution','9th Latest Contribution']]\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=0)\nclassifier = LogisticRegression(solver='lbfgs',random_state=0)\nclassifier.fit(X_train, Y_train)\npredicted_y = classifier.predict(X_test)\npredicted_y\nprint('Accuracy: {:.2f}'.format(classifier.score(X_test, Y_test)))","d7ff524a":"#get_ipython().system('pip install folium')\nimport folium\nfrom folium import plugins\nfrom io import StringIO\nimport folium \n\n\nstatesll=StringIO(\"\"\"State,Latitude,Longitude\nAlabama,32.806671,-86.791130\nAlaska,61.370716,-152.404419\nArizona,33.729759,-111.431221\nArkansas,34.969704,-92.373123\nCalifornia,36.116203,-119.681564\nColorado,39.059811,-105.311104\nConnecticut,41.597782,-72.755371\nDelaware,39.318523,-75.507141\nDistrict of Columbia,38.897438,-77.026817\nFlorida,27.766279,-81.686783\nGeorgia,33.040619,-83.643074\nHawaii,21.094318,-157.498337\nIdaho,44.240459,-114.478828\nIllinois,40.349457,-88.986137\nIndiana,39.849426,-86.258278\nIowa,42.011539,-93.210526\nKansas,38.526600,-96.726486\nKentucky,37.668140,-84.670067\nLouisiana,31.169546,-91.867805\nMaine,44.693947,-69.381927\nMaryland,39.063946,-76.802101\nMassachusetts,42.230171,-71.530106\nMichigan,43.326618,-84.536095\nMinnesota,45.694454,-93.900192\nMississippi,32.741646,-89.678696\nMissouri,38.456085,-92.288368\nMontana,46.921925,-110.454353\nNebraska,41.125370,-98.268082\nNevada,38.313515,-117.055374\nNew Hampshire,43.452492,-71.563896\nNew Jersey,40.298904,-74.521011\nNew Mexico,34.840515,-106.248482\nNew York,42.165726,-74.948051\nNorth Carolina,35.630066,-79.806419\nNorth Dakota,47.528912,-99.784012\nOhio,40.388783,-82.764915\nOklahoma,35.565342,-96.928917\nOregon,44.572021,-122.070938\nPennsylvania,40.590752,-77.209755\nRhode Island,41.680893,-71.511780\nSouth Carolina,33.856892,-80.945007\nSouth Dakota,44.299782,-99.438828\nTennessee,35.747845,-86.692345\nTexas,31.054487,-97.563461\nUtah,40.150032,-111.862434\nVermont,44.045876,-72.710686\nVirginia,37.769337,-78.169968\nWashington,47.400902,-121.490494\nWest Virginia,38.491226,-80.954453\nWisconsin,44.268543,-89.616508\nWyoming,42.755966,-107.302490\"\"\")\n\ntempdf = states_df = data['State'].value_counts()\nt1 = pd.DataFrame()\nt1['Donor State'] = tempdf.index\nt1['Donor Count'] = tempdf.values\n\nstates = {\n        'AK': 'Alaska',\n        'AL': 'Alabama',\n        'AR': 'Arkansas',\n        'AS': 'American Samoa',\n        'AZ': 'Arizona',\n        'CA': 'California',\n        'CO': 'Colorado',\n        'CT': 'Connecticut',\n        'DC': 'District of Columbia',\n        'DE': 'Delaware',\n        'FL': 'Florida',\n        'GA': 'Georgia',\n        'GU': 'Guam',\n        'HI': 'Hawaii',\n        'IA': 'Iowa',\n        'ID': 'Idaho',\n        'IL': 'Illinois',\n        'IN': 'Indiana',\n        'KS': 'Kansas',\n        'KY': 'Kentucky',\n        'LA': 'Louisiana',\n        'MA': 'Massachusetts',\n        'MD': 'Maryland',\n        'ME': 'Maine',\n        'MI': 'Michigan',\n        'MN': 'Minnesota',\n        'MO': 'Missouri',\n        'MP': 'Northern Mariana Islands',\n        'MS': 'Mississippi',\n        'MT': 'Montana',\n        'NA': 'National',\n        'NC': 'North Carolina',\n        'ND': 'North Dakota',\n        'NE': 'Nebraska',\n        'NH': 'New Hampshire',\n        'NJ': 'New Jersey',\n        'NM': 'New Mexico',\n        'NV': 'Nevada',\n        'NY': 'New York',\n        'OH': 'Ohio',\n        'OK': 'Oklahoma',\n        'OR': 'Oregon',\n        'PA': 'Pennsylvania',\n        'PR': 'Puerto Rico',\n        'RI': 'Rhode Island',\n        'SC': 'South Carolina',\n        'SD': 'South Dakota',\n        'TN': 'Tennessee',\n        'TX': 'Texas',\n        'UT': 'Utah',\n        'VA': 'Virginia',\n        'VI': 'Virgin Islands',\n        'VT': 'Vermont',\n        'WA': 'Washington',\n        'WI': 'Wisconsin',\n        'WV': 'West Virginia',\n        'WY': 'Wyoming'\n}\n#gapminder_df['pop']= gapminder_df['continent'].map(pop_dict)\nt1['Donor State'] = t1['Donor State'].map(states)\n\n# tempdf = combined_df.groupby(['Donor State']).agg({'Donation Amount':'sum'}).reset_index()\n# t1 = tempdf.sort_values('Donation Amount', ascending=False)\n\nsdf = pd.read_csv(statesll).rename(columns={'State':'Donor State'})\nsdf = sdf.merge(t1, on='Donor State', how='inner')\nsdf \n\nmap4 = folium.Map(location=[39.50, -98.35], tiles='CartoDB dark_matter', zoom_start=3.5)\nfor j, rown in sdf.iterrows():\n    rown = list(rown)\n    folium.CircleMarker([float(rown[1]), float(rown[2])], popup=\"<b>State:<\/b>\" + rown[0].title() +\"<br> <b>Donors:<\/b> \"+str(int(rown[3])), radius=float(rown[3])*0.006, color='#be0eef', fill=True).add_to(map4)\nmap4","1eedb5eb":"# select columns that are easy to interpret for the output models\n\ncols = [\"Fall Donation (Binary)\",'Dollars of Fall 1995 Donations','State',\n        'Years Longevity','Largest Contribution','Gender','Times Contributed Lifetime',\n        'Dollars Contribution Lifetime', 'Times Solicitated Lifetime',\n       'Ave Donation Amount per Contribution',\n       'Ave Donation Amount per Solicitation', 'Donation Rate','Most active donation month', 'Most active year',\n       'Distance to the current year', 'Ave Days between Donations',\n       'Latest Contribution','Total of Latest 10 Contributions',\n       '10 Latest Contribution Match Rate']\ndata_mg = data[cols]\n# one hot encoding\ndata_mg_ohe = pd.get_dummies(data_mg, columns=['State', 'Gender'])\n\n# feature scaling\nscaler = MinMaxScaler()\ndata_scaled =pd.DataFrame(scaler.fit_transform(data_mg_ohe),\n            columns=data_mg_ohe.columns, index=data.index) \n\nX = data_scaled.loc[:,'Years Longevity':'Gender_U']\ny = data_scaled[\"Fall Donation (Binary)\"]\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","ce7a9d0d":"data = pd.read_excel('..\/input\/data-beautifiedv1\/data_beautifiedv1.xlsx', index_col=None, header=0) \n#data_beautifiedv1 = pd.read_excel('data_beautifiedv1.xlsx', index_col=None, header=0) \ndata.head()","422501e0":"# native bayes\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\n\nprint(\"Navie Bayes - Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\nfrom sklearn.metrics import classification_report, confusion_matrix\n#print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# logisic regression\nfrom sklearn.linear_model import LogisticRegression\nlgc = LogisticRegression(max_iter=200, solver='liblinear').fit(X_train, y_train)\ny_pred = lgc.predict(X_test)\nprint(\"Logistic Regression - Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n#print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"KNN - Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n#print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","f6b08496":"# relevant columns from dataset for analysis.\ndata_debset = data[[\"Latest Contribution\",\"Fall Donation (Binary)\", \"Dollars of Fall 1995 Donations\", \"State\", \"Years Longevity\", \"Gender\", \"Times Contributed Lifetime\", \"Dollars Contribution Lifetime\", \"Times Solicitated Lifetime\", \"Ave Donation Amount per Contribution\", \"Ave Donation Amount per Solicitation\", \"Donation Rate\", \"Latest Contribution Month\", \"Latest Contribution Year\", \"Most active donation month\", \"Most active year\", \"Years between most active and current years\", \"Ave Days between Donations\", \"Total of Latest 10 Contributions\", \"10 Latest Contribution Match Rate\"]]\ndata_debset2 = data[[\"Latest Contribution\", \"Dollars of Fall 1995 Donations\", \"State\", \"Years Longevity\", \"Gender\", \"Times Contributed Lifetime\", \"Dollars Contribution Lifetime\", \"Times Solicitated Lifetime\", \"Ave Donation Amount per Contribution\", \"Ave Donation Amount per Solicitation\", \"Donation Rate\", \"Latest Contribution Month\", \"Latest Contribution Year\", \"Most active donation month\", \"Most active year\", \"Years between most active and current years\", \"Ave Days between Donations\", \"Total of Latest 10 Contributions\", \"10 Latest Contribution Match Rate\"]]\n\n#one hot encoding categorical variables\ndata_ohe = pd.get_dummies(data_debset, columns=['State', 'Gender', \"Latest Contribution Month\", \"Latest Contribution Year\", \"Most active donation month\", \"Most active year\"])\ndata_ohe2 = pd.get_dummies(data_debset2, columns=['State', 'Gender', \"Latest Contribution Month\", \"Latest Contribution Year\", \"Most active donation month\", \"Most active year\"])\n\n#scaled data \ndata_scaled = pd.DataFrame(preprocessing.scale(data_ohe),columns = data_ohe.columns) \ndata_scaled2 = pd.DataFrame(preprocessing.scale(data_ohe2),columns = data_ohe2.columns) \n\n# relevant columns from dataset\ndata_deb1 = data_scaled.drop(\"Latest Contribution\", axis=1)\nrows, cols = data_deb1.shape\ndata_deb2 = data_scaled2\n\ny1 = data_scaled[\"Latest Contribution\"]\ny2 = data_debset[\"Fall Donation (Binary)\"]\n\ndata_unscaled1 = data_ohe.drop(\"Latest Contribution\", axis=1)\ndata_unscaled2 = data_ohe2\ny1_unscaled = data_debset[\"Latest Contribution\"]\ny2_unscaled = data[\"Fall Donation (Binary)\"]","d99fb4d3":"#LASSO for variable selection\nfeature_names1 = list(data_deb1.columns.values)\nfeature_names2 = list(data_deb2.columns.values)\n\nclf1 = LassoCV().fit(data_deb1, y1)\nimportance1 = np.abs(clf1.coef_)\n#print(importance1)\nnum_var1 = len(np.where(importance1>0)[0])\n#print(\"number of relevant variables = \", num_var1)\n\nclf2 = LassoCV().fit(data_deb2, y2)\nimportance2 = np.abs(clf2.coef_)\n#num_var2 = len(np.where(importance2>0)[0])\n\nidx_features1 = (-importance1).argsort()[:num_var1] #number of features based on selected from LASSO\nname_features1 = np.array(feature_names1)[idx_features1]\nidx_features2 = (-importance2).argsort()[:10] #top 10 features\nname_features2 = np.array(feature_names2)[idx_features2]\n\nprint()\nprint('Selected features: {}'.format(name_features1), idx_features1)\nprint()\nprint('Selected features: {}'.format(name_features2), idx_features2)\n\n#datasets with only the relevant variables as determined in LASSO\ndata_reg1 = data_deb1[name_features1]\ndata_reg2 = data_deb2[name_features2]","a6d9bf29":"def continuous(X_train, X_test, y_train, y_test):\n\n    #Linear Regression \n    model_LR = LinearRegression()\n    model_LR.fit(X_train, y_train)\n    model_LR.predict(X_test)\n    LR_score = model_LR.score(X_test, y_test)\n    print(\"Linear Regression Model Score= \", LR_score)\n    print()","010e8552":"def classification(X_train2, X_test2, y_train2, y_test2):\n    \n    # Logistic Regression\n    model_LogR = LogisticRegression().fit(X_train2, y_train2)\n    predLog = model_LogR.predict(X_test2)\n    print(\"Logistic Regression Model Score = \", model_LogR.score(X_train2, y_train2))\n    print(classification_report(y_test2, predLog))\n    print()\n    \n    # Gaussian Naive Bayes\n    nb = GaussianNB(priors = None, var_smoothing = 1e-03)\n    y_predNB = nb.fit(X_train2, y_train2).predict(X_test2)\n    NBscore = metrics.accuracy_score(y_test2, y_predNB)\n    print(\"Naive Bayes Accuracy Score = \", NBscore)\n    print(classification_report(y_test2, y_predNB))\n    print()\n    \n    # KNN\n    knn = KNeighborsClassifier()\n    neighborlist = list(range(1,100))\n    parameters = {'n_neighbors':neighborlist}\n    model = GridSearchCV(knn, param_grid=parameters) #gridsearch to find best k value.\n    model.fit(X_train2, y_train2)\n    prediction = model.predict(X_test2)\n    knnscore = metrics.accuracy_score(y_test2, prediction)\n    print(\"KNN Accuracy Score = \", knnscore)\n    print(classification_report(y_test2, prediction))\n    print(\"best param = \", model.best_params_)\n    print()\n    \n    # SVM Linear\n    svm = SVC(kernel=\"linear\")\n    svm.fit(X_train2, y_train2)\n    predictionsSVM = svm.predict(X_test2)\n    SVMScore = metrics.accuracy_score(y_test2, predictionsSVM)\n    print(classification_report(y_test2, predictionsSVM))\n    print(\"SVM Accuracy Score = \", SVMScore)\n    print()\n    \n    # Neural Network\n    mlp = MLPClassifier(hidden_layer_sizes=(20,10),max_iter=500)\n    mlp.fit(X_train2, y_train2)\n    predictionsNN = mlp.predict(X_test2)\n    NNScore = metrics.accuracy_score(y_test2, predictionsNN)\n    print(classification_report(y_test2, predictionsNN))\n    print(\"Neural Network Accuracy Score = \", NNScore)\n    print()\n","5c02f940":"def reducesize (X, y):\n    #random subset of data for classification algorithms\n    datafile = pd.DataFrame(np.column_stack([X, y]))\n    select_data = datafile.sample(15000)   #sample from big dataset\n    x_small = select_data.iloc[:,0:-1]\n    y_small = select_data.iloc[:,-1]\n    return (x_small, y_small)","c9bd51fa":"#split the data version 1 for continuous response using selected data.\n#scaled data\nX_train, X_test, y_train, y_test = split_data(data_reg1, y1)\n#unscaled data\nX_train3, X_test3, y_train3, y_test3 = split_data(data_unscaled1, y1_unscaled)\n\n#get subset of data\nx_small, y_small = reducesize(data_reg2, y2)\nx_big, y_big = reducesize(data_unscaled2, y2_unscaled)\n\n#scaled and reduced data\nX_train2, X_test2, y_train2, y_test2 = split_data(x_small, y_small)\nX_train4, X_test4, y_train4, y_test4 = split_data(x_big, y_big)\n\n#run the algorithms\nprint(\"scaled data\")\ncontinuous(X_train, X_test, y_train, y_test)\nprint(\"unscaled data\")\ncontinuous(X_train3, X_test3, y_train3, y_test3)\n\nprint(\"scaled data\")\nclassification(X_train2, X_test2, y_train2, y_test2)\nprint(\"unscaled data\")\nclassification(X_train4, X_test4, y_train4, y_test4)","ee50c1bc":"data_beautifiedv2 = pd.read_excel('..\/input\/data-beautifiedv2\/data_beautifiedv2.xlsx', index_col=None, header=0) \n#data_beautifiedv2 = pd.read_excel('data_beautifiedv2.xlsx', index_col=None, header=0) \ndata_beautifiedv2.head()","9ed90d4d":"data_beautifiedv2 = pd.get_dummies(data_beautifiedv2, columns=['State', 'Gender'])\ndata_beautifiedv2_nn = data_beautifiedv2.drop(['Donor ID','First Contribution Date','Largest Contribution Date','Latest Contribution Date', '2nd Latest Contribution Date','3rd Latest Contribution Date','4th Latest Contribution Date','5th Latest Contribution Date','6th Latest Contribution Date','7th Latest Contribution Date','8th Latest Contribution Date','9th Latest Contribution Date','10th Latest Contribution Date'], 1)\npd.set_option('display.max_rows', data_beautifiedv2_nn.shape[1]+1)\nprint(data_beautifiedv2_nn.dtypes)","faa27b3e":"a = data_beautifiedv2_nn['Latest Contribution']","1c13fdc1":"hist = a.hist(bins=100)","79bdba28":"data_beautifiedv2_nn['Latest Contribution bin'] = pd.cut(data_beautifiedv2_nn['Latest Contribution'],bins=[0,1,2,3,4,5,8,10,11,12,13,14,15,16,17,18,19,20,30,40,50,60 ,70, 80 , 90,100,120,140,160,180,200 ,250,300,350,400,450,500,600,700,800,900,1000], labels=[0,1,2,3,4,5,8,10,11,12,13,14,15,16,17,18,19,20,30,40,50,60 ,70, 80 , 90,100,120,140,160,180,200 ,250,300,350,400,450,500,600,700,800,900])","275995e4":"data_beautifiedv2_nn = data_beautifiedv2_nn.drop('Latest Contribution', 1)\nx_factor = data_beautifiedv2_nn.drop('Latest Contribution bin', 1)\ny_response = data_beautifiedv2_nn[\"Latest Contribution bin\"]\n\n#training and test index\nmsk = np.random.rand(len(data_beautifiedv2_nn)) < 0.8\nfactor_train_nn = x_factor[msk]\nfactor_test_nn = x_factor[~msk]\nresponse_train_nn = y_response[msk]\nresponse_test_nn = y_response[~msk]","6ed69e63":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes = (2, 10),random_state=1, max_iter=100).fit(factor_train_nn, response_train_nn)\nypred_nn = clf.predict(factor_test_nn)\nprint(\"Accuracy:\",metrics.accuracy_score(response_test_nn, ypred_nn))","5fef68fe":"ypred_nn","8b332b34":"y_response","a7e33ee1":"np.unique(ypred_nn)","c853d632":"#Import knearest neighbors Classifier model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Create KNN Classifier\nknn = KNeighborsClassifier(n_neighbors=20)\n\n#Train the model using the training sets\nknn.fit(factor_train_nn, response_train_nn)\n\n#Predict the response for test dataset\nypred_knn = knn.predict(factor_test_nn)","a34bf68f":"print(\"Accuracy:\",metrics.accuracy_score(response_test_nn, ypred_knn))","27ea9e70":"np.unique(ypred_knn)","989f6c6d":"from sklearn.svm import SVC\n\n#SVM_linear = SVC(kernel = 'linear')\n#SVM_linear.fit(factor_train_nn, response_train_nn)\n#ypred_svm_linear = SVM_linear.predict(factor_test_nn) ","497d8074":"data_beautifiedv2_chang = pd.read_excel('..\/input\/data-beautifiedv2\/data_beautifiedv2.xlsx', index_col=None, header=0) \n#data_beautifiedv2_change = pd.read_excel('data_beautifiedv2.xlsx', index_col=None, header=0) \ndata_beautifiedv2_change.head()\ndata_beautifiedv2_change = pd.get_dummies(data_beautifiedv2_change, columns=['State', 'Gender'])\ndata_beautifiedv2_nn_change = data_beautifiedv2_change.drop(['Donor ID','First Contribution Date','Largest Contribution Date','Latest Contribution Date', '2nd Latest Contribution Date','3rd Latest Contribution Date','4th Latest Contribution Date','5th Latest Contribution Date','6th Latest Contribution Date','7th Latest Contribution Date','8th Latest Contribution Date','9th Latest Contribution Date','10th Latest Contribution Date'], 1)\npd.set_option('display.max_rows', data_beautifiedv2_nn_change.shape[1]+1)\nprint(data_beautifiedv2_nn_change.dtypes)","51aa8fa0":"a = data_beautifiedv2_nn_change['Latest Contribution']","8ee9bcc5":"data_beautifiedv2_nn_change['Latest Contribution bin'] = pd.cut(data_beautifiedv2_nn_change['Latest Contribution'],bins=[0,1,  2,  3,  4,  5,  8, 10, 11, 14, 17, 19, 20, 30, 40, 90,150,200 ,400, 600,900,1000], labels=[0,1,  2,  3,  4,  5,  8, 10, 11, 14, 17, 19, 20, 30, 40, 90,150,200 ,400, 600,900])","e93ddafb":"# neural network smaller brackets\nclf_change = MLPClassifier(hidden_layer_sizes = (10, 100),random_state=1, max_iter=100).fit(factor_train_nn_change, response_train_nn_change)\nypred_nn_change = clf_change.predict(factor_test_nn_change)\nprint(\"Accuracy:\",metrics.accuracy_score(response_test_nn_change, ypred_nn_change))","f0c3c562":"np.unique(ypred_nn_change)","542f0057":"confusion_nn = confusion_matrix(response_test_nn_change, ypred_nn_change)\nprint('Confusion Matrix for KNN, k=5\\n')\nprint(confusion_nn)","f2676fb0":"# neural network larger brackets , huper paramerter tuning\nclf = MLPClassifier(hidden_layer_sizes = (5, 50),random_state=1, max_iter=100).fit(factor_train_nn, response_train_nn)\nypred_nn = clf.predict(factor_test_nn)\nprint(\"Accuracy:\",metrics.accuracy_score(response_test_nn, ypred_nn))","a1d27f21":"np.random.seed(0)\nn = 20  # for 2 random indices\nindex = np.random.choice(data_beautifiedv2.shape[0], n, replace=False)  \ndata_beautifiedv2_random = data_beautifiedv2.ix[index]","0e2ead3c":"index","b88ce1b9":"data_beautifiedv2_random","4b1a6d86":"data_v2 = pd.read_excel('..\/input\/data-cleanedv2\/data_cleanedv2.xlsx', index_col=None, header=0) \ndata_v2.head()"}}