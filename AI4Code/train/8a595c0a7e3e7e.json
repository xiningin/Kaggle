{"cell_type":{"a157dff8":"code","3fca15dc":"code","b234133d":"code","83bd6ce1":"code","1393d830":"code","f0e6c231":"code","549b8fbe":"code","aa4c0872":"code","35fd5ece":"code","8ae29aef":"code","1d2eedc0":"code","5b5eac66":"code","ca024c46":"code","ebfbf85e":"code","1b2a7451":"code","9379a7bc":"code","fab50951":"code","1e4f5df8":"code","64e953a9":"code","e46d3a06":"code","421712d0":"code","06cdeadc":"code","6616d1ec":"code","a02bf635":"code","dba908c9":"code","0a193111":"code","6514212b":"code","7e661147":"code","5a57c3d0":"code","6d83cbb3":"code","215b3b36":"code","b99b263e":"code","920a92ff":"code","277de655":"code","d059d58a":"code","e81946db":"code","0e4c60ac":"code","1e557d24":"code","6c1fbf0d":"code","20e5a9b3":"code","209409a5":"code","466e5c3c":"code","f20fe9f9":"code","45fa7c02":"code","ba304cf9":"code","08ac602a":"markdown","a26fb011":"markdown","3eb122a2":"markdown","b9a4fae1":"markdown","53e88ca4":"markdown","b000935f":"markdown","b99b09b1":"markdown","8a0cf27e":"markdown","62658f8b":"markdown","d76eea84":"markdown","d17398ef":"markdown","220344cd":"markdown","38e6e9f4":"markdown","dc539b6c":"markdown","7c029352":"markdown","9ee2745f":"markdown","8a26683b":"markdown","7d931d68":"markdown","68d391e9":"markdown"},"source":{"a157dff8":"##### IMPORT LIBRARIES\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3fca15dc":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntraindf = train.copy()\ntestdf = test.copy()","b234133d":"train.head()","83bd6ce1":"test.head()","1393d830":"trainshape = (\"Train Data:\",train.shape[0],\"obs, and\", train.shape[1], \"features\" )\nprint(\"\\033[95m {}\\033[00m\" .format(trainshape))\ntestshape = (\"Test Data:\",test.shape[0],\"obs, and\", test.shape[1], \"features\" )\nprint(\"\\033[95m {}\\033[00m\" .format(testshape))","f0e6c231":"# save id \ntrain_id = train[\"Id\"]\ntest_id = test[\"Id\"]\n\n# drop id\ntrain.drop(\"Id\" , axis = 1 , inplace = True)\ntest.drop(\"Id\" , axis = 1 , inplace = True)","549b8fbe":"train.describe().T","aa4c0872":"# Focus Target Variable\n\nsns.distplot(train[\"SalePrice\"] , color = \"g\", bins = 60 , hist_kws={\"alpha\": 0.4});","35fd5ece":"sns.distplot(np.log1p(train[\"SalePrice\"]) , color = \"b\", bins = 60 , hist_kws={\"alpha\": 0.4});","8ae29aef":"corrmatrix = train.corr()\nplt.figure(figsize = (10,6))\ncolumnss = corrmatrix.nlargest(8, \"SalePrice\")[\"SalePrice\"].index\ncm = np.corrcoef(train[columnss].values.T)\nsns.set(font_scale = 1.1)\nhm = sns.heatmap(cm, cbar = True, annot = True, square = True, cmap = \"RdPu\" ,  fmt = \".2f\", annot_kws = {\"size\": 10},\n                 yticklabels = columnss.values, xticklabels = columnss.values)\nplt.show()","1d2eedc0":"f, ax = plt.subplots(figsize = (10, 7))\nsns.boxplot(x = \"OverallQual\", y = \"SalePrice\", data = train);","5b5eac66":"sns.jointplot(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], kind = \"reg\");","ca024c46":"sns.boxplot(x = train[\"GarageCars\"], y = train[\"SalePrice\"]);","ebfbf85e":"train = train.drop(train[(train[\"GrLivArea\"] > 4000) \n                         & (train[\"SalePrice\"] < 200000)].index).reset_index(drop = True)\ntrain = train.drop(train[(train[\"GarageCars\"] > 3) \n                         & (train[\"SalePrice\"] < 300000)].index).reset_index(drop = True)","1b2a7451":"sns.jointplot(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], kind = \"reg\");","9379a7bc":"sns.boxplot(x = train[\"GarageCars\"], y = train[\"SalePrice\"]);","fab50951":"df = pd.concat((train, test)).reset_index(drop = True)\ndf.drop([\"SalePrice\"], axis = 1, inplace = True)\ndf.shape","1e4f5df8":"##Focus missing values\n\ndf.isna().sum().nlargest(35)","64e953a9":"sns.set_style(\"whitegrid\")\nf , ax = plt.subplots(figsize = (12, 6))\nmiss = round(df.isnull().mean()*100,2)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace = True)\nmiss.plot.bar(color = \"b\")\nax.set(title=\"Percent missing data by variables\");","e46d3a06":"some_miss_columns = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n                  \"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"MasVnrType\",\"MSSubClass\"]\n\nfor i in some_miss_columns :\n        df[i].fillna(\"None\" , inplace = True)","421712d0":"df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")","06cdeadc":"some_miss_columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\",\"MSZoning\",\n                      \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\",\"MasVnrArea\"]\nfor i in some_miss_columns2:\n    df[i].fillna(df[i].mode()[0], inplace = True)","6616d1ec":"some_miss_columns3 = [\"GarageYrBlt\", \"GarageArea\", \"GarageCars\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\"]\nfor i in some_miss_columns3 :\n    df[i] = df[i].fillna(0)","a02bf635":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","dba908c9":"##We've filled out all the missing data. Let's control.\ndf.isna().sum().nlargest(3)","0a193111":"## Transform for some variables\nNm = [\"MSSubClass\",\"MoSold\",\"YrSold\"]\nfor col in Nm:\n    df[col] = df[col].astype(str)","6514212b":"label = LabelEncoder()\nencodecolumns = (\"FireplaceQu\",\"BsmtQual\",\"BsmtCond\",\"ExterQual\",\"ExterCond\",\"HeatingQC\",\"GarageQual\",\n                \"GarageCond\",\"PoolQC\",\"KitchenQual\",\"BsmtFinType1\",\"BsmtFinType2\",\"Functional\",\"Fence\",\n                \"BsmtExposure\",\"GarageFinish\",\"LandSlope\",\"LotShape\",\"PavedDrive\",\"Street\",\"Alley\",\n                \"CentralAir\",\"MSSubClass\",\"OverallCond\",\"YrSold\",\"MoSold\")\nfor i in encodecolumns :\n    label.fit(list(df[i].values))\n    df[i] = label.transform(list(df[i].values))","7e661147":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.values\ny[:5]","5a57c3d0":"numeric = df.dtypes[df.dtypes != \"object\"].index\nskewed_var = df[numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\nskewness = pd.DataFrame({\"Skewed Features\" :skewed_var})\nskewness.head()","6d83cbb3":"skewness = skewness[abs(skewness) > 0.75]\nskewed_var2 = skewness.index\nfor i in skewed_var2:\n    df[i] = boxcox1p(df[i], 0.15)\n    df[i] += 1","215b3b36":"df = pd.get_dummies(df)\ndf.head()","b99b263e":"X_train = df[:train.shape[0]]\nX_test = df[train.shape[0]:]","920a92ff":"dff = df.copy()\n##df_standardize = StandardScaler().fit_transform(dff)\n##I didn't standardize it again because the data is already close to the standard.\npca = PCA()\npca_fit = pca.fit_transform(dff)\npca = PCA().fit(dff)\nplt.plot(np.cumsum(pca.explained_variance_ratio_));","277de655":"pca = PCA(n_components = 30)\npca_fit = pca.fit_transform(dff)\npca_df = pd.DataFrame(data = pca_fit)\npca_df.head()","d059d58a":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(X_train.values)\n    rmse = np.sqrt(-cross_val_score(model, X_train.values, y, scoring = \"neg_mean_squared_error\", cv = kf))\n    return(rmse)","e81946db":"model_xgb = xgb.XGBRegressor(colsample_bytree = 0.2, gamma = 0.0 ,\n                             learning_rate = 0.05, max_depth = 6, \n                             min_child_weight = 1.5, n_estimators = 7200,\n                             reg_alpha = 0.9, reg_lambda = 0.6,\n                             subsample = 0.2,seed = 42,\n                             random_state = 7)\n\nmodel_gbm = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05,\n                                   max_depth = 4, max_features = \"sqrt\",\n                                   min_samples_leaf = 15, min_samples_split = 10, \n                                   loss = \"huber\", random_state = 5)","0e4c60ac":"score = rmsle_cv(model_xgb)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_gbm)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1e557d24":"## we need this func\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","6c1fbf0d":"model_xgb.fit(X_train, y)\nxgb_train_pred = model_xgb.predict(X_train)\nxgb_pred = np.expm1(model_xgb.predict(X_test))\nprint(rmsle(y, xgb_train_pred))","20e5a9b3":"xgb_pred[:5]","209409a5":"model_gbm.fit(X_train, y)\ngbm_train_pred = model_gbm.predict(X_train)\ngbm_pred = np.expm1(model_gbm.predict(X_test.values))\nprint(rmsle(y, gbm_train_pred))","466e5c3c":"gbm_pred[:5]","f20fe9f9":"best = (0.5 * xgb_pred ) + (0.5 * gbm_pred)","45fa7c02":"submission = pd.DataFrame({\"Id\": test_id, \"SalePrice\": best})\nsubmission.head(5)","ba304cf9":"submission.to_csv(\"submission.csv\", index = False)","08ac602a":"Log Transform for SalePrice\n\nApply logarithmic transformation to our target variable.Because ML models work better with normal distribution.","a26fb011":"Outliers\n\nCan you see two points at the bottom right on GrLivArea. Yes ! It's outliers !\n\nCar garages result in less Sale Price? That doesn't make much sense.\n\nWe need to remove outliers.","3eb122a2":"Let's create the best 8 correlation with heatmap.","b9a4fae1":"Now let's look at the distribution of the variable with the 3 highest correlations.","53e88ca4":"Next step is to concanete train and test data for some cleaning operations.","b000935f":"Next step is dummy variables !\n\nIn statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.","b99b09b1":"Introduction\n\nFor this competiton, we are given a data set of 1460 homes, each with a few dozen features of types: float, integer, and categorical. We are tasked with building a regression model to estimate a home's sale price. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\nWhat I will show you on this notebook?\n\nUnderstanding the data\nExploratory Data Analysis\nData Preprocessing\nPCA Trial\nGBM and XGBoost Models\nSubmission","8a0cf27e":"Label Encoder\n\nConvert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.","62658f8b":"**SUBMISSION**","d76eea84":"Fixing \"Skewed\" features\n\nFix all of the skewed data to be more normal so that our models will be more accurate when making predictions.","d17398ef":"GBM (Gradient Boosting Machines)","220344cd":"I applied a PCA.\n\nPCA (Principal component analysis)\n\nPCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. Lets try it.\n\nNote : First standardize the data before using PCA.","38e6e9f4":"**Filling missing values**\n\nFor a few columns there is lots of NaN entries.\n\nHowever, reading the data description we find this is not missing data:\n\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.\n\nNow, lets filling NA values ","dc539b6c":"**Now, we will predict models ! Firstly start Cross-validation with k-folds**","7c029352":"**XGBoost**","9ee2745f":"With about 30 variables, we can explain 90% of the variance in the dataset.How do we do that ?","8a26683b":"As you can see above, the target variable SalePrice is not distributed normally.\n\nThis can reduce the performance of the ML regression models because some of them assume normal distribution.\n\nTherfore we need to log transform.","7d931d68":"**Victoria Maslova. July 2021**","68d391e9":"Apply a Box Cox transformation\n\n\nA Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn\u2019t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n"}}