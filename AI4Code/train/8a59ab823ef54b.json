{"cell_type":{"769826e2":"code","09afd6c0":"code","d076247c":"code","2170081b":"code","3c80fd22":"code","2188b4c2":"code","f53e3fcd":"code","d81013ef":"code","085d9c42":"code","f57abda9":"code","48b52816":"code","f7686344":"code","96323800":"code","cd3fd2f2":"code","bc42222d":"code","ac04c1e7":"code","45a12c67":"code","a57c70fc":"code","7b970b2f":"markdown","547a81a0":"markdown","95c0a1c9":"markdown","9971e9db":"markdown","4eb52b9b":"markdown","c4dd265c":"markdown","1e369c08":"markdown","ac66c4bc":"markdown","b105ce25":"markdown","6db8cf20":"markdown","4b1d9267":"markdown","3a0746fe":"markdown"},"source":{"769826e2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","09afd6c0":"# Read csv file\n\ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf.head()","d076247c":"df.info()","2170081b":"# Correlation\n\nCor_heat = df.corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(Cor_heat, cmap = \"RdBu_r\", vmax=0.9, square=True)","3c80fd22":"## Lets see what most important features we have\n\nIF = Cor_heat['price'].sort_values(ascending=False).head(10).to_frame()\nIF.head(5)","2188b4c2":"# Split the Data\n\nFeature_data = df.drop(['price','date', 'id'], axis=1)\nTarget_data = df['price']","f53e3fcd":"# Check Target Data Skewness\n\nprint('Skew Value : ' + str(Target_data.skew()))\nsns.distplot(Target_data)","d81013ef":"# transform target\n\nfrom scipy.special import inv_boxcox\nfrom scipy.stats import boxcox","085d9c42":"f = plt.figure(figsize=(16,16))\n\n# log 1 Transform\nax = f.add_subplot(221)\nL1p = np.log1p(Target_data)\nsns.distplot(L1p,color='b',ax=ax)\nax.set_title('skew value Log 1 transform: ' + str(np.log1p(Target_data).skew()))\n\n# Square Log Transform\nax = f.add_subplot(222)\nSRT = np.sqrt(Target_data)\nsns.distplot(SRT,color='c',ax=ax)\nax.set_title('Skew Value Square Transform: ' + str(np.sqrt(Target_data).skew()))\n\n# Log Transform\nax = f.add_subplot(223)\nLT = np.log(Target_data)\nsns.distplot(LT, color='r',ax=ax)\nax.set_title('Skew value Log Transform: ' + str(np.log(Target_data).skew()))\n\n# Box Cox Transform\nax = f.add_subplot(224)\nBCT,fitted_lambda = boxcox(Target_data,lmbda=None)\nsns.distplot(BCT,color='g',ax=ax)\nax.set_title('Skew Value Box Cox Transform: ' + str(pd.Series(BCT).skew()))","f57abda9":"Target_data = BCT","48b52816":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","f7686344":"# I'm using 5 fold in this cross val score\n\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, Feature_data, Target_data, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","96323800":"# Initiate Models\n## Gradient Boosting\n\nGB = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","cd3fd2f2":"## XGBoost\n\nXGB = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=220,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","bc42222d":"## LightGBM\n\nLGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=320,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","ac04c1e7":"model = [GB, XGB, LGB]\n\nfor x in model:    \n    score = rmse_cv(x).mean()\n    print('RMSE Score with ' + str(x.__class__.__name__) + ' : ' + str(score))","45a12c67":"Predicted_data = []\n\nfor x in model:\n    result = cross_val_predict(x, Feature_data, Target_data, cv=5)\n    Predicted_data.append(result)","a57c70fc":"f = plt.figure(figsize=(15,5))\n\n# Gradient Boosting\nax = f.add_subplot(131)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[0], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with Gradient Boosting')\n\n# XGBoost\nax = f.add_subplot(132)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[1], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with XGBoost')\n\n# LightGBM\nax = f.add_subplot(133)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[2], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with LightGBM')\n\nplt.show()","7b970b2f":"## Building Models\n\nIt's time to make the models","547a81a0":"## Introduction\n\nHai Kagglers, today i'm going to show you simple tutorial on how to handle regression problem while producing high accuracy models as well. Models i;m using today is Gradient Boosting, XGBoost, and Light GBM. Let's do it.","95c0a1c9":"From the data above we know that this dataset contain 21613 row with no missing value. The target column is price and the rest is the feature. This time i'm not gonna use date and id column, so let's drop it.","9971e9db":"Next is we need to check the target column skewness to make sure we can get a high accuration models.","4eb52b9b":"Gradient Boosting own the lowest score with 0.0084, this make Gradient Boosting the best model.\nLet's perform comparison to see the distribution between actual and predicted data.","c4dd265c":"4.02 ??\n\nThat's teribble, let's use a few transformation technique to make sure we can best skew value (the nearest value to 0).","1e369c08":"## Evaluation\n\nLet's see which models has the lowest RMSE value","ac66c4bc":"## End\n\nThat is how we handle regression problem, thank you.","b105ce25":"## Import Modules","6db8cf20":"## Quick Look\n\nLet's take a look at our dataset","4b1d9267":"The best skew value obtain by applying box cox transformation, let's use this as our target column.","3a0746fe":"We can see here Gradient Boosting distibution is almost alike, which is good. on the other side XGBoost perform quite bad this time."}}