{"cell_type":{"aaa35956":"code","02790ca5":"code","9f1af53b":"code","40d19d60":"code","e1e30a03":"code","faa918b6":"code","677dc074":"code","94d07a82":"code","457c54da":"code","cf5347bb":"code","ff9b70d8":"code","f0512391":"code","a420e699":"code","025a3da5":"code","69745eac":"code","9e683157":"code","225cde51":"code","ee667d0b":"code","40cd9dab":"code","265f2acc":"code","10607caf":"code","0b93eee1":"markdown","33ed9bc6":"markdown","85cf6ba8":"markdown","0168009c":"markdown","accb468d":"markdown","21c12fde":"markdown","b820a752":"markdown","10f48e33":"markdown","4c7a46d9":"markdown","68965125":"markdown","396fe49e":"markdown","41ebc61e":"markdown","490a437f":"markdown","9b964740":"markdown","36b9f998":"markdown","7fc9feb9":"markdown","23873bb6":"markdown","ffab5207":"markdown","dde573db":"markdown","dfc641a2":"markdown","5daa6712":"markdown"},"source":{"aaa35956":"import pandas as pd\n# for data exploration\nimport numpy as np\n#for numerical caluation handling multidimensional array\nimport matplotlib.pyplot as plt\nimport seaborn as  sns\n# for data visualisation\nimport keras\nimport tensorflow\n# for CNN model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense,Dropout\nfrom keras.utils.np_utils import to_categorical\nfrom keras.datasets import mnist","02790ca5":"df_train=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test=pd.read_csv('..\/input\/digit-recognizer\/test.csv')","9f1af53b":"print(type(df_test))\nprint(type(df_train))\ndf_test.isnull().sum()\ndf_train.isnull().sum()\n\n# from the above we cannot predict what actually it show\n\n# for simplicity\ndf_test.isnull().sum().sum()\ndf_train.isnull().sum().sum()","40d19d60":"df_train.head()","e1e30a03":"\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# target corresponding to 28x28 pixel\ntarget=df_train.iloc[:,1:]\nlabel=df_train.iloc[:,0]\n\n\n#Here we have to convert dataframe to numpy array for numerical computation  \ntarget=np.array(target)\ndf_test=np.array(df_test)\n\n\nprint(target.shape)\nprint(df_test.shape)\n\n\n# reshaping the dataset\ntarget=target.reshape(42000,28,28)\ndf_test=df_test.reshape(28000,28,28)\n\n#max value of each pixel can be 255 ,it difficult for cpu to process such large value\n# Reduce each pixel from 0 to 1\ntarget=target\/target.max()\ndf_test=df_test\/df_test.max()\n\n\nprint(target.shape)\nprint(df_test.shape)\n","faa918b6":"import matplotlib.pyplot as plt\nimg=df_test[2]\nplt.imshow(img,cmap='gray')\nprint(type(img))\n\n# No change in imgae after normlization","677dc074":"# why we convert it into numerical\n# for categorical_crossentropy we have to convert into categorical data \n# for sparse_categorical_crossentropy we make remain unchage\n# But I strongly recommend you to use categorical_crossentropy\n\n\n\ntarget=target.reshape(42000,28,28,1)\ndf_test=df_test.reshape(28000,28,28,1)\n\nlabel=np.array(label)\nlabel_cat=to_categorical(label,10)\n\n\nprint(label.shape)\nprint(label_cat.shape)\nprint(target.shape)\nprint(df_test.shape)","94d07a82":"from keras.layers.normalization import BatchNormalization\n\nclassifier = Sequential()\n\nclassifier.add(Convolution2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(BatchNormalization())\nclassifier.add(Convolution2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nclassifier.add(BatchNormalization())\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))","457c54da":"classifier.add(Convolution2D(filters =256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nclassifier.add(BatchNormalization())\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Convolution2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nclassifier.add(BatchNormalization())\nclassifier.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nclassifier.add(Dropout(0.25))","cf5347bb":"classifier.add(Flatten())\nclassifier.add(Dense(256, activation = \"relu\"))\nclassifier.add(Dropout(0.3))\nclassifier.add(Dense(10, activation = \"softmax\"))","ff9b70d8":"classifier.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])","f0512391":"classifier.fit(target,label_cat,epochs=50)","a420e699":"results=classifier.predict_classes(df_test)\nprint(results)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n#submission.to_csv(\"F:\\\\PYTHON PROGRAM\\\\JaiShreeRammnist11.csv\",index=False)\nsubmission.to_csv(\"submission.csv\",index=False,header=True)","025a3da5":"submission.head()","69745eac":"(X_train,y_train),(X_test,y_test)=mnist.load_data()\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(type(X_train))\n\n\n\nimport matplotlib.pyplot as plt\nimg=X_train[0]\nplt.imshow(img,cmap='gray')\nprint(type(img))","9e683157":"print(y_train)\nX_train=X_train\/X_train.max()\nX_test=X_test\/X_test.max()\nimport matplotlib.pyplot as plt\nimg=X_train[0]\nplt.imshow(img,cmap='gray')\nprint(type(img))","225cde51":"X_train=X_train.reshape(60000,28,28,1)\nX_test=X_test.reshape(10000,28,28,1)\n\ny_cat_train=to_categorical(y_train,10)\ny_cat_test=to_categorical(y_test,10)","ee667d0b":"classifier.fit(X_train,y_cat_train,epochs=25)","40cd9dab":"results=classifier.predict_classes(df_test)\nprint(results)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n#submission.to_csv(\"F:\\\\PYTHON PROGRAM\\\\JaiShreeRammnist11.csv\",index=False)\nsubmission.to_csv(\"submission1.csv\",index=False,header=True)","265f2acc":"classifier.fit(X_test,y_cat_test,epochs=25)","10607caf":"results=classifier.predict_classes(df_test)\nprint(results)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n#submission.to_csv(\"F:\\\\PYTHON PROGRAM\\\\JaiShreeRammnist11.csv\",index=False)\nsubmission.to_csv(\"submission2.csv\",index=False,header=True)","0b93eee1":"fitting train and test data ","33ed9bc6":"# Lets Try Another Way","85cf6ba8":"A Set of Fully Connected Layers\nA fully connected network is our RegularNet where each parameter is linked to one another to determine the true relation and effect of each parameter on the labels. Since our time-space complexity is vastly reduced thanks to convolution and pooling layers, we can construct a fully connected network in the end to classify our images. A set of fully connected layers looks like this:\n![image.png](attachment:image.png)","0168009c":"Building the Convolutional Neural Network\n\n\n\nWe will build our model by using high level Keras API which uses either TensorFlow or Theano on the backend. I would like to mention that there are several high level TensorFlow APIs such as Layers, Keras, and Estimators which helps us create neural networks with high level knowledge. However, this may lead to confusion since they all varies in their implementation structure. Therefore, if you see completely different codes for the same neural network although they all use tensorflow, this is why. I will use the most straightforward API which is Keras. Therefore, I will import the Sequential Model from Keras and add Conv2D, MaxPooling, Flatten, Dropout, and Dense layers. I have already talked about Conv2D, Maxpooling, and Dense layers. In addition, Dropout layers fight with the overfitting by disregarding some of the neurons while training while Flatten layers flatten 2D arrays to 1D array before building the fully connected layers.\n","accb468d":"Downloading the Mnist Data\nThe MNIST dataset is one of the most common datasets used for image classification and accessible from many different sources. In fact, even Tensorflow and Keras allow us to import and download the MNIST dataset directly from their API. Therefore, I will start with the following two lines to import tensorflow and MNIST dataset under the Keras API.","21c12fde":"Now collected more Image by change its dimension","b820a752":"Reshaping and Normalizing the Images\n\n\n\nTo be able to use the dataset in Keras API, we need 4-dims numpy arrays. However, as we see above, our array is 3-dims. In addition, we must normalize our data as it is always required in neural network models. We can achieve this by dividing the RGB codes to 255 (which is the maximum RGB code minus the minimum RGB code). This can be done with the following code:\n","10f48e33":"***Introdutcion**\n\n\nThis is a Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. I choosed to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will get the data (handwritten digits images) from kaggle  then i will focus on the CNN modeling and evaluation.\n\nI achieved 99.828% of accuracy with this CNN trained i collected the dataset from kaggle nearly (42000 images) and Keras mnist about (70000 images) and also more images you can generate using Image Data Generators by rotating images or changing the dimension of image \n\n\n\n***Before you developing you cnn model first you check you tensorflow-gpu is install on your system\n \n *Required module*\n1.  numpy\n2. pandas\n3. opencv\n4. keras\n5. tensorflow-gpu \n6. tensorflow\n \n\n*** Extra Knowledge  ***\n1. How We Resolve Python Error \n* from behalf of my experience it was one of difficult task to reslove the error in python :  You can follow these point to solve error\n1. Error last line describe the type of error .copy and paste  that line in google chrome first two link is either stackoverflow or can be  github about 95% chances these two were very helpfull to resolve the error\n\n2. Second type of error can occur either due to module found error or something you have not install some package\n           ****for this i recommended you to refer documentation of that module or you tube video****\n                                                                               ","4c7a46d9":"Pooling Layer\nWhen constructing CNNs, it is common to insert pooling layers after each convolution layer to reduce the spatial size of the representation to reduce the parameter counts which reduces the computational complexity. In addition, pooling layers also helps with the overfitting problem. Basically we select a pooling size to reduce the amount of the parameters by selecting the maximum, average, or sum values inside these pixels. Max Pooling, one of the most common pooling techniques, may be demonstrated as follows:\n![image.png](attachment:image.png)","68965125":"****First Import Required Library****","396fe49e":"**\nWhen you start learning deep learning with neural network, you realize that one of the most powerful supervised deep learning techniques is the Convolutional Neural Networks (abbreviated as \u201cCNN\u201d). The final structure of a CNN is actually very similar to Regular Neural Networks (RegularNets) where there are neurons with weights and biases. In addition, just like in RegularNets, we use a loss function (e.g. crossentropy or softmax) and an optimizer (e.g. adam optimizer) in CNNs [2]. Additionally though, in CNNs, there are also Convolutional Layers, Pooling Layers, and Flatten Layers. CNNs are mainly used for image classification although you may find other application areas such as natural language processing.**\n\n\n\n**Why Convolutional Neural Networks**\n\n\n**The main structural feature of RegularNets is that all the neurons are connected to each other. For example, when we have images with 28 by 28 pixels with only greyscale, we will end up having 784 (28 x 28 x 1) neurons in a layer which seems manageable. However, most images have way more pixels and they are not grey-scaled. Therefore, assuming that we have a set of color images in 4K Ultra HD, we will have 26,542,080 (4096 x 2160 x 3) different neurons connected to each other in the first layer which is not really manageable. Therefore, we can say that RegularNets are not scalable for image classification. However, especially when it comes to images, there seems to be little correlation or relation between two individual pixels unless they are close to each other. This leads to the idea of Convolutional Layers and Pooling Layers.**","41ebc61e":"**Now Its Time To collect little much more data**","490a437f":"**Visualization of data**","9b964740":"> Convolutional Layers\nConvolutional layer is the very first layer where we extract features from the images in our datasets. Due to the fact that pixels are only related with the adjacent and close pixels, convolution allows us to preserve the relationship between different parts of an image. Convolution is basically filtering the image with a smaller pixel filter to decrease the size of the image without loosing the relationship between pixels. When we apply convolution to 5x5 image by using a 3x3 filter with 1x1 stride (1 pixel shift at each step). We will end up having a 3x3 output (64% decrease in complexity).\n![image.png](attachment:image.png)","36b9f998":"**What Notebook **\n1. Complete description about Cnn\n \n3. Introduction\n4. Data Prepration\n5. CNN\n6. Evalute The Model\n7. Predication and Submission\n8. Tunning the Parameter\n9. for sake of simplicity I am not showing output of an ","7fc9feb9":"**I Know This is Wrong Method of collecting data from keras model but this is also an research on data**\n**If you  want to escape this part you can do**","23873bb6":"Layers in a CNN\n> **We are capable of using many different layers in a convolutional neural network. \nHowever, convolution, pooling, and fully connect layers are the most important ones. \nTherefore, I will quickly introduce these layers before implementing them.**\n","ffab5207":"Compiling and Fitting the Model\n\n\n\n\nWith the above code, we created an non-optimized empty CNN. Now it is time to set an optimizer with a given loss function which uses a metric. Then, we can fit the model by using our train data. We will use the following code for these tasks:","dde573db":"Now that you have some idea about the individual layers that we will use, I think it is time to share an overview look of a complete convolutional neural network.\n![image.png](attachment:image.png)\nAnd now that you have an idea of convolutional neural network that you can build for image classification, we can get the most cliche dataset for classification: MNIST dataset, which stands for Modified National Institute of Standards and Technology database. It is a large database of handwritten digits that is commonly used for training various image processing systems.\n","dfc641a2":"First By Normal fit method method","5daa6712":"# Check for Missing Value"}}