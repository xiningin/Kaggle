{"cell_type":{"6d9626e0":"code","8c068246":"code","720069a1":"code","cc103b55":"code","b80cb8f9":"code","acecff52":"code","29f6eec7":"code","c339e3d7":"code","6a0468ed":"code","388ad790":"code","44bc94ad":"code","233301c7":"code","0a06e4b0":"code","fadf040d":"code","76939338":"code","f72cbbcb":"code","c086fdbe":"code","5ffc6ff1":"code","599bba50":"code","eba09309":"code","90ccb960":"code","16727ac3":"code","e6f21198":"code","83d73556":"code","662fd22a":"code","709f35a8":"code","23097a0c":"code","961f3c4c":"code","5257cbe6":"code","586ac9df":"code","7fb3a81e":"code","947ab79f":"code","ae7dac30":"code","49f53e22":"code","a2a81b31":"code","ca5b6a0a":"code","d594c730":"code","92257edd":"code","8cbed6ee":"code","130f92b5":"code","74a10c43":"code","ea22420e":"code","f2a77021":"code","49fa3845":"code","6db4f2de":"code","97aca981":"code","cc1fa333":"code","13753b42":"code","741aff51":"code","962f08ee":"code","f7f10fa2":"code","d0a992e6":"code","c8fbd9d8":"code","c11fb935":"code","38d8676d":"code","04437d7a":"code","a1353b4d":"code","8c898aa8":"code","bbde22a5":"markdown","37863a3e":"markdown","d3cbbae6":"markdown","8e3ec4dc":"markdown","1beb1dd1":"markdown","bde8e168":"markdown","107748bd":"markdown","99ff0685":"markdown","1282a4a7":"markdown","f2f4106b":"markdown","25db08e9":"markdown","784d7570":"markdown","61ba5482":"markdown","ba4ee87c":"markdown","43e33063":"markdown","7e77057b":"markdown","c15213c4":"markdown","a91db881":"markdown","08d939db":"markdown","74877a56":"markdown","e8ff8889":"markdown","a9aae817":"markdown","5b89caa0":"markdown","84f0c468":"markdown","a754763c":"markdown","84a0dcf4":"markdown","660c08a2":"markdown","11ba3ec1":"markdown","33a7a90d":"markdown","75fe9631":"markdown","2444a5a6":"markdown","e2a26665":"markdown"},"source":{"6d9626e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nss = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","8c068246":"def checkcategories(train, test, fthreshold):\n    #1 Check to see if all unique test values exist in train\n    #If not, convert to NaN\n    \n    trt = pd.concat((train,test),axis=0)\n    trtshape = trt.shape\n    alt = np.zeros(trtshape[0])\n    \n    tru = train.unique()\n    teu = test.unique()\n    \n    for values in teu:\n        if (values in tru) == False:\n            trt[(trt==values)==True] = 'zzztempzzz'\n    \n    #2 Convert all NaN to its own category\n    \n    trt[(trt.isnull())==True] = 'zzztempzzz'\n    \n    #3 Do frequency count, provide threshold that converts dummy variables less than threshold to the NaN category in step 2, remove columns of dummy variables that are less than threshold\n    \n    tdict = trt.value_counts().to_dict()\n    \n    for values in tdict:\n        if tdict[values] < fthreshold:\n            trt[(trt==values)==True] = 'zzztempzzz'\n    \n    #3 Get Dummies\n    \n    dummies = pd.get_dummies(trt)\n    \n    #4 Split back to train,test\n    \n    train = dummies.iloc[0:train.shape[0],:]\n    test = dummies.iloc[train.shape[0]:train.shape[0]+test.shape[0]+1,:]\n    \n    return train,test\n\ntraintemp,testtemp = checkcategories(train.loc[:,'Fence'],test.loc[:,'Fence'],100)","720069a1":"catcolumns = [1,2,5,6,7,8,9,10,11,12,13,14,15,16,21,22,23,24,25,26,27,28,29,30,31,32,33,35,39,40,41,42,47,48,49,50,52,53,55,56,57,58,59,60,61,63,64,65,72,73,74,77,78,79]\nnumcolumns = [3,4,17,18,19,20,34,36,37,38,43,44,45,46,51,54,62,66,67,68,69,70,71,75,76]","cc103b55":"for count,values in enumerate(catcolumns):\n    traintemp,testtemp = checkcategories(train.iloc[:,values],test.iloc[:,values],100)\n    if count == 0:\n        traincat = traintemp.to_numpy()\n        testcat = testtemp.to_numpy()\n    else:\n        traincat = np.concatenate((traincat,traintemp.to_numpy()),axis=1)\n        testcat = np.concatenate((testcat,testtemp.to_numpy()),axis=1)","b80cb8f9":"for count,values in enumerate(numcolumns):\n    traintemp = train.iloc[:,values]\n    testtemp = test.iloc[:,values]\n    \n    mx = max(traintemp.max(),testtemp.max())\n    mi = min(traintemp.min(),testtemp.min())\n    \n    traintemp = (traintemp - mi)\/(mx-mi)\n    testtemp = (testtemp - mi)\/(mx-mi)\n    \n    a = traintemp.to_numpy()\n    b = testtemp.to_numpy()\n    \n    temp = np.concatenate((a.reshape(-1,1),b.reshape(-1,1)))\n    s = skew(temp)\n    \n    if s > .75:\n        temp = np.log1p(temp)\n    \n    traintemp = temp[0:traintemp.shape[0]]\n    testtemp = temp[traintemp.shape[0]:traintemp.shape[0]+testtemp.shape[0]]\n    \n    if count == 0:\n        trainnum = traintemp\n        testnum = testtemp\n    else:\n        trainnum = np.concatenate([trainnum,traintemp],axis=1)\n        testnum = np.concatenate([testnum,testtemp],axis=1)","acecff52":"trainall = np.concatenate((traincat,trainnum),axis=1)\ntestall = np.concatenate((testcat,testnum),axis=1)\ntrainall[np.isnan(trainall)==True]=.5\ntestall[np.isnan(testall)==True]=.5","29f6eec7":"X_train, X_test, y_train, y_test = train_test_split(trainall,train.iloc[:,80].to_numpy(), test_size=0.2, random_state=42)","c339e3d7":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg.fit(X_train, y_train)\npreds = xg_reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds)-np.log(y_test)))))","6a0468ed":"from sklearn import linear_model\nreg = linear_model.Ridge(alpha=0.01, fit_intercept=True, normalize=True, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\npreds2[preds2<0]=np.mean(preds2)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","388ad790":"from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","44bc94ad":"from sklearn import linear_model\nreg = linear_model.Lasso(alpha=0.001, max_iter = 50000)\nreg.fit(X_train, y_train)\npreds3 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","233301c7":"from sklearn import linear_model\nreg = linear_model.ElasticNet(alpha=0.4, l1_ratio = .5, max_iter = 50000)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","0a06e4b0":"from sklearn import linear_model\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nreg = linear_model.LassoLars(alpha=0.5)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","fadf040d":"from sklearn import linear_model\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nreg = linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","76939338":"from sklearn import linear_model\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nreg = linear_model.BayesianRidge()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","f72cbbcb":"from warnings import filterwarnings\nfilterwarnings('ignore')\nreg = linear_model.SGDRegressor()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","c086fdbe":"from warnings import filterwarnings\nfrom sklearn import kernel_ridge\nfilterwarnings('ignore')\nreg = kernel_ridge.KernelRidge(alpha=.001, kernel='poly', gamma=None, degree=3, coef0=1, kernel_params=None)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","5ffc6ff1":"from sklearn import svm\nfilterwarnings('ignore')\n#C=1000000\nreg = svm.SVR(kernel='poly', C=100000, gamma='auto', degree=7, epsilon=.001,\n               coef0=1,shrinking=True,)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","599bba50":"from sklearn import svm\nfilterwarnings('ignore')\n#C=1000000\nreg = svm.NuSVR(kernel='linear', C=10000, gamma='auto', degree=7,\n               coef0=1,shrinking=True,)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","eba09309":"from sklearn import neighbors\nfilterwarnings('ignore')\nreg = neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","90ccb960":"from sklearn import neighbors\nfilterwarnings('ignore')\nreg = neighbors.RadiusNeighborsRegressor()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","16727ac3":"from sklearn import gaussian_process\nfilterwarnings('ignore')\nreg = gaussian_process.GaussianProcessRegressor(kernel=None, alpha=.0001, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","e6f21198":"from sklearn import cross_decomposition\nfilterwarnings('ignore')\nreg = cross_decomposition.PLSRegression(n_components=2, scale=False, max_iter=5000, tol=.01, copy=True)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","83d73556":"from sklearn import cross_decomposition\nfilterwarnings('ignore')\nreg = cross_decomposition.PLSCanonical(n_components=2, scale=False, algorithm='svd', max_iter=5000, tol=.1, copy=True)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","662fd22a":"from sklearn.naive_bayes import GaussianNB\nfilterwarnings('ignore')\nreg = GaussianNB(priors=None, var_smoothing=.1)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","709f35a8":"from sklearn.naive_bayes import MultinomialNB\nfilterwarnings('ignore')\nreg = MultinomialNB(alpha=.01, fit_prior=True, class_prior=None)\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","23097a0c":"from sklearn.naive_bayes import ComplementNB\nfilterwarnings('ignore')\nreg = ComplementNB()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","961f3c4c":"from sklearn.naive_bayes import BernoulliNB\nfilterwarnings('ignore')\nreg = BernoulliNB()\nreg.fit(X_train, y_train)\npreds2 = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","5257cbe6":"from sklearn import tree\nclf = tree.DecisionTreeRegressor(max_depth=8)\nclf = clf.fit(X_train, y_train)\npreds2 = clf.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","586ac9df":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\nreg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=100)\nereg = VotingRegressor(estimators=[('gb', xg_reg), ('rf', reg2)])\nereg = ereg.fit(X_train, y_train)\npreds2 = ereg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","7fb3a81e":"trainall.shape","947ab79f":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(.01)\ntemp = sel.fit_transform(trainall)\ntemp.shape","ae7dac30":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ntemp = SelectKBest(chi2, k=100).fit_transform(trainall, train.iloc[:,80])\ntemp.shape","49f53e22":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(trainall, train.iloc[:,80])\nclf.feature_importances_  \nmodel = SelectFromModel(clf, prefit=True)\ntemp = model.transform(trainall)\ntemp.shape","a2a81b31":"X_train, X_test, y_train, y_test = train_test_split(trainall,train.iloc[:,80].to_numpy(), test_size=0.2, random_state=42)","ca5b6a0a":"from sklearn import neural_network\nreg = neural_network.MLPRegressor(hidden_layer_sizes=(100,100,100), activation='relu', solver='lbfgs', alpha=.001, \n                                          batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n                                          power_t=0.5, max_iter=600, shuffle=True)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds)-np.log(y_test)))))","d594c730":"# from sklearn.model_selection import GridSearchCV\n# parameters = {'solver': ['lbfgs','adam'], 'random_state':[0,1]}\n# clf_grid = GridSearchCV(neural_network.MLPRegressor(hidden_layer_sizes=(100,100,100), activation='relu', solver='lbfgs', alpha=.001, \n#                                           batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n#                                           power_t=0.5, max_iter=600, shuffle=True), parameters, n_jobs=-1)\n# clf_grid.fit(X_train,y_train)\n# print(\"-----------------Original Features--------------------\")\n# print(\"Best score: %0.4f\" % clf_grid.best_score_)\n# print(\"Using the following parameters:\")\n# print(clf_grid.best_params_)","92257edd":"# from sklearn.model_selection import GridSearchCV\n# parameters = {'colsample_bytree': [.4,.5,.6,.7], 'learning_rate':[.1,.2,.3], 'n_estimators':[200,400,600],'reg_lambda':[.4,.5,.7]}\n# clf_grid = GridSearchCV(xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n#          max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle'), parameters, n_jobs=-1)\n# clf_grid.fit(X_train,y_train)\n# print(\"-----------------Original Features--------------------\")\n# print(\"Best score: %0.4f\" % clf_grid.best_score_)\n# print(\"Using the following parameters:\")\n# print(clf_grid.best_params_)","8cbed6ee":"xg_reg1 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .5, learning_rate = .1,\n         max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg1.fit(X_train, y_train)\npreds1 = xg_reg1.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds1)-np.log(y_test)))))","130f92b5":"xg_reg2 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .5, learning_rate = .1,\n         max_depth = 3, reg_lambda = .6, n_estimators = 700, evaluation_metric = 'rmsle')\nxg_reg2.fit(X_train, y_train)\npreds2 = xg_reg1.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","74a10c43":"x = np.linspace(0,800000,100)","ea22420e":"import matplotlib.pyplot as plt\n\nplt.scatter(y_test,preds1)\nplt.plot(x,x)","f2a77021":"plt.scatter(y_test,preds2)\nplt.plot(x,x)","49fa3845":"plt.scatter(y_test,preds3)\nplt.plot(x,x)","6db4f2de":"salesprice = train.iloc[:,80]","97aca981":"np.median(salesprice)","cc1fa333":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=600)\nclf = clf.fit(trainall, train.iloc[:,80])\nclf.feature_importances_  \nmodel = SelectFromModel(clf, prefit=True,threshold='median')\ntemp = model.transform(trainall)\ntemp.shape","13753b42":"X_train, X_test, y_train, y_test = train_test_split(trainall,train.iloc[:,80].to_numpy(), test_size=0.2, random_state=42)","741aff51":"xg_reg1 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg1.fit(X_train, y_train)\npreds_test = xg_reg1.predict(X_test)\npreds_train = xg_reg1.predict(X_train)\nprint(math.sqrt(np.mean(np.square(np.log(preds_test)-np.log(y_test)))))\n\na = np.zeros(80)\nthresh = 210000\nfor i in range(0,80):\n    print(i)\n    thresh = 100000 + 2500*i\n    trainall1 = X_train[preds_train < thresh]\n    trainall2 = X_train[preds_train >= thresh]\n    testall1 = X_test[preds_test < thresh]\n    testall2 = X_test[preds_test >= thresh]\n    y_train1 = y_train[preds_train < thresh]\n    y_train2 = y_train[preds_train >= thresh]\n    y_test1 = y_test[preds_test < thresh]\n    y_test2 = y_test[preds_test >= thresh]\n\n    xg_reg2 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n             max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\n    xg_reg2.fit(trainall1, y_train1)\n    preds1 = xg_reg2.predict(testall1)\n    #print(math.sqrt(np.mean(np.square(np.log(preds1)-np.log(y_test1)))))\n\n    xg_reg3 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n             max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\n    xg_reg3.fit(trainall2, y_train2)\n    preds2 = xg_reg3.predict(testall2)\n    #print(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test2)))))\n\n    predsall = np.concatenate((preds1, preds2),axis=0)\n    y_testall = np.concatenate((y_test1,y_test2),axis=0)\n    print(math.sqrt(np.mean(np.square(np.log(predsall)-np.log(y_testall)))))\n    a[i] = math.sqrt(np.mean(np.square(np.log(predsall)-np.log(y_testall))))","962f08ee":"b = np.arange(1,81)\nb\nplt.plot(b,a)","f7f10fa2":"b = np.arange(1,81)\nb\nplt.plot(b,a)","d0a992e6":"plt.scatter(predsall,y_test)\nplt.plot(x,x)","c8fbd9d8":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.linear_model import LinearRegression\nimport mlxtend\nfrom sklearn import ensemble\n\nxg_reg1 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg1.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg2 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = 1, n_estimators = 600, evaluation_metric = 'rmsle')\nxg_reg2.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg3 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .9, n_estimators = 400, evaluation_metric = 'rmsle')\nxg_reg3.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg4 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .3, learning_rate = .1,\n         max_depth = 3, reg_lambda = .5, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg4.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg5 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .5, learning_rate = .1,\n         max_depth = 3, reg_lambda = .3, n_estimators = 800, evaluation_metric = 'rmsle')\nxg_reg5.fit(trainall, train.iloc[:,80].to_numpy())\n\nereg = ensemble.VotingRegressor(estimators=[('gb', xg_reg1), ('rf', xg_reg2), ('rf1', xg_reg3), ('rf2', xg_reg4), ('rf3', xg_reg5)])\nereg = ereg.fit(X_train, y_train)\npreds2 = ereg.predict(X_test)\nprint(math.sqrt(np.mean(np.square(np.log(preds2)-np.log(y_test)))))","c11fb935":"xg_reg1 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .8, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg1.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg2 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = 1, n_estimators = 600, evaluation_metric = 'rmsle')\nxg_reg2.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg3 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .7, learning_rate = .1,\n         max_depth = 3, reg_lambda = .9, n_estimators = 800, evaluation_metric = 'rmsle')\nxg_reg3.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg4 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .3, learning_rate = .1,\n         max_depth = 3, reg_lambda = .5, n_estimators = 500, evaluation_metric = 'rmsle')\nxg_reg4.fit(trainall, train.iloc[:,80].to_numpy())\n\nxg_reg5 = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = .5, learning_rate = .1,\n         max_depth = 3, reg_lambda = .3, n_estimators = 800, evaluation_metric = 'rmsle')\nxg_reg5.fit(trainall, train.iloc[:,80].to_numpy())\n\nereg = VotingRegressor(estimators=[('gb', xg_reg1), ('rf', xg_reg2), ('rf1', xg_reg3), ('rf2', xg_reg4), ('rf3', xg_reg5)])\n\nereg = ereg.fit(trainall, train.iloc[:,80].to_numpy())\npreds = ereg.predict(testall)","38d8676d":"# from sklearn.inspection import plot_partial_dependence\n# ##plot_partial_dependence(xg_reg1, X_train[:,1].reshape(1,-1), y_train.reshape(1,-1)) \n\n# from sklearn.datasets import make_hastie_10_2\n# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.inspection import plot_partial_dependence\n\n# X, y = make_hastie_10_2(random_state=0)\n# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,random_state=0).fit(trainall, train.iloc[:,80].to_numpy())\n# features = [0, 1, (0, 1)]\n# plot_partial_dependence(clf, trainall, features) ","04437d7a":"y.shape","a1353b4d":"ss.iloc[:,1] = preds","8c898aa8":"ss.to_csv('ss.csv', index=False)","bbde22a5":"Purpose of this function is to one-hot-encode categorical features after NaN values, low frequency categories, and categories that only exist in the testing set have been converted to an 'other' category","37863a3e":"**MLP Regressor**","d3cbbae6":"**Ordinary Least Squares**","8e3ec4dc":"**Lars Lasso**","1beb1dd1":"Gaussian Process Regression","bde8e168":"**Bernoulli NB**","107748bd":"**Complement NB**","99ff0685":"**Decision Tree Regressor**","1282a4a7":"**Elastic Net**","f2f4106b":"**Gaussian NB**","25db08e9":"**Gradient Boosting Regressor**","784d7570":"**I wanted to plot the predicted values to actual values for visualization purposes for three different models to get a sense of what is going on**\n**First two models are trained directly below (reg1, reg2), preds 3 is taken from the ridge regression model trained above**","61ba5482":"**K-Nearest Neighbors**","ba4ee87c":"**Below we have tested three feature selection techniques**","43e33063":"**NuSVR**","7e77057b":"**Bayesian Ridge**","c15213c4":"XGB Regressor","a91db881":"**Best**","08d939db":"**Cross Decomposition**","74877a56":"**We implemented a gridsearch but decided to keep it commented out for right now**","e8ff8889":"**Radius Neighbors**","a9aae817":"**Kernel Ridge**","5b89caa0":"**Ridge Regression**","84f0c468":"**Stochastic Gradient Descent**","a754763c":"**Orthogonal Matching Pursuit**","84a0dcf4":"Features have been classified as categorical and numerical","660c08a2":"**Support Vector Regression**","11ba3ec1":"**Lasso Regression**","33a7a90d":"**Below is a series of models that we have tested**","75fe9631":"**Multinomial NB**","2444a5a6":"A quick observation can be made that all three plots are very similar. The largest deviances come from data points from the tail end of the dependent variable. Hmmm, I wonder if we can do anything about that...","e2a26665":"PLSRegression"}}