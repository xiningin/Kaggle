{"cell_type":{"d589ac91":"code","2cfb4794":"code","da67383d":"code","7af7172a":"code","099d5606":"code","f3fe8dc9":"code","9798e91d":"code","b6467455":"code","39345e99":"code","1b1b61a2":"code","ea058c01":"code","c070dfb7":"code","1250cef1":"code","0f3546af":"code","8a2b87a5":"code","f9ce1e07":"code","f13599f2":"code","240720a1":"code","204778d4":"code","d81fbd41":"code","f2c57c2a":"code","8ef6ab81":"code","28d91d26":"code","14f91486":"code","57978b67":"code","5ddc00e7":"code","85113a90":"code","62ee3419":"code","b90104e8":"code","d073465c":"code","7134137d":"code","1ed4c365":"code","89c1bbaf":"code","4dd9c560":"code","2905ce31":"code","adbe0cda":"code","97736c3a":"code","076e4c9b":"code","4f95719b":"code","164cbc0a":"code","9e2fd336":"code","5e4e5fef":"code","ff2133d5":"code","67e88372":"code","0004165a":"code","d254ef74":"code","8a184da2":"code","d30640fd":"code","3761bd0e":"code","0493326c":"code","522dbd8e":"markdown","fe749698":"markdown","e8b9c08c":"markdown","9e69d7a7":"markdown","b513a143":"markdown","887b7def":"markdown","a3ce3a30":"markdown","eaf19d17":"markdown","ae7ea580":"markdown","b7192db3":"markdown","3a709023":"markdown","644ff8c4":"markdown","55cc50f3":"markdown","ceaa3b84":"markdown","f1da1218":"markdown","07580e1e":"markdown","42668f44":"markdown"},"source":{"d589ac91":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2cfb4794":"df = pd.read_csv(\"..\/input\/adult-dataset\/adult.csv\", header=None)","da67383d":"df.head()","7af7172a":"columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', \\\n           'occupation', 'relationship', 'race', 'gender', 'gain', 'loss', 'hpw', 'country', \\\n           'income']\n# gain    ==> capital gain\n# loss    ==> capital loss\n# hpw     ==> hours per week\n# country ==> native country\ndf.columns = columns","099d5606":"def missing_count(X):\n    L = len(X)\n    ms = X.isnull().sum()    \n    df = {\"Name\":X.columns, \"Total\":[L]*X.shape[1], \"Missing\":ms, \"Missing(%)\":round(ms\/L*100, 2), \"Dtypes\":X.dtypes}\n    df = pd.DataFrame(df)\n    return df\nmissing_count(df)","f3fe8dc9":"categorical = [col for col in df.columns if df[col].dtype == \"O\"]\nprint(categorical)","9798e91d":"numerical = [col for col in df.columns if df[col].dtype != \"O\"]\nprint(numerical)","b6467455":"for col in categorical:\n    print(\"=====\", col, \"=====\")\n    print(df[col].unique(), end=\"\\n\\n\")","39345e99":"for col in categorical:\n    def mapper(val):\n        return val[1:]\n    df[col] = df[col].apply(mapper)","1b1b61a2":"df[\"workclass\"].value_counts()","ea058c01":"1836 \/ len(df) * 100  #  % values missing\n# We will take care about it later","c070dfb7":"df[\"occupation\"].value_counts()","1250cef1":"1843 \/ len(df) * 100  #  % values missing\n# We will take care about it later","0f3546af":"df[\"country\"].value_counts()","8a2b87a5":"583 \/ len(df) * 100  #  % values missing\n# We will take care about it later","f9ce1e07":"for col in [\"workclass\", \"occupation\", \"country\"]:\n    df[col].replace(\"?\", np.NaN, inplace=True)","f13599f2":"for col in [\"workclass\", \"occupation\", \"country\"]:\n    print(df[col].unique(), end=\"\\n\\n\")","240720a1":"df[\"income\"].unique()","204778d4":"def mapper(val):\n    if(val == \"<=50K\"):\n        return 0\n    else:\n        return 1\ndf[\"income\"] = df[\"income\"].apply(mapper)","d81fbd41":"null_rows = []\nfor i in range(len(df)):\n    if(df.loc[i].isnull().sum()):\n        null_rows.append(i)","f2c57c2a":"len(null_rows)","8ef6ab81":"len(null_rows) \/ len(df) * 100","28d91d26":"missing_count(df)","14f91486":"for i in [\"workclass\", \"occupation\", \"country\"]:\n    df[i].fillna(df[i].mode()[0], inplace=True)","57978b67":"missing_count(df)","5ddc00e7":"X = df.drop([\"income\"], axis=1)\ny = df[\"income\"]","85113a90":"import category_encoders as ce\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'gender', 'country'])\nX = encoder.fit_transform(X)","62ee3419":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 77)","b90104e8":"X_train.shape, X_test.shape","d073465c":"cols = X.columns","7134137d":"from sklearn.preprocessing import RobustScaler\n\nrobust = RobustScaler()\nX_train = robust.fit_transform(X_train)\nX_test = robust.transform(X_test)","1ed4c365":"X_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])","89c1bbaf":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)","4dd9c560":"y_pred = gnb.predict(X_test)","2905ce31":"\"train score\", gnb.score(X_train, y_train)","adbe0cda":"\"test score\", gnb.score(X_test, y_test)","97736c3a":"maxi = y_train.value_counts().idxmax()\nnull_acc = y_test.value_counts()[maxi] \/ y_test.shape[0]\nprint(\"Null Accuracy: \", null_acc)","076e4c9b":"from sklearn.metrics import confusion_matrix\n\ndef confusion_heatmap(y_test, y_pred, label_mapping=None, normalize=None):\n    labels = np.unique(np.concatenate((np.unique(y_test), np.unique(y_pred)), axis=0))\n    cm = confusion_matrix(y_test, y_pred, labels=labels, normalize=normalize)\n    \n    mapping = labels\n    if(label_mapping):\n        mapping = [name_mapping[l] for l in labels]\n\n    d = pd.DataFrame(cm)\n    d.columns = mapping\n    d.index = mapping\n\n    sns.heatmap(d, annot=True, fmt=\".4g\", cmap=\"Blues\", )\n    plt.ylabel('True label',fontsize=12)\n    plt.xlabel('Predicted label',fontsize=12)\n    plt.show();\nconfusion_heatmap(y_test, y_pred)","4f95719b":"from sklearn.metrics import classification_report\n\nround(pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T, 2)","164cbc0a":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nTP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","9e2fd336":"classification_accuracy = (TP + TN) \/ float(TP + TN + FP + FN)\nclassification_accuracy","5e4e5fef":"classification_error = (FP + FN) \/ float(TP + TN + FP + FN)\nclassification_error","ff2133d5":"precision = TP \/ float(TP + FP)\nprecision","67e88372":"recall = TP \/ float(TP + FN)\nrecall","0004165a":"true_positive_rate = TP \/ float(TP + FN)\ntrue_positive_rate","d254ef74":"false_positive_rate = FP \/ float(FP + TN)\nfalse_positive_rate","8a184da2":"specificity = TN \/ (TN + FP)\nspecificity","d30640fd":"y_pred = gnb.predict_proba(X_test)[:, 1]\n\nplt.rcParams['font.size'] = 12\nplt.hist(y_pred, bins =10)\nplt.title('Histogram of predicted probabilities of 1')\nplt.xlim(0,1)\nplt.xlabel('Predicted probabilities of 1')\nplt.ylabel('Frequency')\nplt.show()","3761bd0e":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, linewidth=2)\nplt.plot([0,1], [0,1], 'k--' )\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Salaries')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show()","0493326c":"from sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred)\nROC_AUC","522dbd8e":"# Model Buliding","fe749698":"Let's first see how many rows(with atleast 1 null value) in dataset","e8b9c08c":"Let's remove first space from every value in all categorical features","9e69d7a7":"# Column Values Cleaning","b513a143":"## Unique Values","887b7def":"Means there are 1836 missing values in \"workclass\" column","a3ce3a30":"# Import Data","eaf19d17":"## \"income\" Column","ae7ea580":"There are \"?\" value in [\"workclass\", \"occupation\", \"country\"] features\nThese are missing values","b7192db3":"Means there are 1843 missing values in occupation column","3a709023":"Well it's look like there are no missing values in dataset, but there are some, we will look it further","644ff8c4":"## \"workclass\", \"occupation\", \"country\"","55cc50f3":"Means there are 583 missing values in country column","ceaa3b84":"means there are 7.36% rows have atleast one feature missing","f1da1218":"Replace all \"?\" with np.NaN","07580e1e":"Rename Column Names","42668f44":"# Null values Filling using mode"}}