{"cell_type":{"8afda883":"code","369b8408":"code","c49af03a":"code","e7b2d1d1":"code","af7c7c57":"code","e949cc76":"code","4249ddbe":"code","625b2d20":"code","f195aab6":"code","b9074651":"code","19c95ebc":"code","b0b59c8d":"code","007f4528":"code","af925ce5":"code","f8b4fc0c":"code","c7d8b314":"code","423897c1":"code","737a43de":"code","8d347986":"markdown","bc53130f":"markdown","c9f51cd0":"markdown","5519b7d6":"markdown","fea92eca":"markdown","3b50e92b":"markdown","3aa110e7":"markdown","cf3cc837":"markdown"},"source":{"8afda883":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import cross_val_score, GroupKFold, TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import fbeta_score, confusion_matrix, make_scorer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom matplotlib import pyplot as plt\nfrom lightgbm import LGBMClassifier\n\n%config InlineBackend.figure_format = 'retina'","369b8408":"employees_data = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/employees.csv', parse_dates=['HiringDate', 'DismissalDate'])\nhistory_data = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/history.csv', parse_dates=['Date'])\nsubmission_data = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/submission.csv')\n\nprint(employees_data.shape, history_data.shape, submission_data.shape)\nprint(employees_data['EmployeeID'].nunique(), history_data['EmployeeID'].nunique(), submission_data['EmployeeID'].nunique())","c49af03a":"# PREDICT_MONTHS = 3\nPREDICT_MONTHS = 5","e7b2d1d1":"df = history_data.merge(employees_data, how='outer', on='EmployeeID')\ndf['months_to_dissmiss'] = (df['DismissalDate'].sub(df['Date']) \/ np.timedelta64(1, 'M')).round()\ndf['target'] = (df['months_to_dissmiss'] <= PREDICT_MONTHS).astype(int)\ndf['experience_months'] = (df['Date'].sub(df['HiringDate']) \/ np.timedelta64(1, 'M')).round()\ndf['experience_years'] = (df['Date'].sub(df['HiringDate']) \/ np.timedelta64(1, 'Y')).round()\n\ndf['ProjectID'] = df['ProjectID'].fillna(0)\n\ndf.shape","af7c7c57":"# clean data\nguys_who_word_after_dissmissal = df[df['Date'] > df['DismissalDate']]['EmployeeID'].unique()\ndf.drop(df[df['EmployeeID'].isin(guys_who_word_after_dissmissal)].index, inplace=True)","e949cc76":"applications = pd.read_csv('..\/input\/trends-data-from-dou\/applications.csv', parse_dates=['Date'])\nvacancies = pd.read_csv('..\/input\/trends-data-from-dou\/vacancies.csv', parse_dates=['Date'])\ndf = df.merge(applications, on='Date', how='outer')\ndf = df.merge(vacancies, on='Date', how='outer')\ndf.shape","4249ddbe":"cols_categorical = ['DevCenterID', 'SBUID', 'PositionID', 'CustomerID', 'ProjectID', \n               'CompetenceGroupID', 'FunctionalOfficeID', 'PaymentTypeId']\ncols_numerical = ['PositionLevel', \n              'LanguageLevelID', 'IsTrainee', 'IsInternalProject', 'OnSite', 'Utilization', 'HourVacation', \n                'HourMobileReserve', 'HourLockedReserve', 'BonusOneTime', 'APM', \n                'WageGross', \n                  'MonthOnPosition', 'MonthOnSalary', \n                  'experience_months', \n                  'experience_years', \n#                   'experience_yearmonths',\n                  'times_wage_changed', 'times_posit_lvl_changed', 'times_lang_lvl_changed',\n#               'times_wage_inc', 'times_wage_dec', 'times_posit_lvl_inc', \n#                   'times_posit_lvl_dec', 'times_lang_lvl_inc', 'times_lang_lvl_dec',\n                  'times_project_changed', 'times_customer_changed', 'times_position_changed',\n                  'times_dev_center_changed', 'times_sbuid_changed', 'times_compet_group_changed',\n                  'times_funct_office_changed', 'times_payment_type_changed',\n#                   'num_unique_projects', 'num_unique_customers', 'num_unique_positions',\n#                   'num_unique_position_lvls', 'num_unique_lang_lvls', \n#                   'num_unique_dev_centers', 'num_unique_sbuids', 'num_unique_compet_groups', \n#                   'num_unique_funct_offices', 'num_unique_payment_types', \n                  'cumulative_hour_mobile_reserve', 'cumulative_hour_locked_reserve', 'cumulative_hour_vacation',\n                  'cumulative_bonus', 'cumulative_wage', 'cumulative_apm',\n#                   'mean_hour_vacation', 'mean_bonus', 'mean_wage', 'mean_apm', 'mean_utilization',\n                  'max_hour_vacation', 'max_bonus', \n#                   'max_wage', \n                  'max_apm', \n#                   'min_wage', \n                  'min_apm', \n                  'months_on_internal_proj', 'months_on_site',\n                  'was_trainee',\n#                  'wage_normalized_for_position_id', 'wage_normalized_for_position_lvl',\n#                   'cumulative_wage_6months', 'cumulative_wage_3months',\n                  'wage_back_1month', 'wage_back_2months', \n                  'wage_back_3months', 'wage_back_4months', \n                  'wage_back_5months',\n#                   'wage_back_6months', \n#                   'wage_back_7months', 'wage_back_8months',\n                  'lang_lvl_back_1month', 'lang_lvl_back_2months', 'lang_lvl_back_3months', \n                  'lang_lvl_back_4months', 'lang_lvl_back_5months',\n                  'Vacancies', 'Applications'\n                 ]\n\nprint(len(cols_categorical), len(cols_numerical))","625b2d20":"%%time\nchanges = df.groupby('EmployeeID').apply(lambda x: pd.concat((x['EmployeeID'], \n#     (x['WageGross'].diff() > 0).cumsum().rename('times_wage_inc'),\n#     (x['WageGross'].diff() < 0).cumsum().rename('times_wage_dec'),\n    (x['WageGross'].diff() != 0).cumsum().rename('times_wage_changed'),\n#     (x['PositionLevel'].diff() > 0).cumsum().rename('times_posit_lvl_inc'),\n#     (x['PositionLevel'].diff() < 0).cumsum().rename('times_posit_lvl_dec'),\n    (x['PositionLevel'].diff() != 0).cumsum().rename('times_posit_lvl_changed'),\n#     (~x['PositionLevel'].duplicated()).cumsum().rename('num_unique_position_lvls'),\n#     (x['LanguageLevelID'].diff() > 0).cumsum().rename('times_lang_lvl_inc'),\n#     (x['LanguageLevelID'].diff() < 0).cumsum().rename('times_lang_lvl_dec'),\n    (x['LanguageLevelID'].diff() != 0).cumsum().rename('times_lang_lvl_changed'),\n    x['LanguageLevelID'].shift(periods=1, fill_value=0).rename('lang_lvl_back_1month'),\n    x['LanguageLevelID'].shift(periods=2, fill_value=0).rename('lang_lvl_back_2months'),\n    x['LanguageLevelID'].shift(periods=3, fill_value=0).rename('lang_lvl_back_3months'),\n    x['LanguageLevelID'].shift(periods=4, fill_value=0).rename('lang_lvl_back_4months'),\n    x['LanguageLevelID'].shift(periods=5, fill_value=0).rename('lang_lvl_back_5months'),\n#     (~x['LanguageLevelID'].duplicated()).cumsum().rename('num_unique_lang_lvls'),\n#     (~x['ProjectID'].duplicated()).cumsum().rename('num_unique_projects'),\n#     (~x['CustomerID'].duplicated()).cumsum().rename('num_unique_customers'),\n#     (~x['PositionID'].duplicated()).cumsum().rename('num_unique_positions'),\n#     (~x['DevCenterID'].duplicated()).cumsum().rename('num_unique_dev_centers'),\n#     (~x['SBUID'].duplicated()).cumsum().rename('num_unique_sbuids'),\n#     (~x['CompetenceGroupID'].duplicated()).cumsum().rename('num_unique_compet_groups'),\n#     (~x['FunctionalOfficeID'].duplicated()).cumsum().rename('num_unique_funct_offices'),\n#     (~x['PaymentTypeId'].duplicated()).cumsum().rename('num_unique_payment_types'),\n    x['ProjectID'].ne(x['ProjectID'].shift(1).bfill()).cumsum().rename('times_project_changed'),\n    x['CustomerID'].ne(x['CustomerID'].shift(1).bfill()).cumsum().rename('times_customer_changed'),\n    x['PositionID'].ne(x['PositionID'].shift(1).bfill()).cumsum().rename('times_position_changed'),\n    x['DevCenterID'].ne(x['DevCenterID'].shift(1).bfill()).cumsum().rename('times_dev_center_changed'),\n    x['SBUID'].ne(x['SBUID'].shift(1).bfill()).cumsum().rename('times_sbuid_changed'),\n    x['CompetenceGroupID'].ne(x['CompetenceGroupID'].shift(1).bfill()).cumsum().rename('times_compet_group_changed'),\n    x['FunctionalOfficeID'].ne(x['FunctionalOfficeID'].shift(1).bfill()).cumsum().rename('times_funct_office_changed'),\n    x['PaymentTypeId'].ne(x['PaymentTypeId'].shift(1).bfill()).cumsum().rename('times_payment_type_changed'),\n    x['HourMobileReserve'].cumsum().rename('cumulative_hour_mobile_reserve'),\n    x['HourLockedReserve'].cumsum().rename('cumulative_hour_locked_reserve'),\n    x['HourVacation'].cumsum().rename('cumulative_hour_vacation'),\n    x['HourVacation'].cummax().rename('max_hour_vacation'),\n#     x['HourVacation'].expanding().mean().rename('mean_hour_vacation'),\n    x['BonusOneTime'].cumsum().rename('cumulative_bonus'),\n    x['BonusOneTime'].cummax().rename('max_bonus'),\n#     x['BonusOneTime'].expanding().mean().rename('mean_bonus'),\n    x['WageGross'].cumsum().rename('cumulative_wage'),\n#     x['WageGross'].cummax().rename('max_wage'),\n#     x['WageGross'].cummin().rename('min_wage'),\n#     x['WageGross'].rolling(min_periods=1, window=6).sum().rename('cumulative_wage_6months'),\n#     x['WageGross'].rolling(min_periods=1, window=3).sum().rename('cumulative_wage_3months'),\n    x['WageGross'].shift(periods=1, fill_value=0).rename('wage_back_1month'),\n    x['WageGross'].shift(periods=2, fill_value=0).rename('wage_back_2months'),\n    x['WageGross'].shift(periods=3, fill_value=0).rename('wage_back_3months'),\n    x['WageGross'].shift(periods=4, fill_value=0).rename('wage_back_4months'),\n    x['WageGross'].shift(periods=5, fill_value=0).rename('wage_back_5months'),\n#     x['WageGross'].shift(periods=6, fill_value=0).rename('wage_back_6months'),\n#     x['WageGross'].shift(periods=7, fill_value=0).rename('wage_back_7months'),\n#     x['WageGross'].shift(periods=8, fill_value=0).rename('wage_back_8months'),\n#     x['WageGross'].expanding().mean().rename('mean_wage'),\n    x['APM'].cumsum().rename('cumulative_apm'),\n    x['APM'].cummax().rename('max_apm'),  \n    x['APM'].cummin().rename('min_apm'),\n#     x['APM'].expanding().mean().rename('mean_apm'),\n#     x['Utilization'].expanding().mean().rename('mean_utilization'),\n    x['IsInternalProject'].cumsum().rename('months_on_internal_proj'),\n    x['OnSite'].cumsum().rename('months_on_site'),                                                          \n    x['Date']), axis=1))\n\ndf_with_feats = df.merge(changes, on=['EmployeeID', 'Date'], how='outer')\n# changes","f195aab6":"was_trainee = df_with_feats.groupby('EmployeeID')['IsTrainee'].max().rename('was_trainee')\ndf_with_feats = df_with_feats.merge(was_trainee, on='EmployeeID', how='outer')","b9074651":"df_with_feats[cols_categorical] = df_with_feats[cols_categorical].astype(str)\ndf_with_feats[cols_numerical] = df_with_feats[cols_numerical].astype(float)\n\ntrain_raw = df_with_feats[~(df_with_feats['DismissalDate'].isna())].copy()\ntest_raw = df_with_feats[df_with_feats['DismissalDate'].isna()].copy()\n\nprint(train_raw.shape, test_raw.shape)","19c95ebc":"train_ones = train_raw[train_raw['target'] == 1]\n\n# select rows exept last 3 rows, per employee\ntrain_zeros = test_raw.groupby('EmployeeID').apply(lambda df: df[df['Date'] <= \n                                                             (df['Date'].max() - np.timedelta64(3, 'M'))]) \\\n    .reset_index(level=0, drop=True)\n# select random row per employee\ntrain_zeros = train_zeros.groupby('EmployeeID') \\\n    .apply(lambda df: df.sample(1, random_state=(abs(hash(df.iloc[0]['EmployeeID'])) % (10 ** 9)))) \\\n    .reset_index(drop=True)\n\n# train_zeros = pd.concat((train_zeros, train_zeros_1))\ntrain_zeros['target'] = 0\n\ntrain = pd.concat((train_zeros, train_ones))\n\n# shuffle (to mix 1s and 0s) and sort by date\ntrain = train.sample(frac=1, random_state=1)\ntrain = train.sort_values(by='Date')\n\nprint(train.shape)\ntrain['target'].value_counts()","b0b59c8d":"X_train = train.drop('target', axis=1)\ny_train = train['target']\n\n# test on last month from \"test_raw\" dataframe\ntest_date = test_raw.groupby('EmployeeID')['Date'].max()\nX_test = test_raw.drop('target', axis=1)\nX_test = X_test.merge(test_date, on=['EmployeeID', 'Date'], how='inner')\n\n# remove redundant 200 employees\nX_test = X_test.merge(submission_data, on='EmployeeID', how='inner')\nX_test = X_test.drop('target', axis=1)\n\nprint(X_train.shape, y_train.shape, X_test.shape)","007f4528":"# cv = GroupKFold(n_splits=10)\ncv = TimeSeriesSplit(n_splits=10)\nscorer = make_scorer(fbeta_score, beta=1.7)","af925ce5":"class ThresholdRandomForestClassifier(RandomForestClassifier):\n    def __init__(self, n_estimators=100,\n                        criterion='gini',\n                        max_depth=None,\n                        min_samples_split=2,\n                        min_samples_leaf=1,\n                        min_weight_fraction_leaf=0.0,\n                        max_features='auto',\n                        max_leaf_nodes=None,\n                        min_impurity_decrease=0.0,\n                        min_impurity_split=None,\n                        bootstrap=True,\n                        oob_score=False,\n                        n_jobs=None,\n                        random_state=None,\n                        verbose=0,\n                        warm_start=False,\n                        class_weight=None,\n                        ccp_alpha=0.0,\n                        max_samples=None,\n                        threshold=0.5):\n        super().__init__(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n                         min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease,\n                        min_impurity_split, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start,\n                        class_weight, ccp_alpha, max_samples)\n        self.threshold = threshold\n        \n    def predict(self, X):\n        return (RandomForestClassifier.predict_proba(self, X)[:, 1] > self.threshold).astype(int)","f8b4fc0c":"class ItemSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]\n\n# clf = LGBMClassifier(max_depth=64, n_estimators=1000, random_state=1)\nclf = ThresholdRandomForestClassifier(threshold=0.5, n_estimators=2000, random_state=1, class_weight='balanced')\n# clf = ExtraTreeClassifier(random_state=1, class_weight='balanced')\n    \npipe = Pipeline([\n    ('union', FeatureUnion([\n        ('column_transformer', ColumnTransformer([\n            ('ohe', OneHotEncoder(handle_unknown='ignore'), cols_categorical),\n#             ('scaler', StandardScaler(), cols_need_scaling)\n        ])),\n        ('item_selector', ItemSelector(cols_numerical))\n    ])),\n    ('clf', clf)\n])","c7d8b314":"def make_submission(model, X_train, y_train, X_test, submission_file_name='submission.csv'):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    submission = pd.DataFrame({'EmployeeID': X_test['EmployeeID'], 'target': y_pred})\n    submission.to_csv(submission_file_name, index=False)\n    \n    return submission","423897c1":"score = cross_val_score(pipe, X_train, y_train, cv=cv, groups=X_train['EmployeeID'], \n                        n_jobs=-1, scoring=scorer)\nscore, score.mean(), score.std()","737a43de":"submission = make_submission(pipe, X_train, y_train, X_test, 'submission.csv')\nsubmission['target'].value_counts()","8d347986":"## Pipeline","bc53130f":"1) Take \"target 1\" samples from \"train\", and \"target 0\" samples from \"test without last 3 months\" (thus we are sure employee will not dissmiss in next 3 month, while also maintaining data variance)","c9f51cd0":"### Random forest","5519b7d6":"2) Make sure all 3 \"target 1\" samples for one person go entirely to train or test fold - to prevent data leak - use GroupKFold, where groups are EmployeeIDs","fea92eca":"## Composing train df, CV scheme","3b50e92b":"## Feature engineering","3aa110e7":"## Testing","cf3cc837":"Additional data source - https:\/\/jobs.dou.ua\/trends\/ (2 charts)"}}