{"cell_type":{"91e85568":"code","3ba837e8":"code","f403dbab":"code","a3d90f12":"code","f8ccc8aa":"markdown","feebac14":"markdown","fe7275d5":"markdown"},"source":{"91e85568":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ba837e8":"import pickle\ndata_path = '\/kaggle\/input\/iemocap\/IEMOCAP_features.pkl'\n\n# users should use this instructor to load pkl dataset. \nvideoIDs, videoSpeakers, videoLabels, videoText,\\\n    videoAudio, videoVisual, videoSentence, trainVid,\\\n        testVid = pickle.load(open(data_path, 'rb'), encoding='latin1')","f403dbab":"print(len(videoIDs))\n\nprint(videoIDs['Ses03M_impro08b'])\n\nprint(len(trainVid))\n\nprint(trainVid)\n\nprint(len(testVid))\n\nprint(testVid)","a3d90f12":"print(videoSpeakers['Ses03M_impro08b'], '\\n')\nprint(videoLabels['Ses03M_impro08b'], '\\n')\nprint(len(videoText['Ses03M_impro08b']), videoText['Ses03M_impro08b'][0].shape,'\\n')\nprint(len(videoAudio['Ses03M_impro08b']), videoAudio['Ses03M_impro08b'][0].shape, '\\n')\nprint(len(videoVisual['Ses03M_impro08b']), videoVisual['Ses03M_impro08b'][0].shape, '\\n')\nprint(videoSentence['Ses03M_impro08b'], '\\n')","f8ccc8aa":"- Datasets Content (***Dict DialogueId -> corresponding Features***)\n\n    - videoSpeakers :  There are multiple participators in one dialogue. ***videoSpeakers*** maps utterance to its speakers\n    - videoLabels   : The emotion Labels for each utterance in a dialogue.\n    - videoText     : The text features extracts using TextCNN.\n    - videoAudio    : The video features extracts using openSMILE kitools\n    - videoVisual   : The visual features extracts using 3d-CNN.\n    - videoSentence : The raw text info in a dialogue.","feebac14":"# Dataset Exploring \n\n> In this section, I want to show how to make use of the IEMOCAP Dataset.\n\n> Showing what the **dataset contain, the label format, tasks** can we try with this .pkl  file.\n\n## (0) First Loading the dataset.","fe7275d5":"## (1) Task Defination\n\nLet there be $M$ parties\/participants $p_1, p_2, \\cdots, p_M$ ($M = 2$ for the datasets we used) in a conversation. The task is to predict the emotion labels (happy, sad, neutral, angry, excited, and frustrated) of the constituent utterances $u_1, u_2, \\cdots , u_N$ , where utterance ut is uttered by party $p_{s(u_t)}$, while s being the mapping between utterance and index of its corresponding party. Also, $ut \u2208 R^{D_m}$ is the utterance representation, obtained using feature extractors described below.\n\n\n## (2) All Items\n\n- Train Test Splits INFO\n\n    - **trainVid** (*DICT* dialogueId -> UtteranceID List): **all dialogue IDs for trainset (total: 120)**\n    - **testVid**  (*LIST* UtteranceID Set): **all dialogue IDs for testset  (total: 31)**\n    - **videoIDs** (*LIST* UtteranceID Set): **all dialogue IDs for whole dataset (total: 120 + 31)**\n"}}