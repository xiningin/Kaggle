{"cell_type":{"99b3d1cf":"code","12f2026b":"code","bbca9c71":"code","8ac1bea9":"code","960f1009":"code","6c976faa":"code","6d81528c":"code","3d841093":"code","07e6a0d8":"code","f8a0459f":"code","7936567b":"code","2b23b777":"code","65464e12":"code","bf111d3b":"code","30295226":"code","4b9721c6":"code","7a43394b":"code","22b320cb":"code","5a50be64":"code","a648df2a":"code","8e4dee6d":"code","45d2eb83":"code","4c89a9b2":"code","6974f224":"code","8da45950":"code","6e456cc2":"code","8b249488":"code","e0de4098":"code","dd34825d":"code","35895c17":"code","6acabe0d":"code","481eedd8":"code","927f622a":"code","1dee188f":"code","32051702":"code","79bf86dd":"code","c274fe47":"code","d16ac5c0":"code","73d84ad3":"code","71bce821":"code","7f3d8a95":"code","3ff16591":"code","e750c6b9":"code","5e55e639":"code","c3c833b5":"code","3b1cf7d0":"code","21c4e8df":"code","1a6c670e":"code","cf3c938f":"code","4819c5ef":"code","4aac2f84":"code","4f9629c5":"code","9bb7da4d":"code","dbd76290":"code","23aa4b1a":"code","fba69fec":"code","deb3a929":"code","7f94b4fe":"code","d4ed6d7c":"code","807e0e8a":"code","936022b2":"code","0f6a099d":"markdown","03222fc0":"markdown","83272c9c":"markdown","7a48eeca":"markdown","474f4c14":"markdown","d60b6a2b":"markdown","3c83da4a":"markdown","d5a2a44b":"markdown"},"source":{"99b3d1cf":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom glob import glob\nfrom pickle import dump,load","12f2026b":"images_path = '..\/input\/flickr8k\/Images\/'\nimages = glob(images_path+'*.jpg')\nlen(images)","bbca9c71":"images[:5]","8ac1bea9":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)","960f1009":"from keras.applications import ResNet50\n\nincept_model = ResNet50(include_top=True)","6c976faa":"from keras.models import Model\nlast = incept_model.layers[-2].output\nmodele = Model(inputs = incept_model.input,outputs = last)\nmodele.summary()","6d81528c":"# saving resnet50 model for later use\nprint('Saving ResNet50 model for working offline')\nmodele.save('ResNet50.h5')\nprint('Saved ResNet50 model')","3d841093":"# extracting image features\nprint('Extracting image features')\n\nimages_features = {}\ncount = 0\nfor i in images:\n    img = cv2.imread(i)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (224,224))\n    \n    img = img.reshape(1,224,224,3)\n    pred = modele.predict(img).reshape(2048,)\n        \n    im_name = i.split('\/')[-1]\n    #img_name=im_name[18:43]\n    img_name = im_name\n    images_features[img_name] = pred\n    \n    count += 1\n    limit = 1499\n    if count > limit:\n        break\n        \n    elif count % 50 == 0:\n        print('Processed:',count)\n        \nprint('Extracted image feature of 1500 images')","07e6a0d8":"images[0].split('\/')[-1]","f8a0459f":"img_name","7936567b":"len(images_features)","2b23b777":"images_features","65464e12":"# saving image features for later use\nfrom pickle import dump\nprint('Saving image features')\ndump(images_features,open('features1500.p','wb'))\nprint('Saved feature map of images')","bf111d3b":"caption_path = '..\/input\/flickrcaption\/Flickr8k.token.txt'","30295226":"captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')","4b9721c6":"print('Sample captions from caption collection')\ncaptions[:20]","7a43394b":"print('Total number of captions present:',len(captions))","22b320cb":"print('Sample image')\ncaptions[0].split('\\t')[0][:-2]","5a50be64":"print('Sample Caption')\ncaptions[0].split('\\t')[1]","a648df2a":"print('Preparing caption dictionary')\ncaptions_dict = {}\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2] \n        caption = i.split('\\t')[1]\n        if img_name in images_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption]\n                \n            else:\n                captions_dict[img_name].append(caption)\n            \n    except:\n        pass","8e4dee6d":"print('Image Caption Dictionary')\ncaptions_dict","45d2eb83":"print('Length of Image Caption Dictionary:',len(captions_dict))","4c89a9b2":"import matplotlib.pyplot as plt\nprint('Viewing some sample images along with there respective captions')\nfor i in range(5):\n    plt.figure()\n    img_name = images[i]\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('\/')[-1]])\n    plt.imshow(img)","6974f224":"import matplotlib.pyplot as plt\n\nfor k in images_features.keys():\n    plt.figure()\n    \n    img_name = '..\/input\/flickr8k\/Images\/' + k\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('\/')[-1]])\n    plt.imshow(img)\n    \n    break","8da45950":"# preparing captions for feeding to model\nprint('Preprocessing captions')\ndef preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified\n    ","6e456cc2":"print('Modifiying current captions by adding startofseq and endofseq to captions')\nfor k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)","8b249488":"print('Preparing word dictionary')\ncount_words = {}\nfor k,vv in captions_dict.items():\n    for v in vv:\n        for word in v.split():\n            if word not in count_words:\n\n                count_words[word] = 0\n\n            else:\n                count_words[word] += 1","e0de4098":"print('Number of words in all the captions:',len(count_words))","dd34825d":"print('Count of different words inside captions')\ncount_words","35895c17":"print('Preparing word dictionary, attaching an integer with every word')\nTHRESH = -1\ncount = 1\nnew_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        new_dict[k] = count\n        count += 1\n        ","6acabe0d":"print('Word Dictionary')\nnew_dict","481eedd8":"print('Length of word dictionary:',len(new_dict))","927f622a":"# adding a new word 'OUT' in the word dictionary\n# this word is used for detecting the end of dictionary\nnew_dict['<OUT>'] = len(new_dict) ","1dee188f":"# saving the new dictionary\nfrom pickle import dump\ndump(new_dict,open('new_dict1500.p','wb'))\nprint('Saved new_dict.p')","32051702":"# creating a backup of dictionary\ncaptions_backup = captions_dict.copy()","79bf86dd":"captions_dict = captions_backup.copy()","c274fe47":"# model can take integers only, so we have converted the captions into integer caption form\n# we are attaching image with there integer caption list\nprint('Attaching images with dictionary format captions')\nfor k, vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():  \n            if word not in new_dict:\n                encoded.append(new_dict['<OUT>'])\n            else:\n                encoded.append(new_dict[word])\n\n\n        captions_dict[k][vv.index(v)] = encoded","d16ac5c0":"print('Images attached with there integer captions')\ncaptions_dict","73d84ad3":"from keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences","71bce821":"MAX_LEN = 0\nfor k, vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","7f3d8a95":"print('Maximum length caption in our set of captions')\nMAX_LEN","3ff16591":"Batch_size = 5000\nVOCAB_SIZE = len(new_dict)\n\nprint('Preparing input for model to be trained')\ndef generator(photo, caption):\n    n_samples = 0\n    \n    X = []\n    y_in = []\n    y_out = []\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range(1, len(v)):\n                X.append(photo[k])\n                # the caption known till now\n                in_seq= [v[:i]]\n                # next word in the caption\n                out_seq = v[i]\n                \n                # sequence padded with trailing zeros to match every caption of length to MAX_LEN\n                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n                # next word in caption is the one having highest probability among the words in word dictionary\n                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n                \n                #append the know caption\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n            \n    return X, y_in, y_out\n\n","e750c6b9":"print('Cooking Input')\nX, y_in, y_out = generator(images_features, captions_dict)\nprint('Input Cooked')","5e55e639":"print('Length of different inputs that are to be given to the model for training')\nprint('X lenght:',len(X))\nprint('y_in length:',len(y_in))\nprint('y_out length:',len(y_out))","c3c833b5":"print('Converting to numpy array form')\nX = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')\n\nprint('Converted to numpy array. Ready to be feeded to the model')","3b1cf7d0":"print('Shape of inputs')\nX.shape, y_in.shape, y_out.shape","21c4e8df":"print('Sample Numpy X input value')\nX[710]","1a6c670e":"print('Sample y_in value')\ny_in[2]","cf3c938f":"from keras.utils import plot_model\nfrom keras.layers.merge import add\nfrom tensorflow.keras.models import Model, Sequential\n#from keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate","4819c5ef":"embedding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(new_dict)\nprint('Configuring Image Model')\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nprint()\nprint('Configuring Language Model')\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\n\nprint()\nprint('Combining the model')\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","4aac2f84":"from keras.utils import plot_model\nplot_model(model,to_file='model.png', show_shapes=True)","4f9629c5":"print('Training the model')\nmodel.fit([X, y_in], y_out, batch_size=512, epochs=75)\nprint('Training Done')","9bb7da4d":"# inverse dictionary contains words maked against numbers\n# this can be used to obtain word for corresponding number which was predicted as the highest\n# probability number by the model\nprint('Preparing Inverse Dictionary')\ninv_dict = {v:k for k, v in new_dict.items()}","dbd76290":"# saving inverse dictionary\nfrom pickle import dump\ndump(inv_dict,open('inv_dict1500.p','wb'))\nprint('Saved inverse dictionary')","23aa4b1a":"print('Saving the model.h5 file for later use')\nmodel.save('trainedmodel1500.h5')\nprint('Model saved')","fba69fec":"print('Saving model weights')\nmodel.save_weights('mine_model_weights.h5')\nprint('Saved model weights')","deb3a929":"print('Saving new_dict in the form of numpy dict')\nnp.save('vocab.npy', new_dict)\n","7f94b4fe":"print('Retesting image path')\nimages[:5]","d4ed6d7c":"# helper function for accessing images from the test images folder provided\n\ndef getImage(x):\n    \n    test_img_path = images[x]\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (224,224))\n\n    test_img = np.reshape(test_img, (1,224,224,3))\n    \n    return test_img","807e0e8a":"print('Sample test feature of 2500th image')\ntest_feature = modele.predict(getImage(2500)).reshape(1,2048)\ntest_feature","936022b2":"print('Predicting captions for 5 random images in range 1500 to 6000')\nfor i in range(5):\n    \n    no = np.random.randint(0,1500,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(new_dict[i])\n\n        encoded = [encoded]\n\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n        \n        if sampled_word == 'endofseq':\n            break\n        caption = caption + ' ' + sampled_word\n            \n        text_inp.append(sampled_word)\n        \n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","0f6a099d":"# **Text Preprocess**","03222fc0":"# **Visualize Images with captions**","83272c9c":"# **Create Vocabulary**","7a48eeca":"# **MODEL**","474f4c14":"# **Build Generator Function**","d60b6a2b":"# **Image Preprocess**","3c83da4a":"------------------------------------------------------------------------------------------------------","d5a2a44b":"# **Predictions**"}}