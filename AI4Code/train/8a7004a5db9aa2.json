{"cell_type":{"ad06377e":"code","542ecda6":"code","aaf2bc48":"code","00348ef5":"code","04ba486b":"code","72cd181d":"code","ada2dd45":"code","013e67ef":"code","da4094e9":"code","4d835584":"code","1a647d2f":"code","bce7f1a2":"code","c2271764":"code","2b6c3a86":"code","528c6eec":"code","f326bd6e":"code","be53fb7a":"code","f39b81bd":"code","3663c059":"code","074f8920":"code","d7c66a6c":"code","022180d6":"code","256d584f":"code","8f3aff2f":"code","2a832898":"code","e182f6ed":"code","bfc71214":"code","eabb3a54":"code","f71f034d":"code","94ef755e":"code","26787f5b":"code","fa21e0e4":"code","afb6132a":"code","a7e24bc9":"code","3e042cfd":"code","fae30ba3":"code","fe8f75cb":"code","43773909":"code","21601ff4":"code","54d4b6ec":"code","f622fb48":"code","aaf8b09c":"code","b61286dd":"code","5f0404be":"code","4fce6b10":"code","835f3a18":"code","a3a0cd7d":"code","cfb56f41":"code","edb262f4":"code","5b21da97":"code","02bfaeaa":"code","11699fb0":"markdown","d8cdca66":"markdown","3b72b6fe":"markdown","7ea60615":"markdown","56e0e6f3":"markdown","92a36e8d":"markdown","4dc5c47b":"markdown","411d1a13":"markdown","776146ad":"markdown","f1d7871b":"markdown","b1d07859":"markdown","1be1abba":"markdown","ccc7774a":"markdown","328ba7aa":"markdown","0265d018":"markdown","a84f6ef9":"markdown","902d9968":"markdown","0cbf9812":"markdown","a98958aa":"markdown","18855b79":"markdown","e0c778cc":"markdown","b2d315e7":"markdown","ab143cc2":"markdown","2d67c6f0":"markdown","428610bb":"markdown","bcb159b4":"markdown","e8d607cc":"markdown","638af72f":"markdown","5f5a381e":"markdown","4c2e971a":"markdown","917188e4":"markdown","f836d0c6":"markdown","e0cad80d":"markdown"},"source":{"ad06377e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport statistics as stats\nimport datetime\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn import preprocessing","542ecda6":"#Import Google Play Apps Dataset\ndf_googleplay = pd.read_csv(\"..\/input\/google-play-store-analysis\/Google-Playstore.csv\")\n\ndf_googleplay.head()","aaf2bc48":"print(list(df_googleplay.shape)[0],'Rows')\nprint(list(df_googleplay.shape)[1],'Columns')","00348ef5":"percent_missing = df_googleplay.isnull().sum() * 100 \/ len(df_googleplay)\nmissing_value_df = pd.DataFrame({'column_name': df_googleplay.columns,\n                                 'percent_missing': percent_missing})\n\nmissing_value_df","04ba486b":"#Descriptive Statistics\ndf_googleplay.describe()","72cd181d":"#Medians of each numerical variable\npd.DataFrame(df_googleplay.median()).rename(columns={0:'Median_Value'})","ada2dd45":"#Modes of each variable\nmodes = []\nfor i in list(df_googleplay.columns):\n    modes.append([i,list(df_googleplay[i].mode())[0]])\n\ndf_modes = pd.DataFrame(modes).rename(columns={0:'Variable',1:'Mode'}).set_index(keys='Variable')\ndf_modes\n","013e67ef":"#Variance of each variable\nvariances = []\ndf_gpdescribe = pd.DataFrame(df_googleplay.describe())\nfor i in df_gpdescribe.columns:\n    variances.append([i,pow(list(df_gpdescribe[i])[2],2)])\n\ndf_variances = pd.DataFrame(variances).rename(columns={0:'Variable',1:'Variance'}).set_index(keys='Variable')\ndf_variances","da4094e9":"df_googleplay.corr()","4d835584":"df_googleplay.dtypes","1a647d2f":"df_googleplay['Released'] = df_googleplay['Released'].astype(str).str[:10]\ndf_googleplay['Last Updated'] = df_googleplay['Last Updated'].astype(str).str[:10]\n\n\n#Indentify Interquartile Range\nQ1 = df_googleplay.quantile(0.25)\nQ3 = df_googleplay.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n#Create DataFrame that identifies outliers variables based on Interquartile Range.\n#Outlier values are indicated by 'True'\ndf_gpoutliers = (df_googleplay < (Q1 - 1.5 * IQR)) | (df_googleplay > (Q3 + 1.5 * IQR))\ndf_gpoutliers","bce7f1a2":"#Summarize the number of Outliers in each variable\noutliers = []\nfor i in list(df_gpoutliers.columns):\n    outliers.append([i, sum(list(df_gpoutliers[i]))])\n\ndf_outliercount = pd.DataFrame(outliers).rename(columns={0:'Variable',1:'Outlier_QTY'}).set_index(keys='Variable')\n\noutlier_percentage = []\nfor i in list(df_gpoutliers.columns):\n    outlier_percentage.append([sum(list(df_gpoutliers[i]))\/len(list(df_googleplay[i]))])\n    \ndf_outliercount['Outlier_QTY_Percentage'] = outlier_percentage\n\ndf_outliercount","c2271764":"# Total number of rows that contain at least 1 Outlier value\nprint(sum(df_gpoutliers.any(axis=1)),' of ',list(df_googleplay.shape)[0],'records have at least 1 outlier value. (',sum(df_gpoutliers.any(axis=1))\/df_googleplay.shape[0],')')","2b6c3a86":"#Apps that are Ad Supported vs not\ndf_googleplay['Ad Supported'].value_counts().index\ndf_googleplay['Ad Supported'].value_counts().values\n\nsns.set(style = 'darkgrid')\nax = sns.barplot(x=df_googleplay['Ad Supported'].value_counts().index,\n                 y=df_googleplay['Ad Supported'].value_counts().values,\n                 color='blue')\n","528c6eec":"df_installs_pie = pd.DataFrame(df_googleplay.groupby('Installs')['Installs'].count())\ndf_installs_pie.index.names = ['Value']\n\n\nx_1 = list(df_googleplay.groupby('Installs')['Installs'].count().index)\ny_1 = list(df_googleplay.groupby('Installs')['Installs'].count())\n\nexplode_list = []\nfor i in list(df_googleplay.groupby('Installs')['Installs'].count().index):\n    explode_list.append(.05)\n\n# pie_label_list = []\n# for i in list(df_googleplay.groupby('Installs')['Installs'].count().index):\n#     pie_label_list.append('')\n\nplt.pie(y_1, explode=explode_list, labels=x_1, autopct='%1.1f%%',\n        startangle=100, labeldistance=1.1, radius=2, rotatelabels=True)\n# plt.legend(df_installs_pie.sort_values('Installs',\n#             ascending=False).index, bbox_to_anchor=(1.5, 1.5), loc='upper left')","f326bd6e":"#Summary of apps by Content Rating\n\nsizes = df_googleplay['Content Rating'].value_counts().sort_values() \/ df_googleplay['Content Rating'].value_counts().sum() * 100\nplt.pie(sizes, labels=sizes.index,\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.legend(sizes.index, loc=\"best\")\nplt.tight_layout()\nplt.legend(df_googleplay['Content Rating'].value_counts().sort_values().index, bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()","be53fb7a":"# Ratings\nax = sns.distplot(df_googleplay['Rating'], bins = 5) ","f39b81bd":"#Because there are no numerical variables with a standard deviation of 0, we will be dropping ->\n  #non-numerical IDs and URL fields that will have no relevance to our models\ndf_googleplay.drop(columns=['Developer Id', 'Developer Website','Developer Email','Privacy Policy'], axis = 1,inplace=True) \ndf_googleplay.head()\n","3663c059":"#Since we have begun preprocessing the data, we will now reevaluate the previous script to identify outliers and remove\n#any rows that contain at least 1 outlier outside of the interquartile range\n\n#Indentify Interquartile Range\nQ1 = df_googleplay.quantile(0.25)\nQ3 = df_googleplay.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\n#Create DataFrame that identifies outliers variables based on Interquartile Range.\n#Outlier values are indicated by 'True'\ndf_gpoutliers = (df_googleplay < (Q1 - 1.5 * IQR)) | (df_googleplay > (Q3 + 1.5 * IQR))\ndf_gpoutliers","074f8920":"#Summarize the number of Outliers in each variable\noutliers = []\nfor i in list(df_gpoutliers.columns):\n    outliers.append([i, sum(list(df_gpoutliers[i]))])\n\ndf_outliercount = pd.DataFrame(outliers).rename(columns={0:'Variable',1:'Outlier_QTY'}).set_index(keys='Variable')\n\noutlier_percentage = []\nfor i in list(df_gpoutliers.columns):\n    outlier_percentage.append([sum(list(df_gpoutliers[i]))\/len(list(df_googleplay[i]))])\n    \ndf_outliercount['Outlier_QTY_Percentage'] = outlier_percentage\n\ndf_outliercount","d7c66a6c":"# Total number of rows that contain at least 1 Outlier value\nprint(sum(df_gpoutliers.any(axis=1)),' of ',list(df_googleplay.shape)[0],'records have at least 1 outlier value. (',sum(df_gpoutliers.any(axis=1))\/df_googleplay.shape[0],')')","022180d6":"#Adjust the df_googleplay dataframe to only intake values within the Interquartile range\ndf_googleplay = df_googleplay[~((df_googleplay < (Q1 - 1.5 * IQR)) |(df_googleplay > (Q3 + 1.5 * IQR))).any(axis=1)]\n\n#Count the number of rows to verify how many were removed\nprint(list(df_googleplay.shape)[0],'Rows')","256d584f":"#Create a list of variables that will categorized (Category & Content Rating)\ncategorical_dummies = list(set(df_googleplay[['Category', 'Content Rating','Minimum Android']].columns))\n\n#Create dummy variables using onehot encoding\ndummy_cat_df = pd.get_dummies(df_googleplay[categorical_dummies], drop_first=True) \n\n#Drops categorical variables from the df_googleplay\ndf_googleplay = df_googleplay.drop(categorical_dummies, axis = 1) \n\n#Adds the newly created dummy variables\ndf_googleplay = pd.concat([df_googleplay, dummy_cat_df], axis = 1)\n\ndf_googleplay.head()","8f3aff2f":"df_googleplay.dtypes","2a832898":"# Creates an empty list for categorical variables with zero variation\nzero_card = []\n\n# Create a list of categorical variables\ncategorical_var = list(set(df_googleplay.dtypes[df_googleplay.dtypes == object].index))\n\n# Appends zero_card list with variables with 1 level\nfor i in categorical_var:\n    if len(df_googleplay[i].value_counts().index) == 1:\n        zero_card.append(i)\n\nprint(zero_card) # Prints the list of variables with low cardinality","e182f6ed":"# Drops variables with 0 variance\ndf_googleplay = df_googleplay.drop(zero_card, axis = 1)\n\ndf_googleplay.head()","bfc71214":"print(list(df_googleplay.shape)[0],'Rows')","eabb3a54":"# Creates a list of categorical variables with high cardinality\nhigh_card = []\n\n# Create a list of categorical variables ecluding App as we need that to identify each application\ncategorical_var = list(set(df_googleplay.dtypes[df_googleplay.dtypes == object].index) - set(['App']))\n\nfor i in categorical_var: # for each categorical variables\n    if len(df_googleplay[i].value_counts().index) > 200000: # check how many levels it has and if it is more\n        high_card.append(i) # than 200000, variable has many levels\n        # so append it to the list of categorical variables with high cardinality\n        \nprint(high_card) # Prints the list of variables with high cardinality","f71f034d":"# Drops variables with high cardinality\ndf_googleplay = df_googleplay.drop(high_card, axis = 1)\n\ndf_googleplay.head()","94ef755e":"df_gpnull = df_googleplay\nlist_gpnull = []\nfor i in list(df_gpnull.columns):\n    list_gpnull.append([i, df_gpnull[i].isna().sum()])\n    \ndf_nullcount = pd.DataFrame(list_gpnull).rename(columns={0:'Variable',1:'Null_QTY'}).set_index(keys='Variable')\n\nnull_percentage = []\nfor i in list(df_gpnull.columns):\n    null_percentage.append([df_gpnull[i].isna().sum()\/len(list(df_googleplay[i]))])\n    \ndf_nullcount['Null_QTY_Percentage'] = null_percentage\n\ndf_nullcount","26787f5b":"#Drop null rows from the dataframe\ndf_googleplay = df_googleplay.dropna(how='any',axis=0)\n\n#Confirmation\ndf_gpnull = df_googleplay\nlist_gpnull = []\nfor i in list(df_gpnull.columns):\n    list_gpnull.append([i, df_gpnull[i].isna().sum()])\n    \ndf_nullcount = pd.DataFrame(list_gpnull).rename(columns={0:'Variable',1:'Null_QTY'}).set_index(keys='Variable')\n\nnull_percentage = []\nfor i in list(df_gpnull.columns):\n    null_percentage.append([df_gpnull[i].isna().sum()\/len(list(df_googleplay[i]))])\n    \ndf_nullcount","fa21e0e4":"#Drop Columns containing all 0's\ndf_googleplay = df_googleplay.loc[:, (df_googleplay != 0).any(axis=0)]\n\ndf_googleplay.head()","afb6132a":"#Rename Index\ndf_googleplay.index.names = ['ID']\n\n# Create a random sample of 1000 records for regression (excluding all 0 binary columns)\ndf_gp_sample = df_googleplay.sample(1000, random_state=52)#.loc[:, (df_googleplay.sample(1000, random_state=52) != 0).any(axis=0)]\n\n#convert fields to be scaled to integers\n\ndf_gp_sample","a7e24bc9":"df_gp_sample.corr()","3e042cfd":"df_gpInstallsRegression=df_gp_sample.copy()\ndf_gpInstallsRegression = df_gpInstallsRegression[['Maximum Installs','Minimum Installs','Rating','Rating Count','Category_Parenting']]\ndf_gpInstallsRegression.head()","fae30ba3":"#Create a list of the numerical variables to scale (exclude dependents)\nto_scale =  ['Minimum Installs','Rating Count','Rating']\n\n#To use this library, we need to convert a pandas dataframe into a numpy array by doing the following:\nInstallsRegressionarray = df_gpInstallsRegression[to_scale].values\n\n#Create a min max scaler - this sets all values between 0 and 1 within the numpy array\ndata_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n\n#Apply the scaler and overwrite the data into the existing dataframe\ndf_scaled = pd.DataFrame(data_scaler.fit_transform(InstallsRegressionarray), columns = to_scale)\n\ndf_gpInstallsRegression['Minimum Installs'] = df_scaled['Minimum Installs'].values\ndf_gpInstallsRegression['Rating Count'] = df_scaled['Rating Count'].values\n\n\n\ndf_gpInstallsRegression.head()","fe8f75cb":"#Reponse Variable\ntarget_col= 'Maximum Installs'\n\n#determines the iput variables\ninput_col= list(set(df_gpInstallsRegression.columns)-set(['Maximum Installs']))\n# creates a linear regression instance\nmodel= LinearRegression()\n# fits the data to the model\nmodel.fit(df_gpInstallsRegression[input_col],df_gpInstallsRegression[target_col])\n\n#prints the model coefficients\nprint('Model coefficients:')\nprint(model.coef_)\n\n# prints the R2 value of the model\nprint('\\n')\nprint('R2 value:'+ str(round(model.score(df_gpInstallsRegression[input_col],df_gpInstallsRegression[target_col]),2)))\nprint('\\n')\n# calculate the residuals and print the results\npred_vs_actual= pd.DataFrame()\npred_vs_actual['actual']=df_gpInstallsRegression[target_col]\npred_vs_actual['predicted']=np.round(model.predict(df_gpInstallsRegression[input_col]),6)\npred_vs_actual['error']=pred_vs_actual['actual']-pred_vs_actual['predicted']\nprint(pred_vs_actual.head())","43773909":"g = sns.jointplot(x=\"actual\", y=\"predicted\", data=pred_vs_actual, kind='reg',\n                  joint_kws={'line_kws':{'color':'cyan'}})\nregline = g.ax_joint.get_lines()[0]\nregline.set_color('red')","21601ff4":"P=df_gpInstallsRegression.loc[:,df_gpInstallsRegression.columns!=target_col]\nR=df_gpInstallsRegression.loc[:,target_col]","54d4b6ec":"P=sm.add_constant(P)\nmodel=sm.OLS(R,P)","f622fb48":"results=model.fit()\nresults.params","aaf8b09c":"# Regression Summary Table\nprint(results.summary())","b61286dd":"df_gpRatingsRegression=df_gp_sample.copy()\ndf_gpRatingsRegression = df_gpRatingsRegression[['Rating Count','Maximum Installs','Minimum Installs','Rating','Category_Finance','Category_Video Players & Editors']]\ndf_gpRatingsRegression.head()","5f0404be":"#Create a list of the numerical variables\nto_scale =  ['Maximum Installs','Minimum Installs','Rating']\n\n#To use this library, we need to convert a pandas dataframe into a numpy array by doing the following:\nRatingsRegressionarray = df_gpRatingsRegression[to_scale].values\n\n#Create a min max scaler - this sets all values between 0 and 1 within the numpy array\ndata_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n\n#Apply the scaler and overwrite the data into the existing dataframe\ndf_scaled = pd.DataFrame(data_scaler.fit_transform(RatingsRegressionarray), columns = to_scale)\n\ndf_gpRatingsRegression['Minimum Installs'] = df_scaled['Minimum Installs'].values\ndf_gpRatingsRegression['Maximum Installs'] = df_scaled['Maximum Installs'].values\n\ndf_gpRatingsRegression.head()","4fce6b10":"target= 'Rating Count'","835f3a18":"#determines the iput variables\ninput_col= list(set(df_gpRatingsRegression.columns)-set([\"Rating Count\"]))\n# creates a linear regression instance\nmodel= LinearRegression()\n# fits the data to the model\nmodel.fit(df_gpRatingsRegression[input_col],df_gpRatingsRegression[target])\n\n#prints the model coefficients\nprint('Model coefficients:')\nprint(model.coef_)\n\n# prints the R2 value of the model\nprint('\\n')\nprint('R2 value:'+ str(round(model.score(df_gpRatingsRegression[input_col],df_gpRatingsRegression[target]),2)))\nprint('\\n')\n# calculate the residuals and print the results\npred_vs_actual= pd.DataFrame()\npred_vs_actual['actual']=df_gpRatingsRegression[target]\npred_vs_actual['predicted']=np.round(model.predict(df_gpRatingsRegression[input_col]),6)\npred_vs_actual['error']=pred_vs_actual['actual']-pred_vs_actual['predicted']\nprint(pred_vs_actual.head())","a3a0cd7d":"g = sns.jointplot(x=\"actual\", y=\"predicted\", data=pred_vs_actual, kind='reg',\n                  joint_kws={'line_kws':{'color':'cyan'}})\nregline = g.ax_joint.get_lines()[0]\nregline.set_color('red')","cfb56f41":"P=df_gpRatingsRegression.loc[:,df_gpRatingsRegression.columns!=target]\nR=df_gpRatingsRegression.loc[:,target]","edb262f4":"P=sm.add_constant(P)\nmodel=sm.OLS(R,P)","5b21da97":"results=model.fit()\nresults.params","02bfaeaa":"# Regression Summary Table\nprint(results.summary())","11699fb0":"### Model Visualization","d8cdca66":"## Medians","3b72b6fe":"## Removing Non-Significant Variables","7ea60615":"### Ratings Distribution","56e0e6f3":"## Removing Outliers\n","92a36e8d":"## Mean, Standard Deviation, Min and Max using Descriptive Statistics","4dc5c47b":"## Number of Columns and Rows","411d1a13":"### Model Fitting and Output","776146ad":"## Modes","f1d7871b":"### Model Fitting and Output","b1d07859":"## Random 1000","1be1abba":"## Encoding Categorical Variables","ccc7774a":"## Percent of missing values by Columns","328ba7aa":"## Removing Null Values","0265d018":"# Importing Libraries and Tables","a84f6ef9":"### Ads Supported","902d9968":"## Variance","0cbf9812":"## Outliers","a98958aa":"### Model Visualization","18855b79":"## Data Visualizations","e0c778cc":"## Correlations","b2d315e7":"## Dropping Categorical Variables with Zero Variance","ab143cc2":"### Scale Independent Numerical Variables","2d67c6f0":"### Scale Independent Numerical Variables","428610bb":"# Regression Models","bcb159b4":"### Install QTY by Release Date","e8d607cc":"# Descriptive Analysis","638af72f":"### Install QTY Categories","5f5a381e":"## Dropping Categorical Variables with High Variance","4c2e971a":"## Multiple Linear Regression - Maximum Installs","917188e4":"## Multiple Linear Regression - Rating Count","f836d0c6":"### Content Rating","e0cad80d":"# Data Preprocessing"}}