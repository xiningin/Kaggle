{"cell_type":{"20d0255c":"code","d0859d7e":"code","07bda816":"code","1767f862":"code","7945245c":"code","66934e1e":"code","714cab58":"code","d51bc785":"code","1baf3528":"code","96fe4de3":"code","4035807d":"code","4b4658d2":"code","03172453":"code","5a03f462":"code","91e8e5f2":"code","760bf32f":"code","89d37aaa":"code","d3b11a9f":"markdown","66b5d047":"markdown","a70eb4e0":"markdown","8b9a78ea":"markdown","38435384":"markdown","064aae06":"markdown","152b50bb":"markdown","aff48fee":"markdown","22af8053":"markdown","36f4533e":"markdown","6de39c81":"markdown","28a3b65e":"markdown","2372a0cf":"markdown","fc2794a4":"markdown","392ff8bf":"markdown","50a29bdb":"markdown","7404ddbd":"markdown","7040a30d":"markdown","9b5bb66a":"markdown"},"source":{"20d0255c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tensorflow.keras.models import Sequential #CNN model\nfrom tensorflow.keras.layers import Dense,Flatten,MaxPooling2D #Layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #for data augmentation","d0859d7e":"#Labels:\nprint(os.listdir('..\/input\/flowers-recognition\/flowers'))","07bda816":"tulip_path = '..\/input\/flowers-recognition\/flowers\/tulip'\ndandelion_path = '..\/input\/flowers-recognition\/flowers\/dandelion'\nsunflower_path = '..\/input\/flowers-recognition\/flowers\/sunflower'\ndaisy_path = '..\/input\/flowers-recognition\/flowers\/daisy'\nrose_path = '..\/input\/flowers-recognition\/flowers\/rose'","1767f862":"from tqdm import tqdm\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nflower_types = [tulip_path,dandelion_path,sunflower_path,daisy_path,rose_path]\nX_full = []\ny_full = []\nfor flower in flower_types: \n    for i in tqdm(os.listdir(flower)):\n        try:\n            img = load_img(flower+'\/'+i,target_size=(150,150))\n        except:\n            continue\n        img = img_to_array(img)\n        X_full.append(np.array(img))\n        y_full.append(flower[37:])    ","7945245c":"print(len(X_full))\nprint(len(y_full))","66934e1e":"X_full = np.array(X_full)\nX_full = X_full.reshape(4323,150,150,3)\nX_full = X_full\/255","714cab58":"#Visualising\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nfrom PIL import Image\nimport random as rn\n%matplotlib inline  \n\n\nfig,ax=plt.subplots(2,2)\nfig.set_size_inches(7,7)\nfor i in range(2):\n    for j in range (2):\n        l=rn.randint(0,len(y_full))\n        ax[i,j].imshow(X_full[l])\n        ax[i,j].set_title('Flower: '+y_full[l])\n        \nplt.tight_layout()","d51bc785":"#One-hot encoding y\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nimport tensorflow as tf\nLE = LabelEncoder()\ny_full = LE.fit_transform(y_full)\n#4-tulip,1-dandelion,3-Sunflower,0-Daisy,2-Rose -> after label encoding\ny_full = tf.keras.utils.to_categorical(y_full,num_classes=5)","1baf3528":"#Train-test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X_full,y_full,test_size=0.2,random_state=42)","96fe4de3":"#Create flower model\nfrom tensorflow.keras.layers import Conv2D,Dropout \nnum_classes = 5\nrows,cols=150,150\n\n#Input layer\nflower_model = Sequential()\nflower_model.add(Conv2D(64,activation='relu',kernel_size=(3,3),input_shape=(rows,cols,3)))\nflower_model.add(MaxPooling2D(pool_size=(2,2)))\n#Layer-2\nflower_model.add(Conv2D(64,activation='relu',kernel_size=(3,3)))\nflower_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n\n#Layer-3\nflower_model.add(Conv2D(128,activation='relu',kernel_size=(4,4)))\nflower_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n\n#Layer-4\nflower_model.add(Conv2D(128,activation='relu',kernel_size=(4,4)))\nflower_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\nflower_model.add(Dropout(0.25))\n#Flatten\nflower_model.add(Flatten())\n#Add Dense layer\nflower_model.add(Dense(512,activation='relu'))\nflower_model.add(Dropout(0.25))\n#Output layer\nflower_model.add(Dense(num_classes,activation='softmax'))\n\n#Metrics,loss and optimizer for our model\nflower_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n#Data-augmentation to prevent over-fitting\ndatagen_train = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.2,height_shift_range=0.2)\ntrain_gen = datagen_train.flow(X_train,y_train,batch_size=64)","4035807d":"from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n\nbest_weights = ModelCheckpoint(filepath='best_weights.hdf5',verbose=2,save_best_only=True)   #Saving best weights to a file\nreduce_LR = ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=2,factor=0.2)          #Reducing Learning Rate when val_loss doesn't improve\nearly_stop = EarlyStopping(monitor='val_loss',patience=20,verbose=2)                         #Stopping model when val_loss doesn't improve","4b4658d2":"#fit\nmodel_history = flower_model.fit_generator(\n                 train_gen,\n                 verbose=1,\n                 epochs=100,\n                 validation_data=(X_val,y_val),\n                 callbacks=[best_weights,reduce_LR])","03172453":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Validation'])\nplt.show()","5a03f462":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Validation'])\nplt.show()","91e8e5f2":"flower_model.load_weights('best_weights.hdf5')\nflower_model.evaluate(X_val,y_val)","760bf32f":"#4-tulip,1-dandelion,3-Sunflower,0-Daisy,2-Rose -> after label encoding\ntarget_labels = {}\nflower_labels = ['Daisy','Dandelion','Rose','Sunflower','Tulip']\nfor i in range(0,4):\n    target_labels[i] = flower_labels[i]","89d37aaa":"#Method for preprocessing a single image\ndef to_img(PATH):\n    img = load_img(PATH,target_size=(150,150))\n    img = img_to_array(img)\n    img = img.reshape(1,150,150,3)\n    img = img\/255\n    return img\n\n#Any flower image from the net\npath_of_image = '..\/input\/my-images\/rose.jpg'\nimage_conv = to_img(path_of_image)\npredicted_flower = flower_model.predict_classes(image_conv) #Returns the label-encoded value\nprint(\"The picture is a:\",target_labels[predicted_flower[0]])\nplt.imshow(image_conv[0])","d3b11a9f":"*Finding length of X data:*","66b5d047":"**Evaluating Model Performance**","a70eb4e0":"***Checking our model performance by feeding a flower photo from net***","8b9a78ea":"***If you like my notebook please do upvote :)***\n","38435384":"**Testing out our best weights**","064aae06":"**Converting data to be suitable enough to feed DL models**","152b50bb":"**Contents:**\n* Importing necessary libraries\n* Data-preprocessing\n* Visualizing\n* Encoding target data\n* Train-val Split\n* Creating CNN model\n* Call-backs for model\n* Running model\n* Evaluating Model Performance\n* Testing Model  ","aff48fee":"**Creating a dictionary for our target variables:**","22af8053":"***Please be free to edit this notebook and try out your own ideas***","36f4533e":"*Creating CNN model*\n> **Dropouts:** It is used to speed up model and prevent overfitting of data \n> \n> **MaxPooling2D:** It downsamples the input representation by taking the maximum value over the window(pool_size)","6de39c81":"**Call backs for our model**","28a3b65e":"**Labels in the dataset**","2372a0cf":"**Data prepocessing**","fc2794a4":"*Splitting X and y into train and validation*","392ff8bf":"*Fitting our model*","50a29bdb":"**Visualing sample images from dataset**","7404ddbd":"**Import the necessary libraries**","7040a30d":"# **Identifying the types of flowers using CNN**","9b5bb66a":"**Label and One-hot Encoding target**"}}