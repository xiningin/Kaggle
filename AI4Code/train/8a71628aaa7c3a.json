{"cell_type":{"70feeefe":"code","848b5917":"code","acda3a49":"code","cbf90ad0":"code","62b0ba1e":"code","6c96462f":"code","2667da5d":"code","7ef74394":"code","0d267b26":"code","ec050ba1":"code","d9c150a4":"code","7421c2d9":"code","372f91ee":"code","93c235a2":"code","8ff98f83":"code","d78ff409":"code","0a3253f8":"code","0fd4d927":"code","c8736255":"code","171ec38e":"code","efee1ef9":"code","fdc54883":"code","6eb236f7":"code","1456fd45":"code","b6d45736":"code","dd834cc1":"code","8e1c0d40":"code","912bc47c":"code","86ee2630":"code","7cd4f629":"code","dd2a1bbd":"code","43147e79":"code","09cd9f31":"code","1c62f626":"code","e046ef28":"code","967f03de":"code","238bfd8b":"code","fb6a6e49":"code","c335f069":"code","52fefcdc":"code","0237b19d":"code","53c09ab7":"code","b7f3078b":"code","6a032ade":"code","29f2cff3":"code","70946134":"code","ea363c09":"code","d6592b6e":"code","501b3814":"markdown","c5806acd":"markdown","af7ab8ff":"markdown","fd0b5eec":"markdown","1cebc0fb":"markdown","40fc3f0a":"markdown","cbf38430":"markdown","bf947ee4":"markdown","dffaa1a5":"markdown","4c00c335":"markdown","6be8b671":"markdown","89df90e7":"markdown","ee6cce0f":"markdown","3e0a888f":"markdown","a13bba8f":"markdown","b1b865f0":"markdown","976fab32":"markdown","c84b6b76":"markdown","3bce1650":"markdown","3218845f":"markdown","98699212":"markdown","8f8f34d3":"markdown"},"source":{"70feeefe":"# IMPORT REQUIRED PACKAGES \n\n# DATA EXPLORATION \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# DATA PROCESSING \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# MODELS AND EVALUATION \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n\n# CROSS VALIDATION \nfrom sklearn.model_selection import cross_val_score\n\nprint('complete')","848b5917":"# IMPORT THE DATA AND VIEW FIRST 10 ROWS \n\ndf1 = pd.read_csv('..\/input\/titantic-survial\/train.csv')\ndf1.head(10)","acda3a49":"# sibsp = number of siblings \/ spouses aboard the Titanic\n# parch = number of parents \/ children aboard the Titanic\n# embarked = Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n# survival = Survival 0=No, 1=Yes","cbf90ad0":"# OVERVIEW OF THE COLUMNS, VALUES AND DATA TYPES \n\ndf1.info()","62b0ba1e":"# USING PANDAS DESCRIBE FUNCTION TO GET AN OVERVIEW OF THE NUMERIC COLUMNS \n# LOOKS LIKE THE MAJORITY OF PEOPLE WERE QUITE YOUNG <40\n# THE MAX VALUE IN FARE LOOKS VERY HIGH IN COMPARISON TO THE MEAN AND 75% PERCENTILE \n# THERE ARE MISSING VALUES IN AGE \n\n\ndf1.describe()","6c96462f":"# COUNT THE VOLUME AND PERCENTAGE OF TOTAL NAN PER COLUMN\n# CABIN HAS THE MOST MISSING VALUES AT 77% - NEEDS MORE INVESTIGATION\n# AGE HAS 263 MISSING VALUES \n\n\n# SUM THE NULL VALUES \ntotal = df1.isnull().sum().sort_values(ascending=False)\n\n# WORK OUT THE PERCENTAGE OF TOTAL VALUES MISSING \npercent_1 = round(df1.isnull().sum()\/df1.isnull().count()*100,1).sort_values(ascending=False)\n\n# COMBINE THE DATAFRAMES \nmissing_data = pd.concat([total, percent_1], axis=1, keys=['Total', '%'])\n\n# SHOW THE RESULT \nmissing_data.head()","2667da5d":"# PLOT A HISTOGRAM OF PASSENGER AGES \n# APPEARS THE MAJORITY OF PASSENGERS WERE BETWEEN 20-40\n\n\n# BECAUSE THE AGE COLUMN CONTAINS NAN VALUES THE DROPNA COMMAND IS REQUIRED TO PLOT THE DATA \nax = plt.hist(x=df1['Age'].dropna(), bins='auto')\n\nplt.title('Age of passengers')\nplt.show()","7ef74394":"# SPLIT BETWEEN AGE, GENDER AND SURVIAL \n# MORE WOMEN THAN MEN SURVIVED - A WIDER RANGE OF AGED WOMEN SURVIVED THAN MEN \n# LOOKS LIKE THERE WERE MORE MEN ABOARD \n\n# LABELS FOR THE GRAPH LEGNED\nmale = 'male'\nfemale = 'female'\n\n# CREATE SEPERATE SURVIAL DATAFRAMES \nsurvived = df1[df1['Survived'] ==1]\nnot_survived = df1[df1['Survived']==0]\n\n# SET UP THE SUBPLOT TO DISPLAY THE GRAPHS \nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\n\n# SURVIVED SPLIT BY MALE AND FEMALE \nax = sns.histplot(survived[survived['Sex']=='male'].Age.dropna(), label=male, bins='auto', kde=False, ax=axes[0])\nax = sns.histplot(survived[survived['Sex']=='female'].Age.dropna(), label=female, bins='auto', kde=False, ax=axes[0])\nax.legend()\nax.set_title('Survived')\n\n# NOT SURVIVED SPLIT BY MALE AND FEMALE \nax = sns.histplot(not_survived[not_survived['Sex']=='male'].Age.dropna(), label=male, bins='auto', kde=False, ax=axes[1])\nax = sns.histplot(not_survived[not_survived['Sex']=='female'].Age.dropna(), label=female, bins='auto', kde=False, ax=axes[1])\nax.legend()\nax2 = ax.set_title('Not_survived')","0d267b26":"# SURVIVAL BY PASSENGER CLASS \n\n# BELOW IS USEFUL BECAUSE IT GROUPS THE DATA TOGETHER \n# APPENDS THE TWO DATAFRAMES TOGETHER \n# BUT THIS IS NOT WHAT I WANTED TO DO \n\nclass_s = survived.groupby(['Survived'])['Pclass'].agg('count').reset_index().rename(columns={'Pclass':'Passengers'})\nclass_ns = not_survived.groupby(['Survived'])['Pclass'].agg('count').reset_index().rename(columns={'Pclass':'Passengers'})\n\nclass_survival = class_s.append(class_ns)\nclass_survival","ec050ba1":"# SURVIVAL BY PASSENGER CLASS \n# CREATES A NEW DATAFRAME WITH THE PASSENGER CLASS AS THE COLUMNS \n# DONE BY CREATING NEW DATAFRAMES, GROUPING THE REQUIRED DATA TOGTHER \n# THEN JOINING THE GROUPED DATAFRAMES TOGETHER \n\n# MIGHT BE A CLEANER WAY TO DO THE TWO JOINS \n# https:\/\/stackoverflow.com\/questions\/44327999\/python-pandas-merge-multiple-dataframes\n\npc1 = df1[df1['Pclass']==1]\npc2 = df1[df1['Pclass']==2]\npc3 = df1[df1['Pclass']==3]\n\npc1_df = pc1.groupby(['Survived'])['Pclass'].agg('count').reset_index().rename(columns={'Pclass':'1'})\npc2_df = pc2.groupby(['Survived'])['Pclass'].agg('count').reset_index().rename(columns={'Pclass':'2'})\npc3_df = pc3.groupby(['Survived'])['Pclass'].agg('count').reset_index().rename(columns={'Pclass':'3'})\n\nclass_sur = pd.merge(pc1_df, pc2_df, on='Survived')\nclass_survial = pd.merge(class_sur, pc3_df, on='Survived')\n\nclass_survial","d9c150a4":"# CREATE GRAPH OF SURVIVAL STATUS BY PASSENGER CLASS \n# THE MAJORITY OF PEOPLE THAT DIED WERE IN 3RD CLASS \n# MOST SURVIVORS CAME FROM 1ST CLASS \n\nclass_survial.plot(x='Survived',\n        kind='bar',\n        stacked=False,\n        title='Grouped Bar Graph with dataframe')","7421c2d9":"# SURVIVAL BY WHERE THE PASSENGER EMBARKED \n\n#CREATE A DATAFRAME THAT GROUPS THE DATA BY SURVIVAL STATUS AND EMBARKED LOCATION\n# RESETS THE INDEX AND RENAMES THE COLUMN\nembarked = df1.groupby(['Survived','Embarked'])['PassengerId'].agg('count').reset_index().rename(columns={'PassengerId':'passengers'})\n\n# ADDS A NEW COLUMN TO THE DATAFRAME THAT IS THE PERCENTAGE OF EMBARKED TOTAL\nembarked['pc_of_location'] = round((embarked.passengers \/ embarked.groupby('Embarked')['passengers'].transform('sum'))*100,2)\n\n# SORT THE DATAFRAME BY EMBARKED LOCATION \nembarked.sort_values('Embarked', axis = 0, ascending = True, inplace = True)\n\n# SHOW THE DATAFRAME \nembarked\n\n# THE MAJORITY OF PEOPLE EMBARKED FROM SOUTHHAMPTION AND THE HIGHEST AMOUNT OF DEATHS CAME FROM SOUTHHAMPTON PASSENGERS BOTH IN \n# TERMS OF VOLUME AND PERCENTAGE ","372f91ee":"#SET THE SEABORN STYLE\nsns.set()\n\n# THE POINT PLOT SHOWS THE AVERAGE VALUE BY EMBARKED LOCATION AND SPLIT BY GENDER \n# MORE PEOPLE ON AVERAGE SURVIVED THAT EMBARKED FROM Cherbourg\n# MORE WOMEN SURVIVED FROM ALL EMBARKED LOCATIONS \n\nax = sns.pointplot(x=\"Embarked\", y=\"Survived\", hue='Sex', data=df1)","93c235a2":"# SURVIAL BY PASSENGERS THAT HAD SIBLINGS AND\/OR PARENTS ABOARD \n\n# DATA CONATINED IN \n# sibsp = number of siblings \/ spouses aboard the Titanic\n# parch = number of parents \/ children aboard the Titanic\n\n# CREATE A NEW COLUMN COMBINING sibsp & parch INTO A NEW COLUMN STATING IF THE PASSENGERS WAS NOT ALONE \n\ndata = [df1] \n\nfor column in data:\n    column['relatives'] = column['SibSp'] + column['Parch'] # CREATE A COLUMNN ('relative') IN df1 SUMING THE VALUES \n    column.loc[column['relatives']>0, 'travelling_alone'] = 0 # IS THE VALUE >0 IF YES THEN 0 IN THE NEW COLUMND travelling_alone\n    column.loc[column['relatives'] == 0, 'travelling_alone'] = 1 # SAME AS ABOVE BUT ==0 \n    column['travelling_alone'] = column['travelling_alone'].astype(int) # CHANGE THE OUTPUT TO AN INT \n    \ndf1.head()\n","8ff98f83":"# IF YOU HAD 1-3 RELATIVES ABOARD YOU HAD A BETTER CHANCE OF SURVIAL \n# PAST 4 YOUR CHANCE OF SURVIAL DROPS \n\nax = sns.pointplot(x=\"relatives\", y=\"Survived\", data=df1)","d78ff409":"# CAN WE LEARN ANYTHING FROM THE FARE COLUMN?\n# DID HOW MUCH YOU PAID EFFECT SURVIAL? \n\n\n# WHAT DOES THE FARE COLUMN LOOK LIKE \ndf1['Fare'].describe()","0a3253f8":"# THERE IS ONE PASSENGER THAT PAID 512 \n# THIS OUTLIER DOES NOT HELP THE DATA \n# FROM ABOVE WE CAN SEE THE 75th PERCENTILE IS 31\n# THERE ARE SEVERAL OUTLIERS ABOVE THE MAXIMUM OF THE BOX PLOT \n\nplt.figure(figsize = (12,6))\n\nax = sns.boxplot(x=df1[\"Fare\"])","0fd4d927":"# THE WIDE RANGE OF VALUES WILL NEED TO BE DEALT WITH IN THE DATA PROCESSING PHASE \nplt.figure(figsize = (12,6))\n\n# CREATE SEPERATE SURVIAL DATAFRAMES \nsurvived = df1[df1['Survived'] ==1]\nnot_survived = df1[df1['Survived']==0]\n\n\n# CREATE A HISTOGRAM SHOWING SURVIVAL BY FARE PAID \nax = sns.histplot(survived['Fare'], label='survived', bins='auto', kde=False)\nax = sns.histplot(not_survived['Fare'], label='not_survived', bins='auto', kde=False)\nax.legend()\nax.set_title('Survived')\n","c8736255":"# WHAT DOES THE CABIN COLUMN TELL US? \n# WE KNOW FROM ABOVE 77% OF THE ROWS ARE NULL \n# OF THE 204 ROWS 147 OF THEM ARE UNIQUE \n# HOWEVER THE LETTER PART OF THE CABIN DOES REVEAL WHAT LEVEL OF THE SHIP A PASSENGER WAS ON \n# COULD BE WORTH EXATRACT THE LETTER AND CREATING A NEW COLUMN\n\ndf1['Cabin'].describe()","171ec38e":"# UNSURPRISINGLY THE NAME COLUMN CONTAINS 891 UNIQUE ROWS \n\ndf1['Name'].describe()","efee1ef9":"# FOR THIS EXERCISE THE TEST DATA IS ALREADY IN A SEPERATE CSV TO THE TRAIN DATA \n\ndf_test = pd.read_csv('..\/input\/titantic-survial\/test.csv')\ndf_test.head()","fdc54883":"df_test.info()","6eb236f7":"# NUMBER OF AND PERCENTAGE OF NULL VALUES IN TEST DATA \n\ntotal = df_test.isnull().sum().sort_values(ascending=False)\npercent_1 = round(df_test.isnull().sum()\/df_test.isnull().count()*100,1).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_1], axis=1, keys=['Total', '%'])\nmissing_data.head()","1456fd45":"# HOW TO DEAL WITH THE NULL VALUES IN AGE \n# THE BOX PLOT SHOWS THERE ARE ONLY A COUPLE OUTLIERS IN THE COLUMN THE MAJORITY OF THE DATA FITS WITHIN THE MIN AND MAX \n# THE MEAN AND MEDIAN AGES LOOK PRETTY CLOSE TOGETHER \n# THEREFORE I WILL FILL NUULL VALUES WITH THE AVERAGE AGE \n\nsns.boxplot(x= df_test['Age'])\n\nprint('Average age test = ', round(df_test['Age'].mean(),0))\nprint('Average age train = ', round(df1['Age'].mean(),0))","b6d45736":"# LOOP BELOW FILLS THE NULL VALUES ACROSS THE TEST (df_test) and TRAIN (df1) DATASETS  IN THE AGE COLUMN \ndata = [df1, df_test]\n\nfor column in data:\n    avg_age = round(df_test['Age'].mean(),0)\n    column['Age'] = column['Age'].fillna(avg_age)\n    \n\n# COUNT NULLS IN EACH AGE COLUMN \nprint('df1 nulls = ', df1[\"Age\"].isnull().sum())\nprint('df_test nulls = ', df_test[\"Age\"].isnull().sum())","dd834cc1":"# CREATE THE COMBINED sibsp AND parch COLUMN FOR THE TEST DATA \n\n# DATA CONATINED IN \n# sibsp = number of siblings \/ spouses aboard the Titanic\n# parch = number of parents \/ children aboard the Titanic\n\n# CREATE A NEW COLUMN COMBINING sibsp & parch INTO A NEW COLUMN STATING IF THE PASSENGERS WAS NOT ALONE \n\ndata = [df_test, df1] \n\nfor column in data:\n    column['relatives'] = column['SibSp'] + column['Parch'] # CREATE A COLUMNN ('relative') IN df1 SUMING THE VALUES \n    column.loc[column['relatives']>0, 'travelling_alone'] = 0 # IS THE VALUE >0 IF YES THEN 0 IN THE NEW COLUMND travelling_alone\n    column.loc[column['relatives'] == 0, 'travelling_alone'] = 1 # SAME AS ABOVE BUT ==0 \n    column['travelling_alone'] = column['travelling_alone'].astype(int) # CHANGE THE OUTPUT TO AN INT \n    \n#df_test.head()\n","8e1c0d40":"# CONVERT THE SEX COLUMN FROM A SINGLE STRING COLUMN TO INDIVUDAL GENDER COLUMNS \n\n\ndata = [df1, df_test]\n\nfor column in data:\n    column.loc[column['Sex'] == 'male', 'passenger_male']=1\n    column.loc[column['Sex'] == 'female', 'passenger_male']=0\n    column.loc[column['Sex'] == 'female', 'passenger_female']=1\n    column.loc[column['Sex'] == 'male', 'passenger_female']=0      \n\n#df_test.head()\n","912bc47c":"# NORMALIZE THE AGE AND FARE COLUMNS \n\n# THE TEST DATA HAS 1 NULL VALUE IN THE FARE COLUMN, THIS WILL BE FILLED WITH THE AVERAGE \navg_fare = df1['Fare'].mean()\ndf_test['Fare'] = df1['Fare'].fillna(avg_fare)\n\ndata = [df1, df_test]\n\nfor dataset in data:\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(dataset[['Age', 'Fare']])\n    dataset['Age'] = scaled[:,0]\n    dataset['Fare'] = scaled[:,1]","86ee2630":"# CREATE NEW COLUMNS FOR EACH PASEENGER CLASS \n# REASON BEING A MODEL MAY INFER THAT PCLASS 3 IS GREATER THAN 1 WHICH WOULD NOT BE HELPFUL IN ASSESSING IF A PASSENGER \n    #SURVIVED\n# PLUS IN THIS SCENARIO 1 IS GREATER THAN 3 BECAUSE IT REFERS TO THE PASSENGER CLASS \n\ndata = [df1, df_test]\n\nfor dataset in data:\n    ohe = OneHotEncoder(categories = 'auto')\n    hot = ohe.fit_transform(dataset[['Pclass']]).toarray()\n    dataset['first_class'] = hot[:,0]\n    dataset['second_class'] = hot[:,1]\n    dataset['third_class'] = hot[:,2]\n","7cd4f629":"# CREATE NEW COLUMNS FOR WHERE THE CUSTOMERS EMBARKED \n# THE TRAINING DATA HAS 2 NULL VALUES WHICH WILL BE FILLED WITH THE MOST COMMON VALUE - S \n# THE TEST DATA DID NOT HAVE ANY NULL VALUES FOR EMBARKED \n\ncommon_value = ('S')\ndf1['Embarked'] = df1['Embarked'].fillna(common_value)\n\ndata = [df1, df_test]\n\nfor dataset in data:\n    ohe = OneHotEncoder(categories = 'auto')\n    hot = ohe.fit_transform(dataset[['Embarked']]).toarray()\n    dataset['embarked_c'] = hot[:,0]\n    dataset['embarked_q'] = hot[:,1]\n    dataset['embarked_s'] = hot[:,2]\n","dd2a1bbd":"X_train = df1[['Age','Fare','passenger_male','passenger_female','relatives','travelling_alone','first_class',\n              'second_class','third_class','embarked_c','embarked_q','embarked_c']]\n\nY_train = df1[['Survived']]\n\nX_test = df_test[['Age','Fare','passenger_male','passenger_female','relatives','travelling_alone','first_class',\n              'second_class','third_class','embarked_c','embarked_q','embarked_c']]","43147e79":"print(f'x_train shape', X_train.shape)\nprint(f'y_train shape', Y_train.shape)\nprint(f'x_test shape', X_test.shape)","09cd9f31":"random_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, Y_train.values.ravel())\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 4)\n\nprint('complete')","1c62f626":"logreg = LogisticRegression()\n\nlogreg.fit(X_train, Y_train.values.ravel())\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 4)\n\nprint('complete')","e046ef28":"knn = KNeighborsClassifier(n_neighbors = 3) \n\nknn.fit(X_train, Y_train.values.ravel())  \n\nY_pred = knn.predict(X_test)  \n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 4)\n\nprint('complete')","967f03de":"decision_tree = DecisionTreeClassifier() \n\ndecision_tree.fit(X_train, Y_train)  \n\nY_pred = decision_tree.predict(X_test)  \n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 4)\n\nprint('complete')","238bfd8b":"results = pd.DataFrame({\n                        'Model': ['Random Forst','Logistic Regression','KNN', 'Decision Tree'],\n                        'Score': [acc_random_forest, acc_log, acc_knn, acc_decision_tree ]\n                        })\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","fb6a6e49":"#INSTANTIATE THE MODEL\nrf = RandomForestClassifier(n_estimators=100)\n\n# SET UP CROSS VALIDATION USING RANDOM FOREST, 10 FOLDS AND ACCURACY AS THE SCORING METHOD  \nscores = cross_val_score(rf, X_train, Y_train.values.ravel(), cv=10, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","c335f069":"#CREATE A DATAFRAME CONTAINING THE FEATURE NAMES FROM THE X_TRAIN DATASET AND THE FEATURE IMPORTANCE \nfi = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\n\n# SORT THE DATAFRAME BY DESCENDING IMPORTANCE\nfi = fi.sort_values('importance',ascending=False).set_index('feature')\nfi","52fefcdc":"fi.plot.bar(figsize=(12,6))","0237b19d":"X_train = df1[['Age','Fare','passenger_male','passenger_female','relatives','travelling_alone','first_class',\n              'second_class','third_class']]\n\nY_train = df1[['Survived']]\n\nX_test = df_test[['Age','Fare','passenger_male','passenger_female','relatives','travelling_alone','first_class',\n              'second_class','third_class']]","53c09ab7":"print(f'x_train shape', X_train.shape)\nprint(f'y_train shape', Y_train.shape)\nprint(f'x_test shape', X_test.shape)","b7f3078b":"random_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, Y_train.values.ravel())\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 4)","6a032ade":"print(f'Accuracy', round(acc_random_forest,4,), \"%\")","29f2cff3":"# SET UP CROSS VALIDATION USING RANDOM FOREST, 10 FOLDS AND ACCURACY AS THE SCORING METHOD  \nscores = cross_val_score(rf, X_train, Y_train.values.ravel(), cv=10, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","70946134":"predictions = cross_val_predict(random_forest, X_train, Y_train.values.ravel(), cv=10)\n\nprint(\"Precision:\", round(precision_score(Y_train, predictions)*100,2))\nprint(\"Recall:\", round(recall_score(Y_train, predictions)*100,2))","ea363c09":"# GETTING THE PROBABILITIES OF OUR PREDICTIONS \ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\n# ROC VALUES \nfpr, tpr, thresholds = roc_curve(Y_train, y_scores)\nr_a_score = round(roc_auc_score(Y_train, y_scores),4)\n\n# PLOTTING THE VALUES \nplt.figure(figsize=(14, 7))\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC AUC CURVE', fontsize=16) \n\nplt.show()\n\n# PRINT THE ROC-AUC SCORE\nprint(\"ROC-AUC-Score:\", r_a_score)","d6592b6e":"r_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","501b3814":"## ROC AUC Curve\nROC AUC curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances). The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Of course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is.","c5806acd":"Data taken from Kaggle\nhttps:\/\/www.kaggle.com\/c\/titanic","af7ab8ff":"## Data Processing","fd0b5eec":"### K Nearest Neighbor","1cebc0fb":"## Precision and Recall\n\nPrecision \n\nPrecision attempts to answer the following question: What proportion of positive identifications was actually correct? \n\n\nRecall\n\nRecall attempts to answer the following question. What proportion of actual positives was identified correctly? The recall is the measure of our model correctly identifying True Positives\n\nWe've created the predictions using k-fold cross validation again. \n\nThe precision tells us when our model predictos someone will survive it is right 77% of the time. \nThe recall tells us that is predicted the survival of 74% of the people who actually survived. ","40fc3f0a":"## K-Fold Cross Validation\nCross validation returns a more realistic mean accuracy of 80% and a standard deviation of 5%. Meaning the accuracy of our model can differ +\/- 5% ","cbf38430":"## Modelling ","bf947ee4":"This shows us that fare and age are the most importance features and the embarked columns are not very useful ","dffaa1a5":"### K-Fold Cross Validation\nCross validation returns a more realistic mean accuracy of 80% and a standard deviation of 5%. Meaning the accuracy of our model can differ +\/- 5% ","4c00c335":"## Training random forest again\nRemoving the embarked features from the data and training random forest again","6be8b671":"## Model Evaluation\nReturn the accuracy (the percentage of labels that were correctly predicted) for each model applied to the training set","89df90e7":"### Logistic Regression","ee6cce0f":"## Random Forest\nRandom forst comes out on top but it's score is quite high at 97%. This could be the training data or overfitting. As such I'll try using cross validation on the training set to see if the accuracy score changes. ","3e0a888f":"### Decision Tree ","a13bba8f":"### Feature importance \nA benefit of using random forest is we are able to see the importance of each feature us SKlearn. In decision trees, every node is a condition of how to split values in a single feature, so that similar values of the dependent variable end up in the same set after the split. The condition is based on impurity, which in case of classification problems is Gini impurity\/information gain (entropy). In the case of Random Forest, we are talking about averaging the decrease in impurity over trees. SKlearn computes this score automaticall for each feature after training and scales the results so that the sum of all importances is equal to 1.","b1b865f0":"### Random Forest","976fab32":"## Create the tables for modelling","c84b6b76":"## ROC AUC Score\nThe ROC AUC Score is the corresponding score to the ROC AUC Curve. It is computed by measuring the area under the curve, which is called AUC.\nA classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.","3bce1650":"## Data Exploration","3218845f":"# Titanic Survival project","98699212":"## Random forest evaluation two\n### Accuracy\nDespite having a smaller amount of features the model returns the same accuracy score as before. ","8f8f34d3":"## Load packages and CSV"}}