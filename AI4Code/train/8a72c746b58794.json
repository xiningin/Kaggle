{"cell_type":{"916fbc0c":"code","2fd15349":"code","8729303c":"code","898cb931":"code","d21eb4f5":"code","95733da7":"code","ca05b767":"code","73901187":"code","777d583f":"code","3bfbdfd8":"code","d4e4f5e2":"code","768c12fa":"code","5ecc9795":"code","9523a5c0":"code","58f4bdab":"code","96713259":"code","db4a2149":"code","55bb6a74":"code","7d60d3f3":"code","b8f0f4a1":"code","cac287f0":"code","8905921a":"code","86907a9f":"code","4489557f":"code","dfe1a41f":"code","dade5928":"code","a74d6497":"code","0a9fedb3":"code","4a832d60":"code","57eded4e":"code","429466ba":"code","46943117":"code","21b74899":"code","f825b583":"code","88f71194":"code","fb1c28b9":"code","6e130f3e":"code","a685491e":"code","a52498f2":"code","8e043bc5":"code","3e0f9353":"code","e26323e4":"code","eb7b9056":"code","c13e8433":"code","eae0780a":"code","c7d5f63f":"code","8947c2df":"code","ad57568a":"code","984c1a88":"code","ff21823c":"code","f83f5e2b":"code","b201b354":"code","34484e45":"code","c4a8f99a":"code","740a09e0":"code","4fdbd539":"code","1f4f7871":"code","76f5a63c":"code","5287526b":"code","dcc79e65":"code","5e06be3b":"code","403290df":"code","6fdcaf9b":"code","29ab3311":"code","9ad75c76":"code","21936dbf":"code","dacc4827":"code","40c664cf":"code","4129aa1c":"code","7aa4b406":"code","19ed02ef":"code","875176f9":"code","e4cee34b":"code","52ebc694":"code","07ea0bee":"code","c393c1d5":"code","f6278564":"code","1f20770c":"code","50886824":"code","7b7acbdb":"code","71d59d27":"code","8b2416ef":"code","7b6aa974":"code","4b57e991":"code","d12bc9dc":"code","b1dd2bc3":"code","bd6252d0":"code","e3b19b60":"code","8ff3b4cf":"code","8700f135":"code","d5392c5c":"code","073088cc":"code","9bf90262":"code","57255574":"code","f30a6494":"code","0074b147":"code","223c4e7a":"code","b78b81cc":"code","effdcab0":"markdown","604937cf":"markdown","9c55a111":"markdown","d5b8aef8":"markdown","1941b077":"markdown","81dce24a":"markdown","f6797deb":"markdown","63f34812":"markdown","f2b91689":"markdown","dc02e27a":"markdown","a649723f":"markdown","5d06a54f":"markdown","ba3d60ca":"markdown","d29b1a9f":"markdown","f912633f":"markdown","f5b45d88":"markdown","f02bec91":"markdown","ac8044b5":"markdown","089082c9":"markdown","19acf0cf":"markdown","6fabf616":"markdown","d9b82595":"markdown","c4f0188c":"markdown","f5434798":"markdown","5f436485":"markdown","c8fa5c4d":"markdown","03be8fca":"markdown","8cc7ca3b":"markdown","d3a9c2d6":"markdown","3a359079":"markdown","f543d3e7":"markdown","39c2527c":"markdown","2b2aa750":"markdown","8eeb85f6":"markdown","7af0a541":"markdown","7deff508":"markdown","858ab166":"markdown","8d34f099":"markdown","366d459e":"markdown","fb4fd19d":"markdown","fe71c0b7":"markdown","2591bf96":"markdown","ea7f16d6":"markdown","0998b79c":"markdown","69516c75":"markdown","788a76c2":"markdown","c5492e96":"markdown","5023380a":"markdown","e71fa133":"markdown","4c79d210":"markdown","965a8e0c":"markdown","66b7164a":"markdown","f785f916":"markdown","b5d216f2":"markdown","c889cfe4":"markdown","b0c42e7a":"markdown","adbbdd06":"markdown","56493654":"markdown","32bfed25":"markdown","642142ce":"markdown","8de54b19":"markdown","b46d254d":"markdown","f87bb095":"markdown","13984b5b":"markdown","c6782f8a":"markdown","bc8b1f6e":"markdown","47ad4c45":"markdown"},"source":{"916fbc0c":"import random\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nimport types\nimport inspect\nimport plotly.plotly as py\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\nfrom matplotlib import cm\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\npy.init_notebook_mode(connected=False)\n\nnp.random.seed(201807005)\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame\n    \ndef plt_hist(x, bins=30):\n    # plt.hist() can be very slow.\n    histo, edges = np.histogram(x, bins=bins)\n    plt.bar(0.5 * edges[1:] + 0.5 * edges[:-1], histo, width=(edges[-1] - edges[0])\/(len(edges) + 1))","2fd15349":"data = pd.read_csv('..\/input\/gaia-dr2-rave-35.csv', dtype={'source_id': str})","8729303c":"len(data)","898cb931":"train_mask = np.random.rand(len(data)) < 0.9\nwork_data = data[train_mask]\nwork_data.reset_index(inplace=True, drop=True)\ntest_data = data[~train_mask]\ntest_data.reset_index(inplace=True, drop=True)","d21eb4f5":"data.columns","95733da7":"def get_cv_model_transform(data_frame, label_extractor, var_extractor, trainer, response_column='Response', id_column='source_id', n_splits=2, scale = False):\n    shuffled_frame = data_frame.sample(frac=1).reset_index(drop=True)\n    nrow = len(data_frame)\n    kf = KFold(n_splits=n_splits)\n    response_map = dict()\n    default_model = None\n    default_scaler = None\n    split_idx = 0\n    for train_idx, test_idx in kf.split(shuffled_frame):\n        train_frame = shuffled_frame.iloc[train_idx]\n        test_frame = shuffled_frame.iloc[test_idx]\n        train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n        train_vars = var_extractor(train_frame)\n        test_vars = var_extractor(test_frame)\n        if scale:\n            default_scaler = StandardScaler()  \n            default_scaler.fit(train_vars)\n            train_vars = default_scaler.transform(train_vars)  \n            test_vars = default_scaler.transform(test_vars) \n        default_model = trainer.fit(train_vars, train_labels)\n        test_frame.reset_index(inplace=True, drop=True)\n        test_responses = default_model.predict(test_vars)\n        test_id = test_frame[id_column]\n        assert len(test_id) == len(test_responses)\n        for i in range(len(test_id)):\n            response = test_responses[i]\n            key = str(test_id[i])\n            response_map[key] = response\n        split_idx += 1\n    response_id_set = set(response_map)\n        \n    def _transform(_frame):\n        _in_trained_set = _frame[id_column].astype(str).isin(response_id_set)\n        _trained_frame = _frame[_in_trained_set].copy()\n        _trained_frame.reset_index(inplace=True, drop=True)\n        if len(_trained_frame) > 0:\n            _trained_id = _trained_frame[id_column]\n            _tn = len(_trained_id)\n            _response = pd.Series([None] * _tn)\n            for i in range(_tn):\n                _response[i] = response_map[str(_trained_id[i])]\n            _trained_frame[response_column] = _response\n        _remain_frame = _frame[~_in_trained_set].copy()\n        if len(_remain_frame) > 0:\n            _vars = var_extractor(_remain_frame)\n            if default_scaler is not None:\n                _vars = default_scaler.transform(_vars)\n            _response = default_model.predict(_vars)\n            _remain_frame[response_column] = _response\n        _frames_list = [_trained_frame, _remain_frame]\n        _concat_frame = pd_concat(_frames_list)\n        _concat_frame.reset_index(inplace=True, drop=True)\n        return _concat_frame\n    return _transform","ca05b767":"def print_evaluation(data_frame, label_column, response_column):\n    _response = response_column(data_frame) if isinstance(response_column, types.FunctionType) else data_frame[response_column]\n    _label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    _error = _response - _label\n    _rmse = math.sqrt(sum(_error ** 2) \/ len(data_frame))\n    _correl = stats.pearsonr(_response, _label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (_rmse, _correl,), flush=True)\n    ","73901187":"MIN_PARALLAX = 2.5\nMAX_PARALLAX_ERROR = 0.2","777d583f":"def transform_init(data_frame):    \n    parallax = data_frame['parallax']\n    parallax_error = data_frame['parallax_error']\n    new_frame = data_frame[(parallax >= MIN_PARALLAX) & (parallax_error <= MAX_PARALLAX_ERROR)].copy()\n    new_frame.reset_index(inplace=True, drop=True)\n    distance = 1000.0 \/ new_frame['parallax']\n    new_frame['distance'] = distance\n    new_frame['abs_mag_ne'] = new_frame['phot_g_mean_mag'] - 5 * (np.log10(distance) - 1)\n    return new_frame","3bfbdfd8":"work_data = transform_init(work_data)\nlen(work_data)","d4e4f5e2":"wd_distance_mod = 5 * (np.log10(work_data['distance']) - 1)\nwd_photo_distance_mod = 5 * (np.log10(1000.0 \/ work_data['r_parallax']) - 1)","768c12fa":"np.sqrt(mean_squared_error(wd_distance_mod, wd_photo_distance_mod))","5ecc9795":"GR = 100","9523a5c0":"def extract_model_vars(data_frame):\n    distance = data_frame['distance'].values\n    log_distance = np.log(distance)\n    g_mag = data_frame['phot_g_mean_mag']\n    bp_mag = data_frame['phot_bp_mean_mag']\n    rp_mag = data_frame['phot_rp_mean_mag']\n    longitude_raw = data_frame['l'].values\n    longitude = [(lng if lng <= 180 else lng - 360) for lng in longitude_raw]\n    latitude = data_frame['b'].values\n    sin_lat = np.sin(np.deg2rad(latitude))\n    lat_ext_metric_prelim = np.abs(GR \/ sin_lat)\n    lat_ext_metric = [min(distance[i], lat_ext_metric_prelim[i]) for i in range(len(data_frame))]\n    metallicity = data_frame['r_metallicity']\n    radial_velocity = data_frame['r_hrv']\n    mg = data_frame['r_mg']\n    si = data_frame['r_si']\n    fe = data_frame['r_fe']\n    jmag = data_frame['r_jmag_2mass']\n    hmag = data_frame['r_hmag_2mass']\n    kmag = data_frame['r_kmag_2mass']\n    aw_m1 = data_frame['r_w1mag_allwise']\n    aw_m2 = data_frame['r_w2mag_allwise']\n    aw_m3 = data_frame['r_w3mag_allwise']\n    aw_m4 = data_frame['r_w4mag_allwise']\n    denis_imag = data_frame['r_imag_denis']\n    denis_jmag = data_frame['r_jmag_denis']\n    denis_kmag = data_frame['r_kmag_denis']    \n    apass_bmag = data_frame['r_bmag_apassdr9']\n    apass_vmag = data_frame['r_vmag_apassdr9']\n    apass_rpmag = data_frame['r_rpmag_apassdr9']\n    apass_ipmag = data_frame['r_ipmag_apassdr9']\n    \n    color1 = hmag - jmag\n    color2 = kmag - hmag\n    color3 = rp_mag - kmag\n    color4 = g_mag - rp_mag\n    color5 = bp_mag - g_mag\n    color6 = aw_m2 - aw_m1\n    color7 = aw_m3 - aw_m2\n    color8 = aw_m4 - aw_m3\n    color9 = rp_mag - aw_m4\n    color10 = g_mag - denis_imag\n    color11 = denis_imag - denis_jmag\n    color12 = denis_jmag - denis_kmag    \n    color13 = apass_rpmag - apass_ipmag\n    color14 = apass_vmag - apass_rpmag\n    color15 = apass_bmag - apass_vmag\n    color16 = g_mag - apass_bmag\n    \n    return np.transpose([log_distance, distance,\n            color1, color2, color3, color4, color5,\n            color6, color7, color8, color9,\n            color10, color11, color12,\n            color13, color14, color15, color16,\n            mg, si, fe, metallicity,\n            latitude, longitude, lat_ext_metric\n            ])    ","58f4bdab":"LABEL_COLUMN = 'phot_g_mean_mag'","96713259":"transform_linear = get_cv_model_transform(work_data, LABEL_COLUMN, extract_model_vars, linear_model.LinearRegression(), n_splits=2, response_column='linear_' + LABEL_COLUMN, scale=True)\nwork_data = transform_linear(work_data)","db4a2149":"print_evaluation(work_data, LABEL_COLUMN, 'linear_' + LABEL_COLUMN)","55bb6a74":"def get_gbm_trainer():\n    return xgb.XGBRegressor(n_estimators=550, learning_rate=0.05, gamma=0.01, subsample=0.75,\n                           colsample_bytree=1.0, max_depth=8, random_state=np.random.randint(1,10000))","7d60d3f3":"def get_gbm_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, \n                get_gbm_trainer(), \n                n_splits=2, response_column='gbm_' + label_column)","b8f0f4a1":"transform_gbm = get_gbm_transform(LABEL_COLUMN)\nwork_data = transform_gbm(work_data)","cac287f0":"print_evaluation(work_data, LABEL_COLUMN, 'gbm_' + LABEL_COLUMN)","8905921a":"def get_gbm2_trainer():\n    return xgb.XGBRegressor(n_estimators=500, learning_rate=0.07, gamma=0.003, subsample=0.80,\n                           colsample_bytree=1.0, max_depth=7, random_state=np.random.randint(1,10000))","86907a9f":"def get_gbm2_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, \n                get_gbm2_trainer(), \n                n_splits=2, response_column='gbm2_' + label_column)","4489557f":"transform_gbm2 = get_gbm2_transform(LABEL_COLUMN)\nwork_data = transform_gbm2(work_data)","dfe1a41f":"print_evaluation(work_data, LABEL_COLUMN, 'gbm2_' + LABEL_COLUMN)","dade5928":"nn_seed = np.random.randint(1,10000)\ndef get_nn_trainer():\n    return MLPRegressor(hidden_layer_sizes=(30, 10), max_iter=500, alpha=0.1, random_state=nn_seed)","a74d6497":"def get_nn_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, get_nn_trainer(), \n        n_splits=3, response_column='nn_' + label_column, scale=True)","0a9fedb3":"transform_nn = get_nn_transform(LABEL_COLUMN)\nwork_data = transform_nn(work_data)","4a832d60":"print_evaluation(work_data, LABEL_COLUMN, 'nn_' + LABEL_COLUMN)","57eded4e":"def extract_blend_vars(data_frame):\n    gbm_responses = data_frame['gbm_' + LABEL_COLUMN].values\n    gbm2_responses = data_frame['gbm2_' + LABEL_COLUMN].values\n    nn_responses = data_frame['nn_' + LABEL_COLUMN].values\n    linear_responses = data_frame['linear_' + LABEL_COLUMN].values\n    return np.transpose([gbm_responses, gbm2_responses, nn_responses, linear_responses])","429466ba":"def get_blend_trainer():\n    return linear_model.LinearRegression()","46943117":"def get_blend_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_blend_vars, get_blend_trainer(), \n                n_splits=5, response_column='blend_' + label_column)","21b74899":"transform_blend = get_blend_transform(LABEL_COLUMN)\nwork_data = transform_blend(work_data)","f825b583":"print_evaluation(work_data, LABEL_COLUMN, 'blend_' + LABEL_COLUMN)","88f71194":"MODEL_PREFIX = 'blend_'","fb1c28b9":"def error(data_frame, label_column):\n    return data_frame[label_column] - data_frame[MODEL_PREFIX + label_column]","6e130f3e":"def transform_error(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['error_' + LABEL_COLUMN] = error(data_frame, LABEL_COLUMN)\n    return new_frame","a685491e":"work_data = transform_error(work_data)","a52498f2":"def get_sq_res_label(data_frame):\n    return data_frame['error_' + LABEL_COLUMN] ** 2","8e043bc5":"def extract_error_vars(data_frame):\n    parallax = data_frame['parallax']\n    parallax_error = data_frame['parallax_error']\n    parallax_high = parallax + parallax_error\n    parallax_low = parallax - parallax_error\n    var_error_diff = np.log(parallax_high) - np.log(parallax_low)\n    distance = data_frame['distance']\n    longitude = data_frame['l']\n    latitude = data_frame['b']\n    radial_velocity = data_frame['r_hrv']\n    return np.transpose([\n        var_error_diff,\n        distance,\n        longitude,\n        latitude,\n        radial_velocity\n    ])","3e0f9353":"def get_error_trainer():\n    return RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=2)","e26323e4":"transform_expected_sq_res = get_cv_model_transform(work_data, get_sq_res_label, extract_error_vars, get_error_trainer(), \n                n_splits=3, response_column='expected_sq_res_' + LABEL_COLUMN)","eb7b9056":"work_data = transform_expected_sq_res(work_data)","c13e8433":"print_evaluation(work_data, get_sq_res_label, 'expected_sq_res_' + LABEL_COLUMN)","eae0780a":"def transform_anomaly(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['anomaly_' + LABEL_COLUMN] = new_frame['error_' + LABEL_COLUMN] \/ np.sqrt(new_frame['expected_sq_res_' + LABEL_COLUMN].astype(float))\n    return new_frame","c7d5f63f":"work_data = transform_anomaly(work_data)","8947c2df":"transform_list = [transform_init, \n                  transform_linear, transform_gbm, transform_gbm2, transform_nn, \n                  transform_blend,\n                  transform_error, transform_expected_sq_res, transform_anomaly]","ad57568a":"def combined_transform(data_frame):\n    _frame = data_frame\n    for t in transform_list:\n        _frame = t(_frame)\n    return _frame","984c1a88":"test_data = combined_transform(test_data)","ff21823c":"np.std(test_data['error_' + LABEL_COLUMN])","f83f5e2b":"data = combined_transform(data)","b201b354":"CAND_SD_THRESHOLD = 3.0","34484e45":"data_anomalies = data['anomaly_' + LABEL_COLUMN]","c4a8f99a":"anomaly_std = np.std(data_anomalies)","740a09e0":"anomaly_std","4fdbd539":"cand_threshold = anomaly_std * CAND_SD_THRESHOLD\ncandidates = data[data_anomalies >= cand_threshold]\nlen(candidates)","1f4f7871":"bright_control_group = data.sort_values('anomaly_' + LABEL_COLUMN, ascending=True).head(len(candidates))","76f5a63c":"normal_star_pool = data[(data_anomalies < anomaly_std) & (data_anomalies > -anomaly_std)]\nnormal_control_group = normal_star_pool.sample(len(candidates))","5287526b":"data_sample = data.sample(1000)","dcc79e65":"def abs_mag_value(data_frame, mag_column):\n    _distance_mod = 5 * np.log10(data_frame['distance']) - 5\n    return data_frame[mag_column] - _distance_mod","5e06be3b":"MODEL_RESPONSE_COLUMN = MODEL_PREFIX + LABEL_COLUMN\n\nplt.rcParams['figure.figsize'] = (10, 5)\nplt.scatter(abs_mag_value(data_sample, MODEL_RESPONSE_COLUMN), abs_mag_value(data_sample, LABEL_COLUMN), color=(0.5,0.5,0.5,0.5,), s=1)\nplt.scatter(abs_mag_value(candidates, MODEL_RESPONSE_COLUMN), abs_mag_value(candidates, LABEL_COLUMN), color='green', s=6)\nplt.scatter(abs_mag_value(bright_control_group, MODEL_RESPONSE_COLUMN), abs_mag_value(bright_control_group, LABEL_COLUMN), color='orange', s=2)\nplt.ylim(-1, 11)\nplt.gca().invert_yaxis()\nplt.title('Model Diagram')\nplt.xlabel('Modeled absolute magnitude')\nplt.ylabel('Observed absolute magnitude')\nplt.show()","403290df":"def color_value(data_frame):\n    return data_frame['phot_bp_mean_mag'] - data_frame['phot_rp_mean_mag']","6fdcaf9b":"def separation_y(color_index):\n    return -0.1 + 4.6 * color_index","29ab3311":"plt.rcParams['figure.figsize'] = (10, 5)\nplt.scatter(color_value(data_sample), abs_mag_value(data_sample, LABEL_COLUMN), color=(0.5,0.5,0.5,0.5,), s=1)\nplt.scatter(color_value(bright_control_group), abs_mag_value(bright_control_group, LABEL_COLUMN), color='orange', s=2)\nplt.scatter(color_value(candidates), abs_mag_value(candidates, LABEL_COLUMN), color='green', s=6)\nsep_x = np.linspace(0.5, 2.1, 100)\nsep_y = separation_y(sep_x)\nplt.plot(sep_x, sep_y, '--', color='blue')\nplt.ylim(-1, 11)\nplt.xlim(0.5, 2.1)\nplt.gca().invert_yaxis()\nplt.title('H-R Diagram')\nplt.xlabel('BP - RP color index')\nplt.ylabel('Absolute magnitude')\nplt.show()","9ad75c76":"mainseq_mask = abs_mag_value(candidates, LABEL_COLUMN) > separation_y(color_value(candidates))\ncandidates_mainseq = candidates[mainseq_mask]\ncandidates_bright = candidates[~mainseq_mask]","21936dbf":"len(candidates_mainseq)","dacc4827":"len(candidates_bright)","40c664cf":"def get_position_frame(data_frame):\n    new_frame = pd.DataFrame()\n    new_frame['source_id'] = data_frame['source_id'].values\n    distance = data_frame['distance'].values\n    latitude = np.deg2rad(data_frame['b'].values)\n    longitude = np.deg2rad(data_frame['l'].values)\n    new_frame['z'] = distance * np.sin(latitude)\n    projection = distance * np.cos(latitude)\n    new_frame['x'] = projection * np.cos(longitude)\n    new_frame['y'] = projection * np.sin(longitude)\n    new_frame['is_mainseq'] = (abs_mag_value(data_frame, LABEL_COLUMN) > separation_y(color_value(data_frame))).values\n    return new_frame","4129aa1c":"candidates_pos_frame = get_position_frame(candidates)","7aa4b406":"def plot_pos_frame(pos_frame, mainseq_color = 'red', other_color= 'red'):\n    star_color = [(mainseq_color if v else other_color) for v in pos_frame['is_mainseq'].values]\n    trace1 = go.Scatter3d(\n        x=pos_frame['x'],\n        y=pos_frame['y'],\n        z=pos_frame['z'],\n        mode='markers',\n        text=candidates['source_id'],\n        marker=dict(\n            size=4,\n            color=star_color,\n            opacity=0.67\n        )\n    )\n    scatter_data = [trace1]\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)","19ed02ef":"plot_pos_frame(candidates_pos_frame, 'blue', 'green')","875176f9":"bright_control_group_pos_frame = get_position_frame(bright_control_group)\nplot_pos_frame(bright_control_group_pos_frame, 'red', 'red')","e4cee34b":"candidates['distance'].describe()","52ebc694":"bright_control_group['distance'].describe()","07ea0bee":"anomalous_pos_frame = pd_concat([candidates_pos_frame, bright_control_group_pos_frame])","c393c1d5":"apf_len = len(anomalous_pos_frame)\napf_source_id_idx = anomalous_pos_frame.columns.get_loc('source_id')\napf_x_idx = anomalous_pos_frame.columns.get_loc('x')\napf_y_idx = anomalous_pos_frame.columns.get_loc('y')\napf_z_idx = anomalous_pos_frame.columns.get_loc('z')\nnew_row_list = []\nfor i in range(apf_len):\n    row1 = anomalous_pos_frame.iloc[i]\n    source1 = row1[apf_source_id_idx]\n    x1 = row1[apf_x_idx]\n    y1 = row1[apf_y_idx]\n    z1 = row1[apf_z_idx]\n    for j in range(i + 1, apf_len):\n        row2 = anomalous_pos_frame.iloc[j]\n        source2 = row2[apf_source_id_idx]\n        x2 = row2[apf_x_idx]\n        y2 = row2[apf_y_idx]\n        z2 = row2[apf_z_idx]\n        distance_sq = (x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2\n        new_row_list.append([source1, source2, distance_sq])\ncross_distance_frame = pd.DataFrame(new_row_list, columns=['source1', 'source2', 'distance_sq'])\ncross_distance_frame.sort_values('distance_sq', inplace=True)","f6278564":"candidate_source_set = set(candidates['source_id'])\ncross_distance_frame['source1_dim'] = cross_distance_frame['source1'].isin(candidate_source_set)\ncross_distance_frame['source2_dim'] = cross_distance_frame['source2'].isin(candidate_source_set)","1f20770c":"cross_distance_frame.head(5)","50886824":"len(cross_distance_frame[cross_distance_frame['source1_dim'] & cross_distance_frame['source2_dim']]) \/ len(cross_distance_frame)","7b7acbdb":"dim_match_frequency = pd.DataFrame(columns=['count', 'frequency'])\nfor ss in range(5, 2000, 10):\n    sub_frame = cross_distance_frame.iloc[:ss]\n    count = len(sub_frame)\n    freq = len(sub_frame[sub_frame['source1_dim'] & sub_frame['source2_dim']]) \/ count\n    dim_match_frequency.loc[len(dim_match_frequency)] = [count, freq]","71d59d27":"plt.rcParams['figure.figsize'] = (10, 5)\nexp_line_x = [0, 2000]\nexp_line_y = [0.25, 0.25]\nhs_counts = dim_match_frequency['count']\ntt_freqs = dim_match_frequency['frequency']\n# standard margin of error\nstd_moe = np.sqrt(0.25 * (1 - 0.25) \/ hs_counts)\ntt_freqs_low_95 = tt_freqs - 1.96 * std_moe\ntt_freqs_low_99 = tt_freqs - 2.575 * std_moe\nplt.plot(exp_line_x, exp_line_y, '--', color='orange')\nplt.plot(hs_counts, tt_freqs, color='black', linewidth=5)\nplt.plot(hs_counts, tt_freqs_low_95, color='red', linewidth=1)\nplt.plot(hs_counts, tt_freqs_low_99, color='blue', linewidth=1)\nplt.yticks(np.linspace(0, 1, 11))\nplt.ylim(0, 1.0)\nplt.xlim(0, 2000)\nplt.title('Results of clustering test')\nplt.xlabel('Size of headset of ordered cross-distance frame')\nplt.ylabel('Frequency of True-True rows')\nplt.show()","8b2416ef":"len(normal_star_pool)","7b6aa974":"def distance_matched_sample(data_frame, sample_size = 3000):\n    sorted_data_frame = data_frame.sort_values('distance').reset_index(drop=True)\n    sorted_pool_frame = normal_star_pool.sample(sample_size).sort_values('distance').reset_index(drop=True)\n    pool_index = 0\n    result_frame = pd.DataFrame(columns=data_frame.columns)\n    for i in range(len(sorted_data_frame)):\n        row1 = sorted_data_frame.iloc[i]\n        d1 = row1['distance']\n        min_diff = None\n        row_to_add = None\n        while True:\n            if pool_index >= len(sorted_pool_frame):\n                assert row_to_add is not None\n                result_frame.loc[len(result_frame)] = row_to_add\n                break                \n            row2 = sorted_pool_frame.iloc[pool_index]\n            d2 = row2['distance']\n            diff = np.abs(d2 - d1)\n            if min_diff is not None and diff >= min_diff:\n                result_frame.loc[len(result_frame)] = row_to_add\n                break\n            min_diff = diff\n            row_to_add = row2\n            pool_index += 1\n    return result_frame\n            ","4b57e991":"_dms = distance_matched_sample(candidates)","d12bc9dc":"_dms['distance'].describe()","b1dd2bc3":"def harmonic_mean_dist(data_frame):\n    apf_len = len(data_frame)\n    pos_frame = get_position_frame(data_frame)\n    apf_x_idx = pos_frame.columns.get_loc('x')\n    apf_y_idx = pos_frame.columns.get_loc('y')\n    apf_z_idx = pos_frame.columns.get_loc('z')\n    global_inv_d_sum = 0\n    pair_count = 0\n    for i in range(apf_len):\n        row1 = pos_frame.iloc[i]\n        x1 = row1[apf_x_idx]\n        y1 = row1[apf_y_idx]\n        z1 = row1[apf_z_idx]\n        for j in range(i + 1, apf_len):\n            row2 = pos_frame.iloc[j]\n            x2 = row2[apf_x_idx]\n            y2 = row2[apf_y_idx]\n            z2 = row2[apf_z_idx]\n            dist = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2)\n            assert dist != 0, '%d and %d' % (i, j,)\n            global_inv_d_sum += 1 \/ dist\n            pair_count += 1\n    return pair_count \/ global_inv_d_sum","bd6252d0":"candidates_pwdhm = harmonic_mean_dist(candidates)\ncandidates_pwdhm","e3b19b60":"def simulated_harmonic_mean_dists(data_frame, n = 100):\n    cm_list = []\n    for s in range(n):\n        dm_sample = distance_matched_sample(data_frame)\n        cm = harmonic_mean_dist(dm_sample)\n        cm_list.append(cm)\n    return cm_list    ","8ff3b4cf":"sim_pwdhms = simulated_harmonic_mean_dists(candidates)","8700f135":"sum(1 for cm in sim_pwdhms if cm < candidates_pwdhm)","d5392c5c":"SAVED_COLUMNS = ['source_id', 'ra', 'dec', 'pmra', 'pmdec', 'l', 'b', 'distance', 'abs_mag_ne', \n                 'error_' + LABEL_COLUMN, 'anomaly_' + LABEL_COLUMN]","073088cc":"def save_data(data_frame, file_name):\n    data_frame[SAVED_COLUMNS].to_csv(file_name, index=False)","9bf90262":"save_data(data, 'all-sources.csv')\nsave_data(candidates, 'dim-candidates.csv')\nsave_data(candidates_mainseq, 'dim-candidates-mainseq.csv')\nsave_data(bright_control_group, 'bright-controls.csv')\nsave_data(normal_control_group, 'normal-controls.csv')","57255574":"def sc(data_frame):\n    new_frame = data_frame[['source_id', 'ra', 'dec', 'pmra', 'pmdec', 'distance', 'abs_mag_ne', 'error_phot_g_mean_mag', 'anomaly_phot_g_mean_mag']]\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame","f30a6494":"sc(candidates[abs_mag_value(candidates, LABEL_COLUMN) >= 1.45 + separation_y(color_value(candidates))])","0074b147":"closest_cand = candidates.sort_values('distance').head(15)\nsc(closest_cand[closest_cand['ra'] >= 300])","223c4e7a":"data_close = data[data['distance'] < 75]\nnp.mean(np.abs(data_close['pmra']))","b78b81cc":"np.mean(np.abs(data_close['pmdec']))","effdcab0":"## Second GBM Model\nGBMs are stochastic, meaning that their responses include randomness that varies from one run to the next. For this reason, we will train a second GBM, with slightly different hyperparameters, so that the final model blend is more stable.","604937cf":"The discrepancy between the distance distributions of candidates and controls can change substantially from one run to the next. This could mean candidates cluster better\/worse simply because they are closer to us. We'll have to take this into account.\n\nWe will now concatenate the position frames of the candidates and bright controls.","9c55a111":"Our blend is only slightly better than our best individual model. A blend is a combination of multiple models, so it should generally be more stable (less long-tailed) than the individual models.","d5b8aef8":"## Candidate selection\nIt's not obvious how candidates should be selected. One possibility is to check how the error distribution compares to a normal distribution. The problem with this approach is that real-world data does not have a normal distribution. Instead, it's typically long-tailed. Outliers are to be expected and they are not indicative that we've found anything \"unnatural\", nor are non-outliers necessarily \"natural\".\n\nFor these reasons we will simply use a 3-sigma threshold to select our list of anomalously dim candidates.","1941b077":"The *extract_model_vars* function takes a data frame and returns a variable matrix that is ready to be used with *scikit-learn*-style regressors with a *fit* function.","81dce24a":"Any subsample of the cross-distance frame should also have a frequency of True-True rows of ~25%, within statistical uncertainty, if there's no clustering that depends on the modeled anomaly.\n\nThe following code tests incremental headsets of the ordered cross-distance frame, starting with the first 5 rows.","f6797deb":"The *abs_mag_value* function below will convert a magnitude column in a data frame into an absolute magnitude.","63f34812":"The harmonic mean of pairwise distances in the *candidates* set is:","f2b91689":"## Introduction\n\nDysonian SETI is a proposed approach in the search for extraterrestrial civilizations (Bradbury et al. 2011). Instead of performing radio searches, the approach involves looking for stars that host artificial megastructures.\n\nZackrisson et al. (2018) suggested that stars with Dyson Swarms would have spectrophotometric distances that exceed their parallax distances. In other words, Dysonian stars should be dimmer than their spectral characteristics would indicate. \n\nIt's possible to approximate the absolute magnitude of a star, and therefore its probable distance, given the star's color. With additional spectral features, a better approximation of the absolute magnitude can be derived.\n\nIn this kernel we will model the magnitude of stars rather than their distance modulus. There is a practical reason for this. We would like to use the Gaia DR2 parallax distance (not just its logarithm) as a variable of the model, in order to account for interstellar extinction. We won't be using the RAVE spectrophotometric distance as a variable, given that we expect it to leak information about magnitude. But we will take advantage of photometric features provided by RAVE DR5 from various other databases, in addition to features found in Gaia DR2.\n\nWe will test machine learning algorithms that are able to deal with complex nonlinearities. Specifically, we will blend Gradient Boosting Machines (GBMs) and a Neural Network. Then we will build a meta-model of the model's squared residuals, given that each star's measurements have errors that depend on its distance and other features. All model responses will be out-of-bag, as we rely on k-fold cross-validation.","dc02e27a":"The following function can produce a set of stars with roughly the same distance distribution as the provided *data_frame*. It does so by sampling 3000 stars out of our \"normal\" star pool, and then iteratively selecting the stars whose distances are closest to those in *data_frame*.","a649723f":"It's valid to use the response of a model as a variable of another model, so long as the response is out-of-bag. Our responses are cross-validated, so they should work. If they didn't, that would be evident once we look at the 'test' data we set aside.\n\nWe will use linear regression to produce the blend model. A blend does not have to be linear, but in this case that appears to be the best choice.","5d06a54f":"Let's test the function.","ba3d60ca":"## Data\n\nData is obtained from the [RAVE project](https:\/\/www.rave-survey.org\/project\/) and made available in a [Kaggle dataset](https:\/\/www.kaggle.com\/solorzano\/rave-dr5-gaia-dr2-consolidated\/home). In addition to RAVE DR5 measurements, the dataset  includes cross-identified features from Gaia DR2, APASS DR9, ALLWISE and DENIS.","d29b1a9f":"Note that we don't use magnitude features. That could be explored, but it seems it would allow the meta-model to discover brightness- and color-based patterns, which would cause us to discard potentially valid candidates.\n\nThe regressor we will use in this case is a Random Forest. There is a technical reason for this. We want a model that always produces positive responses. A Random Forest essentially produces averages of labels over some training examples, which means the meta-model will produce variances that are local in the variable space.","f912633f":"## Other subsets\nIn the H-R diagram there are a few anomalously dim stars that are slightly below the main sequence. They can be picked out as follows.","f5b45d88":"Let's set aside a 'test' data frame we will double-check only after all of the cross-validated models and transformations are tested using a 'work' data frame.","f02bec91":"## Acknowledgments\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.","ac8044b5":"With this position frame, we can now produce an 3D scatter chart showing the candidates in our region of space. Earth is at (0, 0, 0). Candidates in the main sequence or below are shown in blue. The rest are shown in green.","089082c9":"We will also add a couple decorator columns that are useful in visualizations: the \"distance\" to the star in parsecs, and the absolute magnitude (\"abs_mag_ne\") of the star, based on the G magnitude from Gaia DR2. Interstellar extinction is neglected in the absolute magnitude estimate.","19acf0cf":"## Residual modeling\n\nWe have distance-based and parallax error-based exclusion criteria, which helps improve model performance. However, within included data there are still differences in model error that are characterizable. It seems sensible to make adjustments based on each star's expected error.\n\nAdditionally, in preliminary analysis we found that anomalously dim candidates cluster, so we are making sure that any regional error patters are taken into account, by using distance, longitude and latitude as variables of the residual model.\n\nWe will define a function that extracts the *squared* residual array from a frame so we can use it as a regression label.","6fabf616":"Now we'll create a function that runs the whole transform sequence on a frame, and apply it to the 'test' data we had initially set aside.","d9b82595":"Let's plot the results of this test. The dashed orange line shows the expected (null hypothesis) frequency. Two additional lines show the lower bound of the 95% (red) and 99% (blue) confidence intervals.","c4f0188c":"## Decorators & data selection\n\nWe will start by limiting the data we will use to train models and obtain candidates. In preliminary analysis we found that many potential candidates appear to be below the main sequence in an H-R diagram, but upon further examination, it was clear these are stars with high parallax error. Additionally, model error is distance-dependent.\n\nNoisy data affects model precision, and also increases the likelihood that candidates will be spurious.\n\nDefining a threshold is a subjective exercise. We want to minimize the error, but we also want a reasonable number of candidates for follow-up analyses. We will use a minimum parallax of 2.5 (400 parsecs) and a maximum parallax error of 0.2.\n\nThese thresholds can be changed below.","f5434798":"## Model error\nLet's create a transform that will add an 'error_' column with the residuals of the blend.","5f436485":"## Linear modelling\nOur first regressor will be linear. Note that we will be modeling the G magnitude of a star, as provided by Gaia DR2. The reader can change the *LABEL_COLUMN* variable below to model, say, RP or BP magnitudes. There is just one function later in the analysis, called *separation_y*, that is visually derived and would need to be adjusted accordingly.","c8fa5c4d":"## Clustering analysis\nAnomalously dim candidates, particularly those in the main sequence or below (the blue ones in the 3D chart), appear to cluster. We can mathematically test this conjecture.\n\nLet's compare anomalously dim candidates with bright controls. Both of these subsets should have a distance bias, given that we control for local residual variance, and those residuals are distance-dependent. Let's start by checking summary statistics of each of the subsets.","03be8fca":"We will also define a function that compares two frame columns, e.g. a model response and a label, for purposes of model evaluation.","8cc7ca3b":"## Model variables\nLet's define the variables we will use to train regression models. Primarily, we will use \"color index\" features, which are simply differences between magnitudes observed with different photometric filters. RAVE DR5 also provides metal abundance and metallicity features that seem to help.\n\nTheoretically, the difference between the magnitude of a star and its absolute magnitude is a linear function of the logarithm of the star's distance. But there's an additional extinction component that is roughly dependent on distance. That's why we include both the distance and the logarithm of the distance as variables.\n\nSince extinction is likely irregular in our region of space, we include galactic latitude and longitude as variables. We additionally include a special transformation of the latitude based on the assumption that a region close to latitude zero has increased extinction. *GR*, defined below, is a hyperpameter of this special variable.","d3a9c2d6":"Per the methodology previously outlined, we will use this first transform function to alter our *work_data* frame.","3a359079":"## Modeling strategy\n\nOur strategy will consist of transforming the *work_data* frame, typically by adding columns to it at each step. In order to do this, we will create transform functions that take a frame and produce a new frame with altered data.\n\nIn particular, we need a function that can produce a transform from an arbitrary regression model. The transform will add model responses as a new column of the frame. Model responses are cross-validated, so they can be used as inputs of other models. The transform is also able to produce output for data that was not used in training.","f543d3e7":"Anomalously dim candidates (green dots) occur both in the main sequence, and above it. We will split the *candidates* frame into two subgroups called *candidates_bright* and *candidates_mainseq*, using the dashed blue line.","39c2527c":"## Final transformation and validation\nLet's put all our transform functions into a list called *transform_list*.","2b2aa750":"## Space distribution of candidates\nLet's define a function that takes a data frame and converts distance, galactic longitude and latitude to x-y-z coordinates.","8eeb85f6":"We're interested in rows where both sources are anomalously dim. The expectation is that True-True rows occur 25% of the time. Indeed, this is what we find if we look at the entire cross-distance frame.","7af0a541":"We'll also get an equally sized list of ordinary controls.","7deff508":"It should be noted that all of the above are high proper motion stars. A high proper motion is not unusual, however, for stars that are close to Earth. Let's check what the mean absolute proper motion values are for stars that are at most 75 parsecs away.","858ab166":"## Synopsis\nA model of stellar magnitude was presented in *[Dysonian SETI With Machine Learning](https:\/\/www.kaggle.com\/solorzano\/dysonian-seti-with-machine-learning)*. Anomalously dim candidate selection was based on error adjustment that in turn relied on a meta-model of *absolute* residuals. In this cloned kernel, *squared* residuals are modeled instead. This is intuitively more sound, as we're standardizing residuals according to their local variance. As a result, the \"anomaly\" distribution has a standard deviation of ~1.0. New collections of candidates and controls are made available in the output tab. A more robust clustering test has also been added.","8d34f099":"## Blend\nWe will now blend the 4 individual models we previously produced. First, we need a function that extracts the \"variables\" of the blend model, which are simply the responses of the individual models.","366d459e":"Note that the 3D scatter is interactive.\n\nLet's produce a second 3D scatter containing only the bright controls.","fb4fd19d":"This will vary from run to run, but it should typically be no more than a few. Clustering is therefore significant.","fe71c0b7":"The following transform function adds an 'anomaly_' column to a frame, defined as the ratio between the model's residual and the squared root of the expected squared residual.","2591bf96":"Columns found in the data frame follow.","ea7f16d6":"Let's check the 'error_' column that the 'test' data should now have.","0998b79c":"So we have over a hundred anomalously dim candidates, at the 3-sigma level. Let's also get an equal number of anomalously bright controls.","69516c75":"## First GBM model\nNext, we will build a Gradient Boosting Machine (GBM). We will use the TreeBoost implementation from [xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/model.html).","788a76c2":"How many simulations are better clustered than *candidates*?","c5492e96":"The cross-distance frame looks as follows.","5023380a":"## Model and H-R diagrams\n\nWe will now look at some visualizations of model results. First, let's get a random sample of 1000 stars from the data.","e71fa133":"We will run 100 simulations (which takes a few minutes) and obtain harmonic means for distance-matched samples with the same size as *candidates*.","4c79d210":"Note that the standard deviation of the anomaly distribution is ~1.0. This supports the validity of modeling *squared* residuals as opposed to *absolute* ones.","965a8e0c":"This result, as noted, could be due to differences in the distance bias of the two groups.","66b7164a":"Additionally, in the interactive 3D scatter chart, we find a group of a few stars that are close to one another and relatively close to Earth.","f785f916":"We will add a dashed blue line to the H-R diagram, intended to separate main sequence stars from giants. The separation line is defined by the *separation_y* function below, which is visually derived.","b5d216f2":"## Boilerplate code","c889cfe4":"## Neural Network","b0c42e7a":"## References\n\nBradbury et al. (2011). _Dysonian Approach to SETI: A Fruitful Middle Ground?_ Journal of the British Interplanetary Society, vol. 64, p. 156-165\n\nKunder et al. (2016). _The Radial Velocity Experiment (RAVE): Fifth Data Release_. arXiv:1609.03210\n\nZackrisson et al. (2018). _SETI with Gaia: The observational signatures of nearly complete Dyson spheres_. arXiv:1804.08351 ","adbbdd06":"Next, we will define a function that obtains the variables of the squared residual meta-model.","56493654":"## Discussion\nWe have produced a model of stellar magnitude as a function of Gaia DR2 parallax distance and spectrophotometric features from a number of databases. Model RMSE is ~0.23 magnitudes, when we select only stars with parallax of 2.5 or more and parallax error of 0.2 or less. This RMSE is nearly a third of the discrepancy between Gaia parallax and RAVE spectrophotometric distance moduli.\n\nIn a Gaia-only H-R diagram, anomalously bright sources are typically giants. Anomalously dim sources are sometimes giants as well, but most occur in the main sequence.\n\nWe find statistically significant stellar clustering in the group of anomalously dim candidates. Clustering is more evident visually among candidates that are in the main sequence or below.\n\nClustering could be explained as (1) a star type that exhibits clustering and can't be pinned down mathematically with the spectrophotometric features available to us, or (2) a model or data artifact.\n\nOne idea is that, since the model uses galactic latitude and longitude as variables, it might have introduced random regional irregularities in model responses. This hypothesis is easily discarded by removing latitude and longitude from the set of variables.\n\nClustering is, in fact, more pronounced if we remove latitude and longitude features. This should be taken into account going forward: The model seems to smooth out anomalies (though not entirely) as it detects region-dependent patterns. If there's clustering of star types of interest, that's somewhat of a problem. We could be discarding valid candidates.\n\nRegional error that is not reflected in the *parallax_error* field should have been discovered by the squared residual meta-model. In any case, we did not find evidence of significant regional error patterns. The reason why galactic latitude and longitude are variables of the residual meta-model is precisely to try to address observed clustering. The variables were kept because of their slight contribution to meta-model performance.\n\nThere are pairs of candidates and bright controls that are very close to one another in the sky. The possibility of contamination or incorrect cross-identification should be studied further. However, we don't believe such a systematic explains the observed clustering.","32bfed25":"## New clustering analysis\nWe should compare the group of candidates with samples of stars that have the same distance distribution. We have a pool of \"normal\" stars we can use:","642142ce":"This is consistent with the RMSE observed in *work_data*. We can apply the full transform to the entire dataset.","8de54b19":"The next chart will be a Hertzsprung\u2013Russell (H-R) diagram. We will use a color index in the X axis defined as the difference between the Gaia-provided BP and RP magnitudes.","b46d254d":"## Spectrophotometic distance error\n\nLet's calculate a baseline Root Mean Squared Error (RMSE) for reference. We will compare the Rave DR5 spectrophotometric distance modulus with the Gaia DR2 parallax-based distance modulus. Distance modulus errors should be equivalent to magnitude errors in terms of scaling.","f87bb095":"## Output files\n\nThe following files are made available in the output tab of this kernel.\n\n* **all-sources.csv** All sources that were used in training and candidate selection.\n* **dim-candidates.csv** The list of anomalously dim candidates.\n* **dim-candidates-mainseq.csv** Anomalously dim candidates that are in the main sequence or below.\n* **bright-controls.csv** A list of anomalously bright controls.\n* **normal-controls.csv** An equal number of ordinary stars.","13984b5b":"Notice that the distance distribution of *_dms* is nearly identical to that of *candidates*.\n\nNow we need some way to measure clustering. We will simply use the harmonic mean of pairwise distances. A harmonic mean will be closer to the shortest pairwise distances than even a geometric mean.","c6782f8a":"First, let's look at a chart of modeled absolute magnitude vs. observed absolute magnitude. Our 1000 random stars are shown in gray. The set of anomalously dim stars is shown in green. The set of bright controls is shown in orange.","bc8b1f6e":"Let's calculate squared distances between all pairs of sources in the concatenated frame. We will create a new frame, where each row contains a pair of sources and their squared distance. The new frame will be ordered by distance.","47ad4c45":"Further, we will add two boolean columns to the cross-distance frame indicating whether each of the sources in a pair are anomalously dim."}}