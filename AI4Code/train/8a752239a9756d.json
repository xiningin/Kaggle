{"cell_type":{"1131f207":"code","5fdfc418":"code","8829d921":"code","10b3586c":"code","ebb3fd32":"code","95f11510":"code","518a302c":"code","a72e5b43":"code","32a1a45d":"code","ad66e1d1":"code","7386d073":"code","a9ac1340":"code","5fccbca1":"code","1b50bc16":"code","963f9333":"code","de08c608":"code","38f5fac9":"code","4c3a6253":"code","f2e1ef15":"code","584f6ad8":"code","f80c6844":"code","1dba7857":"code","a1dc1dc7":"code","bc9b9788":"code","c62e8055":"code","ca5c04a7":"code","2baee82b":"code","b93ad996":"code","27b83f33":"code","d9ba2346":"code","3c3f5524":"code","27383598":"code","9a3c1bc8":"code","f6ec3f7b":"code","3737e71e":"code","5c5a6904":"code","663fa66b":"code","ef7ec940":"code","ab3d8901":"code","06b3b864":"code","2e0cef2d":"code","7c429b82":"code","19ef2ca7":"code","6e186751":"markdown","7fc95a91":"markdown","e64bd6dc":"markdown","70e3be33":"markdown","bad5330d":"markdown","ef5a15fa":"markdown","2d43b01e":"markdown","649b2b6c":"markdown","daaf32f6":"markdown","489b8648":"markdown","d6cb3529":"markdown","742951a8":"markdown","08040f67":"markdown","41dfcd2a":"markdown","41ab9637":"markdown"},"source":{"1131f207":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fdfc418":"import xgboost as xgb # XGBoost stuff\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer # for scoring during cross validation\nfrom sklearn.model_selection import GridSearchCV # cross validation\nfrom sklearn.metrics import confusion_matrix # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix","8829d921":"data = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","10b3586c":"data.dtypes","ebb3fd32":"data['Over18'].unique()","95f11510":"data['EmployeeNumber'].unique()","518a302c":"data['EmployeeCount'].unique()","a72e5b43":"data.drop(['Over18','EmployeeNumber','EmployeeCount'],axis = 1,inplace = True)","32a1a45d":"data.shape","ad66e1d1":"data.isnull().sum()","7386d073":"data['Attrition'].value_counts()","a9ac1340":"X = data.drop('Attrition',axis = 1).copy()","5fccbca1":"X.dtypes","1b50bc16":"X_encode = pd.get_dummies(X,columns = ['BusinessTravel','Department','EducationField',\n                                      'Gender','JobRole','MaritalStatus','OverTime'])","963f9333":"X_encode.shape","de08c608":"data['Attrition'] = data['Attrition'].replace({'No': 0, 'Yes': 1}).astype(int)","38f5fac9":"y = data['Attrition'].copy()","4c3a6253":"y.head()","f2e1ef15":"X_train,X_test,y_train,y_test = train_test_split(X_encode,y,\n                                                random_state = 1,\n                                                stratify = y)\n","584f6ad8":"print(y_train.shape)\nprint(y_test.shape)","f80c6844":"print(sum(y)\/len(y))\nprint(sum(y_train)\/len(y_train))\nprint(sum(y_test)\/len(y_test))","1dba7857":"from sklearn.linear_model import LogisticRegression\nlog_reg=LogisticRegression(penalty=\"l2\",solver=\"liblinear\",C=0.5,max_iter=100)\nlog_reg.fit(X_train,y_train)\nprint('--------------------------------------------------------------------------')\nprint('Logistic Regression:')\nprint('Traning Model accruracy scores: {:.3f}'.format(log_reg.score(X_train,y_train)))\nprint('Test Model accruracy scores: {:.3f}'.format(log_reg.score(X_test,y_test)))\nprint('--------------------------------------------------------------------------')","a1dc1dc7":"plot_confusion_matrix(log_reg,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","bc9b9788":"log_reg=LogisticRegression(penalty=\"l2\",solver=\"liblinear\",C=0.5,max_iter=100,class_weight=\"balanced\")\nlog_reg.fit(X_train,y_train)\nprint('--------------------------------------------------------------------------')\nprint('Logistic Regression:')\nprint('Traning Model accruracy scores: {:.3f}'.format(log_reg.score(X_train,y_train)))\nprint('Test Model accruracy scores: {:.3f}'.format(log_reg.score(X_test,y_test)))\nprint('--------------------------------------------------------------------------')","c62e8055":"plot_confusion_matrix(log_reg,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","ca5c04a7":"clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic',\n                           eval_metric = 'logloss',\n                           seed = 42,\n                           use_label_encoder = False)\nclf_xgb.fit(X_train,\n           y_train,\n           verbose = True,\n           early_stopping_rounds = 10,\n           eval_metric = 'aucpr',\n           eval_set = [(X_test,y_test)])","2baee82b":"plot_confusion_matrix(clf_xgb,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","b93ad996":"clf_xgb = xgb.XGBClassifier(seed=42,\n                        objective='binary:logistic',\n                        eval_metric=\"logloss\", ## this avoids warning...\n                        gamma=0.25,\n                        learning_rate=0.1,\n                        max_depth=5,\n                        reg_lambda=10,\n                        scale_pos_weight=6,\n                        subsample=0.8,\n                        colsample_bytree=0.5,\n                        use_label_encoder=False)","27b83f33":"clf_xgb.fit(X_train,\n            y_train,\n            early_stopping_rounds = 10,\n            eval_metric = 'auc',\n            eval_set = [(X_test,y_test)],\n            verbose = True)\n\nplot_confusion_matrix(clf_xgb,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","d9ba2346":"clf_xgb = xgb.XGBClassifier(seed=42,\n                        objective='binary:logistic',\n                        eval_metric=\"logloss\", ## this avoids warning...\n                        gamma=0.25,\n                        learning_rate=0.1,\n                        max_depth=5,\n                        reg_lambda=10,\n                        scale_pos_weight=6,\n                        subsample=0.8,\n                        colsample_bytree=0.5,\n                        use_label_encoder=False)\nclf_xgb.fit(X_train,\n            y_train,\n            early_stopping_rounds = 10,\n            eval_metric = 'aucpr',\n            eval_set = [(X_test,y_test)],\n            verbose = True)","3c3f5524":"features = X_encode.columns\nimportances = clf_xgb.feature_importances_\n\nimportances_df = pd.DataFrame()\nimportances_df['feature_name'] = features\nimportances_df['feature_imp'] = importances\nimportances_df.sort_values('feature_imp',ascending = False)","27383598":"import shap\nexplainer = shap.Explainer(clf_xgb)\nshap_value = explainer(X_test)","9a3c1bc8":"shap.plots.beeswarm(shap_value)","f6ec3f7b":"from lightgbm import LGBMClassifier\nclf_lgb = LGBMClassifier(random_state = 42)\nclf_lgb.fit(X_train,y_train)","3737e71e":"plot_confusion_matrix(clf_lgb,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","5c5a6904":"parameters = {'num_leaves':[6,12,20],\n             'n_estimators':[50,80,130],\n             'learning_rate':[0.05,0.1,0.5]}\nclf_lgb = LGBMClassifier(random_state = 42)\ngrid_search = GridSearchCV(clf_lgb,parameters,scoring = 'roc_auc',cv = 5)\ngrid_search.fit(X_train,y_train,early_stopping_rounds = 10,eval_metric = 'auc',eval_set = [(X_test,y_test)],verbose = False)\ngrid_search.best_params_","663fa66b":"parameters = {'num_leaves':[12],\n             'n_estimators':[20,30,50],\n             'learning_rate':[0.1],\n             'scale_pos_weight':[5,6,7]},\n\nclf_lgb = LGBMClassifier(random_state = 42)\ngrid_search = GridSearchCV(clf_lgb,parameters,scoring = 'roc_auc',cv = 5)\ngrid_search.fit(X_train,y_train)\ngrid_search.best_params_","ef7ec940":"\nclf_lgb1 = LGBMClassifier(num_leaves = 12,\n                          n_estimators= 50,\n                          learning_rate=0.1,\n                          scale_pos_weight = 5,\n                          random_state = 42)\n\nclf_lgb1.fit(X_train,y_train)\n\nplot_confusion_matrix(clf_lgb1,\n                     X_test,\n                     y_test,\n                     values_format = 'd',\n                     display_labels= ['Did not leave','left'])","ab3d8901":"features = X_encode.columns\nimportances = clf_lgb1.feature_importances_\n\nimportances_df = pd.DataFrame()\nimportances_df['feature_name'] = features\nimportances_df['feature_imp'] = importances\nimportances_df.sort_values('feature_imp',ascending = False)","06b3b864":"import seaborn as sns\nsns.displot(x = data['MonthlyIncome'],hue = 'Attrition',kde = True,data = data)","2e0cef2d":"sns.displot(x = data['JobSatisfaction'],hue = 'Attrition',kde = True,data = data)","7c429b82":"sns.displot(x = data['StockOptionLevel'],hue = 'Attrition',kde = True,data = data)","19ef2ca7":"from sklearn.metrics import classification_report\npred_test_lr = log_reg.predict(X_test)\npred_test_xgb = clf_xgb.predict(X_test)\n#pred_test_cb = cb_model.predict(X_test)\npred_test_lgb = clf_lgb1.predict(X_test)\nprint(classification_report(y_test,pred_test_lr))\nprint('*************************************')\nprint(classification_report(y_test,pred_test_xgb))\nprint('*************************************')\nprint(classification_report(y_test,pred_test_lgb))\nprint('************************************')","6e186751":"In the confusion matrix, we see that of the 368 people that did not leave, 301 (82%) were correctly classified. And of the 59 people that left the company, 19 (32%) were correctly classified. So the XGBoost model was not awesome. ","7fc95a91":"# SHAP for XGBoost","e64bd6dc":"# #LightGBM model","70e3be33":"# Model1:Logistic Regression for the model","bad5330d":"# sample balance exploration","ef5a15fa":"# Missing data","2d43b01e":"In the confusion matrix, we see that of the 368 people that did not leave, 301 (82%) were correctly classified. And of the 59 people that left the company, 19 (32%) were correctly classified. So the XGBoost model was not awesome. ","649b2b6c":"# Formatting the Data for modeling","daaf32f6":"In the confusion matrix, And of the 59 people that left the company, 45 (76%) were correctly classified. ","489b8648":"1: Split the Data into Dependent and Independent Variables","d6cb3529":"# Model2:Building a Preliminary XGBoost Model","742951a8":"# Import data ","08040f67":"It seems the sample data is not imbalanced","41dfcd2a":"Compare with XGBoost and LR model,XGBoost works a little better than LR","41ab9637":"# Building, Drawing, Interpreting and Evaluating the Optimized XGBoost Model"}}