{"cell_type":{"76721a04":"code","ca997eb6":"code","6c09de2c":"code","72a8acb1":"code","a837dbd9":"code","11d33f1b":"code","4011eb67":"code","93100e03":"code","4537397f":"code","30f64bf4":"code","bea63f81":"code","ee92bc20":"code","58841824":"code","ddb9afdc":"code","ef18765c":"code","3eddef04":"code","8f3587a9":"code","74260969":"code","e676283d":"code","bbafbbe2":"code","b6511ed3":"code","c669ac7f":"code","9d71392c":"markdown","0a03968b":"markdown","81cfb27d":"markdown","5b63cdcf":"markdown","30661bc5":"markdown","ea8fe5cf":"markdown","bafb1672":"markdown","9885339b":"markdown","92717bff":"markdown","8bfc26c5":"markdown","249dbcd3":"markdown","2f2590b8":"markdown","56440e9c":"markdown","31b6c621":"markdown","11098e4e":"markdown","6d2b7f07":"markdown","9e3858ed":"markdown","1ec96e56":"markdown","caa16247":"markdown","a0d6e302":"markdown","61847b23":"markdown"},"source":{"76721a04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca997eb6":"# Data Manupilation\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Deep Learning\nimport tensorflow as tf\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\n# Text\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopw = stopwords.words('english')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#\u5bfc\u5165\u6240\u9700\u7684\u6a21\u5757\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence\n\n\nfrom keras.models import Model\nfrom keras.models import Sequential\n\nfrom keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import SpatialDropout1D, concatenate,LSTM, Activation\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n\nfrom keras.callbacks import Callback\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.models import load_model\nfrom keras.utils.vis_utils import plot_model","6c09de2c":"import pandas as pd\ndf = pd.read_csv('..\/input\/bbc-fulltext-and-category\/bbc-text.csv')\nprint('total samples:',df.shape[0])\ndf.head()","72a8acb1":"# \u5bf9\u56e0\u53d8\u91cf\u8fdb\u884c\u6807\u7b7e\u7f16\u7801\nlabelencoder = LabelEncoder()\ndf['label'] = labelencoder.fit_transform(df['category'])","a837dbd9":"df['text_length'] = df['text'].apply(lambda x:len(x.split()))\n\nplt.figure(figsize=(12,6),dpi = 300)\nplt.hist(df['text_length'],bins = 40)\nplt.xlabel(\"Word Number:\")\nplt.ylabel('Counts:')\nplt.title('Text Lengths')\nplt.show()","11d33f1b":"print(df['category'].value_counts())\ntemp = df['category'].value_counts()\nfig = plt.figure(figsize = (5,5),dpi =120)\nsns.barplot(temp.index,temp.values)","4011eb67":"# \u6587\u672c\u5904\u7406\u9636\u6bb5\nfrom gensim import utils\nimport gensim.parsing.preprocessing as gsp\n\nfilters = [gsp.strip_tags, \n           gsp.strip_punctuation,\n           gsp.strip_multiple_whitespaces,\n           gsp.strip_numeric,\n           gsp.remove_stopwords, \n           gsp.strip_short, \n           gsp.stem_text]\n\n\ndef clean_text_one(s):\n    s = s.lower()\n    s = utils.to_unicode(s)\n    for f in filters:\n        s = f(s)\n    return s\n\ndef clean_text_two(text):\n    text = re.sub(r'[^a-zA-Z\\']',' ',text)\n    text = text.split()\n    text = [word for word in text if word not in stopw]\n    text = ' '.join(text)\n    text = re.sub(r'  ', ' ', text)\n    text = re.sub(r'   ', ' ', text)\n    return text\n\nprint('*'*40)\nprint('primary sentence:\\n')\nprint(df.iloc[2,1])\nprint('*'*40)\nprint('after clean_text_one:\\n')\nprint(clean_text_one(df.iloc[2,1]))\nprint('*'*40)\nprint('after clean_text_two:\\n')\nprint(clean_text_two(df.iloc[2,1]))\n\n#\u6784\u9020\u65b0\u53d8\u91cf\u2018clean_text\u2019\u4e3a\u6e05\u6d17\u597d\u7684\u53d8\u91cf\ndf['clean_text'] = df['text'].apply(lambda x:clean_text_two(x))","93100e03":"# \u518d\u6b21\u67e5\u770b\u6570\u636e\u96c6\ndf.head()","4537397f":"X_train,X_test,Y_train,Y_test = train_test_split(df['clean_text'],df['label'],test_size=0.3)","30f64bf4":"# \u67e5\u770b\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7684size\nprint(X_train.shape)\nprint(X_test.shape)","bea63f81":"# \u5229\u7528CountVectorizer\u5c06\u5206\u8bcd\u540e\u8f6c\u5316\u4e3a\u8bcd\u7279\u5f81\u5411\u91cf\nCountVec = CountVectorizer(stop_words = 'english')\n\n# \u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u62df\u5408\u548c\u8f6c\u6362\nfeatures = CountVec.fit_transform(X_train).toarray()\n\nprint(features.shape)\nprint(\"Each of the %d complaints is represented by %d features (CountVectorizer score of unigrams and bigrams)\" %(features.shape))\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# \u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u5728\u8bad\u7ec3\u96c6\u4e0a\u8ba1\u7b97\u51c6\u786e\u7387\u5f97\u5206\npl_random_forest_CountVec = Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores = cross_val_score(pl_random_forest_CountVec, features,Y_train, cv=5,scoring='accuracy')\nprint('Accuracy for CountVec & RandomForest : ', scores.mean())","ee92bc20":"# \u5229\u7528TF-IDF\u5c06\u5206\u8bcd\u540e\u8f6c\u5316\u4e3a\u8bcd\u7279\u5f81\u5411\u91cf\ntfidf = TfidfVectorizer(min_df=5,max_df = 100,stop_words='english')\n\n# \u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u62df\u5408\u548c\u8f6c\u6362\nfeatures = tfidf.fit_transform(X_train).toarray()\n\nprint(features.shape)\nprint(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# \u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u5728\u8bad\u7ec3\u96c6\u4e0a\u8ba1\u7b97\u51c6\u786e\u7387\u5f97\u5206\npl_random_forest_tf_idf = Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores = cross_val_score(pl_random_forest_tf_idf, features,Y_train, cv=5,scoring='accuracy')\nprint('Accuracy for Tf-Idf & RandomForest : ', scores.mean())","58841824":"#\u6bcf\u4e2a\u6587\u672c\u7684\u6700\u5927\u957f\u5ea6\u4e3a256(\u5982\u679c\u67d0\u4e2a\u6587\u672c\u7684\u957f\u5ea6\u4e0d\u8db3256\u5219\u8fdb\u884c\u586b\u5145)\n#\u8bbe\u7f6e\u5d4c\u5165\u7684\u7ef4\u5ea6\u4e3a300\u7ef4\nmax_len = 256\nembedding_dim = 300\n\n#\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u5206\u8bcd\u5e76\u8fdb\u884c\u7f16\u53f7\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\n#\u5c06\u8bad\u7ec3\u96c6\u8f6c\u5316\u4e3a\u5e8f\u5217\uff0c\u5e76\u8fdb\u884c\u586b\u5145\nsequences_train = tokenizer.texts_to_sequences(X_train)\nmatrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len)\n\n#\u5c06\u6d4b\u8bd5\u96c6\u8f6c\u5316\u4e3a\u5e8f\u5217\uff0c\u5e76\u8fdb\u884c\u586b\u5145\nsequences_test = tokenizer.texts_to_sequences(X_test)\nmatrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)\n\nprint(\"The vocab size is :\",vocab_size)\nprint(\"The word embdding dim is :\",embedding_dim)\nprint(\"The input size of sentence is :\",max_len)","ddb9afdc":"# \u67e5\u770b\u5206\u8bcd\u5e76\u586b\u5145\u540e\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u770b\u5230\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u957f\u5ea6\u5747\u4e3a256\nprint(matrix_train.shape)\nprint(matrix_test.shape)\n# \u67e5\u770b\u6570\u636e\u7684\u7b2c\u4e00\u6761\uff0c\u6570\u636e\u5df2\u7ecf\u8f6c\u53d8\u4e3a\u6574\u578b\u6570\u636e\nprint(matrix_train[0])","ef18765c":"# \u5efa\u7acb\u6a21\u578b\ndef model1():\n    embedding_dim = 300\n    #\u8bbe\u5b9aembedding\u5c42\u8f93\u51fa\u7ef4\u5ea6\u4e3a300\uff0cinput_length\u4e3a256\uff0c\u56e0\u4e3a\u5728\u5206\u8bcd\u9636\u6bb5\u5c06\u6bcf\u4e2a\u6837\u672c\u6620\u5c04\u6210\u4e86256\u7ef4\n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size, embedding_dim,input_length = max_len,trainable = False)(inp)\n    #\u53cc\u5411LSTM\u5c42\n    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n    #Dropout\u5c42\u968f\u673a\u2018\u820d\u53bb\u201930%\u7684\u795e\u7ecf\u5143\uff0c\u4e0d\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0\uff0c\u7528\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\n    x = Dropout(0.3)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = Dropout(0.3)(x)\n    #\u5168\u8fde\u63a5\u5c42\uff0c\u6fc0\u6d3b\u51fd\u6570\u4e3arelu\n    x = Dense(32, activation = 'relu')(x)\n    x = Dropout(0.3)(x)\n    #softmax\u6fc0\u6d3b\u51fd\u6570\u5c06\u6700\u7ec8\u8f93\u51fa\u6620\u5c04\u81f35\u7c7b\n    outp = Dense(5, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\n#\u67e5\u770b\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u548c\u53c2\u6570\u91cf\nmodel1 = model1()\nmodel1.summary()\n\n#\u8bad\u7ec3\u6a21\u578b\u5e76\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u9009\u53d6\u8bad\u7ec3\u96c6\u4e2d20%\u7684\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\nhist = model1.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model1.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\n# \u8f93\u51fa\u6bcf\u8f6e\u8bad\u7ec3\u7684loss\u548c\u51c6\u786e\u7387\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u53ef\u4ee5\u770b\u5230\uff0c\u6a21\u578b\u7684loss\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u7684\u589e\u52a0\uff0c\u6574\u4f53\u5448\u4e0b\u964d\u8d8b\u52bf\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u7684\u6ce2\u52a8\n\u800c\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u7684\u8868\u73b0\u4e5f\u7c7b\u4f3c\uff0c\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u589e\u52a0\uff0c\u51c6\u786e\u7387\u6574\u4f53\u4e0a\u5347\uff0c\u4e5f\u5b58\u5728\u6ce2\u52a8\u8d8b\u52bf\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u7684\u8868\u73b0\u4e00\u822c\uff0c\u4ec5\u4e3a62%\uff0c\u8be5\u7f51\u7edc\u7ed3\u6784\u7684\u8868\u73b0\u8f83\u5dee\u3002\n\u5e76\u4e14\u91cd\u590d\u591a\u6b21\u8bad\u7ec3\u65f6\uff0c\u4e0d\u540c\u8bd5\u9a8c\u7ed3\u679c\u5dee\u5f02\u5927\n'''","3eddef04":"def model2():\n    embedding_dim = 300\n    \n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size,embedding_dim, input_length=256,trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    # \u53cc\u5411GRU\uff0c\u6bd4LSTM\u66f4\u5bb9\u6613\u8ba1\u7b97\n    x = Bidirectional(GRU(100, return_sequences=True))(x)\n    # \u6c60\u5316\u5c42\uff0c\u4f5c\u7528\u662f\u5728\u4fdd\u7559\u4e3b\u7279\u5f81\u7684\u524d\u63d0\u4e0b\uff0c\u51cf\u5c11\u4e0b\u4e00\u5c42\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u9632\u6b62\u8fc7\u62df\u5408\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    # \n    outp = Dense(5, activation=\"softmax\")(conc)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel2 = model2()\nmodel2.summary()\n\nhist = model2.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model2.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u76f8\u6bd4\u4e8e\u6a21\u578b1\uff0c\u6a21\u578b2\u7684loss\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u7684\u589e\u52a0\uff0c\u6574\u4f53\u4e0b\u964d\u8d8b\u52bf\u66f4\u5e73\u6ed1\uff0c\u4e14\u6ce2\u52a8\u8f83\u5c0f\n\u6a21\u578b2\u5728\u9a8c\u8bc1\u96c6\u7684\u51c6\u786e\u7387\uff0c\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u589e\u52a0\uff0c\u51c6\u786e\u7387\u6574\u4f53\u4e0a\u5347\uff0c\u6ce2\u52a8\u8d8b\u52bf\u66f4\u5c0f\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u7684\u8868\u73b0\u8f83\u597d\uff0c\u51c6\u786e\u7387\u4e3a91%\uff0c\n\u76f8\u6bd4\u4e8e\u6a21\u578b1\uff0c\u6a21\u578b2\u7684\u7a33\u5b9a\u7a0b\u5ea6\u66f4\u9ad8\uff0c\u4e14\u9884\u6d4b\u51c6\u786e\u7387\u4e5f\u66f4\u597d\u3002\n'''","8f3587a9":"# \u5efa\u7acb\u6a21\u578b\ndef model3():\n    embedding_dim = 300\n\n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size, embedding_dim,input_length = max_len, trainable = False)(inp)\n    x = Conv1D(32, 8, activation=\"relu\")(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dropout(0.7)(x)\n    x = Dense(10, activation=\"relu\")(x)\n    outp = Dense(5, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel3 = model3()\nmodel3.summary()\n\nhist = model3.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model3.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u76f8\u6bd4\u4e8e\u6a21\u578b1\u548c\u6a21\u578b2\uff0c\u6a21\u578b3\u7684loss\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u7684\u589e\u52a0\uff0c\u6574\u4f53\u4e0b\u964d\u8d8b\u52bf\u6781\u5ea6\u5e73\u6ed1\n\u6a21\u578b3\u5728\u9a8c\u8bc1\u96c6\u7684\u51c6\u786e\u7387\uff0c\u968f\u7740\u8bad\u7ec3\u8f6e\u6570\u589e\u52a0\uff0c\u51c6\u786e\u7387\u4e5f\u6574\u4f53\u4e0a\u5347\n\u6a21\u578b3\u7684\u7a33\u5065\u6027\u4e5f\u76f8\u5bf9\u8f83\u9ad8\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u4e5f\u6bd4\u8f83\u597d\u3002\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u7684\u51c6\u786e\u7387\u4e3a84.6%\uff0c\u8868\u73b0\u826f\u597d\u3002\n\u5e76\u4e14\u6a21\u578b3\u7684\u8bad\u7ec3\u901f\u5ea6\u6781\u5feb\uff0c\u51e0\u4e4e\u662f\u77ac\u95f4\u5b8c\u6210\n'''","74260969":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n#\u5bfc\u5165\u5df2\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\n# golve\u4e3a\u8bcd\u5411\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u672c\u62a5\u544a\u9009\u62e9\u7684\u662f220\u4e07\u4e2a\u8bcd\uff0c\u7ef4\u5ea6\u4e3a300\u7ef4\u3002\nglove_embeddings = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\n\nword_embeddings = 256\nembedding_dim = 300\n\n#\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u5206\u8bcd\ntokenizer = Tokenizer(oov_token = '<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\n#\u5c06\u8bad\u7ec3\u96c6\u8f6c\u5316\u4e3a\u5e8f\u5217\uff0c\u5e76\u8fdb\u884c\u586b\u8865\nsequences_train = tokenizer.texts_to_sequences(X_train)\nmatrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len)\n\n#\u5c06\u6d4b\u8bd5\u96c6\u8f6c\u5316\u4e3a\u5e8f\u5217\uff0c\u5e76\u8fdb\u884c\u586b\u8865\nsequences_test = tokenizer.texts_to_sequences(X_test)\nmatrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)\n\nprint(\"The vocab size is :\",vocab_size)\nprint(\"The word embdding dim is :\",embedding_dim)\nprint(\"The input size of sentence is :\",max_len)\n\n# \u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u521b\u5efa\u4e00\u4e2a\u5d4c\u5165\u77e9\u9635\nword_embeddings = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    if word in glove_embeddings.keys():\n        word_embeddings[i-1] = glove_embeddings[word]","e676283d":"## \u5efa\u7acb\u6a21\u578b\ndef model4():\n    embedding_dim = 300\n\n    inp = Input(shape=(256,))\n    #\u8fd9\u91cc\u6307\u5b9a\u6743\u91cd\u4e3a\u4e4b\u524d\u5bf9\u6bcf\u4e00\u4e2a\u5355\u8bcd\u6784\u5efa\u597d\u7684\u6743\u91cd\u77e9\u9635\n    x = Embedding(vocab_size, embedding_dim,input_length = max_len,weights = [word_embeddings], trainable = False)(inp)\n    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n    x = Dropout(0.5)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation = 'relu')(x)\n    x = Dropout(0.5)(x)\n    outp = Dense(5, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel4 = model4()\nmodel4.summary()\n\nhist = model4.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model4.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u7531\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u53ef\u4ee5\u770b\u5230\uff0c\u6a21\u578b4\u8868\u73b0\u76f8\u6bd4\u4e8e\u6a21\u578b1\u6709\u4e86\u4e00\u5b9a\u63d0\u5347\u3002\n\u4e24\u4e2a\u6a21\u578b\u7684\u5dee\u522b\u4e3a\u662f\u5426\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u3002\n\u6a21\u578b4\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u540e\uff0c\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u8868\u73b0\u8f83\u597d\uff0c\u51c6\u786e\u7387\u4e00\u5ea6\u4e3a97%-98%\uff0c\n\u4f46\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u8fc7\u62df\u5408\u7684\u95ee\u9898\u5bfc\u81f4\u3002\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a72%\uff0c\u4f46\u6548\u679c\u4ecd\u7136\u4e0d\u4f73\n'''","bbafbbe2":"def model5():\n    embedding_dim = 300\n    \n    #\u589e\u52a0\u4e86\u9884\u8bad\u7ec3\u7684\u6743\u91cd\n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size,embedding_dim, input_length=256,weights = [word_embeddings],trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    # \u53cc\u5411GRU\uff0c\u6bd4LSTM\u66f4\u5bb9\u6613\u8ba1\u7b97\n    x = Bidirectional(GRU(100, return_sequences=True))(x)\n    # \u6c60\u5316\u5c42\uff0c\u4f5c\u7528\u662f\u5728\u4fdd\u7559\u4e3b\u7279\u5f81\u7684\u524d\u63d0\u4e0b\uff0c\u51cf\u5c11\u4e0b\u4e00\u5c42\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u9632\u6b62\u8fc7\u62df\u5408\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(5, activation=\"softmax\")(conc)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel5 = model5()\nmodel5.summary()\n\nhist = model5.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model5.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u7531\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9a8c\u8bc1\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u6a21\u578b5\u7684\u7ed3\u679c\u5e76\u4e0d\u5982\u6a21\u578b2\n\u6a21\u578b5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u751a\u81f3\u672a\u9ad8\u4e8e90%\uff0c\u5728\u6b64\u8868\u73b0\u4e0d\u4f73\u3002\n\u4f46\u7ecf\u8fc7\u591a\u6b21\u8bd5\u9a8c\u540e\u53d1\u73b0\u3002\u6a21\u578b\u7a33\u5065\u6027\u8f83\u5dee\uff0c\u4e0d\u540c\u8bd5\u9a8c\u7684\u7ed3\u679c\u5dee\u5f02\u8f83\u5927\n'''","b6511ed3":"# \u5efa\u7acb\u6a21\u578b\ndef model6():\n    embedding_dim = 300\n\n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size, embedding_dim,input_length = max_len,weights = [word_embeddings], trainable = False)(inp)\n    x = Conv1D(32, 8, activation=\"relu\")(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(10, activation=\"relu\")(x)\n    outp = Dense(5, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel6 = model6()\nmodel6.summary()\n\nhist = model6.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model6.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u6a21\u578b6\u76f8\u6bd4\u4e8e\u6a21\u578b3\uff0c\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002\n\u4f46\u662f\u6548\u679c\u53cd\u800c\u8f83\u5dee\uff0c\u5e76\u4e14\u8ba1\u7b97\u901f\u5ea6\u6709\u6240\u51cf\u7f13\u3002\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a65%\n'''","c669ac7f":"def model7():\n    embedding_dim = 300\n    inp = Input(shape=(256,))\n    x = Embedding(vocab_size, embedding_dim, weights=[word_embeddings], input_length=256, trainable=True)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(GRU(100, return_sequences=True))(x)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(5, activation=\"softmax\")(conc)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='SparseCategoricalCrossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel7 = model7()\n\nhistory = model7.fit(matrix_train, Y_train,epochs = 30, verbose = 1,validation_split=0.2)\nresult = model7.evaluate(matrix_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(result[0],result[1]))\n\nfig= plt.figure(figsize = (16,8))\nax1 = fig.add_subplot(121)\nax1.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\n\nax2 = fig.add_subplot(122)\nax2.plot(hist.history['accuracy'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()\n\n'''\n\u6a21\u578b7\u9009\u62e9\u4f7f\u7528\u4e86GRU\u5c42\u3001\u5377\u79ef\u5c42\u3001\u6c60\u5316\u5c42\u7b49\u3002\n\u5728\u5efa\u6a21\u9636\u6bb5\u8868\u73b0\u5341\u5206\u4f18\u5f02\uff0c\u591a\u6b21\u5728\u8bad\u7ec3\u96c6\u7684\u51c6\u786e\u7387\u4e3a1\u7684\u540c\u65f6\uff0c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u80fd\u63a5\u8fd198%\n\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a97.2%\uff0c\u6a21\u578b\u6027\u80fd\u6700\u4f73\n'''","9d71392c":"## 5.7 \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b5(GRU+Pooling)","0a03968b":"## 5.4 \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u2014\u6a21\u578b3\uff08CNN+Pooling\uff09","81cfb27d":"## 5. \u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\n### \u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5c06\u5305\u62ec\u4ee5\u4e0b\u51e0\u4e2a\u6a21\u578b\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b1(LSTM)\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b2(GRU+Pooling)\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b3(CNN+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b4(LSTM)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b5(GRU+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b6(CNN+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b7(GRU+CNN+Pooling)","5b63cdcf":"## 4. \u673a\u5668\u5b66\u4e60\u6a21\u578b\n### \u672c\u62a5\u544a\u5c06\u9009\u7528\u4ee5\u4e0b\u51e0\u7c7b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6587\u672c\u7c7b\u578b\u8fdb\u884c\u9884\u6d4b\n* CountVectorize + RandomForest\n* TF-IDF + RandomForest","30661bc5":"## README","ea8fe5cf":"## 5.1 \u6570\u636e\u9884\u5904\u7406","bafb1672":"## 5.2 \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u2014\u6a21\u578b1\uff08LSTM\uff09","9885339b":"## 5.3 \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u2014\u6a21\u578b2\uff08GRU+Pooling\uff09","92717bff":"## 3. \u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6","8bfc26c5":"## 1. \u6570\u636e\u7b80\u8981\u8bf4\u660e\u53ca\u9884\u5904\u7406","249dbcd3":"## 4.1 CountVectorize + RandomForest\n### \u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a96.1466%","2f2590b8":"### \u9996\u5148\u5bf9\u6587\u672c\u8fdb\u884c\u6570\u636e\u6e05\u6d17\u7684\u5de5\u4f5c\uff0c\u5305\u62ec\u53bb\u9664\u505c\u7528\u8bcd\u3001\u5220\u9664\u7a7a\u683c\u3001\u53bb\u9664\u6807\u70b9\u7b26\u53f7\u7b49\u64cd\u4f5c\n### \u5728\u8fd9\u91cc\u5206\u5e03\u5b9a\u4e49\u4e86\u4e24\u79cd\u5206\u8bcd\u53ca\u6570\u636e\u6e05\u6d17\u65b9\u6cd5\uff0c\u5e76\u4e14\u5c06\u7528\u8fd9\u4e24\u79cd\u65b9\u6cd5\u6e05\u6d17\u8fc7\u7684\u6587\u672c\u4e0e\u539f\u59cb\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\n### \u7ed3\u679c\u8ba4\u4e3a\u7b2c\u4e8c\u79cd\u65b9\u5f0f\u5212\u5206\u51fa\u7684\u8bcd\u8bed\u7ed3\u679c\u66f4\u597d\uff0c\u56e0\u6b64\u9009\u7528\u7b2c\u4e8c\u7c7b\u65b9\u6cd5\u8fdb\u884c\u5206\u8bcd","56440e9c":"## 5.9 \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b7(GRU+CNN+Pooling)","31b6c621":"## README\n### 0. \u9996\u5148\u5411\u8001\u5e08\u8bf4\u660e\u6211\u5bf9\u60c5\u51b5\uff1a\u7531\u4e8e\u6211\u7684\u7535\u8111\u6ca1\u6709GPU\uff0c\u4f46\u662f\u53c8\u60f3\u5c1d\u8bd5\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u5185\u5bb9(\u4f8b\u5982\u8bfe\u5802\u4e0a\u8bb2\u8fc7\u7684LSTM\u3001CNN\u7b49\u65b9\u6cd5)\uff0c\u56e0\u6b64\u9009\u62e9\u4f7f\u7528\u4e86Kaggle\u5e73\u53f0\u5b8c\u6210\u8fd9\u4efd\u4f5c\u4e1a\u3002\u5f53\u6211\u5b8c\u6210\u540e\u5bf9ipynb\u8fdb\u884c\u4fdd\u5b58\u540e\uff0c\u5374\u53d1\u73b0\u6253\u5f00\u6587\u4ef6\u540e\u6240\u6709cell\u7684\u8fd0\u884c\u7ed3\u679c\u5e76\u6ca1\u6709\u4fdd\u5b58\u3002\u6240\u4ee5\u53ea\u597d\u5c06\u6b64\u4efd\u6587\u6863\u751f\u6210\u94fe\u63a5\uff0c\u53d1\u9001\u7ed9\u8001\u5e08\uff0c\u5e0c\u671b\u8001\u5e08\u80fd\u591f\u8c05\u89e3\uff01\n### \u63a5\u4e0b\u6765\u5f00\u59cb\u6b63\u5f0f\u4ecb\u7ecd\u4f5c\u4e1a\n### 1. \u672c\u62a5\u544a\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\n* \u6570\u636e\u63cf\u8ff0\u53ca\u6570\u636e\u6e05\u6d17\n* \u673a\u5668\u5b66\u4e60\u5efa\u6a21\n* \u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\n### \u5728\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u9636\u6bb5\u53c8\u5206\u4e3a\n* CountVectorize + RandomForest\n* TF-IDF + RandomForest\n### \u5728\u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\u9636\u6bb5\u53c8\u5206\u4e3a\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b1(LSTM)\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b2(GRU+Pooling)\n* \u672a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u2014\u6a21\u578b3(CNN+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b4(LSTM)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b5(GRU+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b6(CNN+Pooling)\n* \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b7(GRU+CNN+Pooling)\n### \u6211\u5728\u5b8c\u6210\u4f5c\u4e1a\u65f6\uff0c\u5206\u522b\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u548c\u65b9\u6cd5\u8fdb\u884c\u5efa\u6a21\u3002\n### \u5728\u7ecf\u8fc7\u6a21\u578b\u7684\u591a\u6b21\u8bad\u7ec3\u540e\uff0c\u673a\u5668\u5b66\u4e60\u7684\u4e24\u4e2a\u65b9\u6cd5\u5747\u53ef\u4ee5\u4f7f\u9884\u6d4b\u7684\u51c6\u786e\u7387\u8fbe\u523095%\u4ee5\u4e0a\uff1b\n### \u800c\u5728\u6df1\u5ea6\u5b66\u4e60\u9636\u6bb5\uff0c\u4ec5\u6709\u6a21\u578b7\u53ef\u4ee5\u7a33\u5b9a\u4f7f\u9884\u6d4b\u7684\u51c6\u786e\u7387\u8fbe\u523095%\u4ee5\u4e0a\n### \u5728\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u65f6\uff0c\u7ecf\u5e38\u4f1a\u53d1\u73b0\u4e24\u79cd\u60c5\u51b5\n* 1. \u8bad\u7ec3\u96c6\u7684\u62df\u5408\u7a0b\u5ea6\u8f83\u597d\uff0c\u4f46\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\u3002\u5373\u51fa\u73b0\u8fc7\u62df\u5408\u72b6\u6001\u3002\n* 2. \u5bf9\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5\uff0c\u6211\u901a\u5e38\u4f1a\u91c7\u7528\u589e\u52a0dropout\u5c42\u7684\u6bd4\u91cd\u6765\u63a7\u5236\uff0c\u4f46\u8fd9\u53c8\u4f1a\u7ecf\u5e38\u4f7f\u5f97\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u7684\u4e2d\u95f4\u8fc7\u7a0b\u4e2d\uff0c\u51fa\u73b0\u65ad\u5d16\u5f0f\u5730\u4e0b\u8dcc\u3002\n### \u5bf9\u4e8e\u8fd9\u4e24\u79cd\u60c5\u51b5\uff0c\u7ecf\u5e38\u89c9\u5f97\u6bd4\u8f83\u56f0\u6270\uff0c\u6ca1\u6709\u80fd\u591f\u5f88\u597d\u7684\u89e3\u51b3\u65b9\u6cd5\u3002\u56e0\u6b64\u6700\u7ec8\u5c1d\u8bd5\u5230\u4e86\u6a21\u578b7\u7684\u65b9\u6cd5\u3002\n### \u6a21\u578b7\u7684\u65b9\u6cd5\u540c\u65f6\u4f7f\u7528\u4e86GRU\u548cCNN\u7684\u65b9\u5f0f\uff0c\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u589e\u5f3a\u4e86\u7279\u5f81\u7684\u8868\u8fbe\u80fd\u529b(GRU\u643a\u5e26\u6709\u8fc7\u53bb\u7684\u4fe1\u606f\uff0c\u800cCNN\u5c42\u53c8\u901a\u8fc7\u5377\u79ef\u7684\u65b9\u5f0f\u63d0\u53d6\u7279\u5f81)\u3002\u56e0\u6b64\u8fd9\u4e24\u4e2a\u5c42\u7684\u7ed3\u5408\u4f7f\u5f97\u6a21\u578b\u7684\u6548\u679c\u975e\u5e38\u51fa\u8272\u3002","11098e4e":"## 2. \u6587\u672c\u5904\u7406\u9636\u6bb5","6d2b7f07":"## 5.6 \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b4(LSTM)","9e3858ed":"### \u540c\u6837\u7684\u7ed8\u5236\u6807\u7b7e\u7684\u5206\u5e03\u72b6\u6001\n### \u53ef\u4ee5\u53d1\u73b0\u5404\u4e2a\u6807\u7b7e\u5206\u5e03\u6bd4\u8f83\u5747\u5300\uff0c\u4e0d\u5b58\u5728\u6837\u672c\u4e0d\u5e73\u8861\u7684\u73b0\u8c61","1ec96e56":"### \u8ba1\u7b97\u6bcf\u884c\u6587\u672c\u7684\u5355\u8bcd\u6570\u91cf\uff0c\u5e76\u7ed8\u5236\u56fe\u50cf\n### \u53ef\u4ee5\u770b\u5230\u6570\u636e\u6574\u4f53\u5448\u73b0\u53f3\u504f\u5206\u5e03\uff0c\u4f46\u5355\u8bcd\u6570\u91cf\u5927\u90e8\u5206\u96c6\u4e2d\u57280-500\u7684\u8303\u56f4\u5185\n","caa16247":"## 5.8 \u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165-\u6a21\u578b6(CNN+Pooling)","a0d6e302":"## 4.2 TF-IDF + RandomForest\n### \u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a93.3863%","61847b23":"## 5.5 \u4f7f\u7528 GloVe \u9884\u8bad\u7ec3\u8bcd\u5d4c\u5165\u524d\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5904\u7406"}}