{"cell_type":{"fe640fee":"code","90e1b960":"code","d4aefbcf":"code","271e7094":"code","c9b79b49":"code","31305fa6":"code","99c3d6cd":"code","59730c94":"code","d91291d1":"code","169443dd":"code","e2c9e218":"code","6e2e9489":"code","4c9afebb":"code","d753df5f":"code","937ccdb1":"markdown"},"source":{"fe640fee":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression","90e1b960":"from sklearn.model_selection import StratifiedKFold\n\n\nclass ContinuousTargetStratifiedKFold:\n    \"\"\"\n    \u5c06\u8fde\u7eed\u53d8\u91cf\u7684\u6570\u636e\u4f7f\u7528StratifiedKFold\u8fdb\u884c\u8f93\u51fa\n    -------\n    Parameters\n    -------\n    n_splits(int): \u5206\u5272\u6570\u636e\u7684\u591a\u5c11\n    nbins(int): \u5206\u6876\u6570, from pd.cut\n    shuffle(bool): \u662f\u5426\u6253\u4e71\n    random_state(int): \u6253\u4e71\u7684\u968f\u673a\u6570\u79cd\u5b50\n    -------\n    Sources\n    -------\n    - https:\/\/www.codenong.com\/30040597\/\n    - https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification\n    \"\"\"\n    def __init__(self, n_splits=3, nbins=100, shuffle=False,\n                 random_state=None):\n        self.skf = StratifiedKFold(n_splits=n_splits,\n                                   shuffle=shuffle,\n                                   random_state=random_state)\n        self.nbins = nbins\n        self.n_splits = n_splits\n\n    def split(self, X, y):\n        y = pd.cut(y, self.nbins, labels=False)\n        for train_index, test_index in self.skf.split(y, y):\n            yield train_index, test_index\n\n    def get_n_splits(self, X=None, y=None):\n        return self.n_splits","d4aefbcf":"kfold = 10","271e7094":"def return_df(seed=42):\n    df = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\n    df['kfold'] = -1\n    kf = ContinuousTargetStratifiedKFold(n_splits=kfold, nbins=1000,shuffle=True, random_state=seed)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(df, df.target)):\n        df.loc[val_ind, 'kfold'] = fold\n    return df","c9b79b49":"debug = False","31305fa6":"df = return_df(42)\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]","99c3d6cd":"#Target Encoding\nfor col in object_cols:\n    temp_data = []\n    temp_test_feat = None\n    feat = df.groupby(col)[\"target\"].agg(\"median\")\n    feat = feat.to_dict()\n    df_test.loc[:, f\"tar_enc_{col}\"] = df_test[col].map(feat)\n    df.loc[:, f\"tar_enc_{col}\"] = df[col].map(feat)","59730c94":"useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n\nobject_cols_oh = [col for col in useful_features if col.startswith(\"cat\") and col!='cat9']\n#categorical cols for ordinal encoding\nobject_cols_od = ['cat9']\n#Numerical cols\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\n\ndf_test = df_test[useful_features]","d91291d1":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(kfold):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    #ordinal-encode categorical columns\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols_od] = ordinal_encoder.fit_transform(xtrain[object_cols_od])\n    xvalid[object_cols_od] = ordinal_encoder.transform(xvalid[object_cols_od])\n    xtest[object_cols_od] = ordinal_encoder.transform(xtest[object_cols_od])\n    \n    #one-hot-encode categorical columns\n    oh_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n    X_train_ohe = oh_encoder.fit_transform(xtrain[object_cols_oh])\n    X_valid_ohe = oh_encoder.transform(xvalid[object_cols_oh])\n    X_test_ohe= oh_encoder.transform(xtest[object_cols_oh])\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])]) \n    X_valid_ohe = pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])]) \n    X_test_ohe = pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])]) \n    \n    num_X_train = xtrain.drop(object_cols_oh, axis=1)\n    num_X_valid = xvalid.drop(object_cols_oh, axis=1)\n    num_X_test = xtest.drop(object_cols_oh, axis=1) \n    \n    xtrain = pd.concat([num_X_train,X_train_ohe],axis=1)\n    xvalid = pd.concat([num_X_valid,X_valid_ohe],axis=1)\n    xtest = pd.concat([num_X_test,X_test_ohe],axis=1)       \n\n    \n    xgb_params = {\n        'learning_rate': 0.15834717111407332,\n        'reg_lambda': 0.008347697504479864,\n        'reg_alpha': 28.61195680804279,\n        'subsample': 0.9996345489574131,\n        'colsample_bytree': 0.10330010325726227,\n        'max_depth': 2,\n        \"n_estimators\":7000,\n        \"random_state\":42\n    }\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        xtrain, ytrain, \n        early_stopping_rounds=300, \n        eval_set=[(xvalid, yvalid)],\n        verbose=1000\n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\n    \n\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n# final_valid_predictions.columns = [\"id\", \"pred\"]\nfinal_valid_predictions.to_csv(\"train_pred.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\n# sample_submission.columns = [\"id\", \"pred\"]\nsample_submission.to_csv(\"test_pred.csv\", index=False)","169443dd":"def return_over_df(df, seed=42):\n    df['kfold'] = -1\n    kf = ContinuousTargetStratifiedKFold(n_splits=kfold, nbins=1000,shuffle=True, random_state=seed)\n    for fold, (trn_ind, val_ind) in enumerate(kf.split(df, df.target)):\n        df.loc[val_ind, 'kfold'] = fold\n    return df","e2c9e218":"sample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf_test[\"id\"] = sample_submission.id\ndf_test[\"target\"] = sample_submission.target\ndf_test['kfold'] = -1\n\ndf = pd.concat([df, df_test], axis=0).reset_index(drop=True)\ndf = return_over_df(df)","6e2e9489":"df_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n\nuseful_features = [c for c in df_test.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\n#Target Encoding\nfor col in object_cols:\n    temp_data = []\n    temp_test_feat = None\n    feat = df.groupby(col)[\"target\"].agg(\"median\")\n    feat = feat.to_dict()\n    df_test.loc[:, f\"tar_enc_{col}\"] = df_test[col].map(feat)\n    df.loc[:, f\"tar_enc_{col}\"] = df[col].map(feat)","4c9afebb":"useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n\nobject_cols_oh = [col for col in useful_features if col.startswith(\"cat\") and col!='cat9']\n#categorical cols for ordinal encoding\nobject_cols_od = ['cat9']\n#Numerical cols\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\n\ndf_test = df_test[useful_features]\n\n","d753df5f":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(kfold):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    #ordinal-encode categorical columns\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols_od] = ordinal_encoder.fit_transform(xtrain[object_cols_od])\n    xvalid[object_cols_od] = ordinal_encoder.transform(xvalid[object_cols_od])\n    xtest[object_cols_od] = ordinal_encoder.transform(xtest[object_cols_od])\n    \n    #one-hot-encode categorical columns\n    oh_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n    X_train_ohe = oh_encoder.fit_transform(xtrain[object_cols_oh])\n    X_valid_ohe = oh_encoder.transform(xvalid[object_cols_oh])\n    X_test_ohe= oh_encoder.transform(xtest[object_cols_oh])\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])]) \n    X_valid_ohe = pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])]) \n    X_test_ohe = pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])]) \n    \n    num_X_train = xtrain.drop(object_cols_oh, axis=1)\n    num_X_valid = xvalid.drop(object_cols_oh, axis=1)\n    num_X_test = xtest.drop(object_cols_oh, axis=1) \n    \n    xtrain = pd.concat([num_X_train,X_train_ohe],axis=1)\n    xvalid = pd.concat([num_X_valid,X_valid_ohe],axis=1)\n    xtest = pd.concat([num_X_test,X_test_ohe],axis=1)       \n\n    \n    xgb_params = {\n        'learning_rate': 0.15834717111407332,\n        'reg_lambda': 0.008347697504479864,\n        'reg_alpha': 28.61195680804279,\n        'subsample': 0.9996345489574131,\n        'colsample_bytree': 0.10330010325726227,\n        'max_depth': 2,\n        \"n_estimators\":7000,\n        \"random_state\":42\n    }\n\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        xtrain, ytrain, \n        early_stopping_rounds=300, \n        eval_set=[(xvalid, yvalid)],\n        verbose=1000\n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \n\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n# final_valid_predictions.columns = [\"id\", \"pred\"]\nfinal_valid_predictions.to_csv(\"train_pred_overfitting.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\n# sample_submission.columns = [\"id\", \"pred\"]\nsample_submission.to_csv(\"test_pred_overfitting.csv\", index=False)","937ccdb1":"- Base Model 1: XGB(Version 1, 0.716258 \/ 0.71753 \/ 0.71587, 0.553695 \/ 0.71748 \/ 0.71587) params from [blending blending blending](https:\/\/www.kaggle.com\/abhishek\/blending-blending-blending) \n- Base Model 2: XGB(Version 2, 0.716455 \/ 0.71750 \/ 0.71590, 0.553715 \/ 0.71742 \/ 0.71584) params from [blending blending blending](https:\/\/www.kaggle.com\/abhishek\/blending-blending-blending) \n- Base Model 3: XGB(Version 3, 0.716154 \/ 0.71743 \/ 0.71581, 0.553615 \/ 0.71747 \/ 0.71587) params from [Tuning params](https:\/\/www.kaggle.com\/adityastark07\/tuning-params) \n- Base Model 4: XGB(Version 4, 0.716419 \/ 0.71757 \/ 0.71609, 0.553674 \/ 0.71752 \/ 0.71606) params from [KFold XGBoost - Edited with Optuna tuning](https:\/\/www.kaggle.com\/elegent\/kfold-xgboost-edited-with-optuna-tuning) \n- Base Model 5: XGB(Version 5, 0.716577 \/ 0.71792 \/ 0.71631, 0.553906 \/ 0.71778 \/ 0.71620) params from [30 days ml \/ XGBR and auto-eda](https:\/\/www.kaggle.com\/boneacrabonjac\/30-days-ml-xgbr-and-auto-eda)  \n- Base Model 6: XGB(Version 8, 0.716238 \/ 0.71745 \/ 0.71583, 0.553547 \/ 0.71737 \/ 0.71578) params from [XGBoost tuned hyperparams + EDA](https:\/\/www.kaggle.com\/sergeyzemskov\/xgboost-tuned-hyperparams-eda)  \n- Base Model 7: XGB(Version 15, 0.716311 \/ 0.71741 \/ 0.71587) params from [Target Encoding](https:\/\/www.kaggle.com\/agneypraseed\/target-encoding), only used in `Normal`\n\n\\**(Version, Normal CV\/LB\/PB, Pseudo-Label CV\/LB\/PB)*"}}