{"cell_type":{"134e3489":"code","20804311":"code","424e6107":"code","baedb720":"code","c397ef88":"code","d580274f":"code","83b0eb4b":"code","a5b72192":"code","9b9653e0":"code","83702e91":"code","33e9e3ca":"code","4c70609f":"code","e56101fb":"code","c0050c4b":"code","c65d44e8":"code","48c9cfb1":"code","b97afc9b":"code","f29e50af":"code","901840d6":"code","3c7240ce":"code","1cd5536e":"code","394557b3":"code","a78ebda8":"code","e43ee8e7":"code","a745e2f5":"code","94476d50":"code","9dee91cb":"code","62e20eb6":"code","7ddc305d":"code","c5935c02":"code","f08c27a6":"code","f9338ccf":"code","8e722a9f":"code","01347758":"code","5877af64":"code","0f980f59":"code","b2013f26":"code","1ceb0c07":"code","4f7e339b":"code","71e24ac0":"code","71695ce1":"code","ea09135c":"code","74ba424a":"code","ed06066b":"code","b5c8c4e5":"code","1ec635cd":"code","8c1f8d66":"code","53081569":"code","15ccc9c1":"code","3c873c36":"code","07d60e7e":"code","b506b540":"code","08423ad5":"code","5811eef9":"code","ff039f73":"code","3f968b25":"code","78b1aec5":"code","edf9c15c":"code","531f21ac":"code","be47cbe0":"code","46352418":"code","9423f9cc":"code","23f40278":"code","a6cd7bea":"code","9b5b649a":"code","90851f73":"code","f6222e2c":"code","4621cb35":"code","979f90be":"code","3e875d61":"code","6b207cf4":"code","900a3d0e":"code","83276369":"code","824610cd":"code","caeac153":"code","e42f7e69":"code","e7b77be4":"code","bf9c90ef":"code","2083a45d":"code","80b7325c":"code","565abfe2":"code","a4531e58":"code","a968d334":"code","3d75e7aa":"code","d4be0b54":"code","4497daa6":"code","4e160a6e":"code","12f80d01":"code","22cab2f5":"code","1d26c78e":"code","bd98393d":"code","934d9ddc":"code","0147cea9":"code","bbb349ee":"code","3ea90871":"code","821a970f":"code","01eb984c":"code","a09f6ccc":"markdown","c5dbe464":"markdown","81ff4577":"markdown","84d95d0d":"markdown","7152fcc9":"markdown","7c7a7550":"markdown","80fdaecc":"markdown","1699eb08":"markdown","4cd3eef3":"markdown","1ab65992":"markdown","a82006f6":"markdown","9692fe9f":"markdown","38ce8515":"markdown","3d9d205f":"markdown","ef1cf71f":"markdown","059db16e":"markdown","ccab7bb5":"markdown","28fedfae":"markdown","70aebe20":"markdown","f0608b2b":"markdown","8940f5e8":"markdown","6e2f0196":"markdown","82ca3068":"markdown","671dcf53":"markdown","5da219cd":"markdown"},"source":{"134e3489":"import numpy as np # linear algebra\nimport pandas as pd \nimport os\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Audio\nimport tensorflow as tf","20804311":"pip install auditok","424e6107":"import auditok","baedb720":"sample_speech = auditok.load('..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_ANG_XX.wav')","c397ef88":"sample_speech.splitp()","d580274f":"sample_speech = auditok.load('..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_HAP_XX.wav')\nsample_speech.splitp()","83b0eb4b":"samples = np.asarray(sample_speech)\nsamples","a5b72192":"def create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","9b9653e0":"def get_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    img = librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    height = img._meshHeight\n    img = img.get_array().reshape(img._meshWidth, img._meshHeight)\n    img = np.array(img)\n    return img","83702e91":"emotion='happy'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_MTI_HAP_XX.wav'\ndata, sampling_rate = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","33e9e3ca":"emotion='happy'\npath = '..\/input\/urdu-language-speech-dataset\/Happy\/SF1_F10_H010.wav'\ndata1, sampling_rate1 = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data1, sampling_rate1, emotion)\nAudio(path)","4c70609f":"emotion='happy'\npath = '..\/input\/urdu-language-speech-dataset\/Happy\/SF1_F3_H03.wav'\ndata2, sampling_rate2 = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","e56101fb":"from skimage.metrics import structural_similarity as ssim","c0050c4b":"emotion='Neutral'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_NEU_XX.wav'\ndata, sampling_rate = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","c65d44e8":"emotion='Angry'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_ANG_XX.wav'\ndata, sampling_rate = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","48c9cfb1":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_FEA_XX.wav'\ndata, sampling_rate = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","b97afc9b":"!pip install SpeechRecognition","f29e50af":"import speech_recognition as sr\nsr.__version__\nrecog = sr.Recognizer()","901840d6":"speech = sr.AudioFile('..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav')\nwith speech as source:\n    audio = recog.record(source)\nrecog.recognize_google(audio)","3c7240ce":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ndata, sampling_rate = librosa.load(path)\n#create_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","1cd5536e":"speech = sr.AudioFile('..\/input\/urdu-language-speech-dataset\/Happy\/SF1_F1_H01.wav')\nwith speech as source:\n    audio1 = recog.record(source)\nrecog.recognize_google(audio1, language='ur-In')","394557b3":"speech = sr.AudioFile('..\/input\/hindi-speech-recognition\/test\/audio\/0116_036.wav')\nwith speech as source:\n    audio1 = recog.record(source)\nrecog.recognize_google(audio1, language='hi-In')","a78ebda8":"sample_speech = auditok.load('..\/input\/hindi-speech-recognition\/test\/audio\/0116_036.wav')\nsample_speech.splitp()","e43ee8e7":"len(sample_speech)","a745e2f5":"print(sample_speech.duration) # duration in seconds\nprint(sample_speech.sampling_rate) # alias `sr`\nprint(sample_speech.sample_width) # alias `sw`\nprint(sample_speech.channels) # alias `ch`","94476d50":"from pydub import AudioSegment\nfrom pydub.silence import split_on_silence\n\nsound_file = AudioSegment.from_wav(\"..\/input\/hindi-speech-recognition\/test\/audio\/0116_036.wav\")\naudio_chunks = split_on_silence(sound_file, \n    # must be silent for at least half a second\n    min_silence_len=100,\n\n    # consider it silent if quieter than -16 dBFS\n    silence_thresh=-16\n)\n\nfor i, chunk in enumerate(audio_chunks):\n\n    out_file = \"chunk{0}.wav\".format(i)\n    print(\"exporting\", out_file)\n    chunk.export(out_file, format=\"wav\")","9dee91cb":"import os","62e20eb6":"text_path= ['..\/input\/hindi-speech-recognition\/test\/audio\/0116_003.wav','..\/input\/hindi-speech-recognition\/test\/audio\/0116_008.wav','..\/input\/hindi-speech-recognition\/test\/audio\/0116_025.wav','..\/input\/hindi-speech-recognition\/test\/audio\/0136_003.wav']\ntext_list = []","7ddc305d":"lan = 'hi-IN'","c5935c02":"for path in text_path:\n    texts = ''\n    audio_regions = auditok.split(path)\n\n    for i, r in enumerate(audio_regions):\n        print(\"Region {i}: {r.meta.start:.3f}s -- {r.meta.end:.3f}s\".format(i=i, r=r))  \n        filename = r.save(\"region_{meta.start:.3f}-{meta.end:.3f}.wav\")\n        speech = sr.AudioFile(filename)\n        print(speech)\n        with speech as source:\n            audio1 = recog.record(source)\n        try:\n            text = recog.recognize_google(audio1, language=lan)\n            texts+= str(text)\n            os.remove(filename)\n        except:\n            pass\n    text_list.append(texts)","f08c27a6":"for path in text_path:\n    sample_speech = auditok.load(path)\n    sample_speech.splitp()","f9338ccf":"speech = sr.AudioFile(text_path[3])\nwith speech as source:\n    audio1 = recog.record(source)\ntext = recog.recognize_google(audio1, language=lan)\nprint(text)","8e722a9f":"Audio(path)","01347758":"from transformers import AutoModel, AutoTokenizer","5877af64":"tokenizer = AutoTokenizer.from_pretrained('..\/input\/modelf1')","0f980f59":"text_list","b2013f26":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport random\nimport tqdm","1ceb0c07":"path = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_DFA_ANG_XX.wav'\ndata, sampling_rate = librosa.load(path)","4f7e339b":"mfcc = librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=40)\nmfcc","71e24ac0":"mean_mfcc = np.mean(mfcc.T, axis=0)\nmean_mfcc","71695ce1":"lpcs = librosa.lpc(data, 16)\nlpcs","ea09135c":"from numpy import *","74ba424a":"def autocorr(self, order=None):\n    if order is None:\n        order = len(self) - 1\n    return [sum(self[n] * self[n + tau] for n in range(len(self) - tau)) for tau in range(order + 1)]","ed06066b":"def core_lpcc(seq, err_term, order=None):\n    if order is None:\n        order = len(seq) - 1\n    lpcc_coeffs = [np.log(err_term), -seq[0]]\n    for n in range(2, order + 1):\n        # Use order + 1 as upper bound for the last iteration\n        upbound = (order + 1 if n > order else n)\n        lpcc_coef = -sum(i * lpcc_coeffs[i] * seq[n - i - 1]\n                         for i in range(1, upbound)) * 1. \/ upbound\n        lpcc_coef -= seq[n - 1] if n <= len(seq) else 0\n        lpcc_coeffs.append(lpcc_coef)\n    return lpcc_coeffs","b5c8c4e5":"  def lpcc(lpcorder=None, cepsorder=None):\n        coefs =  librosa.lpc( lpcorder,cepsorder)\n        acseq =  np.array(autocorr(lpcorder, cepsorder))\n        err_term =np.sqrt( acseq[0] + sum(a * c for a, c in zip(acseq[1:], coefs)))\n        return core_lpcc(coefs, err_term, cepsorder)","1ec635cd":"lpccs = lpcc(data,16)\nlpccs","8c1f8d66":"import itertools","53081569":"def lsf(data,order=None,rectify=True):\n        coefs =  librosa.lpc( data,order)\n        return core_lsf(coefs,rectify)","15ccc9c1":"def core_lsf(lpcseq,rectify=True):\n    rhs = [0] + lpcseq[::-1] + [1]\n    P = []\n    Q = []\n    # Assuming constant coefficient is 1, which is required. Moreover z^{-p+1} does not exist on the lhs, thus appending 0\n    lpcseq = [1] + lpcseq[:] + [0]\n    for l,r in itertools.zip_longest(lpcseq,rhs):\n        P.append(l + r)\n        Q.append(l - r)\n    p_roots = np.roots(P[::-1])\n    q_roots = np.roots(Q[::-1])\n    lsf_p = sorted(np.angle(p_roots))\n    lsf_q = sorted(np.angle(q_roots))\n    if rectify:\n        return sorted(i for i in lsf_q + lsf_p if (i > 0))[:-1]\n    else:\n        return sorted(i for i in lsf_q + lsf_p)","3c873c36":"lsfs = lsf(data,16)\nlsfs","07d60e7e":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_spectrogram(data, sampling_rate, emotion)","b506b540":"def create_melspectrogram(data,sr,e):\n    D = np.abs(librosa.stft(data))**2\n    S = librosa.feature.melspectrogram(S=D, sr=sr)\n    Xdb = librosa.amplitude_to_db(abs(S))\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    ax.set(title='Mel-frequency spectrogram')\n    ","08423ad5":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_melspectrogram(data, sampling_rate, emotion)\n","5811eef9":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ndata, sr = librosa.load(path)\nS = np.abs(librosa.stft(data, n_fft=4096))**2\nchroma = librosa.feature.chroma_stft(S=S, sr=sr)\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='Chromagram')","ff039f73":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ndata, sr = librosa.load(path)\nchroma_cq = librosa.feature.chroma_cqt(y=data, sr=sr)\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='Constant Q Chromagram')","3f968b25":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nchroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='Chroma CENS')","78b1aec5":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nS, phase = librosa.magphase(librosa.stft(y))\nrms = librosa.feature.rms(S=S)\nfig, ax = plt.subplots(nrows=2, sharex=True)\ntimes = librosa.times_like(rms)\nax[0].set(title='RMS')\nax[0].semilogy(times, rms[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='log Power spectrogram')","edf9c15c":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\ncentroid = librosa.feature.spectral_centroid(y=y, sr=sr)\ncentroid","531f21ac":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nspec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\nspec_bw","be47cbe0":"emotion='Fear'\npath = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nS = np.abs(librosa.stft(y))\ncontrast = librosa.feature.spectral_contrast(S=S, sr=sr)\ncontrast","46352418":"fig, ax = plt.subplots(nrows=2, sharex=True)\nimg1 = librosa.display.specshow(librosa.amplitude_to_db(S,\n                                                 ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[0])\nfig.colorbar(img1, ax=[ax[0]], format='%+2.0f dB')\nax[0].set(title='Power spectrogram')\nax[0].label_outer()\nimg2 = librosa.display.specshow(contrast, x_axis='time', ax=ax[1])\nfig.colorbar(img2, ax=[ax[1]])\nax[1].set(ylabel='Frequency bands', title='Spectral contrast')","9423f9cc":"flatness = librosa.feature.spectral_flatness(y=y)\nflatness","23f40278":"rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.99)\nrolloff","a6cd7bea":"y = librosa.effects.harmonic(y)\ntonnetz = librosa.feature.tonnetz(y=y, sr=sr)\ntonnetz","9b5b649a":"fig, ax = plt.subplots(nrows=2, sharex=True)\nimg1 = librosa.display.specshow(tonnetz,\n                                y_axis='tonnetz', x_axis='time', ax=ax[0])\nax[0].set(title='Tonal Centroids (Tonnetz)')\nax[0].label_outer()\nimg2 = librosa.display.specshow(librosa.feature.chroma_cqt(y, sr=sr),\n                                y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(title='Chroma')\nfig.colorbar(img1, ax=[ax[0]])\nfig.colorbar(img2, ax=[ax[1]])","90851f73":"path = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nlibrosa.feature.zero_crossing_rate(y)","f6222e2c":"path = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nhop_length = 512\noenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\ntempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,\n                                      hop_length=hop_length)","4621cb35":"tempogram","979f90be":"ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\nac_global = librosa.util.normalize(ac_global)\ntempo = librosa.beat.tempo(onset_envelope=oenv, sr=sr,\n                           hop_length=hop_length)[0]\ntempo","3e875d61":"fig, ax = plt.subplots(nrows=4, figsize=(15, 15))\ntimes = librosa.times_like(oenv, sr=sr, hop_length=hop_length)\nax[0].plot(times, oenv, label='Onset strength')\nax[0].label_outer()\nax[0].legend(frameon=True)\nlibrosa.display.specshow(tempogram, sr=sr, hop_length=hop_length,x_axis='time', y_axis='tempo', cmap='magma',ax=ax[1])\nax[1].axhline(tempo, color='w', linestyle='--', alpha=1,\n            label='Estimated tempo={:g}'.format(tempo))\nax[1].legend(loc='upper right')\nax[1].set(title='Tempogram')\nx = np.linspace(0, tempogram.shape[0] * float(hop_length) \/ sr,\n                num=tempogram.shape[0])\nax[2].plot(x, np.mean(tempogram, axis=1), label='Mean local autocorrelation')\n#ax[2].plot(x, ac_global, '--', alpha=0.75, label='Global autocorrelation')\nax[2].set(xlabel='Lag (seconds)')\nax[2].legend(frameon=True)\nfreqs = librosa.tempo_frequencies(tempogram.shape[0], hop_length=hop_length, sr=sr)\nax[3].semilogx(freqs[1:], np.mean(tempogram[1:], axis=1),\n             label='Mean local autocorrelation', basex=2)\n#ax[3].semilogx(freqs[1:], ac_global[1:], '--', alpha=0.75,label='Global autocorrelation', basex=2)\nax[3].axvline(tempo, color='black', linestyle='--', alpha=.8,\n            label='Estimated tempo={:g}'.format(tempo))\nax[3].legend(frameon=True)\nax[3].set(xlabel='BPM')\nax[3].grid(True)","6b207cf4":"mfcc = librosa.feature.mfcc(y=y, sr=sr)\nmfcc_delta = librosa.feature.delta(mfcc)\nmfcc_delta","900a3d0e":"mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\nmfcc_delta2","83276369":"fig, ax = plt.subplots(nrows=3, sharex=True, sharey=True,figsize=(15,15))\nimg1 = librosa.display.specshow(mfcc, ax=ax[0], x_axis='time')\nax[0].set(title='MFCC')\nax[0].label_outer()\nimg2 = librosa.display.specshow(mfcc_delta, ax=ax[1], x_axis='time')\nax[1].set(title=r'MFCC-$\\Delta$')\nax[1].label_outer()\nimg3 = librosa.display.specshow(mfcc_delta2, ax=ax[2], x_axis='time')\nax[2].set(title=r'MFCC-$\\Delta^2$')\nfig.colorbar(img1, ax=[ax[0]])\nfig.colorbar(img2, ax=[ax[1]])\nfig.colorbar(img3, ax=[ax[2]])","824610cd":"path = '..\/input\/speech-emotion-recognition-en\/Crema\/1001_ITS_FEA_XX.wav'\ny, sr = librosa.load(path)\nchroma = librosa.feature.chroma_stft(y=y, sr=sr)\ntempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)\nbeats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])\nchroma_sync = librosa.util.sync(chroma, beats)\nchroma_lag = librosa.feature.stack_memory(chroma_sync, n_steps=3,\n                                          mode='edge')\nfig, ax = plt.subplots(figsize=(15,15))\nbeat_times = librosa.frames_to_time(beats, sr=sr, hop_length=512)\nlibrosa.display.specshow(chroma_lag, y_axis='chroma', x_axis='time',\n                         x_coords=beat_times, ax=ax)\nax.set(yticks=[0, 12, 24], yticklabels=['Lag=0', 'Lag=1', 'Lag=2'],\n          title='Time-lagged chroma')\nax.hlines([0, 12, 24], beat_times.min(), beat_times.max(), color='w')","caeac153":"data = {'Path':[],\n         'language':[],\n        'Emotion':[]}\nData = pd.DataFrame(data)\nData","e42f7e69":"Emotion = {'SAD':  'Sad', 'ANG':'Angry','DIS' :  'Disgust', 'FEA' : 'Fear', 'HAP' : 'Happy', 'NEU': 'Neutral',\n            'sa':  'Sad', 'a':'Angry','d' :  'Disgust', 'f' : 'Fear', 'h' : 'Happy', 'n': 'Neutral', 'su': 'Surprise',\n            'Sad':  'Sad', 'angry':'Angry','disgust' :  'Disgust', 'Fear' : 'Fear', 'happy' : 'Happy', 'neutral': 'Neutral',\n           'sad' : 'Sad', 'Pleasant':'Pleasant Surprise','fear':'Fear','pleasant':'Pleasant Surprise', \n            '04':  'Sad', '05':'Angry','07' :  'Disgust', '06' : 'Fear', '03' : 'Happy', '01': 'Neutral', '02':'Calm','08' : 'Surprise',\n            'T':  'Sad', 'W':'Angry','E' :  'Disgust', 'A' : 'Fear', 'F' : 'Happy', 'N': 'Neutral','L':'Boredom',\n          'dis' : 'Disgust', 'gio':'Happy', 'pau' : 'Fear', 'rab' : 'Anger' , 'tri':'Sad','sor':'Surprise', 'neu' : 'Neutral'}","e7b77be4":"directory = '..\/input\/speech-emotion-recognition-en\/Crema'\nlan = 'en-US'\nfor filename in os.listdir(directory):\n    emotion = filename.split('_')[2]\n    f = os.path.join(directory, filename)\n    Data.loc[len(Data.index)] = [f, lan, Emotion[emotion]] ","bf9c90ef":"Data","2083a45d":"directory = '..\/input\/speech-emotion-recognition-en\/Savee'\nlan = 'en-US'\nfor filename in os.listdir(directory):\n    emotion = str(filename.split('_')[1])[0]\n    if emotion == 's':\n        emotion += str(filename.split('_')[1])[1]\n    f = os.path.join(directory, filename)\n    Data.loc[len(Data.index)] = [f, lan, Emotion[emotion]] ","80b7325c":"Data","565abfe2":"directory = '..\/input\/speech-emotion-recognition-en\/Tess'\nlan = 'en-US'\nfor sub_directory in os.listdir(directory):\n    emotion = sub_directory.split('_')[1]\n    direc = os.path.join(directory,sub_directory)\n    for file in os.listdir(direc):\n        f = os.path.join(direc,file)\n        Data.loc[len(Data.index)] = [f, lan, Emotion[emotion]]","a4531e58":"Data","a968d334":"directory = '..\/input\/speech-emotion-recognition-en\/Ravdess\/audio_speech_actors_01-24'\nlan = 'en-US'\nfor sub_directory in os.listdir(directory):\n    direc = os.path.join(directory,sub_directory)\n    for file in os.listdir(direc):\n        emotion = file.split('-')[2]\n        f = os.path.join(direc,file)\n        Data.loc[len(Data.index)] = [f, lan, Emotion[emotion]]","3d75e7aa":"Data","d4be0b54":"directory = '..\/input\/urdu-language-speech-dataset'\nlan = 'ur-In'\nfor sub_directory in os.listdir(directory):\n    emotion = sub_directory\n    direc = os.path.join(directory,sub_directory)\n    if direc == '..\/input\/urdu-language-speech-dataset\/README.md':\n        pass\n    else:\n        for file in os.listdir(direc):\n            f = os.path.join(direc,file)\n            Data.loc[len(Data.index)] = [f, lan, emotion]","4497daa6":"Data","4e160a6e":"directory = '..\/input\/berlin-database-of-emotional-speech-emodb\/wav'\nlan = 'de-DE'\nfor filename in os.listdir(directory):\n    emotion = filename[5]\n    f = os.path.join(directory, filename)\n    Data.loc[len(Data.index)] = [f, lan, Emotion[emotion]] ","12f80d01":"Data","22cab2f5":"data = {'Path':[],\n         'language':[],\n        'Emotion':[]}\nForeign_Data = pd.DataFrame(data)\nForeign_Data","1d26c78e":"directory = '..\/input\/emovo-italian-ser-dataset\/EMOVO'\nlan = 'it-IT'\nfor sub_directory in os.listdir(directory):\n    direc = os.path.join(directory,sub_directory)\n    if sub_directory == 'documents':\n        pass\n    else:\n        for file in os.listdir(direc):\n            emotion = file.split('-')[0]\n            f = os.path.join(direc,file)\n            Foreign_Data.loc[len(Foreign_Data.index)] = [f, lan, Emotion[emotion]]","bd98393d":"Data","934d9ddc":"Foreign_Data","0147cea9":"import speech_recognition as sr\nsr.__version__\nrecog = sr.Recognizer()","bbb349ee":"text_list = []\ntext_path = Data['Path'].tolist()\nlan = Data['language'].tolist()\nfor it in range(len(text_path)):\n    try:\n        texts = ''\n        audio_regions = auditok.split(text_path[it])\n\n        for i, r in enumerate(audio_regions):\n            #print(\"Region {i}: {r.meta.start:.3f}s -- {r.meta.end:.3f}s\".format(i=i, r=r))  \n            filename = r.save(\"region_{meta.start:.3f}-{meta.end:.3f}.wav\")\n            speech = sr.AudioFile(filename)\n            #print(speech)\n            with speech as source:\n                audio1 = recog.record(source)\n            try:\n                text = recog.recognize_google(audio1, language=lan[it])\n                texts+= str(text)\n                os.remove(filename)\n            except:\n                pass\n        text_list.append(texts)\n    except:\n        text_Empty = \" \"\n        text_list.append(text_Empty)\n        \n    if it%1000 == 0:\n        print(it)","3ea90871":"Data['text'] = text_list","821a970f":"Data.to_csv('Data.csv',index=False)","01eb984c":"Foreign_Data.to_csv('ForeignData.csv',index=False)","a09f6ccc":"### Spectral roll off","c5dbe464":"### Chroma CENS","81ff4577":"### ","84d95d0d":"### ","7152fcc9":"### Tempogram","7c7a7550":"### Continous Wavelet Transform","80fdaecc":"## Spectral Analysis","1699eb08":"# Data Prepration","4cd3eef3":"## Rythm Features","1ab65992":"### Linear prediction cepstral coefficients (LPCC)","a82006f6":"## Time Lagged Chromogram","9692fe9f":"### Spectral Bandwidth","38ce8515":"### Tonnetz","3d9d205f":"### Mel frequency cepstral coefficients (MFCC)","ef1cf71f":"### Spectral Contrast","059db16e":"### Chroma stft","ccab7bb5":"### Centroid","28fedfae":"## Delta Feature","70aebe20":"### Constant-Q chromagram","f0608b2b":"### Zero Crossings","8940f5e8":"### Spectral Flatness","6e2f0196":"### Linear prediction  coefficients (LPC)\u00b6","82ca3068":"### Line spectral frequencies (LSF)","671dcf53":"### RMS","5da219cd":"# Speech Analysis"}}