{"cell_type":{"dfa6c30a":"code","967ad551":"code","edf33c41":"code","f93b9032":"code","f38dc3a3":"code","530d2e97":"code","92b1b4ff":"code","344a5fcb":"code","dad5e1a0":"code","dc3cd448":"code","0e619f1b":"code","55e3390d":"code","09ac699d":"code","e34fb2f0":"code","c5efde98":"code","c7cfa9b1":"code","6536debf":"code","e6b0847b":"code","cd0cd1a6":"code","2393cdeb":"code","37abdb76":"code","1831544b":"code","9ec70acc":"code","f65af2c5":"code","b128fe7f":"code","39dafd45":"code","f4883b0d":"code","f673c4ea":"code","28ad11f2":"code","976e5e16":"code","6a23e713":"code","8270b786":"code","7ca11b49":"code","481972fd":"code","7f9b17a6":"code","1ad89b88":"code","60d0ec55":"code","12afe92f":"code","39b1d405":"code","e66eea31":"code","77d96e30":"code","8a32b3ab":"code","d621d091":"code","0706058c":"code","24ee8731":"code","682ff879":"code","11b1987c":"code","83421631":"code","72e37b9e":"code","18162d71":"code","c1b3a159":"code","f308ecdc":"code","80dee85d":"code","aeecce42":"code","706551c5":"code","4f3ac6c5":"code","c645802c":"code","f7d7dc60":"code","b4ee73b8":"code","fe61bcab":"code","30d67141":"code","ed16db1e":"code","715a7a84":"code","3966a70a":"code","60dd3ec4":"code","82744ba3":"code","f9b45085":"code","5c056d6e":"code","0fb0c114":"code","ffcaab5a":"code","7c85b376":"code","f983b6ad":"code","ca3a1a69":"code","fd7857c8":"code","b149c44f":"code","703642e8":"code","58e148fe":"code","b28cc9dc":"code","ba968a27":"code","8b6ea91e":"code","dd34c7e3":"code","e599daed":"code","9b12613c":"code","f2115f51":"code","d6e67f33":"code","759133dc":"code","7cc38eb3":"code","279469f5":"code","943a75e2":"code","d67ac423":"code","ea73e139":"code","a8dc2bbe":"code","7a8f0088":"code","e80b49d1":"code","85b7f8b5":"code","6673732c":"code","c8dded86":"code","96c66224":"code","7f6b2996":"code","f0f1528a":"code","2099024c":"code","cb3e73e8":"code","52ad43b4":"code","d114c270":"code","fcbeba2f":"code","141a2f0b":"code","34dd80b0":"code","5562fa62":"code","646ab98f":"code","c7e02766":"code","91adc9d7":"code","fbadb142":"code","b89f5a67":"code","918f6832":"code","119200db":"code","fa5af593":"code","13926052":"code","d295262b":"code","465b691d":"code","7f34ab74":"code","48505ece":"code","36e62479":"code","eed97576":"code","ab19beda":"code","3cc806d0":"code","872a1631":"code","de70eb71":"code","69006819":"code","1696ad4a":"code","5b62d635":"code","1454c1a3":"code","daa04a63":"code","02564061":"code","e685c93c":"code","d50c3158":"code","17fd16e9":"code","fc57caab":"code","7a2a5e73":"code","09d2c19f":"code","0d0827b3":"code","3ada4108":"code","6d81b5a2":"code","a0bc0b0e":"code","b4c0a1ad":"code","ce7df7bb":"code","055511cb":"code","0372181b":"code","f9ddd958":"code","4b467adb":"code","4b650565":"code","b5d0bd0b":"code","37d5950c":"code","0cf58e94":"code","6aa1a820":"code","c4f9b86d":"code","e3629486":"code","45b7bee5":"markdown","8a0be7c5":"markdown","9b2a62b7":"markdown","5f162dd9":"markdown","84bd6e2d":"markdown","bff015be":"markdown","e13c414d":"markdown","02c3a4e9":"markdown","565a9132":"markdown","c9a83b3d":"markdown","f34e9ad0":"markdown","453547fb":"markdown","730b7f75":"markdown","022d66f1":"markdown","a02c71e3":"markdown","38c2b980":"markdown","3bbbf8e2":"markdown","001b3dad":"markdown","1e9d92e0":"markdown","e5e914cd":"markdown","b5759ccd":"markdown","02d7041a":"markdown","712eaca7":"markdown","ca30acaa":"markdown","7218db4b":"markdown","ecc4d600":"markdown","2f1a96b0":"markdown","a989e051":"markdown","fd9eee09":"markdown","92f27393":"markdown","20c4181d":"markdown","987a2e04":"markdown","a9a4a765":"markdown","3bd51f11":"markdown","b7d650dc":"markdown","c0040549":"markdown","d7849ad3":"markdown","5ec1318e":"markdown","67c32ff4":"markdown","100f0f3c":"markdown","27969d1c":"markdown","9079dd9c":"markdown","f9d26afb":"markdown","1debf051":"markdown","a8787caf":"markdown","ed8bcf92":"markdown","dfc250fc":"markdown","e4a212bd":"markdown","b86e5e42":"markdown","93889ddc":"markdown","34afe487":"markdown","9b355106":"markdown","8b2cbce3":"markdown","8b9cfa58":"markdown","e4845190":"markdown","55fa8712":"markdown","34dcbaa9":"markdown","1d3070d1":"markdown","0066f8ee":"markdown","cfcc3fb3":"markdown","8cf331f9":"markdown","ca1c8cbc":"markdown","41cf8489":"markdown","320d479c":"markdown","32a87a47":"markdown","87931f33":"markdown","41c9d965":"markdown","78b3e638":"markdown","06b52c57":"markdown","450bc9f6":"markdown","c85ee63e":"markdown","3c9192f1":"markdown","67da2995":"markdown","6aca834b":"markdown","a15e945c":"markdown","effc23a0":"markdown","5907119f":"markdown","b68a0087":"markdown","611c0ab6":"markdown","fe5a00ad":"markdown","466b165d":"markdown","6b63cb12":"markdown","ad17a83b":"markdown","0f3ec9fe":"markdown","8ac1b35c":"markdown","9a655d19":"markdown","f423c16b":"markdown","a8397a8d":"markdown","af6395e9":"markdown","213f4948":"markdown","312e8c1c":"markdown","ccbd3934":"markdown","6861cac0":"markdown","909e5856":"markdown","01d6f688":"markdown","7237e4f7":"markdown","aae99658":"markdown","3cdef669":"markdown","dd48bd79":"markdown","0ba4014a":"markdown","c6077160":"markdown","84393b6c":"markdown","2c1b7bc6":"markdown","540779fb":"markdown","37e1dc28":"markdown","09654d1b":"markdown","fa996ac9":"markdown","1a414b80":"markdown","6930a05f":"markdown","e6f92793":"markdown","79ca79f0":"markdown","dd3d9e3a":"markdown","7255c45c":"markdown","a92b91b5":"markdown","8ba66959":"markdown","dd224f3f":"markdown"},"source":{"dfa6c30a":"# import libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\n\n# For visualising our data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imageio\nfrom wordcloud import WordCloud, STOPWORDS\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, LabelSet\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\n\n# to deal wrangle text data\nimport nltk\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom textblob import TextBlob\nimport string\nimport re\n\n# models used\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import TruncatedSVD\n","967ad551":"# import the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","edf33c41":"# Function to customise table display\ndef multi_table(table_list):\n    from IPython.core.display import HTML\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' +\n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )","f93b9032":"multi_table([train.head(), test.head()])","f38dc3a3":"##Lets examine how many representatives there are for each personality type in the dataset\nax = sns.countplot(y=train.type, order = train['type'].value_counts().index)","530d2e97":"list1 = []\nlist2 = []\nlist3 = []\nlist4 = []\n\nfor cat in list(train['type']):\n\n    \n    list1.append(cat[0])\n    list2.append(cat[1])\n    list3.append(cat[2])\n    list4.append(cat[3])\n\nfig, ax =plt.subplots(1,4,figsize=(20,10))\n\nsns.countplot(x=list1, ax=ax[0], palette='Set1')\nsns.countplot(x=list2, ax=ax[1], palette='Set2')\nsns.countplot(x=list3, ax=ax[2], palette='colorblind')\nsns.countplot(x=list4, ax=ax[3], palette='Set3')\nfig.show()","92b1b4ff":"##The test dataset lacks the type column, an artificial one will be created \ntest['type'] = 'test'\ntest_id = test['id']\ntest.drop(\"id\", axis=1, inplace=True)","344a5fcb":"mbti = pd.concat((train, test)).reset_index(drop=True)\nmbti.shape","dad5e1a0":"##Remove the separator\n##mbti['seperated_post'] = mbti['posts'].apply(lambda x: x.replace(\"|||\",''))","dc3cd448":"##Obtain the index for the train and test subsets so this concatenated document can be separated later\ntest_ind = mbti[mbti['type'] == 'test'].index.tolist()\ntrain_ind = mbti[mbti['type'] != 'test'].index.tolist()","0e619f1b":"##Vectorise the text\n##tfd = TfidfVectorizer()\n##X_tf = tfd.fit_transform(mbti['seperated_post'])","55e3390d":"## This function will allow for a rapid conversion of the 16 personality types into four binary columns.\n\ndef pers_conv(df):\n    df['mind'] = df['type'].apply(lambda x: x[0] == 'E').astype('int')\n    df['energy'] = df['type'].apply(lambda x: x[1] == 'N').astype('int')\n    df['nature'] = df['type'].apply(lambda x: x[2] == 'T').astype('int')\n    df['tactics'] = df['type'].apply(lambda x: x[3] == 'J').astype('int')\n    return df","09ac699d":"##Convert the single type column to four binary columns, using the pers_conv function defined above\n##y_train = mbti.iloc[train_ind][['type']]\n##y_train = pers_conv(y_train)\n##y_train.drop(\"type\", axis=1, inplace=True)","e34fb2f0":"##Separate the train and test data from each other\n##X_train = X_tf[train_ind,:]\n##X_test = X_tf[test_ind,:]","c5efde98":"##Initiate the random forest classifier\n##rfr = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features= 0.5, random_state=0, n_jobs=-2)","c7cfa9b1":"##Trained the model\n##rfr.fit(X_train, y_train)","6536debf":"##Obtained a multilabel prediction, renamed the  columns to match those used in the pers_conv function, added the ids \n##and saved to csv\n##rfr_pred = rfr.predict(X_test)\n##multiclass_rfr = pd.DataFrame(rfr_pred, columns=y_train.columns)\n##multiclass_rfr_id = pd.concat([test_id, multiclass_rfr], axis=1)\n##\n##multiclass_rfr_id.to_csv('Rfr_first_prediction.csv', index=False)","e6b0847b":"##We will keep this sorted dictionary full of feature importance for later after we experiment with approaches of obtaining \n##predictions for our model.\n##feat_imp = sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd.get_feature_names()), reverse=True)","cd0cd1a6":"##Rather than a multilabel prediction, each of the four letters of the personality are predicted individually and concatenated\n##to a four column dataframe.\n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    rfr.fit(X_train, y_train[t])\n##    temp_var = pd.DataFrame(rfr.predict(X_test), columns=[t])\n##    concat_pred = pd.concat([concat_pred,temp_var], axis=1)","2393cdeb":"##The predictions were concatenated to the test_ids and saved to a csv so a submission to Kaggle could be made.\n##comb_rfr_id = pd.concat([test_id, concat_pred], axis=1)\n##comb_rfr_id.to_csv('Rfr_first_prediction_separated.csv', index=False)","37abdb76":"##Initiate the random forest classifier with 100 estimators\n##rfr = RandomForestClassifier(n_estimators=100, min_samples_leaf=3, max_features= 0.5, random_state=0, n_jobs=-2)","1831544b":"##Rather than a multilabel prediction, each of the four letters of the personality are predicted individually and concatenated\n##to a four column dataframe.\n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    rfr.fit(X_train, y_train[t])\n##    temp_var = pd.DataFrame(rfr.predict(X_test), columns=[t])\n##    concat_pred = pd.concat([concat_pred,temp_var], axis=1)\n##    \n####The predictions were concatenated to the test_ids and saved to a csv so a submission to Kaggle could be made.\n##comb_rfr_id = pd.concat([test_id, concat_pred], axis=1)\n##comb_rfr_id.to_csv('RFR_pred_100_est.csv', index=False)","9ec70acc":"##Import the oversamplers from the imbalanced learn toolkit\n##from imblearn.over_sampling import RandomOverSampler\n##from imblearn.over_sampling import SMOTE","f65af2c5":"##Initiate the over-samplers\n##ros = RandomOverSampler(random_state=9000)\n##oversampler = SMOTE(random_state=0)","b128fe7f":"##Obtain the dependent variable\n##Y = train['type']","39dafd45":"##Performed random over-sampling\n##X_tfidf_resampled, Y_tfidf_resampled = ros.fit_sample(X_train, Y)","f4883b0d":"##Performed a SMOTE\n##os_X, os_y = oversampler.fit_sample(X_train,Y)","f673c4ea":"##The transformed dependent variable is transformed to a dataframe and the number of representatives per personality type is\n##investigated. The same was done for the SMOTE experiment\n##y_train = pd.DataFrame(Y_tfidf_resampled, columns=['type']) \n##y_train['type'].value_counts()","28ad11f2":"##Get the number of representatives for the minority class (30) and each of the personality types.\n##type_summary = pd.DataFrame(train['type'].value_counts())\n##smallest = type_summary.loc[type_summary['type'].idxmin()]['type']\n##pers_id = train['type'].unique().tolist()","976e5e16":"##This function can be used to create a smaller dataset where the number of representatives per personality profile are reduced\n##to that of the minorty class. The posts from each personality profile are shuffled randomly and a subset of 30 is selected.\n##def reduce_size(df, state = 42):\n##    b = pd.DataFrame()\n##    for per in pers_id:\n##        a = df[df['type'] == per]\n##        a = shuffle(a, random_state=state)\n##       a = a[:smallest]\n##        b = pd.concat([b,a], axis=0)\n##    return b","6a23e713":"##Obtain a reduced datase\n##small_set = reduce_size(train)","8270b786":"##Check that each personality profile has the same number of representatives\n##small_set['type'].value_counts()","7ca11b49":"##Making a new smaller dataset\n##mbti_s = pd.concat((small_set, test)).reset_index(drop=True)\n##mbti_s.shape","481972fd":"##feat_imp","7f9b17a6":"##Small_set will be converted to a longer format which we will refer to as small set long\n##small_set_long = []\n##for i, row in small_set.iterrows():\n##    for post in row['posts'].split('|||'):\n##        small_set_long.append([row['type'], post])\n##small_set_long = pd.DataFrame(small_set_long, columns=['type', 'posts'])\n##print(small_set_long.shape)","1ad89b88":"##Since we are tokenising the posts, we will encounter a problem when we perform TfidfVectorisation as the vectoriser has a \n##built-in tokeniser and pre-processor. One of the preprocessing steps is making text lowercase, we will do this so long.\n##small_set_long['posts'] = small_set_long['posts'].str.lower()","60d0ec55":"##This dataframe will contain all the scores of our troubleshooting efforts.\n##Testlogger = pd.DataFrame()","12afe92f":"##Lets set all these classifiers to default but allow them to run as quickly as possible with the exception of random forests\n##as we want to identifiy the most informative features.\n##NB = MultinomialNB()\n##ada = AdaBoostClassifier()\n##lorg = LogisticRegression(n_jobs=-1)\n##rfr = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features= 0.5, random_state=0, n_jobs=-2)","39b1d405":"def get_scores(df, logdf, name, x_col, y_col):\n    \n    ##TfidfVectorizer performs tokenisation and preprocessing automatically, we have already performed some preprocessing and\n    ##tokenisation. So this function is a work around to use the vectorizer \n    def dummy(doc):\n        return doc\n    ##We want to use the get_feature_names command for this vectoriser when investigating feature importance using random \n    ##forests classifier, but in it's current form, tfd_2 is a local variable and won't be accessible outside of the function.\n    ##This can be circumvented by declaring tfd_2 as a global variable.\n    ##Later, we will also be extracting the word vector, so X_tf_2 will also need to be a global variable.\n    global tfd_2\n    global X_tf_2\n    tfd_2 = TfidfVectorizer(lowercase=False, tokenizer=dummy, preprocessor=dummy)\n    X_tf_2 = tfd_2.fit_transform(df[x_col])\n    \n    y = df[['type']]\n    y = pers_conv(y)\n    y.drop(\"type\", axis=1, inplace=True)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_tf_2,y[y_col], shuffle=True, random_state=42, test_size=0.2)\n    \n    ##Naive Bayes\n    NB.fit(X_train,y_train)\n    nb_pred = NB.predict(X_test)\n\n    ##Adaboost\n    ada.fit(X_train,y_train)\n    ada_pred = ada.predict(X_test)\n\n    ##Logisitic regression\n    lorg.fit(X_train, y_train)\n    lorg_pred = lorg.predict(X_test)\n    \n    ##Random forests\n    rfr.fit(X_train, y_train)\n    rfr_pred = rfr.predict(X_test)\n\n    tempvar = pd.DataFrame([{'Description': name}])\n    tempvar['NaiveBayesAcc'] = NB.score(X_test,y_test)\n    tempvar['NBLogLoss'] = log_loss(y_test,nb_pred)\n    tempvar['AdaboostAcc'] = ada.score(X_test,y_test)\n    tempvar['AdaBLoss'] = log_loss(y_test,ada_pred)\n    tempvar['LogisticRegAcc'] = lorg.score(X_test,y_test)\n    tempvar['LogRegLogLoss'] = log_loss(y_test,lorg_pred)\n    tempvar['RandomForestsAcc'] = rfr.score(X_test,y_test)\n    tempvar['RandomForestsLogLoss'] = log_loss(y_test,rfr_pred)\n    \n    \n    logdf = pd.concat([logdf, tempvar], axis=0).reset_index(drop=True)    \n    \n    return logdf","e66eea31":"##Tokenised the posts. We will use the TreeBankWordTokenizer since it is quicker than the word_tokenise function.\n##tokeniser = TreebankWordTokenizer()\n##small_set_long['tokens'] = small_set_long['posts'].apply(tokeniser.tokenize)","77d96e30":"##pers_id_lower = list(map(str.lower, pers_id))\n##Added an s to the end of the personality type\n##plur_pers = [c + \"s\" for c in pers_id_lower]\n##Added a period to the end of the personality types\n##dot_pers = [c + \".\" for c in pers_id_lower]\n##Added a period to the end of the plural version of the personality types\n##dot_plur_pers = [c + \".\" for c in plur_pers]\n##pers_total = pers_id_lower + plur_pers + dot_pers + dot_plur_pers","8a32b3ab":"##This function loops through the tokens and only keeps those that are not in the pers_total list\ndef remove_personality(tokens):    \n    return [t for t in tokens if t not in pers_total]","d621d091":"##The personality types were removed from the dataset\n##small_set_long['no_pers'] = small_set_long['tokens'].apply(remove_personality)","0706058c":"##Investigated the effect of removing personality types from the text data by running the logger.\n##Testlogger = get_scores(small_set_long, Testlogger, 'tokenised posts','tokens','mind')\n##Testlogger = get_scores(small_set_long, Testlogger, 'personality type removed','no_pers','mind')\n##Testlogger","24ee8731":"##sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)","682ff879":"def remove_punctuation(post):\n    return ''.join([l for l in post if l not in string.punctuation])","11b1987c":"#small_set_long['post_rp'] = small_set_long['posts'].apply(remove_punctuation)\n#small_set_long['tokens_rp'] = small_set_long['post_rp'].apply(tokeniser.tokenize)\n##small_set_long['no_pers_rp'] = small_set_long['tokens_rp'].apply(remove_personality)","83421631":"##Testlogger = get_scores(small_set_long, Testlogger, 'punctuation removed','tokens_rp','mind')\n##Testlogger = get_scores(small_set_long, Testlogger, 'personality type and punct removed','no_pers_rp','mind')\n##Testlogger","72e37b9e":"##sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)","18162d71":"##Define the stemmer\n##stemmer = SnowballStemmer('english')","c1b3a159":"##This function loops through all the words in the list and applies the stemmer.\ndef mbti_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words]","f308ecdc":"##Apply the stemmer function to the data with punctuation removed.\n##small_set_long['stem'] = small_set_long['no_pers_rp'].apply(mbti_stemmer, args=(stemmer, ))","80dee85d":"##Define the lemmatizer. \n##lemmatizer = WordNetLemmatizer()","aeecce42":"##This function loops through all the words in the list and applies the lemmatiser.\ndef mbti_lemma(words, lemmatizer):\n    return [lemmatizer.lemmatize(word) for word in words] ","706551c5":"##Apply the lemmatiser function to the data with punctuation removed.\n##small_set_long['lemma'] = small_set_long['no_pers_rp'].apply(mbti_lemma, args=(lemmatizer, ))","4f3ac6c5":"##Testlogger = get_scores(small_set_long, Testlogger, 'stemmatized','stem','mind')\n##Testlogger = get_scores(small_set_long, Testlogger, 'lemmatized','lemma','mind')\n##Testlogger","c645802c":"##stop_words = stopwords.words('english')","f7d7dc60":"def remove_stop_words(tokens):    \n    return [t for t in tokens if t not in stopwords.words('english')]","b4ee73b8":"#small_set_long['no_stop_stem'] = small_set_long['stem'].apply(remove_stop_words)\n##small_set_long['no_stop_lemma'] = small_set_long['lemma'].apply(remove_stop_words)","fe61bcab":"##Testlogger = get_scores(small_set_long, Testlogger, 'stemmatized_stoprem','no_stop_stem','mind')\n##Testlogger = get_scores(small_set_long, Testlogger, 'lemmatized_stoprem','no_stop_lemma','mind')\n##Testlogger","30d67141":"##sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)##","ed16db1e":"def post_stats(data):\n    # create column with number of words \n    data['num_words'] = data['posts'].apply(lambda x:len(x.split()))\n    \n     # create column with number of different images   \n    data['num_jpg_images'] = data['posts'].apply(lambda x:x.count('jpg'))\n    data['num_png_images'] = data['posts'].apply(lambda x:x.count('png'))\n    data['num_gif_images'] = data['posts'].apply(lambda x:x.count('gif'))\n    \n    # create a column with number of urls\n    data['num_links'] = data['posts'].apply(lambda x:x.count('http'))\n\n    # create a column with number of question and exclamation marks\n    data['num_qmarks'] = data['posts'].apply(lambda x:x.count('?'))\n    data['num_excls'] = data['posts'].apply(lambda x:x.count('!'))\n    \n    # create a column with number of periods and commas\n    data['num_period'] = data['posts'].apply(lambda x:x.count('.'))\n    data['num_comma'] = data['posts'].apply(lambda x:x.count(','))\n    \n    ##Difficult to count emojis, but the colon represent the eyes\n    data['num_colon'] = data['posts'].apply(lambda x:x.count(':')) \n    \n    # create a column with number of hashtags used \n    data['num_hashtags'] = data['posts'].apply(lambda x: x.count('#'))\n    return data","715a7a84":"##mbti_with_stats_long = post_stats(small_set_long)\n##post_stat_cols = ['num_words',\n                  'num_jpg_images', \n                  'num_png_images', \n                  'num_gif_images', \n                  'num_links', \n                  'num_qmarks', \n                  'num_excls', \n                  'num_period', \n                  'num_comma', \n                  'num_colon', \n                  'num_hashtags']\n##mbti_with_stats_long[post_stat_cols].head()","3966a70a":"##Add these engineered features.\n##mbti_with_stats_wide = post_stats(small_set)","60dd3ec4":"##feat_eng_log = pd.DataFrame()","82744ba3":"def get_scores_feat_eng(df, logdf, name, x_col, y_col):\n    \n    \n    y = df[['type']]\n    y = pers_conv(y)\n    y.drop(\"type\", axis=1, inplace=True)\n    \n    X_train, X_test, y_train, y_test = train_test_split(df[x_col],y[y_col], shuffle=True, random_state=42, test_size=0.2)\n    \n    ##Naive Bayes\n    NB.fit(X_train,y_train)\n    nb_pred = NB.predict(X_test)\n\n    ##Adaboost\n    ada.fit(X_train,y_train)\n    ada_pred = ada.predict(X_test)\n\n    ##Logisitic regression\n    lorg.fit(X_train, y_train)\n    lorg_pred = lorg.predict(X_test)\n    \n    ##Random forests\n    rfr.fit(X_train, y_train)\n    rfr_pred = rfr.predict(X_test)\n\n    tempvar = pd.DataFrame([{'Description': name}])\n    tempvar['NaiveBayesAcc'] = NB.score(X_test,y_test)\n    tempvar['NBLogLoss'] = log_loss(y_test,nb_pred)\n    tempvar['AdaboostAcc'] = ada.score(X_test,y_test)\n    tempvar['AdaBLoss'] = log_loss(y_test,ada_pred)\n    tempvar['LogisticRegAcc'] = lorg.score(X_test,y_test)\n    tempvar['LogRegLogLoss'] = log_loss(y_test,lorg_pred)\n    tempvar['RandomForestsAcc'] = rfr.score(X_test,y_test)\n    tempvar['RandomForestsLogLoss'] = log_loss(y_test,rfr_pred)\n    \n    \n    logdf = pd.concat([logdf, tempvar], axis=0).reset_index(drop=True)    \n    \n    return logdf","f9b45085":"##feat_eng_log = get_scores_feat_eng(mbti_with_stats_long, feat_eng_log, ##'Posts long format_counts', post_stat_cols, 'mind')\n##feat_eng_log = get_scores_feat_eng(mbti_with_stats_wide, feat_eng_log, 'Posts wide format_counts', post_stat_cols, 'mind')\n##feat_eng_log","5c056d6e":"##sorted(zip(map(lambda x: x, rfr.feature_importances_), post_stat_cols), reverse=True)","0fb0c114":"# Display the lengths of the posts per personality type\n##plt.figure(figsize=(15,10))\n##small_set[\"text_size\"] = small_set[\"posts\"].apply(len)\n##sns.swarmplot(\"type\", \"text_size\", data=small_set)","ffcaab5a":"# Fraction of unique words per post\ndef unique_word_fraction(row):\n    \"\"\"function to calculate the fraction of unique words on total words of the posts\"\"\"\n    post = row['posts']\n    post_split = post.split(' ')\n    post_split = [''.join(c for c in s if c not in string.punctuation) for s in post_split]\n    post_split = [s for s in post_split if s]\n    word_count = post_split.__len__()\n    unique_count = list(set(post_split)).__len__()\n    return (unique_count \/ word_count)\n\n# stop words count\neng_stopwords = set(stopwords.words(\"english\"))\ndef stopwords_count(row):\n    \"\"\" Number of stopwords fraction in a posts\"\"\"\n    text = row['posts'].lower()\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    stopwords_count = len([w for w in text_splited if w in eng_stopwords])\n    return (stopwords_count\/word_count)\n\n# Fraction of punctuations per post\ndef punctuations_fraction(row):\n    \"\"\"functiopn to claculate the fraction of punctuations over total number of characters for a given posts \"\"\"\n    text = row['posts']\n    char_count = len(text)\n    punctuation_count = len([c for c in text if c in string.punctuation])\n    return (punctuation_count\/char_count)\n\n# Character counts per post\ndef char_count(row):\n    \"\"\"function to return number of chracters \"\"\"\n    return len(row['posts'])\n\n# Fraction of Nouns per post\ndef fraction_noun(row):\n    \"\"\"function to give us fraction of noun over total words \"\"\"\n    text = row['posts']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = pos_tag(text_splited)\n    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n    return (noun_count\/word_count)\n\n# Fraction of Adjectives per post\ndef fraction_adj(row):\n    \"\"\"function to give us fraction of adjectives over total words in given posts\"\"\"\n    text = row['posts']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = pos_tag(text_splited)\n    adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n    return (adj_count\/word_count)\n\n# Fraction of Verbs per post\ndef fraction_verbs(row):\n    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n    text = row['posts']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = pos_tag(text_splited)\n    verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n    return (verbs_count\/word_count)","7c85b376":"##Create new columns in the long format dataset by applying the above functions\n#mbti_with_stats_long['unique_word_fraction'] = mbti_with_stats_long.apply(lambda row: unique_word_fraction(row), axis=1)\n#mbti_with_stats_long['stopwords_count'] = mbti_with_stats_long.apply(lambda row: stopwords_count(row), axis=1)\n#mbti_with_stats_long['punctuations_fraction'] = mbti_with_stats_long.apply(lambda row: punctuations_fraction(row), axis=1)\n#mbti_with_stats_long['char_count'] = mbti_with_stats_long.apply(lambda row: char_count(row), axis=1)\n#mbti_with_stats_long['fraction_noun'] = mbti_with_stats_long.apply(lambda row: fraction_noun(row), axis=1)\n#mbti_with_stats_long['fraction_adj'] = mbti_with_stats_long.apply(lambda row: fraction_adj(row), axis=1)\n#mbti_with_stats_long['fraction_verbs'] = mbti_with_stats_long.apply(lambda row: fraction_verbs(row), axis=1)\n\n\n##Create new columns in the wide format dataset by applying the above functions\n##mbti_with_stats_wide['unique_word_fraction'] = mbti_with_stats_wide.apply(lambda row: unique_word_fraction(row), axis=1)\n##mbti_with_stats_wide['stopwords_count'] = mbti_with_stats_wide.apply(lambda row: stopwords_count(row), axis=1)\n####mbti_with_stats_wide['punctuations_fraction'] = mbti_with_stats_wide.apply(lambda row: punctuations_fraction(row), axis=1)\n##mbti_with_stats_wide['char_count'] = mbti_with_stats_wide.apply(lambda row: char_count(row), axis=1)\n##mbti_with_stats_wide['fraction_noun'] = mbti_with_stats_wide.apply(lambda row: fraction_noun(row), axis=1)\n##mbti_with_stats_wide['fraction_adj'] = mbti_with_stats_wide.apply(lambda row: fraction_adj(row), axis=1)\n##mbti_with_stats_wide['fraction_verbs'] = mbti_with_stats_wide.apply(lambda row: fraction_verbs(row), axis=1)\n\n##Create a list of all the new columns to allow them to be subsetted\n##POS_cols = ['unique_word_fraction', \n##            'stopwords_count', \n##            'punctuations_fraction', \n##            'char_count',\n##            'fraction_noun', \n##            'fraction_adj', \n##            'fraction_verbs']","f983b6ad":"##feat_eng_log = get_scores_feat_eng(mbti_with_stats_wide, feat_eng_log, 'Posts wide format POS', POS_cols, 'mind')","ca3a1a69":"##feat_eng_log","fd7857c8":"##sorted(zip(map(lambda x: x, rfr.feature_importances_), POS_cols), reverse=True)","b149c44f":"def ranked_sentiments(data):   \n    from IPython.display import display_html\n    \n    # retrieve the sentiments from all the posts in the dataset\n    data[['polarity', 'subjectivity']] = data['posts'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n\n    # Ranking positive sentiment according to the polarity\n    pos_senti_df = data.sort_values('polarity', ascending=False).drop_duplicates(['type'])\n    pos_senti_df = pos_senti_df[['type', 'polarity']].groupby(by='type').mean()\n    pos_senti_df = pos_senti_df.sort_values(by='polarity', ascending=False)\n    pos_senti_df = pos_senti_df.reset_index()\n    # Ranking negative sentiments according to the polarity\n    neg_senti_df = data.sort_values('polarity', ascending=True).drop_duplicates(['type'])\n    neg_senti_df = neg_senti_df[['type', 'polarity']].groupby(by='type').mean()\n    neg_senti_df = neg_senti_df.sort_values(by='polarity')\n    neg_senti_df = neg_senti_df.reset_index()\n    \n    # prepare the display side by side\n    df1_styler = pos_senti_df.style.set_table_attributes(\"style='display:inline'\").set_caption('Ranked Positive Sentiments')\n    df2_styler = neg_senti_df.style.set_table_attributes(\"style='display:inline'\").set_caption('Ranked Negative Sentiments')\n    \n    return display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)\n    \n##ranked_sentiments(small_set)","703642e8":"##small_set_long[['polarity', 'subjectivity']] = small_set_long['posts'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))","58e148fe":"##plotting the average polarity per post for each personality type.\n##plt.figure(figsize=(15,10))\n##sns.barplot(data=small_set_long, x='type', y=\"polarity\")\n##x=plt.xticks(rotation=90)","b28cc9dc":"##Plotting the average subjectivity per post\n##plt.figure(figsize=(15,10))\n##sns.barplot(data=small_set_long, x='type', y=\"subjectivity\")\n##x=plt.xticks(rotation=90)","ba968a27":"##Testlogger = get_scores(small_set_long,  Testlogger, 'Mind insight stop included','lemma','mind')","8b6ea91e":"##Extract the feature importance from random forest classifier, but display only the features with a gini >= 0.005\n##feat_imp_mind_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##feat_imp_mind_stop was too long to write out so shortened to fims\n##fims_subset = feat_imp_mind_stop[feat_imp_mind_stop[0] >= 0.005]\n##fims_subset","dd34c7e3":"##Convert the word vector to a datatframe\n##inside_vect_mind = pd.DataFrame(X_tf_2.A, columns=tfd_2.get_feature_names())","e599daed":"##Testlogger = get_scores(small_set_long, Testlogger, 'Mind insight no stop','no_stop_lemma','mind')","9b12613c":"##Extract the feature importance from random forest classifier, but display only the feature with a gini >= 0.005\n##feat_imp_mind_no_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##fimns_subset = feat_imp_mind_no_stop[feat_imp_mind_no_stop[0] >= 0.005]\n##fimns_subset\n","f2115f51":"##df = pers_conv(small_set_long)\n##fig, ax = plt.subplots(len(df['mind'].unique()), sharex=True, figsize=(15,10*len(df['mind'].unique())))\n\n##k = 0\n##for i in df['mind'].unique():\n##    df_4 = df[df['mind'] == i]\n##    wordcloud = WordCloud().generate(df_4['lemma'].to_string())\n##    ax[k].imshow(wordcloud)\n##    ax[k].set_title(i)\n##    ax[k].axis(\"off\")\n##    k+=1","d6e67f33":"##mind_pred_words = fims_subset[1].tolist() + fimns_subset[1].tolist()\n##mind_pred_words_unique = list(set(mind_pred_words))\n##len(mind_pred_words_unique)","759133dc":"##vector_subset = inside_vect_mind[mind_pred_words_unique]\n##vector_subset_type_mind = pd.concat([df[['mind']],vector_subset], axis=1)\n##vector_subset_type_mind.groupby(['mind']).mean()","7cc38eb3":"##rfr = RandomForestClassifier(n_estimators=40, max_depth=4, max_features=0.5, random_state=0, n_jobs=-2)","279469f5":"##X_train, X_test, y_train, y_test = train_test_split(X_tf_2,df['mind'], shuffle=True, random_state=42, test_size=0.2)","943a75e2":"##rfr.fit(X_train, y_train)","d67ac423":"##pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)).head(10)","ea73e139":"#Draw the decision tree at estimator 0\n##dot_data = StringIO()\n##estimator = rfr.estimators_[0]\n##export_graphviz(estimator, out_file='tree_limited.dot',feature_names=tfd_2.get_feature_names(),\n##                class_names=np.asarray(['introvert', 'extrovert']),\n##                filled=True, rounded=True,\n##                special_characters=True)\n##!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600 \n##Image(filename='tree_limited.png')","a8dc2bbe":"#Reset the random forests classifier\n##rfr = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, random_state=0, n_jobs=-2)","7a8f0088":"#Get the predictions for the energy subcategory\n##Testlogger = get_scores(small_set_long, Testlogger, 'Energy insight stop included','lemma','energy')","e80b49d1":"##Extract the feature importance from random forest classifier, but display only the features with a gini >= 0.005\n##feat_imp_energy_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##feat_imp_energy_stop was too long to write out so shortened to fies\n##fies_subset = feat_imp_energy_stop[feat_imp_energy_stop[0] >= 0.005]\n##fies_subset","85b7f8b5":"##Convert the word vector to a datatframe\n##inside_vect_energy = pd.DataFrame(X_tf_2.A, columns=tfd_2.get_feature_names())","6673732c":"##run the model on the posts without stop words\n##Testlogger = get_scores(small_set_long,  Testlogger, 'Energy insight no stop','no_stop_lemma','energy')","c8dded86":"##Extract the feature importance from random forest classifier, but display only the feature with a gini >= 0.005\n##feat_imp_energy_no_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##fiens_subset = feat_imp_energy_no_stop[feat_imp_energy_no_stop[0] >= 0.005]\n##fiens_subset","96c66224":"#df = pers_conv(small_set_long)\n##fig, ax = plt.subplots(len(df['energy'].unique()), sharex=True, figsize=(15,10*len(df['energy'].unique())))\n\n##k = 0\n##for i in df['energy'].unique():\n##    df_4 = df[df['energy'] == i]\n##    wordcloud = WordCloud().generate(df_4['lemma'].to_string())\n##    ax[k].imshow(wordcloud)\n##    ax[k].set_title(i)\n##    ax[k].axis(\"off\")\n##    k+=1","7f6b2996":"##energy_pred_words = fies_subset[1].tolist() + fiens_subset[1].tolist()\n##energy_pred_words_unique = list(set(energy_pred_words))\n##len(energy_pred_words_unique)","f0f1528a":"##vector_subset_energy = inside_vect_energy[energy_pred_words_unique]\n##vector_subset_type_energy = pd.concat([df[['energy']],vector_subset_energy], axis=1vector_subset_type_energy.groupby(['energy']).mean()","2099024c":"##rfr = RandomForestClassifier(n_estimators=40, max_depth=4, max_features=0.5, random_state=0, n_jobs=-2)","cb3e73e8":"##X_train, X_test, y_train, y_test = train_test_split(X_tf_2,df['energy'], shuffle=True, random_state=42, test_size=0.2)","52ad43b4":"##rfr.fit(X_train, y_train)","d114c270":"##pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)).head(10)","fcbeba2f":"#Draw the decision tree at estimator 0\n##dot_data = StringIO()\n##estimator = rfr.estimators_[0]\n##export_graphviz(estimator, out_file='tree_limited.dot',feature_names=tfd_2.get_feature_names(),\n##                class_names=np.asarray(['sensing', 'intuitive']),\n##                filled=True, rounded=True,\n##                special_characters=True)\n##!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600 \n##Image(filename='tree_limited.png')","141a2f0b":"#Reset the random forests classifier\n##rfr = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, random_state=0, n_jobs=-2)","34dd80b0":"#Get the predictions for the nature subcategory\n##Testlogger = get_scores(small_set_long, Testlogger, 'Nature insight stop included','lemma','nature')","5562fa62":"##Extract the feature importance from random forest classifier, but display only the features with a gini >= 0.005\n##feat_imp_nature_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##feat_imp_nature_stop was too long to write out so shortened to fins\n##fins_subset = feat_imp_nature_stop[feat_imp_nature_stop[0] >= 0.005]\n##fins_subset","646ab98f":"##Convert the word vector to a datatframe\n##inside_vect_nature = pd.DataFrame(X_tf_2.A, columns=tfd_2.get_feature_names())","c7e02766":"##Testlogger = get_scores(small_set_long, Testlogger, 'Nature insight no stop','no_stop_lemma','nature')","91adc9d7":"##Extract the feature importance from random forest classifier, but display only the feature with a gini >= 0.005\n##feat_imp_nature_no_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##finns_subset = feat_imp_nature_no_stop[feat_imp_nature_no_stop[0] >= 0.005]\n##finns_subset","fbadb142":"##fig, ax = plt.subplots(len(df['nature'].unique()), sharex=True, figsize=(15,10*len(df['nature'].unique())))\n\n##k = 0\n##for i in df['nature'].unique():\n##    df_4 = df[df['nature'] == i]\n##    wordcloud = WordCloud().generate(df_4['lemma'].to_string())\n##    ax[k].imshow(wordcloud)\n##    ax[k].set_title(i)\n##    ax[k].axis(\"off\")\n##    k+=1","b89f5a67":"##nature_pred_words = fins_subset[1].tolist() + finns_subset[1].tolist()\n##nature_pred_words_unique =  list(set(nature_pred_words))\n##len(nature_pred_words_unique)","918f6832":"##vector_subset_nature = inside_vect_nature[nature_pred_words_unique]\n##vector_subset_type_nature = pd.concat([df[['nature']],vector_subset_nature], axis= 1)\n##vector_subset_type_nature.groupby(['nature']).mean()","119200db":"##rfr = RandomForestClassifier(n_estimators=40, max_depth=4, max_features=0.5, random_state=0, n_jobs=-2)","fa5af593":"##X_train, X_test, y_train, y_test = train_test_split(X_tf_2,df['nature'], shuffle=True, random_state=42, test_size=0.2)","13926052":"##rfr.fit(X_train, y_train)","d295262b":"##pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)).head(10)","465b691d":"#Draw the decision tree at estimator 0\n##dot_data = StringIO()\n##estimator = rfr.estimators_[0]\n##export_graphviz(estimator, out_file='tree_limited.dot',feature_names = tfd_2.get_feature_names(),\n##                class_names = np.asarray(['feeling', 'thinking']),\n##                filled=True, rounded=True,\n##                special_characters=True)\n##!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600 \n##Image(filename='tree_limited.png')","7f34ab74":"#Reset the random forests classifier\n##rfr = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, random_state=0, n_jobs=-2)","48505ece":"#Get the predictions for the tactics subcategory\n##Testlogger = get_scores(small_set_long, Testlogger, 'Tactics insight stop included','lemma','tactics')","36e62479":"##Extract the feature importance from random forest classifier, but display only the features with a gini >= 0.005\n##feat_imp_tactics_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##feat_imp_tactics_stop was too long to write out so shortened to fies\n##fits_subset = feat_imp_tactics_stop[feat_imp_tactics_stop[0] >= 0.005]\n##fits_subset","eed97576":"##Convert the word vector to a datatframe\n##inside_vect_tactics = pd.DataFrame(X_tf_2.A, columns=tfd_2.get_feature_names())","ab19beda":"##Testlogger = get_scores(small_set_long, Testlogger, 'Tactics insight no stop','no_stop_lemma','tactics')","3cc806d0":"##Extract the feature importance from random forest classifier, but display only the feature with a gini >= 0.005\n##feat_imp_tactics_no_stop = pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True))\n##fitns_subset = feat_imp_tactics_no_stop[feat_imp_tactics_no_stop[0] >= 0.005]\n##fitns_subset","872a1631":"##fig, ax = plt.subplots(len(df['tactics'].unique()), sharex=True, figsize=(15,10*len(df['tactics'].unique())))\n\n##k = 0\n##for i in df['tactics'].unique():\n##    df_4 = df[df['tactics'] == i]\n##    wordcloud = WordCloud().generate(df_4['lemma'].to_string())\n##    ax[k].imshow(wordcloud)\n##    ax[k].set_title(i)\n##    ax[k].axis(\"off\")\n##    k+=1","de70eb71":"##tactics_pred_words = fits_subset[1].tolist() + fitns_subset[1].tolist()\n##tactics_pred_words_unique =  list(set(tactics_pred_words))\n##len(tactics_pred_words_unique)","69006819":"##vector_subset_tactics = inside_vect_tactics[tactics_pred_words_unique]\n##vector_subset_type_tactics = pd.concat([df[['tactics']],vector_subset_tactics], axis=1)\n##vector_subset_type_tactics.groupby(['tactics']).mean()","1696ad4a":"##rfr = RandomForestClassifier(n_estimators=40, max_depth=4, max_features=0.5, random_state=0, n_jobs=-2)","5b62d635":"##X_train, X_test, y_train, y_test = train_test_split(X_tf_2,df['tactics'], shuffle=True, random_state=42, test_size=0.2)","1454c1a3":"##rfr.fit(X_train, y_train)","daa04a63":"##pd.DataFrame(sorted(zip(map(lambda x: x, rfr.feature_importances_), tfd_2.get_feature_names()), reverse=True)).head(10)","02564061":"#Draw the decision tree at estimator 0\n##dot_data = StringIO()\n##estimator = rfr.estimators_[0]\n##export_graphviz(estimator, out_file='tree_limited.dot',feature_names=tfd_2.get_feature_names(),\n##                class_names = np.asarray(['feeling', 'thinking']),\n##                filled=True, rounded=True,\n##                special_characters=True)\n##!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600 \n##Image(filename = 'tree_limited.png')","e685c93c":"##Predicting_pers_words = list(set(mind_pred_words_unique + energy_pred_words_unique + nature_pred_words_unique + tactics_pred_words_unique))##\n##len(Predicting_pers_words)","d50c3158":"def dummy(doc):\n        return doc\n##tfd_3 = TfidfVectorizer(lowercase=False, tokenizer=dummy, preprocessor=dummy, max_features=50)\n##X_tf_3 = tfd_3.fit_transform(df['no_stop_lemma'])","17fd16e9":"##svd_model = TruncatedSVD(n_components=100, algorithm='randomized', n_iter=100, random_state=122)","fc57caab":"##No_stop_lemma = svd_model.fit_transform(X_tf_3.T)","7a2a5e73":"##Initialise the bokeh \n##output_notebook()","09d2c19f":"##df_svd = pd.DataFrame(columns=['x', 'y', 'word'])\n##df_svd['x'], df_svd['y'], df_svd['word'] = No_stop_lemma[:,0], No_stop_lemma[:,1], tfd_3.get_feature_names()\n\n##Makes an interactive scatter plot with text labels for each point\n##Due to the interactivity of the plot, namely the ability to zoom using either selecting over an area or using the scrolling \n##fun, bokeh was chosen over matplotlib.\n\n##source = ColumnDataSource(ColumnDataSource.from_df(df_svd))\n##labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n##                  text_font_size=\"8pt\", text_color=\"#555555\",\n##                  source=source, text_align='center')\n \n##plot = figure(plot_width=600, plot_height=600)\n##plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n##plot.add_layout(labels)\n##show(plot, notebook_handle=True)","0d0827b3":"##terms = tfd_3.get_feature_names()\n\n##for i, comp in enumerate(svd_model.components_):\n##    terms_comp = zip(terms, comp)\n##    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:7]\n##    print(\"Topic \"+str(i)+\": \")\n##    for t in sorted_terms:\n##        print(t[0])\n ##       print(\" \")","3ada4108":"##tfd_4 = TfidfVectorizer(lowercase = False, tokenizer=dummy, preprocessor=dummy, max_features=50)\n##X_tf_4 = tfd_4.fit_transform(df['lemma'])","6d81b5a2":"##Stop_lemma = svd_model.fit_transform(X_tf_4.T)##","a0bc0b0e":"##df_svd_2 = pd.DataFrame(columns=['x', 'y', 'word'])\n##df_svd_2['x'], df_svd_2['y'], df_svd_2['word'] = Stop_lemma[:,0], Stop_lemma[:,1], tfd_4.get_feature_names()\n\n##Makes an interactive scatter plot with text labels for each point\n##Due to the interactivity of the plot, namely the ability to zoom using either selecting over an area or using the scrolling wheel on the mouse \n##, bokeh was chosen over matplotlib.\n\n##source = ColumnDataSource(ColumnDataSource.from_df(df_svd_2))\n##labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n##                  text_font_size=\"8pt\", text_color=\"#555555\",\n##                  source=source, text_align='center')\n \n##plot = figure(plot_width=600, plot_height=600)\n##plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n##plot.add_layout(labels)\n##show(plot, notebook_handle=True)","b4c0a1ad":"##terms_stop = tfd_4.get_feature_names()\n\n##for i, comp in enumerate(svd_model.components_):\n##    terms_comp = zip(terms_stop, comp)\n##    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:7]\n##    print(\"Topic \"+str(i)+\": \")\n##    for t in sorted_terms:\n ##       print(t[0])\n##        print(\" \")","ce7df7bb":"##mbti['sep_post_rp'] = mbti['posts'].apply(remove_punctuation)\n##mbti['tokens_rp'] = mbti['sep_post_rp'].apply(tokeniser.tokenize)\n##mbti['no_pers_rp'] = mbti['tokens_rp'].apply(remove_personality)\n##mbti['lemma'] = mbti['no_pers_rp'].apply(mbti_lemma, args=(lemmatizer, ))","055511cb":"##tfd_5 = TfidfVectorizer(lowercase=False, tokenizer=dummy, preprocessor=dummy)\n##X_tf_5 = tfd_5.fit_transform(mbti['lemma'])\n##Full_vector = pd.DataFrame(X_tf_5.A, columns=tfd_5.get_feature_names())\n##Full_subset = Full_vector[Predicting_pers_words]\n##Full_subset.shape","0372181b":"##X_train = Full_subset[train_ind,:]\n##X_test = Full_subset[test_ind,:]","f9ddd958":"## Reset all algorithms\n##NB = MultinomialNB()\n##ada = AdaBoostClassifier()\n##lorg = LogisticRegression(n_jobs=-1)\n##rfr = RandomForestClassifier(n_estimators=40, n_jobs=-1)","4b467adb":"##param_grid_NB = dict(alpha=[1,0.1,0.01,0.001])\n##param_grid_AB = dict(n_estimators=[[50,100,200]], learning_rate=[.001,0.01,.1])\n##param_grid_logreg = dict(C=[10,1,0.1,0.01,0.001], penalty=[\"l1\",\"l2\"])\n##param_grid_rfr = dict(min_samples_leaf=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], max_features=[0.1,0.3, 0.5,0.7,0.9,\"log2\",\"sqrt\"])","4b650565":"\n##def get_best_params(iterations = 30):\n##    pg = {\"NB\": param_grid_NB, \"adaboost\": param_grid_AB, \"logreg\": param_grid_logreg, \"rfr\" : param_grid_rfr}\n##    md = {\"NB\" : NB, \"adaboost\": ada, \"logreg\": lorg, \"rfr\" : rfr}\n##    b = pd.DataFrame()\n    #This loops through the dictionary that contains the three regressors.\n##    for key,val in pg.items():\n##        random = RandomizedSearchCV(estimator=md[key], param_distributions=val, cv=5, n_iter=iterations, random_state=2)\n##        random_result = random.fit(X_train, y_train)\n##        a = pd.DataFrame([{'model': key, 'BestScore' : random_result.best_score_, 'BestParameters' : random_result.best_params_}])\n##        b = b.append(a)\n##    return b","b5d0bd0b":"#get_best_params(100)","37d5950c":"##Tuned hyper paramters\n##NB = MultinomialNB(alpha=0.1)\n##ada = AdaBoostClassifier(n_estimators=200, learning_rate=0.01)\n##lorg = LogisticRegression(C=0.001, penalty='l1', n_jobs=-1)\n##rfr = RandomForestClassifier(min_samples_leaf=3, max_features=0.5, n_estimators=40,n_jobs=-1)","0cf58e94":"## Run a prediction using a tuned Naive Bayes classifier on our full dataset with only informative features.\n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    NB.fit(X_train, y_train[t])\n##    temp_var = pd.DataFrame(NB.predict(X_test), columns=[t])\n##    concat_pred = pd.concat([concat_pred,temp_var], axis=1)\n\n##comb_NB_id = pd.concat([test_id, concat_pred], axis=1)\n#comb_NB_id.to_csv('Naive_bayes_full.csv', index=False)","6aa1a820":"## Run a prediction using a tuned Adaboost classifier on our full dataset with only informative features.\n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    ada.fit(X_train, y_train[t])\n##    temp_var = pd.DataFrame(ada.predict(X_test), columns=[t])\n##    concat_pred = pd.concat([concat_pred,temp_var], axis=1)\n\n##comb_ada_id = pd.concat([test_id, concat_pred], axis=1)\n#comb_ada_id.to_csv('Adaboost_full.csv', index=False)","c4f9b86d":"## Run a prediction using a tuned Logistic regression classifier on our full dataset with only informative features. \n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    lorg.fit(X_train, y_train[t])\n ##   temp_var = pd.DataFrame(lorg.predict(X_test), columns=[t])\n ##   concat_pred = pd.concat([concat_pred,temp_var], axis=1)\n\n##comb_lorg_id = pd.concat([test_id, concat_pred], axis=1)\n#comb_lorg_id.to_csv('lorg_full.csv', index=False)","e3629486":"## Run a prediction using a tuned Random forest classifier on our full dataset with only informative features.\n##types = y_train.columns.tolist()\n##concat_pred = pd.DataFrame()\n##for t in types:\n##    rfr.fit(X_train, y_train[t])\n##    temp_var = pd.DataFrame(rfr.predict(X_test), columns=[t])\n##    concat_pred = pd.concat([concat_pred,temp_var], axis=1)\n##\n##comb_rfr_id = pd.concat([test_id, concat_pred], axis=1)\n#comb_rfr_id.to_csv('rfr_full.csv', index=False)","45b7bee5":"Adaboost: Number of estimators has to do with how many trees are created by the algorithm. The learning rate indicates how each tree contributes to the overall results.\nhttps:\/\/educationalresearchtechniques.com\/2019\/01\/02\/adaboost-classification-in-python\/","8a0be7c5":"From this point on, we performed steps 1 to 8 from the above baseline protocol were performed, we just replaced the respective variables with the ones used in this section.","9b2a62b7":"# Explore the data\n<a id=\"eda\"><\/a>","5f162dd9":"# Oversampling\n<a id='oversampling'><\/a>","84bd6e2d":"Need to generate new tokens and new columns to examine the effect of removing punctuation.","bff015be":"# Conclusions\n<a id='conclusion'><\/a>","e13c414d":"-Best prediction score was obtained by just removing separators from the posts, performing tf-idf vectorization, and running random forest classifier with 100 estimators, min_samples_leaf of 3 and max_features of 0.5. This gave us a score of 5.50437","02c3a4e9":"### Explore some of the training data to get insights on each personality\n<a id='training'><\/a>","565a9132":"With stopwords removed, we see very slight clustering, such as the words 'don't' and  'think'. But with the main informative features removed, the words do not show any sort of structure. ","c9a83b3d":"# **Classification using NLP on MBTI Personality Types** \n\n![image](https:\/\/cdn.shopify.com\/s\/files\/1\/0100\/5392\/articles\/Mouthpiece_VdayMeyersBriggs1.progressive.jpg?v=1549578121)\n\nAs a species, we have developed high level of cultural and social constructs to keep us evolutionary marvels. To create such constructs, the human brain compartmentalises other humans into categories. This makes absolute sense in terms of survival as we can gauge threats, especially since humans are intellectually complex... and hierarchical.\n\nOne such psychological classification technique we developed as humans is the MBTI personality type predictor. Designed as a set of questions which essentially try scale you into 16 different personality types.\n\nBeing able to predict someone's personality type just from the words they have written has application in sales, marketting, recruitment as well as human resources. \n\nProblem statement: build a classifier model that can predict a persons personality type from their twitter posts but is sufficiently generalised that it will be able to do so from any other source of text.","f34e9ad0":"![](http:\/\/)This results in a score of 5.65562. Many estimators are not capable of processing a multilabel input, so from now on, we will be using this method rather than multiclass prediction since the score is very similar.","453547fb":"-Make custom made preprocessor to fix all spelling mistakes and remove stopwords that didn't have apostrophes in them that were skipped when using NLTKs stopword list.","730b7f75":"Are you seeing what I am seeing? The best predictors for personality types are the personality types themselves, since these posts were obtained from a twitter page dedicated to discussions about personality, this makes sense. You will know what personality type a person is if they tell you directly. However, this does not help in predicting personality types from text that does not contain personality type information, so we need to remove these so other words that predict personality types can be identified.","022d66f1":"### Combine the train and test set to preprocess together","a02c71e3":"Allows us to investigate the features and their replationships in two dimensional space.","38c2b980":"Due to time and technical restraints, there are still a few questions we wanted to address. We will work on these at a later date.","3bbbf8e2":"It seems as if punctuation does have an effect on predicting personality types, while weblink related counts do not. It is interesting that the number of words used is a predictor. Let us have a look into that.","001b3dad":"For a random forest classifier, important factors to consider is how many samples should be in each terminal leaf before stopping. This will be determined by setting min_samples_leaf. Everytime a split is made by the regressor, needs to consider number of features when looking for the best split. Eg, 0.1 indicates that a random subset of 10% of the features will be available to consider for the best split. While log2 is the log2 of the number of features whole sqrt is the square root of the number of features.\nhttp:\/\/course18.fast.ai\/ml.html","1e9d92e0":"From examining the feature importance list, we need a list a the personality types, as well as their plural. Since punctuation was not removed at this stage, will add the single and plurals with a period to the list as well.","e5e914cd":"Since the we are after insight, having only 30 representatives per personality type is a bit small, so we decided to split the posts using the '|||' separator in order to get a larger representative dataset ","b5759ccd":"## Energy","02d7041a":"### Step 6","712eaca7":"Tactics: Perceiving (0), Judging (1)","ca30acaa":"As with the example above, removing stopwords affected the performance of the the different algorithms differently. We also saw that stopwords were among the features that had the highest importance. Perhaps other aspects of sentence structure have the ability to predict personality types.","7218db4b":"Now that the stopwords are back, we can see that there is some clear patterns, such as I and you clustering away from each other as well as my and your. ","ecc4d600":"Multilabel classification using the random forest classifier results in a score of 5.62375. But what if we did each prediciton individually? How would the score change?","2f1a96b0":"Each of the four sub-categories in a personality type fall into two letters (explained in the next section), let us see the representation of each.","a989e051":"### Step 4","fd9eee09":"-Visit the twitter feed where this data was obtained and obtain data for the under-represented personality types","92f27393":"# Sentiment analysis\n<a id='sentiment'><\/a>","20c4181d":"Random forest classifier also has a feature importance function which allows you to investigate which features \nare most important in explaining the target variable.\nSo when training a tree we can compute how much each feature contributes to decreasing the weighted impurity\nin classification, Gini impurity \/ information gain (entropy) is the metric used. The higher the score, the more informative\nthe feature.","987a2e04":"## Tactics","a9a4a765":"Before we do this comparison, the posts need to be tokenised to make removing the personality types possible","3bd51f11":"-Stopwords such as articles, personal pronouns and prepositions seem to be good predictors of personality types.","b7d650dc":"### Step 7","c0040549":"View both tables simultaneously","d7849ad3":"# Future work\n<a id='futurework'><\/a>","5ec1318e":"# Data clean-up\n<a id='datacleaning'><\/a>","67c32ff4":"### Remove punctation\n<a id='nopunc'><\/a>","100f0f3c":"### Step 2","27969d1c":"Before we start, let us set a new baseline for the reduced dataset to gauge the effect that the data cleaning is having on our ability to predict personality types. We will also be using three additional classifiers to identify whether another classifier produces better predictions than random forests and what factors may influence the classifiers ability to make a prediction.","9079dd9c":"From investigating the type data, we find that there are imbalances in the number of representatives per personality group, there are several approaches to deal with this problem. Here, we compare our baseline model with two oversampling approaches and one undersampling approach.","f9d26afb":"Another way to clean data is to employ stemming or lemmatising. Words such as run, ran, running all come from the word run. Stemming removes the endings of words, this will work for running but not ran. While lemmatising finds the root of the word called lemma, so it can convert all the example words to run.","1debf051":"### Splitting the posts","a8787caf":"Rather than providing 16 labels for the classifiers to predict, each of these can be broken down into four sets of binary labels. This format is required for submissions to this Kaggle challenge. Besides submission requirements, this format will allow us to break down each aspect of the personality individually","ed8bcf92":"We want to explore how the different personality types communicate. Using TextBlob's sentiment analyser we can do this with ease. \n- First we will simply rank the personality types according to how positive or negative their posts average sentiments are.\n- Next we will use graphs to visualise the sentiments","dfc250fc":"# Part of speech analysis\n<a id='pos'><\/a>","e4a212bd":"Some results became better when punctuation was removed while others got worse. Additionally, punctuation was among the more informative features. We will remove them for now, but use this insight to do feature engineering such as counting punctuation.","b86e5e42":"We saw that many of the columns had 0 in them quite often, so we wanted to investigate how these features would perform in predicting personality types when we compare the long vs wide format of the text data.","93889ddc":"# Insight\n<a id='insights'><\/a>","34afe487":"Nature: Feeling (0), Thinking (1)","9b355106":"We can see that there is punctuation present in the data. We will remove these and see how our score is impacted.","8b2cbce3":"Energy: Sensing (0) , Intuitive (1)","8b9cfa58":"Seems like despite the lemmatizer being better in theory, it seems that the model performed better when a stemmer was applied to the tokens.","e4845190":"# Investigating the data imbalance\n<a id='imbalance'><\/a>","55fa8712":"It seems that the score gets worse when the personality types are removed, but this makes sense as they were the most informative features. We can once again check what features are informative.","34dcbaa9":"-From our analysis, the fraction of the sentence that a part of speech occupies is not an informative feature for predicting personality types.","1d3070d1":"Seems like 74 words mainly composed of articles, personal pronouns and prepositions are the strongest predictors of personality type.","0066f8ee":"-Investigate the effect of adding bi-grams and tri-grams to the model","cfcc3fb3":"# Feature engineering\n<a id='featureeng'><\/a>","8cf331f9":"The scores are not particularily good, but they do show some predictive power. Interestingly, it seems that half of the algorithms perform better with the long format while the other half perform better with long format.","ca1c8cbc":"## Counting post properties\n<a id='countpostprop'><\/a>","41cf8489":"Logistic regression: C determines how much regularisation is done on the data, while penalty will provide the type of regularisation to perform. L1 represents Lasso regularisation while L2 represents ridge regularisation.\nhttps:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_grid_search\/","320d479c":"## Removing stopwords\n<a id='nostopwords'><\/a>","32a87a47":"## Mind","87931f33":"-More in-depth part of speech analysis where each category of adjective, noun and verb are investigated. Normal counts rather than fractions would also be investigated.","41c9d965":"Polarity seems to differ slightly between personality types, while subjectivity remains relatively constant. Despite this, we will see whether sentiment can be used to predict personality types.","78b3e638":"From this point, steps 4 to 8 from the above baseline protocol were performed, we just replaced the respective variables with the ones used in this section.","06b52c57":"With majority of the posts coming from six personality types, this measn the data is unbalanced, we will see how this affects the acuracy of our prediction later on.","450bc9f6":"## Stemming vs lemmatizing\n<a id='stemlem'><\/a>","c85ee63e":"It seems that there are still a few stopwords in the data, and it seems that there are a few spelling mistakes. ","3c9192f1":"It was found that both random oversampling (6.74692) and SMOTE (6.39643) were not able to provide a better prediction that the default dataset","67da2995":"Mind: Introverted (0), Extroverted (1)","6aca834b":"64 main predicting words for identifying between introverts and extroverts.","a15e945c":"Since the number of estimators determines how many decision trees are generated and average, the higher the n_estimators, the better the score, we tried this out.","effc23a0":"### Core set of predicting words","5907119f":"### Step 5","b68a0087":"Random over-sampling increases the under-represented data by drawing from existing data points. Synthetic Minority Over-sampling Technique (SMOTE) creates new data points based on the information of those that already exist. It picks a point at random and computes the new data point using k-nearest neighbours. This is explained in greater detail in the following kernel:https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets","611c0ab6":"-Apply a recursive neural network in Keras on the data to see whether a better prediction can be obtained.","fe5a00ad":"The differences may not be too pronounced, but it seems like the majority of the extroverted personality types tend to have longer posts.","466b165d":"## Nature","6b63cb12":"Under-sampling involves reducing the number of samples from each category to that of the group with the least representatives. ","ad17a83b":"# Hyperparameter tuning","0f3ec9fe":"Topics 0 to 3 seem to make sense why these words would be clustered together. ","8ac1b35c":"-The relatively similar group of words seem to be used by random forests to predict personality types, perhaps the model is sufficiently general to be applicable to other forms of writing, not just tweets.","9a655d19":"Now that we have looked at the dependent variables, let's establish a baseline model. This approach was adapted from Fast.AI  (lessons 1 to 4: http:\/\/course18.fast.ai\/ml.html) and relies on using random forests to get not only a prediction, but extract insight on feature importance and how the decisions based on the data are made by the model. Random forests does not require the data to be clean and is not affected by distribution of dependent variable.","f423c16b":"Code for generating the decision tree obtained from https:\/\/www.kaggle.com\/willkoehrsen\/visualize-a-decision-tree-w-python-scikit-learn","a8397a8d":"# Applying insights to full dataset\n<a id='applyinsight'><\/a>","af6395e9":"We found that removing punctuation affected the performance of the model in its ability to predict personality types both positively and negatively. This may be an indication that the words themselves may not be the only predictors of personality. Perhaps we need to look a little deeper into the posts themselves and see what insights we can find . To do this, we will create common text summaries on the reduced data set and see how they effect the models perfomance.","213f4948":"Since these functions mostly relate to fractions of parts of speech present, only the wide format was used to determine whether any of these features had any predictive power. The code for the long format data remains but it seems that certain posts had nothing in them since errors related to dividing by zero were returned. ","312e8c1c":"-Perform this analysis on the full dataset rather than a small subset, however due to the time taken to complete operations, it was not feasible at this time.","ccbd3934":"### Terminology:","6861cac0":"The scores are pretty bad, perhaps in combination with other features or more samples, these features may have more predictive power.","909e5856":"-We see that the model prediction accuracy improved as the number of estimators increased in our baseline set, perhaps the larger datasets produced by Random oversampling and SMOTE will provide better results if more estimators were used, such as 500. Due to the size of the dataset, when more estimators were added, the Kaggle kernel timed out.","01d6f688":"Not only are the number of representatives for the 16 personalities imbalanced, the same trend is seen for the four sub-categories. ","7237e4f7":"# Removing personality types\n<a id='nopers'><\/a>","aae99658":"# Undersampling\n<a id='undersampling'><\/a>","3cdef669":"# Dimension reduction\n<a id='dimension'><\/a>","dd48bd79":"Create new columns with the summaries of the different elements making up the posts and print them out","0ba4014a":"# Baseline\n<a id='baseline'><\/a>","c6077160":"We see that this approach also does not perform as well as the default dataset with a score of 11.43870. However, it was found that the random forest classifier ran very quickly as compared to the three datasets above, so from this point on, we will be using this smaller dataset to mine insight from the text data and troubleshoot. We will then apply these findings at the end to the full dataset. We will see how these findings will affect the score and the accuracy of our model.","84393b6c":"## Table of Contents\n***\n1. [Explore the data](#eda)\n    * [Insights from the Training set](#training)\n    * [Establishing a Baseline](#baseline)\n    * [Investigating the imbalance](#imbalance)\n        * [Applying Oversampling](#oversampling)\n        * [Applying Undersampling](#undersampling)\n***    \n2. [Data Cleaning](#datacleaning)\n    * [Remove Personality Types](#nopers)\n    * [Remove Punctuation](#nopunc)\n    * [Stemmatising and Lemmatising](#stemlem)\n    * [Remove Stop Words](#nostopwords)\n***\n3. [Feature Engineering](#featureeng)\n    * [Counting Post Properties](#countpostprop)\n    * [Parts of Speech Analysis](#pos)\n    * [Sentiment Analysis](#sentiment)\n    * [Insights](#insights)\n    * [Dimensionality Reduction](#dimension)\n***\n4. [Applying Insights to Full Dataset](#applyinsight)\n***\n5. [Conclusion](#conclusion)\n***\n6. [Future Work](#futurework)\n","2c1b7bc6":"The main predicting words seem to be stopwords, this is strange since they are among the first features of text to be removed in NLP practises. However, it seems that stopwords may actually be good predictors of personality, and it seems that this article seems to agree : https:\/\/www.scientificamerican.com\/article\/you-are-what-you-say\/?redirect=1. We will remove stopwords later to identify other words that may be of interest.","540779fb":"Some of these topics make sense that the words would be grouped together. Some examples include topic 0 and 1.","37e1dc28":"We will investigate with and without stopwords","09654d1b":"Besides just punctuation, sentence structure may have some predictive value. Let us investigate this further.","fa996ac9":"Lets examine what features may be important in this prediction.","1a414b80":"Our assumption ran true, this approach gave us a score 5.50437.","6930a05f":"Naive Bayes: alpha determines the level of smoothing the model does, with 0 indicating no smoothing. It is a form of regularisation for Naive Bayes. \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html","e6f92793":"Let's test which of these performs better.","79ca79f0":"From the tables above we it looks like ENFJ's exibit more positive sentiments in their posts as oppose to the other type. INTP's display express the most negative sentiments in their posts\/comments. This gives a bit of insight into how we can expect when we communicate with these personality types","dd3d9e3a":"-Slightly longer posts by extroverts","7255c45c":"### Step 8","a92b91b5":"### Step 1","8ba66959":"Eventhough the mind predictor has been the test in the data cleaning and EDA section, we still need to extract some insight on\nwhat words are associated with introverts and extroverts. For this section, we will run the model with and without stopwords included. We will extract features with a gini score above 0.005 for both experiments. We will extract them from the word vector and combine them with the engineered features to see how this affects predictions. Finally, we will produce a mind map of the most common words in introverts and extroverts as well as a countplot that shows the count of the informative words between introverts and extroverts.","dd224f3f":"### Step 3"}}