{"cell_type":{"be0abb7c":"code","1981cbae":"code","c214d097":"code","f1ad53f6":"code","017e0635":"code","2a7d6b8d":"code","73de4be6":"code","47cdaa31":"code","507c9ecd":"code","aa89ebbb":"code","0fc28a98":"code","d458fa35":"code","44dcb15e":"code","66cf79b9":"code","859e97c7":"code","352b4cac":"code","9ff2ed58":"code","ceac6c97":"code","20a80e87":"code","e656c0e3":"code","58d5be04":"code","c81eb395":"code","750e9b25":"code","55ad45dd":"code","ec40e433":"code","9d61d643":"code","1aedd3e5":"code","3eb767e7":"code","a0e77d7f":"code","b9b62dcb":"code","7cadcdd2":"markdown","a5d9da08":"markdown","0ba9c0bb":"markdown","3c1cdafe":"markdown","f77302ad":"markdown","9e4367ce":"markdown","adf434b2":"markdown","38e5ff6c":"markdown","5fc7f3d7":"markdown","8e199bd4":"markdown","a68580e0":"markdown","c94c8160":"markdown","f53e056e":"markdown"},"source":{"be0abb7c":"BATCH_SIZE = 48\nEDGE_CROP = 16\nGAUSSIAN_NOISE = 0.1\nUPSAMPLE_MODE = 'SIMPLE'\n# downsampling inside the network\nNET_SCALING = (1, 1)\n# downsampling in preprocessing\nIMG_SCALING = (3, 3)\n# number of validation images to use\nVALID_IMG_COUNT = 900\n# maximum number of steps_per_epoch in training\nMAX_TRAIN_STEPS = 7\nMAX_TRAIN_EPOCHS = 99\nAUGMENT_BRIGHTNESS = False","1981cbae":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage2d as montage\nfrom skimage.morphology import binary_opening, disk, label\nimport gc; gc.enable() # memory is tight\n\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nship_dir = '..\/input'\ntrain_image_dir = os.path.join(ship_dir, 'train')\ntest_image_dir = os.path.join(ship_dir, 'test')\n\ndef multi_rle_encode(img, **kwargs):\n    '''\n    Encode connected regions as separated masks\n    '''\n    labels = label(img)\n    if img.ndim > 2:\n        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n    else:\n        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n\n# ref: https:\/\/www.kaggle.com\/paulorzp\/run-length-encode-and-decode\ndef rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    if np.max(img) < min_max_threshold:\n        return '' ## no need to encode if it's all zeros\n    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n        return '' ## ignore overfilled mask\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.uint8)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks |= rle_decode(mask)\n    return all_masks\n\ndef masks_as_color(in_mask_list):\n    # Take the individual ship masks and create a color mask array for each ships\n    all_masks = np.zeros((768, 768), dtype = np.float)\n    scale = lambda x: (len(in_mask_list)+x+1) \/ (len(in_mask_list)*2) ## scale the heatmap image to shift \n    for i,mask in enumerate(in_mask_list):\n        if isinstance(mask, str):\n            all_masks[:,:] += scale(i) * rle_decode(mask)\n    return all_masks","c214d097":"masks = pd.read_csv(os.path.join('..\/input\/', 'train_ship_segmentations.csv'))\nnot_empty = pd.notna(masks.EncodedPixels)\nprint(not_empty.sum(), 'masks in', masks[not_empty].ImageId.nunique(), 'images')\nprint((~not_empty).sum(), 'empty images in', masks.ImageId.nunique(), 'total images')\nmasks.head()","f1ad53f6":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize = (16, 5))\nrle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\nimg_0 = masks_as_image(rle_0)\nax1.imshow(img_0)\nax1.set_title('Mask as image')\nrle_1 = multi_rle_encode(img_0)\nimg_1 = masks_as_image(rle_1)\nax2.imshow(img_1)\nax2.set_title('Re-encoded')\nimg_c = masks_as_color(rle_0)\nax3.imshow(img_c)\nax3.set_title('Masks in colors')\nimg_c = masks_as_color(rle_1)\nax4.imshow(img_c)\nax4.set_title('Re-encoded in colors')\nprint('Check Decoding->Encoding',\n      'RLE_0:', len(rle_0), '->',\n      'RLE_1:', len(rle_1))\nprint(np.sum(img_0 - img_1), 'error')","017e0635":"masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\nunique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\nunique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\nunique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n# some files are too small\/corrupt\nunique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n                                                               os.stat(os.path.join(train_image_dir, \n                                                                                    c_img_id)).st_size\/1024)\nunique_img_ids = unique_img_ids[unique_img_ids['file_size_kb'] > 50] # keep only +50kb files\nunique_img_ids['file_size_kb'].hist()\nmasks.drop(['ships'], axis=1, inplace=True)\nunique_img_ids.sample(7)","2a7d6b8d":"unique_img_ids['ships'].hist(bins=unique_img_ids['ships'].max())","73de4be6":"SAMPLES_PER_GROUP = 2000\nbalanced_train_df = unique_img_ids.groupby('ships').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\nbalanced_train_df['ships'].hist(bins=balanced_train_df['ships'].max()+1)\nprint(balanced_train_df.shape[0], 'masks')","47cdaa31":"from sklearn.model_selection import train_test_split\ntrain_ids, valid_ids = train_test_split(balanced_train_df, \n                 test_size = 0.3, \n                 stratify = balanced_train_df['ships'])\ntrain_df = pd.merge(masks, train_ids)\nvalid_df = pd.merge(masks, valid_ids)\nprint(train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')","507c9ecd":"def make_image_gen(in_df, batch_size = BATCH_SIZE):\n    all_batches = list(in_df.groupby('ImageId'))\n    out_rgb = []\n    out_mask = []\n    while True:\n        np.random.shuffle(all_batches)\n        for c_img_id, c_masks in all_batches:\n            rgb_path = os.path.join(train_image_dir, c_img_id)\n            c_img = imread(rgb_path)\n            c_mask = np.expand_dims(masks_as_image(c_masks['EncodedPixels'].values), -1)\n            if IMG_SCALING is not None:\n                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n            out_rgb += [c_img]\n            out_mask += [c_mask]\n            if len(out_rgb)>=batch_size:\n                yield np.stack(out_rgb, 0)\/255.0, np.stack(out_mask, 0)\n                out_rgb, out_mask=[], []","aa89ebbb":"train_gen = make_image_gen(train_df)\ntrain_x, train_y = next(train_gen)\nprint('x', train_x.shape, train_x.min(), train_x.max())\nprint('y', train_y.shape, train_y.min(), train_y.max())","0fc28a98":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (30, 10))\nbatch_rgb = montage_rgb(train_x)\nbatch_seg = montage(train_y[:, :, :, 0])\nax1.imshow(batch_rgb)\nax1.set_title('Images')\nax2.imshow(batch_seg)\nax2.set_title('Segmentations')\nax3.imshow(mark_boundaries(batch_rgb, \n                           batch_seg.astype(int)))\nax3.set_title('Outlined Ships')\nfig.savefig('overview.png')","d458fa35":"%%time\nvalid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\nprint(valid_x.shape, valid_y.shape)","44dcb15e":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 45, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  horizontal_flip = True, \n                  vertical_flip = True,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last')\n# brightness can be problematic since it seems to change the labels differently from the images \nif AUGMENT_BRIGHTNESS:\n    dg_args[' brightness_range'] = [0.5, 1.5]\nimage_gen = ImageDataGenerator(**dg_args)\n\nif AUGMENT_BRIGHTNESS:\n    dg_args.pop('brightness_range')\nlabel_gen = ImageDataGenerator(**dg_args)\n\ndef create_aug_gen(in_gen, seed = None):\n    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n    for in_x, in_y in in_gen:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        g_x = image_gen.flow(255*in_x, \n                             batch_size = in_x.shape[0], \n                             seed = seed, \n                             shuffle=True)\n        g_y = label_gen.flow(in_y, \n                             batch_size = in_x.shape[0], \n                             seed = seed, \n                             shuffle=True)\n\n        yield next(g_x)\/255.0, next(g_y)","66cf79b9":"cur_gen = create_aug_gen(train_gen)\nt_x, t_y = next(cur_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n# only keep first 9 samples to examine in detail\nt_x = t_x[:9]\nt_y = t_y[:9]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage_rgb(t_x), cmap='gray')\nax1.set_title('images')\nax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')\nax2.set_title('ships')","859e97c7":"gc.collect()","352b4cac":"from keras import models, layers\n# Build U-Net model\ndef upsample_conv(filters, kernel_size, strides, padding):\n    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\ndef upsample_simple(filters, kernel_size, strides, padding):\n    return layers.UpSampling2D(strides)\n\nif UPSAMPLE_MODE=='DECONV':\n    upsample=upsample_conv\nelse:\n    upsample=upsample_simple\n    \ninput_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\npp_in_layer = input_img\n\nif NET_SCALING is not None:\n    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n    \npp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\npp_in_layer = layers.BatchNormalization()(pp_in_layer)\n\nc1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\nc1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = layers.MaxPooling2D((2, 2)) (c1)\n\nc2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = layers.MaxPooling2D((2, 2)) (c2)\n\nc3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = layers.MaxPooling2D((2, 2)) (c3)\n\nc4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n\n\nc5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = layers.concatenate([u6, c4])\nc6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = layers.concatenate([u7, c3])\nc7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = layers.concatenate([u8, c2])\nc8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = layers.concatenate([u9, c1], axis=3)\nc9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\nd = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n# d = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\n# d = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\nif NET_SCALING is not None:\n    d = layers.UpSampling2D(NET_SCALING)(d)\n\nseg_model = models.Model(inputs=[input_img], outputs=[d])\nseg_model.summary()","9ff2ed58":"import keras.backend as K\nimport tensorflow as tf\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\ndef custom_loss(y_true, y_pred):\n    focal = focal_loss()\n    IoU = IoU_loss()\n    return 0.995*IoU(y_true, y_pred) + 0.005*focal(y_true, y_pred)\n\n## focal\ndef focal_loss(gamma=2., alpha=.25, eps=1e-7):\n    def focal_loss_fixed(y_true, y_pred):\n        y_pred = K.clip(y_pred, eps, 1.0-eps) # improve the stability\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))        \n        fl_1 = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1), axis=[1,2,3])\n        fl_0 = -K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0), axis=[1,2,3])\n        return K.mean(fl_0 + fl_1, axis=0)\n    return focal_loss_fixed\n\n## Dice\n## laplace smooth or additive smooth for preventing overfitting\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\n\n## BCE - Dice\ndef dice_p_bce(in_gt, in_pred):\n    return binary_crossentropy(in_gt, in_pred) - K.log(dice_coef(in_gt, in_pred))\n\n## TP rate\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true) * K.flatten(K.round(y_pred))) \/ K.sum(y_true)\n\n## IOU\ndef IoU_loss(eps=1e-6):\n    def IoU(y_true, y_pred):\n        if np.max(y_true) == 0.0:\n            return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n        intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n        union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n        return 1-K.mean( (intersection + eps) \/ (union + eps), axis=0)\n    return IoU\n\ndef IoU_coef(y_true, y_pred, eps=1e-6):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n    return K.mean( (intersection + eps) \/ (union + eps), axis=0)","ceac6c97":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                                   patience=1, verbose=1, mode='min',\n                                   min_delta=0.0001, cooldown=0, min_lr=1e-8)\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2,\n                      patience=10) # probably needs to be more patient, but kaggle time is limited\n\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","20a80e87":"def fit():\n    seg_model.compile(optimizer=Adam(5e-4, decay=1e-5), loss=dice_p_bce, metrics=[true_positive_rate, \n                                                                                   IoU_coef, \n                                                                                   dice_coef, \n                                                                                   'binary_accuracy'])\n    \n    step_count = min(MAX_TRAIN_STEPS, train_df.shape[0]\/\/BATCH_SIZE)\n    aug_gen = create_aug_gen(make_image_gen(train_df))\n    loss_history = [seg_model.fit_generator(aug_gen,\n                                 steps_per_epoch=step_count,\n                                 epochs=MAX_TRAIN_EPOCHS,\n                                 validation_data=(valid_x, valid_y),\n                                 callbacks=callbacks_list,\n                                workers=1 # the generator is not very thread safe\n                                           )]\n    return loss_history\n\nwhile True:\n    loss_history = fit()\n    if np.min([mh.history['val_loss'] for mh in loss_history]) < 1:\n        break","e656c0e3":"def show_loss(loss_history):\n    epochs = np.concatenate([mh.epoch for mh in loss_history])\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n    \n    _ = ax1.plot(epochs, np.concatenate([mh.history['loss'] for mh in loss_history]), 'b-',\n                 epochs, np.concatenate([mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('Loss')\n    \n    _ = ax2.plot(epochs, np.concatenate([mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n                 epochs, np.concatenate([mh.history['val_binary_accuracy'] for mh in loss_history]), 'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('Binary Accuracy (%)')\n    \n    _ = ax3.plot(epochs, np.concatenate([mh.history['IoU_coef'] for mh in loss_history]), 'b-',\n                 epochs, np.concatenate([mh.history['val_IoU_coef'] for mh in loss_history]), 'r-')\n    ax3.legend(['Training', 'Validation'])\n    ax3.set_title('IoU (%)')\n    \n    _ = ax4.plot(epochs, np.concatenate([mh.history['dice_coef'] for mh in loss_history]), 'b-',\n                 epochs, np.concatenate([mh.history['val_dice_coef'] for mh in loss_history]), 'r-')\n    ax4.legend(['Training', 'Validation'])\n    ax4.set_title('dice_coef (%)')\n\nshow_loss(loss_history)","58d5be04":"seg_model.load_weights(weight_path)\nseg_model.save('seg_model.h5')","c81eb395":"pred_y = seg_model.predict(valid_x)\nprint(pred_y.shape, pred_y.min(axis=0).max(), pred_y.max(axis=0).min(), pred_y.mean())","750e9b25":"fig, ax = plt.subplots(1, 1, figsize = (6, 6))\nax.hist(pred_y.ravel(), np.linspace(0, 1, 20))\nax.set_xlim(0, 1)\nax.set_yscale('log', nonposy='clip')","55ad45dd":"if IMG_SCALING is not None:\n    fullres_model = models.Sequential()\n    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n    fullres_model.add(seg_model)\n    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\nelse:\n    fullres_model = seg_model\nfullres_model.save('fullres_model.h5')","ec40e433":"def raw_prediction(img, path=test_image_dir):\n    c_img = imread(os.path.join(path, c_img_name))\n    c_img = np.expand_dims(c_img, 0)\/255.0\n    cur_seg = fullres_model.predict(c_img)[0]\n    return cur_seg, c_img[0]\n\ndef smooth(cur_seg):\n    return binary_opening(cur_seg>0.5, np.expand_dims(disk(2), -1))\n\ndef predict(img, path=test_image_dir):\n    cur_seg, c_img = raw_prediction(img, path=path)\n    return smooth(cur_seg), c_img\n\n## Get a sample of each group of ship count\nsamples = valid_df.groupby('ships').apply(lambda x: x.sample(1))\nfig, m_axs = plt.subplots(samples.shape[0], 4, figsize = (15, samples.shape[0]*4))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\n\nfor (ax1, ax2, ax3, ax4), c_img_name in zip(m_axs, samples.ImageId.values):\n    first_seg, first_img = raw_prediction(c_img_name, train_image_dir)\n    ax1.imshow(first_img)\n    ax1.set_title('Image: ' + c_img_name)\n    ax2.imshow(first_seg[:, :, 0], cmap=get_cmap('jet'))\n    ax2.set_title('Model Prediction')\n    reencoded = masks_as_color(multi_rle_encode(smooth(first_seg)[:, :, 0]))\n    ax3.imshow(reencoded)\n    ax3.set_title('Prediction Masks')\n    ground_truth = masks_as_color(masks.query('ImageId==\"{}\"'.format(c_img_name))['EncodedPixels'])\n    ax4.imshow(ground_truth)\n    ax4.set_title('Ground Truth')\n    \nfig.savefig('validation.png')","9d61d643":"test_paths = np.array(os.listdir(test_image_dir))\nprint(len(test_paths), 'test images found')","1aedd3e5":"from tqdm import tqdm_notebook\n\ndef pred_encode(img, **kwargs):\n    cur_seg, _ = predict(img)\n    cur_rles = multi_rle_encode(cur_seg, **kwargs)\n    return [[img, rle] for rle in cur_rles if rle is not None]\n\nout_pred_rows = []\nfor c_img_name in tqdm_notebook(test_paths[:30000]): ## only a subset as it takes too long to run\n    out_pred_rows += pred_encode(c_img_name, min_max_threshold=1.0)","3eb767e7":"sub = pd.DataFrame(out_pred_rows)\nsub.columns = ['ImageId', 'EncodedPixels']\nsub = sub[sub.EncodedPixels.notnull()]\nsub.head()","a0e77d7f":"## let's see what we got\nTOP_PREDICTIONS=5\nfig, m_axs = plt.subplots(TOP_PREDICTIONS, 2, figsize = (9, TOP_PREDICTIONS*5))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\n\nfor (ax1, ax2), c_img_name in zip(m_axs, sub.ImageId.unique()[:TOP_PREDICTIONS]):\n    c_img = imread(os.path.join(test_image_dir, c_img_name))\n    c_img = np.expand_dims(c_img, 0)\/255.0\n    ax1.imshow(c_img[0])\n    ax1.set_title('Image: ' + c_img_name)\n    ax2.imshow(masks_as_color(sub.query('ImageId==\"{}\"'.format(c_img_name))['EncodedPixels']))\n    ax2.set_title('Prediction')","b9b62dcb":"sub1 = pd.read_csv('..\/input\/sample_submission.csv')\nsub1 = pd.DataFrame(np.setdiff1d(sub1['ImageId'].unique(), sub['ImageId'].unique(), assume_unique=True), columns=['ImageId'])\nsub1['EncodedPixels'] = None\nprint(len(sub1), len(sub))\n\nsub = pd.concat([sub, sub1])\nprint(len(sub))\nsub.to_csv('submission.csv', index=False)\nsub.head()","7cadcdd2":"# Decode all the RLEs into Images\nWe make a generator to produce batches of images","a5d9da08":"# Submission","0ba9c0bb":"# Make sure encode\/decode works\nGiven the process\n$$  RLE_0 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_0 \\stackrel{Encode}{\\longrightarrow} RLE_1 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_1 $$\nWe want to check if\/that\n$ \\textrm{Image}_0 \\stackrel{?}{=} \\textrm{Image}_1 $\nWe could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts.\n\n","3c1cdafe":"# Split into training and validation groups\nWe stratify by the number of boats appearing so we have nice balances in each set","f77302ad":"# Augment Data","9e4367ce":"# Build a Model\nHere we use a slight deviation on the U-Net standard","adf434b2":"# Make the Validation Set","38e5ff6c":"# Overview\nWe try here to improve another public U-Net model: https:\/\/www.kaggle.com\/kmader\/baseline-u-net-model-part-1 which shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have also been done.\n\nWe are using a different loss function (closer to the competition scoring) and also fix and improve some visualisation functions and the submission itself.","5fc7f3d7":"# Prepare Full Resolution Model\nHere we account for the scaling so everything can happen in the model itself","8e199bd4":"## Model Parameters\nWe might want to adjust these later (or do some hyperparameter optimizations)","a68580e0":"# Visualize predictions","c94c8160":"# Undersample Empty Images\nHere we undersample the empty images to get a better balanced group with more ships to try and segment","f53e056e":"### Examine Number of Ship Images\nHere we examine how often ships appear and replace the ones without any ships with 0"}}