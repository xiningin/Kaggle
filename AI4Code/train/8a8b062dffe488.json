{"cell_type":{"c6f4395e":"code","a790663e":"code","1893f739":"code","c3e3c0a1":"code","b270da10":"code","17e233d2":"code","089280d5":"code","507b09de":"code","64fe4844":"code","c23e7d6f":"code","d495b17b":"code","4ea38e6f":"code","429ec3a9":"markdown","a9160d0b":"markdown","fc2f0931":"markdown","da180e68":"markdown","0f979706":"markdown"},"source":{"c6f4395e":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\npd.reset_option('^display.', silent=True)\n\nX_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ny_train = X_train.Survived\ny_test_ids = X_test['PassengerId']\nX_train.drop(['Survived'], axis=1, inplace=True)\n\n# Drop name and ticket col\nX_train.drop(['Name', 'Ticket'], axis=1, inplace=True)\nX_test.drop(['Name', 'Ticket'], axis=1, inplace=True)\n\nX_train.head()","a790663e":"print(\"Total training samples:\", len(X_train), \"\\n\")\nprint(\"Partial data\\n\", X_train.iloc[0:4, 0:6], \"\\n\")\nprint(\"Samples per sex\\n\", X_train.groupby('Sex')['Sex'].count())","1893f739":"X_train.describe()","c3e3c0a1":"# Print a survivor as a sample\nsample_index = 25\nprint(X_train.iloc[sample_index])","b270da10":"# Show the column types we are dealing with\n\nX_train.dtypes.value_counts()\ncategorical_columns = X_train.select_dtypes('object').columns\nprint(len(X_train.columns)-len(X_train.select_dtypes('object').columns),'numerical columns:')\nprint([i for i in list(X_train.columns) if i not in list(X_train.select_dtypes('object').columns)], '\\n')\nprint(len(X_train.select_dtypes('object').columns),'categorical columns:')\nprint(list(X_train.select_dtypes('object').columns))","17e233d2":"# Find and print out columns that contain NaN's\n\nfrom sklearn.impute import SimpleImputer\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nX_train_missing_num = [col for col in X_train.columns if X_train[col].isnull().any() and is_numeric_dtype(X_train[col])]\nX_test_missing_num = [col for col in X_test.columns if X_test[col].isnull().any() and is_numeric_dtype(X_test[col])]\nX_train_missing_obj = [col for col in X_train.columns if X_train[col].isnull().any() and is_string_dtype(X_train[col])]\nX_test_missing_obj = [col for col in X_test.columns if X_test[col].isnull().any() and is_string_dtype(X_test[col])]\n\nprint('X_train num missing cols:', X_train_missing_num)\nprint('X_test num missing cols:', X_test_missing_num)\nprint('X_train obj missing cols:', X_train_missing_obj)\nprint('X_test obj missing cols:', X_test_missing_obj)","089280d5":"# Use the SimpleImputer to impute values for numerical NaN's\nobject_cols = ['Sex', 'Cabin', 'Embarked']\nX_train_obj = X_train[object_cols]\nX_test_obj = X_test[object_cols]\nX_train.drop(object_cols, axis=1, inplace=True)\nX_test.drop(object_cols, axis=1, inplace=True)\n\nsi = SimpleImputer()\nX_train_imputed = pd.DataFrame(si.fit_transform(X_train))\nX_test_imputed = pd.DataFrame(si.transform(X_test))\n\nX_train_imputed.columns = X_train.columns\nX_test_imputed.columns = X_test.columns\n\nX_train = pd.merge(X_train_imputed, X_train_obj, left_index=True, right_index=True)\nX_test = pd.merge(X_test_imputed, X_test_obj, left_index=True, right_index=True)\n\n# Handle the Cabin and Embarked columns\nX_train['Cabin'].fillna('None', inplace=True)\nX_test['Cabin'].fillna('None', inplace=True)\n\nnan_inv = pd.isna(X_train['Embarked'])\nnan_entries = list(nan_inv[nan_inv == True].index)\nmean_embarked = X_train['Embarked'].mode() # S\n\nfor idx in nan_entries:\n    X_train.at[idx, 'Embarked'] = 'S'","507b09de":"print(X_train.isnull().values.any())\nprint(X_test.isnull().values.any())","64fe4844":"from sklearn.preprocessing import OneHotEncoder\n\ndef encode_inputs(X_train, X_test, object_cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    X_train_enc = pd.DataFrame(ohe.fit_transform(X_train[object_cols]))\n    X_test_enc = pd.DataFrame(ohe.transform(X_test[object_cols]))\n    X_train_enc.columns = ohe.get_feature_names(object_cols)\n    X_test_enc.columns = ohe.get_feature_names(object_cols)\n    X_train_enc.index = X_train.index\n    X_test_enc.index = X_test.index\n    return X_train_enc, X_test_enc\n\n# Use OH encoder to encode cat cols, then remove and add enc\nX_train_enc, X_test_enc = encode_inputs(X_train, X_test, object_cols)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\nX_train = pd.concat([num_X_train, X_train_enc], axis=1)\nX_test = pd.concat([num_X_test, X_test_enc], axis=1)\n\n# Drop cabin cols\nX_train = X_train.loc[:, ~X_train.columns.str.startswith('Cabin')]\nX_test = X_test.loc[:, ~X_test.columns.str.startswith('Cabin')]","c23e7d6f":"X_train.head()","d495b17b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression          \nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom time import time\n\nnames = [\"Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"RBF SVM\",\n         \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"LDA\", \"QDA\", \"GradientBoosting\"]\n\nN_JOBS=24\nrs = np.random.RandomState(2)\n\nclassifiers = [\n    GridSearchCV(KNeighborsClassifier(),\n                 param_grid={'n_neighbors': [2, 4, 6, 8, 10, 15, 20, 50, 100]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(DecisionTreeClassifier(random_state=rs),\n                 param_grid={'max_depth': [2, 4, 6, 8, 10]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(RandomForestClassifier(random_state=rs),\n                 param_grid={'max_depth': [2, 4, 6],\n                             'n_estimators': [100, 250, 300],\n                             'max_features': [4, 5, 6, 'sqrt'],\n                             'min_samples_leaf': [25, 30]}, \n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(LogisticRegression(max_iter=1000, random_state=rs),\n                 param_grid={'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e5, 1e10],\n                             'penalty': ['none', 'l2']},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n   \n    GridSearchCV(SVC(kernel='rbf', random_state=rs, \n                     class_weight='balanced'),\n                 param_grid={'C': [0.1, 1, 2, 2.2, 2.5, 2.7, 3, 5, 10]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(MLPClassifier(max_iter=1000, random_state=rs),\n                 param_grid={'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e3],\n                             'hidden_layer_sizes': [(50,50,50), (100, 100, 100), (100,)],\n                             'activation': ['tanh', 'relu'],\n                             'learning_rate': ['constant','adaptive']},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(max_depth=4),\n                                    random_state=rs, n_estimators=200),\n                 param_grid={'n_estimators': [10, 20, 50, 100],\n                             'learning_rate': [0.01, 0.1, 0.5, 1, 10]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(GaussianNB(),\n                 param_grid={'var_smoothing': [1e-9, 1e-8, 1e-5, 1e-1]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(LinearDiscriminantAnalysis(),\n                param_grid={'solver': ['svd', 'lsqr', 'eigen'],\n                           'shrinkage': ['auto', 0, 0.5, 1],\n                           'tol': [1.0e-4, 1.0e-3, 1.0e-2, 1.0e-1]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(QuadraticDiscriminantAnalysis(),\n                param_grid={'reg_param': [0.0, 0.01, 0.05, 0.1, 0.5],\n                           'tol': [1.0e-4, 1.0e-3, 1.0e-2, 1.0e-1]},\n                 refit=True, cv=3, n_jobs=N_JOBS),\n    \n    GridSearchCV(GradientBoostingClassifier(random_state=rs),\n                param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                           'n_estimators': [50, 100, 200, 500],\n                           'max_depth': [3, 5, 10, 20, 30]},\n                 refit=True, cv=3, n_jobs=N_JOBS)\n]\n\nclf_trained_dict = {}\npreds = {}\n\n# Iterate over classifiers\nfor name, clf in zip(names, classifiers):\n    print(f\"Evaluating classifier: {name}\")\n\n    scaler = StandardScaler()  # Scale to mean = 0 and std_dev = 1\n    X_train_scaled = scaler.fit_transform(X_train) # Fit to training data\n    \n    # Fit the classifer on the tranining set\n    t0 = time()\n    clf.fit(X_train_scaled, y_train)\n    print(\"Training time:\", round(time()-t0, 4), \"s\")\n\n    # Make predictions on the training set\n    t1 = time()\n    predictions = clf.predict(X_train_scaled)\n    print(\"Prediction time:\", round(time()-t1, 4), \"s\")\n\n    # Evaluate classifier\n    print(f\"The best parameters are {clf.best_params_} (score = {clf.best_score_:2f}) \\n\")\n    print(\"Grid scores on training set:\", \"\\n\")\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n    \n    # Save predictions and classifier\n    preds[f'{name}'] = predictions\n    clf_trained_dict[f'{name}'] = clf","4ea38e6f":"# Train and submit a RBF SVM classfier using best parameters found\nscaler = StandardScaler()\n\nX_test_scaled = scaler.fit_transform(X_test)\nclf = clf_trained_dict[\"RBF SVM\"]\ntest_preds = clf.predict(X_test_scaled)\n\noutput = pd.DataFrame({'PassengerId': y_test_ids,\n                       'Survived': test_preds})\noutput.to_csv('submission.csv', index=False)","429ec3a9":"# Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nThis notebook demonstrates how we can find an optimal classfier for the Titanic problem by cross-validated grid-search over a parameter grid. It performs some simple feature exploration and encoding, before specifying 10 different classfiers and their parameters for an exhaustive search.","a9160d0b":"![](https:\/\/i.imgur.com\/nRh1GdK.jpg)","fc2f0931":"Finally we use the classifer that did the best job to make predictions on our test set. Note that it has already been fit at this point.","da180e68":"# Feature encoding\n\nWe do some simple handling of missing data values and categorical columns in the training and test set.","0f979706":"# Exhaustive Grid Search\n\nWe create 10 different classifers for an exhaustive grid search using GridSearchCV. The apparent best classifier is then used to make predictions on the test set, which yields a decent accuracy without further feature engineering. Find more inspiration and details here:\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html"}}