{"cell_type":{"8091af34":"code","80d89a65":"code","72c5f863":"code","f0556a32":"code","db01f511":"code","aec2246d":"code","0696ac1d":"code","67dc2c1b":"code","8bc0ed2d":"code","fdd8c3cd":"code","351d70c5":"code","d8da6881":"code","c1208027":"code","3cc61b88":"code","e72ed268":"code","6aa12883":"code","2366b429":"code","422e0bc4":"code","9c7d6e22":"code","d4305f5c":"code","3b712c92":"code","413e55ef":"code","9151aae2":"markdown","802fee34":"markdown","7f0cbbfe":"markdown","a2b0806c":"markdown","1f6b601a":"markdown","b11258ce":"markdown","aebb4cb6":"markdown","b93c1246":"markdown","14bb1a96":"markdown","c0b481c2":"markdown","6771af35":"markdown","9ceb56e3":"markdown","d1149afb":"markdown"},"source":{"8091af34":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","80d89a65":"train_df=pd.read_csv('emotions.csv')\ntrain_df.head()","72c5f863":"train_df.describe","f0556a32":"# Data contains variables: fft(Fast fourier Tranform), correlate, entropy, logm, eigen, covmat, min_q, max_q\n# moments, mean, stddev\n# All are numerical type variable (float)\n#output is Label : Postive\/Negative\/Neutral\n","db01f511":"#class distribution from column label (Output is label)\nplt.figure(figsize=(12,5))\nsns.countplot(x=train_df.label, color='red')\nplt.title('Brain Wave Data', fontsize=14)\nplt.xlabel('Class Label', fontsize=14)\nplt.ylabel('Class count', fontsize=14)","aec2246d":"#Null data\ntrain_df.isnull().sum()","0696ac1d":"label_df=train_df['label']\ntrain_df.drop('label', axis=1, inplace=True)\ntrain_df.head()","67dc2c1b":"#Using cross validation (10 fold in this case)\n#Pipeline based approach\n#No of dimensions are high. Hence we will start with random forest classifier which works well on high-dimension data\n#Since its probablity based classifier, no pre-processing stages like scaling or noise removal are required\n#not affected by scale factors","8bc0ed2d":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n\nmodel_randomForest=Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores=cross_val_score(model_randomForest, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Random Forest = ', scores.mean())","fdd8c3cd":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n\nmodel_logisticRegression=Pipeline(steps=[('scalar', StandardScaler()),\n                                         ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores=cross_val_score(model_logisticRegression, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regression= ', scores.mean())","351d70c5":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(train_df)\npca=PCA(n_components=20)\npca_vectors=pca.fit_transform(scaled_df)\nfor index, var in enumerate(pca.explained_variance_ratio_):\n    print(\"Explained variance ratio by Principal Component \", (index+1) ,\" : \" , var)","d8da6881":"#Using mathematical mapping 2549 variables mapped to 20 variables\n#Of 2549 variables, 10 are of most importance","c1208027":"plt.figure(figsize=(25,8))\nsns.scatterplot(x=pca_vectors[:,0], y=pca_vectors[:,1],\n               hue=label_df)\nplt.title('PC V\/s Class', fontsize=14)\nplt.xlabel('PC 1', fontsize=14)\nplt.ylabel('PC 2', fontsize=14)\nplt.xticks(rotation='vertical');","3cc61b88":"# it can be seen that if we use Logistic regression the first classifier will seperate NEUTRAL class from other two\n# and the second classifier will seperate NEGATIVE and POSITIVE\n# Applying Logistic regression model on 2 main PCs","e72ed268":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_lg_pca=Pipeline(steps=[('scaler', StandardScaler()),\n                            ('pca', PCA(n_components=2)),\n                            ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga',max_iter=200 ))])\nscores=cross_val_score(model_lg_pca, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regression :', scores.mean())","6aa12883":"# Taking 10 PCs and running the model ","2366b429":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_lg_pca_10=Pipeline(steps=[('Scaler', StandardScaler()),\n                               ('pca', PCA(n_components=10)),\n                               ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\n\nscores=cross_val_score(model_lg_pca_10, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regressionwith 10 PCs :', scores.mean())","422e0bc4":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nmodel_mlp=Pipeline(steps=[('scaler', StandardScaler()),\n                         ('mlp_classifier', MLPClassifier(hidden_layer_sizes=(1275, 637)))])\nscores=cross_val_score(model_mlp, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for ANN Classifier: ', scores.mean())","9c7d6e22":"# General convention is to start with 50% of the data size for the first hidden layer \n# and 50% of previous size in subsequent layer \n# Number of hidden layers can be taken as a hyper-parameter and can be used to tune for better accuracy\n# Hidden layers in this ccase is 2\n# Or number of hidden neurons =  average of the input and output layers summed together.\n# The upper bound on the number of hidden neurons that won't result in over-fitting is: \ud835\udc41\u210e=\ud835\udc41\ud835\udc60\/(\ud835\udefc\u2217(\ud835\udc41\ud835\udc56+\ud835\udc41\ud835\udc5c))\n# \ud835\udc41\ud835\udc56= number of input neurons.\n# \ud835\udc41\ud835\udc5c= number of output neurons.\n# \ud835\udc41\ud835\udc60= number of samples in training data set.\n# \u03b1= an arbitrary scaling factor usually 2-10.","d4305f5c":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_SVM=Pipeline(steps=[('Scaler', StandardScaler()),\n                         ('svm', LinearSVC())])\nscores=cross_val_score(model_SVM, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Linear SVM :', scores.mean())\n","3b712c92":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport xgboost as xgb\n\nmodel_xgb=Pipeline(steps=\n                   [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\nscores=cross_val_score(model_xgb, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Extreme Gradient Boosting is :', scores.mean())","413e55ef":"# xgboost performs well in GPU Machines\n# os has been imported due to dead kernel problem\n# CONCLUSIONS\n# 1. For Accuracy XGBoost is most favourable\n# 2. Random FOrest is a perfect choice if \"time taken\" is also considered\n# 3. Simple classifiers like Logistic regression can give better accuracy with poper feauture engineering\n# 4. other classifiers don't need much feauture engineering effort","9151aae2":"# Linear Support Vector Machines Classifier","802fee34":"# Logistic Regression Classifier","7f0cbbfe":"# Random Forest Classifier","a2b0806c":"# PCA ","1f6b601a":"### Accuracy is good (97%) but time taken increases","b11258ce":"### Accuracy little less than Random Forest Classifier but more time efficient than ANN","aebb4cb6":"### Accuracy is maximum for XGBoost but time taken is quite high.\nhigh running time due to internal ensemble model structure","b93c1246":"#### Accuracy less than Random Forest Classifier and time taken is higher","14bb1a96":"### Accuracy reduced but time improved sigificantly for Logistic Regression model","c0b481c2":"# Artifical Neural Network Classifier (ANN)","6771af35":"###### Accuracy is good and total time taken is short (4.34 secs)\n","9ceb56e3":"### Improved Accuracy of 86% compared to 2 PC cases with marginal increase in time taken","d1149afb":"# Extreme Gradient Boosting Classifier (XGBoost)"}}