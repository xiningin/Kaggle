{"cell_type":{"7663d6f2":"code","07f664e4":"code","41b100fc":"code","6fa53a66":"code","e3627dff":"code","2ecd2818":"code","d7a5d629":"code","63125094":"code","e3bfcd72":"code","fbb67763":"code","9a41a0f8":"code","e3167620":"code","ea871f20":"code","8d758657":"code","da9b6368":"code","45cb90d5":"code","1e7c1f12":"code","4644df1d":"code","849479cd":"code","f839f5cd":"code","709d013c":"code","25ac2d22":"code","0421ff0d":"code","56fff115":"code","5ebe965c":"code","ff942276":"code","8fae3000":"code","1010e653":"code","2b58c2e6":"code","ddcdbf1e":"code","4e34e07b":"code","878fd31a":"code","72117026":"code","9586a39f":"code","f4267a6c":"code","d929ea75":"code","fc401bd5":"code","8afc5e00":"code","104eb14b":"code","9f06194e":"code","381bfed4":"code","fc0d2400":"code","1441b42d":"code","9bae80ac":"code","85a546d7":"markdown","cc28b224":"markdown","d03ce090":"markdown","02840c46":"markdown","17470e47":"markdown","146cc5d7":"markdown","5b6744c0":"markdown","f98831c3":"markdown","6190ff21":"markdown","cf689111":"markdown","45490756":"markdown","5d34e6b4":"markdown","d1cdb9af":"markdown","943f87b8":"markdown","61b7355a":"markdown","8eb8ab2c":"markdown","c86ffa2e":"markdown","916d85b1":"markdown","60d13b29":"markdown","a0302c57":"markdown","934c8a5b":"markdown","2d5bfa3e":"markdown","31b1f534":"markdown","54d2e241":"markdown","575014fd":"markdown","4301e33e":"markdown","67d5751f":"markdown","a2e78a84":"markdown"},"source":{"7663d6f2":"# First let's import Tensorflow\nimport tensorflow as tf","07f664e4":"# Now import some additional libraries\nfrom numpy import zeros\nimport numpy as np\nfrom datetime import datetime","41b100fc":"# Benchmark function for dataset\nimport time\ndefault_timeit_steps = 1000\nBATCH_SIZE = 1\n\n# Iterate through each element of a dataset. An element is a pair \n# of image and label.\ndef timeit(ds: tf.data.TFRecordDataset, steps: int = default_timeit_steps, \n           batch_size: int = BATCH_SIZE) -> None:\n    \n    start = time.time()\n    it = iter(ds)\n    \n    for i in range(steps):\n        batch = next(it)\n        \n        if i%10 == 0:\n            print('.',end='')\n    print()\n    end = time.time()\n    \n    duration = end-start\n    print(\"{} batches: {} s\".format(steps, duration))\n    print(\"{:0.5f} Images\/s\".format(batch_size*steps\/duration))","6fa53a66":"# Global variables\n\n# Paths where images are located\nFILENAMES = 'gs:\/\/tf-data-pipeline\/*\/*.jpg'\n\n# Paths where labels can be parsed\nFOLDERS = 'gs:\/\/tf-data-pipeline\/*'\n\n# Image resolution and shape\nRESOLUTION = (224,224)\nIMG_SHAPE=(224,224,3)\n\n# tf.data AUTOTUNE\nAUTOTUNE = tf.data.experimental.AUTOTUNE","e3627dff":"# Get labels from folder's name and create a map to an ID\ndef get_label_map(path: str) -> (dict, dict):\n    #list folders in this path\n    folders_name = tf.io.gfile.glob(path)\n\n    labels = []\n    for folder in folders_name:\n        labels.append(folder.split(sep='\/')[-1])\n\n    # Generate a Label Map and Interted Label Map\n    label_map = {labels[i]:i for i in range(len(labels))}\n    inv_label_map = {i:labels[i] for i in range(len(labels))}\n    \n    return label_map, inv_label_map","2ecd2818":"# One hot encode the image's labels\ndef one_hot_encode(label_map: dict, filepath: list) -> dict:\n    labels = dict()\n    \n    for i in range(len(filepath)):\n        encoding = zeros(len(label_map), dtype='uint8')\n        encoding[label_map[filepath[i].split(sep='\/')[-2]]] = 1\n        \n        labels.update({filepath[i]:list(encoding)})\n    \n    return labels","d7a5d629":"label_map, inv_label_map = get_label_map(FOLDERS)","63125094":"list(label_map.items())[:5]","e3bfcd72":"# List all files in bucket\nfilepath = tf.io.gfile.glob(FILENAMES)\nNUM_TOTAL_IMAGES = len(filepath)","fbb67763":"# Split the features (image path) from labels\ndataset = one_hot_encode(label_map, filepath)\ndataset = [[k,v] for k,v in dataset.items()]\n\nfeatures = [i[0] for i in dataset]\nlabels = [i[1] for i in dataset]","9a41a0f8":"# Create Dataset from Features and Labels\ndataset = tf.data.Dataset.from_tensor_slices((features, labels))","e3167620":"# Example of one element of the dataset\n# At this point we have a dataset containing the path and labels of an image\nprint(next(iter(dataset)))","ea871f20":"# Download image bytes from Cloud Storage\ndef get_bytes_label(filepath, label):\n    raw_bytes = tf.io.read_file(filepath)\n    return raw_bytes, label","8d758657":"# Preprocess Image\ndef process_image(raw_bytes, label):\n    image = tf.io.decode_jpeg(raw_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize(image, (224,224))\n    \n    return image, label","da9b6368":"# Map transformations for each element inside the dataset\n# Maps are separated as IO Intensive and CPU Intensive\ndef build_dataset(dataset, batch_size=BATCH_SIZE, cache=False):\n    \n    dataset = dataset.shuffle(NUM_TOTAL_IMAGES)\n    \n    # Extraction: IO Intensive\n    dataset = dataset.map(get_bytes_label, num_parallel_calls=AUTOTUNE)\n\n    # Transformation: CPU Intensive\n    dataset = dataset.map(process_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=batch_size)\n    \n    if cache:\n        if isinstance(cache, str):\n            dataset = dataset.cache(filename=cache)\n        else:\n            dataset = dataset.cache()\n    \n    # Pipeline next iteration\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","45cb90d5":"# Apply transformations to the dataset with images paths and labels\ntrain_ds = build_dataset(dataset)","1e7c1f12":"local_ds = train_ds.take(1).cache().repeat()","4644df1d":"timeit(local_ds, 20000, batch_size=1)","849479cd":"# Iterate through this dataset for 1000 steps.\ntimeit(train_ds, batch_size=1, steps=1000)","f839f5cd":"# Memory\ntrain_cache_ds = build_dataset(dataset, cache=True)\ntimeit(train_cache_ds, batch_size=1, steps=50000)","709d013c":"# Local Cache File\ntrain_local_cache_ds = build_dataset(dataset, cache='.\/dog.tfcache', batch_size=1)\ntimeit(train_local_cache_ds, batch_size=1, steps=50000)","25ac2d22":"tf.summary.trace_off()\ntf.summary.trace_on(graph=False, profiler=True)\n\ntrain_ds = build_dataset(dataset)\ntimeit(train_ds, steps=1000)\n\ntf.summary.trace_export('Data Pipeline', profiler_outdir='\/home\/jupyter\/tensorflow-data-pipeline\/logs\/')","0421ff0d":"# Load the TensorBoard notebook extension.\n%load_ext tensorboard","56fff115":"# Start tensorboard inside one cell\n%tensorboard --logdir=\/home\/jupyter\/tensorflow-data-pipeline\/logs","5ebe965c":"# Function to download bytes from Cloud Storage\ndef get_bytes_label_tfrecord(filepath, label):\n    raw_bytes = tf.io.read_file(filepath)\n    return raw_bytes, label","ff942276":"# Preprocess Image\ndef process_image_tfrecord(raw_bytes, label):\n    image = tf.io.decode_jpeg(raw_bytes, channels=3)\n    image = tf.image.resize(image, (224,224), method='nearest')\n    image = tf.io.encode_jpeg(image, optimize_size=True)\n    \n    return image, label","8fae3000":"# Read images, preprocess and return a dataset\ndef build_dataset_tfrecord(dataset):\n    \n    dataset = dataset.map(get_bytes_label_tfrecord, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(process_image_tfrecord, num_parallel_calls=AUTOTUNE)\n    \n    return dataset","1010e653":"def tf_serialize_example(image, label):\n    \n    def _bytes_feature(value):\n        \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n        if isinstance(value, type(tf.constant(0))):\n            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n    def _float_feature(value):\n        \"\"\"Returns a float_list from a float \/ double.\"\"\"\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n    def _int64_feature(value):\n        \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))    \n    \n    def serialize_example(image, label):\n        \n        feature = {\n            'image': _bytes_feature(image),\n            'label': _int64_feature(label)\n        }\n\n        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n        \n        return example_proto.SerializeToString()\n    \n    tf_string = serialize_example(image, label)\n\n    return tf_string","2b58c2e6":"# Create TFRecord with `n_shards` shards\ndef create_tfrecord(ds, n_shards):\n\n    for i in range(n_shards):\n        batch = map(lambda x: tf_serialize_example(x[0],x[1]), ds.shard(n_shards, i)\n                    .apply(build_dataset_tfrecord)\n                    .as_numpy_iterator())\n        \n        with tf.io.TFRecordWriter('output_file-part-{i}.tfrecord'.format(i=i), 'GZIP') as writer:\n            print('Creating TFRecord ... output_file-part-{i}.tfrecord'.format(i=i))\n            for a in batch:\n                writer.write(a)","ddcdbf1e":"# We sharded into 4 files with 130MB each.\n# If the dataset is bigger, you can create more shards\ncreate_tfrecord(dataset, 4)","4e34e07b":"TFRECORDS = 'gs:\/\/renatoleite-nb\/tfrecords\/*'","878fd31a":"# Create a description of the features.\nfeature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n    'label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True)\n}","72117026":"@tf.function\ndef _parse_function(example_proto):\n    # Parse the input `tf.Example` proto using the dictionary above.\n    return tf.io.parse_single_example(example_proto, feature_description)","9586a39f":"# List all the TFRecords and create a dataset from it\nfilenames = tf.io.gfile.glob(TFRECORDS)\nfilenames_dataset = tf.data.Dataset.from_tensor_slices(filenames)","f4267a6c":"# Preprocess Image\n@tf.function\ndef process_image_tfrecord(record):  \n    image = tf.io.decode_jpeg(record['image'], channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    \n    label = record['label']\n    \n    return image, label","d929ea75":"# Create a Dataset composed of TFRecords (paths to bucket)\n@tf.function\ndef get_tfrecord(filename):\n    return tf.data.TFRecordDataset(filename, compression_type='GZIP', num_parallel_reads=AUTOTUNE)","fc401bd5":"def build_dataset_test(dataset, batch_size=BATCH_SIZE):\n    \n    dataset = dataset.interleave(get_tfrecord, num_parallel_calls=AUTOTUNE)\n    \n    # Transformation: IO Intensive \n    dataset = dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n\n    # Transformation: CPU Intensive\n    dataset = dataset.map(process_image_tfrecord, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size=batch_size)\n    \n    # Pipeline next iteration\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","8afc5e00":"test_ds = build_dataset_test(filenames_dataset, batch_size=32)","104eb14b":"timeit(test_ds, steps=20000, batch_size=32)","9f06194e":"def build_dataset_test(dataset, batch_size=BATCH_SIZE):\n    \n    dataset = dataset.interleave(get_tfrecord, num_parallel_calls=AUTOTUNE)\n\n    dataset = dataset.batch(batch_size=batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(process_image_tfrecord, num_parallel_calls=AUTOTUNE)\n\n    dataset = dataset.repeat()\n    # Pipeline next iteration\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","381bfed4":"@tf.function\ndef _parse_function(example_proto):\n    \n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True)\n    }\n    \n    # Parse the input `tf.Example` proto using the dictionary above.\n    return tf.io.parse_example(example_proto, feature_description)","fc0d2400":"# Preprocess Image\n@tf.function\ndef process_image_tfrecord(record):\n    \n    image = tf.map_fn(tf.io.decode_jpeg, record['image'], dtype=tf.uint8)\n    image = tf.map_fn(lambda image: \n                      tf.image.convert_image_dtype(image, dtype=tf.float32), image, dtype=tf.float32)\n    \n    label = record['label']\n    \n    return image, label","1441b42d":"test_ds = build_dataset_test(filenames_dataset, batch_size=32)","9bae80ac":"timeit(test_ds, steps=20000, batch_size=32)","85a546d7":"Next we define some preprocessing functions to:\n - Read the data from Cloud Storage\n - Decode JPEG\n - Convert image to a range between 0 and 1, as float\n - Resize image","cc28b224":"This execution exausted the memory of my host VM with 16GB of RAM and gave the following error.\n\n>ResourceExhaustedError: OOM when allocating tensor with shape[688,813,3] and type float on \/job:localhost\/replica:0\/task:0\/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image\/Cast}}]]","d03ce090":"When we cache after taking an element, the preprocess won't be repeated.","02840c46":"## Batch before we Map!\n\nOne last performance optimization we can try is to batch the elements before applying the map transformation.\nThis technique is called \"Vectorizing maps\" which is recommended to user-defined function (that is, have it operate over a batch of inputs at once) and apply the batch transformation before the map transformation. \n\nLet's redefine the build_dataset and the preprocess map transformation:","17470e47":"This new benchmark gives us around 2100 images per second, a much better version of the original pipeline developed (reading images individually).   \nTo speedup the training process and utilized better your resources like GPUs and TPUs, it is critical to build a very efficient data pipeline. otherwise this can quickly become a bottleneck in you training loop.\n\nWe could also try to cache the data at different stages, like the example bellow (from tensorflow documentation), but I am assuming the data won't fit into the host VM memory, so it is needed to read Cloud Storage each epoch.\n\n> dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)","146cc5d7":"### Any performance improvement?\n\nUsing the memory of the host VM as a cache mechanism, we exausted all the resources without improving the throughtput of the dataset.\\\nWhile using a local storage we could cache all the data, but no performance gain was perceived.\n\nTo solve this problem we can follow some best practices for designing a performant TensorFlow data input pipeline (from the Tensorflow documentation [1]):\n\n - Use the prefetch transformation to overlap the work of a producer and consumer.\n - Parallelize the data reading transformation using the interleave transformation.\n - Parallelize the map transformation by setting the num_parallel_calls argument.\n - Use the cache transformation to cache data in memory during the first epoch\n - Vectorize user-defined functions passed in to the map transformation\n - Reduce memory usage when applying the interleave, prefetch, and shuffle transformations.\n \nBut before we continue, let's do some tracing to understand what is going on.\n\nI would add another factor:\n - Bundle your data, preprocessed if possible, in TFRecord files.\n\n[1] https:\/\/www.tensorflow.org\/guide\/data_performance\n","5b6744c0":"## Using TF.Record for speedup de reading process\n\nUp to now the images were read one by one, which proved to be a very inefficient process. \\\nTo mitigate this problem, one solution is to preprocess and write the images and labels to TFRecord files.\n\nWe can get the motivation on why creating TFRecode files with our images would be a good idea:\n\n> To read data efficiently it can be helpful to serialize your data and store it in a set of files (100-200MB each) that can each be read linearly. This is especially true if the data is being streamed over a network. This can also be useful for caching any data-preprocessing.\n\n> The TFRecord format is a simple format for storing a sequence of binary records.\n\nIn the following steps, the images are preprocessed and written to TFRecords.\nThe following steps are followed:\n - Read the data from Cloud Storage\n - Decode the JPEG and resize the image\n - Encode the JPEG\n - Serialize the images into Bytes (tf.train.BytesList) and Labels into Ints (tf.train.Int64List)\n - Create a tf.Example with this two components and return a serialized string.","f98831c3":"## Ok, let's put some local cache in action\n\ntf.data.Dataset implements a cache function. \n\nIf no parameter is passad to the cache function, it uses the memory of the host to cache the data.\nThe problem is if your dataset is bigger than your host memory and you can't cache the epoch in memory. In this case the cache won't help and we still have an IO bottleneck.\nIt is also possible to cache the images in a local storage for reuse in future epochs.\n\nFirst let's test the throughput using cache in memory and than in as a local file.\nNote that we need to pass at least twice through the dataset in order to the cache to have any effect.","6190ff21":"Note that our preprocess function don't resize the image anymore.\\\nThis is because we store the images in the TFRecord files already resized.\n\nThe TFRecordDataset has a flag \"num_parallel_reads\" to parallelize the number of reads by the runtime. \\\nThis flag is set to AUTOTUNE to let Tensorflow decide how many threads are necessary to optimize the process.","cf689111":"## First Attempt, without caching\n\nIn this first benchmark no caching mecanism is used and the images are read one by one from the bucket.\n\nThe biggest problem here is to read 1000's of files one by one.\nSince there are thousands of images, this process can take longer. From the tensorflow documentation: \n> In a real-world setting, the input data may be stored remotely (for example, GCS or HDFS). A dataset pipeline that works well when reading data locally might become bottlenecked on I\/O when reading data remotely because of the following differences between local and remote storage:\n\n>  - Time-to-first-byte: Reading the first byte of a file from remote storage can take orders of magnitude longer than from local storage.\n>  - Read throughput: While remote storage typically offers large aggregate bandwidth, reading a single file might only be able to utilize a small fraction of this bandwidth.\n\n\nLet's call our \"timeit\" function to measure the time needed for the load. ","45490756":"With this new approach we could improve by ~15% the throughput performance of our input pipeline, reaching ~2300 images\/sec.","5d34e6b4":"To read the Serialized data inside each TFRecord, it is necessary to pass a description of the features (image and label) previously encoded as a tf.feature. \nTo do so, create a dictionary describing each component we will read.","d1cdb9af":"The new function to build the dataset has the following changes:\n - Use of \"interleave\" to parallelize the opening of files. From tensorflow documentation:\n > To mitigate the impact of the various data extraction overheads, the tf.data.Dataset.interleave transformation can be used to parallelize the data loading step, interleaving the contents of other datasets (such as data file readers). The number of datasets to overlap can be specified by the cycle_length argument, while the level of parallelism can be specified by the num_parallel_calls argument. Similar to the prefetch transformation, the interleave transformation supports tf.data.experimental.AUTOTUNE which will delegate the decision about what level of parallelism to use to the tf.data runtime.\n \n - Use of the \"\\_parse_function\" to extract and deserialize the image and label.\n - Lighter version of preprocess, without resizing it (images are stored in TFRecord file already resized).","943f87b8":"## Benchmark function\n\nThe benchmark will measure the number of images ingested (read) per second from Cloud Storage to the host virtual machine. \nThere are several ways to implement this calculation, but a simple function was used to iterate through the dataset and measure the time.\n\nThe following python function ('timeit' function) from Tensorflow documentation [1] (as of 03\/18\/2020 - version 2.1) is used.\nSince tf.data.Dataset implements \\__iter__, it is possible to iterate on this data to observe the progression.\n\n[1] https:\/\/www.tensorflow.org\/tutorials\/load_data\/images#performance","61b7355a":"Notice that it is necessary to redefine the \\_parce_function and process_image_tfrecord to receive a batch of elements and process all of them.","8eb8ab2c":"First all the files were listed inside the specified bucketand created a dataset using \".from_tensorf_slices\", but it would be possible to create a TFRecordDataset directly from this listing.  \nThe reason this was done is because the dataset with listing is used later.","c86ffa2e":"# Consuming the TFRecord and Re-Running the Benchmark\n\nThe TFRecords are saved in the local filesystem. To continue our benchmark, it is necessary to copy the files to a bucket in Cloud Storage. \nThe files were copied to the following path:","916d85b1":"#### Building the dataset\n\nFrom the Dataset already built with image paths and labels, the preprocessing functions are applied for each element to download the bytes from Cloud Storage and apply some transformations to the images.\nThese steps are only performed when the dataset is iterated.\n\nAt this point, all the steps are executed while streaming the data, including:\n - IO intensive operations like download de images (get_bytes_label)\n - CPU intensive operations like decode and resize the image (process_image)\n\nSome observations for the code below:\n - \"num_parallel_calls = tf.data.experimental.AUTOTUNE\" was used to let tensorflow runtime decide the best parametrization for its functions.\n - \"dataset.cache\" was implemented, but as we are reading a large amount of data, this may not fit into memory and become impossible to use.\n - \"dataset.prefetch\" allows buffering of elements in order to increase performance.","60d13b29":"<table style=\"width:100%\">\n  <tr>\n    <th>High Level View<\/th>\n    <th>Zoom View<\/th> \n  <\/tr>\n  <tr>\n    <td><img src=\"https:\/\/storage.cloud.google.com\/renatoleite-nb\/images\/trace1.png\"><\/td>\n    <td><img src=\"https:\/\/storage.cloud.google.com\/renatoleite-nb\/images\/trace2.png\"><\/td>\n  <\/tr>\n<\/table>","a0302c57":"Reading files individually took a long time and is far from an ideal this throughput.","934c8a5b":"Two threads were created to read the files in parallel.\nWe won't go into much details on Tensorboard, but it would be useful to analyse the time each operation took to execute.\n\nThe next step is to bundle together all the images in a TFRecord file, so let's do it.","2d5bfa3e":"## Benchmark baseline\n\nLet's first create a baseline for our benchmark with a local cache to understand how fast we can go with this process.\nTo do that, read a single file, cache it in memory and repeate forever.\nWith this dataset, let's run our benchmark for 20000 steps.","31b1f534":"This test achieved a pick throughput of ~10500 images per second.","54d2e241":"Then a function can parse an example from the TFRecord, using the description created before.","575014fd":"#### tip: Performance analysis with Tensorboard\n\nIf you want to go deeper and investigate why the performance of your benchmark if not going well, you can trace the tensorflow ops to see whats going on. \\\nAs we are not running a model training loop, we can start tracing individually for this operation.","4301e33e":"## Let's create the Dataset using tf.data - Reading images individually\n\nAll the images are located in a bucket in Google Cloud Storage (example: gs:\/\/cloud_bucket\/label\/image.jpeg).\nLabels are extracted (parsed) from the image name.\n\nIn this first step, the dataset is created from the images file paths (gs:\/\/...), and labels are extracted and one-hot encoded.\n\nThis dataset maps each image in the bucket individually.","67d5751f":"# Building High Performance Data Pipelines with tf.Data and Google Cloud Storage\n\nThis article goes through the steps of building a high performance data input pipeline using Tensorflow and Google Cloud Storage.\nThe concepts and techniques are evolved at each step, going from the slowest to the fastest solution.\n\nThis article uses the Stanford Dogs Dataset [1] with ~20000 images and 120 classes.\n\n[1] https:\/\/www.kaggle.com\/jessicali9530\/stanford-dogs-dataset","a2e78a84":"The following function shards the dataset into batches of images and labels.\nFor each shard, the images and labels are serialized and written to the TFRecord file.\n\nThe TFRecordWriter allows the compression of files to some formats. One chosen here is GZIP."}}