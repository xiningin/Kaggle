{"cell_type":{"ddf56493":"code","10b490a8":"code","67230325":"code","0c58cf6a":"code","5484a1cb":"code","7ef54a6c":"code","406219d6":"code","17ef51d8":"code","854e934f":"code","44c320c9":"code","81c54074":"code","0bad3b2d":"code","df628fcb":"code","a0f6af1c":"code","b7302db6":"code","8c500a23":"code","704bde68":"code","3a8b4c8a":"code","a3a1ad49":"code","0ba57a16":"code","34814503":"code","e22ed6ea":"code","bc88a5ff":"markdown","cbea1a74":"markdown","4297aee4":"markdown","56998390":"markdown","e14a7ecd":"markdown","6779667d":"markdown","8ef47b22":"markdown"},"source":{"ddf56493":"#Credits: https:\/\/www.kaggle.com\/pestipeti\/competition-metric-map-0-4","10b490a8":"!pip install pycocotools","67230325":"import pandas as pd\nimport numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval","0c58cf6a":"study_level_df = pd.read_csv(\"\/kaggle\/input\/siim-covid19-detection\/train_study_level.csv\")","5484a1cb":"study_level_df.head()","7ef54a6c":"names = np.array(['negative', 'typical', 'indeterminate', 'atypical'])\nstudy_level_df['class_id'] = np.where(study_level_df.iloc[:,1:])[1]\nstudy_level_df['class_name'] = [names[i] for i in study_level_df['class_id'].values]","406219d6":"study_level_df[['x_min','y_min', 'x_max', 'y_max']] = np.array([0.,0.,1.,1.])","17ef51d8":"study_level_df['image_id'] = study_level_df['id']","854e934f":"study_level_df.head()","44c320c9":"class VinBigDataEval:\n    \"\"\"Helper class for calculating the competition metric.\n    \n    You should remove the duplicated annoatations from the `true_df` dataframe\n    before using this script. Otherwise it may give incorrect results.\n\n        >>> vineval = VinBigDataEval(valid_df)\n        >>> cocoEvalResults = vineval.evaluate(pred_df)\n\n    Arguments:\n        true_df: pd.DataFrame Clean (no duplication) Training\/Validating dataframe.\n\n    Authors:\n        Peter (https:\/\/kaggle.com\/pestipeti)\n\n    See:\n        https:\/\/www.kaggle.com\/pestipeti\/competition-metric-map-0-4\n\n    Returns: None\n    \n    \"\"\"\n    def __init__(self, true_df):\n        \n        self.true_df = true_df\n\n        self.image_ids = true_df[\"image_id\"].unique()\n        self.annotations = {\n            \"type\": \"instances\",\n            \"images\": self.__gen_images(self.image_ids),\n            \"categories\": self.__gen_categories(self.true_df),\n            \"annotations\": self.__gen_annotations(self.true_df, self.image_ids)\n        }\n        \n        self.predictions = {\n            \"images\": self.annotations[\"images\"].copy(),\n            \"categories\": self.annotations[\"categories\"].copy(),\n            \"annotations\": None\n        }\n\n        \n    def __gen_images(self, image_ids):\n        print(\"Generating image data...\")\n        results = []\n\n        for idx, image_id in enumerate(image_ids):\n\n            # Add image identification.\n            results.append({\n                \"id\": idx,\n            })\n            \n        return results\n    \n    \n    def __gen_categories(self, df):\n        print(\"Generating category data...\")\n        \n        if \"class_name\" not in df.columns:\n            df[\"class_name\"] = df[\"class_id\"]\n        \n        cats = df[[\"class_name\", \"class_id\"]]\n        cats = cats.drop_duplicates().sort_values(by='class_id').values\n        \n        results = []\n        \n        for cat in cats:\n            results.append({\n                \"id\": cat[1],\n                \"name\": cat[0],\n                \"supercategory\": \"none\",\n            })\n            \n        return results\n\n    \n    def __gen_annotations(self, df, image_ids):\n        print(\"Generating annotation data...\")\n        k = 0\n        results = []\n        \n        for idx, image_id in enumerate(image_ids):\n\n            # Add image annotations\n            for i, row in df[df[\"image_id\"] == image_id].iterrows():\n\n                results.append({\n                    \"id\": k,\n                    \"image_id\": idx,\n                    \"category_id\": row[\"class_id\"],\n                    \"bbox\": np.array([\n                        row[\"x_min\"],\n                        row[\"y_min\"],\n                        row[\"x_max\"],\n                        row[\"y_max\"]]\n                    ),\n                    \"segmentation\": [],\n                    \"ignore\": 0,\n                    \"area\":(row[\"x_max\"] - row[\"x_min\"]) * (row[\"y_max\"] - row[\"y_min\"]),\n                    \"iscrowd\": 0,\n                })\n\n                k += 1\n                \n        return results\n\n    def __decode_prediction_string(self, pred_str):\n        data = list(map(float, pred_str.split(\" \")))\n        data = np.array(data)\n\n        return data.reshape(-1, 6)    \n    \n    def __gen_predictions(self, df, image_ids):\n        print(\"Generating prediction data...\")\n        k = 0\n        results = []\n        \n        for i, row in df.iterrows():\n            \n            image_id = row[\"image_id\"]\n            preds = self.__decode_prediction_string(row[\"PredictionString\"])\n\n            for j, pred in enumerate(preds):\n\n                results.append({\n                    \"id\": k,\n                    \"image_id\": int(np.where(image_ids == image_id)[0]),\n                    \"category_id\": int(pred[0]),\n                    \"bbox\": np.array([\n                        pred[2], pred[3], pred[4], pred[5]\n                    ]),\n                    \"segmentation\": [],\n                    \"ignore\": 0,\n                    \"area\": (pred[4] - pred[2]) * (pred[5] - pred[3]),\n                    \"iscrowd\": 0,\n                    \"score\": pred[1]\n                })\n\n                k += 1\n                \n        return results\n                \n    def evaluate(self, pred_df, n_imgs = -1):\n        \"\"\"Evaluating your results\n        \n        Arguments:\n            pred_df: pd.DataFrame your predicted results in the\n                     competition output format.\n\n            n_imgs:  int Number of images use for calculating the\n                     result.All of the images if `n_imgs` <= 0\n                     \n        Returns:\n            COCOEval object\n        \"\"\"\n        \n        if pred_df is not None:\n            self.predictions[\"annotations\"] = self.__gen_predictions(pred_df, self.image_ids)\n\n        coco_ds = COCO()\n        coco_ds.dataset = self.annotations\n        coco_ds.createIndex()\n        \n        coco_dt = COCO()\n        coco_dt.dataset = self.predictions\n        coco_dt.createIndex()\n        \n        imgIds=sorted(coco_ds.getImgIds())\n        \n        if n_imgs > 0:\n            imgIds = np.random.choice(imgIds, n_imgs)\n\n        cocoEval = COCOeval(coco_ds, coco_dt, 'bbox')\n        cocoEval.params.imgIds  = imgIds\n        cocoEval.params.useCats = True\n        cocoEval.params.iouType = \"bbox\"\n        cocoEval.params.iouThrs = np.array([0.5])\n\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n        \n        return cocoEval","81c54074":"# df = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")\n# df.fillna(0, inplace=True)\n# df.loc[df[\"class_id\"] == 14, ['x_max', 'y_max']] = 1.0\n\n# df.head()","0bad3b2d":"# # Removing duplications! DO NOT USE THIS in your training!!!\n# df = df.groupby(by=['image_id', 'class_id']).first().reset_index()","df628fcb":"# You only need to run this once.\nvineval = VinBigDataEval(study_level_df)","a0f6af1c":"# Predicting with 1 class\n# {0: 'negative', 1: 'typical', 2: 'indeterminate', 3: 'atypical'}\npred_df = study_level_df[[\"image_id\"]]\npred_df = pred_df.drop_duplicates()\nclass_id = 0\npred_df[\"PredictionString\"] = f\"{class_id} 1.0 0 0 1 1\"\npred_df.reset_index(drop=True, inplace=True)\n\npred_df.head()","b7302db6":"# You should evaluate after every n epochs.\ncocoEvalRes = vineval.evaluate(pred_df)","8c500a23":"cocoEvalRes.stats[1]*2\/3 ","704bde68":"class_probas = pd.value_counts(study_level_df['class_id'], normalize=True); class_probas","3a8b4c8a":"# Predicting with all classes\ndfs = []\nfor class_id in range(4):\n    pred_df = study_level_df[[\"image_id\"]]\n    pred_df = pred_df.drop_duplicates()\n    proba = class_probas[class_id]\n    pred_df[\"PredictionString\"] = f\"{class_id} {proba} 0 0 1 1\"\n    pred_df.reset_index(drop=True, inplace=True)\n    dfs.append(pred_df)\npred_df = pd.concat(dfs)\npred_df.head()","a3a1ad49":"# You should evaluate after every n epochs.\ncocoEvalRes = vineval.evaluate(pred_df)","0ba57a16":"cocoEvalRes.stats[1]*2\/3 ","34814503":"%%capture\nstats = []\n\n# Recalculate the validation score using randomly selected images\nfor i in range(100):\n    cocoEvalRes = vineval.evaluate(pred_df = None, n_imgs = 300)\n    stats.append(cocoEvalRes.stats[0])\n    \navg = np.array(stats).mean()","e22ed6ea":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=[x for x in range(len(stats))], y=stats, mode=\"markers\", name=\"Stats\"))\nfig.add_trace(go.Scatter(x=[0, 100], y=[avg, avg], mode=\"lines\", name=\"Mean\"))\nfig.add_trace(go.Scatter(x=[0, 100], y=[0.052, 0.052], mode=\"lines\", name=\"Public Baseline\"))\n\nfig.update_yaxes(\n    range=[0.03, 0.07]\n)\n\nfig.update_layout(title='Results of mAP@0.4 (randomly selected 300 images)',\n                  yaxis_title='Score',\n                  xaxis_title='')\n\nfig.show()","bc88a5ff":"### Predict single class for each study","cbea1a74":"Probability doesn't matter when predicting all classes since all IOUs = 1.0 and 1 box is TP and remaining 3 all always FP.","4297aee4":"#### Recalculating with random samples","56998390":"We get same results as LB probing for negative which is 0.050. We need to multiply mAP for study by 4\/6 to get contributions of 4 study classes to final LB score. You can check this [discussion](https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/244066) for LB probing to study level predictions.","e14a7ecd":"### Predict all classes for each study","6779667d":"# Usage","8ef47b22":"# Competiton metric calculator\n\n> The challenge uses the standard [PASCAL VOC 2010 mean Average Precision (mAP)](http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2010\/devkit_doc_08-May-2010.pdf) at IoU > 0.5."}}