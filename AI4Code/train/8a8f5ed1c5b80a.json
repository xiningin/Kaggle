{"cell_type":{"7cffb3bf":"code","1b1651db":"code","f69662c6":"code","7aa7d80b":"code","a1cf7f9e":"code","be2307dd":"code","aea44f64":"code","28567ec8":"code","e4492ef1":"code","1b06e62f":"code","7e95f5b1":"code","6bffb724":"code","021994c8":"code","496a9f58":"code","1097dd30":"code","bb999107":"code","495ca2b3":"code","4a4342c7":"code","f804bd74":"code","aa21f255":"code","d127447d":"code","f262935e":"code","2460ab2e":"code","4f2ac7b0":"code","fc13a158":"code","ab56f1c3":"code","0c67b875":"code","6f66ce15":"code","a7d12f88":"code","7d39220b":"code","0494eb18":"code","2a5075b3":"code","05408738":"code","53e6d3b8":"code","08139cf3":"code","9bd58062":"code","522918fc":"code","8188fb5c":"code","7927b28e":"code","58c9719b":"code","009e8b00":"code","aac4976e":"code","67d2e9c4":"code","a5fa964c":"code","586fb404":"code","741808c9":"code","0374202d":"code","5cfef9f8":"code","ba6d2ebd":"code","c2d11f36":"code","335f563d":"code","6e953659":"code","954d7368":"code","6ebbdda8":"code","a7f01e54":"code","7df676ea":"code","0826e57c":"code","5ce1d6e7":"code","13da2e1c":"code","034946df":"code","201bf021":"code","47b8f580":"code","a49c4213":"code","bc586e12":"code","49c1e425":"code","562e826c":"code","b7bf57ab":"code","a3246463":"code","1aaf8e9b":"code","b8c0ccf2":"code","279b946e":"code","17668575":"code","75062545":"code","b30383ef":"code","5bdd68d8":"code","59bdf66d":"code","1f82e649":"code","c231bdb9":"code","c6a60aff":"code","348e26ef":"code","5b99be2d":"code","65c3abc2":"code","9221a0fd":"code","5693ba7c":"code","bb87abc2":"code","ea843fda":"markdown","9cdcda80":"markdown","3756cd8c":"markdown","7220bc76":"markdown","dbf4e354":"markdown","5a433a41":"markdown","fc63156b":"markdown","f94266a3":"markdown","19e6227a":"markdown","cdbd7be3":"markdown","9c0ee16f":"markdown","49a06b67":"markdown","64e990af":"markdown","45911b8f":"markdown","217998f0":"markdown","3d87746a":"markdown","5c6a5143":"markdown","64b4304e":"markdown","363adcf0":"markdown","f8602be3":"markdown","dadfde11":"markdown","bc81337b":"markdown","40e66cb2":"markdown","b193ff01":"markdown","fb45e193":"markdown","b94c0613":"markdown","6d81aff4":"markdown","2f75455d":"markdown","5958f8ca":"markdown","0a4553ac":"markdown","7ed2b3e5":"markdown"},"source":{"7cffb3bf":"# Importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1b1651db":"# Importing the dataset\n\ndataset = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","f69662c6":"# Lets look at the top 5 rows\ndataset.head()","7aa7d80b":"# Checking for null values\ndataset.isnull().sum()","a1cf7f9e":"# Feature Selection\n\nplt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","be2307dd":"# We will select only 3 features : time, ejection_fraction, serum_creatinine","aea44f64":"# Boxplot for ejection_fraction\n\nsns.boxplot(x = dataset.ejection_fraction, color = 'teal')\nplt.show()","28567ec8":"# We can see there are two outliers. Lets remove them","e4492ef1":"dataset[dataset['ejection_fraction']>=70]","1b06e62f":"dataset = dataset[dataset['ejection_fraction']<70]","7e95f5b1":"# Finding outliers in time","6bffb724":"sns.boxplot(x=dataset.time, color = 'teal')\nplt.show()","021994c8":"# No outliers in time.","496a9f58":"# Boxplot for ejection_fraction\nsns.boxplot(x=dataset.serum_creatinine, color = 'teal')\nplt.show()","1097dd30":"# Before dealing with outliers we require knowledge about the outlier, the dataset and possibly some domain knowledge.\n# Removing outliers without a good reason will not always increase accuracy. Without a deep understanding of what are the possible ranges that\n# exist within each feature, removing outliers becomes tricky.\n\n# When I researched a bit I found that all the values in serum_creatinine falls in possible range of values. So they are not outliers. \n# They are actual data points that helps in predicting DEATH_EVENT. ","bb999107":"# Distribution of Age\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['age'],\n    xbins=dict( # bins used for histogram\n        start=40,\n        end=95,\n        size=2\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='AGE DISTRIBUTION',\n    xaxis_title_text='AGE',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","495ca2b3":"# Hover over the graph to get the count of people of different age groups ","4a4342c7":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"age\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=dataset.columns, \n                   title =\"Distribution of AGE Vs DEATH_EVENT\", \n                   labels={\"age\": \"AGE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"}\n                  )\nfig.show()","f804bd74":"# Similarly lets get insights of other features as well","aa21f255":"# Distribution of creatinine_phosphokinase\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['creatinine_phosphokinase'],\n    xbins=dict( # bins used for histogram\n        start=23,\n        end=582,\n        size=15\n    ),\n    marker_color='#FE6F5E',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='CREATININE PHOSPHOKINASE DISTRIBUTION',\n    xaxis_title_text='CREATININE PHOSPHOKINASE',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","d127447d":"# Hover over the graph to get the count of people having creatinine phosphokinase levels at same range","f262935e":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"creatinine_phosphokinase\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=dataset.columns,\n                   title =\"Distribution of CREATININE PHOSPHOKINASE Vs DEATH_EVENT\", \n                   labels={\"creatinine_phosphokinase\": \"CREATININE PHOSPHOKINASE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","2460ab2e":"# Distribution of ejection_fraction\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['ejection_fraction'],\n    xbins=dict( # bins used for histogram\n        start=14,\n        end=80,\n        size=2\n    ),\n    marker_color='#A7F432',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='EJECTION FRACTION DISTRIBUTION',\n    xaxis_title_text='EJECTION FRACTION',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","4f2ac7b0":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"ejection_fraction\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=dataset.columns,\n                   title =\"Distribution of EJECTION FRACTION Vs DEATH_EVENT\", \n                   labels={\"ejection_fraction\": \"EJECTION FRACTION\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","fc13a158":"# Distribution of platelets\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['platelets'],\n    xbins=dict( # bins used for histogram\n        start=25000,\n        end=300000,\n        size=5000\n    ),\n    marker_color='#50BFE6',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='PLATELETS DISTRIBUTION',\n    xaxis_title_text='PLATELETS',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","ab56f1c3":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"platelets\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=dataset.columns,\n                   title =\"Distribution of PLATELETS Vs DEATH_EVENT\", \n                   labels={\"platelets\": \"PLATELETS\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","0c67b875":"# Distribution of serum_creatinine\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['serum_creatinine'],\n    xbins=dict( # bins used for histogram\n        start=0.5,\n        end=9.4,\n        size=0.2\n    ),\n    marker_color='#E77200',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='SERUM CREATININE DISTRIBUTION',\n    xaxis_title_text='SERUM CREATININE',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","6f66ce15":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"serum_creatinine\", color=\"DEATH_EVENT\", marginal=\"violin\", hover_data=dataset.columns,\n                   title =\"Distribution of SERUM CREATININE Vs DEATH_EVENT\", \n                   labels={\"serum_creatinine\": \"SERUM CREATININE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","a7d12f88":"# Distribution of serum_sodium\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = dataset['serum_sodium'],\n    xbins=dict( # bins used for histogram\n        start=113,\n        end=148,\n        size=1\n    ),\n    marker_color='#AAF0D1',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='SERUM SODIUM DISTRIBUTION',\n    xaxis_title_text='SERUM SODIUM',\n    yaxis_title_text='COUNT', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False },\n    template = 'plotly_dark'\n)\n\nfig.show()","7d39220b":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"serum_sodium\", color=\"DEATH_EVENT\", marginal=\"violin\",hover_data=dataset.columns,\n                   title =\"Distribution of SERUM SODIUM Vs DEATH_EVENT\", \n                   labels={\"serum_sodium\": \"SERUM SODIUM\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","0494eb18":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"sex\"]==1)]\nd2 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"sex\"]==1)]\nd3 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"sex\"]==0)]\nd4 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"sex\"]==0)]\n\nlabel1 = [\"Male\",\"Female\"]\nlabel2 = ['Male - Survived','Male - Died', \"Female -  Survived\", \"Female - Died\"]\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"GENDER\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"GENDER VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"GENDER DISTRIBUTION IN THE DATASET  \\\n                   GENDER VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='GENDER', x=0.19, y=0.5, font_size=10, showarrow=False),\n                 dict(text='GENDER VS DEATH_EVENT', x=0.84, y=0.5, font_size=9, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\n\nfig.show()","2a5075b3":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"diabetes\"]==0)]\nd2 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"diabetes\"]==1)]\nd3 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"diabetes\"]==0)]\nd4 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"diabetes\"]==1)]\n\nlabel1 = [\"No Diabetes\",\"Diabetes\"]\nlabel2 = ['No Diabetes - Survived','Diabetes - Survived', \"No Diabetes -  Died\", \"Diabetes  - Died\"]\nvalues1 = [(len(d1)+len(d3)), (len(d2)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"DIABETES\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"DIABETES VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"DIABETES DISTRIBUTION IN THE DATASET \\\n                  DIABETES VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='DIABETES', x=0.20, y=0.5, font_size=10, showarrow=False),\n                 dict(text='DIABETES VS DEATH_EVENT', x=0.84, y=0.5, font_size=8, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\nfig.show()","05408738":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"anaemia\"]==0)]\nd2 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"anaemia\"]==0)]\nd3 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"anaemia\"]==1)]\nd4 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"anaemia\"]==1)]\n\nlabel1 = [\"No Anaemia\",\"Anaemia\"]\nlabel2 = ['No Anaemia - Survived','No Anaemia - Died', \"Anaemia -  Survived\", \"Anaemia  - Died\"]\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"ANAEMIA\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"ANAEMIA VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"ANAEMIA DISTRIBUTION IN THE DATASET \\\n                  ANAEMIA VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='ANAEMIA', x=0.20, y=0.5, font_size=10, showarrow=False),\n                 dict(text='ANAEMIA VS DEATH_EVENT', x=0.84, y=0.5, font_size=8, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\nfig.show()","53e6d3b8":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"high_blood_pressure\"]==0)]\nd2 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"high_blood_pressure\"]==0)]\nd3 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"high_blood_pressure\"]==1)]\nd4 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"high_blood_pressure\"]==1)]\n\nlabel1 = [\"No High BP\",\"High BP\"]\nlabel2 = ['No High BP - Survived','No High BP - Died', \"High BP -  Survived\", \"High BP  - Died\"]\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"HIGH BP\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"HIGH BP VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"HIGH BP DISTRIBUTION IN THE DATASET \\\n                  HIGH BP VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='HIGH BP', x=0.20, y=0.5, font_size=10, showarrow=False),\n                 dict(text='HIGH BP VS DEATH_EVENT', x=0.84, y=0.5, font_size=8, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\nfig.show()","08139cf3":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nd1 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"smoking\"]==0)]\nd2 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"smoking\"]==0)]\nd3 = dataset[(dataset[\"DEATH_EVENT\"]==0) & (dataset[\"smoking\"]==1)]\nd4 = dataset[(dataset[\"DEATH_EVENT\"]==1) & (dataset[\"smoking\"]==1)]\n\nlabel1 = [\"No Smoking\",\"Smoking\"]\nlabel2 = ['No Smoking - Survived','No Smoking - Died', \"Smoking - Survived\", \"Smoking - Died\"]\nvalues1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]\nvalues2 = [len(d1),len(d2),len(d3),len(d4)]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=label1, values=values1, name=\"SMOKING\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=label2, values=values2, name=\"SMOKING VS DEATH_EVENT\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent\")\n\nfig.update_layout(\n    title_text=\"SMOKING DISTRIBUTION IN THE DATASET \\\n                  SMOKING VS DEATH_EVENT\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='SMOKING', x=0.20, y=0.5, font_size=10, showarrow=False),\n                 dict(text='SMOKING VS DEATH_EVENT', x=0.84, y=0.5, font_size=8, showarrow=False)],\n    autosize=False,width=1200, height=500, paper_bgcolor=\"white\")\nfig.show()","9bd58062":"# \"Distribution of AGE Vs DIABETES\"\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"age\", color=\"diabetes\", marginal=\"violin\",hover_data=dataset.columns,\n                   title =\"Distribution of AGE Vs DIABETES\", \n                   labels={\"diabetes\": \"DIABETES\", \"age\": \"AGE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","522918fc":"# \"Distribution of AGE Vs ANAEMIA\"\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"age\", color=\"anaemia\", marginal=\"violin\",hover_data=dataset.columns,\n                   title =\"Distribution of AGE Vs ANAEMIA\", \n                   labels={\"anaemia\": \"ANAEMIA\", \"age\": \"AGE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","8188fb5c":"# \"Distribution of AGE Vs HIGH BLOOD PRESSURE\"\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"age\", color=\"high_blood_pressure\", marginal=\"violin\",hover_data=dataset.columns,\n                   title =\"Distribution of AGE Vs HIGH BLOOD PRESSURE\", \n                   labels={\"high_blood_pressure\": \"HIGH BLOOD PRESSURE\", \"age\": \"AGE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","7927b28e":"# \"Distribution of AGE Vs SMOKING\"\nimport plotly.express as px\nfig = px.histogram(dataset, x=\"age\", color=\"smoking\", marginal=\"violin\",hover_data=dataset.columns,\n                   title =\"Distribution of AGE Vs SMOKING\", \n                   labels={\"smoking\": \"SMOKING\", \"age\": \"AGE\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"RebeccaPurple\", \"1\": \"MediumPurple\"})\nfig.show()","58c9719b":"x = dataset.iloc[:, [4,7,11]].values\ny = dataset.iloc[:,-1].values","009e8b00":"print(x)","aac4976e":"print(y)","67d2e9c4":"# Splitting the dataset into training set and test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state =0)","a5fa964c":"print(x_train)","586fb404":"print(y_test)","741808c9":"# Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","0374202d":"# Applying logistic regression on the training set\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(x_train, y_train)","5cfef9f8":"# Predicting the test set\n\ny_pred = classifier.predict(x_test)","ba6d2ebd":"# Making Confusion Matrix and calculating accuracy score\n\nmylist = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","c2d11f36":"# Finding the optimum number of neighbors \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,10):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10)), list1)\nplt.show()","335f563d":"# Training the K Nearest Neighbor Classifier on the Training set\n\nclassifier = KNeighborsClassifier(n_neighbors=6)\nclassifier.fit(x_train, y_train)","6e953659":"# Predicting the Test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","954d7368":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","6ebbdda8":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    classifier = SVC(C = c, random_state=0, kernel = 'rbf')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","a7f01e54":"# Training the Support Vector Classifier on the Training set\n\nfrom sklearn.svm import SVC\nclassifier = SVC(C = 0.6, random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)","7df676ea":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","0826e57c":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","5ce1d6e7":"# Finding the optimum number of max_leaf_nodes\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,10):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,10)), list1)\nplt.show()","13da2e1c":"# Training the Decision Tree Classifier on the Training set\n\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 3, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","034946df":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","201bf021":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","47b8f580":"#Finding the optimum number of n_estimators\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30)), list1)\nplt.show()","a49c4213":"# Training the RandomForest Classifier on the Training set\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 11, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)","bc586e12":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","49c1e425":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","562e826c":"np.random.seed(0)\nimport tensorflow as tf\n\n# Initialising the ANN\n\nann = tf.keras.models.Sequential()","b7bf57ab":"# Adding the input layer and the first hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","a3246463":"# Adding the second hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","1aaf8e9b":"# Adding the third hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","b8c0ccf2":"# Adding the fourth hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))","279b946e":"# Adding the output layer\n\nann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))","17668575":"# Compiling the ANN\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )","75062545":"# Training the ANN on the training set\n\nann.fit(x_train, y_train, batch_size = 32, epochs = 100)","b30383ef":"# Predicting the test set results\n\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.5)\nnp.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","5bdd68d8":"# Making the confusion matrix, calculating accuracy_score \n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint()\n\n# accuracy\nac = accuracy_score(y_test,y_pred)\nprint(\"Accuracy\")\nprint(ac)\nmylist.append(ac)","59bdf66d":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","1f82e649":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10, max_depth=12, subsample=0.7)\nclassifier.fit(x_train,y_train)","c231bdb9":"y_pred = classifier.predict(x_test)\nprint(y_pred)","c6a60aff":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","348e26ef":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train)","5b99be2d":"y_pred = classifier.predict(x_test)\nprint(y_pred)","65c3abc2":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","9221a0fd":"# Plotting accuracy score of different models\nmylist","5693ba7c":"mylist2 = [\"Logistic Regression\", \"KNearestNeighbours\",\"SupportVector\",\"DecisionTree\",\"RandomForest\",\"ANN\", \"XGBOOST\",\"CATBOOST\"]","bb87abc2":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","ea843fda":"2. K NEAREST NEIGHBOR","9cdcda80":"3. SUPPORT VECTOR CLASSIFIER","3756cd8c":"Finding outliers","7220bc76":"-----","dbf4e354":"----","5a433a41":"<div class=\"alert alert-block alert-info\">\n<b>Insight:<\/b>From the above subplot we can conclude that in our dataset 56.9% are NON ANAEMIC (out of which 40.1% survived and 16.8% died) and 43.1% are ANAEMIC (out of which 27.9% survived and 15.2% died).\n<\/div>","fc63156b":"-----","f94266a3":"----","19e6227a":"----","cdbd7be3":"<div class=\"alert alert-block alert-info\">\n<b>Insight:<\/b>From the above subplot we can conclude that in our dataset 67.7% do not SMOKE (out of which 45.8% survived and 21.9% died) and 32.3% do SMOKE (out of which 22.2% survived and 10.1% died).\n<\/div>","9c0ee16f":"-----","49a06b67":"----","64e990af":"----","45911b8f":"<div class=\"alert alert-block alert-info\">\n<b>Insight:<\/b>From the above subplot we can conclude that in our dataset 65% do not have HIGH BLOOD PRESSURE (out of which 45.8% survived and 19.2% died) and 35% have HIGH BLOOD PRESSURE (out of which 22.2% survived and 12.8% died).\n<\/div>","217998f0":"-----","3d87746a":"----","5c6a5143":"<div class=\"alert alert-block alert-info\">\nWider sections of the violin plot represent a higher probability of observations taking a given value, the thinner sections correspond \nto a lower probability and the value of probability is given by kde value for given x\n<\/div>","64b4304e":"-----","363adcf0":"---","f8602be3":"5. RANDOM FOREST CLASSIFCATION","dadfde11":"1. LOGISTIC REGRESSION","bc81337b":"<div class=\"alert alert-block alert-info\">\n<b>Insight:<\/b> From the above subplot we can conclude that in our dataset 65.3% are MALE (out of which 44.4% survived and 20.9% died) and 34.7% are FEMALE (out of which 23.6% survived and 11.1% died).\n<\/div>","40e66cb2":"6. ANN","b193ff01":"4. DECISION TREE CLASSIFIER","fb45e193":"----","b94c0613":"----","6d81aff4":"-----","2f75455d":"----","5958f8ca":"<div class=\"alert alert-block alert-info\">\n<b>Insight:<\/b>From the above subplot we can conclude that in our dataset 57.9% are NON DIABETIC (out of which 39.4% survived and 18.5% died) and 42.1% are DIABETIC (out of which 28.6% survived and 13.5% died).\n<\/div>","0a4553ac":"-----","7ed2b3e5":"LETS GET SOME INSIGHTS OF THE DATASET"}}