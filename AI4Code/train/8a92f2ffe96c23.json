{"cell_type":{"7df7800f":"code","59ac2555":"code","098e85bb":"code","0f21b24c":"code","a4d3b58f":"code","0c10864a":"code","122b587a":"code","7c14d872":"code","b8d85a81":"code","f9e3449a":"code","83839949":"code","693d82b9":"code","8f70d7f2":"code","462886ce":"code","2bf6e1e5":"code","ca9a3d55":"code","bfe38cbc":"code","5ec7e979":"code","1d177799":"code","b8eff162":"code","fda3c5ba":"code","42bd47f0":"code","f4dc39fc":"code","4435ddb0":"code","81e46e8b":"code","eed1c2d5":"code","b0414dc3":"code","f12643dc":"code","866f9907":"code","16aa191b":"code","015c6cb3":"code","0de11b75":"markdown","d70c6508":"markdown","6a530e2f":"markdown","fa983355":"markdown","a04948b1":"markdown","f657327f":"markdown","33f5efab":"markdown","2ab2e5ee":"markdown","27bf93d2":"markdown","58957634":"markdown","3e8d2c2a":"markdown","65721c5c":"markdown","808dd204":"markdown","a05ef34b":"markdown","0a6cfa03":"markdown","61d98573":"markdown","a1dba4ed":"markdown","2a0dfefe":"markdown","51192f2e":"markdown","8921d351":"markdown","6386d357":"markdown","444a7bb4":"markdown"},"source":{"7df7800f":"## Import Libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nfrom scipy.stats import norm, skew\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","59ac2555":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n","098e85bb":"train.shape , test.shape ","0f21b24c":"train.head()","a4d3b58f":"train.info()","0c10864a":"print(train['SalePrice'].describe())\n","122b587a":" sns.distplot(train['SalePrice'])","7c14d872":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice'], fit=norm);\n","b8d85a81":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(corrmat, vmax=.8, square=True);","f9e3449a":"corr = train.corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[highest_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","83839949":"corr[\"SalePrice\"].sort_values(ascending=False)\n","693d82b9":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols])","8f70d7f2":"y_train = train['SalePrice']\ntest_id = test['Id']\nall_data = pd.concat([train, test], axis=0, sort=False)\nall_data = all_data.drop(['Id', 'SalePrice'], axis=1)\n","462886ce":"Total = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum() \/ all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)\n","2bf6e1e5":"all_data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)\nprint(all_data.isnull().sum().max())","ca9a3d55":"total = all_data.isnull().sum().sort_values(ascending=False)\ntotal.head(19)","bfe38cbc":"# filling the numeric data\nnumeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageArea',\n                  'GarageCars']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)","5ec7e979":"#filling categorical data\ncategorical_missed = ['Exterior1st',\n                  'Exterior2nd',\n                  'SaleType',\n                  'MSZoning',\n                   'Electrical',\n                     'KitchenQual']\n\nfor feature in categorical_missed:\n    all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])\n","1d177799":"#Fill in the remaining missing values with the values that are most common for this feature.\n\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\n","b8eff162":"all_data.drop(['Utilities'], axis=1, inplace=True)\n","fda3c5ba":"all_data.isnull().sum().max() #just checking that there's no missing data missing...\n","42bd47f0":"numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew\n","f4dc39fc":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])\n","4435ddb0":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","81e46e8b":"all_data = pd.get_dummies(all_data)\nall_data.head()","eed1c2d5":"x_train =all_data[:len(y_train)]\nx_test = all_data[len(y_train):]\n","b0414dc3":"x_test.shape , x_train.shape","f12643dc":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(x_train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_train, y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_test, y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\n","866f9907":"import xgboost as XGB\n\nthe_model = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, nthread = -1)\nthe_model.fit(x_train, y_train)\n","16aa191b":"y_predict = np.floor(np.expm1(the_model.predict(x_test)))\ny_predict\n","015c6cb3":"sub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('mysubmission.csv',index=False)","0de11b75":"#### What we note?\n* It's important to know what you do and how benefit from it. We can see 'OverQual' in the top of highest correlation it's 0.79!\n* 'GarageCars' & 'GarageArea' like each other (correlation between them is 0.88) \n* 'TotalBsmtSF' & '1stFlrSF' also like each other (correlation betwwen them is 0.82), so we can keep either one of them or add the1stFlrSF to the Toltal.\n* 'TotRmsAbvGrd' & 'GrLivArea' also has a strong correlation (0.83), I decided to keep 'GrLivArea' because it's correlation with 'SalePrice' is higher.\n","d70c6508":"#### We cleaned the data very well, and now let's separate the data to its origin (train, test)","6a530e2f":"## 5. Apply ML Model","fa983355":"Ok, now as you see the correlation between features.. The colours show to us the strong and weak correlation.\nBut what we really need? we need the highest correlation between features and SalesPrice, so let's do it.","a04948b1":"As we see, we have a positive skew, we must fix it.","f657327f":"## 4. Feature Engineering","33f5efab":"Obviously we have some features that have many missing values. we'll deal with it in a little while.\n","2ab2e5ee":"### 2. Let's know more about the Target and make some analysis\nYou may wonder what the target is? \nIt's the 'SalePrice' column. ","27bf93d2":"let's check if we have another missing values.","58957634":"## 2. Looking for Missing Data ","3e8d2c2a":"Let's show the features and the number of Missing values","65721c5c":"#### Fix The Skewness in the other features\n","808dd204":"#### ok let's focus on the features that have highest correlation.","a05ef34b":"## What we want?\n1. Gathering Data\n2. Analysis the target and understand what is the important features\n3. Looking for missing values\n4. Feature Engineering\n5. Converting categorical to numerical\n6. Modeling","0a6cfa03":"## 1. Gathering Data","61d98573":"## Introduction: \nThis is my second work of machine learning in kaggle. In This kernel I will go to solve House Pricing with Advanced Regression Analysis.\nIf there are any recommendations or changes you would like to see in my notebook, please leave a comment at the end of this kernel, I will be glad to answer any questions you may have in the comments. If you like this notebook, Please UPVOTE.","a1dba4ed":"Well, if we look at these features that have many missing values, we will note that they are not important features, none of them has (correlation > 0.5), so if we delete them we will not miss the data.","2a0dfefe":"#### Before looking for Missing data: \nWe can concatenate train and test datasets, preprocess, and then divide them again. I think it will be easy for us.","51192f2e":"## 5. Converting the categorical to numerical.","8921d351":"Now, We explored the data and know the important features.","6386d357":"### Now, Filling the missing Data","444a7bb4":"#### Let's add a new features"}}