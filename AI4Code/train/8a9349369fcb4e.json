{"cell_type":{"1c7dc0ea":"code","a890a1ff":"code","886e0e2f":"code","85c7372a":"code","895750cf":"code","2e3b305f":"code","d24b682c":"code","c3c42a3c":"code","6eceac83":"code","01586416":"code","5cce3b09":"code","c480db46":"code","10646e24":"code","93fdb987":"code","15c0d004":"code","d547133c":"code","3c66ad23":"code","7cbca651":"code","d9c32520":"code","e52cdf04":"code","36b55d74":"code","ca790f97":"markdown","70bbfc1d":"markdown","c04b55da":"markdown","43d82203":"markdown","78238cba":"markdown","847eeac2":"markdown","5f5b6135":"markdown","ea97a92d":"markdown","92a89f7c":"markdown","bb799684":"markdown","af263a4b":"markdown","8a7beb1d":"markdown","df9439f7":"markdown","a636f373":"markdown","ed2bc734":"markdown"},"source":{"1c7dc0ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a890a1ff":"# Check the versions of libraries\n\n# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","886e0e2f":"# Load libraries\nfrom pandas import read_csv\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","85c7372a":"# Load Kaggle filename\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","895750cf":"# Load dataset\nurl = \"\/kaggle\/input\/iris\/Iris.csv\"\ndataset = read_csv(url)","2e3b305f":"# shape\n# it has 150 instances and 5 attributes\nprint(dataset.shape)","d24b682c":"# head\n# print the first 25 rows of the data\nprint(dataset.head(25))","c3c42a3c":"# tail\n# print the last 25 rows of the data\nprint(dataset.tail(25))","6eceac83":"# descriptions\n# use iloc for data selection in Python Pandas. In this case, select all rows and all columns except 'Id'\n# print descriptions\nprint(dataset.iloc[:,1:].describe())","01586416":"# First, create a dataset backup\ndataset_bak = dataset","5cce3b09":"# Remove first column - Id\ndataset = dataset.drop('Id',axis=1)\n# Head\nprint(dataset.head(20))","c480db46":"# Change column names\ndataset.columns = ['Sepal-length', 'Sepal-width', 'Petal-length', 'Petal-width', 'Species']\nprint(dataset.head(20))","10646e24":"# Class distribution, to see the number of rows that belong to each species\nprint(dataset.groupby('Species').size())","93fdb987":"# Box and whisker plots. Univariate plots, one for each individual variable\nfig=plt.figure(figsize=(10,5), dpi=100, facecolor='w', edgecolor='k')\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\nplt.show()","15c0d004":"# Histograms. Create a histogram of each input variable to get an idea of the distribution\ndataset.hist()\nplt.show()","d547133c":"# Scatter plot matrix. See all pairs of attributtes, to detect correlations or relationships\nscatter_matrix(dataset)\nplt.show()","3c66ad23":"# Split-out validation dataset\narray = dataset.values\n# All rows and colums except species column\nX = array[:,0:4]\n# Species column\ny = array[:,4]\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)","7cbca651":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","d9c32520":"# Compare algorithms\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","e52cdf04":"# Make predictions on validation dataset\nmodel = SVC(gamma='auto')\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_validation)","36b55d74":"# Evaluate predictions by comparing them to the expected results in the validation set\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","ca790f97":"- Support Vector Machines (SVM) has the largest estimated accuracy score: ~ 98%\n- Linear Discriminant Analysis (LDA) has the second one: ~ 97%","70bbfc1d":"## 6. Evaluate Some Algorithms","c04b55da":"### 6.2. Build models and evaluate them","43d82203":"## 2. Import Libraries","78238cba":"## 1. Data Set Information:\n\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\n![](https:\/\/archive.ics.uci.edu\/ml\/assets\/MLimages\/Large53.jpg)\n","847eeac2":"## 7. Making predictions and evaluate them","5f5b6135":"### 5.2. Multivariate Plots","ea97a92d":"## Load Dataset\n\nLoading .csv dataset from url provided by kaggle, using Pandas","92a89f7c":"## 4. Data cleansing","bb799684":"### 6.1. Create a validation dataset","af263a4b":"### 5.1. Univariate Plots","8a7beb1d":"- We need to know that the model we created is good.\n- Use statistical methods to estimate the accuray of the models on unseen data: split the dataset in two, 80% to train, evaluate and select among our models and 20% hold back as a validation dataset","df9439f7":"We will use stratified 10-fold cross validation to estimate model accuracy: split the dataset in 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits\n\nTest 6 different algorithms:\n\n1. Logistic Regression (LR)\n2. Linear Discriminant Analysis (LDA)\n3. K-Nearest Neighbors (KNN)\n4. Classification and Regression Trees (CART)\n5. Gaussian Naive Bayes (NB)\n6. Support Vector Machines (SVM)","a636f373":"## 5. Data Visualization","ed2bc734":"## 3. Sumarize the Dataset"}}